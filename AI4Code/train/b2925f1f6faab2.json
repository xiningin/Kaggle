{"cell_type":{"1b84888a":"code","ae77a1e3":"code","4f428e44":"code","ab5f5874":"code","43f22f7d":"code","7b35627d":"code","7976c96c":"code","ced770cd":"code","b3b05c27":"code","66499319":"code","162a2562":"code","98abc87d":"code","a94d994e":"code","ab912c33":"code","3a3adb83":"code","48b90e08":"code","9fc59d88":"code","4d6945b7":"code","de0dbf96":"code","7a91875b":"code","612c8836":"code","5d751018":"code","7b27ca0c":"code","9a2aa8ea":"code","ab274b5b":"code","516a0b2d":"code","94bf5589":"code","961dc4dd":"code","bfec1b85":"code","7b284838":"code","a5c95c4d":"code","11908440":"code","01785d05":"code","f510c4fb":"code","49d9cd5f":"code","47fcd9ae":"code","b07aee94":"code","d27ac94e":"code","5fe49af2":"code","78291476":"code","cd5f3bdf":"code","8bf4a173":"code","c69cd371":"code","ca4bc4d7":"code","a6861ee8":"code","3aa37eaf":"code","2984a586":"code","e7c3f7a0":"code","576cfd78":"code","c179d29d":"code","d310f012":"code","4ba1506f":"code","9c6a2820":"code","c5409870":"code","7e8bcb64":"code","fa8e8de5":"code","5cef9ea4":"code","2c3f90dc":"code","240aa4af":"code","b11c40fc":"code","f94e7cf9":"code","01c87ebe":"code","ee8e658c":"code","c5316bef":"code","3efd2a04":"code","e3c2cf56":"code","f35af08d":"code","1a0708dc":"code","bac6edbe":"code","5833fffc":"code","7d9b5a2a":"code","a040341f":"code","99e048e8":"code","b78b394f":"code","4b9c8c14":"code","c3785205":"code","8fc33b87":"code","0e354772":"code","0fba3aee":"code","1ed795df":"code","b86a2ce6":"code","3424af75":"code","a1d912cf":"code","c77d4fe1":"code","abc517e2":"code","b3be898a":"code","de6f22e1":"code","5c4e4e08":"code","ab2bcb2b":"code","6833361d":"code","d0f238d2":"code","1642fdd4":"code","66bf0f46":"code","deb96438":"code","290c2c86":"code","a4be5f2c":"code","0a51c96f":"code","8ddee0c4":"code","002a754a":"code","9636d633":"code","69a01687":"code","366a8f96":"code","25cd2936":"code","ea389d79":"code","827eeaa3":"code","4b476a15":"code","6780fa22":"code","430221dc":"code","9b933f8d":"code","fecfe667":"code","391a6f31":"code","8444d768":"code","b0f40ce7":"code","b5b4f054":"code","1169b720":"code","435b83c1":"code","cd9f0efd":"code","f92e6323":"code","046db992":"code","f5897d79":"code","2491d33c":"code","94d76fe1":"code","c6dad5ad":"code","02a5acae":"code","b883bb25":"code","1d52ff39":"code","bbca6b46":"code","dfc2cb20":"code","c8e91969":"code","d3f5100d":"code","d36aac2e":"code","f6f210c8":"code","6acea853":"code","aea961b7":"code","9834819b":"code","09353865":"code","5a02cced":"code","26049d6f":"markdown","fd3fa1d2":"markdown","0b6c933c":"markdown","2513b993":"markdown","c525bedc":"markdown","64b0819b":"markdown","1fc1f7fa":"markdown","6207c602":"markdown","59fe8778":"markdown","0418b0b4":"markdown","f8664179":"markdown","917f7b79":"markdown","091c1bb6":"markdown","6671f1ad":"markdown","6d225e73":"markdown","1d5670ee":"markdown","0dee950e":"markdown","e42c52f7":"markdown","30dcd9da":"markdown","9e2135bf":"markdown","4d73d609":"markdown","6389b05f":"markdown","05afd1c3":"markdown","f628aa9e":"markdown","9029ee6e":"markdown","358bc5f7":"markdown","5d07cd54":"markdown","c231d264":"markdown","f3cf9570":"markdown","500cc97d":"markdown","7cac1299":"markdown","930ef9cc":"markdown","dd1ac3b7":"markdown","dd74d021":"markdown","b501ed53":"markdown","ed3b31db":"markdown","a28da12b":"markdown","4461152f":"markdown","7f3a6e9c":"markdown","18dddcfb":"markdown","35aa85d6":"markdown","2cab9df0":"markdown","428a80a2":"markdown","b3fe712f":"markdown","3d83d1c0":"markdown","551c92ba":"markdown","bf460491":"markdown","5d0def27":"markdown","5f1e8e64":"markdown","0f3ebe48":"markdown","6cc48e0a":"markdown","ec4450ab":"markdown","dc13a9e9":"markdown","788820af":"markdown","68e94c86":"markdown","169ca913":"markdown","aed59714":"markdown","77663b20":"markdown","026ae14f":"markdown","89ceec5b":"markdown","8361c17f":"markdown","1a4adf7a":"markdown","3dd5d2ff":"markdown","23d33f4a":"markdown","b1daed9b":"markdown","2f0bed71":"markdown","dc5d226a":"markdown","fc9380c8":"markdown","08231fe8":"markdown","a9e9d820":"markdown","c70d9735":"markdown","a538b557":"markdown","6d8782c2":"markdown","896f146a":"markdown","5f8659a0":"markdown","a09a4e30":"markdown","d55f578d":"markdown","68de05ff":"markdown","75084366":"markdown","65d9ac36":"markdown","a72b150a":"markdown","26a16d29":"markdown","5b14c6db":"markdown","f35c14d2":"markdown","e78a84c5":"markdown","eefd53cf":"markdown","76358d35":"markdown","1f5a1f36":"markdown","b7c66a86":"markdown","80018f26":"markdown","5f98d2bd":"markdown","bb5aeabe":"markdown","909ee872":"markdown","b932bef6":"markdown","500695b5":"markdown","4fa01b4f":"markdown","df657d83":"markdown","219b7605":"markdown"},"source":{"1b84888a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n!pip install pycountry-convert\nfrom pycountry_convert import country_name_to_country_alpha2,country_alpha2_to_country_name","ae77a1e3":"data = pd.read_csv(\"..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv\")\ndata","4f428e44":"pd.set_option('display.float',str)\ndata[data.columns[0]][1:].astype('int32').describe()","ab5f5874":"countries = []\nlat = []\nlon = []\nun, counts = np.unique(data.Q3.values[1:],return_counts=True)\nfor i in range(len(un)):\n    try:\n        countries.append(country_name_to_country_alpha2(un[i]))\n    except:\n        pass\n    \ncounts = pd.read_csv(\"..\/input\/locations\/world_country_and_usa_states_latitude_and_longitude_values.csv\")\nfor con in countries:\n    lat.append(counts[counts[\"country_code\"] == con].latitude.values[0])\n    lon.append(counts[counts[\"country_code\"] == con].longitude.values[0])","43f22f7d":"import folium\nfrom folium.plugins import MarkerCluster\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)\nfor i in range(len(lat)):\n        if(lat[i] == 0 ):\n            continue\n        radius=5\n        popup_text = \"\"\"Country : {}\"\"\"\n        popup_text = popup_text.format(country_alpha2_to_country_name(countries[i]))\n        folium.CircleMarker(location = [lat[i], lon[i]] ,radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\nworld_map","7b35627d":"plt.rcParams[\"axes.linewidth\"]  = 0\nplt.rcParams.update({'axes.facecolor':'whitesmoke'})\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nunique,counts = np.unique(data.Q1[1:],return_counts=True)\nplt.title(\"Age group\")\nplt.xlabel(\"Age group\")\nplt.ylabel(\"Count\")\nplt.bar(unique,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='black', linestyle='solid', linewidth=0.1, alpha=0.5,zorder=0)\n\nplt.subplot(1,2,2)\nunique,counts = np.unique(data.Q2[1:],return_counts=True)\nplt.title(\"Gender\")\nplt.ylabel(\"Gender\")\nplt.xlabel(\"Count\")\nplt.barh(unique,counts,zorder=3,color=['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\nplt.figure(figsize=(15,7))\nplt.subplot(1,2,1)\ndata.Q4.replace(\"Bachelor\u2019s degree\",\"BS\",inplace=True)\ndata.Q4.replace(\"Doctoral degree\",\"Phd\",inplace=True)\ndata.Q4.replace(\"Master\u2019s degree\",\"MS\",inplace=True)\ndata.Q4.replace(\"I prefer not to answer\",\"No Answer\",inplace=True)\ndata.Q4.replace(\"No formal education past high school\",\"High School\",inplace=True)\ndata.Q4.replace(\"Some college\/university study without earning a bachelor\u2019s degree\",\"College\",inplace=True)\nunique,counts = np.unique(data.Q4[1:],return_counts=True)\nplt.title(\"Education\")\nplt.xlabel(\"Level\")\nplt.pie(counts,labels=unique)\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\n\nplt.subplot(1,2,2)\ndata.Q6.replace(\"I have never written code\",\"Never\",inplace=True)\nunique,counts = np.unique(data.Q6[1:],return_counts=True)\nplt.title(\"Coding Experience\")\nplt.ylabel(\"Years\")\nplt.xlabel(\"Count\")\nplt.barh(unique,counts,zorder=3,color=['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\nplt.figure(figsize=(15,6))\nunique,counts = np.unique(data.Q5[1:],return_counts=True)\nplt.title(\"Profession\")\nplt.ylabel(\"Type\")\nplt.xlabel(\"Count\")\nplt.barh(unique,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n","7976c96c":"languages_used = data[data.columns[data.columns.get_loc(\"Q7_Part_1\") : data.columns.get_loc(\"Q7_OTHER\") + 1 ]]\nlanguages = []\nfor col in languages_used.columns:\n        languages.append([languages_used[col][0].split(\"-\")[2]])\n        languages.append(languages_used[col][1:].dropna().values)","ced770cd":"languages = list(itertools.chain.from_iterable(languages))\nlanguages = [i.strip() for i in languages]\nlanguages, counts = np.unique(languages,return_counts=True)","b3b05c27":"plt.figure(figsize=(15,5))\nplt.bar(languages,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Programming Language used on Regular basis\")\nplt.xlabel(\"Language\")\nplt.ylabel(\"Count\")\nplt.show()","66499319":"languages_preferred = data.Q8.dropna().to_numpy()[1:]\nlanguages_preferred, counts = np.unique(languages_preferred,return_counts=True)\nplt.figure(figsize=(6,8))\nplt.title(\"Recommended Languages for Aspiring Data Scientists\")\nplt.ylabel(\"Language\")\nplt.xlabel(\"Count\")\nplt.barh(languages_preferred,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.show()","162a2562":"ide_used = data[data.columns[data.columns.get_loc(\"Q9_Part_1\") : data.columns.get_loc(\"Q9_OTHER\") + 1 ]]\nides = []\nfor col in ide_used.columns:\n        ides.append([ide_used[col][0].split(\"-\")[2]])\n        ides.append(ide_used[col][1:].dropna().values)","98abc87d":"ides = list(itertools.chain.from_iterable(ides))\nides = [i.strip() for i in ides]\nides, counts = np.unique(ides,return_counts=True)","a94d994e":"ides = ides[1:]\nides[0] = \"Jupyter\"\nides[-1] = \"VS Code\"\ncounts[1] += counts[0]\ncounts = counts[1:]","ab912c33":"plt.figure(figsize=(18,5))\nplt.bar(ides,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"IDEs used on Regular basis\")\nplt.xlabel(\"IDE\")\nplt.ylabel(\"Count\")\nplt.show()","3a3adb83":"notebooks_used = data[data.columns[data.columns.get_loc(\"Q10_Part_1\") : data.columns.get_loc(\"Q10_OTHER\") + 1 ]]\nnotebooks = []\nfor col in notebooks_used.columns:\n        notebooks.append([notebooks_used[col][0].split(\"-\")[2]])\n        notebooks.append(notebooks_used[col][1:].dropna().values)","48b90e08":"notebooks = list(itertools.chain.from_iterable(notebooks))\nnotebooks = [i.strip() for i in notebooks]\nnotebooks, counts = np.unique(notebooks,return_counts=True)","9fc59d88":"plt.figure(figsize=(10,7))\nplt.barh(notebooks,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Notebook Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","4d6945b7":"platform = data.Q11.dropna().values[1:]\nplatform, counts = np.unique(platform,return_counts=True)","de0dbf96":"plt.figure(figsize=(8,8))\nplt.pie(counts,labels=platform)\nplt.title(\"Platform used for Data Science Projects\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\nplt.show()","7a91875b":"hardware_used = data[data.columns[data.columns.get_loc(\"Q12_Part_1\") : data.columns.get_loc(\"Q12_OTHER\") + 1 ]]\nhardwares = []\nfor col in hardware_used.columns:\n        hardwares.append([hardware_used[col][0].split(\"-\")[2]])\n        hardwares.append(hardware_used[col][1:].dropna().values)","612c8836":"hardwares = list(itertools.chain.from_iterable(hardwares))\nhardwares = [i.strip() for i in hardwares]\nhardwares, counts = np.unique(hardwares,return_counts=True)","5d751018":"plt.figure(figsize=(15,5))\nplt.bar(hardwares,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Specialized hardware used on Regular basis\")\nplt.xlabel(\"Hardware\")\nplt.ylabel(\"Count\")\nplt.show()","7b27ca0c":"tpu = data.Q13.dropna().values[1:]\ntpu, counts = np.unique(tpu,return_counts=True)","9a2aa8ea":"plt.figure(figsize=(10,7))\nplt.bar(tpu,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Usage of TPU\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Count\")\nplt.show()","ab274b5b":"vis_used = data[data.columns[data.columns.get_loc(\"Q14_Part_1\") : data.columns.get_loc(\"Q14_OTHER\") + 1 ]]\ntools = []\nfor col in vis_used.columns:\n        tools.append([vis_used[col][0].split(\"-\")[2]])\n        tools.append(vis_used[col][1:].dropna().values)","516a0b2d":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","94bf5589":"plt.figure(figsize=(5,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Visualization tools used on Regular basis\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","961dc4dd":"years = data.Q15.dropna().values[1:]\nyears, counts = np.unique(years,return_counts=True)","bfec1b85":"plt.figure(figsize=(5,7))\nplt.barh(years,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Years of using Machine Learning Methods\")\nplt.ylabel(\"Experience\")\nplt.xlabel(\"Count\")\nplt.show()","7b284838":"frameworks_used = data[data.columns[data.columns.get_loc(\"Q16_Part_1\") : data.columns.get_loc(\"Q16_OTHER\") + 1 ]]\nframeworks = []\nfor col in frameworks_used.columns:\n        frameworks.append([frameworks_used[col][0].split(\"-\")[2]])\n        frameworks.append(frameworks_used[col][1:].dropna().values)","a5c95c4d":"frameworks = list(itertools.chain.from_iterable(frameworks))\nframeworks = [i.strip() for i in frameworks]\nframeworks, counts = np.unique(frameworks,return_counts=True)","11908440":"frameworks = np.delete(frameworks,14)\ncounts[15] = counts[15] + counts[14]\ncounts = np.delete(counts,14)","01785d05":"plt.figure(figsize=(5,9))\nplt.barh(frameworks,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"1\")\nplt.ylabel(\"Framework\")\nplt.xlabel(\"Count\")\nplt.show()","f510c4fb":"algo_used = data[data.columns[data.columns.get_loc(\"Q17_Part_1\") : data.columns.get_loc(\"Q17_OTHER\") + 1 ]]\nalgos = []\nfor col in algo_used.columns:\n        algos.append([algo_used[col][0].split(\"-\")[2]])\n        algos.append(algo_used[col][1:].dropna().values)","49d9cd5f":"algos = list(itertools.chain.from_iterable(algos))\nalgos = [i.strip() for i in algos]\nalgos, counts = np.unique(algos,return_counts=True)\n\ncounts[0] += counts[1]\nalgos = np.delete(algos,len(algos) - 2)\ncounts = np.delete(counts,len(counts) -2)","47fcd9ae":"plt.figure(figsize=(5,9))\nplt.barh(algos,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"ML Algorithms used on Regular Basis\")\nplt.ylabel(\"Algorithm\")\nplt.xlabel(\"Count\")\nplt.show()","b07aee94":"computer_vision_methods_used = data[data.columns[data.columns.get_loc(\"Q18_Part_1\") : data.columns.get_loc(\"Q18_OTHER\") + 1 ]]\nmethods = []\nfor col in computer_vision_methods_used.columns:\n        methods.append([computer_vision_methods_used[col][0].split(\"-\")[2]])\n        methods.append(computer_vision_methods_used[col][1:].dropna().values)","d27ac94e":"methods = list(itertools.chain.from_iterable(methods))\nmethods = [i.strip() for i in methods]\nmethods, counts = np.unique(methods,return_counts=True)\n","5fe49af2":"methods = np.delete(methods,3)\ncounts[4] = counts[4] + counts[3]\ncounts = np.delete(counts,3)","78291476":"plt.figure(figsize=(5,9))\nplt.barh(methods,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Computer Vision methods used on Regular Basis\")\nplt.ylabel(\"Method\")\nplt.xlabel(\"Count\")\nplt.show()","cd5f3bdf":"nlp_methods_used = data[data.columns[data.columns.get_loc(\"Q19_Part_1\") : data.columns.get_loc(\"Q19_OTHER\") + 1 ]]\nmethods = []\nfor col in nlp_methods_used.columns:\n        methods.append([nlp_methods_used[col][0].split(\"-\")[2]])\n        methods.append(nlp_methods_used[col][1:].dropna().values)","8bf4a173":"methods = list(itertools.chain.from_iterable(methods))\nmethods = [i.strip() for i in methods]\nmethods, counts = np.unique(methods,return_counts=True)","c69cd371":"methods = np.delete(methods,5)\ncounts[6] = counts[6] + counts[5]\ncounts = np.delete(counts,5)\n\nmethods = np.delete(methods,1)\ncounts[2] = counts[1] + counts[2]\ncounts = np.delete(counts,1)","ca4bc4d7":"plt.figure(figsize=(5,9))\nplt.barh(methods,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"NLP methods used on Regular Basis\")\nplt.ylabel(\"Method\")\nplt.xlabel(\"Count\")\nplt.show()","a6861ee8":"employer_ind = data.Q20.dropna().values[1:]\nemployer_ind, counts = np.unique(employer_ind,return_counts=True)","3aa37eaf":"plt.figure(figsize=(5,9))\nplt.barh(employer_ind,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Employer Industry\")\nplt.ylabel(\"Industry\")\nplt.xlabel(\"Count\")\nplt.show()","2984a586":"employer_size = data.Q21.dropna().values[1:]\nemployer_size, counts = np.unique(employer_size,return_counts=True)","e7c3f7a0":"plt.figure(figsize=(5,9))\nplt.barh(employer_size,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Employer Size\")\nplt.ylabel(\"Size\")\nplt.xlabel(\"Count\")\nplt.show()","576cfd78":"data_science_count = data.Q22.dropna().values[1:]\ndata_science_count, counts = np.unique(data_science_count,return_counts=True)","c179d29d":"plt.figure(figsize=(9,5))\nplt.bar(data_science_count,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Number of Individuals responsible for Data Science Workload\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Count\")\nplt.show()","d310f012":"plt.figure(figsize=(15,9))\nplt.subplot(2,2,1)\nun,counts = np.unique(data[data.Q21 == \"0-49 employees\"].Q22.dropna().values,return_counts=True)\nplt.title(\"Small Scale Industries\")\nplt.xlabel(\"Data Science Team Count\")\nplt.ylabel(\"Count\")\nplt.bar(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n\nplt.subplot(2,2,2)\nun,counts = np.unique(data[(data.Q21 == \"50-249 employees\") | (data.Q21 == \"250-999 employees\")].Q22.dropna().values,return_counts=True)\nplt.title(\"Medium Scale Industries\")\nplt.xlabel(\"Data Science Team Count\")\nplt.ylabel(\"Count\")\nplt.bar(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n\nplt.subplot(2,2,3)\nun,counts = np.unique(data[(data.Q21 == \"1000-9,999 employees\") | (data.Q21 == \"10000 or more employees\")].Q22.dropna().values,return_counts=True)\nplt.title(\"Large Scale Industries\")\nplt.xlabel(\"Data Science Team Count\")\nplt.ylabel(\"Count\")\nplt.bar(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n\nplt.tight_layout()","4ba1506f":"ML_business = data.Q23.dropna().values[1:]\nML_business, counts = np.unique(ML_business,return_counts=True)","9c6a2820":"plt.figure(figsize=(9,5))\nplt.barh(ML_business,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Use of ML by Current Employer\")\nplt.ylabel(\"Use\")\nplt.xlabel(\"Count\")\nplt.show()","c5409870":"activities_work = data[data.columns[data.columns.get_loc(\"Q24_Part_1\") : data.columns.get_loc(\"Q24_OTHER\") + 1 ]]\nactivities = []\nfor col in activities_work.columns:\n        activities.append([activities_work[col][0].split(\"-\")[2]])\n        activities.append(activities_work[col][1:].dropna().values)","7e8bcb64":"activities = list(itertools.chain.from_iterable(activities))\nactivities = [i.strip() for i in activities]\nactivities, counts = np.unique(activities,return_counts=True)","fa8e8de5":"plt.figure(figsize=(9,5))\nplt.barh(activities,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Major Activity at work\")\nplt.ylabel(\"Activity\")\nplt.xlabel(\"Count\")\nplt.show()","5cef9ea4":"compensation = data.Q25.dropna().values[1:]\ncompensation, counts = np.unique(compensation,return_counts=True)","2c3f90dc":"plt.figure(figsize=(9,10))\nplt.barh(compensation,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Current Yearly Compensation\")\nplt.ylabel(\"Compensation Range\")\nplt.xlabel(\"Count\")\nplt.show()","240aa4af":"money_spent = data.Q26.dropna().values[1:]\nmoney_spent, counts = np.unique(money_spent,return_counts=True)","b11c40fc":"plt.figure(figsize=(15,5))\nplt.bar(money_spent,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Money spent on ML \/ Cloud Computing Services at Home or at Work in past 5 years\")\nplt.xlabel(\"Money Spent\")\nplt.ylabel(\"Count\")\nplt.show()","f94e7cf9":"plt.figure(figsize=(20,30))\nunique,counts = np.unique(data.Q26.dropna().values[1:],return_counts=True)\nfor i in range(len(unique)):\n    plt.subplot(len(unique),2,i+1)\n    un,counts = np.unique(data[data.Q26 == unique[i]].Q5.dropna().values,return_counts=True)\n    plt.title(unique[i])\n    plt.xlabel(\"Count\")\n    plt.ylabel(\"Profession\")\n    plt.barh(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\n    plt.tick_params(bottom=False,left=False)\n    plt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()","01c87ebe":"platforms_used = data[data.columns[data.columns.get_loc(\"Q27_A_Part_1\") : data.columns.get_loc(\"Q27_A_OTHER\") + 1 ]]\nplatforms = []\nfor col in platforms_used.columns:\n        platforms.append([platforms_used[col][0].split(\"-\")[2]])\n        platforms.append(platforms_used[col][1:].dropna().values)","ee8e658c":"platforms = list(itertools.chain.from_iterable(platforms))\nplatforms = [i.strip() for i in platforms]\nplatforms, counts = np.unique(platforms,return_counts=True)","c5316bef":"plt.figure(figsize=(6,6))\nplt.pie(counts,labels=platforms)\nplt.title(\"Cloud Computing Platforms used on Regular basis\")\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\nplt.show()","3efd2a04":"platforms_familiar = data[data.columns[data.columns.get_loc(\"Q27_B_Part_1\") : data.columns.get_loc(\"Q27_B_OTHER\") + 1 ]]\nplatforms = []\nfor col in platforms_familiar.columns:\n        platforms.append([platforms_familiar[col][0].split(\"-\")[2]])\n        platforms.append(platforms_familiar[col][1:].dropna().values)","e3c2cf56":"platforms = list(itertools.chain.from_iterable(platforms))\nplatforms = [i.strip() for i in platforms]\nplatforms, counts = np.unique(platforms,return_counts=True)","f35af08d":"plt.figure(figsize=(6,6))\nplt.pie(counts,labels=platforms)\nplt.title(\"     Cloud Computing Platforms preferred to become familiar\")\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\nplt.show()","1a0708dc":"best_platform = data.Q28.dropna().values[1:]\nbest_platform, counts = np.unique(best_platform,return_counts=True)","bac6edbe":"plt.figure(figsize=(15,8))\nplt.barh(best_platform,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Cloud Computing Platforms with best Developer Experience\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.show()","5833fffc":"products_used = data[data.columns[data.columns.get_loc(\"Q29_A_Part_1\") : data.columns.get_loc(\"Q29_A_OTHER\") + 1 ]]\nproducts = []\nfor col in products_used.columns:\n        products.append([products_used[col][0].split(\"-\")[2]])\n        products.append(products_used[col][1:].dropna().values)","7d9b5a2a":"products = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)","a040341f":"plt.figure(figsize=(20,5))\nplt.bar(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Cloud Computing Products used on Regular basis\")\nplt.xlabel(\"Product\")\nplt.ylabel(\"Count\")\nplt.show()","99e048e8":"products_preferred = data[data.columns[data.columns.get_loc(\"Q29_B_Part_1\") : data.columns.get_loc(\"Q29_B_OTHER\") + 1 ]]\nproducts = []\nfor col in products_preferred.columns:\n        products.append([products_preferred[col][0].split(\"-\")[2]])\n        products.append(products_preferred[col][1:].dropna().values)","b78b394f":"products = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)","4b9c8c14":"plt.figure(figsize=(20,5))\nplt.bar(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Cloud Computing Products preferred to become familiar\")\nplt.xlabel(\"Product\")\nplt.ylabel(\"Count\")\nplt.show()","c3785205":"products_used = data[data.columns[data.columns.get_loc(\"Q30_A_Part_1\") : data.columns.get_loc(\"Q30_A_OTHER\") + 1 ]]\nproducts = []\nfor col in products_used.columns:\n        products.append([products_used[col][0].split(\"-\")[2]])\n        products.append(products_used[col][1:].dropna().values)","8fc33b87":"products = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)","0e354772":"plt.figure(figsize=(10,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Storage Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","0fba3aee":"products_preferred = data[data.columns[data.columns.get_loc(\"Q30_B_Part_1\") : data.columns.get_loc(\"Q30_B_OTHER\") + 1 ]]\nproducts = []\nfor col in products_preferred.columns:\n        products.append([products_preferred[col][0].split(\"-\")[2]])\n        products.append(products_preferred[col][1:].dropna().values)","1ed795df":"products = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)","b86a2ce6":"plt.figure(figsize=(10,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Storage Products preferred to become familiar with\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","3424af75":"ml_products_used = data[data.columns[data.columns.get_loc(\"Q31_A_Part_1\") : data.columns.get_loc(\"Q31_A_OTHER\") + 1 ]]\nml_products = []\nfor col in ml_products_used.columns:\n        ml_products.append([ml_products_used[col][0].split(\"-\")[2]])\n        ml_products.append(ml_products_used[col][1:].dropna().values)","a1d912cf":"ml_products = list(itertools.chain.from_iterable(ml_products))\nml_products = [i.strip() for i in ml_products]\nml_products, counts = np.unique(ml_products,return_counts=True)","c77d4fe1":"plt.figure(figsize=(10,7))\nplt.barh(ml_products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Managed ML Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","abc517e2":"ml_products_preferred = data[data.columns[data.columns.get_loc(\"Q31_B_Part_1\") : data.columns.get_loc(\"Q31_B_OTHER\") + 1 ]]\nml_products = []\nfor col in ml_products_preferred.columns:\n        ml_products.append([ml_products_preferred[col][0].split(\"-\")[2]])\n        ml_products.append(ml_products_preferred[col][1:].dropna().values)","b3be898a":"ml_products = list(itertools.chain.from_iterable(ml_products))\nml_products = [i.strip() for i in ml_products]\nml_products, counts = np.unique(ml_products,return_counts=True)","de6f22e1":"plt.figure(figsize=(10,7))\nplt.barh(ml_products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Managed ML Products preferred to get familiar with\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","5c4e4e08":"products_used = data[data.columns[data.columns.get_loc(\"Q32_A_Part_1\") : data.columns.get_loc(\"Q32_A_OTHER\") + 1 ]]\nproducts = []\nfor col in products_used.columns:\n        products.append([products_used[col][0].split(\"-\")[2]])\n        products.append(products_used[col][1:].dropna().values)","ab2bcb2b":"products = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)","6833361d":"plt.figure(figsize=(10,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Data Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","d0f238d2":"products_preferred = data[data.columns[data.columns.get_loc(\"Q32_B_Part_1\") : data.columns.get_loc(\"Q32_B_OTHER\") + 1 ]]\nproducts = []\nfor col in products_preferred.columns:\n        products.append([products_preferred[col][0].split(\"-\")[2]])\n        products.append(products_preferred[col][1:].dropna().values)","1642fdd4":"products = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)","66bf0f46":"plt.figure(figsize=(10,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Data Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","deb96438":"products = data.Q33.dropna().values[1:]\nproducts, counts = np.unique(products,return_counts=True)","290c2c86":"plt.figure(figsize=(10,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Data Products used most often\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.show()","a4be5f2c":"bi_used = data[data.columns[data.columns.get_loc(\"Q34_A_Part_1\") : data.columns.get_loc(\"Q34_A_OTHER\") + 1 ]]\nbi = []\nfor col in bi_used.columns:\n        bi.append([bi_used[col][0].split(\"-\")[2]])\n        bi.append(bi_used[col][1:].dropna().values)","0a51c96f":"bi = list(itertools.chain.from_iterable(bi))\nbi = [i.strip() for i in bi]\nbi, counts = np.unique(bi,return_counts=True)","8ddee0c4":"plt.figure(figsize=(10,7))\nplt.barh(bi,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Intelligence Tools used on Regular basis\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","002a754a":"tools_preferred = data[data.columns[data.columns.get_loc(\"Q34_B_Part_1\") : data.columns.get_loc(\"Q34_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)","9636d633":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","69a01687":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Intelligence Tools preferred to become familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","366a8f96":"bi_used = data.Q35.dropna().values[1:]\nbi_used,counts = np.unique(bi_used,return_counts=True)","25cd2936":"plt.figure(figsize=(10,7))\nplt.barh(bi_used,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Intelligence Tools used most often\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","ea389d79":"tools_used = data[data.columns[data.columns.get_loc(\"Q36_A_Part_1\") : data.columns.get_loc(\"Q36_A_OTHER\") + 1 ]]\ntools = []\nfor col in tools_used.columns:\n        tools.append([tools_used[col][0].split(\"-\")[2]])\n        tools.append(tools_used[col][1:].dropna().values)","827eeaa3":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","4b476a15":"tools = np.delete(tools,4)\ncounts[5] = counts[4] + counts[5]\ncounts = np.delete(counts,4)","6780fa22":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools used most often\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","430221dc":"tools_preferred = data[data.columns[data.columns.get_loc(\"Q36_B_Part_1\") : data.columns.get_loc(\"Q36_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)","9b933f8d":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","fecfe667":"tools = np.delete(tools,4)\ncounts[5] = counts[4] + counts[5]\ncounts = np.delete(counts,4)","391a6f31":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools preferred to get familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","8444d768":"tools_used = data[data.columns[data.columns.get_loc(\"Q37_A_Part_1\") : data.columns.get_loc(\"Q37_A_OTHER\") + 1 ]]\ntools = []\nfor col in tools_used.columns:\n        tools.append([tools_used[col][0].split(\"-\")[2]])\n        tools.append(tools_used[col][1:].dropna().values)","b0f40ce7":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","b5b4f054":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools used on Regular basis\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","1169b720":"tools_preferred = data[data.columns[data.columns.get_loc(\"Q37_B_Part_1\") : data.columns.get_loc(\"Q37_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)","435b83c1":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","cd9f0efd":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools preferred to become familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","f92e6323":"tools_used = data[data.columns[data.columns.get_loc(\"Q38_A_Part_1\") : data.columns.get_loc(\"Q38_A_OTHER\") + 1 ]]\ntools = []\nfor col in tools_used.columns:\n        tools.append([tools_used[col][0].split(\"-\")[2]])\n        tools.append(tools_used[col][1:].dropna().values)","046db992":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","f5897d79":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Tools used to manage Machine Learning Projects\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","2491d33c":"tools_preferred = data[data.columns[data.columns.get_loc(\"Q38_B_Part_1\") : data.columns.get_loc(\"Q38_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)","94d76fe1":"tools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)","c6dad5ad":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Tools preferred to become familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","02a5acae":"share_loc = data[data.columns[data.columns.get_loc(\"Q39_Part_1\") : data.columns.get_loc(\"Q39_OTHER\") + 1 ]]\nshare = []\nfor col in share_loc.columns:\n        share.append([share_loc[col][0].split(\"-\")[2]])\n        share.append(share_loc[col][1:].dropna().values)","b883bb25":"share = list(itertools.chain.from_iterable(share))\nshare = [i.strip() for i in share]\nshare, counts = np.unique(share,return_counts=True)","1d52ff39":"plt.figure(figsize=(10,7))\nplt.barh(share,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Platform used for sharing projects publicly\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.show()","bbca6b46":"platforms_used = data[data.columns[data.columns.get_loc(\"Q40_Part_1\") : data.columns.get_loc(\"Q40_OTHER\") + 1 ]]\nplatform = []\nfor col in platforms_used.columns:\n        platform.append([platforms_used[col][0].split(\"-\")[2]])\n        platform.append(platforms_used[col][1:].dropna().values)","dfc2cb20":"platform = list(itertools.chain.from_iterable(platform))\nplatform = [i.strip() for i in platform]\nplatform, counts = np.unique(platform,return_counts=True)","c8e91969":"platform = np.delete(platform,0)\ncounts[1] = counts[0] + counts[1]\ncounts = np.delete(counts,0)","d3f5100d":"plt.figure(figsize=(10,7))\nplt.barh(platform,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Platform used for learning Data Science\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.show()","d36aac2e":"tools_used = data.Q41.dropna().values[1:]\ntools,counts = np.unique(tools_used,return_counts=True)","f6f210c8":"plt.figure(figsize=(10,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Primary Tool used to Analyze data\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.show()","6acea853":"media_sources = data[data.columns[data.columns.get_loc(\"Q42_Part_1\") : data.columns.get_loc(\"Q42_OTHER\") + 1 ]]\nsources = []\nfor col in media_sources.columns:\n        sources.append([media_sources[col][0].split(\"-\")[2]])\n        sources.append(media_sources[col][1:].dropna().values)","aea961b7":"sources = list(itertools.chain.from_iterable(sources))\nsources = [i.strip() for i in sources]\nsources, counts = np.unique(sources,return_counts=True)","9834819b":"sources = np.delete(sources,3)\ncounts[4] = counts[4] + counts[3]\ncounts = np.delete(counts,3)","09353865":"plt.figure(figsize=(10,7))\nplt.barh(sources,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Media Sources used that report Data Science Topics\")\nplt.ylabel(\"Source\")\nplt.xlabel(\"Count\")\nplt.show()","5a02cced":"from IPython.display import display\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.rcParams[\"axes.linewidth\"]  = 0\nplt.rcParams.update({'axes.facecolor':'whitesmoke'})\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nunique,counts = np.unique(data.Q1[1:],return_counts=True)\nplt.title(\"Age group\")\nplt.xlabel(\"Age group\")\nplt.ylabel(\"Count\")\nplt.bar(unique,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='black', linestyle='solid', linewidth=0.1, alpha=0.5,zorder=0)\n\nplt.subplot(1,2,2)\nunique,counts = np.unique(data.Q2[1:],return_counts=True)\nplt.title(\"Gender\")\nplt.ylabel(\"Gender\")\nplt.xlabel(\"Count\")\nplt.barh(unique,counts,zorder=3,color=['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\nplt.figure(figsize=(15,7))\nplt.subplot(1,2,1)\ndata.Q4.replace(\"Bachelor\u2019s degree\",\"BS\",inplace=True)\ndata.Q4.replace(\"Doctoral degree\",\"Phd\",inplace=True)\ndata.Q4.replace(\"Master\u2019s degree\",\"MS\",inplace=True)\ndata.Q4.replace(\"I prefer not to answer\",\"No Answer\",inplace=True)\ndata.Q4.replace(\"No formal education past high school\",\"High School\",inplace=True)\ndata.Q4.replace(\"Some college\/university study without earning a bachelor\u2019s degree\",\"College\",inplace=True)\nunique,counts = np.unique(data.Q4[1:],return_counts=True)\nplt.title(\"Education\")\nplt.xlabel(\"Level\")\nplt.pie(counts,labels=unique)\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\n\nplt.subplot(1,2,2)\ndata.Q6.replace(\"I have never written code\",\"Never\",inplace=True)\nunique,counts = np.unique(data.Q6[1:],return_counts=True)\nplt.title(\"Coding Experience\")\nplt.ylabel(\"Years\")\nplt.xlabel(\"Count\")\nplt.barh(unique,counts,zorder=3,color=['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\nplt.figure(figsize=(15,6))\nunique,counts = np.unique(data.Q5[1:],return_counts=True)\nplt.title(\"Profession\")\nplt.ylabel(\"Type\")\nplt.xlabel(\"Count\")\nplt.barh(unique,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\ncountries = []\nlat = []\nlon = []\nun, counts = np.unique(data.Q3.values[1:],return_counts=True)\nfor i in range(len(un)):\n    try:\n        countries.append(country_name_to_country_alpha2(un[i]))\n    except:\n        pass\n    \ncounts = pd.read_csv(\"..\/input\/locations\/world_country_and_usa_states_latitude_and_longitude_values.csv\")\nfor con in countries:\n    lat.append(counts[counts[\"country_code\"] == con].latitude.values[0])\n    lon.append(counts[counts[\"country_code\"] == con].longitude.values[0])\n\nimport folium\nfrom folium.plugins import MarkerCluster\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)\nfor i in range(len(lat)):\n        if(lat[i] == 0 ):\n            continue\n        radius=5\n        popup_text = \"\"\"Country : {}\"\"\"\n        popup_text = popup_text.format(country_alpha2_to_country_name(countries[i]))\n        folium.CircleMarker(location = [lat[i], lon[i]] ,radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\ndisplay(world_map)\n#<p style=\"text-align : justify;\"><b>Inference : <\/b>People from age group <b>18-30<\/b> are more on Kaggle and reduces from there onwards. Majority of the people working on Kaggle are pursuing either <b>BS<\/b>, <b>MS<\/b> or <b>Phd<\/b> which shows that students and researches are more in number. Coders with <b>1-5<\/b> years of experience mostly use kaggle compared to experienced people (<b>5+ years<\/b>). As observed before, <b>Students<\/b> are the major users of kaggle in comparison with industry experts. Within, professionals <b>Data Scientist<\/b>, <b>Data Analyst<\/b> and <b>Software Engineer<\/b> are the major users of Kaggle.<\/p>\n\n#<h2>Question 7 - What programming languages do you use on a regular basis? (Select all that apply)<\/h2>\n\nlanguages_used = data[data.columns[data.columns.get_loc(\"Q7_Part_1\") : data.columns.get_loc(\"Q7_OTHER\") + 1 ]]\nlanguages = []\nfor col in languages_used.columns:\n        languages.append([languages_used[col][0].split(\"-\")[2]])\n        languages.append(languages_used[col][1:].dropna().values)\n\nlanguages = list(itertools.chain.from_iterable(languages))\nlanguages = [i.strip() for i in languages]\nlanguages, counts = np.unique(languages,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.bar(languages,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Programming Language used on Regular basis\")\nplt.xlabel(\"Language\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of people on Kaggle use <b>Python<\/b> and <b>SQL<\/b> on a regular basis along with <b>R<\/b> in a good amount. This aligns with the fact from previous visualisation that a large number of people, including students on Kaggle belong to Data Science and Software Engineering Field.<\/p>\n\n#<h2>Question 8 - What programming language would you recommend an aspiring data scientist to learn first?<\/h2>\n\nlanguages_preferred = data.Q8.dropna().to_numpy()[1:]\nlanguages_preferred, counts = np.unique(languages_preferred,return_counts=True)\nplt.figure(figsize=(15,8))\nplt.title(\"Recommended Languages for Aspiring Data Scientists\")\nplt.ylabel(\"Language\")\nplt.xlabel(\"Count\")\nplt.barh(languages_preferred,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>As heard usually, <b>Python<\/b>, <b>R<\/b> and <b>SQL<\/b> are the most recommended languages for an aspiring Data Scientist.<\/p>\n\n#<h2>Question 9 - Which of the following integrated development environments (IDE's) do you use on a regular basis?<\/h2>\n\nide_used = data[data.columns[data.columns.get_loc(\"Q9_Part_1\") : data.columns.get_loc(\"Q9_OTHER\") + 1 ]]\nides = []\nfor col in ide_used.columns:\n        ides.append([ide_used[col][0].split(\"-\")[2]])\n        ides.append(ide_used[col][1:].dropna().values)\n\nides = list(itertools.chain.from_iterable(ides))\nides = [i.strip() for i in ides]\nides, counts = np.unique(ides,return_counts=True)\n\nides = ides[1:]\nides[0] = \"Jupyter\"\nides[-1] = \"VS Code\"\ncounts[1] += counts[0]\ncounts = counts[1:]\n\nplt.figure(figsize=(15,5))\nplt.bar(ides,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"IDEs used on Regular basis\")\nplt.xlabel(\"IDE\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Jupyter<\/b>, <b>PyCharm<\/b> and <b>VS Code<\/b> are the most commonly used IDEs since they support Notebook Environments.<\/p>\n\n#<h2>Question 10 - Which of the following hosted notebook products do you use on a regular basis?<\/h2>\n\nnotebooks_used = data[data.columns[data.columns.get_loc(\"Q10_Part_1\") : data.columns.get_loc(\"Q10_OTHER\") + 1 ]]\nnotebooks = []\nfor col in notebooks_used.columns:\n        notebooks.append([notebooks_used[col][0].split(\"-\")[2]])\n        notebooks.append(notebooks_used[col][1:].dropna().values)\n\nnotebooks = list(itertools.chain.from_iterable(notebooks))\nnotebooks = [i.strip() for i in notebooks]\nnotebooks, counts = np.unique(notebooks,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(notebooks,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Notebook Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Although a good number of peole does not use any hosted notebook IDEs on a regular  basis, <b>Kaggle Notebooks<\/b> and <b>Colab Notebooks<\/b> are common and mostly used among the people who uses hosted IDEs.<\/p>\n\n#<h2>Question 11 - What type of computing platform do you use most often for your data science projects?<\/h2>\n\nplatform = data.Q11.dropna().values[1:]\nplatform, counts = np.unique(platform,return_counts=True)\n\nplt.figure(figsize=(15,8))\nplt.pie(counts,labels=platform)\nplt.title(\"Platform used for Data Science Projects\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the population uses a PC (Laptop \/ Desktop), few use <b>Cloud Platforms<\/b> as well (maybe for higher workloads or in competitions).<\/p>\n\n#<h1>Question 12 - Which types of specialized hardware do you use on a regular basis?<\/h1>\n\nhardware_used = data[data.columns[data.columns.get_loc(\"Q12_Part_1\") : data.columns.get_loc(\"Q12_OTHER\") + 1 ]]\nhardwares = []\nfor col in hardware_used.columns:\n        hardwares.append([hardware_used[col][0].split(\"-\")[2]])\n        hardwares.append(hardware_used[col][1:].dropna().values)\n\nhardwares = list(itertools.chain.from_iterable(hardwares))\nhardwares = [i.strip() for i in hardwares]\nhardwares, counts = np.unique(hardwares,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.bar(hardwares,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Specialized hardware used on Regular basis\")\nplt.xlabel(\"Hardware\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the users do not prefer any accelarators (CPU only), <b>GPUs<\/b> and <b>TPUs<\/b> are popular among the people who use accelarators.<\/p>\n\n#<h1>Question 13 - Approximately how many times have you used a TPU (tensor processing unit)?<\/h1>\n\ntpu = data.Q13.dropna().values[1:]\ntpu, counts = np.unique(tpu,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.bar(tpu,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Usage of TPU\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>TPU is used almost never or really less in number.<\/p>\n\n#<h1>Question 14 - What data visualization libraries or tools do you use on a regular basis?<\/h1>\n\nvis_used = data[data.columns[data.columns.get_loc(\"Q14_Part_1\") : data.columns.get_loc(\"Q14_OTHER\") + 1 ]]\ntools = []\nfor col in vis_used.columns:\n        tools.append([vis_used[col][0].split(\"-\")[2]])\n        tools.append(vis_used[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Visualization tools used on Regular basis\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Seaborn<\/b>, <b>Matplotlib<\/b>, <b>Plotly<\/b> and <b>Ggplot<\/b> are the most commonly used Visualization Libraries with <b>Matplotlib<\/b> as the most used followed by <b>Seaborn<\/b>.<\/p>\n\n#<h1>Question 15 - For how many years have you used machine learning methods?<\/h1>\n\nyears = data.Q15.dropna().values[1:]\nyears, counts = np.unique(years,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(years,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Years of using Machine Learning Methods\")\nplt.ylabel(\"Experience\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>People who have not much experience with ML Methods are more, pointing to the fact that they are either students or fresh in career.<\/p>\n\n#<h1>Question 16 - Which of the following machine learning frameworks do you use on a regular basis?<\/h1>\n\nframeworks_used = data[data.columns[data.columns.get_loc(\"Q16_Part_1\") : data.columns.get_loc(\"Q16_OTHER\") + 1 ]]\nframeworks = []\nfor col in frameworks_used.columns:\n        frameworks.append([frameworks_used[col][0].split(\"-\")[2]])\n        frameworks.append(frameworks_used[col][1:].dropna().values)\n\nframeworks = list(itertools.chain.from_iterable(frameworks))\nframeworks = [i.strip() for i in frameworks]\nframeworks, counts = np.unique(frameworks,return_counts=True)\n\nframeworks = np.delete(frameworks,14)\ncounts[15] = counts[15] + counts[14]\ncounts = np.delete(counts,14)\n\nplt.figure(figsize=(15,9))\nplt.barh(frameworks,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"ML Frameworks used on Regular Basis\")\nplt.ylabel(\"Framework\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Xgboost<\/b>, <b>Scikit-learn<\/b>, <b>Tensorflow<\/b>, <b>Pytorhc<\/b> and <b>Keras<\/b> are the most used ML Frameworks<\/b><\/p>\n\n#<h1>Question 17 - Which of the following ML algorithms do you use on a regular basis? <\/h1>\n\nalgo_used = data[data.columns[data.columns.get_loc(\"Q17_Part_1\") : data.columns.get_loc(\"Q17_OTHER\") + 1 ]]\nalgos = []\nfor col in algo_used.columns:\n        algos.append([algo_used[col][0].split(\"-\")[2]])\n        algos.append(algo_used[col][1:].dropna().values)\n\nalgos = list(itertools.chain.from_iterable(algos))\nalgos = [i.strip() for i in algos]\nalgos, counts = np.unique(algos,return_counts=True)\n\ncounts[0] += counts[1]\nalgos = np.delete(algos,len(algos) - 2)\ncounts = np.delete(counts,len(counts) -2)\n\nplt.figure(figsize=(15,9))\nplt.barh(algos,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"ML Algorithms used on Regular Basis\")\nplt.ylabel(\"Algorithm\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Traditional ML Algorithms like <b>Regression<\/b>, <b>Decision Trees<\/b>, <b>Random Forests<\/b> and <b>Bayesian Models<\/b> are more used in comparison with <b>Neural Networks<\/b>.<\/p>\n\n#<h1>Question 18 - Which of the following Computer Vision Methods do you use on a regular basis?<\/h1>\n\ncomputer_vision_methods_used = data[data.columns[data.columns.get_loc(\"Q18_Part_1\") : data.columns.get_loc(\"Q18_OTHER\") + 1 ]]\nmethods = []\nfor col in computer_vision_methods_used.columns:\n        methods.append([computer_vision_methods_used[col][0].split(\"-\")[2]])\n        methods.append(computer_vision_methods_used[col][1:].dropna().values)\n\nmethods = list(itertools.chain.from_iterable(methods))\nmethods = [i.strip() for i in methods]\nmethods, counts = np.unique(methods,return_counts=True)\n\n\nmethods = np.delete(methods,3)\ncounts[4] = counts[4] + counts[3]\ncounts = np.delete(counts,3)\n\nplt.figure(figsize=(15,9))\nplt.barh(methods,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Computer Vision methods used on Regular Basis\")\nplt.ylabel(\"Method\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Image Classification<\/b> and <b>Object Detection<\/b> models like VGG, ResNet, YOLO etc are commonly used along with libraries like <b>PIL<\/b> and <b>CV2<\/b>. Image Classification stands ahead of everything.<\/p>\n\n#<h1>Question 19 - Which of the following natural language processing (NLP) methods do you use on a regular basis?<\/h1>\n\nnlp_methods_used = data[data.columns[data.columns.get_loc(\"Q19_Part_1\") : data.columns.get_loc(\"Q19_OTHER\") + 1 ]]\nmethods = []\nfor col in nlp_methods_used.columns:\n        methods.append([nlp_methods_used[col][0].split(\"-\")[2]])\n        methods.append(nlp_methods_used[col][1:].dropna().values)\n\nmethods = list(itertools.chain.from_iterable(methods))\nmethods = [i.strip() for i in methods]\nmethods, counts = np.unique(methods,return_counts=True)\n\nmethods = np.delete(methods,5)\ncounts[6] = counts[6] + counts[5]\ncounts = np.delete(counts,5)\n\nmethods = np.delete(methods,1)\ncounts[2] = counts[1] + counts[2]\ncounts = np.delete(counts,1)\n\nplt.figure(figsize=(15,9))\nplt.barh(methods,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"NLP methods used on Regular Basis\")\nplt.ylabel(\"Method\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Word Embedding Algorithms like <b>word2vec<\/b> etc are used more often. For modelling, pre-trained <b>Transformer Models<\/b> and <b>Encoder-Decoder models<\/b> are preferred.<\/p>\n\n#<h1>Question 20 - In what industry is your current employer\/contract (or your most recent employer if retired)?<\/h1>\n\nemployer_ind = data.Q20.dropna().values[1:]\nemployer_ind, counts = np.unique(employer_ind,return_counts=True)\n\nplt.figure(figsize=(15,9))\nplt.barh(employer_ind,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Employer Industry\")\nplt.ylabel(\"Industry\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>As observed before as well, users majorly belong to <b>Technology<\/b>, <b>Academics<\/b> or <b>Education<\/b> field.<\/p>\n\n#<h1>Question 21 - What is the size of the company where you are employed?\n#<\/h1>\n\nemployer_size = data.Q21.dropna().values[1:]\nemployer_size, counts = np.unique(employer_size,return_counts=True)\n\nplt.figure(figsize=(15,9))\nplt.barh(employer_size,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Employer Size\")\nplt.ylabel(\"Size\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Most People are working in a small scale industries (with approx 50 people) in comparison with the count of people working in Medium and Large scale industries.<\/p>\n\n#<h1>Question 22 - Approximately how many individuals are responsible for data science workloads at your place of business?<\/h1>\n\ndata_science_count = data.Q22.dropna().values[1:]\ndata_science_count, counts = np.unique(data_science_count,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.bar(data_science_count,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Number of Individuals responsible for Data Science Workload\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\nplt.figure(figsize=(15,9))\nplt.subplot(2,2,1)\nun,counts = np.unique(data[data.Q21 == \"0-49 employees\"].Q22.dropna().values,return_counts=True)\nplt.title(\"Small Scale Industries\")\nplt.xlabel(\"Data Science Team Count\")\nplt.ylabel(\"Count\")\nplt.bar(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n\nplt.subplot(2,2,2)\nun,counts = np.unique(data[(data.Q21 == \"50-249 employees\") | (data.Q21 == \"250-999 employees\")].Q22.dropna().values,return_counts=True)\nplt.title(\"Medium Scale Industries\")\nplt.xlabel(\"Data Science Team Count\")\nplt.ylabel(\"Count\")\nplt.bar(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n\nplt.subplot(2,2,3)\nun,counts = np.unique(data[(data.Q21 == \"1000-9,999 employees\") | (data.Q21 == \"10000 or more employees\")].Q22.dropna().values,return_counts=True)\nplt.title(\"Large Scale Industries\")\nplt.xlabel(\"Data Science Team Count\")\nplt.ylabel(\"Count\")\nplt.bar(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\n\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>This visualisation returns mixed results - There are industries where a big team (<b>20+<\/b>) handles data science workloads while on the other side a small team (<b>1-2<\/b>) is also deployed for data science workloads. The smaller teams belong to the <b>Startups<\/b> while <b>Medium Scale<\/b> industries has a mixed team of smalle and large for varying workloads. <b>Large Scale<\/b> deploy larger teams (20+) because of their realtively.<\/p>\n\n#<h1>Question 23 - Does your current employer incorporate machine learning methods into their business?<\/h1>\n\nML_business = data.Q23.dropna().values[1:]\nML_business, counts = np.unique(ML_business,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.barh(ML_business,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Use of ML by Current Employer\")\nplt.ylabel(\"Use\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the industries belong to the category where the ML domain is only getting explored or not even considered for using but at the same time, there are a good number of industries who have recently started or already use ML methods.<\/p>\n\n#<h1>Question 24 - Select any activities that make up an important part of your role at work<\/h1>\n\nactivities_work = data[data.columns[data.columns.get_loc(\"Q24_Part_1\") : data.columns.get_loc(\"Q24_OTHER\") + 1 ]]\nactivities = []\nfor col in activities_work.columns:\n        activities.append([activities_work[col][0].split(\"-\")[2]])\n        activities.append(activities_work[col][1:].dropna().values)\n\nactivities = list(itertools.chain.from_iterable(activities))\nactivities = [i.strip() for i in activities]\nactivities, counts = np.unique(activities,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.barh(activities,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Major Activity at work\")\nplt.ylabel(\"Activity\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Industries often use analyatics to generate business decisions while there are a good number who built it into production or apply experimentation to find areas for implementing ML or improve the existing ones. There also exists industries who work on Data Science workloads related to data storage and analytics infrastructure.<\/p>\n\n#<h1>Question 25 - What is your current yearly compensation (approximate $USD)?<\/h1>\n\ncompensation = data.Q25.dropna().values[1:]\ncompensation, counts = np.unique(compensation,return_counts=True)\n\nplt.figure(figsize=(15,10))\nplt.barh(compensation,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Current Yearly Compensation\")\nplt.ylabel(\"Compensation Range\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>0-999 USD<\/b> is the most chosen Compensation.<\/p>\n\n#<h1>Question 26 - Approximately how much money have you (or your team) spent on machine learning and\/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?<\/h1>\n\nmoney_spent = data.Q26.dropna().values[1:]\nmoney_spent, counts = np.unique(money_spent,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.bar(money_spent,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Money spent on ML \/ Cloud Computing Services at Home or at Work in past 5 years\")\nplt.xlabel(\"Money Spent\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\nplt.figure(figsize=(15,30))\nunique,counts = np.unique(data.Q26.dropna().values[1:],return_counts=True)\nfor i in range(len(unique)):\n    plt.subplot(len(unique),2,i+1)\n    un,counts = np.unique(data[data.Q26 == unique[i]].Q5.dropna().values,return_counts=True)\n    plt.title(unique[i])\n    plt.xlabel(\"Count\")\n    plt.ylabel(\"Profession\")\n    plt.barh(un,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\n    plt.tick_params(bottom=False,left=False)\n    plt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Most people do not spend much on Cloud Servies, although there are few people who spent money on Cloud Services. The larger spending mostly came from Work while the smaller ones may belong to personal projects \/ learning etc. The profession of the group of people spending on Cloud Servies mainly belong to <b>Software Engineer<\/b>, <b>Data Scientist<\/b> or <b>Data Analyst<\/b>.<\/p>\n\n#<h1>Question 27 Part A - Which of the following cloud computing platforms do you use on a regular basis?<\/h1>\n\nplatforms_used = data[data.columns[data.columns.get_loc(\"Q27_A_Part_1\") : data.columns.get_loc(\"Q27_A_OTHER\") + 1 ]]\nplatforms = []\nfor col in platforms_used.columns:\n        platforms.append([platforms_used[col][0].split(\"-\")[2]])\n        platforms.append(platforms_used[col][1:].dropna().values)\n\nplatforms = list(itertools.chain.from_iterable(platforms))\nplatforms = [i.strip() for i in platforms]\nplatforms, counts = np.unique(platforms,return_counts=True)\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,2,1)\nplt.pie(counts,labels=platforms)\nplt.title(\"     Cloud Computing Platforms used on Regular basis\")\n#plt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\n\n\n#<p><b>Inference : <\/b><b>AWS<\/b> is used on a large number followed by <b>GCP<\/b> and <b>Azure<\/b>. They can be either for personal work or for Professional Purpose.<\/p>\n#<h1>Question 27 Part B -Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?<\/h1>\n\nplatforms_familiar = data[data.columns[data.columns.get_loc(\"Q27_B_Part_1\") : data.columns.get_loc(\"Q27_B_OTHER\") + 1 ]]\nplatforms = []\nfor col in platforms_familiar.columns:\n        platforms.append([platforms_familiar[col][0].split(\"-\")[2]])\n        platforms.append(platforms_familiar[col][1:].dropna().values)\n\nplatforms = list(itertools.chain.from_iterable(platforms))\nplatforms = [i.strip() for i in platforms]\nplatforms, counts = np.unique(platforms,return_counts=True)\n\nplt.subplot(1,2,2)\nplt.pie(counts,labels=platforms)\nplt.title(\"     Cloud Computing Platforms preferred to become familiar\")\n#plt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>The commonly used <b>AWS<\/b>, <b>GCP<\/b> and <b>Azure<\/b> are the ones people prefer to get familiar.<\/p>\n\n#<h1>Question 28 - Of the cloud platforms that you are familiar with, which has the best developer experience (most enjoyable to use)?<\/h1>\n\nbest_platform = data.Q28.dropna().values[1:]\nbest_platform, counts = np.unique(best_platform,return_counts=True)\n\nplt.figure(figsize=(15,8))\nplt.barh(best_platform,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Cloud Computing Platforms with best Developer Experience\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>AWS<\/b> has a good number of votes followed by <b>GCP<\/b> and <b>Azure<\/b>. Large group of people also find all of the cloud platforms to have a similar user experience.<\/p>\n\n#<h1>Question 29 Part A - Do you use any of the following cloud computing products on a regular basis?<\/h1>\n\nproducts_used = data[data.columns[data.columns.get_loc(\"Q29_A_Part_1\") : data.columns.get_loc(\"Q29_A_OTHER\") + 1 ]]\nproducts = []\nfor col in products_used.columns:\n        products.append([products_used[col][0].split(\"-\")[2]])\n        products.append(products_used[col][1:].dropna().values)\n\nproducts = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.bar(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Cloud Computing Products used on Regular basis\")\nplt.xlabel(\"Product\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Cloud VMs<\/b> are the mostly used product.<\/p>\n\n#<h1>Question 29 Part B - In the next 2 years, do you hope to become more familiar with any of these specific cloud computing products? <\/h1>\n\nproducts_preferred = data[data.columns[data.columns.get_loc(\"Q29_B_Part_1\") : data.columns.get_loc(\"Q29_B_OTHER\") + 1 ]]\nproducts = []\nfor col in products_preferred.columns:\n        products.append([products_preferred[col][0].split(\"-\")[2]])\n        products.append(products_preferred[col][1:].dropna().values)\n\nproducts = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,5))\nplt.bar(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Cloud Computing Products preferred to become familiar\")\nplt.xlabel(\"Product\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Again, <b>Cloud VMs<\/b> are the preferred product to get familiar with. <b>GCP<\/b> is the most chosen platform followed for <b>AWS<\/b> and <b>Azure<\/b>.<\/p>\n\n#<h1>Question 30 Part A - Do you use any of the following data storage products on a regular basis?<\/h1>\n\nproducts_used = data[data.columns[data.columns.get_loc(\"Q30_A_Part_1\") : data.columns.get_loc(\"Q30_A_OTHER\") + 1 ]]\nproducts = []\nfor col in products_used.columns:\n        products.append([products_used[col][0].split(\"-\")[2]])\n        products.append(products_used[col][1:].dropna().values)\n\nproducts = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Storage Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Basic storage products like <b>GCS<\/b> and <b>S3<\/b> are more commonly used.<\/p>\n\n#<h1>Question 30 Part B - In the next 2 years, do you hope to become more familiar with any of these specific data storage products?<\/h1>\n\nproducts_preferred = data[data.columns[data.columns.get_loc(\"Q30_B_Part_1\") : data.columns.get_loc(\"Q30_B_OTHER\") + 1 ]]\nproducts = []\nfor col in products_preferred.columns:\n        products.append([products_preferred[col][0].split(\"-\")[2]])\n        products.append(products_preferred[col][1:].dropna().values)\n\nproducts = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Storage Products preferred to become familiar with\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Really few votes for this question - But all products are equally preferred.<\/p>\n\n#<h1>Question 31 Part A - Do you use any of the following managed machine learning products on a regular basis?<\/h1>\n\nml_products_used = data[data.columns[data.columns.get_loc(\"Q31_A_Part_1\") : data.columns.get_loc(\"Q31_A_OTHER\") + 1 ]]\nml_products = []\nfor col in ml_products_used.columns:\n        ml_products.append([ml_products_used[col][0].split(\"-\")[2]])\n        ml_products.append(ml_products_used[col][1:].dropna().values)\n\nml_products = list(itertools.chain.from_iterable(ml_products))\nml_products = [i.strip() for i in ml_products]\nml_products, counts = np.unique(ml_products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(ml_products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Managed ML Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Managed ML Products<\/p>\n\n#<h1>Question 31 Part B - In the next 2 years, do you hope to become more familiar with any of these managed machine learning products?<\/h1>\n\nml_products_preferred = data[data.columns[data.columns.get_loc(\"Q31_B_Part_1\") : data.columns.get_loc(\"Q31_B_OTHER\") + 1 ]]\nml_products = []\nfor col in ml_products_preferred.columns:\n        ml_products.append([ml_products_preferred[col][0].split(\"-\")[2]])\n        ml_products.append(ml_products_preferred[col][1:].dropna().values)\n\nml_products = list(itertools.chain.from_iterable(ml_products))\nml_products = [i.strip() for i in ml_products]\nml_products, counts = np.unique(ml_products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(ml_products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Managed ML Products preferred to get familiar with\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>For people willing to Managed ML proucts, <b>Google Cloud Vertex AI<\/b>, <b>Azure ML Studio<\/b> and <b>Amazon SageMaker<\/b>.<\/p>\n\n#<h1>Question 32 Part A - Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis?<\/h1>\n\nproducts_used = data[data.columns[data.columns.get_loc(\"Q32_A_Part_1\") : data.columns.get_loc(\"Q32_A_OTHER\") + 1 ]]\nproducts = []\nfor col in products_used.columns:\n        products.append([products_used[col][0].split(\"-\")[2]])\n        products.append(products_used[col][1:].dropna().values)\n\nproducts = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Data Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>SQL related databases are used most followed by Big Data Processing systems like <b>BigQuery<\/b>, <b>BigTable<\/b> etc.<\/p>\n\n#<h1>Question 32 Part B - Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you hope to become more familiar with in the next 2 years?<\/h1>\n\nproducts_preferred = data[data.columns[data.columns.get_loc(\"Q32_B_Part_1\") : data.columns.get_loc(\"Q32_B_OTHER\") + 1 ]]\nproducts = []\nfor col in products_preferred.columns:\n        products.append([products_preferred[col][0].split(\"-\")[2]])\n        products.append(products_preferred[col][1:].dropna().values)\n\nproducts = list(itertools.chain.from_iterable(products))\nproducts = [i.strip() for i in products]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Data Products used on Regular basis\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>SQL related databases are used mostly followed by Big Data Processing Systems like <b>BigQuery<\/b>, <b>BigTable<\/b>, <b>Redshift<\/b> etc. <\/p>\n\n#<h1>Question 33 - Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?<\/h1>\n\nproducts = data.Q33.dropna().values[1:]\nproducts, counts = np.unique(products,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(products,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Data Products used most often\")\nplt.ylabel(\"Product\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>SQL related databases are used most followed by Big Data Processing systems like <b>BigQuery<\/b>, <b>BigTable<\/b> etc.<\/p>\n\n#<h1>Question 34 Part A - Which of the following business intelligence tools do you use on a regular basis?<\/h1>\n\nbi_used = data[data.columns[data.columns.get_loc(\"Q34_A_Part_1\") : data.columns.get_loc(\"Q34_A_OTHER\") + 1 ]]\nbi = []\nfor col in bi_used.columns:\n        bi.append([bi_used[col][0].split(\"-\")[2]])\n        bi.append(bi_used[col][1:].dropna().values)\n\nbi = list(itertools.chain.from_iterable(bi))\nbi = [i.strip() for i in bi]\nbi, counts = np.unique(bi,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(bi,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Intelligence Tools used on Regular basis\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Tableau<\/b>, <b>Power BI<\/b> and <b>Data Studio<\/b> are used on a regular basis, although a good number of users don't use any business intelligence tools.<\/p>\n\n#<h1>Question 34 Part B - Which of the following business intelligence tools do you hope to become more familiar with in the next 2 years?<\/h1>\n\ntools_preferred = data[data.columns[data.columns.get_loc(\"Q34_B_Part_1\") : data.columns.get_loc(\"Q34_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Intelligence Tools preferred to become familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Tableau<\/b>, <b>Power BI<\/b> and <b>Data Studio<\/b> are the tools preferred to become familiar<\/p>\n\n#<h1>Question 35 - Which of the following business intelligence tools do you use most often?<\/h1>\n\nbi_used = data.Q35.dropna().values[1:]\nbi_used,counts = np.unique(bi_used,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(bi_used,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Big Intelligence Tools used most often\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Tableau<\/b>, <b>Power BI<\/b> and <b>Data Studio<\/b> are the tools used most often.<\/p>\n\n#<h1>Question 36 Part A - Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis?<\/h1>\n\ntools_used = data[data.columns[data.columns.get_loc(\"Q36_A_Part_1\") : data.columns.get_loc(\"Q36_A_OTHER\") + 1 ]]\ntools = []\nfor col in tools_used.columns:\n        tools.append([tools_used[col][0].split(\"-\")[2]])\n        tools.append(tools_used[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\ntools = np.delete(tools,4)\ncounts[5] = counts[4] + counts[5]\ncounts = np.delete(counts,4)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools used most often\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the users not use any Auto ML tools.<\/p>\n\n#<h1>Question 36 Part B - Which categories of automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years? <\/h1>\n\ntools_preferred = data[data.columns[data.columns.get_loc(\"Q36_B_Part_1\") : data.columns.get_loc(\"Q36_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\ntools = np.delete(tools,4)\ncounts[5] = counts[4] + counts[5]\ncounts = np.delete(counts,4)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools preferred to get familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Users majorly prefer tools related to Automation of <b>ML Pipelines<\/b>, <b>Model Selection<\/b>, <b>Hyperparameter Tuning<\/b>, <b>Feature Engieering<\/b>, <b>Data Augmentation<\/b>.<\/p>\n\n#<h1>Question 37 Part A - Which of the following automated machine learning tools (or partial AutoML tools) do you use on a regular basis?<\/h1>\n\ntools_used = data[data.columns[data.columns.get_loc(\"Q37_A_Part_1\") : data.columns.get_loc(\"Q37_A_OTHER\") + 1 ]]\ntools = []\nfor col in tools_used.columns:\n        tools.append([tools_used[col][0].split(\"-\")[2]])\n        tools.append(tools_used[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools used on Regular basis\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the users do not use any Auto ML prodoucts on a regular basis but there are good number of users for <b>Google Cloud AutoML<\/b> and <b>Azure Automated ML<\/b> and <b>AWS Sagemaker<\/b>.<\/p>\n\n#<h1>Question 37 Part B - Which specific automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years?<\/h1>\n\ntools_preferred = data[data.columns[data.columns.get_loc(\"Q37_B_Part_1\") : data.columns.get_loc(\"Q37_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Auto ML Tools preferred to become familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>The most commonly hear <b>Google Cloud AutoML<\/b>, <b>Azure Automated Machine Learning<\/b>, <b>AWS Sagemaker<\/b> are the ones which users prefer to get familiar.<\/p>\n\n#<h1>Question 38 Part A - Do you use any tools to help manage machine learning experiments?<\/h1>\n\ntools_used = data[data.columns[data.columns.get_loc(\"Q38_A_Part_1\") : data.columns.get_loc(\"Q38_A_OTHER\") + 1 ]]\ntools = []\nfor col in tools_used.columns:\n        tools.append([tools_used[col][0].split(\"-\")[2]])\n        tools.append(tools_used[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Tools used to manage Machine Learning Projects\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the users do not use any tools for managing ML Products, although there are a good number of users opting for <b>Weights & Biases<\/b>, <b>TensorBoard<\/b> and <b>MLflow<\/b>.<\/p>\n\n#<h1>Question 38 Part B - In the next 2 years, do you hope to become more familiar with any of these tools for managing ML experiments<\/h1>\n\ntools_preferred = data[data.columns[data.columns.get_loc(\"Q38_B_Part_1\") : data.columns.get_loc(\"Q38_B_OTHER\") + 1 ]]\ntools = []\nfor col in tools_preferred.columns:\n        tools.append([tools_preferred[col][0].split(\"-\")[2]])\n        tools.append(tools_preferred[col][1:].dropna().values)\n\ntools = list(itertools.chain.from_iterable(tools))\ntools = [i.strip() for i in tools]\ntools, counts = np.unique(tools,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Tools preferred to become familiar\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Majority of the users do not prefer any tools for managing ML Projects, but there are good number of votes for <b>Weights & Biases<\/b>, <b>Tensorboard<\/b> and <b>MLflow<\/b>.<\/p>\n\n#<h1>Question 39 - Where do you publicly share your data analysis or machine learning applications?<\/h1>\n\nshare_loc = data[data.columns[data.columns.get_loc(\"Q39_Part_1\") : data.columns.get_loc(\"Q39_OTHER\") + 1 ]]\nshare = []\nfor col in share_loc.columns:\n        share.append([share_loc[col][0].split(\"-\")[2]])\n        share.append(share_loc[col][1:].dropna().values)\n\nshare = list(itertools.chain.from_iterable(share))\nshare = [i.strip() for i in share]\nshare, counts = np.unique(share,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(share,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Platform used for sharing projects publicly\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>Users who share prefer <b>Personal Blog<\/b>, <b>Kaggle<\/b>, <b>GitHub<\/b> or <b>Colab<\/b> as their platform while a good number of users do not prefer sharing as well.<\/p>\n\n#<h1>Question 40 - On which platforms have you begun or completed data science courses?<\/h1>\n\nplatforms_used = data[data.columns[data.columns.get_loc(\"Q40_Part_1\") : data.columns.get_loc(\"Q40_OTHER\") + 1 ]]\nplatform = []\nfor col in platforms_used.columns:\n        platform.append([platforms_used[col][0].split(\"-\")[2]])\n        platform.append(platforms_used[col][1:].dropna().values)\n\nplatform = list(itertools.chain.from_iterable(platform))\nplatform = [i.strip() for i in platform]\nplatform, counts = np.unique(platform,return_counts=True)\n\nplatform = np.delete(platform,0)\ncounts[1] = counts[0] + counts[1]\ncounts = np.delete(counts,0)\n\nplt.figure(figsize=(15,7))\nplt.barh(platform,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.legend(loc=\"lower right\",bbox_to_anchor=(0,1))\nplt.title(\"     Platform used for learning Data Science\")\nplt.ylabel(\"Platform\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b>For external sources, <b>Cousera<\/b>, <b>Kaggle Courses<\/b> and <b>Udemy<\/b> stands forward. In the case formal education, <b>University Courses<\/b> or a <b>Degree<\/b> is preferrred.<\/p>\n\n#<h1>Question 41 - What is the primary tool that you use at work or school to analyze data?<\/h1>\n\ntools_used = data.Q41.dropna().values[1:]\ntools,counts = np.unique(tools_used,return_counts=True)\n\nplt.figure(figsize=(15,7))\nplt.barh(tools,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Primary Tool used to Analyze data\")\nplt.ylabel(\"Tool\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\n#<p><b>Inference : <\/b><b>Local IDEs<\/b> or <b>Basic Statistical Software<\/b> is used the most to analyze data.<\/p>\n\n#<h1>Question 42 - Who\/what are your favorite media sources that report on data science topics?<\/h1>\n\nmedia_sources = data[data.columns[data.columns.get_loc(\"Q42_Part_1\") : data.columns.get_loc(\"Q42_OTHER\") + 1 ]]\nsources = []\nfor col in media_sources.columns:\n        sources.append([media_sources[col][0].split(\"-\")[2]])\n        sources.append(media_sources[col][1:].dropna().values)\n\nsources = list(itertools.chain.from_iterable(sources))\nsources = [i.strip() for i in sources]\nsources, counts = np.unique(sources,return_counts=True)\n\nsources = np.delete(sources,3)\ncounts[4] = counts[4] + counts[3]\ncounts = np.delete(counts,3)\n\nplt.figure(figsize=(15,7))\nplt.barh(sources,counts,zorder=3,color = ['forestgreen','green','springgreen','aquamarine','darkslategrey','aqua','powderblue','lightskyblue','lightslategray','lightsteelblue'])\nplt.tick_params(bottom=False,left=False)\nplt.grid(True,color='white', linestyle='solid', linewidth=2.0, alpha=0.5,zorder=0)\nplt.title(\"Media Sources used that report Data Science Topics\")\nplt.ylabel(\"Source\")\nplt.xlabel(\"Count\")\nplt.tight_layout()\n\nplt.savefig(\"fig.png\")\n#<p><b>Inference : <\/b><b>Blogs<\/b>, <b>Kaggle<\/b> and <b>Youtube Channels<\/b> are most preferred platforms that report Data Science topics.<\/p>","26049d6f":"<p><b>Inference : <\/b>Really few votes for this question - But all products are equally preferred.<\/p>","fd3fa1d2":"<h1>Question 32 Part A - Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis?<\/h1>","0b6c933c":"<h1>Question 42 - Who\/what are your favorite media sources that report on data science topics?<\/h1>","2513b993":"<h1>Question 30 Part A - Do you use any of the following data storage products on a regular basis?<\/h1>","c525bedc":"<h2>Geospatial Analysis<\/h2>","64b0819b":"<h2>Question 9 - Which of the following integrated development environments (IDE's) do you use on a regular basis?<\/h2>","1fc1f7fa":"<p><b>Inference : <\/b><b>Image Classification<\/b> and <b>Object Detection<\/b> models like VGG, ResNet, YOLO etc are commonly used along with libraries like <b>PIL<\/b> and <b>CV2<\/b>. Image Classification stands ahead of everything.<\/p>","6207c602":"<p><b>Inference : <\/b><b>Tableau<\/b>, <b>Power BI<\/b> and <b>Data Studio<\/b> are used on a regular basis, although a good number of users don't use any business intelligence tools.<\/p>","59fe8778":"<h2>Question 8 - What programming language would you recommend an aspiring data scientist to learn first?<\/h2>","0418b0b4":"<h1>Question 33 - Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?<\/h1>","f8664179":"<h1>Data Loading<\/h1>","917f7b79":"<h1>Question 36 Part A - Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis?<\/h1>","091c1bb6":"<p><b>Inference : <\/b>Majority of the users do not use any tools for managing ML Products, although there are a good number of users opting for <b>Weights & Biases<\/b>, <b>TensorBoard<\/b> and <b>MLflow<\/b>.<\/p>","6671f1ad":"<h1>Question 41 - What is the primary tool that you use at work or school to analyze data?<\/h1>","6d225e73":"<h1>Dashboard<\/h1>","1d5670ee":"<h1>Question 18 - Which of the following Computer Vision Methods do you use on a regular basis?<\/h1>","0dee950e":"<p><b>Inference : <\/b>SQL related databases are used most followed by Big Data Processing systems like <b>BigQuery<\/b>, <b>BigTable<\/b> etc.<\/p>","e42c52f7":"<p><b>Inference : <\/b><b>Xgboost<\/b>, <b>Scikit-learn<\/b>, <b>Tensorflow<\/b>, <b>Pytorch<\/b> and <b>Keras<\/b> are the most used ML Frameworks<\/b><\/p>","30dcd9da":"<p><b>Inference : <\/b>The most commonly hear <b>Google Cloud AutoML<\/b>, <b>Azure Automated Machine Learning<\/b>, <b>AWS Sagemaker<\/b> are the ones which users prefer to get familiar.<\/p>","9e2135bf":"<p><b>Inference : <\/b><b>0-999 USD<\/b> is the most chosen Compensation.<\/p>","4d73d609":"<p><b>Inference : <\/b>Managed ML Products<\/p>","6389b05f":"<p><b>Inference : <\/b>Majority of participants from <b>North and South America<\/b>, <b>Europe<\/b> and <b>Asia<\/b>.<\/p>","05afd1c3":"<p><b>Inference : <\/b>Although a good number of peole does not use any hosted notebook IDEs on a regular  basis, <b>Kaggle Notebooks<\/b> and <b>Colab Notebooks<\/b> are common and mostly used among the people who uses hosted IDEs.<\/p>","f628aa9e":"<h1>Question 22 - Approximately how many individuals are responsible for data science workloads at your place of business?<\/h1>","9029ee6e":"<p><b>Inference : <\/b>As observed before as well, users majorly belong to <b>Technology<\/b>, <b>Academics<\/b> or <b>Education<\/b> field.<\/p>","358bc5f7":"<p><b>Inference : <\/b>SQL related databases are used most followed by Big Data Processing systems like <b>BigQuery<\/b>, <b>BigTable<\/b> etc.<\/p>","5d07cd54":"<h1>Question 21 - What is the size of the company where you are employed?\n<\/h1>","c231d264":"<p><b>Inference : <\/b><b>Blogs<\/b>, <b>Kaggle<\/b> and <b>Youtube Channels<\/b> are most preferred platforms that report Data Science topics.<\/p>","f3cf9570":"<h1>Question 37 Part A - Which of the following automated machine learning tools (or partial AutoML tools) do you use on a regular basis?<\/h1>","500cc97d":"<p><b>Inference : <\/b>Users who share prefer <b>Personal Blog<\/b>, <b>Kaggle<\/b>, <b>GitHub<\/b> or <b>Colab<\/b> as their platform while a good number of users do not prefer sharing as well.<\/p>","7cac1299":"<p><b>Inference : <\/b><b>Local IDEs<\/b> or <b>Basic Statistical Software<\/b> is used the most to analyze data.<\/p>","930ef9cc":"<p><b>Inference : <\/b>Most people do not spend much on Cloud Servies, although there are few people who spent money on Cloud Services. The larger spending mostly came from Work while the smaller ones may belong to personal projects \/ learning etc. The profession of the group of people spending on Cloud Servies mainly belong to <b>Software Engineer<\/b>, <b>Data Scientist<\/b> or <b>Data Analyst<\/b>.<\/p>","dd1ac3b7":"<h1>Question 20 - In what industry is your current employer\/contract (or your most recent employer if retired)?<\/h1>","dd74d021":"<h1>Question 19 - Which of the following natural language processing (NLP) methods do you use on a regular basis?<\/h1>","b501ed53":"<h1>Question 25 - What is your current yearly compensation (approximate $USD)?<\/h1>","ed3b31db":"<h1>Question 28 - Of the cloud platforms that you are familiar with, which has the best developer experience (most enjoyable to use)?<\/h1>","a28da12b":"<p><b>Inference : <\/b>Again, <b>Cloud VMs<\/b> are the preferred product to get familiar with. <b>GCP<\/b> is the most chosen platform followed for <b>AWS<\/b> and <b>Azure<\/b>.<\/p>","4461152f":"<h1>Question 37 Part B - Which specific automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years?<\/h1>","7f3a6e9c":"<h1>Question 23 - Does your current employer incorporate machine learning methods into their business?<\/h1>","18dddcfb":"<h1>Visualisations with Insights<\/h1>","35aa85d6":"<p><b>Inference : <\/b>TPU is used almost never or really less in number.<\/p>","2cab9df0":"<h1>Question 16 - Which of the following machine learning frameworks do you use on a regular basis?<\/h1>","428a80a2":"<p style=\"text-align : justify;\"><b>Inference : <\/b>People from age group <b>18-30<\/b> are more on Kaggle and reduces from there onwards. Majority of the people working on Kaggle are pursuing either <b>BS<\/b>, <b>MS<\/b> or <b>Phd<\/b> which shows that students and researches are more in number. Coders with <b>1-5<\/b> years of experience mostly use kaggle compared to experienced people (<b>5+ years<\/b>). As observed before, <b>Students<\/b> are the major users of kaggle in comparison with industry experts. Within, professionals <b>Data Scientist<\/b>, <b>Data Analyst<\/b> and <b>Software Engineer<\/b> are the major users of Kaggle.<\/p>","b3fe712f":"<p><b>Inference : <\/b>Users majorly prefer tools related to Automation of <b>ML Pipelines<\/b>, <b>Model Selection<\/b>, <b>Hyperparameter Tuning<\/b>, <b>Feature Engieering<\/b>, <b>Data Augmentation<\/b>.<\/p>","3d83d1c0":"<h1>Question 12 - Which types of specialized hardware do you use on a regular basis?<\/h1>","551c92ba":"<p><b>Inference : <\/b>15-20 minutes was taken by majority of the people to complete the survey.<\/p>","bf460491":"<h2>Question 7 - What programming languages do you use on a regular basis? (Select all that apply)<\/h2>","5d0def27":"<p><b>Inference : <\/b>Industries often use analyatics to generate business decisions while there are a good number who built it into production or apply experimentation to find areas for implementing ML or improve the existing ones. There also exists industries who work on Data Science workloads related to data storage and analytics infrastructure.<\/p>","5f1e8e64":"<p><b>Inference : <\/b>Majority of the industries belong to the category where the ML domain is only getting explored or not even considered for using but at the same time, there are a good number of industries who have recently started or already use ML methods.<\/p>","0f3ebe48":"<p><b>Inference : <\/b>Majority of the users not use any Auto ML tools.<\/p>","6cc48e0a":"<p><b>Inference : <\/b><b>Tableau<\/b>, <b>Power BI<\/b> and <b>Data Studio<\/b> are the tools preferred to become familiar<\/p>","ec4450ab":"<h1>Question 24 - Select any activities that make up an important part of your role at work<\/h1>","dc13a9e9":"<h1>Question 34 Part A - Which of the following business intelligence tools do you use on a regular basis?<\/h1>","788820af":"<h1>Question 38 Part A - Do you use any tools to help manage machine learning experiments?<\/h1>","68e94c86":"<p><b>Inference : <\/b>The commonly used <b>AWS<\/b>, <b>GCP<\/b> and <b>Azure<\/b> are the ones people prefer to get familiar.<\/p>","169ca913":"<p><b>Inference : <\/b>People who have not much experience with ML Methods are more, pointing to the fact that they are either students or fresh in career.<\/p>","aed59714":"<h1>Question 29 Part A - Do you use any of the following cloud computing products on a regular basis?<\/h1>","77663b20":"<p><b>Inference : <\/b>Majority of the users do not prefer any tools for managing ML Projects, but there are good number of votes for <b>Weights & Biases<\/b>, <b>Tensorboard<\/b> and <b>MLflow<\/b>.<\/p>","026ae14f":"<p><b>Inference : <\/b>Majority of the users do not use any Auto ML prodoucts on a regular basis but there are good number of users for <b>Google Cloud AutoML<\/b> and <b>Azure Automated ML<\/b> and <b>AWS Sagemaker<\/b>.<\/p>","89ceec5b":"<p><b>Inference : <\/b>Traditional ML Algorithms like <b>Regression<\/b>, <b>Decision Trees<\/b>, <b>Random Forests<\/b> and <b>Bayesian Models<\/b> are more used in comparison with <b>Neural Networks<\/b>.<\/p>","8361c17f":"<h1>Question 27 Part B -Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?<\/h1>","1a4adf7a":"<h1>Question 32 Part B - Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you hope to become more familiar with in the next 2 years?<\/h1>","3dd5d2ff":"<h1>Question 31 Part A - Do you use any of the following managed machine learning products on a regular basis?<\/h1>","23d33f4a":"<p><b>Inference : <\/b>This visualisation returns mixed results - There are industries where a big team (<b>20+<\/b>) handles data science workloads while on the other side a small team (<b>1-2<\/b>) is also deployed for data science workloads. The smaller teams belong to the <b>Startups<\/b> while <b>Medium Scale<\/b> industries has a mixed team of smalle and large for varying workloads. <b>Large Scale<\/b> deploy larger teams (20+) because of their realtively.<\/p>","b1daed9b":"<p><b>Inference : <\/b>SQL related databases are used mostly followed by Big Data Processing Systems like <b>BigQuery<\/b>, <b>BigTable<\/b>, <b>Redshift<\/b> etc. <\/p>","2f0bed71":"<p><b>Inference : <\/b>Majority of people on Kaggle use <b>Python<\/b> and <b>SQL<\/b> on a regular basis along with <b>R<\/b> in a good amount. This aligns with the fact from previous visualisation that a large number of people, including students on Kaggle belong to Data Science and Software Engineering Field.<\/p>","dc5d226a":"<h1>Question 40 - On which platforms have you begun or completed data science courses?<\/h1>","fc9380c8":"<h2>Question 11 - What type of computing platform do you use most often for your data science projects?<\/h2>","08231fe8":"<p><b>Inference : <\/b><b>Tableau<\/b>, <b>Power BI<\/b> and <b>Data Studio<\/b> are the tools used most often.<\/p>","a9e9d820":"<p><b>Inference : <\/b><b>AWS<\/b> has a good number of votes followed by <b>GCP<\/b> and <b>Azure<\/b>. Large group of people also find all of the cloud platforms to have a similar user experience.<\/p>","c70d9735":"<h1>Question 36 Part B - Which categories of automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years? <\/h1>","a538b557":"<h1>Question 13 - Approximately how many times have you used a TPU (tensor processing unit)?<\/h1>","6d8782c2":"<p><b>Inference : <\/b>Majority of the population uses a PC (Laptop \/ Desktop), few use <b>Cloud Platforms<\/b> as well (maybe for higher workloads or in competitions).<\/p>","896f146a":"<h1>Question 34 Part B - Which of the following business intelligence tools do you hope to become more familiar with in the next 2 years?<\/h1>","5f8659a0":"<p><b>Inference : <\/b>Word Embedding Algorithms like <b>word2vec<\/b> etc are used more often. For modelling, pre-trained <b>Transformer Models<\/b> and <b>Encoder-Decoder models<\/b> are preferred.<\/p>","a09a4e30":"<h1>Question 17 - Which of the following ML algorithms do you use on a regular basis? <\/h1>","d55f578d":"<h1>Question 35 - Which of the following business intelligence tools do you use most often?<\/h1>","68de05ff":"<h1>Question 14 - What data visualization libraries or tools do you use on a regular basis?<\/h1>","75084366":"<p><b>Inference : <\/b>Basic storage products like <b>GCS<\/b> and <b>S3<\/b> are more commonly used.<\/p>","65d9ac36":"<h1>Question 27 Part A - Which of the following cloud computing platforms do you use on a regular basis?<\/h1>","a72b150a":"<h1>Question 29 Part B - In the next 2 years, do you hope to become more familiar with any of these specific cloud computing products? <\/h1>","26a16d29":"<p><b>Inference : <\/b>Most People are working in a small scale industries (with approx 50 people) in comparison with the count of people working in Medium and Large scale industries.<\/p>","5b14c6db":"<h1>Question 31 Part B - In the next 2 years, do you hope to become more familiar with any of these managed machine learning products?<\/h1>","f35c14d2":"<h1>2021 Kaggle Survey Dashboard<\/h1>\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23724\/logos\/header.png?t=2020-10-31-23-22-58\" style=\"width : 100%;margin : auto;\">","e78a84c5":"<p><b>Inference : <\/b><b>Jupyter<\/b>, <b>PyCharm<\/b> and <b>VS Code<\/b> are the most commonly used IDEs since they support Notebook Environments.<\/p>","eefd53cf":"<p><b>Inference : <\/b>For people willing to Managed ML proucts, <b>Google Cloud Vertex AI<\/b>, <b>Azure ML Studio<\/b> and <b>Amazon SageMaker<\/b>.<\/p>","76358d35":"<p><b>Inference : <\/b>Majority of the users do not prefer any accelarators (CPU only), <b>GPUs<\/b> and <b>TPUs<\/b> are popular among the people who use accelarators.<\/p>","1f5a1f36":"<p><b>Inference : <\/b>As heard usually, <b>Python<\/b>, <b>R<\/b> and <b>SQL<\/b> are the most recommended languages for an aspiring Data Scientist.<\/p>","b7c66a86":"<p><b>Inference : <\/b><b>Seaborn<\/b>, <b>Matplotlib<\/b>, <b>Plotly<\/b> and <b>Ggplot<\/b> are the most commonly used Visualization Libraries with <b>Matplotlib<\/b> as the most used followed by <b>Seaborn<\/b>.<\/p>","80018f26":"<p><b>Inference : <\/b><b>Cloud VMs<\/b> are the mostly used product.<\/p>","5f98d2bd":"<p><b>Inference : <\/b>For external sources, <b>Cousera<\/b>, <b>Kaggle Courses<\/b> and <b>Udemy<\/b> stands forward. In the case formal education, <b>University Courses<\/b> or a <b>Degree<\/b> is preferrred.<\/p>","bb5aeabe":"<h1>Question 15 - For how many years have you used machine learning methods?<\/h1>","909ee872":"<h1>Question 30 Part B - In the next 2 years, do you hope to become more familiar with any of these specific data storage products?<\/h1>","b932bef6":"<p><b>Inference : <\/b><b>AWS<\/b> is used on a large number followed by <b>GCP<\/b> and <b>Azure<\/b>. They can be either for personal work or for Professional Purpose.<\/p>","500695b5":"<h2>Question 10 - Which of the following hosted notebook products do you use on a regular basis?<\/h2>","4fa01b4f":"<h1>Question 26 - Approximately how much money have you (or your team) spent on machine learning and\/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?<\/h1>","df657d83":"<h1>Question 38 Part B - In the next 2 years, do you hope to become more familiar with any of these tools for managing ML experiments<\/h1>","219b7605":"<h1>Question 39 - Where do you publicly share your data analysis or machine learning applications?<\/h1>"}}