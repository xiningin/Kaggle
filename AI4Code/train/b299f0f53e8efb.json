{"cell_type":{"e0afe006":"code","5151e0e4":"code","748e3bbf":"code","c2a33bc9":"code","c5ddbc7b":"code","6217dc44":"code","4d29ec6f":"code","661f1097":"code","c44ed3f2":"code","54b70b37":"code","126538e8":"code","70a3529a":"code","7a48a4d8":"code","ed3e6a85":"code","b4986310":"code","52bb3a6f":"code","8a4fb567":"code","50a8593f":"code","96207f16":"code","09db6a3c":"code","d09fce61":"code","99b70123":"code","bd3ab8fc":"code","7ac098f0":"code","700f6a88":"code","b95f7341":"code","11dbd43a":"markdown","43e82f95":"markdown","810083f4":"markdown","bc381045":"markdown","7db0a97d":"markdown","f1477047":"markdown"},"source":{"e0afe006":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n","5151e0e4":"# import lib \nimport torch\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","748e3bbf":"#fix_random_state\nnp.random.seed(17)\ntorch.manual_seed(17)\ntorch.cuda.manual_seed(17)","c2a33bc9":"# Checking device: cuda or cpu\ntorch.cuda.is_available()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice ","c5ddbc7b":"#Create transforms. Resize and add random color transformation\nsize_pictures = 32\ntransforms = transforms.Compose([transforms.Resize((size_pictures, size_pictures)),\n                                 transforms.ToTensor(),\n                                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0)])","6217dc44":"# load pictures\ndataset_path = '\/kaggle\/input\/credit-card-number-images\/Credit Card Number Dataset\/'\ndataset = datasets.ImageFolder(root=dataset_path,transform=transforms)","4d29ec6f":"#Show random pictures with transformation\nplt.rcParams[\"font.family\"] = 'serif'\n\ndf = [list(i) for i in dataset]\nX = []\ny = []\nfor i in df:\n    X.append(i[0])\n    y.append(i[1])\nX = torch.stack(X)\n\nn = 10 # count_random_elements\n\nnumber_of_rows = X.shape[0]\nrandom_index = np.random.choice(number_of_rows, size=n, replace=False)\nrandom_elements = X[random_index,:]\n\nplt.figure(figsize=(18,18));\nfor i in range(n):\n    plt.subplot(1,n, i + 1)\n    plt.imshow(random_elements[i].permute(1, 2, 0));\n    plt.title(f'label: {y[random_index[i]]}');\n    plt.yticks([]);\n    plt.xticks([]);\n","661f1097":"#Show data balance\nlabels_count = {}\nfor image in dataset.imgs:\n    label = image[1]\n    if label not in labels_count.keys():\n        labels_count[label] = 0\n    labels_count[label] += 1\nclass_counts = labels_count.copy()","c44ed3f2":"#Visualize\nfor n,key in enumerate(labels_count):\n    plt.bar(list(labels_count.keys())[n],\n            list(labels_count.values())[n],\n            linewidth = 1, edgecolor='maroon',\n            label = f'{list(labels_count.keys())[n]}- {list(labels_count.values())[n]}');     \nplt.title('Number of elements in classes', fontsize=16)\nplt.xticks(range(0,10), fontsize=12 );\nplt.yticks((min(list(labels_count.values())), max(list(labels_count.values()))), fontsize=14);\nplt.legend(bbox_to_anchor=(1,1),fontsize=12, title_fontsize=14, markerscale=None);","54b70b37":"# Attempt 1 \nsplit_value = 0.2\ndataset_lenght = X.shape[0]\ntest_size =  int(dataset_lenght * split_value) \ntrain_size = dataset_lenght - test_size\nprint(f'train size:{train_size}, test size:{test_size}')","126538e8":"# Used  pytorch train\/test split \ntrain_set, val_set = torch.utils.data.random_split(dataset, (train_size, test_size)) ","70a3529a":"labels_count = {}\nfor label in train_set:\n    label = image[1]\n    if label not in labels_count.keys():\n        labels_count[label] = 0\n    labels_count[label] += 1\n    \nlabels_count \n#Bad idea","7a48a4d8":"#What part of all the data is this label?\nclass_weights = list(class_counts.values())\nclass_weights \/= np.sum(class_weights)\nsampler = torch.utils.data.sampler.WeightedRandomSampler(class_weights, sum(class_counts.values()))\nw = np.around(class_weights,2)\nw","ed3e6a85":"# Attempt 2\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)","b4986310":"unique, counts = np.unique(y_test, return_counts=True)\nclass_test_count = dict(zip(unique, counts))\nclass_train_count = {}\nfor key in class_test_count:\n    class_train_count[key] = class_counts[key] - class_test_count[key]\nprint(class_counts)\nprint(class_test_count)","52bb3a6f":"X_test_qty = list(class_test_count.keys())\nX_train_qty = list(class_train_count.keys())\ny_test_qty = list(class_test_count.values())\ny_train_qty = list(class_train_count.values())\n","8a4fb567":"# All labels are in the training and test set\nwidth = 0.4\nplt.title('Number of instances', fontsize=14)\nplt.bar([i - width \/ 2 for i in X_test_qty], y_test_qty, width, color ='seagreen', label='test')\nplt.bar([i + width \/ 2 for i in X_train_qty], y_train_qty, width, color ='navy', label='train')\nplt.xticks(range(0,10), fontsize=12 );\nplt.yticks([]);\nplt.legend(bbox_to_anchor=(1,1),fontsize=12, title_fontsize=12, markerscale=None);","50a8593f":"# Labels in train and test in the same proportions\nfor n,i in enumerate(class_test_count.keys()):\n    print(f'label: {n}| w in df: {w[n]:.2f}| w in test:{round(class_test_count[i] \/ sum(list(class_test_count.values())),2):.2f}')","96207f16":"from torch.utils.data import Dataset, DataLoader\nclass NumberDataset(Dataset):\n    def __init__(self):\n        self.x = X_train\n        self.y = y_train\n        self.n_samples = X_train.shape[0]\n        \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n        \n    def __len__(self):\n        return self.n_samples","09db6a3c":"class NumberDatasetTEST(Dataset):\n    def __init__(self):\n        self.x = X_test\n        self.y = y_test\n        self.n_samples = X_test.shape[0]\n        \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n        \n    def __len__(self):\n        return self.n_samples","d09fce61":"ds_test = NumberDatasetTEST()\ndataset = NumberDataset()","99b70123":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n        self.conv2 = nn.Conv2d(in_channels=4, out_channels=10, kernel_size=5, padding=2)\n        self.act2 = nn.ReLU()\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n        self.fc1 = nn.Linear(640, 64 )\n        self.act3 = nn.ReLU()\n        \n        self.fc2 = nn.Linear(64, 32)\n        self.act4 = nn.Sigmoid()\n        \n        self.fc3 = nn.Linear(32, 10)\n        \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act1(x)\n        x = self.pool1(x)\n        \n        x = self.conv2(x)\n        x = self.act2(x)\n        x = self.pool2(x)  \n        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n        x = self.fc1(x)\n        x = self.act3(x)\n        \n        x = self.fc2(x)\n        x = self.act4(x)\n        \n        x = self.fc3(x)\n        return x","bd3ab8fc":"model = NeuralNetwork()\nmodel = model.to(device)\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),  lr = 10 **(-3))\nepoch = 2000\nbatch_size = 32","7ac098f0":"acc = []\ndef training(model, batch_size, epochs, loss, optimizer):\n    for epoch in range(1, epochs + 1):\n        model.train()\n        dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n        for (X_batch,y_batch) in dataloader: \n            optimizer.zero_grad()\n            \n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            preds = model.forward(X_batch)\n            \n            loss_value = loss(preds, y_batch.long())\n            loss_value.backward()\n            \n            optimizer.step()\n\n        dataloader_test = DataLoader(dataset=ds_test, batch_size=266)\n        model.eval()\n        with torch.no_grad():\n            summa = 0\n            for (X_batch,y_batch) in dataloader_test:\n                X_batch = X_batch.to(device)\n                y_batch = y_batch.to(device)\n                preds = model.forward(X_batch)\n                preds = torch.max(F.softmax(preds, dim=1), dim=1)\n                correct= torch.eq(preds[1], y_batch)\n                summa += torch.sum(correct).item()\n\n            acc.append(summa \/ len(X_batch))\n            if ((epoch + 1) % 5 == 0 and epoch < 50) or (epoch + 1) % 50 == 0:\n                print(f'epoch: {epoch + 1}, acc:{acc[-1]:.2%}')","700f6a88":"%%time\ntraining(model, batch_size, epoch, loss, optimizer)","b95f7341":"#Vizualize accuracy_score\nacc_max = max(acc)\ny_acc_max = acc.index(max(acc))\nplt.title('Accuracy', fontsize = 15 );\nplt.xticks([1,500,1000,1500,2000]);\nplt.scatter( y_acc_max, acc_max, color='aqua', label = f'max_value:{acc_max:.2%}');\nplt.plot(acc, color='darkslategray', );\nplt.grid()\nplt.legend(fontsize = 12);","11dbd43a":"# 2. TRAIN \/ TEST ","43e82f95":"# 5. CREATE MODEL TRAINING","810083f4":"# 3. CREATE TEST\/TRAIN DATASET","bc381045":"# 4. CREATE NETWORK","7db0a97d":"### Data isn't balanced!","f1477047":"# 1.LOAD DATA"}}