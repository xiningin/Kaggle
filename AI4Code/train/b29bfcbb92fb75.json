{"cell_type":{"b429ccba":"code","2a51d7db":"code","a17dab1a":"code","a08c4f69":"code","470c26b8":"code","3dae4629":"code","3480c2b5":"code","4f1f47a3":"code","c9afbb6d":"code","f19a21e3":"code","2b801ba6":"code","5784a8a1":"code","8f021dd3":"code","3532fe6b":"code","51cb6f04":"code","33854c56":"code","c32cda85":"code","e9793c53":"code","af6e62a1":"code","bfde84ce":"code","8f1a39a0":"markdown","4265d11e":"markdown","b5e8fbf2":"markdown","3744c3f1":"markdown"},"source":{"b429ccba":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom xgboost import XGBClassifier\nimport time\nfrom sklearn.feature_selection import SelectKBest\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np","2a51d7db":"holdout = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","a17dab1a":"train.columns","a08c4f69":"holdout.columns","470c26b8":"# Separate Survived label since it only exists in the train dataset\nsurvived = train[\"Survived\"]\ntrain = train.drop(\"Survived\",axis=1)","3dae4629":"holdout.shape","3480c2b5":"train.shape","4f1f47a3":"## concatenate all data to guarantee that holdout and train datasets have the same number of\n## features after one hote encoding\nall_data = pd.concat([train,holdout],axis=0)","c9afbb6d":"all_data.describe()","f19a21e3":"def process_ticket(df):\n    # see https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n    # Get first digit of tickets, which groups people by their ticket\n    Ticket = []\n    for i in list(df.Ticket):\n        if not i.isdigit():\n            #Take prefix\n            Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) \n        else:\n            Ticket.append(\"X\")\n    df[\"Ticket\"] = Ticket\n    return df\n\ndef process_missing(df):\n\n    # process missing values in the features that need it\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n    return df\n\ndef process_age(df):\n\n    # group people by age\n\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    cut_points = [-1,0,5,12,18,35,60,100]\n    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    \n    df = df.drop(\"Age\",axis=1)\n    \n    return df\n\ndef process_fare(df):\n\n    # group fares by price\n\n    cut_points = [-1,12,50,100,1000]\n    label_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n    \n    df = df.drop(\"Fare\",axis=1)\n    \n    return df\n\ndef process_cabin(df):\n\n    # Group people by the firs letter of cabin, whihc indicates in which deck \n    # was their cabin\n\n    df[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'Unknown' for i in df[\"Cabin\"]])\n\n    # aux  = df[\"Cabin\"]\n    # df[\"Cabin\"] = aux.str[0]\n    # df[\"Cabin\"] = df[\"Cabin\"].fillna(\"Unknown\")\n\n    return df\n\ndef process_family_size(df):\n\n   # Uses information in SibSp and Parch to compute the family size of an someone,\n   # including themselves\n\n    Fsize = df[\"SibSp\"] + df[\"Parch\"] + 1\n    \n    cut_points = [-1,1,2,4,20]\n    label_names = [\"Single\",\"Small\",\"Medium\",\"Big\"]\n    df[\"Fsize\"] = pd.cut(Fsize,cut_points,labels=label_names)\n\n  # dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\n  # dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n  # dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n  # dataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)\n    return df\n\ndef process_titles(df):\n\n    # Extract peoples titles from their names. More influent passengers might have\n    # priority in escaping\n\n    titles = {\n        \"Mr\" :         \"Mr\",\n        \"Mme\":         \"Mrs\",\n        \"Ms\":          \"Mrs\",\n        \"Mrs\" :        \"Mrs\",\n        \"Master\" :     \"Master\",\n        \"Mlle\":        \"Miss\",\n        \"Miss\" :       \"Miss\",\n        \"Capt\":        \"Officer\",\n        \"Col\":         \"Officer\",\n        \"Major\":       \"Officer\",\n        \"Dr\":          \"Officer\",\n        \"Rev\":         \"Officer\",\n        \"Jonkheer\":    \"Royalty\",\n        \"Don\":         \"Royalty\",\n        \"Sir\" :        \"Royalty\",\n        \"Countess\":    \"Royalty\",\n        \"Dona\":        \"Royalty\",\n        \"Lady\" :       \"Royalty\"\n    }\n    extracted_titles = df[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\n    df[\"Title\"] = extracted_titles.map(titles)\n    return df\n\ndef create_dummies(df,column_name):\n\n    # Enconde variables with one hot encoding\n\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df","2b801ba6":"# Complete preprocessing function\n\ndef pre_process(df):\n    df = process_ticket(df)\n    df = process_missing(df)\n    df = process_age(df)\n    df = process_fare(df)\n    df = process_titles(df)\n    df = process_cabin(df)\n    df = process_family_size(df)\n\n    for col in [\"Age_categories\",\"Fare_categories\",\n                \"Title\",\"Cabin\",\"Sex\",\"Ticket\",\"Pclass\",\"Fsize\",\"Embarked\"]:\n        df = create_dummies(df,col)\n    \n    df = df.drop(columns=[\"Age_categories\",\"Fare_categories\",\n                \"Title\",\"Cabin\",\"Sex\",\"Ticket\",\"Pclass\",\"Fsize\",'Name','Embarked'],axis=1)\n    \n    return df\n\n# preprocess all data\nall_data_processed = pre_process(all_data)\n\n# separate training and holdout data, and add back \"Survived\" label to training data\nprocessed_data = all_data_processed.iloc[:891]\nprocessed_data = pd.concat([processed_data,survived],axis=1)\nholdout = all_data_processed.iloc[891:]","5784a8a1":"from sklearn.model_selection import train_test_split\n\nseed = 42\nX_train, X_test, y_train, y_test = train_test_split(processed_data.drop(columns=[\"Survived\",'PassengerId'],axis=1),\n                                                    processed_data[\"Survived\"],\n                                                    test_size=0.20,\n                                                    random_state=seed,\n                                                    shuffle=True,\n                                                    stratify=processed_data[\"Survived\"])","8f021dd3":"pipe = Pipeline(steps = [\n                          # (\"fs\",SelectKBest()),\n                         (\"clf\",XGBClassifier())])\n\n# During testing, it was found out that using all features worked best\n\nsearch_space = [\n                {\"clf\":[RandomForestClassifier()],\n                 \"clf__n_estimators\": [250,100,150,200],\n                 \"clf__criterion\": [\"entropy\",\"gini\"],\n                 \"clf__max_depth\": [4,6,8],\n                 \"clf__max_leaf_nodes\": [16,32,8,64],\n                 \"clf__random_state\": [seed],\n                 },\n                {\"clf\":[XGBClassifier()],\n                 \"clf__n_estimators\": [100,150,200],\n                 \"clf__max_depth\": [6,8],\n                 \"clf__learning_rate\": [0.001,0.1],\n                 \"clf__random_state\": [seed],\n                 \"clf__subsample\": [1.0],\n                 }\n                ]\nscoring = {'Accuracy': make_scorer(accuracy_score)}\nnum_folds = 10\n\n# create grid search\nkfold = StratifiedKFold(n_splits=num_folds,random_state=seed)\n\n# return_train_score=True\ngrid = GridSearchCV(estimator=pipe, \n                    param_grid=search_space,\n                    cv=kfold,\n                    scoring=scoring,\n                    return_train_score=True,\n                    n_jobs=-1,\n                    refit=\"Accuracy\")\n\ntmp = time.time()\n\n# fit grid search\nbest_model = grid.fit(X_train,y_train)\n\nprint(\"CPU Training Time: %s seconds\" % (str(time.time() - tmp)))","3532fe6b":"print(\"Best: %f using %s\" % (best_model.best_score_,best_model.best_params_))","51cb6f04":"result = pd.DataFrame(best_model.cv_results_)\nresult.head()","33854c56":"result_acc = result[['mean_train_Accuracy', 'std_train_Accuracy','mean_test_Accuracy', 'std_test_Accuracy','rank_test_Accuracy']].copy()\nresult_acc[\"std_ratio\"] = result_acc.std_test_Accuracy\/result_acc.std_train_Accuracy\nresult_acc.sort_values(by=\"rank_test_Accuracy\",ascending=True)","c32cda85":"# best model\npredict_first = best_model.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test, predict_first))","e9793c53":"# Drop the PassengerId before predicting!\npredict_final = best_model.best_estimator_.predict(holdout.drop(columns=[\"PassengerId\"],axis=1))","af6e62a1":"holdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": predict_final}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission.csv\",index=False)","bfde84ce":"files.download('submission.csv') ","8f1a39a0":"# 1 - Introducing data science workflows\n","4265d11e":"\n#3 - Building the model\n\n","b5e8fbf2":"# 2 - Preprocesing the Data","3744c3f1":"I obtained a final score of 0.79904 in the competition, which, at the time, put me in 1636 out of 13782. I could not improve it futher with grid search alone. I want to revisit this problem when I have more time, and see if I can push past 0.82%! My next step is to try futher feature engineering and selecion. I also need to do a more in-depth EDA to allow me to select features better."}}