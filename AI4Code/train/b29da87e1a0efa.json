{"cell_type":{"d765d25d":"code","570929db":"code","46e64759":"code","18070a44":"code","2652f8c7":"code","0417913d":"code","4e3f724e":"code","18b1d6e5":"code","a0f547cc":"code","0e6e568b":"code","daef4ca6":"code","40ec98fc":"code","33a00850":"code","4afd2713":"code","d5d1d2b4":"code","32912937":"code","e2e80a5f":"code","37eadb59":"code","6cd8fcfd":"code","a1ff3c25":"code","88f69555":"code","58dc4b59":"code","574d3991":"code","c657e21b":"code","9e54213e":"code","718cfd8e":"code","62034204":"code","c810b624":"code","f9f05ca3":"code","671d17d5":"code","a5b20505":"code","e661a2c6":"code","e0661385":"code","9f38b44c":"code","3eb090d1":"markdown","f4735fe9":"markdown","34a6226c":"markdown","07275f14":"markdown","af4138e6":"markdown","56941b5d":"markdown","474e6c79":"markdown","a7a4d5bc":"markdown","a99fbc2d":"markdown","a17807c6":"markdown","d4aceda7":"markdown","f403a6f3":"markdown","38cb9ef2":"markdown","2c9a2ce0":"markdown","92251d2b":"markdown","109d02ac":"markdown","efba8ec6":"markdown","a0f5a227":"markdown","fa5864fc":"markdown","7f436ae4":"markdown","63161558":"markdown","5d67332a":"markdown","a5de9ca5":"markdown","3539f366":"markdown","8096ff3d":"markdown","987f061d":"markdown","4d3f0674":"markdown","1e715248":"markdown","a9143175":"markdown","ccfee0a9":"markdown","c662042d":"markdown","80ed2b57":"markdown","a070598c":"markdown"},"source":{"d765d25d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os","570929db":"df=pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","46e64759":"df.dtypes","18070a44":"df.isnull().any()","2652f8c7":"front=df['Attrition']\ndf.drop(labels=['Attrition'],axis=1,inplace=True)\ndf.insert(0,'Attrition',front)\ndf.head()","0417913d":"#Delete Unwanted Records\ndf.drop(labels=['EmployeeCount','EmployeeNumber','StockOptionLevel','StandardHours'],axis=1,inplace=True)\ndf.head()","4e3f724e":"#df['Gender']=df['Gender'].map({'Male':0,'Female':1}) Map doesnt work\nAttrition={'Yes':1,'No':0}\ndf.Attrition=[Attrition[item] for item in df.Attrition]","18b1d6e5":"#Get categorical values of column \ndf.EducationField.unique()","a0f547cc":"# creating a dict file \nGender={'Male':1,'Female':0}\n# traversing through dataframe Gender column and writing values where key matches\ndf.Gender=[Gender[item] for item in df.Gender]\n\nField={'Life Sciences':2,'Medical':1,'Other':0,'Marketing':3,'Technical Degree':4,'Human Resources':5}\ndf.EducationField=[Field[item] for item in df.EducationField]","0e6e568b":"#Summary based on Attrition\ndf1=df.groupby('Attrition')\ndf1.mean()","daef4ca6":"corr=df.corr()\ncorr=(corr)\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr,\n           xticklabels=corr.columns.values,\n           yticklabels=corr.columns.values,cmap='Blues')\ncorr\n","40ec98fc":"Attrition_Rate=df.Attrition.value_counts()\/len(df)\nAttrition_Rate","33a00850":"sns.barplot(x='Attrition',y='MonthlyIncome',hue='Gender',data=df,color='green').set_title('Employee Income Gender Distribution')\nplt.figure(figsize=(10, 10))\nplt.show()","4afd2713":"sns.barplot(x='Attrition',y='DistanceFromHome',hue='Gender',data=df,color='blue').set_title('Employee Distance Gender Distribution')\nplt.show()","d5d1d2b4":"df['Income_Range']=pd.cut(df['MonthlyIncome'],[1000,5000,10000,15000,20000])","32912937":"f, ax = plt.subplots(figsize=(15, 4))\nsns.countplot(y='Income_Range',hue='Attrition',data=df).set_title('Employee Salary Attrition Distribution')\nplt.plot()","e2e80a5f":"fig=plt.figure(figsize=(15,4))\nax=sns.kdeplot(df.loc[(df['Attrition']==0),'JobSatisfaction'],color='g',shade=False,label='No Attrition')\nax=sns.kdeplot(df.loc[(df['Attrition']==1),'JobSatisfaction'],color='r',shade=True,label='Attrition')\nax.set(xlabel='Employee Job Satisfaction Rating',ylabel='Frequency')\nplt.title('Employee Job Satisfaction Rating - Attrition vs No Attrition')","37eadb59":"fig=plt.figure(figsize=(15,4))\nax=sns.kdeplot(df.loc[(df['Attrition']==0),'WorkLifeBalance'],color='g',shade=False,label='No Attrition')\nax=sns.kdeplot(df.loc[(df['Attrition']==1),'WorkLifeBalance'],color='r',shade=True,label='Attrition')\nax.set(xlabel='Employee WorkLifeBalance Rating',ylabel='Frequency')\nplt.title('Employee WorkLifeBalance Rating - Attrition vs No Attrition')","6cd8fcfd":"fig=plt.figure(figsize=(15,4))\nax=sns.kdeplot(df.loc[(df['Attrition']==0),'RelationshipSatisfaction'],color='g',shade=False,label='No Attrition')\nax=sns.kdeplot(df.loc[(df['Attrition']==1),'RelationshipSatisfaction'],color='r',shade=True,label='Attrition')\nax.set(xlabel='Employee RelationshipSatisfaction Rating',ylabel='Frequency')\nplt.title('Employee Relationship Satisfaction Rating - Attrition vs No Attrition')","a1ff3c25":"fig=plt.figure(figsize=(15,4))\nax=sns.kdeplot(df.loc[(df['Attrition']==0),'YearsAtCompany'],color='g',shade=False,label='No Attrition')\nax=sns.kdeplot(df.loc[(df['Attrition']==1),'YearsAtCompany'],color='r',shade=True,label='Attrition')\nax.set(xlabel='Employee YearsAtCompany ',ylabel='Frequency')\nplt.title('Employee YearsAtCompany - Attrition vs No Attrition')","88f69555":"fig=plt.figure(figsize=(15,8))\nvalue=df['YearsAtCompany']<11\ndf3=df[value]\nsns.countplot(x='YearsAtCompany',hue='Attrition',data=df3)\nplt.show()","58dc4b59":"fig=plt.figure(figsize=(10,6))\nsns.countplot(x='YearsWithCurrManager',hue='Attrition',data=df,color='black')\nplt.show()","574d3991":"fig=plt.figure(figsize=(10,6))\nsns.countplot(x='YearsSinceLastPromotion',hue='Attrition',data=df,color='green')\nplt.show()","c657e21b":"total_records= len(df)\ncolumns = [\"Gender\",\"MaritalStatus\",\"WorkLifeBalance\",\"EnvironmentSatisfaction\",\"JobSatisfaction\",\n           \"JobLevel\",'NumCompaniesWorked',\"JobInvolvement\",\"BusinessTravel\",'Department']\n\nj=0\nfor i in columns:\n    j +=1\n    plt.subplot(5,2,j)\n    ax1 = sns.countplot(data=df,x= i,hue=\"Attrition\")\n    if(j==9 or j== 10):\n        plt.xticks( rotation=90)\n    for p in ax1.patches:\n        height = p.get_height()\n        #ax1.text(p.get_x()+p.get_width()\/2.,\n               # height + 3,\n                #'{:1.2f}'.format(height\/total_records,0),\n                #ha=\"center\",rotation=0) \n\n# Custom the subplot layout\nplt.subplots_adjust(bottom=0.1, top=4)\nplt.show()","9e54213e":"#Selecting numeric paremeters for Feature Engineering\ndf3=df[['JobLevel','EnvironmentSatisfaction','JobInvolvement','JobSatisfaction','PerformanceRating','RelationshipSatisfaction','WorkLifeBalance','Attrition']]","718cfd8e":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,6)\n\n# Create train and test splits\ntarget_name = 'Attrition'\nX = df3.drop('Attrition', axis=1)\n\ny=df3[target_name]\nX_train,X_test,y_train,t_test=train_test_split(X,y,test_size=0.15, random_state=123, stratify=y)\n\ndtree = tree.DecisionTreeClassifier(\n    #max_depth=3,\n    class_weight=\"balanced\",\n    min_weight_fraction_leaf=0.01\n    )\ndtree = dtree.fit(X_train,y_train)\n\n## plot the importances ##\nimportances = dtree.feature_importances_\nfeat_names = df3.drop(['Attrition'],axis=1).columns\n\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12,6))\nplt.title(\"Feature importances by DecisionTreeClassifier\")\nplt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\nplt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\nplt.xlim([-1, len(indices)])\nplt.show()\n","62034204":"# Create an intercept term for the logistic regression equation\ndf['value'] = 1\nindep_var = ['JobLevel','JobInvolvement','EnvironmentSatisfaction','value', 'Attrition']\ndf = df[indep_var]\n\n# Create train and test splits\ntarget_name = 'Attrition'\nX = df.drop('Attrition', axis=1)\n\ny=df[target_name]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=123, stratify=y)\n\nX_train.head()","c810b624":"import statsmodels.api as sm\niv = ['JobLevel','JobInvolvement','EnvironmentSatisfaction', 'value']\nlogReg = sm.Logit(y_train, X_train[iv])\nanswer = logReg.fit()\n\nanswer.summary\nanswer.params","f9f05ca3":"# Create function to compute coefficients\ncoef = answer.params\ndef y (coef,JobLevel,JobInvolvement , EnvironmentSatisfaction) : \n    return coef[3] + coef[0]*JobLevel + coef[1]*JobInvolvement + coef[2]*EnvironmentSatisfaction\n\nimport numpy as np\n\n# An Employee Having at level 1 and rating 1 for EnvironmentSatisfaction and 1 for JobInvolvement a 54% chance of attrition\ny1 = y(coef, 1, 1, 1)\np = np.exp(y1) \/ (1+np.exp(y1))\np","671d17d5":"# Compare the Logistic Regression Model V.S. Decision Tree Model V.S. Random Forest Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom numpy.core.umath_tests import inner1d\n\n\n#Logistic Regression Model\nmodel1 = LogisticRegression(class_weight=\"balanced\",)\nmodel1.fit(X_train, y_train)\nprint (\"\\n\\n ---Logistic Model---\")\nlogit_roc_auc = roc_auc_score(y_test, model1.predict(X_test))\nprint (\"Logistic AUC = %2.2f\" % logit_roc_auc)\nprint(classification_report(y_test, model1.predict(X_test)))\n\n#Decision Tree Model\nmodel2=DecisionTreeClassifier(min_weight_fraction_leaf=0.01,class_weight=\"balanced\",)\nmodel2.fit(X_train,y_train)\nprint(\"\\n\\n ---Decision Tree Model ---\")\ndtree_roc_auc=roc_auc_score(y_test,model2.predict(X_test))\nprint(\"Decision Tree AUC = %2.2f\" % dtree_roc_auc)\nprint(classification_report(y_test,model2.predict(X_test)))\n\n#Random Forest Model\nmodel3=RandomForestClassifier( n_estimators=1000,max_depth=None,min_samples_split=10,class_weight=\"balanced\")\nmodel3.fit(X_train,y_train)\nprint(\"\\n\\n --- Random Forest Model ----\")\nrforest_roc_auc=roc_auc_score(y_test,model2.predict(X_test))\nprint(\"Random forest AUC = %2.2f\" % rforest_roc_auc)\nprint(classification_report(y_test,model3.predict(X_test)))\n\n","a5b20505":"# Using 10 fold Cross-Validation to train Logistic Regression Model\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = LogisticRegression(class_weight = \"balanced\")\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","e661a2c6":"# Using 10 fold Cross-Validation to train Decision Tree Model\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = DecisionTreeClassifier(class_weight = \"balanced\",min_weight_fraction_leaf=0.01)\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","e0661385":"# Using 10 fold Cross-Validation to train Random Forest Model\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestClassifier\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = RandomForestClassifier(n_estimators=1000,max_depth=None,min_samples_split=10,class_weight=\"balanced\")\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","9f38b44c":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, model1.predict_proba(X_test)[:,1])\n#The first column is the probability that the entry has the -1 label \n#and the second column is the probability that the entry has the +1 label.\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, model3.predict_proba(X_test)[:,1])\ndt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, model2.predict_proba(X_test)[:,1])\n\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rforest_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree (area = %0.2f)' % dtree_roc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","3eb090d1":"### Conclusion:\n\n### Monthly income,Job level is highly correlated with TotalWorkingYears\n### PerformanceRating is highly correlated with PercentSalaryhike","f4735fe9":"The company wants to understand what factors contributed most to employee turnover and to create a model that can predict if a certain employee will leave the company or not. The goal is to create or improve different retention strategies on targeted employees. Overall, the implementation of this model will allow management to create better decision-making actions.","34a6226c":"### 3.7 Employee Relationship Satisfaction vs Attrition","07275f14":"### 4.1 Feature Engineering\n","af4138e6":"### 4.2  Using Logistic Regression Coefficients","56941b5d":"### 1.a Know the Datatypes\n","474e6c79":"### 3.1 Correlation between parameters","a7a4d5bc":"Summary :\n\nBy using a decision tree classifier, it could rank the features used for the prediction. The top three features were JobLevel, JobInvolvment, and EnviornmentSatisfaction. This is helpful in creating our model for logistic regression because it\u2019ll be more interpretable to understand what goes into our model when we utilize less features.","a99fbc2d":"### 3.6 Employee WorkLifeBalance Rating vs Attrition","a17807c6":"# Business Problem","d4aceda7":"### 3.5 Employee Job Satisfaction Rating vs Attrition","f403a6f3":"### Part 3: Exploring the Data","38cb9ef2":"### 3.9  Employee YearsSinceLastPromotion  vs Attrition\n","2c9a2ce0":"### 3.8  Employee YearsAtCompany vs Attrition","92251d2b":"### 3.10  Analysis of parameter vs Attrition","109d02ac":"Parameters and their score range","efba8ec6":"### 4 . Modeling the Data: Logistic Regression Analysis","a0f5a227":"IBM is an American multinational technology company and they wants to know why employees are leaving the company(Fictional dataset)","fa5864fc":"Logistic Regression commonly deals with the issue of how likely an observation is to belong to each group. This model is commonly used to predict the likelihood of an event occurring. In contrast to linear regression, the output of logistic regression is transformed with a logit function. This makes the output either 0 or 1. This is a useful model to take advantage of for this problem because we are interested in predicting whether an employee will leave (0) or stay (1).\n\nAnother reason for why logistic regression is the preferred model of choice is because of its interpretability. Logistic regression predicts the outcome of the response variable (turnover) through a set of other explanatory variables, also called predictors. In context of this domain, the value of our response variable is categorized into two forms: 0 (zero) or 1 (one). The value of 0 (zero) represents the probability of an employee not leaving the company and the value of 1 (one) represents the probability of an employee leaving the company.\n","7f436ae4":"### Conclusion:\n\n### Employee leaving the company at initial stage","63161558":"### 3.2  Employee income & Gender vs Attrition","5d67332a":"### 3.9  Employee YearswithCurrentManager  vs Attrition\n","a5de9ca5":"Education 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n\nEnvironmentSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobInvolvement \n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nPerformanceRating \n1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\n\nRelationshipSatisfaction \n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nWorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'\n\nStandardHours=80","3539f366":"Converting Monthly income into 5 income range","8096ff3d":"# Objective","987f061d":"### Part 1: Obtaining the Data","4d3f0674":"###  Compare Logistic Regression Model V.S. Decision Tree Model V.S. Random Forest Model","1e715248":"### Part 2: Scrubbing the Data","a9143175":"### 3.3 DistanceFromHome  & Gender vs Attrition","ccfee0a9":"### Looks like about 83% of employees stayed and 16% of employees left\n","c662042d":"Modeling the Data: Logistic Regression Analysis","80ed2b57":"### 3.4  Employee Salary vs Attrition","a070598c":"### 4.3  Function to compute coefficients"}}