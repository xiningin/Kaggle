{"cell_type":{"b8e4b709":"code","5a2c40fe":"code","0f28bd4b":"code","a3ff50a0":"code","e3ae2c9c":"code","afbef264":"code","d7bb0bb1":"code","3fc2b95d":"code","12db97c1":"code","02dd98fc":"code","71bcc0da":"code","338b70b3":"code","0d23886d":"code","6a338820":"code","0211203d":"code","60bb5f9c":"code","de07ea4f":"code","4a6b21d0":"code","126b07c2":"code","e80a81f7":"code","4f58efe3":"code","7de25e6b":"code","b2d30ce2":"code","23d34f63":"code","516a45dd":"code","9c4ed2fd":"code","01a89ddb":"code","0c3af394":"code","4306ca68":"code","b3ad3d9c":"code","983f651c":"code","f686dd96":"code","ff11ed3d":"code","7699c8c4":"code","23acbf09":"code","cd332841":"code","c3d41e76":"code","e851664f":"code","1bb1e8a9":"code","520b057d":"code","79054d29":"code","51619b48":"code","2a058c23":"code","d34964ac":"code","c5ed6f26":"code","aaaa6a08":"code","43a6859b":"code","4f1ab7e7":"code","2d758d46":"code","19ac176f":"code","2f488f44":"code","5533b4da":"code","deae7b58":"code","a8626924":"code","39dd44cd":"code","b1c70e75":"code","b7dc574c":"code","d3dbbb97":"code","ec250479":"code","b7956117":"code","9afb828f":"code","a8105646":"code","eed1f16f":"code","c78aba3c":"code","e5bba3a6":"code","c7fc5ba5":"code","86843138":"code","58877151":"code","506d9fbf":"code","eb9d7be1":"code","0740d138":"code","96c2a4b5":"code","b4979423":"code","d0f8fb81":"code","ed444732":"code","1e2f1a58":"code","f7cde44e":"code","71d8ad63":"code","0bbe6fa4":"code","eaa3cf0c":"code","da5f88ee":"code","0018cf0b":"code","ed40f322":"code","9599ebb9":"code","4365bd47":"code","dc8d0c8b":"code","c3567955":"code","868ad27e":"code","3637e61e":"code","2871385e":"code","8967b138":"code","527998bc":"code","0a8c0838":"code","e138daef":"code","2f83f2a0":"code","f3b65e68":"code","01e31ebe":"code","8331cdba":"code","e0332b66":"code","49c871c0":"code","3fe61bdf":"code","49cfc7a3":"code","2afde6eb":"code","4af05d0e":"code","6daca003":"code","8cb74ee3":"code","57973a8b":"markdown","b4325ddc":"markdown","910a1b10":"markdown","8a2a4fb6":"markdown","e8b6ee27":"markdown","66d27951":"markdown","de867322":"markdown","1baee170":"markdown","45f19d62":"markdown","ebdb6692":"markdown","78d661c9":"markdown","6cdd81bb":"markdown","3af6d44b":"markdown","64b1f524":"markdown","ed07e015":"markdown","15c7b72d":"markdown","14960904":"markdown","ea5c6f0f":"markdown","0d4206f6":"markdown","5e529120":"markdown","c4b8ab78":"markdown","1984856e":"markdown","b72d93ec":"markdown","dd9807f8":"markdown","7ead6e87":"markdown","efe343a7":"markdown","1ddac9e5":"markdown","e2c55627":"markdown","7ba2d49e":"markdown","040af4a2":"markdown","123dce67":"markdown","003a6ef4":"markdown","1f1a6dcf":"markdown","10976490":"markdown","3cccb055":"markdown","8b03f561":"markdown","1f443c4e":"markdown"},"source":{"b8e4b709":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a2c40fe":"# EDA\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\n# Evaluation\nfrom sklearn.metrics import mean_squared_error\n\n# Algorithms\nimport lightgbm as lgbm\nfrom sklearn.linear_model import ElasticNetCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import LinearSVR\n","0f28bd4b":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv\")","a3ff50a0":"train","e3ae2c9c":"cat_feat = [x for x in train.columns if train[x].name.startswith(\"cat\")]\nnum_feat = [x for x in train.columns if train[x].name.startswith(\"cont\")]","afbef264":"skewed_features = train[num_feat].skew().sort_values(ascending=False)\nskewed_index = skewed_features.index","d7bb0bb1":"plt.figure(figsize=(8,5), dpi=110)\nskewed_features.plot(kind=\"bar\")\nplt.title(\"Numerical features skewness\");","3fc2b95d":"fig = plt.figure(figsize=(18, 30))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(10,3, i+1)\n    sns.histplot(x=train[col], bins=20, kde=True, color='green')\n    plt.tight_layout()\nfig.show()","12db97c1":"def skewness(df):\n    for col in num_feat:\n        q3 = np.quantile(df[col], 0.75)\n        q1 = np.quantile(df[col], 0.25)\n        iqr = q3 - q1\n        upper_limit = q3 + (1.5*iqr)\n        lower_limit = q1 - (1.5*iqr)\n        df = df[df[col].apply(lambda x: lower_limit <= x <= upper_limit)]\n    return df","02dd98fc":"train = skewness(train)","71bcc0da":"# Target column\nsns.displot(x=train['target'], kde=True)","338b70b3":"train['target'].describe()","0d23886d":"sns.displot(x=np.log1p(train['target']))","6a338820":"num_feat.append(\"target\")","0211203d":"corr = train[num_feat].corr()\nfig = plt.figure(figsize=(12,8))\nsns.heatmap(corr, annot=True, cmap=plt.cm.Reds)","60bb5f9c":"# This plot might take some time to plot it\n# sns.pairplot(data=train[num_feat])","de07ea4f":"# Different in the categorical values\nfor col in cat_feat:\n    diff = set(train[col].unique()) - set(test[col].unique())\n    #print(f\"Train column {col} has unique: {train[col].unique()}\")\n    #print(f\"Test column {col} has unique: {test[col].unique()}\")\n    print(f\"Train and Test dataset :{col}  unique different is: {diff}\")","4a6b21d0":"fig = plt.figure(figsize=(18,20))\n\nfor i, col in enumerate(cat_feat):\n    plt.subplot(8,3, i+1)\n    sns.boxplot(x=col, y='target', data=train)\n    plt.ylabel(\"target\")\n    plt.tight_layout()","126b07c2":"fig = plt.figure(figsize=(18, 30))\n\nfor i, col in enumerate(cat_feat):\n    plt.subplot(8,3, i+1)\n    sns.countplot(x=train[col], palette=\"viridis_r\")\n    plt.tight_layout()\nfig.show()","e80a81f7":"train.describe(include='object').T","4f58efe3":"all_data = pd.concat([train, test], axis=0)\n\nmissingno.matrix(all_data, figsize=(16,3))","7de25e6b":"#drop_list = [\"cat0\", \"cat2\", \"cat4\", \"cat6\", \"cat7\"]\n#all_data.drop(drop_list, axis=1, inplace=True)\n#for cat in drop_list:\n    #cat_feat.remove(cat)","b2d30ce2":"# Dummy variables\ndummy_data = pd.get_dummies(all_data[cat_feat], drop_first=True)\n\nfor col in cat_feat:\n    all_data.drop(col, axis=1, inplace=True)\n    \nconcat_data = pd.concat([dummy_data, all_data], axis=1)","23d34f63":"new_train = concat_data[concat_data['target'] >= 0]\nnew_test = concat_data[concat_data['target'].isna()]","516a45dd":"#for col in cat_feat:\n    #train[col] = pd.Series(train[col], dtype=\"category\")\n    #test[col] = pd.Series(test[col], dtype=\"category\")","9c4ed2fd":"X = new_train.drop([\"id\", \"target\"], axis=1)\ny = new_train['target']","01a89ddb":"# Let's find the base model_performance","0c3af394":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","4306ca68":"rmse_score = []\n\n\n \nparams ={\"objective\": \"regression\",\n         \"metric\": \"rmse\",\n         \"verbosity\": -1,\n         \"boosting_type\": \"gbdt\",\n         \"feature_fraction\": 0.5,\n         \"max_depth\": 10,\n         \"num_leaves\": 60,\n         \"lambda_l1\": 2,\n         \"lambda_l2\": 2,\n         \"learning_rate\": 0.01,\n         \"min_child_samples\":50,\n         \"bagging_fraction\": 0.7,\n         \"bagging_freq\": 1, \n         \"max_bin\": 80,}\n          #\"is_unbalance\":True,\n          #\"subsample\":0.3}\n    \n    \nlgb_train = lgbm.Dataset(X_train, y_train)\nlgb_val = lgbm.Dataset(X_val, y_val)\ngbm = lgbm.train(params,\n                 lgb_train,\n                 valid_sets=[lgb_train, lgb_val],\n                 num_boost_round=10000,\n                 verbose_eval=100,\n                 early_stopping_rounds=100,\n                 )\n    \n# Extra Boosting\nlgb_train = lgbm.Dataset(X_train, y_train)\nlgb_val = lgbm.Dataset(X_val, y_val)\nparams = {\"objective\": \"regression\",\n          \"metrics\": \"rmse\",\n          \"verbosity\": -1,\n          \"boosting_type\": \"gbdt\",\n          \"feature_fraction\": 0.5,\n          \"max_depth\": 10,\n          \"num_leaves\":200,\n          \"lambda_l1\": 2,\n          \"labmda_l2\": 2,\n          \"learning_rate\": 0.003,\n          \"min_child_samples\": 50,\n          \"max_bin\": 80,\n          #\"is_unbalance\":True,\n          #\"subsample\":0.3\n          \"bagging_fraction\": 0.7,\n          \"bagging_freq\": 1,}\n    \ngbm = lgbm.train(params,\n                 lgb_train,\n                 valid_sets = [lgb_train, lgb_val],\n                 verbose_eval = 100,\n                 num_boost_round = 10000,\n                 early_stopping_rounds=100,\n                 init_model = gbm)\n    \ny_pred = gbm.predict(X_val)\nrmse_score.append(np.sqrt(mean_squared_error(y_val, y_pred)))","b3ad3d9c":"id_col= new_test['id']\nnew_test = new_test.drop([\"id\",\"target\"], axis=1)","983f651c":"# Submmit to kaggle and see where we are.","f686dd96":"gbm1 = gbm.predict(new_test)","ff11ed3d":"first_sub = pd.DataFrame({\"id\":id_col,\n                          \"target\": gbm1})\n\nfirst_sub.to_csv(\"sub2_tbs_feb-2021.csv\", index=False)","7699c8c4":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold","23acbf09":"# I've used these parameters first\n#param_grid = {\"reg_alpha\":[0.1, 0.5, 1],\n              #\"max_depth\": [5,8,10,20],\n              #\"num_leaves\": [30,60,120],\n              #\"max_bin\":[20, 40, 80],\n              #\"n_estimators\":[100, 500, 1000]}","cd332841":"#kf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n#param_grid = {\"max_depth\": [20,40],\n              #\"num_leaves\": [120, 150, 200],\n              #\"max_bin\":[80, 120],\n              #\"n_estimators\":[1000, 1500]}\n\n#lgbm_model = lgbm.LGBMRegressor(objective=\"regression\",\n                                #metrics='rmse',\n                                #boosting_type='gbdt',\n                                #feature_fraction=0.5,\n                                #bagging_fraction=0.7,\n                                #learning_rate=0.01,\n                                #bagging_freq=1)\n\n#grid_lgbm = GridSearchCV(lgbm_model,\n                         #param_grid,\n                         #scoring='neg_root_mean_squared_error'\n                         #cv=kf,\n                         #verbose=2,\n                         #n_jobs=-1)\n\n#grid_lgbm.fit(X_train, y_train)","c3d41e76":"#grid_lgbm.best_score_","e851664f":"#grid_lgbm.best_params_","1bb1e8a9":"from sklearn.svm import LinearSVR","520b057d":"%%time\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nparam_grid = {\"loss\": [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n              \"C\": [1,5, 10, 100,]}\n\nlsvr_model = LinearSVR()\n\ngrid_svr = GridSearchCV(lsvr_model,\n                        param_grid, \n                        verbose=2,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kf,\n                        n_jobs=-1)\ngrid_svr.fit(X_train, y_train)","79054d29":"grid_svr.best_params_","51619b48":"params = grid_svr.best_params_","2a058c23":"l_svr = LinearSVR(**params)\nl_svr.fit(X_train, y_train)","d34964ac":"y_pred_lrsvr = l_svr.predict(X_val)\nrmse = np.sqrt(mean_squared_error(y_val, y_pred_lrsvr))\nrmse","c5ed6f26":"from xgboost import XGBRegressor","aaaa6a08":"kf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nparam_grid = {\"objective\":[\"reg:squarederror\"],\n              \"max_depth\": [6, 10, 14],\n              \"n_estimators\": [500, 1000, 1500]}\n\nxgboost_model = XGBRegressor()\n\nrandom_xgboost = RandomizedSearchCV(xgboost_model,\n                                    param_grid,\n                                    scoring=\"neg_root_mean_squared_error\"\n                                    verbose=2,\n                                    cv = kf,\n                                    n_jobs=-1)\n\n\nrandom_xgboost.fit(X_train,y_train)","43a6859b":"xgboost_model = XGBRegressor(objective=\"reg:squarederror\")\n\nxgboost_model.fit(X_train, y_train)","4f1ab7e7":"y_pred_xgb = xgboost_model.predict(X_val)\n\nrmse = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\nrmse","2d758d46":"from sklearn.ensemble import RandomForestRegressor","19ac176f":"rfr_model = RandomForestRegressor()\n\nrfr_model.fit(X_train, y_train)","2f488f44":"y_pred_rfr = rfr_model.predict(X_val)\nrmse = np.sqrt(mean_squared_error(y_val, y_pred_rfr))\nprint(f\"Random Forest Regressor\")\nprint(f\"RMSE: {rmse}\")","5533b4da":"from sklearn.linear_model import ElasticNetCV","deae7b58":"el_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\nel_model.fit(X_train, y_train)\n\ny_pred_el = el_model.predict(X_val)\nrmse_el = np.sqrt(mean_squared_error(y_val, y_pred_el))\nprint(f\"ElasticNetCV\")\nprint(f\"RMSE: {rmse_el}\")","a8626924":"el_model.l1_ratio_","39dd44cd":"from sklearn.ensemble import StackingRegressor","b1c70e75":"# Define the base models\nlevel_0 = list()\nlevel_0.append((\"el_model\", ElasticNetCV(l1_ratio=1.0)))\nlevel_0.append((\"rfr_model\", RandomForestRegressor()))\nlevel_0.append((\"xgboost\", XGBRegressor(objective=\"reg:squarederror\")))\nlevel_0.append((\"linearSVR\", LinearSVR(**params)))\n\n# Define meta learner model\nlevel_1 = lgbm.LGBMRegressor(objective=\"regression\",\n                             metrics='rmse',\n                             boosting_type='gbdt',\n                             feature_fraction=0.5,\n                             bagging_fraction=0.7,\n                             learning_rate=0.01,\n                             bagging_freq=1, \n                             max_bin=120,\n                             max_depth=40,\n                             n_estimators=1500,\n                             num_leaves=120)\n\n# Instantiate ensemble model\nens_model = StackingRegressor(estimators=level_0, \n                              final_estimator=level_1,\n                              cv=kf,\n                              verbose=2)\n\n# Fit the model\nens_model.fit(X_train, y_train)","b7dc574c":"y_pred_ens = ens_model.predict(X_val)\nrmse_ens = np.sqrt(mean_squared_error(y_val, y_pred_ens))\nprint(f\"Stacking Regressor\")\nprint(f\"RMSE: {rmse_ens}\")","d3dbbb97":"ens_model.fit(X,y)\n\n\nens_pred = ens_model.predict(new_test)","ec250479":"stack_sub = pd.DataFrame({\"id\": id_col,\n                         \"target\": ens_pred})\n\nstack_sub.to_csv(\"enemble_model_sub_tbs.csv\", index=False)","b7956117":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv\")","9afb828f":"cat_feat = [x for x in train.columns if train[x].name.startswith(\"cat\")]\nnum_feat = [x for x in train.columns if train[x].name.startswith(\"cont\")]","a8105646":"def skewness(df):\n    for col in num_feat:\n        q3 = np.quantile(df[col], 0.75)\n        q1 = np.quantile(df[col], 0.25)\n        iqr = q3 - q1\n        upper_limit = q3 + (1.5*iqr)\n        lower_limit = q1 - (1.5*iqr)\n        df = df[df[col].apply(lambda x: lower_limit <= x <= upper_limit)]\n    return df","eed1f16f":"train = skewness(train)","c78aba3c":"train[num_feat]","e5bba3a6":"#num_feat","c7fc5ba5":"quantile_list = [0, .25, .5, .75, 1.]\nquantiles = train['cont0'].quantile(quantile_list)\nquantiles","86843138":"fig, ax = plt.subplots(figsize=(6,4))\ntrain[\"cont0\"].hist(bins=30, edgecolor=\"black\", grid=False)\n\nfor quantile in quantiles:\n    plt.axvline(quantile, color='red')\n    \nax.set_title(\"Cont0 feature\")","58877151":"quantile_labels = [\"0-25Q\",\"25-50Q\",\"50-75Q\",\"70-100Q\"]\ntrain[\"cont0_range\"] = pd.qcut(train[\"cont0\"], q=quantile_list)\ntrain[\"cont0_labels\"] = pd.qcut(train[\"cont0\"], q=quantile_list, labels=quantile_labels)","506d9fbf":"train[[\"cont0\", \"cont0_range\", \"cont0_labels\"]].head()","eb9d7be1":"train[\"cont0_labels\"].value_counts()","0740d138":"sns.boxplot(x=\"cont0_labels\", y=\"target\", data=train)","96c2a4b5":"train.drop([\"cont0_range\", \"cont0_labels\"], axis=1, inplace=True)","b4979423":"def numerical_binning(df):\n    \n    quantile_list = [0, .25, .50, .75, 1.]\n    quantile_labels = [\"0-25Q\",\"25-50Q\",\"50-75Q\",\"75-100Q\"]\n    binning_df = pd.DataFrame()\n    \n    for col in num_feat:\n        #quantiles = df[col].quantile(quantile_list)\n        #binning_df[str(col)+\"_range\"] = pd.qcut(df[col], q=quantile_list)\n        binning_df[str(col)+\"_label\"] = pd.qcut(df[col], q=quantile_list, labels=quantile_labels)\n        \n    return binning_df","d0f8fb81":"new_train = numerical_binning(train)\nnew_train.head()","ed444732":"fig = plt.figure(figsize=(18,20))\n\nfor i, col in enumerate(new_train.columns):\n    plt.subplot(8,3, i+1)\n    sns.boxplot(x=new_train[col], y='target', data=train)\n    plt.ylabel(\"target\")\n    plt.tight_layout()","1e2f1a58":"train = train.drop('cat6', axis=1)\ntest = test.drop('cat6', axis=1)\n\ncat_feat.remove(\"cat6\")","f7cde44e":"from sklearn.preprocessing import KBinsDiscretizer\n\ntrans = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\n\ntrain[num_feat] = trans.fit_transform(train[num_feat])\ntest[num_feat] = trans.transform(test[num_feat])","71d8ad63":"train","0bbe6fa4":"# Get dummy variables for categorical features\ntrain_dummies = pd.get_dummies(train[cat_feat], drop_first=True)\ntest_dummies = pd.get_dummies(test[cat_feat], drop_first=True)","eaa3cf0c":"# Drop categorical columns\ntrain.drop(cat_feat, axis=1, inplace=True)\ntest.drop(cat_feat, axis=1, inplace=True)","da5f88ee":"# Concatenate the dataframes together\nnew_train = pd.concat([train_dummies, train], axis=1)\nnew_test = pd.concat([test_dummies, test], axis=1)","0018cf0b":"id_col = new_test['id']\nnew_test = new_test.drop(\"id\", axis=1)","ed40f322":"X = new_train.drop([\"id\", \"target\"], axis=1)\ny = new_train['target']","9599ebb9":"#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=45)","4365bd47":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor","dc8d0c8b":"def evaluate_model(model):\n    kf = KFold(n_splits=5, shuffle=True, random_state=45)\n    \n    n_scores = cross_val_score(model,\n                               X, y,\n                               scoring=\"neg_root_mean_squared_error\",\n                               verbose=2,\n                               n_jobs=-1,\n                               cv=kf)\n    return n_scores","c3567955":"xgboost_model = XGBRegressor()\n\nn_scores = evaluate_model(xgboost_model)","868ad27e":"print(f\"XGBoost RMSE: {np.mean(-n_scores)}\")","3637e61e":"from sklearn.neighbors import KNeighborsRegressor","2871385e":"k = round(np.sqrt(len(train)))","8967b138":"knn = KNeighborsRegressor(n_neighbors=k)\n\nknn_scores = evaluate_model(knn)","527998bc":"print(f\"KNN RMSE: {np.mean(-knn_scores)}\")","0a8c0838":"lsvr_model = LinearSVR()\n\nlsvr_scores = evaluate_model(lsvr_model)","e138daef":"print(f\"LinearSVR RMSE: {np.mean(-lsvr_scores)}\")","2f83f2a0":"#features = [col for col in X.columns if col.startswith(\"c\")]","f3b65e68":"set(new_test.columns) == set(X.columns)","01e31ebe":"X = new_train.drop([\"id\", \"target\"], axis=1)\ny = new_train['target']\n\nkf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n\n\n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.5,\n                  \"max_depth\": 10,\n                  \"num_leaves\": 120,\n                  \"lambda_l1\": 2,\n                  \"lambda_l2\": 2,\n                  \"learning_rate\": 0.01,\n                  \"min_child_samples\":50,\n                  #\"bagging_fraction\": 0.7,\n                  #\"bagging_freq\": 1, \n                  #\"max_bin\": 80,\n                  \"is_unbalance\":True,\n                  \"subsample\":0.3}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.5,\n                  \"max_depth\": 10,\n                  \"num_leaves\": 120,\n                  \"lambda_l1\": 2,\n                  \"lambda_l2\": 2,\n                  \"learning_rate\": 0.003,\n                  \"min_child_samples\":50,\n                  #\"bagging_fraction\": 0.7,\n                  #\"bagging_freq\": 1, \n                  #\"max_bin\": 80,\n                  \"is_unbalance\":True,\n                  \"subsample\":0.3}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1000,\n                            early_stopping_rounds=100,\n                            init_model = model\n                            )\n\n    \n    \n        y_pred_list.append(model.predict(X_val))\n        test_preds.append(model.predict(new_test))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","8331cdba":"print(score_list)\nprint(np.mean(score_list))","e0332b66":"lgbm = model.predict(new_test)","49c871c0":"first_sub = pd.DataFrame({\"id\":id_col,\n                          \"target\": lgbm})\n\nfirst_sub.to_csv(\"sub2_tbs_feb-2021.csv\", index=False)","3fe61bdf":"import optuna","49cfc7a3":"def objective(trial):\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=45)\n    dtrain = lgbm.Dataset(X_train, label=y_train)\n    \n    param = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'dart',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int(\"max_depth\", 1, 100),\n        'max_bin': trial.suggest_int('max_bin', 1, 255)\n    }\n    \n    gbm = lgbm.train(param, dtrain)\n    preds = gbm.predict(X_val)\n    pred_labels = np.rint(preds)\n    rmse = np.sqrt(mean_squared_error(y_val, pred_labels))\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial: \", study.best_trial.params)","2afde6eb":"# id_col = new_test['id']","4af05d0e":"X = new_train.drop([\"id\", \"target\"], axis=1)\ny = new_train['target']\n\nkf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n\n\n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"dart\", # better performance on 'gbdt'\n                  \"feature_fraction\": 0.5577195606178571,\n                  \"max_depth\": 27,\n                  \"num_leaves\": 130,\n                  \"lambda_l1\": 7.613380888310388,\n                  \"lambda_l2\": 4.877151813351834e-06,\n                  \"learning_rate\": 0.01,\n                  \"min_child_samples\":45,\n                  \"bagging_fraction\": 0.4018534965764494,\n                  \"bagging_freq\": 5,\n                  \"max_bin\": 149,}\n                  #\"is_unbalance\":True,\n                  #\"subsample\":0.3}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=2000,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"dart\",\n                  \"feature_fraction\": 0.5065829762803686,\n                  \"max_depth\": 27,\n                  \"num_leaves\": 130,\n                  \"lambda_l1\": 5.20581102862389e-05,\n                  \"lambda_l2\": 0.003260006773537875,\n                  \"learning_rate\": 0.003,\n                  \"min_child_samples\":45,\n                  \"bagging_fraction\": 0.4018534965764494,\n                  \"bagging_freq\": 5,\n                  \"max_bin\": 149,}\n                  #\"is_unbalance\":True,\n                 # \"subsample\":0.3}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1900,\n                            early_stopping_rounds=100,\n                            init_model = model\n                            )\n\n    \n    \n        y_pred_list.append(model.predict(X_val))\n        #test_preds.append(model.predict(new_test))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","6daca003":"print(score_list)\nprint(np.mean(score_list))","8cb74ee3":"preds = model.predict(new_test)\n\nsub = pd.DataFrame({\"id\":id_col,\n                    \"target\": preds})\n\nsub.to_csv(\"sub2_with_optuna.csv\", index=False)","57973a8b":"\u201cAt the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\u201d\n\u2014 Prof. Pedro Domingos","b4325ddc":"## A new approach. to be continued...","910a1b10":"### RandomizedSearchCV for XGBoost","8a2a4fb6":"### GridSearchCV and KFolds for lightgbm","e8b6ee27":"#Submmit to Kaggle","66d27951":"So after removing unbalanced features our model performed worse, so that didn't help.","de867322":"Because of big O which is in this case (n_features * n_observations**2) for SVR algorithm I need to use LinearSVR as it is much faster for a big datasets.","1baee170":"After removing outliers from numerical features we also removed some outliers from categorical features, looks better now. However there is not to much variance between categorical features.","45f19d62":"### GridSearchCV and KFolds for LinearSVR","ebdb6692":"I tried to GridSearchCV for only these very limited hyperparameters and I couldn't complete search.\n\nparam_grid = {\"n_estimators\":[50,100,500],\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n              \"bootstrap\": [True, False]}\n","78d661c9":"My original strategy was to GridSearchCV best hyperparameters for chosen algorithms and than use stacking algorithm to lower my rmse. It is a good technique for a small datasets but with a dataset like this (300000,26) some algorithms e.g SVR or RandomForestRegressor takes forever and then on some point we running out of memory. So I think that we have 2 options. First, we could take a sample and than we could gridsearchCV. But here question arise if the sample we take is representative for a hole dataset. Second, we could find the right reguralization parameter for a given algorithm, that would reduce our computation time. Last, we could change the aproach completely, leave for now parameter tuning and start from the begining which is feature engineering. I just come across an article about discretization of continues features and I am dying to try it out and what effect that could be on performance as well as on computation time. ","6cdd81bb":"Note from this plot that most of the categorical features are unbalanced except feature cat1.","3af6d44b":"## Feature engineering","64b1f524":"**LinearSVR**","ed07e015":"In categorical features we have that troublesome columns \"cat6\", I will drop it from both datasets.","15c7b72d":"These hyperparameters has been found by gridsearchCV:\n\n{'max_bin': 120, 'max_depth': 40, 'n_estimators': 1500, 'num_leaves': 120}","14960904":"### Adaptive bining\n\n1. Quantile based bining strategy - Quantiles are a specific values or cut-points which help in partitioning continues distribution into descrete contiguous bins or intervals. q-Quantiles help us partitioning continues feature into q equal partitions. Popular examples of quantiles:\n\n    * 2-Quantile known as median which divides the data distribution into 2 equal bins,\n    * 4-Quantiles know as quartiles which divides the data into 4 equal bins,\n    * 10-Quantiles asno known as deciles,","ea5c6f0f":"Sklearn provides us with a method which help us categorize numerical features and also encode it as ordinal.","0d4206f6":"## Stacking Regressor","5e529120":"Acceptable value of skewness is between -0.5 < x < 0.5. One of the method to remove outliers is by calculating their upper and lower limits using quantiles.","c4b8ab78":"An important point to remember here is that the resultant outcome of binning leads to discrete valued categorical features and you might need an additional step of feature engineering on the categorical data before using it in any model","1984856e":"So, now I have got categorical features which need to be onehotencoded and new dataframe with numerical transformed into form of categorical. Now, question is how I am suppose to treat them, as categorical nominal or categorical ordinal? I think I should transform them using methods for ordinal features.","b72d93ec":"As we can see, this time we get a dataset with categorical and numerical type, therefore we have to deal with them separately.","dd9807f8":"### Optuna to the rescue.","7ead6e87":"### RandomForestRegressor with default parameters","efe343a7":"### Stacking model","1ddac9e5":"### ElastiNetCV","e2c55627":"The distribution of our numeric features including target column is in most cases multimodial or bimodial. Now the question is what I should do about it? **need more reading**","7ba2d49e":"### Categorical features","040af4a2":"{'C': 1, 'loss': 'squared_epsilon_insensitive'}","123dce67":"The distribution of continues features look-a-like bimodial or multimodial.","003a6ef4":"### Numerical features","1f1a6dcf":"As we can see there is a unique values difference between Train and Test dataset in column \"cat6\". We need to udress it before we onehotencode them. ","10976490":"Introduction.\n\nHey Kaggles! \nFirst competiton was a great fun and also a litte bit frustrating. I tried many techniques to prepare the dataset and I tried many algorithms to find the one. I also gridsearchCV for the best hyperparameters. Finally I used ensemble model to get even better results, but I end up only on top_55% in kaggle scoreboard. The top places in the first competition were decided by 4th and 5th place in decimal point. So I was wondering what else can be done to improve the final score. In this notebook I will try to learn and use one of the win-win competition algorithm Lightgbm according to  and see what happen.","3cccb055":"I didn't scale numeric data as they seems to be in the same range and from the previous competition there was no model improvement. So for now I won't do it but I might to come back and change that here. ","8b03f561":"## From here we can jump to new approach section or continue with the first one","1f443c4e":"There is some correlation between features but non of them have correletion with the target and to be frank in some cases the corelation is very weak. I am not sure if that is important factor which need to be somehow adress."}}