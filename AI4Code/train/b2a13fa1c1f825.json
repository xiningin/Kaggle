{"cell_type":{"ec1ef765":"code","a87aa74c":"code","41f0fdf5":"code","f7bd2c2d":"code","002b9c72":"code","95aa9fcc":"code","ddd7eeac":"code","77c573c0":"code","30e7c4ee":"code","a4a57819":"code","96d5ad08":"code","0d6d976d":"code","ed673438":"code","595e044e":"code","a1e27a51":"code","586354d3":"code","acc91e42":"code","79fe801c":"code","2ac1f621":"code","82306eee":"code","4e3c4e92":"code","e1377eb6":"code","e924ae15":"code","b63ce0d3":"code","5d45829f":"code","1eedb6e7":"code","3fe18584":"code","ae867c6c":"code","c3eaa72c":"code","feae4928":"code","230ae6bf":"code","3d00db65":"code","7b3db1a1":"code","f13844ea":"code","633877da":"code","39af6b34":"code","1703cb32":"code","d43a5c2b":"code","bd8e0235":"code","7a72fd02":"code","316ebbc5":"code","b10bf0cd":"code","37ced4b7":"code","a942b0c5":"code","3092f643":"code","aacfd57d":"code","17b0a92b":"code","44e87e03":"code","e0c973ee":"code","b94b360a":"code","e38dae94":"code","08e28202":"code","2beedf59":"code","cf750aff":"code","6cbd4a36":"code","e6177e4e":"code","abc69ee8":"code","95a8da1f":"code","ee454550":"code","9469a9f4":"code","f094d737":"code","8d386f19":"code","74fce731":"code","7c3946c2":"code","6407eb72":"code","09322cab":"code","7929f67f":"code","5623b537":"code","f48d5a67":"code","676f7059":"code","eb26971d":"code","f1129ae8":"code","3d4da65d":"code","d6ce40e1":"code","eb510c1b":"code","0afd8835":"code","9456d07b":"code","eaa1a45a":"code","53e5c6cd":"code","a7ec98a1":"code","cc4284f3":"code","bba4b756":"code","6f90fdc1":"code","83cc84f4":"code","0ec785ff":"code","c100871e":"code","a21a5c7a":"code","7a4a006e":"code","82584ffc":"code","9deed5df":"code","0775533d":"code","13063a3e":"code","b1e3cc91":"code","47457369":"code","794e730f":"code","db20c4ac":"code","1be0f3de":"code","1a753e7b":"code","eda2bfd7":"code","81b2151f":"markdown","6d26a4db":"markdown","bc221ade":"markdown","8b209fe2":"markdown","eaf8e069":"markdown","ffb51a88":"markdown","03da2554":"markdown","eb309123":"markdown","deb5d366":"markdown","2254122d":"markdown","49f14bfb":"markdown","3786c760":"markdown","03a75bf8":"markdown","0cbadd8e":"markdown","e1a12581":"markdown","d53b4477":"markdown","12e3e53e":"markdown","476bdc74":"markdown","d82cf8a8":"markdown","40dc5a4a":"markdown","d9d7d823":"markdown","49330537":"markdown","af4a5929":"markdown","34107073":"markdown","43b673d5":"markdown","a6ff2893":"markdown","d7a54cc2":"markdown","11b9a00d":"markdown","d5491a91":"markdown","248d2f86":"markdown","0af3d28e":"markdown","9b5b24ae":"markdown","278013df":"markdown","c30bdc99":"markdown","178ae367":"markdown","aea07e3a":"markdown","81b8fdfe":"markdown","314cf4f5":"markdown","dbc0b6ef":"markdown","dd25c8fd":"markdown","e1dcea8d":"markdown","15415b65":"markdown","44889a07":"markdown","28b6d2f2":"markdown","182468e7":"markdown","cacd8021":"markdown","0b0c62d5":"markdown","7bc57453":"markdown","6942641f":"markdown","77657574":"markdown","90f2a694":"markdown","3649aaad":"markdown","b49fe031":"markdown","36a6df49":"markdown","5bf40011":"markdown","cb5883b5":"markdown","6cd02905":"markdown","8538951f":"markdown","6351ac08":"markdown","effde487":"markdown","c5c9d4b1":"markdown","6dacfb1a":"markdown","7e501eb3":"markdown","2dc8b158":"markdown","95223bab":"markdown","b336db1f":"markdown","eed47e96":"markdown","0f345624":"markdown","e2b85e38":"markdown","cfe8df89":"markdown","5cd349ea":"markdown","dbac2e4d":"markdown","5175cca2":"markdown","5901a1ab":"markdown","78d18157":"markdown","fb43dd9f":"markdown","76e89101":"markdown","0c0a04a1":"markdown","707549a0":"markdown","d8813370":"markdown","4afbab64":"markdown","f228b67c":"markdown","eecafaa9":"markdown","b7a0ffbf":"markdown","8a6a682c":"markdown","70db76e1":"markdown","c9b76db3":"markdown","6717d88a":"markdown","fd112675":"markdown","045169fc":"markdown","b3bada6d":"markdown","8f75d04a":"markdown","7b05074c":"markdown","acbe473b":"markdown","b0d459a2":"markdown","d8ef08b2":"markdown","d925366c":"markdown","4d76d711":"markdown","829d3acc":"markdown","bcee0934":"markdown","9bfb8aba":"markdown","af3df318":"markdown","c540ebf1":"markdown","7925717e":"markdown","4adc85a3":"markdown","de366216":"markdown","61293dd5":"markdown","c39dc2dc":"markdown","2a9fb3e8":"markdown"},"source":{"ec1ef765":"# Python libraries\n# Classic,data manipulation and linear algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport squarify\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\n\n\n# Stats\nimport scipy.stats as ss\nfrom scipy import interp\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n# Time\nfrom contextlib import contextmanager\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') ","a87aa74c":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","41f0fdf5":"display(data.info(),data.head())","f7bd2c2d":"# 2 datasets\nD = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]\n\n#------------COUNT-----------------------\ndef target_count():\n    trace = go.Bar( x = data['Outcome'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=data['Outcome'].value_counts().values.tolist(), \n                    textfont=dict(size=15),\n                    textposition = 'auto',\n                    opacity = 0.8,marker=dict(\n                    color=['lightskyblue', 'gold'],\n                    line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  'Count of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\n#------------PERCENTAGE-------------------\ndef target_percent():\n    trace = go.Pie(labels = ['healthy','diabetic'], values = data['Outcome'].value_counts(), \n                   textfont=dict(size=15), opacity = 0.8,\n                   marker=dict(colors=['lightskyblue', 'gold'], \n                               line=dict(color='#000000', width=1.5)))\n\n\n    layout = dict(title =  'Distribution of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","002b9c72":"target_count()\ntarget_percent()","95aa9fcc":"data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","ddd7eeac":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((len(dataset[key]) - (len(dataset[key]) - dataset.isnull().sum()))\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n    ","77c573c0":"# Plotting \nmissing_plot(data, 'Outcome')","30e7c4ee":"plt.style.use('ggplot') # Using ggplot2 style visuals \n\nf, ax = plt.subplots(figsize=(11, 15))\n\nax.set_facecolor('#fafafa')\nax.set(xlim=(-.05, 200))\nplt.ylabel('Variables')\nplt.title(\"Overview Data Set\")\nax = sns.boxplot(data = data, \n  orient = 'h', \n  palette = 'Set2')","a4a57819":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)","96d5ad08":"correlation_plot()","0d6d976d":"def median_target(var):   \n    temp = data[data[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","ed673438":"def plot_distribution(data_select, size_bin) :  \n    # 2 datasets\n    tmp1 = D[data_select]\n    tmp2 = H[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['diabetic', 'healthy']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig, filename = 'Density plot')","595e044e":"plot_distribution('Insulin', 0)","a1e27a51":"median_target('Insulin')","586354d3":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5","acc91e42":"plot_distribution('Glucose', 0)","79fe801c":"median_target('Glucose')","2ac1f621":"data.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140","82306eee":"plot_distribution('SkinThickness', 10)","4e3c4e92":"median_target('SkinThickness')","e1377eb6":"data.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32","e924ae15":"plot_distribution('BloodPressure', 5)","b63ce0d3":"median_target('BloodPressure')","5d45829f":"data.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","1eedb6e7":"plot_distribution('BMI', 0)","3fe18584":"median_target('BMI')","ae867c6c":"data.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3","c3eaa72c":"#plot distribution \nplot_distribution('Age', 0)\nplot_distribution('Pregnancies', 0)\nplot_distribution('DiabetesPedigreeFunction', 0)","feae4928":"missing_plot(data, 'Outcome')","230ae6bf":"def plot_feat1_feat2(feat1, feat2) :  \n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    trace0 = go.Scatter(\n        x = D[feat1],\n        y = D[feat2],\n        name = 'diabetic',\n        mode = 'markers', \n        marker = dict(color = '#FFD700',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = H[feat1],\n        y = H[feat2],\n        name = 'healthy',\n        mode = 'markers',\n        marker = dict(color = '#7EC0EE',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = feat1 +\" \"+\"vs\"+\" \"+ feat2,\n                  yaxis = dict(title = feat2,zeroline = False),\n                  xaxis = dict(title = feat1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)","3d00db65":"def barplot(var_select, sub) :\n    tmp1 = data[(data['Outcome'] != 0)]\n    tmp2 = data[(data['Outcome'] == 0)]\n    tmp3 = pd.DataFrame(pd.crosstab(data[var_select],data['Outcome']), )\n    tmp3['% diabetic'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n\n    color=['lightskyblue','gold' ]\n    trace1 = go.Bar(\n        x=tmp1[var_select].value_counts().keys().tolist(),\n        y=tmp1[var_select].value_counts().values.tolist(),\n        text=tmp1[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='diabetic',opacity = 0.8, marker=dict(\n        color='gold',\n        line=dict(color='#000000',width=1)))\n\n    \n    trace2 = go.Bar(\n        x=tmp2[var_select].value_counts().keys().tolist(),\n        y=tmp2[var_select].value_counts().values.tolist(),\n        text=tmp2[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='healthy', opacity = 0.8, marker=dict(\n        color='lightskyblue',\n        line=dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x=tmp3.index,\n        y=tmp3['% diabetic'],\n        yaxis = 'y2',\n        name='% diabetic', opacity = 0.6, marker=dict(\n        color='black',\n        line=dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select)+' '+(sub),\n              xaxis=dict(), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [-0, 75], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= '% diabetic'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)","7b3db1a1":"# Define pie plot to visualize each variable repartition vs target modalities : Survived or Died (train)\ndef plot_pie(var_select, sub) :\n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    \n    col =['Silver', 'mediumturquoise','#CF5C36','lightblue','magenta', '#FF5D73','#F2D7EE','mediumturquoise']\n    \n    trace1 = go.Pie(values  = D[var_select].value_counts().values.tolist(),\n                    labels  = D[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hole = 0.5, \n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [.0,.48]),\n                    name    = \"Diabetic\",\n                    marker  = dict(colors = col, line = dict(width = 1.5)))\n    trace2 = go.Pie(values  = H[var_select].value_counts().values.tolist(),\n                    labels  = H[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hole = 0.5,\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 1.5)),\n                    domain  = dict(x = [.52,1]),\n                    name    = \"Healthy\" )\n\n    layout = go.Layout(dict(title = var_select + \" distribution by target <br>\"+(sub),\n                            annotations = [ dict(text = \"Diabetic\"+\" : \"+\"268\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .22, y = -0.1),\n                                            dict(text = \"Healthy\"+\" : \"+\"500\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .8,y = -.1)]))\n                                          \n\n    fig  = go.Figure(data = [trace1,trace2],layout = layout)\n    py.iplot(fig)","f13844ea":"plot_feat1_feat2('Glucose','Age')","633877da":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['Age'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N1', size=25, color='black', xy=(80, 30), xytext=(60, 35),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([50, 120], [30, 30], linewidth=2, color = 'red')\nplt.plot([120, 120], [20, 30], linewidth=2, color = 'red')\nplt.plot([50, 120], [20, 20], linewidth=2, color = 'red')\nplt.plot([50, 50], [20, 30], linewidth=2, color = 'red')\nplt.title('Glucose vs Age')\nplt.show()","39af6b34":"data.loc[:,'N1']=0\ndata.loc[(data['Age']<=30) & (data['Glucose']<=120),'N1']=1","1703cb32":"barplot('N1', ':Glucose <= 120 and Age <= 30')","d43a5c2b":"plot_pie('N1', '(Glucose <= 120 and Age <= 30)')","bd8e0235":"data.loc[:,'N2']=0\ndata.loc[(data['BMI']<=30),'N2']=1","7a72fd02":"barplot('N2', ': BMI <= 30')","316ebbc5":"plot_pie('N2', 'BMI <= 30')","b10bf0cd":"plot_feat1_feat2('Pregnancies','Age')","37ced4b7":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Pregnancies'], y = data['Age'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N3', size=25, color='black', xy=(6, 25), xytext=(10, 25),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([0, 6], [30, 30], linewidth=2, color = 'red')\nplt.plot([6, 6], [20, 30], linewidth=2, color = 'red')\nplt.plot([0, 6], [20, 20], linewidth=2, color = 'red')\nplt.plot([0, 0], [20, 30], linewidth=2, color = 'red')\nplt.title('Pregnancies vs Age')\nplt.show()","a942b0c5":"data.loc[:,'N3']=0\ndata.loc[(data['Age']<=30) & (data['Pregnancies']<=6),'N3']=1","3092f643":"barplot('N3', ': Age <= 30 and Pregnancies <= 6')","aacfd57d":"plot_pie('N3', 'Age <= 30 and Pregnancies <= 6')","17b0a92b":"plot_feat1_feat2('Glucose','BloodPressure')","44e87e03":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['BloodPressure'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N4', size=25, color='black', xy=(70, 80), xytext=(50, 110),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([40, 105], [80, 80], linewidth=2, color = 'red')\nplt.plot([40, 40], [20, 80], linewidth=2, color = 'red')\nplt.plot([40, 105], [20, 20], linewidth=2, color = 'red')\nplt.plot([105, 105], [20, 80], linewidth=2, color = 'red')\nplt.title('Glucose vs BloodPressure')\nplt.show()","e0c973ee":"data.loc[:,'N4']=0\ndata.loc[(data['Glucose']<=105) & (data['BloodPressure']<=80),'N4']=1","b94b360a":"barplot('N4', ': Glucose <= 105 and BloodPressure <= 80')","e38dae94":"plot_pie('N4', 'Glucose <= 105 and BloodPressure <= 80')","08e28202":"data.loc[:,'N5']=0\ndata.loc[(data['SkinThickness']<=20) ,'N5']=1","2beedf59":"barplot('N5', ':SkinThickness <= 20')","cf750aff":"plot_pie('N5', 'SkinThickness <= 20')","6cbd4a36":"plot_feat1_feat2('SkinThickness','BMI')","e6177e4e":"data.loc[:,'N6']=0\ndata.loc[(data['BMI']<30) & (data['SkinThickness']<=20),'N6']=1","abc69ee8":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['SkinThickness'], y = data['BMI'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N6', size=25, color='black', xy=(20, 20), xytext=(50, 25),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([0, 20], [30, 30], linewidth=2, color = 'red')\nplt.plot([0, 0], [16, 30], linewidth=2, color = 'red')\nplt.plot([0, 20], [16, 16], linewidth=2, color = 'red')\nplt.plot([20, 20], [16, 30], linewidth=2, color = 'red')\nplt.title('SkinThickness vs BMI')\nplt.show()","95a8da1f":"barplot('N6', ': BMI < 30 and SkinThickness <= 20')","ee454550":"plot_pie('N6', 'BMI < 30 and SkinThickness <= 20')","9469a9f4":"plot_feat1_feat2('Glucose','BMI')","f094d737":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['BMI'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N7', size=25, color='black', xy=(70, 35), xytext=(40, 60),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([105, 105], [16, 30], linewidth=2, color = 'red')\nplt.plot([40, 40], [16, 30], linewidth=2, color = 'red')\nplt.plot([40, 105], [16, 16], linewidth=2, color = 'red')\nplt.plot([40, 105], [30, 30], linewidth=2, color = 'red')\nplt.title('Glucose vs BMI')\nplt.show()","8d386f19":"data.loc[:,'N7']=0\ndata.loc[(data['Glucose']<=105) & (data['BMI']<=30),'N7']=1","74fce731":"barplot('N7', ': Glucose <= 105 and BMI <= 30')","7c3946c2":"plot_pie('N7', 'Glucose <= 105 and BMI <= 30')","6407eb72":"plot_distribution('Insulin', 0)","09322cab":"data.loc[:,'N9']=0\ndata.loc[(data['Insulin']<200),'N9']=1","7929f67f":"barplot('N9', ': Insulin < 200')","5623b537":"plot_pie('N9', 'Insulin < 200')","f48d5a67":"data.loc[:,'N10']=0\ndata.loc[(data['BloodPressure']<80),'N10']=1","676f7059":"barplot('N10', ': BloodPressure < 80')","eb26971d":"plot_pie('N10', 'BloodPressure < 80')","f1129ae8":"plot_distribution('Pregnancies', 0)","3d4da65d":"data.loc[:,'N11']=0\ndata.loc[(data['Pregnancies']<4) & (data['Pregnancies']!=0) ,'N11']=1","d6ce40e1":"barplot('N11', ': Pregnancies > 0 and < 4')","eb510c1b":"plot_pie('N11', 'Pregnancies > 0 and < 4')","0afd8835":"data['N0'] = data['BMI'] * data['SkinThickness']\n\ndata['N8'] =  data['Pregnancies'] \/ data['Age']\n\ndata['N13'] = data['Glucose'] \/ data['DiabetesPedigreeFunction']\n\ndata['N12'] = data['Age'] * data['DiabetesPedigreeFunction']\n\ndata['N14'] = data['Age'] \/ data['Insulin']\n","9456d07b":"D = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]","eaa1a45a":"plot_distribution('N0', 0)","53e5c6cd":"data.loc[:,'N15']=0\ndata.loc[(data['N0']<1034) ,'N15']=1","a7ec98a1":"barplot('N15', ': N0 < 1034')","cc4284f3":"plot_pie('N15', 'N0 < 1034')","bba4b756":"target_col = [\"Outcome\"]\ncat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\ncat_cols   = [x for x in cat_cols ]\n#numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col]\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")","6f90fdc1":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)\n","83cc84f4":"correlation_plot()","0ec785ff":"#!pip install --quiet -r requirements.txt; ","c100871e":"# Load libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","a21a5c7a":"col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n# load dataset\npima = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","7a4a006e":"pima.head()","82584ffc":"#split dataset in features and target variable\nfeature_cols = ['Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\nX = pima[feature_cols] # Features \ny = pima.Outcome # Target variable","9deed5df":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test|","0775533d":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","13063a3e":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","b1e3cc91":"from sklearn import tree\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 5), dpi=500)\ntree.plot_tree(clf,\n               feature_names=feature_cols,\n               class_names=[\"0\", \"1\"],\n               filled=True,\n               rounded=True);\nplt.savefig('dtree.png')","47457369":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","794e730f":"plt.figure(figsize=(8, 3), dpi=200)\ntree.plot_tree(clf,\n               feature_names=feature_cols,\n               class_names=[\"0\", \"1\"],\n               filled=True,\n               rounded=True);\nplt.savefig('diabetes.png')","db20c4ac":"from sklearn.metrics import confusion_matrix\nmatrix= confusion_matrix(y_test, y_pred)\nsns.heatmap(matrix,annot = True, fmt = \"d\")","1be0f3de":"from sklearn.metrics import precision_score\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision: \",precision)","1a753e7b":"from sklearn.metrics import recall_score\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall is: \",recall)","eda2bfd7":"print((2*precision*recall)\/(precision+recall))","81b2151f":"Loading the libraries","6d26a4db":"* **Pregnancies**","bc221ade":"## <a id='6.3'>6.3. Decision Tree Classifier Building in Scikit-learn<\/a> \n### Importing Required Libraries\n\nLet's first install the required packages and load the required libraries.","8b209fe2":"The basic idea behind any decision tree algorithm is as follows:\n\n1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n3. Start tree building by repeating this process recursively for each child until one of the condition will match:\n    - All the tuples belong to the same attribute value.\n    - There are no more remaining attributes.\n    - There are no more instances.\n    \n![](graphics\/figure2.jpeg)","eaf8e069":"### Visualizing Decision Trees\n\nYou can use Scikit-learn's `plot_tree` function from the tree module to display the tree within a Jupyter notebook. ","ffb51a88":"Healthy persons are concentrate with an age <= 30 and glucose <= 120","03da2554":"107 for a healthy person and 140 for a diabetic person","eb309123":"* ** Did you watch Inception ? ** Here is the same! It's not a dream in a dream but a new feature extract from a new feature","deb5d366":"## Attribute Selection Measures\n\nAttribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute ([Source](http:\/\/www.ijoart.org\/docs\/Construction-of-Decision-Tree--Attribute-Selection-Measures.pdf)). In the case of a continuous-valued attribute, split points for branches also need to define.  Most popular selection measures are Information Gain, Gain Ratio, and Gini Index.","2254122d":"27 for a healthy person and 32 for a diabetic person","49f14bfb":"### Gini index\n\nAnother decision tree algorithm CART (Classification and Regression Tree) uses the Gini method to create split points.\n\n$$Gini(D) = 1 - \\sum_{i = 1}^{m} P_i^2 $$","3786c760":"----------\n**Pima Indians Diabetes - EDA & Prediction (0.776%)**\n=====================================\n\n***LINKEDIN : https:\/\/www.linkedin.com\/in\/qasim-hassan\/ *** <br>\n***GITHUB : https:\/\/github.com\/qasim1020*** <br>\n***Official Facebook Page: https:\/\/www.facebook.com\/aiwithqasim\/***<br>\n***Complete Source with report: https:\/\/github.com\/qasim1020\/Diabetes-EDA-and-Prediction-***\n\n\n*July 2021*\n\n----------","03a75bf8":"Healthy persons are concentrate with a BMI < 30 and skin thickness <= 20","0cbadd8e":"The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","e1a12581":"* ** BloodPressure** : Diastolic blood pressure (mm Hg)","d53b4477":"A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.","12e3e53e":"* **Pregnancies and Age**","476bdc74":"## Thank you all :)","d82cf8a8":"Insulin's medians by the target are really different ! 102.5 for a healthy person and 169.5 for a diabetic person","40dc5a4a":"**To fill these Nan values the data distribution needs to be understood against the target**. ","d9d7d823":"## <a id='2.2'>2.2 Target<\/a> ","49330537":"## <a id='3.4'>3.4. BloodPressure<\/a> ","af4a5929":"* ** SkinThickness** : Triceps skin fold thickness (mm)","34107073":"# <a id='1'>1. Load libraries and read the data<\/a> ","43b673d5":"* **Insulin** : 2-Hour serum insulin (mu U\/ml)","a6ff2893":"Now, we can look at where are missing values : ","d7a54cc2":"## <a id='6.2'>6.2. How does the Decision Tree algorithm work?<\/a> ","11b9a00d":"# <a id='2'>2. Overview<\/a> ","d5491a91":"The attribute with the highest gain ratio is chosen as the splitting attribute ([Source](http:\/\/www.enggjournals.com\/ijcse\/doc\/IJCSE10-02-09-092.pdf)).","248d2f86":"## <a id='3.5'>3.5. BMI<\/a> ","0af3d28e":"## <a id='3.3'>3.3. SkinThickness<\/a> ","9b5b24ae":"#### Precision","278013df":"## <a id='1.1'>1.1. Load libraries<\/a> ","c30bdc99":"# Who is Pima Indians ?\n\n\"The Pima (or Akimel O'odham, also spelled Akimel O'otham, \"River People\", formerly known as Pima) are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The majority population of the surviving two bands of the Akimel O'odham are based in two reservations: the Keli Akimel O'otham on the Gila River Indian Community (GRIC) and the On'k Akimel O'odham on the Salt River Pima-Maricopa Indian Community (SRPMIC).\" Wikipedia","178ae367":"# <a id='7'>7. Credits<\/a> ","aea07e3a":"OK, all missing values are encoded with NaN value","81b8fdfe":"### Building Decision Tree Model\n\nLet's create a Decision Tree Model using Scikit-learn.","314cf4f5":"* **Insulin**","dbc0b6ef":"### Loading Data\n\nLet's first load the required Pima Indian Diabetes dataset using pandas' read CSV function. You can download the data [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n","dd25c8fd":"Now, we can compute correlation matrix ","e1dcea8d":"* **Glucose** : Plasma glucose concentration a 2 hours in an oral glucose tolerance test","15415b65":"The gain ratio can be defined as:\n\n$$ \\textrm{GainRatio}(A) = \\dfrac{\\textrm{Gain(A)}}{\\textrm{SplitInfo}_A(D)} $$","44889a07":"Loading dataset with pandas (pd)","28b6d2f2":"Compplete code is avaliable at : https:\/\/github.com\/qasim1020\/Diabetes-EDA-and-Prediction-","182468e7":"In case of a discrete-valued attribute, the subset that gives the minimum gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with smaller gini index chosen as the splitting point.\n\n$$ \\Delta Gini(A) = Gini(D) - Gini_A(D)$$\n","cacd8021":"## <a id='1.2'>1.2. Read data<\/a> ","0b0c62d5":"### Feature Selection\n\nHere, you need to divide given columns into two types of variables dependent(or target variable) and independent variable(or feature variables).","7bc57453":"This pruned model is less complex, explainable, and easy to understand than the previous decision tree model plot.\n\n## Pros\n\n- Decision trees are easy to interpret and visualize.\n- It can easily capture Non-linear patterns.\n- It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n- It can be used for feature engineering such as predicting missing values, suitable for variable selection.\n- The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm.\n([Source](https:\/\/scikit-learn.org\/stable\/modules\/tree.html))\n\n## Cons\n\n- Sensitive to noisy data. It can overfit noisy data.\n- The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.\n- Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.\n\n## Conclusion\n\nCongratulations, you have made it to the end of this tutorial!\n\nIn this Notebook, I had covered a lot of details about how can do a better EDA and then using a Decision Tree; It's working, attribute selection measures such as Information Gain, Gain Ratio, and Gini Index, decision tree model building, visualization and evaluation on diabetes dataset using Python Scikit-learn package. Also, discussed its pros, cons, and optimizing Decision Tree performance using parameter tuning.\n\nHopefully, you can now utilize the Decision tree algorithm to analyze your own datasets.","6942641f":"Well, the classification rate increased to 77.05%, which is better accuracy than the previous model.","77657574":"# What is diabetes ? \nAcccording to NIH, \"**Diabetes** is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nSometimes people call diabetes \u201ca touch of sugar\u201d or \u201cborderline diabetes.\u201d These terms suggest that someone doesn\u2019t really have diabetes or has a less serious case, but every case of diabetes is serious.\n\n**What are the different types of diabetes?**\nThe most common types of diabetes are type 1, type 2, and gestational diabetes.\n\n**Type 1 diabetes**\nIf you have type 1 diabetes, your body does not make insulin. Your immune system attacks and destroys the cells in your pancreas that make insulin. Type 1 diabetes is usually diagnosed in children and young adults, although it can appear at any age. People with type 1 diabetes need to take insulin every day to stay alive.\n\n**Type 2 diabetes**\nIf you have type 2 diabetes, your body does not make or use insulin well. You can develop type 2 diabetes at any age, even during childhood. However, this type of diabetes occurs most often in middle-aged and older people. Type 2 is the most common type of diabetes.\n\n**Gestational diabetes**\nGestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you\u2019ve had gestational diabetes, you have a greater chance of developing type 2 diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes.\n\n**Other types of diabetes**\nLess common types include monogenic diabetes, which is an inherited form of diabetes, and cystic fibrosis-related diabetes .\"","90f2a694":"## <a id='5.2'>5.2. Correlation Matrix<\/a> ","3649aaad":"## <a id='2.3'>2.3. Missing values<\/a> ","b49fe031":"#### recall","36a6df49":"* **SkinThickness and BMI**","5bf40011":"## <a id='6.4'>6.4. Model Performance<\/a> ","cb5883b5":"\nBelow, you can see the accuracy of LGBM with replacement of the NaN values by the variable's mean (same results with the median)","6cd02905":"* **BloodPressure**","8538951f":"### Splitting Data\n\nTo understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n\nLet's split the dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size.\n","6351ac08":"* **Glucose and Age**","effde487":"A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.","c5c9d4b1":"What's target's distribution ? ","6dacfb1a":"Decision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and number of attributes in the given data. The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy.  ","7e501eb3":"## <a id='3.1'>3.1. Insulin<\/a> ","2dc8b158":"Healthy persons are concentrate with an blood pressure <= 80 and glucose <= 105","95223bab":"To measure the performance of a model, we need several elements :\n\nThis part is essential\n\n* **Confusion matrix** : also known as the error matrix, allows visualization of the performance of an algorithm :\n\n    * true positive (TP) : Diabetic correctly identified as diabetic\n    * true negative (TN) : Healthy correctly identified as healthy\n    * false positive (FP) : Healthy incorrectly identified as diabetic\n    * false negative (FN) : Diabetic incorrectly identified as healthy\n\n* **Metrics ** :\n\n    * Accuracy : (TP +TN) \/ (TP + TN + FP +FN)\n    * Precision : TP \/ (TP + FP)\n    * Recall : TP \/ (TP + FN)\n    * F1 score : 2 x ((Precision x Recall) \/ (Precision + Recall))\n\n* **Roc Curve** : The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n* **Precision Recall Curve** :  shows the tradeoff between precision and recall for different threshold","b336db1f":"#### fscore","eed47e96":"The above graph shows that the data is unbalanced. The number of non-diabetic is 268 the number of diabetic patients is 500","0f345624":"Where:\n\n- **Info(D)** is the average amount of information needed to identify the class label of a tuple in D.\n- **|Dj|\/|D|** acts as the weight of the jth partition.\n- **InfoA(D)** is the expected information required to classify a tuple from D based on the partitioning by A.\n\nThe attribute A with the highest information gain, **Gain(A)**, is chosen as the splitting attribute at node N().","e2b85e38":"The attribute with minimum Gini index is chosen as the splitting attribute.","cfe8df89":"### Confusion Matrix","5cd349ea":"### TABLE OF CONTENT","dbac2e4d":"Missing values : \n* Insulin = 48.7% - 374\n* SkinThickness = 29.56% - 227\n* BloodPressure = 4.56% - 35\n* BMI = 1.43% - 11\n* Glucose = 0.65% - 5","5175cca2":"### Evaluating Model\n\nLet's estimate, how accurately the classifier or model can predict the type of cultivars.\n\nAccuracy can be computed by comparing actual test set values and predicted values.","5901a1ab":"* **Glucose and BloodPressure**","78d18157":"Credits  : \n* https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n* https:\/\/en.wikipedia.org\/wiki\/Body_mass_index\n* http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/\n* https:\/\/www.news-medical.net\/health\/What-is-Diabetes.aspx\n* https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\n* http:\/\/ogrisel.github.io\/scikit-learn.org\/sklearn-tutorial\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n* https:\/\/www.scikit-yb.org\/en\/latest\/api\/classifier\/threshold.html\n* https:\/\/www.analyticsindiamag.com\/why-is-random-search-better-than-grid-search-for-machine-learning\/\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n* https:\/\/scikit-learn.org\/stable\/modules\/preprocessing_targets.html#preprocessing-targets\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n* https:\/\/twitter.com\/bearda24\n* https:\/\/www.slideshare.net\/DhianaDevaRocha\/qcon-rio-machine-learning-for-everyone\n* https:\/\/medium.com\/@sebastiannorena\/some-model-tuning-methods-bfef3e6544f0\n* https:\/\/www.niddk.nih.gov\/health-information\/diabetes\/overview\/what-is-diabetes\n* https:\/\/en.wikipedia.org\/wiki\/Pima_people","fb43dd9f":"# <a id='5'>5. Prepare dataset<\/a> ","76e89101":"According to wikipedia \"The body mass index (BMI) or Quetelet index is a value derived from the mass (weight) and height of an individual. The BMI is defined as the body mass divided by the square of the body height, and is universally expressed in units of kg\/m2, resulting from mass in kilograms and height in metres.\"\n\n30 kg\/m\u00b2 is the limit to obesity","0c0a04a1":"Where, Pi is the probability that an arbitrary tuple in D belongs to class Ci.\n\n$$\\textrm{Info}_A(D) = \\sum_{j = 1}^{V} \\dfrac{|D_j|}{|D|} \\textrm{ x }\\textrm{Info}(D_j)$$\n\n$$\\textrm{Gain}(A) = \\textrm{Info}(D) - \\textrm{Info}_A(D)$$\n\n\n","707549a0":"## Optimizing Decision Tree Performance\n\n- **criterion :  optional (default=\u201dgini\u201d) or Choose attribute selection measure**: This parameter allows us to use the different-different attribute selection measure. Supported criteria are \u201cgini\u201d for the Gini index and \u201centropy\u201d for the information gain.\n\n- **splitter : string, optional (default=\u201dbest\u201d) or Split Strategy**: This parameter allows us to choose the split strategy. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n\n- **max_depth : int or None, optional (default=None) or Maximum Depth of a Tree**: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting ([Source](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)).\n\nIn Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning. In the following the example, you can plot a decision tree on the same data with max_depth=3.  Other than pre-pruning parameters, You can also try other attribute selection measure such as entropy.\n","d8813370":"To replace missing values, we'll use median by target (Outcome)","4afbab64":"Checking data head and info","f228b67c":"All features are complete ! \nNow, we can create new features","eecafaa9":"We saw on data.head() that some features contain 0, it doesn't make sense here and this indicates missing value\nBelow we replace 0 value by NaN :","b7a0ffbf":"* **Glucose and BMI**","8a6a682c":"- <a href='#1'>1. Load libraries and read the data<\/a>  \n\n    - <a href='#1.1'>1.1. Load libraries<\/a> \n    - <a href='#1.2'>1.2. Read the data<\/a> \n    \n- <a href='#2'>2. Overview<\/a> \n\n    - <a href='#2.1'>2.1. Head<\/a> \n    - <a href='#2.2'>2.2. Target<\/a> \n    - <a href='#2.3'>2.3. Missing values<\/a> \n    \n- <a href='#3'>3. Replace missing values and EDA<\/a>\n\n    - <a href='#3.1'>3.1. Insulin<\/a> \n    - <a href='#3.2'>3.2. Glucose<\/a> \n    - <a href='#3.3'>3.3. SkinThickness<\/a>\n    - <a href='#3.4'>3.4. BloodPressure<\/a>\n    - <a href='#3.5'>3.5. BMI<\/a>\n    \n- <a href='#4'>4. New features (16) and EDA<\/a>\n\n- <a href='#5'>5. Prepare dataset<\/a> \n    - <a href='#5.1'>5.1. StandardScaler and LabelEncoder<\/a> \n    - <a href='#5.2'>5.2. Correlation Matrix<\/a>\n    \n- <a href='#6'>6.Machine Learning<\/a> \n\n    - <a href='#6.1'>6.1. Decision Tree Algorithm<\/a> \n    - <a href='#6.2'>6.2. How does the Decision Tree algorithm work?<\/a>\n    - <a href='#6.3'>6.3. Decision Tree Classifier Building in Scikit-learn<\/a>\n    - <a href='#6.4'>6.4. Model Performance<\/a>\n    \n- <a href='#7'>7. Credits<\/a> ","70db76e1":"### Visualizing Decision Trees again","c9b76db3":"* **BMI** : Body mass index (weight in kg\/(height in m)^2)","6717d88a":"# <a id='3'>3. Replace missing values and EDA<\/a> ","fd112675":"Open up `dtree.png` that was generated inside the graphics folder and zoom in to inspect the decision tree chart. ","045169fc":"* **Age** : Age (years)\n* **DiabetesPedigreeFunction** : Diabetes pedigree function\n* **Pregnancies** : Number of times pregnant","b3bada6d":"Where, pi is the probability that a tuple in D belongs to class Ci.\n\nThe Gini Index considers a binary split for each attribute. You can compute a weighted sum of the impurity of each partition. If a binary split on attribute A partitions data D into D1 and D2, the Gini index of D is:\n\n$$Gini_A(D) = \\frac{|D_1|}{D} Gini(D_1) + \\frac{D_2}{D} Gini(D_2)$$","8f75d04a":"## <a id='3.2'>3.2. Glucose<\/a> ","7b05074c":"### Information Gain     \n\nShannon invented the concept of entropy, which measures the impurity of the input set. In physics and mathematics, entropy referred as the randomness or the impurity in the system. In information theory, it refers to the impurity in a group of examples. Information gain is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n\n$$\\textrm{Info}(D) = -\\sum_{i = 1}^{m} p_i\\log_2 p_i$$\n\n","acbe473b":"### Gain Ratio\n\nInformation gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values.  For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n\nC4.5, an improvement of ID3, uses an extension to information gain known as the gain ratio. Gain ratio handles the issue of bias by normalizing the information gain using Split Info.  Java implementation of the C4.5 algorithm is known as J48,  which is available in WEKA data mining tool.\n\n$$ \\textrm{SplitInfo}_A(D) = - \\sum_{j = 1}^{V} \\dfrac{|D_j|}{|D|} \\textrm{ x } log_2(\\frac{|D_j|}{|D|})$$","b0d459a2":"* **BMI **","d8ef08b2":"## <a id='6.1'>6.1. Decision Tree<\/a> \nA decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value.  It partitions the tree in a recursive manner called recursive partitioning. This flowchart-like structure helps you in decision making. It can be visualized like a flowchart diagram which easily mimics human level thinking. That is why decision trees are easy to understand and interpret.\n\n![](https:\/\/github.com\/qasim1020\/Diabetes-EDA-and-Prediction-\/blob\/main\/graphics\/figure1.jpeg)","d925366c":"Here, we define 3 plots functions","4d76d711":"* **Discrimination Threshold** :\nA visualization of precision, recall, f1 score, and queue rate with respect to the discrimination threshold of a binary classifier. The discrimination threshold is the probability or score at which the positive class is chosen over the negative class","829d3acc":"* **SkinThickness**","bcee0934":"## <a id='2.1'>2.1. Head<\/a> ","9bfb8aba":"* **Others**","af3df318":"## <a id='6'>6. Machine Learning<\/a> ","c540ebf1":"# <a id='4'>4. New features (16) and EDA<\/a> ","7925717e":"### Metrics","4adc85a3":"Where:\n- |Dj|\/|D| acts as the weight of the jth partition.\n- v is the number of discrete values in attribute A.","de366216":"\nIn the decision tree chart, each internal node has a decision rule that splits the data. Gini referred as Gini ratio, which measures the impurity of the node. You can say a node is pure when all of its records belong to the same class, such nodes known as the leaf node.  \n\nHere, the resultant tree is unpruned. This unpruned tree is unexplainable and not easy to understand. In the next section, let's optimize it by pruning.\n","61293dd5":"## <a id='5.1'>5.1. StandardScaler and LabelEncoder<\/a> ","c39dc2dc":"Well, you got a classification rate of 67.09%, considered as good accuracy. You can improve this accuracy by tuning the parameters in the Decision Tree Algorithm.","2a9fb3e8":"* ** StandardScaler** :\n\nStandardize features by removing the mean and scaling to unit variance : \n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the set. Mean and standard deviation are then stored to be used on later data using the transform method.\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n* ** LabelEncoder** : Encode labels with value between 0 and n_classes-1.\n\nBellow we encode the data to feed properly to our algorithm"}}