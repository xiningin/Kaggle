{"cell_type":{"886ec960":"code","c25a9b26":"code","6807e15b":"code","32ace406":"code","f89cd84b":"code","64dc8615":"code","40d061e2":"code","a75b21de":"code","a2c44229":"code","41cebde8":"code","242ab710":"code","e1f49139":"code","574caa3e":"code","aab5a417":"code","1173bf10":"code","6f2bafbd":"code","194b8afc":"code","17b72b6c":"code","38ee3b3c":"code","2cf483de":"code","39c2dcc4":"code","e32b9ca8":"code","5e417521":"code","ba1c8006":"code","6fb407b5":"code","37d3532e":"code","d555bec3":"code","4a4de503":"code","d126d422":"code","25eca3c2":"code","6661a127":"code","2703a2b9":"code","2e47c52e":"code","6f935bb6":"code","dae956b6":"code","3bb16237":"code","96f4b354":"code","17402c3a":"code","b2f1012f":"code","cc8b916d":"code","a6ef973b":"code","dc9b5f00":"code","94bb51c6":"code","df2667d5":"code","daec39a7":"code","9b8a779b":"code","1cf0c773":"code","acaf6b9f":"code","bf5018b6":"code","772a6ac1":"code","85ad9109":"code","4dd4c7cc":"code","84e179e4":"code","f5f8311f":"code","00d7c2bd":"code","028041ab":"code","5164ceb8":"code","541540b0":"code","3d9a39d4":"code","d3a3e5d2":"code","856dedc9":"code","65adae6f":"code","7ad65eb3":"code","4306fdd5":"code","dad05339":"code","7fb26f7f":"code","fe0d1c9d":"code","1a630d9c":"code","31d003c9":"code","d23cac81":"code","aa00b053":"code","aa8732e9":"code","2e6f8253":"code","c774493d":"code","b1eb4640":"code","ac90d58b":"code","afdb880d":"code","c3b36413":"code","21819938":"code","98024724":"code","b51e8f1c":"code","74d576cd":"code","6cd8a6da":"code","42b1d0a5":"code","38d3013f":"code","1674b2a7":"markdown","2266cc87":"markdown","8ef0a9a7":"markdown","f3e5c08b":"markdown","bdea929d":"markdown","69c6f97e":"markdown","f8c6e9d2":"markdown","aadd2895":"markdown","9df234c9":"markdown","a6114d61":"markdown","5bacf97e":"markdown","6e0630a2":"markdown","5570edcb":"markdown","40094e9f":"markdown","9942b958":"markdown","b7be8094":"markdown","c6280478":"markdown","9cfd479b":"markdown","85e10d59":"markdown","07b9014c":"markdown","ca6de95e":"markdown","1c928e92":"markdown","a67313f1":"markdown","d20a7856":"markdown","a1b10bbe":"markdown","8bea3a72":"markdown","cda6d7b9":"markdown","6b05b4e9":"markdown","9163b7fc":"markdown","a71b2f5b":"markdown","b4d1b02a":"markdown","da372602":"markdown","466b47f5":"markdown","c67675a0":"markdown","83a28848":"markdown","db342771":"markdown","2cde8ab1":"markdown","8efd0543":"markdown","8bf33b0d":"markdown","4c0538d6":"markdown","288af9c4":"markdown","50c94817":"markdown","21d7e4c9":"markdown","3b49c61d":"markdown","4db51c9e":"markdown","bf7b25ef":"markdown","36569301":"markdown","af722785":"markdown","ffdf0b09":"markdown","f0eeb099":"markdown","f7c20d14":"markdown","923aabe4":"markdown","e752356f":"markdown","9c212a4e":"markdown","34c1f6c2":"markdown","250e9af3":"markdown","a53d47c3":"markdown","b005c757":"markdown","54d6ecb7":"markdown","5819c0b2":"markdown","7ed8ac97":"markdown","992b38f5":"markdown","1b822954":"markdown","c35cf461":"markdown","5632c547":"markdown","f441d882":"markdown","e877b135":"markdown","a621c6ee":"markdown","8c727164":"markdown"},"source":{"886ec960":"import timeit\nmain_start_time = timeit.default_timer()\nfrom IPython.display import Image\nurl = 'https:\/\/www.netclipart.com\/pp\/m\/4-45364_free-cartoon-taxi-cab-clip-art-taxi-clipart.png'\nImage(url,width=200, height=200,retina=True)","c25a9b26":"!pip install preprocessing\n!pip install dataprep","6807e15b":"import pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport numpy as np\n#import plotly_express as px\nfrom dataprep.eda import plot,plot_correlation\n\n\nsns.set_style('whitegrid')\n%matplotlib inline \n","32ace406":"import os\nprint(os.listdir('..\/input\/nytaxi'))","f89cd84b":"nyc_taxi=pd.read_csv('..\/input\/nytaxi\/ny_taxi.csv')\nnyc_taxi.info()","64dc8615":"nyc_taxi.isnull().sum()","40d061e2":"## Checking Duplicate Values \nprint(nyc_taxi.shape)\ntemp = nyc_taxi[nyc_taxi.duplicated()]\nprint(temp.shape)\nprint(\" From the Above Code we can see there are no Duplicate Values\")\ndel temp","a75b21de":"print(\"There are %d unique id's in Training dataset, which is equal to the number of records\"%(nyc_taxi.id.nunique()))","a2c44229":"nyc_taxi.describe(include=['O'])","41cebde8":"nyc_taxi.describe()","242ab710":"#Distance  function to calculate distance between given longitude and latitude points.\nfrom math import radians, cos, sin, asin, sqrt\n\ndef distance(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n    return c * r","e1f49139":"start_time = timeit.default_timer()\n#sns.kdeplot(nyc_taxi.trip_duration)\nprint(nyc_taxi.trip_duration.min())\nprint(nyc_taxi.trip_duration.max())\nprint(nyc_taxi.trip_duration.mean())\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(\"Time Taken by :\",elapsed)","574caa3e":"start_time = timeit.default_timer()\nnyc_taxi['distance'] = nyc_taxi.apply(lambda x: distance(x['pickup_longitude'],x['pickup_latitude'],x['dropoff_longitude'],x['dropoff_latitude']), axis = 1)\nnyc_taxi['speed']= (nyc_taxi.distance\/(nyc_taxi.trip_duration\/3600))\nnyc_taxi['dropoff_datetime']= pd.to_datetime(nyc_taxi['dropoff_datetime']) \nnyc_taxi['pickup_datetime']= pd.to_datetime(nyc_taxi['pickup_datetime'])\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(\"Time Taken by :\",elapsed)","aab5a417":"#plot(nyc_taxi)","1173bf10":"## Checking Correlation  Data.\n#plot_correlation(nyc_taxi)","6f2bafbd":"nyc_taxi_new=nyc_taxi.drop(['id','vendor_id','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','store_and_fwd_flag'],axis=1)\nnyc_taxi_bkup=nyc_taxi_new.copy()","194b8afc":"def date_param(date1):\n    min1=day1.minute\n    hours=day1.hour\n    day= day1.day\n    month=day1.month\n    month_name=day1.month_name()\n    day_name=day1.day_name()\n    day_no=day1.weekday()\n    return min1,hours\n    #day_no,day_name,month,month_name\n","17b72b6c":"#nyc_taxi_new['vendor_id'] = nyc_taxi_new['vendor_id'].replace(2,0)\nstart_time = timeit.default_timer()\nnyc_taxi_new['pickup_min'] = nyc_taxi_new['pickup_datetime'].apply(lambda x : x.minute)\nnyc_taxi_new['pickup_hour'] = nyc_taxi_new['pickup_datetime'].apply(lambda x : x.hour)\nnyc_taxi_new['pickup_day'] = nyc_taxi_new['pickup_datetime'].apply(lambda x : x.day)\nnyc_taxi_new['pickup_month']= nyc_taxi_new['pickup_datetime'].apply(lambda x : int(x.month))\nnyc_taxi_new['pickup_weekday'] = nyc_taxi_new['pickup_datetime'].dt.day_name()\nnyc_taxi_new['pickup_month_name'] = nyc_taxi_new['pickup_datetime'].dt.month_name()\n\nnyc_taxi_new['drop_hour'] = nyc_taxi_new['dropoff_datetime'].apply(lambda x : x.hour)\nnyc_taxi_new['drop_month'] = nyc_taxi_new['dropoff_datetime'].apply(lambda x : int(x.month))\nnyc_taxi_new['drop_day'] = nyc_taxi_new['dropoff_datetime'].apply(lambda x : x.day)\nnyc_taxi_new['drop_min'] = nyc_taxi_new['dropoff_datetime'].apply(lambda x : x.minute)\nnyc_taxi_new['drop_weekday'] = nyc_taxi_new['dropoff_datetime'].dt.day_name()\nnyc_taxi_new['drop_month_name'] = nyc_taxi_new['dropoff_datetime'].dt.month_name()\nnyc_taxi_bkup=nyc_taxi_new.copy()\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(\"Time Taken in Adding Columns :\",elapsed)","38ee3b3c":"### There is hardly any Correlation between Passenger Count and Trip Duration evident from below linear plot . \ntemp1=nyc_taxi.head(10000)\nsns.lmplot(x='distance' ,y='trip_duration',hue='passenger_count' ,data=temp1,palette='RdBu')","2cf483de":"nyc_taxi_new.distance.groupby(pd.cut(nyc_taxi_new.distance, np.arange(0,100,10))).count().plot(kind='bar')\nplt.show()","39c2dcc4":"plt.figure(figsize = (10,5))\nsns.boxplot(nyc_taxi_new.distance)\nplt.show()","e32b9ca8":"sns.countplot(nyc_taxi_new.passenger_count)\nplt.show()","5e417521":"sns.violinplot(nyc_taxi_new.trip_duration)","ba1c8006":"## Trip Count and Duration in Buckets\nnyc_taxi_new.trip_duration.groupby(pd.cut(nyc_taxi_new.trip_duration, np.arange(1,7200,600))).count().plot(kind='bar')\nplt.xlabel('Trip Counts')\nplt.ylabel('Trip Duration (seconds)')\nplt.show()","6fb407b5":"sns.countplot(nyc_taxi_new.pickup_hour)\nplt.show()","37d3532e":"sns.countplot(nyc_taxi_new.pickup_month)\nplt.ylabel('Trip Counts')\nplt.xlabel('Months')\nplt.show()","d555bec3":"# Showing pickup and dropoff in charts\n\n#setting up canvas\nfigure,ax=plt.subplots(nrows=2,ncols=1,figsize=(10,10))\n\n# chart for pickup_day\nsns.countplot(x='pickup_weekday',data=nyc_taxi_new,ax=ax[0])\nax[0].set_title('Number of Pickups done on each day of the week')\n\n# chart for dropoff_day\nsns.countplot(x='drop_weekday',data=nyc_taxi_new,ax=ax[1])\n\nax[1].set_title('Number of dropoffs done on each day of the week')\n\nplt.tight_layout()","4a4de503":"figure,ax=plt.subplots(nrows=1,ncols=2,figsize=(12,10))\ngroup1 = nyc_taxi_new.groupby('pickup_hour').trip_duration.mean()\ngroup2 = nyc_taxi_new.groupby('drop_hour').trip_duration.mean()\n\nsns.pointplot(group1.index, group1.values,color='b',ax=ax[0])\nsns.pointplot(group1.index, group1.values,color='r',ax=ax[1])\nplt.ylabel('Trip Duration (seconds)')\nplt.xlabel('Pickup Hour')\nax[0].set_title('Pickup & Drop Hourly ')\nax[1].set_title('Drop Hourly')\nplt.show()\nplt.tight_layout()","d126d422":"figure,ax=plt.subplots(nrows=1,ncols=2,figsize=(8,6))\ngroup3 = nyc_taxi_new.groupby('pickup_month').trip_duration.mean()\ngroup4 = nyc_taxi_new.groupby('drop_month').trip_duration.mean()\n\nsns.pointplot(group3.index, group3.values,color='b',ax=ax[0])\nsns.pointplot(group4.index, group4.values,color='r',ax=ax[1])\nplt.ylabel('Trip Duration (seconds)')\nplt.xlabel('Pickup & Drop Month')\nplt.show()\nplt.tight_layout()","25eca3c2":"group6 = nyc_taxi_new.groupby('pickup_weekday').distance.mean()\nsns.pointplot(group6.index, group6.values)\nplt.ylabel('Distance (km)')\nplt.show()\nplt.tight_layout()","6661a127":"group7 = nyc_taxi_new.groupby('pickup_month').distance.mean()\nsns.pointplot(group7.index, group7.values)\nplt.ylabel('Distance (km)')\nplt.show()\nplt.tight_layout()","2703a2b9":"figure,ax=plt.subplots(nrows=1,ncols=2,figsize=(12,10))\ngroup9 = nyc_taxi_new.groupby('pickup_hour').speed.mean()\ngroup10 = nyc_taxi_new.groupby('drop_hour').speed.mean()\nsns.pointplot(group9.index, group9.values,ax=ax[0],colour='b')\nsns.pointplot(group10.index, group10.values,ax=ax[1],colour='r')\nplt.show()\nplt.tight_layout()","2e47c52e":"figure,ax=plt.subplots(nrows=1,ncols=2,figsize=(12,10))\ngroup11 = nyc_taxi_new.groupby('pickup_weekday').speed.mean()\ngroup12 = nyc_taxi_new.groupby('drop_weekday').speed.mean()\nsns.pointplot(group11.index, group11.values,ax=ax[0],colour='b')\nsns.pointplot(group12.index, group12.values,ax=ax[1],colour='r')\nplt.show()\nplt.tight_layout()","6f935bb6":"figure,ax=plt.subplots(nrows=1,ncols=2,figsize=(8,6))\nnyc_taxi_bkup1=nyc_taxi_new.copy()\n#nyc_taxi_new.trip_duration.plot(kind='kde')\nsns.distplot(np.log(nyc_taxi_new.trip_duration),ax=ax[0])\nsns.distplot(nyc_taxi_new.trip_duration,ax=ax[1])\nplt.show()\nplt.tight_layout()","dae956b6":"print('Longitude Bounds: {} to {}'.format(max(nyc_taxi.pickup_longitude.min(),nyc_taxi.dropoff_longitude.min()),max(nyc_taxi.pickup_longitude.max(),nyc_taxi.dropoff_longitude.max())))\nprint('Lattitude Bounds: {} to {}'.format(max(nyc_taxi.pickup_latitude.min(),nyc_taxi.dropoff_latitude.min()),max(nyc_taxi.pickup_latitude.max(),nyc_taxi.dropoff_latitude.max())))","3bb16237":"#Visualizing Passenger road map for picking up\nfig, ax = plt.subplots(ncols=1, nrows=1,figsize=(10,8))\nplt.ylim(40.63, 40.85)\nplt.xlim(-74.03,-73.75)\nax.scatter(nyc_taxi['pickup_longitude'],nyc_taxi['pickup_latitude'], s=0.02, alpha=1)","96f4b354":"## Drop Latitude ","17402c3a":"fig, ax = plt.subplots(ncols=1, nrows=1,figsize=(10,8))\nplt.ylim(40.63, 40.85)\nplt.xlim(-74.03,-73.75)\nax.scatter(nyc_taxi['dropoff_longitude'],nyc_taxi['dropoff_latitude'], s=0.02, alpha=1)","b2f1012f":"start_time = timeit.default_timer()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[(nyc_taxi_new['speed']<1)&(nyc_taxi_new['distance']==0)]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","cc8b916d":"nyc_taxi_new1=nyc_taxi_new.copy()","a6ef973b":"i=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[(nyc_taxi_new['pickup_day']< nyc_taxi_new['drop_day'])& (nyc_taxi_new['trip_duration']> 10000) &(nyc_taxi_new['distance'] <5) & (nyc_taxi_new['pickup_hour']<23)]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","dc9b5f00":"nyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[(nyc_taxi_new['speed']<1) & (nyc_taxi_new['distance']< 1) ]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","94bb51c6":"# Droping the 3 Columns where looks to be interstate\nnyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\nnyc_taxi_new[nyc_taxi_new['trip_duration']\/60 >10000]['trip_duration'].plot(kind='bar')\nnyc_taxi_new.drop([978383,680594,355003],inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","df2667d5":"nyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[nyc_taxi_new['distance']< .2]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","daec39a7":"#nyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[nyc_taxi_new['passenger_count']==0]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","9b8a779b":"#nyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[nyc_taxi_new['passenger_count']==0]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","1cf0c773":"nyc_taxi_new.trip_duration.groupby(pd.cut(nyc_taxi_new.passenger_count, np.arange(1,9,1))).count().plot(kind='bar')\nplt.xlabel('passenger_count')\nplt.ylabel('Trip Duration (seconds)')\nplt.show()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[nyc_taxi_new['passenger_count']>6]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","acaf6b9f":"nyc_taxi_new.describe()","bf5018b6":"import datetime as dt\nprint(len(nyc_taxi_new[nyc_taxi_new['dropoff_datetime'].dt.year>2016]))\nprint(len(nyc_taxi_new[nyc_taxi_new['dropoff_datetime'].dt.year<2016]))","772a6ac1":"#nyc_taxi_new.sort_values(by='speed',ascending=False).head(10)\n###Assuming all the Trips people take is atleast more than 1 Minutes\nnyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[nyc_taxi_new['trip_duration']<120]\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])","85ad9109":"nyc_taxi_bkup=nyc_taxi_new.copy()\ni=nyc_taxi_new.shape[0]\ntemp1=nyc_taxi_new[nyc_taxi_new['speed']>50]['speed']\n#sns.hist(temp1,bins=10)\nnyc_taxi_new.drop(temp1.index,inplace=True)\nprint(\"No of Records Deleted is \",i-nyc_taxi_new.shape[0])\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(\"Time Taken for Entire Cleaning :\",elapsed)","4dd4c7cc":"#plot(nyc_taxi_new)\n","84e179e4":"nyc_taxi_bkup=nyc_taxi_new.copy()\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nnyc_taxi_new.columns","f5f8311f":"print(nyc_taxi_new.isnull().sum())\nnyc_taxi_new[nyc_taxi_new['distance'].isnull()]\nnyc_taxi_new.dropna(inplace=True)\n","00d7c2bd":"nyc_taxi_new=nyc_taxi_bkup.copy()","028041ab":"figure,ax=plt.subplots(nrows=1,ncols=2,figsize=(10,6))\ni=nyc_taxi_new.shape[0]\nax[0].title.set_text('Trip Duration Before Removing Outliers')\nsns.distplot(nyc_taxi_new.trip_duration,ax=ax[0])\ntemp1=nyc_taxi_new.drop({'pickup_datetime','dropoff_datetime'},axis=1)\nQ1 = nyc_taxi_new.quantile(0.030)\nQ3 = nyc_taxi_new.quantile(0.970)\nIQR = Q3 - Q1\nmds_out = temp1[~((temp1 < (Q1 - 1.5 * IQR)) |(temp1 > (Q3 + 1.5 * IQR))).any(axis=1)]\nsns.distplot(mds_out.trip_duration,ax=ax[1])\nax[1].title.set_text('Trip Duration After Removing Outliers')\nprint(\"No of Records Deleted is \",i - mds_out.shape[0])\nplt.show()\nplt.tight_layout()\n\nmds_out['distance'].describe()\nnyc_taxi_new=mds_out.copy()\nnyc_taxi_new['pickup_datetime']=nyc_taxi['pickup_datetime'].max()\nnyc_taxi_new['dropoff_datetime']=nyc_taxi['dropoff_datetime'].max()","5164ceb8":"start_time = timeit.default_timer()\nimport category_encoders as ce \ntemp_encd=nyc_taxi_new.copy()\ntest1=temp_encd.drop({'pickup_datetime', 'dropoff_datetime','distance', 'speed',\n                     'pickup_weekday', 'pickup_month_name','drop_month_name','drop_weekday'},axis=1)\ncolums1=['passenger_count','pickup_min', 'pickup_hour', 'pickup_day','pickup_month','drop_hour',\n 'drop_month', 'drop_day', 'drop_min']\nencoder= ce.BinaryEncoder(cols=colums1)\ndfbin=encoder.fit_transform(test1[colums1])\nBin_Encoded=pd.concat([test1,dfbin],axis=1)\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(Bin_Encoded.shape)\nprint(elapsed)\nBin_Encoded","541540b0":"def Train_Test(sam_size=.01,scal=False):\n    sam_size=.75\n    Bin_Enc1=Bin_Encoded.sample(frac=sam_size,random_state=1)  \n    X = Bin_Enc1.drop('trip_duration',axis=1)\n    y = Bin_Enc1['trip_duration']\n    features=Bin_Enc1.drop('trip_duration',axis=1)\n    if scal==True:\n        X=preprocessing.scale(X) \n        X=pd.DataFrame(X)\n        y = Bin_Enc1['trip_duration']\n        return X,y,features\n    else: return X,y,features","3d9a39d4":"#del summ\ncol_list=['Model_Name','MenAbErr','MenSqErr','RMSE','Min_Err','Max_Err','Comments','Sample','time_sec']\nsumm=pd.DataFrame(columns=col_list)\nsumm['MenAbErr'] = summ['MenAbErr'].astype(float)\nsumm['MenSqErr'] = summ['MenSqErr'].astype(float)\nsumm['RMSE']     = summ['RMSE'].astype(float)\nsumm['Min_Err']  = summ['Min_Err'].astype(float)\nsumm['Max_Err']  = summ['Max_Err'].astype(float)\nsumm['Sample']   = summ['Sample'].astype(int)\nsumm['time_sec']  = summ['time_sec'].astype(float)\nsumm.info()","d3a3e5d2":"start_time = timeit.default_timer()\nX,y,feature_columns=Train_Test(.25,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n\nreg =linear_model.LinearRegression()\nreg.fit(X_train,y_train)\n\n#print(\"reg.intercept_=> %10.10f\" %(reg.intercept_))\n#print(list(zip(feature_columns, reg.coef_)))\ny_pred=reg.predict(X_test)\n\n#################Calculate the Error Percentages###########\nModel_Name=\"Linear Regeression\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse_val=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error   :',mabs)  \nprint('Mean Squared Error    :',mse )  \nprint('Root Mean Squared     :',rmse_val )\nprint(\"Maximum Error is      :\",Min_Error)\nprint(\"Minimum Error is      :\",Max_Error)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse_val,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\nsumm.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\nsumm","856dedc9":"#summ.drop(1,inplace=True)\n","65adae6f":"#from sklearn.metrics import mean_squared_log_error\n#feature_columns=nyc_taxi_new.drop(['drop_weekday','drop_month_name','pickup_month_name','pickup_weekday',\n#                      'pickup_datetime','dropoff_datetime','trip_duration','passenger_count','speed'],axis=1)\n#X2=nyc_taxi_new.drop(['drop_weekday','drop_month_name','pickup_month_name','pickup_weekday',\n#                      'pickup_datetime','dropoff_datetime','passenger_count','speed'],axis=1)\n\n#X,y,feature_columns=Train_Test(.05,False)\n#print(feature_columns.shape)\n#X=pd.concat([X,y],axis=1)\n#print(X.columns)#\n#X1=np.log(X)\n#X1.replace([np.inf, -np.inf,np.inf], np.nan, inplace=True)\n#X1.dropna(inplace=True)                      \n#X=pd.DataFrame(X1)\n#y=X['trip_duration']\n#X1=X.drop('trip_duration',axis=1)\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=.8,test_size=0.2, random_state=111)\n\n#reg =linear_model.LinearRegression()\n#reg.fit(X_train,y_train)\n#print(\"reg.intercept_=> %10.10f\" %(reg.intercept_))\n#print(list(zip(feature_columns, reg.coef_)))\n#y_pred=reg.predict(X_test)\n#rmse_val=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n##################Calculate the Error Percentages###########\n#Model_Name=\"Linear Regeression With Log \"\n#mabs=metrics.mean_absolute_error(y_test, y_pred)\n#mse= metrics.mean_squared_error(y_test, y_pred)\n#rmse= np.sqrt(mean_squared_log_error( y_test, y_pred ))\n#coments='Here RMSE is RMSLE Scale is Log'\n#df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\n#Min_Error=df.Error.min()\n#Max_Error=df.Error.max()\n\n#print(\"================================================================\")\n#print(\"\\n Total No of Sample Used :\",X.shape[0])\n#print(\" Total No of Test Sample COllection :\",X_test.shape[0])\n#print('\\nMean Absolute Error    :',mabs)  \n#print('Mean Squared Error     :',mse )  \n#print('Root Mean Squared Error:',rmse )\n#print(\"Maximum Error is       :\",Min_Error)\n#print(\"Minimum Error is       :\",Max_Error)\n\n#summ=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n#                  'Comments':coments,'Sample':X.shape[0],\n#                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n#                 },ignore_index=True)\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\n#summ","7ad65eb3":"feature_columns.shape\n","4306fdd5":"start_time = timeit.default_timer()\nnp.random.seed(0)\nimport lime\nimport lime.lime_tabular\nimport numpy as np\n#X,y,feature_columns=Train_Test(.25,True)\nexplainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train),feature_names=feature_columns[:30:-1], \n             verbose=True, mode='regression')","dad05339":"exp = explainer.explain_instance(X_test.iloc[350], reg.predict)\nexp.as_pyplot_figure()\npd.DataFrame(exp.as_list())","7fb26f7f":"exp = explainer.explain_instance(X_test.iloc[5000], reg.predict)\nexp.show_in_notebook(show_table=True, show_all=False)","fe0d1c9d":"exp = explainer.explain_instance(X_test.iloc[21], reg.predict)\nexp.show_in_notebook(show_table=True, show_all=False)\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(\"Time Taken by Lime :\",elapsed)","1a630d9c":"start_time = timeit.default_timer()\ny_pred_test=reg.predict(X_train)\nrmse_val=np.sqrt(metrics.mean_squared_error(y_train, y_pred_test))\nprint('Train Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_test)))\n\nelapsed = round(timeit.default_timer() - start_time,2)\nprint(\"Time Taken to Print Null RMSE :\",elapsed)","31d003c9":"start_time = timeit.default_timer()\nX,y,feature_columns=Train_Test(.25,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111,test_size=.2)\ny_null = np.zeros_like(y_test, dtype=int)\ny_null.fill(y_test.mean())\ny_pred=reg.predict(X_test)\n\nModel_Name=\"NULL Value Linear Regeression\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse_Null=np.sqrt(metrics.mean_squared_error(y_test, y_null))\ncoments='NULL Value'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error             :',mabs)  \nprint('Mean Squared Error              :',mse )  \nprint('Root Mean Squared Error For NuLL:',rmse_Null )\nprint(\"Maximum Error is                :\",Min_Error)\nprint(\"Minimum Error is                :\",Max_Error)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse_Null,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\n\nsumm\n","d23cac81":"#summ.drop(1,inplace=True)\n","aa00b053":"start_time = timeit.default_timer()\nX,y,feature_columns=Train_Test(.25,True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nreg =linear_model.LinearRegression()\n\nreg.fit(X_train,y_train)\n#print(\"reg.intercept_=> %10.10f\" %(reg.intercept_))\n#print(list(zip(feature_columns, reg.coef_)))\ny_pred=reg.predict(X_test)\n\n#################Calculate the Error Percentages###########\nModel_Name=\"Linear Regerr with Scaler\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse_val=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Scale of RMSE is Different'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse_val )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse_val,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\nsumm\n\n\n","aa8732e9":"pd.DataFrame(exp.as_list())","2e6f8253":"start_time = timeit.default_timer()\n\nimport xgboost as xgb\n#from xgboost import plot_importance, plot_tree\n#from sklearn.model_selection import RandomizedSearchCV ,cross_val_score, KFold\n\nX,y,feature_columns=Train_Test(.01,True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=111)\n\nmodel = xgb.XGBRegressor()\nmodel.fit(X_train,y_train)\nprint(model)\ny_pred = model.predict(data=X_test)\n\nModel_Name=\"XGBoost\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Standardised Data'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('\\nMean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n#summ.drop({'Min_Err','Max_Err'},inplace=True,axis=1)\nsumm","c774493d":"start_time = timeit.default_timer()\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\n## training the model\n\nX,y,feature_columns=Train_Test(.01,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)\n\n#ridgeReg = Ridge(alpha=0.05, normalize=True)\nridgeReg = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\nridgeReg.fit(X_train,y_train)\n\n#clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n\ny_pred = ridgeReg.predict(X_test)\n\nModel_Name=\"Linear Ridge Regeression\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Scale is Preprocessing'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm\n","b1eb4640":"start_time = timeit.default_timer()\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nX,y,feature_columns=Train_Test(.05,True)\nscaler=StandardScaler()\nX_norm = scaler.fit_transform(X)\npca = PCA()\npca.fit_transform(X_norm)\nvariance_ratio_cum_sum=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nprint(variance_ratio_cum_sum)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.annotate('30',xy=(30, .9652))","ac90d58b":"comp=30\nx_pca = PCA(n_components=comp)\nX_norm_final = x_pca.fit_transform(X_norm)\n# correlation between the variables after transforming the data with PCA is 0\n\ncorrelation = pd.DataFrame(X_norm_final).corr()\nsns.heatmap(correlation, vmax=1, square=True,cmap='viridis')\nplt.title('Correlation between different features')\nX_norm_final=pd.DataFrame(X_norm_final)","afdb880d":"#X2=preprocessing.scale(X_norm_final)\nX=pd.DataFrame(X_norm_final)\n#y=preprocessing.scale(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nreg =linear_model.LinearRegression()\n\nreg.fit(X_train,y_train)\ny_pred=reg.predict(X_test)\nprint(\"reg.intercept_=> %10.10f\" %(reg.intercept_))\nprint(list(zip(feature_columns, reg.coef_)))\n\n#################Calculate the Error Percentages###########\nModel_Name=\"Principal Component Analysis\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='PCA with '+str(comp)\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\n\nsumm","c3b36413":"start_time = timeit.default_timer()\nfrom sklearn.tree import DecisionTreeRegressor\nX,y,feature_columns=Train_Test(.01,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ndt=DecisionTreeRegressor()\ndt.fit(X_train,y_train)\ny_pred=dt.predict(X_test)\n\n\nModel_Name=\"DecisionTreeRegressor\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm","21819938":"start_time = timeit.default_timer()\nfrom sklearn.ensemble import RandomForestRegressor\nX,y,feature_columns=Train_Test(.01,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nrf=RandomForestRegressor()\nrf.fit(X_train,y_train)\ny_pred=rf.predict(X_test)\n\nModel_Name=\"RandomForestRegressor\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm","98024724":"start_time = timeit.default_timer()\nfrom sklearn.ensemble import AdaBoostRegressor\nab=AdaBoostRegressor()\nX,y,feature_columns=Train_Test(.01,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nab.fit(X_train,y_train)\ny_pred=ab.predict(X_test)\n\nModel_Name=\"AdaBoostRegressor\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm\n","b51e8f1c":"start_time = timeit.default_timer()\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor()\nX,y,feature_columns=Train_Test(.01,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ngb.fit(X_train,y_train)\ny_pred = gb.predict(X_test)\n\nModel_Name=\"GradientBoostingRegressor\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm","74d576cd":"start_time = timeit.default_timer()\nimport lightgbm as lgb\nlgbm = lgb.LGBMRegressor()\nX,y,feature_columns=Train_Test(.01,False)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nlgbm.fit(X_train,y_train)\ny_pred = lgbm.predict(X_test)\n\nModel_Name=\"Light GBM\"\nmabs=metrics.mean_absolute_error(y_test, y_pred)\nmse= metrics.mean_squared_error(y_test, y_pred)\nrmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\ncoments='Normal'\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,'Error':y_test -y_pred})\nMin_Error=df.Error.min()\nMax_Error=df.Error.max()\n\nprint('Mean Absolute Error    :',mabs)  \nprint('Mean Squared Error     :',mse )  \nprint('Root Mean Squared Error:',rmse )\nprint(\"Maximum Error is       :\",Min_Error)\nprint(\"Minimum Error is       :\",Max_Error)\n\n\nelapsed = round(timeit.default_timer() - start_time,2)\nsumm=summ.append({'Model_Name':Model_Name, 'MenAbErr':mabs, 'MenSqErr':mse, 'RMSE':rmse,\n                  'Comments':coments,'Sample':X.shape[0],\n                  'Min_Error':Min_Error, 'Max_Error':Max_Error,\n                  'time_sec':elapsed},ignore_index=True)\nsumm\n","6cd8a6da":"print(\"                    Summary of Error is in the Report             \")\nsumm","42b1d0a5":"plt.figure(figsize = (12,8))\n#temp=summ.copy()\n#temp.drop(2,inplace=True)\nprint(\" Here is the Summary of Each of the Models \")\nsns.lineplot(x='Model_Name',y='RMSE',data=summ)\nplt.title(\"The Figure Shows RMSE and Model Name \")","38d3013f":"elapsed = round(timeit.default_timer() - main_start_time,2)\nprint(\"Total Time Taken in Running the Notebook in Minutes :\",round(elapsed\/60,2))","1674b2a7":"## Function to Create Distance based upon Latitude and Longitude . ","2266cc87":"## Trip Duration Explained . \n1. There are 0 Seconds Trip\n1. Some trips are over 100000 Duration which seems outliers.","8ef0a9a7":"### Normalise Data for PCA","f3e5c08b":"## Removing all the rows where speed and distance both are less than 1","bdea929d":"# Now Let's Visvualise data Using data\n1. Passenger Count and Distance Correlation","69c6f97e":"# Machine Learning Algorithms","f8c6e9d2":"## Creation Train Test Function ","aadd2895":"## The Below Plot Shows the Max Passenger between 1-3 .\n1. There are also 0 Passenger \n1. Maximux there are 1 Passenger in Taxi \n","9df234c9":"## Trip Pickup Explained\n1. Maximum Pickups are between 5-10 PM \n1. Baring 2-5 am all the data looks Normalised","a6114d61":"### Lime Feature Explaination\n1. drop_dateint: features that have high negative correlations with the target are shown in maron, otherwise red.\n1. pickup_dateint : This has hight Positive correlation shown in green.\n1. distance : This has Low Correlation with value of 0.11 shown in Green .","5bacf97e":"## Removing all those records where speed is less than 1 and distance is 0","6e0630a2":"## Running Linear Regerresion Algorithum . \n1. The Max and Min error rate is checked to see the Varioution of Error on the Dataset . ","5570edcb":"##  Building AdaBoost Regressor Model","40094e9f":"## Running the Model with Standard Scaler ","9942b958":"## Building Random Forest Regressor Model","b7be8094":"## Pickup and Drop In Hourly basis\n1. Its Clear that 0-6 Hours has Least Pickup and Drops\n1. There is Sharp Rise in Pickup and Drop in from 6am to 7am \n1. Pickup and Drop Steadily Rises from 7am to Evening .\n","c6280478":"## Importing Libararies ","9cfd479b":"## Calculating Null RMSE ","85e10d59":"## Creating New Dataframe and Droping Unwanted Columns not required for Analysis","07b9014c":"## Using Plotly Graph for Data Visualsiation\n1. dataprep.eda is package imported to run this\n1. Id is uniquely Distributed\n1. Vendor id has 1 and 2 Values only\n1. Pickup datetime and Dropoff datetime and Evenly Distributed\n1. Passenger Count is Skewed towards left meaning max values between 0-2 \n","ca6de95e":"# Thanks for completing !!\n# Please Upvote if you Like . \n1. These are my initial days work the EDA part lot of time and effort had been sent . \n1. The prediction of Problem can be using the trip price also here i pridicting only trip durations \n\nNotebook created by: <a href = \"https:\/\/www.linkedin.com\/in\/narendrasharma\/\">Narendra Sharma ( Click on Name for Link)<\/a>\n# Feel Free to Drop a Comment \/Suggestion , Post on Linked!!\n<hr>\nThis notebook Solely for Education and Training and Need not to be reproduced or Copied without expilict permission from Author !!","1c928e92":"## Calculating Trip Distance \n1. Distance is Calcuated from Pickup Latitude and Latitude of Pickup and Drop\n1. Speed is Distance \/ Time as time is Seconds dividing it by 3600 to conver it to Hour.\n1. Drop off date and Pickup date time are just converted from object to date ","a67313f1":"### PCA Graph\n1. PCA Correlation Using Heat Map","d20a7856":"## Checking ID Columns for Uniqueness","a1b10bbe":"## Print Removing Pickup date and Drop are Different \n\n1. Also Distance travelled is less than 5km in approx 3 Hours\n2. Taken the night time into consideration as well ","8bea3a72":"## Pickup Count Monthwise Explained\n1. Data is Balance for Pickup Month","cda6d7b9":"# Cleaning DataSet ","6b05b4e9":"# Manipulating Data and Feature Engineering ","9163b7fc":"## Removing Columns where Passenger Count is More than 6","a71b2f5b":"## Average Speed for Pickup and Drop \n1. Average Speed is Highest During Morning Hours . \n1. Rush hours 8-18 Speed is on Lower Side","b4d1b02a":"## Building LGBM Regressor Model","da372602":"## PCA Model ","466b47f5":"### Creating Data Frame to Log the Value of Different Inputs ","c67675a0":"## Checking Trip Duration Min Max and Scale","83a28848":" ## Building GradientBoosting Regressor Model","db342771":"## Building Decision Tree Regressor Model","2cde8ab1":"## Removing Records where Speed is Over 50 Km as rest are outliers . ","8efd0543":"## Importing the Dataset\n1. Checking Data info ","8bf33b0d":"## Trip Duration Month Wise Distrbution\n1. For Pickup Avg Trip duration rises from Month of Feb\n2. For Drop Avg Drop Duration Rises from month of June","4c0538d6":"## Checking the Bell Curve with Normal Distribution and with Log ","288af9c4":"## Pickup and Drop Showng Similar Distribution for Pickup and Drop\n1. Friday and Saturday has Maximum Pickup and Drop ","50c94817":"## Droping the 3 Columns where looks to be interstate","21d7e4c9":"## Using the Quartile Method to remove Outliers still left\n1. We can See the First curve is more pointed then second one.\n1. Also see the Max Duration coming down from 2000 -1500 .\n1. Area Under the Curve will increase after this step.","3b49c61d":"## RMSLE Checking Output with Root Mean Square Error Log \n1. We can Observe the Dataset when used gives better result even without deleting outliers\n1. The method to Calcuate RMSLE is little different so mean_squared_log_error method is imported.\n1. We made the Comparision of running the RMSL with log for with Cleaning and Without Cleaning.\n1. Without Cleaning the RMSLE was 0.0.090433 and With Cleaned dataset the Result was 0.056\n   hence cleaned dataset was used below","4db51c9e":"## Summary Notes\n1. We can See Random Forest is Performing the Best in terms of RMSE and Mean Absolute Error .\n1. Incidently time taken by Random Forest is also on the Highest Side . \n1. There was some hard coding done to make the all algos run with same no or records this can be changed to no of value. I have tested that all linear model can be run with 100% but would fail even with 20% dataset for Xboost Random Forest on Memory.\n1. It was Observed that One Hot Encoding of Multiple columns all attribute of date(Month, Hour, Date,Minute) Yields better result as these have correlation to trip duration . \n1. Running the Model with Random Dataset is approach followed if the dataset has huge volume like this One wither over Million records. \n1. The code is written in a manner all or any one Model can be run independtly to increase efficency . \n1. Time to run the Algos of ML should also be considered while running the Models. ","bf7b25ef":"## Lime to Predict the Model","36569301":"## XGBoost Machine Learning Model","af722785":"## Maximum Pickup Weekday Wise\n1. Its clear Sunday has maximum distance covered ","ffdf0b09":"## Average Distance Covered Month Wise Trip.\n1. Its Clear May has Highest Distance Per Trip Covered.","f0eeb099":"# NYC taxi trip duration\n\n1. id - a unique identifier for each trip \n1. vendor_id - a code indicating the provider associated with the trip record \n1. pickup_datetime - date and time when the meter was engaged \n1. dropoff_datetime - date and time when the meter was disengaged\n1. passenger_count - the number of passengers in the vehicle (driver entered value)\n1. pickup_longitude - the longitude where the meter was engaged\n1. pickup_latitude - the latitude where the meter was engaged\n1. dropoff_longitude - the longitude where the meter was disengaged\n1. dropoff_latitude - the latitude where the meter was disengaged\n1. store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor\n1. because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n1. trip_duration - duration of the trip in seconds\n","f7c20d14":"## Average Speed Weekday \n1. Average Speed is Highest on Sunday\n2. Weekend Speed is Higher than on Weekday.","923aabe4":"### Running the Program with 6 Features given by PCA","e752356f":"## Importing the Data set\n\n1. Dataprep.eda is Used to Plot Multiple Graph","9c212a4e":"## Describe on Numeric Column \n","34c1f6c2":"\n## Checking Describe of Catagorical Columns","250e9af3":"## Ploting the Graph of Various Models with RMSE Values.\n1. RMSE is Root Mean Square Error\n1. RMSLE is the Root Mean Squar Log Error Where the Graph Dips to Bottom","a53d47c3":"### Validating the Entire data set belongs to only year 2016","b005c757":"## Observation on Catagorical Column\n1. There are 4 columns which are of Object type\n1. store_and_fwd_flag have only two type of unique values\n1. pickup_datetime and dropoff_datetime are of type Object which actually should be datetime for better analysis","54d6ecb7":"## Removing Distance Less than 500 Meteres ","5819c0b2":"## Removing Records when Passenger Count is 0","7ed8ac97":"## Trup Duration in Buckets Explained\n1. Maximum Trip are 1-10 Minutes\n1. Second Highest Trip is 10-20 Minutes\n3. There are over 60K Seconds trip also there.","992b38f5":"## Observation on Numeric Columns \n\n1. Vendor id is Unique\n1. Passenger Count has only 1 and 2 Value\n1. Pickup Longitude , Pickup Latitude , Drop Longitutude , Latitude Stores the distance \n1. Trip Duration is second with Outliers .\n1. Distance and Speed are Derived Values ","1b822954":"## Removing Columns where Trip Duration is less than 2 Minutes ","c35cf461":"## Checking Duplicate Values ","5632c547":"## Implementing Ridge ","f441d882":"## Trip Distance \n1. We can observe Maximum Trip are between 0-10 and 10% between 10-20KM Distance  \n1. There some trips with over 100 km distance.\n1. There are 0 KM distance Trip also","e877b135":"## Checking Null Values","a621c6ee":"## Adding Mulitiple Columns in New Data Frame \n1. Pickup Minutes , Pickup Hour , Pickup Day Month \n1. Similar to Pickup Adding Dropoff correlated columns","8c727164":"## Prediction on Train Data Set "}}