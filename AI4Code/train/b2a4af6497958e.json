{"cell_type":{"c6728590":"code","e8024772":"code","080a20e2":"code","99e03bc7":"code","6322bfcd":"code","c1105caa":"code","d0661a06":"code","1d5ca284":"code","49b2cb85":"code","17001b53":"code","02c7cdc4":"code","f2c7a1fc":"code","d9a19f6c":"code","b40d66d4":"code","4979e5b6":"code","eebee2e4":"code","39eff173":"code","9822823f":"code","f3ce0698":"code","53bf71ca":"code","5cc0bc24":"code","af58df9e":"code","265889f8":"code","02fced23":"code","57853d77":"code","14a3b661":"code","a1416bff":"code","796f6f21":"code","a223e2d5":"code","c16b54ed":"code","2a4a83bf":"code","dae3dc0b":"code","2aadee29":"code","82a7353f":"code","9065f17c":"code","1753e017":"code","8eef3b77":"code","0f9f66fe":"code","eac7762f":"code","8441393f":"code","08bd7122":"code","c2bff8a6":"code","0ad02a69":"code","b7ffbaf9":"code","56f5bfc2":"code","c200e621":"code","e61bf3ed":"code","440119d5":"code","01ef41f5":"code","89aa172f":"code","0a55f060":"code","27c4da2c":"code","948255a8":"code","c2bd4d3a":"code","a73fc843":"code","342e5868":"code","03e1a512":"code","4cdee03e":"code","d580e788":"code","605e47e8":"code","cf4a8558":"code","5080ca5f":"code","68091b0c":"code","5755de6c":"code","f6c8d2c7":"code","0153ae33":"code","f787a013":"code","36b621c1":"code","1775f38b":"code","228e1fc3":"code","e9f25927":"code","c6f0a6be":"code","9f35b7b5":"code","15edac0c":"code","b8baabaf":"code","7e0e272c":"code","d3c963aa":"code","bd9fc995":"code","ee700146":"markdown","b3ef452e":"markdown","ebe844ac":"markdown","856bd610":"markdown","612ff913":"markdown","e3c8f4ec":"markdown","bfbd2ecb":"markdown","326f5cbe":"markdown","a1323ba0":"markdown","65c8ad4c":"markdown","17546e16":"markdown","b02d7e2c":"markdown","c1392fe8":"markdown","937331b4":"markdown","11147025":"markdown","316b12a9":"markdown","2b2a22a6":"markdown","d9eb2a4b":"markdown","8ab69f3e":"markdown","245e5f0f":"markdown","33c71232":"markdown","291bdcd9":"markdown","cc41b0b3":"markdown","b07d0de2":"markdown","0eb63ac8":"markdown","9a3e5232":"markdown","27e8a6ec":"markdown","4a0c088a":"markdown","36cfcc9f":"markdown","e3d7c3fa":"markdown","9d0f37d2":"markdown","163be9ec":"markdown","aba12b03":"markdown"},"source":{"c6728590":"This is a novice attempt toward an attitude to learn andd experiment. \nConsequently, It has been taken from multiple source\n\nDescription:Context This database contains 76 attributes. \nAttribute Information:\n\n1. age\n2. sex\n3.chest pain type (4 values)\n4. resting blood pressure\n5. serum cholestoral in mg\/dl\n6. fasting blood sugar > 120 mg\/dl\n7. resting electrocardiographic results (values 0,1,2)\n8. maximum heart rate achieved\n9. exercise induced angina\n10. oldpeak = ST depression induced by exercise relative to rest\n11. the slope of the peak exercise ST segment\n12. number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defec\n\n    Attempt to \ni) To create as many new features as possible \nii) Use RandomForest to make predictions \niii) Evaluate feature importance","e8024772":"# 1.0 Clear memory\n%reset -f\n\n# 1.1 Call data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# 1.2 Feature creation libraries\nfrom sklearn.random_projection import SparseRandomProjection as sr  # Projection features\nfrom sklearn.cluster import KMeans                    # Cluster features\nfrom sklearn.preprocessing import PolynomialFeatures  # Interaction features\n\n# 1.3 For feature selection\n# Ref: http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria\n# 1.4 Data processing\n# 1.4.1 Scaling data in various manner\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n# 1.4.2 Transform categorical (integer) to dummy\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 1.5 Splitting data\nfrom sklearn.model_selection import train_test_split\n\n# 1.6 Decision tree modeling\n# http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.tree\n# http:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree\nfrom sklearn.tree import  DecisionTreeClassifier as dt\n\n# 1.7 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\n# 1.8 Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1.9 plot tree\nfrom sklearn.tree import export_graphviz\n# 1.10 for model evaluation\n\nfrom sklearn.metrics import roc_curve, auc \n\n# 1.11 for model evaluation\n\nfrom sklearn.metrics import confusion_matrix \n\n# 1.11 Misc\nimport os, time, gc\n\n","080a20e2":"# 2.1 Read train\/test files\nheart = pd.read_csv(\"..\/input\/heart.csv\")","99e03bc7":"heart.head(5)","6322bfcd":"heart.shape ","c1105caa":"# 2.2 Split into Test and Training Data\nX_train, X_test, y_train, y_test = train_test_split(\n        heart.drop('target', 1), \n        heart['target'], \n        test_size = 0.3, \n        random_state=10\n        )","d0661a06":"# 2.3 Look at data\nX_train.head(2)\nX_train.shape                        # 212 x 13\nX_test.shape                         # 91 x 13\n\ny_test.shape                        # 91 x \ny_train.shape                       # 212 x\n\n# Data types\nX_train.dtypes.value_counts()   # All afeatures re integers ","1d5ca284":"# 2.4 Target classes are almost balanced\nheart.target.value_counts()","49b2cb85":"# 3 Check if there are Missing values? None\nX_train.isnull().sum().sum()  # 0\nX_test.isnull().sum().sum()   # 0","17001b53":"#  4. Feature 1: Row sums of features 1:13. More successful\n#                when data is binary.\n\nX_train['sum'] = X_train.sum(numeric_only = True, axis=1)  # numeric_only= None is default\nX_test['sum'] = X_test.sum(numeric_only = True,axis=1)","02c7cdc4":"# 4.1 Assume that value of '0' in a cell implies missing feature\n#     Transform train and test dataframes\n#     replacing '0' with NaN\n#     Use pd.replace()\ntmp_train = X_train.replace(0, np.nan)\ntmp_test = X_test.replace(0,np.nan)","f2c7a1fc":"# 4.2 Check if tmp_train is same as train or is a view\n#     of train? That is check if tmp_train is a deep-copy\n\ntmp_train is X_train                # False\ntmp_train._is_view                # False","d9a19f6c":"# 4.3 Check if 0 has been replaced by NaN\ntmp_train.head(1)\ntmp_test.head(1)","b40d66d4":"# 5. Feature 2 : For every row, how many features exist\n#                that is are non-zero\/not NaN.\n#                Use pd.notna()\ntmp_train.notna().head(1)\nX_train[\"count_not0\"] = tmp_train.notna().sum(axis = 1)\nX_test['count_not0'] = tmp_test.notna().sum(axis = 1)","4979e5b6":"# 6. Similary create other statistical features\n#    Feature 3\n#    Pandas has a number of statistical functions\n#    Ref: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/frame.html#computations-descriptive-stats\n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    X_train[i] = tmp_train.aggregate(i,  axis =1)\n    X_test[i]  = tmp_test.aggregate(i,axis = 1)","eebee2e4":"# 7 Delete not needed variables and release memory\ndel(tmp_train)\ndel(tmp_test)\ngc.collect()","39eff173":"# 7.1 So what do we have finally\nX_train.shape                \nX_train.head(1)\nX_test.shape                 \nX_test.head(2)","9822823f":"# 8. Before we proceed further, keep target feature separately\ntarget = y_train\ntarget.tail(2)","f3ce0698":"# 9. Store column names of our data somewhere\n#     We will need these later (at the end of this code)\ncolNames = X_train.columns.values\ncolNames","53bf71ca":"# 11. Generate features using random projections\n#     First stack train and test data, one upon another\ntmp = pd.concat([X_train,X_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )","5cc0bc24":"# 12.1\ntmp.shape     # 303 X 21","af58df9e":"# 12.2 Transform tmp t0 numpy array\n#      Henceforth we will work with array only\ntmp = tmp.values\ntmp.shape       # 303 X 21","265889f8":"# 13. Let us create 8 random projections\/columns\n#     This decision, at present, is arbitrary\nNUM_OF_COM = 8","02fced23":"# 13.1 Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)","57853d77":"# 13.2 fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\nrp = rp_instance.fit_transform(tmp[:, :13])","14a3b661":"# 13.3 Look at some features\nrp[: 5, :  3]","a1416bff":"# 13.4 Create some column names for these columns\n#      We will use them at the end of this code\nrp_col_names = [\"r\" + str(i) for i in range(8)]\nrp_col_names","796f6f21":"# 14. Before clustering, scale data\n# 15.1 Create a StandardScaler instance\nse = StandardScaler()\n# 15.2 fit() and transform() in one step\ntmp = se.fit_transform(tmp)\n# 15.3\ntmp.shape               # 303 X 21 ","a223e2d5":"# 16. Perform kmeans using 13 features.\n#     No of centroids is no of classes in the 'target'\ncenters = target.nunique()    # 2 unique classes\ncenters               # 2","c16b54ed":"# 17.1 Begin clustering\nstart = time.time()\n\n# 17.2 First create object to perform clustering\nkmeans = KMeans(n_clusters=centers, # How many clusters\n                n_jobs = 4)         # Parallel jobs for n_init\n\n# 17.3 Next train the model on the original data only\nkmeans.fit(tmp[:, : 13])\n\nend = time.time()\n(end-start)\/60.0      # 0.007 minutes","2a4a83bf":"# 18 Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size   # 303","dae3dc0b":"# 19.1 Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)","2aadee29":"# 19.2 Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     # reshape(-1,1) recommended by fit()\n                                          # '-1' is a placeholder for actual","82a7353f":"# 19.3 Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    # 303 X 2 (as many as there are classes)","9065f17c":"# 19.4 We will use the following as names of new two columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names","1753e017":"# 21. Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )","8eef3b77":"# 21.1 Consider only first 8 features\n#      fit and transform\ndf =  poly.fit_transform(tmp[:, : 8])\n\n\ndf.shape     # 303 X 36","0f9f66fe":"# 21.2 Generate some names for these 36 columns\npoly_names = [ \"poly\" + str(i)  for i in range(36)]\npoly_names","eac7762f":"tmp.shape          # 303 X 21","8441393f":"#  22.1 If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, df])\nelse:\n    tmp = np.hstack([tmp,rp, df])       # No kmeans      <==\n\n\ntmp.shape          # 303 X 67","08bd7122":"# 22.1 Combine train and test into X and y to split compatible datasets\nX = tmp\nX.shape        # 303 X 67","c2bff8a6":"# 22.2 Combine y_train and y_test into y to split into compatible datasets later\ny = pd.concat([y_train,y_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )\ny.shape        # 303,","0ad02a69":"# 22.3 Delete tmp - as a good programming practice\ndel tmp\ngc.collect()","b7ffbaf9":"# 23. Split the feature engineered data into new training and test dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.3)","56f5bfc2":"# 23.1\nX_train.shape    # 212 X 67","c200e621":"X_test.shape     # 91 X 67","e61bf3ed":"# 24 Decision tree classification\n# 24.1 Create an instance of class\nclf1_dt = dt(min_samples_split = 5,\n         min_samples_leaf= 3\n        )","440119d5":"start = time.time()\n# 24.2 Fit\/train the object on training data\n#      Build model\nclf1_dt = clf1_dt.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60                     # << 1 minute","01ef41f5":"# 24.3 Use model to make predictions\nclasses1_dt = clf1_dt.predict(X_test)","89aa172f":"# 24.4 Check accuracy\n(classes1_dt == y_test).sum()\/y_test.size    ","0a55f060":"# 25. Instantiate RandomForest classifier\nclf1_rf = rf(n_estimators=50)","27c4da2c":"# 25.1 Fit\/train the object on training data\n#      Build model\n\nstart = time.time()\nclf1_rf = clf1_rf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60                    # 0.001 min","948255a8":"# 25.2 Use model to make predictions\nclasses1_rf = clf1_rf.predict(X_test)","c2bd4d3a":"# 25.3 Check accuracy\n(classes1_rf == y_test).sum()\/y_test.size  ","a73fc843":"# 26. Get feature importance\nclf1_rf.feature_importances_        # Column-wise feature importance\nclf1_rf.feature_importances_.size   # 67","342e5868":"# 26.1 To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       # If dummy_clusterlabels labels are defined\n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names      # No kmeans      <==","03e1a512":"# 26.1.1 So how many columns?\nlen(colNames)           # 67 with kmeans ","4cdee03e":"# 26.2 Create a dataframe of feature importance and corresponding\n#      column names. Sort dataframe by importance of feature\nfeat_imp = pd.DataFrame({\"importance\": clf1_rf.feature_importances_ ,\n                   \"featureNames\" : colNames\n                  }).sort_values(by = \"importance\", ascending=False)","d580e788":"feat_imp.shape                   # 67 X 2 \nfeat_imp.head(13)","605e47e8":"# 26.3 Plot feature importance for first 20 features\ng = sns.barplot(x = feat_imp.iloc[  : 20 ,  1] , y = feat_imp.iloc[ : 20, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","cf4a8558":"# 27 Select top 13 columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex = feat_imp.index.values[:13]\nnewindex","5080ca5f":"#Create DTree classifier object\nclf2_dt = dt(min_samples_split = 5, min_samples_leaf= 3)","68091b0c":"# 28.2 Train the object on data\nstart = time.time()\nclf2_dt = clf2_dt.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)\/60 ","5755de6c":"# 28.3  Make prediction\nclasses2_dt = clf2_dt.predict(X_test[: , newindex])","f6c8d2c7":"# 28.4 Accuracy?\n(classes2_dt == y_test).sum()\/y_test.size ","0153ae33":"# 28.5  Create RForest classifier object\nclf2_rf = rf(n_estimators=50)","f787a013":"# 28.6 Traion the object on data\nstart = time.time()\nclf2_rf = clf2_rf.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)\/60  ","36b621c1":"# 28.7  Make prediction\nclasses2_rf = clf2_rf.predict(X_test[: , newindex])","1775f38b":"# 28.8 Accuracy?\n(classes2_rf == y_test).sum()\/y_test.size  ","228e1fc3":"# 29 Select top 20 columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex2 = feat_imp.index.values[:20]\nnewindex2","e9f25927":"# 30.1  Create DTree classifier object\nclf3_dt = dt(min_samples_split = 5, min_samples_leaf= 3)","c6f0a6be":"# 30.2 Train the object on data\nstart = time.time()\nclf3_dt = clf3_dt.fit(X_train[: , newindex2], y_train)\nend = time.time()\n(end-start)\/60 ","9f35b7b5":"# 30.3  Make prediction\nclasses3_dt = clf3_dt.predict(X_test[: , newindex2])","15edac0c":"# 30.4 Accuracy?\n(classes3_dt == y_test).sum()\/y_test.size","b8baabaf":"# 30.5  Create RForest classifier object\n# increasing the number of estimators to 100 from 50...\nclf3_rf = rf(n_estimators=100)","7e0e272c":"# 30.6 Train the object on data\nstart = time.time()\nclf3_rf = clf3_rf.fit(X_train[: , newindex2], y_train)\nend = time.time()\n(end-start)\/60    ","d3c963aa":"# 30.7  Make prediction\nclasses3_rf = clf3_rf.predict(X_test[: , newindex2])","bd9fc995":"# 26.4 Accuracy?\n(classes3_rf == y_test).sum()\/y_test.size ","ee700146":"Use these top 13 columns for classification\n","b3ef452e":"Notice the K-means parameters... they're found to be most relevant!!","ebe844ac":"3. Rforest provides highest accuracy of 80.2% with feature selection... ok\n","856bd610":"2.0 Set working directory and read file","612ff913":"\n76.9% accuracy from dtree after feature selection - 20 features","e3c8f4ec":"(i) Random projection is a fast dimensionality reduction feature\n(ii) Also used to look at the structure of data","bfbd2ecb":"X_test: test data with new features\n","326f5cbe":"1. DTree accuracy has gone up from 67% to 73.6% with feature selection... ok\n","a1323ba0":"Feature creation using kmeans","65c8ad4c":"Using Polynomials","17546e16":"Using feature importance given by model","b02d7e2c":"Notice the number of derived parameters making into the top important features","c1392fe8":"73.6% accuracy from dtree after feature selection - 13 features\n","937331b4":"Feature selection","11147025":"y_train: expected output for training data\n","316b12a9":"76.9% accuracy from rforest with all features","2b2a22a6":"Cluster labels are categorical. So convert them to dummy","d9eb2a4b":"Feature creation Using Random Projections","8ab69f3e":"Let's try something else to check impact on accuracy?","245e5f0f":"Using Statistical Numbers","33c71232":"X_train: Training Data with new features\n","291bdcd9":"Now try using top 20 features for classification as per newindex2","cc41b0b3":"Creating Interaction features","b07d0de2":"i)   Shooting in dark. These features may help or may not help\nii)  There is no exact science as to which features will help","0eb63ac8":"67% accuracy from dtree with all features\n","9a3e5232":"concatenate all features now\nAppend now all generated features together\nAppend random projections, kmeans and polynomial features to tmp array","27e8a6ec":"Note:\n","4a0c088a":"BB. Feature Engineering","36cfcc9f":"2. Rforest accuracy has gone up from 76.9% to 80.2% with feature selection... ok\n","e3d7c3fa":"Accuracy from rforest after feature selection - 13 features","9d0f37d2":"y_test: expected output for test data","163be9ec":"Model building","aba12b03":"we now have the following data for model creation and testing\n"}}