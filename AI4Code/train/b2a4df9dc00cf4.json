{"cell_type":{"9bdd5a0b":"code","6bfea9a0":"code","770a3d82":"code","4b11fc89":"code","88f5fadf":"code","e6748616":"code","51745543":"code","da888637":"code","2fe011f0":"code","d58afcdb":"code","b74de26b":"code","0ba8750c":"code","9283e271":"code","e663d40d":"code","b2184fce":"code","45b682c0":"code","0d999ca8":"markdown","7785e1cb":"markdown","66bf8116":"markdown","ebf89a43":"markdown","0a410d7a":"markdown","7516dd1e":"markdown","cb322728":"markdown","41513e2a":"markdown","aa5414e0":"markdown","bfc3002f":"markdown","88a30466":"markdown"},"source":{"9bdd5a0b":"import os\nimport time\nimport math\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import DistilBertTokenizer, DistilBertModel","6bfea9a0":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","770a3d82":"OUTPUT_DIR = \".\/\"\nBASE_DATA_PATH = Path(\"..\/input\/commonlitreadabilityprize\/\")\n\n!ls {BASE_DATA_PATH}","4b11fc89":"df_train = pd.read_csv(BASE_DATA_PATH \/ \"train.csv\")\ndf_test = pd.read_csv(BASE_DATA_PATH \/ \"test.csv\")\ndf_sub = pd.read_csv(BASE_DATA_PATH \/ \"sample_submission.csv\")","88f5fadf":"df_train.head(1)","e6748616":"df_train.target.max(), df_train.target.min()","51745543":"df_test.head(1)","da888637":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n        \nLOGGER = init_logger()","2fe011f0":"class CommonLitDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_length):\n    \n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        tokenized_input = self.tokenizer(row.excerpt, return_tensors=\"pt\", \n                                         max_length=self.max_length, \n                                         padding=\"max_length\", truncation=True)\n        return {\n            \"ids\": tokenized_input[\"input_ids\"][0],\n            \"masks\": tokenized_input[\"attention_mask\"][0],\n            \"targets\": torch.tensor(row.target).float()\n        }","d58afcdb":"class TextRegressionModel(nn.Module):\n    \n    def __init__(self, model_name, dropout_p=0.1):\n        super(TextRegressionModel, self).__init__()\n        \n        self.model = AutoModel.from_pretrained(model_name)\n        self.features = nn.Linear(768, 768)\n        self.dropout = nn.Dropout(dropout_p)\n        self.out = nn.Linear(768, 1)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        output = F.relu(self.features(output.last_hidden_state[:, 0]))\n        output = self.dropout(output)\n        output = self.out(output)\n        return output","b74de26b":"def train_step(model, criterion, optimizer, data_loader, epoch, device=device):\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    train_loss = AverageMeter()\n    model.train()\n    \n    start = end = time.time()\n    \n    for step, batch in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        \n        input_ids = batch[\"ids\"].to(device)\n        attention_masks = batch[\"masks\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        bs = input_ids.size(0)\n        \n        output = model(input_ids, attention_masks)\n        loss = criterion(output.squeeze(1), targets)\n        train_loss.update(loss.item(), bs)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % CFG.print_freq == 0 or step == (len(data_loader) - 1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                      epoch+1, step, len(data_loader), batch_time=batch_time,\n                      data_time=data_time, loss=train_loss,\n                      remain=timeSince(start, float(step+1)\/len(data_loader)))\n                 )\n\n    return train_loss.avg\n        \n    \ndef eval_step(model, criterion, data_loader, epoch, device=device):\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    eval_loss = AverageMeter()\n    model.eval()\n    \n    start = end = time.time()\n    \n    for step, batch in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        \n        input_ids = batch[\"ids\"].to(device)\n        attention_masks = batch[\"masks\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        bs = input_ids.size(0)\n        \n        output = model(input_ids, attention_masks)\n        loss = criterion(output.squeeze(1), targets)\n        eval_loss.update(loss.item(), bs)\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if step % CFG.print_freq == 0 or step == (len(data_loader) - 1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                      epoch+1, step, len(data_loader), batch_time=batch_time,\n                      data_time=data_time, loss=eval_loss,\n                      remain=timeSince(start, float(step+1)\/len(data_loader)))\n                 )\n\n    return eval_loss.avg","0ba8750c":"class CFG:\n    model_name = \"distilbert-base-cased\"\n    \n    max_length = 256\n    dropout_p = 0.5\n    batch_size = 32\n    n_epochs = 5\n    weight_decay = 1e-6\n    lr = 3e-4\n    min_lr = 1e-6\n    scheduler = \"CosineAnnealingLR\"\n    T_max = 10\n    seed = 42\n    n_folds = 5    \n    print_freq = 50\n    num_workers = 4\n","9283e271":"def train_loop(folds, fold):\n    \n    train_index = folds[folds[\"fold\"] != fold].index\n    valid_index = folds[folds[\"fold\"] == fold].index\n    \n    train_folds = folds.loc[train_index].reset_index(drop=True)\n    valid_folds = folds.loc[valid_index].reset_index(drop=True)\n    \n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.model_name)\n    tokenizer.save_pretrained(f\"{CFG.model_name}_tokenizer\")\n    \n    train_dataset = CommonLitDataset(df=train_folds, tokenizer=tokenizer, max_length=CFG.max_length)\n    valid_dataset = CommonLitDataset(df=valid_folds, tokenizer=tokenizer, max_length=CFG.max_length)\n    \n    train_data_loader = DataLoader(train_dataset, \n                                   batch_size=CFG.batch_size, \n                                   shuffle=True, \n                                   num_workers=CFG.num_workers, \n                                   pin_memory=True)\n    valid_data_loader = DataLoader(valid_dataset, \n                                   batch_size=CFG.batch_size, \n                                   shuffle=False, \n                                   num_workers=CFG.num_workers, \n                                   pin_memory=True)\n    \n    def get_scheduler(optimizer):\n        if CFG.scheduler=='CosineAnnealingLR':\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n    \n    model = TextRegressionModel(model_name=CFG.model_name, dropout_p=CFG.dropout_p)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    scheduler = get_scheduler(optimizer)\n    \n    criterion = nn.MSELoss().to(device)\n    best_loss = np.inf\n    \n    for epoch in range(CFG.n_epochs):\n        \n        start_time = time.time()\n        train_loss = train_step(model, criterion, optimizer, train_data_loader, epoch)\n        eval_loss = eval_step(model, criterion, valid_data_loader, epoch)\n        scheduler.step()\n        \n        elapsed = time.time() - start_time\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {train_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - avg_eval_loss: {eval_loss:.4f}')\n        \n        if eval_loss < best_loss:\n            best_loss = eval_loss\n            \n            torch.save({\n                \"model\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict()\n            }, f\"{CFG.model_name}_fold_{fold}_best.pth\")\n        ","e663d40d":"from sklearn.model_selection import KFold\n\nfolds = df_train.copy()\nFold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\nfor n, (train_idx, valid_idx) in enumerate(Fold.split(folds)):\n    folds.loc[valid_idx, \"fold\"] = int(n)\n    \nfolds[\"fold\"] = folds[\"fold\"].astype(int)\nprint(folds.groupby([\"fold\"]).size())","b2184fce":"def main():\n    \n    # oof_df = pd.DataFrame()\n    for fold in range(CFG.n_folds):\n        train_loop(folds, fold)\n        ","45b682c0":"seed_torch(seed=CFG.seed)\nmain()","0d999ca8":"# Config","7785e1cb":"# Main","66bf8116":"# Imports","ebf89a43":"# Model","0a410d7a":"# Utils","7516dd1e":"# Dataset","cb322728":"# Train Loop","41513e2a":"# Helper Functions","aa5414e0":"# Data Loading","bfc3002f":"# About this notebook\n\n* Pytorch DistilBert training code.\n* Inference notebook is [here](https:\/\/www.kaggle.com\/snnclsr\/commonlit-pytorch-distilbert-inference\/).\n\n\nIf this notebook is helpful, feel free to upvote :)\n\n**Some of the parts of this notebook taken from [Y.Nakama](https:\/\/www.kaggle.com\/yasufuminakama)'s notebooks. Please also check his notebooks as well from [here](https:\/\/www.kaggle.com\/yasufuminakama\/code)**","88a30466":"# CV Split"}}