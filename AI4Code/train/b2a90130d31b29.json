{"cell_type":{"16ffed98":"code","c7c7b8fa":"code","8a3e05ff":"code","2bcb114e":"code","d400de39":"code","322f946e":"markdown","50b412be":"markdown","919a6900":"markdown","d44eabd8":"markdown","aa4f2f0a":"markdown"},"source":{"16ffed98":"import numpy as np, os\nimport pandas as pd\n\nimport optuna\n\n# https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/274717 \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nDEBUG = False\nTRAIN_MODEL = False\nINFER_TEST = True\nONE_FOLD_ONLY = True\nCOMPUTE_LSTM_IMPORTANCE = True\n\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\npressure_values = np.sort( train.pressure.unique() )\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*1000]","c7c7b8fa":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nprint('Train dataframe shape',train.shape)\ntrain.head()","8a3e05ff":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\n\nCOLS = list(train.columns)\nprint('Number of feature columns =', len(COLS) )\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","2bcb114e":"EPOCH = 300\nBATCH_SIZE = 1024\nNUM_FOLDS = 10\n\n# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# GET GPU STRATEGY\ngpu_strategy = tf.distribute.get_strategy()\n\nwith gpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        K.clear_session()\n        \n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        if TRAIN_MODEL:\n            model = keras.models.Sequential([\n                keras.layers.Input(shape=train.shape[-2:]),\n                keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True)),\n                keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n                keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n                keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n                keras.layers.Dense(128, activation='selu'),\n                keras.layers.Dense(1),\n            ])\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n            \n        else:\n            model = keras.models.load_model('..\/input\/finetune-of-tensorflow-bidirectional-lstm\/'+checkpoint_filepath)\n\n        if INFER_TEST:\n            print(' Predicting test data...')\n            test_preds.append(model.predict(test,verbose=0).squeeze().reshape(-1, 1).squeeze())\n                    \n        if COMPUTE_LSTM_IMPORTANCE:\n            results = []\n            print(' Computing LSTM feature importance...')\n            \n            # COMPUTE BASELINE (NO SHUFFLE)\n            oof_preds = model.predict(X_valid, verbose=0).squeeze() \n            baseline_mae = np.mean(np.abs( oof_preds-y_valid ))\n            results.append({'feature':'BASELINE','mae':baseline_mae})           \n\n            for k in tqdm(range(len(COLS))):\n                \n                # SHUFFLE FEATURE K\n                save_col = X_valid[:,:,k].copy()\n                np.random.shuffle(X_valid[:,:,k])\n                        \n                # COMPUTE OOF MAE WITH FEATURE K SHUFFLED\n                oof_preds = model.predict(X_valid, verbose=0).squeeze() \n                mae = np.mean(np.abs( oof_preds-y_valid ))\n                results.append({'feature':COLS[k],'mae':mae})\n                X_valid[:,:,k] = save_col\n         \n            # DISPLAY LSTM FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('mae')\n            plt.figure(figsize=(10,20))\n            plt.barh(np.arange(len(COLS)+1),df.mae)\n            plt.yticks(np.arange(len(COLS)+1),df.feature.values)\n            plt.title('LSTM Feature Importance',size=16)\n            plt.ylim((-1,len(COLS)+1))\n            plt.plot([baseline_mae,baseline_mae],[-1,len(COLS)+1], '--', color='orange',\n                     label=f'Baseline OOF\\nMAE={baseline_mae:.3f}')\n            plt.xlabel(f'Fold {fold+1} OOF MAE with feature permuted',size=14)\n            plt.ylabel('Feature',size=14)\n            plt.legend()\n            plt.show()\n                               \n            # SAVE LSTM FEATURE IMPORTANCE\n            df = df.sort_values('mae',ascending=False)\n            df.to_csv(f'lstm_feature_importance_fold_{fold+1}.csv',index=False)\n                               \n        # ONLY DO ONE FOLD\n        if ONE_FOLD_ONLY: break","d400de39":"if INFER_TEST:\n    PRESSURE_MIN = pressure_values[0]\n    PRESSURE_MAX = pressure_values[-1]\n    PRESSURE_STEP = pressure_values[1] - pressure_values[0]\n\n    # NAME POSTFIX\n    postfix = ''\n    if ONE_FOLD_ONLY: \n        NUM_FOLDS = 1\n        postfix = '_fold_1'\n        \n    # ENSEMBLE FOLDS WITH MEAN\n    submission[\"pressure\"] = sum(test_preds)\/NUM_FOLDS\n    submission.to_csv(f'submission_mean{postfix}.csv', index=False)\n\n    # ENSEMBLE FOLDS WITH MEDIAN\n    submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n    submission.to_csv(f'submission_median{postfix}.csv', index=False)\n\n    # ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n    submission[\"pressure\"] =\\\n        np.round( (submission.pressure - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n    submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n    submission.to_csv(f'submission_median_round{postfix}.csv', index=False)\n    \n    # DISPLAY SUBMISSION.CSV\n    print(f'submission{postfix}.csv head')\n    display( submission.head() )","322f946e":"# Load Libraries and Data ","50b412be":"# Write Submission CSV\nIf we set `INFER_TEST` boolean to `True` in first code block, then we will write `submission.csv` below. Note that `submission.csv` will be either all folds ensemble or just the first fold depending on the variable `ONE_FOLD_ONLY`.","919a6900":"# Engineer Features","d44eabd8":"# LSTM Feature Importance (aka permutation importance)\nWhen using an XGB model (gradient boosted trees), we have the advantage of displaying XGB Feature Importance. With Neural Networks, we don't have that advantage. However, we can use a technique called [Permutation Feature Importance][1] to compute the Feature Importance for any model. Therefore we can compute LSTM Feature Importance using permuation importance. Permutation importance has the advantage that we only need to train 1 model versus training multiple models for each feature we wish to evaluate!\n\nOther types of feature importances that can be applied to any model are [SHAP Feature Importance][3] and [LOFO Feature Importance][4].\n\nWe will compute LSTM feature importance for Zhangxin's notebook [here][2]. (Thus our notebook here has been forked from his notebook and we will use his saved model weights).\n\n[1]: https:\/\/christophm.github.io\/interpretable-ml-book\/feature-importance.html#feature-importance\n[2]: https:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm\n[3]: https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html\n[4]: https:\/\/www.kaggle.com\/aerdem4\/google-ventilator-lofo-feature-importance","aa4f2f0a":"# Compute LSTM Feature Importance\nAfter we train (or load) each fold model, we will compute LSTM feature importance for all of our features. We do this with a for-loop of size `N` where `N` is the number of features we have. For each feature we wish to evaluate, we infer our OOF with that feature column randomly shuffled. If this feature column is important to our LSTM model, then the OOF MAE will become worse for that for-loop step. After our for-loop, we display bars equal to the size of how much MAE worsened without each feature, which is the importance of each feature.\n\nNote that computing LSTM feature importance after each fold will add about 1 minute for every 5 features."}}