{"cell_type":{"6ab4a415":"code","dec0de00":"code","f03f78fc":"code","ef083c4a":"code","966c0364":"code","d810cc13":"code","bb4547d0":"code","0fa7f374":"code","708b176b":"code","e9d0389c":"code","062e0cf4":"code","ee286880":"code","b48b7f1e":"code","70a186d6":"code","ab8dab74":"code","dace6e28":"code","d88d631a":"code","446bbab6":"code","c2eee0b0":"code","29c45f16":"code","c0a7d775":"code","d13fe8d8":"code","bcc70a56":"code","ebbd8cb4":"code","93583056":"code","4e9d6c2a":"code","b5e38140":"code","eee68c93":"code","9980baee":"code","b41d3523":"code","3fc2c3e0":"code","05f93660":"code","360e72ff":"code","f4385607":"code","1868a013":"code","37fc262e":"code","7eb7babf":"code","a28f0c13":"code","087d1e63":"code","f805fe60":"code","3c90a525":"code","6f069c9a":"code","f349fde7":"code","770610e5":"code","792940fb":"code","1eb31130":"code","99e58d8b":"code","f3b598dc":"code","0a076c14":"code","1bdccc81":"code","da461e27":"code","b91bde7d":"code","9a783434":"code","10ff1ea4":"markdown","bf491c3d":"markdown","fb7e10fe":"markdown","0b610bb6":"markdown","d8d1bb4a":"markdown","d70929ee":"markdown","64651335":"markdown","2738a42c":"markdown","7c89c058":"markdown","734cf66c":"markdown","87107c53":"markdown","ce37f98f":"markdown","943575bf":"markdown","836fc8dc":"markdown","31ac7de1":"markdown","e19a7869":"markdown","794cb5c2":"markdown","5874a57e":"markdown","9c8b3e35":"markdown","7d34530f":"markdown","62c316af":"markdown","d5c680ac":"markdown","072154b5":"markdown","57260962":"markdown","97fe407a":"markdown","42011512":"markdown","4311beba":"markdown","03b2d554":"markdown","62615087":"markdown","9c2991f9":"markdown","d9c9e752":"markdown","fe6e8228":"markdown","c9f7ac5e":"markdown","63880839":"markdown","f00559f9":"markdown","4b350b59":"markdown","0b60c0da":"markdown","34ffbd96":"markdown","5d355955":"markdown","4ea84fcc":"markdown","d1cfacfc":"markdown","d5a78b07":"markdown","68d151db":"markdown","f3540277":"markdown","9f7b7837":"markdown","d3b75129":"markdown","d2d28ad2":"markdown","d58c85ba":"markdown","e9902a93":"markdown"},"source":{"6ab4a415":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport json\nimport seaborn as sns","dec0de00":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nimport string","f03f78fc":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import  TfidfTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier","ef083c4a":"from wordcloud import WordCloud, STOPWORDS","966c0364":"users = []\nwith open('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json') as fl:\n    for i, line in enumerate(fl):\n        users.append(json.loads(line))\n        if i+1 >= 100000:\n            break\nrev = pd.DataFrame(users)\nrev.head()","d810cc13":"rev.shape\nprint(rev.info())","bb4547d0":"reviews = rev.drop(['date','business_id','review_id','user_id'], axis =1)","0fa7f374":"reviews['text length'] = rev['text'].apply(len)\nreviews.head()","708b176b":"reviews.isnull().any().any()","e9d0389c":"cplot = sns.countplot(x='stars', data=reviews, palette=\"Set3\")\ncplot.set_title('stars')\ncplot.set_ylabel('count')","062e0cf4":"fig = plt.figure(figsize=(15,10))\nsns.distplot(reviews['text length'], kde=True, bins=50, color='#e28743')\nplt.title('Text Length Distribution')","ee286880":"text = reviews[['stars','text']]\ntext.head()","b48b7f1e":"cachedStopWords = stopwords.words(\"english\")\n\ndef remove_punc_stopword(text):\n\n    remove_punc = [word for word in text.lower() if word not in string.punctuation]\n    remove_punc = ''.join(remove_punc)\n    return [word for word in remove_punc.split() if word not in cachedStopWords]","70a186d6":"cleaned_text = text.copy()\ncleaned_text['text'] = text['text'].apply(remove_punc_stopword)\ncleaned_text.head()","ab8dab74":"def plot_Freq(data):\n    FreqDist(np.concatenate(data.text.reset_index(drop=True))).plot(20, cumulative=False)","dace6e28":"plot_Freq(cleaned_text[cleaned_text['stars']==1])","d88d631a":"def reviewCloud(star):\n    startext = ' '.join(text[text['stars']==star]['text'])\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black',\n        stopwords = STOPWORDS).generate(str(startext))\n\n    fig = plt.figure(\n        figsize = (10, 7),\n        facecolor = 'k',\n        edgecolor = 'c')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)","446bbab6":"reviewCloud(1)","c2eee0b0":"rev.loc[86353,:]","29c45f16":"restaurant = rev[rev.business_id==rev.loc[14544,:].business_id]\nrestaurant.head()","c0a7d775":"selected_words = ['chinese', 'japanese', 'asian' , 'fastfood', 'seafood', 'steakhouse', 'mexican', 'thai', \n                  'turkish', 'diet','healthy']","d13fe8d8":"vectorizer = CountVectorizer(vocabulary=selected_words, lowercase=False)\n\nselected_word_count = vectorizer.fit_transform(restaurant['text'].values.astype('U'))\nvectorizer.get_feature_names()","bcc70a56":"word_count_array = selected_word_count.toarray()\nword_count_array.shape","ebbd8cb4":"rest = pd.DataFrame(index=vectorizer.get_feature_names(), \\\n                    data=word_count_array.sum(axis=0)).rename(columns={0: 'Count'})\n\nrest.plot(kind='bar', stacked=False, figsize=[7,7], colormap='summer')","93583056":"selected_words2 = ['awesome', 'great', 'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', \n                  'awful', 'wow', 'hate' , 'delicious' , 'perfect', 'best', 'good' , 'happy']","4e9d6c2a":"vectorizer = CountVectorizer(vocabulary=selected_words2, lowercase=False)\n\nselected_word_count = vectorizer.fit_transform(restaurant['text'].values.astype('U'))\nvectorizer.get_feature_names()","b5e38140":"word_count_array = selected_word_count.toarray()\nword_count_array.shape","eee68c93":"rest = pd.DataFrame(index=vectorizer.get_feature_names(), \\\n                    data=word_count_array.sum(axis=0)).rename(columns={0: 'Count'})","9980baee":"rest.plot(kind='bar', stacked=False, figsize=[7,7], colormap='spring')","b41d3523":"def plot_Freq(data):\n    FreqDist(np.concatenate(data.text.reset_index(drop=True))).plot(20, cumulative=False)\n    \nplot_Freq(cleaned_text[cleaned_text['stars']==5])","3fc2c3e0":"def reviewCloud(star):\n    startext = ' '.join(text[text['stars']==star]['text'])\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black',\n        stopwords = STOPWORDS).generate(str(startext))\n\n    fig = plt.figure(\n        figsize = (10, 7),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    \nreviewCloud(5)","05f93660":"select_stars = text[(text['stars']==1) | (text['stars']==5)]","360e72ff":"x = select_stars['text']\ny = select_stars['stars']","f4385607":"count_vec = CountVectorizer().fit(x)\nx = count_vec.transform(x)","1868a013":"print(\"Shape of the sparse matrix: \", x.shape)","37fc262e":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)","7eb7babf":"mnb = MultinomialNB()\n\nmnb.fit(x_train,y_train)\nmnb_pred = mnb.predict(x_test)\n\nprint(\"Confusion Matrix for Multinomial Naive Bayes:\")\nprint(confusion_matrix(y_test,mnb_pred))\nprint(\"Score:\",round(accuracy_score(y_test,mnb_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,mnb_pred))","a28f0c13":"ranfor= RandomForestClassifier(n_estimators = 10, criterion = \"entropy\").fit(x_train,y_train)\n\nranfor_pred=ranfor.predict(x_test)\n\n\nprint(\"Confusion Matrix for Random Forest:\")\nprint(confusion_matrix(y_test,ranfor_pred))\nprint(\"Score:\",round(accuracy_score(y_test,ranfor_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,ranfor_pred))","087d1e63":"from sklearn.neighbors import KNeighborsClassifier\nk = 4\nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\n\nknn_pred = neigh.predict(x_test)\n\nprint(\"Confusion Matrix for KNN:\")\nprint(confusion_matrix(y_test,knn_pred))\nprint(\"Score:\",round(accuracy_score(y_test,knn_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,knn_pred))","f805fe60":"dectree  = DecisionTreeClassifier(criterion = \"entropy\")\ndectree.fit(x_train,y_train)\ndectree_pred = dectree.predict(x_test)\n\nprint(\"Confusion Matrix for Decision Tree:\")\nprint(confusion_matrix(y_test,dectree_pred))\nprint(\"Score:\",round(accuracy_score(y_test,dectree_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,dectree_pred))","3c90a525":"LR = LogisticRegression(C=0.01, solver='sag').fit(x_train,y_train)\nLR_pred = LR.predict(x_test)\n\nprint(\"Confusion Matrix for Logistic Regression:\")\nprint(confusion_matrix(y_test,LR_pred))\nprint(\"Score:\",round(accuracy_score(y_test,LR_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,LR_pred))","6f069c9a":"algos_countvec=[\"KNN\",\"Decision Tree\", \"Logistic Regression\", \"Random Forest\", \"Multinomial Naive Bayes\"]\npredictions_countvec=[round(accuracy_score(y_test,knn_pred)*100,2),round(accuracy_score(y_test,dectree_pred)*100,2), round(accuracy_score(y_test,LR_pred)*100,2),round(accuracy_score(y_test,ranfor_pred)*100,2),round(accuracy_score(y_test,mnb_pred)*100,2)]","f349fde7":"plt.figure(figsize = (15,10))\nsns.barplot(y = predictions_countvec, x = algos_countvec, palette = 'Set3')","770610e5":"tfidf_transformer = TfidfTransformer()\nx_tfidf = tfidf_transformer.fit_transform(x)\n\nx_tfidf_train, x_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(x_tfidf,y, test_size=0.2, random_state=42)","792940fb":"mnb_tfidf = MultinomialNB()\n\nmnb_tfidf.fit(x_tfidf_train,y_tfidf_train)\npred_mnb_tfidf = mnb_tfidf.predict(x_tfidf_test)\n\nprint(\"Confusion Matrix for Multinomial Naive Bayes:\")\nprint(confusion_matrix(y_tfidf_test,pred_mnb_tfidf))\nprint(\"Score:\",round(accuracy_score(y_tfidf_test,pred_mnb_tfidf)*100,2))\nprint(\"Classification Report:\",classification_report(y_tfidf_test,pred_mnb_tfidf))","1eb31130":"ranfor_tfidf= RandomForestClassifier(n_estimators = 10, criterion = \"entropy\").fit(x_tfidf_train,y_tfidf_train)\n\nranfor_tfidf_pred=ranfor_tfidf.predict(x_tfidf_test)\n\n\nprint(\"Confusion Matrix for Random Forest:\")\nprint(confusion_matrix(y_tfidf_test,ranfor_tfidf_pred))\nprint(\"Score:\",round(accuracy_score(y_tfidf_test,ranfor_tfidf_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_tfidf_test,ranfor_tfidf_pred))","99e58d8b":"from sklearn.neighbors import KNeighborsClassifier\nk = 4\nneigh_tfidf = KNeighborsClassifier(n_neighbors = k).fit(x_tfidf_train,y_tfidf_train)\n\nknn_tfidf_pred = neigh_tfidf.predict(x_tfidf_test)\n\nprint(\"Confusion Matrix for KNN:\")\nprint(confusion_matrix(y_tfidf_test,knn_tfidf_pred))\nprint(\"Score:\",round(accuracy_score(y_tfidf_test,knn_tfidf_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_tfidf_test,knn_tfidf_pred))","f3b598dc":"dectree_tfidf  = DecisionTreeClassifier(criterion = \"entropy\")\ndectree_tfidf.fit(x_tfidf_train,y_tfidf_train)\ndectree_tfidf_pred = dectree_tfidf.predict(x_tfidf_test)\n\nprint(\"Confusion Matrix for Decision Tree:\")\nprint(confusion_matrix(y_tfidf_test,dectree_tfidf_pred))\nprint(\"Score:\",round(accuracy_score(y_tfidf_test,dectree_tfidf_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_tfidf_test,dectree_tfidf_pred))","0a076c14":"LR_tfidf = LogisticRegression(C=0.01, solver='newton-cg').fit(x_tfidf_train,y_tfidf_train)\nLR_tfidf_pred = LR_tfidf.predict(x_tfidf_test)\n\nprint(\"Confusion Matrix for Logistic Regression:\")\nprint(confusion_matrix(y_tfidf_test,LR_tfidf_pred))\nprint(\"Score:\",round(accuracy_score(y_tfidf_test,LR_tfidf_pred)*100,2))\nprint(\"Classification Report:\",classification_report(y_tfidf_test,LR_tfidf_pred))","1bdccc81":"algos_tfidf=[\"KNN\",\"Decision Tree\", \"Logistic Regression\", \"Random Forest\", \"Multinomial Naive Bayes\"]\npredictions_tfidf=[round(accuracy_score(y_test,knn_pred)*100,2),round(accuracy_score(y_tfidf_test,dectree_tfidf_pred)*100,2), round(accuracy_score(y_tfidf_test,LR_tfidf_pred)*100,2),round(accuracy_score(y_tfidf_test,ranfor_tfidf_pred)*100,2),round(accuracy_score(y_tfidf_test,pred_mnb_tfidf)*100,2)]","da461e27":"plt.figure(figsize = (15,10))\nsns.barplot(y = predictions_tfidf, x = algos_tfidf, palette = 'Set3')","b91bde7d":"combining=list(set(algos_countvec).intersection(algos_tfidf))\ncombining","9a783434":"algos=[\"KNN\",\"Decision Tree\", \"Logistic Regression\", \"Random Forest\", \"Multinomial Naive Bayes\"]\npredictions_tfidf=[round(accuracy_score(y_test,knn_pred)*100,2),round(accuracy_score(y_tfidf_test,dectree_tfidf_pred)*100,2), round(accuracy_score(y_tfidf_test,LR_tfidf_pred)*100,2),round(accuracy_score(y_tfidf_test,ranfor_tfidf_pred)*100,2),round(accuracy_score(y_tfidf_test,pred_mnb_tfidf)*100,2)]\npredictions_countvec=[round(accuracy_score(y_test,knn_pred)*100,2),round(accuracy_score(y_test,dectree_pred)*100,2), round(accuracy_score(y_test,LR_pred)*100,2),round(accuracy_score(y_test,ranfor_pred)*100,2),round(accuracy_score(y_test,mnb_pred)*100,2)]","10ff1ea4":"Bu grafikten, bu restoran\u0131n tercih edilebilir bir restoran oldu\u011funu anlayabiliriz. En \u00e7ok kullan\u0131lan kelimeler 'good' ve 'great'\nK\u00f6t\u00fc yorumlar da var ama bunlar\u0131n az\u0131nl\u0131kta oldu\u011funu g\u00f6r\u00fcyoruz.","bf491c3d":"CountVectorizer() transformer\u0131, corpus ba\u011flam\u0131n\u0131 hesaba katmaz. Belgedeki tokenlar\u0131n frekans\u0131n\u0131, di\u011fer belgedeki frekanslar\u0131na g\u00f6re de\u011ferlendirmek i\u00e7in ba\u015fka yakla\u015f\u0131m gerekir. (\u00d6rnek: TF-IDF -Term Frequency-Inverse Document Frequency)","fb7e10fe":"**0'dan 99999'a 100000 datam\u0131z oldu\u011funu, 9 \u00e7e\u015fit kolona sahip oldu\u011funu ve bunlar\u0131n be\u015finin obje, \u00fc\u00e7\u00fcn\u00fcn say\u0131 \u015feklinde (iki integer, bir float) tutuldu\u011funu g\u00f6zlemleriz. Ayr\u0131ca ele\u015ftirilerin y\u0131ld\u0131z say\u0131s\u0131n\u0131n (1-5) stars ile tutuldu\u011funu ve ele\u015ftiriler i\u00e7in useful, funny, cool \u00f6zellikler tan\u0131mland\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz.**","0b610bb6":"# **Olumlu \u0130ncelemelerde En S\u0131k Kar\u015f\u0131la\u015f\u0131lan Kelimelerin Tespiti**","d8d1bb4a":"**A\u015fa\u011f\u0131daki h\u00fccrelerde baz\u0131 metin temizleme i\u015flemleri yap\u0131yoruz. Bunu stopwords ve noktalama i\u015faretlerini kald\u0131rabilen, k\u00fc\u00e7\u00fck harfe d\u00f6n\u00fc\u015ft\u00fcrebilen ve yaln\u0131zca \u0130ngilizce incelemeleri tutabilen bir i\u015flev tan\u0131mlayarak yapaca\u011f\u0131z. Ard\u0131ndan fonksiyonumuzu uygulamak i\u00e7in 'cleaned_text' verilerimizin bir kopyas\u0131n\u0131 \u00e7\u0131karaca\u011f\u0131z.**","d70929ee":"# **K\u00dcT\u00dcPHANELER**","64651335":"Buradan restoran\u0131n asya ve tai usul\u00fc, sa\u011fl\u0131kl\u0131 tarzda yemekler \u00e7\u0131kard\u0131\u011f\u0131n\u0131 anlar\u0131z.","2738a42c":"**Ard\u0131ndan, veri k\u00fcmemizdeki y\u0131ld\u0131zlar\u0131n da\u011f\u0131l\u0131m\u0131n\u0131 g\u00f6r\u00fcnt\u00fclemek i\u00e7in bir count plot (say\u0131m grafi\u011fi) olu\u015fturaca\u011f\u0131z.**","7c89c058":"# # **K-Nearest Neighbour**","734cf66c":"Vekt\u00f6rize edip diziye d\u00f6n\u00fc\u015ft\u00fcr\u00fcr\u00fcz","87107c53":"**Veri setinin ilk 100000 verisini a\u015fa\u011f\u0131daki gibi y\u00fckleriz**","ce37f98f":"**Ard\u0131ndan analizimiz i\u00e7in yaln\u0131zca 'y\u0131ld\u0131z' ve 'metin' s\u00fctunlar\u0131n\u0131 se\u00e7iyoruz. Bu iki s\u00fctun, analizimizi ger\u00e7ekle\u015ftirebilmemiz i\u00e7in gerekli t\u00fcm bilgileri sa\u011flar.**","943575bf":"# # **Logistic Regression**","836fc8dc":"# # **Modelleme \u0130\u00e7in Kullan\u0131lan K\u00fct\u00fcphaneler**","31ac7de1":"**\u015eimdi ele\u015ftirilerden bilgi \u00e7\u0131karabilmek i\u00e7in kelime bulutu grafi\u011fi ve s\u0131kl\u0131k say\u0131lar\u0131n\u0131 olu\u015fturaca\u011f\u0131z. 5 y\u0131ld\u0131zl\u0131 yorumlar olumlu iken 1 y\u0131ld\u0131zl\u0131 yorumlara olumsuz ele\u015ftiri diyebiliriz. 3 y\u0131ld\u0131zl\u0131 yorumlar ise i\u00e7inde hem olumlu hem de olumsuz ele\u015ftiriyi benzer oranda ta\u015f\u0131yan yorumlard\u0131r. Bu nedenle \u00fc\u00e7 y\u0131ld\u0131zl\u0131 ele\u015ftirileri tek bir duyguyla ba\u011fda\u015ft\u0131r\u0131p bu \u00e7al\u0131\u015fmaya dahil etmek do\u011fru olmayacakt\u0131r.**","e19a7869":"# **DATA TEM\u0130ZL\u0130\u011e\u0130**","794cb5c2":"# **Term Frequency\u2013Inverse Document Frequency transformer(TF-IDF) Kullan\u0131m\u0131**","5874a57e":"# **CountVectorizer() Kullan\u0131m\u0131**","9c8b3e35":"# # **Random Forest**","7d34530f":"# # **Ba\u015fl\u0131ca K\u00fct\u00fcphaneler**","62c316af":"**Datasetin boyutuna, \u00f6zelliklerine ve \u015fekline shape ve info() fonksiyonlar\u0131n\u0131 kullanarak bakar\u0131z.**","d5c680ac":"# # **Multinomial Naive Bayes Model**","072154b5":"Bussiness ID kullanarak bir restoran se\u00e7ip t\u00fcm ele\u015ftirilerini bir araya toplayal\u0131m","57260962":"CountVectorizer() transformer\u0131, corpus ba\u011flam\u0131n\u0131 hesaba katmaz. Belgedeki tokenlar\u0131n frekans\u0131n\u0131, di\u011fer belgedeki frekanslar\u0131na g\u00f6re de\u011ferlendirmek i\u00e7in ba\u015fka yakla\u015f\u0131m gerekir. \n(\u00d6rnek: TF-IDF -Term Frequency-Inverse Document Frequency)","97fe407a":"**Text uzunluklar\u0131n\u0131n da\u011f\u0131l\u0131m\u0131n\u0131 g\u00f6zlemlemek i\u00e7in bir distplot grafi\u011fi olu\u015fturuyoruz.**","42011512":"**Ard\u0131ndan verimizin bo\u015f de\u011fer i\u00e7erip i\u00e7ermedi\u011fini kontrol edece\u011fiz.**","4311beba":"Olumsuz incelemelerde en s\u0131k kullan\u0131lan kelimelerle bir WordCloud() grafi\u011fi olu\u015fturan fonksiyonu tan\u0131mlad\u0131k","03b2d554":"Ele\u015ftirilerde en s\u0131k kullan\u0131lan 20 kelimeyi tespit eden bir fonk yazd\u0131k.","62615087":"# # **Decision Tree**","9c2991f9":"**A\u015fa\u011f\u0131daki h\u00fccrelerde, analizimizde kullan\u0131lmayacak olan baz\u0131 s\u00fctunlar\u0131 kald\u0131raca\u011f\u0131z; metin s\u00fctununun uzunlu\u011funu hesaplay\u0131p ekleyece\u011fiz ve nihai verilerimizi g\u00f6r\u00fcnt\u00fcleyece\u011fiz.**","d9c9e752":"sklearn.feature_extraction modelinden i\u00e7e aktar\u0131l\u0131r.","fe6e8228":"Ard\u0131ndan get_feature_names() i\u015flevini kullanarak \u00f6zelliklerimizi ay\u0131kl\u0131yoruz, bunlar\u0131 pandas veri \u00e7er\u00e7evesine d\u00f6n\u00fc\u015ft\u00fcr\u00fcyoruz ve sum() i\u015flevini kullanarak bunlar\u0131 bir araya getiriyoruz","c9f7ac5e":"# **Olumsuz Yorumlarda En S\u0131k Kar\u015f\u0131la\u015f\u0131lan Kelimelerin Tespiti**","63880839":"# # **Kelimeleri Ayr\u0131\u015ft\u0131rma ve Se\u00e7me\/Eleme \u0130\u015fleminde Kullan\u0131lan K\u00fct\u00fcphaneler**","f00559f9":"# # **K-Nearest Neighbour**","4b350b59":"# # **Logistic Regression**","0b60c0da":"Restoran\u0131n ne t\u00fcr yemek satt\u0131\u011f\u0131n\u0131 anlamak i\u00e7in ","34ffbd96":"# # **Decision Tree**","5d355955":"Negatif ele\u015ftirilerde en s\u0131k kullan\u0131lan 20 kelimeyi tespit ettirdik","4ea84fcc":"# # **Random Forest**","d1cfacfc":"**Verimizde bo\u015f de\u011fer yoktur.**","d5a78b07":"pre_processing, tokenization ve normalization y\u00f6ntemlerine sahiptir.","68d151db":"* sag, saga, newton-cg, liblinear, lbfgs->78,55","f3540277":"# **MODELLEME**","9f7b7837":"* sag->95,95\n* saga->95,93\n* liblinear->95,52\n* lbfgs->95,48\n* newton-cg->95,48\n","d3b75129":"# # **Multinomial Naive Bayes**","d2d28ad2":"**Verimizin sola \u00e7arp\u0131k oldu\u011funu ve 5 y\u0131ld\u0131z say\u0131s\u0131n\u0131n daha fazla oldu\u011funu g\u00f6zlemliyoruz.**","d58c85ba":"Belirli bir metni, t\u00fcm metinde ge\u00e7en her kelimenin s\u0131kl\u0131k say\u0131m\u0131 temelinde bir vekt\u00f6re d\u00f6n\u00fc\u015ft\u00fcrmek i\u00e7in kullan\u0131l\u0131r. Yani her h\u00fccrenin de\u011feri, s\u00f6z konusu metindeki kelimenin say\u0131s\u0131ndan ba\u015fka bir \u015fey de\u011fildir.","e9902a93":"Ard\u0131ndan, incelemelerimizi olumlu veya olumsuz olarak s\u0131n\u0131fland\u0131rmak i\u00e7in bir model olu\u015fturaca\u011f\u0131z. Bunun i\u00e7in modelimizi e\u011fitmek i\u00e7in sadece '1' ve '5' y\u0131ld\u0131z de\u011ferlendirmeleri se\u00e7ece\u011fiz. Veri setini x ve y de\u011fi\u015fkenlerine ay\u0131raca\u011f\u0131z."}}