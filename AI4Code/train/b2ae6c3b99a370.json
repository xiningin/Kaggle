{"cell_type":{"d170c28c":"code","0519f77d":"code","41747fa2":"code","fd2bec35":"code","430516af":"code","f93bf8cd":"code","bb542282":"code","7f33e695":"code","07664472":"code","288af1d9":"code","41775ae7":"code","4b33fffc":"code","fcb90e1c":"code","8e8f23e0":"code","3bec6fdd":"code","b214feff":"code","be047772":"code","0073b676":"code","1128b622":"code","80601c54":"code","2c3f2d92":"code","6703d76c":"code","9a1230a8":"markdown","f9d47088":"markdown","d1d5199f":"markdown","c91480ec":"markdown","0d189e9a":"markdown","cae99192":"markdown","77a285b5":"markdown","8ad9d0f2":"markdown","779acae6":"markdown","032d400f":"markdown","9cd6e50b":"markdown","35aeb0fc":"markdown","faf4c001":"markdown","57b372c9":"markdown","65b249fb":"markdown","24779c67":"markdown","65d01ff3":"markdown","3af728c9":"markdown","e09b2e56":"markdown","c2407bde":"markdown","95c44b07":"markdown","4622a63e":"markdown","2176c6b4":"markdown","1c480505":"markdown","fd5a0b15":"markdown","b6ed01fb":"markdown","6a66d46f":"markdown","3ee40ba7":"markdown","d5875d2d":"markdown","ff060964":"markdown","b65c31cb":"markdown","8d7a2425":"markdown","87b0f27c":"markdown","fd67c4bb":"markdown","57b47537":"markdown","5e972c4a":"markdown","9f21ac22":"markdown","185eb555":"markdown","f47ea217":"markdown"},"source":{"d170c28c":"# Lorenzo Carlo Causin, 1234098\n# Andrea Bertola, 2055557\n# Eleonora Fattorini, 2037886\n# Paolo Pavone, 2045646\n# Simone Perra, 2037013\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tqdm import tqdm\nimport os\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport ipywidgets as widgets\nimport io\nfrom PIL import Image\nfrom IPython.display import display,clear_output\nfrom warnings import filterwarnings\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0519f77d":"colors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\ncolors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\ncolors_green = ['#01411C','#4B6F44','#4F7942','#74C365','#D0F0C0']\n\nsns.palplot(colors_dark)\nsns.palplot(colors_green)\nsns.palplot(colors_red)","41747fa2":"labels = ['glioma_tumor','no_tumor','meningioma_tumor','pituitary_tumor']","fd2bec35":"from numpy import expand_dims\n#from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom matplotlib import pyplot\n\n\nX_train = []\ny_train = []\nimage_size = 150\nfor i in labels:\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri','Training',i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        img = cv2.resize(img,(image_size, image_size))\n        \n       #selecting only no tumor images sorting by labels \n        if i=='no_tumor':\n            data = img_to_array(img)\n            #expand ranks of the images\n            samples = expand_dims(data, 0)\n            \n            # create image data augmentation generator\n            \n            datagen = ImageDataGenerator(rotation_range=90 , horizontal_flip=True)\n            \n            \n            # prepare iterator\n            \n            it = datagen.flow(samples, batch_size=1)\n            \n           #generate samples and plot\n            for k in range(2):\n                \n                # define subplot\n                #pyplot.subplot(330 + 1 + i)\n                \n                \n                # generate batch of images\n                batch = it.next()\n               \n                \n                # convert to unsigned integers for viewing\n                image = batch[0].astype('uint8')\n                \n                #pyplot.imshow(image)\n                \n            # show the figure\n            #pyplot.show()\n                \n                X_train.append(image)\n                y_train.append(i)\n        else:\n            X_train.append(img)\n            y_train.append(i)\n        \nfor i in labels:\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri','Testing',i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        img = cv2.resize(img,(image_size,image_size))\n        X_train.append(img)\n        y_train.append(i)\n        \nX_train = np.array(X_train)\n#print X_train lenght to check if the array length increase by 395\nprint(len(X_train))\ny_train = np.array(y_train)","430516af":"k=0\nfig, ax = plt.subplots(1,4,figsize=(20,20))\nfig.text(s='Sample Image From Each Label',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.62,x=0.4,alpha=0.8)\nfor i in labels:\n    j=0\n    while True :\n        if y_train[j]==i:\n            ax[k].imshow(X_train[j])\n            ax[k].set_title(y_train[j])\n            ax[k].axis('off')\n            k+=1\n            break\n        j+=1","f93bf8cd":"X_train, y_train = shuffle(X_train,y_train, random_state=101)","bb542282":"X_train.shape","7f33e695":"X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size=0.1,random_state=101)","07664472":"y_train_new = []\nfor i in y_train:\n    y_train_new.append(labels.index(i))\ny_train = y_train_new\ny_train = tf.keras.utils.to_categorical(y_train)\n\n\ny_test_new = []\nfor i in y_test:\n    y_test_new.append(labels.index(i))\ny_test = y_test_new\ny_test = tf.keras.utils.to_categorical(y_test)","288af1d9":"effnet = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))","41775ae7":"model = effnet.output\n#test a data augmentation with the keras leyer \n#model = tf.keras.layers.RandomTranslation(\n    #height_factor = 0.3, width_factor=0.3, fill_mode='nearest',\n    #interpolation='nearest', seed=10, fill_value=0.0)(model)\n    \n#this represents a 2D convolutional filtering attempt, which led to severe worsened performances     \n#input_shape = (3659, 150, 150, 3)  \n#model = tf.keras.layers.Conv2D( 2, 3, activation='relu', input_shape=input_shape)(model)\n    \n    \nmodel = tf.keras.layers.GlobalAveragePooling2D()(model)\nmodel = tf.keras.layers.Dropout(rate=0.5)(model)\nmodel = tf.keras.layers.Dense(4,activation='softmax')(model)\nmodel = tf.keras.models.Model(inputs=effnet.input, outputs = model)\n","4b33fffc":"model.summary()","fcb90e1c":"model.compile(loss='categorical_crossentropy',optimizer = 'Adam', metrics= ['accuracy'])","8e8f23e0":"tensorboard = TensorBoard(log_dir = 'logs')\ncheckpoint = ModelCheckpoint(\"effnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n                              mode='auto',verbose=1)","3bec6fdd":"history = model.fit(X_train,y_train,validation_split=0.1, epochs =12, verbose=1, batch_size=32,\n                   callbacks=[tensorboard,checkpoint,reduce_lr])","b214feff":"filterwarnings('ignore')\n\nepochs = [i for i in range(12)]\nfig, ax = plt.subplots(1,2,figsize=(14,7))\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nfig.text(s='Epochs vs. Training and Validation Accuracy\/Loss',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n\nsns.despine()\nax[0].plot(epochs, train_acc, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label = 'Training Accuracy')\nax[0].plot(epochs, val_acc, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Accuracy')\nax[0].legend(frameon=False)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\n\nsns.despine()\nax[1].plot(epochs, train_loss, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Loss')\nax[1].plot(epochs, val_loss, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Loss')\nax[1].legend(frameon=False)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Training & Validation Loss')\n\nfig.show()","be047772":"pred = model.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_test,axis=1)","0073b676":"print(classification_report(y_test_new,pred))","1128b622":"fig,ax=plt.subplots(1,1,figsize=(14,7))\nsns.heatmap(confusion_matrix(y_test_new,pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True,\n           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\nfig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.92,x=0.28,alpha=0.8)\n\nplt.show()","80601c54":"def img_pred(upload):\n    for name, file_info in uploader.value.items():\n        img = Image.open(io.BytesIO(file_info['content']))\n    opencvImage = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n    img = cv2.resize(opencvImage,(150,150))\n    img = img.reshape(1,150,150,3)\n    p = model.predict(img)\n    p = np.argmax(p,axis=1)[0]\n\n    if p==0:\n        p='Glioma Tumor'\n    elif p==1:\n        print('The model predicts that there is no tumor')\n    elif p==2:\n        p='Meningioma Tumor'\n    else:\n        p='Pituitary Tumor'\n\n    if p!=1:\n        print(f'The Model predicts that it is a {p}')","2c3f2d92":"uploader = widgets.FileUpload()\ndisplay(uploader)","6703d76c":"button = widgets.Button(description='Predict')\nout = widgets.Output()\ndef on_button_clicked(_):\n    with out:\n        clear_output()\n        try:\n            img_pred(uploader)\n            \n        except:\n            print('No Image Uploaded\/Invalid Image File')\nbutton.on_click(on_button_clicked)\nwidgets.VBox([button,out])","9a1230a8":"---","f9d47088":"# Evaluation","d1d5199f":"# Introduction","c91480ec":"# Prediction","0d189e9a":"This is where you can upload the image by clicking on the **Upload** button:","cae99192":"In this notebook, I've used **CNN** to perform Image Classification on the Brain Tumor dataset.<br>\nSince this dataset is small, if we train a neural network to it, it won't really give us a good result.<br>\nTherefore, I'm going to use the concept of **Transfer Learning** to train the model to get really accurate results.","77a285b5":"**Note**: The training takes alot of time! ~ 2 hours for me (Using CPU)<br>\nBarely took 5 minutes with the GPU.","8ad9d0f2":"---","779acae6":"# Bonus Content: Widgets","032d400f":"Performing **One Hot Encoding** on the labels after converting it into numerical values:","9cd6e50b":"---","35aeb0fc":"---","faf4c001":"# <center>Thank You!","57b372c9":"---","65b249fb":"We finally compile our model.","24779c67":"In this notebook, I performed Image Classification with the help of CNN using Transfer Learning which gave an accuracy of around 98%.<br>\nI also made widgets which can make predictions on an image from your local machine!","65d01ff3":"We start off by appending all the images from the  directories into a Python list and then converting them into numpy arrays after resizing it.","3af728c9":"---","e09b2e56":"### Note","c2407bde":"**GlobalAveragePooling2D** -> This layer acts similar to the Max Pooling layer in CNNs, the only difference being is that it uses the Average values instead of the Max value while *pooling*. This really helps in decreasing the computational load on the machine while training.\n<br><br>\n**Dropout** -> This layer omits some of the neurons at each step from the layer making the neurons more independent from the neibouring neurons. It helps in avoiding overfitting. Neurons to be ommitted are selected at random. The **rate** parameter is the liklihood of a neuron activation being set to 0, thus dropping out the neuron\n\n**Dense** -> This is the output layer which classifies the image into 1 of the 4 possible classes. It uses the **softmax** function which is a generalization of the sigmoid function.","95c44b07":"# Conclusion","4622a63e":"After uploading the image, you can click on the **Predict** button below to make predictions:","2176c6b4":"---","1c480505":"**Callbacks** -> Callbacks can help you fix bugs more quickly, and can help you build better models. They can help you visualize how your model\u2019s training is going, and can even help prevent overfitting by implementing early stopping or customizing the learning rate on each iteration.<br><br>\nBy definition, \"A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\"\n\nIn this notebook, I'll be using **TensorBoard, ModelCheckpoint and ReduceLROnPlateau** callback functions","fd5a0b15":"# Color","b6ed01fb":"---","6a66d46f":"If you find this notebook helpful and intuitive, feel free to upvote it! Do give me your suggestions or opinions so that I can improve my work! I will highly appreciate it! Thank you! :)","3ee40ba7":"# Importing Libraries","d5875d2d":"I've made these Widgets in which we can upload images from our local machine and predict whether the MRI scan has a Brain Tumour or not and to classify which Tumor it is.<br>\nUnfortunately, it doesn't work on Kaggle but you can play around with this by downloading the notebook on your machine :)","ff060964":"# Training The Model","b65c31cb":"In this, <br>\n0 - Glioma Tumor<br>\n1 - No Tumor<br>\n2 - Meningioma Tumor<br>\n3 - Pituitary Tumor<br>","8d7a2425":"# Transfer Learning","87b0f27c":"---","fd67c4bb":"Dividing the dataset into **Training** and **Testing** sets.","57b47537":"Deep convolutional neural network models may take days or even weeks to train on very large datasets.\n\nA way to short-cut this process is to re-use the model weights from pre-trained models that were developed for standard computer vision benchmark datasets, such as the ImageNet image recognition tasks. Top performing models can be downloaded and used directly, or integrated into a new model for your own computer vision problems.\n\nIn this notebook, I'll be using the **EfficientNetB0** model which will use the weights from the **ImageNet** dataset.\n\nThe include_top parameter is set to *False* so that the network doesn't include the top layer\/ output layer from the pre-built model which allows us to add our own output layer depending upon our use case!","5e972c4a":"# Data Preperation","9f21ac22":"I've used the *argmax function* as each row from the prediction array contains four values for the respective labels. The **maximum** value which is in each row depicts the predicted output out of the 4 possible outcomes.<br>\nSo with *argmax*, I'm able to find out the index associated with the predicted outcome.","185eb555":"<img src=\"https:\/\/miro.medium.com\/max\/2800\/1*TUF_YfybJPQ8WA0siWeJTg@2x.jpeg\" alt=\"Brain\" style=\"width: 800px;\">","f47ea217":"---"}}