{"cell_type":{"a40141b7":"code","423fd1f9":"code","12c7ab77":"code","7c8ee290":"code","fdf71866":"code","a30ad1b0":"code","022467b1":"code","ce03ee06":"code","add7aa4f":"code","d6feb887":"code","0c01bee8":"code","c49cd066":"code","80ee3874":"code","3b333438":"code","1c94ef1e":"code","0999b902":"code","415c2ab3":"code","5cc10df2":"code","86848f91":"code","da977360":"code","a901be01":"code","14db6c54":"code","91b2e061":"code","bc36e2c0":"code","bbdd7a44":"code","0b8c314e":"code","5db31af5":"code","e7d78cfc":"code","d7836033":"code","5aa9aeeb":"code","36ff3934":"code","d294e737":"code","6100661c":"code","dd4b8d13":"code","26eb9276":"code","fb91e8ba":"code","4f6810ae":"code","242fa28c":"code","3f1ff01d":"code","44f4bc2e":"code","ccf020f3":"code","a40473bb":"code","e3903227":"code","d1936aa7":"code","0d78a5b7":"code","b6268912":"code","dc229b00":"code","90702378":"code","c5d2ca81":"code","38e9cc9a":"code","0106ee88":"code","3852e2a4":"code","fc2fe3f3":"code","6cc8cc56":"code","04f63ebe":"code","902f9ba2":"code","ea71aa3c":"code","40bc1b46":"code","929aafe0":"code","0119caf4":"code","60135bb7":"code","0004775b":"code","57689cbd":"code","c0eb192f":"code","7c6a1513":"code","51ac666a":"code","f9273e34":"code","006284e5":"code","bf0e5d01":"code","64708039":"code","265ea593":"code","00463ad2":"code","340334ab":"code","3d3d563a":"code","d5d645e7":"code","5996f352":"code","d7f32c24":"code","91ecd65b":"code","9daf033c":"code","d694b815":"code","5f521a4a":"code","14eef23f":"code","49ffbe2c":"code","c07cf430":"code","116dc52d":"code","5d1e72c0":"code","a4d6d963":"code","29d4577a":"code","7297cc57":"code","7ceabb1e":"code","31925c6d":"code","026b54cc":"code","c6e310e8":"code","e686f930":"code","8b6884a3":"code","f8532713":"code","94ef14a4":"code","38416a07":"code","32b41ce1":"code","e5f71a50":"code","e2178b1f":"code","cff06f29":"code","7b052cf2":"code","d80b787d":"code","484316c6":"code","5614b1ed":"code","1ca2a345":"code","544388dd":"code","e807e09e":"code","224a41a1":"code","ff2268e9":"code","1c6831e9":"code","7ee26336":"code","2072ad3b":"code","da59bf59":"code","a50047a7":"code","4e94207d":"code","17d35625":"code","11770a39":"code","a3669d98":"code","1ed2dbb7":"code","90d821b8":"code","3843d138":"code","33ba18ce":"code","c9ec6c3b":"code","e118dc9c":"code","cf78bf08":"code","988d3f5e":"code","0bafa85c":"code","7f4125eb":"code","acccd7ab":"code","683450bc":"code","90a2eef2":"code","8b287474":"code","c9ef07e7":"code","ec75ca06":"code","546569dd":"code","20290b0b":"code","dae93915":"code","cf6e8933":"code","7ab353f8":"code","8ba0e075":"code","0c95ecce":"code","13b4cf52":"code","b4067b99":"code","32a4cb29":"code","ce25cf37":"code","84f94da0":"code","348e66e7":"code","12e2bee4":"code","14442f52":"code","af266e6f":"code","674b046d":"code","75e3982c":"code","761997a7":"code","21a5b774":"code","c2a3ee41":"code","1542610c":"code","b83e0bbe":"code","104f6f1e":"code","ae4c8f07":"code","886e1fed":"code","b1640aa0":"code","dadadc1e":"code","a013d968":"code","91779885":"code","9ca20aad":"code","8dac6f78":"code","1abb6442":"code","4e132a85":"code","8ce37516":"code","a68ae022":"code","4c2c2620":"code","465474d8":"code","1611ed49":"code","59809451":"code","39ba1700":"code","b204a9a2":"code","e78f572a":"code","aa9350d1":"code","069bab39":"code","cb6fd1c4":"markdown","ff0d35f0":"markdown","ee8e41ca":"markdown","6a74c326":"markdown","2788ee54":"markdown","7f8d6c95":"markdown","ab96f85e":"markdown","1035c366":"markdown","02c3499e":"markdown","d5b25abf":"markdown","958a2938":"markdown","85a0d82e":"markdown","04541c97":"markdown","050e17a5":"markdown","2bb799be":"markdown","83da9a3c":"markdown","bc66e86b":"markdown","a8aa36c4":"markdown","4c6d357e":"markdown","5f44f3f5":"markdown","d7b91c64":"markdown","d53d2186":"markdown","605e006e":"markdown","454bf633":"markdown","06bee1c1":"markdown","98c4a7e6":"markdown","7ea2c3b6":"markdown","4f0b0f72":"markdown","d2505162":"markdown","64442178":"markdown","21013cf2":"markdown","4677e806":"markdown","b062c976":"markdown","313dc235":"markdown","5798115d":"markdown","6ec5ea45":"markdown","8ec9a425":"markdown","00e88299":"markdown","c7dd0b61":"markdown","d26c6462":"markdown","3599422f":"markdown","91a1c046":"markdown","56eda9cd":"markdown","f0b5590e":"markdown","1301d57f":"markdown","5d8ca489":"markdown","9eb5359a":"markdown","87cc3351":"markdown","4071f302":"markdown","057205c2":"markdown","65d5aa38":"markdown","7bdc4842":"markdown","fdf5c4f8":"markdown","ca5101e6":"markdown","f88a9643":"markdown","074a39ae":"markdown","2434689e":"markdown","189560aa":"markdown","21b9a473":"markdown","c76804bb":"markdown","6d3814d7":"markdown","80fe51a2":"markdown","666ef399":"markdown","c49308ea":"markdown","6837d55c":"markdown","2f9cb0b1":"markdown","b4ff277d":"markdown","d60edcbc":"markdown","e8351c8c":"markdown","7e927d2a":"markdown","d82118b6":"markdown","ab829cef":"markdown","ddca4d47":"markdown","051e7d4a":"markdown","61b92ea2":"markdown","879518e0":"markdown","b0eb52d2":"markdown","792fac56":"markdown","efcd8a6d":"markdown","789b2f95":"markdown","21338713":"markdown","2249608e":"markdown","d82f1a00":"markdown","765d09fa":"markdown","6d40806f":"markdown","1dc04e73":"markdown","1e345c63":"markdown","2668866e":"markdown","2813c268":"markdown","5d10a24d":"markdown","3e538843":"markdown","f64d5dca":"markdown"},"source":{"a40141b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","423fd1f9":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","12c7ab77":"path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)","7c8ee290":"tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\nbs=128\n\nil = ImageList.from_files(path, tfms=tfms)\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\nll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)","fdf71866":"nfs = [32,64,128,256]","a30ad1b0":"cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback,\n        partial(BatchTransformXCallback, norm_imagenette)]","022467b1":"\nlearn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)","ce03ee06":"\nrun.fit(1, learn)","add7aa4f":"class Optimizer():\n    def __init__(self, params, steppers, **defaults): #param is a list of list \n        # might be a generator\n        self.param_groups = list(params) #this gonna give a list of all the parameter tensors. so all the weights ans all the biases #in fastai we also call a parameter group a lyaer group they are the same thing \n        #so remember that a parameter is pythorch. remember we made a linear layer we had a weight and bias tensor and they are both parameters. it is a paramter tensor\n        # ensure params is a list of lists since we then can do parameter groups, and change the learning rate or other things in different groups under training like we wont to change the learning rate on the last to layers \n        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups] #we will check if it is a list of list (self.param_groups[0], list)\n            #and if it is not a list of list we will turn ir into one (self.param_groups)- defined param_groups above\n        self.hypers = [{**defaults} for p in self.param_groups] #so each parameter group can have its own set of hyperparamters like learinng rate, omentum beta nad adam etc.\n        #these hyperparamters are gonna be stored as a dictornary. so there are gonna be one dictornary for each parameter group(param_groups). so for each param_groups (p) has\n        #a dictornary and what is in the dictornary is whatever you passed to the constructer (__init__). \n        self.steppers = listify(steppers) #steppers is a function and one look likes in the sgd_step function in hte cell below \n\n    def grad_params(self):\n        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n            for p in pg if p.grad is not None]\n\n    def zero_grad(self): #so we need a zzero grad, that are gonna go though some parameters and zero them out  \n        for p,hyper in self.grad_params():\n            p.grad.detach_() #and also remove any gradient computation history \n            p.grad.zero_()\n\n    def step(self): #and we are gonna have a step function that does some kind of a  step \n        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper) #so when step function is called it goes through our paramters and compose togetther our steppers \n            #which is just one thing (sgd_step) and call the parameter(p) \n            #grad_params is just for conviniens \n            #note **hyper is all our hyper paramters since mayde some stepper want to use them like learning rate ","d6feb887":"def sgd_step(p, lr, **kwargs): #so laerning rate(lr) comes from the **hyper above in the step function \n    p.data.add_(-lr, p.grad.data) #it does a sgd step which we hva seen before \n    #where p is paramter is it going through one parameter - learning rata and take the gradient to p \n    return p","0c01bee8":"opt_func = partial(Optimizer, steppers=[sgd_step]) #here we create a optimezer function with the optimazer class and the steppers set = to sgd_step","c49cd066":"\n#export\nclass Recorder(Callback):\n    def begin_fit(self): self.lrs,self.losses = [],[]\n\n    def after_batch(self):\n        if not self.in_train: return\n        self.lrs.append(self.opt.hypers[-1]['lr'])\n        self.losses.append(self.loss.detach().cpu())        \n\n    def plot_lr  (self): plt.plot(self.lrs)\n    def plot_loss(self): plt.plot(self.losses)\n        \n    def plot(self, skip_last=0):\n        losses = [o.item() for o in self.losses]\n        n = len(losses)-skip_last\n        plt.xscale('log')\n        plt.plot(self.lrs[:n], losses[:n])\n\nclass ParamScheduler(Callback):\n    _order=1\n    def __init__(self, pname, sched_funcs):\n        self.pname,self.sched_funcs = pname,listify(sched_funcs)\n\n    def begin_batch(self):  \n        if not self.in_train: return\n        fs = self.sched_funcs\n        if len(fs)==1: fs = fs*len(self.opt.param_groups)\n        pos = self.n_epochs\/self.epochs\n        for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)\n            \nclass LR_Find(Callback):\n    _order=1\n    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n        self.best_loss = 1e9\n        \n    def begin_batch(self): \n        if not self.in_train: return\n        pos = self.n_iter\/self.max_iter\n        lr = self.min_lr * (self.max_lr\/self.min_lr) ** pos\n        for pg in self.opt.hypers: pg['lr'] = lr\n            \n    def after_step(self):\n        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n            raise CancelTrainException()\n        if self.loss < self.best_loss: self.best_loss = self.loss","80ee3874":"\nsched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)])","3b333438":"\ncbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback, Recorder,\n        partial(ParamScheduler, 'lr', sched)]","1c94ef1e":"learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=opt_func)","0999b902":"\n%time run.fit(1, learn)","415c2ab3":"run.recorder.plot_loss()","5cc10df2":"run.recorder.plot_lr()","86848f91":"#add a stepper weight decay \ndef weight_decay(p, lr, wd, **kwargs):\n    p.data.mul_(1 - lr*wd) #just doing the formular above \n    return p\nweight_decay._defaults = dict(wd=0.)","da977360":"#another stepper for 12_reg\ndef l2_reg(p, lr, wd, **kwargs):\n    p.grad.data.add_(wd, p.data) #add_ in pythorch normally just add the p.data tensor to the p.grad tensor but if you add a scalor(wd-weight decay) they will\n    #multiply first to the scalor(wd) will multipy to the p.data tensor and then add it to the p.grad tensor \n    return p\nl2_reg._defaults = dict(wd=0.) #so we add a default where we just make the dictornary for weight deca 0, so this is now a function you can call on\n#so you can turn off the weight decay (this is used below)","a901be01":"#export\ndef maybe_update(os, dest, f):\n    for o in os: #goes through each of the things in os\n        for k,v in f(o).items(): #goes through each of the things in the dictornary \n            if k not in dest: dest[k] = v #check if it is not there, and if it is not it will update it \n\ndef get_defaults(d): return getattr(d,'_defaults',{})","14db6c54":"\nclass Optimizer():\n    def __init__(self, params, steppers, **defaults):\n        self.steppers = listify(steppers)\n        maybe_update(self.steppers, defaults, get_defaults) #so we are maybe update going to update our defaults(defined longer up) with whatever self.stepper has\n        #and there defaults(get_defaults). And the reason it is maybe update is because if you give a direct weight decay it is not gonna update it \n        #it will only pdate it if it is missing \n        # might be a generator\n        self.param_groups = list(params)\n        # ensure params is a list of lists\n        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n        self.hypers = [{**defaults} for p in self.param_groups]\n\n    def grad_params(self):\n        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n            for p in pg if p.grad is not None]\n\n    def zero_grad(self):\n        for p,hyper in self.grad_params():\n            p.grad.detach_()\n            p.grad.zero_()\n\n    def step(self):\n        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)","91b2e061":"#export \nsgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step]) #note now we have to steppers in the stepper function ","bc36e2c0":"\nlearn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=sgd_opt)","bbdd7a44":"model = learn.model","0b8c314e":"#create teh optimazer with the sgd_opt defined above \nopt = sgd_opt(model.parameters(), lr=0.1) #use sgd optimazer with our models parameters and a learning rate 0.1 \ntest_eq(opt.hypers[0]['wd'], 0.) #check that the hyperparamter for the weight deacy is 0 \ntest_eq(opt.hypers[0]['lr'], 0.1) #and the hyperparamter for learning rate is 0.1 ","5db31af5":"opt = sgd_opt(model.parameters(), lr=0.1, wd=1e-4) #make a new optimazer and give it a weight decay and a new learning rate\ntest_eq(opt.hypers[0]['wd'], 1e-4) #check if it passes, so it has a weight decay on 1e-04  \ntest_eq(opt.hypers[0]['lr'], 0.1) #check if it passes, so it has a learning rate on 0.1 ","e7d78cfc":"cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback]","d7836033":"\nlearn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=partial(sgd_opt, wd=0.01))","5aa9aeeb":"\nrun.fit(1, learn)","36ff3934":"#momwntum needs mote then just paramters and hyperparamters it uses stats, which is so that momentum knows what the activations was pdatet last time \n#since the formular for momentum is the current momentum times whatever you did list time(last momentum) plus the new step \n\nclass StatefulOptimizer(Optimizer):\n    def __init__(self, params, steppers, stats=None, **defaults): \n        self.stats = listify(stats)\n        maybe_update(self.stats, defaults, get_defaults)\n        super().__init__(params, steppers, **defaults)\n        self.state = {} #so when we track every singel parameter for what happen last time, it gets stored in state\n        \n    def step(self):\n        for p,hyper in self.grad_params(): #so step are gonna look at each of our paramters...\n            if p not in self.state: #...and it is gonna check to see if that paramter already exsist in our -state dictornary(self.state = {})\n                #if it hasnt been initailized then create a state for p and call all the statistics to initialize it.\n                self.state[p] = {} #so we will initailiz it wift a empty dictornary \n                maybe_update(self.stats, self.state[p], lambda o: o.init_state(p)) #and then we will update it with the ini_state (defined in below cell block)\n            state = self.state[p] #grap the state for each parameter \n            for stat in self.stats: state = stat.update(p, state, **hyper) #call update #note its gonna use the pass arguments in the stat.udate in the below formular to use the momentum \n            compose(p, self.steppers, **state, **hyper) #now we can compose it all #note we also uses our **state \n            self.state[p] = state","d294e737":"#stat is a lot like steppers so when we are going to create a state, this tells it how to do that \nclass Stat():\n    _defaults = {}\n    def init_state(self, p): raise NotImplementedError #\n    def update(self, p, state, **kwargs): raise NotImplementedError","6100661c":"class AverageGrad(Stat): #momentum is simply avergeing the gradient #note this just a definesion of a stat class \n    _defaults = dict(mom=0.9)\n\n    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)} #so we are gonna create a int_state that are gonna create the inited state \n    def update(self, p, state, mom, **kwargs):\n        state['grad_avg'].mul_(mom).add_(p.grad.data) #so we take whatever the gradient had before ( state['grad_avg']) we multiply it by monentum(mul_(mom)) and we add the current gradient (p.grad.data)\n        return state","dd4b8d13":"#we can now create our momentum stepper \ndef momentum_step(p, lr, grad_avg, **kwargs): #note grad_avg is defined above \n    p.data.add_(-lr, grad_avg) #and here is how we use a momentum step which is just the averge gradient times the learning rate \n    return p","26eb9276":"sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step,weight_decay],\n                  stats=AverageGrad(), wd=0.01) #creating a sgd momentum optimazer ","fb91e8ba":"learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=sgd_mom_opt)","4f6810ae":"\nrun.fit(1, learn)","242fa28c":"#make some experments to see what momentum does \nx = torch.linspace(-4, 4, 200) #create a 200 numbers equarly spaces between -4 and and 4 \ny = torch.randn(200) + 0.3 #and lets create another 200 random numbers wth the averge pong of 0.3 \nbetas = [0.5, 0.7, 0.9, 0.99] #each point will go in for the calulation for momentum and used for plotting ","3f1ff01d":"def plot_mom(f):\n    _,axs = plt.subplots(2,2, figsize=(12,8))\n    for beta,ax in zip(betas, axs.flatten()):\n        ax.plot(y, linestyle='None', marker='.')\n        avg,res = None,[]\n        for i,yi in enumerate(y):\n            avg,p = f(avg, beta, yi, i)\n            res.append(p)\n        ax.plot(res, color='red')\n        ax.set_title(f'beta={beta}')","44f4bc2e":"def mom1(avg, beta, yi, i): \n    if avg is None: avg=yi\n    res = beta*avg + yi #this is the momentum function \n    return res,res\n#so we plot res for each value of beta \nplot_mom(mom1)","ccf020f3":"def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2 #called exponentail weightet moving averge ","a40473bb":"\ndef mom2(avg, beta, yi, i):\n    if avg is None: avg=yi\n    avg = lin_comb(avg, yi, beta) #same ass befor but using exponentail weightet moving averge \n    return avg, avg\nplot_mom(mom2)","e3903227":"#so lets try with a function\ny = 1 - (x\/3) ** 2 + torch.randn(200) * 0.1","d1936aa7":"#where y(0)=0.5 s\u00e5 we get a point that is out of order from the function \ny[0]=0.5","0d78a5b7":"plot_mom(mom2)","b6268912":"def mom3(avg, beta, yi, i):\n    if avg is None: avg=0\n    avg = lin_comb(avg, yi, beta) #exponentail weighting moving averge from before \n    return avg, avg\/(1-beta**(i+1)) #and we have to divide it with (1-beta**(i+1) ->from the formular above to debias it.\n#the reason it works is because in debiasing we always start at 0, so even if we start at a abnormal point from for a function it wont effect the exponential weighng moving averge\nplot_mom(mom3)","dc229b00":"#export\nclass AverageGrad(Stat):\n    _defaults = dict(mom=0.9)\n    \n    def __init__(self, dampening:bool=False): self.dampening=dampening\n    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}\n    def update(self, p, state, mom, **kwargs):\n        state['mom_damp'] = 1-mom if self.dampening else 1. #so if you set dampeping to True it will 1-momentum else we will set it to 1\n        state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data) #same as before just with dampening \n        return state","90702378":"#export\nclass AverageSqrGrad(Stat): #same as AvergeGrad class but we multiply p.grad by it self so we get \"kvadroden\"(p.grad.data, p.grad.data) of it\n    #and note we store them in different names\n    _defaults = dict(sqr_mom=0.99)\n    \n    def __init__(self, dampening:bool=True): self.dampening=dampening\n    def init_state(self, p): return {'sqr_avg': torch.zeros_like(p.grad.data)}\n    def update(self, p, state, sqr_mom, **kwargs):\n        state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.\n        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)\n        return state","c5d2ca81":"#for debias we also need what step we are up to.\n#so the below class just count the step  \nclass StepCount(Stat):\n    def init_state(self, p): return {'step': 0}\n    def update(self, p, state, **kwargs):\n        state['step'] += 1\n        return state","38e9cc9a":"\n#exdebias function note we use formular written above\ndef debias(mom, damp, step): return damp * (1 - mom**step) \/ (1-mom)","0106ee88":"#export\ndef adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):\n    debias1 = debias(mom,     mom_damp, step)\n    debias2 = debias(sqr_mom, sqr_damp, step)\n    p.data.addcdiv_(-lr \/ debias1, grad_avg, (sqr_avg\/debias2).sqrt() + eps)\n    return p\nadam_step._defaults = dict(eps=1e-5)","3852e2a4":"#so this is our adam optimazer \ndef adam_opt(xtra_step=None, **kwargs):\n    return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step),\n                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)","fc2fe3f3":"\nlearn,run = get_learn_run(nfs, data, 0.001, conv_layer, cbs=cbfs, opt_func=adam_opt())","6cc8cc56":"run.fit(3, learn)","04f63ebe":"#use the formulars from above \ndef lamb_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, wd, **kwargs):\n    debias1 = debias(mom,     mom_damp, step)\n    debias2 = debias(sqr_mom, sqr_damp, step)\n    r1 = p.data.pow(2).mean().sqrt()\n    step = (grad_avg\/debias1) \/ ((sqr_avg\/debias2).sqrt()+eps) + wd*p.data\n    r2 = step.pow(2).mean().sqrt()\n    p.data.add_(-lr * min(r1\/r2,10), step)\n    return p\nlamb_step._defaults = dict(eps=1e-6, wd=0.)","902f9ba2":"lamb = partial(StatefulOptimizer, steppers=lamb_step, stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()])","ea71aa3c":"learn,run = get_learn_run(nfs, data, 0.003, conv_layer, cbs=cbfs, opt_func=lamb)","40bc1b46":"run.fit(3, learn)","929aafe0":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","0119caf4":"#export\nfrom exp.nb_09b import *\nimport time\nfrom fastprogress import master_bar, progress_bar\nfrom fastprogress.fastprogress import format_time","60135bb7":"path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)","0004775b":"\ntfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\nbs = 64\n\nil = ImageList.from_files(path, tfms=tfms)\nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\nll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)","57689cbd":"nfs = [32]*4 #this gives a four 32 layers ","c0eb192f":"# export \nclass AvgStatsCallback(Callback):\n    def __init__(self, metrics):\n        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n    \n    def begin_fit(self):\n        met_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]\n        names = ['epoch'] + [f'train_{n}' for n in met_names] + [\n            f'valid_{n}' for n in met_names] + ['time']\n        self.logger(names) #just print names at this stage \n    \n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n        self.start_time = time.time()\n        \n    def after_loss(self):\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad(): stats.accumulate(self.run)\n    \n    def after_epoch(self):\n        stats = [str(self.epoch)] \n        for o in [self.train_stats, self.valid_stats]:#same as before but here we are storing our stats in a array \n            stats += [f'{v:.6f}' for v in o.avg_stats] \n        stats += [format_time(time.time() - self.start_time)]\n        self.logger(stats) #and we are just passing off the array to the logger ","7c6a1513":"# this makes the process bar when training  \nclass ProgressCallback(Callback):\n    _order=-1\n    def begin_fit(self):\n        self.mbar = master_bar(range(self.epochs))#create mater bar which are the thing that tracks the epochs \n        self.mbar.on_iter_begin() #tell the mater bar we are starting \n        self.run.logger = partial(self.mbar.write, table=True) #and replace the logger function master bar(mbar).write so it will htlm into that\n        \n    def after_fit(self): self.mbar.on_iter_end() # and when we are done fitting tell the mater bar we are done \n    def after_batch(self): self.pb.update(self.iter) #after we have done a batch update our process bar \n    def begin_epoch   (self): self.set_pb() #begin epoch\n    def begin_validate(self): self.set_pb() #begin validating \n        \n    def set_pb(self): #make new progess bar \n        self.pb = progress_bar(self.dl, parent=self.mbar)\n        self.mbar.update(self.epoch)","51ac666a":"cbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback,\n        ProgressCallback,\n        partial(BatchTransformXCallback, norm_imagenette)]","f9273e34":"learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs)","006284e5":"learn.fit(2)","bf0e5d01":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","64708039":"#export\nfrom exp.nb_09c import *","265ea593":"\n#export\nmake_rgb._order=0","00463ad2":"path = datasets.untar_data(datasets.URLs.IMAGENETTE)\ntfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]","340334ab":"def get_il(tfms): return ImageList.from_files(path, tfms=tfms)","3d3d563a":"\nil = get_il(tfms)","d5d645e7":"show_image(il[0])","5996f352":"img = PIL.Image.open(il.items[0])#open the original whitout resizing it so to see how it looks in full size ","d7f32c24":"img","91ecd65b":"img.getpixel((1,1))","9daf033c":"import numpy as np","d694b815":"%timeit -n 10 a = np.array(PIL.Image.open(il.items[0]))","5f521a4a":"img.resize((128,128), resample=PIL.Image.ANTIALIAS)","14eef23f":"\nimg.resize((128,128), resample=PIL.Image.BILINEAR)","49ffbe2c":"img.resize((128,128), resample=PIL.Image.NEAREST)","c07cf430":"img.resize((256,256), resample=PIL.Image.BICUBIC).resize((128,128), resample=PIL.Image.NEAREST) #combining two ","116dc52d":"%timeit img.resize((224,224), resample=PIL.Image.BICUBIC)","5d1e72c0":"%timeit img.resize((224,224), resample=PIL.Image.BILINEAR)","a4d6d963":"%timeit -n 10 img.resize((224,224), resample=PIL.Image.NEAREST)","29d4577a":"import random","7297cc57":"def pil_random_flip(x):\n    return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<0.5 else x","7ceabb1e":"il1 = get_il(tfms)# create a itemlist \nil1.items = [il1.items[0]]*64#lets replace the items with just the first item with 64 copies of it \ndl = DataLoader(il1, 8)","31925c6d":"\nx = next(iter(dl))","026b54cc":"#export\ndef show_image(im, ax=None, figsize=(3,3)):\n    if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)\n    ax.axis('off')\n    ax.imshow(im.permute(1,2,0))\n\ndef show_batch(x, c=4, r=None, figsize=None):\n    n = len(x)\n    if r is None: r = int(math.ceil(n\/c))\n    if figsize is None: figsize=(c*3,r*3)\n    fig,axes = plt.subplots(r,c, figsize=figsize)#go through our batch and show all the images \n    for xi,ax in zip(x,axes.flat): show_image(xi, ax)","c6e310e8":"show_batch(x)","e686f930":"il1.tfms.append(pil_random_flip)","8b6884a3":"\nx = next(iter(dl))\nshow_batch(x)","f8532713":"class PilRandomFlip(Transform):\n    _order=11\n    def __init__(self, p=0.5): self.p=p #p is the properbilit for the image to flip in a given direction \n    def __call__(self, x):\n        return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<self.p else x","94ef14a4":"\n#export\nclass PilTransform(Transform): _order=11\n\nclass PilRandomFlip(PilTransform):\n    def __init__(self, p=0.5): self.p=p\n    def __call__(self, x):\n        return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<self.p else x","38416a07":"\ndel(il1.tfms[-1])\nil1.tfms.append(PilRandomFlip(0.8))#note p=0.8 so most of them are flipped ","32b41ce1":"\nx = next(iter(dl))\nshow_batch(x)","e5f71a50":"PIL.Image.FLIP_LEFT_RIGHT,PIL.Image.ROTATE_270,PIL.Image.TRANSVERSE","e2178b1f":"img = PIL.Image.open(il.items[0])\nimg = img.resize((128,128), resample=PIL.Image.NEAREST)\n_, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i,ax in enumerate(axs.flatten()):\n    if i==0: ax.imshow(img)\n    else:    ax.imshow(img.transpose(i-1))\n    ax.axis('off')","cff06f29":"#export\nclass PilRandomDihedral(PilTransform):\n    def __init__(self, p=0.75): self.p=p*7\/8 #Little hack to get the 1\/8 identity dihedral transform taken into account.\n    def __call__(self, x):\n        if random.random()>self.p: return x\n        return x.transpose(random.randint(0,6))","7b052cf2":"del(il1.tfms[-1])\nil1.tfms.append(PilRandomDihedral())","d80b787d":"show_batch(next(iter(dl)))","484316c6":"\nimg = PIL.Image.open(il.items[0])\nimg.size","5614b1ed":"img.crop((60,60,320,320)).resize((128,128), resample=PIL.Image.BILINEAR)","1ca2a345":"\ncnr2 = (60,60,320,320)\nresample = PIL.Image.BILINEAR","544388dd":"%timeit -n 10 img.crop(cnr2).resize((128,128), resample=resample)","e807e09e":"img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample)","224a41a1":"%timeit -n 10 img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample)","ff2268e9":"#export\nfrom random import randint\n\ndef process_sz(sz):\n    sz = listify(sz)\n    return tuple(sz if len(sz)==2 else [sz[0],sz[0]])\n\ndef default_crop_size(w,h): return [w,w] if w < h else [h,h]\n\nclass GeneralCrop(PilTransform):\n    def __init__(self, size, crop_size=None, resample=PIL.Image.BILINEAR): \n        self.resample,self.size = resample,process_sz(size)\n        self.crop_size = None if crop_size is None else process_sz(crop_size)\n        \n    def default_crop_size(self, w,h): return default_crop_size(w,h)\n\n    def __call__(self, x):\n        csize = self.default_crop_size(*x.size) if self.crop_size is None else self.crop_size\n        return x.transform(self.size, PIL.Image.EXTENT, self.get_corners(*x.size, *csize), resample=self.resample)\n    \n    def get_corners(self, w, h): return (0,0,w,h)\n\nclass CenterCrop(GeneralCrop):\n    def __init__(self, size, scale=1.14, resample=PIL.Image.BILINEAR):\n        super().__init__(size, resample=resample)\n        self.scale = scale\n        \n    def default_crop_size(self, w,h): return [w\/self.scale,h\/self.scale]\n    \n    def get_corners(self, w, h, wc, hc):\n        return ((w-wc)\/\/2, (h-hc)\/\/2, (w-wc)\/\/2+wc, (h-hc)\/\/2+hc)","1c6831e9":"il1.tfms = [make_rgb, CenterCrop(128), to_byte_tensor, to_float_tensor]","7ee26336":"show_batch(next(iter(dl)))","2072ad3b":"\n# export\nclass RandomResizedCrop(GeneralCrop):\n    def __init__(self, size, scale=(0.08,1.0), ratio=(3.\/4., 4.\/3.), resample=PIL.Image.BILINEAR): #(3.\/4., 4.\/3.) changing the size of the person like the man is a bit thinner on and image and a bit fatter on another\n        super().__init__(size, resample=resample)\n        self.scale,self.ratio = scale,ratio\n    \n    def get_corners(self, w, h, wc, hc):\n        area = w*h\n        #Tries 10 times to get a proper crop inside the image.\n        for attempt in range(10):\n            area = random.uniform(*self.scale) * area\n            ratio = math.exp(random.uniform(math.log(self.ratio[0]), math.log(self.ratio[1])))\n            new_w = int(round(math.sqrt(area * ratio)))\n            new_h = int(round(math.sqrt(area \/ ratio)))\n            if new_w <= w and new_h <= h:\n                left = random.randint(0, w - new_w)\n                top  = random.randint(0, h - new_h)\n                return (left, top, left + new_w, top + new_h)\n        \n        # Fallback to squish\n        if   w\/h < self.ratio[0]: size = (w, int(w\/self.ratio[0]))\n        elif w\/h > self.ratio[1]: size = (int(h*self.ratio[1]), h)\n        else:                     size = (w, h)\n        return ((w-size[0])\/\/2, (h-size[1])\/\/2, (w+size[0])\/\/2, (h+size[1])\/\/2)","da59bf59":"\nil1.tfms = [make_rgb, RandomResizedCrop(128), to_byte_tensor, to_float_tensor]","a50047a7":"show_batch(next(iter(dl)))","4e94207d":"\n# export\nfrom torch import FloatTensor,LongTensor\n\ndef find_coeffs(orig_pts, targ_pts):\n    matrix = []\n    #The equations we'll need to solve.\n    for p1, p2 in zip(targ_pts, orig_pts):\n        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n\n    A = FloatTensor(matrix)\n    B = FloatTensor(orig_pts).view(8, 1)\n    #The 8 scalars we seek are solution of AX = B\n    return list(torch.solve(B,A)[0][:,0])","17d35625":"# export\ndef warp(img, size, src_coords, resample=PIL.Image.BILINEAR):\n    w,h = size\n    targ_coords = ((0,0),(0,h),(w,h),(w,0))\n    c = find_coeffs(src_coords,targ_coords)\n    res = img.transform(size, PIL.Image.PERSPECTIVE, list(c), resample=resample)\n    return res","11770a39":"\ntarg = ((0,0),(0,128),(128,128),(128,0))\nsrc  = ((90,60),(30,280),(310,280),(250,60))","a3669d98":"c = find_coeffs(src, targ)\nimg.transform((128,128), PIL.Image.PERSPECTIVE, list(c), resample=resample)","1ed2dbb7":"%timeit -n 10 warp(img, (128,128), src)","90d821b8":"%timeit -n 10 warp(img, (128,128), src, resample=PIL.Image.NEAREST)","3843d138":"warp(img, (64,64), src, resample=PIL.Image.BICUBIC)","33ba18ce":"warp(img, (64,64), src, resample=PIL.Image.NEAREST)","c9ec6c3b":"\n# export\ndef uniform(a,b): return a + (b-a) * random.random()","e118dc9c":"class PilTiltRandomCrop(PilTransform):\n    def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.NEAREST): \n        self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude\n        self.crop_size = None if crop_size is None else process_sz(crop_size)\n        \n    def __call__(self, x):\n        csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size\n        up_t,lr_t = uniform(-self.magnitude, self.magnitude),uniform(-self.magnitude, self.magnitude)\n        left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1])\n        src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])\n        src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float()\n        src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])\n        return warp(x, self.size, src_corners, resample=self.resample)","cf78bf08":"il1.tfms = [make_rgb, PilTiltRandomCrop(128, magnitude=0.1), to_byte_tensor, to_float_tensor]","988d3f5e":"\nx = next(iter(dl))\nshow_batch(x)","0bafa85c":"\n# export\nclass PilTiltRandomCrop(PilTransform):\n    def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.BILINEAR): \n        self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude\n        self.crop_size = None if crop_size is None else process_sz(crop_size)\n        \n    def __call__(self, x):\n        csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size\n        left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1])\n        top_magn = min(self.magnitude, left\/csize[0], (x.size[0]-left)\/csize[0]-1)\n        lr_magn  = min(self.magnitude, top \/csize[1], (x.size[1]-top) \/csize[1]-1)\n        up_t,lr_t = uniform(-top_magn, top_magn),uniform(-lr_magn, lr_magn)\n        src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])\n        src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float()\n        src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])\n        return warp(x, self.size, src_corners, resample=self.resample)","7f4125eb":"\nil1.tfms = [make_rgb, PilTiltRandomCrop(128, 200, magnitude=0.2), to_byte_tensor, to_float_tensor]","acccd7ab":"\nx = next(iter(dl))\nshow_batch(x)","683450bc":"[(o._order,o) for o in sorted(tfms, key=operator.attrgetter('_order'))]","90a2eef2":"\n#export\nimport numpy as np\n\ndef np_to_float(x): return torch.from_numpy(np.array(x, dtype=np.float32, copy=False)).permute(2,0,1).contiguous()\/255.\nnp_to_float._order = 30","8b287474":"\n%timeit -n 10 to_float_tensor(to_byte_tensor(img))","c9ef07e7":"%timeit -n 10 np_to_float(img)","ec75ca06":"\nil1.tfms = [make_rgb, PilTiltRandomCrop(128, magnitude=0.2), to_byte_tensor, to_float_tensor]","546569dd":"\ndl = DataLoader(il1, 64)","20290b0b":"\nx = next(iter(dl))","dae93915":"\nfrom torch import FloatTensor","cf6e8933":"def affine_grid_cpu(size): #create a affine grid which is just koordinates of where is every pixsel \n    N, C, H, W = size\n    grid = FloatTensor(N, H, W, 2)\n    linear_points = torch.linspace(-1, 1, W) if W > 1 else tensor([-1])\n    grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, :, 0])\n    linear_points = torch.linspace(-1, 1, H) if H > 1 else tensor([-1])\n    grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, :, 1])\n    return grid","7ab353f8":"grid = affine_grid_cpu(x.size())","8ba0e075":"\ngrid.shape","0c95ecce":"grid[0,:5,:5] #like litaraly where a re the pixsel koordinates from -1 to 1 ","13b4cf52":"\nm = tensor([[1., 0., 0.], [0., 1., 0.]])\ntheta = m.expand(x.size(0), 2, 3)","b4067b99":"\nm = tensor([[1., 0., 0.], [0., 1., 0.]])\ntheta = m.expand(x.size(0), 2, 3)","32a4cb29":"%timeit -n 10 grid = F.affine_grid(theta, x.size())","ce25cf37":"\n%timeit -n 10 grid = F.affine_grid(theta.cuda(), x.size())","84f94da0":"\ndef affine_grid(x, size):\n    size = (size,size) if isinstance(size, int) else tuple(size)\n    size = (x.size(0),x.size(1)) + size\n    if x.device.type == 'cpu': return affine_grid_cpu(size) \n    m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device)\n    return F.affine_grid(m.expand(x.size(0), 2, 3), size)","348e66e7":"grid = affine_grid(x, 128)","12e2bee4":"from torch import stack,zeros_like,ones_like","14442f52":"\ndef rotation_matrix(thetas):\n    thetas.mul_(math.pi\/180)\n    rows = [stack([thetas.cos(),             thetas.sin(),             torch.zeros_like(thetas)], dim=1),\n            stack([-thetas.sin(),            thetas.cos(),             torch.zeros_like(thetas)], dim=1),\n            stack([torch.zeros_like(thetas), torch.zeros_like(thetas), torch.ones_like(thetas)], dim=1)]\n    return stack(rows, dim=1)","af266e6f":"\nthetas = torch.empty(x.size(0)).uniform_(-30,30)","674b046d":"thetas[:5]","75e3982c":"m = rotation_matrix(thetas)","761997a7":"m.shape, m[:,None].shape, grid.shape","21a5b774":"grid.view(64,-1,2).shape","c2a3ee41":"a = m[:,:2,:2]\nb = m[:, 2:,:2]\ntfm_grid = (grid.view(64,-1,2) @ a + b).view(64, 128, 128, 2)","1542610c":"%timeit -n 10 tfm_grid = grid @ m[:,None,:2,:2] + m[:,2,:2][:,None,None]","b83e0bbe":"\n%timeit -n 10 tfm_grid = torch.einsum('bijk,bkl->bijl', grid, m[:,:2,:2]) + m[:,2,:2][:,None,None]","104f6f1e":"%timeit -n 10 tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2][:,None,None]","ae4c8f07":"\n%timeit -n 10 tfm_grid = (torch.bmm(grid.view(64,-1,2), m[:,:2,:2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2)","886e1fed":"grid = grid.cuda()\nm = m.cuda()","b1640aa0":"%timeit -n 10 tfm_grid = grid @ m[:,None,:2,:2] + m[:,2,:2][:,None,None]","dadadc1e":"%timeit -n 10 tfm_grid = torch.einsum('bijk,bkl->bijl', grid, m[:,:2,:2]) + m[:,2,:2][:,None,None]","a013d968":"%timeit -n 10 tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2][:,None,None]","91779885":"\n%timeit -n 10 tfm_grid = (torch.bmm(grid.view(64,-1,2), m[:,:2,:2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2)","9ca20aad":"tfm_grid = torch.bmm(grid.view(64,-1,2), m[:,:2,:2]).view(-1, 128, 128, 2)","8dac6f78":"\ntfm_x = F.grid_sample(x, tfm_grid.cpu())","1abb6442":"\nshow_batch(tfm_x, r=2)","4e132a85":"tfm_x = F.grid_sample(x, tfm_grid.cpu(), padding_mode='reflection')# padding_mode='reflection' removes the black 'hj\u00f8rner'","8ce37516":"\nshow_batch(tfm_x, r=2)","a68ae022":"def rotate_batch(x, size, degrees):\n    grid = affine_grid(x, size)\n    thetas = x.new(x.size(0)).uniform_(-degrees,degrees)\n    m = rotation_matrix(thetas)\n    tfm_grid = grid @ m[:,:2,:2].unsqueeze(1) + m[:,2,:2][:,None,None]\n    return F.grid_sample(x, tfm_grid)","4c2c2620":"\nshow_batch(rotate_batch(x, 128, 30), r=2)","465474d8":"\n%timeit -n 10 tfm_x = rotate_batch(x, 128, 30)","1611ed49":"%timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30)","59809451":"\nfrom torch import Tensor","39ba1700":"\nfrom torch.jit import script\n\n@script\ndef rotate_batch(x:Tensor, size:int, degrees:float) -> Tensor:\n    sz = (x.size(0),x.size(1)) + (size,size)\n    idm = torch.zeros(2,3, device=x.device)\n    idm[0,0] = 1.\n    idm[1,1] = 1.\n    grid = F.affine_grid(idm.expand(x.size(0), 2, 3), sz) #do the affine grid here instead of above work just fine \n    thetas = torch.zeros(x.size(0), device=x.device).uniform_(-degrees,degrees)\n    m = rotation_matrix(thetas)\n    tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2].unsqueeze(1).unsqueeze(2)\n    return F.grid_sample(x, tfm_grid)","b204a9a2":"m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device)","e78f572a":"%timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30)","aa9350d1":"def rotate_batch(x, size, degrees):\n    size = (size,size) if isinstance(size, int) else tuple(size)\n    size = (x.size(0),x.size(1)) + size\n    thetas = x.new(x.size(0)).uniform_(-degrees,degrees)\n    m = rotation_matrix(thetas)\n    grid = F.affine_grid(m[:,:2], size)\n    return F.grid_sample(x.cuda(), grid)","069bab39":"%timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30)","cb6fd1c4":"# Optimizer tweaks","ff0d35f0":"\nOther recent variants of optimizers:\n\nLarge Batch Training of Convolutional Networks (LARS also uses weight statistics, not just gradient statistics. Can you add that to this class?)\nAdafactor: Adaptive Learning Rates with Sublinear Memory Cost (Adafactor combines stats over multiple sets of axes)\nAdaptive Gradient Methods with Dynamic Bound of Learning Rate","ee8e41ca":"\nWe can see it gets to a zero-constant when the data is purely random. If the data has a certain shape, it will get that shape (with some delay for high beta).","6a74c326":"# Momentum experiments\nWhat does momentum do to the gradients exactly? Let's do some plots to find out!","2788ee54":"\n## Step 1: generate the grid","7f8d6c95":"Let's allow steppers to add to our defaults (which are the default values of all the hyper-parameters). This helper function adds in dest the key\/values it finds while going through os and applying f when they was no key of the same name.","ab96f85e":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","1035c366":"\nOur time budget: aim for 5 mins per batch for imagenet on 8 GPUs. 1.25m images in imagenet. So on one GPU per minute that's 1250000\/8\/5 == 31250, or 520 per second. Assuming 4 cores per GPU, then we want ~125 images per second - so try to stay <10ms per image. Here we have time to do more things. For instance, we can do the crop and resize in the same call to transform, which will give a smoother result.","02c3499e":"\nHere is a convenience function to look at images in a batch.","d5b25abf":"Here is an example of Stat:","958a2938":"Now that we have changed the optimizer, we will need to adjust the callbacks that were using properties from the PyTorch optimizer: in particular the hyper-parameters are in the list of dictionaries opt.hypers (PyTorch has everything in the the list of param groups).","85a0d82e":"\nSo let's check we didn't break anything and that recorder and param scheduler work properly.","04541c97":"# Data augmentation","050e17a5":"\n# Refining the optimizer\nIn PyTorch, the base optimizer in torch.optim is just a dictionary that stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups (different groups can have different learning rates\/momentum\/weight decay... which is what lets us do discriminative learning rates).\n\nIt contains a method step that will update our parameters with the gradients and a method zero_grad to detach and zero the gradients of all our parameters.\n\nWe build the equivalent from scratch, only ours will be more flexible. In our implementation, the step function loops over all the parameters to execute the step using stepper functions that we have to provide when initializing the optimizer.","2bb799be":"With momentum\n\n\nMomentum requires to add some state. We need to save the moving average of the gradients to be able to do the step and store this inside the optimizer state. To do this, we introduce statistics. Statistics are object with two methods:\n\n    - init_state, that returns the initial state (a tensor of 0. for the moving average of gradients)\n    - update, that updates the state with the new gradient value\nWe also read the _defaults values of those objects, to allow them to provide default values to hyper-parameters.","83da9a3c":"\nIt's then super easy to implement a new optimizer. This is LAMB from a very recent paper:\n\n![image.png](attachment:image.png)","bc66e86b":"\nBut if we provide a value, it overrides the default.","a8aa36c4":"\nBefore trying to train, let's check the behavior works as intended: when we don't provide a value for wd, we pull the corresponding default from weight_decay.","4c6d357e":"This is the same as before, we just take the default values of the steppers when none are provided in the kwargs.","5f44f3f5":"\nWe can also make that transform a class so it's easier to set the value of the parameter p. As seen before, it also allows us to set the _order attribute.","d7b91c64":"![image.png](attachment:image.png)","d53d2186":"This is pretty fast in PIL:","605e006e":"# Part 3 10_augmentation ","454bf633":"Without data augmentation:","06bee1c1":"# L2 reularization has no regularizing effect when combined with normalization","98c4a7e6":"\nThen the Adam step is just the following:","7ea2c3b6":"\nThe speed of this depends a lot on what card you have. On a V100 it is generally about 3x faster than non-JIT (as at April 2019) although PyTorch JIT is rapidly improving.\n\n## affine multiplication with affine_grid\nAnd even faster if we give the matrix rotation to affine_grid.","4f0b0f72":"## Flip\nFlip can be done with PIL very fast.","d2505162":"The interpolation to find our coordinates back is done by grid_sample.","64442178":"it looks pretty nice through it is not magic so t wont be perfect ","21013cf2":"\nThis is already better than the baseline!","4677e806":"\nPIL can also do the whole dihedral group of transformations (random horizontal flip, random vertical flip and the four 90 degrees rotation) with the transpose method. Here are the codes of a few transformations:","b062c976":"\nAnd we can implement it like this:","313dc235":"\n# RandomResizeCrop\nThis is the usual data augmentation used on ImageNet (introduced here) that consists of selecting 8 to 100% of the image area and a scale between 3\/4 and 4\/3 as a crop, then resizing it to the desired size. It combines some zoom and a bit of squishing at a very low computational cost.","5798115d":"so what we see is that is finde for small momentum but when we use a higher momentn its overshut and gets wrong. Not this is because we multiply the 'out of order' point with a high number, so this will be carried along. so to fix this we will use debiasing ","6ec5ea45":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","8ec9a425":"\nIt's a little bit slower but still fast enough for our purpose, so we will use this. We then define a general crop transform and two subclasses: one to crop at the center (for validation) and one to randomly crop. Each time, the subclass only implements the way to get the four corners passed to PIL.","00e88299":"# LAMB","c7dd0b61":"looks fine with random numbers","d26c6462":"\nProblem is that black padding appears as soon as our target points are outside of the image, so we have to limit the magnitude if we want to avoid that.","3599422f":"\nTo do basic SGD, this what a step looks like:","91a1c046":"# Part 2 process bar","56eda9cd":"# Adding progress bars to Learner","f0b5590e":"And on the GPU","1301d57f":"## Step 2: Affine multiplication\nIn 2D an affine transformation has the form y = Ax + b where A is a 2x2 matrix and b a vector with 2 coordinates. It's usually represented by the 3x3 matrix\n\nA[0,0]  A[0,1]  b[0]\nA[1,0]  A[1,1]  b[1]\n   0       0     1\nbecause then the composition of two affine transforms can be computed with the matrix product of their 3x3 representations.","5d8ca489":"\n# Weight decay\n\n\nBy letting our model learn high parameters, it might fit all the data points in the training set with an over-complex function that has very sharp changes, which will lead to overfitting.","9eb5359a":"## Random crop","87cc3351":"One thing has been missing all this time, and as fun as it is to stare at a blank screen waiting for the results, it's nicer to have some tool to track progress.","4071f302":"\nNow let's fit.","057205c2":"\nAs we can see, with a too high value, it may go way too high with no way to change its course.\n\nAnother way to smooth noisy data is to do an exponentially weighted moving average. In this case, there is a dampening of (1-beta) in front of the new value, which is less trusted than the current average. We'll define lin_comb (linear combination) to make this easier (note that in the lesson this was named ewma).","65d5aa38":"\nDebiasing is here to correct the wrong information we may have in the very first batch. The debias term corresponds to the sum of the coefficient in our moving average. At the time step i, our average is:\n\n$\\begin{align*}\navg_{i} = \\beta\\ avg_{i-1} + (1-\\beta)\\ v_{i} = \\beta\\ (\\beta\\ avg_{i-2} + (1-\\beta)\\ v_{i-1}) + (1-\\beta)\\ v_{i} \\\\\n= \\beta^{2}\\ avg_{i-2} + (1-\\beta)\\ \\beta\\ v_{i-1} + (1-\\beta)\\ v_{i} \\\\\n= \\beta^{3}\\ avg_{i-3} + (1-\\beta)\\ \\beta^{2}\\ v_{i-2} + (1-\\beta)\\ \\beta\\ v_{i-1} + (1-\\beta)\\ v_{i} \\\\\n\\vdots \\\\\n= (1-\\beta)\\ \\beta^{i}\\ v_{0} + (1-\\beta)\\ \\beta^{i-1}\\ v_{1} + \\cdots + (1-\\beta)\\ \\beta^{2}\\ v_{i-2} + (1-\\beta)\\ \\beta\\  v_{i-1} + (1-\\beta)\\ v_{i}\n\\end{align*}$\n\nand so the sum of the coefficients is\n\n$\\begin{align*}\nS  =(1-\\beta)\\ \\beta^{i} + (1-\\beta)\\ \\beta^{i-1} + \\cdots + (1-\\beta)\\ \\beta^{2} + (1-\\beta)\\ \\beta + (1-\\beta) \\\\\n  = (\\beta^{i} - \\beta^{i+1}) + (\\beta^{i-1} - \\beta^{i}) + \\cdots + (\\beta^{2} - \\beta^{3}) + (\\beta - \\beta^{2}) + (1-\\beta) \\\\\n  = 1 - \\beta^{i+1}\n\\end{align*}$\n\nsince all the other terms cancel out each other.\n\nBy dividing by this term, we make our moving average a true average (in the sense that all the coefficients we used for the average sum up to 1).","7bdc4842":"\nWe also need to track the moving average of the gradients squared.","fdf5c4f8":"This helper function computes the debias term. If we dampening, damp = 1 - mom and we get the same result as before. If we don't use dampening, (damp = 1) we will need to divide by 1 - mom because that term is missing everywhere.","ca5101e6":"\nIt is actually faster to combine to_float_tensor and to_byte_tensor in one transform using numpy.","f88a9643":"\nThen we add the momentum step (instead of using the gradients to perform the step, we use the average).","074a39ae":"We rewrite the AvgStatsCallback to add a line with the names of the things measured and keep track of the time per epoch.","2434689e":"\n# Batch data augmentation\nYou can write your own augmentation for your domain's data types, and have them run on the GPU, by using regular PyTorch tensor operations. Here's an example for images. The key is to do them on a whole batch at a time. Nearly all PyTorch operations can be done batch-wise.\n\nOnce we have resized our images so that we can batch them together, we can apply more data augmentation on a batch level. For the affine\/coord transforms, we proceed like this:\n\ngenerate a grid map of the size of our batch (bs x height x width x 2) that contains the coordinates of a grid of size height x width (this will be the final size of the image, and doesn't have to be the same as the current size in the batch)\napply the affine transforms (which is a matrix multiplication) and the coord transforms to that grid map\ninterpolate the values of the final pixels we want from the initial images in the batch, according to the transformed grid map\nFor 1. and 3. there are PyTorch functions: F.affine_grid and F.grid_sample. F.affine_grid can even combine 1 and 2 if we just want to do an affine transformation.","189560aa":"Then we add the progress bars... with a Callback of course! master_bar handles the count over the epochs while its child progress_bar is looping over all the batches. We just create one at the beginning or each epoch\/validation phase, and update it at the end of each batch. By changing the logger of the Learner to the write function of the master bar, everything is automatically written there.\n\nNote: this requires fastprogress v0.1.21 or later.","21b9a473":"\n## View images","c76804bb":"# Note: du skal bare tage alle de steder i koden der st\u00e5r eksport, da det er hvad vi bruger senere i udviklingen af modeller ","6d3814d7":"\nWe have to apply our rotation to every point in the grid. The matrix a is given by the first two rows and two columns of m and the vector b is the first two coefficients of the last column. Of course we have to deal with the fact that here m is a batch of matrices.","80fe51a2":"\nThis is the baseline of training with vanilla SGD.","666ef399":"The matrix for a rotation that has an angle of theta is:\n\ncos(theta) -sin(theta) 0\nsin(theta)  cos(theta) 0\n0           0          1\nHere we have to apply the reciprocal of a regular rotation (exercise: find why!) so we use this matrix:\n\n cos(theta) sin(theta) 0\n-sin(theta) cos(theta) 0\n 0          0          1\nthen we draw a different theta for each version of the image in the batch to return a batch of rotation matrices (size bs x 3 x 3).","c49308ea":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","6837d55c":"Then a model:","2f9cb0b1":"\n## Step 3: interpolate\nSince bmm is always the fastest, we use this one for the matrix multiplication.","b4ff277d":"To crop an image with PIL we have to specify the top\/left and bottom\/right corner in this format: (left, top, right, bottom). We won't just crop the size we want, but first crop the section we want of the image and then apply a resize. In what follows, we call the first one the crop_size.","d60edcbc":"\nThis is the regular momentum.","e8351c8c":"So we write our own version that dispatches on the CPU with our function and uses PyTorch's on the GPU.","7e927d2a":"With random flip:","d82118b6":"\nNot bad for 64 rotations!","ab829cef":"Weight decay comes from the idea of L2 regularization, which consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\n\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield to a state where it generalizes better. Going back to the theory a little bit, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss:\n\nloss_with_wd = loss + (wd\/2) * (weights**2).sum()\nIn practice though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, the derivative of p**2 with respect to p is 2*p. So adding that big sum to our loss is exactly the same as doing:\n\nweight.grad += wd * weight\nfor every weight in our model, which in the case of vanilla SGD is equivalent to updating the parameters with:\n\nweight = weight - lr*(weight.grad + wd*weight)\nThis technique is called \"weight decay\", as each weight is decayed by a factor lr * wd, as it's shown in this last formula.\n\nThis only works for standard SGD, as we have seen that with momentum, RMSProp and Adam, the update has some additional formulas around the gradient. In those cases, the formula that comes from L2 regularization:\n\nweight.grad += wd * weight\nis different than weight decay\n\nnew_weight = weight - lr * weight.grad - lr * wd * weight\nMost libraries use the first one, but as it was pointed out in Decoupled Weight Regularization by Ilya Loshchilov and Frank Hutter, it is better to use the second one with the Adam optimizer, which is why fastai made it its default.\n\nWeight decay is subtracting lr*wd*weight from the weights. We need this function to have an attribute _defaults so that we are sure there is an hyper-parameter of the same name in our Optimizer.","ddca4d47":"# Imagenette data\nWe grab the data from the previous notebook.","051e7d4a":"\n## PIL transforms\nWe start with PIL transforms to resize all our images to the same size. Then, when they are in a batch, we can apply data augmentation to all of them at the same time on the GPU. We have already seen the basics of resizing and putting on the GPU in 08, but we'll look more into it now.","61b92ea2":"\n## Perspective warping\nTo do perspective warping, we map the corners of the image to new points: for instance, if we want to tilt the image so that the top looks closer to us, the top\/left corner needs to be shifted to the right and the top\/right to the left. To avoid squishing, the bottom\/left corner needs to be shifted to the left and the bottom\/right corner to the right. For instance, if we have an image with corners in:\n\n    (60,60,60,280,280,280,280,60)\n(top\/left, bottom\/left, bottom\/right, top\/right) then a warped version is:\n\n    (90,60,30,280,310,280,250,60)\nPIL can do this for us but it requires 8 coefficients we need to calculate. The math isn't the most important here, as we've done it for you. We need to solve this equation. The equation solver is called torch.solve in PyTorch.","879518e0":"\n# Adam and friends\nIn Adam, we use the gradient averages but with dampening (not like in SGD with momentum), so let's add this to the AverageGrad class.","b0eb52d2":"\n# Jit version\nBut we can be even faster!","792fac56":"du kan sltid tjekke senere om der er noget af koden som kan fjernes efter d har f\u00e5et dden til at virke ","efcd8a6d":"We can add a transform to do this perspective warping automatically with the rand resize and crop.","789b2f95":"\nL2 regularization is adding wd*weight to the gradients.","21338713":"By making the progress bar a callback, you can easily choose if you want to have them shown or not.","2249608e":"# Faster tensor creation","d82f1a00":"\nBe careful of resampling methods, you can quickly lose some textures! look at the mans shirt","765d09fa":"\nIt takes a padding_mode argument.","6d40806f":"\nBe careful that img.transpose(0) is already one transform, so doing nothing requires a separate case, then we have 7 different transformations.","1dc04e73":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","1e345c63":"so beta here is gonna be our different values of momentum ","2668866e":"## Imagenette data","2813c268":"We will also need the number of steps done during training for the debiasing.","5d10a24d":"# Timing\nLet's look at the speed now!","3e538843":"We can also do this without the view by using broadcasting.","f64d5dca":"Coords in the grid go from -1, to 1 (PyTorch convention).\n\nPyTorch version is slower on the CPU but optimized to go very fast on the GPU"}}