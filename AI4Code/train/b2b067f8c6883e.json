{"cell_type":{"951dc687":"code","105632cf":"code","6112222b":"code","2312ec7b":"code","6263c8ad":"code","858c8bd4":"code","d51f12fb":"code","b240c3bc":"code","5148dda4":"code","f5995600":"code","91e85a4e":"code","247412b1":"code","17c4e984":"code","ac3fe915":"code","36373638":"code","6e3d3105":"code","43c15725":"code","b1762d3e":"code","27acb10b":"markdown","00eef98f":"markdown","9b0f1378":"markdown","0925bca0":"markdown","a874cb9d":"markdown","e0d8d35e":"markdown","11306d29":"markdown","5ba53773":"markdown","713ba8c6":"markdown"},"source":{"951dc687":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","105632cf":"df = pd.read_csv('..\/input\/chronic-kidney-disease\/new_model.csv')","6112222b":"df.head()","2312ec7b":"df.describe()","6263c8ad":"pos = df.loc[df['Class']==1]\nneg = df.loc[df['Class']==0]","858c8bd4":"heatmap = sns.heatmap(df.drop(columns=['Class']).corr(), cmap='RdBu')\nheatmap.set_title('Feature Correlation')\nplt.show()","d51f12fb":"def sigmoid(x):\n    return 1\/(1 + np.e**(-x))\n\n\ndef hypothesis(weights, X):\n    z = np.dot(weights, X.T)\n    return sigmoid(z)\n\n\ndef cost_function(X, y, weights):\n    y_hat = hypothesis(weights, X)\n    \n    predict_1 = y * np.log(y_hat)\n    predict_0 = (1-y) * np.log(1 - y_hat)\n    \n    return -(1\/len(X)) * np.sum(predict_0 + predict_1)\n\n\ndef train(X, y, epochs, learn_rate):\n    \n    costs = [] # empty list to track loss as training progresses\n    \n    n = len(X) # number of training examples\n    \n    # add a column of ones to X to use as input to the bias term\n    X = np.append(np.ones((n,1)), X, 1)\n    \n    # randomly initialize weights for each feature, as well as the bias term\n    weights = np.random.rand(1, X.shape[1])\n\n    \n    for epoch in range(epochs):\n        \n        # make predictions\n        y_hat = hypothesis(weights, X)\n        \n        # update weights with derivative of cost function\n        weights -= (learn_rate\/n) * np.dot((y_hat - y), X)\n        \n        # calculate current cost to monitor it\n        costs.append(cost_function(X, y, weights))\n    \n    # Plot learning curve during training\n    plt.plot(list(range(epochs)), costs)\n    plt.xlabel('Epochs')\n    plt.ylabel('Cost')\n    plt.title('Learning Curve')\n    plt.show()\n    \n    return weights","b240c3bc":"# set random seed\nnp.random.seed(1)","5148dda4":"# define training features\ndata = df.to_numpy()\n\ndata_normalized = data.copy()\n\n# Loop through each feature to normalize to 0 to 1 range\nfeatures_min_max = []\nfor feature in range(data_normalized.shape[1]-1):\n    feature_min = data_normalized[:,feature].min()\n    feature_max = data_normalized[:,feature].max()\n    \n    # Min-max normalize each feature\n    data_normalized[:,feature] = (data_normalized[:,feature] - feature_min)\/(feature_max-feature_min)\n    \n    features_min_max.append((feature_min,feature_max))\n\nfor index, min_max in enumerate(features_min_max):\n    print('Feature:', df.columns[index])\n    print('Min:', min_max[0], '\\nMax:', min_max[1])\n    print('Normalized Min: 0.0\\nNormalized Max: 1.0\\n')","f5995600":"# define a random training set (80% of data)\nnp.random.shuffle(data_normalized)\ntrain_data = data_normalized[0:round(len(data)*0.8),:] # first 80% of the rows, all columns\ntest_data = data_normalized[round(len(data)*0.8):,:] # last 20% of the rows, all columns\n\nprint('Training examples:', len(train_data), 'Test examples:', len(test_data))","91e85a4e":"# last column is Class (1 or 0 for Y or N on disease diagnosis)\ntrain_data.shape, test_data.shape","247412b1":"# Define model X and y\nX_train = train_data[:, 0:-1]\ny_train = train_data[:, -1]\n\nX_test = test_data[:, 0:-1]\ny_test = test_data[:, -1]","17c4e984":"X_train.shape, y_train.shape","ac3fe915":"trained_weights = train(X_train, y_train, epochs=1000, learn_rate=0.5)","36373638":"trained_weights","6e3d3105":"decision_boundary = 0.5\n\nfor i in range(4):\n    \n    i = np.random.randint(0, X_test.shape[0])\n    \n    sample = X_test[i,:]\n    sample = np.insert(sample, 0, [1.])\n    printing = [print('X'+str(index)+':',round(x_i, 3)) for index, x_i in enumerate(sample)]\n    \n    prediction = sigmoid(np.dot(trained_weights, sample))\n    print('Prediction:', round(prediction[0],4))\n    \n    if prediction < decision_boundary:\n        print('Predicting that patient does not have chronic kidney disease.')\n    if prediction > decision_boundary:\n        print('Predicting that patient has chronic kidney disease.')\n          \n    actual = y_test[i]\n    \n    if actual == 0.0:\n        print('In reality, this patient was not diagnosed with chronic kidney disease.')\n    if actual == 1.0:\n        print('In reality, this patient was diagnosed with chronic kidney disease.')\n    \n    if prediction <= decision_boundary and actual == 0.0:\n        print('Prediction correct! :) \\n')\n    elif prediction > decision_boundary and actual == 1.0:\n        print('Prediction correct! :) \\n')\n    else:\n        print('Prediction incorrect. :( \\n')","43c15725":"tp = 0\nfp = 0\ntn = 0\nfn = 0\n\nfor i in range(X_test.shape[0]):\n    sample = X_test[i, :]\n    sample = np.insert(sample, 0, [1.])\n    \n    y_actual = y_test[i]\n    \n    prediction = sigmoid(np.dot(trained_weights, sample))\n    \n    if prediction <= decision_boundary:\n        if y_actual == 0:\n            tn += 1\n        if y_actual == 1:\n            fn += 1\n    if prediction > decision_boundary:\n        if y_actual == 0:\n            fp += 1\n        if y_actual == 1:\n            tp += 1\n\n            \nprint('              *CONFUSION MATRIX*\\n')\nprint('             Predicted Neg     Predicted Pos')\nprint('Actual Neg        ',tn,'              ',fp)\nprint('Actual Pos        ',fn,'              ',tp)","b1762d3e":"accuracy = (tp+tn)\/(tp+tn+fp+fn)\nsensitivity = tp\/(tp+fn)\nspecificity = tn\/(tn+fp)\n\nprint('Model Accuracy:',round(accuracy,4))\nprint('\\nOut of all the patients that DO have\\nthe disease, how many got positive results?')\nprint('--> Model Sensitivity:',round(sensitivity,4))\nprint('\\nOut of all the patients that do NOT have\\nthe disease, how many got negative results?')\nprint('--> Model Specificity:',round(specificity,4))","27acb10b":"# Evaluate Model Accuracy, Specificity, Sensitivity","00eef98f":"# Train model","9b0f1378":"# Logistic Regression from Scratch\n\nThis notebook shows an implemenation of a multivariate logistic regression from \"scratch\", meaning not using higher-level machine learning libraries such as ```sklearn```. Implementing this with Python's standard library alone would be a different story...\n\nImplementing this from \"scratch\" was done to help gain intuition on how logistic regression works \"under the hood\".","0925bca0":"# Prepare data for model training","a874cb9d":"## Explore feature relationships","e0d8d35e":"# Predict on the test set","11306d29":"# Prepare confusion matrix","5ba53773":"# Define logistic regression functions","713ba8c6":"# Explore dataset"}}