{"cell_type":{"1856e294":"code","7b883b1d":"code","a229cc66":"code","f205163b":"code","5bfe2534":"code","6fb7f13c":"code","9d818773":"code","cea88315":"code","bd5ad344":"code","1ab828de":"code","0e3922dc":"code","f5be6620":"code","78a1a4b9":"code","ca75dd89":"code","41fd934b":"code","78057aa3":"code","2dc0e93c":"code","cb7cb725":"code","d00ac500":"code","cf60930a":"code","82b679cf":"code","d37ec87e":"code","55f0f2b3":"code","ad4173bf":"code","0aef85bb":"code","feabb99b":"code","aa89d12d":"code","a8dce6f2":"code","4df32097":"code","e3616c13":"code","0c7943c2":"code","5085e7d9":"code","29864e0c":"code","ab8f7792":"code","780b4eb1":"code","ea0d9c77":"code","e8c36a50":"code","a175c6c5":"code","78982297":"code","0a476ded":"code","9042c290":"code","46e509cf":"code","f3e34378":"code","36843298":"code","d9e3acb7":"markdown","74f95e51":"markdown","fdfca9eb":"markdown","3693c7f4":"markdown","a3ff32c2":"markdown","e8bc0670":"markdown","124aaa78":"markdown","8ec50e7f":"markdown","2c9d0125":"markdown","381120ba":"markdown","2fb35bca":"markdown","4d28391b":"markdown","4eff6d58":"markdown","dc908327":"markdown","68a21fb1":"markdown","3432e946":"markdown","27f04401":"markdown","8440055b":"markdown","a9b98415":"markdown","786165ef":"markdown","5ff33b6b":"markdown","4688a69e":"markdown"},"source":{"1856e294":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7b883b1d":"data  = pd.read_csv('..\/input\/Sales_Transactions_Dataset_Weekly.csv')","a229cc66":"data.head()","f205163b":"data.describe()","5bfe2534":"data","6fb7f13c":"data.isnull().values.any()","9d818773":"data_norm = data.copy()\n\ndata_norm[['Normalized {}'.format(i) for i in range(0,52)]].head()","cea88315":"data_norm = data_norm[['Normalized {}'.format(i) for i in range(0,52)]]","bd5ad344":"data_norm.head()","1ab828de":"data_norm.diff(axis=1).head()","0e3922dc":"data_norm_diff = data_norm.diff(axis=1).drop('Normalized 0', axis=1).copy()","f5be6620":"data_norm_diff.head()","78a1a4b9":"data_norm_diff.head()","ca75dd89":"data_norm_diff_prod1 =  data_norm_diff.values - data_norm_diff.values[0,:]","41fd934b":"data_norm_diff_prod1","78057aa3":"data_norm_diff_prod1_sum = (data_norm_diff_prod1**2).sum(axis=1)\ndata_norm_diff_prod1_sum.shape","2dc0e93c":"import seaborn as sb\nsb.jointplot(x = np.arange(0,811,1), \n             y = data_norm_diff_prod1_sum,\n            kind='scatter')\n#plt.scatter(range(0,811),data_norm_diff_prod1_sum)","cb7cb725":"prod1_velocities = pd.DataFrame(data_norm_diff_prod1_sum**2, columns=[\"Vel_total_diff\"])","d00ac500":"prod1_velocities.sort_values(by=\"Vel_total_diff\")","cf60930a":"def getWeeklyDiffs(products_sales_table):\n    \n    return products_sales_table.diff(axis=1).drop(products_sales_table.columns[0], axis=1).copy()\n\ndef getProductErrors(product_index, products_diffs):\n    \n    return products_diffs - products_diffs.iloc[product_index]\n    \ndef getTotalSquaredError(per_product_error):\n    \n    return pd.DataFrame((per_product_error**2).sum(axis=1), columns=[\"Total Error\"])\n    \ndef makeProductVelErrorMatrix(products_diffs, nproducts):\n    \n    product_error_matrix = pd.DataFrame()\n    \n    for i in range(0,nproducts):\n    \n        product_errors_table = getProductErrors(i, product_diffs)\n        \n        product_errors_sumsq = getTotalSquaredError(product_errors_table)\n        \n        product_error_matrix[i] = product_errors_sumsq\n        \n    return product_error_matrix\n        \n        \n    ","82b679cf":"product_diffs  = getWeeklyDiffs(data_norm)","d37ec87e":"error_matrix = makeProductVelErrorMatrix(product_diffs, 811)\n","55f0f2b3":"\nplt.figure(figsize=(15,15))\n\nsb.heatmap(error_matrix, \n           square=True)\n","ad4173bf":"def getTotalSquaredError(per_product_error, signed = True):\n    \n    if signed == False:\n        return pd.DataFrame((per_product_error**2).sum(axis=1), columns=[\"Total Error\"])\n    else:\n        return pd.DataFrame((per_product_error).sum(axis=1)**2, columns=[\"Total Error\"])","0aef85bb":"error_matrix_signed = makeProductVelErrorMatrix(product_diffs, 811)\n\nplt.figure(figsize=(15,15))\n\nsb.heatmap(error_matrix_signed, \n           square=True)","feabb99b":"from sklearn.decomposition import PCA","aa89d12d":"pca = PCA(n_components=3)","a8dce6f2":"pca_data_norm = PCA(n_components=2)","4df32097":"pca_data_norm.fit_transform(data_norm.T)","e3616c13":"print(pca_data_norm.explained_variance_ratio_)\nprint(pca_data_norm.explained_variance_ratio_.sum())","0c7943c2":"def determineNComponents(data, variance_threshold=0.90):\n    \n    n_components = 0\n    sum_explained_variance = 0\n    sum_list = []\n    \n    while sum_explained_variance < variance_threshold:\n        n_components += 1\n        pca_data_norm = PCA(n_components=n_components)\n        pca_data_norm.fit_transform(data)\n        sum_explained_variance = pca_data_norm.explained_variance_ratio_.sum()\n        \n        sum_list.append(sum_explained_variance)\n        \n    plt.scatter(np.arange(1,n_components+1,1),\n               sum_list)\n    plt.xlabel(\"Number of PCA Components\")\n    plt.ylabel(\"Total explained variance ratio\")\n    \n    print(\"At least {} components needed to explain {}% of the variance.\".format(n_components,variance_threshold*100))\n                \n    return n_components\n\nmin_components  = determineNComponents(data_norm.T)","5085e7d9":"pca_data_norm = PCA(n_components=2)\npca_data_norm.fit_transform(data_norm.T)","29864e0c":"sb.jointplot(x=pca_data_norm.components_[0,:], \n             y=pca_data_norm.components_[1,:],\n            kind='hex')\nsb.jointplot(x=pca_data_norm.components_[0,:], \n             y=pca_data_norm.components_[1,:],\n            kind='scatter')","ab8f7792":"components_data_norm = pca_data_norm.components_","780b4eb1":"from sklearn.cluster import KMeans  ","ea0d9c77":"kmeans = KMeans(n_clusters=min_components)  \nkmeans.fit(data_norm)  ","e8c36a50":"print(kmeans.cluster_centers_)  ","a175c6c5":"plt.scatter(x= components_data_norm[0,:],\n            y= components_data_norm[1,:],\n            c=kmeans.labels_, \n            cmap='rainbow') \n\nplt.xlabel(\"1st Princiapl Component\")\nplt.xlabel(\"2nd Princiapl Component\")","78982297":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)","0a476ded":"components_tsne = tsne.fit_transform(data_norm)","9042c290":"plt.scatter(x= components_tsne[:,0],\n            y= components_tsne[:,1],\n            cmap='rainbow') ","46e509cf":"kmeans_tsne = KMeans(n_clusters=3, random_state=42)  \nkmeans_tsne.fit(data_norm)  ","f3e34378":"plt.scatter(x    = components_tsne[:,0],\n            y    = components_tsne[:,1],\n            cmap = 'rainbow',\n            c    = kmeans_tsne.labels_) ","36843298":"kmeans_tsne = KMeans(n_clusters=42, random_state=42)  \nkmeans_tsne.fit(data_norm)  \n\nplt.scatter(x    = components_tsne[:,0],\n            y    = components_tsne[:,1],\n            cmap = 'rainbow',\n            c    = kmeans_tsne.labels_) ","d9e3acb7":"# Import data","74f95e51":"This new error matrix shows each product's net change in normalized sales, relative to the other products, over the whole year. \nIn other words, darker colors indicate products that, over the course of a year, tended to vary together, in terms of sales increases or decreases. Brighter colors indicate products that didn't tend to vary together.\n\nWhile still speculative, there seems to be a cluster of products (#1-200) that make up a \"dark bloc\" in the upper left of the matrix--- these products appear to be more closely related with each other. There is another, larger block, product #s 200-811, that also appear related, but the trend seems less uniform than with the smaller \u3002\n \nThis probably just indicates that the products, in the order they are given, are already somehow pre-catgorized. We would probably want to have more information about these categories before acting. Are they food vs. non-food items? Generic vs. brand-name?\n\nIn general, we can use a fine-tuned form of the method above, to determine perhaps smaller groupings of similar products. A similar approach can be applied on a per-month basis.\nWe could haven try clustering products based on the first 6 months of the year, then use the latter 6 months as a test case. It would be nice to have at least another year of data though.","fdfca9eb":"Again we have somewhat of a mess. It's difficult to say whether the tSNE projection or the PCA projection better reflects the cluster assignments.\nIt would be up to an independent validation to say if these cluster choices are realistic. Perhaps a comparison with the actual human-assigned product categories? (Presumably such data is available...)\nAlternatively the clusters could be used to inform hypothetical restocking orders for the coming weeks or months. One could then ask what affect on costs such a bundling strategy would have had. \n\nOf course, we could do such a test with the present data: cluster based on one half, test with the latter half, but this could prove challenging. \n    Any subset of the present data (1 calendar year) would have us training and testing on different seasons. More data is desired... at the very least, two calendar years.","3693c7f4":"The difficulty so far is that PCA is not able to squeeze much variance into only 2 components, or even 3.\nThat's a problem because 3 is the most that we mere mortals can plot.\n\nIf we take a look at the variance ratio for the 1st component, 8.65%--- that implies that even though we don't get a nice clean dimension reduction, we can still do better than 811 dimensions.\nLet's to a quick test, to see how many components we need to explain an arbitrary 90% of the variance:","a3ff32c2":"Using the sum of squared errors, there are not any readily discernible groupings. At least none we can see by eye. But then again, since these are the sumsof squares errors over the year, what we're really measuring is each products volatility throughout the year. Let's say, just for example, two products have the same exact average monthly sales rate. But one of them fluctuates more than the other, on a weekly basis. We'd be correct in purchasing together, if we had a monthly restocking scheddule. However the summed squared error score would tell us the opposite.\n\nTHus it might be better to consider the sales \"displacement\". In other words, we sum the signed errors over whatever time period we're interested in. Product pairs that have similar sales trajectories over that time period will thus have lower errors, as we want-- even if they fluctuate more on smaller timescales.\n\nWe'll need to write a slightly modified version of our 'getTotalSquaredError' function, so that it takes the signed error by default.","e8bc0670":"What if we use instead the sum of signed errors? This would rate products according how much their per-week sales changed over the course of the whole year.\nIn other words, two products both having a net sales gain of 10% of the whole year would be rated as similar. (The same would be true if they both had a net sales loss of 10%)","124aaa78":"It seems like this particular data set doesn't have any missing values. That's not to say there isn't any noise in the data, just that none of the elements are undefined.\n\nWe may have to exclude other data points later, depending on the exact goals of the analysis. I.e. what do we do with products which have flat sales throughout the year?","8ec50e7f":"No obvious indications of bad or missing values here. We could just have a look at the whole frame, since it's just 811 rows, but that's not really a scaleable approach. \nWhat we're mainly looking for are null values, like NaN, etc. Pandas lets us search those via the 'isnull()' method.\n","2c9d0125":"None of the techniques applied so far give us a desirable degree of clustering among the proudcts. \nWhile k-means is happy to assign products to clusters for us, the results can often be nonsensical.\n\nNot that this should be surprising. We already know that only 2 PCA components is insufficent. Thus even if the k-Means clustering worked well, such a PCA scatter plot may not show as much.\n\nGetting a usable result my come down to the dimensionality reduction technique, not necessarily the clustering technique.\nOther reprojection techniques like \"Independent Component Analysis\" (ICA), \"Non-negative Matrix Factorization\" (NMF), \"t-Distributed Stochastic Neighbor Embedding\" (t-SNE)\ncould also be tried, but there's no obvious indication that they would work better than PCA in this case. In my understanding, ICA in fact doesn't reduce dimensionality, rather it maximizes the independence between dimensions.\n\nFor the last experiment, I'll just do a quick demo of how TSNE could be applied. A major drawback of TSNE though is that it becomes harder to explain the physical meanin behind the output.","381120ba":"# Feature selection\nHaving had a look at the columns above, we see that there's product counts per week as well normalized sales. Without making any assumptions on the types of products in the list, we don't know that the per-item sales counts are a valid way to compare products. Some products tend to be purchased several at a time, while others only one at a time. We'll consider that their relative variations are more important, and keep only the normalized columns for now.\n\nNOTE: An alternative way to look at this problem, is to think of the weeks of the year as the data points, and the products as features. We may have to give this a try later.","2fb35bca":"At least subjectively, it looks like there is more structure preserved in this 2D TSNE projection than in the top 2 PCA dimensions.\n(We have to be careful though, as tSNE can sometimes induce artificial structure.)\nLet's do a k-means clustering, adopting a rather hasty n-clusters of 3, just by visual inspection.","4d28391b":"# Import packages","4eff6d58":"# Experiment with PCA\n\nNow that I've satisfied some curiosity, time to test out a more conventional approach.\n\n PCA is often used to reproject\/compress data, so that it can be readily visualized. The idea is to find the eigenvectors of the covariance matrix--- if we order these vectors by their eigenvalues, we hope to find a set of convenient principal vectors. You can think of it has finding fixed \"templates\" that explain common trends in your data. However it only works well if there is some redundancy among your features. It's common practice to try and reduce the dimensionality down to 3 or 2, for easy visualization. There are cases though where this doesn't quite make sense, such as when the first 2 or 3 components don't explain a large fraction of the variance in your data.\n\nWe'll do a quick test, compressing as much variance as we can into 2 components.","dc908327":"We can compress our data from 811 to 42 dimensions, and still explain 99% of the variance. \nIf we were dead-set getting a quick K-Means working model up and running, we could use this number as an intial guess of the number of K-means clusters.\nLet's go ahead and try that.\n\nFirst run PCA again with the optimal number of components we just determined. Then run K-means on a scatter plot of the first two PCA components.\n","68a21fb1":"Next we drop the now nonsense first column- we don't have data before the first week of the year, so the first difference is undefined.","3432e946":"## Example relatedness test for Product 1\nHaving gotten the differences for all of the products, accross the whole year, now we can see how each product varied which the others on a weekly basis.","27f04401":"Let's plot the \"errors\" for Product 1, relative to all of the other products.\nWe'll prefer the 'jointplot' function in 'seaborn', since it lets us easily make hexbin density plots.","8440055b":"### Conclusions:\n    \n* It may be possible to cluster products based on their net sales change over the year.\n* This particular dataset seems difficult to compress to a human-readable number of dimensions\n* PCA indicates we need at elast 42 dimensions to explain 90% of the total variance\n* Applying k-means (with 4 clusters) on the top 2 PCA dimensions produces wildly overlapping clusters.\n* Reducing dimensionality to 2 with t-SNE certainly gives a plot with more apparent structure, but it's difficult to determine if this is real are a tSNE artifact.\n\n### Possible future work:\n\n* Test the K-means clustering results by experiment: Use the assigned clusters to place virtual restocking orders throughout the coming year. Assess how such orders would have affected bottom line, if implemented.\n* Approach the problem first as a sales prediction problem- maybe using the first half of the year to train, and the latter quarter to test, etc.\n* It would be great to have more than a single year of data! Difficult to really model seasonal variations with only one example per calendar week.\n* Any of the methods used here, even if implemented very well, will need to be considerably refined according to actual restocking criteria: period, volume, price limits, etc.\n* Try categorizing by weeks, instead of items: cluster weeks with similar sales profiles, then recover the top selling items for those clusters. These top selling items can be integrated into the restocking strategy for those calendar weeks. (But we again come back to the data time-span limitation.)\n","a9b98415":"# Weekly sales differences (\"velocities\")\nTo understand of products are related, we probably want to know if their sales vary together week to week. \nTo approach this question, it might be helpful to calculate the \"sales velocites\", or the difference matrix showing how much the sales went up or down in each week. The assumption is that products with similar sales fluctuations are similar, and should be restocked around the same time.\n\nNOTE: In reality, this should be subject to constraints on the restock-order volume, and frequency.","786165ef":"The cluster choices here appear reasonable, but what if we choose 42 clusters (following our PCA explained variance test), as before?","5ff33b6b":"# Have a look","4688a69e":"# General approach\n\nFirst we'll put together a rough method for finding groups of products that are more related to one another.   \nIn other words, we want to know which products have similar sales patterns, even if they're from completely different categories. \nAs for the time scale: to start with, we'll just look at the overall pattern over the year for which we have data. A potential challenge is that with such a limited data set, we only have one example per calendar day. It may be hard to separate seasonal, monthly, weekly patterns from noise."}}