{"cell_type":{"b0495e38":"code","00b2c01a":"code","1c86bf01":"code","2307613f":"code","d8464ead":"code","1f49ed3e":"code","24809ece":"code","a07a1f3a":"code","7b02667e":"code","b9decbb0":"code","805d5953":"code","27d135fd":"code","1b2e972d":"code","99bdd591":"code","3e8ae824":"code","3c6ed4cd":"code","36856c6b":"markdown","e8b7b6c2":"markdown","0886572b":"markdown","f6ee2f0e":"markdown","fd583cee":"markdown","78a54371":"markdown","9963d3fd":"markdown"},"source":{"b0495e38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00b2c01a":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer,  SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score,roc_auc_score \nfrom sklearn.linear_model import Perceptron,SGDClassifier,LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.ensemble import  VotingClassifier, RandomForestClassifier\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1c86bf01":"test=pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","2307613f":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()\n","d8464ead":"train.isna().sum()","1f49ed3e":"columns = ['Age', 'Fare']\n\nfor feature in columns:\n    g = sns.kdeplot(train[feature][(train[\"Survived\"] == 0) & (train[feature].notnull())], color=\"Red\", shade = True)\n    g = sns.kdeplot(train[feature][(train[\"Survived\"] == 1) & (train[feature].notnull())], ax =g, color=\"Blue\", shade= True)\n    g.set_xlabel(feature)\n    g.set_ylabel(\"Frequency\")\n    g = g.legend([\"Not Survived\",\"Survived\"])\n    plt.figure()\n    ","24809ece":"train[\"Sex\"]","a07a1f3a":"categorical = ['Pclass','Sex','SibSp','Parch', 'Embarked']\n       \n\nfor feature in categorical:\n    g = sns.barplot(x=feature,y=\"Survived\",data=train)\n    g = g.set_ylabel(\"Survival Probability\")\n    plt.figure()","7b02667e":"y = train['Survived']\nX=train.drop('Survived',axis='columns')\n\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.33, random_state=42)","b9decbb0":"# use the first letter on the cabin name as a cabin nuber feature\ndef cabine_edit(X):\n    X=X.fillna('U')\n    X=X.applymap(lambda x: x[0])\n    X=X.applymap(lambda x: x.replace('T', 'U'))\n    return X\n\ndef name_edit(X):\n    common = [\"Mr.\",\"Miss.\",\"Mrs.\",\"Master.\",\"Dr.\"]\n    X=X.applymap(lambda x: x[1])\n    X=X.applymap(lambda x: x if x in common else 'Rare')\n    return X\n\n# pipe for numeric features, fill Nan and scale\nnumer_pipe=make_pipeline(IterativeImputer(max_iter=10,random_state=0),\n                      StandardScaler())\n\n# wrap the cabin_edin func to fit into pipline\ncabine_pipe=make_pipeline(FunctionTransformer(cabine_edit),\n                          OrdinalEncoder())\n\n# fill missing values and  one hot encode\nEmbarked_pipe=make_pipeline(SimpleImputer(missing_values=np.nan,\n                            strategy='most_frequent')\n                            ,OneHotEncoder())\n\n\n# fill missing values and  one hot encode\nName_pipe=make_pipeline(FunctionTransformer(name_edit)\n                            ,OneHotEncoder())\n\n\ncolumn_trans = make_column_transformer(\n    (numer_pipe, ['Age','Fare']),\n    (OneHotEncoder(), ['Sex']),\n    (Embarked_pipe, ['Embarked']),\n    (cabine_pipe, ['Cabin']),\n    (Name_pipe,['Name']),\n    ('drop',['PassengerId','Ticket']),\n    remainder='passthrough')","805d5953":"sgd = SGDClassifier(max_iter=5, tol=None)\nrandom_forest = RandomForestClassifier(n_estimators=100)\nlogreg = LogisticRegression()\nknn = KNeighborsClassifier(n_neighbors = 5)\ngaussian = GaussianNB()\nperceptron = Perceptron()\nlinear_svc = LinearSVC(dual=False)\ndecision_tree = DecisionTreeClassifier()\n\nmodels= [sgd,random_forest,logreg,knn,gaussian,perceptron,\n        linear_svc,decision_tree]\n\n\nfor model in models:\n    print(model)\n    pipe=make_pipeline(column_trans,model)\n    pipe.fit(X_train,y_train)\n    y_pred = pipe.predict(X_test)\n    \n    print(\"cross validation: {:.2f}\".format(cross_val_score(pipe,X_train,y_train,cv=5,scoring='accuracy').mean()))\n    print(\"Acuraccy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n    print(\"F1: {:.2f}\".format(f1_score(y_test, y_pred)))\n    print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred)))\n    print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred)))\n    print(\"ROC-AUC-Score: {:.2f}\".format(roc_auc_score(y_test, y_pred)))\n    print(\"\")","27d135fd":"eclf1 = VotingClassifier(estimators=[\n ('lr', linear_svc),\n ('random_forest', random_forest),\n ('logreg', logreg)],\n  voting='hard')\n\npipe=make_pipeline(column_trans,eclf1)\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\n\nprint(\"cross validation: {:.2f}\".format(cross_val_score(pipe,X_train,y_train,cv=5,scoring='accuracy').mean()))\nprint(\"Acuraccy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1: {:.2f}\".format(f1_score(y_test, y_pred)))\nprint(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred)))\nprint(\"ROC-AUC-Score: {:.2f}\".format(roc_auc_score(y_test, y_pred)))\nprint(\"\")","1b2e972d":"params = {'C' : np.arange(0.01,100,10)}\nclf =  GridSearchCV(linear_svc, params)\npipe=make_pipeline(column_trans,clf)\npipe.fit(X_train,y_train)\ny_pred = pipe.predict(X_test)\nprint(\"cross validation: {:.2f}\".format(cross_val_score(pipe,X_train,y_train,cv=5,scoring='accuracy').mean()))\nprint(\"Acuraccy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1: {:.2f}\".format(f1_score(y_test, y_pred)))\nprint(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred)))\nprint(\"ROC-AUC-Score: {:.2f}\".format(roc_auc_score(y_test, y_pred)))\nprint(\"\")","99bdd591":"def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=12))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=128,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer=\"Adam\",loss='binary_crossentropy',metrics=['accuracy'])\n    return classifier\n\n\nclassifier = KerasClassifier(build_fn = build_classifier)\nparam_grid = dict(epochs=[10, 20, 50], batch_size=[16, 25, 32])\ngrid = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy')\n\npipe=make_pipeline(column_trans,grid)\npipe.fit(X_train,y_train)\n\ny_pred=pipe.predict(X_test)\nprint(\"Acuraccy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1: {:.2f}\".format(f1_score(y_test, y_pred)))\nprint(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred)))\nprint(\"ROC-AUC-Score: {:.2f}\".format(roc_auc_score(y_test, y_pred)))\nprint(\"\")","3e8ae824":"test=pd.read_csv('..\/input\/titanic\/test.csv')\npipe.fit(X,y)\ny_pred = pipe.predict(test)","3c6ed4cd":"flat_pred = list(itertools.chain(*preds))\ntest.insert(2,column=\"Survived\",value=flat_pred)\nfile=test.drop(['Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked'],axis=1)\nfile.to_csv('Titanic_modelP_lr.csv',index=False)","36856c6b":"# **Perform Grid Search on best Model**","e8b7b6c2":"# **Fit on the complete dataset**","0886572b":"# **Trying different models**","f6ee2f0e":"# **Trying neural network**","fd583cee":"# **Build Data PreProcessing Pipeline**","78a54371":"# **Trying ensemble model**","9963d3fd":"# **Make Submission file**"}}