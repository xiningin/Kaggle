{"cell_type":{"a25ca799":"code","27b8f692":"code","2c8a1eef":"code","a7317ba7":"code","39618536":"code","7922abe9":"code","a30a832a":"code","239f67c2":"code","0823d51b":"code","c6694438":"code","5b7aed63":"code","98c6f2a9":"code","86ed654a":"code","de0888f9":"code","24f59618":"code","1b37ec6f":"code","1560da40":"code","8c512ae3":"code","a0bf2390":"code","670bb3fd":"code","df78c943":"code","617c76b7":"code","11a0b4fe":"code","547be926":"code","974f3227":"code","ffe7f5ca":"code","d9fb2d40":"code","49997e6e":"code","939c304c":"code","de1979b2":"code","f3a6ff0e":"code","60503d77":"code","8fa5896b":"code","df1320ea":"code","f0d3af70":"code","186627e2":"code","551e554a":"code","4bac56d6":"code","004b584e":"code","1b262ab9":"code","a9914f23":"code","e7fe951f":"code","470a423c":"code","6d0cd0f5":"code","f07f0de1":"code","855c6271":"code","2b794d56":"code","635da08a":"code","d4fe6765":"code","9c95337b":"code","5d40266c":"code","a7d2a275":"markdown","0d714f53":"markdown","0797c953":"markdown","f70c3901":"markdown","50bf7f1a":"markdown","a654f484":"markdown","ddff2093":"markdown","d9029f46":"markdown","7f919d1a":"markdown","20cb8945":"markdown","ea114b55":"markdown","3f6da9e6":"markdown","fa81a536":"markdown","e6910dca":"markdown","78d4d874":"markdown","1fa91f38":"markdown","99e55ffc":"markdown","6024d7c8":"markdown","5ee40643":"markdown","04766af4":"markdown","e8d39edc":"markdown","d9ec15a6":"markdown","2c016aea":"markdown","aab5f68a":"markdown","a254f8c4":"markdown","cfdbe561":"markdown","328682d2":"markdown","7ce19540":"markdown","00e8e414":"markdown"},"source":{"a25ca799":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","27b8f692":"df = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')","2c8a1eef":"#Importando as Bibliotecas\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline","a7317ba7":"#Verificando o carregamento da base\ndf.shape","39618536":"#Vari\u00e1veis e dicion\u00e1rio de dados\nVAR = [[\"BAD\", \"1 representa mal pagador e 0 para bom pagador\"],\n             [\"LOAN\", \"Montante da solicita\u00e7\u00e3o de empr\u00e9stimo\"],\n             [\"MORTDUE\", \"Valor devido da hipoteca existente\"],\n             [\"VALUE\", \"Valor da propriedade atual\"],\n             [\"REASON\", \"Raz\u00e3o da Consolida\u00e7\u00e3o da d\u00edvida\/Melhoramento da casa\"],\n             [\"JOB\", \"Seis categorias profissionais\"],\n             [\"YOJ\", \"Anos no emprego atual\"],\n             [\"DEROG\", \"N\u00famero de principais relat\u00f3rios depreciativos\"],\n             [\"DELINQ\", \"N\u00famero de linhas de cr\u00e9dito inadimplentes\"],\n             [\"CLAGE\", \"Idade da linha comercial mais antiga em meses\"],\n             [\"NINQ\", \"N\u00famero de linhas de cr\u00e9dito recentes\"],\n             [\"CLNO\", \"N\u00famero de linhas de cr\u00e9dito\"],\n             [\"DEBTINC\", \"Raz\u00e3o da d\u00edvida \/ rendimento\"]]\n            \nDF_VAR = pd.DataFrame(VAR, columns=[\"Variavel\", \"Descri\u00e7\u00e3o\"])\nDF_VAR\n","7922abe9":"# Verificando os tipos e tamanhos dos dados\ndf.info()","a30a832a":"# Conferindo amostra dos dados\ndf.sample(5).T","239f67c2":"df.update(df['JOB'].fillna('Other'))\ndf.head(20).T","0823d51b":"df = df.dropna(subset=['LOAN'])\ndf.head(20).T","c6694438":"df.shape","5b7aed63":"df.fillna(0, inplace=True)","98c6f2a9":"df.describe()","86ed654a":"df.nlargest(5, 'LOAN')[['REASON', 'LOAN']].style.hide_index()","de0888f9":"df.nlargest(5, 'MORTDUE')[['MORTDUE']].style.hide_index()","24f59618":"plt.figure(figsize=(30,6))\nplt.subplot(1,2,1)\nfig = df.LOAN.hist(bins=25)\nfig.set_title('Distribui\u00e7\u00e3o de Valores Empr\u00e9stimos (LOAN)')\nfig.set_xlabel('Valores de empr\u00e9stimos')\nfig.set_ylabel('Quantidade de empr\u00e9stimos')","1b37ec6f":"plt.figure(figsize=(30,6))\nplt.subplot(1,2,1)\nfig = df.MORTDUE.hist(bins=25)\nfig.set_title('Distribui\u00e7\u00e3o de Valores Hipotacas (MORTDUE)')\nfig.set_xlabel('Valores de hipotecas')\nfig.set_ylabel('Quantidade de hipotecas')","1560da40":"jobs = df['JOB'].dropna().unique()\nplt.figure(figsize=(18,19))\nc=1\nfor i in jobs:\n    plt.subplot(7,1,c)\n    plt.title(i)\n    df[df['JOB'] == i]['VALUE'].hist(bins=20)\n    c+=1\nplt.tight_layout() ","8c512ae3":"f, ax = plt.subplots(figsize=(25,12))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', linecolor='blue', ax=ax, lw=.7)","a0bf2390":"# Dividir a base em treino e teste\ntrain, test = train_test_split(df, test_size=0.20, random_state=42)\n\n# verificando tamanhos\ntrain.shape, test.shape","670bb3fd":"# Lista das colunas nao usadas\nremoved_cols = ['BAD','REASON','JOB']\n\n# Criar a lista das colunas de entrada\nfeats = [c for c in train.columns if c not in removed_cols]","df78c943":"# XGBoost\n# Instanciar o modelo\nxgb = XGBClassifier(n_estimators=200, n_jobs=-1, random_state=42, learning_rate=0.05)","617c76b7":"scores = cross_val_score(xgb, train[feats], train['BAD'], n_jobs=-1, cv=5)\nscores, scores.mean()","11a0b4fe":"xgb.fit(train[feats], train['BAD'])","547be926":"# Fazendo predi\u00e7\u00f5es\npreds = xgb.predict(test[feats])\npreds","974f3227":"preds","ffe7f5ca":"# Verificando o real\ntest['BAD'].head(3)","d9fb2d40":"# Medir o desempenho do modelo XGB\naccuracy_score(test['BAD'], preds)","49997e6e":"# Instanciar o modelo\nrf = RandomForestRegressor(random_state=42, n_jobs=-1)","939c304c":"#Treinando o modelo RandomForest\nrf.fit(train[feats], train['BAD'])","de1979b2":"# Fazendo previs\u00f5es em cima dos dados de teste da \u00e1rvore\npredstree = rf.predict(test[feats])","f3a6ff0e":"# Verificando as previsoes\npredstree","60503d77":"# Aplicando a metrica\nmean_squared_error(test['BAD'], predstree)**(1\/2)","8fa5896b":"# Conhecendo os valores da vari\u00e1vel JOB\ndf.JOB.unique()","df1320ea":"# Conhecendo os valores da vari\u00e1vel REASON\ndf.REASON.unique()","f0d3af70":"# Criando dummys para as vari\u00e1veis do tipo \"objeto\" (JOB e REASON)\ndf = pd.get_dummies(df, columns=['JOB','REASON'], dtype=int)","186627e2":"#Novo tamanho do dataset\ndf.shape","551e554a":"# Dividir a base em treino e valida\u00e7\u00e3o\ntrain, test = train_test_split(df, random_state=42)","4bac56d6":"# verificando tamanhos\ntrain.shape, test.shape","004b584e":"# Lista das colunas nao usadas\nremoved_cols = ['BAD']\n\n# Criar a lista das colunas de entrada\nfeats = [c for c in train.columns if c not in removed_cols]","1b262ab9":"# XGBoost\n# Instanciar o modelo\nxgb = XGBClassifier(n_estimators=200, n_jobs=-1, random_state=42, learning_rate=0.05)\nxgb.fit(train[feats], train['BAD'])\n","a9914f23":"scores = cross_val_score(xgb, train[feats], train['BAD'], n_jobs=-1, cv=5)\nscores, scores.mean()","e7fe951f":"# Fazendo predi\u00e7\u00f5es\npreds = xgb.predict(test[feats])\n","470a423c":"preds","6d0cd0f5":"# Verificando o real\ntest['BAD'].head(3)","f07f0de1":"# Medir o desempenho do modelo XGB\naccuracy_score(test['BAD'], preds)","855c6271":"# Instanciar o modelo\nrf = RandomForestRegressor(random_state=42, n_jobs=-1)","2b794d56":"# Treinando o modelo\nrf.fit(train[feats], train['BAD'])","635da08a":"# Fazendo previs\u00f5es em cima dos dados de valida\u00e7\u00e3o\npredstree = rf.predict(test[feats])","d4fe6765":"# Verificando as previsoes\npredstree\n","9c95337b":"# Verificando o real\ntest['BAD'].head(3)","5d40266c":"# Aplicando a metrica\nmean_squared_error(test['BAD'], preds)**(1\/2)","a7d2a275":"3. 2. Os 5 maiores valores de pedidos de empr\u00e9stimos","0d714f53":"1. 2. Carregando as bibliotecas:","0797c953":"3. 4. Gr\u00e1fico demonstrando a distribui\u00e7\u00e3o de valores de empr\u00e9timos pela sua quantidade[](http:\/\/)","f70c3901":"3. 7. Gr\u00e1fico de calor demonstrando a correla\u00e7\u00e3o entre vari\u00e1veis","50bf7f1a":"1. 1. Carregando os dados:","a654f484":"4. 4. Prepara\u00e7\u00e3o para feature engineering","ddff2093":"3. 5. Gr\u00e1fico demonstrando a distribui\u00e7\u00e3o dos valores de hipoteca pela sua quantidade ","d9029f46":"# 5- Conclus\u00f5es:","7f919d1a":"2. 3. Descartando linhas Nan na coluna LOAN","20cb8945":"4. 8. Usando Random Forest para treinamento e predi\u00e7\u00e3o","ea114b55":"2. 1. Conhecendo os dados","3f6da9e6":"\n![image.png](attachment:image.png)\n**Instituto de Educa\u00e7\u00e3o Superior de Bras\u00edlia**\n\n**P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia de Dados**\n\n**Data Mining e Machine Learning II**\n\n**Kamilla Matos \u2013 1931133012**","fa81a536":"# 2- TRATAMENTO DOS DADOS","e6910dca":"1. 4.  Dicion\u00e1rio de dados","78d4d874":"4. 5. Feature engineering","1fa91f38":"3. 3. Os 5 maiores valores de Hipotecas","99e55ffc":"# 1- INTRODU\u00c7\u00c3O#\nEste trabalho tem como objetivo analisar o dataset HMEQ_Data que cont\u00e9m dados sobre tomadas de empr\u00e9stimos. \nPara isso ser\u00e1 utilizado modelo estatistico que possibilite, a partir da an\u00e1lise das vari\u00e1veis existentes e de como elas influenciam a variavel resultado, \"BAD\". \n","6024d7c8":"2. 5. Preenchendo o restante dos valores Nan com 0","5ee40643":"Observando o resultado obtido para os modelos antes e depois de realizado o Feature Engineering observa-se que, para os dois modelos, a confian\u00e7a \u00e9 reduzida ap\u00f3s aumentar a quantidade de vari\u00e1veis. No caso do modelo XGB a acuracia de 91,44% foi reduzida para 91,27%, no caso do modelo Random Forest o desvio padr\u00e3o dos res\u00edduos tamb\u00e9m aumentou, de 0.25 foi para 0.29, significando aumento no tamanho dos erros.","04766af4":"4. 1. Usando XGB e Cross Validation","e8d39edc":"2. 4. Novo tamanho do dataset","d9ec15a6":"# 3- AN\u00c1LISE EXPLORAT\u00d3RIA","2c016aea":"2. 2. Imputando valores para nan na coluna JOBS","aab5f68a":"1. 3. Verificando carregamento dos dados:","a254f8c4":"4. 6. Usando XGB e Cross Validation","cfdbe561":"4. 3. Usando Random Forest para treinamento e predi\u00e7\u00e3o","328682d2":"3. 1. Fazendo an\u00e1lises estat\u00edsticas b\u00e1sicas","7ce19540":"# 4- PREVIS\u00d5ES","00e8e414":"3. 6. Gr\u00e1fico demosntrando os valores tomados em empr\u00e9stimo por Profiss\u00f5es"}}