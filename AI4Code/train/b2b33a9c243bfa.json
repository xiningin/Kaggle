{"cell_type":{"a6f13248":"code","1ec6e59f":"code","299c6827":"code","c3411236":"code","101a2534":"code","35a3dd5f":"code","bd205f67":"code","0b432099":"code","c50bceeb":"code","5882f0df":"code","20e96073":"code","9a8fe4a5":"code","aa2f9fc2":"code","1dac1bdb":"code","9e51db3a":"code","170974d2":"code","16019e0e":"code","235d207e":"code","0441b66b":"code","23ed6890":"code","f2f9c360":"code","ff31f6ae":"code","308954ab":"code","828612b4":"code","73caafa9":"code","9a2b42ba":"code","92d4abcb":"code","f7174053":"code","d4f3860a":"code","f9fdd6d3":"code","ba983e72":"code","113791e7":"markdown","2c672e88":"markdown","e39eeedb":"markdown","ae3d56d8":"markdown","a3c63701":"markdown","9a3834a9":"markdown","5a02a44c":"markdown","caa3a416":"markdown","b03fd1bd":"markdown","d64c184a":"markdown","0b36b098":"markdown","eea3baec":"markdown","bdc32246":"markdown","b899cb25":"markdown","bdef2bcc":"markdown","90db0420":"markdown","5ba8984e":"markdown","3aa1c463":"markdown","f59ae146":"markdown","81f32b38":"markdown","0d9b75f5":"markdown","e7193d20":"markdown","647cd3e6":"markdown","9b9542ce":"markdown","660f50db":"markdown","bed5e2be":"markdown","00b686bf":"markdown","9f9e60ce":"markdown","765275c4":"markdown","947d169b":"markdown","854a6e79":"markdown","a016e2be":"markdown","d7985f11":"markdown","cb975ce8":"markdown","c487eed8":"markdown","6a962731":"markdown","dd1ec590":"markdown","891e5a90":"markdown","322a084d":"markdown"},"source":{"a6f13248":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ec6e59f":"import numpy as np\nimport pandas as pd\nimport datatable as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","299c6827":"%%time\n\ntrain_filename = \"..\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntest_filename = \"..\/input\/tabular-playground-series-sep-2021\/test.csv\"\n\ntrain_orig = dt.fread(train_filename).to_pandas()\ntest_orig = dt.fread(test_filename).to_pandas()\n\ntrain_orig = train_orig.set_index('id')\ntest_orig = test_orig.set_index('id')","c3411236":"train_orig.shape","101a2534":"train_orig.info()\nprint()\ntest_orig.info()","35a3dd5f":"pd.set_option('display.max_columns', 120)\ntrain_orig.describe()","bd205f67":"train_orig.head(10)","0b432099":"train_memory_orig = train_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original training set(in MB): {}'.format(train_memory_orig))\n\ndef reduce_memory(df):\n    for col in df.columns:\n        if str(df[col].dtypes)[:5] == 'float':\n            low = df[col].min()\n            high = df[col].max()\n            if((low > np.finfo(np.float16).min) and (high < np.finfo(np.float16).max)):\n                df[col] = df[col].astype('float16')\n            elif((low > np.finfo(np.float32).min) and (high < np.finfo(np.float).max)):\n                df[col] = df[col].astype('float32')\n    return df\n\nreduce_memory(train_orig)\ntrain_memory_reduced = train_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced training set(in MB): {}'.format(train_memory_reduced))","c50bceeb":"test_memory_orig = test_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original test set(in MB): {}'.format(test_memory_orig))\n\nreduce_memory(test_orig)\ntest_memory_reduced = test_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced test set(in MB): {}'.format(test_memory_reduced))","5882f0df":"#prints the number of duplicated entries(rows)\nn_duplicates = train_orig.duplicated().sum()\nprint(\"Number of duplicated entries: {}\".format(n_duplicates))","20e96073":"claim_dist = train_orig.claim.value_counts()\ndisplay(claim_dist)","9a8fe4a5":"plt.figure(figsize = (10,6))\nclaim_dist.plot.pie(autopct = '%.1f', colors = ['powderblue', 'slateblue'])\nplt.title(\"Claim vlaue distribution pie chart\", pad = 20, fontdict = {'size' : 15, 'color' : 'darkblue', 'weight' : 'bold'})\nplt.show()","aa2f9fc2":"train_frac = train_orig.sample(frac = 0.01).reset_index(drop = True)\n#train_frac = train_orig[0:9579]\ntarget = train_frac.claim\n#txt = \"Kernel Density Estimation Plots w.r.t. the target 'claim' for {} training examples\".format(train_frac.shape[0]).center(110)\n#print(txt)\n\nc = 4\n#r_ = int(np.ceil(len(train_frac.columns)\/4))\nr = int(np.ceil(train_frac.shape[1]\/4))\n#print(r, r_)\nfig, ax = plt.subplots(nrows = r, ncols = c, figsize = (25,80))\ni = 1\nfor col in train_frac.columns:\n    plt.subplot(r, c, i)\n    ax = sns.kdeplot(train_frac[col], hue = target, fill = True, multiple = 'stack')\n    plt.xlabel(col, fontsize = 15)\n    i = i + 1\n    \nfig.tight_layout(pad = 2.0)\nfig.subplots_adjust(top = 0.97)\nplt.suptitle(\"Kernel Density Estimation Plots w.r.t. the target 'claim' for {} training examples\".format(train_frac.shape[0]), fontsize = 20)\nplt.show()\n       \n    ","1dac1bdb":"corrMat = train_frac.corr()\n\nfig, ax = plt.subplots(figsize = (20,20))\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\nmask = np.triu(np.ones_like(corrMat, dtype = bool))\nsns.heatmap(corrMat, square = True, annot = False, linewidths = 1, cmap = cmap, mask = mask)","9e51db3a":"from sklearn.model_selection import train_test_split\n\nX = train_orig.copy()\nY = X.claim\nX.drop('claim', axis = 1, inplace = True)\n\nX_train_orig, X_valid_orig, Y_train_orig, Y_valid_orig = train_test_split(X, Y, test_size = 0.2,\n                                                                         random_state = 42)\nX_test_orig = test_orig.copy()","170974d2":"missing_val_cols = X_train_orig.isnull().sum().sort_values(ascending = False)\nmissing_val_cols = missing_val_cols[missing_val_cols > 0]\nratio_of_missing = missing_val_cols \/ X_train_orig.shape[0]\nmissing = pd.concat([missing_val_cols,ratio_of_missing], axis = 1, \n                   keys = ['Count','%'])\nmissing","16019e0e":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = False)\nmy_imputer.fit(X_train_orig)\nX_train_imputed = my_imputer.transform(X_train_orig)\nX_valid_imputed = my_imputer.transform(X_valid_orig)\nX_test_imputed = my_imputer.transform(X_test_orig)","235d207e":"from sklearn.preprocessing import RobustScaler, StandardScaler\n\nrobust_scaler = RobustScaler()\nrobust_scaler.fit(X_train_imputed)\nX_train_robust = robust_scaler.transform(X_train_imputed)\nX_valid_robust = robust_scaler.transform(X_valid_imputed)\nX_test_robust = robust_scaler.transform(X_test_imputed)\n\nstandard_scaler = StandardScaler()\nstandard_scaler.fit(X_train_robust)\nX_train_scaled = standard_scaler.transform(X_train_robust)\nX_valid_scaled = standard_scaler.transform(X_valid_robust)\nX_test_scaled = standard_scaler.transform(X_test_robust)","0441b66b":"X_train_final = pd.DataFrame(X_train_scaled, index = X_train_orig.index,\n                            columns = X_train_orig.columns)\nX_valid_final = pd.DataFrame(X_valid_scaled, index = X_valid_orig.index, \n                            columns = X_valid_orig.columns)\nX_test_final = pd.DataFrame(X_test_scaled, index = X_test_orig.index, \n                           columns = X_test_orig.columns)","23ed6890":"#final training set\nX_train_final.describe()","f2f9c360":"#get a fraction of training set\ntrain_final = pd.concat([X_train_final, Y_train_orig], axis = 1)\nX_train_final_frac = train_final.sample(frac = 0.01).reset_index(drop = True)\nY_train_final_frac = X_train_final_frac.claim\nX_train_final_frac.drop('claim', axis = 1, inplace = True)\n\n#get a fraction of validation set\nvalid_final = pd.concat([X_valid_final,Y_valid_orig], axis = 1)\nX_valid_final_frac = valid_final.sample(frac = 0.01).reset_index(drop = True)\nY_valid_final_frac = X_valid_final_frac.claim\nX_valid_final_frac.drop('claim', axis = 1, inplace = True)","ff31f6ae":"from sklearn.linear_model import LogisticRegression\nimport time\nfrom sklearn.metrics import roc_auc_score\n\ndef get_score(actual, preds):\n    score = roc_auc_score(actual,preds)\n    return score\n\nlr = LogisticRegression(solver = 'sag')\n\n#Logistic Regression for entire training set\nstart_full = time.time()\nlr.fit(X_train_final,Y_train_orig)\npreds_full = lr.predict_proba(X_valid_final)[:,1]\nscore_full = get_score(Y_valid_orig,preds_full)\nprint('roc-auc for full fit: {}'.format(score_full))\nend_full = time.time()\n\n#Logistic Regression for 1\/100th of training set\nstart_frac = time.time()\nlr.fit(X_train_final_frac,Y_train_final_frac)\npreds_frac = lr.predict_proba(X_valid_final)[:,1]\nscore_frac = get_score(Y_valid_orig,preds_frac)\nprint('roc-auc score for sample fit: {}'.format(score_frac))\nend_frac = time.time()\n\nprint('full train time: {}\\nsample train time: {}'.format(end_full-start_full,end_frac-start_frac))","308954ab":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\n#Naive Bayes for entire training set\nnb_full_start = time.time()\nnb.fit(X_train_final,Y_train_orig)\nnb_full_preds = nb.predict_proba(X_valid_final)[:,1]\nnb_full_score = get_score(Y_valid_orig,nb_full_preds)\nnb_full_end = time.time()\nprint('roc-auc for full fit: {}'.format(nb_full_score))\n\n#Naive Bayes for 1\/100th of training set\nnb_frac_start = time.time()\nnb.fit(X_train_final_frac,Y_train_final_frac)\nnb_frac_preds = nb.predict_proba(X_valid_final)[:,1]\nnb_frac_score = get_score(Y_valid_orig,nb_frac_preds)\nnb_frac_end = time.time()\nprint('roc-auc for sample fit: {}'.format(nb_frac_score))\n\nprint('full train time: {}\\nsample train time: {}'.format(nb_full_end-nb_full_start,nb_frac_end-nb_frac_start))","828612b4":"from xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedKFold","73caafa9":"xgb = XGBClassifier()\n\nxgb_fit_start = time.time()\nxgb.fit(X_train_final,Y_train_orig)\nxgb_fit_end = time.time()\n\n#make predictions\nxgb_valid_preds = xgb.predict_proba(X_valid_final)[:,1]\nxgb_train_preds = xgb.predict_proba(X_train_final)[:,1]\n\n#get scores\nxgb_valid_score = get_score(Y_valid_orig,xgb_valid_preds)\nxgb_train_score = get_score(Y_train_orig,xgb_train_preds)\n\nxgb_fit_time = xgb_fit_end - xgb_fit_start\nxgb_of = xgb_train_score - xgb_valid_score","9a2b42ba":"cat = CatBoostClassifier(learning_rate = 0.1)\n\ncat_fit_start = time.time()\ncat.fit(X_train_final,Y_train_orig, verbose = False)\ncat_fit_end = time.time()\n\n#prediction\ncat_valid_preds = cat.predict_proba(pd.DataFrame(X_valid_final))[:,1]\ncat_train_preds = cat.predict_proba(pd.DataFrame(X_train_final))[:,1]\n\n#reshape distorted prediction array\ncat_valid_preds = cat_valid_preds.reshape(len(cat_valid_preds),)\ncat_train_preds = cat_train_preds.reshape(len(cat_train_preds),)\n\n#get scores\ncat_valid_score = get_score(Y_valid_orig,cat_valid_preds)\ncat_train_score = get_score(Y_train_orig,cat_train_preds)\n\ncat_fit_time = cat_fit_end - cat_fit_start\ncat_of = cat_train_score - cat_valid_score","92d4abcb":"lgbm = LGBMClassifier()\n\nlgbm_fit_start = time.time()\nlgbm.fit(X_train_final,Y_train_orig)\nlgbm_fit_end = time.time()\n\n#make predictions\nlgbm_valid_preds= lgbm.predict_proba(X_valid_final)[:,1]\nlgbm_train_preds = lgbm.predict_proba(X_train_final)[:,1]\n\n#get score\nlgbm_valid_score = get_score(Y_valid_orig,lgbm_valid_preds)\nlgbm_train_score = get_score(Y_train_orig,lgbm_train_preds)\n\nlgbm_fit_time = lgbm_fit_end - lgbm_fit_start\nlgbm_of = lgbm_train_score - lgbm_valid_score","f7174053":"models = [(xgb, 'XGBoost'), (cat, 'CatBoost'), (lgbm, 'LightGBM')]\n\nxgb_eval = {'Model' : 'XGBoost', 'Train Time' : xgb_fit_time,\n           'Train Score' : xgb_train_score, 'Val. Score' : xgb_valid_score,\n           'Overfitting' : xgb_of}\ncat_eval = {'Model' : 'CatBoost', 'Train Time' : cat_fit_time,\n           'Train Score' : cat_train_score, 'Val. Score' : cat_valid_score,\n           'Overfitting' : cat_of}\nlgbm_eval = {'Model' : 'LightGBM', 'Train Time' : lgbm_fit_time,\n           'Train Score' : lgbm_train_score, 'Val. Score' : lgbm_valid_score,\n           'Overfitting' : lgbm_of}\n\nevaluations = pd.DataFrame({'Model' : [], 'Train Time' : [],\n                            'Train Score' : [], 'Val. Score' : [],\n                           'Overfitting' : []})\nevaluations = evaluations.append([xgb_eval,cat_eval,lgbm_eval], ignore_index = True)\nevaluations.set_index('Model', inplace = True)\n\nevaluations","d4f3860a":"X_test_final.shape","f9fdd6d3":"#prediction\ncat_test_preds = cat.predict_proba(pd.DataFrame(X_test_final))[:,1]\n\n#reshape distorted prediction array\ncat_test_preds = cat_test_preds.reshape(len(cat_test_preds),)","ba983e72":"output = pd.DataFrame({'id' : X_test_final.index, 'claim' : cat_test_preds})\noutput.to_csv('submission.csv', index = False)","113791e7":"# **PROBLEM STATEMENT**\n\nWe are given the following:\n\n1. A train dataset (.csv) containing the index column (0 to n_train_examples-1), features ('f1' to 'f118') and the ground truth *claim* (0 or 1) respectively.\n2. A test dataset (.csv) containing the index column (0 to n_test_examples-1), features ('f1' to 'f118') respectively.\n\nWe are required to implement a binary-classification algorithm which predicts for each example of the test dataset, whether a customer made a claim upon an insurance policy. A '1' value means a claim was made, and '0' means a claim was not made.","2c672e88":"# **5. SUBMISSION**\n\nLet's predict the *claim* variable for test set and submit our results!","e39eeedb":"Seems that the score for simple Logistic Regression fit on a sample of the training set is pretty close to the score for the model fit on the entire training set. But we did cut off on the training time (approx. 100x faster). This might be the cause of having a well balanced dataset w.r.t. the target variable, and thus having taken only a small sample of the dataset we were able to feed much of the useful information about the dataset to our model. \n\nOf course, this may not be entirely true for more complex models and I don't recommend cutting down on the training examples in such fashion. But, for this dataset only, I believe that if we take a sample of the training set to train our models, then we can train different models for comparison and frequently tweak the hyperparameters without having to wait for long minutes everytime. However, this is just a gamble and I don't recommend adopting this technique for better results.","ae3d56d8":"**1.2 FEATURE ENGINEERING**\n\nGenerally, when we talk about feature engineering, we mean combining the existing features (or engineering new features from them for that matter). However, for this dataset, we have no knowledge about what the features are, neither their impact on the target feature. So, my opinion is that engineering new features won't help us much for this dataset.\n\nFurthermore, there is no need to classify features into different data-types (this helps later while processing the dataset) as all features are of type *float* only.","a3c63701":"We got a tiny 1% improvement in our score. However, this is far from the performance we would expect from our final model. Now let's get serious and train more reasonable models.","9a3834a9":"Seems pretty well balanced. Let's confirm this notion through a pie chart (because \"visual data is always more convincing\").","5a02a44c":"**CONCLUSION**\n\n- None of the models show signs of overfitting.\n- Out of the three, *CatBoostClassifier* gave the best results on the Validation Set.\n- *LightGBM* is the fastest among the three and took approximately 10x less time to fit the training set as compared to the next fastest model *CatBoostClassifier*.\n\nAll things considered, these are pretty decent scores by standards of basline models. If we have to pick one, I think *CatBoostClassifier* should be chosen for prediction on test set. However, if computation speedup is an important priority, then *lightGBM* can be picked for the cost of a little less roc-auc score.","caa3a416":"**4.1.2 CATBOOST CLASSIFIER**","b03fd1bd":"**4.0.0 NAIVE BAYES**\n\nAnother famous classifier is the Naive-Bayes classifier. It has the additional advantage that it is very fast for large datasets such as the one we're working on. ","d64c184a":"**1.4 CORRELATION ANALYSIS**\n\nWe noticed earlier that the relation between features and the target variable is most likely weak. To check that further, we'll make use of a correlation matrix. Also, this matrix will help us to check which features are strongly related to one another.","0b36b098":"# **2. DATA CLEANING**\n\n**2.0 DATASET SPLIT**\n\nBefore proceeding any further, it is recommended to split the dataset into a training set and a hold-out cross-validation set. This is to ensure that the model we build won't be adversely affected by data leakage.\n\n> Any feature whose value would not actually be available in practice at the time you\u2019d want to use the model to make a prediction, is a feature that can introduce leakage to your model\n\nNow, when we talk about splitting the data into train and test sets, we normally have two options:\n\n* Splitting the dataset into a training set and a test set using *train_test_split()*.\n* Using k-fold cross-validation sets.\n\nThe performance of the two techniques typically depends on the size of the dataset. \n- When we have a small or limited training dataset, then using *K fold CV* is recommended. This is because we are always looking for a method that could maximize the data that we train our model on. Also, for a small training set, *train_test_split* could lead to inconsistent predictions for the test set.\n- On the other hand, for a large dataset like the one we're given here, using *K fold* would greatly compromise on the computation speed of our model. Also, since we have a large number of training examples, using only *train_test_split* should be enough information for our model to properly learn the parameters.\n\nSo, as a conclusion to the above two points, we will use *train_test_split* for this dataset.","eea3baec":"# **IMPORT LIBRARIES**","bdc32246":"**4.1 BASELINE MODELS**\n\nOne good practice is to always train a baseline model first on the training set and observe its performance. It gives us a starting step to compare our later models.","b899cb25":"All the features in the dataset are of type *float64*, and the ground truth column, i.e. *claim* is of type *int64*.","bdef2bcc":"**Conclusion:** We can now safely say that none of the features have a strong correlation among one another, or with the target variable. This marks the end of a fruitless correlation analysis. ","90db0420":"This means that our dataset has only unique entries. Having ensured this, now we can proceed to the actual EDA for our dataset.","5ba8984e":"# **0. DATASET**","3aa1c463":"There are a total of *957919* training examples, having *118* features ranging from 'f1' to 'f118', and *1* target column, i.e. *claim* which corresponds to - whether the claim was made (1) or not (0).","f59ae146":"For a large dataset such as this one, one might often face situations where the system runs out of RAM. Thus, it would be wise to cut down on the memory usage.","81f32b38":"**0.0 LOADING DATASET**","0d9b75f5":"**4.1.1 XGB CLASSIFIER**","e7193d20":"**1.1 TARGET COLUMN**\n\nNow, let's look at how the target column *claim* is distributed throughout the dataset.","647cd3e6":"# **1. EXPLORATORY DATA ANALYSIS**\n\n**1.0 DUPLICATE REMOVAL**\n\nFirst off, there is always a possibility that our dataset is having duplicate entries. This is typically a fault of the data acquisition step.","9b9542ce":"**4.3 COMPARE PERFORMANCES**","660f50db":"**4.0 LOGISTIC REGRESSION**\n\nAs a personal observation, I want to see how a simple logistic regression model would perform on a random sample of the dataset as compared to the entire dataset. ","bed5e2be":"There are a few darker cells, which represent relatively strong correlation between the concerning features\/variables. However, even these *relatively* strong correlations have very small correlation coefficient values from a general P.O.V. To elaborate, the slider on the right depicts that the upper bound on positive correlations is approx 0.04 and the lower bound on negative correlations is approx -0.06. These two bounds are too small to declare a strong correlation between the features.\n\n**Ps:** Here, I define a *strong correlation* as one having correlation coefficient value greater than 0.6 (meaning strong positive correlation) or less than -0.6 (meaning strong negative correlation). Of course, these thresholds are subject to the author.","00b686bf":"# **Thank You For Reading!**\n\nThis was all for this notebook. In the next notebook, I worked on these baseline models and tried optimizing their performance using **Optuna**. You can find it here --> [TPS Sept: EDA & LightGBM + Optuna](https:\/\/www.kaggle.com\/jaikr18\/tps-sept-detailed-eda-lightgbm-optuna)","9f9e60ce":"# **4. MODEL FITTING AND EVALUATION**\n\nWhen we talk about a binary class classification problem, the simplest model that comes to mind is *Logistic Regression*. So, first off, we will use logistic regression as our base model for comparison. Naturally, it won't do very well on such a complex dataset, but it's always better to start from the bottom and build to the top.","765275c4":"**2.1 MISSING VALUES**\n\nAs we saw earlier, most of the features have missing values. We will take care of that now.\n\nLuckily, for the given dataset, we have only numerical features and hence, imputation will be lot more simpler. For numerical data, two most suitable imputation techniques that could be used here are *mean imputation* and *median imputation*. I will try both these techniques and compare their performance on the validation set. In the final notebook, you will only see the technique which performed better. ","947d169b":"That's a lot of plots to look at. However, at a quick glance at all the plots, there doesn't seem to be a pattern in any of the distributions w.r.t. the target variable. We will now analyse these weak relations further using a correlation matrix.","854a6e79":"**0.1 DATASET OVERVIEW**","a016e2be":"**4.1.3 LIGHT GBM**","d7985f11":">\"Perfectly balanced, as all things should be.\"","cb975ce8":"# **3. FEATURE SCALING**\n\n\n\nMany machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n\nStandardizing is a popular scaling technique that subtracts the mean from values and divides by the standard deviation, transforming the probability distribution for an input variable to a standard Gaussian (zero mean and unit variance). Standardization can become skewed or biased if the input variable contains outlier values.\n\nTo overcome this, the median and interquartile range can be used when standardizing numerical input variables, generally referred to as robust scaling.","c487eed8":"**0.2 MEMORY REDUCTION**","6a962731":"As expected, the dataset is far from standard with some features taking exponentially large values while some other taking exponentially small values. Also, most features seem to be having missing values, so we will have to take care of these things at a later point.","dd1ec590":"On passing through the Scalers, our Data Frame has now been converted to a numpy array. So, for convention, we will convert the array back to a Data Frame.","891e5a90":"Surprisingly, every feature has missing entries. However, the number of missing entries as compared to the entire dataset is quite small.","322a084d":"**1.3 DISTRIBUTION ANALYSIS**\n\nLet's see how the features are distributed w.r.t. the target variable.\n\n**NOTE:** Since we have a very large dataset, we will plot these distributions taking a small sample from the dataset. For better estimations, we will take a random sample, preferably of fraction 1\/100 of the original dataset. This will help in faster generation of plots."}}