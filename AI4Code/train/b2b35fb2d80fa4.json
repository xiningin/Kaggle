{"cell_type":{"d291c9a1":"code","01da540c":"code","94441b3f":"code","7be0c1fe":"code","643f1c34":"code","c18e0068":"code","4a820495":"code","c5584a6d":"code","bef4e20f":"code","c663a5d2":"code","a45ca84f":"code","bed3de7a":"code","608d13f9":"code","f1e07927":"code","8c43453f":"code","4c980d3b":"code","c4ef7a25":"code","2d034873":"code","acd1945f":"code","b2795aff":"code","cccf58ce":"code","646b10b0":"code","dbd02e9f":"code","a04f05ad":"code","b85b3897":"code","889c4b8d":"code","4720e8bf":"code","a7573dda":"code","da798e1f":"code","bb66d956":"code","0126330d":"code","c182ca89":"code","a61f6069":"code","061a9015":"code","1a70ec87":"code","aa9dfd46":"markdown","e0f28f1e":"markdown","8bcee708":"markdown","b10a3fc9":"markdown","e9d508f4":"markdown","f44db6e0":"markdown","396e2583":"markdown","9dd45e20":"markdown","7b8cfb6b":"markdown","9f8750ec":"markdown","c47380c9":"markdown","de76a72d":"markdown","cb9f2b6f":"markdown","90dd6d56":"markdown","a85f6ccd":"markdown","00aa8fce":"markdown","5f12d16c":"markdown","7c7b6127":"markdown","60bcfe2c":"markdown","f9a0aaf4":"markdown","bc014d20":"markdown","26fc7531":"markdown","e3d989b8":"markdown","a1dbe508":"markdown","01d079a7":"markdown","6d0d7128":"markdown","119a9354":"markdown","743193cb":"markdown","f836fc5b":"markdown","7ae8eff3":"markdown","e403f111":"markdown","9c900ec4":"markdown","85043618":"markdown","45f78b2e":"markdown"},"source":{"d291c9a1":"# Install PySpark\n!pip -q install pyspark flasgger","01da540c":"import plotly.express as px\nfrom pyspark.sql import SparkSession, functions as f\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","94441b3f":"spark = SparkSession.builder.master('local[3]').appName('BankNoteAuthentication').getOrCreate()","7be0c1fe":"df = spark.read.csv('..\/input\/bank-note-authentication-uci-data\/BankNote_Authentication.csv',header=True,inferSchema=True)","643f1c34":"df.show(5)","c18e0068":"df.printSchema()","4a820495":"df.select([f.sum(f.isnan(f.col(c)).cast('int')).alias(f'null_{c}') for c in df.columns]).show()","c5584a6d":"print(\"(\",df.count(),\",\",len(df.columns),\")\")","bef4e20f":"data = df.select('class').toPandas()\nsns.countplot(data=data,x='class')\n_=plt.title(\"Countplot\")","c663a5d2":"#Seperating the dataset according to class for easy plotting\ndata_0=df.where(f.col(\"class\")==0)\ndata_1=df.where(f.col(\"class\")==1)\n\n#KDE plots\ncols = df.columns\ncols.remove('class')\nfig,ax =plt.subplots(2,2,figsize=(8,8))\nfor feature,axes in zip(cols,ax.ravel()):\n    sns.kdeplot(data_0.select(feature).toPandas()[feature],color='blue',ax=axes)\n    sns.kdeplot(data_1.select(feature).toPandas()[feature],color='orange',ax=axes)\n    axes.set_title(feature)\nplt.tight_layout()","a45ca84f":"data = df.toPandas()\nsns.pairplot(data=data,hue='class')","bed3de7a":"#displaying the dataframe\ndf.show(3)","608d13f9":"#Import VectorAssembler\nfrom pyspark.ml.feature import VectorAssembler","f1e07927":"#'cols' is the list of feature names that we have\nprint(cols)","8c43453f":"VecAssembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n#A demo of the output of vector assembler. Later we will assemble all feature transformations in a single pipeline\nVecAssembler.transform(df).select(\"features\",\"class\").show(3)","4c980d3b":"#let's choose 0.8 as the training split length, because we have only a few rows of data\ntrain,test = df.randomSplit([0.8,0.2])","c4ef7a25":"from pyspark.ml.feature import StandardScaler\nfrom pyspark.ml import Pipeline","2d034873":"#specify input col and output col and fit the scaler\nscaler = StandardScaler(inputCol=\"features\",outputCol=\"features_scaled\")\n\n#transform the datasets using a pipeline\nfeature_pipe = Pipeline(stages=[VecAssembler,scaler]).fit(train)\ntrain = feature_pipe.transform(train)\ntest = feature_pipe.transform(test)","acd1945f":"train.select(\"features\",\"features_scaled\").show(3)","b2795aff":"from pyspark.ml.feature import PCA\nfrom pyspark.ml.functions import vector_to_array","cccf58ce":"# PCA decmposition to 2 features\npca = PCA(k=3,inputCol=\"features\",outputCol=\"pca\").fit(train)\ndata = pca.transform(train).select(\"pca\",\"class\")\ndata.show(3)","646b10b0":"data = data.withColumn(\"pca\",vector_to_array(\"pca\")).select(f.col(\"class\"),f.col(\"pca\")[0],f.col(\"pca\")[1],f.col(\"pca\")[2])\ndata.show(3)","dbd02e9f":"data = data.toPandas()","a04f05ad":"#plotting\nfig = px.scatter_3d(data, x='pca[0]', y='pca[1]', z='pca[2]',color='class',title=\"3D Scatterplot of PCA Features\",color_continuous_scale=px.colors.sequential.Viridis)\nfig.update(layout_coloraxis_showscale=False)\nfig.show()","b85b3897":"from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator","889c4b8d":"lr = LogisticRegression(featuresCol=\"features\",labelCol=\"class\",predictionCol='prediction_lr')\nparams = ParamGridBuilder().addGrid(lr.maxIter,[50,75,100,150,200]).addGrid(lr.regParam,[0,0.0001,0.001,0.01,0.1,0.5]).build()\n\nevaluator = MulticlassClassificationEvaluator(predictionCol='prediction_lr',labelCol='class',metricName='f1')\nCValidator = CrossValidator(estimator=lr,estimatorParamMaps=params,evaluator=evaluator,numFolds=5)\nCValidator_lr = CValidator.fit(train)\n\n#predictions on test data\npredictions_lr= CValidator_lr.transform(test).select('prediction_lr','class')\npredictions_lr.show(3)","4720e8bf":"data = CValidator_lr.bestModel.summary.roc.toPandas()\nsns.lineplot(data=data,x='FPR',y='TPR',color='green')\nprint(\"Area under the ROC curve:\",format(CValidator_lr.bestModel.summary.areaUnderROC,'.4f'))\nprint(\"F1 Score:\",format(evaluator.evaluate(predictions_lr),'.4f'))\nevaluator.setMetricName('accuracy')\nprint(\"Accuracy:\",format(evaluator.evaluate(predictions_lr),'.4f'))","a7573dda":"from pyspark.ml.classification import LinearSVC","da798e1f":"svc = LinearSVC(featuresCol='features_scaled',labelCol='class',predictionCol='prediction_svc')\nparams = ParamGridBuilder().addGrid(svc.maxIter,[50,100,200]).addGrid(svc.regParam,[0,0.001,1]).build()\nevaluator.setPredictionCol('prediction_svc')\nevaluator.setMetricName('f1')\n\nCValidator_svc = CrossValidator(estimator=svc,estimatorParamMaps=params,evaluator=evaluator).fit(train)","bb66d956":"predictions_svc = CValidator_svc.transform(test).select(\"prediction_svc\",\"class\")","0126330d":"print(\"f1 Score\",format(evaluator.evaluate(predictions_svc),'.4f'))\nevaluator.setMetricName('accuracy')\nprint(\"Accuracy score: \",format(evaluator.evaluate(predictions_svc),'.4f'))","c182ca89":"#Save feature pipe\n\nfeature_pipe.save('feature_pipe')\n\n#Save the model\n\nCValidator_svc.bestModel.save('bank_note_model_svc')","a61f6069":"!pip -q install streamlit","061a9015":"from pyspark.sql import SparkSession\nfrom pyspark.ml.classification import LinearSVCModel\nfrom pyspark.ml.pipeline import PipelineModel\nimport streamlit as st","1a70ec87":"#Load feature_pipe and model\n\nfeature_pipe = PipelineModel.load('feature_pipe')\nmodel = LinearSVCModel.load('bank_note_model_svc')\nspark = SparkSession.builder.master('local').appName('deployPyspark').getOrCreate()\n\ndef predict(variance,skewness,curtosis,entropy,spark):\n    schema = \"variance FLOAT, skewness FLOAT, curtosis FLOAT, entropy FLOAT\"\n    data = spark.createDataFrame([[variance,skewness,curtosis,entropy]],schema=schema)\n    data = feature_pipe.transform(data)\n    prediction = model.transform(data).select(\"prediction_svc\").collect()[0][0]\n    return \"Fake Note\" if(prediction) else \"Authentic Note\"\n    \ndef noteAuth():\n    st.title(\"Bank Note Authentication\")\n    st.markdown(\"Application for predicting the authenticity of Bank Notes\")\n\n    variance = float(st.text_input(\"Variance\", 3.6216))\n    skewness = float(st.text_input(\"Skewness\",8.6661))\n    curtosis = float(st.text_input(\"Curtosis\",-2.8073))\n    entropy = float(st.text_input(\"Entropy\",-0.44699))\n\n    if(st.button(\"Predict\")):\n        result = predict(variance,skewness,curtosis,entropy,spark)\n        st.success(f\"Prediction: {result}\")\n\nif __name__ == '__main__':\n    noteAuth()","aa9dfd46":"**Creating a Spark Session**","e0f28f1e":"**Let's print the schema of the data**","8bcee708":"**Now the vectors in 'pca' column has to be split to two to plot it**","b10a3fc9":"#### This is an example code for deployment. Please try it on a local machine.\nThe following code can be saved, say as deploy.py and run as \"streamlit run deploy.py\"","e9d508f4":"**'class' is the original label, 'prediciton' is the predicted label and 'features_scaled' represent the scaled features**\n\n**Let's plot the ROC curve and print the area under it**","f44db6e0":"**Earlier we saw in the KDE graphs that the features were nearly following a normal distribution. Hence let' use a StandardScaler to scale the features**","396e2583":"**Let's create the Feature column using VectorAssembler**","9dd45e20":"### Importing initial libraries","7b8cfb6b":"**Reading the CSV file**","9f8750ec":"**There are clear separations shown, especially for pairs of features having 'variance'. The curtosis-entropy scatterplot exhibits the lowest separation**\n\n**Let's perform a PCA analysis to bring down the features to two and plot them. Before that we have to create a single vector of features.**","c47380c9":"**Most of the features nearly follow a normal distribution. From the plots it is understood that 'variance' is a feature that can help distinguish classes the most. 'Entropy' on the other hand exhibit the same distribution for both classes.**","de76a72d":"**Both the classes have nearly equal count. Hence, the dataset has good balance**","cb9f2b6f":"#### Training - Support Vector Classifier","90dd6d56":"**Saving the model and feature pipeline**\n\n**Let's save the SVC model**","a85f6ccd":"**The dataset is very clean and has no null values present.**\n\n**Let's print the shape of the dataset**","00aa8fce":"#### Training - Logistic Regression","5f12d16c":"**Let's separate each feature by class and visualize their distribution.**","7c7b6127":"**Let's visualize a pairplot for better understanding**","60bcfe2c":"**Now that we are ready with the feature vectors, let's perform the PCA decomposition**","f9a0aaf4":"**Spark has correctly inferred the schema for this data. Hence type casting is not required.**","bc014d20":"### Data Exploration","26fc7531":"**As shown above, we have 5 columns of which the last column is the target class**","e3d989b8":"**Searching for null values in df**","a1dbe508":"**Wow! we have a nice score!**","01d079a7":"**Using PCA, we tried to reduce the number of features to 3 from 4, so that they can be visualized. As you can see, there is an excellent separation between both the classes. Let's use Logistic Classifier or Support Vector Classifier for making the model**","6d0d7128":"**Description of the data:**\n\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n\n**Class 0 is for Authentic notes and Class 1 is for Fake notes**\n\n**Objective:**\n**To build a classification model that can predict the authenticity of banknotes and deploy it using streamlit library**","119a9354":"## Bank Note Authentication - PySpark - StreamLit","743193cb":"**Let's have a look at the countplot of the classes available. As of May 26 2021, PySpark doesn't have any plotting functionality. We shall do this using other Python plotting libraries.**","f836fc5b":"## Thank You","7ae8eff3":"**Let's print the first few elements in the dataset**","e403f111":"**Let's have a look at the scaled features**","9c900ec4":"**Note that we have a features and a class column now**<br>\n**Let's use the randomSplit method to split the dataframe to train and test sets**","85043618":"## Deployment using StreamLit Library","45f78b2e":"**Let's create a 3D Scatter plot**"}}