{"cell_type":{"efdc22cf":"code","d958ea5a":"code","4daab051":"code","7330852d":"code","0de26ee1":"code","a58eae4d":"code","c99f346d":"code","3031325f":"code","74b0e583":"code","81ab40fe":"code","1aca2ed3":"code","af708adb":"code","a4f4d4d2":"markdown","43b0d498":"markdown","4b942984":"markdown"},"source":{"efdc22cf":"import numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom catboost import CatBoostClassifier\nimport pdb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import (ExtraTreesClassifier,\n                              GradientBoostingClassifier,\n                              BaggingClassifier,\n                              StackingClassifier)\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)","d958ea5a":"data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge\/')\ntraining_path = data_path \/ 'training'\nevaluation_path = data_path \/ 'evaluation'\ntest_path = data_path \/ 'test'","4daab051":"sample_sub = pd.read_csv(data_path\/'sample_submission.csv')\nsample_sub = sample_sub.set_index('output_id')\nsample_sub.head()","7330852d":"def plot_result(test_input, test_prediction,\n                input_shape):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 2, figsize=(15,15))\n    test_input = test_input.reshape(input_shape[0],input_shape[1])\n    axs[0].imshow(test_input, cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Actual Target')\n    test_prediction = test_prediction.reshape(input_shape[0],input_shape[1])\n    axs[1].imshow(test_prediction, cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Model Prediction')\n    plt.tight_layout()\n    plt.show()\n    \ndef plot_test(test_prediction, task_name):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 1, figsize=(15,15))\n    axs.imshow(test_prediction, cmap=cmap, norm=norm)\n    axs.axis('off')\n    axs.set_title(f'Test Prediction {task_name}')\n    plt.tight_layout()\n    plt.show()","0de26ee1":"# https:\/\/www.kaggle.com\/inversion\/abstraction-and-reasoning-starter-notebook\ndef flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","a58eae4d":"def get_moore_neighbours(color, cur_row, cur_col, nrows, ncols):\n    # pdb.set_trace()\n\n    if (cur_row<=0) or (cur_col>ncols-1): top = -1\n    else: top = color[cur_row-1][cur_col]\n        \n    if (cur_row>=nrows-1) or (cur_col>ncols-1): bottom = -1\n    else: bottom = color[cur_row+1][cur_col]\n        \n    if (cur_col<=0) or (cur_row>nrows-1): left = -1\n    else: left = color[cur_row][cur_col-1]\n        \n    if (cur_col>=ncols-1) or (cur_row>nrows-1): right = -1\n    else: right = color[cur_row][cur_col+1]\n        \n    return top, bottom, left, right\n\ndef get_tl_tr(color, cur_row, cur_col, nrows, ncols):\n        \n    if cur_row==0:\n        top_left = -1\n        top_right = -1\n    else:\n        if cur_col==0: top_left=-1\n        else: top_left = color[cur_row-1][cur_col-1]\n        if cur_col==ncols-1: top_right=-1\n        else: top_right = color[cur_row-1][cur_col+1]   \n        \n    return top_left, top_right\n\ndef get_vonN_neighbours(color, cur_row, cur_col, nrows, ncols):\n        \n    if cur_row==0:\n        top_left = -1\n        top_right = -1\n    else:\n        if cur_col==0: top_left=-1\n        else: top_left = color[cur_row-1][cur_col-1]\n        if cur_col==ncols-1: top_right=-1\n        else: top_right = color[cur_row-1][cur_col+1]\n        \n\n    if cur_row==nrows-1:\n        bottom_left = -1\n        bottom_right = -1\n    else:\n        \n        if cur_col==0: bottom_left=-1\n        else: bottom_left = color[cur_row+1][cur_col-1]\n        if cur_col==ncols-1: bottom_right=-1\n        else: bottom_right = color[cur_row+1][cur_col+1]       \n        \n    return top_left, top_right, bottom_left, bottom_right\n","c99f346d":"def make_features(input_color, nfeat):\n    nrows, ncols = input_color.shape\n    feat = np.zeros((nrows*ncols,nfeat))\n    cur_idx = 0\n    for i in range(nrows):\n        for j in range(ncols):\n            feat[cur_idx,0] = i\n            feat[cur_idx,1] = j\n            feat[cur_idx,2] = input_color[i][j]\n            feat[cur_idx,3:7] = get_moore_neighbours(input_color, i, j, nrows, ncols)\n            try:\n                feat[cur_idx,7] = len(np.unique(input_color[i-1,:]))\n                feat[cur_idx,8] = len(np.unique(input_color[:,j-1]))\n            except IndexError:\n                pass\n\n            feat[cur_idx,9] = len(np.unique(input_color[i,:]))\n            feat[cur_idx,10] = len(np.unique(input_color[:,j]))\n            feat[cur_idx,11] = len(np.unique(input_color[i-local_neighb:i+local_neighb,\n                                                        j-local_neighb:j+local_neighb]))\n\n            feat[cur_idx,12:16] = get_moore_neighbours(input_color, i+1, j, nrows, ncols)\n            feat[cur_idx,16:20] = get_moore_neighbours(input_color, i-1, j, nrows, ncols)\n\n            feat[cur_idx,20:24] = get_moore_neighbours(input_color, i, j+1, nrows, ncols)\n            feat[cur_idx,24:28] = get_moore_neighbours(input_color, i, j-1, nrows, ncols)\n\n            feat[cur_idx,28] = len(np.unique(feat[cur_idx,3:7]))\n            try:\n                feat[cur_idx,29] = len(np.unique(input_color[i+1,:]))\n                feat[cur_idx,30] = len(np.unique(input_color[:,j+1]))\n            except IndexError:\n                pass\n            cur_idx += 1\n        \n    return feat","3031325f":"def features(task):\n    global local_neighb, nfeat\n    mode = 'train'\n    cur_idx = 0\n    num_train_pairs = len(task[mode])\n#     total_inputs = sum([len(task[mode][i]['input'])*len(task[mode][i]['input'][0]) for i in range(num_train_pairs)])\n\n    feat, target = [], []\n    for task_num in range(num_train_pairs):\n        for a in range(3):\n            input_color = np.array(task[mode][task_num]['input'])\n            target_color = task[mode][task_num]['output']\n            if a==1:\n                input_color = np.fliplr(input_color)\n                target_color = np.fliplr(target_color)\n            if a==2:\n                input_color = np.flipud(input_color)\n                target_color = np.flipud(target_color)\n\n\n            nrows, ncols = len(task[mode][task_num]['input']), len(task[mode][task_num]['input'][0])\n\n            target_rows, target_cols = len(task[mode][task_num]['output']), len(task[mode][task_num]['output'][0])\n            \n            if (target_rows!=nrows) or (target_cols!=ncols):\n                print('Number of input rows:',nrows,'cols:',ncols)\n                print('Number of target rows:',target_rows,'cols:',target_cols)\n                not_valid=1\n                return None, None, 1\n            \n            imsize = nrows*ncols\n            offset = imsize*task_num*3 #since we are using three types of aug\n            feat.extend(make_features(input_color, nfeat))\n            target.extend(np.array(target_color).reshape(-1,))\n            cur_idx += 1\n            \n    return np.array(feat), np.array(target), 0","74b0e583":"# mode = 'eval'\nmode = 'test'\nif mode=='eval':\n    task_path = evaluation_path\nelif mode=='train':\n    task_path = training_path\nelif mode=='test':\n    task_path = test_path\n\nall_task_ids = sorted(os.listdir(task_path))\n\nnfeat = 31\nlocal_neighb = 5\nvalid_scores = {}\nmodel_accuracies = {'ens': []}\npred_taskids = []\nfor task_id in all_task_ids:\n\n    task_file = str(task_path \/ task_id)\n    with open(task_file, 'r') as f:\n        task = json.load(f)\n\n    feat, target, not_valid = features(task)\n    if not_valid:\n        print('ignoring task', task_file)\n        print()\n        not_valid = 0\n        continue\n\n    estimators = [\n                    ('xgb', XGBClassifier(n_estimators=25, n_jobs=-1)),\n                    ('extra_trees', ExtraTreesClassifier() ),\n                    ('bagging', BaggingClassifier()),\n                    ('LogisticRegression',LogisticRegression())\n                 ]\n    clf = StackingClassifier(\n        estimators=estimators, final_estimator=XGBClassifier(n_estimators=10, n_jobs=-1)\n    )\n\n\n    clf.fit(feat, target)\n\n#     training on input pairs is done.\n#     test predictions begins here\n\n    num_test_pairs = len(task['test'])\n    cur_idx = 0\n    for task_num in range(num_test_pairs):\n        input_color = np.array(task['test'][task_num]['input'])\n        nrows, ncols = len(task['test'][task_num]['input']), len(\n            task['test'][task_num]['input'][0])\n\n        feat = make_features(input_color, nfeat)\n\n        print('Made predictions for ', task_id[:-5])\n\n        preds = clf.predict(feat).reshape(nrows,ncols)\n        \n        if (mode=='train') or (mode=='eval'):\n            ens_acc = (np.array(task['test'][task_num]['output'])==preds).sum()\/(nrows*ncols)\n\n            model_accuracies['ens'].append(ens_acc)\n\n            pred_taskids.append(f'{task_id[:-5]}_{task_num}')\n\n            print('ensemble accuracy',(np.array(task['test'][task_num]['output'])==preds).sum()\/(nrows*ncols))\n            print()\n\n        preds = preds.astype(int).tolist()\n        plot_test(preds, task_id)\n        if mode=='test':\n            sample_sub.loc[f'{task_id[:-5]}_{task_num}',\n                        'output'] = flattener(preds)\n","81ab40fe":"if (mode=='train') or (mode=='eval'):\n    df = pd.DataFrame(model_accuracies, index=pred_taskids)\n    print(df.head(10))\n\n    print(df.describe())\n    for c in df.columns:\n        print(f'for {c} no. of complete tasks is', (df.loc[:, c]==1).sum())\n\n    df.to_csv('ens_acc.csv')","1aca2ed3":"sample_sub.head()","af708adb":"sample_sub.to_csv('submission.csv')","a4f4d4d2":"# Code for getting moore and von Neumann neighbors","43b0d498":"In this notebook i apply the decision tree approach on the evaluation set and provide some details about this approach.\nThe challenges of ARC Challenge:\n1. The tasks have variable sizes (ranging from 2x2 to 30x30).\n2. Each task has very few samples (on average around 2). So, supervised learning approaches particularly over parameterised \nneural networks are likely to overfit.\n3. It's not trivial to augment data to generate new samples. Most of the tasks involve abstract reasoning, so commonly used augmentation\ntechniques are not suitable.\n\nThe decision tree approach addresses some of the problems.\nI flatten the input images and use each pixel as an observation. This helps handle variable task sizes (as long as input and output are same size). Flattening the image also has the advantage of giving more number of samples for training. It's also possible to control the number of estimators, tree depth, regularization which can help fight overfitting.\n\nThe problem with flattening the image is that it loses global structure information. So, i designed some features that can capture some global information about the environment of pixel like the moore neighbours, no. of unique colors in the row and column etc.  For this approach, feature engineering is going to be very important.\n\nFor completeness sake, i also show how to stack predictions. \n","4b942984":"# Generating features. Add your features here."}}