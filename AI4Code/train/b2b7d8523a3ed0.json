{"cell_type":{"f8c01df4":"code","513b30e5":"code","436622d0":"code","5254b4e1":"code","0bc8e6e3":"code","82bf2865":"code","184590e0":"code","e19d6837":"code","240c1770":"code","e75b4940":"code","fe8aaf93":"code","428ac338":"code","c5531a20":"code","3b72ee85":"code","b55daeac":"code","d243b970":"code","7f0d8e6d":"code","bbfd47e9":"code","f58cb40c":"code","bf3ad4a6":"code","c260ed42":"code","4fa99d2d":"code","ade8581e":"code","9d7fb79d":"code","dcf7533c":"code","7aeedf83":"code","db05d50d":"code","cfa5e657":"code","81bdd0bc":"code","362f6eec":"code","fbe1d68c":"code","008f8b01":"code","5fd2bf06":"code","7b0d54b5":"markdown","79734750":"markdown","3b871bbb":"markdown"},"source":{"f8c01df4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","513b30e5":"from PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport torch as pt \nimport tensorflow as tf\nfrom torch.utils.data import Dataset, DataLoader , Subset, ConcatDataset, ChainDataset\nfrom torch.nn import functional as F\nfrom torchvision.datasets import MNIST, FashionMNIST","436622d0":"## If this give the HTTP error just run it again after 15-30 seconds. There's some server issue in the MNIST\/FMNIST directory.\n## This takes around 4-5 mins to download the data\n\n(CIFAR_train_data, CIFAR_train_labels), (CIFAR_test_data, CIFAR_test_labels) = tf.keras.datasets.cifar10.load_data()\n(MNIST_train_data, MNIST_train_labels), (MNIST_test_data, MNIST_test_labels) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")","5254b4e1":"MNIST_train_data = tf.image.resize(MNIST_train_data[...,tf.newaxis], CIFAR_train_data.shape[1:-1], method='area', preserve_aspect_ratio=False, name=None)\nMNIST_test_data = tf.image.resize(MNIST_test_data[...,tf.newaxis], CIFAR_train_data.shape[1:-1], method='area', preserve_aspect_ratio=False, name=None)","0bc8e6e3":"MNIST_train_data = np.repeat(MNIST_train_data,3,axis=-1)\nMNIST_test_data = np.repeat(MNIST_test_data,3,axis=-1)","82bf2865":"CIFAR_train_data=CIFAR_train_data\/255\nCIFAR_test_data=CIFAR_test_data\/255\nMNIST_train_data = MNIST_train_data\/255\nMNIST_test_data=MNIST_test_data\/255","184590e0":"class eulid_dist(tf.keras.layers.Layer):\n    def __init__(self, output_classes):\n        super(eulid_dist, self).__init__()\n        self.output_classes = output_classes\n        \n    def build(self, input_shape):\n        self.kernel = self.add_weight(\"kernel\",\n                                  shape=[int(input_shape[-1]),\n                                         self.output_classes],\n                                     initializer=tf.keras.initializers.HeNormal())\n        \n    def call(self, x):\n        return -tf.norm(tf.expand_dims(x,axis=-1)-self.kernel, ord='euclidean',axis=-2)","e19d6837":"class guassian_eulid_dist(tf.keras.layers.Layer):\n    def __init__(self, output_classes):\n        super(guassian_eulid_dist, self).__init__()\n        self.output_classes = output_classes\n        \n    def build(self, input_shape):\n        self.kernel = self.add_weight(\"kernel\",\n                                  shape=[int(input_shape[-1]),\n                                         self.output_classes],\n                                     initializer=tf.keras.initializers.HeNormal())\n        \n    def call(self, x):\n        return tf.math.exp(-tf.norm(tf.expand_dims(x,axis=-1)-self.kernel, ord='euclidean',axis=-2))","240c1770":"class cosine(tf.keras.layers.Layer):\n    def __init__(self, output_classes):\n        super(cosine, self).__init__()\n        self.output_classes = output_classes\n        \n    def build(self, input_shape):\n        self.kernel = self.add_weight(\"kernel\",\n                                  shape=[int(input_shape[-1]),\n                                         self.output_classes],\n                                     initializer=tf.keras.initializers.HeNormal())\n    def call(self, x):\n        kernel_norm = tf.norm(self.kernel, ord='euclidean',axis=0)\n        input_norm = tf.norm(x, ord='euclidean',axis=-1,keepdims=True)\n        \n        output = (x@self.kernel)\/(kernel_norm*input_norm+1e-9)\n        return output ","e75b4940":"import tensorflow as tf\n\n## Tensorflow model\n\n\nclass GOdin_Model():\n    def __init__(self, L2_WEIGHT_DECAY = 1e-5, similarity = 'I'):\n        rate = 0.2\n        self.inputlayer = tf.keras.layers.Input(shape=(32,32,3),name='initial_input')\n        self.conv_1 = tf.keras.layers.Conv2D(4,3,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        self.conv_2 = tf.keras.layers.Conv2D(8,3,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        self.conv_3 = tf.keras.layers.Conv2D(16,3,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        self.conv_4 = tf.keras.layers.Conv2D(32,3,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        self.conv_5 = tf.keras.layers.Conv2D(32,3,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        \n        self.pool_1 = tf.keras.layers.MaxPool2D(2)\n        self.pool_2 = tf.keras.layers.MaxPool2D(2)\n        self.flatten = tf.keras.layers.Flatten()\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.bn1b = tf.keras.layers.BatchNormalization()\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        \n        self.dense_1 = tf.keras.layers.Dense(128,activation = 'relu',\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        \n        \n        \n        self.dense_1b = tf.keras.layers.Dense(128,activation = 'relu',\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        \n        \n        \n        self.dropout_1 = tf.keras.layers.Dropout(rate)\n        self.dropout_1b = tf.keras.layers.Dropout(rate)\n        self.dropout_2 = tf.keras.layers.Dropout(rate)\n    \n        self.g_func = tf.keras.layers.Dense(1)\n        \n        self.g_norm = tf.keras.layers.BatchNormalization()\n        self.g_activation = tf.keras.layers.Activation('sigmoid')\n        \n        if similarity == \"I\":\n            self.dense_2 = tf.keras.layers.Dense(128,activation = 'relu',\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n            self.dropout_3 = tf.keras.layers.Dropout(0.2)\n            self.h_func = tf.keras.layers.Dense(10)\n        \n        elif similarity == \"E\":\n            self.dense_2 = tf.keras.layers.Dense(128)\n            self.dropout_3 = tf.keras.layers.Dropout(0)\n            self.h_func = eulid_dist(10)\n            \n        elif similarity == \"G\":\n            self.dense_2 = tf.keras.layers.Dense(128)\n            self.dropout_3 = tf.keras.layers.Dropout(0)\n            self.h_func = guassian_eulid_dist(10)\n            \n        elif similarity == 'C':\n            self.dense_2 = tf.keras.layers.Dense(128)\n            self.dropout_3 = tf.keras.layers.Dropout(0)\n            self.h_func = cosine(10)\n            \n        else:\n            assert False,\"Incorrect similarity Measure\"\n            \n        self.div = tf.keras.layers.Lambda(lambda x:tf.math.divide(x[0]+1e-9, x[1]+1e-9))\n        self.pred = tf.keras.layers.Softmax(axis=-1)\n                    \n    def get_train_model(self, return_train_model=True):\n        \n        inputs = self.inputlayer\n        x = self.conv_1(inputs)\n        x = self.conv_2(x)\n        x = self.pool_1(x)\n        x = self.conv_3(x)\n        x = self.conv_4(x)\n        x = self.pool_2(x)\n        x = self.conv_5(x)\n        x = self.flatten(x)\n        x = self.bn1(x)\n        \n        x = self.dropout_1(x)\n        x = self.dense_1(x)\n        x = self.bn1b(x)\n        \n        x = self.dropout_1b(x)\n        x = self.dense_1b(x)\n        x = self.bn2(x)\n        \n        x = self.dropout_2(x)\n        x = self.dense_2(x)\n        x = self.dropout_3(x)\n        \n        h = self.h_func(x)\n        \n        g = self.g_func(x)\n        g = self.g_norm(g)\n        g = self.g_activation(g)\n\n        pred = self.div([h,g])\n        pred = self.pred(pred)\n        \n        if return_train_model:\n            return tf.keras.Model(inputs,pred)\n        else:\n            return tf.keras.Model(inputs,[pred,x,g,h])","fe8aaf93":"tf.keras.backend.clear_session()\nnet = GOdin_Model(similarity=\"C\")\ntraining_net = net.get_train_model()\nprediction_net = net.get_train_model(return_train_model=False)\ntraining_net.summary()","428ac338":"training_net.compile(optimizer=tf.keras.optimizers.Adam(),\n                     loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])","c5531a20":"hist = training_net.fit(x=MNIST_train_data,y=MNIST_train_labels,\n                 validation_data = (MNIST_test_data,MNIST_test_labels),\n                 epochs=50,batch_size=128,\n                 verbose=2,\n                 shuffle=True,\n                 callbacks = [tf.keras.callbacks.EarlyStopping(patience=3)])","3b72ee85":"def create_pertubation(input_image,model,return_preds=False):\n    image = tf.cast(input_image, tf.float32)\n    with tf.GradientTape() as tape:\n        tape.watch(image)\n        prediction = model(image)\n        prediction = tf.math.reduce_max(prediction,axis=-1)\n    \n    if return_preds:\n        return prediction\n    else:\n        # Get the gradients of the prediction w.r.t to the input image.\n        gradient = tape.gradient(prediction, image)\n        # Get the sign of the gradients to create the perturbation\n        signed_grad = tf.sign(gradient)\n        return signed_grad","b55daeac":"epsi_list = [0.0025, 0.005, 0.01, 0.02, 0.04, 0.08]\n\nscores = []\n\nfor epsi in tqdm(epsi_list):\n    ii = CIFAR_test_data.copy()\n    grads = create_pertubation(ii,training_net)\n    ii = ii - epsi*(grads.numpy())\n    ii[ii<0]=0\n    ii[ii>1]=1\n    preds = create_pertubation(ii,training_net,return_preds=True)\n    scores.append(np.sum(preds))","d243b970":"scores,epsi_list[np.argmax(scores)]","7f0d8e6d":"perturbed_inputs = []\nepsi = epsi_list[np.argmax(scores)]\nfor ii in tqdm([MNIST_train_data,MNIST_test_data,CIFAR_test_data]):\n    grads = create_pertubation(ii,training_net)\n    ii = ii - epsi*(grads.numpy())\n    ii[ii<0]=0\n    ii[ii>1]=1\n    perturbed_inputs.append(ii)","bbfd47e9":"predictions_perturbed = prediction_net.predict(np.concatenate(perturbed_inputs),verbose=0)","f58cb40c":"predictions_perturbed[2][:50000].mean(),predictions_perturbed[2][50000:60000].mean(),predictions_perturbed[2][60000:].mean()","bf3ad4a6":"labels = np.concatenate([MNIST_train_labels,10+MNIST_test_labels,-np.ones_like(CIFAR_test_labels[:,0],dtype=np.int8)])\n\nsource = np.concatenate([np.zeros_like(MNIST_train_labels),np.ones_like(MNIST_test_labels),-np.ones_like(CIFAR_test_labels[:,0],dtype=np.int8)])\n\nentropies = -np.sum(predictions_perturbed[0]*np.log(predictions_perturbed[0]+1e-9),axis=1)\ng_s = predictions_perturbed[2]\nh_s = predictions_perturbed[3]\n\ndf_perturbed = pd.DataFrame(np.concatenate([labels[...,np.newaxis],entropies[...,np.newaxis],source[...,np.newaxis],g_s,np.max(h_s,axis=1,keepdims=True)],axis=1),columns=['labels','entropies','source','g_s','h_s'])\n\n\nmap_labels = {-1:'Mnist'}\nmap_labels.update({ii:f'Trained_{ii}' for ii in range(10)})\nmap_labels.update({ii:f'Untrained_{ii-10}' for ii in range(10,20)})\n\nsource_labels = {-1:'OoD',0:'Trained_ID',1:'Untrained_ID'}\n\ndf_perturbed['source_names'] = df_perturbed.source.astype(int).map(source_labels)\ndf_perturbed['label_names'] = df_perturbed.labels.astype(int).map(map_labels)\ndf_perturbed['g_s*e'] = df_perturbed['g_s']*df_perturbed['entropies']","c260ed42":"fig,ax=plt.subplots(1,1,figsize=(8,8))\nsns.histplot(data=df_perturbed[df_perturbed['source']!=0],x='g_s',hue='source_names',ax=ax);","4fa99d2d":"sns.jointplot(data=df_perturbed[df_perturbed['source']!=0], x=\"g_s\", y=\"entropies\", hue=\"source_names\",height=8,joint_kws={'alpha':0.7});","ade8581e":"sns.jointplot(data=df_perturbed[df_perturbed['source']!=0], x=\"g_s\", y=\"entropies\", hue=\"source_names\",height=8, kind=\"kde\");","9d7fb79d":"fig,ax=plt.subplots(1,1,figsize=(8,8))\nsns.histplot(data=df_perturbed[df_perturbed['source']!=0],x='g_s*e',hue='source_names',ax=ax,kde=True);","dcf7533c":"tf.keras.backend.clear_session()\n\nfrom numba import cuda\ncuda.select_device(0)\ncuda.close()\n\nfrom cuml.manifold import TSNE\n\nlast_layer_values_perturbed = predictions_perturbed[1]\nX_embedded_perturbed = TSNE(n_components=2, perplexity=15).fit_transform(last_layer_values_perturbed)\n\n\ndf_perturbed['first']=X_embedded_perturbed[:,0]\ndf_perturbed['second']=X_embedded_perturbed[:,1]","7aeedf83":"colours = [\"#000000\"]\ncolours_10 = [\"#E53935\",\n            \"#F4511E\",\n            \"#283593\", \n            \"#03A9F4\", \n            \"#00ACC1\",\n            \"#689F38\", \n            \"#FBC02D\",\n            \"#FB8C00\",\n            \"#7B1FA2\",\n            \"#757575\",]\n\ncolours_20 = [\"#F5B7B1\",\n                \"#EDBB99\", \n                \"#7FB3D5\",\n                \"#D6EAF8\",\n                \"#80DEEA\", \n                \"#C5E1A5\",\n                \"#FFF59D\",\n                \"#FAD7A0\",\n                \"#D2B4DE\",\n                \"#E5E7E9\",]\n\n\nhue_order = [\"Mnist\"]\nhue_order_10 =  [f'Trained_{ii}' for ii in range(10)]\nhue_order_20 = [f'Untrained_{ii-10}' for ii in range(10,20)]","db05d50d":"df = df_perturbed","cfa5e657":"sns.set(rc={'axes.facecolor':'#F9F9F9'})\n\ns = 15\nalpha = 0.8\n\nfig,ax = plt.subplots(1,1, figsize=(18,18))\nsns.scatterplot(x='first',y='second',data=df[df['source']==1], \n                hue='label_names',hue_order=hue_order_20, palette=colours_20,\n                legend='full', s=s, alpha = alpha, ax=ax);\nsns.scatterplot(x='first',y='second',data=df[df['source']==0], \n                hue='label_names',hue_order=hue_order_10, palette=colours_10,\n                legend='full', s=s, ax=ax);\nsns.scatterplot(x='first',y='second',data=df[df['source']==-1], \n                hue='label_names',hue_order=hue_order, palette=colours,\n                legend='full', s=s, alpha = alpha, ax=ax);\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([]);","81bdd0bc":"sns.set(rc={'axes.facecolor':'#F9F9F9'})\n\ns = 15\nalpha = 0.8\n\nfig,ax = plt.subplots(1,1, figsize=(18,18))\nsns.scatterplot(x='first',y='second',data=df[df['source']==1], \n                hue='label_names',hue_order=hue_order_20, palette=colours_20,\n                legend='full', s=s, alpha = alpha, ax=ax);\nsns.scatterplot(x='first',y='second',data=df[df['source']==-1], \n                hue='label_names',hue_order=hue_order, palette=colours,\n                legend='full', s=s, alpha = alpha, ax=ax);\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([]);","362f6eec":"sns.set(rc={'axes.facecolor':'#D7D7D7'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['entropies'].min(), df['entropies'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 8\nalpha = 0.8\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(20,16))\ndf.sort_values('entropies',ascending=True, inplace=True)\n\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==0], \n                hue='entropies', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha, edgecolor=None);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='entropies', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha, edgecolor=None);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='entropies', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha, edgecolor=None);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='Entropy');","fbe1d68c":"sns.set(rc={'axes.facecolor':'#BCD7FF'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['h_s'].min(), df['h_s'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 10\nalpha = 1\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(20,16))\n\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==0], \n                hue='h_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='h_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='h_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='H_value');","008f8b01":"sns.set(rc={'axes.facecolor':'#BCD7FF'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['g_s'].min(), df['g_s'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 10\nalpha = 1\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(16,12))\ndf.sort_values('g_s',ascending=True, inplace=True)\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==0], \n                hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='G_value');","5fd2bf06":"sns.set(rc={'axes.facecolor':'#BCD7FF'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['g_s'].min(), df['g_s'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 10\nalpha = 1\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(16,12))\n\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='G_value');","7b0d54b5":"## Visualizaitions without TSNE","79734750":"## TSNE Visualization","3b871bbb":"## Model"}}