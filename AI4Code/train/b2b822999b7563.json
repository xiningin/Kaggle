{"cell_type":{"f70a8705":"code","7c40e512":"code","01c70951":"code","e6635757":"code","93f36a66":"code","7db693e7":"code","72d64555":"code","3ddf6b69":"code","b18cad97":"code","1ba6b3de":"code","446420e3":"code","0154177f":"code","4b4c8e3e":"code","ed7613fc":"code","532c009e":"code","9845b0d4":"code","4dcfb8cf":"markdown","abff80f8":"markdown","e46c7c78":"markdown","5db85257":"markdown","2131b650":"markdown","d0b5d472":"markdown"},"source":{"f70a8705":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nimport seaborn as sns\nfrom sklearn.preprocessing import Normalizer, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom xgboost import XGBClassifier\n","7c40e512":"#load the dataframe and rename the columns so that it's easier to understand\n\ndf = pd.read_csv('..\/input\/spam.csv', encoding='latin-1')\ndf = df.iloc[:,:2]\ndf = pd.get_dummies(df, columns = ['v1'], drop_first = True)\ndf.rename(columns = {'v1_spam': 'Spam'}, inplace = True)\ndf.rename(columns = {'v2': 'Text'}, inplace = True)\ndf.head()","01c70951":"#check whats the balance between spam and ham \n\nsns.countplot(df['Spam'])\nplt.title('# of Spam vs Ham')\n\ndf['Spam'].value_counts()","e6635757":"def AmountUpper(x):\n    count = 0\n    for letter in x:\n        if letter.isupper():\n            count = count + 1\n            \n    return count\n\ndf['Count_Upper'] = df['Text'].apply(AmountUpper)\ndf.head()","93f36a66":"df.groupby(['Spam'])['Count_Upper'].mean().plot(kind = 'bar')\nplt.ylabel('Average Upper Letters in Text')\nplt.title('Upper Letters Spam vs Ham')","7db693e7":"#preprocessing the data\n#remove stopwords\nremoveWords = set(stopwords.words('english')+list(punctuation))\n\n\n# Use TextBlob for stemming\ndef textblob_tokenizer(str_input):\n    blob = TextBlob(str_input.lower())\n    tokens = blob.words\n    words = [token.stem() for token in tokens]\n    return words\n\n#tfidfvectorizer\nvectorizer = TfidfVectorizer(lowercase=False, tokenizer = textblob_tokenizer, stop_words=removeWords)\nX = vectorizer.fit_transform(df['Text'])\ntext_columns = vectorizer.get_feature_names()\n#X_df = pd.SparseDataFrame(X,columns = text_columns, default_fill_value=0)\n\n#X_df['Count_Upper'] = df['Count_Upper']\n\n#Normalizer Scaler - Text\nstdScaler = Normalizer()\nX = stdScaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, df['Spam'], test_size=0.25, random_state=101)","72d64555":"len(text_columns)","3ddf6b69":"#logistic Regression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nscore = model.score(X_test,y_test)\nprint('Logistic Regression')\nprint(score)","b18cad97":"#support vector machine with a grid search\ndef SVM_Model_Reports(X_train,y_train,X_test,y_test):\n    model = SVC()\n\n    parameters = [\n        {'kernel':['linear', 'poly', 'rbf'],\n         'C':[1,10,100]}\n                 ]\n\n    Grid = GridSearchCV(model, parameters, cv = 4)\n    Grid.fit(X_train,y_train)\n    means = Grid.cv_results_['mean_test_score']\n    stds = Grid.cv_results_['std_test_score']\n    params = Grid.cv_results_['params']\n    print('SVM with Grid Search')\n    for mean, std, params in zip(means, stds, params):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    \n#create the confusion matrix\n    \n    y_pred = Grid.predict(X_test)\n    _,recall,_,_ = precision_recall_fscore_support(y_test,y_pred)\n    confusion_array= confusion_matrix(y_test,y_pred)\n    confusion_df = pd.DataFrame(confusion_array, columns = ['Pred 0','Pred 1'], index = ['True 0', 'True 1'])\n    sns.heatmap(confusion_df, annot = True, cmap=\"YlGnBu\", fmt='g')\n    plt.title(f'Recall {recall}')\n    \n    print('========================================================')\n    \n    print('Classification Report')\n    print(classification_report(y_test, y_pred))","1ba6b3de":"SVM_Model_Reports(X_train,y_train,X_test,y_test)\n#recall of 87% can be improved","446420e3":"sm = SMOTE(random_state=101)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n\nSVM_Model_Reports(X_train_res,y_train_res,X_test,y_test)","0154177f":"length = X_train.shape[1]\n\n# simple early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n\nmodel = Sequential()\nmodel.add(Dense(64,input_shape=(length,) , activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])","4b4c8e3e":"history = model.fit(X_train_res, y_train_res,batch_size=32,epochs=10, callbacks=[es], verbose=1,validation_split=0.2)","ed7613fc":"y_pred = model.predict_classes(X_test)\n\nprint('Classification Report from Kera Sequential Model')\nprint(classification_report(y_test, y_pred))","532c009e":"# plot training history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","9845b0d4":"model = XGBClassifier()\nmodel.fit(X_train_res,y_train_res)\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\nprint('Classification Report from XGBoost Model')\nprint(classification_report(y_test, predictions))","4dcfb8cf":"It is interesting to see that Spam text have more upper letters, will keep this as an additional feature","abff80f8":"Programming an Sequential model using Keras package using the SMOTE data","e46c7c78":"Programmed an XGBoost Classifer to see if the recall is better","5db85257":"Conclusion: the strongest model is keras neural network\n\nKey learning: TfidfVectorizer with stop words, and stemming","2131b650":"Since the data is skewed, spam to ham ratio is not even, will program a SMOTE to even out the ratio and return the model","d0b5d472":"Logistic Regression accuracy is 95.7%, will try more models below. \nThe metric we will be tracking is more focused on recall"}}