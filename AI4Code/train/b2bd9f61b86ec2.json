{"cell_type":{"63fbb85d":"code","cdc0da74":"code","1dcf450f":"code","cf01d2a5":"code","92a013c0":"code","a6ec4f38":"code","beec45fd":"code","d27f3f07":"code","4afa0c85":"code","138c46e8":"code","7679e215":"code","2c6a18a8":"code","5e2a1d20":"code","073e9a2d":"code","425f84a1":"code","3b069a9f":"code","8aadb99e":"code","51f5caf7":"code","4d11fd9a":"code","2e8f14cd":"code","f6c3de6a":"code","16c74495":"code","62a81346":"markdown"},"source":{"63fbb85d":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom surprise import Reader, Dataset, SVD, evaluate\n\nimport warnings; warnings.simplefilter('ignore')","cdc0da74":"md = pd. read_csv('..\/input\/movies_metadata.csv')\nmd.head()","1dcf450f":"md['genres'] = md['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\nmd['year'] = pd.to_datetime(md['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)","cf01d2a5":"C = md['vote_average'].mean()\nm= md['vote_count'].quantile(0.95)\nprint(\"C is %f, and m is %d\"%(C,m))","92a013c0":"qualified = md[(md['vote_count'] >= m) & (md['vote_count'].notnull()) & (md['vote_average'].notnull())]\nqualified.shape","a6ec4f38":"def weighted_rating(x):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v\/(v+m) * R) + (m\/(m+v) * C)\nqualified = qualified[['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]\nqualified['score'] = qualified.apply(weighted_rating, axis=1)\nqualified = qualified.sort_values('score', ascending=False)\nqualified.head(10)","beec45fd":"links = pd.read_csv('..\/input\/links_small.csv')\nlinks = links[links['tmdbId'].notnull()]['tmdbId'].astype('int')\nprint (md[pd.to_numeric(md['id'], errors='coerce').isnull()])","d27f3f07":"md = md.drop([19730, 29503, 35587])\nmd['id'] = md['id'].astype('int')\n","4afa0c85":"def get_recommendations(title, cosine_sim):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]","138c46e8":"credits = pd.read_csv('..\/input\/credits.csv')\nkeywords = pd.read_csv('..\/input\/keywords.csv')\nkeywords['id'] = keywords['id'].astype('int')\ncredits['id'] = credits['id'].astype('int')\nmd = md.merge(credits, on='id')\nmd = md.merge(keywords, on='id')\nsmd1 = md[md['id'].isin(links)]\n\nfeatures = ['cast', 'crew', 'keywords']\nfor feature in features:\n    smd1[feature] = smd1[feature].apply(literal_eval)","7679e215":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan\nsmd1['director'] = smd1['crew'].apply(get_director)\nsmd1.head()","2c6a18a8":"def get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n    return []\nfeatures = ['cast', 'keywords']\nfor feature in features:\n    smd1[feature] = smd1[feature].apply(get_list)\nsmd1[['title', 'cast', 'director', 'keywords', 'genres']].head(3)","5e2a1d20":"def clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''\n# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    smd1[feature] = smd1[feature].apply(clean_data)\nsmd1['director'] = smd1['director'].apply(lambda x: [x,x, x])\nsmd1.head(3)","073e9a2d":"def create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast'])  + ' '.join(x['director']) + ' '.join(x['genres'])\nsmd1['soup'] = smd1.apply(create_soup, axis=1)\nsmd1[['title', 'cast', 'director', 'keywords', 'genres', 'soup']].head(3)","425f84a1":"count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ncount_matrix = count.fit_transform(smd1['soup'])\ncosine_sim = cosine_similarity(count_matrix, count_matrix)\n\nsmd1 = smd1.reset_index()\ntitles = smd1['title']\nindices = pd.Series(smd1.index, index=smd1['title'])\n\nindices.head()\n","3b069a9f":"get_recommendations('Toy Story',cosine_sim)","8aadb99e":"reader = Reader()\nratings = pd.read_csv('..\/input\/ratings_small.csv')\nratings.head()","51f5caf7":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\ndata.split(n_folds=5)\nsvd = SVD()\nevaluate(svd, data, measures=['RMSE', 'MAE'])","4d11fd9a":"trainset = data.build_full_trainset()\nsvd.train(trainset)\nratings[ratings['userId'] == 1]","2e8f14cd":"svd.predict(1, 302)","f6c3de6a":"def convert_int(x):\n    try:\n        return int(x)\n    except:\n        return np.nan\n    \nid_map = pd.read_csv('..\/input\/links_small.csv')[['movieId', 'tmdbId']]\nid_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)\nid_map.columns = ['movieId', 'id']\nid_map = id_map.merge(smd1[['title', 'id']], on='id').set_index('title')\nindices_map = id_map.set_index('id')\n","16c74495":"def hybrid(userId, title):\n    idx = indices[title]\n    tmdbId = id_map.loc[title]['id']\n    #print(idx)\n    movie_id = id_map.loc[title]['movieId']\n    \n    sim_scores = list(enumerate(cosine_sim[int(idx)]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:26]\n    movie_indices = [i[0] for i in sim_scores]\n    movies = smd1.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]\n    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)\n    movies = movies.sort_values('est', ascending=False)\n    return movies.head(10)\nhybrid(1, 'Avatar')\nhybrid(500, 'Avatar')","62a81346":"Content Based"}}