{"cell_type":{"2e1ec51a":"code","a7623d62":"code","6f95283a":"code","a9984dfd":"code","1fbd5211":"code","65a367d1":"code","d7c37896":"code","1dd6a4dc":"code","0063ee90":"code","d756101f":"markdown","82fd9c30":"markdown","95ba0691":"markdown","ccb9784a":"markdown","874a2718":"markdown","bade1784":"markdown","84e85fc0":"markdown","04a71cb8":"markdown"},"source":{"2e1ec51a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a7623d62":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\nsubmission=pd.read_csv('..\/input\/sample_submission.csv')","6f95283a":"x_train=train.copy()\ny_train=train['winPlacePerc']\nx_train=x_train.drop(columns=['Id','groupId','matchId','numGroups','maxPlace','winPlacePerc','damageDealt','headshotKills','roadKills','vehicleDestroys'])\nx_test=test.copy()\nx_test=x_test.drop(columns=['Id','groupId','matchId','numGroups','maxPlace','damageDealt','headshotKills','roadKills','vehicleDestroys'])\n\nx_train.sort_index(axis=1,inplace=True)\nx_test.sort_index(axis=1,inplace=True)","a9984dfd":"import xgboost as xgb\n\nmodel=xgb.XGBRegressor()\nmodel.fit(x_train,y_train)\n\ny_pred=model.predict(x_test)","1fbd5211":"from matplotlib import pyplot as plt\nfrom xgboost import plot_importance\n\nplot_importance(model)\nplt.show()","65a367d1":"import shap\n\nexplainer=shap.TreeExplainer(model)\nshap_values=explainer.shap_values(x_test)\n\nshap.summary_plot(shap_values,x_test)","d7c37896":"shap.dependence_plot('walkDistance',shap_values,x_test,interaction_index='killPlace')","1dd6a4dc":"from keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.layers import Dropout\n\nmodel1=models.Sequential()\nmodel1.add(layers.Dense(512,activation='relu',input_shape=(x_train.shape[1],)))\nmodel1.add(Dropout(0.2))\nmodel1.add(layers.Dense(512,activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(layers.Dense(1))\n\nmodel1.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n\nmodel1.fit(x_train,y_train,epochs=16,batch_size=512)\n\ny_pred_DL=model1.predict(x_test)","0063ee90":"submission['winPlacePerc']=y_pred_DL\nsubmission.to_csv('sample_submission.csv',index=False)","d756101f":"Reading the data files.","82fd9c30":"Preparing the submission file.","95ba0691":"In the simple XGBoost model, the score comes to around 0.0833. Let's try to understand the features in detail and improve the model.","ccb9784a":"Let's use Deep Learning model to predict.","874a2718":"Let's remove the 4 features with 0 feature importance and then see the improvement in accuracy score. Removing the 4 features with 0 importance doesn't alter the predictions at all. However, if I remove any more features with low importance, then the predictions get slightly worse. So keeping all the features with non-zero importance.\nEither we do hyperparameter tuning or better try Deep learning. Dataset with 4.5 million rows is huge, can try with a simple feed forward neural network to compare predictions with XGBoost.\n\nBefore getting into Deep Learning, let's use SHAP values to better understand the dataset in current model.","bade1784":"Making feature and prediction set from training and test data. Have removed only the identifiers for now.","84e85fc0":"Let's use a simple XGBoost model to do the predictions- remove the identifiers in the intial feature set. With the data formatted and in numbers, let's not spend much time in data pre-processing.\n\nThen spend more time in Permutation Importance, partial dependency plot and SHAP analysis to identify the key features and make qualitative sense of what impacts a good game performance in PUBG.","04a71cb8":"Using the simple XGBoost model to make predictions and then will use methods to understand the model and do feature engineering."}}