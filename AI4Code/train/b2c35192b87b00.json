{"cell_type":{"8171f14b":"code","8209c7e1":"code","0aa6da15":"code","ab67fac4":"code","f013c104":"code","24ce5815":"code","745975da":"code","fc7e45e8":"code","c043d92f":"code","35131c65":"code","f4e9f496":"code","2570b1bd":"code","da93aed4":"code","5d79b607":"code","6a14d84e":"code","ccb8ccc7":"code","420b342c":"code","924b1620":"code","3a6b54a8":"code","19b3fcaa":"code","81485389":"code","6b246636":"code","b3185168":"code","27c4d34a":"code","332fac3c":"code","addd4f07":"code","9c14938c":"code","df58c3c2":"code","35c59747":"code","a212e364":"code","603807ba":"code","98251b15":"code","a0fc3036":"code","da9108ad":"code","1393078c":"code","781d7094":"code","54996f38":"code","28941cda":"code","b2e9a154":"code","df231882":"code","91e7e76e":"code","5f01ac87":"code","08aa8614":"code","66c1a40f":"code","df321965":"markdown","427a18a6":"markdown","55c9d533":"markdown","5017bbc4":"markdown","bfc629e7":"markdown","6e716b5c":"markdown","70449324":"markdown","73df4fcd":"markdown","a8059667":"markdown","fec95ee8":"markdown","64191e79":"markdown","510f9b2c":"markdown","e7cad1b7":"markdown","10c2a6b1":"markdown","0a717a60":"markdown","28573bb9":"markdown","388d3a46":"markdown","0003d517":"markdown","c2308895":"markdown","c6443043":"markdown","b6f3af8d":"markdown","4dfe9b98":"markdown","4cf2631f":"markdown","63a81a39":"markdown"},"source":{"8171f14b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8209c7e1":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor, NeighborhoodComponentsAnalysis\nfrom sklearn.decomposition import PCA\nfrom lightgbm import LGBMClassifier","0aa6da15":"cancer = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf = cancer.copy()\ndf.head()","ab67fac4":"df.drop(columns=[\"id\", \"Unnamed: 32\"], axis = 1, inplace=True)","f013c104":"df.info()","24ce5815":"df.isnull().sum()","745975da":"sns.countplot(df[\"diagnosis\"], )","fc7e45e8":"le = LabelEncoder()\ndf[\"Diagnosis\"] = le.fit_transform(df[\"diagnosis\"])","c043d92f":"df.drop(columns=[\"diagnosis\"], axis=1, inplace=True)\ndf.head()","35131c65":"corr_matrix = df.corr()\nplt.figure(figsize=(15,15))\nplt.title(\"Correlation Between Features\")\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\")","f4e9f496":"threshold = 0.7\nfilter = np.abs(corr_matrix[\"Diagnosis\"]) > threshold\ncorr_features = corr_matrix.columns[filter].tolist()\nplt.title(\"CORRELATION BETWEEN FEATURES (CORR > 0.7)\")\nsns.heatmap(df[corr_features].corr(), annot=True, fmt=\".2f\")","2570b1bd":"sns.pairplot(df[corr_features], hue=\"Diagnosis\")","da93aed4":"df.hist(corr_features, figsize=(10,10));","5d79b607":"def OutliersBox(df, nameOfFeature):\n    trace0 = go.Box(y = df[nameOfFeature],\n                    name = \"All Points\",\n                    jitter = 0.3,\n                    pointpos = -1.8,\n                    boxpoints = \"all\")\n    trace1 = go.Box(y = df[nameOfFeature],\n                    name = \"Only Whiskers\",\n                    boxpoints = False)\n    trace2 = go.Box(y = df[nameOfFeature],\n                    name = \"Suspected Outliers\",\n                    boxpoints = \"suspectedoutliers\",\n                    marker = dict(color = 'rgb(8,81,156)',\n                                outliercolor = 'rgba(219, 64, 82, 0.6)', line = dict(outliercolor = 'rgba(219, 64, 82, 0.6)',\n                                                                                   outlierwidth = 2)),\n                    line = dict(color = 'rgb(8,81,156)') )\n    trace3 = go.Box(y = df[nameOfFeature],\n                    name = \"Whiskers and Outliers\",\n                    boxpoints = \"outliers\")\n    \n    data_ = [trace0, trace1, trace2, trace3]\n    layout_ = go.Layout(\n        title = \"{} Outliers\".format(nameOfFeature)\n    )\n    fig = go.Figure(data=data_, layout = layout_)\n    py.iplot(fig, filename = \"Outliers\")","6a14d84e":"OutliersBox(df, corr_features[0])","ccb8ccc7":"OutliersBox(df, corr_features[1])","420b342c":"OutliersBox(df, corr_features[2])","924b1620":"OutliersBox(df, corr_features[3])","3a6b54a8":"OutliersBox(df, corr_features[4])","19b3fcaa":"OutliersBox(df, corr_features[5])","81485389":"OutliersBox(df, corr_features[6])","6b246636":"OutliersBox(df, corr_features[7])","b3185168":"y = df.Diagnosis\nX = df.drop([\"Diagnosis\"], axis=1)\ncolumns = X.columns.tolist()","27c4d34a":"clf = LocalOutlierFactor()\ny_pred_outlier = clf.fit_predict(X)\nX_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score","332fac3c":"threshold = -1.75\nfilter_outlier = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filter_outlier].index.tolist()","addd4f07":"plt.figure(figsize=(14,8))\nplt.scatter(X.iloc[outlier_index,0], X.iloc[outlier_index,1], color=\"blue\", s=50,\n            label=\"Outliers\")\nplt.scatter(X.iloc[:,0], X.iloc[:,1], color=\"k\", s=3, label=\"Data Points\")\n\nradius = (X_score.max() - X_score) \/ (X_score.max() - X_score.min()) \noutlier_score[\"radius\"] = radius\nplt.scatter(X.iloc[:,0], X.iloc[:,1], s=1000*radius, edgecolors=\"r\", \n            facecolors=\"none\", label=\"Outlier Scores\")\nplt.legend()\nplt.show()","9c14938c":"X = X.drop(outlier_index)\ny = y.drop(outlier_index).values","df58c3c2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 3, stratify=y)","35c59747":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","a212e364":"X_train_df = pd.DataFrame(X_train_scaled, columns=columns)\nX_train_df[\"target\"] = y_train\ndata_melted_2 = pd.melt(X_train_df, id_vars=\"target\",\n                        var_name=\"features\",\n                        value_name=\"value\")\nplt.figure(figsize=(18,10))\nplt.title(\"BOX PLOT AFTER SCALING\")\nsns.boxplot(x=\"features\", y=\"value\", hue=\"target\", data=data_melted_2)\nplt.xticks(rotation=90);","603807ba":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")","98251b15":"def KNN_best_params(X_train, X_test, y_train, y_test):\n    k_range = np.arange(1,31)\n    weight = [\"uniform\", \"distance\"]\n    params = dict(n_neighbors = k_range, weights = weight)\n    \n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn, params, cv=10, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n    grid.fit(X_train, y_train)\n    \n    print(\"Best training score: {} wtih params: {}\".format(grid.best_score_,grid.best_params_))\n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(X_train, y_train)\n    y_pred_test = knn.predict(X_test)\n    y_pred_train = knn.predict(X_train)\n    \n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n    \n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n    \n    y_pred_proba = knn.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    \n    print(\"Test Score: {}, Train Score: {}\".format(acc_test,acc_train))\n    print(\"CM TEST\")\n    print(cm_test)\n    print(\"CM TRAIN\")\n    print(cm_train)\n    print(\"Precision Score\", precision_score(y_test, y_pred_test))\n    print(\"recall Score\",recall_score(y_test, y_pred_test))\n    print(\"ROC Score\", roc_auc_score(y_test, y_pred_proba))\n    plot_roc_curve(fpr, tpr, thresholds)\n    \n    return grid","a0fc3036":"grid = KNN_best_params(X_train_scaled, X_test_scaled, y_train, y_test)","da9108ad":"def lgbm_best_params(X_train, X_test, y_train, y_test):\n    lgbm_params = {\"n_estimators\" : [100,200,500,1000,2000],\n               \"subsample\" : [0.6,0.8,1.0],\n               \"max_depth\" : [5,10,15,20,25,30,35],\n               \"learning_rate\" : [0.1, 0.01, 0.02, 0.5],\n               \"min_child_samples\" : np.arange(2,50)}\n    lgbm = LGBMClassifier()\n    random = RandomizedSearchCV(lgbm, lgbm_params, cv=10, random_state=1, n_jobs=-1, verbose=2)\n    #grid = GridSearchCV(lgbm, lgbm_params, cv=10, verbose=2, n_jobs=-1)\n    random.fit(X_train, y_train)\n    \n    print(\"Best training score: {} wtih params: {}\".format(random.best_score_,random.best_params_))\n    \n    lgbm = LGBMClassifier(**random.best_params_)\n    lgbm.fit(X_train, y_train)\n    y_pred_test = lgbm.predict(X_test)\n    y_pred_train = lgbm.predict(X_train)\n    \n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n    \n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n    \n    y_pred_proba = lgbm.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    \n    print(\"Test Score: {}, Train Score: {}\".format(acc_test,acc_train))\n    print(\"CM TEST\")\n    print(cm_test)\n    print(\"CM TRAIN\")\n    print(cm_train)\n    print(\"Precision Score\", precision_score(y_test, y_pred_test))\n    print(\"recall Score\",recall_score(y_test, y_pred_test))\n    print(\"ROC Score\", roc_auc_score(y_test, y_pred_proba))\n    plot_roc_curve(fpr, tpr, thresholds)\n    \n    return grid","1393078c":"random_lgbm = lgbm_best_params(X_train_scaled, X_test_scaled, y_train, y_test)","781d7094":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","54996f38":"pca = PCA(n_components=2)\npca.fit(X_scaled)\nX_reduced_pca = pca.transform(X_scaled)","28941cda":"pca_data = pd.DataFrame(X_reduced_pca, columns=[\"p1\",\"p2\"])\npca_data[\"target\"] = y\nplt.figure(figsize=(14,8))\nsns.scatterplot(x=\"p1\", y=\"p2\", hue=\"target\", data=pca_data)","b2e9a154":"X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_reduced_pca, y, test_size = 0.20, random_state = 3, stratify=y)","df231882":"grid_pca = KNN_best_params(X_train_pca, X_test_pca, y_train_pca, y_test_pca)","91e7e76e":"nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)\nnca.fit(X_scaled, y)\nX_reduced_nca = nca.transform(X_scaled)","5f01ac87":"nca_data = pd.DataFrame(X_reduced_nca, columns=[\"p1\",\"p2\"])\nnca_data[\"target\"] = y\nplt.figure(figsize=(14,8))\nsns.scatterplot(x=\"p1\", y=\"p2\", hue=\"target\", data=nca_data)","08aa8614":"X_train_nca, X_test_nca, y_train_nca, y_test_nca = train_test_split(X_reduced_nca, y, test_size = 0.20, random_state = 3, stratify=y)","66c1a40f":"grid_nca = KNN_best_params(X_train_nca, X_test_nca, y_train_nca, y_test_nca)","df321965":"# OUTLIERS","427a18a6":"# READING DATA","55c9d533":"# SPLITTING AND SCALING DATA","5017bbc4":"We use Standard Scalar in order to scale the magnitude of the feature in a certain range. Generally, what data we get from the real world, they have a great difference between them and that have direct impact over the performance of the model. So, it\u2019s always a best practice to scale the data before processing it","bfc629e7":"# EXAMINE TARGET VARIABLE AND LABEL ENCODER","6e716b5c":"An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.","70449324":"# Missing Values\nThere is not any missing value.","73df4fcd":"# EDA","a8059667":"Above, we first gave all variables except the \"Diagnosis\" variable to the X variable and gave the variable \"Diagnosis\" to the y variable. Then we split the data into train and test data. X_train and y_train show the dependent and independent variables to be used to test the model, while X_test and y_test are used to develop the model. Test_size specifies how many of data (20%) will be used for testing. Random_state is used to see the same distinction every time we run the program. Stratify provides a balanced separation of classes in the y variable when separating.","fec95ee8":"LightGBM, short for Light Gradient Boosting Machine, is a free and open source distributed gradient boosting framework for machine learning originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.","64191e79":"Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables.","510f9b2c":"# USING NCA","e7cad1b7":"# USING KNN","10c2a6b1":"# USING LGBM ","0a717a60":"### As a result we have 0.99 accuracy score","28573bb9":"Neighbourhood components analysis is a supervised learning method for classifying multivariate data into distinct classes according to a given distance metric over the data. Functionally, it serves the same purposes as the K-nearest neighbors algorithm, and makes direct use of a related concept termed stochastic nearest neighbours.","388d3a46":"In this data set there is one object variable = diagnosis","0003d517":"\"id\" and \"Unnamed: 32\" columns are useless for project so we can drop them.","c2308895":"Principal component analysis, or PCA, is a statistical procedure that allows you to summarize the information content in large data tables by means of a smaller set of \u201csummary indices\u201d that can be more easily visualized and analyzed.","c6443043":"* K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n* K-NN algorithm assumes the similarity between the new case\/data and available cases and put the new case into the category that is most similar to the available categories.\n* K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n* K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.","b6f3af8d":"We will examine corr_features outliers.","4dfe9b98":"We will use plot_roc_curve method for the plottin ROC curve in methods.","4cf2631f":"# USING PCA","63a81a39":"### Histogram\nA histogram is a bar graph representation of a grouped data distribution. In other words, it is the transfer of data consisting of repetitive numbers to the table first, and to the chart by using the table, in other words, the graph of the data groups is displayed in rectangular columns."}}