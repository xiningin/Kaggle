{"cell_type":{"8e033cad":"code","283084cc":"code","99666f72":"code","1d807094":"code","5a209445":"code","ae6980dd":"code","0a330f9a":"code","c17053ad":"code","9747b5d8":"code","c9279fcd":"code","f59b0846":"code","1f5f99e6":"code","3dda38f8":"code","84786093":"code","d4d516c2":"code","fcb270d0":"code","f4b45f72":"code","fc2d061f":"code","1208087a":"code","4f92e693":"code","45c1f989":"code","569791d2":"code","5428dba9":"code","fb28881b":"code","a5d8259f":"code","3d1e2694":"code","c0b40f4f":"code","6133c4ab":"code","c67a9c03":"code","15ba1195":"code","9a02f429":"code","d660f94b":"code","83916057":"code","ec9ecbb3":"code","ae41175c":"code","da3c9270":"code","52bcc4a1":"code","bc8ecc35":"code","57c45b0a":"code","76f16e8b":"code","30d6354d":"code","424671b0":"code","405f4f64":"code","c810410a":"code","3616cd99":"code","3a2c8c0e":"code","a6fd0bc3":"code","48ab718e":"code","4c2ff27c":"code","5d342a98":"code","a9ca03d9":"code","edc3808e":"code","9a43e545":"code","76d24812":"code","bae62461":"code","6a32ab70":"code","5eae062b":"code","14415a87":"code","a26d75c6":"code","098a4b48":"code","4d32111c":"code","61c91d4a":"code","b972bb8d":"code","e4e63df8":"code","2d46a4b6":"code","a767e8b3":"code","2105e653":"code","5ad6f314":"markdown","c25e8a0e":"markdown","ff40da02":"markdown","c25bf0c8":"markdown","845aef03":"markdown","5e2b9a36":"markdown","afed3664":"markdown","b5c71cf5":"markdown","0b6a8e70":"markdown","1104edd9":"markdown","6e2893db":"markdown","4fcafe5c":"markdown","04a74a48":"markdown","9821f724":"markdown","7bb5d122":"markdown"},"source":{"8e033cad":"# Importing Libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, classification_report, confusion_matrix","283084cc":"# Setting the numpy random seed\n\nnp.random.seed(37)","99666f72":"# Loading Dataset\n\ndf = pd.read_csv('..\/input\/attrition_data.csv')\nprint('Dataframe shape: ', df.shape)","1d807094":"df.head()","5a209445":"# Checking for missing values\n\ndf.isnull().sum()","ae6980dd":"# Dropping irrelevant columns\n\ndf.drop(['EMP_ID', 'JOBCODE', 'TERMINATION_YEAR'], axis=1, inplace=True)\ndf.drop(df.iloc[:, -5:], axis=1, inplace=True)","0a330f9a":"df['REFERRAL_SOURCE'].fillna(df['REFERRAL_SOURCE'].mode()[0], inplace=True)","c17053ad":"df.head()","9747b5d8":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"STATUS\", data=df, palette=sns.xkcd_palette([\"azure\", \"light red\"]))\nplt.xlabel('Status')\nplt.ylabel('Count')\n# plt.savefig('.\/plots\/status_count.png')\nplt.show()","c9279fcd":"fig=plt.figure(figsize=(8,4))\nfor x in ['T','A']:\n    df['AGE'][df['STATUS']==x].plot(kind='kde')\n    \nplt.title('Status V\/S Age Density Distribution')\nplt.legend(('T','A'))\nplt.xlabel('Age')\n# plt.savefig('.\/plots\/status_age_distribution.png')\nplt.show()","f59b0846":"sns.countplot(x='PERFORMANCE_RATING', data=df, hue='STATUS', palette=sns.xkcd_palette([\"azure\", \"light red\"]))\nplt.title(\"Performance Rating Count Plot\")\nplt.xlabel('Performance Rating')\nplt.ylabel('Count')\n# plt.savefig('.\/plots\/performance_count.png')\nplt.show()","1f5f99e6":"sns.countplot(x='JOB_SATISFACTION', data=df, hue='STATUS', palette=sns.xkcd_palette([\"aqua\", \"periwinkle\"]))\nplt.title(\"Job Satisfaction Count Plot\")\nplt.xlabel('Job Satisfaction')\nplt.ylabel('Count')\n# plt.savefig('.\/plots\/satisfaction_count.png')\nplt.show()","3dda38f8":"sns.boxplot(x='JOB_SATISFACTION',data=df,hue='STATUS',y='AGE', palette=sns.xkcd_palette([\"pastel purple\", \"pastel yellow\"]))\nplt.title(\"Job Satisfaction and Age Boxplot\")\nplt.xlabel('Job Satisfaction')\nplt.ylabel('Age')\n# plt.savefig('.\/plots\/age_satisfaction_box.png')\nplt.show()","84786093":"# Label Encoding categorical features\n\nle = LabelEncoder()\ndf['NUMBER_OF_TEAM_CHANGED'] = le.fit_transform(df['NUMBER_OF_TEAM_CHANGED'])\ndf['REHIRE'] = le.fit_transform(df['REHIRE'])\ndf['IS_FIRST_JOB'] = le.fit_transform(df['IS_FIRST_JOB'])\ndf['TRAVELLED_REQUIRED'] = le.fit_transform(df['TRAVELLED_REQUIRED'])\ndf['DISABLED_EMP'] = le.fit_transform(df['DISABLED_EMP'])\ndf['DISABLED_VET'] = le.fit_transform(df['DISABLED_VET'])\ndf['EDUCATION_LEVEL'] = le.fit_transform(df['EDUCATION_LEVEL'])\ndf['STATUS'] = le.fit_transform(df['STATUS'])","d4d516c2":"# Correlation Heatmap\n\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(df.corr(), annot = True, ax=ax)\n# plt.savefig('.\/plots\/correlation_heatmap.png')\nplt.show()","fcb270d0":"df.drop(['HRLY_RATE'], axis=1, inplace=True)","f4b45f72":"# One-Hot Encoding categorical features\n\ndf['HIRE_MONTH'] = df['HIRE_MONTH'].astype('category')\ndf['JOB_GROUP'] = df['JOB_GROUP'].astype('category')\ndf['REFERRAL_SOURCE'] = df['REFERRAL_SOURCE'].astype('category')\ndf['ETHNICITY'] = df['ETHNICITY'].astype('category')\ndf['SEX'] = df['SEX'].astype('category')\ndf['MARITAL_STATUS'] = df['MARITAL_STATUS'].astype('category')\ndf = pd.get_dummies(df, columns=['HIRE_MONTH', 'JOB_GROUP', 'REFERRAL_SOURCE', 'SEX', 'MARITAL_STATUS', 'ETHNICITY'])","fc2d061f":"# X = features & y = Target class\n\nX = df.drop(['STATUS'], axis=1)\ny = df['STATUS']","1208087a":"# Normalizing the all the features\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(X)","4f92e693":"# Splitting dataset into training and testing split with 70-30% ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","45c1f989":"# K-fold splits\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)","569791d2":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'penalty': ['l1','l2'],\n    'C': [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10]\n}\n\n# Building model\nlogreg = LogisticRegression(solver='liblinear')\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(logreg, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","5428dba9":"logreg_grid_val_score = grid.best_score_\nprint('Best Score:', logreg_grid_val_score)\nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","fb28881b":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\nlogreg_grid = grid.best_estimator_\ny_pred = logreg_grid.predict(X_test)","a5d8259f":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","3d1e2694":"# Calculating metrics\n\nlogreg_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', logreg_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","c0b40f4f":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'n_neighbors': [3,5,11,19],\n    'weights': ['uniform','distance']\n}\n\n# Building model\nknn = KNeighborsClassifier()\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(knn, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","6133c4ab":"knn_grid_val_score = grid.best_score_\nprint('Best Score:', knn_grid_val_score)\nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","c67a9c03":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\nknn_grid= grid.best_estimator_\ny_pred = knn_grid.predict(X_test)","15ba1195":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","9a02f429":"# Calculating metrics\n\nknn_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', knn_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","d660f94b":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# No such parameters for Gaussian Naive Bayes\nparams = {}\n\n# Building model\ngb = GaussianNB()\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(gb, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","83916057":"gb_grid_val_score = grid.best_score_\nprint('Best Score:', gb_grid_val_score)\nprint('Best Estimator:', grid.best_estimator_)","ec9ecbb3":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\ngb_grid= grid.best_estimator_\ny_pred = gb_grid.predict(X_test)","ae41175c":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","da3c9270":"# Calculating metrics\n\ngb_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', gb_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","52bcc4a1":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'C': [0.001, 0.01, 0.1, 1, 10], \n    'gamma' : [0.001,0.001, 0.01, 0.1, 1]\n}\n\n# Building model\nsvc = SVC(kernel='rbf', probability=True) ## 'rbf' stands for gaussian kernel\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(svc, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","bc8ecc35":"svm_grid_val_score = grid.best_score_\nprint('Best Score:', svm_grid_val_score)\nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","57c45b0a":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\nsvm_grid= grid.best_estimator_\ny_pred = svm_grid.predict(X_test)","76f16e8b":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","30d6354d":"# Calculating metrics\n\nsvm_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', svm_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","424671b0":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'max_features': [1, 3, 10],\n    'min_samples_split': [2, 3, 10],\n    'min_samples_leaf': [1, 3, 10],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\n# Building model\ndtc = DecisionTreeClassifier()\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(dtc, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","405f4f64":"dtc_grid_val_score = grid.best_score_\nprint('Best Score:', dtc_grid_val_score)\nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","c810410a":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\ndtc_grid= grid.best_estimator_\ny_pred = dtc_grid.predict(X_test)","3616cd99":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","3a2c8c0e":"# Calculating metrics\n\ndtc_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', dtc_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","a6fd0bc3":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'max_features': [1, 3, 10],\n    'min_samples_split': [2, 3, 10],\n    'min_samples_leaf': [1, 3, 10],\n    'bootstrap': [False],\n    'n_estimators' :[100,300],\n    'criterion': [\"entropy\", \"gini\"]\n}\n\n# Building model\nrfc = RandomForestClassifier()\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(rfc, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","48ab718e":"rfc_grid_val_score = grid.best_score_\nprint('Best Score:', rfc_grid_val_score)\nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","4c2ff27c":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\nrfc_grid= grid.best_estimator_\ny_pred = rfc_grid.predict(X_test)","5d342a98":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","a9ca03d9":"# Calculating metrics\n\nrfc_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', rfc_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","edc3808e":"# Defining our neural network model\n\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(4, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model","9a43e545":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'optimizer': ['rmsprop', 'adam'],\n    'epochs': [100, 200, 400],\n    'batch_size': [5, 10, 20]\n}\n\n# Building model\nnn = KerasClassifier(build_fn=create_model)\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(nn, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","76d24812":"nn_grid_val_score = grid.best_score_\nprint('Best Score:', nn_grid_val_score) \nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","bae62461":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\nnn_grid= grid.best_estimator_\ny_pred = nn_grid.predict(X_test)","6a32ab70":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","5eae062b":"# Calculating metrics\n\nnn_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', nn_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","14415a87":"# Building our model with K-fold validation and GridSearch to find the best parameters\n\n# Defining all the parameters\nparams = {\n    'max_depth': range (2, 10, 1),\n    'n_estimators': range(60, 220, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n}\n\n# Building model\nxgb = XGBClassifier(objective='binary:logistic')\n\n# Parameter estimating using GridSearch\ngrid = GridSearchCV(xgb, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n\n# Fitting the model\ngrid.fit(X_train, y_train)","a26d75c6":"xgb_grid_val_score = grid.best_score_\nprint('Best Score:', xgb_grid_val_score) \nprint('Best Params:', grid.best_params_)\nprint('Best Estimator:', grid.best_estimator_)","098a4b48":"# Using the best parameters from the grid-search and predicting on test feature dataset(X_test)\n\nxgb_grid= grid.best_estimator_\ny_pred = xgb_grid.predict(X_test)","4d32111c":"# Confusion matrix\n\npd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )","61c91d4a":"# Calculating metrics\n\nxgb_grid_score = accuracy_score(y_test, y_pred)\nprint('Model Accuracy:', xgb_grid_score)\nprint('Classification Report:\\n', classification_report(y_test, y_pred))","b972bb8d":"score_df = pd.DataFrame(\n    [\n        ['Logistic Regression', logreg_grid_score, logreg_grid_val_score],\n        ['K-Nearest Neighbors', knn_grid_score, knn_grid_val_score],\n        ['Gaussian Na\u00efve Bayes', gb_grid_score, gb_grid_val_score],\n        ['Support Vector Machines', svm_grid_score, svm_grid_val_score],\n        ['Decision Tree Classifier', dtc_grid_score, dtc_grid_val_score],\n        ['Random Forest Tree Classifier', rfc_grid_score, rfc_grid_val_score],\n        ['Artificial Neural Networks', nn_grid_score, nn_grid_val_score],\n        ['GBM - XGBoost', xgb_grid_score, xgb_grid_val_score], \n    ],\n    columns= ['Model', 'Test Score', 'Validation Score']\n)\nscore_df['Test Score'] = score_df['Test Score']*100\nscore_df['Validation Score'] = score_df['Validation Score']*100","e4e63df8":"score_df","2d46a4b6":"fig, ax1 = plt.subplots(figsize=(10, 5))\ntidy = score_df.melt(id_vars='Model').rename(columns=str.title)\nsns.barplot(x='Model', y='Value', hue='Variable', data=tidy, ax=ax1, palette=sns.xkcd_palette([\"azure\", \"light red\"]))\nplt.ylim(20, 90)\nplt.xticks(rotation=45, horizontalalignment=\"right\")\n# plt.savefig('.\/plots\/result.png')\nsns.despine(fig)","a767e8b3":"time_df = pd.DataFrame(\n    [\n        ['Logistic Regression', 1.2],\n        ['K-Nearest Neighbors', 1.0],\n        ['Gaussian Na\u00efve Bayes', 0.0034],\n        ['Support Vector Machines', 51.7],\n        ['Decision Tree Classifier', 0.068],\n        ['Random Forest Tree Classifier', 15.1],\n        ['Artificial Neural Networks', 454.2],\n        ['GBM - XGBoost', 40.8], \n    ],\n    columns= ['Model', 'Training Time']\n)","2105e653":"fig, ax1 = plt.subplots(figsize=(10, 5))\nsns.barplot(data=time_df, x='Model', y='Training Time', palette=sns.color_palette('husl'))\nplt.xticks(rotation=45, horizontalalignment=\"right\")\nplt.ylabel('Training Time(in mins)')\n# plt.savefig('.\/plots\/training_time.png')\nsns.despine(fig)","5ad6f314":"## RESULTS","c25e8a0e":"### Artificial Neural Networks","ff40da02":"## VISUALIZATIONS","c25bf0c8":"### Logistic Regression","845aef03":"# BINARY CLASSIFICATION","5e2b9a36":"### Gaussian Naive Bayes","afed3664":"### Decision Tree Classifier","b5c71cf5":"## DATA CLEANUP","0b6a8e70":"### K-Nearest Neighbor Classifier(KNN)","1104edd9":"## FEATURE ENGINEERING","6e2893db":"### Random Forest Classifier","4fcafe5c":"##### We see that HRLY_RATE and ANNUAL_RATE are highly correlated with correlation of 1, so we can take the ANNUAL_RATE and discard HRLY_RATE","04a74a48":"### Gradient Boosting Machines - XGBoost","9821f724":"## MODELLING","7bb5d122":"### Support Vector Machines"}}