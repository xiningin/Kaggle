{"cell_type":{"7237979e":"code","1242cf3d":"code","a584e5f6":"code","4f6133b7":"code","b56aaffd":"code","9a730ef7":"code","a4b459ae":"code","02e23f69":"code","0cbba1a1":"code","b69c1d7e":"code","4826d695":"code","f7f4a7be":"code","6432f25e":"code","38b439f3":"code","d45c155e":"code","506065e1":"code","28bccc57":"code","8e8a28d3":"code","043c9764":"code","3451a075":"code","369ebf56":"code","cf73ec23":"code","e4b5ffef":"code","afdd6a82":"code","089647d3":"code","4a167730":"code","99a50d2b":"code","b21e5a36":"code","39a66f1f":"code","31bb1d1a":"code","7f53b3bf":"code","14c5c17b":"code","db729e38":"code","9c89628d":"code","e1a3d933":"code","3264572f":"markdown","51b578b2":"markdown","b49ea134":"markdown","72fbc8e0":"markdown","84a57644":"markdown","e3eb9e78":"markdown","04b73b2d":"markdown"},"source":{"7237979e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1242cf3d":"import sklearn","a584e5f6":"sklearn.__version__","4f6133b7":"# Linear regression on a categorical variable using one-hot and dummy codes","b56aaffd":"from sklearn import linear_model\n\n# Define a toy dataset of apartment rental prices in \n# New York, San Francisco, and Seattle\ndf = pd.DataFrame({\n     'City': ['SF', 'SF', 'SF', 'NYC', 'NYC', 'NYC', \n              'Seattle', 'Seattle', 'Seattle'],\n     'Rent': [3999, 4000, 4001, 3499, 3500, 3501, 2499, 2500, 2501]\n})\ndf['Rent'].mean()","9a730ef7":"# Convert the categorical variables in the DataFrame to one-hot encoding\n# and fit a linear regression model\none_hot_df = pd.get_dummies(df, prefix=['city'])\none_hot_df","a4b459ae":"model = linear_model.LinearRegression()\nmodel.fit(one_hot_df[['city_NYC', 'city_SF', 'city_Seattle']],\n           one_hot_df['Rent'])\nmodel.coef_","02e23f69":"model.intercept_","0cbba1a1":"dummy_df = pd.get_dummies(df, prefix=['city'], drop_first=True)\ndummy_df","b69c1d7e":"model.fit(dummy_df[['city_SF', 'city_Seattle']], dummy_df['Rent'])\nmodel.coef_\n","4826d695":"model.intercept_","f7f4a7be":"# bias coefficient\ndf[df['City'] == 'NYC'].mean()","6432f25e":"# The coefficient for the x1 feature\ndf[df['City'] == 'NYC'].mean() - 3500","38b439f3":"# The coefficient for the x2 feature\ndf[df['City'] == 'SF'].mean() - 3500","d45c155e":"# The coefficient for the x3 feature\ndf[df['City'] == 'Seattle'].mean() - 3500","506065e1":"effect_df = dummy_df.copy()\neffect_df.loc[3:5, ['city_SF', 'city_Seattle']] = -1.0\neffect_df","28bccc57":"model.fit(effect_df[['city_SF', 'city_Seattle']], effect_df['Rent'])\nmodel.coef_","8e8a28d3":"model.intercept_","043c9764":"import json","3451a075":"# Load Yelp reviews data\nwith open('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json') as review_file:\n    review_df = pd.DataFrame([json.loads(next(review_file)) for x in range(10000)])","369ebf56":"# Define m as equal to the unique number of business_ids\nm = len(review_df.business_id.unique())\nm","cf73ec23":"from sklearn.feature_extraction import FeatureHasher\n\nh = FeatureHasher(n_features=m, input_type='string')\nf = h.transform(review_df['business_id'])","e4b5ffef":"# How does this affect feature interpretability?\nreview_df['business_id'].unique().tolist()[0:5]","afdd6a82":"f.toarray()","089647d3":"# Not great. BUT, let's see the storage size of our features.\nfrom sys import getsizeof\nprint('Our pandas Series, in bytes: ', getsizeof(review_df['business_id']))\nprint('Our hashed numpy array, in bytes: ', getsizeof(f))","4a167730":"# train_subset data is first 10K rows of 6+GB set\ndf = pd.read_csv('..\/input\/avazu-ctr-prediction\/train\/train.csv', nrows=10000)","99a50d2b":"df.head()","b21e5a36":"# How many unique features should we have after?\nlen(df['device_id'].unique())","39a66f1f":"def click_counting(x, bin_column):\n    clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(), name='clicks')\n    no_clicks = pd.Series(x[x['click'] < 1][bin_column].value_counts(), name='no_clicks')\n    \n    counts = pd.DataFrame([clicks,no_clicks]).T.fillna('0')\n    counts['total_clicks'] = counts['clicks'].astype('int64') + counts['no_clicks'].astype('int64')\n    return counts\n\ndef bin_counting(counts):\n    counts['N+'] = counts['clicks']\\\n                    .astype('int64')\\\n                    .divide(counts['total_clicks'].astype('int64'))\n    counts['N-'] = counts['no_clicks']\\\n                    .astype('int64')\\\n                    .divide(counts['total_clicks'].astype('int64'))\n    counts['log_N+'] = counts['N+'].divide(counts['N-'])\n    # If we wanted to only return bin-counting properties, \n    # we would filter here\n    bin_counts = counts.filter(items= ['N+', 'N-', 'log_N+'])\n    return counts, bin_counts","31bb1d1a":"bin_column = 'device_id'\ndevice_clicks = click_counting(df.filter(items=[bin_column, 'click']), bin_column)\ndevice_all, device_bin_counts = bin_counting(device_clicks.copy())\n","7f53b3bf":"device_clicks.head()","14c5c17b":"device_all.head()","db729e38":"device_bin_counts.head()","9c89628d":"len(device_bin_counts)","e1a3d933":"device_all.sort_values(by = 'total_clicks', ascending=False).head(4)","3264572f":"## Chapter 5. Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nThis chapter is one of the chapters of the book, Feature Engineering for Machine Learning. Since I do not have enough resource on my local machine, I have ended up creating this kernel to practice the source code of the chapter 5 while reading the book. You can also examine all the main chapters' code over the original GitHub repository of the book: https:\/\/github.com\/alicezheng\/feature-engineering-book\n","51b578b2":"Linear regression learned coefficients\n\n|                  \t| x1     \t|   x2   \t|      x3 \t| b       \t|\n|------------------\t|--------\t|:------:\t|--------:\t|---------\t|\n| One-hot encoding \t| 166.67 \t| 666.67 \t| \u2013833.33 \t| 3333.33 \t|\n| Dummy coding     \t| 0      \t|   500  \t|   -1000 \t| 3500    \t|","b49ea134":"## Effect Coding","72fbc8e0":"## Feature Hashing","84a57644":"## Encoding Categorical Variables","e3eb9e78":"References:\n1. https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features","04b73b2d":"## Bin Counting"}}