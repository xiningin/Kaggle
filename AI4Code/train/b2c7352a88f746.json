{"cell_type":{"ba39cc86":"code","a2721205":"code","905ddd1f":"code","edebc2a6":"code","9ce48c89":"code","e4eebd53":"code","bbdf2f64":"code","aaa445f2":"code","9d6e03aa":"code","4e21a9aa":"code","89a3a8cc":"code","69d27eb7":"code","dbd69597":"code","50e7d975":"code","f6fd51dd":"code","cd398ab1":"code","dfb96cf3":"code","5f4458f5":"code","774b1534":"code","bb6ca22b":"code","05dd412f":"code","70fa8961":"code","4da9ab3c":"code","ecfc2c92":"code","7e24aae0":"code","6888ec6b":"code","b9e9ff1d":"code","e93dafad":"code","6e3d9261":"code","6adde2a6":"code","5530e6b3":"code","27b37e19":"code","3fef15f3":"code","d2940467":"code","35f31a00":"code","acb253e6":"code","976985d8":"code","ad77805b":"code","29398a6c":"code","b4a7696a":"code","d06c7461":"code","80271187":"code","7514fea3":"code","35b0c880":"code","08b3096a":"code","7375b9ae":"code","68d241e6":"code","8730c0b6":"code","10a31a68":"code","f306fea3":"code","5bac67a4":"code","45dea6ff":"code","421aa65f":"code","b54c57b4":"code","7b0571dc":"code","0c163404":"code","24a617d3":"code","a9b8e01d":"code","0af47d04":"code","db321a6b":"code","82b96336":"code","a32c3e91":"code","acf53660":"code","918daffc":"code","6eb9f275":"code","2bb35554":"code","b705906d":"code","f17c0465":"code","195350d8":"code","f7bc641b":"code","c5722b25":"code","1c69db76":"code","af1bb1a2":"code","8f9d011c":"code","5a8d6651":"code","a9712a15":"code","438960ac":"code","426e4148":"code","7694e0d3":"code","eecf1f34":"code","4c557eaa":"code","b5dde0c0":"code","f08a4e3c":"code","90f61de0":"code","b0ecaa38":"code","15774a70":"code","4149e87f":"code","781927d8":"code","9982d959":"code","6e3ac433":"code","b105dbd0":"code","1d2caaeb":"code","370b69b6":"code","04041e96":"code","47fedd07":"code","7473f762":"code","df1c0c6a":"code","8ffd5d6b":"code","b869f779":"code","8d402278":"code","91ef3883":"code","d48f571d":"code","9ee1a5cf":"code","d11658d8":"code","4680c0cd":"code","992d1a0c":"code","ad5002f1":"code","4c969b25":"code","5aab94d6":"code","b2875114":"code","68662ddc":"code","201186c3":"code","2bc6f852":"code","495d4ee6":"code","fa9de862":"code","bfb157ed":"code","971fafc9":"code","02293e86":"code","bfdc4800":"code","f338159e":"code","504c3b1a":"code","5554bcdf":"code","5992a51d":"code","a71707d8":"code","e7420000":"code","84997cee":"code","04700c6d":"code","462afba3":"code","3fdea094":"code","f11bea9b":"code","ae7591a6":"code","15218b1c":"code","ee3ea9b1":"code","a0f0f9f0":"code","ad521928":"code","85c96a54":"code","eb32241a":"code","ff3327af":"code","1bc90a70":"code","591f2a7c":"code","db15769d":"code","ddb7d422":"code","7e021a84":"code","bb40de94":"code","8b6fcd48":"code","dc43a761":"code","2008510d":"code","0cf19901":"code","5356774c":"code","f757802a":"code","465d12f1":"code","e6d2026a":"code","e52288e6":"code","be8f0edc":"code","8b3ea2ef":"code","abf5c59d":"code","e52f8103":"code","d489b29f":"code","30caab00":"code","b017dee1":"code","99cb8968":"code","6bcea54d":"code","26da9624":"code","b540a5de":"code","c6b45afa":"code","088d3fa9":"code","1187479f":"code","e501b58a":"code","4c610a1e":"code","55c7afc1":"code","33a919ee":"code","d358f0df":"code","40910255":"code","ffdb1852":"code","3fe770f5":"markdown","ce696868":"markdown","dbde0cc8":"markdown","09c4d845":"markdown","9137f95f":"markdown","0818b919":"markdown","471cad64":"markdown","58a29157":"markdown","b4550a33":"markdown","1cfdb6f7":"markdown","3480c5b4":"markdown","2c096e99":"markdown","781655af":"markdown","a2f5daae":"markdown","8de3dfc6":"markdown","deca6401":"markdown","f3415e5e":"markdown","2085d843":"markdown","06f4e11e":"markdown","394466f9":"markdown","57c65b09":"markdown","301af283":"markdown","928c0a8f":"markdown","25662b1e":"markdown","eb5a3360":"markdown","1d16ae6b":"markdown","391ac647":"markdown","d355e348":"markdown","7325ab52":"markdown","bf1f62c4":"markdown","1916ba39":"markdown","e014fad9":"markdown","5e86308a":"markdown","cd11c225":"markdown","09650e43":"markdown","ef4824d1":"markdown","aa6a4007":"markdown","ffe8bdf4":"markdown","7d5bfe83":"markdown","d81bf5a1":"markdown","be02444f":"markdown"},"source":{"ba39cc86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a2721205":"import pandas as pd\ndata = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\")\nother_text_responses = pd.read_csv(\"..\/input\/kaggle-survey-2019\/other_text_responses.csv\")\nquestions_only = pd.read_csv(\"..\/input\/kaggle-survey-2019\/questions_only.csv\")\nsurvey_schema = pd.read_csv(\"..\/input\/kaggle-survey-2019\/survey_schema.csv\")\nreq = data[1:]","905ddd1f":"india = req[req.Q3 == 'India'].reset_index(drop=True)","edebc2a6":"#2018 survey responses\nimport pandas as pd\nSurveySchema = pd.read_csv(\"..\/input\/kaggle-survey-2018\/SurveySchema.csv\")\nfreeFormResponses = pd.read_csv(\"..\/input\/kaggle-survey-2018\/freeFormResponses.csv\")\ndata2 = pd.read_csv(\"..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv\")\nreq2 = data2[1:]","9ce48c89":"#2018 survey respondants from India\nindia2 = req2[req2.Q3 == 'India'].reset_index(drop=True)","e4eebd53":"current_title = india.groupby('Q5').count().reset_index().loc[:,'Q5':'Time from Start to Finish (seconds)'].rename(columns={'Q5':'Current_Postion_Of_Participant','Time from Start to Finish (seconds)':'Frequency'})","bbdf2f64":"t = current_title.sort_values(by=['Frequency'],ascending=False).reset_index(drop=True)","aaa445f2":"t['perc'] = (t['Frequency']*100)\/t['Frequency'].sum()","9d6e03aa":"import plotly.graph_objects as go\n\nlabels = t['Current_Postion_Of_Participant']\nvalues = t['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\"Proportion of Work Profession of survey paricipants in India\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=13,\n        color=\"#7f7f7f\"\n    )\n)\n","4e21a9aa":"current_age2 = india2.groupby('Q1').count().reset_index().loc[:,'Q1':'Time from Start to Finish (seconds)'].rename(columns={'Q1':'Age_Group','Time from Start to Finish (seconds)':'Frequency'})","89a3a8cc":"current_age2.sort_values(by=['Frequency'],ascending=False,inplace=True)","69d27eb7":"current_age2['perc']=(current_age2['Frequency']*100)\/current_age2['Frequency'].sum()","dbd69597":"import plotly.graph_objects as go\n\nlabels = current_age2['Age_Group']\nvalues = current_age2['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\"Proportion of Age-Groups of survey paricipants in India in 2019\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=15,\n        color=\"#7f7f7f\"\n    )\n)\n","50e7d975":"current_age_2 = india2.groupby('Q2').count().reset_index().loc[:,'Q2':'Time from Start to Finish (seconds)'].rename(columns={'Q2':'Age_Group','Time from Start to Finish (seconds)':'Frequency'})","f6fd51dd":"current_age_2.sort_values(by=['Frequency'],ascending=False,inplace=True)","cd398ab1":"current_age_2['perc']=(current_age_2['Frequency']*100)\/current_age_2['Frequency'].sum()","dfb96cf3":"import plotly.graph_objects as go\n\nlabels = current_age_2['Age_Group']\nvalues = current_age_2['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\"Proportion of Age-Groups of survey paricipants in India in 2018\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=13,\n        color=\"#7f7f7f\"\n    )\n)\n","5f4458f5":"current_degree = india.groupby('Q4').count().reset_index().loc[:,'Q4':'Time from Start to Finish (seconds)'].rename(columns={'Q4':'Educational_level','Time from Start to Finish (seconds)':'Frequency'})","774b1534":"current_degree.sort_values(by=['Frequency'],ascending=False,inplace=True)","bb6ca22b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,10))\n\nplt.xticks(current_degree.index, current_degree.Educational_level.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Educational_level\", y=\"Frequency\",data=current_degree)\nax.set_title('Proportion of participants from a Given Educational Background in India in 2019',fontsize=15)\nax.set(xlabel='Educational Level of participant', ylabel='Number of participants in the survey')\n","05dd412f":"current_degree2 = india2.groupby('Q4').count().reset_index().loc[:,'Q4':'Time from Start to Finish (seconds)'].rename(columns={'Q4':'Educational_level','Time from Start to Finish (seconds)':'Frequency'})","70fa8961":"current_degree2.sort_values(by=['Frequency'],ascending=False,inplace=True)","4da9ab3c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,10))\n\nplt.xticks(current_degree2.index, current_degree2.Educational_level.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Educational_level\", y=\"Frequency\",data=current_degree2)\nax.set_title('Proportion of participants from a Given Educational Background in India in 2018',fontsize=15)\nax.set(xlabel='Educational Level of participant', ylabel='Number of participants in the survey')\n","ecfc2c92":"current_degree_job_compensation = india.groupby(['Q4','Q5','Q10']).count().reset_index().loc[:,'Q4':'Time from Start to Finish (seconds)'].rename(columns={'Q4':'Educational_level','Q5':'Work_profession','Q10':'Salary','Time from Start to Finish (seconds)':'Frequency'})","7e24aae0":"current_degree_job_compensation['perc']=(current_degree_job_compensation['Frequency']*100)\/(current_degree_job_compensation['Frequency'].sum())","6888ec6b":"cur = current_degree_job_compensation[current_degree_job_compensation.Work_profession == 'Data Scientist']","b9e9ff1d":"c = cur[(cur.Salary == '150,000-199,999')|(cur.Salary == '20,000-24,999')|(cur.Salary == '25,000-29,999')|(cur.Salary == '125,000-149,000')|(cur.Salary == '300,000-500,000')|(cur.Salary == '30,000-39,999')|(cur.Salary == '$0-999')|(cur.Salary == '1,000-1,999')|(cur.Salary == '2,000-2,999')]","e93dafad":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nfig, ax = plt.subplots(1, 1, figsize=(15,10))\nplt.xticks(c.index, c.Educational_level.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Educational_level\", y=\"perc\",hue='Salary', data=c)\nax.set_title('Proportion of participants in different salary brackets working as a Data-Scientist in India',fontsize=12)\nax.set(xlabel='Work Profession', ylabel='Percentage of participants in the survey')\nax.legend(bbox_to_anchor=(1.1, 1.05))\n\n","6e3d9261":"current_degree_job_compensation2 = india2.groupby(['Q4','Q6','Q9']).count().reset_index().loc[:,'Q4':'Time from Start to Finish (seconds)'].rename(columns={'Q4':'Educational_level','Q6':'Work_profession','Q9':'Salary','Time from Start to Finish (seconds)':'Frequency'})","6adde2a6":"current_degree_job_compensation2['perc']=(current_degree_job_compensation2['Frequency']*100)\/(current_degree_job_compensation2['Frequency'].sum())","5530e6b3":"cur = current_degree_job_compensation2[current_degree_job_compensation2.Work_profession == 'Data Scientist']","27b37e19":"c = cur[(cur.Salary == '150,000-200,000')|(cur.Salary == '20-30,000')|(cur.Salary == '30-40,000')|(cur.Salary == '125-150,000')|(cur.Salary == '300,000-400,000')|(cur.Salary == '30-40,000')|(cur.Salary == '0-10,000')]","3fef15f3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nfig, ax = plt.subplots(1, 1, figsize=(15,10))\nplt.xticks(c.index, c.Educational_level.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Educational_level\", y=\"perc\",hue='Salary', data=c)\nax.set_title('Proportion of participants in different salary brackets working as a Data-Scientist in India in 2018',fontsize=12)\nax.set(xlabel='Work Profession', ylabel='Percentage of participants in the survey')\nax.legend(bbox_to_anchor=(1.1, 1.05))\n\n","d2940467":"students = india[india.Q5 == 'Student'].reset_index(drop=True)","35f31a00":"#platforms_students = students.groupby('Q13').count().reset_index().loc[:,'Q13':'Time from Start to Finish (seconds)'].rename(columns={'Q13':'Platforms','Time from Start to Finish (seconds)':'Frequency'})\n\nplatforms = ['Q13_Part_1','Q13_Part_2','Q13_Part_3','Q13_Part_4','Q13_Part_5','Q13_Part_6','Q13_Part_7','Q13_Part_8','Q13_Part_9','Q13_Part_10','Q13_Part_11','Q13_Part_12']\nl = list()\nfor i in platforms:\n    df = students.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Platforms'}, inplace = True)\n    l.append(df)\nplatforms_students = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","acb253e6":"platforms_students.sort_values(by=['Frequency'],ascending=False,inplace=True)","976985d8":"platforms_students['perc'] = (platforms_students['Frequency']*100)\/(platforms_students['Frequency'].sum())","ad77805b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(platforms_students.index, platforms_students.Platforms.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Platforms\", y=\"perc\",data=platforms_students)\nax.set_title('Proportion of Platforms used to study data science courses by students in India in 2019',fontsize=15)\nax.set(xlabel='Tools Used', ylabel='Percentage of Participants in the survey')\n","29398a6c":"students = india2[india2.Q6 == 'Student'].reset_index(drop=True)","b4a7696a":"#platforms_students = students.groupby('Q13').count().reset_index().loc[:,'Q13':'Time from Start to Finish (seconds)'].rename(columns={'Q13':'Platforms','Time from Start to Finish (seconds)':'Frequency'})\n\nplatforms = ['Q36_Part_1','Q36_Part_2','Q36_Part_3','Q36_Part_4','Q36_Part_5','Q36_Part_6','Q36_Part_7','Q36_Part_8','Q36_Part_9','Q36_Part_10','Q36_Part_11','Q36_Part_12','Q36_Part_13']\nl = list()\nfor i in platforms:\n    df = students.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Platforms'}, inplace = True)\n    l.append(df)\nplatforms_students = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","d06c7461":"platforms_students.sort_values(by=['Frequency'],ascending=False,inplace=True)","80271187":"platforms_students['perc'] = (platforms_students['Frequency']*100)\/(platforms_students['Frequency'].sum())","7514fea3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(platforms_students.index, platforms_students.Platforms.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Platforms\", y=\"perc\",data=platforms_students)\nax.set_title('Proportion of Platforms used to study data science courses by students in India in 2018',fontsize=15)\nax.set(xlabel='Tools Used', ylabel='Percentage of Participants in the survey')\n","35b0c880":"students = india[india.Q5 == 'Student'].reset_index(drop=True)","08b3096a":"#platforms_students = students.groupby('Q13').count().reset_index().loc[:,'Q13':'Time from Start to Finish (seconds)'].rename(columns={'Q13':'Platforms','Time from Start to Finish (seconds)':'Frequency'})\n\nsources = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_Part_6','Q12_Part_7','Q12_Part_8','Q12_Part_9','Q12_Part_10','Q12_Part_11','Q12_Part_12']\nl = list()\nfor i in sources:\n    df = students.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Sources'}, inplace = True)\n    l.append(df)\nsources_students = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","7375b9ae":"sources_students.sort_values(by=['Frequency'],ascending=False,inplace=True)","68d241e6":"sources_students['perc']=(sources_students['Frequency']*100)\/sources_students['Frequency'].sum()","8730c0b6":"sources_students.reset_index(drop=True,inplace=True)","10a31a68":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(sources_students.index, sources_students.Sources.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Sources\", y=\"perc\",data=sources_students)\nax.set_title('Proportion of Fav Sources that report on data science courses by students in India in 2019',fontsize=15)\nax.set(xlabel='Sources', ylabel='Percentage of Participants in the survey')\n","f306fea3":"students2 = india2[india2.Q6 == 'Student'].reset_index(drop=True)","5bac67a4":"#platforms_students = students.groupby('Q13').count().reset_index().loc[:,'Q13':'Time from Start to Finish (seconds)'].rename(columns={'Q13':'Platforms','Time from Start to Finish (seconds)':'Frequency'})\n\nsources = ['Q38_Part_1','Q38_Part_2','Q38_Part_3','Q38_Part_4','Q38_Part_5','Q38_Part_6','Q38_Part_7','Q38_Part_8','Q38_Part_9','Q38_Part_10','Q38_Part_11','Q38_Part_12','Q38_Part_13','Q38_Part_14','Q38_Part_15','Q38_Part_16','Q38_Part_17','Q38_Part_18','Q38_Part_19','Q38_Part_20','Q38_Part_21','Q38_Part_22']\nl = list()\nfor i in sources:\n    df = students2.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Sources'}, inplace = True)\n    l.append(df)\nsources_students = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","45dea6ff":"sources_students.sort_values(by=['Frequency'],ascending=False,inplace=True)","421aa65f":"sources_students['perc']=(sources_students['Frequency']*100)\/sources_students['Frequency'].sum()","b54c57b4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(sources_students.index, sources_students.Sources.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Sources\", y=\"perc\",data=sources_students)\nax.set_title('Proportion of Fav Sources that report on data science courses by students in India in 2019',fontsize=15)\nax.set(xlabel='Sources', ylabel='Percentage of Participants in the survey')\n","7b0571dc":"#platforms_students = students.groupby('Q13').count().reset_index().loc[:,'Q13':'Time from Start to Finish (seconds)'].rename(columns={'Q13':'Platforms','Time from Start to Finish (seconds)':'Frequency'})\n\ntools = ['Q14']\nl = list()\nfor i in tools:\n    df = students.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Tools'}, inplace = True)\n    l.append(df)\ntools_students = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","0c163404":"tools_students.sort_values(by=['Frequency'],ascending=False,inplace=True)","24a617d3":"tools_students['perc'] = (tools_students['Frequency']*100)\/(tools_students['Frequency'].sum())","a9b8e01d":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(tools_students.index, tools_students.Tools.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Tools\", y=\"perc\",data=tools_students)\nax.set_title('Tools used by students to analyze data in India in 2019',fontsize=15)\nax.set(xlabel='Sources', ylabel='Percentage of Participants in the survey')\n","0af47d04":"#platforms_students = students.groupby('Q13').count().reset_index().loc[:,'Q13':'Time from Start to Finish (seconds)'].rename(columns={'Q13':'Platforms','Time from Start to Finish (seconds)':'Frequency'})\n\ntools = ['Q12_MULTIPLE_CHOICE']\nl = list()\nfor i in tools:\n    df = students2.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Tools'}, inplace = True)\n    l.append(df)\ntools_students = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","db321a6b":"tools_students.sort_values(by=['Frequency'],ascending=False,inplace=True)","82b96336":"tools_students['perc'] = (tools_students['Frequency']*100)\/(tools_students['Frequency'].sum())","a32c3e91":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(tools_students.index, tools_students.Tools.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Tools\", y=\"perc\",data=tools_students)\nax.set_title('Tools used by students to analyze data in India in 2018',fontsize=15)\nax.set(xlabel='Sources', ylabel='Percentage of Participants in the survey')\n","acf53660":"indian_data_scientists = india[india.Q5 == 'Data Scientist'].reset_index(drop=True)\n","918daffc":"tools_to_analyze = indian_data_scientists.groupby('Q14').count().reset_index().loc[:,'Q14':'Time from Start to Finish (seconds)'].rename(columns={'Q14':'Tools_Used_To_Analyze_Data_At_Workplace','Time from Start to Finish (seconds)':'Frequency'})","6eb9f275":"tools_to_analyze = tools_to_analyze[tools_to_analyze.Tools_Used_To_Analyze_Data_At_Workplace != 0]","2bb35554":"tools_to_analyze.sort_values(by=['Frequency'],inplace=True, ascending=False)","b705906d":"tools_to_analyze['perc']=(tools_to_analyze['Frequency']*100)\/tools_to_analyze['Frequency'].sum()","f17c0465":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(tools_to_analyze.index, tools_to_analyze.Tools_Used_To_Analyze_Data_At_Workplace.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Tools_Used_To_Analyze_Data_At_Workplace\", y=\"perc\",data=tools_to_analyze)\nax.set_title('Proportion of tools used to analyze data by Indian Data Scientists in 2019',fontsize=15)\nax.set(xlabel='Tools Used', ylabel='Percentage of Participants in the survey')\n","195350d8":"usa = req[req.Q3 == 'United States of America'].reset_index(drop=True)","f7bc641b":"us_data_scientists = usa[usa.Q5 == 'Data Scientist'].reset_index(drop=True)\n","c5722b25":"tools_to_analyze = us_data_scientists.groupby('Q14').count().reset_index().loc[:,'Q14':'Time from Start to Finish (seconds)'].rename(columns={'Q14':'Tools_Used_To_Analyze_Data_At_Workplace','Time from Start to Finish (seconds)':'Frequency'})","1c69db76":"tools_to_analyze = tools_to_analyze[tools_to_analyze.Tools_Used_To_Analyze_Data_At_Workplace != 0]","af1bb1a2":"tools_to_analyze.sort_values(by=['Frequency'],inplace=True, ascending=False)","8f9d011c":"tools_to_analyze['perc'] = (tools_to_analyze['Frequency']*100)\/tools_to_analyze['Frequency'].sum()","5a8d6651":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(tools_to_analyze.index, tools_to_analyze.Tools_Used_To_Analyze_Data_At_Workplace.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Tools_Used_To_Analyze_Data_At_Workplace\", y=\"perc\",data=tools_to_analyze)\nax.set_title('Proportion of tools used to analyze data by US data scientists in 2019',fontsize=15)\nax.set(xlabel='Tools Used', ylabel='Percentage of Participants in the survey')\n","a9712a15":"#Comparison of cloud based API's usage among participants from 2018- 2019\nindia_ds_2018 = india2[india2.Q6 == 'Data Scientist'].reset_index(drop=True)\n","438960ac":"tools_to_analyze = india_ds_2018.groupby('Q12_MULTIPLE_CHOICE').count().reset_index().loc[:,'Q12_MULTIPLE_CHOICE':'Time from Start to Finish (seconds)'].rename(columns={'Q12_MULTIPLE_CHOICE':'Tools_Used_To_Analyze_Data_At_Workplace','Time from Start to Finish (seconds)':'Frequency'})","426e4148":"tools_to_analyze = tools_to_analyze[tools_to_analyze.Tools_Used_To_Analyze_Data_At_Workplace != 0]","7694e0d3":"tools_to_analyze.sort_values(by=['Frequency'],inplace=True, ascending=False)","eecf1f34":"tools_to_analyze['perc'] = (tools_to_analyze['Frequency']*100)\/tools_to_analyze['Frequency'].sum()","4c557eaa":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(tools_to_analyze.index, tools_to_analyze.Tools_Used_To_Analyze_Data_At_Workplace.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Tools_Used_To_Analyze_Data_At_Workplace\", y=\"perc\",data=tools_to_analyze)\nax.set_title('Proportion of tools used to analyze data by India data scientists in 2018',fontsize=15)\nax.set(xlabel='Tools Used', ylabel='Percentage of Participants in the survey')\n","b5dde0c0":"ide_used = ['Q16_Part_1','Q16_Part_2','Q16_Part_3','Q16_Part_4','Q16_Part_5','Q16_Part_6','Q16_Part_7','Q16_Part_8','Q16_Part_9','Q16_Part_10','Q16_Part_11','Q16_Part_12']\nl = list()\nfor i in ide_used:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'ide_used'}, inplace = True)\n    l.append(df)\nide_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","f08a4e3c":"ide_used = ide_used[ide_used.ide_used!=0].reset_index(drop=True)","90f61de0":"ide_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","b0ecaa38":"ide_used['perc'] = (ide_used['Frequency']*100)\/ide_used['Frequency'].sum()","15774a70":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(ide_used.index, ide_used.ide_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"ide_used\", y=\"perc\",data=ide_used)\nax.set_title('Proportion of IDEs used to analyze data by Indian data scientists in 2019',fontsize=15)\nax.set(xlabel='IDEs Used', ylabel='Percentage of Participants in the survey')\n","4149e87f":"ide_used = ['Q16_Part_1','Q16_Part_2','Q16_Part_3','Q16_Part_4','Q16_Part_5','Q16_Part_6','Q16_Part_7','Q16_Part_8','Q16_Part_9','Q16_Part_10','Q16_Part_11','Q16_Part_12']\nl = list()\nfor i in ide_used:\n    df = us_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'ide_used'}, inplace = True)\n    l.append(df)\nide_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","781927d8":"ide_used = ide_used[ide_used.ide_used!=0].reset_index(drop=True)","9982d959":"ide_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","6e3ac433":"ide_used['perc'] = (ide_used['Frequency']*100)\/ide_used['Frequency'].sum()","b105dbd0":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(ide_used.index, ide_used.ide_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"ide_used\", y=\"perc\",data=ide_used)\nax.set_title('Proportion of IDEs used to analyze data by US data scientists in 2019',fontsize=15)\nax.set(xlabel='IDEs Used', ylabel='Percentage of Participants in the survey')\n","1d2caaeb":"languages_used = ['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_Part_7','Q18_Part_8','Q18_Part_9','Q18_Part_10','Q18_Part_11','Q18_Part_12']\nl = list()\nfor i in languages_used:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'languages_used'}, inplace = True)\n    l.append(df)\nlanguages_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","370b69b6":"languages_used = languages_used[languages_used.languages_used!=0].reset_index(drop=True)","04041e96":"languages_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","47fedd07":"languages_used['perc'] = (languages_used['Frequency']*100)\/languages_used['Frequency'].sum()","7473f762":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(languages_used.index, languages_used.languages_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"languages_used\", y=\"perc\",data=languages_used)\nax.set_title('Proportion of Programming Languages used to analyze data by Indian data scientists in 2019',fontsize=15)\nax.set(xlabel='Programming Languages Used', ylabel='Percentage of Participants in the survey')\n","df1c0c6a":"languages_used = ['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_Part_7','Q18_Part_8','Q18_Part_9','Q18_Part_10','Q18_Part_11','Q18_Part_12']\nl = list()\nfor i in languages_used:\n    df = us_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'languages_used'}, inplace = True)\n    l.append(df)\nlanguages_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","8ffd5d6b":"languages_used = languages_used[languages_used.languages_used!=0].reset_index(drop=True)","b869f779":"languages_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","8d402278":"languages_used['perc'] = (languages_used['Frequency']*100)\/languages_used['Frequency'].sum()","91ef3883":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(languages_used.index, languages_used.languages_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"languages_used\", y=\"perc\",data=languages_used)\nax.set_title('Proportion of Programming Languages used to analyze data by US data scientists in 2019',fontsize=15)\nax.set(xlabel='Programming Languages Used', ylabel='Percentage of Participants in the survey')\n","d48f571d":"data_vis_used = ['Q20_Part_1','Q20_Part_2','Q20_Part_3','Q20_Part_4','Q20_Part_5','Q20_Part_6','Q20_Part_7','Q20_Part_8','Q20_Part_9','Q20_Part_10','Q20_Part_11','Q20_Part_12']\nl = list()\nfor i in data_vis_used:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'data_vis_used'}, inplace = True)\n    l.append(df)\ndata_vis_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","9ee1a5cf":"data_vis_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","d11658d8":"data_vis_used['perc']= (data_vis_used['Frequency']*100)\/data_vis_used['Frequency'].sum()","4680c0cd":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(data_vis_used.index, data_vis_used.data_vis_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"data_vis_used\", y=\"perc\",data=data_vis_used)\nax.set_title('Proportion of Data Visualization Tools used to analyze data by Indian data scientists in 2019',fontsize=15)\nax.set(xlabel='Data Visualization Used', ylabel='Percentage of Participants in the survey')\n","992d1a0c":"data_vis_used = ['Q20_Part_1','Q20_Part_2','Q20_Part_3','Q20_Part_4','Q20_Part_5','Q20_Part_6','Q20_Part_7','Q20_Part_8','Q20_Part_9','Q20_Part_10','Q20_Part_11','Q20_Part_12']\nl = list()\nfor i in data_vis_used:\n    df = us_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'data_vis_used'}, inplace = True)\n    l.append(df)\ndata_vis_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","ad5002f1":"data_vis_used = data_vis_used[data_vis_used.data_vis_used!=0].reset_index(drop=True)","4c969b25":"data_vis_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","5aab94d6":"data_vis_used['perc']=(data_vis_used['Frequency']*100)\/data_vis_used['Frequency'].sum()","b2875114":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(data_vis_used.index, data_vis_used.data_vis_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"data_vis_used\", y=\"perc\",data=data_vis_used)\nax.set_title('Proportion of Data Visualization Tools used to analyze data by US data scientists in 2019',fontsize=15)\nax.set(xlabel='Data Visualization Used', ylabel='Percentage of Participants in the survey')\n","68662ddc":"languages_used = ['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_Part_7','Q18_Part_8','Q18_Part_9','Q18_Part_10','Q18_Part_11','Q18_Part_12']\nl = list()\nfor i in languages_used:\n    df = india.groupby(['Q5',i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'languages_used','Q5':'Profession'}, inplace = True)\n    l.append(df)\nlanguages_used = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","201186c3":"languages_used = languages_used[languages_used.languages_used!=0].reset_index(drop=True)","2bc6f852":"languages_used.sort_values(by=['Frequency'],ascending=False,inplace=True)","495d4ee6":"languages_used['perc']=(languages_used['Frequency']*100)\/(languages_used['Frequency'].sum())","fa9de862":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(languages_used.index, languages_used.languages_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"languages_used\", y=\"perc\",hue='Profession',data=languages_used)\nax.set_title('Proportion of Data Visualization Tools used to analyze data by Indian Survey Participants of different Work profession in 2019',fontsize=15)\nax.set(xlabel='Data Visualization Used', ylabel='Percentage of Participants in the survey')\n","bfb157ed":"tpu_usage = indian_data_scientists.groupby('Q22').count().reset_index().loc[:,:'Time from Start to Finish (seconds)']","971fafc9":"tpu_usage.rename(columns = {'Q22':'tpu_usage','Time from Start to Finish (seconds)':'Frequency'},inplace=True)","02293e86":"tpu_usage.sort_values(by=['Frequency'],ascending=False,inplace=True)","bfdc4800":"tpu_usage['perc']=(tpu_usage['Frequency']*100)\/tpu_usage['Frequency'].sum()","f338159e":"#import seaborn as sns\n#import matplotlib.pyplot as plt\n#fig, ax = plt.subplots(1, 1, figsize=(15,5))\n\n#plt.xticks(hardware_used.index, hardware_used.hardware_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\n#sns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\n#ax = sns.barplot(x=\"hardware_used\", y=\"perc\",data=hardware_used)\n#ax.set_title('Proportion of Specialized Hardware used by India data scientists in 2019',fontsize=15)\n#ax.set(xlabel='Specialized Hardware Used', ylabel='Percentage of Participants in the survey')\n\n#import pandas.plotting \n#from pandas.plotting import table\n#plt.figure(figsize=(16,8))\n# plot chart\n#ax1 = plt.subplot(121, aspect='equal')\n#tpu_usage.plot(kind='pie', y = 'perc', ax=ax1, autopct='%1.1f%%', \n #startangle=50, shadow=False, labels=tpu_usage['tpu_usage'], legend = False, fontsize=7)\n#ax1.set_title('Proportion of TPU Usage By Data Scientists taking the survey in India')\n\n#ax2 = plt.subplot(122)\n#plt.axis('off')\n##tbl = table(ax2, tpu_usage, loc='center')\n#tbl.auto_set_font_size(False)\n#tbl.set_fontsize(10)\n#plt.show()\n\nlabels = tpu_usage['tpu_usage']\nvalues = tpu_usage['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\"Proportion of TPU Usage By Data Scientists taking the survey in India\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=13,\n        color=\"#7f7f7f\"\n    )\n)\n","504c3b1a":"tpu_usage = us_data_scientists.groupby('Q22').count().reset_index().loc[:,:'Time from Start to Finish (seconds)']","5554bcdf":"tpu_usage.rename(columns = {'Q22':'Number_of_times_TPU_is_used','Time from Start to Finish (seconds)':'Frequency'},inplace=True)","5992a51d":"tpu_usage.sort_values(by=['Frequency'],ascending=False,inplace=True)","a71707d8":"tpu_usage['perc']=(tpu_usage['Frequency']*100)\/tpu_usage['Frequency'].sum()","e7420000":"#import seaborn as sns\n#import matplotlib.pyplot as plt\n#fig, ax = plt.subplots(1, 1, figsize=(15,5))\n\n#plt.xticks(hardware_used.index, hardware_used.hardware_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\n#sns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\n#ax = sns.barplot(x=\"hardware_used\", y=\"perc\",data=hardware_used)\n#ax.set_title('Proportion of Specialized Hardware used by India data scientists in 2019',fontsize=15)\n#ax.set(xlabel='Specialized Hardware Used', ylabel='Percentage of Participants in the survey')\n\n#import pandas.plotting \n#from pandas.plotting import table\n#plt.figure(figsize=(16,8))\n# plot chart\n#ax1 = plt.subplot(121, aspect='equal')\n#tpu_usage.plot(kind='pie', y = 'perc', ax=ax1, autopct='%1.1f%%', \n #startangle=50, shadow=False, labels=tpu_usage['Number_of_times_TPU_is_used'], legend = False, fontsize=7)\n#ax1.set_title('Proportion of TPU Usage By Data Scientists taking the survey in US')\n\n#ax2 = plt.subplot(122)\n#plt.axis('off')\n#tbl = table(ax2, tpu_usage, loc='center')\n#tbl.auto_set_font_size(False)\n#tbl.set_fontsize(10)\n#plt.show()\n#import plotly.graph_objects as go\n\nlabels = tpu_usage['Number_of_times_TPU_is_used']\nvalues = tpu_usage['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\"Proportion of TPU Usage By Data Scientists taking the survey in US\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=13,\n        color=\"#7f7f7f\"\n    )\n)\n","84997cee":"alg_ml = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_Part_8','Q24_Part_9','Q24_Part_10','Q24_Part_11','Q24_Part_12']\nl = list()\nfor i in alg_ml:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'alg_ml'}, inplace = True)\n    l.append(df)\nalg_ml = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","04700c6d":"alg_ml.sort_values(by=['Frequency'],ascending=False,inplace=True)","462afba3":"alg_ml['perc']=(alg_ml['Frequency']*100)\/alg_ml['Frequency'].sum()","3fdea094":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(alg_ml.index, alg_ml.alg_ml.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"alg_ml\", y=\"perc\",data=alg_ml)\nax.set_title('Proportion of ML algorithms used by India data scientists in 2019',fontsize=15)\nax.set(xlabel='ML algorithms', ylabel='Percentage of Participants in the survey')\n","f11bea9b":"alg_ml = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_Part_8','Q24_Part_9','Q24_Part_10','Q24_Part_11','Q24_Part_12']\nl = list()\nfor i in alg_ml:\n    df = us_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'alg_ml'}, inplace = True)\n    l.append(df)\nalg_ml = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","ae7591a6":"alg_ml.sort_values(by=['Frequency'],ascending=False,inplace=True)","15218b1c":"alg_ml['perc']=(alg_ml['Frequency']*100)\/alg_ml['Frequency'].sum()","ee3ea9b1":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(alg_ml.index, alg_ml.alg_ml.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"alg_ml\", y=\"perc\",data=alg_ml)\nax.set_title('Proportion of ML algorithms used by US data scientists in 2019',fontsize=15)\nax.set(xlabel='ML algorithms', ylabel='Percentage of Participants in the survey')\n","a0f0f9f0":"nlp = ['Q27_Part_1','Q27_Part_2','Q27_Part_3','Q27_Part_4','Q27_Part_5','Q27_Part_6']\nl = list()\nfor i in nlp:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'nlp'}, inplace = True)\n    l.append(df)\nnlp = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","ad521928":"nlp.sort_values(by=['Frequency'],ascending=False,inplace=True)","85c96a54":"\nnlp['perc']=(nlp['Frequency']*100)\/nlp['Frequency'].sum()","eb32241a":"#import seaborn as sns\n#import matplotlib.pyplot as plt\n#fig, ax = plt.subplots(1, 1, figsize=(15,5))\n\n#plt.xticks(hardware_used.index, hardware_used.hardware_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\n#sns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\n#ax = sns.barplot(x=\"hardware_used\", y=\"perc\",data=hardware_used)\n#ax.set_title('Proportion of Specialized Hardware used by India data scientists in 2019',fontsize=15)\n#ax.set(xlabel='Specialized Hardware Used', ylabel='Percentage of Participants in the survey')\n\nimport pandas.plotting \nfrom pandas.plotting import table\nplt.figure(figsize=(16,8))\n# plot chart\nax1 = plt.subplot(121, aspect='equal')\nnlp.plot(kind='pie', y = 'perc', ax=ax1, autopct='%1.1f%%', \n startangle=50, shadow=False, labels=nlp['nlp'], legend = False, fontsize=10)\nax1.set_title('Proportion of NLP techniques used By Data Scientists taking the survey in India')\n\n\n","ff3327af":"nlp = ['Q27_Part_1','Q27_Part_2','Q27_Part_3','Q27_Part_4','Q27_Part_5','Q27_Part_6']\nl = list()\nfor i in nlp:\n    df = us_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'NLP_Techniques'}, inplace = True)\n    l.append(df)\nnlp = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","1bc90a70":"nlp.sort_values(by=['Frequency'],ascending=False,inplace=True)","591f2a7c":"\nnlp['perc']=(nlp['Frequency']*100)\/nlp['Frequency'].sum()","db15769d":"#import seaborn as sns\n#import matplotlib.pyplot as plt\n#fig, ax = plt.subplots(1, 1, figsize=(15,5))\n\n#plt.xticks(hardware_used.index, hardware_used.hardware_used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\n#sns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\n#ax = sns.barplot(x=\"hardware_used\", y=\"perc\",data=hardware_used)\n#ax.set_title('Proportion of Specialized Hardware used by India data scientists in 2019',fontsize=15)\n#ax.set(xlabel='Specialized Hardware Used', ylabel='Percentage of Participants in the survey')\n\nimport pandas.plotting \nfrom pandas.plotting import table\nplt.figure(figsize=(16,8))\n# plot chart\nax1 = plt.subplot(121, aspect='equal')\nnlp.plot(kind='pie', y = 'perc', ax=ax1, autopct='%1.1f%%', \n startangle=50, shadow=False, labels=nlp['NLP_Techniques'], legend = False, fontsize=10)\nax1.set_title('Proportion of NLP techniques used By Data Scientists taking the survey in US')\n\n\n","ddb7d422":"framework = ['Q28_Part_1','Q28_Part_2','Q28_Part_3','Q28_Part_4','Q28_Part_5','Q28_Part_6','Q28_Part_7','Q28_Part_8','Q28_Part_9','Q28_Part_10','Q28_Part_11','Q28_Part_12']\nl = list()\nfor i in framework:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'ML_framework_tools'}, inplace = True)\n    l.append(df)\nframework = pd.concat(l).reset_index(drop=True)\n\n#rawn = role_at_work_num.reset_index(drop=True)","7e021a84":"framework.sort_values(by=['Frequency'],inplace=True,ascending=False)","bb40de94":"framework['perc'] = ((framework['Frequency']*100)\/framework['Frequency'].sum())","8b6fcd48":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(framework.index, framework.ML_framework_tools.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"ML_framework_tools\", y=\"perc\",data=framework)\nax.set_title('Proportion of Machine Learning Framework tools used by India data scientists in 2019',fontsize=15)\nax.set(xlabel='Machine Learning Tools ', ylabel='perc')","dc43a761":"framework = ['Q28_Part_1','Q28_Part_2','Q28_Part_3','Q28_Part_4','Q28_Part_5','Q28_Part_6','Q28_Part_7','Q28_Part_8','Q28_Part_9','Q28_Part_10','Q28_Part_11','Q28_Part_12']\nl = list()\nfor i in framework:\n    df = us_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'ML_framework_tools'}, inplace = True)\n    l.append(df)\nframework = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","2008510d":"framework.sort_values(by=['Frequency'],inplace=True,ascending=False)","0cf19901":"framework['perc'] = ((framework['Frequency']*100)\/framework['Frequency'].sum())","5356774c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(framework.index, framework.ML_framework_tools.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"ML_framework_tools\", y=\"perc\",data=framework)\nax.set_title('Proportion of Machine Learning Framework tools used by US data scientists in 2019',fontsize=15)\nax.set(xlabel='Machine Learning Tools ', ylabel='perc')","f757802a":"cloud_platforms = ['Q29_Part_1','Q29_Part_2','Q29_Part_3','Q29_Part_4','Q29_Part_5','Q29_Part_6','Q29_Part_7','Q29_Part_8','Q29_Part_9','Q29_Part_10','Q29_Part_11','Q29_Part_12']\nl = list()\nfor i in cloud_platforms:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Cloud_Platforms_Used'}, inplace = True)\n    l.append(df)\ncloud = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","465d12f1":"cloud.sort_values(by=['Frequency'],inplace=True,ascending=False)","e6d2026a":"cloud['perc'] = ((cloud['Frequency']*100)\/cloud['Frequency'].sum())","e52288e6":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(cloud.index, cloud.Cloud_Platforms_Used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Cloud_Platforms_Used\", y=\"perc\",data=cloud)\nax.set_title('Proportion of Cloud Platforms used by Indian data scientists in 2019',fontsize=15)\nax.set(xlabel='Cloud Platforms', ylabel='perc')","be8f0edc":"cloud_platforms = ['Q15_Part_1','Q15_Part_2','Q15_Part_3','Q15_Part_4','Q15_Part_5','Q15_Part_6','Q15_Part_7']\nl = list()\nfor i in cloud_platforms:\n    df = india_ds_2018.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Cloud_Platforms_Used'}, inplace = True)\n    l.append(df)\ncloud = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","8b3ea2ef":"cloud.sort_values(by=['Frequency'],inplace=True,ascending=False)","abf5c59d":"cloud['perc'] = ((cloud['Frequency']*100)\/cloud['Frequency'].sum())","e52f8103":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(15,5))\n\nplt.xticks(cloud.index, cloud.Cloud_Platforms_Used.str.upper(), rotation=60, horizontalalignment='right', fontsize=10)\nsns.set(style=\"whitegrid\")\n#tips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"Cloud_Platforms_Used\", y=\"perc\",data=cloud)\nax.set_title('Proportion of Cloud Platforms used by Indian data scientists in 2018',fontsize=15)\nax.set(xlabel='Cloud Platforms', ylabel='perc')","d489b29f":"big_data = ['Q31_Part_1','Q31_Part_2','Q31_Part_3','Q31_Part_4','Q31_Part_5','Q31_Part_6','Q31_Part_7','Q31_Part_8','Q31_Part_9','Q31_Part_10','Q31_Part_11','Q31_Part_12']\nl = list()\nfor i in big_data:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Big_Data_Products_Used_in_2019'}, inplace = True)\n    l.append(df)\nbig_data = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","30caab00":"big_data.sort_values(by=['Frequency'],inplace=True,ascending=False)","b017dee1":"big_data['perc'] = ((big_data['Frequency']*100)\/big_data['Frequency'].sum())","99cb8968":"import matplotlib.pyplot as plot\nax = big_data.plot.barh(x='Big_Data_Products_Used_in_2019', y='perc', title=\"Usage of big data products by data scientists in India in 2019\");\n\n\n# Despine\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\n# Switch off ticks\nax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n\n# Draw vertical axis lines\nvals = ax.get_xticks()\nfor tick in vals:\n    ax.axvline(x=tick, linestyle='dashed', alpha=0.4, zorder=1)\n\n# Set x-axis label\nax.set_xlabel(\"Big Data products used\", labelpad=20, weight='bold', size=12)\n\n# Set y-axis label\nax.set_ylabel(\"Proportion of people\", labelpad=20, weight='bold', size=12)\n\n# Format y-axis label\n#ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,g}'))\nplot.show(block=True)","6bcea54d":"big_data2 = ['Q30_Part_1','Q30_Part_2','Q30_Part_3','Q30_Part_4','Q30_Part_5','Q30_Part_6','Q30_Part_7','Q30_Part_8','Q30_Part_9','Q30_Part_10','Q30_Part_11','Q30_Part_12','Q30_Part_13','Q30_Part_14','Q30_Part_15','Q30_Part_16','Q30_Part_17','Q30_Part_18','Q30_Part_19','Q30_Part_20','Q30_Part_21','Q30_Part_22','Q30_Part_23','Q30_Part_24']\nl = list()\nfor i in big_data2:\n    df = india_ds_2018.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Big_Data_Products_Used_in_2018'}, inplace = True)\n    l.append(df)\nbig_data2 = pd.concat(l).reset_index(drop=True)","26da9624":"big_data2.sort_values(by=['Frequency'],inplace=True,ascending=False)","b540a5de":"big_data2['perc'] = ((big_data2['Frequency']*100)\/big_data2['Frequency'].sum())","c6b45afa":"import matplotlib.pyplot as plot\nax = big_data2.plot.barh(x='Big_Data_Products_Used_in_2018', y='perc', title=\"Usage of big data products by data scientists in India in 2018\");\n\n\n# Despine\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\n# Switch off ticks\nax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n\n# Draw vertical axis lines\nvals = ax.get_xticks()\nfor tick in vals:\n    ax.axvline(x=tick, linestyle='dashed', alpha=0.4, zorder=1)\n\n# Set x-axis label\nax.set_xlabel(\"Big Data products used\", labelpad=20, weight='bold', size=12)\n\n# Set y-axis label\nax.set_ylabel(\"Proportion of people\", labelpad=20, weight='bold', size=12)\n\n# Format y-axis label\n#ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,g}'))\nplot.show(block=True)","088d3fa9":"#Q34, 12 Parts\nrel_dat = ['Q29_Part_1','Q29_Part_2','Q29_Part_3','Q29_Part_4','Q29_Part_5','Q29_Part_6','Q29_Part_7','Q29_Part_8','Q29_Part_9','Q29_Part_10','Q29_Part_11','Q29_Part_12','Q29_Part_13','Q29_Part_14','Q29_Part_15','Q29_Part_16','Q29_Part_17','Q29_Part_18','Q29_Part_19','Q29_Part_20','Q29_Part_21','Q29_Part_22','Q29_Part_23','Q29_Part_24','Q29_Part_7','Q29_Part_8','Q29_Part_9','Q29_Part_10','Q29_Part_11','Q29_Part_12','Q29_Part_13','Q29_Part_14','Q29_Part_15','Q29_Part_16','Q29_Part_17','Q29_Part_18','Q29_Part_19','Q29_Part_20','Q29_Part_21','Q29_Part_22','Q29_Part_23','Q29_Part_24','Q29_Part_25','Q29_Part_26','Q29_Part_27','Q29_Part_28']\nl = list()\nfor i in rel_dat:\n    df = india_ds_2018.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Relational_Database_Used_in_2018'}, inplace = True)\n    l.append(df)\nrel_dat = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","1187479f":"rel_dat.sort_values(by=['Frequency'],inplace=True,ascending=False)\n","e501b58a":"rel_dat = rel_dat.reset_index(drop=True)","4c610a1e":"rel_dat['perc'] = ((rel_dat['Frequency']*100)\/rel_dat['Frequency'].sum())","55c7afc1":"labels = rel_dat['Relational_Database_Used_in_2018']\nvalues = rel_dat['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\" 2018 Relational Database Tools used By Data Scientists in India\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=13,\n        color=\"#7f7f7f\"\n    )\n)\n","33a919ee":"#Q34, 12 Parts\nrel_dat = ['Q34_Part_1','Q34_Part_2','Q34_Part_3','Q34_Part_4','Q34_Part_5','Q34_Part_6','Q34_Part_7','Q34_Part_8','Q34_Part_9','Q34_Part_10','Q34_Part_11','Q34_Part_12']\nl = list()\nfor i in rel_dat:\n    df = indian_data_scientists.groupby([i]).count().loc[:,:'Time from Start to Finish (seconds)'].reset_index()\n    df.rename(columns = {'Time from Start to Finish (seconds)':'Frequency'}, inplace = True)\n    df.rename(columns = {i:'Relational_Database_Used_in_2019'}, inplace = True)\n    l.append(df)\nrel_dat = pd.concat(l).reset_index(drop=True)\n#rawn = role_at_work_num.reset_index(drop=True)","d358f0df":"rel_dat.sort_values(by=['Frequency'],inplace=True,ascending=False)\n","40910255":"rel_dat['perc'] = ((rel_dat['Frequency']*100)\/rel_dat['Frequency'].sum())","ffdb1852":"labels = rel_dat['Relational_Database_Used_in_2019']\nvalues = rel_dat['perc']\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title=\"2019 Relational Database Tools used By Data Scientists in India\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=13,\n        color=\"#7f7f7f\"\n    )\n)\n","3fe770f5":"1. Comparing the NLP techniques used. Higher proportion of US data scientists dont implement NLP techniques as compared to India data scientists. Other trends seem to be the same.\nNow comparing machine learning framework tools","ce696868":"<img src=\"http:\/\/images.clipartpanda.com\/scientist-clipart-Rcdg5eLai.gif\" width=\"200\"> \nDuring my coursework in grad school, I took several machine learning courses where I was int commonly faced issue or a commonly faced search parameter which most aspiring data scientists use is: \"What are the common skill-sets required to become a data scientist\" or \"What level of coding knowldege is required to land a good job\". These questions have various forms of answers on the internet but there's never been a source of hard and fast, clean data exactly describing and answering the very same questions any newbie in the field might possess. Being from India, and currently in the US, my curiosity to know how data scientists are different from the two countries started to build. I decided to conduct a comparative analysis of data-scientists in India and compare them .","dbde0cc8":"Higher proportion of SQL and R users amongst data scientists in the US. An interesting observation is that data scientists in the US seem to prefer Bash while data scientists in India are not soo fond of the programming language. Bash has become by far the most popular shell among users of Linux, becoming the default interactive shell on that operating system's various distributions which can imply that more users seem to be using Bash in the US as thre are more Linux servers in the US as compared to India. Linux is preferred to Windows in some cases as Linux is open source and it's source code can be modified according to the needs of the Now what can we imply from this ?\n\n<img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/3\/3a\/Hacker_inside.jpg\" width=\"200\">\nUS has been poised to a lot of secruity breaches and securing systems from hacking and other potential threats has been something the US has ever increasingly been working towards. This might imply that companies would prefer to maintain Linux servers.\nWhile no system is immune to hacking and malware attacks, Linux tends to be a low-profile target. Because Windows runs the majority of the software in the world, hackers head for the low-hanging fruit\u2014Windows.","09c4d845":"As we can see, there is a shift from most used Big Data anlytical product from Databricks in 2018 to Google BigQuery in 2019. While both Databricks and Google BigQuery are categorized as Big Data Processing and Distribution, Google BigQuery is categorized as Data Warehouse.Google cloud machine learning engines software and tools are great at building ML models using data and make predictions. Use of ML to analyze data sets and create a better software for a given problem is a new territory, being explored across various industries. Google being one of the pioneers in machine learning and artificial intelligence have well developed framework compared to other players in filed. For example google tensor flow framework is quite helpful to get started with speech recognition and image analysis which is why, in this day and age where ML application has become soo extensive, more people would to go for the more versatile Google BigQuery over databricks","9137f95f":"Being a student, with no background in the field of data-science, I came to the US just like any other graduate student with a lot of opportunities and career paths to choose from and pursue. I was someone who was exposed to the \"Indian\" style of education for most of my life, I noticed a drastic change in methods used to teach various subjects in the US. Both of the methods had it's unique pro's and con's. \n<img src=\"http:\/\/images.clipartpanda.com\/pc-clipart-0511-1111-0813-0133_Little_Nerd_Boy_Sitting_on_Phone_Books_so_He_Can_Work_at_a_PC_Computer_clipart_image.jpg\" width=\"200\">\nI noticed that the educational system in the US trains it's students to handle hands-on situations better while not emphasizing much on whats written inside a textbook as compared to the Indian education system. But at the same time,the level of in-depth textual knowledge of important concepts a student picks up in a school in the Indian education system is much more as compared to the US. (something I noticed on a personal level)\n\nAfter enrolling for multiple courses during my graduate school pertaining to machine learning, my interest in the field of AI and machine learning grew more than ever as I was researching various algorithms and coding techniques to excel in the same and this is when something struck me:\n<img src=\"https:\/\/chatboten.com\/wp-content\/uploads\/2016\/12\/robot-machine-learning.png\" width=\"200\"> \n\"Data science is a field which requires a vivid and strong understanding of theoritical mathematical concepts. At the same time, application of these algorithms and translating them into code and again translating it back into results which can be understood by a newbie in the field requires a lot of hands-on experience.\n    \n    \nThis made me wonder: Has the education system brought up differently skilled data-scientists in India versus the USA. Can small changes made in the susequent education systems bring about a holisitic change in the data scientists of the future generation.\n\nPutting across my line of thought, let's begin this small journey of mine.\nFollowing are the contents of my analyses\n","0818b919":" **Chapter 1: How is AI Interest in India Manifesting Itself Today?**\n\nWith the government\u2019s growing interest around AI applications in India, Deepak Garg the Director at NVIDIA-Bennett Center of Research in Artificial Intelligence (and Director LeadingIndia.ai) believes that there has been a significant growth in interest levels around AI across all industry sectors in India. He explains that although AI attention is considerably smaller in India than in China or the USA, the increased AI interest has manifested itself in the following three ways:\n1) Industries have started working to skill their manpower to enable themselves to compete with other global players\n2) Educational institutions have started working on their curricula to include courses on machine learning and other relevant areas\n3) Individuals and professionals have started acquiring these skills and are comfortable investing in upgrading their own skills\nThese three areas will be the main focus of preliminary analysis to understand the extent to which AI has manifested itself in the population of India. First, let's see how industries have been adapting themselves to nurture AI in their existing ecosystems. Let's look at how much the distribtution of various participants in the survey.\n","471cad64":"Matplotlib and seaborn seem to be leading the race followed the R pacakge ggplot\/ggplot2 amongst our indian data scientists as most of applications are being run on Python and Python based IDE'S. Now, let's see whats happening in the US","58a29157":"\nAWS seems to be leading cloud service provider to data scientists. Part of the reason for its popularity is undoubtedly the massive scope of its operations. Though the GCP entered late into the market, it can be seen from the above statistics that GCP has caught up in the race towards cloud storage and comuting over Microsoft Azure. The increase the proportion of GCP versus Azure can be related to the fact that as the companies all over are increasing in size and trying to implement AI related techniques into their existing business sytems, the volumes of data processed are continually increasing forcing them to resort to cloud platforms with *specialized in-built big data analytics and machine learning* like that found in Google cloud Platform such as BigQyery and AWS RedShift  \\\n\n\n\nAnother interesting point to be noted is the increase in proportion of google cloud platform users in India. The company had been struggling initally against the lights of Azure and AWS, but the recent move to appoint Thomas Kuried(ex-Oracle head) as the leader of the GCP platform can be an indicator of a measure taken to try and increase sales of the google cloud in India(which might be indicated by a rise in the proportion of GCP user\n","b4550a33":"As expected, from the previous discussion, huge dominance by Python in the list of programming languages used by data scientists in India. Lets switch the focus to data scientists in the US. It can also be noticed that data scientists in India tend to stick to the top three languages as proportion of data scientists using the other languages is significantly low(below 3 percent)","1cfdb6f7":"Let's see the favorite sources students in India like to follow.","3480c5b4":"It is getting clear from the above plot that as expected, programming  languages C,C++ and Javascript are more often used by software engineers as expected(primary part of backend coding is carried out either in C or C++)\nStudents dominate the usage of most other platforms. But an interesting point to be noted is that, most data analysts seem to use R over Python\nStudents and Sofware engineers use more python over R. This is because R is mainly a statitical software which migh not find much application amongst Software Engineers and Students. But, data analysts tend to prefer R over Python. Now why might this be.\nMy assumption is that given the job of a data analyst, pulling out plots to support his analysis is a very important part of his job. This is facilitated by better visualization packages in R and for thos companies who don't want to invest on a separate visualization tool like Tableau, R seems to be the next best bet. \nNow moving on back to our original analysis of Indian Data scientists versus the US Data Scientists, ","2c096e99":"Now let's see the segregation of various age-groups of participants from India in the survey.","781655af":"From the above analyses, it is pretty evident that there is larger proportion of data scientists using MySQL in  versus PostGres SQL in 2019.","a2f5daae":"\nOver 30 percentage of the respondents seem to be jupyter notebooks which is almost double of the number of participants using RStudio. The large domination of jupyter over RStudio can be probably correlated to the preference of coding language of participants with an overwhelming preference towards Python over R.\nNow, let' see see how the folks in the US are doing. \nThe difference in proportion of IDE usage is comparable in India vs the US. This can be correlated to the fact that Indians, in general, not trying ot be biased, have a tendency to acquire the \"jack-of-all\" knowledge. That is, people tend to have the tendency to be \"well versed in a lot\" over \"specialized in a few\" in India which might be translated into comparable ranges of IDE's used.","8de3dfc6":"Similar trends seem to exist between both the categories of data scientists. Noticeable differences are increased usage of the software Shiny by participants in the US as compared to the participants in India which can be correlated to the fact that proportion of data scientists using R from the US is higher and hence, the relevant packages would be used more often. Now, let's move on to the specialized hardware usage by data scientists in India.\nThis is the second time I've observed a trend between the country and proportion of R users. Taking a deeper dive into what professions usally use R and what profession usually use Python","deca6401":"It is important to note that India has a huge scope for extensive NLP applications especially considering the wide variety of languages in the couuntry and a potential to build an advnaced neural network which can encapture all these languages effectively so that AI technologies can be successfully implemented from a language level for better communication with different communities in India.","f3415e5e":"Data analysis finds it's application at the highest level with the help of open source IDE'S like RStudio and JupyterLab which is translated in the above plot. The trend seems to be carried on from 2018 and as expected, since the infrastructure invested in educational institutions in the field of data science is just emerging, you wouldn't expect to find a lot of universities investing on paid platfroms like Salesforce, Tableau, etc. or advanced statistical softwares to teach students. ","2085d843":"\n\nAs from the above plot, logistic regression is the most widely used ML algorithm by data-scientists in India as logistic regression is one such model which has an added advantage of ease of explanability to a person with no data-science background to convery the results. When the label is that of categorical or discrete nature, we use log odds ratio to optimize business problems such as \u2013 scoring customers and then predict those that are most likely to default on the loan payment, or predict higher response rates to a certain marketing strategy and the explanation of these labels and features would be easier for a data scientist to a newbie in the field versus explaining a deep neural network to the same person. ","06f4e11e":"It is pretty evident from the above statistics that the leaders in the big data analytic products hail from a \"Amazon\" or \"Google\" background hence, making it sensible to incorporate the usage of GCP's and AWS's into their company ecosystems. Databricks is a one of softwares hailing from the other big name in the tech industry, \"Microsoft\".","394466f9":"<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/04\/jupyter.png\" width=\"200\">\nThe above observation is again due to the fact that a large number of data scientist survey participants from India study in universities that tend not to invest in expenisive software like SPSS, Tableau, SAS etc. Again, as mentioned earlier, a large proportion of the participants seem to be using free local IDE's such as JupyterLab for data analysis due to ease of access and range of availability of functions.\n\n\n\nIt is interesting to see that the use of basic statistical software is on the decline and cloud based API's are on the rise which can be explained to the fact that older methods for storing data by companies in India has become difficult as India is a developing country increase relations with more companies and as trade volumes increase, the amount of data being transactioned on a daily basis is continually increasing. Due to increaed storage requirements and the need to optimize time involved in analysis, cloud based software are on the rise.","57c65b09":"It is pretty evident that participants with only a Bachelor's degree are the most actively involved in the survey.\nFrom the above plot, it is pretty evident that order of survey participant numbers is \n(1) Bachelors (2321 participants)\n(2) Masters (1683 participants)\nI would've proceeded to my next analysis but I stop to wonder: Why the number of people with a Bachelors degree are likely to participate more in such surveys. Is there any correlation betwwen the degree you posess and the compensation you get? \nOne plausible explanation to why there are more Bachelor's degree participants is because it is more common to have subjects related to the field of AI and ML during a Masters degree or Doctoral degree as compared to Bachelor's degreee which implies people posessing a Bachelor's degree has a more scope to learn about the relevant field and hence, ends up spending more time on such surveys and platforms.\nAs discussed from the article posted earlier, the realms of data science are slowly catching up with India and this can be seen by a steady surge in the number of participants doing research who are currently pursuing a doctoral degree shows the extent of application of ML and AI in areas of research. Also, many people not even related to the particular field are showing up on the survey (Category: No formal education past high school)\n\nNow, from one of the best sources to draw employment salary information, Linkedin(Link used: https:\/\/www.linkedin.com\/salary\/data-scientist-salaries-in-india), it can be seen that the average range of salaries for data scientists is divided into ranges as follows:\n* A Data Scientist salary can't be decided in its highest term , because the average salary that the Data Scientist earns in India amounts to a total of 13,329 dollars annually and that is just on average. \n* The minimum salary a Data Scientist can earn in India is approximately 5,017.14 United States Dollars annually while the maximum amount is approximately 30,660.31 United States Dollar.\nSo, analyzing the salary ranges my mission is to find out if professional background of an individual has a role to play in the compensation levels of data-scientists in India.","301af283":"Now let's compare NLP techniques used by the indian data scientist survey participants and US data scientist survey participants","928c0a8f":"Now let's answer the following question: \nDo data scientists in India use TPU'S ?? How often do they? Let's find out","25662b1e":"<img src=\"https:\/\/recruitingdaily.com\/wp-content\/uploads\/sites\/6\/2018\/01\/jobsearch.jpg\" width=\"200\"> \nNow, after analysing the status of data scientists in the US and India, and getting a basic idea of the skills i need to catch up on, next I decided to familiarize myself with the different , more adavnced Machine learning tools used at a workplace to facilitate and easiliy incorportate machine learning techniques: \n(1) What cloud platforms are preferred in the India today and how it has changed from 2018\n(2) What relational databases are preferred in theIndia to obtain cross relations between large volumes of data","eb5a3360":"**CONCLUSION**\n\nSo guys we've finally reached the end of my first Kaggle project and im pretty delighted to share my key takeaways from this analysis summarized as follows:\n(1) FUTURE PROSPECTIVES FOR Students and Educational Institutions in the field of AI and ML in India:\n* Th****e introduction of more specialized courses in the field of artificial intelligence and data science have to be introduced from a Bachelor's degree level itself. This can help shape fundamental concepts from a grass root level.\n* Getting a master's degree is pretty important to grow and end up in a high-paying data scientist position.\n* The most frequently sorted to online course platform is Coureseera and is probably the preferrable educational platform to go to.\n\n(2) What should I work on to be a successful data scientist in India:\n* Python as a programming language is the most used amongst data scientists followed by R and SQL. If you're looking to apply for data scientist positions in the US, it's an added advantage to familiarize yourself with Bash and Linux usage as well\n* Familiarity with usage of Jupyter Notebook as an IDE is unavoidable\n* If you ever look for a careeer switch as a data analyst, be sure to improve your R programming skills as it;s visualization tools are fancier and better for presentations\n* Artificial intelligence is finding its reaches in three major sectors in India namely Healthcare, Agriculture and Language Neural Network. In the first two sections, there is a lot of object detection and image processing techniques involved and hence, relevant topics and algorithms like CNN's are good to be familiarized with before entering the job market\n* Word embedding techniques seems to share a large proportion of NLP techniques used in both India and US and hence seems like a segment of NLP to concentrate more on\n* Google cloud platform is going to see a steady rise in increase of usage of its products(with an added advantage of it's cloud lead being of Indian descent) and hence, familiarity with these Google machine learning products will be an added advantage for future data scientist position jobs.\n\n(3) An open ended question to all of you  \n**AM I ON THE RIGHT PATH TOWARDS AI** or more over **IS INDIA ON THE RIGHT PATH TOWARDS AI**","1d16ae6b":"It seems like a huge proportion of data sceintists in India with a Master's degree seem to be not only be dominant in the pay scale range but also, they seem to be the dominant in the proportion of participants in the survey. Looking above,it is pretty evident that the low pay scale employees find a larger proportion of participants with a Bachelor's degree versus the high pay scale employees amongst the participants with a Master's degree. \nSo this explains that there is definetly a good amount of knowledge being acquired during a Master's degree which is helping data-scientists boost their payscale. Comparing the stats from 2018,it seems the above mentioned trend seems to carry on where the boost in payscale is seen as we move from participants from a Bachelor's degree background to a Master's degree.\nBut, to become a data scientist, is it enough to just depend on university courses? Will we need to research and study more on our own to make it successfull in the field. \n\n\n\n\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b8\/MOOC_-_Massive_Open_Online_Course_logo.svg\" width=\"400\">\nThere has been a surge in the people using MOOC's (Massive Open Online Courses) in this day and age to pick up required job specific skills. Some of the main reasons behind the success of online courses is: \n(1) Studying online gives you more flexibility\n(2) Lower costs and debts\n(3) More choice of course topics\nIn India, the use of these platforms have been ever-increasing and defintely provide a key of access to valuable information regardless of your professional background.\nBeing a fellow student from India, my area of interest is to identify what online course platforms are being used frequently by students in India.","391ac647":"Now diving into the comparison of Big Data Analytical products used:","d355e348":"Hi Guys. For starters, sorry if the header pun was bad :) .  I am Shaun and I welcome you all to be a part of my journey as I walk you through what it is like to pursue a field as complex as AI and data science by a fellow Mechanical Engineer(myself) or anyone from an unrelated field for that matter.A graduate student in the US trying to make it big in the field of data science and AI, I landed in the United States with absolutely no knowledge in the field at all. \n<img src=\"https:\/\/making-the-web.com\/sites\/default\/files\/clipart\/150758\/college-student-clipart-150758-9196441.jpg\" width=\"300\">","7325ab52":"From the above analysis it is pretty evident that Kaggle is the number one resorted to platform that reports on data science subjects. A good proportion of participants resort to YouTube as it is defintely the best option considering the fact you get online coaching as well as it is free of charge which makes it more easily accessible to students in India who generally don't have the practice of doing part-time jobs and earning while studying as compared to foreign countries like the US.","bf1f62c4":"Sci-kit learn is the most widely used machine learning tool which could be due to multiple reasons:\n(1) Very well maintained documentation\n(2) As a curated library, users don\u2019t have to choose from multiple competing implementations of the same algorithm (a problem that R users often face)\n(3) scikit-learn scales to most data problems. Other techniques like sampling or ensemble learning can also be used to train models on massive data sets.\nFrom the above analysis, it is also evident that the proportion of PyTorch users greater in India when compared to US: One of the most important advantages of PyTorch over TensorFlow is that Pytorch believes in a dynamic graph. So what does this mean? In Tensorflow, you first have to define the entire computation graph of the model and then run your ML model versus in PyTorch where you can define\/manipulate your graph on-the-go. This is particularly helpful while using variable length inputs in RNNs.\nNow, our previous comparison, the proportion of data scientists using RNN's in India is in greater proportion as compared to the proportion of data scientists using RNN's in the US. This might be directly correleational to the fact that a higher proportion of data scientists from India use PyTorch as compared to US to facilitate the higher usage of RNN's.","1916ba39":"Now let's shift our analysis to the algorithms of ML tools used by data scientists in India on a regular basis","e014fad9":"Now, let's analyze the various tools students in India use to analyze data at workplace","5e86308a":"[](http:\/\/)\nIt is interesting to note that more students are depending on online university courses more in 2019 over 2018. It is a very encouraging fact to note that more students in India are gaining knowledge in the field of ML and AI through university based online courses. It wont be a matter of time before these Universitites offering online courses in these fields start to incorparate them in the educational programs as a separate course.\nIt is pretty evident from the above plot that majority of the survey participants from India who are students resort to Courseera as an online platform to complete data-science courses and the number is pretty significant compared Udemy, Kaggle courses, etc. Well, I dove a little deeper into why the disproportionate amount of people study from different online platforms and what drives these preferences. When considering the two most used platforms, Couresera and Udemy: \nWhen it comes to price, Coursera has a fixed monthly payment option (you pay US$50 to make as many courses as you\u2019d like to), I think it\u2019s all a matter of deciding how much time you\u2019re willing to spend on online courses every month to see which of these will end up being cheaper to you. Udemy may be cheaper if you take courses at a slower rates or in smaller quantities, but not so if you have lots of spare time to invest in lots of courses.\nHowever, the differences aren\u2019t only in price and payment methods. For example, whereas Udemy has courses developed by individuals, Coursera has mainly academic courses from top-notch universities. This may seem like a huge advantage for Coursera, as you know, *CERTIFICATES*, but it comes with a few problems. For instance, you will never see any university offering a \u201cMicrosoft Office Course\u201d. Of course, universities teach their alumni to use this sort of software, but always as part of some bigger subject. So, if you\u2019re looking forward to understand a software that may be useful to your career, be it Excel, Autocad or Photoshop, it may prove hard to find the right course for you in Coursera.\nBut, from the points discussed above a major factor is the business model Couresera follows of using a monthly subscription plan to have access to multiple courses. This targets the population who's used to monthly payment of bills and would prefer if they have access to multiple entities in one transaction.\n\nNow, that we've established why I think online platforms can have different impacts on different indivuals, taking the case of India, a general notion amongst the Indian crowd is emphasis of heavy weightage on the certificates attained.\n<img src=\"https:\/\/www.mooclab.club\/attachments\/coursera-certificate-jpg.539\/\">\n\nA great deal of importance is given to \"proof\"(\"certificate\") of accolades in general which might lead to the notion of going for Coureera which offers fancier and more authorized versions of certifications from top notch universities as compared to Udemy.\n\nKUDOS TO THE DESIGNER OF THE CERTIFICATE BTW!!\n\n\nIt is also interesting to notice the surge in participants learning ML related stuff in their University courses which is a clear indicator that the country is investing in introducing data science courses in educational insititutions.","cd11c225":"In the US, greater proportion of data scientists seem to be using Bayesian Approaches versus data scientists in India where a larger proportion seem to be using CNN's. This can be an indicator to the fact that data science in the US involves a large chunk of text mining applications Text mining, sentiment analysis, document categorization, spam filtering, disease prediction are the main applications of Bayesian Approaches as compared tp India where there is a higher proportion of CNN's used. \nCNNs are useful for identifying objects in images and video. The extent of implementation and research of artificial intelligence techniques in India is limited to the three sectors so far:\n\n(1) Precision Agriculture: \nThe government has initiated a proof of concept pilot in 15 districts (counties) in India to use artificial intelligence based real-time advisory based on satellite imagery, weather data, etc. to increase farm yields where the farm production levels are low\n(2) Healthcare:\nPathologists and Radiologists are very few in number India relative to the overall population, (especially in rural areas) and these are applications which can be augmented through image recognition AI. We are working on augmenting the productivity of existing pathologists and radiologists as the first (of many planned) pilot project in healthcare. NITI Aayog is working on early diagnosis and detection of Diabetic Retinopathy and Cardiac Risk based on the AI models. Such initiatives would in the long run help patients on proactive medication in early stages rather than reactive healthcare in advanced stages \u2013 bringing down healthcare costs and better chances of recovery. \n(3) Indian Languages Project:\nWe have initiated a long-term project to build a complete natural language processing platform for Indian languages. This would aid in the development of several applications, like conversational general and career counseling through chatbots and assistants, conversing in 22 Indian languages.\u201d\n\n****Of the two out of three main research areas for AI, what do you see in common: Both of them involve analyzing large anmounts of image data in order to identify trends. This is where CNN's come to play and this also can be an explanation to why CNN's are used in a larger proportion as compared to data scientists in the US.","09650e43":"India has been continually working hard to establish the concepts and fundamentals of AI and Machine learning in students as early as possible. Earlier, it was'nt possible to pursue an undergraduate degree in the field and it was something people usually tend to specialize in. But, as an initiative to expand the realms of AI, the following was established:\n\n\n\n\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/0\/0f\/Indian_Institute_of_Technology_Hyderabad_logo.svg\" width=\"500\">\n**IIT Hyderabad - Introduction of A Bachelor's Degree BTech in Artificial Intelligence**\nIIT Hyderabad became the first educational institution to establish a full fledged undergraduate program in the field of artificial intelligence to produce better, focussed data scientists of the future. This is only the third institution to offer It has become the first Indian educational institution to offer such a full-fledged BTech programme in AI and likely the third institute globally after Carnegie Mellon University and Massachusetts Institute of Technology (MIT) in the USA. The course will have an intake of around 20 students.\n\nFrom the above plot, 33 participants in the survey from India are students which can be facilitated by the above fact the our country is taking steps to introduce AI concepts and fundamentals at grass root levels. to incorporate subjects on machine learning and related areas in the coursework which can be a indicator of the advancement of data science amongst the learning population of India(comprised of school and college students).An increase in the curiosity to learn about the fields can be explained by the increase in proportion of students participation Followed by next are data scientists (not a shocker) followed by sofwtare engineer who comprise major chunk of the share too(around 16 percent)\n\nSeeing initiatives as such above, I wanted to take a deeper dive by analyzing how students have changed over the course of 2018 to 2019 in terms of their exposure to and involvment in the field of Machine Learning and AI","ef4824d1":"Higher percentage of data scientists taking the survey in the US that havent used a TPU . \nTPU is an AI accelerator chip specifically designed for neural network machine learning. \nTPUs are good for deep learning tasks involving TensorFlow, while GPUs are more general purpose and flexible massively-parallel processors. \nAgain, the evident less usage of TPU's exists because of its limited functionality. In the current scenario, GPUs can be used as a conventional processor and can be programmed to efficiently carry out neural network operations. TPU, on the other hand, is not a fully generic processor. Since no support (such as compilers) is available yet, TPU can hardly run something other than a TensorFlow model. Hence, companies would prefer to make investments in generic processors to perform variety of applications using a single entity.","aa6a4007":"\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/b\/be\/Cloud_Computing.jpg\" width=\"500\">\nThe trends observed amongst indian data scientists regarding the usage of tools to analyze data seems to be similar amongst data scientists in the US. Higher percentage of US data scientists seem to be using cloud based software on a regular basis though the percentage difference isn't highly noticeable. This might be related to the fact that US in general has more companies that have invested in cloud based software due to the greater advancements in the field of AI which in-turn leads to increased computing and storage requirements. Hence, overall spending capapbility of companies might be higher as compared to India which might translate to lower usage of cloud based API's in India.\nNow, speaking of cloud based API's, I had stumbled upon an article a few days back telling me about how the cloud market in India has been ever booming from 2017(1.8 billion) with a projected cloud market of 4.1 billion in 2020.\nFrom the above analysis, comparing the results of 2018 and 2019, more data scientist survey participants seem to be dealing with cloud based software and API's in 2019 compared to 2018 which can be an indicator that the cloud market is on a continual increase.\n\nNow, coming back to what our initial goal is, to compare data scientists in India and USA, let's look at the IDE's used by both data scientists","ffe8bdf4":"Next, let's move onto the data visualization tools used on a regular basis by data scientists in India","7d5bfe83":"To back my prior analysis of the recent surge of interest in AI and machine learning analysis with the age-groups invoved in participants of the survey, it is pretty evident that 18-21 age group (generally the undergraduate crowd) and the 22-24 age group(generally the master's crowd) are highly involved in the survey. By looking at the plot from 2018, it is also evident that there is a surge in the participants in the age-groups 18-21 and 21-24 from 2018-2019 while a decline in the other age groups. This is encouraging of the fact that more students and entry level working participants are being more exposed to the world of ML and AI.\n\nThere is a good number of participants in the age group 25-29 which typically points towards the younger working population of India who have just made an entry and trying to make a mark in the field. Given the strong IT background and estbalishments existing in India today, while software used to be often taught through books and in classrooms exclusively, many of the latest artificial intelligence approaches are available to learn online \u2013 along with huge suites of open-source tools (from scikit-learn to TensorFlow and beyond).\n\nGoing in, we knew that one of the key advantages for India would, in fact, be the very IT and ITeS sectors which will make it easy for Indian tech providers to transition into AI services, given that well-developed ecosystems have evolved over the past 25 years in cities like Bangalore and Hyderabad.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e8\/US_Navy_100519-N-7367K-001_A_Sailor_and_a_Soldier_based_in_southern_Mississippi_sprint_to_the_finish_line_during_the_Run_for_Relief_5K_Challenge.jpg\" width=\"200\">\nWith the ever continuous increase in the scope and development of AI, we can just expect to see a surge in the number of participants from this age group as they'll have to continually update themselves with data science related skills to stay in the race with AI. \nNext, let's see if the distribution of professional backgrounds of various participants.\n","d81bf5a1":"Next, lets analyze the different programming languages used by data scientists in India and compare them to data scientists in the US.","be02444f":"The everlong battle between RStudio and Python:\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/0\/04\/Python_Regius_Pastel.jpg\" width=\"200\">\nOver the recent years, Python has gained popularity as a programming laguage amongst data scientists and has overtaken the usage of Rstudio. Logistic regression is only one of the many predictive models you can build with Python and R. Though Rstudio offers brilliant data visualization libraries, one aspect where Python edged out R is its well-built deep learning modules. Popular Python deep learning libraries including Tensorflow, Theano and Keras. These libraries are sufficiently documented and I am sure Siraj Raval has hundreds of Youtube tutorials on how to use them. To be completely honest, I\u2019d rather spend an hour coding dCNNs (deep convolutional neural networks) in Keras than spending half a day figuring out how to implement them in R.\nDue to the rapid advancement in usage of Deep learning, the requirement of relevant easy modules became essential and this available with Python. This also can be another factor why is there is high proportion of users using Jupyter Notebooks.\nAnd yes, indeed Python has become the dominant force like the breed of snake shown in the picture above :)"}}