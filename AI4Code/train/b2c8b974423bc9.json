{"cell_type":{"9dfe6a92":"code","b63bfc3f":"code","a2beab1c":"code","6413024b":"code","45db88c1":"code","0a4192b3":"code","993b478a":"code","5aee0cab":"code","e4e35edf":"code","b2911906":"code","e8c45857":"code","f2726763":"code","5af0de44":"code","c78fe719":"code","8639bbfd":"code","80196eb4":"code","dfbb33ba":"code","6a3d8854":"code","0bb66b9b":"code","80e1b790":"code","cac6ab5a":"code","651a688f":"code","a4232264":"code","56c72528":"code","bc7afbb9":"code","0dc6b9b5":"code","9a810a9d":"code","1eff7120":"code","9c5bdde5":"code","5cee4c76":"code","33d52e8b":"code","691a9a71":"code","b5ab5f30":"code","6bbeb5fa":"code","b6bb1749":"code","08a61d26":"code","23aa5d1b":"code","00550563":"code","e3bcd46d":"code","a076fccc":"code","71b589b1":"code","edaf2ac5":"code","c6235bf5":"code","030c6120":"code","354a4b36":"code","ec59d430":"code","2b87e854":"code","36e73a73":"code","f7acf2c2":"code","5e5d6ba2":"code","02b7094f":"code","4c89334a":"code","f0847324":"code","a99c459f":"code","66fdfc25":"code","e7475e55":"code","7b942cea":"code","69a9c5e7":"code","398ff95c":"code","7758e482":"code","85affc39":"code","632b7c2c":"code","5ca5e44e":"code","993d8b06":"code","c68845d9":"code","d6306ac4":"code","d1dd24c2":"code","4c6c996c":"code","eaccc3cb":"code","a610e6c4":"code","dda26d2c":"code","ddd8ee6a":"code","d5d8479f":"code","6f7c0c7b":"code","26500b45":"code","42ab68e4":"code","7f4224c5":"code","36499beb":"code","bdf26fe3":"code","51a6f331":"code","142241c9":"markdown","d5b1460e":"markdown","a650c5ca":"markdown","297472f2":"markdown","e3b90426":"markdown","4e1f88af":"markdown","9d53486d":"markdown","ee9a532c":"markdown","7883b28e":"markdown","c05cd107":"markdown","eb1e76f3":"markdown","870d2af3":"markdown","2a34d0dd":"markdown","1f5809f3":"markdown","7b3369ef":"markdown"},"source":{"9dfe6a92":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","b63bfc3f":"Train_Data_Path = Path(\"..\/input\/landuse-scene-classification\/images_train_test_val\/train\")\nTest_Data_Path = Path(\"..\/input\/landuse-scene-classification\/images_train_test_val\/test\")\nValidation_Data_Path = Path(\"..\/input\/landuse-scene-classification\/images_train_test_val\/validation\")","a2beab1c":"Train_PNG_Path = list(Train_Data_Path.glob(r\"*\/*.png\"))\nTest_PNG_Path = list(Test_Data_Path.glob(r\"*\/*.png\"))\nValidation_PNG_Path = list(Validation_Data_Path.glob(r\"*\/*.png\"))","6413024b":"Train_PNG_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Train_PNG_Path))\nTest_PNG_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Test_PNG_Path))\nValidation_PNG_Labels = list(map(lambda x : os.path.split(os.path.split(x)[0])[1],Validation_PNG_Path))","45db88c1":"Labels_List = [\"agricultural\",\"agricultural\",\"airplane\",\"baseballdiamond\",\"beach\",\n \"buildings\",\"chaparral\",\"denseresidential\",\"forest\",\"freeway\",\"golfcourse\",\"harbor\",\n \"intersection\",\"mediumresidential\",\"mobilehomepark\",\"overpass\",\n \"parkinglot\",\"river\",\"runway\",\"sparseresidential\",\"storagetanks\",\"tenniscourt\"]","0a4192b3":"for labels in Labels_List:\n    print(f\"{labels}: \", Train_PNG_Labels.count(labels))","993b478a":"for labels in Labels_List:\n    print(f\"{labels}: \", Test_PNG_Labels.count(labels))","5aee0cab":"for labels in Labels_List:\n    print(f\"{labels}: \", Validation_PNG_Labels.count(labels))","e4e35edf":"Train_PNG_Path_Series = pd.Series(Train_PNG_Path,name=\"PNG\").astype(str)\nTest_PNG_Path_Series = pd.Series(Test_PNG_Path,name=\"PNG\").astype(str)\nValidation_PNG_Path_Series = pd.Series(Validation_PNG_Path,name=\"PNG\").astype(str)","b2911906":"print(Train_PNG_Path_Series)\nprint(\"---\"*20)\nprint(Test_PNG_Path_Series)\nprint(\"---\"*20)\nprint(Validation_PNG_Path_Series)","e8c45857":"Train_PNG_Labels_Series = pd.Series(Train_PNG_Labels,name=\"CATEGORY\")\nTest_PNG_Labels_Series = pd.Series(Test_PNG_Labels,name=\"CATEGORY\")\nValidation_PNG_Labels_Series = pd.Series(Validation_PNG_Labels,name=\"CATEGORY\")","f2726763":"print(Train_PNG_Labels_Series.value_counts())\nprint(\"---\"*20)\nprint(Test_PNG_Labels_Series.value_counts())\nprint(\"---\"*20)\nprint(Validation_PNG_Labels_Series.value_counts())","5af0de44":"Main_Train_Data = pd.concat([Train_PNG_Path_Series,Train_PNG_Labels_Series],axis=1)\nMain_Test_Data = pd.concat([Test_PNG_Path_Series,Test_PNG_Labels_Series],axis=1)\nMain_Validation_Data = pd.concat([Validation_PNG_Path_Series,Validation_PNG_Labels_Series],axis=1)","c78fe719":"print(Main_Train_Data.head(-1))\nprint(\"---\"*20)\nprint(Main_Test_Data.head(-1))\nprint(\"---\"*20)\nprint(Main_Validation_Data.head(-1))","8639bbfd":"print(Main_Train_Data[\"CATEGORY\"].value_counts())","80196eb4":"Main_Train_Data = Main_Train_Data.sample(frac=1).reset_index(drop=True)\nMain_Test_Data = Main_Test_Data.sample(frac=1).reset_index(drop=True)\nMain_Validation_Data = Main_Validation_Data.sample(frac=1).reset_index(drop=True)","dfbb33ba":"print(Main_Train_Data.head(-1))\nprint(\"---\"*20)\nprint(Main_Test_Data.head(-1))\nprint(\"---\"*20)\nprint(Main_Validation_Data.head(-1))","6a3d8854":"plt.style.use(\"dark_background\")","0bb66b9b":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(Main_Train_Data[\"PNG\"][20])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][20])","80e1b790":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(Main_Train_Data[\"PNG\"][55])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][55])","cac6ab5a":"figure = plt.figure(figsize=(10,10))\nx = plt.imread(Main_Train_Data[\"PNG\"][1405])\nplt.imshow(x)\nplt.xlabel(x.shape)\nplt.title(Main_Train_Data[\"CATEGORY\"][1405])","651a688f":"rows = 3\ncols = 5\n\nfigure,axis = plt.subplots(rows,cols,figsize=(12,12))\n\nfor i in range(0,rows):\n    for j in range(0,cols):\n        axis[i,j].xaxis.set_ticklabels([])\n        axis[i,j].yaxis.set_ticklabels([])\n        axis[i,j].imshow(plt.imread(Main_Train_Data[\"PNG\"][np.random.randint(1000)]))\nplt.show()","a4232264":"rows = 3\ncols = 5\n\nfigure,axis = plt.subplots(rows,cols,figsize=(12,12))\n\nfor i,ax in enumerate(axis.flat):\n    ax.imshow(plt.imread(Main_Train_Data[\"PNG\"][i]))\n    ax.set_title(Main_Train_Data[\"CATEGORY\"][i])\nplt.tight_layout()\nplt.show()","56c72528":"figure,axis = plt.subplots(1,4,figsize=(12,12))\n\nimage = Main_Train_Data[\"PNG\"][30]\nimage = cv2.imread(image)\nimage_blurred = cv2.GaussianBlur(image, (0, 0), 3)\nimage_color = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\nimage_edges = cv2.Canny(image_color,30,200)\nimage_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\naxis[0].imshow(image_edges)\naxis[1].imshow(image_color)\naxis[2].imshow(image_blurred)\naxis[3].imshow(image_sharp)\n","bc7afbb9":"figure,axis = plt.subplots(1,4,figsize=(12,12))\n\nimage_Two = Main_Train_Data[\"PNG\"][2005]\nimage_Two = cv2.imread(image_Two)\nimage_blurred_Two = cv2.GaussianBlur(image_Two, (0, 0), 3)\nimage_color_Two = cv2.cvtColor(image_Two,cv2.COLOR_RGB2GRAY)\nimage_edges_Two = cv2.Canny(image_color_Two,30,200)\nimage_sharp_Two = cv2.addWeighted(image_Two, 1.5, image_blurred_Two, -0.5, 0)\naxis[0].imshow(image_edges_Two)\naxis[1].imshow(image_color_Two)\naxis[2].imshow(image_blurred_Two)\naxis[3].imshow(image_sharp_Two)","0dc6b9b5":"figure,axis = plt.subplots(1,4,figsize=(12,12))\n\nimage_Three = Main_Train_Data[\"PNG\"][300]\nimage_Four = Main_Train_Data[\"PNG\"][1255]\n\nimage_Three = cv2.imread(image_Three)\nimage_Four = cv2.imread(image_Four)\n\nimage_edges_Three = cv2.Canny(image_Three,30,200)\nimage_edges_Four = cv2.Canny(image_Four,30,200)\n\ncontours, _ = cv2.findContours(image_edges_Three,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n\ncontours_Two, _ = cv2.findContours(image_edges_Four,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n\ncv2.drawContours(image_Three, contours, -1, (0, 255, 0), 1)\n\ncv2.drawContours(image_Four, contours_Two, -1, (0, 0, 255), 1)\n\naxis[0].imshow(image_edges_Three)\naxis[1].imshow(image_Three)\naxis[2].imshow(image_edges_Four)\naxis[3].imshow(image_Four)","9a810a9d":"image_Three = Main_Train_Data[\"PNG\"][300]\nimage_Four = Main_Train_Data[\"PNG\"][1255]\nimage_Two = Main_Train_Data[\"PNG\"][2005]\nimage = Main_Train_Data[\"PNG\"][30]\n\nimage = cv2.imread(image)\nimage_Two = cv2.imread(image_Two)\nimage_Three = cv2.imread(image_Three)\nimage_Four = cv2.imread(image_Four)\n\nlist_image = [image,image_Two,image_Three,image_Four]\n\nret,thresh = cv2.threshold(image,127,255,cv2.THRESH_BINARY_INV)\nret,thresh_Two = cv2.threshold(image_Two,127,255,cv2.THRESH_BINARY_INV)\nret,thresh_Three = cv2.threshold(image_Three,127,255,cv2.THRESH_BINARY_INV)\nret,thresh_Four = cv2.threshold(image_Four,127,255,cv2.THRESH_BINARY_INV)\n\nthresh_list = [thresh,thresh_Two,thresh_Three,thresh_Four]\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=2,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\n\naxes[0,0].imshow(thresh_list[0])\naxes[0,1].imshow(thresh_list[1])\naxes[1,0].imshow(thresh_list[2])\naxes[1,1].imshow(thresh_list[3])","1eff7120":"image_Three = Main_Train_Data[\"PNG\"][300]\nimage_Four = Main_Train_Data[\"PNG\"][1255]\nimage_Two = Main_Train_Data[\"PNG\"][2005]\nimage = Main_Train_Data[\"PNG\"][30]\n\nimage = cv2.imread(image,0)\nimage_Two = cv2.imread(image_Two,0)\nimage_Three = cv2.imread(image_Three,0)\nimage_Four = cv2.imread(image_Four,0)\n\nlist_image = [image,image_Two,image_Three,image_Four]\n\nthresh = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\nthresh_Two = cv2.adaptiveThreshold(image_Two,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\nthresh_Three = cv2.adaptiveThreshold(image_Three,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\nthresh_Four = cv2.adaptiveThreshold(image_Four,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\n\nthresh_list = [thresh,thresh_Two,thresh_Three,thresh_Four]\n\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=2,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\naxes[0,0].imshow(thresh_list[0])\naxes[0,1].imshow(thresh_list[1])\naxes[1,0].imshow(thresh_list[2])\naxes[1,1].imshow(thresh_list[3])","9c5bdde5":"image_Three = Main_Train_Data[\"PNG\"][300]\nimage_Four = Main_Train_Data[\"PNG\"][1255]\nimage_Two = Main_Train_Data[\"PNG\"][2005]\nimage = Main_Train_Data[\"PNG\"][30]\n\nimage = cv2.imread(image,0)\nimage_Two = cv2.imread(image_Two,0)\nimage_Three = cv2.imread(image_Three,0)\nimage_Four = cv2.imread(image_Four,0)\n\nlist_image = [image,image_Two,image_Three,image_Four]\n\nthresh = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\nthresh_Two = cv2.adaptiveThreshold(image_Two,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\nthresh_Three = cv2.adaptiveThreshold(image_Three,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\nthresh_Four = cv2.adaptiveThreshold(image_Four,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n\nthresh_list = [thresh,thresh_Two,thresh_Three,thresh_Four]\n\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=2,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\naxes[0,0].imshow(thresh_list[0])\naxes[0,1].imshow(thresh_list[1])\naxes[1,0].imshow(thresh_list[2])\naxes[1,1].imshow(thresh_list[3])","5cee4c76":"Back_Subt = cv2.createBackgroundSubtractorMOG2()\n\nBack_Mask = Back_Subt.apply(thresh)\nBack_Mask_Two = Back_Subt.apply(thresh_Two)\nBack_Mask_Three = Back_Subt.apply(thresh_Three)\nBack_Mask_Four = Back_Subt.apply(thresh_Four)\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=2,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\naxes[0,0].imshow(Back_Mask)\naxes[0,1].imshow(Back_Mask_Two)\naxes[1,0].imshow(Back_Mask_Three)\naxes[1,1].imshow(Back_Mask_Four)","33d52e8b":"Back_Subt = cv2.createBackgroundSubtractorMOG2()\nNew_PNG_List = []\nimg_size= (160, 160)\n\nfor im in Main_Train_Data[\"PNG\"].values:\n    Read_IMG_New = cv2.imread(im,0)\n    Thresh_IMG = cv2.adaptiveThreshold(Read_IMG_New,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n    Mask_IMG = Back_Subt.apply(Thresh_IMG)\n    Mask_IMG = cv2.resize(Mask_IMG,img_size,interpolation=cv2.INTER_AREA)\n    New_PNG_List.append(Mask_IMG\/255.0)","691a9a71":"New_PNG_List = np.stack(New_PNG_List)","b5ab5f30":"print(type(New_PNG_List))","6bbeb5fa":"print(New_PNG_List.shape)","b6bb1749":"print(New_PNG_List[5].shape)","08a61d26":"encode = LabelEncoder()\nMain_Train_Data[\"CATEGORY\"] = encode.fit_transform(Main_Train_Data[\"CATEGORY\"])","23aa5d1b":"New_Labels_List = []\nfor label in Main_Train_Data[\"CATEGORY\"].values:\n    New_Labels_List.append(label)","00550563":"New_Labels_List = np.asarray(New_Labels_List)","e3bcd46d":"print(type(New_Labels_List))","a076fccc":"print(New_Labels_List.shape)","71b589b1":"print(New_Labels_List[777])","edaf2ac5":"xTrain,xTest,yTrain,yTest = train_test_split(New_PNG_List,New_Labels_List,\n                                             train_size=0.8,\n                                             shuffle=True,\n                                             random_state=42)","c6235bf5":"print(xTrain.shape)","030c6120":"print(xTrain.shape)","354a4b36":"sample_shape = xTrain[0].shape\nimg_width, img_height = sample_shape[0], sample_shape[1]\ninput_shape = (img_width, img_height, 1)","ec59d430":"xTrain = xTrain.reshape(len(xTrain),input_shape[0],input_shape[1],input_shape[2])","2b87e854":"print(xTrain.shape)","36e73a73":"print(yTrain.shape)","f7acf2c2":"yTrain = to_categorical(yTrain)","5e5d6ba2":"xTest = xTest.reshape(len(xTest),input_shape[0],input_shape[1],input_shape[2])","02b7094f":"print(xTest.shape)","4c89334a":"print(yTest.shape)","f0847324":"yTest = to_categorical(yTest)","a99c459f":"xTrain = xTrain \/ 255","66fdfc25":"xTest = xTest \/ 255","e7475e55":"model = Sequential()\nmodel.add(Conv2D(64, (3,3), activation='relu',input_shape=(160,160,1)))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\n\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\nmodel.add(Conv2D(128, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(128, (3,3), activation='relu',padding=\"same\"))\n\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\nmodel.add(Conv2D(256, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(256, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(256, (3,3), activation='relu',padding=\"same\"))\n\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\nmodel.add(Conv2D(512, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(512, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(512, (3,3), activation='relu',padding=\"same\"))\n\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\nmodel.add(Conv2D(512, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(512, (3,3), activation='relu',padding=\"same\"))\nmodel.add(Conv2D(512, (3,3), activation='relu',padding=\"same\"))\n\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(21,activation=\"softmax\"))","7b942cea":"print(model.summary())","69a9c5e7":"plot_model(model,to_file=\"Model_One.png\",show_layer_names=True,show_dtype=True,show_shapes=True)","398ff95c":"Train_Generator = ImageDataGenerator(rescale=1.\/255,\n                                    shear_range=0.3,\n                                    zoom_range=0.2,\n                                    brightness_range=[0.2,0.9],\n                                    rotation_range=30,\n                                    horizontal_flip=True,\n                                    vertical_flip=True,\n                                    fill_mode=\"nearest\",\n                                    validation_split=0.1)","7758e482":"Test_Generator = ImageDataGenerator(rescale=1.\/255,validation_split=0.1)","85affc39":"Train_PNG_Path_Series = pd.Series(Train_PNG_Path,name=\"PNG\").astype(str)\nTest_PNG_Path_Series = pd.Series(Test_PNG_Path,name=\"PNG\").astype(str)\nValidation_PNG_Path_Series = pd.Series(Validation_PNG_Path,name=\"PNG\").astype(str)\nTrain_PNG_Labels_Series = pd.Series(Train_PNG_Labels,name=\"CATEGORY\")\nTest_PNG_Labels_Series = pd.Series(Test_PNG_Labels,name=\"CATEGORY\")\nValidation_PNG_Labels_Series = pd.Series(Validation_PNG_Labels,name=\"CATEGORY\")","632b7c2c":"Main_Train_Data = pd.concat([Train_PNG_Path_Series,Train_PNG_Labels_Series],axis=1)\nMain_Test_Data = pd.concat([Test_PNG_Path_Series,Test_PNG_Labels_Series],axis=1)\nMain_Validation_Data = pd.concat([Validation_PNG_Path_Series,Validation_PNG_Labels_Series],axis=1)","5ca5e44e":"Main_Train_Data = Main_Train_Data.sample(frac=1).reset_index(drop=True)\nMain_Test_Data = Main_Test_Data.sample(frac=1).reset_index(drop=True)\nMain_Validation_Data = Main_Validation_Data.sample(frac=1).reset_index(drop=True)","993d8b06":"Train_IMG_Set = Train_Generator.flow_from_dataframe(dataframe=Main_Train_Data,\n                                                   x_col=\"PNG\",\n                                                   y_col=\"CATEGORY\",\n                                                   color_mode=\"rgb\",\n                                                   class_mode=\"categorical\",\n                                                   batch_size=32,\n                                                   subset=\"training\")","c68845d9":"Test_IMG_Set = Test_Generator.flow_from_dataframe(dataframe=Main_Test_Data,\n                                                   x_col=\"PNG\",\n                                                   y_col=\"CATEGORY\",\n                                                   color_mode=\"rgb\",\n                                                   class_mode=\"categorical\",\n                                                   batch_size=32)","d6306ac4":"Validation_IMG_Set = Test_Generator.flow_from_dataframe(dataframe=Main_Validation_Data,\n                                                   x_col=\"PNG\",\n                                                   y_col=\"CATEGORY\",\n                                                   color_mode=\"rgb\",\n                                                   class_mode=\"categorical\",\n                                                   batch_size=32,\n                                                   subset=\"validation\")","d1dd24c2":"for data_batch,label_batch in Train_IMG_Set:\n    print(\"DATA SHAPE: \",data_batch.shape)\n    print(\"LABEL SHAPE: \",label_batch.shape)\n    break","4c6c996c":"for data_batch,label_batch in Validation_IMG_Set:\n    print(\"DATA SHAPE: \",data_batch.shape)\n    print(\"LABEL SHAPE: \",label_batch.shape)\n    break","eaccc3cb":"for data_batch,label_batch in Test_IMG_Set:\n    print(\"DATA SHAPE: \",data_batch.shape)\n    print(\"LABEL SHAPE: \",label_batch.shape)\n    break","a610e6c4":"print(\"TRAIN: \")\nprint(Train_IMG_Set.class_indices)\nprint(Train_IMG_Set.classes[0:5])\nprint(Train_IMG_Set.image_shape)\nprint(\"---\"*20)\nprint(\"VALIDATION: \")\nprint(Validation_IMG_Set.class_indices)\nprint(Validation_IMG_Set.classes[0:5])\nprint(Validation_IMG_Set.image_shape)\nprint(\"---\"*20)\nprint(\"TEST: \")\nprint(Test_IMG_Set.class_indices)\nprint(Test_IMG_Set.classes[0:5])\nprint(Test_IMG_Set.image_shape)","dda26d2c":"Model = Sequential()\n\nModel.add(SeparableConv2D(32,3,\n                          activation=\"relu\",\n                 input_shape=(226,226,3)))\nModel.add(BatchNormalization())\nModel.add(MaxPooling2D((2)))\n\n#\nModel.add(SeparableConv2D(64,3,\n                 activation=\"relu\"))\nModel.add(SeparableConv2D(128,(3,3),\n                 activation=\"relu\"))\nModel.add(Dropout(0.5))\nModel.add(MaxPooling2D((2)))\n\n#\nModel.add(SeparableConv2D(128,3,\n                 activation=\"relu\"))\nModel.add(SeparableConv2D(128,3,\n                 activation=\"relu\"))\nModel.add(Dropout(0.5))\nModel.add(GlobalAveragePooling2D())\n\n\n#\nModel.add(Flatten())\nModel.add(Dense(256,\n                activation=\"relu\"))\nModel.add(Dropout(0.5))\nModel.add(Dense(21,\n                activation=\"softmax\"))","ddd8ee6a":"Call_Back = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=5,mode=\"min\")","d5d8479f":"Model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n\nCNN_Model = Model.fit(Train_IMG_Set,\n                      validation_data=Validation_IMG_Set,\n                      callbacks=Call_Back,\n                      epochs=100)","6f7c0c7b":"print(Model.summary())","26500b45":"plot_model(Model,to_file=\"Model_Two.png\",show_layer_names=True,show_dtype=True,show_shapes=True)","42ab68e4":"Model_Results = Model.evaluate(Test_IMG_Set)\nprint(\"LOSS:  \" + \"%.4f\" % Model_Results[0])\nprint(\"ACCURACY:  \" + \"%.2f\" % Model_Results[1])","7f4224c5":"plt.plot(CNN_Model.history[\"accuracy\"])\nplt.plot(CNN_Model.history[\"val_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","36499beb":"plt.plot(CNN_Model.history[\"loss\"])\nplt.plot(CNN_Model.history[\"val_loss\"])\nplt.ylabel(\"LOSS\")\nplt.legend()\nplt.show()","bdf26fe3":"plt.plot(CNN_Model.history[\"val_accuracy\"])\nplt.plot(CNN_Model.history[\"val_loss\"])\nplt.ylabel(\"ACCURACY-LOSS\")\nplt.legend()\nplt.show()","51a6f331":"Dict_Summary_One = pd.DataFrame(CNN_Model.history)\nDict_Summary_One.plot()","142241c9":"#### SHUFFLING","d5b1460e":"# DATAFRAME STRUCTURE","a650c5ca":"#### PATH PROCESS","297472f2":"#### MODEL II","e3b90426":"# HISTORY","4e1f88af":"# VISUALIZATION","9d53486d":"# PACKAGES AND LIBRARIES","ee9a532c":"# PATH & LABEL PROCESS","7883b28e":"# MODELING PROCESS","c05cd107":"#### MODEL I","eb1e76f3":"# SERIES STRUCTURE","870d2af3":"#### LABEL PROCESS","2a34d0dd":"#### Context\n* The main aim of this project is the masking of regions where land is being used in satellite images obtained through Landsat which have a low spatial resolution.\n\n#### Content\n* This dataset contains satellite images of 21 classes such as buildings, baseball fields, freeways, etc. The original size of the images is 256x256 pixels. Originally there were 100 images per class. After augmenting each image 4 times the size of each class was brought up to 500 images. This allows for making\n* a more robust model.\n\n#### Acknowledgements\n* The above dataset was obtained from the UC Merced Dataset\n* Dataset: Yi Yang and Shawn Newsam, \"Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification,\" ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS), 2010.","1f5809f3":"#### DATA PATH","7b3369ef":"#### IMAGE GENERATOR FOR MODEL II"}}