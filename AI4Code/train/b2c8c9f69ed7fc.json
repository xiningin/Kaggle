{"cell_type":{"bcc97301":"code","9e4ded52":"code","9f293d44":"code","a00eeac4":"code","8925486b":"code","1ba45f38":"code","76655b18":"code","071cf480":"code","c4b1d549":"code","684d5969":"code","d0e4f50d":"code","4eabc6a4":"code","5ec73ef7":"code","784d4827":"code","6b343814":"code","567c0246":"code","7e69368d":"code","ab547674":"code","cc1c6fb3":"code","52653b5b":"code","55cf9cb7":"code","27dddb26":"code","2ae05676":"code","e290f859":"code","ad0185bd":"code","089f5e91":"code","a2298b22":"code","a0bd247b":"code","7ab61082":"code","b5bb9084":"code","f8d95491":"markdown","5710efd7":"markdown"},"source":{"bcc97301":"#installing imutils\n!pip install imutils","9e4ded52":"\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","9f293d44":"\n#the libraries that we will use in the deploment step\nfrom tensorflow.keras.models import load_model\nfrom imutils.video import VideoStream\nimport imutils\nimport time\nimport cv2","a00eeac4":"#initialize learning rate , number of epochs and batch_size\nINIT_LR = 1e-4\nEPOCHS = 20\nBATCH_SIZE = 32","8925486b":"directory = r\"..\/input\/face-datasets\"\ncategories = [\"with_mask\" , 'without_mask']","1ba45f38":"#grab the list of images in our dataset directory , then initialize the list of data( images) and class names\nprint(\"[INFO] loading images...\")\ndata = []\nlabels = []\n\nfor category in categories:\n    path = os.path.join(directory , category)\n    for img in os.listdir(path):\n        img_path = os.path.join(path , img)\n        image = load_img(img_path , target_size=(224 , 224))\n        image = img_to_array(image)\n        image = preprocess_input(image)\n        \n        data.append(image)\n        labels.append(category)","76655b18":"#perform one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)","071cf480":"data = np.array(data , \"float32\")\nlabels = np.array(labels)","c4b1d549":"#splitting the data\n(Xtrain , Xtest , ytrain , ytest) = train_test_split(data , labels , test_size = 0.20 , random_state = 42 , stratify = labels)","684d5969":"#construct the training image generator for data augmentation\naug = ImageDataGenerator(rotation_range=20,\n                        zoom_range=0.15,\n                        width_shift_range=0.2,\n                        height_shift_range=0.2,\n                        shear_range=0.15,\n                        horizontal_flip=True,\n                        fill_mode = \"nearest\")","d0e4f50d":"#load the MobileNetV2 network , ensuring the head FC layer sets are left off\nbase_model = MobileNetV2(weights=\"imagenet\" , include_top=False , input_tensor=Input(shape = (224 , 224 , 3)))","4eabc6a4":"#construct the head of the model that will be placed on top of the base_model\nhead_model = base_model.output\nhead_model = AveragePooling2D(pool_size = (7,7))(head_model)\nhead_model = Flatten(name = \"flatten\")(head_model)\nhead_model = Dense(128 , activation = \"relu\")(head_model)\nhead_model = Dropout(0.5)(head_model)\nhead_model = Dense(2 , activation = \"softmax\")(head_model)","5ec73ef7":"#place the head_model on top of the base_model (this will be the actual model that we will train)\nmodel = Model(inputs = base_model.input , outputs = head_model)","784d4827":"#loop over all layers of base model and freeze them so they will not be updated during the first training process\nfor layer in base_model.layers:\n    layer.trainable = True","6b343814":"#compiling the model\nprint(\"[INFO] compiling the model...\")\nopt = Adam(lr = INIT_LR , decay = INIT_LR \/ EPOCHS)\nmodel.compile(loss = \"binary_crossentropy\" , optimizer = opt , metrics=[\"accuracy\"])","567c0246":"#train the head of the network\nprint(\"[info] training the head...\")\nhistory = model.fit(aug.flow(Xtrain , ytrain , batch_size=BATCH_SIZE),\n             steps_per_epoch = len(Xtrain)\/\/BATCH_SIZE , \n             validation_data = (Xtest , ytest),\n             validation_steps = len(Xtest)\/\/BATCH_SIZE,\n             epochs = EPOCHS)","7e69368d":"#make predictions on the testing set\nprint(\"[INFO] evaluating the model...\")\npred_idxs = model.predict(Xtest , batch_size=BATCH_SIZE)","ab547674":"#for each image in the testing set we need to find the index of the label with corresponding to the largest predicted prbability\npred_idxs = np.argmax(pred_idxs , axis=1)","cc1c6fb3":"#show a nicely formatted classification report\nprint(classification_report(ytest.argmax(axis = 1) , pred_idxs , target_names=lb.classes_))","52653b5b":"#saving the model\nprint(\"[INFO] saving mask detector model...\")\nmodel.save(\"face_mask_detector.model\" , save_format=\"h5\")","55cf9cb7":"#plot the loss\nN = EPOCHS\nplt.plot(np.arange(0,N) , history.history[\"loss\"] , \"r--\")\nplt.plot(np.arange(0,N) , history.history[\"val_loss\"] ,\"b-\")\nplt.legend([\"Training Loss\" , \"Validation Loss\"])\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"Loss\")\nplt.savefig(\"loss_plot.png\")\nplt.show()","27dddb26":"#plot the Accuracy\nN = EPOCHS\nplt.plot(np.arange(0,N) , history.history[\"accuracy\"] , \"r--\")\nplt.plot(np.arange(0,N) , history.history[\"val_accuracy\"] ,\"b-\")\nplt.legend([\"Training Accuracy\" , \"Validation Accuracy\"])\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"Accuracy\")\nplt.show()\nplt.savefig(\"accuracy_plot.png\")","2ae05676":"def detect_and_predict_mask(frame , face_net , mask_net):\n    #grap the dimensions of the frame and then construct a blob from it\n    (h,w) = frame.shape[:2]\n    blob = cv2.dnn.blobFromImage(frame , 1.0 , (224,224) , (104.0 , 177.0 , 123.0))\n    \n    #pass the blob through the network and obtain the face detections\n    face_net.setInput(blob)\n    detections = face_net.forward()\n    print(detections.shape)\n    \n    #initialize our list of faces , their corresponding locations and the list of predictions from our face mask network\n    faces = []\n    locs  = []\n    preds = []\n    \n    #loop over the detections\n    for i in range(0 , detections.shape[2]):\n        #extract the confidence (propability) associated with the detection\n        confidence = detections[0,0,i,2]\n        \n        #filter out weak detections by ensuring the confidence is greater than the minimum confidence\n        if confidence > 0.5:\n            #compute the (x,y) coordinates of the bounding box for the object\n            box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n            (startX , starty , endX , endy) = box.astype(\"int\")\n            \n            #ensure bounding boxes fall within the dimension of the frame\n            (startX , starty) = (max(0 , startX) , max(0 , starty))\n            (endX , endy) = (min(w-1 , endX) , min(h-1 , endy))\n            \n            #extract the face ROI , convert it from BGR to RGB channel ordering , resize it to 224 * 224 and preprocess it\n            face = frame[starty:endy , startX:endX]\n            face = cv2.cvtColor(face , cv2.COLOR_BGR2RGB)\n            face = cv2.resize(face , (224,224))\n            face = img_to_array(face)\n            face = preprocess_input(face)\n            \n            #add the face and bounding boxes to their respective lists\n            faces.append(face)\n            locs.append((startX , starty , endX , endy))\n            \n    #only make a predictions if at least one face was detected \n    if  len(faces) > 0:\n        #for faster inference we will make batch predictions on \"all\" faces at the same time rather than one-by-one\n        faces = np.array(faces , dtype = \"float32\")\n        preds = mask_net.predict(faces , batch_size = 32)\n        \n    \n    #return a 2-tuple of the face locations and their corresponding locations\n    return(locs , preds)\n    ","e290f859":"#load our serialized face detector model from disk\nprototxt_path = r\"..\/input\/face-detector\/deploy.prototxt\"\nweights_path  = r\"..\/input\/face-detector\/res10_300x300_ssd_iter_140000.caffemodel\"\nface_net = cv2.dnn.readNet(prototxt_path , weights_path)","ad0185bd":"#load the Face Mask Detection model from disk\nmask_net = load_model(\".\/face_mask_detector.model\")","089f5e91":"#initialize the videostream\nprint(\"[INFO] starting the video stream...\")\nvs = VideoStream(src = 0).start()","a2298b22":"\nwhile True:\n    #grap the frame from the threaded videostream and resize it to have a maximum width of 400 pixels\n    frame = vs.read()\n    frame = imutils.resize(frame , width=400)\n    \n    #detect faces in the frame and determine if they are wearing face mask or not\n    (locs , preds) = detect_and_predict_mask(frame , face_net , mask_net)\n    \n    #loop over the detected face locations and their corresponding locations\n    for (box , pred) in zip(locs , preds):\n        (startX , starty , endX , endy) = box\n        (mask , without_mask) = pred\n        \n        #determine the class label and color we will use to draw the bounding box and text\n        label = \"Mask\" if mask > without_mask else \"No Mask\"\n        color = (0,255,0) if label == \"Mask\" else (0,0,255)\n        \n        #include the probability in the label\n        label = \"{}:{:.2f}\".format(label , max(mask , without_mask)*100)\n        \n        #display the label and bounding box rectangle on the output frame\n        cv2.putText(frame , label , (startX , starty-10) , cv2.FONT_HERSHEY_SIMPLEX , 0.45 , color , 2)\n        cv2.rectangle(frame , (startX , starty) , (endX , endy) , color , 2)\n        \n    #show the output frame\n    cv2.imshow(\"Frame\" , frame)\n    key = cv2.waitKey(1)&0xff\n    \n    #if the \"q\" key is pressed , break from the loop\n    if key == ord(\"q\"):\n        break","a0bd247b":"cv2.destroyAllWindows()\nvs.stop()","7ab61082":"#this what you will show if you run the code in another notebook like google colab or jupyter\nfrom IPython.display import display, Image\n\nwithout_mask = \"..\/input\/model-deploying-pictures\/without_mask.png\"\ndisplay(Image(filename = without_mask))\n","b5bb9084":"with_mask = \"..\/input\/model-deploying-pictures\/with_mask.png\"\ndisplay(Image(filename = with_mask))","f8d95491":"**Deplying the model**","5710efd7":"The reason of the previous error is the kaggle notebook can't access the webcam , but if we try the same code in google colab or any other notebook it will work correctly "}}