{"cell_type":{"0d1c0dac":"code","6b0149ee":"code","0de7f035":"code","f2ec5fb9":"code","4c3caa5e":"code","4c0e863f":"code","49d131ca":"code","770e494d":"code","616b8934":"code","c3016dfe":"code","0aa66c27":"code","e258171e":"code","d08ef86f":"code","084e188b":"code","0c4773a1":"code","3ae4bf86":"code","4fec21e2":"markdown"},"source":{"0d1c0dac":"import os\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\n\nimport scipy as sp\nfrom scipy.fftpack import fft\nfrom tsfresh.feature_extraction import feature_calculators\n\nimport gc\n%matplotlib inline\nprint(os.listdir(\"..\/input\"))","6b0149ee":"%time\ntrain_df = pd.read_csv(os.path.join(\"..\/input\",'train.csv'), dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","0de7f035":"train_df.shape","f2ec5fb9":"rows = 150000\nsegments = int(np.floor(train_df.shape[0] \/ rows))\nprint(\"Number of segments: \", segments)","4c3caa5e":"#Prepare empty frame\ntrain_X = pd.DataFrame(index=range(segments), dtype=np.float64)\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","4c0e863f":"def create_features(seg_id,seg, X):\n    xc = seg[\"acoustic_data\"]\n    \n    X.loc[seg_id,\"num_peaks_1\"] = feature_calculators.number_peaks(xc,1)\n    X.loc[seg_id,\"num_peaks_5\"] = feature_calculators.number_peaks(xc,5)\n    X.loc[seg_id,\"num_peaks_10\"] = feature_calculators.number_peaks(xc,10)\n    \n    X.loc[seg_id,\"cid_ce_1\"] = feature_calculators.cid_ce(xc, 1)\n    \n    X.loc[seg_id,'moment_4'] = sp.stats.moment(xc, 4)\n    X.loc[seg_id,'moment_2'] = sp.stats.moment(xc, 2)\n    \n    X.loc[seg_id,\"range_m1000_0\"] = feature_calculators.range_count(xc, -1000, 0)\n    X.loc[seg_id,\"c_5\"] = feature_calculators.c3(xc, 5)\n    X.loc[seg_id,\"mean\"] = xc.mean()\n    X.loc[seg_id,\"binned_entropy_5\"] = feature_calculators.binned_entropy(xc, 5)\n    X.loc[seg_id,\"autocorrelation_10\"] = feature_calculators.autocorrelation(xc, 10)\n    \n    \n    window_size = 10\n    xc_rolled = xc.rolling(window_size)\n    xc_rolled_var = xc_rolled.var().dropna()\n    xc_rolled_mean = xc_rolled.mean().dropna()\n        \n    window_str = str(window_size)\n\n    X.loc[seg_id,\"rollingMean\"+window_str+\"_quantile_4\"] = xc_rolled_mean.quantile(0.04)\n    rolled_var_quantiles = xc_rolled_var.quantile([0.01,0.04])\n    X.loc[seg_id,\"rollingVar\"+window_str+\"_quantile_4\"] = rolled_var_quantiles[0.04]\n    X.loc[seg_id,\"rollingVar\"+window_str+\"_quantile_1\"] = rolled_var_quantiles[0.01]\n    \n    window_size = 10\n    window_str = str(window_size)\n    xc_rolled = xc.rolling(300)\n    xc_rolled_var = xc_rolled.var().dropna()\n    X.loc[seg_id,\"rollingVar\"+window_str+\"_quantile_2\"] = xc_rolled_var.quantile(0.02)","49d131ca":"#create features from train\nfor seg_id in tqdm_notebook(range(segments)):\n    seg = train_df.iloc[seg_id*rows:seg_id*rows+rows]\n    create_features(seg_id, seg,train_X)\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]    ","770e494d":"#create features from test\nsegment_names = [file for file in os.listdir(\"..\/input\/test\") if file.startswith(\"seg\")]\ntest_df = pd.DataFrame(index=segment_names, dtype=np.float64)\ntest_df.index = test_df.index.str[:-4]\nfor file in tqdm_notebook(segment_names):\n    seg_id = file[:-4]\n    segment = pd.read_csv(os.path.join(\"..\/input\/test\",file),dtype={'acoustic_data': np.int16})\n    create_features(seg_id,segment,test_df)\n","616b8934":"train_X.head()","c3016dfe":"train_X.shape","0aa66c27":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\nparams = {\n    'lambda_l1': 0.012465994599126015, \n    'bagging_freq': 15, \n    'verbose': -1, \n    'min_data_in_leaf': 5, \n    'feature_fraction': 0.7143153769050614, \n    'objective': 'MAE',\n    'lambda_l2': 0.055052283158846985, \n    'metric': 'MAE', \n    'bagging_fraction': 0.4871803105884792,\n    'max_depth': -1, \n    'learning_rate': 0.007017896834582354, \n    'boosting_type': 'gbdt', \n    'num_leaves': 9\n}","e258171e":"def train_lgb(train_X,train_y,test_df,params,folds):\n    features_importance = pd.DataFrame({\"features\":train_X.columns,\n                                        \"importance\":np.zeros(train_X.columns.shape[0])})\n    predictions = pd.DataFrame({\"seg_id\":test_df.index,\"time_to_failure\":np.zeros(test_df.shape[0])})\n    oof = np.zeros(train_X.shape[0])\n\n    for train_idx,val_idx in folds.split(train_X,train_y):\n        X_train,y_train = train_X.iloc[train_idx],train_y.iloc[train_idx]\n        X_val,y_val = train_X.iloc[val_idx],train_y.iloc[val_idx]\n\n        model = lgb.LGBMRegressor(**params, n_estimators = 20000,n_jobs=-1)\n        model.fit(X_train,y_train,\n                  eval_set=[(X_train,y_train),(X_val,y_val)], \n                  verbose=1000,\n                  early_stopping_rounds=1000)\n\n        oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n\n        features_importance[\"importance\"] += model.feature_importances_\n        predictions[\"time_to_failure\"] += model.predict(test_df, num_iteration=model.best_iteration_)\n    return oof,predictions,features_importance\n\noof,predictions,features_importance = train_lgb(train_X,train_y,test_df,params,folds)","d08ef86f":"mean_absolute_error(train_y,oof)","084e188b":"features_importance[\"importance\"] = features_importance[\"importance\"]\/5\npredictions[\"time_to_failure\"] = predictions[\"time_to_failure\"]\/5\n\nplt.figure(figsize=(10,10))\nax = sns.barplot(x=\"importance\", y=\"features\", data=features_importance.sort_values(by=\"importance\",ascending=False))","0c4773a1":"predictions.head()","3ae4bf86":"predictions.to_csv(\"submission_lgb_15_col.csv\",index=False)","4fec21e2":"Thanks for sharing great [kernel](https:\/\/www.kaggle.com\/abhishek\/quite-a-few-features-1-51), which gave me some ideas of the features.  "}}