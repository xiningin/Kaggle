{"cell_type":{"a1a96e5c":"code","27115086":"code","a63e163f":"code","46f75a04":"code","b4411780":"code","3905d48c":"code","274f1526":"code","18bdf8cc":"code","d3b17c27":"code","88cbe759":"code","afb4db4d":"code","9744b2fa":"code","295b3bf1":"code","e5c1f568":"code","1e6b48d4":"code","baec1e83":"code","0000ab8b":"code","5cda32b4":"code","434e4fd6":"code","0c6ed8e7":"code","bf511975":"code","d3ad18da":"code","f4d17862":"code","284608ca":"code","36b1dac0":"code","06b0e33d":"code","36790848":"code","5723050f":"code","78065113":"code","32d3aa6b":"code","385fcb81":"code","b53103a5":"code","8c5ac58f":"code","604f2eec":"code","f0a497f8":"code","05291bb7":"code","94832fe6":"code","767081f7":"code","ca2af11d":"code","5a00faf0":"code","5fb9ce2b":"code","dfeba28b":"code","f3a90fc2":"code","155e498c":"code","31a9489f":"code","45bbd0f8":"code","6e74e72a":"code","4703f636":"code","70fe644c":"code","81a7a350":"code","c0814326":"code","d5312901":"code","e3ef3ea7":"code","90e8d001":"code","06033c73":"code","0ed690c7":"code","f61b7874":"code","8b37da6f":"code","2b1c8c4a":"code","ed68e5c9":"code","efcfcc7e":"code","01529c82":"code","43e6070d":"code","e3e1e64c":"code","27c16e23":"code","48f3482c":"code","8ee31fa6":"code","2a3b0e14":"code","199d719d":"code","eba8dbab":"code","b1429f91":"code","ff166dd7":"code","e96ef2d1":"code","43ef2f8e":"code","c54750b2":"code","bfb3374f":"code","cd61a1a7":"code","254d5dd2":"code","85c74c84":"code","fed30e38":"code","f9f023d3":"code","1b817368":"code","600c3090":"code","8e8d0792":"markdown","b80d2c32":"markdown","41e08f04":"markdown","bc48d8ad":"markdown","74b4774b":"markdown","50e6f1ed":"markdown","d24fd2b7":"markdown","04d4569c":"markdown","5020b3f9":"markdown","e2046cd4":"markdown","2209f59a":"markdown","cfadd4d3":"markdown"},"source":{"a1a96e5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","27115086":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing, metrics\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, classification\nimport time\nimport random","a63e163f":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","46f75a04":"def model_evaluator(model,train_data,target_data,test_data):\n    model.fit(train_data,target_data)\n    score=model_selection.cross_val_score(model,train_data,target_data,cv=3,scoring='f1')\n    return score","b4411780":"#exploring data\ntrain.head()","3905d48c":"#count vectorizing string-data\ncount_vectorizer=feature_extraction.text.CountVectorizer()\ntrain_vectors=count_vectorizer.fit_transform(train.text[:])\ntest_vectors=count_vectorizer.transform(test.text[:])\ntarget_vectors=train.target[:]","274f1526":"diff=train_vectors.mean(axis=0)-test_vectors.mean(axis=0)\nsns.scatterplot(x=np.arange(diff.shape[1]),y=np.array(diff)[0])","18bdf8cc":"#evaluating val score of count_vectrized Ridge_data\nclf=linear_model.RidgeClassifier()\nclf.fit(train_vectors,target_vectors)\nscore=model_selection.cross_val_score(clf,train_vectors,target_vectors,cv=5,scoring='f1')\na=score\nprint('cross validation score for count_vectorized data:',a.mean())","d3b17c27":"#predicting for test data\nclf.fit(train_vectors,target_vectors)\ntest_preds=clf.predict(test_vectors)\n\n#making final submission\na=pd.DataFrame({'id':test.id,'target':test_preds})\n#a.to_csv('submission.csv',index=False)","88cbe759":"# tf-idf processing the data\ntfidf=feature_extraction.text.TfidfTransformer()\ntrain_tfidf=tfidf.fit_transform(train_vectors)\ntest_tfidf=tfidf.transform(test_vectors)\nclf=linear_model.RidgeClassifier()","afb4db4d":"train_tfidf.todense().shape","9744b2fa":"#evaluation tf_idf data with Ridge classifier (score improvement is observed)\nclf=linear_model.RidgeClassifier()\nclf.fit(train_tfidf,target_vectors)\nscore=model_selection.cross_val_score(clf,train_tfidf,target_vectors,cv=5,scoring='f1')\na=score\nprint('cross validation score for count_vectorized data:',a.mean())","295b3bf1":"#predicting for test data\nclf.fit(train_tfidf,target_vectors)\ntest_preds=clf.predict(test_tfidf)\n\n#making final submission\na=pd.DataFrame({'id':test.id,'target':test_preds})\n#a.to_csv('submission.csv',index=False)","e5c1f568":"def text_cleaning(text):\n    a=[char for char in text if char not in string.punctuation]\n    a=''.join(a)\n    a=a.split()\n    #a=[c for c in a if c.lower() not in stopwords.words('english')]\n    return  a","1e6b48d4":"#text cleaning done\ntrain['text'].apply(text_cleaning)","baec1e83":"count_vectorizer=feature_extraction.text.CountVectorizer(analyzer=text_cleaning).fit(train['text'])\nclf=linear_model.RidgeClassifier()","0000ab8b":"#vectorizing and tf_idf transforming again and evaluating\ntrain_vectors=count_vectorizer.transform(train.text)\ntest_vectors=count_vectorizer.transform(test.text)\ntarget_vectors=train.target[:]\ntfidf=feature_extraction.text.TfidfTransformer()\ntrain_tfidf=tfidf.fit_transform(train_vectors)\ntest_tfidf=tfidf.transform(test_vectors)","5cda32b4":"X_train, X_val, y_train, y_val =train_test_split(train_tfidf, target_vectors, test_size=0.33, random_state=42)","434e4fd6":"clf=linear_model.RidgeClassifier()\nclf.fit(X_train,y_train)","0c6ed8e7":"accuracy=[]\nf1_set=[]\nfor i in np.arange(-0.5,1.5,step=0.1):\n    temp=pd.DataFrame(clf.decision_function(X_val))[0].apply(lambda x: 1 if x>i else 0)\n    accuracy.append((temp.values==y_val.values).sum())\n    \n    tn, fp, fn, tp =confusion_matrix(y_val,temp).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)  \n    f1_set.append(f1)\naccuracy=np.array(accuracy)\nf1_set=np.array(f1_set)","bf511975":"plt.scatter(np.arange(len(f1_set))*0.1-0.5,f1_set)","d3ad18da":"f1_set[f1_set.argmax()]","f4d17862":"temp=pd.DataFrame(clf.decision_function(X_val))[0].apply(lambda x: 1 if x>-0 else 0)\nprint((temp.values==y_val.values).sum())","284608ca":"tn, fp, fn, tp =confusion_matrix(y_val,temp).ravel()\nprecision=tp\/(tp+fp)\nrecall=tp\/(tp+fn)\nf1=(2*precision*recall)\/(precision+recall)\nprint(f1)","36b1dac0":"#predicting for test data\nclf.fit(train_tfidf,target_vectors)\ntest_preds=pd.DataFrame(clf.decision_function(test_tfidf))[0].apply(lambda x: 1 if x>-0.11 else 0)\n\n#making final submission\na=pd.DataFrame({'id':test.id,'target':test_preds})\na.to_csv('submission.csv',index=False)","06b0e33d":"target_vectors.mean()","36790848":"test_preds.mean()","5723050f":"clf=linear_model.LogisticRegression(penalty='l2')\nclf.fit(X_train,y_train)","78065113":"sns.distplot(pd.DataFrame(clf.predict_proba(X_val))[1])","32d3aa6b":"pd.DataFrame(clf.predict_proba(X_val))[1].apply(lambda x: 1 if x>0.42 else 0).sum()","385fcb81":"y_val.sum()","b53103a5":"test_preds=pd.DataFrame(clf.predict_proba(test_tfidf))[1].apply(lambda x: 1 if x>0.42 else 0)\n\n#making final submission\na=pd.DataFrame({'id':test.id,'target':test_preds})\na.to_csv('submission.csv',index=False)","8c5ac58f":"test","604f2eec":"from tensorflow import keras\nfrom tensorflow.keras import layers","f0a497f8":"input_shape = [X_train.shape[1]]\n\nmodel = keras.Sequential([layers.BatchNormalization(),\n                         layers.Dense(256,activation='relu',input_shape=input_shape),\n                         layers.BatchNormalization(),\n                         layers.Dropout(0.3),\n                         layers.Dense(1,activation='sigmoid')])","05291bb7":"model.compile(optimizer='adam',\n             loss='binary_crossentropy',\n             metrics=['binary_accuracy'])","94832fe6":"X_train.toarray()","767081f7":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    X_train.toarray(), y_train.values,\n    validation_data=(X_val.toarray(), y_val.values),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping])\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","ca2af11d":"history_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))","5a00faf0":"t=model.predict_proba(X_val.toarray())\nsns.distplot(pd.DataFrame(t))","5fb9ce2b":"t_test=pd.DataFrame(model.predict_proba(test_tfidf.toarray()))\nsns.distplot(pd.DataFrame(t_test))","dfeba28b":"test_preds=pd.DataFrame(model.predict_proba(test_tfidf.toarray()))[0].apply(lambda x: 1 if x>0.7 else 0)\na=pd.DataFrame({'id':test.id,'target':test_preds})\na.to_csv('submission.csv',index=False)","f3a90fc2":"clf=tree.DecisionTreeClassifier(max_depth=180)\nclf.fit(X_train,y_train)\nscore=model_selection.cross_val_score(clf,X_train,y_train,cv=5,scoring='f1')\na=score\nprint('cross validation score for count_vectorized data:',a.mean())","155e498c":"y_pred=clf.predict_proba(X_val)\npredicted_probs=pd.DataFrame(y_pred)\npredicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>0.5 else 1)\ny_prednew=predicted_probs[2].values\ntn, fp, fn, tp =confusion_matrix(y_val,y_prednew).ravel()\nprecision=tp\/(tp+fp)\nrecall=tp\/(tp+fn)\nf1=(2*precision*recall)\/(precision+recall)\nprint(\"f1= \",f1)","31a9489f":"sns.distplot(pd.DataFrame(y_pred)[1])","45bbd0f8":"#predicting for test data\nclf.fit(train_tfidf,target_vectors)\ntest_preds=clf.predict(test_tfidf)\n\n#making final submission\na=pd.DataFrame({'id':test.id,'target':test_preds})\n#a.to_csv('submission.csv',index=False)","6e74e72a":"clf.get_depth()","4703f636":"%%time\nerr=[]\nfor i in np.arange(5,411,step=5): \n    clf=tree.DecisionTreeClassifier(max_depth=i)\n    clf.fit(train_tfidf,target_vectors)\n    score=model_selection.cross_val_score(clf,train_tfidf,target_vectors,cv=5,scoring='f1')\n    a=score\n    print('cross validation score for count_vectorized data:',a.mean())\n    err.append(a.mean())\n\nerr=np.array(err)\nplt.scatter(x=range(len(err)),y=err)","70fe644c":"plt.scatter(x=np.arange(len(err))*5+5,y=err,)","81a7a350":"from sklearn.ensemble import RandomForestClassifier","c0814326":"X_train, X_val, y_train, y_val = train_test_split(train_tfidf,target_vectors, test_size=0.3, random_state=0)","d5312901":"clf=RandomForestClassifier(max_features=\"sqrt\",n_estimators=400,oob_score=False,random_state=0)\nclf.fit(X_train,y_train)","e3ef3ea7":"err=[]\nfor i in range(400):\n    y_pred=clf.estimators_[i].predict(X_val)\n    tn, fp, fn, tp =confusion_matrix(y_val,y_pred).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    err.append(f1)\n    \nerr=np.array(err)","90e8d001":"cum_err=[]\nfor i in range(len(err)):\n    temp=sum(err[0:i]\/(i+1))\n    cum_err.append(temp)\n    \ncum_err=np.array(cum_err)","06033c73":"plt.scatter(x=np.arange(400)+1,y=cum_err)","0ed690c7":"clf=RandomForestClassifier(max_features=\"auto\",n_estimators=40,oob_score=False,random_state=0)\nclf.fit(X_train,y_train)","f61b7874":"err=[]\nfor i in range(40):\n    y_pred=clf.estimators_[i].predict(X_val)\n    tn, fp, fn, tp =confusion_matrix(y_val,y_pred).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    err.append(f1)\n    \nerr=np.array(err)\n\ncum_err=[]\nfor i in range(len(err)):\n    temp=sum(err[0:i]\/(i+1))\n    cum_err.append(temp)\n    \ncum_err=np.array(cum_err)\n\nplt.scatter(x=np.arange(40)+1,y=cum_err)\nplt.xlabel('number of trees')\nplt.ylabel('f1-score')","8b37da6f":"depth=[]\nfor i in range(len(clf.estimators_)):\n    depth.append(clf.estimators_[0].get_depth())\n    \ndepth=np.array(depth)","2b1c8c4a":"err=[]\nfor i in np.arange(1,180):\n    clf=RandomForestClassifier(max_depth=i,n_estimators=40,oob_score=False,random_state=0)\n    clf.fit(X_train,y_train)\n    y_pred=clf.predict(X_val)\n    tn, fp, fn, tp =confusion_matrix(y_val,y_pred).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    err.append(f1)\n    \nerr=np.array(err)","ed68e5c9":"plt.scatter(x=np.arange(len(err))+1,y=err)\nplt.xlabel('max_depth')\nplt.ylabel('f1-score')","efcfcc7e":"err=[]\nfor i in np.arange(1000,26000,step=1000):\n    clf=RandomForestClassifier(max_depth=100,n_estimators=40,max_features=i,random_state=0)\n    clf.fit(X_train,y_train)\n    y_pred=clf.predict(X_val)\n    tn, fp, fn, tp =confusion_matrix(y_val,y_pred).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    err.append(f1)\n    \nerr=np.array(err)","01529c82":"plt.scatter(x=np.arange(len(err))+1,y=err)\nplt.xlabel('max_features')\nplt.ylabel('f1-score')","43e6070d":"err.argmax()","e3e1e64c":"ts=time.time()\nclf=RandomForestClassifier(max_features=\"auto\",n_estimators=150,oob_score=False,random_state=0)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_val)\ntn, fp, fn, tp =confusion_matrix(y_val,y_pred).ravel()\nprecision=tp\/(tp+fp)\nrecall=tp\/(tp+fn)\nf1=(2*precision*recall)\/(precision+recall)\nprint(\"f1= \",f1)\nprint(\"time taken\", time.time()-ts)","27c16e23":"clf.estimators_[3].get_depth()","48f3482c":"f1_set=[]\nfor i in np.arange(0.3,0.9,step=0.01):\n    y_pred=clf.predict_proba(X_val)\n    predicted_probs=pd.DataFrame(y_pred)\n    predicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>i else 1)\n    y_prednew=predicted_probs[2].values\n    tn, fp, fn, tp =confusion_matrix(y_val,y_prednew).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    f1_set.append(f1)\n    \nf1_set=np.array(f1_set)","8ee31fa6":"plt.scatter(np.arange(0.3,0.9,step=0.01),f1_set)","2a3b0e14":"thres=f1_set.argmax()*0.01+0.3","199d719d":"thres","eba8dbab":"clf=RandomForestClassifier(max_features=\"sqrt\",n_estimators=150,oob_score=False,random_state=0)\nclf.fit(train_tfidf,target_vectors)\ntest_preds=clf.predict_proba(test_tfidf)\npredicted_probs=pd.DataFrame(test_preds)\npredicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>0.62 else 1)\ny_prednew=predicted_probs[2].values\n#making final submission\na=pd.DataFrame({'id':test.id,'target':y_prednew})\na.to_csv('submission.csv',index=False)","b1429f91":"predicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>0.62 else 1)\ny_prednew=predicted_probs[2].values\ny_prednew.mean()","ff166dd7":"len(clf.estimators_)","e96ef2d1":"from sklearn.ensemble import GradientBoostingClassifier\nrandom.seed(0) ","43ef2f8e":"ts=time.time()\nclf = GradientBoostingClassifier(n_estimators=600, learning_rate=0.05\n                                 max_depth=3,max_features=\"sqrt\",random_state=0).fit(X_train, y_train)\n\nprint(\"time taken=\",time.time()-ts)","c54750b2":"f1_sets=[]\nfor i in np.arange(0.3,0.9,step=0.01):\n    y_pred=clf.predict_proba(X_val)\n    predicted_probs=pd.DataFrame(y_pred)\n    predicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>i else 1)\n    y_prednew=predicted_probs[2].values\n    tn, fp, fn, tp =confusion_matrix(y_val,y_prednew).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    f1_sets.append(f1)\n\nf1_sets=np.array(f1_sets)","bfb3374f":"thres=f1_sets.argmax()*0.01+0.3","cd61a1a7":"plt.scatter(np.arange(0.3,0.9,step=0.01),f1_sets)","254d5dd2":"thres","85c74c84":"y_pred=clf.predict_proba(X_val)\npredicted_probs=pd.DataFrame(y_pred)\npredicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>thres else 1)\ny_prednew=predicted_probs[2].values\ntn, fp, fn, tp =confusion_matrix(y_val,y_prednew).ravel()\nprecision=tp\/(tp+fp)\nrecall=tp\/(tp+fn)\nf1=(2*precision*recall)\/(precision+recall)\nprint(\"f1=\",f1)","fed30e38":"ts=time.time()\nclf = GradientBoostingClassifier(n_estimators=600, learning_rate=1.0,\n                                 max_depth=1, random_state=0).fit(train_tfidf, target_vectors)\n\nprint(\"time taken=\",time.time()-ts)\n\ny_pred=clf.predict_proba(test_tfidf)\npredicted_probs=pd.DataFrame(y_pred)\npredicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>thres else 1)\ny_prednew=predicted_probs[2].values\na=pd.DataFrame({'id':test.id,'target':y_prednew})\na.to_csv('submission.csv',index=False)","f9f023d3":"ts=time.time()\nf1_sets=[]\nfor  i in np.arange(2000,10000,step=100):\n    clf = GradientBoostingClassifier(n_estimators=i, learning_rate=0.05,\n                                 max_depth=3,max_features=\"sqrt\", random_state=0).fit(X_train, y_train)\n    y_pred=clf.predict_proba(X_val)\n    predicted_probs=pd.DataFrame(y_pred)\n    predicted_probs[2]=predicted_probs[0].apply(lambda x: 0 if x>thres else 1)\n    y_prednew=predicted_probs[2].values\n    tn, fp, fn, tp =confusion_matrix(y_val,y_prednew).ravel()\n    precision=tp\/(tp+fp)\n    recall=tp\/(tp+fn)\n    f1=(2*precision*recall)\/(precision+recall)\n    f1_sets.append(f1)\n    print(\"number of estimators of last run=\",i)\n    print(\"time taken=\",time.time()-ts)\n    \nf1_sets=np.array(f1_sets)","1b817368":"plt.scatter(np.arange(2000,5100,step=100),f1_sets)","600c3090":"print(\"mean of training targets\",y_train.mean())\nprint(\"mean of validation targets\",y_val.mean())\nprint()","8e8d0792":"# LOGISTIC REGRESSION WITH L2 penalty","b80d2c32":"# Random Forest","41e08f04":"### COUNT VECTORIZER","bc48d8ad":"3000 seems fine","74b4774b":"RF final model","50e6f1ed":"# Decision Trees","d24fd2b7":"# Boosting","04d4569c":"# ANN","5020b3f9":"### COUNT VECTORIZER and tf-idf transformation","e2046cd4":"### TEXT CLEANING, COUNT VECTORIZER, TF IDF","2209f59a":"100 trees seem enough","cfadd4d3":"max depth of 100 seems fine"}}