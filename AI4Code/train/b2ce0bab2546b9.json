{"cell_type":{"cbd1d6f5":"code","f8f51dae":"code","78664e40":"code","e57f4f8b":"code","e7f3158d":"code","ff9e27b4":"code","adb3c751":"code","00e2b4a7":"code","29351c3b":"code","812762dd":"code","cf3526c0":"code","9f6e0799":"code","22b8574e":"code","d1f6fe8b":"code","5d1b4006":"code","19379dd7":"code","aca4d8a3":"code","2d51b634":"code","4dbc7c1f":"code","7337c9a7":"code","af98d780":"code","48753532":"code","43ad4319":"code","d9691940":"code","fd9b0100":"code","8c5cae19":"code","16e7be49":"code","a5e89067":"code","cefceb68":"code","8d8bf13c":"code","63d35ba3":"code","bc9813ba":"code","7de6f14b":"code","db6acd0e":"code","8eda34c5":"code","d8eb8108":"code","9200e260":"code","eb00c418":"code","6891dac2":"code","b47c8a8e":"code","d319adb4":"code","16820703":"code","4f977f42":"code","532a1c67":"code","a7ae9362":"code","0227d532":"code","31261cdb":"code","6b8bcffc":"code","8032d8e7":"code","3c559e9f":"code","1644d214":"code","703db498":"code","963cea68":"code","2d0b3e07":"code","7ebf3052":"code","0b251bc7":"code","e9944443":"code","2d6c4d4b":"code","48e28eba":"code","5c6f64ab":"code","028eaaf2":"code","df14e862":"code","4a46d2fc":"code","6113e743":"code","d2ab9ff9":"code","b2761ac0":"code","3ba1c08c":"code","8b1474d9":"code","b569d21d":"code","6f7595cb":"code","ddae8299":"code","81a4efe0":"code","43e89613":"code","b37f68c1":"code","c1321bdc":"code","64355487":"code","549fd683":"code","38d7877c":"code","5ae11e37":"code","e6306fc7":"code","2a26cad7":"code","a3d11e1e":"code","61a5f87d":"code","49bbf026":"code","91af11e3":"code","159c39d7":"code","13ed52fc":"code","e320a6c5":"code","deb0845b":"code","1f5a9b1e":"code","77f31cda":"code","c8f172ed":"code","759af85d":"code","cd0e19b3":"code","3681e6b0":"code","31c451cf":"code","5b7c32a4":"code","9ec3cf7b":"code","fb94d530":"code","dd3b77b0":"code","07c7d4e5":"code","28b075f5":"code","d0cc150a":"code","dfee2101":"code","f6497624":"code","a6cf32f4":"code","4da41f2f":"code","c0164ebe":"code","d24cc863":"code","ec77e432":"code","fae3eda8":"code","601617da":"code","76900e91":"code","807d5dce":"code","ee93bcd8":"code","1a038847":"code","f3672a4a":"code","a5540ebd":"code","b6322001":"code","f715550e":"code","9a0f3130":"code","80a7797a":"code","e7d6e87e":"code","2594caa1":"code","a0ee6c55":"code","165f3646":"code","d440f46c":"code","154c903d":"code","0a405328":"code","9438ff71":"code","8f4e0681":"code","1a02e4c0":"code","8610ad89":"code","28a02c6b":"code","8b8a3023":"code","a91b3fa4":"code","35401cdb":"code","e12250d5":"code","e02a7a93":"code","43afc3b2":"code","9995972c":"code","6ba75c7f":"code","d893b6a3":"code","8736db46":"code","0b17d75f":"code","40376c2d":"code","b7b39218":"code","536e1da3":"code","25c8bf8c":"code","0d4e7672":"code","d5fc1919":"code","7bfa966b":"code","e6a77b0d":"code","7a92f866":"code","e2ea50b2":"code","8448f4ed":"code","8a5797c3":"code","8af3faa3":"code","00ce0907":"code","2ae07d77":"code","5559bf8f":"code","24a287b9":"code","5d450926":"code","d45452e2":"code","9357d533":"code","e29b71a6":"markdown","822ae601":"markdown","665c9d3a":"markdown","a9fdd957":"markdown","2e2eaa4d":"markdown","2d444913":"markdown","5dff1d42":"markdown","184202e1":"markdown","2d761811":"markdown","59d52f93":"markdown","28c1a691":"markdown","0bc4d05f":"markdown","b5aa41b2":"markdown","b44cb6d8":"markdown","0b6f7dc5":"markdown","0079e482":"markdown","53c7c1e6":"markdown","119db764":"markdown","1f81afb5":"markdown","00a17e06":"markdown","29e8cec0":"markdown","7b76826d":"markdown","2abf73aa":"markdown","3c7484b1":"markdown","53994e42":"markdown","7dd76644":"markdown","be87e0a0":"markdown","81b5419c":"markdown","e22f6c49":"markdown","62622170":"markdown","2bb0979e":"markdown","c11ba2c7":"markdown","44eed7f0":"markdown","33f0426c":"markdown","aafee6ed":"markdown"},"source":{"cbd1d6f5":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","f8f51dae":"diabetes=pd.read_csv(\"..\/input\/diabetes\/diabetes.csv\")\ndiabetes.copy()\ndf = diabetes.copy()\ndf.dropna()\ndf.head()","78664e40":"df.isnull().sum().sum() #there isn't any null variable","e57f4f8b":"df.info()","e7f3158d":"df[\"Outcome\"].value_counts() #here we are interested in 1 class and other class is 0\n                             #with these information we will examination data","ff9e27b4":"df[\"Outcome\"].value_counts().plot.barh();","adb3c751":"df.describe().T","00e2b4a7":"y= df[\"Outcome\"] #dependent variable\nX= df.drop([\"Outcome\"],axis=1) #independent variables but we excluded Outcome variable because it is dependent variable","29351c3b":"loj=sm.Logit(y,X)\nloj_model=loj.fit()\nloj_model.summary()","812762dd":"#scikit-learn","cf3526c0":"from sklearn.linear_model import LogisticRegression\nloj=LogisticRegression(solver = \"liblinear\")\nloj_model = loj.fit(X,y)\nloj_model","9f6e0799":"loj_model.intercept_  #constent value","22b8574e":"loj_model.coef_ #independent values coefficient values\n","d1f6fe8b":"y_pred = loj_model.predict(X)","5d1b4006":"confusion_matrix(y,y_pred) ","19379dd7":"accuracy_score(y,y_pred)     # our correct classification rate","aca4d8a3":"print(classification_report(y,y_pred))","2d51b634":"loj_model.predict(X)[0:10]   #predicted values from 0 to 10","4dbc7c1f":"loj_model.predict_proba(X)[0:10][:,0:2]   #probability values of prediction\n                                          # first values is 0 probability and second ones are 1 probability\n                            #for example for 1st example 0 probability is 0.35 and 1 probability is 0.64 so answer is 1","7337c9a7":"y[0:10]   #real values from 0 to 10","af98d780":"y_probs=loj_model.predict_proba(X)\ny_probs = y_probs[:,1]","48753532":"y_probs[0:10]","43ad4319":"y_pred = [1 if i > 0.5 else 0 for i in  y_probs] \n\n#here we set a threshold value. We assigned values \u200b\u200babove 0.5 directly to 1 and those below to 0. \n#We did this for all values \u200b\u200bwith the for loop ","d9691940":"y_pred[0:10]","fd9b0100":"confusion_matrix(y,y_pred) ","8c5cae19":"accuracy_score(y,y_pred)     # our correct classification rate","16e7be49":"print(classification_report(y,y_pred))","a5e89067":"loj_model.predict_proba(X)[:,1][0:5]","cefceb68":"logit_roc_auc = roc_auc_score(y, loj_model.predict(X))\n\nfpr, tpr, thresholds = roc_curve(y, loj_model.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Oran\u0131')\nplt.ylabel('True Positive Oran\u0131')\nplt.title('ROC')\nplt.show()","8d8bf13c":"#First of all for tuning we need to divide model as Train and Test set \nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n","63d35ba3":"loj = LogisticRegression(solver = \"liblinear\")\nloj_model = loj.fit(X_train,y_train)\nloj_model","bc9813ba":"accuracy_score(y_test, loj_model.predict(X_test))","7de6f14b":"cross_val_score(loj_model, X_test, y_test, cv = 10).mean()","db6acd0e":"df = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n\n","8eda34c5":"from sklearn.naive_bayes import GaussianNB","d8eb8108":"nb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\nnb_model","9200e260":"nb_model.predict(X_test)[0:10]","eb00c418":"nb_model.predict_proba(X_test)[0:10]","6891dac2":"y_pred = nb_model.predict(X_test)","b47c8a8e":"accuracy_score(y_test, y_pred)","d319adb4":"cross_val_score(nb_model, X_test, y_test, cv = 10).mean()","16820703":"df = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n","4f977f42":"knn = KNeighborsClassifier()\nknn_model= knn.fit(X_train,y_train)\nknn_model","532a1c67":"y_pred = knn_model.predict(X_test)","a7ae9362":"accuracy_score(y_test,y_pred)","0227d532":"print(classification_report(y_test,y_pred))","31261cdb":"knn_params = {\"n_neighbors\":np.arange(1,50)}","6b8bcffc":"knn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,knn_params,cv=10)    \nknn_cv.fit(X_train,y_train)","8032d8e7":"print(\"best score:\"+str(knn_cv.best_score_))        \nprint(\"best parameters:\"+str(knn_cv.best_params_)) ","3c559e9f":"knn= KNeighborsClassifier(11)\nknn_tuned= knn.fit(X_train,y_train)  #final model","1644d214":"knn_tuned.score(X_test,y_test)","703db498":"y_pred=knn_tuned.predict(X_test)","963cea68":"accuracy_score(y_test,y_pred) #we have optimized the model\n","2d0b3e07":"df = diabetes.copy()\ndf = df.dropna()\ny=df[\"Outcome\"]\nX=df.drop([\"Outcome\"],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n","7ebf3052":"svm_model = SVC(kernel= \"linear\").fit(X_train,y_train) #model created","0b251bc7":"svm_model","e9944443":"y_pred=svm_model.predict(X_test)","2d6c4d4b":"accuracy_score(y_test,y_pred) #classification success\n","48e28eba":"svc_params= {\"C\":np.arange(1,10)}\nsvc=SVC(kernel=\"linear\")                        \n                                                              #it took a lot of time so I made these lines comment\n#svc_cv_model=GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose =2)\n#svc_cv_model.fit(X_train,y_train)","5c6f64ab":"#print(\"best parameters:\"+str(svc_cv_model.best_params_)) #it will return as a 5 so 5 is best parameter for SVC in this equation","028eaaf2":"svc_tuned = SVC(kernel=\"linear\",C=5).fit(X_train,y_train) #we found C value as a 5 from svc_cv_model.fit ","df14e862":"y_pred= svc_tuned.predict(X_test)\naccuracy_score(y_test,y_pred) #final model ","4a46d2fc":"df = diabetes.copy()\ndf = df.dropna()\ny=df[\"Outcome\"]\nX=df.drop([\"Outcome\"],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n","6113e743":"svc_model=SVC(kernel=\"rbf\").fit(X_train,y_train) #this time we choose kernel as a \"rbf\"","d2ab9ff9":"dir(svc_model) #SVC properties","b2761ac0":"y_pred=svc_model.predict(X_test)\naccuracy_score(y_test,y_pred)","3ba1c08c":"svc_params = {\"C\": [0.0001,0.001,0.1,1,5,10,50,100],\n             \"gamma\":[0.0001,0.001,0.1,1,5,10,50,100]}","8b1474d9":"svc=SVC()\nsvc_cv_model = GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=2)\nsvc_cv_model.fit(X_train,y_train)","b569d21d":"print(\"best parameters:\"+str(svc_cv_model.best_params_)) # we found C: 10 Gamma:0.0001","6f7595cb":"svc_tuned= SVC(C=10,gamma=0.0001).fit(X_train,y_train) #final model","ddae8299":"y_pred=svc_tuned.predict(X_test)\naccuracy_score(y_test,y_pred)","81a4efe0":"df = diabetes.copy()\ndf = df.dropna()\ny=df[\"Outcome\"]\nX=df.drop([\"Outcome\"],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n","43e89613":"from sklearn.preprocessing import StandardScaler","b37f68c1":"scaler= StandardScaler()","c1321bdc":"scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","64355487":"X_train_scaled[0:5]","549fd683":"from sklearn.neural_network import MLPClassifier","38d7877c":"mlpc= MLPClassifier().fit(X_train_scaled,y_train)","5ae11e37":"y_pred = mlpc.predict(X_test_scaled)        #test prediction\naccuracy_score(y_test,y_pred)              #accuracy score of classification","e6306fc7":"?mlpc","2a26cad7":"mlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"logistic\"]}\n","a3d11e1e":"mlpc = MLPClassifier()\nmlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\n#mlpc_cv_model.fit(X_train_scaled, y_train)           #it took a lot of time so I made it comment","61a5f87d":"### print(\"Best parameters: \" + str(mlpc_cv_model.best_params_)) ","49bbf026":"mlpc_tuned = MLPClassifier(activation = \"logistic\", \n                           alpha = 0.1, \n                           hidden_layer_sizes = (100, 100, 100),\n                          solver = \"adam\")","91af11e3":"mlpc_tuned.fit(X_train_scaled, y_train)","159c39d7":"y_pred = mlpc_tuned.predict(X_test_scaled)\naccuracy_score(y_test, y_pred)","13ed52fc":"df = diabetes.copy()\ndf = df.dropna()\ny=df[\"Outcome\"]\nX=df.drop([\"Outcome\"],axis=1)\n#X=df[\"Pregnancies\"]\nX=pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n","e320a6c5":"from sklearn.tree import DecisionTreeClassifier","deb0845b":"cart= DecisionTreeClassifier()\ncart_model =cart.fit(X_train,y_train)","1f5a9b1e":"!pip install skompiler\n!pip install astor\nfrom skompiler import skompile\nprint(skompile(cart_model.predict).to(\"python\/code\"))  #we want translate cart model predict function to python code\n\n#it is very complicated because there is a lot of parameter to calculate ","77f31cda":"X=[4] ","c8f172ed":"((0 if X[0] <= 2.5 else 0) if X[0] <= 6.5 else 1 if X[0] <= 13.5 else 1)\n#this is rule set which is translated to python here we are predicting diabetes rate by pregnancy time \n#we made X=df[\"Pregnancies\"] this for making it easy to calculate it takes just 1 independent parameter but the real code\n#is much longer because we use all parameters \n","759af85d":"y_pred = cart_model.predict(X_test)\naccuracy_score(y_test, y_pred)","cd0e19b3":"?cart_model  #cart documentation we can learn cart models functions and attributes etc ","3681e6b0":"cart_grid={\"max_depth\": range(1,10),\n          \"min_samples_split\": list(range(2,50))}","31c451cf":"cart= tree.DecisionTreeClassifier()\ncart_cv=GridSearchCV(cart,cart_grid,cv=10,n_jobs=-1,verbose=2)\ncart_cv_model = cart_cv.fit(X_train,y_train)  #Cross Validation Model","5b7c32a4":"print(\"Best Parameters: \"+str(cart_cv_model.best_params_)) #best parameters f\u0131r CV model","9ec3cf7b":"#Final (Tuned) Model","fb94d530":"cart= tree.DecisionTreeClassifier(max_depth=5,min_samples_split=19) #tuned model creation\ncart_tuned=cart.fit(X_train,y_train)","dd3b77b0":"y_pred = cart_tuned.predict(X_test)  #tuned model score\naccuracy_score(y_test, y_pred)      #it is better than untuned model difference is 7 unit","07c7d4e5":"df = diabetes.copy()\ndf = df.dropna()\ny=df[\"Outcome\"]\nX=df.drop([\"Outcome\"],axis=1)\n#X=df[\"Pregnancies\"]\nX=pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)\n","28b075f5":"from sklearn.ensemble import RandomForestClassifier","d0cc150a":"rf_model = RandomForestClassifier().fit(X_train,y_train)","dfee2101":"y_pred= rf_model.predict(X_test)\naccuracy_score(y_test,y_pred) #primitive score","f6497624":"rf_model","a6cf32f4":"?rf_model #random forest documentation","4da41f2f":"rf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}","c0164ebe":"rf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) ","d24cc863":"#rf_cv_model.fit(X_train, y_train)                #it took a lot of time so I made it comment","ec77e432":"#print(\"Best Parameters: \" + str(rf_cv_model.best_params_))  #best parameters  ","fae3eda8":"#final model","601617da":"rf_tuned = RandomForestClassifier(max_depth = 10, \n                                  max_features = 8, \n                                  min_samples_split = 10,\n                                  n_estimators = 1000)\n\nrf_tuned.fit(X_train, y_train)","76900e91":"y_pred = rf_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)       #Tuned model score 0.757575","807d5dce":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)","ee93bcd8":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Variable Importance Levels\")","1a038847":"df = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\n#X = df[\"Pregnancies\"]\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","f3672a4a":"from sklearn.ensemble import GradientBoostingClassifier","a5540ebd":"gbm_model = GradientBoostingClassifier().fit(X_train, y_train) #model created","b6322001":"y_pred = gbm_model.predict(X_test)\naccuracy_score(y_test, y_pred)   #primitive score","f715550e":"gbm_model","9a0f3130":"?gbm_model","80a7797a":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}","e7d6e87e":"gbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)","2594caa1":"#gbm_cv.fit(X_train, y_train)      #it took a lot of time so I made it comment","a0ee6c55":"#print(\"Best Parameters: \" + str(gbm_cv.best_params_))   ","165f3646":"gbm = GradientBoostingClassifier(learning_rate = 0.01,  #tuned model parameters which we found above\n                                 max_depth = 3,\n                                min_samples_split = 5,\n                                n_estimators = 500)","d440f46c":"gbm_tuned =  gbm.fit(X_train,y_train) #tuned model","154c903d":"y_pred = gbm_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)    #tuned final score","0a405328":"df = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\n#X = df[\"Pregnancies\"]\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","9438ff71":"#!pip install xgboost\nfrom xgboost import XGBClassifier","8f4e0681":"xgb_model = XGBClassifier().fit(X_train, y_train)","1a02e4c0":"xgb_model","8610ad89":"y_pred = xgb_model.predict(X_test)\naccuracy_score(y_test, y_pred)","28a02c6b":"?xgb_model","8b8a3023":"xgb_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_samples_split\": [2,5,10]}","a91b3fa4":"xgb = XGBClassifier()         \n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)","35401cdb":"#xgb_cv_model.fit(X_train, y_train) #it takes a lot of time so I use comment on finding best parameters ","e12250d5":"#xgb_cv_model.best_params_  #best values are in below. Training and fitting took a lot of time. \n                            #at the below I created final and tuned model","e02a7a93":"xgb = XGBClassifier(learning_rate = 0.01,  #tuned model parameters\n                    max_depth = 6,\n                    min_samples_split = 2,\n                    n_estimators = 100,\n                    subsample = 0.8)","43afc3b2":"xgb_tuned =  xgb.fit(X_train,y_train)     #tuned model","9995972c":"y_pred = xgb_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","6ba75c7f":"df = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\n#X = df[\"Pregnancies\"]\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","d893b6a3":"#!conda install -c conda-forge lightgbm\nfrom lightgbm import LGBMClassifier","8736db46":"lgbm_model = LGBMClassifier().fit(X_train, y_train)","0b17d75f":"y_pred = lgbm_model.predict(X_test)\naccuracy_score(y_test, y_pred)","40376c2d":"## Model Tuning","b7b39218":"?lgbm_model","536e1da3":"lgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_child_samples\": [5,10,20]}","25c8bf8c":"lgbm = LGBMClassifier()\n\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose = 2)\n\n","0d4e7672":"#lgbm_cv_model.fit(X_train, y_train)      #it took a lot of time so I made it comment","d5fc1919":"#lgbm_cv_model.best_params_                      #best params are  {'learning_rate': 0.05,\n                                                                     #'max_depth': 3,\n                                                                     #'min_child_samples': 20,\n                                                                     #'n_estimators': 100,\n                                                                     #'subsample': 0.6}\n","7bfa966b":"lgbm = LGBMClassifier(learning_rate = 0.01,            #tuned model parameters\n                       max_depth = 3,\n                       subsample = 0.6,\n                       n_estimators = 500,\n                       min_child_samples = 20)","e6a77b0d":"lgbm_tuned = lgbm.fit(X_train,y_train)        #we created tuned model","7a92f866":"y_pred = lgbm_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)                   #tuned model score","e2ea50b2":"df = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\n#X = df[\"Pregnancies\"]\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","8448f4ed":"#!pip install catboost\nfrom catboost import CatBoostClassifier","8a5797c3":"cat_model = CatBoostClassifier().fit(X_train, y_train)","8af3faa3":"y_pred = cat_model.predict(X_test)\naccuracy_score(y_test, y_pred)          #primitive score it is actually really good for primitive model if we compare with\n                                        #other algorithms","00ce0907":"catb_params = {\n    'iterations': [200,500],\n    'learning_rate': [0.01,0.05, 0.1],\n    'depth': [3,5,8] }","2ae07d77":"catb = CatBoostClassifier()\ncatb_cv_model = GridSearchCV(catb, catb_params, cv=5, n_jobs = -1, verbose = 2)\n#catb_cv_model.fit(X_train, y_train)  #it took a lot of time so I made it comment","5559bf8f":"#catb_cv_model.best_params_","24a287b9":"catb = CatBoostClassifier(iterations = 200, \n                          learning_rate = 0.05, \n                          depth = 5)\n\ncatb_tuned = catb.fit(X_train, y_train)\ny_pred = catb_tuned.predict(X_test)","5d450926":"y_pred = catb_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","d45452e2":"modeller = [\n    knn_tuned,\n    loj_model,\n    svc_tuned,\n    nb_model,\n    mlpc_tuned,\n    cart_tuned,\n    rf_tuned,\n    gbm_tuned,\n    catb_tuned,\n    lgbm_tuned,\n    xgb_tuned\n    \n]\n\n\nfor model in modeller:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    dogruluk = accuracy_score(y_test, y_pred)\n    print(\"-\"*28)\n    print(isimler + \":\" )\n    print(\"Accuracy: {:.4%}\".format(dogruluk))\n    \n    #MLP is %34 it is very normal. Because we scaled independent values of test set and used it on model. But here \n    #every value use normal X_test if we want to use scaled values we need to use if else blocks in loop for MLP\n    #MLP's normal score is around %74","9357d533":"sonuc = []\n\nsonuclar = pd.DataFrame(columns= [\"Modeller\",\"Accuracy\"])\n\nfor model in modeller:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    dogruluk = accuracy_score(y_test, y_pred)    \n    sonuc = pd.DataFrame([[isimler, dogruluk*100]], columns= [\"Modeller\",\"Accuracy\"])\n    sonuclar = sonuclar.append(sonuc)\n    \n    \nsns.barplot(x= 'Accuracy', y = 'Modeller', data=sonuclar, color=\"r\")\nplt.xlabel('Accuracy %')\nplt.title('Accuracy Rates of Models');    ","e29b71a6":"# KNN","822ae601":"## Model & Prediction","665c9d3a":"## Model Tuning","a9fdd957":"# RBF SVC","2e2eaa4d":"### \u0130MPORTANT","2d444913":"## one of the most important part is the loop part and it is very clear for understanding\n## we just made all actions that we made above, in loop","5dff1d42":"# LightGBM","184202e1":"# Support Vector Classifier (SVC)","2d761811":"# Classification Problems","59d52f93":"## Model","28c1a691":"## Model Tuning","0bc4d05f":"## Model & Prediction","b5aa41b2":"## Model Tuning","b44cb6d8":"## Visualization part is a little bit harder to understand","0b6f7dc5":"# Gaussian Naive Bayes","0079e482":"## Model Tuning","53c7c1e6":"## Model Tuning","119db764":"# Artificial neural networks\n","1f81afb5":"## Prediction & Model Tuning","00a17e06":"# CatBoost","29e8cec0":"# comparison of all models\n","7b76826d":"# CART","2abf73aa":"## Model & Prediction","3c7484b1":"# Logistic Regression","53994e42":"## Model Tuning","7dd76644":"## Model & Prediction","be87e0a0":"## Model Tuning","81b5419c":"# Random Forests Classifier (RF)","e22f6c49":"# Gradient Boosting Machines","62622170":"# XGBoost","2bb0979e":"#### now we are going to do tuning ","c11ba2c7":"## Model Tuning","44eed7f0":"## Model Tuning","33f0426c":"## Model & Prediction","aafee6ed":"## Model & Tahmin\n"}}