{"cell_type":{"675ec957":"code","b64c4d3d":"code","ed86ea1d":"code","5043925e":"code","da7d08ed":"code","1a90e658":"code","9b55dcd1":"code","ac0f41f6":"code","97efded5":"code","f9c607dc":"code","a763b9fe":"code","a1a13c11":"code","9f758f6d":"code","a413ea08":"code","d1b9fbd8":"code","0b098d7e":"code","028f8e03":"code","bc18b00a":"code","fabe5b1c":"code","5fb24677":"code","c42b8d55":"code","4633ea20":"code","f4042e86":"code","10377d8c":"code","66d4c4ba":"code","d06659b1":"code","ee9c7551":"code","9179e301":"code","9fe6d1fe":"code","2d5f6fec":"code","238d4df4":"code","5fe9215e":"code","fd56a84b":"markdown","3690d3f1":"markdown","77e92d92":"markdown","79d99380":"markdown","481f6442":"markdown","319a41d0":"markdown","c4881b51":"markdown","28b6840d":"markdown","66975e19":"markdown","cbb91354":"markdown","e41f09b7":"markdown","754f6437":"markdown","692141dc":"markdown","212d4288":"markdown","9cba0617":"markdown","56e37242":"markdown","1f84e74a":"markdown"},"source":{"675ec957":"# load packages\nimport glob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\nfrom sklearn import decomposition, ensemble\nfrom sklearn.decomposition import PCA\nimport xgboost\n\nimport keras\nfrom keras.layers import Input, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport string\nimport pandas as pd\nfrom nltk.stem.porter import PorterStemmer\nimport re\nimport spacy\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom spacy.lang.en import English\nspacy.load('en')\nparser = English()","b64c4d3d":"#Load text files from train folder\npos_tran_files = glob.glob('\/kaggle\/input\/aclimdb\/aclImdb\/train\/pos' + \"\/*.txt\")\nneg_train_files = glob.glob('\/kaggle\/input\/aclimdb\/aclImdb\/train\/neg' + \"\/*.txt\")","ed86ea1d":"#read text files from train folder\npos_train_txt = []\npos_train_label = []\nfor txt in pos_tran_files:\n    data = open(txt,encoding=\"utf-8\").read()\n    pos_train_txt.append(data)\n    pos_train_label.append('pos')\n    \nneg_train_txt = []\nneg_train_label = []\nfor txt in neg_train_files:\n    data = open(txt,encoding=\"utf-8\").read()\n    neg_train_txt.append(data)\n    neg_train_label.append('neg')","5043925e":"#Load text files from test folder\npos_test_files = glob.glob('\/kaggle\/input\/aclimdb\/aclImdb\/test\/pos' + \"\/*.txt\")\nneg_test_files = glob.glob('\/kaggle\/input\/aclimdb\/aclImdb\/test\/neg' + \"\/*.txt\")","da7d08ed":"#read text files from test folder\npos_test_txt = []\npos_test_label = []\nfor txt in pos_test_files:\n    data = open(txt,encoding=\"utf-8\").read()\n    pos_test_txt.append(data)\n    pos_test_label.append('pos')\n    \nneg_test_txt = []\nneg_test_label = []\nfor txt in neg_test_files:\n    data = open(txt,encoding=\"utf-8\").read()\n    neg_test_txt.append(data)\n    neg_test_label.append('neg')","1a90e658":"# create train dataframe\ntrain_pos_DF = pd.DataFrame()\n\ntrain_pos_DF['text'] = pos_train_txt\ntrain_pos_DF['label'] = pos_train_label\n\ntrain_neg_DF = pd.DataFrame()\n\ntrain_neg_DF['text'] = neg_train_txt\ntrain_neg_DF['label'] = neg_train_label\n\ntrainDF = pd.concat([train_pos_DF,train_neg_DF])\ntrainDF = shuffle(trainDF)\ntrainDF = trainDF.reset_index(drop=True)","9b55dcd1":"# create test dataframe\ntest_pos_DF = pd.DataFrame()\n\ntest_pos_DF['text'] = pos_test_txt\ntest_pos_DF['label'] = pos_test_label\n\ntest_neg_DF = pd.DataFrame()\n\ntest_neg_DF['text'] = neg_test_txt\ntest_neg_DF['label'] = neg_test_label\n\ntestDF = pd.concat([test_pos_DF,test_neg_DF])\ntestDF = shuffle(testDF)\ntestDF = testDF.reset_index(drop=True)","ac0f41f6":"trainDF.head()","97efded5":"trainDF.shape","f9c607dc":"testDF.head()","a763b9fe":"testDF.shape","a1a13c11":"trainDF['label'].unique()","9f758f6d":"trainDF['label'].value_counts()","a413ea08":"sns.set(rc={'figure.figsize':(10,10)})\nsns.countplot(trainDF['label'])","d1b9fbd8":"# stop words and spcecial characters \nSTOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) \nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"\u201d\", \"\u201d\",\"''\"]\n\n# Data Cleaner and tokenizer\ndef tokenizeText(text):\n    \n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = text.lower()\n    \n    tokens = parser(text)\n    \n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    \n    # reomve stop words and special charaters\n    tokens = [tok for tok in tokens if tok.lower() not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    \n    tokens = [tok for tok in tokens if len(tok) >= 3]\n    \n    # remove remaining tokens that are not alphabetic\n    tokens = [tok for tok in tokens if tok.isalpha()]\n    \n    # stemming of words\n    #porter = PorterStemmer()\n    #tokens = [porter.stem(word) for word in tokens]\n    \n    tokens = list(set(tokens))\n    #return tokens\n    return ' '.join(tokens[:])","0b098d7e":"# Data cleaning\ntrainDF['text'] = trainDF['text'].apply(lambda x:tokenizeText(x))\ntestDF['text'] = testDF['text'].apply(lambda x:tokenizeText(x))","028f8e03":"# Data preparation\ny_train = trainDF['label'].tolist()\nx_train = trainDF['text'].tolist()\n\ny_test = trainDF['label'].tolist()\nx_test = trainDF['text'].tolist()","bc18b00a":"# Count Vectors as features\n# create a count vectorizer object \ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(x_train+x_test)\n\n# transform the training and test data using count vectorizer object\nxtrain_count =  count_vect.transform(x_train)\nxtest_count =  count_vect.transform(x_test)","fabe5b1c":"tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(x_train+x_test)\nxtrain_tfidf =  tfidf_vect.transform(x_train)\nxtest_tfidf =  tfidf_vect.transform(x_test)","5fb24677":"tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(x_train+x_test)\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\nxtest_tfidf_ngram =  tfidf_vect_ngram.transform(x_test)","c42b8d55":"tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(x_train+x_test)\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(x_train) \nxtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(x_test) ","4633ea20":"# getting train features\nhash_vectorizer = HashingVectorizer(n_features=5000)\nhash_vectorizer.fit(x_train+x_test)\nxtrain_hash_vectorizer =  hash_vectorizer.transform(x_train) \nxtest_hash_vectorizer =  hash_vectorizer.transform(x_test)","f4042e86":"def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    return metrics.accuracy_score(predictions, y_test)","10377d8c":"# Naive Bayes on Count Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, y_train, xtest_count)\nprint(\"NB, Count Vectors: \", accuracy)\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"NB, WordLevel TF-IDF: \", accuracy)\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"NB, N-Gram Vectors: \", accuracy)\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars)\nprint(\"NB, CharLevel Vectors: \", accuracy)","66d4c4ba":"# Linear Classifier on Count Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_count, y_train, xtest_count)\nprint(\"LR, Count Vectors: \", accuracy)\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"LR, WordLevel TF-IDF: \", accuracy)\n\n# Linear Classifier on Ngram Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"LR, N-Gram Vectors: \", accuracy)\n\n# Linear Classifier on Character Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars)\nprint(\"LR, CharLevel Vectors: \", accuracy)\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), xtrain_hash_vectorizer, y_train, xtest_hash_vectorizer)\nprint(\"LR, Hash Vectors: \", accuracy)","d06659b1":"# RF on Count Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(n_estimators=10), xtrain_count, y_train, xtest_count)\nprint(\"RF, Count Vectors: \", accuracy)\n\n# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(n_estimators=10), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"RF, WordLevel TF-IDF: \", accuracy)\n\n# RF on Ngram Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(n_estimators=10), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"RF, N-Gram Vectors: \", accuracy)\n\n# RF on Character Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(n_estimators=10), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars)\nprint(\"RF, CharLevel Vectors: \", accuracy)\n\n# RF on Hash Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(n_estimators=10), xtrain_hash_vectorizer, y_train, xtest_hash_vectorizer)\nprint(\"RF, Hash Vectors: \", accuracy)","ee9c7551":"# Extereme Gradient Boosting on Count Vectorsy_train\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), y_train, xtest_count.tocsc())\nprint(\"Xgb, Count Vectors: \", accuracy)\n\n# Extereme Gradient Boosting on Word Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), y_train, xtest_tfidf.tocsc())\nprint(\"Xgb, WordLevel TF-IDF: \", accuracy)\n\n# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"Xgb, N-Gram Vectors: \", accuracy)\n\n# Extereme Gradient Boosting on Character Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), y_train, xtest_tfidf_ngram_chars.tocsc())\nprint(\"Xgb, CharLevel Vectors: \", accuracy)\n\n# Extereme Gradient Boosting on Hash Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_hash_vectorizer, y_train, xtest_hash_vectorizer)\nprint(\"Xgb, Hash Vectors: \", accuracy)","9179e301":"# get elmo from tensorflow hub\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\n# ELMo Embedding\nembed = hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\", trainable=True)\n\ndef ELMoEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]","9fe6d1fe":"# Data preparation\nle = preprocessing.LabelEncoder()\nle.fit(y_train+y_test)\n\ndef encode(le, labels):\n    enc = le.transform(labels)\n    return keras.utils.to_categorical(enc)\n\ndef decode(le, one_hot):\n    dec = np.argmax(one_hot, axis=1)\n    return le.inverse_transform(dec)\n\ny_train_enc = encode(le, y_train)\ny_test_enc = encode(le, y_test)","2d5f6fec":"# Build Model\ninput_text = Input(shape=(1,), dtype=tf.string)\nembedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\ndense = Dense(256, activation='relu')(embedding)\npred = Dense(2, activation='softmax')(dense)\nmodel = Model(inputs=[input_text], outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(np.asarray(x_train), np.asarray(y_train_enc), epochs=10, batch_size=16)\n    model.save_weights('.\/elmo-model.h5')\n","238d4df4":"# Predict Test data\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('.\/elmo-model.h5')  \n    predicts = model.predict(np.asarray(x_test), batch_size=16)\n\ny_test = decode(le, y_test_enc)\ny_preds = decode(le, predicts)","5fe9215e":"from sklearn import metrics\n\nprint(metrics.confusion_matrix(y_test, y_preds))\n\nprint(metrics.classification_report(y_test, y_preds))\n\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy:\",accuracy_score(y_test,y_preds))","fd56a84b":"###  TF-IDF Vectors","3690d3f1":"### HashingVectorizer","77e92d92":"# Feature Extraction","79d99380":"# Model Building & Classification","481f6442":"### characters level tf-idf","319a41d0":"ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.","c4881b51":"## ELMo","28b6840d":"### Naive Bayes","66975e19":"### Linear Classifier","cbb91354":"### Data preparation","e41f09b7":"### word level tf-idf","754f6437":"### Extereme Gradient Boosting","692141dc":"### RandomForestClassifier","212d4288":"# IMDB Text Classification","9cba0617":"TF-IDF Vectors as features\n \n a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n \n b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores  of N-grams\n \n c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus","56e37242":"### ngram level tf-idf","1f84e74a":"### Count Vectors"}}