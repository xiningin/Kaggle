{"cell_type":{"8f4c2fa8":"code","e2531c7d":"code","709c965e":"code","6decc1e0":"code","32a244ab":"code","011a2fde":"code","f5499b56":"code","aa147e6a":"code","b5c518b9":"code","45e58926":"code","d37a8f5d":"code","f5b078f6":"code","8dc9a1dd":"code","83fbd32c":"code","bf4298a0":"code","96ed41c3":"code","cdcd47c5":"code","c94d9e92":"code","df9d1fc4":"code","8ff1b700":"code","bd725566":"code","e1a1df17":"code","fea2d752":"code","31d858fe":"code","bfa46e8c":"code","ca7fdd32":"code","ae207667":"code","e94f262b":"markdown","54cae1fd":"markdown"},"source":{"8f4c2fa8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n%matplotlib inline","e2531c7d":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head(3)","709c965e":"df_train.info()","6decc1e0":"# Determine correlation and derive feature variables from it.\n# Variables providing less than 0.50 to be discarded. \ndf_train.corr()[['SalePrice']].sort_values(by='SalePrice', ascending=False).head(11)","32a244ab":"# Fill missing values for the variables of interest:\ndf_train['GarageYrBlt'].fillna(round(df_train['GarageYrBlt'].median(), 1), inplace=True)\ndf_train['MasVnrArea'].fillna(0.0, inplace=True)","011a2fde":"# A simple regression plot to visualize correlations on each feature variable to be used:\ndef regplot(x):\n    sns.regplot(x, y=df_train['SalePrice'], data=df_train)","f5499b56":"regplot(x='OverallQual')\nplt.ylim(0, )","aa147e6a":"regplot(x='GrLivArea')\nplt.ylim(0, )","b5c518b9":"regplot(x= 'GarageCars')\nplt.ylim(0, )","45e58926":"regplot(x='GarageArea')\nplt.ylim(0, )","d37a8f5d":"regplot(x= 'TotalBsmtSF')\nplt.ylim(0, )","f5b078f6":"regplot(x='1stFlrSF')\nplt.ylim(0, )","8dc9a1dd":"regplot(x='FullBath')\nplt.ylim(0, )","83fbd32c":"regplot(x= 'TotRmsAbvGrd')\nplt.ylim(0, )","bf4298a0":"regplot(x = 'YearBuilt')\nplt.ylim(0, )","96ed41c3":"regplot(x= 'YearRemodAdd')\nplt.ylim(0, )","cdcd47c5":"X = df_train[['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath',\n         'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'MasVnrArea', 'Fireplaces', 'BsmtFinSF1']]\n# verify for null values\nX.info()","c94d9e92":"# define target variable\ny = df_train[['SalePrice']]\ny.info()","df9d1fc4":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","8ff1b700":"lr = LinearRegression().fit(X_train, y_train)","bd725566":"print('lr.coef_: {}'.format(lr.coef_))\nprint('lr.intercept_: {}'.format(lr.intercept_))","e1a1df17":"print('training set score: {}'.format(lr.score(X_train, y_train)))\nprint('test set score: {}'.format(lr.score(X_test, y_test)))","fea2d752":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=.1)\nridge.fit(X_train, y_train)\nprint('training set score: {}'.format(ridge.score(X_train, y_train)))\nprint('test set score: {}'.format(ridge.score(X_test, y_test)))","31d858fe":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1, max_iter=100000)\nlasso.fit(X_train, y_train)\nprint('training set score: {}'.format(lasso.score(X_train, y_train)))\nprint('test set score: {}'.format(lasso.score(X_test, y_test)))\nprint('number of features used: {}'.format(np.sum(lasso.coef_ != 0)))","bfa46e8c":"from sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor()\ntree.fit(X_train, y_train)\nprint('training set score: {}'.format(tree.score(X_train, y_train)))\nprint('test set score: {}'.format(tree.score(X_test, y_test)))","ca7fdd32":"from sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(random_state=0)\ngbrt.fit(X_train, y_train)\nprint('training set score: {}'.format(gbrt.score(X_train, y_train)))\nprint('test set score: {}'.format(gbrt.score(X_test, y_test)))","ae207667":"# this plot \/ algorithm referenced from the book: Le Machine Learning avec Python \/ O'Reilly \/ 2018\n\ndef plot_feature_importance(model):\n    n_features = X.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X.columns)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Feature')\n    plt.ylim(-1, n_features)\nplot_feature_importance(gbrt)","e94f262b":"As seen above, one inconvenience of decision trees they tend to overfit our training set. To respond to this problem\nlets use random forest.","54cae1fd":"The goal of this kernel is to get a model that provides a reasonable prediction score without applying a thorough model implementation. \n\nI employed a quick EDA, so that the feature variables used were determined from a quick glance. I also restricted the number of model libraries from scikit-learn. The best score obtained with this pragmatic approach was 0.87 on the test set by using Gradient Boosted Regression Trees.   \n\nThe feature variables selected, allowed in particular for no pretreatment such as normalization or standardization. Decision trees work well when there are variables with totally different scales.\n\nYour comments and votes make us all grow ! Cheers ! "}}