{"cell_type":{"9b7555ad":"code","397d94e9":"code","57a16716":"code","d9e348e2":"code","b3f4a75e":"code","41485ec0":"code","310627b1":"code","ca04c524":"code","ff931b5a":"code","717a1fb2":"code","12f0d331":"code","06ae4b4e":"code","3868b782":"code","ad5cf7f4":"code","a400ed5b":"code","629d5e0a":"code","30636d0e":"code","e24169e9":"code","0769f083":"code","a394c739":"code","fe96ddbe":"code","99629c15":"code","641a7c97":"code","c37f5c38":"code","4d1c4be4":"code","efc6d8a6":"code","8916c1c8":"code","9eb21525":"code","c13648ae":"code","d15ade7e":"code","1f45a877":"code","f89ed5c6":"code","d97fc4c0":"code","677056f6":"code","0acfd289":"code","0adba285":"code","c9550bda":"code","1f1e28ee":"code","bd42e233":"code","d47f0ff8":"code","d07e65a7":"code","9a1b6029":"code","cea2bff8":"code","5d42f798":"code","50cb7251":"code","126b836b":"code","18a92e64":"code","66a0d942":"code","b27210d1":"code","687c9e51":"code","5fb89a05":"code","37933454":"code","3cfbff69":"code","72be75b3":"code","bae3f83c":"code","9a2365cb":"code","0364b875":"code","99e9ab6d":"code","3702bfa3":"code","09509b2e":"code","99034e62":"code","b5526564":"code","5404b469":"code","ed2b11c9":"code","08ff29f7":"code","4a65fc4f":"code","c29528ff":"code","880c498a":"code","7bb6c5d9":"code","9b9cfbc3":"code","b9f86630":"code","4b006333":"code","b9d72cf0":"markdown","2388987b":"markdown","9736affe":"markdown","5003f641":"markdown","f30608e1":"markdown","b2a33cb5":"markdown","7ed10dba":"markdown","c60a16cd":"markdown","b08a3fb4":"markdown","0fd1250d":"markdown","8e4f079e":"markdown","4cfb17d3":"markdown","f3f75a52":"markdown","22905712":"markdown","6710bdad":"markdown","732253d8":"markdown","c3a83b0f":"markdown","b445f829":"markdown","0bf9b6c4":"markdown","83a17edc":"markdown","dc359c6e":"markdown","6f1b04f9":"markdown","a984e60b":"markdown","80c4fac3":"markdown","c762ab0f":"markdown","393579fb":"markdown","54dd3dae":"markdown","ba3d2e61":"markdown","a1f31d06":"markdown","b37f1263":"markdown","64c2a24b":"markdown","37b93edd":"markdown","b9239f50":"markdown","0df84c52":"markdown","c70975d4":"markdown","f03e0868":"markdown","12180bbb":"markdown","ca64b38e":"markdown","94d03cce":"markdown","7fdadab6":"markdown","4fe6837c":"markdown","45fe0395":"markdown","c9fdc23b":"markdown","ac65d954":"markdown"},"source":{"9b7555ad":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, confusion_matrix\nfrom sklearn.calibration import CalibratedClassifierCV\n\n\nfrom vecstack import stacking\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","397d94e9":"# uploading data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","57a16716":"train.head()","d9e348e2":"train.info()","b3f4a75e":"train.describe()","41485ec0":"test.head()","310627b1":"test.info()","ca04c524":"test.describe()","ff931b5a":"def merge_data(train, test):\n    return pd.concat([train, test], sort = True).reset_index(drop=True)\ndef divide_data(data):\n    return data.iloc[:891], data.iloc[891:].drop(['Survived'], axis = 1)\n\ndata = merge_data(train, test)","717a1fb2":"def countplot(y, hue, title):\n    plt.figure(figsize = (12, 6))\n    plt.title(title)\n    sns.set()\n    sns.countplot(y = y, hue = hue)\n    plt.grid()\ncountplot(train['Pclass'], train['Survived'], \n          'The part of passengers who survived, depending on the Pclass')\n","12f0d331":"countplot(train['Embarked'], train['Survived'], \n          'The part of passengers who survived, depending on the Embarked')","06ae4b4e":"countplot(train['Sex'], train['Survived'], 'The part of passengers who survived, depending on the Sex')","3868b782":"countplot(train['Parch'], train['Survived'], 'The part of passengers who survived, depending on the Parch')","ad5cf7f4":"countplot(train['SibSp'], train['Survived'], 'The part of passengers who survived, depending on the SibSp')","a400ed5b":"def kdeplot(feature, xlabel, title):\n    plt.figure(figsize = (12, 8))\n    ax = sns.kdeplot(train[feature][(train['Survived'] == 0) & \n                             (train[feature].notnull())], color = 'lightcoral', shade = True)\n    ax = sns.kdeplot(train[feature][(train['Survived'] == 1) & \n                             (train[feature].notnull())], color = 'darkturquoise', shade= True)\n    plt.xlabel(xlabel)\n    plt.ylabel('frequency')\n    plt.title(title)\n    ax.legend(['not survived','survived'])\n    \nkdeplot('Age', 'age', 'The distribution of the surviving passengers depending on the Age')","629d5e0a":"def boxplot(x, y, title):\n    plt.figure(figsize = (12, 8))\n    sns.boxplot(x = x, y = y)\n    plt.title(title)\nboxplot(train['Survived'], train['Age'], 'The boxplot for Age')","30636d0e":"kdeplot('Fare', 'fare', 'The distribution of the surviving passengers depending on the Fare')","e24169e9":"boxplot(train['Survived'], train['Fare'], 'The boxplot for Fare')","0769f083":"data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndata['Title'].unique()","a394c739":"# Exchange many titles with a more common name or classify them as Rare.\ndata.groupby('Title')['Sex'].count()\n\ndata['Title'] = data['Title'].replace(['Capt', 'Col', 'Countess', 'Don', 'Dr', \n                                      'Jonkheer', 'Major', 'Sir', 'Rev', 'Dona'], 'Rare')\ndata['Title'] = data['Title'].replace(['Lady', 'Mlle', 'Mme', 'Ms'], \n                                      ['Mrs', 'Miss', 'Miss', 'Mrs'])","fe96ddbe":"# create a new features - family survival!\ndef family_survival():\n\n    data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n    default_survival_rate = 0.5\n    data['Family_survival'] = default_survival_rate\n\n    for grp, grp_df in data[['Survived', 'Name', 'Last_Name', \n                                 'Fare', 'Ticket', 'PassengerId',\n                                 'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                ID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == ID, \n                                 'Family_survival'] = 1\n                elif (smin == 0.0):\n                    data.loc[data['PassengerId'] == ID, \n                                 'Family_survival'] = 0\n\n    for _, grp_df in data.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_survival'] == 0) | (\n                        row['Family_survival'] == 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    ID = row['PassengerId']\n                    if (smax == 1.0):\n                        data.loc[data['PassengerId'] == ID, \n                                     'Family_survival'] = 1\n                    elif (smin == 0.0):\n                        data.loc[data['PassengerId'] == ID, \n                                     'Family_survival'] = 0\n\n    return data","99629c15":"data = family_survival()","641a7c97":"# Creating a new colomn - Name_length\n#data['Name_length'] = data['Name'].apply(len)\n\n\n#def name_length_category(length):\n#    if length <= 20:\n#        return 0\n#    if 20 < length <= 35:\n#        return 1\n#    if 35 < length <= 45:\n#        return 2\n#    else:\n#        return 3\n\n#data['Name_length'] = data['Name_length'].apply(name_length_category)","c37f5c38":"data['Age'] = data.groupby(['Title', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","4d1c4be4":"# create age groups\n\ndef age_category(age):\n    if age <=2:\n        return 0\n    if 2 < age <= 18:\n        return 1\n    if 18 < age <= 35:\n        return 2\n    if 35 < age <= 65:\n        return 3\n    else:\n        return 4\ndata['Age'] = data['Age'].apply(age_category)","efc6d8a6":"data['Age*Pclass'] = data['Age']*data['Pclass']","8916c1c8":"data[data['Fare'].isnull()]","9eb21525":"# replace the fare value\ndata.loc[data['Fare'].isnull(), 'Fare'] = data.loc[(data['Embarked'] == 'S') \n                                                   & (data['Pclass'] == 3) & (data['SibSp'] == 0)]['Fare'].median()","c13648ae":"data['Fare'].value_counts()","d15ade7e":"# create fare groups\ndef fare_category(fare):\n    if fare <= 7.91:\n        return 0\n    if 7.91 < fare <= 14.454:\n        return 1\n    if 14.454 < fare <= 31:\n        return 2\n    if 31 < fare <= 99:\n        return 3\n    if 99 < fare <= 250:\n        return 4\n    else:\n        return 5\ndata['Fare'] = data['Fare'].apply(fare_category)","1f45a877":"data[data['Embarked'].isnull()]","f89ed5c6":"data.loc[(data['Fare'] < 80) & (data['Pclass'] == 1)]['Embarked'].value_counts()\n# => both embarked in Southhampton\ndata.loc[data['Embarked'].isnull(), 'Embarked'] = 'S'","d97fc4c0":"#print(data['Cabin'].unique())\n#print('Count unique values:', data['Cabin'].nunique())\n# keep all first letters of cabin and use 'N' for each missing values\n#data['Cabin'] = data['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'N')","677056f6":"#data.groupby('Cabin')['Survived'].mean().sort_values(ascending = False)#\n# group together the values\n#data.loc[data['Cabin'] == 'T', 'Cabin'] = 'A'\n#data['Cabin'] = data['Cabin'].replace(['A', 'B', 'C'], 'ABC')\n#data['Cabin'] = data['Cabin'].replace(['D', 'E'], 'DE')\n#data['Cabin'] = data['Cabin'].replace(['F', 'G'], 'FG')\n#data['Cabin'].value_counts()","0acfd289":"#data['Ticket'].value_counts()\n# let's try creating a string length as feature\n#data['Ticket_length'] = data['Ticket'].apply(len)\n# create ticket category\ndef ticket_category(Ticket_length):\n    if Ticket_length <= 6:\n        return 0\n    if 6 < Ticket_length <= 10:\n        return 1\n    else:\n        return 2\n#data['Ticket_length'] = data['Ticket_length'].apply(ticket_category)","0adba285":"# creating a feature Family_size\ndata['Family_size'] = data['Parch'] + data['SibSp'] + 1","c9550bda":"data[['Family_size', 'Survived']].groupby('Family_size').mean()","1f1e28ee":"# create a family_size category\ndata['Single'] = data['Family_size'].map(lambda x: 1 if x == 1 else 0)\n#data['Small_family'] = data['Family_size'].map(lambda x: 1 if 2 <= x <= 4 else 0)\n#data['Medium_family'] = data['Family_size'].map(lambda x: 1 if 5 <= x <= 7 else 0)\n#data['Large_family'] = data['Family_size'].map(lambda x: 1 if x > 7 else 0)","bd42e233":"data['Is_Married'] = 0\ndata['Is_Married'] = data['Title'].map(lambda x: 1 if x == 'Mrs' else 0)","d47f0ff8":"data = data.drop(['Name', 'PassengerId', 'Ticket', 'Cabin', 'Last_Name'], axis = 1)","d07e65a7":"# label encoder\ndef object_to_int(df):\n    if df.dtype=='object':\n        df = LabelEncoder().fit_transform(df)\n    return df\ndata = data.apply(lambda x: object_to_int(x))","9a1b6029":"# correlation matrix\nplt.figure(figsize = (12, 8))\nsns.heatmap(data.corr(), annot = True)\nplt.title('Correlation matrix')","cea2bff8":"# one-hot encoder\n#ohe_columns = data.drop(['Age'], axis = 1)\ndata= pd.get_dummies(data, columns = ['Age', 'Title', 'Embarked'], drop_first = True)","5d42f798":"# add polynomial features\ndef add_polynomial_features(frame, poly_degree=2, interaction=False):\n    poly = PolynomialFeatures(degree = poly_degree, interaction_only = interaction, include_bias = False)\n    poly_features = poly.fit_transform(frame[['Age', 'Name_length', 'Fare']])\n    df_poly = pd.DataFrame(poly_features, columns = poly.get_feature_names())\n    return pd.concat([frame, df_poly.drop(['x0'], axis=1)], axis=1)\n#data = add_polynomial_features(data, 3, False)","50cb7251":"train, test = divide_data(data)","126b836b":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, random_state = 12345)","18a92e64":"# OverSampling\n#sm = SMOTE()\n#X_train, y_train = sm.fit_sample(X_train, y_train.ravel())","66a0d942":"#numeric = ['Age', 'Name_length', 'Fare', 'Age*Pclass']\n#for i in X_train.columns:\n#    if X_train[i].dtype =='float64' or X_train[i].dtype =='float32':\n#        numeric += [i]\n#scaler = StandardScaler()\n#scaler.fit(X_train[numeric])\n#X_train[numeric] = scaler.transform(X_train[numeric])\n#X_valid[numeric] = scaler.transform(X_valid[numeric])\n#test[numeric] = scaler.transform(test[numeric])","b27210d1":"# create functions for confusion matrix and feature importance\ndef confusion_m(model, title):\n    cm = confusion_matrix(y_valid, model.predict(X_valid))\n    f, ax = plt.subplots(figsize = (8, 6))\n    sns.heatmap(cm, annot = True, linewidths = 0.5, color = 'red', fmt = '.0f', ax = ax)\n    plt.xlabel('y_predicted')\n    plt.ylabel('y_true')\n    plt.title(title)\n    plt.show()\n\ndef feature_importance(model, title):\n    dataframe = pd.DataFrame(model, X_train.columns).reset_index()\n    dataframe = dataframe.rename(columns = {'index':'features', 0:'coefficients'})\n    dataframe = dataframe.sort_values(by = 'coefficients', ascending = False)\n    plt.figure(figsize=(13,10), dpi= 60)\n    ax = sns.barplot(x = 'coefficients', y = 'features', data = dataframe ,palette = 'husl')\n    plt.title(title, fontsize = 20)","687c9e51":"lr = LogisticRegression(random_state = 12345)\nparameters_lr = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 3, 5, 7, 10, 15, 20, 25, 30, 50], \n                 'penalty':['l1', 'l2', 'elasticnet', 'none'],\n                 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                 'class_weight': [1, 3, 10],\n                 'max_iter': [200, 500, 800, 1000, 2000]}\nsearch_lr = RandomizedSearchCV(lr, parameters_lr, cv=5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_lr.fit(X_train, y_train)\nbest_lr = search_lr.best_estimator_\npredict_lr = best_lr.predict(X_valid)\nauc_lr = cross_val_score(best_lr, X_valid, y_valid, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_lr = cross_val_score(best_lr, X_valid, y_valid, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for Logistic Regression on validation dataset:', sum(auc_lr)\/len(auc_lr))\nprint('Accuracy for Logistic Regression on validation dataset:', sum(acc_lr)\/len(acc_lr))","5fb89a05":"confusion_m(best_lr, 'Confusion matrix for Logistic Regression')\nfeature_importance(best_lr.coef_[0], 'Feature importance for Logistic Regression')","37933454":"dt = DecisionTreeClassifier(random_state = 12345)\nparameters_dt = {'criterion': ['gini', 'entropy'], \n                 'max_depth':range(1, 100, 1), \n                 'min_samples_leaf': range(1, 20), \n                 'max_features':range(1, X_train.shape[1]+1)}\nsearch_dt = RandomizedSearchCV(dt, parameters_dt, cv=5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_dt.fit(X_train, y_train)\nbest_dt = search_dt.best_estimator_\nacc_dt = cross_val_score(best_dt, X_valid, y_valid, scoring = 'accuracy', cv = 10, n_jobs = -1)   \nauc_dt = cross_val_score(best_dt, X_valid, y_valid, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nprint('AUC-ROC for Decision Tree on validation dataset:', sum(auc_dt)\/len(auc_dt))\nprint('Accuracy for Decision Tree on validation dataset:', sum(acc_dt)\/len(acc_dt))","3cfbff69":"confusion_m(best_dt, 'Confusion matrix for Logistic Regression')\nfeature_importance(best_dt.feature_importances_, 'Feature importance for Decision Tree')","72be75b3":"rf = RandomForestClassifier(random_state = 12345)\nparameters_rf = {'n_estimators': range(1, 1800, 25), \n                 'criterion': ['gini', 'entropy'], \n                 'max_depth':range(1, 100), \n                 'min_samples_split': range(1, 12), \n                 'min_samples_leaf': range(1, 12), \n                 'max_features':['auto', 'log2', 'sqrt', 'None']}\nsearch_rf = RandomizedSearchCV(rf, parameters_rf, cv=5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\n\nsearch_rf.fit(X_train, y_train)\nbest_rf = search_rf.best_estimator_\npredict_rf = best_rf.predict(X_valid)\nauc_rf = cross_val_score(best_rf, X_valid, y_valid, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_rf = cross_val_score(best_rf, X_valid, y_valid, scoring = 'accuracy', cv = 10, n_jobs = -1) \nprint('AUC-ROC for Random Forest on validation dataset:', sum(auc_rf)\/len(auc_rf))\nprint('Accuracy for Random Forest on validation dataset:', sum(acc_rf)\/len(acc_rf))","bae3f83c":"confusion_m(best_rf, 'Confusion matrix for Random Forest')\nfeature_importance(best_rf.feature_importances_, 'Feature importance for Random Forest')","9a2365cb":"xgb = XGBClassifier(random_state = 12345, eval_metric='auc')\nparameters_xgb = {'eta': [0.01, 0.05, 0.1, 0.001, 0.005, 0.04, 0.2, 0.0001],  \n                  'min_child_weight':range(1, 5), \n                  'max_depth':range(1, 6), \n                  'learning_rate': [0.01, 0.05, 0.1, 0.001, 0.005, 0.04, 0.2], \n                  'n_estimators':range(0, 2001, 50)}\nsearch_xgb = RandomizedSearchCV(xgb, parameters_xgb, cv = 5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_xgb.fit(X_train, y_train)\nbest_xgb = search_xgb.best_estimator_\npredict_xgb = best_xgb.predict(X_valid)\nauc_xgb = cross_val_score(best_xgb, X_valid, y_valid, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_xgb = cross_val_score(best_xgb, X_valid, y_valid, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for XGBoost on validation dataset:', sum(auc_xgb)\/len(auc_xgb))\nprint('Accuracy for XGBoost on validation dataset:', sum(acc_xgb)\/len(acc_xgb))","0364b875":"confusion_m(best_xgb, 'Confusion matrix for XGBoost')\nfeature_importance(best_xgb.feature_importances_, 'Feature importance for XGBoost')","99e9ab6d":"cb = CatBoostClassifier(random_state = 12345, iterations = 300, eval_metric='Accuracy', verbose = 100)\nparameters_cb = {'depth': range(6, 11),\n                 'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.0001]}\nsearch_cb = RandomizedSearchCV(cb, parameters_cb, cv = 5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_cb.fit(X_train, y_train, verbose = 100)\nbest_cb = search_cb.best_estimator_\npredict_cb = best_cb.predict(X_valid)\nauc_cb = cross_val_score(best_cb, X_valid, y_valid, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_cb = cross_val_score(best_cb, X_valid, y_valid, scoring = 'accuracy', cv = 10, n_jobs = -1)\nprint('AUC-ROC for CatBoost on validation dataset:', sum(auc_cb)\/len(auc_cb))\nprint('Accuracy for CatBoost on validation dataset:', sum(acc_cb)\/len(acc_cb))","3702bfa3":"confusion_m(best_cb, 'Confusion matrix for CatBoost')\nfeature_importance(best_cb.feature_importances_, 'Feature importance for CatBoost')","09509b2e":"et = ExtraTreesClassifier(random_state = 12345)\nparameters_et = {'n_estimators': range(50, 501, 25), \n                 'criterion': ['gini', 'entropy'], \n                 'max_depth':range(1, 100), \n                 'min_samples_split': range(1, 12), \n                 'min_samples_leaf': range(1, 12), \n                 'max_features':['auto', 'log2', 'sqrt', 'None']}\nsearch_et = RandomizedSearchCV(et, parameters_et, cv=5, scoring = 'accuracy', n_jobs = -1, random_state = 12345)\nsearch_et.fit(X_train, y_train)\nbest_et = search_et.best_estimator_\npredict_et = best_et.predict(X_valid)\nauc_et = cross_val_score(best_et, X_valid, y_valid, scoring = 'roc_auc', cv = 10, n_jobs = -1)\nacc_et = cross_val_score(best_et, X_valid, y_valid, scoring = 'accuracy', cv = 10, n_jobs = -1) \nprint('AUC-ROC for Extra Trees on validation dataset:', sum(auc_et)\/len(auc_et))\nprint('Accuracy for Extra Trees on validation dataset:', sum(acc_et)\/len(acc_et))","99034e62":"confusion_m(best_et, 'Confusion matrix for ExtraTreesClassifier')\nfeature_importance(best_et.feature_importances_, 'Feature importance for ExtraTreesClassifier')","b5526564":"vc = VotingClassifier(estimators=[('lr', best_lr), ('xgb', best_xgb), ('rf', best_rf), ('et', best_et), ('cb', best_cb)], voting='soft')\nvc.fit(X_train, y_train)\npredict_vc = vc.predict(X_valid)\nauc_vc = cross_val_score(vc, X_valid, y_valid, scoring = 'roc_auc', cv = 5, n_jobs = -1)\nacc_vc = cross_val_score(vc, X_valid, y_valid, scoring = 'accuracy', cv = 5, n_jobs = -1)\nprint('AUC-ROC for ensemble models on validation dataset:', sum(auc_vc)\/len(auc_vc))\nprint('Accuracy for ensemble models on validation dataset:', sum(acc_vc)\/len(acc_vc))","5404b469":"confusion_m(vc, 'Confusion matrix for VotingClassifier')","ed2b11c9":"models = ['logistic_regression', 'decision_tree', 'random_forest',\n          'xgboost', 'catboost', \n          'extra_trees', 'voting']\ndict_values = {'accuracy': [acc_lr.mean(), acc_dt.mean(), acc_rf.mean(),\n                            acc_xgb.mean(), acc_cb.mean(), \n                            acc_et.mean(), acc_vc.mean()],\n               'auc_roc': [auc_lr.mean(), auc_dt.mean(), auc_rf.mean(),\n                           auc_xgb.mean(), auc_cb.mean(), \n                           auc_et.mean(), auc_vc.mean()]}\ndf_score = pd.DataFrame(dict_values, index = models, columns = ['accuracy', 'auc_roc'])\ndf_score","08ff29f7":"# stacking\n#models = [best_rf, best_cb, best_et]\n#\n#S_train, S_test = stacking(models, X_train, y_train, test, regression=False, mode='oof_pred_bag', \n#                           needs_proba=False, save_dir=None, metric=accuracy_score, n_folds=4, \n#                           stratified=True, shuffle=True, random_state=12345, verbose=2)","4a65fc4f":"#final_model = best_rf\n#final_model.fit(S_train, y_train)","c29528ff":"# Out-of-fold predictions\nX_train = X_train.values\ntest = test.values\n\nntrain = X_train.shape[0]\nntest = test.shape[0]\nn_folds = 5 \n\nkf = KFold(n_splits = n_folds, random_state = 12345)\n\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((n_folds, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train.iloc[train_index]\n        x_te = x_train[test_index]\n\n        clf.fit(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","880c498a":"et_oof_train, et_oof_test = get_oof(best_et, X_train, y_train, test) \nrf_oof_train, rf_oof_test = get_oof(best_rf,X_train, y_train, test) \ncb_oof_train, cb_oof_test = get_oof(best_cb, X_train, y_train, test)\nxgb_oof_train, xgb_oof_test = get_oof(best_xgb,X_train, y_train, test) \nlr_oof_train, lr_oof_test = get_oof(best_lr,X_train, y_train, test) ","7bb6c5d9":"X_train = np.concatenate((et_oof_train, rf_oof_train, cb_oof_train, xgb_oof_train, lr_oof_train), axis=1)\nX_test = np.concatenate((et_oof_test, rf_oof_test, cb_oof_test, xgb_oof_test, lr_oof_test), axis=1)","9b9cfbc3":"best_model = XGBClassifier(n_estimators= 2000, max_depth= 4, min_child_weight= 2, gamma=0.9, \n                    subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', \n                    nthread= -1,scale_pos_weight=1).fit(X_train, y_train)","b9f86630":"# predict the values for test dataset\ny_test = best_model.predict(X_test)","4b006333":"# create and save predict dataframe\nsubmission = pd.DataFrame({'PassengerId': list(range(892, 1310)), 'Survived': y_test})\nsubmission['Survived'] = submission['Survived'].astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(submission)","b9d72cf0":"### 2.7 Fare","2388987b":"## 5. Create and fit models","9736affe":"#### Result:\n* missing values in the columns of age, cabin and fare","5003f641":"### 3.6 Ticket","f30608e1":"### 2.4 Parch","b2a33cb5":"### 2.2 Embarked","7ed10dba":"### 5.3 Random Forest","c60a16cd":"### 1.1 Analysis of train dataset","b08a3fb4":"### 5.9 Ensemble models","0fd1250d":"#### Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.","8e4f079e":"## 1. Data analysis","4cfb17d3":"## 4. Preprocessing data","f3f75a52":"### 4.2 Label, One-Hot encoding and add polynomial features","22905712":"### 3.5 Cabin","6710bdad":"### 3.1 Name","732253d8":"#### The chance of survival for women is high as compared to men.","c3a83b0f":"### 2.6 Age","b445f829":"### The chances of survival for S are greater than for other cities.","0bf9b6c4":"### 5.8 Table with the values of the accuracy and auc-roc","83a17edc":"### 3.7 Parch and SibSp","dc359c6e":"### 2.5 SibSP","6f1b04f9":"### 3.8 Married or not","a984e60b":"### 4.1 Remove unnecessary columns","80c4fac3":"## 6. Submit task","c762ab0f":"### 3.4 Embarked","393579fb":"### 2.3 Sex","54dd3dae":"### Analysis of test dataset","ba3d2e61":"### There is a visible trend that being a 1st class passenger gives you better chances of survival. The survival rate for 3st class is very low. ","a1f31d06":"### 4.4 OverSampling and StandartScaler","b37f1263":"#### We can see that there\u2019s a survival penalty to singletons and those with family sizes above 4.","64c2a24b":"### 4.3 Divide data into a train, validation and test data","37b93edd":"## **2. Features analysis**\n\n### 2.1 Pclass","b9239f50":"### 5.2 Decision Tree","0df84c52":"#### Result: \n* missing values in columns of age and cabin","c70975d4":"### 3.3 Fare","f03e0868":"### 5.5 CatBoost","12180bbb":"###  It looks like an outlier with a fare of 512 dollars. However, we will keep it for now.","ca64b38e":"## 3. Data cleaning and feature engineering","94d03cce":"### 5.1 Logistic Regression","7fdadab6":"### 5.6 ExtraTreesClassifier","4fe6837c":"### 5.4 XGBoost","45fe0395":"### 5.7 VotingClassifier","c9fdc23b":"### 3.2 Age","ac65d954":"#### The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive.\n"}}