{"cell_type":{"f5581077":"code","737b6b92":"code","393430de":"code","f1fc5ece":"code","d83d91de":"code","a961053b":"code","c0930b27":"code","423ef646":"code","149dad01":"code","7e4f798e":"code","471e94f8":"code","56b5c7dd":"code","e432e10e":"code","80d4c5c4":"code","595a3240":"code","693dbc91":"code","6fb1be4e":"code","9ca54ff4":"markdown","256e9b0c":"markdown","9c8b0171":"markdown","0ffb98d0":"markdown"},"source":{"f5581077":"import pandas as pd\ndf = pd.read_csv('..\/input\/SolarEnergy\/SolarPrediction.csv')\nprint(df.shape)\ndf.head()","737b6b92":"df.isnull().sum()","393430de":"print(df.index)\n\ndf = df.sort_values(['UNIXTime'], ascending = [True])\ndf.index =  pd.to_datetime(df['UNIXTime'], unit='s')\n\nprint(df.index)\n\ndf.head()","f1fc5ece":"import pytz\n\nHST = pytz.timezone('Pacific\/Honolulu')\n                                                                                ## unixtime is in utc\ndf.index = df.index.tz_localize(pytz.utc)                                       ## we can see that by comparing with \"Time\",\ndf.index = df.index.tz_convert(HST)                                             ## converting to hawaiian local time\ndf.head()","d83d91de":"df['DayOfYear'] = df.index.strftime('%j').astype(int)\ndf.head()","a961053b":"df['TimeOfDay(s)'] = df.index.hour*60*60 + df.index.minute*60 + df.index.second\ndf.head()","c0930b27":"df.drop(['TimeSunRise','TimeSunSet', 'Data', 'Time', 'WindDirection(Degrees)'], inplace=True, axis=1)\n\ndf.head()","423ef646":"print(df.shape)","149dad01":"dataset = df.values\n\nX = dataset[:,2:8]\nY = dataset[:,1]\nY = Y.reshape(-1,1)\n\n\nprint(X.shape)\nprint(Y.shape)\nprint(type(X))","7e4f798e":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n\nscaler_X = preprocessing.StandardScaler().fit(X)\nscaler_Y = preprocessing.StandardScaler().fit(Y)\nX_scale = scaler_X.transform(X)\nX_train, X_val_and_test, Y_train_unscaled, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\nX_val, X_test, Y_val_unscaled, Y_test_unscaled = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\nY_train = scaler_Y.transform(Y_train_unscaled)\nY_val =  scaler_Y.transform(Y_val_unscaled)\nY_test =  scaler_Y.transform(Y_test_unscaled)\nprint(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","471e94f8":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import regularizers\n\n\n## kernel_initializer='normal'\n\n\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu',  kernel_initializer='normal', input_shape=(6,), kernel_regularizer=regularizers.l2(l=0.01)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='linear'))","56b5c7dd":"from keras.optimizers import Adam\n\n#optimizer = Adam(lr=1e-3, decay=1e-3 \/ 200)\n\nmodel.compile(loss='mse',\n              optimizer='adam',\n              metrics=['mae'])\nhist = model.fit(X_train, Y_train,\n                batch_size=32, epochs=300,\n                validation_data=(X_val, Y_val))","e432e10e":"from matplotlib import pyplot\n\npyplot.title('Loss \/ Mean Squared Error')\npyplot.plot(hist.history['loss'], label='train')\npyplot.plot(hist.history['val_loss'], label='validate')\npyplot.legend()\npyplot.show()","80d4c5c4":"model.evaluate(X_test, Y_test)","595a3240":"Y_result_scaled= model.predict(X_test)\nY_result = scaler_Y.inverse_transform(Y_result_scaled)\nprint(Y_result)\nprint(Y_test_unscaled.reshape(Y_result.shape))","693dbc91":"import numpy as np\n\naxis_x = [i for i in range(50)]\n\n\npyplot.plot(axis_x, Y_result[:50], label='predict')\npyplot.plot(axis_x, Y_test_unscaled[:50], label='actual')\npyplot.legend()\npyplot.show()","6fb1be4e":"from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n\n\nexplained_variance_score = explained_variance_score(Y_test_unscaled.reshape(Y_result.shape), Y_result)\nmean_squared_error = mean_squared_error(Y_test_unscaled.reshape(Y_result.shape), Y_result)\nr_squared = r2_score(Y_test_unscaled.reshape(Y_result.shape), Y_result)\nprint('explained variance = {}'.format(explained_variance_score))\nprint('mse = {}'.format(mean_squared_error))\nprint('r2 = {}'.format(r_squared))","9ca54ff4":"'TimeOfDay(s)' and 'DayOfYear' extracts all the important features related to 'TimeSunRise', 'TimeSunSet', 'Data', 'Time', so we do not need them anymore. I tried to train the model with and without 'WindDirection(Degrees)', always gave a slightly better result without this feature, probably wind direction has little to no influence on the radiation, just my guess. Tried deleting other features too, only made the model worse, so keeping them in.","256e9b0c":"Still a moderately shallow network. Added some Dropout and L2 regularizers, without which the model was getting overfit to the training data. I am a very beginner in neural networks, this whole network is actually built from intuitions, any suggestions would be very great.","9c8b0171":"I am very new to this whole data world, this is actually one of my first works here in Kaggle, please feel free to put in any kind of suggestions. I have seen some works here with [Solar Radiation Prediction](http:\/\/www.kaggle.com\/dronio\/SolarEnergy) dataset, and most of them are done using random forest classifier, so I tried to build a somewhat shallow neural network to see if it can yield a satisfactory result. Thanks to the author of the first notebook here in the thread currently by upvotes, it came of massive help to understand the behavior of the dataset, [HI-SEAS Solar Irradiance Prediction](http:\/\/www.kaggle.com\/callumdownie\/hi-seas-solar-irradiance-prediction).","0ffb98d0":"This is probably not a very bad model afterall for a small dataset!!"}}