{"cell_type":{"4323ac86":"code","5d98fcbf":"code","ed8e51ab":"code","2d401e40":"code","97ed3803":"code","76eedf63":"code","2b230d77":"code","09246b75":"code","1ece1353":"code","0888ccfb":"code","3a6890dd":"code","854170f0":"code","2b2f04b2":"code","0210f4c5":"code","578ca2d0":"code","0c9a9235":"code","fdab1609":"code","6b5342bc":"code","a8c56e2a":"code","234fa3be":"code","073326ac":"markdown"},"source":{"4323ac86":"from tensorflow.keras.layers import (BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional, GRU)\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.utils import Sequence, to_categorical\nfrom tensorflow.keras import losses, models, optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses\nimport tensorflow as tf\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\nfrom scipy.signal import butter, lfilter, filtfilt, savgol_filter, detrend\nfrom sklearn.model_selection import KFold, train_test_split\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\nfrom sklearn.linear_model import LinearRegression\nfrom tqdm import tqdm_notebook as tqdm\nfrom contextlib import contextmanager\nfrom joblib import Parallel, delayed\nfrom IPython.display import display\nfrom sklearn import preprocessing\nimport scipy.stats as stats\nimport random as rn\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport itertools\nimport warnings\nimport time\nimport pywt\nimport os\nimport gc\n\n\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n%matplotlib inline","5d98fcbf":"EPOCHS=30\nNNBATCHSIZE=2000\nBATCHSIZE = 50000\nSEED = 529\nSELECT = True\nSPLITS = 5\nLR = 0.01\nfe_config = [\n    (True, True, 50000, None),\n    (False, False, 5000, None),\n]","ed8e51ab":"\ndef init_logger():\n    handler = StreamHandler()\n    handler.setLevel(INFO)\n    handler.setFormatter(Formatter(LOGFORMAT))\n    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n    fh_handler.setFormatter(Formatter(LOGFORMAT))\n    logger.setLevel(INFO)\n    logger.addHandler(handler)\n    logger.addHandler(fh_handler)\n    ","2d401e40":"\n@contextmanager\ndef timer(name : Text):\n    t0 = time.time()\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n\nCOMPETITION = 'ION-Switching'\nlogger = getLogger(COMPETITION)\nLOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\nMODELNAME = 'Baseline'\n","97ed3803":"\ndef seed_everything(seed : int) -> NoReturn :\n    \n    rn.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)\n","76eedf63":"\ndef read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \n    train = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int8})\n    test  = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv(os.path.join(base + '\/sample_submission.csv'), dtype={'time': np.float32})\n    \n    return train, test, sub\n\n\n","2b230d77":"\ndef batching(df : pd.DataFrame,\n             batch_size : int,\n             add_index : Optional[bool]=True) -> pd.DataFrame :\n    \n    df['batch_'+ str(batch_size)] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values + 1\n    df['batch_'+ str(batch_size)] = df['batch_'+ str(batch_size)].astype(np.uint16)\n    if add_index:\n        df['batch_' + str(batch_size) +'_idx'] = df.index  - (df['batch_'+ str(batch_size)] * batch_size)\n        df['batch_' + str(batch_size) +'_idx'] = df['batch_' + str(batch_size) +'_idx'].astype(np.uint16)\n        \n    return df\n","09246b75":"def normalize(X_train, X_valid, X_test, normalize_opt, excluded_feat):\n    feats = [f for f in X_train.columns if f not in excluded_feat]\n    if normalize_opt != None:\n        if normalize_opt == 'min_max':\n            scaler = preprocessing.MinMaxScaler()\n        elif normalize_opt == 'robust':\n            scaler = preprocessing.RobustScaler()\n        elif normalize_opt == 'standard':\n            scaler = preprocessing.StandardScaler()\n        elif normalize_opt == 'max_abs':\n            scaler = preprocessing.MaxAbsScaler()\n        scaler = scaler.fit(X_train[feats])\n        X_train[feats] = scaler.transform(X_train[feats])\n        X_valid[feats] = scaler.transform(X_valid[feats])\n        X_test[feats] = scaler.transform(X_test[feats])\n    return X_train, X_valid, X_test","1ece1353":"\ndef reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min\n                        and c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min\n                      and c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    reduction = (start_mem - end_mem) \/ start_mem\n\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n\n    return df\n","0888ccfb":"\ndef lag_with_pct_change(df : pd.DataFrame,\n                        batch_size : int,\n                        shift_sizes : Optional[List]=[1, 2, 3, 4],\n                        add_pct_change : Optional[bool]=False,\n                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n    \n    assert 'batch_' + str(batch_size) +'_idx' in df.columns\n    for shift_size in shift_sizes:    \n        df['signal_shift_pos_'+str(shift_size)] = df['signal'].shift(shift_size).fillna(0)\n        df['signal_shift_neg_'+str(shift_size)] = df['signal'].shift(-1*shift_size).fillna(0)\n        for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(shift_size))].index:\n            df['signal_shift_pos_'+str(shift_size)][i] = np.nan\n        for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(batch_size - shift_size, batch_size))].index:\n            df['signal_shift_neg_'+str(shift_size)][i] = np.nan\n    if add_pct_change:\n        df['pct_change'] = df['signal'].pct_change()\n        if add_pct_change_lag:\n            df['pct_change_shift_pos_'+str(shift_size)] = df['pct_change'].shift(shift_size).fillna(0)\n            df['pct_change_shift_neg_'+str(shift_size)] = df['pct_change'].shift(-1*shift_size).fillna(0)\n            for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(shift_size))].index:\n                df['pct_change_shift_pos_'+str(shift_size)][i] = np.nan\n            for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(batch_size - shift_size, batch_size))].index:\n                df['pct_change_shift_neg_'+str(shift_size)][i] = np.nan \n    return df\n","3a6890dd":"\ndef feature_enginering_by_batch(z : Union[pd.Series, np.array],\n                                batch_size : int,\n                                window_size : Optional[List]=None) -> pd.DataFrame:\n    \n    temp = pd.DataFrame(index=[0], dtype=np.float16)\n    \n    temp['mean'] = z.mean()\n    temp['max'] = z.max()\n    temp['min'] = z.min()\n    temp['std'] = z.std()  \n    temp['mean_abs_chg'] = np.mean(np.abs(np.diff(z)))\n    temp['abs_max'] = np.max(np.abs(z))\n    temp['abs_min'] = np.min(np.abs(z))\n    temp['range'] = temp['max'] - temp['min']\n    temp['max_to_min'] = temp['max'] \/ temp['min']\n    temp['abs_avg'] = (temp['abs_max'] + temp['abs_min']) \/ 2\n    \n    for i in range(2, 5):\n        temp[f'moment_{i}'] = stats.moment(z, i)\n\n    for i in [1, 2]:\n        temp[f'kstatvar_{i}'] = stats.kstatvar(z, i)\n               \n    return temp\n","854170f0":"\ndef parse_sample(sample : pd.DataFrame,\n                 batch_no : int,\n                 batch_size : int,\n                 window_size : List) -> pd.DataFrame:\n    \n    temp = feature_enginering_by_batch(sample['signal'].values, batch_size, window_size)\n    temp['batch_'+ str(batch_size)] = int(batch_no)\n    \n    return temp\n","2b2f04b2":"    \ndef sample_gen(df : pd.DataFrame,\n               batch_size : int,\n               window_size : List,\n               batches : List=[0], ) -> pd.DataFrame:\n    \n        result = Parallel(n_jobs=1, temp_folder='\/tmp', max_nbytes=None, backend='multiprocessing')(delayed(parse_sample)\n                                              (df[df['batch_'+ str(batch_size)]==i], int(i), batch_size, window_size)\n                                                                                              for i in tqdm(batches))\n        data = [r.values for r in result]\n        data = np.vstack(data)\n        cols = result[0].columns\n        cols = [name+'_'+str(batch_size) if name!='batch_'+ str(batch_size) else 'batch_'+ str(batch_size) for name in cols ]\n        X = pd.DataFrame(data, columns=cols)\n        X = reduce_mem_usage(X, False)\n        X = X.sort_values('batch_'+ str(batch_size))\n    \n        return X\n","0210f4c5":"\ndef run_feat_enginnering(df : pd.DataFrame,\n                         create_all_data_feats : bool,\n                         add_index : bool,\n                         batch_size : int,\n                         window_size : List) -> pd.DataFrame:\n    \n    df = batching(df, batch_size=batch_size, add_index=add_index)\n    if create_all_data_feats:\n        df = lag_with_pct_change(df, batch_size, [1, 2, 3, 4],  add_pct_change=True, add_pct_change_lag=True)\n    batches = df['batch_'+ str(batch_size)].unique().tolist()\n    batch_feats=sample_gen(df, batch_size=batch_size, window_size=window_size, batches=batches)\n    df = pd.merge(df, batch_feats, on='batch_'+ str(batch_size), how='left')\n    df = reduce_mem_usage(df, False)\n    \n    return df\n","578ca2d0":"def feature_selection(df : pd.DataFrame,\n                      df_test : pd.DataFrame,\n                      subtract_only : Optional[bool]=True,\n                      idx_cols : List=['time'],\n                      target_col : List=['open_channels']) -> Tuple[pd.DataFrame , pd.DataFrame]:\n    \n    drops = df.columns[df.isna().sum()>25000]\n    df = df.drop(drops, axis=1)\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    df_test = df_test.drop(drops, axis=1)\n    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n    df_test = df_test.fillna(0)\n    gc.collect()\n    df = reduce_mem_usage(df, False)\n    df_test = reduce_mem_usage(df_test, False)\n\n    gc.collect()\n    return df, df_test\n","0c9a9235":"\ndef run_cv_model_by_batch(train : pd.DataFrame,\n                          test : pd.DataFrame,\n                          splits : int,\n                          shuffle : bool,\n                          seed : int,\n                          batch_col : Text,\n                          feats : List,\n                          sample_submission: pd.DataFrame,\n                          nn_epochs : int,\n                          nn_batch_size : int) -> NoReturn:\n    \n    oof_ = np.zeros(len(train))\n    preds_ = np.zeros(len(test))\n    target = 'open_channels'\n    kf = KFold(splits, shuffle, seed)\n    for n_fold, (tr_idx, val_idx) in enumerate(kf.split(train, train[target], groups=train[batch_col])):\n        train_x, train_y = train[feats].iloc[tr_idx], train[target].iloc[tr_idx].values\n        valid_x, valid_y = train[feats].iloc[val_idx], train[target].iloc[val_idx].values\n        train_x,valid_x,test_scaled=normalize(train_x.copy(), valid_x.copy(), test[feats].copy(), 'min_max', [])\n        train_x=train_x.values.reshape(train_x.shape[0],1,train_x.shape[1])\n        valid_x=valid_x.values.reshape(valid_x.shape[0],1,valid_x.shape[1])\n        test_scaled=test_scaled.values.reshape(test_scaled.shape[0],1,test_scaled.shape[1])\n        gc.collect()\n        K.clear_session()\n        shape_ = train[feats].shape[1]\n        model = Regressor(shape_)\n        cb_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n        cb_Early_Stop=EarlyStopping(monitor='val_loss',patience=3)\n        model.fit(train_x,train_y,\n                  epochs=nn_epochs,\n                  callbacks=[cb_schedule, cb_Early_Stop, MacroF1(model, valid_x,valid_y)],\n                  batch_size=nn_batch_size,verbose=0,\n                  validation_data=(valid_x,valid_y))\n        preds_f = model.predict(valid_x).ravel()\n        oof_[val_idx] += preds_f\n        preds_ += model.predict(test_scaled).ravel() \/ SPLITS\n        f1_score_ = f1_score(valid_y,  np.round(np.clip(preds_f, 0, 10)).astype(int), average = 'macro')\n        rmse_score_ = np.sqrt(mean_squared_error(valid_y, preds_f))\n        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f} rmse score : {rmse_score_:1.5f}')\n    f1_score_ = f1_score(train[target], np.round(np.clip(oof_, 0, 10)).astype(int), average = 'macro')\n    rmse_score_ = np.sqrt(mean_squared_error(train[target], oof_))\n    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f} oof rmse score : {rmse_score_:1.5f}')\n    sample_submission['open_channels'] = np.round(np.clip(preds_, 0, 10)).astype(int)\n    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n    display(sample_submission.head())\n    np.save('oof.npy', oof_)\n    np.save('preds.npy', preds_)\n\n    return \n","fdab1609":"def Regressor(shape_):\n\n    inp = Input(shape=(1,shape_))\n    x = BatchNormalization()(inp)\n    x = Bidirectional(GRU(128,return_sequences=True))(x)\n    x = Bidirectional(GRU(128,return_sequences=False))(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)  \n    x = Dense(64,  activation='relu')(x)\n    x = Dense(64,  activation='relu')(x)   \n\n    out = Dense(1, name='out')(x)\n    \n    model = models.Model(inputs=inp, outputs=out)    \n    opt = Adam(lr=LR)\n    model.compile(loss=losses.mean_squared_error, optimizer=opt, metrics=['mse'])\n    return model\n","6b5342bc":"class MacroF1(Callback):\n    def __init__(self, model, X_val, y_val):\n        super().__init__()\n        self.model = model\n        self.X = X_val\n        self.y = y_val.reshape(-1)\n    def on_epoch_end(self, epoch, logs=None):\n        pred = np.round(np.clip((self.model.predict(self.X, batch_size=64).ravel()), 0, 10)).astype(int)\n        score = f1_score(self.y, pred, average='macro')       \n        print(f' F1Macro: {score:.5f}')","a8c56e2a":"\ndef run_everything(fe_config : List) -> NoReturn:\n    not_feats_cols = ['time']\n    target_col = ['open_channels']\n    init_logger()\n    with timer(f'Reading Data'):\n        logger.info('Reading Data Started ...')\n        base = os.path.abspath('\/kaggle\/input\/liverpool-ion-switching\/')\n        train, test, sample_submission = read_data(base)\n        logger.info('Reading Data Completed ...')\n        \n    with timer(f'Creating Features'):\n        logger.info('Feature Enginnering Started ...')\n        for config in fe_config:\n            train = run_feat_enginnering(train, create_all_data_feats=config[0], add_index=config[1], batch_size=config[2], window_size=config[3])\n            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], add_index=config[1], batch_size=config[2], window_size=config[3])\n            not_feats_cols.append('batch_'+str(config[2]))\n            if config[1]:\n                not_feats_cols.append('batch_'+str(config[2])+'_idx')\n        if SELECT:\n            train, test = feature_selection(train, test, subtract_only=True, idx_cols=not_feats_cols, target_col=target_col)\n        logger.info('Feature Enginnering Completed ...')\n\n    with timer(f'Running NN model'):\n        logger.info(f'Training NN model with {SPLITS} folds Started ...')\n        feats = [c for c in train.columns if c not in (not_feats_cols+target_col)]\n        run_cv_model_by_batch(train, test, splits=SPLITS, shuffle=True, seed=SEED, batch_col='batch_50000', feats=feats, sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n        logger.info(f'Training completed ...')\n","234fa3be":"run_everything(fe_config)","073326ac":"The validation scheme is based on [ion-switching-5kfold-lgbm-tracking](https:\/\/www.kaggle.com\/robikscube\/ion-switching-5kfold-lgbm-tracking), the selected features are based on [physically-possible](https:\/\/www.kaggle.com\/jazivxt\/physically-possible) and some parts are borrowed from [1-geomean-nn-and-6featlgbm-2-259-private-lb](https:\/\/www.kaggle.com\/dkaraflos\/1-geomean-nn-and-6featlgbm-2-259-private-lb) and cleaned data is from [data-without-drift](https:\/\/www.kaggle.com\/cdeotte\/data-without-drift)"}}