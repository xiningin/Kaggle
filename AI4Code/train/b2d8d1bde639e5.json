{"cell_type":{"405e1b66":"code","b57d3bc2":"code","7baf34ab":"code","6510ce11":"code","1333b516":"code","dc272e5c":"code","cbdead56":"code","3144085e":"code","ffc058dc":"code","818b5651":"code","939d6e93":"code","8a66d885":"code","1cc23b1b":"code","97452d74":"code","0a022c08":"code","2536ca67":"code","37966c0c":"code","08e69952":"code","2410faeb":"code","9df81a14":"code","8bc80b03":"code","d98aaa2e":"code","9cb302df":"code","73668a70":"code","5091ed53":"code","1513bf6c":"code","fb0f445e":"code","768b8d55":"code","47c9e244":"code","31f5ff3c":"code","bef3720e":"code","e436fe28":"code","a1e670ee":"code","5ce0a560":"code","40598566":"code","cf89bdd2":"markdown","1895afda":"markdown","9277e3c8":"markdown","253ed6ce":"markdown","38448ff1":"markdown","7a8b8ba6":"markdown","718ed05f":"markdown","186c6f6e":"markdown"},"source":{"405e1b66":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom random import seed # for setting seed\nimport tensorflow\nfrom IPython import sys_info\n\nimport gc # garbage collection","b57d3bc2":"MY_SEED = 42 # 480 could work too\nseed(MY_SEED)\nnp.random.seed(MY_SEED)\ntensorflow.set_random_seed(MY_SEED)\n\nprint(sys_info())\n# get module information\n!pip freeze > frozen-requirements.txt\n# append system information to file\nwith open(\"frozen-requirements.txt\", \"a\") as file:\n    file.write(sys_info())","7baf34ab":"from tensorflow.python.client import device_lib\n# print out the CPUs and GPUs\nprint(device_lib.list_local_devices())","6510ce11":"# https:\/\/stackoverflow.com\/questions\/25705773\/image-cropping-tool-python\n# because painting images are hella big\nfrom PIL import Image\nImage.MAX_IMAGE_PIXELS = None","1333b516":"# globals\n\nDATA_DIR = '..\/input\/painters-train-part-1\/'\n\nTRAIN_1_DIR = '..\/input\/painters-train-part-1\/train_1\/train_1\/'\nTRAIN_2_DIR = '..\/input\/painters-train-part-1\/train_2\/train_2\/'\nTRAIN_3_DIR = '..\/input\/painters-train-part-1\/train_3\/train_3\/'\n\nTRAIN_4_DIR = '..\/input\/painters-train-part-2\/train_4\/train_4\/'\nTRAIN_5_DIR = '..\/input\/painters-train-part-2\/train_5\/train_5\/'\nTRAIN_6_DIR = '..\/input\/painters-train-part-2\/train_6\/train_6\/'\n\nTRAIN_7_DIR = '..\/input\/painters-train-part-3\/train_7\/train_7\/'\nTRAIN_8_DIR = '..\/input\/painters-train-part-3\/train_8\/train_8\/'\nTRAIN_9_DIR = '..\/input\/painters-train-part-3\/train_9\/train_9\/'\n\nTRAIN_DIRS = [TRAIN_1_DIR, TRAIN_2_DIR, TRAIN_3_DIR,\n             TRAIN_4_DIR, TRAIN_5_DIR, TRAIN_6_DIR,\n             TRAIN_7_DIR, TRAIN_8_DIR, TRAIN_9_DIR]\n\nTEST_DIR = '..\/input\/painter-test\/test\/test\/'","dc272e5c":"df = pd.read_csv(DATA_DIR + 'all_data_info.csv')\nprint(\"df.shape\", df.shape)","cbdead56":"# quick fix for corrupted files\nlist_of_corrupted = ['3917.jpg','18649.jpg','20153.jpg','41945.jpg',\n'79499.jpg','91033.jpg','92899.jpg','95347.jpg',\n'100532.jpg','101947.jpg']\n# display the corrupted rows of dataset for context\ncorrupt_df = df[df[\"new_filename\"].isin(list_of_corrupted) == True]\nprint(corrupt_df.head(len(list_of_corrupted)))\n\n# completely get rid of them\ndf = df[df[\"new_filename\"].isin(list_of_corrupted) == False]\n\n# try to see if they are still there\nprint(df[df[\"new_filename\"].isin(list_of_corrupted) == True])\n\nprint(\"df.shape\", df.shape)","3144085e":"\ntrain_df = df[df[\"in_train\"] == True]\ntest_df = df[df['in_train'] == False]\ntrain_df = train_df[['artist', 'new_filename']]\ntest_df = test_df[['artist', 'new_filename']]\n\nprint(\"test_df.shape\", test_df.shape)\nprint(\"train_df.shape\", train_df.shape)\n\nartists = {} # holds artist hash & the count\nfor a in train_df['artist']:\n    if (a not in artists):\n        artists[a] = 1\n    else:\n        artists[a] += 1\n\ntraining_set_artists = []\nfor a,count in artists.items():\n    if(int(count) >= 300):\n        training_set_artists.append(a)\n\nprint(\"number of artsits\",len(training_set_artists))\n\nprint(\"\\nlist of artists...\\n\", training_set_artists)\n","ffc058dc":"t_df = train_df[train_df[\"artist\"].isin(training_set_artists)]\n\nt_df.head(5)","818b5651":"t1_df = t_df[t_df['new_filename'].str.startswith('1')]\n\nt2_df = t_df[t_df['new_filename'].str.startswith('2')]\n\nt3_df = t_df[t_df['new_filename'].str.startswith('3')]\n\nt4_df = t_df[t_df['new_filename'].str.startswith('4')]\n\nt5_df = t_df[t_df['new_filename'].str.startswith('5')]\n\nt6_df = t_df[t_df['new_filename'].str.startswith('6')]\n\nt7_df = t_df[t_df['new_filename'].str.startswith('7')]\n\nt8_df = t_df[t_df['new_filename'].str.startswith('8')]\n\nt9_df = t_df[t_df['new_filename'].str.startswith('9')]\n\nall_train_dfs = [t1_df, t2_df, t3_df,\n                t4_df, t5_df, t6_df,\n                t7_df, t8_df, t9_df]\n\nt9_df.head(5)","939d6e93":"from tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n\nfrom tensorflow.python.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","8a66d885":"len(training_set_artists)","1cc23b1b":"num_classes = len(training_set_artists) # one class per artist\nweights_notop_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = Sequential()\nmodel.add(ResNet50(\n  include_top=False,\n  weights=weights_notop_path,\n  pooling='avg'\n))\nmodel.add(Dense(\n  num_classes,\n  activation='softmax'\n))\n\nmodel.layers[0].trainable = False","97452d74":"model.compile(\n  optimizer='adam', # lots of people reccommend Adam optimizer\n  loss='categorical_crossentropy', # aka \"log loss\" -- the cost function to minimize \n  # so 'optimizer' algorithm will minimize 'loss' function\n  metrics=['accuracy'] # ask it to report % of correct predictions\n)","0a022c08":"# model globals\nIMAGE_SIZE = 224\nBATCH_SIZE = 96\nTEST_BATCH_SIZE = 17 # because test has 23817 images and factors of 23817 are 3*17*467\n                     # it is important that this number evenly divides the total num images \nVAL_SPLIT = 0.25","2536ca67":"def setup_generators(\n    val_split, train_dataframe, train_dir,\n    img_size, batch_size, my_seed, list_of_classes,\n    test_dataframe, test_dir, test_batch_size\n):\n    print(\"-\"*20)\n    if not preprocess_input:\n          raise Exception(\"please do import call 'from tensorflow.python.keras.applications.resnet50 import preprocess_input'\")\n\n    # setup resnet50 preprocessing \n    data_gen = ImageDataGenerator(\n        preprocessing_function=preprocess_input,\n        validation_split=val_split)\n\n    print(len(train_dataframe), \"images in\", train_dir, \"and validation_split =\", val_split)\n    print(\"\\ntraining set ImageDataGenerator\")\n    train_gen = data_gen.flow_from_dataframe(\n        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=train_dir,\n        x_col='new_filename',\n        y_col='artist',\n        has_ext=True,\n        target_size=(img_size, img_size),\n        subset=\"training\",\n        batch_size=batch_size,\n        seed=my_seed,\n        shuffle=True,\n        class_mode='categorical',\n        classes=list_of_classes\n    )\n\n    print(\"\\nvalidation set ImageDataGenerator\")\n    valid_gen = data_gen.flow_from_dataframe(\n        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=train_dir,\n        x_col='new_filename',\n        y_col='artist',\n        has_ext=True,\n        subset=\"validation\",\n        batch_size=batch_size,\n        seed=my_seed,\n        shuffle=True,\n        target_size=(img_size,img_size),\n        class_mode='categorical',\n        classes=list_of_classes\n    )\n\n    test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n    print(\"\\ntest set ImageDataGenerator\")\n    test_gen = test_data_gen.flow_from_dataframe(\n        dataframe=test_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=test_dir,\n        x_col='new_filename',\n        y_col=None,\n        has_ext=True,\n        batch_size=test_batch_size,\n        seed=my_seed,\n        shuffle=False, # dont shuffle test directory\n        class_mode=None,\n        target_size=(img_size,img_size)\n    )\n\n    return (train_gen, valid_gen, test_gen)\n\nprint(\"defined setup_generators()\")","37966c0c":"# delete some unused dataframes to free some RAM for training\ndel df\ndel t_df\ndel t1_df\ndel t2_df\ndel t3_df\ndel t4_df\ndel t5_df\ndel t6_df\ndel t7_df\ndel t8_df\ndel t9_df\ngc.collect()","08e69952":"train_gens = [None]*len(TRAIN_DIRS)\nvalid_gens = [None]*len(TRAIN_DIRS)\ntest_gen  = None # only 1 test_gen\ni = 0\nfor i in range(0, len(TRAIN_DIRS)):\n    train_gens[i], valid_gens[i], test_gen = setup_generators(\n        train_dataframe=all_train_dfs[i], train_dir=TRAIN_DIRS[i],\n        val_split=VAL_SPLIT, img_size=IMAGE_SIZE, batch_size=BATCH_SIZE, my_seed=MY_SEED, \n        list_of_classes=training_set_artists, test_dataframe=test_df, \n        test_dir=TEST_DIR, test_batch_size=TEST_BATCH_SIZE\n    )\n    i += 1","2410faeb":"# the tutorial had 10 epochs... \nMAX_EPOCHS = 20 * len(train_gens) # should be a multiple of 9 because need evenly train each train_dir\nDIR_EPOCHS = 1 # fit each train_dir at least this many times before overfitting","9df81a14":"histories = []\n\ne=0\nwhile ( e < MAX_EPOCHS):\n    for i in range(0, len(train_gens)):\n        # train_gen.n = number of images for training\n        STEP_SIZE_TRAIN = train_gens[i].n\/\/train_gens[i].batch_size\n        # train_gen.n = number of images for validation\n        STEP_SIZE_VALID = valid_gens[i].n\/\/valid_gens[i].batch_size\n        print(\"STEP_SIZE_TRAIN\",STEP_SIZE_TRAIN)\n        print(\"STEP_SIZE_VALID\",STEP_SIZE_VALID)\n        histories.append(\n            model.fit_generator(generator=train_gens[i],\n                                steps_per_epoch=STEP_SIZE_TRAIN,\n                                validation_data=valid_gens[i],\n                                validation_steps=STEP_SIZE_VALID,\n                                epochs=DIR_EPOCHS)\n        )\n        e+=1","8bc80b03":"accuracies = []\nval_accuracies = []\nlosses = []\nval_losses = []\nfor hist in histories:\n    if hist:\n        accuracies += hist.history['acc']\n        val_accuracies += hist.history['val_acc']\n        losses += hist.history['loss']\n        val_losses += hist.history['val_loss']","d98aaa2e":"# Plot training & validation accuracy values\nplt.plot(accuracies)\nplt.plot(val_accuracies)\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(losses)\nplt.plot(val_losses)\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","9cb302df":"import time\ntimestr = time.strftime(\"%Y%m%d-%H%M%S\") # e.g: 20181109-180140\nmodel.save('painters_adam_e180_'+timestr+'.h5')","73668a70":"PRED_STEPS = len(test_gen) #100 # default would have been len(test_gen)","5091ed53":"# Need to reset the test_gen before calling predict_generator\n# This is important because forgetting to reset the test_generator results in outputs with a weird order.\ntest_gen.reset()\npred=model.predict_generator(test_gen, verbose=1, steps=PRED_STEPS)","1513bf6c":"print(len(pred),\"\\n\",pred)","fb0f445e":"predicted_class_indices=np.argmax(pred,axis=1)","768b8d55":"print(len(predicted_class_indices),\"\\n\",predicted_class_indices)\nprint(\"it has values ranging from \",min(predicted_class_indices),\"...to...\",max(predicted_class_indices))","47c9e244":"labels = (train_gens[0].class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]","31f5ff3c":"print(\"*\"*20+\"\\nclass_indices\\n\"+\"*\"*20+\"\\n\",train_gens[0].class_indices,\"\\n\")\nprint(\"*\"*20+\"\\nlabels\\n\"+\"*\"*20+\"\\n\",labels,\"\\n\")\nprint(\"*\"*20+\"\\npredictions has\", len(predictions),\"values that look like\",\"'\"+str(predictions[0])+\"' which is the first prediction and corresponds to this index of the classes:\",train_gens[0].class_indices[predictions[0]])","bef3720e":"# Save the results to a CSV file.\nfilenames=test_gen.filenames[:len(predictions)] # because \"ValueError: arrays must all be same length\"\n\nreal_artists = []\nfor f in filenames:\n    real = test_df[test_df['new_filename'] == f].artist.get_values()[0]\n    real_artists.append(real)\n\nresults=pd.DataFrame({\"Filename\":filenames,\n                      \"Predictions\":predictions,\n                      \"Real Values\":real_artists})\nresults.to_csv(\"results.csv\",index=False)","e436fe28":"results.head()","a1e670ee":"len(training_set_artists)","5ce0a560":"print(training_set_artists)","40598566":"count = 0\nmatch = 0\nunexpected_count = 0\nunexpected_match = 0\nmatch_both_expected_unexpected = 0\n\nfor p, r in zip(results['Predictions'], results['Real Values']):\n    if r in training_set_artists:\n        count += 1\n        if p == r:\n            match += 1\n    else:\n        unexpected_count += 1\n        if p == r:\n            unexpected_match += 1\n\nprint(\"test accuracy on new images for TRAINED artsits\")\nacc = match\/count\nprint(match,\"\/\",count,\"=\",\"{:.4f}\".format(acc))\n\nprint(\"test accuracy on new images for UNEXPECTED artsits\")\nu_acc = unexpected_match\/unexpected_count\nprint(unexpected_match,\"\/\",unexpected_count,\"=\",\"{:.4f}\".format(u_acc))\n\nprint(\"test accuracy on new images\")\ntotal_match = match+unexpected_match\ntotal_count = count+unexpected_count\ntotal_acc = (total_match)\/(total_count)\nprint(total_match,\"\/\",total_count,\"=\",\"{:.4f}\".format(total_acc))","cf89bdd2":"# Compile Model","1895afda":"# Predict the output \ud83d\udd2e \ud83c\udfa9","9277e3c8":"So, it seems like the model may have learned some interesting patterns related to the artists that it expects. \n\n**Questions to explore:**\n* [What does the model actually \"see\"](https:\/\/arxiv.org\/abs\/1312.6034) in a painting by [Pablo Picasso](https:\/\/www.wikiart.org\/en\/pablo-picasso\/) as opposed to [Vincent van Gogh](https:\/\/www.wikiart.org\/en\/vincent-van-gogh)?\n* What would happen if we trained the model on the full artist dataset or at least on artists with over 200 paintings in the dataset?\n* Can the accuracy be improved with techniques like data augmentation or with a custom convolutional neural network? How about doing transfer learning with different [pre-trained model](https:\/\/keras.io\/applications)?\n* How can the learning rate be tuned to improve the accuracy?\n* Would a regularization technique like [dropout](https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras\/) be helpful?\n* This notebook uses the [Adam](https:\/\/keras.io\/optimizers\/#adam) optimizer... what if we tried RMSprop?\n* How about using an [ensemble of models](https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/)?","253ed6ce":"# Overview\n\nCan a computer \"learn\" to classify artists by their paintings? \n\nResNet50 is a good model for classifying ImageNet data. How about a set of 38 artists?\n\nWe use transfer learning to re-train a ResNet50 model to identify one of 38 artists who have more than 300 paintings in the dataset. \n\nThis notebook is part of a project for CSC 480 taught by [Dr. Franz J. Kurfess](http:\/\/users.csc.calpoly.edu\/~fkurfess\/) at Cal Poly\n\nA web application is [in development](https:\/\/github.com\/SomethingAboutImages\/WebImageClassifier) to make use of the model that this notebook outputs. ","38448ff1":"# specify the model that classifies 38 artists \ud83c\udfa8 \ud83d\udd8c","7a8b8ba6":"# TRAINING TIME!  \ud83c\udf89 \ud83c\udf8a \ud83c\udf81","718ed05f":"#\u00a0setup the image data generator for each training directory ","186c6f6e":"# Evaluate the model \ud83e\uddd0 \ud83e\udd14"}}