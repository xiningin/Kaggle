{"cell_type":{"d683c8f2":"code","d080a64a":"code","1ecfd929":"code","4ae9805c":"code","1ef9049a":"code","d92c0dde":"code","bd56d1d4":"code","cd22920e":"code","c0b2d7fe":"code","3978c3d3":"code","b25952fe":"code","cfc60793":"code","43aabc3d":"code","6969a1e1":"code","de1e3051":"code","24298a40":"code","1979b1cd":"code","adef6c38":"code","9c6f08bb":"code","ff258cd8":"code","d5c3247d":"code","8326371d":"code","46e3bf3e":"code","7a38a623":"code","842bc3ff":"code","57c91fe5":"code","38f63775":"code","24115c01":"code","4bcdc1e3":"code","4089a522":"code","02f7307f":"code","4d3803e7":"code","9cd2fe00":"code","160b5181":"code","38f854f0":"code","774120fd":"code","190baa57":"code","b4116d0d":"code","69496fc1":"code","f6e63ece":"code","05c0a305":"code","ce064ac2":"code","dfd27e78":"code","ed53a32e":"code","4bcc416e":"code","c2bcf5fc":"code","79fa77d6":"code","e1984e24":"code","bafb3f6e":"code","008e1b23":"code","2df725a8":"markdown","bf855443":"markdown","23c11d8d":"markdown","aa715fa6":"markdown","ede9be89":"markdown","87dedea8":"markdown","e39473e3":"markdown","c2fe507b":"markdown","11441c6e":"markdown","6b8bf5db":"markdown","770fc958":"markdown","a8251d68":"markdown","24a76c64":"markdown","0a1f9072":"markdown","6423af93":"markdown","83374f21":"markdown","b4d2e9f0":"markdown","8b9809bf":"markdown","ed9fe28e":"markdown","da5e56e0":"markdown","9da8c572":"markdown","6337f172":"markdown","eea8515f":"markdown","2f0f68e9":"markdown","629c1f84":"markdown","3b620331":"markdown","3742e28a":"markdown","5ea23be7":"markdown","42da32bc":"markdown"},"source":{"d683c8f2":"# Importing all required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d080a64a":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","1ecfd929":"# Importing carprice.csv\ncar_price_df = pd.read_csv(\"..\/input\/data.csv\")","4ae9805c":"# Getting categorical variable and splitting the and choosing only one of them to reduce the number of columns after converting them to numerical\ncar_price_df['Market Category'] = car_price_df['Market Category'].str.split(',', expand = True)[0]\ncar_price_df['Engine Fuel Type'] = car_price_df['Engine Fuel Type'].str.split(' ', expand = True)[0]\ncar_price_df['Driven_Wheels'] = car_price_df['Driven_Wheels'].str.split(' ', expand = True)[0]\ncar_price_df['Vehicle Style'] = car_price_df['Vehicle Style'].str.split(' ', expand = True)[0]\n# Model is not giving any value addition to prediction so dropping it\ncar_price_df.drop('Model', inplace=True,axis=1)","1ef9049a":"# Check the head of the dataset\ncar_price_df.head()","d92c0dde":"# checking shape of the dataframe\ncar_price_df.shape","bd56d1d4":"# checking values for null\ncar_price_df.info()","cd22920e":"# removing all the rows from DF which has null values\nCarPriceCleanDf=car_price_df.dropna()","c0b2d7fe":"# checking null values after removing them\nCarPriceCleanDf.info()","3978c3d3":"# getting all the important details\/information of dataset\nCarPriceCleanDf.describe()","b25952fe":"# Pair plot to check which feature has strongest relationship with 'MSRP' to get the benchmark model i.e. Simple Linear Regression model\nsns.pairplot(CarPriceCleanDf, x_vars=['Engine HP', 'city mpg', 'Vehicle Size'], y_vars='MSRP',size=4, aspect=1, kind='scatter')\nplt.show()","cfc60793":"fig = plt.figure(figsize=(12,10))\nsns.heatmap(CarPriceCleanDf.corr(),annot=True)","43aabc3d":"# converting all categorical variables to numeric variables\n\n# Let's drop the first column from status df using 'drop_first = True'\ncarmake = pd.get_dummies(CarPriceCleanDf['Make'], drop_first = True)\n\ncarenginefueltype = pd.get_dummies(CarPriceCleanDf['Engine Fuel Type'], drop_first = True)\n\ncartransmissiontype = pd.get_dummies(CarPriceCleanDf['Transmission Type'], drop_first = True)\n\ncardrivenwheels = pd.get_dummies(CarPriceCleanDf['Driven_Wheels'], drop_first = True)\n\ncarmarketcategory =  pd.get_dummies(CarPriceCleanDf['Market Category'], drop_first = True)\n\ncarvehiclesize = pd.get_dummies(CarPriceCleanDf['Vehicle Size'], drop_first = True)\n\ncarvehiclestyle = pd.get_dummies(CarPriceCleanDf['Vehicle Style'], drop_first = True)\n\n# Add the results to the original housing dataframe\nCarPriceCleanDf = pd.concat([CarPriceCleanDf,carmake, carenginefueltype,cartransmissiontype,cardrivenwheels,\n                             carmarketcategory,carvehiclesize,carvehiclestyle], axis = 1)\n\n# Drop all categorical variable original columns\nCarPriceCleanDf.drop(['Make','Engine Fuel Type','Transmission Type','Driven_Wheels', 'Market Category','Vehicle Size','Vehicle Style',], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nCarPriceCleanDf.head()","6969a1e1":"# import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\n\n# splitting train and test dataset for model\ndf_train, df_test = train_test_split(CarPriceCleanDf, train_size = 0.7, test_size = 0.3, random_state = 100)","de1e3051":"# import MinMaxScaler to scale all umerical features(other than 1 and 0)\nfrom sklearn.preprocessing import MinMaxScaler","24298a40":"# creating object of MinMaxScaler\nscaler = MinMaxScaler()","1979b1cd":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['Engine HP', 'Engine Cylinders', 'Number of Doors', 'highway MPG', 'city mpg','Popularity','MSRP']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","adef6c38":"# creating X and y from training dataset\n\ny_train = df_train.pop('MSRP')\nX_train = df_train","9c6f08bb":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","ff258cd8":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","d5c3247d":"# Getiing the rank of features and support value\n\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","8326371d":"# select the feature with 'True' support value\ncol = X_train.columns[rfe.support_]\ncol","46e3bf3e":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","7a38a623":"# Adding a constant variable to get intercept\nimport statsmodels.api as sm  \nX_train_rfe_lm = sm.add_constant(X_train_rfe)","842bc3ff":"# fitting the model\nlm = sm.OLS(y_train,X_train_rfe_lm).fit()  ","57c91fe5":"#Let's see the summary of our linear model with multiple features\nprint(lm.summary())","38f63775":"# Dropping the 'UNKNOWN' column from selected features\nX_train_new1 = X_train_rfe.drop([\"UNKNOWN\"], axis = 1)","24115c01":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm1 = sm.add_constant(X_train_new1)\n\n# creating new model after removing 'UNKNOWN' column\nlm = sm.OLS(y_train,X_train_lm1).fit()   \n\n#Let's see the summary of new linear model\nprint(lm.summary())","4bcdc1e3":"# Dropping Spyker from the selected features\nX_train_new2 = X_train_new1.drop([\"Spyker\"], axis = 1)","4089a522":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm2 = sm.add_constant(X_train_new2)\nlm = sm.OLS(y_train,X_train_lm2).fit()   \n\n#Let's see the summary of our linear model\nprint(lm.summary())","02f7307f":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new2.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new2.values, i) for i in range(X_train_new2.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4d3803e7":"# fitting model to predict the Y-values of training dataset\ny_train_price = lm.predict(X_train_lm2)","9cd2fe00":"# Error term should be normally distributed along mean zero for a stable model\n\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)     ","160b5181":"df_test[num_vars] = scaler.transform(df_test[num_vars])","38f854f0":"y_test = df_test.pop('MSRP')\nX_test = df_test","774120fd":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new2 = X_test[X_train_new2.columns]\n\n# Adding a constant variable \nX_test_new_lm2 = sm.add_constant(X_test_new2)","190baa57":"# Making predictions\ny_pred = lm.predict(X_test_new_lm2)","b4116d0d":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)  ","69496fc1":"# import mean squared error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","f6e63ece":"#Returns the mean squared error; we'll take a square root\nnp.sqrt(mean_squared_error(y_test, y_pred))","05c0a305":"# getting R-squared value\nr_squared = r2_score(y_test, y_pred)\nr_squared","ce064ac2":"# creating X and y from training dataset\nX = CarPriceCleanDf['Engine HP']\ny = CarPriceCleanDf['MSRP']","dfd27e78":"# import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\n\n# splitting train and test dataset for model\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","ed53a32e":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()\n\n# get the model summary\nprint(lr.summary())","4bcc416e":"# fitting model to predict the training value and get the residual\n\ny_train_pred = lr.predict(X_train_sm)\nres = (y_train - y_train_pred)","c2bcf5fc":"# Error term should be normally distributed along mean zero for a stable model\n\nfig = plt.figure()\nsns.distplot(res, bins = 15)\nfig.suptitle('Error Terms', fontsize = 15)                  # Plot heading \nplt.xlabel('y_train - y_train_pred', fontsize = 15)         # X-label\nplt.show()","79fa77d6":"# Drawing scatter plot of residuals\n\nplt.scatter(X_train,res)\nplt.show()","e1984e24":"# Add a constant to X_test\nX_test_sm = sm.add_constant(X_test)\n\n# Predict the y values corresponding to X_test_sm\ny_pred = lr.predict(X_test_sm)","bafb3f6e":"#Returns the mean squared error; we'll take a square root\nnp.sqrt(mean_squared_error(y_test, y_pred))","008e1b23":"# get R-squared value\nr_squared = r2_score(y_test, y_pred)\nr_squared","2df725a8":"#### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables","bf855443":"## Step 6: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","23c11d8d":"#### Visualising data with Heatmap","aa715fa6":"Since there is no pattern found in the error of Y-pred and Y_test and it is scattered randomly so it is giving good prediction value.","ede9be89":"## BenchMark Model (Simple Linear Regression) using 'Engine HP' as feature variable","87dedea8":"The R-squared value obtained is `0.814`\n\n#### Removing field `Spyker` as it is having non zero P-value(i.e. 0.017)","e39473e3":"## Step 2: Visualising the Data\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`.","c2fe507b":"### Conclusion\nR-squared value for\n    - Multiple Linear Regression: `0.814`\n    - Linear Regression: `0.429`\n\nOverall we have a decent model, but we also acknowledge that we could do better. \n\nWe have below option to get more accurate:\n1. Build a non-linear model","11441c6e":"## Step 8: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","6b8bf5db":"As MSE error is very large, it is giving more errors while predictions.","770fc958":"## Step 7: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final model.\n\n#### Applying the scaling on the test sets","a8251d68":"## Step 1: Reading and Understanding the Data\n\nLet us first import NumPy, Pandas, matplotlib and seaborn and read the car dataset ","24a76c64":"As it shows Engine HP has the good linear relation ship","0a1f9072":"## Step 5: Building a linear model\n\nFit a regression line through the training data using `statsmodels`. Remember that in `statsmodels`, you need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default.","6423af93":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","83374f21":"- The Adjusted R-squared value obtained is `0.814`.\n- P-Value for all features are zero.\n- AIC and BIC are lowest values.\n\nAbove three results are expected to be a good model. Now we will check for multicollinearity using VIF.\n","b4d2e9f0":"As we can see, there is a curve like pattern in the residuals scatter plot, hence we can improve it by adding more features as we have done in multiple linear regression.","8b9809bf":"We can see that the equation of our best fitted line is:\n   $ y = -0.0116 + (Engine HP * 0.1070) + (city mpg * 0.0614) + (Bentley * 0.0187) + (Bugatti * 0.7180) + (Ferrari * 0.0221) + (Lamborghini * 0.0538) + (Lotus * -0.0354) + (Maserati * -0.0255) + (Maybach * 0.1536) + (Porsche * 0.0158) + (Rolls-Royce * 0.0797) + (electric * -0.0395) + (Exotic * 0.0495) $","ed9fe28e":"## Car Price Prediction\n\n#### Problem Statement:\n\nPredict the price of a car based on various parameters like car make, car model, car year engine cylinder, driven wheels, market category etc. The company wants to know \u2014\n\n\n- which variables are significant in predicting the price of a car\n- how well those variables describe the price of a car\n\n**So interpretation is important!**","da5e56e0":"#### Dividing into x_test and y_test","9da8c572":"Since Error Term is not Normally distributed along mean zero hence it is not a good model.","6337f172":"## Step 4: Splitting the Data into Training and Testing Sets\n\nAs you know, the first basic step for regression is performing a train-test split.","eea8515f":"For Simple Linear Refression model. here are the out comes.\n\n- R-Squared value is  0.43, which is not good enough to make predictions.\n- AIC and BIC are higher value(positive).\n- Constant value is very large in negative value.\n\nHence SLR is not as good as MLR.","2f0f68e9":"### Rescaling the Features \n\nAs you saw in the demonstration for Simple Linear Regression, scaling doesn't impact your model. Here we can see that except for `Popularity`, all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nThis time, we will use MinMax scaling.","629c1f84":"We generally want a VIF that is less than 5.\n\nNow as you can see, the VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model only.","3b620331":"The R-squared value obtained is `0.815`\n\n#### Removing field `Unknown` as it is having P-value non zero(i.e. 0.039)","3742e28a":"As is visible from the pairplot and the heatmap, the variable `Engine Cylinders` seems to be most correlated with `Engine HP`, the variable `City mpg` seems to be most correlated with `highway mpg`. ","5ea23be7":"## Step 3: Data Preparation\n- You can see that your dataset has many columns has categorical values in String, like Vehicle style: Coupe \/ Convertible.\n\n- But in order to fit a regression line, we would need numerical values and not string. Hence, we need to convert them to 1s and 0s","42da32bc":"Since Error term is normally distributed along mean zero hence it is a stable model."}}