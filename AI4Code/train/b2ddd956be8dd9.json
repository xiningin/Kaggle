{"cell_type":{"a0f8f48b":"code","f6bfd11f":"code","08b30b83":"code","07c8c28b":"code","28d88c92":"code","d893315c":"code","b041fb3c":"code","620dd6f0":"code","e6a03ce6":"code","630211d7":"code","ad2b9102":"code","d0fb7ca6":"code","82b8cf50":"code","c6e2adeb":"code","ea5a1944":"code","a762fb56":"code","7a0e3f2a":"code","b7282bf2":"code","07414abe":"code","c3ffe96a":"code","e1b6f51c":"code","ba334d2a":"code","779fe79c":"code","d8667706":"code","23deb541":"code","750660b3":"code","31816ed3":"code","08835dc4":"code","c8ddebe8":"code","a7348c09":"code","433d9720":"code","d8a1bd50":"code","63b24195":"code","0f6077ef":"code","d5dc0e51":"code","7541a9ed":"code","5fa6062d":"code","aac4f630":"code","aa687b59":"code","9dcbf85c":"code","58c54332":"code","33a42692":"code","ddc77d12":"code","fed9aae3":"code","ef17d6a3":"code","469befd4":"code","0eda4aaf":"code","88c7c440":"code","9121dd13":"code","f75f66d2":"code","2a285ad6":"code","54f93d54":"code","327e663a":"code","d83f21f1":"code","5849d0e2":"code","f9d40ef3":"code","2fe3e0d0":"code","7cbcda87":"code","ae7dbb21":"code","09a91ac0":"code","774efae0":"code","54f14f5e":"code","9c85a8b4":"code","969711bb":"code","1d0e98c7":"code","d0f02ec5":"code","c2e438dd":"code","4d8cbc19":"code","bfc007be":"code","59e47231":"code","39b156d9":"code","95a75c56":"code","5446788a":"code","c939498d":"code","5dcc009d":"code","de25cb89":"code","c50d1d46":"code","98c128b9":"code","443a1be8":"code","917c6d5a":"code","e8a70e67":"code","e519cf41":"code","e5d0d457":"code","4bdc26f0":"code","c1ea8314":"code","1a8ec080":"code","a579c860":"markdown","ed8f9bf2":"markdown","0f83f697":"markdown","bd8756a8":"markdown","990bbf4d":"markdown","f04a72d4":"markdown","dcf50aae":"markdown","89bf1294":"markdown","ebd5d35b":"markdown","e8807651":"markdown","98fdff17":"markdown","a34a9df0":"markdown","6fffa584":"markdown","5ff33927":"markdown","0cf0900e":"markdown","3140250d":"markdown","54521edc":"markdown","f8d1f2b4":"markdown","9019eeed":"markdown","0b74af6c":"markdown","e6cd750f":"markdown","bec3f149":"markdown","29bd423d":"markdown","37113d5a":"markdown","4ebf614e":"markdown","6e63dff9":"markdown","7a22f066":"markdown","6a244d88":"markdown","902c6178":"markdown","96d44ef1":"markdown","91a7d9ca":"markdown","4a1b184d":"markdown","5e759431":"markdown","6ee3f237":"markdown","05ca67ee":"markdown","c085f074":"markdown","9a33b238":"markdown","501c0eb3":"markdown","4abcce87":"markdown","32101c05":"markdown","a1d425fc":"markdown","68322d6b":"markdown","755bd3d9":"markdown","ad57003a":"markdown","cb3633d9":"markdown","f038f594":"markdown","6347c2ec":"markdown","81f16943":"markdown","a5c837a5":"markdown","d091c829":"markdown","bac713f5":"markdown","6a1750eb":"markdown","0a73b873":"markdown"},"source":{"a0f8f48b":"import numpy as np \nimport pandas as pd\n\nimport featuretools as ft\n\nimport warnings\nwarnings.filterwarnings('ignore')","f6bfd11f":"# Raw data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntest['Target'] = np.nan\n\ndata = train.append(test, sort = True)","08b30b83":"train_valid = train.loc[train['parentesco1'] == 1, ['idhogar', 'Id', 'Target']].copy()\ntest_valid = test.loc[test['parentesco1'] == 1, ['idhogar', 'Id']].copy()\n\nsubmission_base = test[['Id', 'idhogar']]","07c8c28b":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Fill in the values with the correct mapping\ndata['dependency'] = data['dependency'].replace(mapping).astype(np.float64)\ndata['edjefa'] = data['edjefa'].replace(mapping).astype(np.float64)\ndata['edjefe'] = data['edjefe'].replace(mapping).astype(np.float64)\n\ndata[['dependency', 'edjefa', 'edjefe']].describe()","28d88c92":"data['v18q1'] = data['v18q1'].fillna(0)\n\n# Fill in households that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\n\n# If individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()\n\ndata.loc[data['rez_esc'] > 5, 'rez_esc'] = 5","d893315c":"# Difference between people living in house and household size\ndata['hhsize-diff'] = data['tamviv'] - data['hhsize']\n\nelec = []\n\n# Assign values\nfor i, row in data.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\ndata['elec'] = elec\ndata['elec-missing'] = data['elec'].isnull()\n\n# Remove the electricity columns\n# data = data.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])\n\n# Wall ordinal variable\ndata['walls'] = np.argmax(np.array(data[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\n\n# data = data.drop(columns = ['epared1', 'epared2', 'epared3'])\n\n# Roof ordinal variable\ndata['roof'] = np.argmax(np.array(data[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\n# data = data.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\ndata['floor'] = np.argmax(np.array(data[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\n# data = data.drop(columns = ['eviv1', 'eviv2', 'eviv3'])\n\n# Create new feature\ndata['walls+roof+floor'] = data['walls'] + data['roof'] + data['floor']\n\n# No toilet, no electricity, no floor, no water service, no ceiling\ndata['warning'] = 1 * (data['sanitario1'] + \n                         (data['elec'] == 0) + \n                         data['pisonotiene'] + \n                         data['abastaguano'] + \n                         (data['cielorazo'] == 0))\n\n# Owns a refrigerator, computer, tablet, and television\ndata['bonus'] = 1 * (data['refrig'] + \n                      data['computer'] + \n                      (data['v18q1'] > 0) + \n                      data['television'])\n\n# Per capita features\ndata['phones-per-capita'] = data['qmobilephone'] \/ data['tamviv']\ndata['tablets-per-capita'] = data['v18q1'] \/ data['tamviv']\ndata['rooms-per-capita'] = data['rooms'] \/ data['tamviv']\ndata['rent-per-capita'] = data['v2a1'] \/ data['tamviv']\n\n# Create one feature from the `instlevel` columns\ndata['inst'] = np.argmax(np.array(data[[c for c in data if c.startswith('instl')]]), axis = 1)\n# data = data.drop(columns = [c for c in data if c.startswith('instlevel')])\n\ndata['escolari\/age'] = data['escolari'] \/ data['age']\ndata['inst\/age'] = data['inst'] \/ data['age']\ndata['tech'] = data['v18q'] + data['mobilephone']\n\nprint('Data shape: ', data.shape)","b041fb3c":"data = data[[x for x in data if not x.startswith('SQB')]]\ndata = data.drop(columns = ['agesq'])\ndata.shape","620dd6f0":"# Create correlation matrix\ncorr_matrix = data.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')\nprint(to_drop)","e6a03ce6":"data = data.drop(columns = to_drop)","630211d7":"import featuretools.variable_types as vtypes","ad2b9102":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing', 'elec-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin','hhsize-diff',\n              'elec',  'walls', 'roof', 'floor', 'walls+roof+floor', 'warning', 'bonus',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding',\n          'phones-per-capita', 'tablets-per-capita', 'rooms-per-capita', 'rent-per-capita']","d0fb7ca6":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['age', 'escolari', 'rez_esc', 'inst', 'tech']\n\nind_cont = ['escolari\/age', 'inst\/age']","82b8cf50":"to_remove = []\nfor l in [hh_ordered, hh_bool, hh_cont, ind_bool, ind_ordered, ind_cont]:\n    for c in l:\n        if c not in data:\n            to_remove.append(c)","c6e2adeb":"for l in [hh_ordered, hh_bool, hh_cont, ind_bool, ind_ordered, ind_cont]:\n    for c in to_remove:\n        if c in l:\n            l.remove(c)","ea5a1944":"len(hh_ordered+hh_bool+hh_cont+ind_bool+ind_ordered+ind_cont) == (data.shape[1] - 3)","a762fb56":"for variable in (hh_bool + ind_bool):\n    data[variable] = data[variable].astype('bool')","7a0e3f2a":"for variable in (hh_cont + ind_cont):\n    data[variable] = data[variable].astype(float)","b7282bf2":"for variable in (hh_ordered + ind_ordered):\n    try:\n        data[variable] = data[variable].astype(int)\n    except Exception as e:\n        print(f'Could not convert {variable} because of missing values.')","07414abe":"import matplotlib.pyplot as plt\n%matplotlib inline","c3ffe96a":"data.dtypes.value_counts().plot.bar(edgecolor = 'k');\nplt.title('Variable Type Distribution');","e1b6f51c":"es = ft.EntitySet(id = 'households')\nes.entity_from_dataframe(entity_id = 'data', \n                         dataframe = data, \n                         index = 'Id')","ba334d2a":"es.normalize_entity(base_entity_id='data', \n                    new_entity_id='household', \n                    index = 'idhogar', \n                    additional_variables = hh_bool + hh_ordered + hh_cont + ['Target'])\nes","779fe79c":"# Deep Feature Synthesis\nfeature_matrix, feature_names = ft.dfs(entityset=es, \n                                       target_entity = 'household', \n                                       max_depth = 2, \n                                       verbose = 1, \n                                       n_jobs = -1, \n                                       chunk_size = 100)\n","d8667706":"all_features = [str(x.get_name()) for x in feature_names]\nfeature_matrix.head()","23deb541":"all_features[-10:]","750660b3":"drop_cols = []\nfor col in feature_matrix:\n    if col == 'Target':\n        pass\n    else:\n        if 'Target' in col:\n            drop_cols.append(col)\n            \nprint(drop_cols)            \nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]         \nfeature_matrix.head()","31816ed3":"feature_matrix.shape","08835dc4":"# Create correlation matrix\ncorr_matrix = feature_matrix.corr().abs()\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] >= 0.99)]\n\nprint('There are {} columns with >= 0.99 correlation.'.format(len(to_drop)))\nto_drop","c8ddebe8":"feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]","a7348c09":"train = feature_matrix[feature_matrix['Target'].notnull()].reset_index()\ntest = feature_matrix[feature_matrix['Target'].isnull()].reset_index()\ntrain.head()","433d9720":"corrs = train.corr()\ncorrs['Target'].sort_values(ascending = True).head(10)","d8a1bd50":"corrs['Target'].sort_values(ascending = True).dropna().tail(10)","63b24195":"train = train[train['idhogar'].isin(list(train_valid['idhogar']))]\ntrain.head()","0f6077ef":"test = test[test['idhogar'].isin(list(test_valid['idhogar']))]\ntest.head()","d5dc0e51":"train_labels = np.array(train.pop('Target')).reshape((-1,))\ntest_ids = list(test.pop('idhogar'))","7541a9ed":"train, test = train.align(test, axis = 1, join = 'inner')\nall_features = list(train.columns)\ntrain.shape","5fa6062d":"%%capture\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb","aac4f630":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","aa687b59":"from IPython.display import display","9dcbf85c":"def model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n    \n    hyp_OPTaaS = { 'boosting_type': 'dart',\n              'colsample_bytree': 0.9843467236959204,\n              'learning_rate': 0.11598629586769524,\n              'min_child_samples': 44,\n              'num_leaves': 49,\n              'reg_alpha': 0.35397370408131534,\n              'reg_lambda': 0.5904910774606467,\n              'subsample': 0.6299872254632797,\n              'subsample_for_bin': 60611}\n\n    # Model hyperparameters\n#     params = {'boosting_type': 'dart', \n#               'colsample_bytree': 0.88, \n#               'learning_rate': 0.028, \n#                'min_child_samples': 10, \n#                'num_leaves': 36, 'reg_alpha': 0.76, \n#                'reg_lambda': 0.43, \n#                'subsample_for_bin': 40000, \n#                'subsample': 0.54}\n\n    model = lgb.LGBMClassifier(**hyp_OPTaaS, class_weight = 'balanced',\n                               objective = 'multiclass', n_jobs = -1, n_estimators = 10000)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        # Dataframe for \n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        predictions = predictions.append(fold_predictions)\n        \n        importances += model.feature_importances_ \/ nfolds   \n        \n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances\n    return submission, feature_importances, valid_scores","58c54332":"len(train_labels) == train.shape[0]","33a42692":"len(test_ids) == test.shape[0]","ddc77d12":"%%capture --no-display\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n\nresults = pd.DataFrame({'version': ['default_5fold'], \n                        'F1-mean': [valid_scores.mean()], \n                        'F1-std': [valid_scores.std()]})","fed9aae3":"def plot_feature_importances(df, n = 15, return_features = False, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    plt.style.use('fivethirtyeight')\n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'blue', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'Top {n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    if return_features:\n        return df","ef17d6a3":"plot_feature_importances(feature_importances)","469befd4":"submission.to_csv('ft_baseline.csv', index = False)","0eda4aaf":"submission['Target'].value_counts().sort_index().plot.bar(color = 'blue');\nplt.title('Distribution of Predicted Labels for Individuals', size = 14);","88c7c440":"data[data['Target'].notnull()]['Target'].value_counts().sort_index().plot.bar(color = 'blue');\nplt.title('Distribution of Labels for Training Individuals', size = 12);","9121dd13":"from featuretools.primitives import make_agg_primitive\n\n# Custom primitive\ndef range_calc(numeric):\n    return np.max(numeric) - np.min(numeric)\n\nrange_ = make_agg_primitive(function = range_calc,\n                            input_types = [ft.variable_types.Numeric], \n                            return_type = ft.variable_types.Numeric)","f75f66d2":"def p_corr_calc(numeric1, numeric2):\n    return np.corrcoef(numeric1, numeric2)[0, 1]\n\npcorr_ = make_agg_primitive(function = p_corr_calc,\n                            input_types = [ft.variable_types.Numeric, ft.variable_types.Numeric], \n                            return_type = ft.variable_types.Numeric)","2a285ad6":"def s_corr_calc(numeric1, numeric2):\n    return spearmanr(numeric1, numeric2)[0]\n\nscorr_ = make_agg_primitive(function = s_corr_calc, \n                           input_types = [ft.variable_types.Numeric, ft.variable_types.Numeric], \n                           return_type = ft.variable_types.Numeric)","54f93d54":"%%capture\nfeature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n                                              agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n                                                             'sum', 'skew', 'std', range_],\n                                          trans_primitives = [], drop_exact = all_features,\n                                          max_depth = 2, \n                                          verbose = 1, n_jobs = -1, \n                                          chunk_size = 100)","327e663a":"all_features += [str(x.get_name()) for x in feature_names_add]\nfeature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\nfeature_matrix.shape","d83f21f1":"def post_process(feature_matrix,\n                 missing_threshold = 0.95, \n                 correlation_threshold = 0.95):\n    \n    # Remove duplicated features\n    start_features = feature_matrix.shape[1]\n    feature_matrix = feature_matrix.iloc[:, ~feature_matrix.columns.duplicated()]\n    n_duplicated = start_features - feature_matrix.shape[1]\n    print(f'There were {n_duplicated} duplicated features.')\n    \n    feature_matrix = feature_matrix.replace({np.inf: np.nan, -np.inf:np.nan}).reset_index()\n    \n    # Remove the ids and labels\n    ids = list(feature_matrix.pop('idhogar'))\n    labels = list(feature_matrix.pop('Target'))\n    \n    # Remove columns derived from the Target\n    drop_cols = []\n    for col in feature_matrix:\n        if col == 'Target':\n            pass\n        else:\n            if 'Target' in col:\n                drop_cols.append(col)\n                \n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]] \n    \n    # One hot encoding (if necessary)\n    feature_matrix = pd.get_dummies(feature_matrix)\n    n_features_start = feature_matrix.shape[1]\n    print('Original shape: ', feature_matrix.shape)\n    \n    # Find missing and percentage\n    missing = pd.DataFrame(feature_matrix.isnull().sum())\n    missing['fraction'] = missing[0] \/ feature_matrix.shape[0]\n    missing.sort_values('fraction', ascending = False, inplace = True)\n\n    # Missing above threshold\n    missing_cols = list(missing[missing['fraction'] > missing_threshold].index)\n    n_missing_cols = len(missing_cols)\n\n    # Remove missing columns\n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n    print('{} missing columns with threshold: {}.'.format(n_missing_cols, missing_threshold))\n    \n    # Zero variance\n    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n    n_zero_variance_cols = len(zero_variance_cols)\n\n    # Remove zero variance columns\n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n    print('{} zero variance columns.'.format(n_zero_variance_cols))\n    \n    # Correlations\n    corr_matrix = feature_matrix.corr()\n\n    # Extract the upper triangle of the correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n    # Select the features with correlations above the threshold\n    # Need to use the absolute value\n    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n\n    n_collinear = len(to_drop)\n    \n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n    print('{} collinear columns removed with correlation above {}.'.format(n_collinear,  correlation_threshold))\n    \n    total_removed = n_duplicated + n_missing_cols + n_zero_variance_cols + n_collinear\n    \n    print('Total columns removed: ', total_removed)\n    print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n    \n    # Extract the ids and labels\n    feature_matrix['idhogar'] = ids\n    feature_matrix['Target'] = labels\n    \n    # Extract out training and testing data\n    train = feature_matrix[feature_matrix['Target'].notnull()]\n    test = feature_matrix[feature_matrix['Target'].isnull()]\n    \n    # Subset to houses with a head of household\n    train = train[train['idhogar'].isin(list(train_valid['idhogar']))]\n    test = test[test['idhogar'].isin(list(test_valid['idhogar']))]\n    \n    # Training labels and testing household ids\n    train_labels = np.array(train.pop('Target')).reshape((-1,))\n    test_ids = list(test.pop('idhogar'))\n    \n    # Align the dataframes to ensure they have the same columns\n    train, test = train.align(test, join = 'inner', axis = 1)\n    \n    assert (len(train_labels) == train.shape[0]), \"Labels must be same length as number of training observations\"\n    assert(len(test_ids) == test.shape[0]), \"Must be equal number of test ids as testing observations\"\n    \n    return train, train_labels, test, test_ids","5849d0e2":"train, train_labels, test, test_ids = post_process(feature_matrix)","f9d40ef3":"%%capture --no-display\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\nresults = results.append(pd.DataFrame({'version': ['additional_5fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))","2fe3e0d0":"plot_feature_importances(feature_importances)","7cbcda87":"%%capture\nfeature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n                                                         'sum', 'skew', 'std', range_, pcorr_],\n                                       trans_primitives = [], drop_exact = list(all_features),\n                                       max_depth = 2, max_features = 1000,\n                                       verbose = 1, n_jobs = -1, \n                                       chunk_size = 100)","ae7dbb21":"all_features += [str(x.get_name()) for x in feature_names_add]\nfeature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\ntrain, train_labels, test, test_ids = post_process(feature_matrix)\ntrain.shape","09a91ac0":"%%capture --no-display\n\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\nresults = results.append(pd.DataFrame({'version': ['additionalft_5fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))","774efae0":"print('Original training shape', train.shape)\ntrain = train[list(feature_importances.loc[feature_importances['importance'] != 0, 'feature'])]\ntest = test[train.columns]\n\nprint('Training shape after removing zero importance features', train.shape)","54f14f5e":"%%capture\nfeature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n                                                         'sum', 'skew', 'std', range_, pcorr_],\n                                       trans_primitives = ['divide'], drop_contains = list(all_features),\n                                       max_depth = 2, max_features = 1000,\n                                       verbose = 1, n_jobs = -1, \n                                       chunk_size = 1000)","9c85a8b4":"all_features += [str(x.get_name()) for x in feature_names_add]\nfeature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\ntrain, train_labels, test, test_ids = post_process(feature_matrix)\ntrain.shape","969711bb":"%%capture --no-display\n\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\nresults = results.append(pd.DataFrame({'version': ['divide1000_5fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))\nsubmission.to_csv('divide1000_featuretools.csv', index = False)","1d0e98c7":"plot_feature_importances(feature_importances)","d0f02ec5":"feature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n                                                         'sum', 'skew', 'std', range_, pcorr_],\n                                       trans_primitives = ['divide'], drop_contains = list(all_features),\n                                       max_depth = 2, max_features = 1500,\n                                       verbose = 1, n_jobs = -1, \n                                       chunk_size = 1000)","c2e438dd":"all_features += [str(x.get_name()) for x in feature_names_add]\nfeature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\ntrain, train_labels, test, test_ids = post_process(feature_matrix)\ntrain.shape","4d8cbc19":"%%capture --no-display\n\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\nresults = results.append(pd.DataFrame({'version': ['divide1500_5fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))\nsubmission.to_csv('divide1500_featuretools.csv', index = False)","bfc007be":"plot_feature_importances(feature_importances)","59e47231":"%%capture\nfeature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n                                                         'sum', 'skew', 'std', range_, pcorr_],\n                                       trans_primitives = ['divide'], drop_exact = list(all_features),\n                                       max_depth = 2, max_features = 2000,\n                                       verbose = 1, n_jobs = -1, \n                                       chunk_size = 100)","39b156d9":"%%capture\nall_features += [str(x.get_name()) for x in feature_names_add]\nfeature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\ntrain, train_labels, test, test_ids = post_process(feature_matrix)\ntrain.shape","95a75c56":"print('Total number of features considered: ', len(all_features))","5446788a":"%%capture --no-display\n\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\nresults = results.append(pd.DataFrame({'version': ['divide2000_5fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))\nsubmission.to_csv('divide2000_featuretools.csv', index = False)","c939498d":"plot_feature_importances(feature_importances)","5dcc009d":"%%capture --no-display\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 10)\nresults = results.append(pd.DataFrame({'version': ['divide2000_10fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))\nsubmission.to_csv('divide2000_10fold_featuretools.csv', index = False)","de25cb89":"%%capture\nfeature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n                                                         'sum', 'skew', 'std', range_, pcorr_],\n                                       trans_primitives = ['divide'], drop_contains = list(all_features),\n                                       max_depth = 2, max_features = 5000,\n                                       verbose = 1, n_jobs = -1, \n                                       chunk_size = 100)","c50d1d46":"all_features += [str(x.get_name()) for x in feature_names_add]\nfeature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\ntrain, train_labels, test, test_ids = post_process(feature_matrix)\ntrain.shape","98c128b9":"%%capture --no-display\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\nresults = results.append(pd.DataFrame({'version': ['divide5000_5fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))\nsubmission.to_csv('divide5000_featuretools.csv', index = False)","443a1be8":"%%capture --no-display\nsubmission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 10)\nresults = results.append(pd.DataFrame({'version': ['divide5000_10fold'], \n                                       'F1-mean': [valid_scores.mean()], \n                                       'F1-std': [valid_scores.std()]}))\nsubmission.to_csv('divide5000_10fold_featuretools.csv', index = False)","917c6d5a":"print('Total number of features considered: ', len(all_features))","e8a70e67":"results.set_index('version', inplace = True)\n\nresults['F1-mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(results['F1-std']))\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');","e519cf41":"feature_matrix = feature_matrix.iloc[:, ~feature_matrix.columns.duplicated()].reset_index()","e5d0d457":"train_ids = list(feature_matrix[(feature_matrix['Target'].notnull()) & (feature_matrix['idhogar'].isin(list(train_valid['idhogar'])))]['idhogar'])","4bdc26f0":"print('Train shape before removing zero importance features:', train.shape)\ntrain = train[list(feature_importances.loc[feature_importances['importance'] != 0, 'feature'])]\ntest = test[train.columns]\nprint('Train shape after removing zero importance features:', train.shape)","c1ea8314":"train['Target'] = train_labels\ntest['Target'] = np.nan\ntrain['idhogar'] = train_ids\ntest['idhogar'] = test_ids\ndata = train.append(test)\n\nresults.to_csv('model_results.csv', index = True)\ndata.to_csv('ft_5000_important.csv', index = False)","1a8ec080":"print('Final shape of data (with testing joined to training): ', data.shape)","a579c860":"## Remove Highly Correlated Columns","ed8f9bf2":"# Normalize Household Table\n\nNormalization allows us to create another table with one unique row per instance. In this case, the instances are households. The new table is derived from the `data` table and we need to bring along any of the household level variables. Since these are the same for all members of a household, we can directly add these as columns in the household table using `additional_variables`. The index of the household table is `idhogar` which uniquely identifies each household.  \n\nAll of the variable types have already been confirmed.","0f83f697":"#  Establish Correct Variable Types\n\nWe need to specify the correct variables types:\n\n1. Individual Variables: these are characteristics of each individual rather than the household\n    * Boolean: Yes or No (0 or 1)\n    * Ordered Discrete: Integers with an ordering\n2. Household variables\n    * Boolean: Yes or No\n    * Ordered Discrete: Integers with an ordering\n    * Continuous numeric\n\nBelow we manually define the variables in each category. This is a little tedious, but also necessary.","bd8756a8":"## Go to 2000\n\nThis is getting ridiculous.\n\n","990bbf4d":"# Modeling with Gradient Boosting Machine\n\nThe hyperparameters used here _have not been optimized_. This is meant only as a first pass at modeling with these features. ","f04a72d4":"# EntitySet and Entities\n\nAn `EntitySet` in Featuretools holds all of the tables and the relationships between them. At the moment we only have a single table, but we can create multiple tables through normalization. We'll call the first table `data` since it contains all the information both at the individual level and at the household level.","dcf50aae":"# Correlations with the target","89bf1294":"# Increase number of features","ebd5d35b":"The cells below remove any columns that aren't in the data (these may have been removed due to correlation).","e8807651":"That one call alone gave us 147 features to train a model! This was only using the default primitives as well. We can use more primitives or write our own to build more features.","98fdff17":"## Increase to 1500 features\n\n1000 is clearly not enough! Most of these features are highly correlated, but we can still find useful features as evidenced by the feature importances.","a34a9df0":"We need to make sure the length of the labels matches the length of the training dataset.","6fffa584":"The cross validation accuracy continues to increase as we add features. I think we should be able to add more features as long as we continue to impose feature selection. The gradient boosting machine seems very good at cutting through the swath of features. Eventually we're probably going to be overfitting to the training data, but the we can address that through regularization and feature selection.","5ff33927":"Below we convert the `Boolean` variables to the correct type. ","0cf0900e":"Then we convert the float variables.","3140250d":"# Add in Divide Primitive\n\nNext we'll add a `divide` transform primitive into the deep feature synthesis call. At first we'll limit the features to 1000. ","54521edc":"# 5000! Features","f8d1f2b4":"Finally, the same with the ordinal variables.","9019eeed":"### Labels for Training","0b74af6c":"### Remove Squared Variables\n\nThe gradient boosting machine does not need the squared version of variables it if already has the original variables. ","e6cd750f":"We need to remove any columns containing derivations of the `Target`. These are created because some of transform primitives might have affected the `Target`.","bec3f149":"I'm not running the GBM with a random seed so the same set of features can produce different cross validation results. A random seed would ensure consistent results, but may have a singificant effect on the predictions.","29bd423d":"### Training and Testing Data","37113d5a":"## Subset to Relevant Data","4ebf614e":"We'll now get into modeling. The gradient boosting machine implemented in LightGBM usually does well! ","6e63dff9":"These shows the predictions on an individual, not household level (we set all individuals to 4 if they did not have a head of household). The distribution is close to what we observe in the training labels, which are provided on the household level.","7a22f066":"# Post Processing Function\n\nThere are a number of steps after generating the feature matrix so let's put all of these in a function.\n\n1. Remove any duplicated columns.\n2. Replace infinite values with `np.nan`\n3. Remove columns with a missing percentage above the `missing_threshold`\n4. Remove columns with only a single unique value.\n5. Remove one out of every pair of columns with a correlation threshold above the `correlation_threshold`\n6. Extract the training and testing data along with labels and ids (needed for making submissions)","6a244d88":"## Results After Post-Processing","902c6178":"# More Featuretools\n\nWhy stop with 150 features? Let's add in a few more primitives and start creating more. To prevent featuretools from building the exact same features we already have, we can add `drop_exact` and pass in the feature names (as strings using the `get_name` functionality. ","96d44ef1":"# Try Modeling with more folds\n\nAs a final model, we'll increase the number of folds to 10 and see if this results in more stable predictions across folds. It's concerning that there is so much variation between folds, but that is going to happen with a small, imbalanced testing set.","91a7d9ca":"To start with, we use the default `agg` and `trans` primitives in a call to `ft.dfs`.","4a1b184d":"## Feature Importances\n\nThe utility function below plots feature importances and can show us how many features are needed for a certain cumulative level of importance. ","5e759431":"# Deep Feature Synthesis\n\nHere is where Featuretools gets to work. Using feature primitives, Deep Feature Synthesis can build hundreds (or 1000s as we will later see) of features from the relationships between tables and the columns in tables themselves. There are two types of primitives, which are operations applied to data:\n\n* Transforms: applied to one or more columns in a _single table_ of data \n* Aggregations: applied across _multiple tables_ using the relationships between tables\n\nWe generate the features by calling `ft.dfs`. This build features using any of the applicable primitives for each column in the data. Featuretools uses the table relationships to aggregate features as required. For example, it will automatically aggregate the individual level data at the household level. ","6ee3f237":"# Comparison of Models\n\nAt this point we might honestly ask if there is any benefit to increasing the number of features. Only one way to find out: through data! Let's look at the performance of models so far.","05ca67ee":"## Missing Values","c085f074":"# Feature Selection\n\nWe can do some rudimentary feature selection, removing one of any pair of columns with a correlation greater than 0.99 (absolute value).","9a33b238":"Featuretools has built features with moderate correlations with the `Target`. Although these correlations only show linear relationships, they can still provide an approximation of what features will be \"useful\" to a machine learning model.","501c0eb3":"We should also make sure the len of `test_ids` (the `idhogar` of the testing households) is the same as the length of the testing dataset.","4abcce87":"## Remove Zero Importance Features","32101c05":"We'll read in the data and join the training and testing set together. ","a1d425fc":"## Custom Evaluation Metric for LightGBM\n\nThis is the F1 Macro score used by the competition. Defining a custom evaluation metric for Light GBM is not exactly straightforward but we can manage.","68322d6b":"Most of these features are aggregations we could have made ourselves. However, why go to the trouble if Featuretools can do that for us?","755bd3d9":"The three columns not in the above lists are `Id`, `Idhogar`, and `Target`. ","ad57003a":"We can also make a custom primitive that calculates the correlation coefficient between two columns.\n\n### Correlation Primitive","cb3633d9":"# Conclusions\n\nFeaturetools certainly can make our job easier for this problem! Adding features continues to improve the validation score with mixed effects on the public leaderboard. The next step is to optimize the model for these features. __Featuretools should be a default part of your data science workflow.__ The tool is incredibly simple to use and delivers considerable value, creating features that we never would have imagined. I look forward to seeing what the community can come up with for this problem! ","f038f594":"# Custom Primitive\n\nTo expand the capabilities of featuretools, we can write our own primitives to be applied to the data. We'll write a simple function that finds the range of a numeric column. \n\n### Range Primitive","6347c2ec":"All that's left is to model! The cell below runs the gradient boosting machine model and saves the results. ","81f16943":"## Domain Knowledge Feature Construction","a5c837a5":"### Data Preprocessing \n\nThese steps are laid out in the kernel [A Complete Introduction and Walkthrough](https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough).  They involve correcting missing values, creating a few features (that Featuretools can build on top of). ","d091c829":"# Featuretools for Good\n\nIn this notebook, we will implement automated feature engineering with [Featuretools](https:\/\/docs.featuretools.com\/#minute-quick-start) for the Costa Rican Household Poverty Challenge. The objective of this data science for good problem is to predict the poverty of households in Costa Rica. \n\n## Automated Feature Engineering\n\nAutomated feature engineering should be a _default_ part of your data science workflow. Manual feature engineering is limited both by human creativity and time constraints but automated methods have no such constraints. At the moment, Featuretools is the only open-source Python library available for automated feature engineering. This library is extremely easy to get started with and very powerful (as the score from this kernel illustrates). \n\nFor anyone new to featuretools, check out the [documentation](https:\/\/docs.featuretools.com\/getting_started\/install.html) or an [introductory blog post here.](https:\/\/towardsdatascience.com\/automated-feature-engineering-in-python-99baf11cc219) ","bac713f5":"# Save Data\n\nWe can save the final selected featuretools feature matrix (created with a maximum of 2000 features). This will be used for Bayesian optimization of model hyperparameters. There still might be additional gains to increasing the number of features and\/or using different custom primitives. My focus is now going to shift to modeling, but I encourage anyone to keep adjusting the featuretools implementation.","6a1750eb":"### 5000 features with 10 fold modeling","0a73b873":"### Table Relationships\n\nNormalizing the entity automatically adds in the relationship between the parent, `household`, and the child, `ind`. This relationship links the two tables and allows us to create \"deep features\" by aggregating individuals in each household."}}