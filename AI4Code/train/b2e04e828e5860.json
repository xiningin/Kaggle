{"cell_type":{"d9b13e02":"code","55df652b":"code","21d92be8":"code","e6541df7":"code","463b6fff":"code","17ed17e2":"code","abff2594":"code","04f0c5f1":"code","0fe3fb22":"code","8e35ad5f":"code","9167111b":"code","bdd6f441":"code","c878ba5c":"code","606684e4":"code","e06ef5bc":"code","56637476":"code","713395e8":"code","c4196de2":"code","4b31bada":"code","d01215b8":"code","abebd4d4":"code","7106fd49":"code","9a1a6f34":"code","36f62eab":"code","e47ee2d0":"code","06b35645":"code","44a83cd8":"code","e0de8c80":"code","e835922a":"code","311bbd28":"code","9b4363ef":"code","f69a1fda":"code","20062d7f":"code","49fc745d":"code","9f5d68ed":"code","55776fed":"code","209e1509":"code","8689873b":"code","63b7669f":"code","89e55094":"code","27406507":"code","da42e989":"code","642167c0":"code","7e0fba74":"code","aa53f437":"code","3ab38df7":"code","50c8d591":"code","27e0465d":"code","a2d26946":"code","86ec5c22":"code","389f3272":"code","85837323":"code","6c6087ca":"code","2d659ba5":"code","3a2f7b90":"code","c074b9ba":"code","e4930d3b":"code","635300cb":"code","3c2a4f1c":"code","36f8541b":"code","3f54cc6b":"code","4901d81f":"markdown","5031f3e5":"markdown","94abfc53":"markdown","3fe57b37":"markdown","67f766e0":"markdown","875cb181":"markdown","21dd43ca":"markdown","cf065f11":"markdown","8c4f3d81":"markdown","50efa503":"markdown","8abc27df":"markdown","6250b951":"markdown","99b2e868":"markdown","ae6c9b1d":"markdown","f33497ae":"markdown","cf44eede":"markdown","aab09d4c":"markdown","f4085291":"markdown","f3f4b61e":"markdown","f6c13cc7":"markdown","fa9955ce":"markdown","de250423":"markdown","ed270c1f":"markdown","ec5f8e20":"markdown","87597771":"markdown","ea11d564":"markdown"},"source":{"d9b13e02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nnp.random.seed(1234)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom underthesea import word_tokenize\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Conv1D, MaxPooling1D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense, concatenate, Activation\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nimport gensim\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models.doc2vec import TaggedDocument\nimport multiprocessing\nfrom sklearn import utils\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nplt.style.use('fivethirtyeight')\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Any results you write to the current directory are saved as output.","55df652b":"cols = ['text','sentiment']\ndf_train = pd.read_csv(\"..\/input\/newone\/train.csv\",header=None, names=cols)\ndf_test = pd.read_csv(\"..\/input\/newone\/Test.csv\",header=None, names=cols)\ndf_dev = pd.read_csv(\"..\/input\/newone\/Dev.csv\",header=None, names=cols)","21d92be8":"from keras.utils.np_utils import to_categorical\nlabels_train = to_categorical(df_train.sentiment, num_classes=3)\nlabels_test = to_categorical(df_test.sentiment, num_classes=3)\nlabels_dev = to_categorical(df_dev.sentiment, num_classes=3)\n","e6541df7":"df_train['sentiment']= labels_train\ndf_test['sentiment']= labels_test\ndf_dev['sentiment']= labels_dev\n","463b6fff":"import string\ndef cleaner_update(text):\n    return text.translate(str.maketrans('','', string.punctuation))","17ed17e2":"clean_test = []\nfor i in range(0,len(df_test)):\n    clean_test.append(cleaner_update(df_test['text'][i]))","abff2594":"clean_dev = []\nfor i in range(0,len(df_dev)):\n    clean_dev.append(cleaner_update(df_dev['text'][i]))","04f0c5f1":"clean_train = []\nfor i in range(0,len(df_train)):\n    clean_train.append(cleaner_update(df_train['text'][i]))","0fe3fb22":"tokenize_df=[]\nfor x in clean_test:\n    tokenize_df.append(word_tokenize(x))","8e35ad5f":"for x in clean_train:\n    tokenize_df.append(word_tokenize(x))","9167111b":"for x in clean_dev:\n    tokenize_df.append(word_tokenize(x))","bdd6f441":"tokenize_df","c878ba5c":"words=[]\nfor m in range(0,len(tokenize_df)):\n    for n in range(0,len(tokenize_df[m])):\n        words.append(tokenize_df[m][n])","606684e4":"df_Count = pd.DataFrame(words,columns=['word'])\ndf_Count['Num']= 1","e06ef5bc":"df_GroupBy=df_Count.groupby('word').count()\ndf_GroupBy.sort_values('Num',ascending=False,inplace=True)","56637476":"filename = '..\/input\/stopword\/StopWord.csv'\ndata = pd.read_csv(filename,names=['word'])\nlist_stopwords = data['word']\nmyarray = np.asarray(list_stopwords)\ndef remove_stopword(text):\n    text2=''\n    for x in text:\n        if x in myarray:\n            text2+=\"\"\n        else:\n            text2+=x+ \" \"\n    return text2\nstorage=[]\nfor x in range(0,len(tokenize_df)):\n    storage.append(remove_stopword(tokenize_df[x]))","713395e8":"x_train = pd.Series(clean_train)\ny_train = pd.Series(df_train['sentiment'])","c4196de2":"x_val = pd.Series(clean_dev)\ny_val = pd.Series(df_dev['sentiment'])","4b31bada":"x_test = pd.Series(clean_test)\ny_test = pd.Series(df_test['sentiment'])","d01215b8":"def labelize_text_ug(tweets,label):\n    result = []\n    prefix = label\n    for i, t in zip(tweets.index, tweets):\n        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n    return result","abebd4d4":"all_x = pd.concat([x_train])\nall_x_w2v = labelize_text_ug(all_x, 'all')\n","7106fd49":"all_x_w2v","9a1a6f34":"cores = multiprocessing.cpu_count()\nmodel_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)","36f62eab":"model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])","e47ee2d0":"%%time\nfor epoch in range(30):\n    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n    model_ug_cbow.alpha -= 0.002\n    model_ug_cbow.min_alpha = model_ug_cbow.alpha","06b35645":"model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\nmodel_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])","44a83cd8":"%%time\nfor epoch in range(30):\n    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n    model_ug_sg.alpha -= 0.002\n    model_ug_sg.min_alpha = model_ug_sg.alpha","e0de8c80":"model_ug_cbow.save('w2v_model_ug_cbow.word2vec')\nmodel_ug_sg.save('w2v_model_ug_sg.word2vec')","e835922a":"model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\nmodel_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')","311bbd28":"embeddings_index = {}\nfor w in model_ug_cbow.wv.vocab.keys():\n    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\nprint('Found %s word vectors.' % len(embeddings_index))","9b4363ef":"np.append(model_ug_cbow.wv['slide'],model_ug_sg.wv['slide'])","f69a1fda":"tokenizer = keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^`{|}~ ')\ntokenizer.fit_on_texts(x_train)","20062d7f":"sequences_digit = tokenizer.texts_to_sequences(x_train)","49fc745d":"length = []\nfor x in x_train:\n    length.append(len(x.split()))\nmax(length)\n","9f5d68ed":"x_train_seq = pad_sequences(sequences_digit, maxlen=150)\nprint('Shape of data tensor:', x_train_seq.shape)","55776fed":"sequences_val = tokenizer.texts_to_sequences(x_val)\nx_val_seq = pad_sequences(sequences_val, maxlen=150)","209e1509":"print('Shape of data tensor:', x_val_seq.shape)","8689873b":"num_words = 10000\nembedding_matrix = np.zeros((num_words, 200))\nfor word, i in tokenizer.word_index.items():\n    if i >= num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","63b7669f":"labels_train = to_categorical(df_train.sentiment, num_classes=3)\nlabels_test = to_categorical(df_test.sentiment, num_classes=3)\nlabels_dev = to_categorical(df_dev.sentiment, num_classes=3)","89e55094":"model_cnn = Sequential()\ne = Embedding(10000, 200, weights=[embedding_matrix], input_length=150, trainable=True)\nmodel_cnn.add(e)\nmodel_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\nmodel_cnn.add(GlobalMaxPooling1D())\nmodel_cnn.add(Dense(256, activation='relu'))\nmodel_cnn.add(Dropout(0.2))\nmodel_cnn.add(Dense(3, activation='sigmoid'))\nmodel_cnn.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc',f1_m,precision_m, recall_m])\nhistory_cnn=model_cnn.fit(x_train_seq, labels_train, validation_data=(x_val_seq, labels_dev), epochs=5, batch_size=40, verbose=2)","27406507":"sequences_test = tokenizer.texts_to_sequences(x_test)\nx_test_seq = pad_sequences(sequences_test, maxlen=150)\n","da42e989":"loss_cnn, accuracy_cnn, f1_score_cnn, precision_cnn, recall_cnn = model_cnn.evaluate(x_test_seq, labels_test, verbose=0)","642167c0":"print(loss_cnn, accuracy_cnn, f1_score_cnn, precision_cnn, recall_cnn)","7e0fba74":"model_cnn_lstm = Sequential()\ne = Embedding(10000, 200, weights=[embedding_matrix], input_length=150, trainable=True)\nmodel_cnn_lstm.add(e)\nmodel_cnn_lstm.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\nmodel_cnn_lstm.add(MaxPooling1D())\nmodel_cnn_lstm.add(Dropout(0.2))\nmodel_cnn_lstm.add(LSTM(300))\nmodel_cnn_lstm.add(Dense(256, activation='relu'))\nmodel_cnn_lstm.add(Dense(3, activation='sigmoid'))\nmodel_cnn_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\nhistory_cnn_lstm=model_cnn_lstm.fit(x_train_seq, labels_train, validation_data=(x_val_seq, labels_dev), epochs=5, batch_size=40, verbose=2)","aa53f437":"loss_cnn_lstm, accuracy_cnn_lstm, f1_score_cnn_lstm, precision_cnn_lstm, recall_cnn_lstm = model_cnn_lstm.evaluate(x_test_seq, labels_test, verbose=0)","3ab38df7":"print(loss_cnn_lstm, accuracy_cnn_lstm, f1_score_cnn_lstm, precision_cnn_lstm, recall_cnn_lstm )","50c8d591":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","27e0465d":"model_lstm_cnn = Sequential()\ne = Embedding(10000, 200, weights=[embedding_matrix], input_length=150, trainable=True)\nmodel_lstm_cnn.add(e)\nmodel_lstm_cnn.add(LSTM(300,return_sequences=True))\nmodel_lstm_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\nmodel_lstm_cnn.add(GlobalMaxPooling1D())\nmodel_lstm_cnn.add(Dropout(0.2))\nmodel_lstm_cnn.add(Dense(256, activation='relu'))\nmodel_lstm_cnn.add(Dense(3, activation='sigmoid'))\nmodel_lstm_cnn.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc',f1_m,precision_m, recall_m])\nfilepath=\"LSTM_CNN_best_weights.{epoch:02d}-{val_acc:.41f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory_lstm_cnn=model_lstm_cnn.fit(x_train_seq, labels_train, batch_size=40, epochs=5,\n                     validation_data=(x_val_seq, labels_dev),verbose=2, callbacks=[checkpoint])","a2d26946":"model_lstm_cnn.summary()","86ec5c22":"loss_lstm_Cnn, accuracy_lstm_Cnn, f1_score_lstm_Cnn, precision_lstm_Cnn, recall_lstm_Cnn = model_lstm_cnn.evaluate(x_test_seq, labels_test, verbose=0)","389f3272":"print(loss_lstm_Cnn, accuracy_lstm_Cnn, f1_score_lstm_Cnn, precision_lstm_Cnn, recall_lstm_Cnn)","85837323":"from keras.layers import Bidirectional","6c6087ca":"from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\n","2d659ba5":"model_lstm_cnn_UPDATE = Sequential()\ne = Embedding(10000, 200, weights=[embedding_matrix], input_length=150, trainable=True)\nmodel_lstm_cnn_UPDATE.add(e)\nmodel_lstm_cnn_UPDATE.add((LSTM(300,return_sequences=True,dropout=0.25, recurrent_dropout=0.1)))\nmodel_lstm_cnn_UPDATE.add(Conv1D(filters=128, kernel_size=7, padding='same', activation='relu', strides=1))\nmodel_lstm_cnn_UPDATE.add(MaxPooling1D())\nmodel_lstm_cnn_UPDATE.add(Conv1D(filters=256,kernel_size=5, activation='relu',padding='same',strides=1))\nmodel_lstm_cnn_UPDATE.add(MaxPooling1D())\nmodel_lstm_cnn_UPDATE.add(Conv1D(filters=512, kernel_size=3, activation='relu',padding='same',strides=1))\nmodel_lstm_cnn_UPDATE.add(MaxPooling1D())\nmodel_lstm_cnn_UPDATE.add(Flatten())\nmodel_lstm_cnn_UPDATE.add(Dense(256, activation='relu'))\nmodel_lstm_cnn_UPDATE.add(Dropout(0.2))\nmodel_lstm_cnn_UPDATE.add(Dense(3, activation='sigmoid'))\nmodel_lstm_cnn_UPDATE.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc',f1_m,precision_m, recall_m])\nfilepath=\"LSTM_CNN_best_weights.{epoch:02d}-{val_acc:.41f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory_lstm_cnn_UPDATE=model_lstm_cnn_UPDATE.fit(x_train_seq, labels_train, batch_size=40, epochs=5,\n                     validation_data=(x_val_seq, labels_dev),verbose=2, callbacks=[checkpoint])","3a2f7b90":"model_lstm_cnn_UPDATE.summary()","c074b9ba":"loss, accuracy, f1_score, precision, recall = model_lstm_cnn_UPDATE.evaluate(x_test_seq, labels_test, verbose=0)\nprint(loss, accuracy, f1_score, precision, recall)","e4930d3b":"objects = ('CNN', 'LSTM-CNN', 'CNN-LSTM','LSTM-CNN Multiple Filters')\nperformance1 = [f1_score_cnn,f1_score_lstm_Cnn,f1_score_cnn_lstm,f1_score]\nperformance2 = [loss_cnn,loss_lstm_Cnn,loss_cnn_lstm,loss]\nbarWidth = 0.5\n# Choose the height of the blue bars\ny_pos = np.arange(len(objects))\nr1 = np.arange(len(performance1))\nplt.bar(r1, performance1, width = barWidth, color = 'lightblue', edgecolor = 'black', capsize=7, label='F1-Score')\nplt.ylabel('percent')\nplt.xticks(y_pos, objects)\nplt.legend()\nplt.ylim(0.91,0.95)\nplt.title('Performance Results')\nplt.show()","635300cb":"y_pos = np.arange(len(objects))\nr1 = np.arange(len(performance1))\nplt.bar(r1, performance2, width = barWidth, color =(0.625, 0.21960784494876862,    0.94117647409439087), edgecolor = 'black', capsize=7, label='Loss')\nplt.ylabel('percent')\nplt.xticks(y_pos, objects)\nplt.legend()\nplt.ylim(0.1,0.2)\nplt.title('Performance Results')\nplt.show()","3c2a4f1c":"# summarize history for accuracy\nplt.plot(history_cnn.history['val_acc'], label=\"CNN\")\n#plt.plot(history_cnn.history['val_acc'])\nplt.plot(history_cnn_lstm.history['val_acc'], label=\"CNN-LSTM\")\nplt.plot(history_lstm_cnn.history['val_acc'])\nplt.plot(history_lstm_cnn_UPDATE.history['val_acc'])\n\nplt.title('Validation Accurancy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['CNN', 'CNN-LSTM','LSTM-CNN','LSTM-CNN Mutiple Filters'], loc='lower right')\nplt.show()\n# summarize history for loss\nplt.plot(history_cnn.history['val_loss'])\nplt.plot(history_cnn_lstm.history['val_loss'])\nplt.plot(history_lstm_cnn.history['val_loss'])\nplt.plot(history_lstm_cnn_UPDATE.history['val_loss'])\n\nplt.title('Validation Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['CNN', 'CNN-LSTM','LSTM-CNN','LSTM-CNN Mutiple Filters'], loc='upper left')\nplt.show()","36f8541b":"# summarize history for accuracy\nplt.plot(history_cnn_lstm.history['acc'])\nplt.plot(history_cnn_lstm.history['val_acc'])\nplt.title('CNN-LSTM model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_cnn_lstm.history['loss'])\nplt.plot(history_cnn_lstm.history['val_loss'])\nplt.title('CNN-LSTM model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3f54cc6b":"# summarize history for accuracy\nplt.plot(history_lstm_cnn.history['acc'])\nplt.plot(history_lstm_cnn.history['val_acc'])\nplt.title('LSTM-CNN model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_lstm_cnn.history['loss'])\nplt.plot(history_lstm_cnn.history['val_loss'])\nplt.title('LSTM-CNN model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","4901d81f":"##### np.array_equal(embedding_matrix[5] ,embeddings_index.get('em'))\n\n","5031f3e5":"## Visualize frequency of word (used for stopwords)","94abfc53":"# CNN ","3fe57b37":"## Remove punctuation","67f766e0":"### Validating on DataSet - Dev","875cb181":"### Appending cbow and sg for better result","21dd43ca":"### Test","cf065f11":"## Training build_vocav with 2 method of skipgam and CBOW","8c4f3d81":"https:\/\/github.com\/diegoschapira\/CNN-Text-Classifier-using-Keras\/blob\/master\/models.py","50efa503":"### Validating on DataSet - Dev","8abc27df":"### Build reference for our data","6250b951":"## Tokenize Words","99b2e868":"### Count Words","ae6c9b1d":"### Test","f33497ae":"- Apply it on our data\n- Check result","cf44eede":"# CNN - LSTM","aab09d4c":"### Test","f4085291":"## Vectorize words into numberic (float)","f3f4b61e":"#### Save results","f6c13cc7":"- BiLSTM-CNN (single filters) 0.1289299549046605 0.9568330157202253 0.9351887610692129 0.9361239191255311 0.9343019583823179\n- BiLSTM-CNN (multiple  filters) 0.1425636049809519 0.9547273164599739 0.9321195284023513 0.9318498289065502 0.932406822300680\n- LSTM- CNN - Sing 0.15189910863143163 0.9552537379050963 0.9259114440341426 0.9321842116414936 0.9336702464806153\n- LSTM-CNN -Multi 0.12803751011852668 0.9556748801518752 0.9335047436603925 0.933660056985637 0.9333543902097137\n- CNN 0.17975928141621947 0.9464097755736968 0.9197612686467487 0.9188679342622329 0.9207201517238244\n- CNN-LSTM 0.16549487028856935 0.944093496190997 0.9161467779232442 0.9160085565593524 0.9162981681106818","fa9955ce":"### Validating on DataSet - Dev","de250423":"Take a max length of vector (reason of all inout have to be in a same size of matrix)","ed270c1f":"## Slit data into train,validation and test","ec5f8e20":"## Get Keyed Vectors","87597771":"# LSTM - CNN","ea11d564":"Now, we got our dictionary for taking a task"}}