{"cell_type":{"2e9f61b6":"code","a4418a25":"code","8a6830cd":"code","1f79f3a2":"markdown","3b265054":"markdown","7b1d697a":"markdown"},"source":{"2e9f61b6":"import torch\nfrom torch import nn\n\ninput = torch.randn(3, 5, requires_grad=True) # shape [m samples, n classes]\n# each element in correct_class_index has to have 0 <= value < n (number of classes)\ncorrect_class_index = torch.tensor([3, 0, 4]) # shape [m samples]\n\nCrossEntropyLoss = nn.CrossEntropyLoss()\nLogSoftmax = nn.LogSoftmax(dim=1)\nNLLLoss = nn.NLLLoss(reduction=\"none\")\n\nprint(\"input:\")\nprint(input)\nprint(\"----------------------------------------------\")\nprint(\"input after log softmax:\")\nprint(LogSoftmax(input))\nprint(\"----------------------------------------------\")\n\noutput = NLLLoss(LogSoftmax(input), correct_class_index)\n\nprint(\"correct_class_index:\")\nprint(correct_class_index)\nprint(\"----------------------------------------------\")\nprint(\"For each sample, nn.NLLLoss() takes the correct class's value, and the negative of that value would be a positive value:\")\nprint(output)\nprint(\"----------------------------------------------\")\nprint(\"mean of output:\")\nprint(output.mean())","a4418a25":"NLLLoss = nn.NLLLoss(reduction=\"mean\")\nNLLLoss(LogSoftmax(input), correct_class_index)","8a6830cd":"assert(CrossEntropyLoss(input, correct_class_index) == NLLLoss(LogSoftmax(input), correct_class_index))","1f79f3a2":"# Understanding nn.CrossEntropyLoss\n\n- `nn.CrossEntropyLoss()` combines `nn.LogSoftmax()` and `nn.NLLLoss(reduction=\"mean\")` in one single class.\n\n- `nn.LogSoftmax()` first softmax and then take natural log.\n\n- For `nn.LogSoftmax()`, input shape is `[m samples, n classes]`, output shape is `[m samples, n classes]`.\n\n- Output of `nn.LogSoftmax()` are all negative numbers. For each sample, `nn.NLLLoss()` simply takes the correct class's value, and the negative of that value would be a positive value.\n    \n- We minimize the negative log likelihood so that the probability of choosing the correct class is maximized.","3b265054":"`nn.CrossEntropyLoss()` combines `nn.LogSoftmax()` and `nn.NLLLoss(reduction=\"mean\")` in one single class.","7b1d697a":"The default of `reduction` argument in `nn.NLLLoss` is `\"mean\"`"}}