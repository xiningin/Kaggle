{"cell_type":{"492fe5a7":"code","6c602a43":"code","5fca3dd6":"code","3f1639d1":"code","d1cdf29f":"code","60df9c96":"code","b27ae73b":"code","b075d978":"code","97e77c96":"code","f3c14542":"code","a28d7150":"code","c72832a3":"code","2bcbeda5":"code","456f444d":"code","b54c21c0":"code","4779005d":"code","3f500ade":"code","28a03ea4":"code","654ac959":"code","bdabd2ab":"code","5801e887":"code","bae88654":"code","2c106800":"code","7a92ae57":"code","5f2f1afa":"code","3eeaefa7":"code","afe58e61":"code","076d465d":"code","2676e8eb":"code","7ac44c41":"code","7f72880d":"code","8a07f1d3":"code","1449afb0":"code","29d34fa2":"code","40a4296b":"code","9b9f0dd5":"code","7ecf1469":"code","9b6609a2":"code","946c92c4":"code","67247cbe":"code","319a3d3f":"code","08fd51ba":"code","982d2c02":"markdown","97ee3291":"markdown","85dbb85a":"markdown","32ed1518":"markdown"},"source":{"492fe5a7":"import os\nroot_dir = \"\"\nproject_dir =  \"\"","6c602a43":"#!mkdir gdrive\/'My Drive'\/ML\/APTOS2019\/\n#!ls {project_dir}","5fca3dd6":"#!cp {project_dir}\/dogs_gan.ipynb gdrive\/'My Drive'\/ML\/APTOS2019\/","3f1639d1":"data_dir = \"..\/input\/\"","d1cdf29f":"!ls {data_dir}\n#!unzip {data_dir}\/all-dogs.zip -d {data_dir}\n#!unzip {data_dir}\/Annotation.zip -d {data_dir}\n#!ls","60df9c96":"import sys\n#sys.path.append(\"gdrive\/My Drive\/ML\/\")\n#import download_utils","b27ae73b":"import tensorflow as tf\nimport keras\nfrom keras import backend as K\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport cv2  # for image processing\nimport scipy.io\nimport os\n#import keras_utils\n#from keras_utils import reset_tf_session \nprint(tf.__version__)\nprint(keras.__version__)","b075d978":"plt.rcParams.update({'axes.titlesize': 'small'})","97e77c96":"def reset_tf_session():\n    curr_session = tf.get_default_session()\n    # close current session\n    if curr_session is not None:\n        curr_session.close()\n    # reset graph\n    K.clear_session()\n    # create new session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    s = tf.InteractiveSession(config=config)\n    K.set_session(s)\n    return s","f3c14542":"IMG_SIZE = 128","a28d7150":"#!ls ..\/input\/all-dogs\/all-dogs","c72832a3":"dogs_imgages = \"..\/input\/all-dogs\/all-dogs\/\"\nannotations = \"..\/input\/annotation\/\"","2bcbeda5":"#!ls {data_dir}\/all-dogs","456f444d":"import glob","b54c21c0":"images = [f for f in glob.glob(dogs_imgages + \"*.jpg\")]\nannotations = [f for f in glob.glob(annotations + \"*\/n*\")]","4779005d":"print (images[:5])\nprint (annotations[:2])","3f500ade":"from PIL import Image\nfrom matplotlib import pyplot as plt\nimport xml.etree.ElementTree as ET\nimport random\n","28a03ea4":"images_to_display = random.choices(images, k=64)\n\nfig = plt.figure(figsize=(25, 16))\nfor ii, img in enumerate(images_to_display):\n    ax = fig.add_subplot(8, 8, ii + 1, xticks=[], yticks=[])\n    img = cv2.imread(img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    #img = Image.open(img_byte)\n    plt.imshow(img)","654ac959":"img = cv2.imread(images[100])\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nprint(img.shape)\n#cv2_imshow(img)\nfig = plt.figure(figsize=(4,8))\nplt.imshow(img)\nplt.show()","bdabd2ab":"def get_bbox(dog_id):\n  breed_folder = [x.rsplit('\/', 1)[0] for x in annotations if dog_id in x.split('\/')[-1] ]\n  #print (breed_folder)\n  if len(breed_folder) != 1:\n    return None\n  breed_folder = breed_folder[0]\n  file_name = \"{}\/{}\".format(breed_folder, dog_id)\n  #print (file_name)\n  root = ET.parse(file_name).getroot()\n  objects = root.findall('object')\n  for obj in objects:\n      bndbox = obj.find('bndbox')\n      xmin = int(bndbox.find('xmin').text)\n      ymin = int(bndbox.find('ymin').text)\n      xmax = int(bndbox.find('xmax').text)\n      ymax = int(bndbox.find('ymax').text)\n  bbox = (xmin, ymin, xmax, ymax)\n  #print(\"Bounding Box: \", bbox)\n  return bbox\n  \n  ","5801e887":"dog_id = 'n02109961_16718'","bae88654":"box = get_bbox(dog_id)","2c106800":"def get_annotated_img(img_file):\n  dog_id = img_file.split('\/')[-1].split('.')[0]\n  #raw_bytes = read_raw_from_zip(zip_dogs,\"all-dogs\/{}.jpg\".format(dog_id))\n  #img = decode_image_from_raw_bytes(raw_bytes)\n  img = cv2.imread(img_file)\n  bbox = get_bbox(dog_id)\n  if bbox:\n    xmin, ymin, xmax, ymax = bbox\n    img = img[ymin:ymax,xmin:xmax]\n  img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n  img = img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  return img\n  ","7a92ae57":"img = get_annotated_img(images[4])\nimg.shape","5f2f1afa":"fig = plt.figure()\nplt.imshow(img)\nplt.show()","3eeaefa7":"import tensorflow as tf\n#from keras_utils import reset_tf_session\ns = reset_tf_session()\n\nimport keras\nfrom keras.models import Sequential\nfrom keras import layers as L","afe58e61":"import random\nfrom scipy import ndarray\nimport skimage as sk\nfrom skimage import transform\nfrom skimage import util\nfrom copy import deepcopy\ndef random_rotation(image_array: ndarray):\n    # pick a random degree of rotation between 25% on the left and 25% on the right\n    random_degree = random.uniform(-25, 25)\n    return sk.transform.rotate(image_array, random_degree)\n\ndef random_noise(image_array: ndarray):\n    # add random noise to the image\n    return sk.util.random_noise(image_array)\n\ndef horizontal_flip(image_array: ndarray):\n    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n    return image_array[:, ::-1]","076d465d":"def generate_training_images(images, batch_size=100):\n  cur_batch = []\n  for image in images:\n    img = get_annotated_img(image)\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE) ) \n    cur_batch.append(img)\n    if len(cur_batch) == batch_size:\n      yield cur_batch\n      cur_batch = []\n    cur_batch.append(random_rotation(deepcopy(img) ))\n    if len(cur_batch) == batch_size:\n      yield cur_batch\n      cur_batch = []\n    cur_batch.append(random_noise(deepcopy(img) ))\n    if len(cur_batch) == batch_size:\n      yield cur_batch\n      cur_batch = []\n    cur_batch.append(horizontal_flip(deepcopy(img) ))\n    if len(cur_batch) == batch_size:\n      yield cur_batch\n      cur_batch = []\n  return cur_batch","2676e8eb":"IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)","7ac44c41":"CODE_SIZE =256\ndim = 8\ndepth = 64*4\ndropout = 0.4\n\ngenerator = Sequential()\n\n#layer 1  : Dense Layer with BatchNormalization and relu activation\ngenerator.add(L.Dense(dim * dim * depth, \n                      input_shape=(CODE_SIZE,)))\ngenerator.add(L.LeakyReLU(alpha=0.2))\ngenerator.add(L.BatchNormalization(momentum=0.8))\n\n\n#layer2 :  Reshape and Dropout\ngenerator.add(L.Reshape((dim, dim, depth)))\n#generator.add(L.Dropout(dropout))\n\n\n#layer 3 : Transpose conv layer\ngenerator.add(L.Deconv2D(int(depth\/2), \n                         kernel_size=(5,5)))\ngenerator.add(L.BatchNormalization(momentum=0.8))\ngenerator.add(L.LeakyReLU(alpha=0.2))\n\n\n\n\n#layer 4 : upsample conv layer\ngenerator.add(L.UpSampling2D())\ngenerator.add(L.Deconv2D(int(depth\/4), \n                         kernel_size=(5,5)))\ngenerator.add(L.Dropout(dropout))\ngenerator.add(L.BatchNormalization(momentum=0.8))\ngenerator.add(L.LeakyReLU(alpha=0.2))\n\n\ngenerator.add(L.Deconv2D(int(depth\/4), \n                         kernel_size=(5,5)))\ngenerator.add(L.Dropout(dropout))\ngenerator.add(L.BatchNormalization(momentum=0.8))\ngenerator.add(L.LeakyReLU(alpha=0.2))\n\n\n#layer 5 : upsample conv layer\ngenerator.add(L.UpSampling2D(size=(2,2)))\ngenerator.add(L.Deconv2D(int(depth\/4), \n                         kernel_size=(5,5)))\ngenerator.add(L.Dropout(dropout))\ngenerator.add(L.BatchNormalization(momentum=0.8))\ngenerator.add(L.LeakyReLU(alpha=0.2))\n\n#layer 5 : Image generation Layer\ngenerator.add(L.Conv2D(3, kernel_size=5, activation=None))\ngenerator.add(L.UpSampling2D())","7f72880d":"#generator = load_model('.\/G')\ngenerator.summary()","8a07f1d3":"discriminator = Sequential()\ndepth = 64\ndropout = 0.4\n\ndiscriminator.add(L.InputLayer(IMG_SHAPE))\n\ndiscriminator.add(L.Conv2D(depth, (3, 3),padding='same') )\ndiscriminator.add(L.LeakyReLU(0.1))\n\ndiscriminator.add(L.Conv2D(depth*2, (3, 3)))\ndiscriminator.add(L.LeakyReLU(0.1))\ndiscriminator.add(L.Dropout(dropout))\n\ndiscriminator.add(L.MaxPool2D())\n\ndiscriminator.add(L.Conv2D(depth*3, (3, 3)))\ndiscriminator.add(L.LeakyReLU(0.1))\n\ndiscriminator.add(L.MaxPool2D())\n\ndiscriminator.add(L.Conv2D(depth, (3, 3)))\ndiscriminator.add(L.LeakyReLU(0.1))\ndiscriminator.add(L.Dropout(dropout))\n\ndiscriminator.add(L.MaxPool2D())\ndiscriminator.add(L.Flatten())\ndiscriminator.add(L.Dense(512,activation='tanh'))\ndiscriminator.add(L.Dense(CODE_SIZE,activation='tanh'))\ndiscriminator.add(L.Dense(2, activation=tf.nn.log_softmax))","1449afb0":"#discriminator = load_model('.\/D')\ndiscriminator.summary()","29d34fa2":"noise = tf.placeholder('float32',[None,CODE_SIZE])\nreal_data = tf.placeholder('float32',[None,]+list(IMG_SHAPE))\n\nlogp_real = discriminator(real_data)\n\ngenerated_data = generator(noise)\n\nlogp_gen = discriminator(generated_data)","40a4296b":"########################\n#discriminator training#\n########################\n\nd_loss = -tf.reduce_mean(logp_real[:,1] + logp_gen[:,0])\n\n#regularize\nd_loss += tf.reduce_mean(discriminator.layers[-1].kernel**2)\n\n#optimize\ndisc_optimizer =  tf.train.GradientDescentOptimizer(1e-3).minimize(d_loss, var_list=discriminator.trainable_weights)","9b9f0dd5":"########################\n###generator training###\n########################\n\ng_loss = -tf.log(1-logp_gen)\n\ngen_optimizer = tf.train.AdamOptimizer(1e-4).minimize(g_loss,var_list=generator.trainable_weights)\n\n    ","7ecf1469":"def sample_noise_batch(bsize):\n    return np.random.normal(size=(bsize, CODE_SIZE)).astype('float32')\n\ndef sample_data_batch(bsize):\n    idxs = np.random.choice(np.arange(len(images)), size = int(bsize\/2) )\n    cur_batch = []\n    for idx in idxs:\n        img = get_annotated_img(images[idx]) \n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE) )\n        #print (img.shape)\n        cur_batch.append( img)\n        \n    idxs = np.random.choice(np.arange(len(images)), size= bsize-int(bsize\/2) )\n    for idx in idxs:\n        img = get_annotated_img(images[idx]) \n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE) )\n        if idxs[0]%3 == 0:\n          cur_batch.append( random_rotation(img))\n        elif idxs[0]%3 == 1:\n          cur_batch.append( random_noise(img))\n        else:\n          cur_batch.append( horizontal_flip(img))\n        #print (img.shape)\n    return np.array(cur_batch)\n\ndef sample_images(nrow,ncol, sharp=False):\n    images = generator.predict(sample_noise_batch(bsize=nrow*ncol))\n    for i in range(nrow*ncol):\n        plt.subplot(nrow,ncol,i+1)\n        if sharp:\n            plt.imshow(images[i].reshape(IMG_SHAPE),cmap=\"gray\", interpolation=\"none\")\n        else:\n            plt.imshow(images[i].reshape(IMG_SHAPE),cmap=\"gray\")\n            \n    plt.show()\n\ndef sample_probas(bsize):\n    plt.title('Generated vs real data')\n    plt.hist(np.exp(discriminator.predict(sample_data_batch(bsize)))[:,1],\n             label='D(x)', alpha=0.5,range=[0,1])\n    plt.hist(np.exp(discriminator.predict(generator.predict(sample_noise_batch(bsize))))[:,1],\n             label='D(G(z))',alpha=0.5,range=[0,1])\n    plt.legend(loc='best')\n    plt.show()","9b6609a2":"#import tqdm_utils","946c92c4":"#!ls {data_dir}\ns.run(tf.global_variables_initializer())","67247cbe":"from time import time\ninit_time = time()","319a3d3f":"from tqdm import tqdm_notebook as tqdm","08fd51ba":"from IPython import display\n\nfor epoch in tqdm(range(50000)):\n    \n    feed_dict = {\n        real_data:sample_data_batch(100),\n        noise:sample_noise_batch(100)\n    }\n    \n    for i in range(5):\n        s.run(disc_optimizer, feed_dict)\n    \n    s.run(gen_optimizer,feed_dict)\n    fl = 0\n    if epoch %100==0 and epoch > 0:\n        display.clear_output(wait=True)\n        sample_images(2,3,True)\n        sample_probas(1000)\n        if epoch%500 ==0 and epoch > 0:\n            generator.save('G_{}'.format(epoch))\n            discriminator.save('D_{}'.format(epoch))\n            if int((time() - init_time)\/3600) > 6:\n                fl = 1\n                break\n    if fl:\n        break\n        \n        ","982d2c02":"## Create Necessry directories in order to structure the project in your google drive\n##our directory structure will be \n\n\n              gdrive\/'My Drive'\/ML\/\n                                  |\n                                  DOGS_GAN\n                                  |-- README.md\n                                  |-- data\n                                  |   |-- all-dogs.zip\n                                  |   |-- Annotations.zip\n                                  |   \n                                  |-- dogs_gan.ipynb\n","97ee3291":"## Install kaggle commands on system to download the dataset\n\n\n*   Make sure you have registered for the kaggle from the same comptetion\n*   Download the  kaggle.json credential file into your ML folder which will be needed to authorize  your download requests for the datasets.\n* copy the kaggle.json credential file .\/kaggle directory  and change permissions\n\n","85dbb85a":"### Download the data set from Kaggle to the data directory directly using command below","32ed1518":"## Kickstart the project\n\n#### Now that we have our working directory setup we can start working with our project\n#### Step 1 is to append the current root directory to python path so that we can import  necessary libraries and python functions from our script\n\n#### Step 2 Below we are importing  kerasutil which is in our root directory"}}