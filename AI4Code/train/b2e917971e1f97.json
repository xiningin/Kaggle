{"cell_type":{"2160afd9":"code","7ee8036a":"code","6d0e664e":"code","7a95550a":"code","cec410eb":"code","a658d59f":"code","d4c07a2f":"code","9ec82beb":"code","4f47b527":"code","d3f2fb6e":"code","6e5cf6c9":"code","790f057e":"code","ebe60bcd":"code","f4cd9f24":"code","3870d773":"code","9a801a9c":"code","31109681":"code","6448aadd":"code","80948f2a":"code","c310f21f":"code","80711935":"markdown","f4a8ae6f":"markdown","717a1235":"markdown","f1e9a59e":"markdown","11070d8c":"markdown","71c10f76":"markdown","205d9330":"markdown","429b3b3f":"markdown"},"source":{"2160afd9":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\nimport tensorflow as tf ","7ee8036a":"df_train=pd.read_csv('\/kaggle\/input\/stumbleupon\/train.tsv',sep='\\t')\n\ndf_test=pd.read_csv('\/kaggle\/input\/stumbleupon\/test.tsv',sep='\\t',usecols=['urlid','boilerplate'])","6d0e664e":"df_train.head()\n\n","7a95550a":"df_test.head()","cec410eb":"\ndf_train.columns","a658d59f":"plt.figure(figsize=(15,10))\nsns.countplot(x=df_train['alchemy_category'],hue=df_train['label']);\nplt.xlabel('Category');\nplt.xticks(rotation=90);","d4c07a2f":"df_train['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_train['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_train['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_train['boilerplate']=df_train['boilerplate'].str.lower()\n\n\n#Cleaning the test dataframe \n\ndf_test['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_test['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_test['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_test['boilerplate']=df_test['boilerplate'].str.lower()\n","9ec82beb":"from transformers import AutoTokenizer, TFAutoModel\n\n\n#Downloading the tokenizer and the Albert model for fine tuning\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nbert=TFAutoModel.from_pretrained('bert-base-uncased')","4f47b527":"#ADD all the variable for the Transformer model \n# because bert base uncased Model can only handle upto 512 tokens at a time\nSEQ_length=512\n\n#Lets create the X and Y matrix from the Df train set \n\nXids=np.zeros((df_train.shape[0],SEQ_length))\nXmask=np.zeros((df_train.shape[0],SEQ_length))\ny=np.zeros((df_train.shape[0],1))\n\n#Preparing the test dataframe\n\nXids_test=np.zeros((df_test.shape[0],SEQ_length))\nXmask_test=np.zeros((df_test.shape[0],SEQ_length))\nXids\n\n\n","d3f2fb6e":"for i,sequence in enumerate(df_train['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids[i,:],Xmask[i,:],y[i,0]=tokens['input_ids'],tokens['attention_mask'],df_train.loc[i,'label']\n    \n\nfor i,sequence in enumerate(df_test['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids_test[i,:],Xmask_test[i,:]=tokens['input_ids'],tokens['attention_mask']","6e5cf6c9":"#Check if the GPU is avalaible\ntf.config.get_visible_devices()","790f057e":"dataset=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask},labels\n\ndataset=dataset.map(map_func)\ndataset=dataset.shuffle(100000).batch(64).prefetch(1000)\n\nDS_size=len(list(dataset))\n\n\ntrain=dataset.take(round(DS_size*0.90))\nval=dataset.skip(round(DS_size*0.90))\n","ebe60bcd":"#Preparing the test dataset\n\ndataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n\ndef map_func(input_ids,mask):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ndataset_test=dataset_test.map(map_func)\ndataset_test=dataset_test.batch(64).prefetch(1000)","f4cd9f24":"from transformers import TFDistilBertModel, DistilBertConfig\ndistil_bert = 'distilbert-base-uncased'\n\nconfig = DistilBertConfig(dropout=0.4, attention_dropout=0.4)\nconfig.output_hidden_states = False\ntransformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n\ninput_ids_in = tf.keras.layers.Input(shape=(SEQ_length,), name='input_ids', dtype='int32')\ninput_masks_in = tf.keras.layers.Input(shape=(SEQ_length,), name='attention_mask', dtype='int32') \n\nembedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\nX = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.4, recurrent_dropout=0.4))(embedding_layer)\nX = tf.keras.layers.GlobalMaxPool1D()(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.3)(X)\nX = tf.keras.layers.Dense(32, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.3)(X)\nX = tf.keras.layers.Dense(1, activation='sigmoid')(X)\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\nfor layer in model.layers[:3]:\n  layer.trainable = False\n","3870d773":"model.summary()","9a801a9c":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer='adam',metrics=[tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()\n])","31109681":"history=model.fit(train,validation_data=val,epochs=10)","6448aadd":"predictions=model.predict(dataset_test)\ndf_test['label']=predictions\n\ndf_test.to_csv('submission.csv',columns=['urlid','label'],index=False)","80948f2a":"input_x=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ninput_x=input_x.map(map_func)\ninput_x=input_x.shuffle(100000).batch(32).prefetch(1000)\n\ny_true = y\ny_true","c310f21f":"y_pred=model.predict(dataset)\ny_pred\n\n\ny_pred = np.round(y_pred)\ny_pred\n\n\nfrom sklearn import metrics\nprint(metrics.classification_report(y_true, y_pred))","80711935":"**Data**\n\nThere are two components to the data provided for this challenge:\n\nThe first component is two files: train.tsv and test.tsv. Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark.\n\ntrain.tsv is the training set and contains 7,395 urls. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) are provided for this set. test.tsv is the test\/evaluation set and contains 3,171 urls.\n\n","f4a8ae6f":"**Cleaning the boilerplate text**\n\nLets remove the title and url word from each description . We will also lower case the words as we are planing to used a uncased version of Transformer model","717a1235":"Precision and recall for both class","f1e9a59e":"Feel free to comment down for any doubts. If you find this notebook helpful, do share and upvote!!\n","11070d8c":"**Approach**\n\nInstead of going for traditional methods, my approach is to try out the effect of the new advances in NLP technology. So to tackle this problem, I use Transformer architecture (BERT) from Hugging face library and TensorFlow 2.0 . Combining this two allows me to build a powerful state of the art model for the classification task.\n\nFurther to improve the score, I attach a bidirectional LSTM (BiLSTM) before passing the embeddings to the Dense Neural Network model.\n\nBiLSTM helps to significantly improves the score. With a few tweaks on hyper parameters, using learning rate scheduler, etc, one can achieve a score of over 0.89 and beat the current SOTA.\n\n","71c10f76":"**Problem Statement**\n\nStumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as \"ephemeral\" or \"evergreen\". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of \"ephemeral\" or \"evergreen\" would greatly improve a recommendation system like ours.\n\nMany people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.","205d9330":"Alchemy catergory does have a role in determining the label for the article\n\nWe see that business, Recreation and health are more likley to be evergreen\n\nWhere as sports computer_internet and arts and entertainment are more like to be non-evergreen.","429b3b3f":"Prediction on test data"}}