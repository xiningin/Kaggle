{"cell_type":{"d7152ccf":"code","bda2b47d":"code","13214b67":"code","dfb4096a":"code","44490936":"code","9fe00630":"code","a6617405":"code","77cd8f0f":"code","05584eea":"code","8367604b":"code","b7c98e49":"code","7bc01888":"code","21490e4f":"code","d72c689e":"code","7506facf":"code","9a7f67f6":"code","32c8f635":"code","dbd515ec":"code","8a3a8f17":"code","0d8a6d96":"code","99fa0936":"code","3cb025b8":"code","d6b35d8b":"code","4402cd4f":"code","f16e5a10":"code","d0b5122f":"code","4f6cfc66":"code","70344835":"code","a56e8e67":"code","a26f2f67":"markdown","5decdab8":"markdown","29cce6a4":"markdown","c658125c":"markdown","b7c87645":"markdown","146b2918":"markdown","dff64fc5":"markdown","d6f36ac7":"markdown","07024850":"markdown","cf12b965":"markdown","0f205ea1":"markdown","ab8b57df":"markdown","a35f9c26":"markdown","a89127f9":"markdown","e2ed0e3e":"markdown","fd268e36":"markdown","dfc23fca":"markdown","316c6449":"markdown","588cd016":"markdown","32b2bb68":"markdown","01e0987d":"markdown","6716e33b":"markdown","2c53d9db":"markdown","d18c4f5b":"markdown","b69c072f":"markdown","14870825":"markdown","991a3c8a":"markdown","7882eaa8":"markdown","2d321ca9":"markdown","d1f5d84c":"markdown","05a77d15":"markdown","86966667":"markdown","9ab76a5d":"markdown"},"source":{"d7152ccf":"import numpy as np\nimport pandas as pd\nimport plotly.express as pltx\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import LabelEncoder\n\n# Upload the dataset\ndata = pd.read_csv(r'..\/input\/travel-insurance-prediction-data\/TravelInsurancePrediction.csv')\n\n# Drop the unnamed field\ndata.drop('Unnamed: 0',axis=1,inplace=True)\n\n# View the first 5 rows\ndata.head()","bda2b47d":"# Explore more in-depth information\nprint('Information overview:')\ndata.shape\ndata.info()","13214b67":"print('Sum of the amount of null values in a column:')\ndata.isnull().sum()","dfb4096a":"print('Describe the information:')\ndata.describe()","44490936":"# Count unique values in the categorical columns\ncatcols = data.select_dtypes(include='object')\nnumcolsgrouped = data[['ChronicDiseases', 'TravelInsurance']]\nallgrouped = pd.concat([catcols, numcolsgrouped], axis=1)\n\nfor col in allgrouped:\n    valcounts = allgrouped[col].value_counts()\n    if len(valcounts) == 2:\n        print('Column: ' + col + 'is sane.')\n    else:\n        print('Column: ' + col + 'is not sane. More information below:')\n        print(valcounts)","9fe00630":"# Split the dataset into numerical and categorical columns\n# Label encoding\ncatenc = catcols.apply(LabelEncoder().fit_transform)\n\n# Get all the other columns\nnumcols = data.select_dtypes(include='int64')\ndata_enc = pd.concat([catenc, numcols], axis=1)\ndata_enc.head()","a6617405":"data = data.rename(columns={\"Employment Type\": \"EmploymentType\"})\ndata_enc = data_enc.rename(columns={\"Employment Type\": \"EmploymentType\"})","77cd8f0f":"# Amount of people who will take the travel insurance\npltx.pie(data, names='TravelInsurance', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='How many customers purchase the insurance?')","05584eea":"# Correlation heatmap\npltx.imshow(data_enc.corr(), color_continuous_scale='dense', title='Correlation between the features')","8367604b":"# Create the histograms\npltx.histogram(data, x='EmploymentType', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='Is employment type important in this equation?')","b7c98e49":"pltx.histogram(data, x='AnnualIncome', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='Does the annual income play an important role?')","7bc01888":"pltx.histogram(data, x='FamilyMembers', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='Big families, small families, who buys the insurance?')","21490e4f":"pltx.histogram(data, x='EverTravelledAbroad', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='Long trips require.. insurance!')","d72c689e":"pltx.histogram(data, x='FrequentFlyer', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='Frequent flyers, are they frequent customers too?')","7506facf":"pltx.histogram(data, x='Age', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='When do people travel the most?')","9a7f67f6":"pltx.histogram(data, x='GraduateOrNot', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='Does education have an impact on the decision?')","32c8f635":"pltx.histogram(data, x='ChronicDiseases', color='TravelInsurance', color_discrete_map={1:'#acc8fc', 0:'#6f6cd4'}, title='How does the health impact the decision?')","dbd515ec":"# Extracting the most wealthy age groups and ones with the most people with annual income over 3.5M\nage_inc_data = age_over_3m = data[['Age', 'AnnualIncome']]\nage_mean_income = age_inc_data[['Age', 'AnnualIncome']].groupby('Age').mean().reset_index()\n\nage_highest_inc = age_inc_data[age_inc_data['AnnualIncome'] > 1300000].groupby('Age').size().reset_index(name='counts')\n\n# Create the plots\npltx.bar(age_mean_income, x='Age', y='AnnualIncome', color_discrete_sequence=['#6f6cd4'], title='Average income by age.')","8a3a8f17":"pltx.bar(age_highest_inc, x='Age', y='counts', color_discrete_sequence=['#6f6cd4'], title='Which age group has the most people who earn >1.3M?')","0d8a6d96":"pltx.histogram(data, x='AnnualIncome', color='EmploymentType', color_discrete_map={'Government Sector':'#acc8fc', 'Private Sector\/Self Employed':'#6f6cd4'}, title='Annual income and employemnt type.')","99fa0936":"pltx.histogram(data, x='FrequentFlyer', color='EmploymentType', color_discrete_map={'Government Sector':'#acc8fc', 'Private Sector\/Self Employed':'#6f6cd4'}, title='Who flies more frequently?')","3cb025b8":"pltx.histogram(data, x='Age', color='EmploymentType', color_discrete_map={'Government Sector':'#acc8fc', 'Private Sector\/Self Employed':'#6f6cd4'}, title='The age range of the gov. sector.')","d6b35d8b":"gov_emp = data.index[data['EmploymentType'] == 'Government Sector'].tolist()\nprv_emp = data.index[data['EmploymentType'] == 'Private Sector\/Self Employed'].tolist()\nhigh_inc = data.index[(data['AnnualIncome'] > 1300000) & (data['Age'] > 26) & (data['Age'] < 34)].tolist()\n\ncorr_gov = list(set(gov_emp).intersection(high_inc))\ncorr_prv = list(set(prv_emp).intersection(high_inc))\nprint(len(corr_gov), 'of', len(gov_emp), 'gov. employees have an income higher than 1.3M')\nprint(len(corr_prv), 'of', len(prv_emp), 'prv. employees have an income higher than 1.3M')","4402cd4f":"pltx.histogram(data, x='AnnualIncome', color='FrequentFlyer', color_discrete_map={'Yes':'#acc8fc', 'No':'#6f6cd4'}, title='Rich people are closer to the sky.')","f16e5a10":"pltx.histogram(data, x='EverTravelledAbroad', color='FrequentFlyer', color_discrete_map={'Yes':'#acc8fc', 'No':'#6f6cd4'}, title='Frequency of air travel vs going abroad.')","d0b5122f":"pltx.histogram(data, x='AnnualIncome', color='GraduateOrNot', color_discrete_map={'Yes':'#acc8fc', 'No':'#6f6cd4'}, title='Employment type and age.')","4f6cfc66":"# Big family, big spendings\ndef f(row):\n    if row['AnnualIncome'] > 800000 and row['FamilyMembers'] >= 5:\n        return 1\n    else:\n        return 0\n    \ndata_enc['BigFamily'] = data_enc.apply(f, axis=1)","70344835":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom imblearn.under_sampling import RandomUnderSampler\n\ny = data_enc['TravelInsurance']\nX = data_enc.drop(['TravelInsurance'], axis=1)\n\n# Undersample the majority\nundersample = RandomUnderSampler(sampling_strategy=0.8)\nX_over, y_over = undersample.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.25)\n\n# Create the pipeline\nclf = Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC(probability=True))])\nclf.fit(X_train, y_train)","a56e8e67":"# Plot the ROC\ny_score = clf.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\n\nroc_fig = pltx.area(\n    x=fpr, y=tpr,\n    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=500\n)\nroc_fig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\n\nroc_fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nroc_fig.update_xaxes(constrain='domain')\nroc_fig.show()\n\n# Cross validation\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3)\n\n# Plot the cross validation scores\nscores = cross_val_score(clf, X, y, cv=cv, scoring='roc_auc')\ncv_fig = pltx.line(scores, title='Cross validation scores, mean of: ' + str(round(np.mean(scores), 5)) + ' AUC')\ncv_fig.show()","a26f2f67":"It appears that we are dealing with:\n- 6 Numerical columns\n- 4 Categorical columns\n- A clean dataset with no empty fields\n\nAll of the numerical columns must be positive but not strictly (0s are okay in all cases). The categorical columns must belong to groups, all of them will undergo binary encoding. Let's see the unique value counts in each of the columns (both types) where information needs to be grouped. We don't want a situation where the options are 1 or 0 but we somehow get a 3 as input.","5decdab8":"*****Thank you for reading! Upvote the notebook if you like my work or the wild CV graph \ud83c\udf3f*****","29cce6a4":"One more graph for the employment type. I simply wanted to see if there is a correlation between the two since we know that the mid of the graph when it comes to age has a drop in income above 1.3M. We also saw that there is a huge increase in people above that income who bought the insurance. Here we can see that some age groups of the range do in fact work for the government, our speculations so far are confirmed.\n\nGov. employees have lower income and are mostly mid age, and we know that the middle of our scale does not have many people with an income above 1.3M. Now I will only look for the information on how much of the people in that range are gov. employees.","c658125c":"It is very commmon in India to be a graduate so I'm afraid that we won't get much from this information alone. As we can see on the graph above the distribution is similar in most places. The degrees vary in the income after graduations, importance of the school is also a factor, we can leave this part of the data set alone.","b7c87645":"Very important information, it seems like our main customers are in their late 20s and early 30s but only the second group is interested in purchasing the insurance. Could be related to life experience, higher income or the offer being more suitable for people with kids that grew anough to travel.","146b2918":"Here we can see another information that will help us identify our target audience. Annual income seems to make a big difference, we can see a very sudden drop in customer who do not purchase an insurance after crossing the 1.3M mark. It will be important for us to explore this infomration and see if there is any correlation betweem the income and other features. Who are the people with the highest income? And why our product is not acessible for lower-income customers? Or maybe it is that they already have that need covered by something else? If there is a correlation between the lower income and working in the government sector for example. Lower income --> less travel --> if travel would happen, there is a higher chance of it being work-related and then the insurance might be covered by the government. Of course that is nothing we can prove as we don't have enough information, but normally I think going in that kind of direction would make sense.","dff64fc5":"We have singificantly less people employed in the government sector, and the amount of people purchasing a travel insurance is smaller than the private sector.\nIn private sector the percentage of all the people is 40%, where in the government sector it is 24%.","d6f36ac7":"These two graphs helped us to identify our first groups of customers. We already knew that the people we are reaching at the moment belong to the more wealthy kind, now we also notice that they are either in their mid-late 20s or early 30s. There also seems to be a drop in the middle of the second graph but I will ignore it because it is most likely related to the economic situation and exploring that topic is not really relevant to the question we are asking.","07024850":"### 5.2 Evaluation","cf12b965":"This graph looks very similar to our income vs annual income one, and it seems like for a good reason. People with more income tend to travel more frequently and so they are also more likely to buy an insurance.","0f205ea1":"Another interesting finding, it looks like people who did travel abroad are more likely to be frequent flyers, about 44.21% of them in fact! That is probably because the travel to foreign countries is more expensive and, as we know already, the income plays a huge role in our problem. The data set does not specify if they travelled abroad by a plane but I think we can assume it considering the size of the country. ","ab8b57df":"### 3.3 Conclusion\nThe conclusion here is easy to make, the interest around the insurance comes down to the budget. We don't know if the company reaches its target audience or not, it could be that the service is targeted to the mid-upper income group of people. If it is not, then the question is: who is the service not reaching and why?\nFrom the EDA we can tell that the company could target a new offer, with a more appropriate price if needed, to people who work in the government sector, are between 26-34 and don't move around a lot. With travel being a global industry where anyone can find something for themselves, a smaller insurance package for casual travel or a coupon for the first timers would most likely do well.\n\nI said \"if needed\" because there is one more factor to consider. What if the service is approprietly priced for what it offers but the customers are not informed about it? Let's think about this case scenario.. You're going on a trip for the first time and finalize your flight ticket purchase, the airline offers a travel insurance but there is not much said about it aside of a few general sentences, the option is placed next to some on-board food set and other less relevant options. Would you be inclined to buy it or skip to the next page? This is a real life example that happened to me this year with one company when I was travelling to the Netherlands. Travel insurance was an option, but I had to look really into it to discover how it can really benefit me.\n\nThis is why I'd suggest to do a study on what customers really know about the insurance and why they purchase it. Is it because they have spare money and buy it just in case like they buy water or is it a decision based on the value it brings to them? I think no better offer will beat information and good marketing, but that is something we can't really measure in this scenario.\n\nWe identified the groups the company's service reaches, and those that it doesn't, therefore the goal has been reached and now I'm going to move into building a model that can predict possible interest in buying the insurance.","a35f9c26":"First of all, we have to notice one thing, this column is named \"Frequent Flyer\", that does not imply that:\n- The company offers only insurance for flights. It kinda does look like it but we simply do not have enough proof, this of course wouldn't be an issue in a real life scenario, but we have to work with what we got.\n- The flights are international.\n\nIn our case this two things won't matter that much (perhaps later in the EDA), since we can easily see that no matter the situation, the government sector simply doesn't travel a lot, neither locally or internationally. That of course doesn't mean that they do not travel in other ways, do not get it twisted. I did in fact checked the India's goverment jobs insurance information at this point to see if I can find anything about travel insurance but nothing came up (aside of the general insurace that covers hospitals etc).","a89127f9":"### 3.2 Looking for relationships\nIn this section I'm going to go trough the information we got from the first section and do a more in-depth analysis on what can cause certain money-spending behaviours in clients. We already know that there is a clear correlation between the annual income and travel insurance clients, let's look further!","e2ed0e3e":"# 3. Explorative Data Analysis (EDA)\nNow I'm going to explore correlation of the features, their importance to the target and what they can tell us about the customers. I will also try to speculate the reasoning behind certain patterns and try to figure out if I'm correct. We're starting with exploring the target.","fd268e36":"I put this right after the graph before because the two resemble each other quite a lot, which really makes me wonder if my assumption about the lack of experience being correlated to ignoring cerain parts of the travel preparation is true. Does the insurance offer only make sense for people who travel frequently? Is the marketing not enough to inform the new travellers about the importance of having an insurance? These are the question the company should ask themselves, and I wish we could explore more data on this topic but sadly we are limited to this one data set.","dfc23fca":"### 3.1 The target","316c6449":"This graph is a following on the employment type vs insurance part we did before and here we can even more clearly see what is an issue - money. Government sector is clearly underpaid, and since there's quite a correlation between travel and money, we can assume that they do travel less or at least not far enough to consider an insurance. Assumptions are not everything, we also need a proof for this, let's move on into the next graph.","588cd016":"Okay, so, we are going to spend some time here because it gets interesting. There is a few things we need to keep in mind, first of all the company has way more customers who did not travel abroad, second of all, just because they did not travel abroad does not mean that they are not travelling there now. **Also, just because they didn't travel abroad doesn't mean that they did not travel at all.**\nWe don't know if the company only offers insurance for flights that go abroad, otherwise we could assume that the lack of experience might make them overlook important things like travel insurance, where people who did travel at least once already know that it is indeed important. That being said, nothing like that had been specified in the dataset so I'm going to ignore it and go with a different assumption to test later. It could be that the people who got to travel abroad already have a higher income and like we saw before, they are more likely to purchase the insurance. ","32b2bb68":"Keep in mind that the currency is in Rupees. There seems to be a clear pattern where the lower end of the spectrum is less likely to purchase the insurance, the exact opposite of the other end. After 3M of the annual income, there is a 89% likelihood that the customer will purchase the insurance.","01e0987d":"One other thing that I have noticed is the inconsistency in column names, I'm going to quickly change one of them to be the same and we can go to the next step. So far I did not notice anything more that should be cleaned.","6716e33b":"# 2. Encoding & Cleaning\nI did a mistake few times before of not encoding a dataset or encoding and then needing the original one, because of that I'm going to keep the og data set and create an encoded one for some of the visualisations and the model later. The rest of the data transformation will be applied after the EDA unless when it's needed for exploration.","2c53d9db":"# 1. Dataset information\nIn this section I will use basic Pandas methods to find out how the data set is structured, if there are any rows missing and if the rows have an appropriate input. I will also look for outliers and make sure that there is no misspelling in group names (ex. in Employment Type column).","d18c4f5b":"# 4. Feature engineering\nIn this section I'm experimenting with different features that can make the model better. I will keep only the ones that work in this notebook. To create them, I ask myself: what kind of story the data set tells that the math won't recognize? I need to make some assumptions now, because the data set did not have enough description on the features, but we'll see what the outcome is.\n\nFirst of all, money is one huge factor in this equation. When do people have a big income but can't spend a lot? One thing we can extract from the data set is the family size. We don't know exactly what kind of family is it, and people define their \"close\" family in different ways. I made a decision to establish that in our data set, a family of 5 or more will be considered big. Two kids, a spouse and one parent is already 5, in this case we can expect a bit more spending going on.","b69c072f":"When it comes to the family members, the ratio of buying the insurance stays the same throughout the whole graph. There is only significantly less customers who make a purchase in the two member family groups. It's hard to say what could be the cause because we do not have any information on what the customers count as family.","14870825":"Just like the graph before, there doesn't seem to be much difference between the two groups of people. I think it is safe to assume that in both cases there is something else that makes people buy the insurance or the otherwise.","991a3c8a":"### 5.1 Building the model\n","7882eaa8":"**Note:**\nThe undersampling and creating a BigFamily feature to signalise more spending increased the AUC by 0.07-0.08, if there was more in-depth information about the customers provided, improving the model should be taken into the direction of creating new features around when people earn or spend more. As visible on the curve, the model has no issues determining who will for sure not purchase the insurance but the opposite is a big issue, at one point it's barely better than random guessing. We can also see the issue on the cross validation graph, it is not very stable and the model crealy needs more information to fill the gaps.","2d321ca9":"I don't think there is more we could look for when it comes to the employment column. We already know most of what we wanted to know. Our target audience so far is people in their late 20s, early 30s, employed in the private sector and it all came from the annual income graph that had a very high correlation with the state of the insurance.","d1f5d84c":"**Our first plot showcases the amount of people (in %) who decided to purchase the travel insurance. From it we can tell that majority of customers is not interested in the service. The question is why? Next plots in this section will try to help us find an answer. It could be many things, personally I suspect one of the following:**\n* The service is not approprietly priced for its value\n* It is approprietly priced but still unaffordable for many\n* The insurance is covered by an outer source\n* The option makes sense to purchase only if you're a part of a specific group of \n\n**The questions we want to answer are:**\n* What kind of group of people the service is interesting for?\n* Why is that group interested in the service?\n* If we want to gain more customers, what should we change make them more inclined to make a purchase?","05a77d15":"![Header](https:\/\/i.imgur.com\/f7qZ0k6.png)\n# Introduction\nThis dataset consists of 9 features from which we have to predict if the customer will purchase travel insurance. This kernel is supposed to showcase a simple EDA with clustering to identify groups of customers. For this task I'm going to use Plotly and scikit's SVM. This notebook should be approachable for people with less experience, if there are any questions related to it, please feel free to leave a comment.\n\n<br>\n\n**Explore trough the legend:**\n1. [Dataset information](#1.-Dataset-information)\n2. [Encoding & cleaning](#2.-Encoding-&-Cleaning)\n3. [EDA](#3.-Explorative-Data-Analysis-(EDA)\n    - *3.1 The target*\n    - *3.2 Looking for relationships*\n    - *3.3 Conclusions*\n4. [Building the model](#4.-Feature-engineering)\n5. [Evaluation](#5.-Predicting-the-behaviour)\n\n\n**Resources:**\n* [Introduction to SVM's by StatQuest](https:\/\/www.youtube.com\/watch?v=efR1C6CvhmE)\n* [Gentle introduction to k-fold cross validation](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/)\n* [Plotly fundamentals](https:\/\/plotly.com\/python\/plotly-fundamentals\/)","86966667":"We can see that there is significantly more graduates in our data set but they don't seem to be that more keen to say yes. 36.11% of the graduates purchases the insurance where in the other group that number drops to 33.56%.","9ab76a5d":"# 5. Predicting the behaviour\nNow that we know something about the data, we are going to build the model that will help us predict who will purchase an insurance. For this job we will use an SVM (Support Vector Machine) and if you haven't seen it yet, feel free to read the guide listed under the resources. In short, an SVM is a model that plots points of data in a n-dimensional space and then performs classification by finding a hyperplane that differentiates between the classes the best."}}