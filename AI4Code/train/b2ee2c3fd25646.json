{"cell_type":{"10d8f9a0":"code","7c0c52fa":"code","61af3b24":"code","5868b806":"code","8649235d":"code","fcf006c3":"code","1309f9c0":"code","125c2ac9":"code","9e6aac2d":"code","93948a97":"code","6396c261":"code","dc9bba63":"code","0799cf29":"code","d3060a74":"code","b19fc76d":"code","995003d5":"code","9cb46192":"code","156a2c0d":"code","80050f4b":"code","1c4958aa":"code","2b86c19d":"code","2a9f860c":"code","c17f25d7":"code","4f40605e":"code","b7f594f9":"code","f7b84e64":"markdown","2e5703c7":"markdown","66e89e10":"markdown","89d94c49":"markdown","231b4a88":"markdown","f79d37d5":"markdown","d5e7a593":"markdown","3a9216c1":"markdown","d8e7f440":"markdown","bda52ffb":"markdown","6236d2b5":"markdown","cfd43590":"markdown","0563d16d":"markdown","7b7e1a4e":"markdown","43059059":"markdown","c116773e":"markdown","aad50ba7":"markdown","3e8b0047":"markdown","71e71601":"markdown"},"source":{"10d8f9a0":"# This source code is mainly copied from the 2nd place solution.\n# And there are some changes from the original one.\n# Please refer to https:\/\/github.com\/matthiasanderer\/m5-accuracy-competition","7c0c52fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\n\n","61af3b24":"nbeats_pred01_df = pd.read_csv('..\/input\/previous-results\/nbeats_toplvl_forecasts1.csv')\nnbeats_pred02_df = pd.read_csv('..\/input\/previous-results\/nbeats_toplvl_forecasts2.csv')\n\n#nbeats_pred_df.head()","5868b806":"BUILD_ENSEMBLE = True","8649235d":"if BUILD_ENSEMBLE:\n    \n    pred_01_df = pd.read_csv('..\/input\/previous-results\/90_submission_v1.csv')\n    pred_02_df = pd.read_csv('..\/input\/previous-results\/93_submission_v2.csv')\n    pred_03_df = pd.read_csv('..\/input\/previous-results\/95_submission_v3.csv')\n    pred_04_df = pd.read_csv('..\/input\/previous-results\/97_submission_v4.csv')\n    pred_05_df = pd.read_csv('..\/input\/previous-results\/99_submission_v5.csv')\n    #pred_06_df = pd.read_csv('..')\n\n    avg_pred = ( np.array(pred_01_df.values[:,1:]) \n                + np.array(pred_02_df.values[:,1:]) \n                + np.array(pred_03_df.values[:,1:])\n                + np.array(pred_04_df.values[:,1:])  \n                + np.array(pred_05_df.values[:,1:])  \n               # + np.array(pred_06_df.values[:,1:])  \n               ) \/5.0\n    \n    ## Loading predictions\n    valid_pred_df = pd.DataFrame(avg_pred, columns=pred_01_df.columns[1:])\n    submission_pred_df = pd.concat([pred_01_df['id'],valid_pred_df],axis=1)\n    \nelse:\n    print('Should not submit single distibution')\n    #submission_pred_df = pd.read_csv('..\/input\/m5-final-13\/submission_v1.csv')","fcf006c3":"validation_gt_data = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nvalidation_gt_data['id'] = validation_gt_data['id'].str.replace('_evaluation','_validation')\nvalidation_gt_data = validation_gt_data.drop(['item_id','dept_id','cat_id','store_id','state_id'],axis=1)\nvalidation_gt_data = pd.concat([validation_gt_data[['id']],validation_gt_data.iloc[:,-28:]],axis=1)\nvalidation_gt_data.columns=submission_pred_df.columns.values\n#validation_gt_data","1309f9c0":"submission_pred_df = pd.concat([validation_gt_data, submission_pred_df.iloc[30490:,:]],axis=0).reset_index(drop=True)\nsubmission_pred_df","125c2ac9":"# Now we have DataFrame to submit.\n# The below codes is for visualizing results and adjusting parameters.","9e6aac2d":"bottom_lvl_pred_df = submission_pred_df.iloc[30490:,:].reset_index(drop=True)\nbottom_lvl_pred_df","93948a97":"name_cols = bottom_lvl_pred_df.id.str.split(pat='_',expand=True)\nname_cols['dept_id']=name_cols[0]+'_'+name_cols[1]\nname_cols['store_id']=name_cols[3]+'_'+name_cols[4]\nname_cols = name_cols.rename(columns={0: \"cat_id\", 3: \"state_id\"})\nname_cols = name_cols.drop([1,2,4,5],axis=1)\nbottom_lvl_pred_df = pd.concat([name_cols,bottom_lvl_pred_df],axis=1)","6396c261":"# Get column groups\ncat_cols = ['id', 'dept_id', 'cat_id',  'store_id', 'state_id']\nts_cols = [col for col in bottom_lvl_pred_df.columns if col not in cat_cols]\nts_dict = {t: int(t[1:]) for t in ts_cols}\n\n# Describe data\nprint('  unique forecasts: %i' % bottom_lvl_pred_df.shape[0])\nfor col in cat_cols:\n    print('   N_unique %s: %i' % (col, bottom_lvl_pred_df[col].nunique()))\n","dc9bba63":"# 1. All products, all stores, all states (1 series)\nall_sales = pd.DataFrame(bottom_lvl_pred_df[ts_cols].sum()).transpose()\nall_sales['id_str'] = 'all'\nall_sales = all_sales[ ['id_str'] +  [c for c in all_sales if c not in ['id_str']] ]","0799cf29":"# 2. All products by state (3 series)\nstate_sales = bottom_lvl_pred_df.groupby('state_id',as_index=False)[ts_cols].sum()\nstate_sales['id_str'] = state_sales['state_id'] \nstate_sales = state_sales[ ['id_str'] +  [c for c in state_sales if c not in ['id_str']] ]\nstate_sales = state_sales.drop(['state_id'],axis=1)","d3060a74":"# 3. All products by store (10 series)\nstore_sales = bottom_lvl_pred_df.groupby('store_id',as_index=False)[ts_cols].sum()\nstore_sales['id_str'] = store_sales['store_id'] \nstore_sales = store_sales[ ['id_str'] +  [c for c in store_sales if c not in ['id_str']] ]\nstore_sales = store_sales.drop(['store_id'],axis=1)","b19fc76d":"# 4. All products by category (3 series)\ncat_sales = bottom_lvl_pred_df.groupby('cat_id',as_index=False)[ts_cols].sum()\ncat_sales['id_str'] = cat_sales['cat_id'] \ncat_sales = cat_sales[ ['id_str'] +  [c for c in cat_sales if c not in ['id_str']] ]\ncat_sales = cat_sales.drop(['cat_id'],axis=1)\n","995003d5":"# 5. All products by department (7 series)\ndept_sales = bottom_lvl_pred_df.groupby('dept_id',as_index=False)[ts_cols].sum()\ndept_sales['id_str'] = dept_sales['dept_id'] \ndept_sales = dept_sales[ ['id_str'] +  [c for c in dept_sales if c not in ['id_str']] ]\ndept_sales = dept_sales.drop(['dept_id'],axis=1)","9cb46192":"all_pred_agg = pd.concat([all_sales,state_sales,store_sales,cat_sales,dept_sales],ignore_index=True)\n","156a2c0d":"all_pred_agg.head()","80050f4b":"nbeats_pred01_df.head()","1c4958aa":"metrics_df = nbeats_pred01_df[['id_str']]\n\n## Calculate errors\n## CAUTION: nbeats_pred_df is \"truth\"\/actual values in this context\nerror = ( np.array(all_pred_agg.values[:,1:]) - np.array(nbeats_pred01_df.values[:,1:]) ) \n\n## Calc RMSSE\nsuccessive_diff = np.diff(nbeats_pred01_df.values[:,1:]) ** 2\ndenom = successive_diff.mean(1)\n\nnum = error.mean(1)**2\nrmsse = num \/ denom\n\nmetrics_df['rmsse'] = rmsse\n\n## Not so clean Pandas action :-) - supressing warnings for now...\nmetrics_df['mean_error'] = error.mean(1)\nmetrics_df['mean_abs_error'] = np.absolute(error).mean(1)\n\nsquared_error = error **2\nmean_squ_err = np.array(squared_error.mean(1), dtype=np.float64) \n\nmetrics_df['rmse'] = np.sqrt( mean_squ_err )\n\nmetrics_df","2b86c19d":"metrics_df = nbeats_pred02_df[['id_str']]\n\n## Calculate errors\n## CAUTION: nbeats_pred_df is \"truth\"\/actual values in this context\nerror = ( np.array(all_pred_agg.values[:,1:]) - np.array(nbeats_pred02_df.values[:,1:]) ) \n\n## Calc RMSSE\nsuccessive_diff = np.diff(nbeats_pred01_df.values[:,1:]) ** 2\ndenom = successive_diff.mean(1)\n\nnum = error.mean(1)**2\nrmsse = num \/ denom\n\nmetrics_df['rmsse'] = rmsse\n\n## Not so clean Pandas action :-) - supressing warnings for now...\nmetrics_df['mean_error'] = error.mean(1)\nmetrics_df['mean_abs_error'] = np.absolute(error).mean(1)\n\nsquared_error = error **2\nmean_squ_err = np.array(squared_error.mean(1), dtype=np.float64) \n\nmetrics_df['rmse'] = np.sqrt( mean_squ_err )\n\nmetrics_df","2a9f860c":"for i in range(0,nbeats_pred01_df.shape[0]):\n    plot_df = pd.concat( [nbeats_pred01_df.iloc[i], all_pred_agg.iloc[i] ]  , axis=1, ignore_index=True)\n    plot_df = plot_df.iloc[1:,]\n    plot_df = plot_df.rename(columns={0:'NBeats',1:'Predictions'})\n    plot_df = plot_df.reset_index()\n    #plot_df\n    \n    plot_df.plot(x='index', y=['NBeats', 'Predictions'] ,figsize=(10,5), grid=True, title=nbeats_pred02_df.iloc[i,0]  )","c17f25d7":"for i in range(0,nbeats_pred02_df.shape[0]):\n    plot_df = pd.concat( [nbeats_pred02_df.iloc[i], all_pred_agg.iloc[i] ]  , axis=1, ignore_index=True)\n    plot_df = plot_df.iloc[1:,]\n    plot_df = plot_df.rename(columns={0:'NBeats',1:'Predictions'})\n    plot_df = plot_df.reset_index()\n    #plot_df\n    \n    plot_df.plot(x='index', y=['NBeats', 'Predictions'] ,figsize=(10,5), grid=True, title=nbeats_pred02_df.iloc[i,0]  )","4f40605e":"submission_pred_df","b7f594f9":"submission_pred_df.to_csv('m5-final-submission.csv', index=False)\n","f7b84e64":"# Calculating comparision metrics","2e5703c7":"## NBeats 02","66e89e10":"## Build aggregates of predictions","89d94c49":"### Overall analysis result: \n\nThe multiplier 0.95 seems to represent the lowest available fit so we build an ensemble with the 2 upper and 2 lower distributions to generate a robust test loss.\n<br><br>\nFinal-11: 0.9 <br>\nFinal-12: 0.93 <br>\nFinal-17: 0.95 <br>\nFinal-13: 0.97 <br>\nFinal-16: 0.99","231b4a88":"# Visualizations","f79d37d5":"## Fill validation rows - we have no info about validation scoring\n","d5e7a593":"## NBeats 01","3a9216c1":"## Load bottom level lgb predictions for alignment","d8e7f440":"## Only work on evaluation forecasts","bda52ffb":"## Interpretation\n\nIf prediction is bigger than \"true\" values error will be positive -> prediction is overshooting (pos error)\n\nIf prediction is smaller than \"true\" values error will be negative -> prediction is undershooting (neg error) \n","6236d2b5":"NBeats predictions trained and predicted on Colab with two different settings (only change in setting is num_epochs to get slightly different ensembles)","cfd43590":"## NBeats 02","0563d16d":"### NBeats 01","7b7e1a4e":"# Overall approach\n\nWe have two different inputs: \n\n1) Bottom level forecasts on item level (30490 signal) that are derived from a lgbm model that models a probability of this item being bought based on datatime features, price features and a few other features that are not time dependent. (Credits: https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe)\n2) Top level forecasts for the levels 1-5 that are created with N-Beats. \n\nWe can now aggregate the bottom level \"probabilit draws\" up to the levels 1-5. By comparing\/aligning the possible results we can select the most suitable probability distribution for the forecast period. ( The multiplier in the custom loss of the bottom level lgbm models seems to help adjust for trend or other effects not fully understood yet)","43059059":"Even though it would not make sense at all to score public validation data it might be safest to set the submission validation values to the ground truth....\n\nSpamming the LB a bit more ... ","c116773e":"Copyright 2020 Matthias Anderer\n\nCopyright for aggregation code snippets 2020 by user: https:\/\/www.kaggle.com\/lebroschar (name unknown)\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.","aad50ba7":"## Load NBEATS reference predictions for global alignment","3e8b0047":"# Submit based on above analysis and manual selection\/clearance","71e71601":"## Reconstruct level descriptions for aggregation"}}