{"cell_type":{"8e2d7120":"code","489814d1":"code","d895d621":"code","6f456dd0":"code","d6df9b23":"code","11870c8c":"code","852a045d":"code","9b38f137":"code","eebd8826":"code","5a2478f3":"code","51cc8d60":"code","84f1b5c1":"code","f2f7432b":"code","703db92e":"code","1f80ca73":"code","dd62ba74":"code","3fd22c17":"code","2297c25f":"code","da238bdc":"code","8652ee82":"code","6e8f933a":"markdown","07c87f12":"markdown","cdea458f":"markdown","8875a7ed":"markdown","331db4c7":"markdown","a06f025b":"markdown","2c96ce87":"markdown","f42e9ea7":"markdown","71adab43":"markdown","01412250":"markdown","36e7f6fa":"markdown","35cad30f":"markdown","d2b4dcaa":"markdown","3e8b3224":"markdown","ff3bef9a":"markdown","c847818b":"markdown"},"source":{"8e2d7120":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nnp.random.seed(5)\nimport os\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.convolutional import Convolution1D, MaxPooling1D\nfrom keras.optimizers import Adam\n\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss, classification_report","489814d1":"dfAvaliacoesAnalisadas = pd.read_csv('..\/input\/all.csv')\ndfAvaliacoesAnalisadas.head()","d895d621":"def converteCategoria(df, coluna):\n    le = preprocessing.LabelEncoder()\n    le.fit(df[coluna])\n    df[coluna] = le.transform(df[coluna])\n    return le\n\nnum_classes = len(dfAvaliacoesAnalisadas.manifest_atendimento.unique())\n\nlabelEncoderManifAtendimento = converteCategoria(dfAvaliacoesAnalisadas, 'manifest_atendimento')\nprint(num_classes)","6f456dd0":"import re\ndef clean_str(string):\n    \"\"\"\n    Tokenization\/string cleaning for all datasets except for SST.\n    Original taken from https:\/\/github.com\/yoonkim\/CNN_sentence\/blob\/master\/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^a-zA-Z0-9\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d2\u00d3\u00d4\u00d5\u00d6\u00d9\u00da\u00db\u00dc\u00dd\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f2\u00f3\u00f4\u00f5\u00f6\u00f9\u00fa\u00fb\u00fc\u00fd\u00ff,!?\\'\\`\\.\\(\\)]\", \" \", string)\n    string = re.sub(r\"INC[0-9]{7,}\", \" <INCIDENTE> \", string)\n    string = re.sub(r\"[+-]?\\d+(?:\\.\\d+)?\", \" <NUMERO> \", string)\n\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"\\.\", \" \\. \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" ( \", string)\n    string = re.sub(r\"\\)\", \" ) \", string)\n    string = re.sub(r\"\\?\", \" ? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip() #.lower()\n\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(clean_str)\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(lambda x : x.split(' '))","d6df9b23":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\n\nsequence_length = dfAvaliacoesAnalisadas['coment'].apply(len).values\nprint(np.percentile(sequence_length, 99.9))\nprint(np.max(sequence_length))\nplt.hist(sequence_length)","11870c8c":"def pad_sentence(sentence, sequence_length, padding_word=\"<PAD\/>\"):\n    if len(sentence) > sequence_length:\n        sentence = sentence[:sequence_length]\n    num_padding = sequence_length - len(sentence)\n    new_sentence = sentence + [padding_word] * num_padding\n    return new_sentence\n\ncorte = np.max(sequence_length)\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(lambda x : pad_sentence(x, corte))","852a045d":"import itertools\nfrom collections import Counter\n\ndef build_vocab(sentences):\n    \"\"\"\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    \"\"\"\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return (vocabulary, vocabulary_inv)\n\ndef build_input_data(sentences, labels, vocabulary):\n    \"\"\"\n    Maps sentencs and labels to vectors based on a vocabulary.\n    \"\"\"\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return (x, y)\n\ncomments = dfAvaliacoesAnalisadas['coment'].values\nlabels = dfAvaliacoesAnalisadas['manifest_atendimento'].values\n\nvocabulary, vocabulary_inv = build_vocab(comments)\nX, ylabels = build_input_data(comments, labels, vocabulary)","9b38f137":"from gensim.models import word2vec\nfrom os.path import join, exists, split\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    \"\"\"\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    \"\"\"\n    model_dir = 'word2vec_models'\n    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name) and False:\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print('Loading existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 4       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(\"Training Word2Vec model...\")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don't plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights","eebd8826":"embedding_dim = 50\nmin_word_count = 1\ncontext = 10\n\nembedding_weights = train_word2vec(X, vocabulary_inv, embedding_dim, min_word_count, context)","5a2478f3":"print(\"Tamanho do vocabul\u00e1rio: {:d}\".format(len(vocabulary)))\nprint(embedding_weights[0].shape) # n\u00famero de palavras x tamanho do vetor definido.","51cc8d60":"filter_sizes = (3, 4, 5) # cada item da lista representa os tamanhos de filtro que usaremos\nnum_filters = 128 # quantidade de filtro para cada um dos tamanhos acima\ndropout_prob = (0.3, 0.5) # probabilidade de cada camada de Dropout\nhidden_dims = 64 # n\u00famero de neur\u00f4nios na camada densa final","84f1b5c1":"def build_model():\n\n    for fsz in filter_sizes:\n        conv = Convolution1D(nb_filter=num_filters,\n                             filter_length=fsz,\n                             border_mode='valid',\n                             activation='relu',\n                             subsample_length=1)\n        pool = MaxPooling1D(pool_length=2)\n        \n    model = Sequential()\n    model.add(Embedding(len(vocabulary), embedding_dim, input_length=corte, weights=embedding_weights))\n    model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(dropout_prob[1]))\n    model.add(Activation('relu'))\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(dropout_prob[1]))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","f2f7432b":"model = build_model()","703db92e":"model.summary()","1f80ca73":"plot_model(model, to_file='plot_model.png', show_shapes=True)","dd62ba74":"from sklearn.model_selection import train_test_split\ny = to_categorical(ylabels)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)","3fd22c17":"y = to_categorical(ylabels)","2297c25f":"model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_valid, y_valid), verbose=1)","da238bdc":"preds = model.predict_proba(X_valid) # as previs\u00f5es s\u00e3o probabilidades para cada uma das 3 classes\n\n#conta o n\u00famero de acertos, considerando a classe de maior probabilidade\nacc_score = np.sum(np.argmax(preds,1)==np.argmax(y_valid,1))\/float(len(y_valid))\n#calcula o categorical log-loss\nlog_loss_score= log_loss(y_valid, preds)","8652ee82":"print('Accuracy: %.4f Categorical log-loss: %.4f' % (acc_score, log_loss_score))","6e8f933a":"## Parte 2 - Gerando as *Word Embeddings*","07c87f12":"Treinando nosso modelo e verificando o desempenho","cdea458f":"Nesse laborat\u00f3rio vamos realizar an\u00e1lise de sentimento utilizando redes neuronais convolucionais, como proposto nesse <a href=\"http:\/\/arxiv.org\/pdf\/1408.5882v2.pdf\">artigo<\/a>. A arquitetura da solu\u00e7\u00e3o pode ser visualizada na figura abaixo:\n\n<img src=\"http:\/\/debajyotidatta.github.io\/assets\/images\/Zhang.png\" align=\"center\"\/>","8875a7ed":"![Estrutura do Modelo](plot_model.png \"Estrutura do Modelo\")","331db4c7":"# Laborat\u00f3rio 7 - An\u00e1lise de Sentimento utilizando Redes Neuronais Convolucionais","a06f025b":"Como mostrado acima, 99,9% das senten\u00e7as (avalia\u00e7\u00f5es) possuem menos de 37 palavras e 100% s\u00e3o menores que 40 palavras . Ent\u00e3o vamos colocar um ponto de corte  em 40. As avalia\u00e7\u00f5es com mais de 40 palavras ser\u00e3o truncadas e as com menos, ter\u00e3o a palavra <PAD\/\\> completando o final at\u00e9 chegar a 40 palavras.","2c96ce87":"Nesse ponto podemos come\u00e7ar a trabalhar com nossos coment\u00e1rios. O primeiro passo \u00e9 remover caracteres indesejados e separar pontua\u00e7\u00e3o das palavras, pois a pontua\u00e7\u00e3o ser\u00e1 tratada no nosso modelo. Tamb\u00e9m converteremos alguns n\u00fameros, emails e incidentes em uma palavra \u00fanica.","f42e9ea7":"Agora estamos prontos para gerar nossas  *Word embeedings* para nosso vocabul\u00e1rio. V\u00e1rias pesquisas tem demonstrado que usar um modelo baseado no corpus espec\u00edfica da tarefa (no caso, avalia\u00e7\u00f5es de atendimento), funciona melhor do que usar corpus gen\u00e9ricos e grandes como a wikip\u00e9dia.","71adab43":"## Parte 1 - Carregando e manipulando os dados","01412250":"Convertemos a manifesta\u00e7\u00e3o (elogio, neutro, reclama\u00e7\u00e3o) em c\u00f3digos:","36e7f6fa":"Os dados que utilizaremos referem-se a dados de avalia\u00e7\u00e3o do atendimento do Service Desk da Petrobras. Queremos classificar os coment\u00e1rios feitos pelos usu\u00e1rios entre elogio, neutro ou reclama\u00e7\u00e3o.","35cad30f":"Separando os dados de treino e testes.","d2b4dcaa":"Na nossa arquitetura, todas as senten\u00e7as devem ter o mesmo n\u00famero de palavras. Temos de escolher um bom tamanho. Para isso, vamos visualizar um histograma dos tamanhos das senten\u00e7as:","3e8b3224":"Primeiro definimos os par\u00e2metros","ff3bef9a":"## Parte 3 - Montando a rede neuronal convolucional","c847818b":"Agora que temos nosso modelo de palavras transformado em vetores (*word embeddings*), podemos us\u00e1-lo na nossa rede convolucional."}}