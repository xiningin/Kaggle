{"cell_type":{"5944d233":"code","eab85309":"code","8656457f":"code","66df27a2":"code","f671348f":"code","ba17ef58":"code","559b4d41":"code","76c25988":"code","c19c688c":"code","a4fe678e":"code","f57ec850":"code","56505bfb":"code","d80a73db":"code","227099c1":"code","96b59014":"code","799675c6":"code","cb4a05ea":"code","74f0958b":"code","7a2f6758":"code","65d2c3aa":"code","e67e8386":"code","e2658c59":"code","6b4a1693":"code","6bb3366c":"code","79fe1f2c":"code","1d006146":"code","b29974f1":"code","b397b5c5":"code","a44af618":"code","e7248d1a":"code","1f3739a7":"code","b5bddb1c":"code","f578e47a":"code","b3d55de2":"code","906dfb55":"code","195b1cb4":"code","85f0ec0a":"code","a5578d33":"code","f2699829":"code","0a1ffc4b":"code","a253f33b":"code","52442f9c":"code","b47bf410":"code","50faf3e4":"code","6946c7ab":"code","d18f1e53":"code","0e5c6442":"code","8c07a5a6":"code","f0a690ab":"code","fdafe0e0":"code","06b8c2b8":"code","c2de8756":"code","6a462847":"code","dce860dc":"code","b950b40a":"code","477840e3":"code","1ea2cfd4":"code","56ae2102":"code","06da5b64":"code","d3d88636":"code","478927ed":"code","fadad6e5":"code","12a8ffe2":"code","0cc45d46":"markdown","406c4206":"markdown","58907d43":"markdown","8001d673":"markdown","1aa6a555":"markdown","8059cb5c":"markdown","e9d50387":"markdown","ea08b533":"markdown","5c31ff6e":"markdown","9eb262cc":"markdown","ecbf1df1":"markdown","6281a4d9":"markdown","7e59a617":"markdown","9c49d54b":"markdown","090640ba":"markdown","973b74c7":"markdown","615ce84c":"markdown","341cdc34":"markdown","3e1b9a13":"markdown","ade5fddd":"markdown","b5d672a4":"markdown","977f0c96":"markdown","8420b3e9":"markdown","71d97066":"markdown","ba178fee":"markdown","b59e70e4":"markdown","85de3f19":"markdown","fccde711":"markdown","8fa4e342":"markdown","01c9a0ce":"markdown","3a7c1641":"markdown","726d5174":"markdown","e8e273af":"markdown","106bbb20":"markdown","0e9196f9":"markdown","0f9bd36e":"markdown","2076bcf5":"markdown","9fffcc8e":"markdown","017765b6":"markdown","34b10546":"markdown","bd1c5056":"markdown","f2271fc3":"markdown","e1d08f07":"markdown","b3f11441":"markdown","27c8af27":"markdown","8a3bb3f0":"markdown","b07fa1cb":"markdown","44b26d84":"markdown","3e364ba5":"markdown","b4d35b43":"markdown","46533c78":"markdown","d4b2cc6d":"markdown","e6648ebd":"markdown","20fc20d2":"markdown","142ae04a":"markdown","0adef67f":"markdown","2bf954be":"markdown","19506831":"markdown","9c2b1ddd":"markdown","e19174d0":"markdown","3a020f2f":"markdown","1cbad607":"markdown","baebf20e":"markdown","76091316":"markdown","41bb4a52":"markdown","5660a69f":"markdown","a7cc9ac0":"markdown","683a24db":"markdown","71fea9a7":"markdown","d50ba92b":"markdown","0572d464":"markdown","95d93d94":"markdown","5385e208":"markdown","3a550c10":"markdown","538fda73":"markdown","3e6a97f8":"markdown","e52a593d":"markdown","e0a02841":"markdown","e460c9e8":"markdown","919c7f33":"markdown","0acf6b2b":"markdown","bf0c4fce":"markdown","57ded7fe":"markdown","77cb5781":"markdown","02a9ed15":"markdown","6183c99d":"markdown","9b171418":"markdown"},"source":{"5944d233":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')","eab85309":"test = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\n\nprint('Test Shape: ', test.shape)\nprint('Train Shape: ', train.shape)\n\ndf = train.append(test, ignore_index=True)\nprint('Total (test & train append) Shape: ', df.shape)","8656457f":"train.sample(5)","66df27a2":"print('Total number of Male passengers: %d Person (%d%%)'\n      % (df.Sex.value_counts()[0],\n      round(df.Sex.value_counts()[0]\/len(df.Sex),2)*100))\nprint('Total number of Female passengers: %d Person (%d%%)'\n      % (df.Sex.value_counts()[1],\n      round(df.Sex.value_counts()[1]\/len(df.Sex),2)*100), '\\n')\n\nprint('Average Cost of Ticket Fare for Male Passengers: %d USD'\n      % round(df.loc[df['Sex']== 'male', 'Fare'].mean(), 2))\nprint('Average Cost of Ticket Fare for Female Passengers: %d USD'\n      % round(df.loc[df['Sex']== 'female', 'Fare'].mean(), 2))","f671348f":"# Let's plot the Results:\nfig, (ax1,ax2) = plt.subplots(2, figsize=(9,6))\n\n# Ax1\np = sns.countplot(data=df, x=df['Sex'], ax=ax1)\np.set(ylabel='Count')\n\n# Ax2\ni = df[['Fare','Sex']].groupby('Sex').mean().plot(kind='bar', ax=ax2, colormap='PiYG')\ni.set_ylabel('Ticket Fare')\n\nplt.xticks(rotation=0)\nplt.tight_layout()","ba17ef58":"train[['Pclass', 'Survived', 'Sex']].groupby(['Pclass', 'Sex']).mean()","559b4d41":"df['Fare'].describe()","76c25988":"df['Fare'].plot()","c19c688c":"df.loc[df['Fare'] > 300]","a4fe678e":"df.loc[(df['Fare'] < 300)]['Fare'].sort_values(ascending=False)[:1]","f57ec850":"df['Fare'].replace({512.3292:263}, inplace=True)","56505bfb":"# Let's plot Fare column again to check whether 512 USD tickets replaced successfully:\ndf['Fare'].plot()","d80a73db":"print(df.isnull().sum().sort_values())","227099c1":"df[df['Fare'].isnull()]","96b59014":"df.loc[df['Pclass']==3.0]['Fare'].mean()","799675c6":"# Fill that NaN value with 13.30 USD\ndf['Fare'] = df['Fare'].fillna('13.30').astype('O')","cb4a05ea":"print(df['Embarked'].value_counts())\ndf[df['Embarked'].isnull()]","74f0958b":"# Let's fill these 2 NaN 'Embarked' values with Southampton port because most of the passengers get on the ship at this port\ndf['Embarked'] = df['Embarked'].fillna('S')","7a2f6758":"df[df.columns].corr()['Age'][1:]","65d2c3aa":"# Age difference in different classes  \ndf.groupby(['Pclass','Sex'])['Age'].mean()","e67e8386":"df['Age']=df.groupby(['Pclass','Sex']).Age.transform(lambda x: x.fillna(x.mean()))","e2658c59":"# Check NaNs on Age column\ndf.Age.isnull().sum()","6b4a1693":"value = round((df.Cabin.isnull().sum()\/df.shape[0])*100,2)\nprint('Around %d%% of the values in Cabin feature is null.' %value)","6bb3366c":"df.drop(['Cabin'],axis=1,inplace=True)","79fe1f2c":"# All the Columns\ncols = df.columns\n\n# Find the Numeric Columns\nnum_cols = df._get_numeric_data().columns\n\n# Find the Categorical Columns\nlist(set(cols)-set(num_cols))","1d006146":"df['Fare'] = pd.to_numeric(df['Fare'])","b29974f1":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['Sex'])\n\ndf['Sex'] = le.transform(df['Sex'])","b397b5c5":"df = pd.get_dummies(df, columns=['Embarked'])","a44af618":"df.drop(['Name','Ticket', 'PassengerId'], axis=1, inplace=True)","e7248d1a":"df.head(2)","1f3739a7":"#1: make a new data frame for polynomial features\npoly_features = df[['Sex', 'Age', 'Pclass', 'Survived']]\n\npoly_target = poly_features['Survived']\npoly_features = poly_features.drop(columns=['Survived'])\n\n#2: create polynomial object with specified degree\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_transformer = PolynomialFeatures(degree=3)\n\n#3: train polynomial features\npoly_transformer.fit(poly_features)\n\n#4: transform the features\npoly_features = poly_transformer.transform(poly_features)\nprint('Polynomial Features shape: ', poly_features.shape)\n","b5bddb1c":"#5: check to see whether any of these new features are correlated with the Target (here: 'Survivde')\npoly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names(['Sex', 'Age', 'Pclass']))\n\n# Add the Target column\npoly_features['Survived'] = poly_target\n\n# Find the correlations with the Target\npoly_corr = poly_features.corr()['Survived'].sort_values()\n\npoly_corr","f578e47a":"df.sample(4)","b3d55de2":"from sklearn.preprocessing import MinMaxScaler\n\n# Create a copy of our df and then drop the 'Target' column as part of it contains null values (from test data set)\ndf_scaled = df.copy()\ndf_scaled.drop('Survived', axis=1)\n\n# Feature Names\nfeatures = list(df_scaled.columns)\n\n# Scale Features \nscaler = MinMaxScaler(feature_range=(0,1))\n\n# Fit on our data\nscaler.fit(df_scaled)\ndf_scaled = scaler.transform(df_scaled)","906dfb55":"i=pd.DataFrame(df_scaled, columns=features)\ni.head(5)","195b1cb4":"# Split our data frame into train and test\ntrain_scaled = i.loc[i['Survived'].notnull()]\ntest_scaled = i.loc[i['Survived'].isnull()]\n\nprint('Train Data Frame (scaled) Shape: ', train_scaled.shape)\nprint('Test Data Frame (scaled) Shape: ', test_scaled.shape)","85f0ec0a":"test_scaled.reset_index(drop=True, inplace=True)\ntrain_scaled.reset_index(drop=True, inplace=True)","a5578d33":"print('Test Shape: ',test.shape)\nprint('Test Scaled Shape', test_scaled.shape)","f2699829":"test_scaled.index.difference(test.index)","0a1ffc4b":"from sklearn.model_selection import train_test_split","a253f33b":"X = train_scaled.drop(columns='Survived', axis=1)\ny = train_scaled['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=100)","52442f9c":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","b47bf410":"prediction = lr.predict(X_test)","50faf3e4":"from sklearn.metrics import confusion_matrix, classification_report\nprint('* Confusion Matrix: \\n', confusion_matrix(y_test, prediction), '\\n')\nprint('* Classification Report: \\n', classification_report(y_test, prediction))","6946c7ab":"from sklearn.metrics import accuracy_score\nlr_asc = round(accuracy_score(y_test, prediction),3)*100\nprint('Logistic Regression Model Accuracy Score is: %', lr_asc)","d18f1e53":"from sklearn.tree import DecisionTreeClassifier\n\n# Create a model called dtree\ndtree = DecisionTreeClassifier()","0e5c6442":"# like other sikit learn models, we will fit this model to the training data\ndtree.fit(X_train, y_train)","8c07a5a6":"predictions = dtree.predict(X_test)","f0a690ab":"print('* Confusion Matrix: \\n', confusion_matrix(y_test, prediction), '\\n')\nprint('* Classification Report: \\n', classification_report(y_test, prediction))","fdafe0e0":"dt_asc = round(accuracy_score(y_test, predictions),3)*100\nprint('Decision Tree Model Accuracy Score is: %', dt_asc)","06b8c2b8":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\n\n# Train on training data\nrfc.fit(X_train, y_train)","c2de8756":"# Make predictions on the test data\nrfc_predict = rfc.predict(X_test)","6a462847":"print('* Confusion Matrix: \\n', confusion_matrix(y_test, rfc_predict), '\\n')\nprint('* Classification Report: \\n', classification_report(y_test, rfc_predict))","dce860dc":"rf_asc = round(accuracy_score(y_test, rfc_predict),3)*100\nprint('Random Forrest Model Accuracy Score is: %', rf_asc)","b950b40a":"# We have our data split, now let's go ahead and train the support vector classifier to grab the support vector classifier model\n\nfrom sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X_train, y_train)","477840e3":"predictions = model.predict(X_test)","1ea2cfd4":"print('* Confusion Matrix: \\n', confusion_matrix(y_test, predictions), '\\n')\nprint('* Classification Report: \\n', classification_report(y_test, predictions))","56ae2102":"svm_asc = round(accuracy_score(y_test, predictions),3)*100\nprint('SVM Model Accuracy Score is: %', svm_asc)","06da5b64":"plt.bar(height=[lr_asc, dt_asc, rf_asc, svm_asc], x=['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM'],\n       color=(0.1, 0.1, 0.1, 0.1),  edgecolor='blue')\nplt.grid(axis='y')\nplt.title(label='Machine Learning Models Accuray Score')","d3d88636":"X_train = train_scaled.drop(columns='Survived', axis=1)\nX_test = test_scaled.drop(columns='Survived', axis=1)\ny_train = train_scaled['Survived']\ny_test = test_scaled['Survived']\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\n\n# Train on training data\nrfc.fit(X_train, y_train)\n# Make predictions on the test data\nrfc_predict = rfc.predict(X_test)","478927ed":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": rfc_predict\n    })","fadad6e5":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": rfc_predict\n    })","12a8ffe2":"filename = 'Titanic_Submission.csv'\nsubmission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","0cc45d46":"## Encoding Categorical Variables","406c4206":"Great, we got rid of missing data. Now it's time to go deeper","58907d43":"As it seems from above table, most of the female (more than 90%) with Class 1 and Class 2 tickets were survived, while this number decreased dramatically for male passengers.","8001d673":"According to above code, after 512 USD tickets, the highest Fare price is 263 USD. So, we will replace 512 with 263:","1aa6a555":"Although **50%** of passengers paid around **15 USD**, but the max Fare price is 513 USD","8059cb5c":"A few information about features we have in our data set:\n- *embarked* (Port of Embarkation):\nC = Cherbourg, Q = Queenstown, S = Southampton\n- *cabin*: Cabin number\n- *pclass*: Ticket class\n- *sibsp*: # of siblings \/ spouses aboard the Titanic\n- *parch*: # of parents \/ children aboard the Titanic","e9d50387":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","ea08b533":"So, it might be a good way to groupby our data frame on **Pclass** and **Sex** and then fill the NaN values with the Age mean of that group:","5c31ff6e":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","9eb262cc":"# Intro","ecbf1df1":"Now it's time to predict 'Survived' values from Test dataframe using ML models. But our data frame is not ready yet, we need to do a small preprocessing.","6281a4d9":"## 2- Decision Tree","7e59a617":"## 1- Logistic Regression","9c49d54b":"# Read in Data","090640ba":"![](https:\/\/drive.google.com\/uc?id=1Q5pPTqAOx0B8y81mMeJY148NtT_uQdW8)","973b74c7":"**HINT**: Microsoft AzureML Team has created a cheat sheet to choose proper Machine Learning algorithm for predictive analytics. It might be useful for you as well (*open it in new tab for higher resolution*): ","615ce84c":"It's a 3rd class ticket, so we can replace the NaN value with the fare mean of 3rd class tickets which is 13.3 USD:","341cdc34":"___","3e1b9a13":"### Who **Survived** most? Male or Female?","ade5fddd":"The **Titanic** was a luxury British steamship that sank in the early hours of April 15, 1912 after striking an iceberg, leading to the deaths of more than 1,500 passengers and crew. [Link](https:\/\/www.history.com\/topics\/early-20th-century-us\/titanic)","b5d672a4":"So, there are 3 main features with NaN values that we need to fix.\nBut first, let's get rid of 4 rows. When I checked null values in Name column, I saw that Embarked is the only column with a value. So we will drop all these 4 rows as these are pretty useless.\n___","977f0c96":"___","8420b3e9":"## Examine Missing Values","71d97066":"___","ba178fee":"## **Male** & **Female** Passengers","b59e70e4":"Good! Now we have a normalized data frame ready to run our ML models over it. And now, to do our ML job, we need to split this normalized dataframe into two part based on Survived feature: *Train* and *Test*","85de3f19":"As it seems from data frame, all of these 4 passengers bought **first class** ticket from **C port** with same **Ticket** number. However, the Fare is **NOT** rational and we can change it into:","fccde711":"### 4.  **Cabin**","8fa4e342":"![](https:\/\/drive.google.com\/uc?id=1kXlLOWN0RXUIT4bCsUgr-DCIz7cPDbRc)","01c9a0ce":"### Polynomial Features","3a7c1641":"Let's take a look at our df_scaled dataframe:","726d5174":"It seems that the most correlated feature with Age is **Pclass** with %40.\n\nMoreover, from code below I found out there was a little difference of Age between male and female in different classes:","e8e273af":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","106bbb20":"To decide which feature should we use to fill NaN values from Age column, let's check the correlation between 'Age' and other features:","0e9196f9":"![](https:\/\/drive.google.com\/uc?id=1n0M4tmDxkj3yJVz_jHZuz8rfNehS5ExB)","0f9bd36e":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","2076bcf5":"First, we need to find all the categorical features in our dataframe:\n\n{PS: * if you want to learn more about Categorical Variables, you can find this very useful Kernel from Will Koehrsen*: [Link](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction#Feature-Engineering)}","9fffcc8e":"# Machine Learning Models\n![](https:\/\/drive.google.com\/uc?id=1ybkxA0PO7gcwDdUgo-sEUUGSN0F-h6R6)","017765b6":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","34b10546":"All the features are likely being in a similar range, except *Age* and *Fare*. In Machine Learning, this huge difference could affect our prediction value. So, we'd better to *normalize* the range of our features.","bd1c5056":"It was predictable that the ticket fare would be much higher for female than male!\n_____\nThose travelling in **first class**, most of them the wealthiest passengers on board, included prominent members of the upper class, businessmen, politicians, high-ranking military personnel, industrialists, bankers, entertainers, socialites, and professional athletes. **Second-class** passengers were middle-class travellers and included professors, authors, clergymen, and tourists. **Third-class** or steerage passengers were primarily emigrants moving to the United States and Canada. [Link](https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_RMS_Titanic#cite_note-Hall-4)\n_____","f2271fc3":"___","e1d08f07":"## 3- Random Forest","b3f11441":"   ### 1.  **Fare**","27c8af27":"Let's take a look at our dataframe again:","8a3bb3f0":"Well, it seems more reasonable now!","b07fa1cb":"### 2.  **Embarked**","44b26d84":"**Submission:**","3e364ba5":"## Outliers","b4d35b43":"**(1)** Change 'Fare' type to numeric","46533c78":"### 3.  **Age**","d4b2cc6d":"Looking at the plot, it shows that we have 4 values with unreasonable fare price:","e6648ebd":"**(2)** Encode 'Sex' column\n\nBecause there's only 2 unique values in 'Sex' column, we'll use **'LabelEncoding'**\n","20fc20d2":"We may all heard what happened to Titanic. But one day when I tried to analysis the incident, I found a significant difference in the number of survivals between **male** and **female**. Reading the following Kernel, you will understand my concern and why did I choose the title of \"*Where are Feminists?*\"","142ae04a":"Well, our **Train** and **Test** data frame are now normalized.","0adef67f":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","2bf954be":"___","19506831":"Greate! No categorical columns are left for ML process.\n![](https:\/\/lh3.googleusercontent.com\/oO60bJ7GZZYZa_81ckzEXNaLM_Ok6fO-dE3JVjERkAXs3mmgX0bGnUIeKc56EAnCFmHHbLcWOy4hYh_UtEHK=w1440-h789-rw)","9c2b1ddd":"Let's say good bye to Jack and other %80 of other **male** passengers...","e19174d0":"**IMPORTANT NOTE**: As the '**Survived**' column in *test* data set is null, there's no way to check the accuracy of our model by predicting values of this feature. So, we need to split *Survived* values from **Train** data set cause we already have the results.\n\n- After testing our model on *Train data set*, we can **generalize** it on the *Test data set* as well.","3a020f2f":"Oops! It seems the type of 'Fare' isn't numeric in our dataframe. So, first we change the type of 'Fare' feature from object to numeric. And then, we will encode two other categorical columns: **Embarked** and **Sex**. (*Because the ticket & name columns are not useful for this kernel*)","1cbad607":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","baebf20e":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","76091316":"## Preprocessing","41bb4a52":"# Featured Engineering","5660a69f":"77% of missing data is pretty high. Plus, the feature itself doesn't seem to have a significant impact over our prediction. So I'll drop this feature from our dataframe:","a7cc9ac0":"## Which model has the highest accuracy score?","683a24db":"Now, all the needed columns are numerics. We won't need 'Name' and 'Ticket' columns so for the final step, let's drop these two:","71fea9a7":"# EXPLORATORY DATA ANALYSIS (EDA)","d50ba92b":"Take a look at our data frame:","0572d464":"Using Logistic Regression, we could predict our y_test values and put them all inside prediction variable. Let's check the accuracy of our model using Confusion Matrix, Classification Report and Accuracy Score:","95d93d94":"**(3)** Encode 'Embarked' column\n\nBecause 'Embarked' has 3 values of 'S', 'C' and 'Q', we'll use **'OneHotEncoding'**","5385e208":"Let me explain what you will get after reading this document:\n- \"**Data Analysis**\" of the Titanic Passengers (*using data analysis techniques in Python*)\n- Using \"**Machine Learning**\" technique to predict if a Passenger survived from the incident.\nSo, this Kernel could be useful for the people who loves data and exploring around it. Yes I didn't use complicated statistic formulas in this Kernel but in some point, I'm pretty sure you will love reading it.\n\n#### ABOUT THE TITLE\nFirst thing first, I do respect women and also Feminists. However, when I worked with this data set, I've found out that the number of passengers survival was hilariously against **men** in Titanic!\n\n- Did you know that only **37% of men** from 1st class cabin survived, while this number was around **97% for woman** with the same class?\n- And it gets worst for the 2nd class tickets. **92% of woman** were survived at this class but only **16% of men** could get on life boats.\nThe reason that I chose the title of \"*Where are feminists*\" for this Kernel was, I strongly believe if the numbers were opposite, it could be still a great issue against men!  **:)**\n\nOnce again, I do respect Feminists and all the women around the world, but next time when you were on a sinking ship, expect the same equality for all the genders. 1 men and 1 women, we should survive side-by-side!\n\n#### DATA SET\nOkay then, it's time to get serious and back to work. In this Kernel, I followed these steps to finally have a good insight to predict our Target feature (*Survived*):\n- 1- Import all the reuquired libraries\n- 2- Read in Data\n- 3- Exploratory Data Analysis (EDA)\n- 3-1- Outliers\n- 3-2- Missing Values\n- 4- Featured Engineering\n- 5- Machine Learning Methods","3a550c10":"Nope, it seems Sex column has higher correlation than the 'Sex' & 'Pclass' columns itself, so we will leave the data frame without any changes in its features.","538fda73":"___","3e6a97f8":"___","e52a593d":"**NOTE**: If you remember, we already normalized our data and it makes our model works fine. But if our data wasn't already adjusted and normalized, we can use **grid search**.\n\nGrid search allows us to find the right parameters such as gamma values.\n\n``` from sklearn.grid_search import GridSearchCV ```\n\nGridSearchCV takes in a dictionary that describes the parameters that should be tried in a model to train. The grid of parameters is defined as a dictionary where the keys are the parameters and the values is basically a list of settings to be tested.","e0a02841":"![](https:\/\/drive.google.com\/uc?id=1qWo67nrlUdvZNqfhZtZAZ_xvNNZ_M7ZX)","e460c9e8":"When I tried to work with NaN values, I found some outliers in 'Fare' column and that is why I added this section into my ML process in order to get rid of them.\n\nCheck out the 'Fare' feature:","919c7f33":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","0acf6b2b":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","bf0c4fce":"First, we need to do the same train_test_split step in order to split our test and train data from each other:","57ded7fe":"![](https:\/\/drive.google.com\/uc?id=1PuQ33oL0QErS0P9knYqVS9634NVD-Y88)","77cb5781":"![](https:\/\/drive.google.com\/uc?id=1zsW-gDKsJCuroZIqp5nzK26tXn_FGXiN)","02a9ed15":"# Import","6183c99d":"Freature Engineering is about make new feaures and learn about the interactions between them and its affects on our Target column (here: **'Survived'**).","9b171418":"## 4- Supported Vector Machines"}}