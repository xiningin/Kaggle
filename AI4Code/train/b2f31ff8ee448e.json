{"cell_type":{"ed14cedc":"code","fe1744ac":"code","75cb4dc8":"code","2cd58cd1":"code","543aa07c":"code","1040fdcb":"code","0e0acb4d":"code","342da686":"code","e80e77ba":"code","de0bc15d":"markdown"},"source":{"ed14cedc":"random_state = 317817398\nuse_augmentation = True\n\nimport re # regex\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nimport keras_tuner as kt\nfrom tensorflow.keras.backend import stop_gradient\n\nfrom sklearn.preprocessing import QuantileTransformer, RobustScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\n\nimport random as rd\nrd.seed(random_state)\nnp.random.seed(random_state)\ntf.random.set_seed(random_state)","fe1744ac":"# auxiliary data from public datasets\n\n# English word use frequency\nword_list = pd.read_csv('\/kaggle\/input\/english-word-frequency\/unigram_freq.csv', dtype={'word':str, 'count':int})\nmax_word_length = np.max(word_list.word.apply(lambda x : len(str(x))))\nword_list.set_index('word', inplace=True)\nword_list['count'] \/= word_list['count'].sum()\n# turn the word_list into a dictionary, a function to handle unknown words too\nword_list = word_list.to_dict()['count']\ndef word_freq (word):\n    try:\n        return word_list[word]\n    except Exception as e:\n        return 0.\n    \n# import word embeddings from the word2vec Kaggle dataset\nembedding_dim = 300 # 50, 100, 200, or 300; see the dataset instructions\nword2vec = {}\nwith open('\/kaggle\/input\/nlpword2vecembeddingspretrained\/glove.6B.%id.txt' % embedding_dim, 'r') as f:\n    for line in tqdm(f, total=400000):\n        fields = line.split()\n        word2vec[fields[0]] = np.array(fields[1:]).astype(np.float)\ndef word_vec (x):\n    if x in word2vec.keys():\n        return word2vec[x]\n    else:\n        return np.zeros(embedding_dim)","75cb4dc8":"# data preprocessing\ndifficult_punctuation = \";\\\"'\u2018\u201c:-()[]+?!$&\/\"\nnumber_characters = \"0123456789\"\n# Let's set a constant length of each excerpt, for parsing with NN. If the text is shorter, we will fill it with empty words\nWORDS_PER_EXCERPT = 205\ndef count_repeated_characters (word):\n    chars = np.array(list(word))\n    return np.sum(chars[1:] == chars[:-1])\n# Does maximum distance between two word embeddings help at all?\ndef max_distance (emb_matrix):\n    max_dist = 0.0\n    n_vectors = emb_matrix.shape[0]\n    for i_vec in range(n_vectors-1):\n        max_dist = max(max_dist, np.max(np.sum((emb_matrix[(i_vec+1):] - emb_matrix[i_vec])**2, axis=1)))\n    return np.sqrt(max_dist)\n    \ndef process_data (\n    csv_train='\/kaggle\/input\/commonlitreadabilityprize\/train.csv',\n    csv_test='\/kaggle\/input\/commonlitreadabilityprize\/test.csv'\n):\n    data_train_val = pd.read_csv(csv_train)\n    data_test = pd.read_csv(csv_test)\n    datasets = [data_train_val, data_test]\n    \n    noTrain_columns = ['target','standard_error','excerpt','id']\n    for data in datasets:\n        # drop url_legal, unique -- they are not in the test dataset, and do not look useful anyways\n        data.drop(['url_legal', 'license'], axis=1, inplace=True)\n\n        # Generate the most basic features\n        data['no_lineBreaks'] = data['excerpt'].str.split('\\n').transform(len)\n        data['no_sentences'] = data['excerpt'].str.split('.').transform(len)\n        data['no_words'] = data['excerpt'].str.split(' ').transform(len)\n        data['no_characters'] = data['excerpt'].apply(len)\n    \n        # potentially useful mean ratios\n        data['mean_sentences_per_lineBreak'] = data['no_sentences'] \/ data['no_lineBreaks']\n        data['mean_words_per_sentence'] = data['no_words'] \/ data['no_sentences']\n        data['mean_characters_per_word'] = data['no_characters'] \/ data['no_words']\n\n        # potentially useful min\/max ratios\n        sentences_per_lineBreak = data.excerpt.str.split('\\n').transform(lambda x : [len(y.split('.')) for y in x])\n        data['min_sentences_per_lineBreak'] = sentences_per_lineBreak.apply(min)\n        data['max_sentences_per_lineBreak'] = sentences_per_lineBreak.apply(max)\n        del sentences_per_lineBreak\n\n        lineBreaks_per_sentence = data.excerpt.str.split('.').transform(lambda x : [len(y.split('\\n')) for y in x])\n        data['min_lineBreaks_per_sentence'] = lineBreaks_per_sentence.apply(min)\n        data['max_lineBreaks_per_sentence'] = lineBreaks_per_sentence.apply(max)\n        del lineBreaks_per_sentence\n\n        words_per_sentence = data.excerpt.str.split('.').transform(lambda x : [len(y.split(' ')) for y in x])\n        data['min_words_per_sentence'] = words_per_sentence.apply(min)\n        data['max_words_per_sentence'] = words_per_sentence.apply(max)\n        del words_per_sentence\n\n        words_per_lineBreak = data.excerpt.str.split('\\n').transform(lambda x : [len(y.split(' ')) for y in x])\n        data['min_words_per_lineBreak'] = words_per_lineBreak.apply(min)\n        data['max_words_per_lineBreak'] = words_per_lineBreak.apply(max)\n        del words_per_lineBreak\n\n        characters_per_word = data.excerpt.str.split(' ').transform(lambda x : [len(y) for y in x])\n        data['min_characters_per_word'] = characters_per_word.apply(min)\n        data['max_characters_per_word'] = characters_per_word.apply(max)\n        del characters_per_word\n\n        characters_per_sentence = data.excerpt.str.split('.').transform(lambda x : [len(y) for y in x])\n        data['min_characters_per_sentence'] = characters_per_sentence.apply(min)\n        data['max_characters_per_sentence'] = characters_per_sentence.apply(max)\n        del characters_per_sentence\n\n        characters_per_lineBreak = data.excerpt.str.split('\\n').transform(lambda x : [len(y) for y in x])\n        data['min_characters_per_lineBreak'] = characters_per_lineBreak.apply(min)\n        data['max_characters_per_lineBreak'] = characters_per_lineBreak.apply(max)\n        del characters_per_lineBreak\n        \n        # punctuation marks count in the text\n        data['punctuation_count'] = data.excerpt.apply(lambda x : sum([c in difficult_punctuation for c in x]))\n        data['punctuation_frequency'] = data['punctuation_count'] \/ data['no_characters']\n\n        # numbers might indicate a text of technical nature, thus more difficult\n        data['number_count'] = data.excerpt.apply(lambda x : sum([c in number_characters for c in x]))\n        data['number_frequency'] = data['number_count'] \/ data['no_characters']\n\n        # multiple letters might indicate colloquial speech, e.g., aahh, oooh, etc.\n        data['multiple_count'] = data.excerpt.apply(count_repeated_characters)\n        data['multiple_count_frequency'] = data['multiple_count'] \/ data['no_characters']\n\n        # create a word list for each excerpt,\n        # remove punctuation and change to lowercase\n        data['word_list'] = data.excerpt.apply(lambda x : re.findall(\"[a-zA-Z]+\", x.lower()))\n        if 'word_list' not in noTrain_columns:\n            noTrain_columns += ['word_list',]\n            \n        # now add a mean, min, max frequency of a word in a given excerpt\n        data['word_frequencies'] = data.word_list.apply(lambda x : [word_freq(y) for y in x])\n        if 'word_frequencies' not in noTrain_columns:\n            noTrain_columns += ['word_frequencies',]\n        data['mean_word_frequecy'] = data.word_frequencies.apply(np.mean)\n        data['median_word_frequecy'] = data.word_frequencies.apply(np.median)\n        data['min_word_frequecy'] = data.word_frequencies.apply(np.min)\n        data['max_word_frequecy'] = data.word_frequencies.apply(np.max)\n        data['std_word_frequecy'] = data.word_frequencies.apply(np.std)\n\n        # some words are not in the dictionary, may not be English, or could be made up (zigzzzz, huzzah)\n        data['non_word_count'] = data.word_frequencies.apply(lambda x : np.sum(np.array(x) == 0))\n        data['non_word_frequency'] = data['non_word_count'] \/ data['no_words']\n\n        # include word embedding data in our dataframe\n        data['word_embeddings'] = data.word_list.apply(lambda x : np.array([word_vec(y) for y in x]))\n        if 'word_embeddings' not in noTrain_columns:\n            noTrain_columns += ['word_embeddings',]\n            \n        # some useful statistics with word embeddings we get right away\n\n        # the topic of the excerpt\n        data[['mean_embedding%i' % i for i in range(embedding_dim)]] = pd.DataFrame(data.word_embeddings.apply(lambda x : np.mean(x, axis=0).tolist()).to_list())\n\n        # the variety of topics touched upon by the excerpt\n        data['stddev_embedding'] = data.word_embeddings.apply(lambda x : np.sum(np.std(x, axis=0)))\n\n        # Does maximum distance between two word embeddings help at all?\n        data['maxdist_embedding'] = data.word_embeddings.apply(max_distance)\n\n    return data_train_val, data_test, noTrain_columns\n\ndef augment_data (data, target='target', std='standard_error', samples_per_row=2):\n    '''This uses the standard_error column to generate copies of each row\n    with target values drawn from a gaussian distribution.\n    This way we can take the confidence measure into account.\n    This should probably be done better than just copying the rows,\n    but it should do for a test...'''\n    # replicate each row\n    res = pd.concat([data,]*samples_per_row).sort_values('id').reset_index(drop=True)\n    # draw samples from a normal distribution\n    for idd in tqdm(data.id.unique()):\n        mask = (res.id == idd)\n        mean, std = res[mask].target.iloc[0], res[mask].standard_error.iloc[0]\n        res.loc[res.index[mask],'target'] = np.random.normal(mean, std, samples_per_row)\n    # shuffle the augmented dataframe\n    res = res.sample(frac=1).reset_index(drop=True)\n    return res\n    \ndef prepare_indices (data_train_val, val_frac=0.2):\n    '''prepare indices for validation where needed'''\n    indices_all = data_train_val.index.to_list()\n    indices_val = rd.sample(indices_all, int(val_frac*len(data_train_val)))\n    indices_train = list(set(indices_all)-set(indices_val))\n    indices = [indices_train, indices_val]\n    return indices\n\n# preprocess the data\nprint('Pre-processing csv data.. ', flush=True, end='')\ndata_train_val, data_test, noTrain_columns = process_data()\nprint('done', flush=True)\n\n# augment the training \/ validation data -- this implementation uses too much memory\nif False:\n    print('Augmenting data.. ', flush=True)\n    data_train_val = augment_data(data_train_val)\n    print('done.', flush=True)\n\n# validation split\nprint('Preparing validation split.. ', flush=True, end='')\nindices = prepare_indices(data_train_val)\nprint('done.', flush=True)","2cd58cd1":"# tools for training NN models\n\ndef split_data (df, target='target', test_size=0.2, pca=False, indices=None, augmented=use_augmentation):\n    if pca:\n        pca = PCA()\n        pca_cols = pca.fit_transform(\n            data[data.columns.difference(noTrain_columns)])\n        X = pd.DataFrame(data=pca_cols,\n            columns=['PCA%i' % i for i in range(pca_cols.shape[1])])\n    else:\n        X = df[df.columns.difference(noTrain_columns)]\n        \n    if augmented:\n        y = df[[target,'standard_error']].to_numpy()\n    else:\n        y = df[target]\n    if indices == None:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    else:\n        train_indices, test_indices = indices\n        X_train = X.iloc[train_indices]\n        X_test = X.iloc[test_indices]\n        if augmented:\n            y_train = y[train_indices]\n            y_test = y[test_indices]\n        else:\n            y_train = y.iloc[train_indices]\n            y_test = y.iloc[test_indices]\n    return X_train, X_test, y_train, y_test, pca\n\ndef fit_from_hp_dict (build_model, hp_dict, \n                      nn_inputs, indices, y_train, y_test, \n                      early_stopping=True, validation=True,\n                      epochs=1024):\n    '''Using saved HyperParameter.values dict,\n    build the tuned model, train it, and plot diagnostics.'''\n    best_hyperparameters = kt.HyperParameters()\n    best_hyperparameters.values = hp_dict\n    best_model = build_model(best_hyperparameters)\n    if early_stopping:\n        stop_early = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=16,\n            restore_best_weights=True\n        )\n        callbacks = [stop_early,]\n    else:\n        callbacks = []\n    if validation:\n        validation_data=(nn_inputs[indices[1]],y_test)\n    else:\n        validation_data=None\n    history = best_model.fit(\n        nn_inputs[indices[0]], y_train,\n        epochs=epochs,\n        validation_data=validation_data,\n        callbacks=callbacks\n    )\n    plt.clf()\n    plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.legend(loc='lower right')\n    plt.show()\n    plt.close()\n    return best_model\n\ndef augmented_loss (loss_instance, y, y_val, sample_weight=None):\n    '''This uses the standard_error column to generate a different\n    sample from the target distribution at every training step,\n    to use for loss calculation.'''\n    if y.shape == y_val.shape:\n        y_sample = y\n    else:\n        y_sample = tf.random.normal([1,], y[:,0], y[:,1], y_val.dtype)\n    return loss_instance(stop_gradient(y_sample), y_val, sample_weight)","543aa07c":"# word length and frequency CNN\n\n# To feed the model, use the first WORDS_PER_EXCERPT words of the excerpt.\n# If less is available, fill with empty\ndef extract_len_freq (word_list, no_words=WORDS_PER_EXCERPT):\n    # cut to the right length, normalize, extract word frequency\n    res = [[len(x) * 1.0 \/ max_word_length, word_freq(x)] for x in word_list[:no_words]]\n    if len(res) < no_words:\n        res += [[0,0],] * (no_words - len(res))\n    res = np.array(res, dtype=np.float)\n    return res\n\ndef nn_preprocess_len_freq (data):\n    return np.vstack(data.word_list.apply(extract_len_freq).to_numpy()).reshape((-1,WORDS_PER_EXCERPT,2))\n\ndef build_cnn_len_freq (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,2)))\n    num_conv_blocks = hp.Int('conv_blocks', 1,5)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('conv_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Conv1D(hp.Int('filters_conv%i' % i,2,16,step=2),\n                (min(nn.output_shape[1], hp.Int('kernel_conv%i' % i,2,16,step=2)),), \n                activation='relu'))\n            pooling_choice = hp.Choice('pooling%i' % i, ['avg', 'max', 'none'])\n            with hp.conditional_scope('pooling%i' % i, ['avg', 'max']):\n                if pooling_choice == 'max':\n                    nn.add(layers.MaxPooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n                elif pooling_choice == 'avg':\n                    nn.add(layers.AveragePooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,3)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,3+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4,0.3, sampling='log')),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn\n\nif use_augmentation:\n    best_hyperparameters_len_freq_dict = {\n        'conv_blocks': 2, 'filters_conv0': 10, \n        'kernel_conv0': 14, 'pooling0': 'max', \n        'kernel_pool0': 4, 'dropout': 0.2, 'dense_blocks': 3,\n        'batch_norm': 1, 'dense0': 64, 'batch_norm_output': 1, \n        'learning_rate': 0.0009090024729461547, \n        'filters_conv1': 2, 'kernel_conv1': 14, \n        'pooling1': 'avg', 'kernel_pool1': 8, 'dense1': 32, \n        'dense2': 60, 'tuner\/epochs': 32, \n        'tuner\/initial_epoch': 0, 'tuner\/bracket': 0, \n        'tuner\/round': 0\n    }\nelse:\n    best_hyperparameters_len_freq_dict = {\n        'conv_blocks': 2, 'filters_conv0': 8, \n        'kernel_conv0': 10, 'pooling0': 'none', \n        'dropout': 0.2, 'dense_blocks': 2, 'batch_norm': 1, \n        'dense0': 12, 'batch_norm_output': 0, \n        'learning_rate': 0.018757792810801824, \n        'filters_conv1': 12, 'kernel_conv1': 6, \n        'pooling1': 'avg', 'kernel_pool1': 4, 'dense1': 4, \n        'tuner\/epochs': 32, 'tuner\/initial_epoch': 11, \n        'tuner\/bracket': 1, 'tuner\/round': 1, \n        'tuner\/trial_id': '0011a1157813e370e78f8a237ca72049'\n    }\n\n# fit the model\nX_train, X_test, y_train, y_test, pca = split_data (data_train_val, target='target', indices=indices, pca=False)\nnn_inputs_len_freq = nn_preprocess_len_freq(data_train_val)\nbest_model_len_freq = fit_from_hp_dict(\n    build_cnn_len_freq, best_hyperparameters_len_freq_dict, \n    nn_inputs_len_freq, indices, y_train, y_test\n)\ndata_train_val['cnn_word_len+freq'] = best_model_len_freq.predict(nn_inputs_len_freq)\ndel X_train, X_test, y_train, y_test, pca, nn_inputs_len_freq\n\n# apply it to the test data\nnn_inputs_len_freq = nn_preprocess_len_freq(data_test)\ndata_test['cnn_word_len+freq'] = best_model_len_freq.predict(nn_inputs_len_freq)\ndel nn_inputs_len_freq","1040fdcb":"# embedding cnn\n\n# For now, let's use the first WORDS_PER_EXCERPT words again. We will expand it to include the entire excerpts later..\n\ndef extract_emb (emb_matrix, no_words=WORDS_PER_EXCERPT):\n    # cut or pad to the right length\n    return np.concatenate([emb_matrix[:no_words,:], np.zeros([max(0,no_words-emb_matrix.shape[0]),emb_matrix.shape[1]])])\n\ndef nn_preprocess_emb (data):\n    return np.vstack(data.word_embeddings.apply(extract_emb).to_numpy()).reshape((-1,WORDS_PER_EXCERPT,embedding_dim))\n\ndef build_cnn_emb (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,embedding_dim)))\n    num_conv_blocks = hp.Int('conv_blocks', 0,5)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('conv_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Conv1D(hp.Int('filters_conv%i' % i,2,16,step=2),\n                (min(nn.output_shape[1], hp.Int('kernel_conv%i' % i,2,16,step=2)),), \n                activation='relu'))\n            pooling_choice = hp.Choice('pooling%i' % i, ['avg', 'max', 'none'])\n            with hp.conditional_scope('pooling%i' % i, ['avg', 'max']):\n                if pooling_choice == 'max':\n                    nn.add(layers.MaxPooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n                elif pooling_choice == 'avg':\n                    nn.add(layers.AveragePooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,5)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4,0.3, sampling='log')),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn\n\nif use_augmentation:\n    best_hyperparameters_emb_dict = {\n        'conv_blocks': 3, 'dense_blocks': 2, \n        'dropout': 0.30000000000000004, 'batch_norm': 0, \n        'dense0': 16, 'batch_norm_output': 1, \n        'learning_rate': 0.08197541995276879, \n        'filters_conv0': 2, 'kernel_conv0': 14, \n        'pooling0': 'max', 'kernel_pool0': 4, \n        'filters_conv1': 6, 'kernel_conv1': 12, \n        'pooling1': 'none', 'dense1': 44, 'filters_conv2': 14, \n        'kernel_conv2': 4, 'pooling2': 'avg', \n        'kernel_pool2': 2, 'tuner\/epochs': 43, \n        'tuner\/initial_epoch': 15, 'tuner\/bracket': 2, \n        'tuner\/round': 1, \n        'tuner\/trial_id': 'dbed8e7c1c9d30298fe0430de255d50f'}\nelse:\n    best_hyperparameters_emb_dict = {\n        'conv_blocks': 1, 'dense_blocks': 2, \n        'dropout': 0.30000000000000004, 'batch_norm': 0, \n        'dense0': 48, 'batch_norm_output': 1, \n        'learning_rate': 0.002693667798794543, \n        'dense1': 12, 'filters_conv0': 10, 'kernel_conv0': 4, \n        'pooling0': 'max', 'kernel_pool0': 6, \n        'tuner\/epochs': 43, 'tuner\/initial_epoch': 15, \n        'tuner\/bracket': 4, 'tuner\/round': 3, \n        'tuner\/trial_id': '6ef193d541fe31f3ba90e45aedbaafdf'\n    }\n\n# fit the model\nX_train, X_test, y_train, y_test, pca = split_data (data_train_val, target='target', indices=indices, pca=False)\nnn_inputs_emb = nn_preprocess_emb(data_train_val)\nbest_model_emb = fit_from_hp_dict(\n    build_cnn_emb, best_hyperparameters_emb_dict, \n    nn_inputs_emb, indices, y_train, y_test\n)\ndata_train_val['cnn_word_embeddings'] = best_model_emb.predict(nn_inputs_emb)\ndel X_train, X_test, y_train, y_test, pca, nn_inputs_emb\n\n# apply it to the test data\nnn_inputs_emb = nn_preprocess_emb(data_test)\ndata_test['cnn_word_embeddings'] = best_model_emb.predict(nn_inputs_emb)\ndel nn_inputs_emb","0e0acb4d":"# embedding lstm\n\ndef extract_emb (emb_matrix, no_words=WORDS_PER_EXCERPT):\n    # cut or pad to the right length\n    return np.concatenate([emb_matrix[:no_words,:], np.zeros([max(0,no_words-emb_matrix.shape[0]),emb_matrix.shape[1]])])\n\ndef nn_preprocess_emb (data):\n    return np.vstack(data.word_embeddings.apply(extract_emb).to_numpy()).reshape((-1,WORDS_PER_EXCERPT,embedding_dim))\n\ndef build_lstm_emb (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,embedding_dim)))\n    num_conv_blocks = hp.Int('lstm_blocks', 1,1)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('lstm_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.LSTM(\n                hp.Int('lstm_units%i' % i, 8,128, sampling='log')\n            ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,5)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(\n                hp.Float('learning_rate', 1e-4,0.3, sampling='log')\n               ),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn\n\nif use_augmentation:\n    best_hyperparameters_emb_lstm_dict = {\n        'lstm_blocks': 1, 'lstm_units0': 55, \n        'dropout': 0.1, 'dense_blocks': 1, \n        'batch_norm': 1, 'dense0': 36, \n        'batch_norm_output': 0, \n        'learning_rate': 0.0036906795279518277, \n        'tuner\/epochs': 128, 'tuner\/initial_epoch': 0, \n        'tuner\/bracket': 0, 'tuner\/round': 0\n    }\nelse:\n    best_hyperparameters_emb_lstm_dict = {\n        'lstm_blocks': 1, 'lstm_units0': 66, 'dropout': 0.2, \n        'dense_blocks': 3, 'batch_norm': 1, 'dense0': 20, \n        'batch_norm_output': 1, \n        'learning_rate': 0.0022843219066342054, 'dense1': 60, \n        'dense2': 56, 'tuner\/epochs': 43, \n        'tuner\/initial_epoch': 0, 'tuner\/bracket': 1, \n        'tuner\/round': 0\n    }\n\n# fit the model\nX_train, X_test, y_train, y_test, pca = split_data (data_train_val, target='target', indices=indices, pca=False)\nnn_inputs_emb = nn_preprocess_emb(data_train_val)\nbest_model_emb_lstm = fit_from_hp_dict(\n    build_lstm_emb, best_hyperparameters_emb_lstm_dict, \n    nn_inputs_emb, indices, y_train, y_test\n)\ndata_train_val['lstm_word_embeddings'] = best_model_emb_lstm.predict(nn_inputs_emb)\ndel X_train, X_test, y_train, y_test, pca, nn_inputs_emb\n\n# apply it to the test data\nnn_inputs_emb = nn_preprocess_emb(data_test)\ndata_test['lstm_word_embeddings'] = best_model_emb_lstm.predict(nn_inputs_emb)\ndel nn_inputs_emb","342da686":"# the final regressor\nscalers = {\n    'QuantileTransf': QuantileTransformer(),\n    'Robust': RobustScaler()\n}\nif use_augmentation:\n    sgdr_best_hps = {\n        'loss': 'epsilon_insensitive', \n        'penalty': 'elasticnet', 'l1_ratio': 0.0, \n        'alpha': 0.1, 'epsilon': 0.001, \n        'learning_rate': 'invscaling', \n        'eta0': 0.01, 'power_t': 0.25\n    }\n    svm_best_hps = {\n        'kernel': 'poly', 'degree': 3, 'gamma': 'auto', \n        'C': 10, 'epsilon': 0.1, 'shrinking': True}\n    svm_best_scaler = 'QuantileTransf'\nelse:\n    sgdr_best_hps = {\n        'loss': 'squared_epsilon_insensitive', 'penalty': 'elasticnet', \n        'l1_ratio': 0.0, 'alpha': 0.1, 'epsilon': 0.001, \n        'learning_rate': 'invscaling', 'eta0': 0.01, 'power_t': 0.25\n    }\n    svm_best_hps = {\n        'kernel': 'rbf', 'degree': 1, 'gamma': 'scale', \n        'C': 0.1, 'epsilon': 0.1, 'shrinking': True}\n    svm_best_scaler = 'Robust'\nsgdr_best_pca = False\nsgdr_best_scaler = 'QuantileTransf'\nsvm_best_pca = False\n\n# first let's see how it does\ndef try_SGDR_opt (data, indices=None, output=False):\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=sgdr_best_pca, indices=indices, augmented=False)\n    reg = make_pipeline(scalers[sgdr_best_scaler],\n        SGDRegressor(\n            **sgdr_best_hps,\n            random_state=random_state\n        ))\n    reg.fit(X_train, y_train)\n    print(\"SGDR RMSE: \", mean_squared_error(y_test, reg.predict(X_test), squared=False))\n    if output:\n        return reg\ndef try_SVM_opt (data, indices=None, output=False):\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=svm_best_pca, indices=indices, augmented=False)\n    reg = make_pipeline(scalers[svm_best_scaler],\n                        SVR(\n                            **svm_best_hps\n                        ))\n    reg.fit(X_train, y_train)\n    print(\"SVM RMSE: \", mean_squared_error(y_test, reg.predict(X_test), squared=False))\n    if output:\n        return reg\n\nprint(\"rmse from validation:\")\ntry_SGDR_opt(data_train_val, indices=indices)\ntry_SVM_opt(data_train_val, indices=indices)\n\n# now retrain on full training data\nX = data_train_val[data_train_val.columns.difference(noTrain_columns)]\ny = data_train_val['target']\nif use_augmentation:\n    reg = make_pipeline(scalers[svm_best_scaler],\n        SVR(\n            **svm_best_hps\n        ))\nelse:\n    reg = make_pipeline(scalers[sgdr_best_scaler],\n        SGDRegressor(\n            **sgdr_best_hps,\n            random_state=random_state\n        ))\nreg.fit(X,y)\nprint(\"rmse from full data:\")\nprint(\"RMSE: \", mean_squared_error(y, reg.predict(X), squared=False))\ndel X, y\n\n# finally, generate predictions for the test dataset\nX = data_test[data_test.columns.difference(noTrain_columns)]\ndata_test['target'] = reg.predict(X)","e80e77ba":"# save the submission\ndata_test[['id','target']].to_csv('submission.csv', index=False)","de0bc15d":"After exploration in 2021_CommonLitReadability notebook, the best model is re-implemented here for clarity."}}