{"cell_type":{"9a0af264":"code","8fa537e6":"code","2339e4d7":"code","d9293279":"code","f983e1da":"code","c333c092":"code","323569fb":"code","ec137d7c":"code","ce1cb8e8":"code","949438d0":"code","7990abe2":"code","be1c7623":"code","bcf46b0d":"code","990686a9":"code","3c8aeea1":"code","ce0a7051":"code","db4fdb5d":"code","608d058e":"code","5248baa5":"code","25875751":"code","debd4585":"code","2a249e97":"code","dbc698a7":"code","69961867":"code","703a21c0":"code","58795f3a":"code","0573c26e":"code","c85b14ae":"code","bd0571b7":"code","38280378":"code","a3e0b738":"code","790afdb2":"code","ac53c199":"code","5275f827":"code","0ded7124":"code","99026ec5":"code","db088877":"code","9ea6cda2":"code","d097e303":"code","65d15a16":"code","9486599f":"code","f9df40b0":"code","e273a2eb":"code","5ee59ab9":"code","d536127d":"code","912afb9d":"code","61b04df2":"code","77674ab2":"code","d874c6a5":"code","8b49b9ee":"code","dab06bf9":"code","12339622":"code","60de7726":"code","47a785d0":"code","fd37075a":"code","50069457":"code","b5829617":"code","e11373c3":"code","2aff5e7f":"code","2b383d5f":"code","0b4388a5":"code","33c13375":"code","8239f03b":"code","c96c1c19":"code","2e9e8f50":"code","50dd7f4c":"code","1aa78d5f":"code","e60896ac":"code","fb930e5e":"code","5eab18ca":"code","c53947df":"code","6c9275a9":"code","b18ecb34":"code","8cdc4f4c":"code","252db68b":"code","8f67dbbb":"code","204f19ad":"code","94dcbf87":"code","cc49df06":"code","755ebd45":"code","c655ca98":"code","99bf0673":"code","6085e546":"code","c843e13f":"code","5f17d579":"code","3e6867e8":"code","624d1300":"code","41ca2ad9":"code","f2b64f60":"code","53427841":"code","58e4d3f5":"code","f26a4740":"code","85531cbe":"code","1f902995":"code","3e390113":"code","8d96307b":"code","6fdd4dc1":"code","1d78f605":"code","32860976":"code","8018e068":"markdown","5062d4c8":"markdown","456a199b":"markdown","3ac66bb3":"markdown","b73b4149":"markdown","88d693ae":"markdown","03568f3a":"markdown","8501ac85":"markdown","527cce58":"markdown","a55460d5":"markdown","5b99260a":"markdown","2beb3cb6":"markdown","51fa225b":"markdown","2143e928":"markdown","1c82bcfd":"markdown","5742ca8d":"markdown","8a4f60fe":"markdown","5833625b":"markdown","8c823e27":"markdown","ee0e15d4":"markdown","61059341":"markdown","720eb2eb":"markdown","dd4c94ea":"markdown","29602abe":"markdown","f2cafd2e":"markdown","5bf6be0f":"markdown","8ac5e511":"markdown","645bc8ed":"markdown","449118e5":"markdown","3febe39a":"markdown","c0eafe78":"markdown","402c36da":"markdown","ac02e32b":"markdown","0129392a":"markdown","9dd12138":"markdown","fa557793":"markdown","1d766dc9":"markdown","232dbc13":"markdown","483ddda2":"markdown","4c083fb9":"markdown","f0cef214":"markdown","4c9874d7":"markdown","71747a1f":"markdown","28186856":"markdown","1746f6df":"markdown","54320b62":"markdown","95821664":"markdown","b42580a9":"markdown","98039476":"markdown","1bd812d7":"markdown","e5873f00":"markdown","3e1e512e":"markdown","89d6a820":"markdown","62b8e1ba":"markdown","fadf9494":"markdown","ff0b0e9b":"markdown","88ca5913":"markdown","76df36b6":"markdown"},"source":{"9a0af264":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","8fa537e6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2339e4d7":"# Import Dataset\ndata = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\ndata.shape","d9293279":"data.head(2)","f983e1da":"data.columns.values","c333c092":"data.drop('Serial No.', axis=1, inplace=True)","323569fb":"data.rename({'Chance of Admit ': 'Chance of Admit', 'LOR ':'LOR'}, axis=1, inplace=True)","ec137d7c":"#Let's see top 10 observation row and column wise\ndata.head(10)","ce1cb8e8":"# Let's see the detail information of dataset\ndata.info()","949438d0":"## General statistics of the data\ndata.describe()","7990abe2":"## Correlation coeffecients heatmap\nsns.heatmap(data.corr(), annot=True).set_title('Correlation Factors Heat Map', color='black', size='20')","be1c7623":"# Isolating GRE Score data\nGRE = pd.DataFrame(data['GRE Score'])\nGRE.describe()","bcf46b0d":"# # Probability Distribution\nsns.distplot(GRE).set_title('Probability Distribution for GRE Test Scores', size='20')\nplt.show()","990686a9":"# Correlation Coeffecients for GRE Score Test\nGRE_CORR = pd.DataFrame(data.corr()['GRE Score'])\nGRE_CORR.drop('GRE Score', axis=0, inplace=True)\nGRE_CORR.rename({'GRE Score': 'GRE Correlation Coeff'}, axis=1, inplace=True)\nGRE_CORR","3c8aeea1":"# Isolating and describing TOEFL Score\nTOEFL = pd.DataFrame(data['TOEFL Score'], columns=['TOEFL Score'])\nTOEFL.describe()","ce0a7051":"# Probability distribution for TOEFL Scores\nsns.distplot(TOEFL).set_title('Probability Distribution for TOEFL Scores', size='20')\nplt.show()","db4fdb5d":"# Isolating and describing the CGPA\nCGPA = pd.DataFrame(data['CGPA'], columns=['CGPA'])\nCGPA.describe()","608d058e":"sns.distplot(CGPA).set_title('Probability Distribution Plot for CGPA', size='20')\nplt.show()","5248baa5":"RES_Count = data.groupby(['Research']).count()\nRES_Count = RES_Count['GRE Score']\nRES_Count = pd.DataFrame(RES_Count)\nRES_Count.rename({'GRE Score': 'Count'}, axis=1, inplace=True)\nRES_Count.rename({0: 'No Research', 1:'Research'}, axis=0, inplace=True)\nplt.pie(x=RES_Count['Count'], labels=RES_Count.index, autopct='%1.1f%%')\nplt.title('Research', pad=5, size=30)\nplt.show()","25875751":"# Isolating and describing \nUniversity_Rating = data.groupby(['University Rating']).count()\nUniversity_Rating = University_Rating['GRE Score']\nUniversity_Rating = pd.DataFrame(University_Rating)\nUniversity_Rating.rename({'GRE Score': 'Count'}, inplace=True, axis=1)\nUniversity_Rating","debd4585":"# Barplot for the distribution of the University Rating\nsns.barplot(University_Rating.index, University_Rating['Count']).set_title('University Rating', size='20')\nplt.show()","2a249e97":"#Isolating and describing\nSOP = pd.DataFrame(data.groupby(['SOP']).count()['GRE Score'])\nSOP.rename({'GRE Score':'Count'}, axis=1, inplace=True)\nSOP","dbc698a7":"# Barplot for SOP \nsns.barplot(SOP.index, SOP['Count']).set_title('Statement of Purpose', size='20')\nplt.show()","69961867":"LOR = pd.DataFrame(data.groupby(['LOR']).count()['GRE Score'])\nLOR.rename({'GRE Score':'Count'}, axis=1, inplace=True)\nLOR","703a21c0":"# Distribution of the LOR\nsns.barplot(LOR.index, LOR['Count']).set_title('Letter of Recommendation', size='20')\nplt.show()","58795f3a":"data['Chance of Admit']\nsns.distplot(data['Chance of Admit']).set_title('Probability Distribution of Chance of Admit', size='20')\nplt.show()","0573c26e":"data.describe()['Chance of Admit']","c85b14ae":"COA_corr = pd.DataFrame(data.corr()['Chance of Admit'])\nCOA_corr.rename({'Chance of Admit': 'Correlation Coeffecient'}, axis=1, inplace=True)\nCOA_corr.drop('Chance of Admit', inplace=True)\nCOA_corr.sort_values(['Correlation Coeffecient'], ascending=False, inplace=True)\nCOA_corr_x = COA_corr.index\nCOA_corr_y = COA_corr['Correlation Coeffecient']\nsns.barplot(y=COA_corr_x,x=COA_corr_y).set_title('Chance of Admit Correlation Coeffecients', size='20')\nplt.show()","bd0571b7":"COA_corr","38280378":"X = data.drop(['Chance of Admit'], axis=1)\ny = data['Chance of Admit']","a3e0b738":"#Standardization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX[['CGPA','GRE Score', 'TOEFL Score']] = scaler.fit_transform(X[['CGPA','GRE Score', 'TOEFL Score']])","790afdb2":"#Splitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)","ac53c199":"#### Linear Regression (All Features)","5275f827":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","0ded7124":"lr = LinearRegression()","99026ec5":"lr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","db088877":"pd.DataFrame({\"Actual\": y_test, \"Predict\": y_test}).head()","9ea6cda2":"from sklearn.metrics import r2_score, mean_squared_error\nlr_r2 = r2_score(y_test, y_pred)\nlr_mse = mean_squared_error(y_test, y_pred)\nlr_rmse = np.sqrt(lr_mse)\nprint('Linear Regression R2 Score: {0} \\nLinear Regression MSE: {1}, \\nLinear Regression RMSE:{2}'.format(lr_r2, lr_mse, lr_rmse))","d097e303":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nsns.distplot((y_test - y_pred))\nplt.title('Linear Regression (All Features) Residuals', fontdict={'fontsize':20}, pad=20)\nplt.show()","65d15a16":"sns.set(rc={'figure.figsize':(12.7,8.27)})\n# sns.(y_test, y_pred)\nsns.scatterplot(y_test, y_pred)\nplt.show()","9486599f":"X_selected = X[['CGPA', 'GRE Score', 'TOEFL Score']]\nX_sel_train, X_sel_test, y_train, y_test = train_test_split(X_selected, y, random_state=101)","f9df40b0":"lr_sel = LinearRegression()\nlr_sel.fit(X_sel_train, y_train)\nlr_sel_predictions = lr_sel.predict(X_sel_test)","e273a2eb":"lr_sel_r2 = r2_score(y_test, lr_sel_predictions)\nlr_sel_mse = mean_squared_error(y_test, lr_sel_predictions)\nlr_sel_rmse = np.sqrt(lr_sel_mse)\nprint('Linear Regression R2 Score: {0} \\nLinear Regression MSE: {1}, \\nLinear Regression RMSE:{2}'.format(lr_sel_r2, lr_sel_mse, lr_sel_rmse))","5ee59ab9":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 101)\nrfr.fit(X_train,y_train)\ny_head_rfr = rfr.predict(X_test) ","d536127d":"from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test, y_head_rfr))","912afb9d":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state = 101)\ndtr.fit(X_train,y_train)\ny_head_dtr = dtr.predict(X_test) ","61b04df2":"from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_dtr))","77674ab2":"y = np.array([r2_score(y_test,y_pred),r2_score(y_test,y_head_rfr),r2_score(y_test,y_head_dtr)])\nx = [\"LinearRegression\",\"RandomForestReg.\",\"DecisionTreeReg.\"]\nplt.bar(x,y)\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Regressor\")\nplt.ylabel(\"r2_score\")\nplt.show()","d874c6a5":"red = plt.scatter(np.arange(0,80,5),y_pred[0:80:5],color = \"red\")\ngreen = plt.scatter(np.arange(0,80,5),y_head_rfr[0:80:5],color = \"green\")\nblue = plt.scatter(np.arange(0,80,5),y_head_dtr[0:80:5],color = \"blue\")\nblack = plt.scatter(np.arange(0,80,5),y_test[0:80:5],color = \"black\")\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Chance of Admit\")\nplt.legend((red,green,blue,black),('LR', 'RFR', 'DTR', 'REAL'))\nplt.show()","8b49b9ee":"data[\"Chance of Admit\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"Chance of Admit\")\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Frequency\")\nplt.show()","dab06bf9":"# reading the dataset\ndf = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\ndf.shape","12339622":"# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","60de7726":"X = df.drop([\"Chance of Admit\"],axis=1)\ny = df[\"Chance of Admit\"].values","47a785d0":"# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)","fd37075a":"# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nX_train[X_train.columns] = scalerX.fit_transform(X_train[X_train.columns])\nX_test[X_test.columns] = scalerX.transform(X_test[X_test.columns])","50069457":"y_train_01 = [1 if each > 0.8 else 0 for each in y_train]\ny_test_01  = [1 if each > 0.8 else 0 for each in y_test]\n\n# list to array\ny_train_01 = np.array(y_train_01)\ny_test_01 = np.array(y_test_01)","b5829617":"from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nlogr.fit(X_train,y_train_01)","e11373c3":"y_predlogr = logr.predict(X_test)","2aff5e7f":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy Score:\", accuracy_score(y_predlogr, y_test_01))","2b383d5f":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test_01,y_predlogr))\ncm_lrc = confusion_matrix(y_test_01,y_predlogr)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29","0b4388a5":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","33c13375":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_predlogr))\nprint(\"recall_score: \", recall_score(y_test_01, y_predlogr))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_predlogr))","8239f03b":"cm_lrc_train = confusion_matrix(y_train_01,logr.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","c96c1c19":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train,y_train_01)\ny_pred_svm = svm.predict(X_test)\nprint(\"score: \", svm.score(X_test,y_test_01))","2e9e8f50":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm = confusion_matrix(y_test_01,y_pred_svm)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_svm","50dd7f4c":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","1aa78d5f":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_svm))\nprint(\"recall_score: \", recall_score(y_test_01,y_pred_svm))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_svm))","e60896ac":"cm_svm_train = confusion_matrix(y_train_01, svm.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm_train, annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","fb930e5e":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train_01)\ny_pred_nb = nb.predict(X_test)\nprint(\"score: \", nb.score(X_test,y_test_01))","5eab18ca":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb = confusion_matrix(y_test_01, y_pred_nb)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_nb","c53947df":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","6c9275a9":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_nb))\nprint(\"recall_score: \", recall_score(y_test_01,y_pred_nb))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_nb))","b18ecb34":"cm_nb_train = confusion_matrix(y_train_01,nb.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","8cdc4f4c":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train,y_train_01)\ny_pred_dtc = dtc.predict(X_test)\nprint(\"score: \", dtc.score(X_test,y_test_01))","252db68b":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_dtc = confusion_matrix(y_test_01, y_pred_dtc)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_dtc","8f67dbbb":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","204f19ad":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_dtc))\nprint(\"recall_score: \", recall_score(y_test_01, y_pred_dtc))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_dtc))","94dcbf87":"cm_dtc_train = confusion_matrix(y_train_01,dtc.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","cc49df06":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrfc.fit(X_train,y_train_01)\n\ny_pred_rfc = rfc.predict(X_test)\n\nprint(\"score: \", rfc.score(X_test, y_test_01))","755ebd45":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rfc = confusion_matrix(y_test_01, y_pred_rfc)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_rfc","c655ca98":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","99bf0673":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_rfc))\nprint(\"recall_score: \", recall_score(y_test_01, y_pred_rfc))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_rfc))","6085e546":"cm_rfc_train = confusion_matrix(y_train_01, rfc.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","c843e13f":"from sklearn.neighbors import KNeighborsClassifier\n\n# finding k value\nscores = []\nfor each in range(1,50):\n    knn_n = KNeighborsClassifier(n_neighbors = each)\n    knn_n.fit(X_train, y_train_01)\n    scores.append(knn_n.score(X_test, y_test_01))\n    \nplt.plot(range(1,50),scores)\nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy\")\nplt.show()","5f17d579":"knn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(X_train, y_train_01)\n\ny_pred_knn = knn.predict(X_test)\nprint(\"score of 3 :\",knn.score(X_test,y_test_01))","3e6867e8":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_test_01, y_pred_knn)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_knn","624d1300":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","41ca2ad9":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_knn))\nprint(\"recall_score: \", recall_score(y_test_01, y_pred_knn))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_knn))","f2b64f60":"cm_knn_train = confusion_matrix(y_train_01,knn.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","53427841":"y = np.array([logr.score(X_test, y_test_01), svm.score(X_test, y_test_01), nb.score(X_test, y_test_01), dtc.score(X_test,y_test_01), rfc.score(X_test, y_test_01), knn.score(X_test, y_test_01)])\n#x = [\"LogisticRegression\",\"SVM\",\"GaussianNB\",\"DecisionTreeClassifier\",\"RandomForestClassifier\",\"KNeighborsClassifier\"]\nx = [\"LogisticReg.\", \"SVM\", \"GNB\", \"Dec.Tree\", \"Ran.Forest\", \"KNN\"]\n\nplt.bar(x,y)\nplt.title(\"Comparison of Classification Algorithms\")\nplt.xlabel(\"Classfication\")\nplt.ylabel(\"Score\")\nplt.show()","58e4d3f5":"data = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\ndata.shape","f26a4740":"data.columns","85531cbe":"data = data.rename(columns = {'Chance of Admit ':'ChanceOfAdmit'})\nserial = data[\"Serial No.\"]\ndata.drop([\"Serial No.\"],axis=1,inplace = True)","1f902995":"data = (data - np.min(data))\/(np.max(data)-np.min(data))\nX = data.drop([\"ChanceOfAdmit\"],axis=1)\ny = data.ChanceOfAdmit","3e390113":"# for data visualization\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 1, whiten= True )  # whitten = normalize\npca.fit(X)\nx_pca = pca.transform(X)\nx_pca = x_pca.reshape(400,)\ndictionary = {\"x\":x_pca,\"y\":y}\ndata1 = pd.DataFrame(dictionary)\nprint(\"data:\")\nprint(data1.head())\nprint(\"\\ndata:\")\nprint(data.head())","8d96307b":"data[\"Serial No.\"] = serial\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"k values\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\nkmeans = KMeans(n_clusters=3)\nclusters_knn = kmeans.fit_predict(X)\n\ndata[\"label_kmeans\"] = clusters_knn\n\n\nplt.scatter(data[data.label_kmeans == 0 ][\"Serial No.\"], data[data.label_kmeans == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(data[data.label_kmeans == 1 ][\"Serial No.\"], data[data.label_kmeans == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(data[data.label_kmeans == 2 ][\"Serial No.\"], data[data.label_kmeans == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\ndata[\"label_kmeans\"] = clusters_knn\nplt.scatter(data1.x[data.label_kmeans == 0 ],data1[data.label_kmeans == 0].y,color = \"red\")\nplt.scatter(data1.x[data.label_kmeans == 1 ],data1[data.label_kmeans == 1].y,color = \"blue\")\nplt.scatter(data1.x[data.label_kmeans == 2 ],data1[data.label_kmeans == 2].y,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","6fdd4dc1":"data[\"Serial No.\"] = serial\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(X, method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n\nfrom sklearn.cluster import AgglomerativeClustering\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3, affinity= \"euclidean\", linkage = \"ward\")\nclusters_hiyerartical = hiyerartical_cluster.fit_predict(X)\n\ndata[\"label_hiyerartical\"] = clusters_hiyerartical\n\nplt.scatter(data[data.label_hiyerartical == 0 ][\"Serial No.\"],data[data.label_hiyerartical == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(data[data.label_hiyerartical == 1 ][\"Serial No.\"],data[data.label_hiyerartical == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(data[data.label_hiyerartical == 2 ][\"Serial No.\"],data[data.label_hiyerartical == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\nplt.scatter(data1[data.label_hiyerartical == 0 ].x,data1.y[data.label_hiyerartical == 0],color = \"red\")\nplt.scatter(data1[data.label_hiyerartical == 1 ].x,data1.y[data.label_hiyerartical == 1],color = \"blue\")\nplt.scatter(data1[data.label_hiyerartical == 2 ].x,data1.y[data.label_hiyerartical == 2],color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","1d78f605":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","32860976":"df = pd.read_csv('..\/input\/Admission_Predict.csv')\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\n\nnewDF = pd.DataFrame()\nnewDF[\"GRE Score\"] = df[\"GRE Score\"]\nnewDF[\"TOEFL Score\"] = df[\"TOEFL Score\"]\nnewDF[\"CGPA\"] = df[\"CGPA\"]\nnewDF[\"Chance of Admit\"] = df[\"Chance of Admit\"]\n\nx_new = df.drop([\"Chance of Admit\"],axis=1)\ny_new = df[\"Chance of Admit\"].values\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train_new, x_test_new, y_train_new, y_test_new = train_test_split(x_new, y_new, test_size = 0.20, random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train_new[x_train_new.columns] = scalerX.fit_transform(x_train_new[x_train_new.columns])\nx_test_new[x_test_new.columns] = scalerX.transform(x_test_new[x_test_new.columns])\n\nfrom sklearn.linear_model import LinearRegression\nlr_new = LinearRegression()\nlr_new.fit(x_train_new, y_train_new)\ny_head_lr_new = lr_new.predict(x_test_new)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test_new, y_head_lr_new))","8018e068":"It is evident that the most contribution factors to the chance of admission are CGPA, GRE Score and TOEFL Score","5062d4c8":"And since one of our goals is to predict the chance of admission, let's take a look on how the different variables correlate with it.","456a199b":"##### Test for Train Dataset:","3ac66bb3":"Test for Train Dataset:","b73b4149":"### 2. K-means Clustering","88d693ae":"##### SOP\nStatement of Purpose (SOP) is a letter written by the student himself to state his purpose and motivation for completing a graduate degree in addition to his goals while and after he completes his study. Many universities find this letter significant because it better describe the student from a personal perspective.","03568f3a":"* The elbow method is used to determine the best number of clusters for k-means clustering. The number is 3.","8501ac85":"* The dendrogram method is used to determine the best number of clusters for hierarchical clustering. The number is 3 again.","527cce58":"##### University Rating\nThe rating of the university the student completed his undergraduate degree from.","a55460d5":"### 3. Decision Tree ","5b99260a":"THE LOR is ordered same as the SOP","2beb3cb6":"### 4. Decision Tree Classification ","51fa225b":"### Step 3: Train Algorithm","2143e928":"### 2. Support Vector Machine","1c82bcfd":"### Step1: Data Collection\/ Data Extraction","5742ca8d":"# Graduate Admission Analysis and Prediction\n\n![](https:\/\/learnrhome.files.wordpress.com\/2019\/05\/graduate.png)\n\n![](http:\/\/debarghyadas.com\/writes\/assets\/grad-main.png)","8a4f60fe":"### Comparison of Classification Algorithms\n\nAll classification algorithms achieved around 90% success. The most successful one is Gaussian Naive Bayes with 96% score.","5833625b":"### 1. Logistic Regression ","8c823e27":"#### Import Libraries","ee0e15d4":"Are students in this sample too good?\n\nFirst looking at the students LOR, SOP and University Ratings, most students score (4-5) on the scales, not many scored (1-2.5).\n\nNonetheless, comparing means for their GRE and TOEFL scores, which are of a universal criteria, they clearly perform better than the average student as the ETS states.\n\nTo conclude, it seems only wise to consider the sample to be somehow above average.","61059341":"##### TOEFL Score\n\nTest of English as a Foreign Language (TOEFL) is a very popular test for English language amongst universities worldwide, it is marked based on three sections: Reading, Listening, Speaking, and Writing, each one of them is out of 30, yielding a maximum score of 120 and a minimum of 0.\n\nETS (the institute that offers the test) recorded a mean score of 82.6 with a standard deviation of 19.5 (https:\/\/www.ets.org\/s\/toefl\/pdf\/94227_unlweb.pdf).\n\nAlthough this is the mean for a wide range of students from all around the world that took the test for different purposes, as students applying for an engineering graduate degree might have a higher average than high school students.","720eb2eb":"#### Correlation between All Columns\n\nThe 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n\nThe 3 least important features for admission to the Master: Research, LOR, and SOP","dd4c94ea":"### 6. K Nearest Neighbors Classification","29602abe":"* All features (x) were collected in one feature with Principal Component Analysis.","f2cafd2e":"### THE THREE IMPORTANT FEATURES","5bf6be0f":"As the distribution plot shows, the GRE test scores are somehow normally distributed.","8ac5e511":"### 1. Preparing Data for Clustering (PCA)","645bc8ed":"Test for Train Dataset:","449118e5":"### Preparing Data for Classification","3febe39a":"The sample's GRE score mean is 316 which is a little bit higher than the mean mentioned previously (306)","c0eafe78":"### Purpose\n\nTo apply for a master's degree is a very expensive and intensive work. With this kernel, students will guess their capacities and they will decide whether to apply for a master's degree or not.\n\n\nSo, basically this set is about the Graduate Admissions data i.e. Given a set of standardized scores like GRE, TOEFL, SOP standard scores, LOR standard scores, what is probability ( basically i have done a YES\/NO scenario ) of gaining admission into a particular school. All those folks who are preparing for MS, might point out this question, from where did you get SOP & LOR scores. These aren\u2019t public figures ? I mean yes, it might not be public, but dont you think universities might be grading these applications on some scale of rating so that the scores can be standardized. Hence the SOP, LOR scores.\n\n### Dataset\n\nThis dataset is created for prediction of graduate admissions and the dataset link is below:\n\n* Features in the dataset:\n\n* GRE Scores (290 to 340)\n\n* TOEFL Scores (92 to 120)\n\n* University Rating (1 to 5)\n\n* Statement of Purpose (1 to 5)\n\n* Letter of Recommendation Strength (1 to 5)\n\n* Undergraduate CGPA (6.8 to 9.92)\n\n* Research Experience (0 or 1)\n\n* Chance of Admit (0.34 to 0.97)","402c36da":"##### Chance of Admission\n\nLet's first take a review on the chances of admission.","ac02e32b":"### Linear Regression (Selected Features)","0129392a":"How good the university is a value between 1 and 5 in integer increment , and since it has positive correlation factors with other variables it's clear that 5 is the highest rating and 1 is the lowest.","9dd12138":"### 2. Random Forest","fa557793":"Exploring this variable, it's ordered from 1 to 5 with 0.5 increments, although the criteria for assessment isn't specified, which will make it harder to deal with new entries.","1d766dc9":"Test for Train Dataset:","232dbc13":"If a candidate's Chance of Admit is greater than 80%, the candidate will receive the 1 label.\n\nIf a candidate's Chance of Admit is less than or equal to 80%, the candidate will receive the 0 label.","483ddda2":"### 3. Hierarchical Clustering","4c083fb9":"* The first results for Linear Regression (7 features): \nr_square score: 0.821208259148699\n\n* The results for Linear Regression now (3 features):\nr_square score: 0.8212241793299223\n\n* The two results are very close. If these 3 features (CGPA, GRE SCORE, and TOEFL SCORE) are used instead of all 7 features together, the result is not bad and performance is increased because less calculation is required.","f0cef214":"Test for Train Dataset:","4c9874d7":"#### Preparing Data for Classification","71747a1f":"It's evident that the sample over performs in the TOEFL.","28186856":"## CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)\n","1746f6df":"### Step4 : Test Algorithm","54320b62":"### 5. Random Forest Classification","95821664":"#### The Three Features for Linear Regression","b42580a9":"## Prediction Models : Regression Algorithm (Supervised Machine Learning)\n\n1. Linear Regression\n2. Decision Tree\n3. Random Forest","98039476":"#### Comparison of Clustering Algorithms\n\nK-means Clustering and Hierarchical Clustering are similarly.","1bd812d7":"##### Research","e5873f00":"##### CGPA\nCumulative Grade Points Average (CGPA) is a measure of a student's marks thus his performance in his undergraduate degree.","3e1e512e":"Test for Train Dataset:","89d6a820":"##### LOR\nLetter of Recommendation (LOR) is a letter written by a person that knows the student and recommends that the university accept his admission, this person can be a professor in his undergraduate degree or a professional whom the student have worked with.","62b8e1ba":"### Step 2:  Data Analysis or Data Exploration","fadf9494":"## Prediction Models : Classification Algorithm (Supervised Machine Learning)\n\n1. Logistic Regression\n2. Support Vector Machine\n3. Gaussian Naive Bayes\n4. Decision Tree Classification\n5. Random Forest Classification\n6. K Nearest Neighbors (KNN) Classification","ff0b0e9b":"Comment:\n\nBecause most candidates in the data have over 70% chance, many unsuccessful candidates are not well predicted","88ca5913":"##### GRE Scores\n\nThe Graduate Record Examinations (GRE) is a the most popular test for graduate schools' admission, it consists of three sections : Analytical Writing, Verbal and Quantitative.\n\nThe test's maximum score is 340 and minimum is 260, and according to an official GRE score document, the mean test score for all individuals from July 1,2014 to June 30,2017 (almost 1,700,000 test taker) is 306.35 which rounds to 306 with an average standard deviation of 7.19\n\nsource: https:\/\/www.ets.org\/s\/gre\/pdf\/gre_interpreting_scores.pdf","76df36b6":"### 3. Gaussian Naive Bayes"}}