{"cell_type":{"c97c0ba1":"code","c201404a":"code","79da4b27":"code","3ab397fb":"code","e61926be":"code","240633ed":"code","bbdbcd62":"code","6d3443f6":"code","c10ccda9":"code","a6b209db":"code","c85b7a5f":"code","4376fc66":"code","799dd22f":"code","1300168a":"code","cfab3d8f":"code","2f007e9f":"code","70a0738e":"code","561558ac":"code","8799c325":"code","a752701c":"code","e1d2d6ac":"code","294593f3":"code","a251d9e2":"code","c2ab8aaf":"code","acae0822":"code","2c2fa1c0":"code","918901fd":"code","69488ea9":"code","e36af289":"code","50fd1f8e":"markdown","3d3beab6":"markdown","62c94826":"markdown","00875d7b":"markdown","2c68341d":"markdown","a22d1f91":"markdown","3cc84abd":"markdown","d382dd77":"markdown","70527b3a":"markdown","40d68973":"markdown","435f66ed":"markdown","3e9cba29":"markdown","139c0599":"markdown","adf9083d":"markdown","28c83b8b":"markdown","56a3d7d2":"markdown","14bf31ff":"markdown","5bf222e3":"markdown","0a62e5ca":"markdown","2b740157":"markdown","c8da9224":"markdown","08cab482":"markdown","4dd71beb":"markdown","493924d6":"markdown","edfb268b":"markdown"},"source":{"c97c0ba1":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport squarify\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib import ticker\nimport seaborn as sns\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n#loading dataset\ndistricts = pd.read_csv('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\nproducts = pd.read_csv('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'):\n    for filename in filenames:\n        engagement_files = list(glob.glob(os.path.join(dirname,'*.*')))\n\nengagement = pd.DataFrame()\nfor file in engagement_files:\n    district_id = file[79:83]\n    engagement_file = pd.read_csv(file)\n    engagement_file['id'] = district_id\n    engagement = pd.concat([engagement, engagement_file], axis=0).reset_index(drop=True)\n\n#mapping for districts dataset\nmapping_1 = {\n    '[0, 0.2[': '0%-20%',\n    '[0.2, 0.4[': '20%-40%',\n    '[0.4, 0.6[': '40%-60%',\n    '[0.6, 0.8[': '60%-80%',\n    '[0.8, 1[': '80%-100%'}\n\nmapping_2 = {\n    '[4000, 6000[': '4000-6000',\n    '[6000, 8000[': '6000-8000',\n    '[8000, 10000[': '8000-10000',\n    '[10000, 12000[': '10000-12000',\n    '[12000, 14000[': '12000-14000',\n    '[14000, 16000[': '14000-16000',\n    '[16000, 18000[': '16000-18000',\n    '[18000, 20000[': '18000-20000',\n    '[20000, 22000[': '20000-22000',\n    '[22000, 24000[': '22000-24000',\n    '[32000, 34000[': '32000-34000'}\n\nmapping_3 = {\n    '[0.18, 1[': '18%-100%',\n    '[1, 2[': '100%-200%'\n}\n\ndistricts['pct_black\/hispanic'] = districts['pct_black\/hispanic'].map(mapping_1)\ndistricts['pct_free\/reduced'] = districts['pct_free\/reduced'].map(mapping_1)\ndistricts['county_connections_ratio'] = districts['county_connections_ratio'].map(mapping_3)\ndistricts['pp_total_raw'] = districts['pp_total_raw'].map(mapping_2)\n\n#separating category\nproducts[['Category', 'Subcategory']] = products['Primary Essential Function'].str.split('-', n=1, expand=True,)\nproducts = products.drop('Primary Essential Function', axis=1)","c201404a":"engagement.head()","79da4b27":"print(f'Number of rows: {engagement.shape[0]};  Number of columns: {engagement.shape[1]}; No of missing values: {sum(engagement.isna().sum())}')","3ab397fb":"print('Number of missing Values in every column:')\nprint(engagement.isna().sum())","e61926be":"engagement.describe()","240633ed":"districts.head()","bbdbcd62":"print(f'Number of rows: {districts.shape[0]};  Number of columns: {districts.shape[1]}; No of missing values: {sum(districts.isna().sum())}')","6d3443f6":"print('Number of missing Values in every column:')\nprint(districts.isna().sum())","c10ccda9":"products.head()","a6b209db":"print(f'Number of rows: {products.shape[0]};  Number of columns: {products.shape[1]}; No of missing values: {sum(products.isna().sum())}')","c85b7a5f":"print('Number of missing Values in every column:')\nprint(products.isna().sum())","4376fc66":"combine = engagement.copy()\ncombine['id'] = combine['id'].astype('int64') \ncombine = combine.merge(districts, left_on='id', right_on='district_id', how='left')\ncombine = combine.merge(products, left_on='lp_id', right_on='LP ID', how='left')\ncombine = combine.drop('district_id', axis=1)\ncombine = combine.drop('LP ID', axis=1)\ncombine['time'] = pd.to_datetime(combine['time'])\ndel engagement\ndel districts\ndel products","799dd22f":"combine.head()","1300168a":"print(f'Number of rows: {combine.shape[0]};  Number of columns: {combine.shape[1]}; No of missing values: {sum(combine.isna().sum())}')","cfab3d8f":"print('Number of missing Values in every column:')\nprint(combine.isna().sum())","2f007e9f":"combine['Provider\/Company Name'].value_counts(dropna=False).shape","70a0738e":"temp = pd.DataFrame(combine.groupby(['time', 'id'])['time'].count())\ntemp = temp.rename(columns={\"time\":\"amount\"})\ntemp = temp.reset_index(drop=False)\ntemp = temp.groupby('time')['amount'].mean()\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['dimgray']*400)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(9, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\n#graph\nax0 = sns.barplot(ax=ax0, x=temp.index, y=temp, zorder=2, linewidth=0.8, saturation=1)\nsummer = np.arange(np.datetime64(\"1970-06-01\"), np.datetime64(\"1970-08-24\"))\nax0.fill_between(summer, np.max(temp), color='#ffd514', alpha=0.5, zorder=2, linewidth=0)\nplt.axvline(np.datetime64(\"1970-02-12\"), color='#ffd514', alpha=0.5)\nplt.axvline(np.datetime64(\"1970-03-11\"), color='#ffd514', alpha=0.5)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', lw=0.3)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', lw=0.3)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim() \nax0.text(x0, y1*1.11, 'Mean Daily Accessed Products', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\nax0.text(x0, y1*1.1, 'After the summer holiday, there are an increased in accessed products', \n        color='#292929', fontsize=5, ha='left', va='top')\nax0.annotate(\"temporary\\nschool closures\", \n             xy=(np.datetime64(\"1970-02-12\"), 430), \n             xytext=(np.datetime64(\"1970-01-09\"), 380), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"COVID-19\\nPandemic\", \n             xy=(np.datetime64(\"1970-03-11\"), 430), \n             xytext=(np.datetime64(\"1970-03-18\"), 350), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"Summer Holiday\", \n             xy=(np.datetime64(\"1970-06-27\"), 350), \n             xytext=(np.datetime64(\"1970-06-27\"), 350), \n             fontsize=5)\n\n#format axis\nax0.set_xlabel(\"date\",fontsize=5, weight='bold')\nax0.set_ylabel(\"products\",fontsize=5, weight='bold')\n\n#format the ticks\nax0.tick_params('both', length=2, which='major', labelsize=5)\nmonths = mdates.MonthLocator()\nax0.xaxis.set_major_locator(months)\nax0.set_xticklabels(['Jan 2020', 'Feb 2020', 'Mar 2020', 'Apr 2020', 'May 2020', 'Jun 2020', 'Jul 2020', \n                     'Aug 2020', 'Sep 2020', 'Oct 2020', 'Nov 2010', 'Dec 2020'])\ny_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.yaxis.set_major_formatter(y_format)\n\nplt.show()","561558ac":"temp = combine.groupby('time')['engagement_index'].sum()\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['dimgray']*400)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(9, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\n#graph\nax0 = sns.barplot(ax=ax0, x=temp.index, y=temp, zorder=2, linewidth=0.8, saturation=1)\nsummer = np.arange(np.datetime64(\"1970-06-01\"), np.datetime64(\"1970-08-24\"))\nax0.fill_between(summer, np.max(temp), color='#ffd514', alpha=0.5, zorder=2, linewidth=0)\nplt.axvline(np.datetime64(\"1970-02-12\"), color='#ffd514', alpha=0.5)\nplt.axvline(np.datetime64(\"1970-03-11\"), color='#ffd514', alpha=0.5)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', lw=0.3)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', lw=0.3)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim() \nax0.text(x0, y1*1.11, 'Daily Page-Load', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\nax0.text(x0, y1*1.1, 'Following the temporary school disclosure, there are an increased in page-load', \n        color='#292929', fontsize=5, ha='left', va='top')\nax0.annotate(\"temporary\\nschool\\nclosures\", \n             xy=(np.datetime64(\"1970-02-12\"), 20000000), \n             xytext=(np.datetime64(\"1970-01-15\"), 15000000), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"COVID-19\\nPandemic\", \n             xy=(np.datetime64(\"1970-03-11\"), 20000000), \n             xytext=(np.datetime64(\"1970-03-18\"), 16000000), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"Summer Holiday\", \n             xy=(np.datetime64(\"1970-06-27\"), 20000000), \n             xytext=(np.datetime64(\"1970-06-27\"), 15000000), \n             fontsize=5)\n\n#format axis\nax0.set_xlabel(\"date\",fontsize=5, weight='bold')\nax0.set_ylabel(\"page-load\",fontsize=5, weight='bold')\n\n#format the ticks\nax0.tick_params('both', length=2, which='major', labelsize=5)\nmonths = mdates.MonthLocator()\nax0.xaxis.set_major_locator(months)\nax0.set_xticklabels(['Jan 2020', 'Feb 2020', 'Mar 2020', 'Apr 2020', 'May 2020', 'Jun 2020', 'Jul 2020', \n                     'Aug 2020', 'Sep 2020', 'Oct 2020', 'Nov 2010', 'Dec 2020'])\ny_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.yaxis.set_major_formatter(y_format)\n\nplt.show()","8799c325":"temp = combine.groupby('time')['lp_id'].nunique()\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['dimgray']*400)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(9, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\n#graph\nax0 = sns.barplot(ax=ax0, x=temp.index, y=temp, zorder=2, linewidth=0.8, saturation=1)\nsummer = np.arange(np.datetime64(\"1970-06-01\"), np.datetime64(\"1970-08-24\"))\nax0.fill_between(summer, np.max(temp), color='#ffd514', alpha=0.5, zorder=2, linewidth=0)\nplt.axvline(np.datetime64(\"1970-02-12\"), color='#ffd514', alpha=0.5)\nplt.axvline(np.datetime64(\"1970-03-11\"), color='#ffd514', alpha=0.5)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', lw=0.3)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', lw=0.3)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim() \nax0.text(x0, y1*1.11, 'No of Products Used', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\nax0.text(x0, y1*1.1, 'There are an increased in no of product used after the temporary school disclosure', \n        color='#292929', fontsize=5, ha='left', va='top')\nax0.annotate(\"temporary\\nschool\\nclosures\", \n             xy=(np.datetime64(\"1970-02-12\"), 4000), \n             xytext=(np.datetime64(\"1970-01-15\"), 3200), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"COVID-19\\nPandemic\", \n             xy=(np.datetime64(\"1970-03-11\"), 4000), \n             xytext=(np.datetime64(\"1970-03-18\"), 3800), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"Summer Holiday\", \n             xy=(np.datetime64(\"1970-06-27\"), 3500), \n             xytext=(np.datetime64(\"1970-06-27\"), 3500), \n             fontsize=5)\n\n#format axis\nax0.set_xlabel(\"date\",fontsize=5, weight='bold')\nax0.set_ylabel(\"page-load\",fontsize=5, weight='bold')\n\n#format the ticks\nax0.tick_params('both', length=2, which='major', labelsize=5)\nmonths = mdates.MonthLocator()\nax0.xaxis.set_major_locator(months)\nax0.set_xticklabels(['Jan 2020', 'Feb 2020', 'Mar 2020', 'Apr 2020', 'May 2020', 'Jun 2020', 'Jul 2020', \n                     'Aug 2020', 'Sep 2020', 'Oct 2020', 'Nov 2010', 'Dec 2020'])\ny_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.yaxis.set_major_formatter(y_format)\n\nplt.show()","a752701c":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(9, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\n\n##########Top 5 Trends##########\ntemp = pd.DataFrame(combine.groupby('lp_id', dropna=False)['time'].min())\ntemp = temp.fillna('Unknown')\ntemp = temp.reset_index(drop=False)\ntemp = pd.DataFrame(temp.groupby(['time'], dropna=False)['lp_id'].count())\ntemp = temp.reset_index(drop=False)\ntemp['time'] = pd.to_datetime(temp['time'])\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['dimgray']*400)\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0 = sns.barplot(ax=ax0, x=temp['time'], y=temp['lp_id'], zorder=2, linewidth=0.5)\nax0.fill_between(summer, np.max(temp['lp_id']), color='#ffd514', alpha=0.5, zorder=2, linewidth=0)\nplt.axvline(np.datetime64(\"1970-02-12\"), color='#ffd514', alpha=0.5)\nplt.axvline(np.datetime64(\"1970-03-11\"), color='#ffd514', alpha=0.5)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', lw=0.3)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', lw=0.3)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim() \nax0.text(x0, y1*1.11, 'No of Products Used', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\nax0.text(x0, y1*1.1, 'There is an increased in new product accessed after the temporary school disclosure', \n        color='#292929', fontsize=5, ha='left', va='top')\nax0.annotate(\"temporary\\nschool\\nclosures\", \n             xy=(np.datetime64(\"1970-02-13\"), 900), \n             xytext=(np.datetime64(\"1970-01-19\"), 800), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"COVID-19\\nPandemic\", \n             xy=(np.datetime64(\"1970-03-11\"), 900), \n             xytext=(np.datetime64(\"1970-03-17\"), 900), \n             arrowprops=dict(arrowstyle=\"->\"), fontsize=5)\nax0.annotate(\"Summer Holiday\", \n             xy=(np.datetime64(\"1970-06-27\"), 1000), \n             xytext=(np.datetime64(\"1970-06-27\"), 1000), \n             fontsize=5)\n\n#format axis\nax0.set_xlabel(\"date\",fontsize=5, weight='bold')\nax0.set_ylabel(\"page-load\",fontsize=5, weight='bold')\n\n#format the ticks\nax0.tick_params('both', length=2, which='major', labelsize=5)\nmonths = mdates.MonthLocator()\nax0.xaxis.set_major_locator(months)\nax0.set_xticklabels(['Jan 2020', 'Feb 2020', 'Mar 2020', 'Apr 2020', 'May 2020', 'Jun 2020', 'Jul 2020', \n                     'Aug 2020', 'Sep 2020', 'Oct 2020', 'Nov 2010', 'Dec 2020'])\ny_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.yaxis.set_major_formatter(y_format)","e1d2d6ac":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4, 3), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=1.1, hspace=1.5)\n\n##########PRODUCT##########\ntemp = pd.DataFrame(combine.groupby('Product Name', dropna=False)['engagement_index'].sum()\/1000000).reset_index()\ntemp.columns = ['product', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\ntemp = temp[0:10]\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(75)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0_sns = sns.barplot(ax=ax0, y=temp['product'], x=temp['amount'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.3)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.3)\n\n#format axis\nax0_sns.set_xlabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"products\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-0.45, 'Top 10 Products', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.2, 'Top 10 products are controlled by Google LLC', fontsize=3, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 55\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.7, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\nx_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.xaxis.set_major_formatter(x_format)\n\n##########PROVIDER##########\ntemp = pd.DataFrame(combine.groupby('Provider\/Company Name', dropna=False)['engagement_index'].sum()\/1000000).reset_index()\ntemp.columns = ['product', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\ntemp = temp[0:10]\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(75)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0_sns = sns.barplot(ax=ax0, y=temp['product'], x=temp['amount'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.3)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.3)\n\n#format axis\nax0_sns.set_xlabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"providers\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-0.47, 'Top 10 Providers', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.2, 'Google LLC is the top provider for digital learning', fontsize=3, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 130\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.7, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\nx_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.xaxis.set_major_formatter(x_format)","294593f3":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4, 4), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.2, hspace=1.5)\n\n##########CATEGORY##########\ntemp = pd.DataFrame(combine.groupby(['Category'], dropna=False)['engagement_index'].sum()).reset_index()\ntemp.columns = ['description', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(75)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0.plot(temp['description'], temp['amount']\/1000000, 'o--', color=\"#ffd514\", markersize=3, markeredgewidth=0, linewidth=0.5, zorder=4)\nax0.fill_between(temp['description'], temp['amount']\/1000000, color=\"#d3d3d3\", zorder=3, alpha=0.5, linewidth=0)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0.set_xlabel(\"category\",fontsize=3, weight='bold')\nax0.set_ylabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0.tick_params(labelsize=3, width=0.5, length=1.5)\nax0.yaxis.set_major_formatter(y_format)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1+285, 'Category & Page Load', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1+160, 'Most of the page-load come from Learning & Curriculum', fontsize=3, ha='left', va='top')\n\n##########SECTORS##########\ntemp = pd.DataFrame(combine.groupby(['Sector(s)'], dropna=False)['engagement_index'].sum()).reset_index()\ntemp.columns = ['description', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(75)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0.plot(temp['description'], temp['amount']\/1000000, 'o--', color=\"#ffd514\", markersize=3, markeredgewidth=0, linewidth=0.5, zorder=4)\nax0.fill_between(temp['description'], temp['amount']\/1000000, color=\"#d3d3d3\", zorder=3, alpha=0.5, linewidth=0)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0.set_xlabel(\"sector\",fontsize=3, weight='bold')\nax0.set_ylabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0.tick_params(labelsize=3, width=0.5, length=1.5)\nax0.yaxis.set_major_formatter(y_format)\nax0.set_xticklabels(['PreK-12;\\nHigher Ed;\\nCorporate', 'Unknown', 'PreK-12', \n                     'PreK-12;\\nHigher Ed', 'Corporate', 'Higher Ed;\\nCorporate'])\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1+300, 'Sectors & Page Load', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1+150, 'PreK-12; Higher Ed; Corporate is dominating', fontsize=3, ha='left', va='top')\n\nplt.show()","a251d9e2":"##########STATE##########\ntemp = pd.DataFrame(combine.groupby('state', dropna=False)['engagement_index'].sum()\/1000000).reset_index()\ntemp.columns = ['state', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(3, 3), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.7, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(75)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0:2, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0_sns = sns.barplot(ax=ax0, y=temp['state'], x=temp['amount'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0_sns.set_xlabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"state\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1*3, 'Page-Load by State', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1*1.9, 'Unknown state is dominating the page-load', fontsize=3, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 50\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.7, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\nx_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.xaxis.set_major_formatter(x_format)\n\n##########LOCALE##########\ntemp = pd.DataFrame(combine.groupby('locale', dropna=False)['engagement_index'].sum()\/1000000).reset_index()\ntemp.columns = ['locale', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\ncolor_map = ['#ffd514', 'dimgray', 'gray', 'darkgray', 'lightgray']\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\n\n#graph\nax0.pie(x=temp['amount'], wedgeprops=dict(width=0.2), colors=color_map, \n        textprops={'fontsize': 3}, autopct='%1.1f%%')\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1*1.49, 'Page-Load Proportion by Locale', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1*1.36, 'Suburb is dominating the page-load', fontsize=3, ha='left', va='top')\nax0.legend(temp['locale'], loc=\"upper left\", bbox_to_anchor=(x0*0.03, y1*0.92), prop={'size': 2.5}, frameon=False, ncol=3)\n\n#format tick\nax0.tick_params(labelsize=3, width=0.5, length=1.5)\n\n##########DISTRICT##########\ntemp = pd.DataFrame((combine.groupby(['state', 'id'], dropna=False)['engagement_index'].sum()\/1000000).reset_index())\ntemp.columns = ['state', 'district', 'amount']\ntemp = temp.fillna('Unknown')\n\ncolor_map = [\"dimgray\" for _ in range(233)]\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[1, 1])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\n\n#graph\nax0_sns = sns.pointplot(x=temp['amount'], y=temp['state'], join=False, \n              ci=None, scale=0.2, color='#ffd514', markers=\"d\", zorder=4, ax=ax0)\nplt.setp(ax0_sns.collections, zorder=4, label=\"\")\nax0_sns = sns.stripplot(x=temp['amount'], y=temp['state'], dodge=True, zorder=3, size=1, ax=ax0)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1*6, \"District's Page-Load by State\", fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1*3.6, 'Yellow diamond is showing the mean on each state', fontsize=3, ha='left', va='top')\n\n#format axis\nax0_sns.set_xlabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"state\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', color='#EEEEEE', linewidth=0.2, zorder=0)\nax0_sns.grid(which='major', axis='y', color='#EEEEEE', linewidth=0.2, zorder=0)\n\n# data label\nx_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.xaxis.set_major_formatter(x_format)","c2ab8aaf":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4, 4), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.7, hspace=0.1)\n\n##########CORRELATION STATE##########\ntemp = combine[['time', 'state', 'engagement_index']]\ntemp['state'].fillna('Unknown', inplace=True)\ntemp = pd.DataFrame(temp.pivot_table(index='time', columns='state', values='engagement_index', \n                                     aggfunc='sum', dropna=False)).reset_index(drop=False)\n\nbackground_color = \"#f6f5f5\"\ncolors = [\"black\", \"#ffd514\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\n#graph\nax0_sns = sns.heatmap(temp.corr(), ax=ax0, annot=True, square=True, xticklabels=True, yticklabels=True,\n            annot_kws={\"size\": 3}, cbar=False, cmap=colormap, linewidths=0.3, \n            linecolor='black', fmt='.1g')\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-1.1, \"Correlation Between State\", fontsize=5, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.5, 'Yellow indicates a high positive correlation', fontsize=3, ha='left', va='top')\n\n#axis\nax0_sns.set_xlabel(\"\")\nax0_sns.set_ylabel(\"\")\nax0_sns.tick_params(length=0, labelsize=3)","acae0822":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(3, 3), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.7, hspace=0.1)\n\n##########CORRELATION LOCALE##########\ntemp = combine[['time', 'locale', 'engagement_index']]\ntemp['locale'].fillna('Unknown', inplace=True)\ntemp = pd.DataFrame(temp.pivot_table(index='time', columns='locale', values='engagement_index', \n                                     aggfunc='sum', dropna=False)).reset_index(drop=False)\n\nbackground_color = \"#f6f5f5\"\ncolors = [\"black\", \"#ffd514\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\n#graph\n#matrix = np.triu(temp.corr())\nax0_sns = sns.heatmap(temp.corr(), ax=ax0, annot=True, square=True, xticklabels=True, yticklabels=True,\n            annot_kws={\"size\": 4}, cbar=False, cmap=colormap, linewidths=0.3, \n            linecolor='black', fmt='.1g')\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-0.38, \"Correlation Between Locale\", fontsize=6, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.2, 'Yellow indicates a high positive correlation', fontsize=4, ha='left', va='top')\n\n#axis\nax0_sns.set_xlabel(\"\")\nax0_sns.set_ylabel(\"\")\nax0_sns.tick_params(length=0, labelsize=3)","2c2fa1c0":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(3, 3), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.7, hspace=0.8)\n\n##########BLACK HISPANIC & DISTRICT##########\ntemp = pd.DataFrame(combine.groupby('pct_black\/hispanic', dropna=False)['id'].nunique()).reset_index()\ntemp.columns = ['black\/hispanic', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(6)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0.scatter(x=temp['black\/hispanic'], y=temp['amount'], s=6, color=color_map, zorder=3)\nax0.vlines(x=temp['black\/hispanic'], ymin=0, ymax=temp['amount'], color=color_map, zorder=3)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0.set_xlabel(\"black\/hispanic\",fontsize=3, weight='bold')\nax0.set_ylabel(\"school districts\",fontsize=3, weight='bold')\nax0.tick_params(labelsize=3, width=0.5, length=1.5)\nplt.xticks(rotation=90)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1+15.9, 'School District by Black\/Hispanic', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1+7, '0%-20% has the most school districts', fontsize=3, ha='left', va='top')\n\n##########BLACK HISPANIC & PAGE LOAD##########\ntemp = pd.DataFrame(combine.groupby('pct_black\/hispanic', dropna=False)['engagement_index'].sum()\/1000000).reset_index()\ntemp.columns = ['pct_black\/hispanic', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nbackground_color = \"#f6f5f5\"\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0_sns = sns.barplot(ax=ax0, y=temp['pct_black\/hispanic'], x=temp['amount'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0_sns.set_xlabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"black\/hispanic\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-0.78, 'Page-Load by Black\/Hispanic', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.3, '0%-20% is dominating the page-load', fontsize=3, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 150\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.7, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\nx_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.xaxis.set_major_formatter(x_format)\n\n##########BLACK HISPANIC & STATE##########\ntemp = pd.DataFrame(combine.groupby(['pct_black\/hispanic', 'state'], dropna=False)['engagement_index'].sum()).reset_index(drop=False)\ntemp = temp.fillna('Unknown')\ntemp['engagement_index'] = temp['engagement_index'] \/ 1000000\n\nbackground_color = \"#f6f5f5\"\ncolors = [\"dimgray\", \"#ffd514\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0 = fig.add_subplot(gs[1, 0:2])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0_sns = sns.scatterplot(ax=ax0, x=temp['state'], y=temp['pct_black\/hispanic'], hue=temp['engagement_index'],\n                          linewidth=0, zorder=2, palette=colormap, s=12)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0_sns.set_xlabel(\"state\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"black\/hispanic\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax0_sns.xaxis.set_minor_locator(ticker.MultipleLocator(50))\nplt.xticks(rotation=90)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-0.9, 'Page-Load by Black\/Hispanic & State (in million)', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.5, 'Connecticut with 0%-20% black\/hispanic has the highest page-load', fontsize=3, ha='left', va='top')\nax0.legend(loc=\"upper left\", bbox_to_anchor=(x0+1.4, y1-0.47), \n           prop={'size': 2.5}, frameon=False, ncol=5, markerscale=0.3)\n\nplt.show()","918901fd":"temp = pd.DataFrame(combine.groupby(['county_connections_ratio'])['state'].nunique()).reset_index()\ntemp.columns = ['County Connections Ratio', 'No of State']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('No of State', ascending=False)\ntemp","69488ea9":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.2, hspace=0.5)\n\n##########PPTE##########\ntemp = pd.DataFrame(combine.groupby(['pp_total_raw'], dropna=False)['engagement_index'].sum()).reset_index()\ntemp.columns = ['pp_total_raw', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(75)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0.step(y=temp['amount']\/1000000, x=temp['pp_total_raw'], \n        zorder=2, linewidth=0.5, alpha=1)\nax0.plot(temp['pp_total_raw'], temp['amount']\/1000000, 'o--', color=\"#4b4b4c\", markersize=2, markeredgewidth=0, alpha=0.3, linewidth=0.2)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0.set_xlabel(\"per-pupil total expenditure\",fontsize=3, weight='bold')\nax0.set_ylabel(\"page-load (million)\",fontsize=3, weight='bold')\nax0.tick_params(labelsize=3, width=0.5, length=1.5)\nplt.xticks(rotation=90)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1+280, 'Per-pupil Total Expenditure & Page Load', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1+160, 'Almost half of page-load is registered as unknown', fontsize=3, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_height():,.0f}'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_y() + p.get_height() + 75\n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.7, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\ny_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.yaxis.set_major_formatter(y_format)\n\n##########PPTE##########\ntemp = pd.DataFrame(combine.groupby(['pp_total_raw'], dropna=False)['id'].nunique()).reset_index()\ntemp.columns = ['pp_total_raw', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\ntemp = temp[0:10]\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"#ffd514\", \"#ecc200\", \"#c5a100\", \"#9d8100\", \"#766100\", \"#4f4100\", \"#4b4b4c\", \"#676767\",\n             \"#808080\", \"#989898\", \"#c6c6c6\", \"#d3d3d3\"]\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\", \"left\", \"bottom\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nsquarify.plot(sizes=temp['amount'], label=temp['pp_total_raw'][:6], color=color_map, ax=ax0, text_kwargs={'fontsize':3, 'wrap':True})\n\n#format axis\nax0.set_yticklabels([])\nax0.set_xticklabels([])\nax0.tick_params(left=False, bottom=False)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1+16, 'Per-pupil Total Expenditure & School Districts', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1+8, 'Higher number of school districts reflecting a higher page-load', fontsize=3, ha='left', va='top')\n\nplt.show()","e36af289":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.3, hspace=0.8)\n\n##########Page Load##########\ntemp = pd.DataFrame(combine.groupby(['pct_free\/reduced'], dropna=False)['engagement_index'].sum()).reset_index()\ntemp.columns = ['description', 'amount']\ntemp = temp.fillna('Unknown')\ntemp = temp.sort_values('amount', ascending=False)\n\nbackground_color = \"#f6f5f5\"\ncolor_map = [\"dimgray\" for _ in range(6)]\ncolor_map[0] = \"#ffd514\"\nsns.set_palette(sns.color_palette(color_map))\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\n#graph\nax0 = sns.barplot(x=temp['description'],y=temp['amount']\/1000000, zorder=3, saturation=1)\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.2)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.2)\n\n#format axis\nax0.set_xlabel(\"Free or Reduced Price\",fontsize=3, weight='bold')\nax0.set_ylabel(\"school districts\",fontsize=3, weight='bold')\nax0.tick_params(labelsize=3, width=0.5, length=1.5)\nplt.xticks(rotation=90)\ny_format = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\nax0.yaxis.set_major_formatter(y_format)\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1+140, 'Free or Reduced Price & Page Load', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1+60, 'Unknown has the highest page-load', fontsize=3, ha='left', va='top')\n\n####################\ntemp = pd.DataFrame(pd.pivot_table(combine, index=['pct_free\/reduced', 'pp_total_raw'], values='engagement_index', aggfunc='sum', dropna=False)).reset_index()\ntemp = pd.pivot_table(temp, index=['pct_free\/reduced'], columns='pp_total_raw', values='engagement_index', aggfunc='sum', dropna=False)\/1000000000\n\nbackground_color = \"#f6f5f5\"\ncolors = [\"black\", \"#ffd514\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\n\n#graph\nax0_sns = sns.heatmap(temp, ax=ax0, annot=True, square=True, xticklabels=True, yticklabels=True,\n            annot_kws={\"size\": 2.5}, cbar=False, cmap=colormap, linewidths=0.3, \n            linecolor=background_color, fmt='.1g')\n\n#title\nx0, x1 = ax0.get_xlim()\ny0, y1 = ax0.get_ylim()\nax0.text(x0, y1-0.8, \"Reduced Price & Per-pupil Total Expenditure\", fontsize=4, ha='left', va='top', weight='bold')\nax0.text(x0, y1-0.4, 'Relation in terms of billion page-load', fontsize=3, ha='left', va='top')\n\n#axis\nax0_sns.set_xlabel(\"\")\nax0_sns.set_ylabel(\"\")\nax0_sns.tick_params(length=0, labelsize=3)\n\nplt.show()","50fd1f8e":"[back to top](#table-of-contents)\n<a id=\"3.2\"><\/a>\n## 3.2 Districts\n\nThe district file includes information about the characteristics of school districts, including data from NCES (2018-19), FCC (Dec 2018), and Edunomics Lab. In this data set, LearnPlatform removed the identifiable information about the school districts. LearnPlatform also used an open source tool ARX (Prasser et al. 2020) to transform several data fields and reduce the risks of re-identification. For data generalization purposes some data points are released with a range where the actual value falls under. Additionally, there are many missing data marked as 'NaN' indicating that the data was suppressed to maximize anonymization of the dataset.\n\nThis dataset consists of below information:\n- **district_id:** The unique identifier of the school district\n- **state:** The state where the district resides in\n- **locale:** NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural.\n- **pct_black\/hispanic:** Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data.\n- **pct_free\/reduced:** Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data\n- **countyconnectionsratio:** ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version).\n- **pptotalraw:** Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource - Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district.\n\n**Observations:**\n- There are `223` rows with `7` columns as mentioned above. \n- This dataset contain missing value of `442` which mainly come from `pp_total_raw` of `115`, `pct_free\/reduced` of `85` and `county_connections_ratio` of `71`.","3d3beab6":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n# 5 Demographic\nWe will try to make an analysis from the perspective of `geographic`, `black\/hispanic`, `county connection ratio` and `per-pupil total expenditure`. We would like to see how these demographic relates to the page-load.\n\n<a id=\"5.1\"><\/a>\n## 5.1 Geographic\nWe will try to see how geographic (which include `state`, `locale` and `district`) relates to `page-load`.\n\n<a id=\"5.1.1\"><\/a>\n### 5.1.1 Geographic and page load\nWe will try to see the relationship between the `geographic` with the `page-load`. `Page-load` analysis can give us more information on student activities in the digital learning. In this analysis we are aggregating all daily page-load in 2020 for a given `State`, `Locale` and `School District` while `School District` will also be grouped on `State` level. There are`233` district in `23` states with `4` locale excluding `Unknown`. \n\n**Observations:**\n- **State**\n    - `Page-load` that doesn't have their corresponding `state` are big which is reaching `594 million` which is in the first position and can not be easily ignored. In the analysis, we called it `Unknown`.\n    - There are `3 states` that have more than `300 million` of `page-load` in 2020, they are `Connecticut`, `Illinois` and `Massachusetts`. They are contributing around `40.7%` of total `page-load` in 2020, if we include `Unknown` state, the contribution increased to `61.7%`.\n- **Locale**\n    - There are `2.8 billion` page-load in 2020, most of it is coming from `Suburb` area that contributes `48%` of total observations.\n    - `Unknown` is the second highest contribution in the `page-load` which is around `594 million` with a contribution of 20.9%.\n    - `City` and `Town` are the lowest locale with `308 million` and `99 million` page-load.\n- **District**\n    - Higher `page-load` in a state doesn't always mean a good things, especially if the state is supported by many school districts. A good indication is to see the mean `page-load` from each school districts in a state. There is only one state that passed `20 million` page-load in 2020 that is `Illinois`. \n    - Though `Unknown` has the highest `page-load`, it only has a mean of around `10 million`, this indicate that there are many school districts that have not be mapped.\n    - `Arizona` has the second highest `page-load` and it has only 1 school district.","62c94826":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n## 4.2 Page Load\nThe `page-load` analysis is trying to supplement our finding in the mean daily accessed products analysis. As a reminder page-load events is calculated per one thousand students of a given product and on a given day. We can see students learning activitis in one day by combining all the pagae-load in a day.\n\n**Observations:**\n- After the temporary school closures, we see there is an increased in `page-load`, meaning digital learning are used more frequently than before. We hardly see the increased of students activity in the mean daily accessed products.\n- Though the mean of `accessed products` in the weekend are about `25%-50%` from the weekday, it doesn't mean the students are studying at the weekend as can be seen on a very low `page-load` in every weekend.\n- Same as the weekend activities, the `page-laod` are also very low in the summer holiday, which also mean the students are not intensively using the products. \n- High number of `accessed products` in the weekend and summer holiday is more due to the variety of products used in by students but not by how intesive the product is used. ","00875d7b":"### 3.2.1 Quick view\nBelow is the first 5 rows of `districts` dataset:","2c68341d":"<a id=\"5.1.2\"><\/a>\n### 5.1.2 Geographic and page load correlation\nWe will try to see below correlation:\n - Correlation between its own `state` and its own `locale` in terms of `page-load`. At first, we will look into the correlation between `state` followed by correlation between `locale.\n - Correlation between the `locale`. There are 5 locale including the `Unknown`, the others are `City`, `Rural`, `Suburb` and `Town`.\n\n**Observations:**\n- **Correlation between state**\n    - The lower correlation between state is betwen `Minnesota` and `North Dakota` which is `-0.121`. This meaning they have an inverse relationship, though it's near zero.\n    - Both of `New York` and `North Dakota` almost don't have any relationship with correlation of `-0.033`.\n    - The highest postive correlation are between `Unknown` state and `Missouri` with a correlation of `0.932`.\n- **Correlation between locale**\n    - Most of the locale have a high positive correlation above `0.8`.\n    - `Suburb` and `Rural` have the highest correlation of `0.98` which is almost 1.","a22d1f91":"[back to top](#table-of-contents)\n<a id=\"5.2\"><\/a>\n## 5.2 Black\/Hispanic\nBlack\/Hispanic is a percentage of students in the school districts that are identified as Black or Hispanic based on 2018-19 NCES data.\n\n**Observations:**\n- Most school districts have `0%-20%` of black\/hispanic which also consistent with the highest number of page-load in year 2020.\n- In a point of view black\/hispanic, higher number of school districts resulting into a higher number page-load.\n- `Unknown` percentage black\/hispanic and state has the highest page-load that are more than `500 million` load.\n- There are 3 states that are worth mentioning, they are `Connecticut`, `Massachusetts` and `Illinois`. These state has more than `200 million` of page-load in school districts that have `0%-20%` of black\/hispanic.\n- In a range of `20%-40%` of black\/hispanic, there are `Illinois` with `63.9 million` page-load and `Utah` with `51 million` page-load.\n- The most page-load for a `60%-80%` black\/hispanic in school districts can be found in `Illinois` with `50 million` page-load.\n- A `30 million` page-load can be found in `New York` state with `80%-100%` black\/hispanic.","3cc84abd":"[back to top](#table-of-contents)\n<a id=\"4.5\"><\/a>\n## 4.5 Top Products & Providers\nAs stated before, there are `369` products (including `Unknown`) that have been successfully mapped in 2020. This mapped products will be used as the basis of the analysis. In this section we are trying to see: \n1. Most used products by students in 2020 using page-load as the basis.\n2. Top providers in 2020 based on the page-load.\n\n**Observations:**\n- **Products**\n    - `Four` of top 5 products (excluding `Unknown`) are managed by `Google LLC`, they are `Google Docs`, `Google Classroom`, `Youtube` and `Meet`. `Google Docs` is the most used products in 2020, it had `769.9 million` page-load`,`Google Classroom`, is in the 3rd place with `373.6 million` page-load. `Youtube` and `Meet` are in position `four` and `five` respectively. \n    - There is 1 product in the top 5 products that doesn't belong to `Google LLC`, the name of the product is `Canvas`. This product is owned by `Instructure, Inc.` and is in the `3rd` position.\n    - `Unknown` is in the second place with `417.1 million` page-load. We can assume it was coming from many products.\n- **Providers**\n    - As expected, `Google LLC` has the highest page load outperforming any others providers.\n    - `Instructure, Inc.`, a company that made `Canvas` is in the third place with `138 million` page-load in 2020.\n    - In 4th place, we can see there is `Kahoot! AS` with `87 million` page-load ","d382dd77":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n# 4 Products\nEngagement dataset represents on how many products (in a school district) that have been accessed by students in a daily basis for year 2020 with the total of `22 million` product accessed in 2020. There are `8,646` products but only `368` products that have been successfully mapped using the `products_info` dataset, unmapped products are categorized as `Unknown`. \n\nTo make a little bit clearer:\n- The dataset is presented in a daily basis.\n- A product will only one product per school district if there is an accessed to the product.\n- There can be 2 or more recore on the same products in the dataset if the product is accessed by two or more different school districts.\n\nIn this part we will also find some analysis related to trend:\n- We will look into the mean `accessed products`. \n- Go deeper by looking into `page-load` that is provided in the dataset.\n- How many products that have been used in a daily basis.\n\n<a id=\"4.1\"><\/a>\n## 4.1 Accessed Products\nThe analysis will focus on mean `accessed products`. What and how will we calculate the mean of `accessed products`? Every observations in the engagement dataset represent an `accessed product` in a school district. We can calculate how many products have been used in a school district and take the average from them. We will see the average of total products that has been used per school district. \n\n**Observations**:\n- Mean daily `accessed products` is around 300 - 500 per school districts in 2020. There is an increased in the daily mean of `accessed products` after the `summer holiday`. Are the students in every school districts using a more diversify products or there are new products to support digital learning?\n- In every month there are volatility in the trend, that follow an order of `5-2` which are `5 days of school` and `2 days of weekend`.\n- Though the number of `accessed products` decreased in the weekend but we still see `accessed products` in the weekend, number of `accessed products` in the weekend are about `25%-50%` from the weekday. Does it mean there are still many students that studying in the weekend?\n- In the mid-February, there is an temporary school closures followed by WHO that characterized COVID-19 as a pandemic on March 11th 2020. We can see there is an increased on `accessed products` starting from mid-February, though the increased are marginal.\n- `Summer holiday` in the United States are differ between schools districts, usually start in `late May \/ early June` and end in `late August \/ early September`, this consistent with lower `accessed products` on those dates but it can be considered high compared to regular date. Once again, does it mean there are many students that still study in the holiday? It seems weird.","70527b3a":"### 3.1.2 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","40d68973":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n- [1 Introduction](#1)\n- [2 Preparations](#2)\n- [3 Datasets Overview](#3)\n    - [3.1 Engagement](#3.1)\n    - [3.2 Districts](#3.2)\n    - [3.3 Products](#3.3)\n    - [3.4 Combine](#3.4)\n- [4 Product](#4)    \n    - [4.1 Accessed Products](#4.1)\n    - [4.2 Page Load](#4.2)\n    - [4.3 Number of Products](#4.3)\n    - [4.4 First Accessed](#4.4)\n    - [4.5 Top Products & Providers](#4.5)\n    - [4.6 Category & Sectors](#4.6)\n- [5 Demographic](#5)\n    - [5.1 Geographic](#5.1)\n        - [5.1.1 Geographic and page load](#5.1.1)\n        - [5.1.2 Geographic and page load correlation](#5.1.2)\n    - [5.2 Black \/ Hispanic](#5.2)\n    - [5.3 County Connection Ratio](#5.3)\n    - [5.4 Per-pupil Total Expenditure](#5.4)\n    - [5.5 Free or Reduced Price](#5.5)\n- [References](#ref)","435f66ed":"[back to top](#table-of-contents)\n<a id=\"4.6\"><\/a>\n## 4.6 Category & Sectors \nIn the first part we would like to see how `category` and `sectors` relates to the `page-load`.  In the perspective of `sector`, a product can be classified into more than 1 sector, there are `5 sectors` excluding the `unknown`. There are `3 categories` in the dataset that are described below:\n1. LC = Learning & Curriculum\n2. CM = Classroom Management \n3. SDO = School & District Operations\n\n**Observations:**\n- **Category**\n    - `Learning & Curriculum` has the highest page-load reaching `1.6 billion` in 2020, this is a good sign as products are used for students to study.\n    - `School & District Operations` is in the second place with `494.9 million` page-load.\n    - `Classroom Management` is in the third place (excluding `Unknown`) with `187.4 million` of page-load.\n- **Sectors**\n    - Almost `2 billion` page-load are coming from products that can be categorize into 3 sector: `PreK-12; Higher Ed; Corporate`.\n    - `Unknown` sectors is in the second place with `424.6 million` page-load.\n    - In the 3rd place there is `PreK-12` with `391.9 million` page-load.","3e9cba29":"### 3.4.1 Quick view\nBelow is the first 5 rows of `combine` dataset:","139c0599":"[back to top](#table-of-contents)\n<a id=\"5.3\"><\/a>\n## 5.3 County Connection Ratio\nCounty Connection Ratio is residential fixed high-speed connections over 200 kbps in at least one direction\/households.\n\n**Observations:**\n- There is only 1 state that has a connection ratio between `100% - 200%` which is `North Dakota`.\n- Most of the state are in the county connection ratio between `18% - 100%`.","adf9083d":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2 Preparations\nWe are preparing packages and source data that will be used in the analysis process. Python packages that will be used in the analysis mainly are for data manipulation (`numpy` and `pandas`) and data visualization (`matplotlib` and `seaborn`). Engagement data are stored by district, we will merge all the individual files under `engagement_data` folder into 1 dataframe.\n\n*(to see the details, please expand)*","28c83b8b":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1 Introduction\n\nNelson Mandela believed education was the most powerful weapon to change the world. But not every student has equal opportunities to learn. Effective policies and plans need to be enacted in order to make education more equitable\u2014and perhaps your innovative data analysis will help reveal the solution.\n\nCurrent research shows educational outcomes are far from equitable. The imbalance was exacerbated by the COVID-19 pandemic. There's an urgent need to better understand and measure the scope and impact of the pandemic on these inequities.\n\nEducation technology company LearnPlatform was founded in 2014 with a mission to expand equitable access to education technology for all students and teachers. LearnPlatform\u2019s comprehensive edtech effectiveness system is used by districts and states to continuously improve the safety, equity, and effectiveness of their educational technology. LearnPlatform does so by generating an evidence basis for what\u2019s working and enacting it to benefit students, teachers, and budgets.\n\nThis analytics competition expects to uncover trends in digital learning. Accomplish this with data analysis about how engagement with digital learning relates to factors like district demographics, broadband access, and state\/national level policies and events.\n\nThe submissions will inform policies and practices that close the digital divide. With a better understanding of digital learning trends, you may help reverse the long-term learning loss among America\u2019s most vulnerable, making education more equitable.\n\n**Problem Statement**\n\nThe COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow.","56a3d7d2":"[back to top](#table-of-contents)\n<a id=\"3.4\"><\/a>\n## 3.4 Combine\nWe will merge `engagement`, `districts` and `products` datasets into 1 big dataset called `combine` that consist all of the information from all dataset and we will delete existing dataset to free up some memory.\n\n*(to see the details, please expand)*","14bf31ff":"### 3.1.1 Quick view\nBelow is the first 5 rows of `engagement` dataset:","5bf222e3":"[back to top](#table-of-contents)\n<a id=\"3.3\"><\/a>\n## 3.3 Products\n\nThe product file includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy. Data were labeled by LearnPlatform team. Some products may not have labels due to being duplicate, lack of accurate url or other reasons.\n\nThis dataset consists of below information:\n- **LP ID:** The unique identifier of the product\n- **URL:** Web Link to the specific product\n- **Product Name:** Name of the specific product\n- **Provider\/Company Name:** Name of the product provider\n- **Sector(s):** Sector of education where the product is used\n- **Category:** The basic function of the product. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. \n- **Subcategory:** Each of these categories have multiple sub-categories with which the products were labeled\n\n**Observations:**\n- There are `372` rows with `7` columns as mentioned above. \n- This dataset contain missing value of `61` which mainly come from `Sectors(s)`, `Category`, `Subcategory` with each of them has `20` missing values and `1` missing value on `Provider\/Company Name`.","0a62e5ca":"[back to top](#table-of-contents)\n<a id=\"5.5\"><\/a>\n## 5.5 Free or Reduced Price\nPercentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data. We would like to see how `free or reduced price` relates to page-load. We may have 2 assumptions which are:\n1. Higher reduced price will have higher page-load, as cheaper products will increase number of students that use digital learning product which will increased the page-load.\n2. We may expect higher reduced price will not have higher page-load, as there are not many school districts get high reduced price. \n\n**Observations:**\n- **Free or Reduced Price & Page-Load**\n    - Sadly, most of `Free or Reduced Price` is unknown which contribute of `936 million` page-load.\n    - From the remaining information, we see that most of the page-load come from school districts that get `0%-20%` reduced price.\n    - We can see that the number page-load continue decreasing as percentage of `reduced price` increased, this may happen due to low school districts that get high reduced price.\n- **Reduced Price & Per-pupil Total Expenditure**\n    - Before dive into the heat map, in this analysis we are excluding the NaN figures which have a total of `1.1 billion` page load instead of `2.8 billion`.\n    - A combination of `40%-60%` reduced price and `8,000-10,000` per-pupil total expenditure has the highest page-load of `141.1 million`.\n    - The next 2 highest page-load are coming from page-load of around `100 million` which are:\n        1. A combination of `0%-20%` reduced price and `10,000-12,000` and \n        2. A combination of `20%-40%` reduced price and `8,000-10,000`","2b740157":"[back to top](#table-of-contents)\n<a id=\"ref\"><\/a>\n# References\n\n- https:\/\/www.who.int\/news\/item\/27-04-2020-who-timeline---covid-19\n- https:\/\/www.edweek.org\/leadership\/the-coronavirus-spring-the-historic-closing-of-u-s-schools-a-timeline\/2020\/07","c8da9224":"### 3.3.1 Quick view\nBelow is the first 5 rows of `products` dataset:","08cab482":"[back to top](#table-of-contents)\n<a id=\"5.4\"><\/a>\n### 5.4 Per-pupil Total Expenditure \nPer-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district. We would like to see how per-pupil total expenditure relates to page-load. We may have 2 assumptions which are:\n1. Higher total expenditure will have higher page-load, as this relate to money that have been spent.\n2. We may expect higher total expenditure will not have higher page-load, as there are not many school districts have high budget spending. \n\n**Observations:**\n- **Page Load**\n    - Unfortunately, almost half of the information on per-pupil total expenditure is tagged as unknown.\n    - From the remaining information, we see that most of the page-load come from school districts that has `8,000 - 10,000` expenditures.\n    - We also see that school districts that have expenditure in a range from `10,000` to `18,000` are having more than `200 million` of page-load in 2020.\n    - Two lowest page-load are coming from `4,000 - 6,000` and `32,000 - 34,000` which are the lowest and highest spending range in the dataset.\n    \n- **School Districts**\n    - `Page-load` in a category is inline with the numbers of `school districts` in a category. \n    - `Unknown` has a highest number of `school districts`, it's also has the highest `page-load`. \n    - There are `30` school districts that have `8,000 - 10,0000` of total expenditures which also explain higher number of `page-load`.","4dd71beb":"[back to top](#table-of-contents)\n<a id=\"4.3\"><\/a>\n## 4.3 Number of Products\nThere are `8,647` products available in 2020, let's see how many products have been used in a daily basis. Are there any changes in the number of product used?\n\n**Observations:**\n- Range of products used in 2020 are around `2,000 - 4,000` from the total of `8,647`.\n- Number of products used before the temporary school closures is below `3,000` and increased to above `3,000` thereafter. This increment indicates an increased in digital learning as supported by daily page-load before.\n- Number of product used in weekend and summer holiday don't decrease significantly as in `mean daily accessed`. there is a consistency of the product used even in weekend and summer holiday but with a lower activity.","493924d6":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3 Dataset Overview\nThe overview is prepared to get the feel on data structure. It will also include a quick analysis on missing values, basic statistics and data manipulation. In general there will 3 datasets: `engagement`, `districts` and `products`.\n\n<a id=\"3.1\"><\/a>\n## 3.1 Engagement\n\nThe engagement data are aggregated at school district level, and each file represents data from one school district. The 4-digit file name represents `district_id` which can be used to link to district information in `district_info`. The `lp_id` can be used to link to product information in `product_info`.\n\nThis dataset consists of below information:\n- **time:** date in \"YYYY-MM-DD\"\n- **lp_id:** The unique identifier of the product\n- **pct_access:** Percentage of students in the district have at least one page-load event of a given product and on a given day\n- **engagement_index:** Total page-load events per one thousand students of a given product and on a given day\n\n**Observations:**\n- There are `22,324,190` rows with `5` columns as mentioned above. \n- This dataset contain missing value of `5,392,397` which come from `lp_id` of `541`, `pct_access` of `13,447` and `engagement_index` `5,378,409`. Missing value in the `engagement_index` can be considered big as it consist of `24.15%` from total observation.","edfb268b":"[back to top](#table-of-contents)\n<a id=\"4.4\"><\/a>\n## 4.4 First Accessed\nIt would be interesting to know when the first time a product has been accessed in relation to digital learning. We would like to expect there is an increased of new accessed products in the COVID-19 situation. This increased may come from a new products (an opportunity for education business) or an old products that recently be accessed. The chart will show the total number of new products accessed in a specific date.\n\n**Observations:**\n* In the start of 2020, we see there are a high new accessed products. This may come from older products that have been used and available before the pandemic.\n* The interesting part is there are a jump of new products accessed after the announcement of temporary school disclosure.\n* We can also barely see, there are many new products accessed in the summer holiday and also a little bit jump in the beginning of November 2020."}}