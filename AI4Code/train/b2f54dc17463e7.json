{"cell_type":{"c7242a62":"code","414e4dc2":"code","6bcf0a2f":"code","203abb5c":"code","b3ecae24":"code","bb152df0":"code","8f10f384":"code","a0e5e16e":"markdown","8cfd28f6":"markdown"},"source":{"c7242a62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","414e4dc2":"%pip install -q easy-tensorflow==1.1.2","6bcf0a2f":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization, IntegerLookup, StringLookup\n\n# import from easyflow\nfrom easyflow.data.mapper import TensorflowDataMapper\nfrom easyflow.preprocessing import Pipeline, FeatureUnion","203abb5c":"dataframe = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nlabels = dataframe.pop(\"target\")\n\nbatch_size = 32\ndataset_mapper = TensorflowDataMapper() \ndataset = dataset_mapper.map(dataframe, labels)\ntrain_data_set, val_data_set = dataset_mapper.split_data_set(dataset)\ntrain_data_set = train_data_set.batch(batch_size)\nval_data_set = val_data_set.batch(batch_size)\n\nNUMERICAL_FEATURES = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope']\nCATEGORICAL_FEATURES = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'ca', 'thal']","b3ecae24":"feature_encoder_list = [\n                        ('numeric_encoder', Normalization(), NUMERICAL_FEATURES),\n                        ('categorical_encoder', IntegerLookup(output_mode='binary'), CATEGORICAL_FEATURES)\n                        ]\n\nencoder = FeatureUnion(feature_encoder_list)\nall_feature_inputs, preprocessing_layer = encoder.encode(train_data_set)","bb152df0":"# setup simple network\nx = tf.keras.layers.Dense(128, activation=\"relu\")(preprocessing_layer)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(inputs=all_feature_inputs, outputs=outputs)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), tf.keras.metrics.AUC(name='auc')])\n\nhistory=model.fit(train_data_set, validation_data=val_data_set, epochs=10)","8f10f384":"model.predict(val_data_set)[:10]","a0e5e16e":"## Setup Preprocessing layer using FeatureUnion","8cfd28f6":"# Easy Tensorflow:\n\nAn interface containing easy tensorflow model building blocks and feature encoding pipelines. This module contains functionality similar to what sklearn does with its Pipeline, FeatureUnion and ColumnTransformer. \n\nPipeline types:\n- Encoder: Preprocess feature based on specified preprocessing layer \n- SequentialEncoder: Preprocessing pipeline to apply multiple encoders in serie\n- Pipeline:  Main interface for transforming features. Apply feature encoder list which can contain both Encoder and SequentialEncoder object types\n- FeatureUnion: Similar tp Pipeline, but concats the Layers from Pipeline"}}