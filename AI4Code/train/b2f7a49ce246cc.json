{"cell_type":{"05cd3518":"code","b9104cd2":"code","e18ba94e":"code","f7cec954":"code","b9375da1":"code","be081f62":"code","54901351":"code","082f7b0c":"code","477b7730":"code","59e736f3":"code","c98ced20":"code","ef2a5121":"code","c37b329d":"code","7e397dea":"code","e6f59c23":"code","ff235bef":"code","ef7d18fd":"code","761af077":"code","87eb7c89":"code","fac5e6f3":"code","109344e0":"code","57627c1d":"code","386442e8":"code","d4bc060b":"code","9d478aa1":"code","e57bb71d":"code","1f7163c1":"code","76177e9f":"code","c24a22d1":"code","53ae839f":"code","c0457dae":"code","6aaebbd1":"code","b3ce5c5b":"code","2b6dc0af":"code","9d34b38c":"code","ded75772":"code","edc835c4":"code","06e77e1c":"code","93705947":"code","429d9d14":"code","0c991af4":"code","92c99a0e":"code","1d3c24fb":"code","d270a1ab":"code","f34d1a45":"code","77cda145":"code","f1fc5ae2":"code","ff45af04":"code","f64565a3":"code","b8d31b8c":"code","250f5873":"code","198a6dd7":"markdown","7bcc1281":"markdown","5cda0b27":"markdown","b47b89f8":"markdown","04420cc1":"markdown","ab8f24d0":"markdown","8a4692a1":"markdown","f97ba703":"markdown","484f39bc":"markdown","9a0223b2":"markdown","3aa82497":"markdown","5998f2cc":"markdown","4559dc62":"markdown","eddcbc74":"markdown","2f8e960e":"markdown","aafcf44a":"markdown","a4cc9506":"markdown","16b2bca6":"markdown","a85b6383":"markdown","33a2c1b1":"markdown","3325d93e":"markdown","4dac9a73":"markdown","6750cf6f":"markdown","43f98bd0":"markdown","6a6453bb":"markdown","ffc8a970":"markdown","60560b4d":"markdown","3bb60d10":"markdown","1ae5b801":"markdown","4eab8e21":"markdown","98df821d":"markdown","2ffeb88e":"markdown","529cf217":"markdown","07d1f08d":"markdown","db4ddf45":"markdown","b26456aa":"markdown","5dba17a6":"markdown","bc6160ad":"markdown"},"source":{"05cd3518":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9104cd2":"#libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n#algorithms\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\n#other\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, KFold\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","e18ba94e":"dataset = pd.read_csv('..\/input\/early-stage-diabetes-risk-prediction-dataset\/diabetes_data_upload.csv')\ndataset.head()\n","f7cec954":"dataset.tail()","b9375da1":"for f in dataset.columns:\n    print(f'{f} {dataset[f].unique()}')","be081f62":"dataset['class'] = dataset['class'].map( {'Positive': 1, 'Negative': 0} ).astype(int)","54901351":"g = sns.countplot('class', data=dataset, palette=\"muted\")\ng.set_xticklabels([\"Negative\", \"Positive\"])\nprint('Positive percentage: ',round(dataset['class'].value_counts()[1]\/len(dataset) * 100,2),'%')\nprint('Negative percentage: ',round(dataset['class'].value_counts()[0]\/len(dataset) * 100,2),'%')","082f7b0c":"dataset.isnull().sum()","477b7730":"features =dataset.columns[1:-1]\nfeatures\n","59e736f3":"number=LabelEncoder()\nfor feature in features:\n    dataset[feature]=number.fit_transform(dataset[feature].astype('str'))","c98ced20":"corrdata = dataset.corr()\nax,fig = plt.subplots(figsize=(15,8))\nsns.heatmap(corrdata,annot=True)","ef2a5121":"#Explore Age distribution\nsns.distplot(dataset['Age'],bins=30)","c37b329d":"g = sns.kdeplot(dataset[\"Age\"][(dataset[\"class\"] == 0)], color=\"Blue\", shade = True)\ng = sns.kdeplot(dataset[\"Age\"][(dataset[\"class\"] == 1)], ax =g, color=\"Red\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Negative\",\"Positive\"])","7e397dea":"# Explore Gender vs Class\ng  = sns.catplot(x=\"Gender\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"Female\", \"Male\"])\ng.set_ylabels(\"diabetes probability\")","e6f59c23":"# Explore Polyuria vs Class\ng  = sns.catplot(x=\"Polyuria\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","ff235bef":"# Explore Polyuria vs class by Gender\ng = sns.catplot(x=\"Polyuria\", y=\"class\", hue=\"Gender\", data=dataset,\n                   height=6, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"diabetes probability\")\ng.set_xticklabels([\"No\", \"Yes\"])\nnew_labels = ['Female', 'Male']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)","ef7d18fd":"# Explore Polydipsia vs Class\ng  = sns.catplot(x=\"Polydipsia\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","761af077":"# Explore Polydipsia vs class by Gender\ng = sns.catplot(x=\"Polydipsia\", y=\"class\", hue=\"Gender\", data=dataset,\n                   height=6, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"diabetes probability\")\ng.set_xticklabels([\"No\", \"Yes\"])\nnew_labels = ['Female', 'Male']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)","87eb7c89":"# Explore Sudden weight loss vs Class\ng  = sns.catplot(x=\"sudden weight loss\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","fac5e6f3":"# Explore Genital thrush vs Class\ng  = sns.catplot(x=\"Genital thrush\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","109344e0":"# Explore Weakness vs Class\ng  = sns.catplot(x=\"weakness\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","57627c1d":"# Explore Polyphagia vs Class\ng  = sns.catplot(x=\"Polyphagia\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","386442e8":"# Explore Visual blurring vs Class\ng  = sns.catplot(x=\"visual blurring\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","d4bc060b":"# Explore Itching vs Class\ng  = sns.catplot(x=\"Itching\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","9d478aa1":"# Explore Irritability vs Class\ng  = sns.catplot(x=\"Irritability\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","e57bb71d":"# Explore Delayed healing vs Class\ng  = sns.catplot(x=\"delayed healing\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","1f7163c1":"# Explore Partial paresis vs Class\ng  = sns.catplot(x=\"partial paresis\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","76177e9f":"# Explore Muscle stiffness vs Class\ng  = sns.catplot(x=\"muscle stiffness\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","c24a22d1":"# Explore Alopecia vs Class\ng  = sns.catplot(x=\"Alopecia\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","53ae839f":"# Explore Obesity vs Class\ng  = sns.catplot(x=\"Obesity\",y=\"class\",data=dataset,kind=\"bar\",palette = \"muted\")\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_ylabels(\"diabetes probability\")","c0457dae":"#split the class column from the features\nX = dataset.drop(labels = [\"class\"],axis = 1)\ny = dataset['class']\nX","6aaebbd1":"#split the data to train and test\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=0)","b3ce5c5b":"classifiers = []\n\nclassifiers.append(SVC())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier()))\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(GradientBoostingClassifier())\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression())\nclassifiers.append(SGDClassifier())\n\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n \ncv_res = pd.DataFrame({\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\"RandomForest\",\"GradientBoosting\",\"KNeighboors\",\n                                    \"LogisticRegression\",\"SGDClassifier\"],\"Cross Val Means\":cv_means,\"Cross Val errors\": cv_std})\ncv_res","2b6dc0af":"#plot models scores using plotly\nfig = go.Figure()\nfig.add_trace(go.Bar(\n            x=cv_means,\n            y=cv_res.iloc[:,0],\n            error_x=dict(type='data', array=cv_std),\n            orientation='h',\n            marker=dict(color = cv_means,\n                     colorscale='Portland')))\nfig.update_layout(\n    width=600,\n    height=500,\n    title_text='Cross Validation Scores',\n    xaxis=dict(\n        title='Mean Accuracy'))\nfig.show()","9d34b38c":"#MODELING  WITH ADABOOST, RANDOM FOREST, SVM, GRADIENTBOOSTING and LOGISTIC REGRESSION\n\n# Adaboost\nDTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC)\n\n# Grid search for optimal parameters\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.001, 0.01, 0.1, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", verbose = 1)\n\ngsadaDTC.fit(X_train,y_train)\n\n#model best estimator\nada_best = gsadaDTC.best_estimator_\n\n#best score\ngsadaDTC.best_score_","ded75772":"# Random Forest\nRFC = RandomForestClassifier()\n\nrf_param_grid = {\"max_depth\": [None,4],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nrf_best = gsRFC.best_estimator_\n\ngsRFC.best_score_","edc835c4":"# Gradient Boosting \nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\ngb_best = gsGBC.best_estimator_\n\ngsGBC.best_score_","06e77e1c":"# SVM \nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf','linear'], \n                  'gamma': [ 0.01, 0.1, 1],\n                  'C': [1, 10, 100, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nsvm_best = gsSVMC.best_estimator_\n\ngsSVMC.best_score_","93705947":"#Logistic Regression\nLR = LogisticRegression()\ngb_param_grid = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n              'max_iter' : [10,100,200],\n              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n              }\ngsLR = GridSearchCV(LR,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", verbose = 1)\ngsLR.fit(X_train,y_train)\nlr_best = gsLR.best_estimator_\n\ngsLR.best_score_","429d9d14":"votingC = VotingClassifier(estimators=[('rfc', rf_best),('svmc', svm_best),\n                                       ('adac',ada_best),('gbc',gb_best),('lrc',lr_best)], voting='soft')\n\nvotingC = votingC.fit(X_train, y_train)\nvotingC.score(X_test,y_test)","0c991af4":"rf_features = rf_best.feature_importances_\nada_features = ada_best.feature_importances_\ngb_features = gb_best.feature_importances_\n","92c99a0e":"#Plot model features importances function\ndef get_features_importances(title,features,features_importances):\n    trace = go.Scatter(\n        y = features_importances,\n        x = features,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 25,\n            color = features_importances,\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = features\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        title= title,\n        hovermode= 'closest',\n        width=600,\n        yaxis=dict(\n            title= 'Feature Importance',\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    fig.show()","1d3c24fb":"features = dataset.columns[:-1]\nget_features_importances('Random Forest Feature Importance',features,rf_features)\nget_features_importances('Adaboost Feature Importance',features,ada_features)\nget_features_importances('Gradient Boosting Feature Importance',features,gb_features)","d270a1ab":"def get_out_of_fold(clf, x_train, y_train, x_test,kf,NFOLDS):\n    oof_train = np.zeros((x_train.shape[0],))\n    oof_test = np.zeros((x_test.shape[0],))\n    oof_test_skf = np.empty((NFOLDS, x_test.shape[0]))\n\n    for i, (train_index, validation_index) in enumerate(kf.split(x_train,y_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_va = x_train[validation_index]\n\n        clf.fit(x_tr, y_tr)\n\n        oof_train[validation_index] = clf.predict(x_va)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    \n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","f34d1a45":"NFOLDS=5\nsvc_oof_train, svc_oof_test = get_out_of_fold(svm_best,X_train.values, y_train.values, X_test.values,kfold,NFOLDS) # Support Vector Classifier\nrf_oof_train, rf_oof_test = get_out_of_fold(rf_best,X_train.values, y_train.values, X_test.values,kfold,NFOLDS) # Random Forest\nada_oof_train, ada_oof_test = get_out_of_fold(ada_best, X_train.values, y_train.values, X_test.values,kfold,NFOLDS) # AdaBoost \ngb_oof_train, gb_oof_test = get_out_of_fold(gb_best,X_train.values, y_train.values, X_test.values,kfold,NFOLDS) # Gradient Boost\nlr_oof_train, lr_oof_test = get_out_of_fold(lr_best,X_train.values, y_train.values, X_test.values,kfold,NFOLDS) # Logistic Regression","77cda145":"#First level output as new features\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel(),\n      'SVM': svc_oof_train.ravel(),\n      'LR': lr_oof_train.ravel()                                 \n    })\nbase_predictions_train.head()","f1fc5ae2":"#concatenate the predictions from all the models, creating the new train data\nx_train = np.concatenate(( rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train,lr_oof_train), axis=1)\nx_test = np.concatenate(( rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test, lr_oof_test), axis=1)","ff45af04":"rfs=rf_best.fit(x_train, y_train)\npredictions = rfs.predict(x_test)\npredictions","f64565a3":"rfs.score(x_test,y_test)","b8d31b8c":"votingC = VotingClassifier(estimators=[('rfc', rf_best),('svc', svm_best),\n                                       ('ada',ada_best),('gbc',gb_best),('lrc',lr_best)], voting='hard')\n\nvotingC = votingC.fit(x_train, y_train)\nvotingC.score(x_test,y_test)","250f5873":"sklearn_stacking =StackingClassifier(estimators=[('rfc', rf_best),('svc', svm_best),\n                                       ('adac',ada_best),('gbc',gb_best),('lrc',lr_best)])\nsklearn_stacking = sklearn_stacking.fit(x_train, y_train)\nsklearn_stacking.score(x_test,y_test)","198a6dd7":"# **Flow:**\n1.  [Importing the data](#importing)\n1.  [Data preprocessing](#preprocessing)\n1.  [Modeling](#modeling)\n    *      3.1 [Test different algorithms](#dif)\n    *      3.2 [Basic ensembling](#basic)\n    *      3.3 [Advanced ensembling](#advanced)","7bcc1281":"* **3.1 Test different algorithms:**","5cda0b27":"![stacking_ML.jpg](attachment:stacking_ML.jpg)","b47b89f8":"* Before moving to the advanced ensembling, I will add a step of checking the features importances:","04420cc1":"* **Step 1:** Run Grid Search.","ab8f24d0":"**Introduction:**\n\n**About the dataset:**\n* Diabetes is a serious, long-term condition and It is among the top 10 causes of death in adults. Just under half a billion people are living with diabetes worldwide and the number is projected to increase by 25% in 2030 and 51% in 2045.\n* The early stage diabetes risk prediction dataset features has been collected using direct questionnaires from the patients of a hospital in Bangladesh and approved by a doctor.\n* In this notebook I will use Machine Learning classification models and ensembling methods for early stage diabetes prediction.\n","8a4692a1":"<div id='importing'><\/div>","f97ba703":"<div id='preprocessing'><\/div>","484f39bc":"<div id='advanced'><\/div>","9a0223b2":"*  **Using Scikit learn StackingClassifier:**","3aa82497":"![stacking_cv_classification_overview.png](attachment:stacking_cv_classification_overview.png)","5998f2cc":"<div id='basic'><\/div>","4559dc62":"From the simple visualization we can conclude that most of the features have a positive relationship with the probability of being diagnosed as diabetic.","eddcbc74":"* **Correlation Matrix:**","2f8e960e":" Stacking\nUsed to ensemble a diverse group of strong learners.\nInvolves training a second-level machine learning algorithm called a \u201cmetalearner\u201d to learn the optimal combination of the base learners.","aafcf44a":"<div id='modeling'><\/div>","a4cc9506":"* **We will optimize our stacking model by adding Majority\/Hard Voting Classifier to choose the best meta model for learning:**\n> Hard voting is the simplest case of majority voting. We predict the class label via majority voting of each classifier.","16b2bca6":"Note: there is a high improvement in the SVM Classifier accuracy after using the Grid Search.","a85b6383":"* **Importing necessary libraries:**","33a2c1b1":"# 2. Preprocessing:","3325d93e":"# 1. Importing the data","4dac9a73":"* **Convert class values to numeric  0,1 instead of Negative, Positive:**","6750cf6f":"* **Second Level prediction, running a meta learner model on the new x train. (I arbitrarily choose the random forest classifier as the meta learner)**","43f98bd0":"* **Load and check data:**","6a6453bb":"* **Check uniqu values for every feature:**\n","ffc8a970":"* **3.2 Basic Ensembling**:","60560b4d":"# 3. Modeling:","3bb60d10":"* **Check for null values:**","1ae5b801":"* **Check unplanced data:**","4eab8e21":" **The dataset seems ready for modeling :)**","98df821d":"Step 1: Feeding the training and test data into our models\/classifiers and use the Out_Of_Fold prediction function to generate our first level predictions:\n* repeat K times:\n1. >  split the **training data** to **train folds** and **validation fold**.\n1. >  train the classifier on the train folds.\n1. >  make prediction on the validation fold.\n1. >  make prediction on the test data (on the whole test data without making any changes).\n\nreturn the predictions on the validation folds\nand the mean of the k predictions on the test data.\n\nStep 2: Second-Level Predictions from the First-level Output.","2ffeb88e":"<div id='dif'><\/div>","529cf217":"# Early Stage Diabetes Risk Prediction Dataset with Ensembling","07d1f08d":"* **Step 2**: Run Soft Voting Classifier.","db4ddf45":"I choose the models (Adaboost ,Random Forest ,Gradient Boosting ,SVM ,Logistic Regression) for this part, first I will do an optimization step:\n> * **Step 1**: Run Grid search on every model to get the best estimator of it.\n> * **Step 2** : Run Soft Voting Classifier on the best estimators we got in step 1.\n> > In Soft Voting we predict the class labels based on the predicted probabilities p for classifier. First we calculate the probabilities and then predict the class label based on the argmax of the sums of the predicted probabilities. For example if the predicted probabilities from different 3 classifiers were: p1=0.2, p2=0.9, p3=0.7 , we sum the values and calculate the average (0.2 + 0.9 + 0.7)\/3=0.6 , the result is bigger than 0.5 so the logic prediction is 1.\n> \nnote: this part can take a long time, especially SVM part.","b26456aa":"* **Convert Categorical features to Numerical:**","5dba17a6":"**Ensembling:**\n* The idea of ensembling is combining the decisions from multiple models to improve the overall performance. The Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. Some of this methods:\n\n> * **Basic Ensemble Techniques:**\n>       1.Soft Voting\n      2.Majority\/Hard Voting\n      2.Averaging\n      3.Weighted Average\n\n> * **Advanced Ensemble Techniques:**\n>       1.Stacking\n      2.Blending\n      3.Bagging\n      4.Boosting\n      \n In this notebook I will use Soft Voting, Hard Voting and Stacking.\n \n With inspiration from:\n*  https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n*  https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python","bc6160ad":"* **3.3 Advanced Ensembling:**"}}