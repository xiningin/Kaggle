{"cell_type":{"fb67d7e2":"code","e53edbe3":"code","147ce836":"code","35c48123":"code","4909ec52":"code","938c4573":"code","a998be07":"code","c709ca41":"code","28d4de78":"code","e70322c5":"code","ec7ce2eb":"code","8d7bbf9b":"code","33883bb5":"code","a67fdc95":"code","f35fb5d3":"code","5c652159":"code","d8595064":"code","f577f1cd":"markdown","91133d79":"markdown","acc7b006":"markdown","b8c14cc5":"markdown","6c999a65":"markdown","28550efd":"markdown","09698e16":"markdown","81c4cad6":"markdown","4dc8cf2f":"markdown","2a69e8c3":"markdown"},"source":{"fb67d7e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport plotly.offline as py\nimport statsmodels\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport pandas_profiling\nfrom math import sqrt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score, TimeSeriesSplit\nimport statsmodels.api as sm\nimport matplotlib.animation as animation\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e53edbe3":"df=pd.read_csv(\"\/kaggle\/input\/earthquakes-in-turkey\/Catalogue.csv\",encoding = \"ISO-8859-1\")\ndf=df.rename(columns={\"Enlem\": \"latitude\", \"Boylam\": \"longtitude\", \"B\u00fcy\u00fckl\u00fck\":\"Magnitude\", \"Derinlik\":\"Depth\", \"Zaman (UTC)\":\"Time\", \"Tip\":\"Type\"})\n","147ce836":"df['Time']=pd.to_datetime(df['Time'], errors='coerce')\ndf.columns","35c48123":"df=df[['Time',  'latitude', 'longtitude', 'Magnitude', 'Depth', 'Type' ]]","4909ec52":"df.profile_report()","938c4573":"fig = go.Figure(go.Densitymapbox(lat=df.latitude, lon=df.longtitude, z=df.Magnitude, radius=5))\nfig.update_layout(mapbox_style=\"stamen-terrain\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","a998be07":"fig = px.line(df, x='Time', y='Magnitude')\nfig.show()","c709ca41":"#And depth:\nfig = px.line(df, x='Time', y='Depth')\nfig.show()","28d4de78":"fig=plt.figure()\nax=fig.add_subplot(111)\nax.plot(df['Magnitude'], df['Depth'], '*')\nplt.axvspan(6, 8, color='red', alpha=0.5)\nplt.xlabel('magnitude')\nplt.ylabel('depth')\nplt.show()","e70322c5":"data=pd.DataFrame(df['Magnitude'].values, index=df['Time'])\ndata.columns = [\"y\"]","ec7ce2eb":"for i in range(150, 500): \n    data[\"lag_{}\".format(i)] = data.y.shift(i)    ","8d7bbf9b":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","33883bb5":"#--->>https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python\ndef timeseries_train_test_split(X, y, test_size):\n    test_index = int(len(X)*(1-test_size))\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","a67fdc95":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ny = data.y\nX = data.drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","f35fb5d3":"#--->>https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python\ntscv = TimeSeriesSplit(n_splits=3) \ndef plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(15, 7))\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=5.0)\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_absolute_error\")\n        mae = cv.mean() * (-1)\n        deviation = cv.std()\n        \n        scale = 1.96\n        lower = prediction - (mae + scale * deviation)\n        upper = prediction + (mae + scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond \/ lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    error = mean_absolute_percentage_error(prediction, y_test)\n    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);","5c652159":"from xgboost import XGBRegressor \nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train)","d8595064":"plotModelResults(xgb, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=False)","f577f1cd":"Check The depth-magnitude relationship","91133d79":"We are going to scale our data here is some reason for scaling data [Why, How and When to Scale your Features](https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e)","acc7b006":"Summarize data with pandas profiling library.","b8c14cc5":"We will use plolty to view the earthquakes on map.","6c999a65":" **Conclusion**\n \n 7.88%  looks pretty googd for such hard to predict. Future work can be optimize existing notebook and try some new approach on different kind of time serious. After this notebook i feel more confident about time series the will come stay tuned.\n Thanks for checking this notebook. Feel free to ask or correct about notebook.","28550efd":"Some cleaning\/correcting before the procces","09698e16":"In this kernel i will try do some analysis and try some ready-to-use models to get a good prediction on chaotic tim series example turkey's earthquake. The dataset contains earthquake over 4.0 magnitude. ","81c4cad6":"The approach i want to implement here is boosting which can be found here [**Time Series**](http:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python\/data)","4dc8cf2f":"Interactive viz for depth and magnitude with plolty","2a69e8c3":"Here we will get the last 250 observation to predict the next 250 ahead. More details can be found in the link above"}}