{"cell_type":{"30dc0cdb":"code","fec3f2bf":"code","af7559bc":"code","5a1b8a5a":"code","e8f95751":"code","dc38e74c":"code","d6b4c3b5":"code","cb8fa5ab":"code","0f6fdb32":"code","bc707832":"code","6c6de170":"code","2a4a449b":"code","1a0753af":"code","977d0adc":"code","14044b60":"code","40de806b":"code","26ee5021":"code","f24af323":"code","79b6d091":"code","40559fce":"code","98bd6f46":"code","425eff30":"code","792347c2":"code","11686d0f":"code","65b3736e":"code","ee00c7e7":"code","30d584ef":"code","de23f471":"code","c66e5e01":"code","f084755e":"code","f459ed93":"code","b3ef034e":"code","f122dbfc":"code","88ab3403":"code","8ad9d8c5":"code","0fbe5ba1":"code","a5cb9fd2":"code","7179204f":"code","f9e27fe8":"code","89cfdb9f":"code","894a9adc":"code","90e3d895":"code","52efb9d2":"code","9d8e27ac":"code","3cf6d6aa":"code","a1d83537":"code","464d6faf":"code","bcd79323":"code","2286ae67":"code","0578888b":"code","d895a12b":"code","6a9bde1e":"markdown","30915ff8":"markdown","fb214d7f":"markdown","b1115d19":"markdown","fc5598fd":"markdown","387eb5a3":"markdown","4017b032":"markdown","b1b9f6ee":"markdown","ef73dce2":"markdown","07c83719":"markdown","330d3961":"markdown","0102b450":"markdown","7d014cc0":"markdown","0a762e0d":"markdown","6812a61a":"markdown","630002ef":"markdown","cfad031e":"markdown","f0dc64f5":"markdown"},"source":{"30dc0cdb":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.feature_selection import RFE, f_regression\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso,LogisticRegression)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom math import sqrt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import mean_squared_error\n ","fec3f2bf":"df_test = pd.read_csv('..\/input\/restaurant-revenue-prediction\/test.csv.zip',parse_dates=['Open Date'])\ndf_test.head()","af7559bc":"df_train = pd.read_csv('..\/input\/restaurant-revenue-prediction\/train.csv.zip',parse_dates=['Open Date']) \ndf_train.head()","5a1b8a5a":"df_train.shape","e8f95751":"df_test.shape","dc38e74c":"plt.figure(figsize=(20,8))\n\ndf_train['Type'].value_counts().plot(kind='bar',\n                                    figsize=(14,8))\ndf_train['Type'].describe()","d6b4c3b5":"plt.figure(figsize=(20,8))\ndf_train['City'].value_counts().plot(kind='bar',\n                                    figsize=(14,8))\ndf_train['City'].describe()","cb8fa5ab":"plt.figure(figsize=(20,10))\nsns.distplot(df_train['revenue'])","0f6fdb32":"#lets make some tranformation in our dataset\nle =LabelEncoder()\ndf_train['City Group'] = le.fit_transform(df_train['City Group'])\ndf_test['City Group'] = le.fit_transform(df_test['City Group'])\n## \ndf_train['Type'] = le.fit_transform(df_train['Type'])\ndf_test['Type'] = le.fit_transform(df_test['Type'])","bc707832":"df_train.head()","6c6de170":"df_test.head()","2a4a449b":"corr_with_revenue = df_train.corr()[\"revenue\"].sort_values(ascending=False)\nplt.figure(figsize=(14,7))\ncorr_with_revenue.drop(\"revenue\").plot.bar()\nplt.show()","1a0753af":"df_train.columns","977d0adc":"df_train = df_train.drop(['Id', 'Open Date', 'City'],axis=1)\ndf_train","14044b60":"df_test = df_test.drop(['Id', 'Open Date', 'City'],axis=1)\ndf_test","40de806b":"selected_df = df_train.iloc[:,:-1]\nselected_df","26ee5021":"# We used PCA to reduce the number of dimensions so that we can visualize the results using a 2D Scatter plot\n\npca = PCA(2)\n \n#Transform the data\nX = pca.fit_transform(selected_df)\n \nX.shape","f24af323":"kmeans = KMeans(n_clusters= 5)\n \n#predict the labels of clusters.\nlabel = kmeans.fit_predict(X)\n \nprint(label)","79b6d091":"#Getting unique labels\n \nu_labels = np.unique(label)\n \n#plotting the results:\n \nfor i in u_labels:\n    plt.scatter(X[label == i , 0] , X[label == i , 1] , label = i)\nplt.legend()\nplt.show()","40559fce":"#visualizing clusters with their centroids\n\n#Getting the Centroids\ncentroids = kmeans.cluster_centers_\nu_labels = np.unique(label)\n\n\n#plotting the results:\nfor i in u_labels:\n    plt.scatter(X[label == i , 0] , X[label == i , 1] , label = i)\nplt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\nplt.legend()\nplt.show()","98bd6f46":"#visualizing clusters with their centroids\n\n#Getting the Centroids\ncentroids = kmeans.cluster_centers_\nu_labels = np.unique(label)\n\n\n#plotting the results:\nfor i in u_labels:\n    plt.scatter(X[label == i , 0] , X[label == i , 1] , label = i)\nplt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\nplt.legend()\nplt.show()","425eff30":"neigh = NearestNeighbors(n_neighbors=4)\nnbrs = neigh.fit(X)\ndistances, indices = nbrs.kneighbors(X)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,3]\nplt.plot(distances);","792347c2":"\ny_pred = DBSCAN(eps = 2.7, min_samples=5).fit_predict(X)\nplt.scatter(X[: ,0] , X[: ,1],c = y_pred);\n","11686d0f":"corr_with_revenue = df_train.corr()[\"revenue\"].sort_values(ascending=False)\nplt.figure(figsize=(14,7))\ncorr_with_revenue.drop(\"revenue\").plot.bar()\nplt.show()","65b3736e":"f, ax = plt.subplots(figsize=(30, 22))\nplt.title('Pearson Correlation of features')\nsns.heatmap(selected_df.corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"cubehelix\", linecolor='k', annot=True)","ee00c7e7":"Y=df_train['revenue']\nX=selected_df\ncolnames=selected_df.columns","30d584ef":"# create a dictionary to store our rankinks\nranks = {}\n# creating a function that store the ranks of features \ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))","de23f471":"from sklearn.feature_selection import RFE\n \n# Build Linear reg model \nlr = LinearRegression(normalize=True)\nlr.fit(X,Y)\n# arr\u00eat - jusqu'\u00e0 la derni\u00e8re caract\u00e9ristique \nrfe = RFE(lr, n_features_to_select=1, verbose =3 )\nrfe.fit(X,Y)\nranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)","c66e5e01":"lr = LinearRegression(normalize=True)\nlr.fit(X,Y)\nranks[\"LinReg\"] = ranking(np.abs(lr.coef_), colnames)\n\n#  Ridge \nridge = Ridge(alpha = 7)\nridge.fit(X,Y)\nranks['Ridge'] = ranking(np.abs(ridge.coef_), colnames)\n\n#  Lasso\nlasso = Lasso(alpha=.05)\nlasso.fit(X, Y)\nranks[\"Lasso\"] = ranking(np.abs(lasso.coef_), colnames)","f084755e":"rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=0)\nrf.fit(X,Y)\nranks[\"RF\"] = ranking(rf.feature_importances_, colnames);","f459ed93":"# Create an empty dictionary that will contains mean value of calculated scores\nr = {}\nfor name in colnames:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)\n \nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n \nprint(\"\\t%s\" % \"\\t\".join(methods))\nfor name in colnames:\n    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n                         [ranks[method][name] for method in methods]))))","b3ef034e":"meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n\n# Sorting  dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)","f122dbfc":"sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n               size=14, aspect=1.9, palette='coolwarm')","88ab3403":"meanplot.count()","8ad9d8c5":"# we choose the 30 best importante features\nfeatures_import=meanplot.head(30)['Feature']","0fbe5ba1":"X = selected_df[features_import]","a5cb9fd2":"features_import","7179204f":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","f9e27fe8":"randomforest = RandomForestRegressor()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nRMSE_RF = sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nprint(RMSE_RF)","89cfdb9f":"from sklearn.linear_model import LogisticRegression\n\nlogisReg = LogisticRegression()\nlogisReg.fit(X_train, y_train)\ny_pred = logisReg.predict(X_test)\nRMSE_LOR = sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nprint(RMSE_LOR)","894a9adc":"from sklearn.ensemble import VotingRegressor\nr1 = LinearRegression()\nrandomforest = RandomForestRegressor()\n\nvotingReg = VotingRegressor([('lr', r1), ('rf', randomforest)])\nvotingReg.fit(X_train, y_train)\ny_pred = votingReg.predict(X_test)\nRMSE_vo = sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nprint(RMSE_vo)","90e3d895":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\n\nada_reg = AdaBoostRegressor(\n    DecisionTreeRegressor(max_depth=30), learning_rate=0.8, random_state=42)\nada_reg.fit(X_train, y_train)\ny_pred_ada = ada_reg.predict(X_test)\nRMSE_ada = sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred_ada))\nRMSE_ada","52efb9d2":"import xgboost as xgb\nxg = xgb.XGBRegressor()\nxg.fit(X_train, y_train)\ny_pred = xg.predict(X_test)\nRMSE_XGB = sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nprint(RMSE_XGB)","9d8e27ac":"model_list=['RandomForest','LogisticReg','Voting','AdaBoost','XGBoost']\nrmse_list=[RMSE_RF,RMSE_LOR,RMSE_vo,RMSE_ada,RMSE_XGB]","3cf6d6aa":"# plot the RMSE for each model\nax=sns.barplot(y=model_list,x=rmse_list)\nax.set_title('Model RMSE Result')","a1d83537":"from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV \nfrom sklearn.model_selection import KFold\n\n\nparam_grid_rf =  [ { \n        'n_estimators': [2500],          \n        'max_depth': np.arange(5,20,2),\n        'max_features': ['sqrt'] ,\n        'min_samples_leaf': [0.04]\n       }]\n\ngrid_rf = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n                               param_grid= param_grid_rf,\n                               scoring= 'neg_mean_squared_error',cv = KFold(n_splits= 3, random_state= 42, shuffle=True),\n                               verbose = 1 )\ngrid_rf.fit(X_train, y_train)\nprint('-------Best score----------')\nprint(grid_rf.best_score_ )\nprint('-------Best params----------')\nprint(grid_rf.best_params_)  ","464d6faf":"rf = RandomForestRegressor(max_depth= 7, max_features='sqrt', min_samples_leaf= 0.04, n_estimators = 2500)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nRMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nprint(RMSE)","bcd79323":"y_pred_ada = ada_reg.predict(df_test[features_import])\ny_pred_ada","2286ae67":"# store the result\nsubmission_df=pd.DataFrame(\n{'Id':df_test.index,\n'Prediction':y_pred_ada}\n)","0578888b":"submission_df","d895a12b":"submission_df.to_csv('Submission.csv',index=False)","6a9bde1e":"#### Logistic Reg","30915ff8":"#### xgboost## KNN ","fb214d7f":"Nous allons afficher juste apr\u00e8s les variables qui ont des valeurs manquantes par ordre d\u00e9croissant","b1115d19":"On peut voire que Istanbul a le plus grand nombre de restaurants","fc5598fd":"On met \u00e0 l'echelle les Features (caract\u00e9ristiuques) en utilisant StandardScaler","387eb5a3":"Le type de restaurant le plus pr\u00e9sent est FC","4017b032":"## Features engineering\n","b1b9f6ee":"#### AdaBoost","ef73dce2":"### Generate prediction for test dataset\n","07c83719":"### KMeans","330d3961":"On d\u00e9tecte les valeurs manquantes pour chaque colonnes et on va d\u00e9cider est-ce qu'on va les supprimer ou les remplisser par le m\u00e9dian ou la moyenne","0102b450":"on va supprimer les colonnes qui ont des donn\u00e9es manquantes","7d014cc0":"#### Voting","0a762e0d":"### Most correlated features with the target","6812a61a":"#### Random Forest","630002ef":"# Analyse exploratoire et visualisation\n## 1. Appliquez les diff\u00e9erentes \u00e9tapes de pr\u00e9traitement sur les donn\u00e9es\n### a. La d\u00e9tection des valeurs manquants","cfad031e":"En haut,on peut voir deux tr\u00e8s grand revenues . Ces valeurs sont des valeurs aberrantes. Par cons\u00e9quent, nous pouvons les supprimer.","f0dc64f5":"On va transformer ces valeurs en valeurs num\u00e9riques avec LabelEncoder"}}