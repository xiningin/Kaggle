{"cell_type":{"3d144199":"code","b0374a47":"code","b631f0cb":"code","2347521b":"code","fae2c759":"code","d77ca399":"code","c1d58c31":"code","ebf26541":"code","0ea257d1":"code","4bf60961":"code","00bde3eb":"code","4e55a238":"code","e6d9f6d3":"code","2e3c7a81":"code","e7b356b5":"code","e92ec5fe":"code","eedf9022":"code","332b73eb":"code","bfe67abb":"code","8578a336":"markdown","bb45a45e":"markdown","d00d0773":"markdown","3bd4d019":"markdown","a7d5605e":"markdown","d4e38de2":"markdown","f058af31":"markdown","fec88448":"markdown","609531fc":"markdown","65a89a6f":"markdown","5d0ff09c":"markdown","8d528061":"markdown","8aac5d4f":"markdown","43d7009f":"markdown","be4c9977":"markdown"},"source":{"3d144199":"# Import Libraries\n# - Tensorflow\n# - Keras\n# - numpy and random\n\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nimport PIL\nimport PIL.Image\nimport random\nimport numpy as np\nimport pathlib\nimport matplotlib.pyplot as plt\n%matplotlib inline","b0374a47":"random.seed(42)         # Initialize the random number generator.\nnp.random.seed(42)      # With the seed reset, the same set of numbers will appear every time. \n#tf.set_random_seed(42)  # sets the graph-level random seed","b631f0cb":"data_dir = pathlib.Path('..\/input\/signdigit\/data')\nimage_count = len(list(data_dir.glob('*\/*.*')))\nprint(image_count)","2347521b":"two = list(data_dir.glob('2\/*'))\nPIL.Image.open(str(two[0]))","fae2c759":"PIL.Image.open(str(two[1]))","d77ca399":"batch_size = 256\nimg_height = 180\nimg_width = 180\n\nXtrain = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.3,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\n\n\nXtest = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nYtest =  tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.1,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","c1d58c31":"class_names = Xtrain.class_names\nprint(class_names)","ebf26541":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in Xtrain.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","0ea257d1":"AUTOTUNE = tf.data.AUTOTUNE\n\nXtrain = Xtrain.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nXtest = Xtest.cache().prefetch(buffer_size=AUTOTUNE)","4bf60961":"\n# Create a Sequential model object\n# Add layers Conv2D for CNN and specify MaxPooling\n# Layer 1 = input layer\nnum_classes = 10\n\nmodel = tf.keras.Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(64, 5, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 5, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32,5, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.25),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","00bde3eb":"# Configure  the model for training, by using appropriate optimizers and regularizations\n# Available optimizer: adam, rmsprop, adagrad, sgd\n# loss:  objective that the model will try to minimize. \n# Available loss: categorical_crossentropy, binary_crossentropy, mean_squared_error\n# metrics: List of metrics to be evaluated by the model during training and testing. \n\n#opt = tf.keras.optimizers.Adam(learning_rate=.001)\nopt = tf.keras.optimizers.SGD(\n    learning_rate=0.01, momentum=0.95)\n\nmodel.compile(\n  optimizer=opt,\n  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=['accuracy'])","4e55a238":"# train the model\n\nhistory = model.fit(Xtrain,validation_data=Xtest, epochs = 20)\nmodel.summary()","e6d9f6d3":"print('Final training loss \\t', history.history['loss'][-1])\nprint('Final training accuracy ', history.history['accuracy'][-1])","2e3c7a81":"# testing the model\n\ntestLoss, testAccuracy = model.evaluate(Ytest)","e7b356b5":"print('Testing loss \\t', testLoss)\nprint('Testing accuracy ', testAccuracy)","e92ec5fe":"# plotting training and validation loss\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, color='red', label='Training loss')\nplt.plot(epochs, val_loss, color='green', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# plotting training and validation accuracy\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, color='red', label='Training acc')\nplt.plot(epochs, val_acc, color='green', label='Validation acc')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","eedf9022":"Y_prediction = model.predict(Ytest)\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_prediction, axis = 1) \n\n# Convert validation observations to one hot vectors\nY_true  = np.concatenate([y for x, y in Ytest], axis=0)","332b73eb":"# Classification Report\n\nfrom sklearn.metrics import classification_report \n\nprint(classification_report(Y_true, Y_pred_classes))","bfe67abb":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_mtx, annot=True, fmt=\"d\");","8578a336":"Sequential Model Building with Activation for each layer.","bb45a45e":"Compile with categorical CE loss and metric accuracy.","d00d0773":"\n\nImport required libraries","3bd4d019":"### Prediction for a specific testing data generte confusion matrix","a7d5605e":"Visualize Sample for each category","d4e38de2":"Model Training with cross validation, with total time taken shown for 20 epochs.","f058af31":"Visualise Loss and Accuracy history","fec88448":"# Title of the Project: Classify the ISL digits.\nSaurabh Bansal 2019BE04005, Jagdish Pant 2019BE04002, Munish Khanna 2019BE04031\n\nAssignment on DeepLearning\n\nM.TechData Science and Engineering \u2013 3st semester 2020-21","609531fc":"Visualize Class Names","65a89a6f":"## Confusion Matrix generation","5d0ff09c":"## Results and Outputs","8d528061":"Visualise samples of dataset.","8aac5d4f":"# Convolutional Neural Network","43d7009f":"## Data preprocessing.","be4c9977":"Dataset Dir Path"}}