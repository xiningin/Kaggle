{"cell_type":{"a1454c14":"code","272679b8":"code","44c3b97e":"code","f0aea7a6":"code","42f5f604":"code","34fd4720":"code","1108db1b":"code","68bd4e4c":"code","1db67651":"code","1799a04c":"code","eee40978":"code","268425c7":"code","8c8aaeb7":"code","d0a182d6":"code","2eb513df":"code","14d15967":"code","c9330ab3":"code","d3390493":"markdown","a9ce8833":"markdown","0ead16c2":"markdown","54ea5bc7":"markdown","a79404ba":"markdown","fe4ab660":"markdown","a1583aa6":"markdown","dfd2d7f2":"markdown","981683d1":"markdown","5db1033e":"markdown","6cbc75e5":"markdown"},"source":{"a1454c14":"!pip install -qq gwpy ","272679b8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom pytorch_lightning import LightningModule, LightningDataModule, Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n\nimport fastai\nfrom fastai import *\nfrom fastai.vision.all import *\n\nfrom gwpy.timeseries import TimeSeries\nfrom gwpy.plot import Plot\nimport numpy as np\nfrom scipy import signal\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    return seed\n    \n    \nSEED = 2704\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed_everything(SEED)","44c3b97e":"%%time\n    \ndf = pd.read_csv(\"..\/input\/g2net-gravitational-wave-detection\/training_labels.csv\")\n# small_df = df\n\nsmall_df = df.sample(frac=0.01, random_state=SEED).reset_index(drop=True)\ntrain_df, test_df = train_test_split(small_df, test_size=0.2, random_state=SEED, shuffle=True, stratify=small_df['target'])\ntrain_df.shape, test_df.shape","f0aea7a6":"import json\nwith open('..\/input\/mean-and-std-calculations-for-the-entire-dataset\/train_stats.json', 'r') as f:\n    train_stats = json.load(f)\n    \ntrain_mu, train_sigma = [], []\nfor item in train_stats['detector']:\n    train_mu += [item['mean']]\n    train_sigma += [item['std']]\n    \ntrain_mu, train_sigma = np.array(train_mu), np.array(train_sigma)\ntrain_mu, train_sigma","42f5f604":"def filters(array, sample_frequency=2048, lf=35, hf=350):\n    \"\"\" Apply preprocessing such as whitening and bandpass \"\"\"\n    strain = TimeSeries(array, sample_rate=int(sample_frequency))\n    # white_data = strain.whiten(fftlength=4, fduration=4)\n    # white_data = strain.whiten(window=(\"tukey\", 0.2))\n    white_data = strain\n    bp_data = white_data.bandpass(lf, hf)\n    return bp_data.value\n\nfrom scipy.ndimage import gaussian_filter1d\nfrom pathlib import Path\nINPUT_PATH = Path(\"..\/input\/g2net-gravitational-wave-detection\/\")\n\ndef load_wave(id_, mu, sigma, nc, folder='train'):\n    path = INPUT_PATH \/ folder \/ id_[0] \/ id_[1] \/ id_[2] \/ f\"{id_}.npy\"\n    waves = np.load(path).astype('float32').T\n    waves = ((waves - mu)\/sigma).astype(np.float32)\n    # waves = gaussian_filter1d(waves, 0.5)\n    for idx in range(nc):\n        waves[:, idx] = filters(waves[:, idx])\n    return waves\n    \n\nclass Anomaly_Dataset(Dataset):\n    def __init__(self, df, nc=3):\n        self.df = df\n        self.nc = nc\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx, :]\n        waves = load_wave(row['id'], train_mu, train_sigma, self.nc)\n        return waves[:, :self.nc], row['target']\n\n    \n# Return only `wnd_size` sized random chunk background\nclass Background_Dataset(Dataset):\n    def __init__(self, df, wnd_size, nc=3):\n        self.df = df[df['target'] == 0].reset_index(drop=True)\n        self.nc = nc\n        self.wnd_size = wnd_size\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx, :]\n        waves = load_wave(row['id'], train_mu, train_sigma, self.nc)\n        \n        rnd_pos = np.random.randint(0, len(waves)-1-self.wnd_size)\n        waves = waves[rnd_pos:rnd_pos+self.wnd_size, :self.nc]\n        \n        return waves, waves","34fd4720":"nc = 3\nwnd_size = 128\nnum_samples = 4096\nn_wnd = num_samples \/\/ wnd_size\n# wnd_size = num_samples \/\/ n_wnd\n\nbds = Background_Dataset(train_df, wnd_size=wnd_size, nc=nc)\nrnd_idx = np.random.randint(len(bds))-1\nwaves = bds[rnd_idx][0]\nprint('rnd_idx:', rnd_idx, waves.shape)\nplt.figure(figsize=(20,4))\nfor _nc in range(nc):\n    plt.plot(waves[:, _nc], label=f'site{_nc}'); \nplt.legend()\n\ndl = DataLoader(bds, bs=4, shuffle=True)\nX, y = next(iter(dl))\nX.shape, y.shape","1108db1b":"# from https:\/\/www.kaggle.com\/hanjoonchoe\/wavenet-lstm-pytorch-ignite-ver        \nclass Wave_Block(nn.Module):\n    \n    def __init__(self,in_channels,out_channels,dilation_rates):\n        super(Wave_Block,self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n        \n        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n        dilation_rates = [2**i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n            \n    def forward(self,x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i+1](x)\n            res = torch.add(res, x)\n        return res\n    \nclass Wavenet_denoiser(nn.Module):\n    def __init__(self, nc, hidden_dim=64, latent_dim=4):\n        super().__init__()\n        torch.cuda.empty_cache()\n        \n        self.conv1 = nn.Conv1d(nc, hidden_dim\/\/2, kernel_size=3, padding=1)\n        self.encoder = nn.Sequential(\n            Wave_Block(hidden_dim\/\/2, hidden_dim, 1),\n            nn.BatchNorm1d(hidden_dim),\n            nn.SiLU(),\n        )\n        self.rnn1 = nn.LSTM(input_size=hidden_dim, hidden_size=latent_dim, num_layers=1, batch_first=True, bidirectional=False)\n        self.rnn2 = nn.LSTM(input_size=latent_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True, bidirectional=False)\n        \n        self.decoder = nn.Sequential(\n            Wave_Block(hidden_dim, hidden_dim\/\/2, 1),\n            nn.BatchNorm1d(hidden_dim\/\/2),\n            nn.SiLU(),\n        )\n        self.conv2 = nn.Conv1d(hidden_dim\/\/2, nc, kernel_size=3, padding=1)\n            \n    def forward(self,x):\n        # ---- Encoder ----\n        x = x.permute(0, 2, 1)\n        x = self.conv1(x)\n        x = self.encoder(x)\n\n        # ---- Bottleneck ----\n        x = x.permute(0, 2, 1)\n        x, _ = self.rnn1(x)\n        \n        # ---- Decoder ----\n        x, _ = self.rnn2(x)\n        x = x.permute(0, 2, 1)\n        x = self.decoder(x)\n        \n        # ---- Output ----\n        x = self.conv2(x)\n        x = x.permute(0, 2, 1)        \n        return x","68bd4e4c":"model = Wavenet_denoiser(nc=nc).to(device)\nmodel","1db67651":"# Sanity check\nwith torch.no_grad():\n    print(X.shape, model(X.to(device)).shape)","1799a04c":"train_ds = Background_Dataset(train_df, wnd_size=wnd_size, nc=nc)\nval_ds = Background_Dataset(test_df, wnd_size=wnd_size, nc=nc)\nanomaly_ds = Anomaly_Dataset(test_df, nc=nc)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbs = 64 if torch.cuda.is_available() else 4\n\ndef RMSELoss(yhat, y):\n    return torch.sqrt(torch.mean((yhat-y)**2))\n\ndls = DataLoaders.from_dsets(train_ds, val_ds, bs=bs, device=device)\nlearn = Learner(\n    dls, model, loss_func=RMSELoss, opt_func=Adam,\n    cbs=[fastai.callback.all.ShowGraphCallback(),fastai.callback.all.SaveModelCallback(fname='best'), fastai.callback.all.CSVLogger()]\n)","eee40978":"learn.lr_find()","268425c7":"learn.fit_one_cycle(n_epoch=200, lr_max=5e-3)\nlearn.save('dae')","8c8aaeb7":"import scipy\n\ndef moving_average(x, w=8):\n    return np.convolve(x, np.ones(w), 'valid') \/ w\n\n# https:\/\/www.kaggle.com\/alexnitz\/pycbc-making-images\ndef bandpass2rgb(data, f_range=(35,350), q_range=(16,32), q_max=10):\n    data = map(lambda x: TimeSeries(x, sample_rate=2048), data.T)\n    # Q-transform\n    data = map(lambda x: x.q_transform(qrange=q_range, frange=f_range, logf=True, whiten=False), data)\n    # Convert to RGB image\n    img = np.stack(list(data), axis = -1)\n    img = np.clip(img, 0, q_max)\/q_max * 255\n    img = img.astype(np.uint8)\n    img = Image.fromarray(img).rotate(90, expand=1)\n    return img\n\ndef plot_denoised(data, folder='train', n_wnd=n_wnd, num_samples=num_samples, nc=nc, figsize=(20, 4)):\n    if isinstance(data, str):\n        waves = load_wave(data, train_mu, train_sigma, nc, folder=folder)\n    else:\n        waves = data\n    rgb = bandpass2rgb(waves)\n    \n    plt.figure(figsize=figsize)\n    plt.title('Q-Transformed')\n    plt.imshow(rgb)\n    \n    plt.figure(figsize=figsize)\n    plt.title('Original')\n    for _nc in range(nc):\n        plt.plot(waves[:, _nc], label=f'site{_nc}'); \n    plt.legend()\n    \n    wnd_size = num_samples \/\/ n_wnd\n    __waves = torch.from_numpy(waves).unsqueeze(dim=0).view(1*n_wnd, num_samples\/\/n_wnd, nc)\n    with torch.no_grad():\n        print(__waves.shape)\n        pred = model(__waves.to(device))\n    raw_rmse = torch.sqrt((pred.cpu()-__waves)**2).reshape(num_samples, nc).mean(-1).numpy()\n    pred = pred.reshape(num_samples, nc).cpu().numpy()\n        \n    # \"\"\"\n    plt.figure(figsize=figsize)\n    plt.title(f'Reconstructed: RMSELoss {raw_rmse.mean():.05f}, maxRMSELoss {np.max(raw_rmse):.05f} @ wnd_idx:[{np.argmax(raw_rmse)+1}\/{n_wnd}]')\n    for _nc in range(nc):\n        plt.plot(pred[:, _nc], label=f'site{_nc}');\n    # for i in range(n_wnd+1):\n    #     plt.axvline(x=i*wnd_size)\n    plt.legend()\n    # \"\"\"\n    \n    plt.figure(figsize=figsize)\n    plt.title('RMSELoss')\n    plt.plot(raw_rmse, label='raw')\n    \n    peaks, _ = scipy.signal.find_peaks(raw_rmse, width=2)\n    print('#peaks', len(peaks))\n    plt.plot(peaks, raw_rmse[peaks], \"x\")\n    plt.legend()","d0a182d6":"# a very clean chirp:\nplot_denoised('0021f9dd71', folder='test', n_wnd=num_samples\/\/1)","2eb513df":"## a not-so-clean chirp\nplot_denoised('000a5b6e5c', n_wnd=num_samples\/\/4)","14d15967":"rnd_idx = np.random.randint(len(anomaly_ds))-1\nrnd_X, rnd_y = anomaly_ds[rnd_idx][0], anomaly_ds[rnd_idx][1]\nplot_denoised(rnd_X)\nprint('GW:', rnd_y==1)","c9330ab3":"rnd_idx = np.random.randint(len(anomaly_ds))-1\nrnd_X, rnd_y = anomaly_ds[rnd_idx][0], anomaly_ds[rnd_idx][1]\nplot_denoised(rnd_X)\nprint('GW:', rnd_y==1)","d3390493":"Some random samples","a9ce8833":"To normalize the data, I used the statistics from: https:\/\/www.kaggle.com\/mistag\/mean-and-std-calculations-for-the-entire-dataset","0ead16c2":"The model can also provide some localization for the Gravitation Waves.","54ea5bc7":"To save time and resources, let's experiment with 1% of the data","a79404ba":"Yes, it's from the liverpool competition :p ","fe4ab660":"### Model","a1583aa6":"### Data","dfd2d7f2":"### Vis","981683d1":"So the idea here is to train an autoencoder with only the Background signal and when presented with a Gravitation Wave, the autoencoder shouldn't be able to reconstruct it and hence should generate an unsually high reconstruction loss.","5db1033e":"### Idea","6cbc75e5":"### Fastai Learner"}}