{"cell_type":{"a319e315":"code","547f03f7":"code","fed98ad4":"code","0d67f661":"code","3a4fa0c9":"code","2eca2637":"code","9f6a5fef":"code","c7d26d63":"code","ebeef660":"code","08a4b483":"code","906d9e39":"code","ca3b22e0":"code","c9e7ba09":"code","843963a9":"code","d3080852":"code","f9932ced":"code","770b7c81":"code","01a930a5":"code","8d0cc1d8":"code","99f5228d":"code","8293e229":"code","67cff64b":"code","e6e94cf6":"code","7f83bbaa":"code","e80940dc":"code","2789b524":"code","f05516bd":"code","d6f3233a":"code","081a6769":"code","4c85b29a":"code","be09bd1b":"code","e72d6d39":"code","24637a63":"code","c61da272":"code","4d4c2dba":"code","75a8dbde":"code","a1d9d78d":"code","32b3ed32":"code","ace7afb9":"code","fabc3a35":"code","c55744dc":"code","36a857e3":"code","687f5df0":"code","10061156":"code","2fab6120":"code","01bc8a0f":"code","77803a39":"code","c1d41082":"code","23256f27":"code","f40cf8ff":"code","04816475":"code","9bb71a17":"code","b40f1cb9":"code","804a87e4":"markdown","ad352d13":"markdown","eafc5a8e":"markdown","35b293ea":"markdown","18b97c64":"markdown","ca7f4753":"markdown","cf685d9a":"markdown","85f0d2cf":"markdown","3087ea12":"markdown","97a357cd":"markdown","aa909faa":"markdown"},"source":{"a319e315":"import pandas as pd\nimport numpy as np\nimport nltk \nimport re\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport sklearn.metrics\nimport sklearn\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","547f03f7":"data=pd.read_csv(\"..\/input\/source-based-news-classification\/news_articles.csv\")","fed98ad4":"data.info()","0d67f661":"data.head()","3a4fa0c9":"data.groupby('label').describe()","2eca2637":"sns.heatmap(data.isnull(),yticklabels=False, cbar=False,cmap='magma')","9f6a5fef":"data=data.dropna()","c7d26d63":"data.head()","ebeef660":"data['text_len'] = data['text'].apply(len)","08a4b483":"data['len_title'] = data['title'].apply(len)","906d9e39":"!pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class","ca3b22e0":"AV = AutoViz_Class()","c9e7ba09":"df = AV.AutoViz(filename=\"\",sep=',', depVar='label', dfte=data, header=0, verbose=2, \n                 lowess=False, chart_format='svg', max_rows_analyzed=150000, max_cols_analyzed=30)","843963a9":"# Plot article type distribution\ndf_type = data['type'].value_counts()\nsns.barplot(np.arange(len(df_type)), df_type)\nplt.xticks(np.arange(len(df_type)), df_type.index.values.tolist(), rotation=90)\nplt.title('Article type count', fontsize=20)\nplt.show()","d3080852":"from nltk.corpus import stopwords\nimport string\nstopwords.words('english')[0:10] # Show some stop words","f9932ced":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Perform lemmatization\n    4. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    lemma = nlp.WordNetLemmatizer()\n    nopunc = [ lemma.lemmatize(word) for word in nopunc]","770b7c81":"data['title'].apply(text_process)","01a930a5":"data['text'].head(5).apply(text_process)","8d0cc1d8":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer=text_process).fit(data['text'])\n\n# Print total number of vocab words\nprint(len(bow_transformer.vocabulary_))","99f5228d":"messages_bow = bow_transformer.transform(data['text'])","8293e229":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(messages_bow)\n\nmessages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)","67cff64b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ny = le.fit_transform(data.label)","e6e94cf6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, label_train, label_test = train_test_split(data['text'], y, test_size=0.2, random_state = 42)\n\nprint(len(X_train), len(X_test), len(X_train) + len(X_test))","7f83bbaa":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(messages_tfidf, data['label'])","e80940dc":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])","2789b524":"pipeline.fit(X_train,label_train)","f05516bd":"predictions1 = pipeline.predict(X_test)","d6f3233a":"from sklearn.metrics import classification_report\nprint(classification_report(predictions1,label_test))","081a6769":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=50, criterion='entropy',random_state=0)\nclassifier.fit(messages_tfidf, data['label'])","4c85b29a":"pipeline_rf = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', RandomForestClassifier()),  # train on TF-IDF vectors w\/ SVM\n])","be09bd1b":"pipeline_rf.fit(X_train,label_train)","e72d6d39":"predictions2 = pipeline_rf.predict(X_test)","24637a63":"print(classification_report(predictions2,label_test))","c61da272":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(messages_tfidf, data['label'])","4d4c2dba":"pipeline_lr = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w\/ SVM\n])","75a8dbde":"pipeline_lr.fit(X_train,label_train)","a1d9d78d":"predictions3 = pipeline_lr.predict(X_test)","32b3ed32":"print(classification_report(predictions3,label_test))","ace7afb9":"from sklearn.neural_network import MLPClassifier\nnn=MLPClassifier(random_state=1)","fabc3a35":"pipeline_nn = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MLPClassifier()),  # train on TF-IDF vectors w\/ SVM\n])","c55744dc":"pipeline_nn.fit(X_train,label_train)","36a857e3":"predictions4 = pipeline_nn.predict(X_test)","687f5df0":"print(classification_report(predictions4,label_test))","10061156":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42, criterion=\"entropy\",\n                             min_samples_split=10, min_samples_leaf=10, max_depth=3, max_leaf_nodes=5)","2fab6120":"pipeline_dt = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', DecisionTreeClassifier()),  # train on TF-IDF vectors w\/ SVM\n])","01bc8a0f":"pipeline_dt.fit(X_train,label_train)","77803a39":"predictions_dt = pipeline_dt.predict(X_test)","c1d41082":"print(classification_report(predictions_dt,label_test))","23256f27":"from sklearn.ensemble import GradientBoostingClassifier\ngbm=GradientBoostingClassifier(learning_rate=0.3,max_depth=4,n_estimators=100 ,random_state=0)","f40cf8ff":"pipeline_gbm = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', GradientBoostingClassifier()),  # train on TF-IDF vectors w\/ SVM\n])","04816475":"pipeline_gbm.fit(X_train,label_train)","9bb71a17":"predictions_gbm = pipeline_gbm.predict(X_test)","b40f1cb9":"print(classification_report(predictions_gbm,label_test))","804a87e4":"# EDA","ad352d13":"# 6. Gradient Boosting ","eafc5a8e":"# 4. Neural Networks","35b293ea":"# CLASSIFICATION MODELS :","18b97c64":"# 3. Logistic Model","ca7f4753":"# 5. Decision Trees","cf685d9a":"Generate length of title and length of text features to see for any trends there","85f0d2cf":"In terms of accuracy, best model turns out to be Random Forest with 0.81 accuracy followed by GBM.\nIn terms of Precision, no model can beat Naive Bayes Classifier.","3087ea12":"# 1.Naive Bayes","97a357cd":"Text Preprocessing and Bag of Words for feature engineering","aa909faa":"# 2. Random Forest"}}