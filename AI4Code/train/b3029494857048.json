{"cell_type":{"b504e538":"code","bfadc594":"code","ad097dd7":"code","35af6fd3":"code","1b5014d9":"code","a1ef949a":"code","79428ba1":"code","e9fc1069":"code","137fddb3":"code","6264a611":"code","59412576":"code","a7b73506":"code","4b4b8c29":"code","f37cda97":"code","51036b6c":"markdown","9efedd20":"markdown","bd04e99f":"markdown","cc6a2623":"markdown","292e522a":"markdown","93524310":"markdown","5bce745f":"markdown","7b197cf6":"markdown","88af6851":"markdown","53b155eb":"markdown","e5d9c992":"markdown","3d06ff41":"markdown","ae8a1706":"markdown","62be7afd":"markdown"},"source":{"b504e538":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bfadc594":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.io.formats import style\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\n\nimport sklearn.preprocessing as sk_prep\n","ad097dd7":"DATA_DIR = '\/kaggle\/input\/tabular-playground-series-jun-2021'\nRANDOM_STATE = 9003","35af6fd3":"train_set = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ntrain_data = train_set.iloc[:, 1:-1] # Feature columns\ntrain_ar = train_data.to_numpy()\n\nle_targ = sk_prep.LabelEncoder()\ntrain_y = train_set['target']\ntrain_y_num = le_targ.fit_transform(train_y)\nclasses = le_targ.classes_\n\n# Train data with label encoded target\ntrain_w_targ = train_data.copy()\ntrain_w_targ['target'] = train_y_num\n\nprint(train_set.shape)\ntrain_set.head()","1b5014d9":"print(\"Data Types of Predictors\")\n\ntrvc = train_data.dtypes.value_counts()\nfor type1, cnt in trvc.iteritems():\n    print(f'  {type1}: {cnt}')\n\nnull_count = np.sum(np.isnan(train_ar))\n\nprint(f'\\nNumber of missing values in features: {null_count}.')\n\nneg_count = np.sum(np.less(train_ar, 0))\n\nprint(f'\\nNumber of negative values in features: {neg_count}')","a1ef949a":"fig, ax = plt.subplots()\n\nax.set_title(\"Count of Samples by Target Class\")\nsns.countplot(x='target', data=train_set, order=classes)","79428ba1":"pos_arr = (train_ar > 0).astype('int32')\nrow_cnts = np.sum(pos_arr, axis=1)\n\nrc_gb = pd.DataFrame({'PosCnt': row_cnts, 'target': train_y}).groupby(by='target')\ncl_cnt = rc_gb.aggregate(['min', 'max', 'mean'])\n\nprint(\"Positive Values in Rows by Classes\")\ncl_cnt","e9fc1069":"row_count = train_data.shape[0]\n\n# Get min, max and mean by feature\n\nagg_df = train_data.agg(['min', 'max', 'mean']).transpose()\n\n# Get positive % by feature\n\nagg2_ls = [\n    (\n        (f1 > 0).sum(),\n        len(f1.unique())\n    )\n    for f1 in [train_data[col] for col in train_data]\n]\n\n# Get number of unique values\n\n\n\n# Combine data\n\nagg2_df = pd.DataFrame(agg2_ls, columns=['PosCnt', 'UniqueCnt'], index=agg_df.index)\nagg2_df['PosProp'] = agg2_df['PosCnt'].div(row_count)\n\nfeat_ch_df = pd.concat([agg_df, agg2_df.iloc[:, [0, 2, 1]]], axis=1)\n\n# Display\n\nstyle.Styler(feat_ch_df, precision=2).background_gradient(cmap='viridis')","137fddb3":"print(f'Maximum positive proportion: {feat_ch_df[\"PosProp\"].max():0.2f}')\nprint(f'Minimum positive proportion: {feat_ch_df[\"PosProp\"].min():0.2f}')\nprint(f'Mean positive proportion:    {feat_ch_df[\"PosProp\"].mean():0.2f}')","6264a611":"pos_gain_by_target = pd.DataFrame(index=classes)\n\nfor col in train_data:\n    col_ser = train_data[col]\n    zero_counts_by_target = train_set.loc[(col_ser == 0), [col, 'target']].groupby(by='target').count()\n    zero_prop_by_target = zero_counts_by_target.divide(zero_counts_by_target.sum())\n    pos_counts_by_target = train_set.loc[(col_ser > 0), [col, 'target']].groupby(by='target').count()\n    pos_prop_by_target = pos_counts_by_target.divide(pos_counts_by_target.sum())\n    pos_gain_by_target[col] = pos_prop_by_target.divide(zero_prop_by_target).sub(1)\n\n# Transpose so that the plot will be tall\npos_gain_by_target = pos_gain_by_target.T\n\n# pd.io.formats.style.Styler(pos_gain_by_target, precision=2).background_gradient(cmap='viridis')","59412576":"fig, ax = plt.subplots()\nfig.set_figheight(20)\n\nsns.heatmap(pos_gain_by_target, annot=True, fmt='0.2f', ax=ax, cmap='viridis')","a7b73506":"corr_df = train_data.corr()\n\nprint(\"Sample of feature correlations\")\n\ncorr_df.iloc[:10, :10]","4b4b8c29":"corr_arr = corr_df.to_numpy()\nc_max = np.max(corr_arr[corr_arr < 1])\nc_min = np.min(corr_arr)\nc_mean = np.mean(corr_arr[corr_arr < 1])\n\nprint(f\"Strongest positive correlation between features: {c_max: 0.3f}\")\nprint(f\"Weakest positive correlation between features: {c_min: 0.3f}\")\nprint(f\"Mean correlation: {c_mean: 0.3f}\")","f37cda97":"# Correlation Map\n\nfig, ax = plt.subplots(figsize=(20, 12))\nax.set_title(\"Correlation Heatmap with Enhanced Color\", fontsize=14)\n\nsns.heatmap(corr_df, vmax=c_max*1.1, center=0.0, annot=False)","51036b6c":"# Basic Characteristics by Feature","9efedd20":"# Correlations\n\nAs we look for correlations between features, we find that they are very small.  If we did a heatmap without significant color enhancement, everything except the diagonal would be the same, essentially a correlation of 0.  The color enhancement is done by setting vmax which controls the upper limit of the color range.","bd04e99f":"# Distribution of Target Values","cc6a2623":"## Data types and missing values","292e522a":"# Characteristics of Rows by Target Classes\n\nThis table shows the minimum, maximum and mean counts of positive values by rows within each target class.","93524310":"# Influence of Positive Features\n\nThe heatmap below shows the improvement in likelihood for each class if it has a positive value in each feature compared to its likelihood if it has a zero in that feature.\n\nFor example, rows with a positive value in feature 0 have 32% chance of being in Class 8, while those with a zero in the feature have 24% change of being in this class.  This is a net improvement of 0.32 \/ 0.24 - 1 = 0.36.","5bce745f":"This table provides the minimum, maximum and mean values for each feature.  The other columns are the counts and proportions of positive values and the count of unique values.","7b197cf6":"# Setup","88af6851":"We can see that the average number of positive values per row by class varies from 18 for class 2 to 31 for class 8. \n\nNote that both classes 2, 3, 5 and 6 all have at least one row with no positive values.","53b155eb":"# Load Data and Top-Level Checks","e5d9c992":"In the May data, the strongest correlation between features was 0.017, a little more than a tenth of the strongest correlation we see here, and not a lot more than the weakest correlation that we see here.  Also May had some negative correlations, which we do not have in these data.\n\nThis stronger correlation between features, though it is still week on the whole, ccould mean that methods that combine features could be more effective this month, e.g. PCA or DAE.","3d06ff41":"# Synopsis\n\nThis is an examination of feature and class characteristics.  The study focuses on zero vs. positive values and comparsions with May TPS data.","ae8a1706":"The June data has no negative feature values, unlike the May data.","62be7afd":"Positive features are much less common than 0's; only 36% of features have positive values.  For comparison, in May only about 20% of all values were positive."}}