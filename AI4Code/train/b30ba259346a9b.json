{"cell_type":{"1083d41b":"code","a30afd49":"code","bf621e3a":"code","cd22b3dc":"code","d1b65665":"code","d02cc196":"code","2f7a4fc7":"code","f1a03351":"code","758e2969":"code","e98b9cda":"code","c634dc3c":"code","24a3f994":"code","196657f5":"code","1b7d71ea":"code","9cbce8f4":"code","1e579cf0":"code","c7cb6742":"code","d465bb27":"code","eed6d7ed":"code","687dfef9":"code","5b716f34":"code","823238e7":"code","92650b0f":"code","73fc1a74":"code","80555a87":"code","146530ba":"code","672e6f73":"code","df3fd91a":"code","88d76b01":"code","d7d8635a":"code","5a8e28f4":"code","86948ad5":"code","4f2261c1":"code","29413231":"code","40049507":"code","ebd70608":"code","e841f19d":"code","15c05958":"code","3af53c21":"code","0b1e40ee":"code","6584b5a9":"code","92d57163":"code","84dd6ed5":"code","16f3e30c":"code","60096747":"code","c3bdac7c":"code","5d836c5c":"markdown","df12ccac":"markdown","f619974a":"markdown","1e3f172d":"markdown","bde362b9":"markdown","382cf383":"markdown","74c1ac3f":"markdown","16ab8671":"markdown","4c6c3616":"markdown","9a1bdead":"markdown","9000e445":"markdown","f7cef089":"markdown","a02db221":"markdown","ea8011f0":"markdown","e8dd4850":"markdown","182a2cfa":"markdown","9f3130f4":"markdown","7520953c":"markdown","67af327d":"markdown","b846fd2a":"markdown","5a67e862":"markdown","bc2a1671":"markdown","8e068f19":"markdown","db7bb526":"markdown","61a90b11":"markdown","ff27bd98":"markdown","87124702":"markdown","5086f948":"markdown","18941738":"markdown"},"source":{"1083d41b":"from IPython.core.display import display, HTML\n\nimport glob\nimport os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\n\nfrom numpy.random import seed\nseed(42)\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\nimport warnings\nwarnings.filterwarnings('ignore')","a30afd49":"path_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","bf621e3a":"# From the best version (commit) 5\nlearning_rate = 0.006\nnum_epochs = 200","cd22b3dc":"commits_df = pd.DataFrame(columns = ['n_commit', 'learning_rate', 'num_epochs', 'FE', 'target4', 'target32_34', 'LB_score'])","d1b65665":"n=0\ncommits_df.loc[n, 'n_commit'] = 0                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.005          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 1000              # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000935             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002423         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20157             # LB score after submitting","d02cc196":"n=1\ncommits_df.loc[n, 'n_commit'] = 3                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.005          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 1100              # Number of epochs (but all calculations \n                                                    # are completed much earlier)\ncommits_df.loc[n, 'FE'] = 1                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001048             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002394         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20209             # LB score after submitting","2f7a4fc7":"n=2\ncommits_df.loc[n, 'n_commit'] = 4                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.004          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 100               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000968             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002465         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20219             # LB score after submitting","f1a03351":"n=3\ncommits_df.loc[n, 'n_commit'] = 5                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.006          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000829             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002325         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20012             # LB score after submitting","758e2969":"n=4\ncommits_df.loc[n, 'n_commit'] = 7                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.007          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001622             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002353         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20101             # LB score after submitting","e98b9cda":"n=5\ncommits_df.loc[n, 'n_commit'] = 8                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0055         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001225             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002434         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20147             # LB score after submitting","c634dc3c":"n=6\ncommits_df.loc[n, 'n_commit'] = 9                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0065         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001466             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002362         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20057             # LB score after submitting","24a3f994":"n=7\ncommits_df.loc[n, 'n_commit'] = 11                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0059         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001019             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002347         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20161             # LB score after submitting","196657f5":"n=8\ncommits_df.loc[n, 'n_commit'] = 12                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0061         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001295             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002450         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20148             # LB score after submitting","1b7d71ea":"n=9\ncommits_df.loc[n, 'n_commit'] = 13                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.00601        # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000979             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002414         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20119             # LB score after submitting","9cbce8f4":"n=10\ncommits_df.loc[n, 'n_commit'] = 14                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.00599        # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001537             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002432         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20216             # LB score after submitting","1e579cf0":"n=11\ncommits_df.loc[n, 'n_commit'] = 15                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0049         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001378             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002514         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20208             # LB score after submitting","c7cb6742":"n=12\ncommits_df.loc[n, 'n_commit'] = 16                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0067         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001633             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002473         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20150             # LB score after submitting","d465bb27":"n=13\ncommits_df.loc[n, 'n_commit'] = 17                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.006          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 65                # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000776             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002251         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20050             # LB score after submitting","eed6d7ed":"# Find and mark minimun value of LB score\ncommits_df['LB_score'] = pd.to_numeric(commits_df['LB_score'])\ncommits_df = commits_df.sort_values(by=['LB_score'], ascending = True).reset_index(drop=True)\ncommits_df['min'] = 0\ncommits_df.loc[0, 'min'] = 1\ncommits_df","687dfef9":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='learning_rate', y='num_epochs', z='LB_score', color = 'min', \n                    symbol = 'FE',\n                    title='    Parameters and LB score visualization of \"ORV Prediction\" solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","5b716f34":"# Interactive plot with targets\nfig = px.scatter_3d(commits_df, x='target4', y='target32_34', z='LB_score', color = 'min', \n                    symbol = 'learning_rate',\n                    title='     Targets and LB score visualization of \"ORV Prediction\" solutions')\nfig.update(layout=dict(title=dict(x=0.2)))","823238e7":"# Interactive plot with learning rate and LB score\ncommits_df = commits_df.sort_values(by=['learning_rate'])\nfig = px.line(commits_df, x='learning_rate', y=\"LB_score\", text='n_commit',\n              title=\"Learning rate and LB score with number of version\",\n              log_y=True,template='gridon',width=800, height=500)\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","92650b0f":"# Interactive plot with target for 0-4 and LB score\ncommits_df = commits_df.sort_values(by=['target4'])\nfig = px.line(commits_df, x='target4', y=\"LB_score\", text='n_commit',\n              title=\"Target4 (for 0-4) and LB score with number of version\",\n              log_y=True,template='gridon',width=800, height=500)\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","73fc1a74":"# Interactive plot with target for 0-32 & 0-34 and LB score\ncommits_df = commits_df.sort_values(by=['target32_34'])\nfig = px.line(commits_df, x='target32_34', y=\"LB_score\", text='n_commit',\n              title=\"Target32 (for 0-32 and 0-34) and LB score with number of version\",\n              log_y=True,template='gridon',width=800, height=500)\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","80555a87":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    \n    return train, test","146530ba":"# Read train and test\ntrain, test = read_train_test()","672e6f73":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df\n\n\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    # Function to early stop with root mean squared percentage error\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","df3fd91a":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","88d76b01":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique'])\ntest['size_tau'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique'])\ntrain['size_tau_400'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_400'])\ntest['size_tau_400'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_400'])\ntrain['size_tau_300'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_300'])\ntest['size_tau_300'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_300'])\ntrain['size_tau_200'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_200'])\ntest['size_tau_200'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_200'])","d7d8635a":"# tau2 \ntrain['size_tau2'] = np.sqrt(1\/train['trade_order_count_sum'])\ntest['size_tau2'] = np.sqrt(1\/test['trade_order_count_sum'])\ntrain['size_tau2_400'] = np.sqrt(0.25\/train['trade_order_count_sum'])\ntest['size_tau2_400'] = np.sqrt(0.25\/test['trade_order_count_sum'])\ntrain['size_tau2_300'] = np.sqrt(0.5\/train['trade_order_count_sum'])\ntest['size_tau2_300'] = np.sqrt(0.5\/test['trade_order_count_sum'])\ntrain['size_tau2_200'] = np.sqrt(0.75\/train['trade_order_count_sum'])\ntest['size_tau2_200'] = np.sqrt(0.75\/test['trade_order_count_sum'])\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","5a8e28f4":"# kfold based on the knn++ algorithm\n\nout_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n# out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# Code to add the just the read data after first execution\n\n# Data separation based on knn ++\nnfolds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n\n# Generates a matriz with the values of \nmat = out_train.values\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\nnind = int(mat.shape[0]\/nfolds) # number of individuals\n\n# Adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\nlineNumber = np.sort(lineNumber)[::-1]\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# Saves index\nfor n in range(nfolds):    \n    values.append([lineNumber[n]])\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n        # Saves the values of index           \n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # Probabilities\n        f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totDist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # Delete line of the value added    \n        for n_iter in range(nfolds):\n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","86948ad5":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')","4f2261c1":"colNames = list(train)\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('stock_id')","29413231":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in colNames:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train[col] = qt.fit_transform(train[[col]])\n    test[col] = qt.transform(test[[col]])    \n    qt_train.append(qt)","40049507":"# Making agg features\n\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\ncorr = train_p.corr()\nids = corr.index\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\nmat = []\nmatTest = []\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat2 = pd.concat(matTest).reset_index()","ebd70608":"matTest = []\nmat = []\nkmeans = []","e841f19d":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","15c05958":"mat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","3af53c21":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_3c1',\n     'total_volume_mean_4c1', \n     'total_volume_mean_6c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_3c1',\n     'trade_size_mean_4c1', \n     'trade_size_mean_6c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',\n     'trade_order_count_mean_6c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',\n     'price_spread_mean_6c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',\n     'bid_spread_mean_6c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',\n     'ask_spread_mean_6c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',\n     'volume_imbalance_mean_6c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'bid_ask_spread_mean_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","0b1e40ee":"train = pd.merge(train,mat1[nnn],how='left',on='time_id')","6584b5a9":"test = pd.merge(test,mat2[nnn],how='left',on='time_id')","92d57163":"mat1 = []\nmat2 = []","84dd6ed5":"# Thanks to https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})","16f3e30c":"hidden_units = (128,64,32)\nstock_embedding_size = 24\ncat_data = train['stock_id']\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(362,), name='num_data')\n\n    # Embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","60096747":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nfor n_count in range(n_folds):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    y_train = train.loc[train.time_id.isin(indexes), target_name]\n    X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n    \n    # NN\n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=num_epochs,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')","c3bdac7c":"# Postprocessing\ntest[target_name] = test[target_name]\/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\n\n# Submission\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)","5d836c5c":"### Commit 9","df12ccac":"### Commit 7","f619974a":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [My upgrade](#2)\n    -  [Commit now](#2.1)\n    -  [Previous commits](#2.2)\n    -  [Parameters and LB score visualization](#2.3)\n1. [Download data](#3)\n1. [FE & Data Preprocessing](#4)\n1. [Modeling and prediction](#5)\n1. [Submission](#6)","1e3f172d":"## 6. Submission <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","bde362b9":"## 2. My upgrade <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","382cf383":"### Commit 3","74c1ac3f":"### 2.3 Parameters and LB score visualization <a class=\"anchor\" id=\"2.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","16ab8671":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","4c6c3616":"### Commit 12","9a1bdead":"## 3. Download data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","9000e445":"### Commit 11","f7cef089":"# Acknowledgements\n\n* [Stock Embedding - FFNN - My features](https:\/\/www.kaggle.com\/alexioslyon\/stock-embedding-ffnn-my-features) from @alexioslyon\n* [Stock Embedding - FFNN - My features](https:\/\/www.kaggle.com\/tatudoug\/stock-embedding-ffnn-my-features) from @tatudoug\n* [Stock Embedding - FFNN - features of the best lgbm](https:\/\/www.kaggle.com\/tatudoug\/stock-embedding-ffnn-features-of-the-best-lgbm) from @tatudoug\n* [NN Starter - Stock Embedding](https:\/\/www.kaggle.com\/lucasmorin\/tf-keras-nn-with-stock-embedding)\n* [Embedding Layers](https:\/\/www.kaggle.com\/colinmorris\/embedding-layers)\n* [Optiver Realized Volatility LGBM Baseline](https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline)\n* tuning and visualization from [Higher LB score by tuning mloss - upgrade & visual](https:\/\/www.kaggle.com\/vbmokin\/higher-lb-score-by-tuning-mloss-upgrade-visual) and [MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual](https:\/\/www.kaggle.com\/vbmokin\/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual)\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)","a02db221":"### 2.1. Commit now <a class=\"anchor\" id=\"2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","ea8011f0":"### Commit 5","e8dd4850":"### Commit 15","182a2cfa":"### Commit 14","9f3130f4":"### Commit 13","7520953c":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","67af327d":"### I use the notebook [Stock Embedding - FFNN - My features](https:\/\/www.kaggle.com\/alexioslyon\/stock-embedding-ffnn-my-features) from [alexioslyon](https:\/\/www.kaggle.com\/alexioslyon) as a basis and tried to tune its various parameters. ","b846fd2a":"## 5. Modeling and prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","5a67e862":"### 2.2 Previous commits <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","bc2a1671":"### Commit 0 (parameters from [Stock Embedding - FFNN - My features](https:\/\/www.kaggle.com\/alexioslyon\/stock-embedding-ffnn-my-features), version 4)","8e068f19":"### Commit 17","db7bb526":"### Commit 16","61a90b11":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Optiver Realized Volatility Prediction](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction)","ff27bd98":"### Commit 4","87124702":"## My upgrade:\n\nImproved notebook structure.\n\nTuning and 3D visualization of prediction results is performed for different:\n\n* Feature engineering\n* Learning rate\n* Number of epochs\n\nFE: I tried to use 350 instead of 300 - it worsened the result to LB = 0.20209.","5086f948":"[Go to Top](#0)","18941738":"### Commit 8"}}