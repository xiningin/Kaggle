{"cell_type":{"a2b6127a":"code","77d608f0":"code","85d37dbb":"code","8349e5cf":"code","37612cbb":"code","02bd53b8":"code","dd7e676d":"code","a28edcc7":"code","de7272da":"code","4af01a31":"code","cd67553c":"code","23f478a7":"code","a228f5c1":"code","46c29b45":"code","504a8c16":"code","9e5457ed":"code","32567b91":"code","38d4fca5":"code","f6af9418":"code","62cea282":"code","16dd986f":"code","da8c18dc":"code","e991b51c":"code","cb4a30ac":"code","d3af1723":"code","0444394d":"code","4e8762fd":"code","3269a6e6":"code","c6681055":"code","b86cfe78":"code","c541d1cc":"code","3c739b1f":"code","da88bc07":"code","7027dd0b":"code","c3886234":"code","56439f0d":"code","26a23a95":"code","34c4045b":"code","9fbafb9b":"code","9893f1bb":"code","0816f81f":"code","101c0ad5":"markdown","66b34207":"markdown"},"source":{"a2b6127a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77d608f0":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\nfrom sklearn.metrics import accuracy_score","85d37dbb":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","8349e5cf":"df_train.head()","37612cbb":"df_train.info()","02bd53b8":"fig = plt.figure(figsize=(12,9))\n\nax1 = fig.add_subplot(121)\nsns.barplot(df_train['keyword'].isnull().value_counts().index, df_train['keyword'].isnull().value_counts().values, palette='mako', ax=ax1)\nax1.set_title('Missing Values in Keyword')\n\nax2 = fig.add_subplot(122)\nsns.barplot(df_train['location'].isnull().value_counts().index, df_train['location'].isnull().value_counts().values, palette='mako', ax=ax2)\nax2.set_title('Missing Values in Location')\n\nfig.suptitle('Missing Values')\nplt.show()","dd7e676d":"plt.figure(figsize=(12,9))\nsns.barplot(df_train['target'].value_counts().index, df_train['target'].value_counts().values)\nplt.title('Target Values')\nplt.xlabel('0:not disaster|1:disaster')\nplt.show()","a28edcc7":"df_tgroup = df_train.groupby('target').size()\n\ndf_tgroup.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['blue','green'])\nplt.title(\"Pie chart of Target\",fontsize=16)\nplt.legend()\nplt.show()","de7272da":"data = df_train.location.value_counts()[:20]\ndata = pd.DataFrame(data)\ndata = data.reset_index()\ndata.columns = ['location', 'counts']\n\ngeolocator = Nominatim(user_agent='Location Map')\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n\ndict_lat = {}\ndict_long = {}\nfor i in data.location.values:\n    print(i)\n    location = geocode(i)\n    dict_lat[i] = location.latitude\n    dict_long[i] = location.longitude\n\ndata['latitude'] = data.location.map(dict_lat)\ndata['longitude'] = data.location.map(dict_long)","4af01a31":"location_map = folium.Map(location=[7.0,7.0], zoom_start=2)\nmarkers=2\n\nfor i,row in data.iterrows():\n  loss = row['counts']\n  if row['counts']>0:\n    count = row['counts']*0.4\n  folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='red', fill=True).add_to(location_map)\n\nlocation_map","cd67553c":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","23f478a7":"def word_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Replace all abbreviations\ndef replace_abbrev(text):\n    string = \"\"\n    for word in text.split():\n        string += word_abbrev(word) + \" \"        \n    return string\ndf_train['cleaned_text'] = df_train['text'].apply(replace_abbrev)\ndf_test['cleaned_text'] = df_test['text'].apply(replace_abbrev)","a228f5c1":"import nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = nltk.corpus.stopwords.words(['english'])\nlem = WordNetLemmatizer()\n\nprint(stop_words)\n\ndef cleaning(data):\n    #remove urls\n    tweet_without_url = re.sub(r'http\\S+',' ', data)\n\n    #remove hashtags\n    tweet_without_hashtag = re.sub(r'#\\w+', ' ', tweet_without_url)\n\n    #3. Remove mentions and characters that not in the English alphabets\n    tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n    \n    #3. Remove Puncs\n    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n    \n    #6. Joining\n    return \" \".join(text_cleaned)","46c29b45":"df_train['cleaned_text'] = df_train['cleaned_text'].apply(cleaning)\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(cleaning)","504a8c16":"# df_train['cleaned_text'] = df_train['cleaned_text'].apply(stemming)\n# df_test['cleaned_text'] = df_test['cleaned_text'].apply(stemming)\n\ndf_train['cleaned_text'] = df_train['cleaned_text'].apply(lambda x : x.lower())\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x : x.lower())","9e5457ed":"df_train.drop_duplicates(subset=['cleaned_text'], inplace=True)\ndf_test.drop_duplicates(subset=['cleaned_text'], inplace=True)","32567b91":"df_train.dropna(how='any', inplace=True, axis=1)\ndf_test.dropna(how='any', inplace=True, axis=1)","38d4fca5":"def collect_tokens(data, target):\n    tokens = []\n    \n    for i in data[data['target'] == target]['cleaned_text'].str.split():\n        for j in i:\n            tokens.append(j)\n    return tokens","f6af9418":"disaster_tokens = collect_tokens(df_train, 1)\nnon_disaster_tokens = collect_tokens(df_train, 0)","62cea282":"plt.figure(figsize=(14,8))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(disaster_tokens[:50]))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most words in disaster ', fontsize=20)\nplt.show()","16dd986f":"plt.figure(figsize=(14,8))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(non_disaster_tokens[:50]))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most words in non disaster ', fontsize=20)\nplt.show()","da8c18dc":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nmax_features = 5000\ncount_vectorizer = CountVectorizer(max_features=max_features)\nsparce_matrix_train=count_vectorizer.fit_transform(df_train['cleaned_text'])\nsparce_matrix_test=count_vectorizer.fit_transform(df_test['cleaned_text'])\n\ndef count_vector(data):\n    count = CountVectorizer()\n    vector = count.fit_transform(data)\n    return vector, count_vectorizer\n\ndef tfidf_vector(data):\n    tfidf = TfidfVectorizer()\n    vector_tfidf = tfidf.fit_transform(data)\n    return vector_tfidf, tfidf\n\nX_train_count, count_vectorizer = count_vector(df_train['cleaned_text'])\nX_train_tfidf, tfidf_vectorizer = tfidf_vector(df_train['cleaned_text'])\n\nX_test_count = count_vectorizer.transform(df_test['cleaned_text'])                                                     \nX_test_tfidf = tfidf_vectorizer.transform(df_test['cleaned_text'])","e991b51c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, classification_report\n\nnp.random.seed(0)\nrandom_state = 29","cb4a30ac":"def fit_pred(model, X_train,X_test,y_train,y_test):\n    clf = model\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    cmatx = confusion_matrix(y_test, y_pred)\n    \n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatx,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(clf))\n    plt.show()\n    \n    train_accuracy = round(clf.score(X_train,y_train)*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n    \n    print(classification_report(y_test,y_pred))    \n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))","d3af1723":"models=[\n        XGBClassifier(max_depth=6, n_estimators=1000),\n        LogisticRegression(random_state=random_state),\n        SVC(random_state=random_state),\n        MultinomialNB(),\n        DecisionTreeClassifier(random_state = random_state),\n        KNeighborsClassifier(),\n        RandomForestClassifier(random_state=random_state),\n       ]","0444394d":"for m in models:\n    y = df_train['target']\n    print('COUNTVECTOR')\n    \n    X = X_train_count\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    fit_pred(m, X_train, X_test, y_train, y_test)\n    \n    print('TFIDFVECTOR')\n    X = X_train_tfidf\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    fit_pred(m, X_train, X_test, y_train, y_test)","4e8762fd":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, MaxPooling1D, GRU\nfrom keras.models import load_model\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","3269a6e6":"X = df_train['cleaned_text']\ny = pd.get_dummies(df_train['target']).values\nnum_classes = df_train['target'].nunique()","c6681055":"seed = 101 # fix random seed for reproducibility\nnp.random.seed(seed)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","b86cfe78":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n","c541d1cc":"from tensorflow.keras.preprocessing import sequence\nmax_words = 30\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_test.shape)","3c739b1f":"import tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM, Dropout\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nbatch_size = 128\nepochs = 20\n\nmax_features = 20000\nembed_dim = 100\n\nnp.random.seed(seed)\nK.clear_session()\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))    \n# model.add(Dropout(0.5))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","da88bc07":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n\ndef callbacks():\n  cb =[]\n  reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n                                       factor=0.5, patience=1, \n                                       verbose=1, mode='min', \n                                       min_delta=0.0001, min_lr=0,\n                                       restore_best_weights=True)\n  cb.append(reduceLROnPlat)\n  log = CSVLogger('log.csv')\n  cb.append(log)\n\n  es = EarlyStopping(monitor='val_loss', patience=5, verbose=0,\n                       mode='min', restore_best_weights=True)\n  cb.append(es)\n\n  return cb","7027dd0b":"history = model.fit(\n    X_train, \n    y_train, \n    validation_data=(X_test, y_test),\n    epochs=epochs, \n    batch_size=batch_size, \n    verbose=2,\n    callbacks = callbacks()\n)","c3886234":"model.evaluate(X_test, y_test)","56439f0d":"# df_test['cleaned_text']\ntest_data = tokenizer.texts_to_sequences(df_test['cleaned_text'])\ntest_data = sequence.pad_sequences(test_data, maxlen=max_words)","26a23a95":"pred = model.predict(test_data)\npred","34c4045b":"idx = [x for x in df_test['id']]\ntarget = [x for x in np.argmax(pred,axis=1)]","9fbafb9b":"submit = pd.DataFrame({\n    'id':idx,\n    'target':target\n})","9893f1bb":"submit","0816f81f":"submit.to_csv('submission.csv', index=False)","101c0ad5":"## 1 Way","66b34207":"## 2 Way"}}