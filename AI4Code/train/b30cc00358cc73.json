{"cell_type":{"c816da70":"code","80bffa12":"code","2827b1ff":"code","c60bd400":"code","1be78ca7":"code","d8bc27bf":"code","2eef789d":"code","1fe1b08a":"code","bd4be295":"code","0a12d9a5":"code","770e2b7d":"code","035e3863":"code","afe7da30":"code","a79a495a":"code","04c4997c":"code","8692c031":"code","3e244e13":"code","7dc26118":"code","84e97f6a":"code","a945abb7":"code","c78fc5b9":"code","84d81ad9":"code","593d7320":"code","f5dce454":"code","2adc9b45":"code","6eac20f1":"markdown","7e491ea9":"markdown","a7dff38b":"markdown","5888373f":"markdown","8bace442":"markdown","1f369ff6":"markdown","ff8e9c13":"markdown","4e44990b":"markdown","67cf8b40":"markdown","b588ff3d":"markdown","343fc817":"markdown","91fd2145":"markdown","d0abbe83":"markdown","7f33c7c7":"markdown","9f69d141":"markdown","b14f46c2":"markdown","b46a603e":"markdown","241e6687":"markdown","ded68a86":"markdown"},"source":{"c816da70":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80bffa12":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport math","2827b1ff":"df = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","c60bd400":"print(f'Shape of heart.csv: {df.shape}')","1be78ca7":"print(f'\\nFirst 5 rows of heart.csv:')\ndf.head()","d8bc27bf":"print(f'Feature information:')\ndf.info()","2eef789d":"print('Number of null values in each column:')\ndf.isnull().sum()","1fe1b08a":"print('Number of unique values in each column:')\ndf.nunique(axis=0)","bd4be295":"print('Variance in each column:')\ndf.var(axis=0)","0a12d9a5":"CATEGORICAL_FEATURES = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'thall', 'caa']","770e2b7d":"df_ = pd.DataFrame()\nfor column in CATEGORICAL_FEATURES:\n    df_[column] = df[column].astype(\"category\")\n\nprint(f'Basic categorical feature statistical information:')\ndf_[CATEGORICAL_FEATURES].describe()","035e3863":"rows = 4\ncolumns = math.ceil(len(CATEGORICAL_FEATURES)\/rows)\nfig, ax = plt.subplots(\n    rows,\n    columns,\n    figsize=(3*columns, 3*rows),\n    sharey=False,\n)\nfor i, column in enumerate(CATEGORICAL_FEATURES):\n    plot = sns.countplot(\n        data=df, \n        x=column, \n        ax=ax[i%rows, i\/\/rows], \n        color=sns.color_palette('deep')[0],\n    )\n    plot.set_xlabel(column, fontsize=14)\n    if i\/\/rows > 0:\n        plot.set(ylabel=None)\nfig.suptitle('Count Plot for categorical features', fontsize=16)\nfig.tight_layout()","afe7da30":"rows = 4\ncolumns = math.ceil(len(CATEGORICAL_FEATURES)\/rows)\nfig, ax = plt.subplots(\n    rows,\n    columns,\n    figsize=(4*columns, 4*rows),\n    sharey=False,\n)\nfor i, column in enumerate(CATEGORICAL_FEATURES):\n    plot = sns.countplot(\n        data=df, \n        x=column, \n        hue='output', \n        ax=ax[i%rows, i\/\/rows], \n        palette=sns.color_palette('deep'),\n    )\n    plot.set_xlabel(column, fontsize=14)\n    if i\/\/rows > 0:\n        plot.set(ylabel=None)\nfig.suptitle('Count plot for categorical features split by target', fontsize=16)\nfig.tight_layout()","a79a495a":"CONTINUOUS_FEATURES = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']","04c4997c":"print(f'Basic continuous feature statistical information:')\ndf[CONTINUOUS_FEATURES].describe()","8692c031":"rows = 3\ncolumns = math.ceil(len(CONTINUOUS_FEATURES)\/rows)\nfig, ax = plt.subplots(\n    rows,\n    columns,\n    figsize=(4*columns, 4*rows),\n    sharey=False,\n)\nax[-1, -1].axis('off')\nfor i, column in enumerate(CONTINUOUS_FEATURES):\n    plot = sns.histplot(\n        data=df, \n        x=column, \n        ax=ax[i%rows, i\/\/rows], \n        linewidth=0,\n        color=sns.color_palette('deep')[0],\n    )\n    plot.set_xlabel(column, fontsize=14)\n    if i\/\/rows > 0:\n        plot.set(ylabel=None)\nfig.suptitle('Histogram for each continuous feature', fontsize=16)\nfig.tight_layout()","3e244e13":"rows = 2\ncolumns = math.ceil(len(CONTINUOUS_FEATURES)\/rows)\nfig, ax = plt.subplots(\n    rows, \n    columns,\n    figsize=(2*columns, 5*rows),\n    sharey=False,\n)\nax[-1, -1].axis('off')\nfor i, column in enumerate(CONTINUOUS_FEATURES):\n    plot = sns.boxenplot(\n        data=df, \n        y=column, \n        x='output', \n        ax=ax[i%rows, i\/\/rows], \n        linewidth=1,\n        palette=sns.color_palette('deep'), \n    )\n    plot.set_ylabel(column, fontsize=14)\n    plot.set(xlabel=None)\nfig.suptitle('Boxen plot for each continuous feature split by target', fontsize=16)\nfig.tight_layout()\nplt.show()","7dc26118":"rows = 3\ncolumns = math.ceil(len(CONTINUOUS_FEATURES)\/rows)\nfig, ax = plt.subplots(\n    rows,\n    columns,\n    figsize=(5*columns, 4*rows),\n    sharey=False,\n)\nax[-1, -1].axis('off')\nfor i, column in enumerate(CONTINUOUS_FEATURES):\n    sns.kdeplot(\n        data=df, \n        x=column, \n        hue='output', \n        ax=ax[i%rows, i\/\/rows],  \n        fill=True, \n        linewidth=0,\n        palette=sns.color_palette('dark')[:2],\n        alpha=.3,\n    )\n    plot.set_xlabel(column, fontsize=14)\n    if i\/\/rows > 0:\n        plot.set(ylabel=None)\nfig.suptitle('KDE Density plot for each continuous feature split by target', fontsize=16)\nfig.tight_layout()","84e97f6a":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split","a945abb7":"# Copy df before creating dummies for categorical features\ndf_copy = df.copy(deep=True)\n\n# Converting categorical features into binary categories\ndf_copy = pd.get_dummies(df_copy, columns=CATEGORICAL_FEATURES, drop_first=True)\n\n# Scaling continuous features using RobustScaler\nscaler = RobustScaler()\ndf_copy[CONTINUOUS_FEATURES] = scaler.fit_transform(df_copy[CONTINUOUS_FEATURES])\n\n# Splitting the data\nX = df_copy.drop(['output'], axis=1)\ny = df_copy[['output']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=75)\nfor name, split in {\n    'X_train': X_train, \n    'y_train': y_train, \n    'X_test': X_test, \n    'y_test': y_test,\n}.items():\n    descripter = f'The shape of {name} is:'\n    print(f'{descripter:<24} {split.shape}')","c78fc5b9":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom time import time","84d81ad9":"def train_and_print_results(classifier, header):\n    start_time = time()\n    model = classifier.fit(X_train, np.ravel(y_train))\n    total_time = time() - start_time\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(header)\n    print(f\"  accuracy: {accuracy:.3f}\")\n    print(f'  time: {total_time*100:.3f}ms')","593d7320":"# No hyperparameter tuning\nclassifier = KNeighborsClassifier()\ntrain_and_print_results(classifier, 'KNeighborsClassifier no tuning')\n\n# Parameter tuning\nparameters = {\n    'n_neighbors': [1, 2, 4, 8, 16, 32, 64],\n    'weights': ['uniform', 'distance'],\n}\nclassifier = GridSearchCV(\n    KNeighborsClassifier(), \n    parameters,\n    cv=3,\n)\ntrain_and_print_results(classifier, 'KNeighborsClassifier with tuning')","f5dce454":"# No tuning\nclassifier = SVC()\ntrain_and_print_results(classifier, 'SVC no tuning')\n\n# Linear\nparameters = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'kernel': ['linear'],\n}\nclassifier = GridSearchCV(\n    SVC(),\n    parameters,\n    cv=3,\n)\ntrain_and_print_results(classifier, 'Linear SVC with tuning')\n\n# Poly\nparameters = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'kernel': ['poly'],\n    'degree': [.5, 2, 3, 4, 5, 10],\n    'gamma': ['scale', 'auto'],\n}\nclassifier = GridSearchCV(\n    SVC(),\n    parameters,\n    cv=3,\n)\ntrain_and_print_results(classifier, 'Poly SVC with tuning')\n\n# RBF\nparameters = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'kernel': ['rbf'],\n    'gamma': ['scale', 'auto'],\n}\nclassifier = GridSearchCV(\n    SVC(),\n    parameters,\n    cv=3,\n)\ntrain_and_print_results(classifier, 'RBF SVC with tuning')","2adc9b45":"# No tuning\nclassifier = SGDClassifier(random_state=75)\ntrain_and_print_results(classifier, 'SGDClassifier no tuning')\n\n# Tuning\nparameters = {\n    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n    'penalty': ['l1', 'l2', 'elasticnet'],\n    'random_state': [75],\n}\nclassifier = GridSearchCV(\n    SGDClassifier(),\n    parameters,\n    cv=3,\n)\ntrain_and_print_results(classifier, 'SGDClassifier with tuning')","6eac20f1":"## 1.4 Categorical feature exploration\n\n### List of categorical features","7e491ea9":"# 2. Data Preparation\n\n## 2.1 Imports","a7dff38b":"# 4. Results\nThe best performing model was the poly kernal Support Vector Classifier, with an accuracy of 85%. It should be noted that the accuracy is fairly dependent on the random state used to split the training and testing data. This is likely due to the relatively small data set provided, with only 303 samples.","5888373f":"# 1. Exploration\n\n## 1.1 Imports","8bace442":"There are 303 records in the dataset, and 14 features to describe them.","1f369ff6":"## 3.3 K-Nearest Neighbor Classifier","ff8e9c13":"As there are no null values in the data, null-filling will not be necessary in the data preparation.","4e44990b":"## 2.2 Creating training set and test set","67cf8b40":"Notes:\n* While it may seem intuitive that greater age would correspond with a high likeliness of heart attack, heart attacks skew younger for this data.","b588ff3d":"## 3.2 Defining a function to train, predict, and score a classifier","343fc817":"## 1.3 Basic data structure exploration","91fd2145":"Notes:\n* Samples of sex category 0 (female) are much more likely to have a target output of 1 (heart attack), especially as compared to sex category 1 (male).\n* exng 0, cp 1 & 2, slp 2, thall 2, and caa 0 have high rates of heart attack.\n\n## 1.5 Continuous feature exploration\n\n### List of continuous features","d0abbe83":"## 1.2 Read CSV","7f33c7c7":"Notes:\n* The high frequency of category 0 in oldpeak is predominantly from output 1 samples.","9f69d141":"With such a wide range of variance across the columns, some features, particularly the continuous features, will require scaling.","b14f46c2":"## 3.5 Stochastic Gradient Descent Classifier","b46a603e":"Notes:\n* There are nearly twice as many of sex category 1 (male) as there are of category 0 (female).","241e6687":"# 3. Models\n\n## 3.1 Imports","ded68a86":"## 3.4 Support Vector Classifier"}}