{"cell_type":{"38b51bb0":"code","7e812c62":"code","8d71d96a":"code","45e50735":"code","16f7b312":"code","1150be72":"code","2d23ee1a":"code","8cd097e1":"code","650c925b":"code","4ee0fceb":"code","8e550c2a":"code","dfadefe5":"code","10e73046":"code","2eacb93e":"code","2c5fed38":"code","6f1d3d49":"code","5860e5ad":"code","bd2dd85f":"code","35aa4851":"code","2c45f826":"code","3d7f8301":"code","68a000e6":"code","1b566638":"code","cc720089":"code","44c0c623":"code","8291ea30":"code","1d9488cc":"code","69f02026":"code","3f7c2c7b":"code","f4aa80f1":"code","8dfb255a":"code","20eed537":"code","7319a4db":"code","eb7bd50d":"code","e658ee36":"code","fb1899a1":"code","0b7ab74f":"code","81aa3224":"code","24dea40d":"code","6dd65a8c":"code","d0bb687d":"code","6effe5e3":"code","0bfde41d":"code","602e137d":"code","f90baea2":"code","f81a1802":"code","78dec7f6":"code","a31f9481":"code","01832508":"code","6303d517":"code","f75299b3":"code","e284d1d6":"code","a2b20505":"markdown","9dd82373":"markdown","e4ed3a50":"markdown","9fbd76b3":"markdown","fd8788a2":"markdown","f86be64e":"markdown","1951c974":"markdown","46f0dc74":"markdown","0641c5cc":"markdown","a0ebf4ae":"markdown","0a0b796c":"markdown","7168b1f1":"markdown","9f2a159c":"markdown","37c0a217":"markdown","7ed3f8bb":"markdown","9041b57f":"markdown","423fa656":"markdown","97f24414":"markdown"},"source":{"38b51bb0":"#Using https:\/\/towardsdatascience.com\/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a","7e812c62":"from pandas import DataFrame, read_csv\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\nfile = '..\/input\/default of credit card clients No Labels.xls'\ndf = pd.read_excel(file, header=0)\n#Import Data","8d71d96a":"import numpy as np\ndata = np.asarray(df)\n#Changing Data to Array","45e50735":"X = data[:,:-1] # X is first column to second last column\ny = data[:,-1]  # y is the last column\nX, y","16f7b312":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=31416*27183)\nx_train, y_train, x_test, y_test\n#Splitting data randomly into testing data and training data, 20% test","1150be72":"X.shape   #Checking Shape of X","2d23ee1a":"x_train.shape   #Checking Shape of Training Set","8cd097e1":"x_test.shape   #Checking Shape of Testing Set","650c925b":"from sklearn.linear_model import LogisticRegression\n#Import Model","4ee0fceb":"# all parameters not specified are set to their defaults\n# scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nlogisticRegr = LogisticRegression(tol=1e-6,penalty='l2',solver='lbfgs', max_iter=1000)","8e550c2a":"logisticRegr.fit(x_train, y_train)\n#Learn data","dfadefe5":"predictions = logisticRegr.predict(x_test)\npredictions","10e73046":"# Use score method to get accuracy of model\nscore = logisticRegr.score(x_test, y_test)\nprint(score)","2eacb93e":"# print(confusion_matrix(y_test, predictions))\n# print(classification_report(y_test, predictions))","2c5fed38":"# Returns a NumPy Array\n# Predict for One Observation\nk = np.random.randint(y_test.size)\nk, logisticRegr.predict(x_test[k].reshape(1,-1)), y_test[k]","6f1d3d49":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","5860e5ad":"file = '..\/input\/default of credit card clients No Labels.xls'\ndf = pd.read_excel(file, header=0)","bd2dd85f":"df","35aa4851":"data = np.asarray(df)\ndata","2c45f826":"X = data[:,:-1] # X is first column to second last column\ny = data[:,-1]  # y is the last column\nX, y","3d7f8301":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nx_train, y_train, x_test, y_test","68a000e6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","1b566638":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(x_train, y_train)","cc720089":"y_pred = classifier.predict(x_test)","44c0c623":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","8291ea30":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train, y_train)\n    pred_i = knn.predict(x_test)\n    error.append(np.mean(pred_i != y_test))","1d9488cc":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","69f02026":"from scipy.stats import randint\nimport seaborn as sns # used for plot interactive graph. \nfrom pandas import set_option\nplt.style.use('ggplot') # nice plots\n\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.model_selection import GridSearchCV # for tuning parameter\nfrom sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.ensemble import RandomForestClassifier\n# import xgboost as xgb\n# from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport os\n#print(os.listdir(\"..\/input\"))","3f7c2c7b":"data = pd.read_csv(\"..\/input\/default of credit card clients.csv\", skiprows=1)\ndata.sample(5)","f4aa80f1":"data.rename(columns={\"default payment next month\": \"Default\"}, inplace=True)\ndata.drop('ID', axis = 1, inplace =True) # drop column \"ID\"\ndata.info()","8dfb255a":"# Separating features and target\ny = data.Default     # target default=1 or non-default=0\nfeatures = data.drop('Default', axis = 1, inplace = False)","20eed537":"# Original dataset\nX = data.drop('Default', axis=1)  \ny = data['Default']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)","7319a4db":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [1,2,3,4,5,6,7,8,9],\n              \"max_features\": [1,2,3,4,5,6,7,8,9],\n              \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_distributions=param_dist, cv=5, random_state=0)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))","eb7bd50d":"Tree = DecisionTreeClassifier(criterion= 'gini', max_depth= 7, \n                                     max_features= 9, min_samples_leaf= 2, \n                                     random_state=0)\nTree.fit(X_train, y_train)\ny_pred = Tree.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(Tree, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)), \n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,Tree.predict(X_test))\n#sns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            #xticklabels = ['Non-default', 'Default'], \n            #yticklabels = ['Non-default', 'Default'])\n#plt.ylabel('True label')\n#plt.xlabel('Predicted label')\n#plt.title(\"Confusion Matrix - Decision Tree\");","e658ee36":"data['EDUCATION']=np.where(data['EDUCATION'] == 5, 4, data['EDUCATION'])\ndata['EDUCATION']=np.where(data['EDUCATION'] == 6, 4, data['EDUCATION'])\ndata['EDUCATION']=np.where(data['EDUCATION'] == 0, 4, data['EDUCATION'])","fb1899a1":"data['EDUCATION'].unique()","0b7ab74f":"data['MARRIAGE'].unique()","81aa3224":"data['MARRIAGE']=np.where(data['MARRIAGE'] == 0, 3, data['MARRIAGE'])\ndata['MARRIAGE'].unique()","24dea40d":"# The frequency of defaults\nyes = data.Default.sum()\nno = len(data)-yes\n\n# Percentage\nyes_perc = round(yes\/len(data)*100, 1)\nno_perc = round(no\/len(data)*100, 1)\n\nimport sys \nplt.figure(figsize=(7,4))\nsns.set_context('notebook', font_scale=1.2)\nsns.countplot('Default',data=data, palette=\"Blues\")\nplt.annotate('Non-default: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\nplt.annotate('Default: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\nplt.annotate(str(no_perc)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\nplt.annotate(str(yes_perc)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\nplt.title('COUNT OF CREDIT CARDS', size=14)\n#Removing the frame\nplt.box(False);","6dd65a8c":"# set_option('display.width', 100)\n# set_option('precision', 2)\n\n# print(\"SUMMARY STATISTICS OF NUMERIC COLUMNS\")\n# print()\n# print(data.describe().T)","d0bb687d":"stdX = (features - features.mean()) \/ (features.std())              # standardization\ndata_st = pd.concat([y,stdX.iloc[:,:]],axis=1)\ndata_st = pd.melt(data_st,id_vars=\"Default\",\n                    var_name=\"features\",\n                    value_name='value')","6effe5e3":"# Original dataset\nX = data.drop('Default', axis=1)  \ny = data['Default']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)","0bfde41d":"# Dataset with standardized features\nXstd_train, Xstd_test, ystd_train, ystd_test = train_test_split(stdX,y, test_size=0.2, stratify=y,\n                                                                random_state=42)","602e137d":"NUM_FEATURES = 3\nmodel = LogisticRegression()\nrfe_stand = RFE(model, NUM_FEATURES)\nfit_stand = rfe_stand.fit(stdX, y)\n#print(\"St Model Num Features:\", fit_stand.n_features_)\n#print(\"St Model Selected Features:\", fit_stand.support_)\nprint(\"Std Model Feature Ranking:\", fit_stand.ranking_)\n# calculate the score for the selected features\nscore_stand = rfe_stand.score(stdX,y)\nprint(\"Standardized Model Score with selected features is: %f (%f)\" % (score_stand.mean(), score_stand.std()))","f90baea2":"feature_names = np.array(features.columns)\nprint('Most important features (RFE): %s'% feature_names[rfe_stand.support_])","f81a1802":"# Dataset with three most important features\nXimp = stdX[['PAY_0', 'BILL_AMT1', 'PAY_AMT2']]\nX_tr, X_t, y_tr, y_t = train_test_split(Ximp,y, test_size=0.2, stratify=y, random_state=42)","78dec7f6":"# Create the random grid\nparam_dist = {'n_estimators': [50,100,150,200,250],\n               \"max_features\": [1,2,3,4,5,6,7,8,9],\n               'max_depth': [1,2,3,4,5,6,7,8,9],\n               \"criterion\": [\"gini\", \"entropy\"]}\n\nrf = RandomForestClassifier()\n\nrf_cv = RandomizedSearchCV(rf, param_distributions = param_dist, \n                           cv = 5, random_state=0, n_jobs = -1)\n\nrf_cv.fit(X, y)\n\nprint(\"Tuned Random Forest Parameters: %s\" % (rf_cv.best_params_))","a31f9481":"Ran = RandomForestClassifier(criterion= 'gini', max_depth= 6, \n                                     max_features= 5, n_estimators= 150, \n                                     random_state=0)\nRan.fit(X_train, y_train)\ny_pred = Ran.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(Ran, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,Ran.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix - Random Forest\");","01832508":"y_pred_proba_RF = Ran.predict_proba(X_test)[::,1]\nfpr1, tpr1, _ = metrics.roc_curve(y_test,  y_pred_proba_RF)\nauc1 = metrics.roc_auc_score(y_test, y_pred_proba_RF)\n\nplt.figure(figsize=(10,7))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr1,tpr1,label=\"Random Forest, auc=\"+str(round(auc1,2)))\nplt.legend(loc=4, title='Models', facecolor='white')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC', size=15)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","6303d517":"#Specifying the model selection\nmodels = []\nmodels.append(('Random Forest', \n              RandomForestClassifier(criterion= 'gini', max_depth= 6, \n                                     max_features= 5, n_estimators= 150, \n                                     random_state=0), 'none'))\n\n# Evaluate the model\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model, Std in models:\n    if Std == 'Std':\n        cv_results = cross_val_score(model, stdX, y, cv=5, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)       \n    elif Std == 'none':\n        cv_results = cross_val_score(model, X, y, cv=5, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    else:\n        cv_results = cross_val_score(model, Ximp, y, cv=5, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","f75299b3":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","e284d1d6":"from astropy.table import Table, Column\ndata_rows = [('Logistic Regression', 0.62, 0.78, 0.69),\n             ('Artificial Neural Network', 0.81, 0.82, 0.81),\n             ('K-Nearest Neighbor', 0.77, 0.80, 0.78),\n             ('Decision Tree', 0.80, 0.82, 0.79),\n             ('Random Forest', 0.80, 0.82, 0.80)\n            ]\nt = Table(rows=data_rows, names=('Model', 'Precision', 'Recall', 'F1'))\nprint(t)","a2b20505":"* Repayment status in September (PAY_0)\n* Amount of bill statement in September (BILL_AMT1)\n* Amount of previous payments in August (PAY_AMT2)","9dd82373":"<a id='tree'><\/a>\n### Decision Tree Classifier\n\n\nIt is a non-parametric supervised learning method used for classification and regression. \nGoal: create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n\nAn internal node represents a feature, the branch represents a decision rule, and each leaf node represents the outcome. \n\nThey require less data preprocessing, i.e., no need to normalize features. \n\nHowever, noisy data can be easily overfitted and results in biased results when the data set is imbalanced.\n","e4ed3a50":"<a id='m'><\/a>\n### Precision, Recall, F1-score\nThe precision of a model is the ratio TP \/ (TP + FP). In this case, it is the ability of the classifier not to label as positive a sample that is negative. Precision is a good metric to use when the costs of false positive (FP) is high.\n\nThe recall of a model is the ratio TP \/ (TP + FN). In this case, it is the ability of the classifier to find all the positive class. Recall is a good metric to use when the cost associated with false negative (FN) is high. In this classification problem there is a high cost for the bank when a default credit card is predicted as non-default, since no actions can be taken. Thus, recall is one important metric to pay attention to.\n\nF1-score is a weighted average of precision and recall. Thus, it considers FP and FN. This metric is very useful when we have uneven class distribution, as it seeks a balance between precision and recall.","9fbd76b3":"Grouping the categories into a single class, e.g.: Education is '4'.","fd8788a2":"<a id='stat'><\/a>\n### Descriptive Statistics","f86be64e":"<a id='ml'><\/a>\n## Machine Learning: Classification models\n\nTo build machine learning models the original data was divided into features (X) and target (y) and then split into train (80%) and test (20%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm).\n\n<a id='sp'><\/a>\n### Spliting the data into train and test sets","1951c974":"<a id='eda'><\/a>\n## Exploratory Analysis\n\n<a id='map'><\/a>\n### Mapping the target: categorizing\n\nFrom this sample of 30,000 credit card holders, there were 6,636 default credit cards; that is, the proportion of default in the data is 22,1%.","46f0dc74":"There are variables that need to be converted to categories:\n* __SEX:__ Gender   \n                    1 = male \n                    2 = female\n* __EDUCATION:__     \n                     1 = graduate school \n                     2 = university \n                     3 = high school \n                     4 = others \n                     5 = unknown \n                     6 = unknown\n* __MARRIAGE:__ Marital status \n                    1 = married\n                    2 = single\n                    3 = others\n* __PAY_0,2,3,4,5,6:__ Repayment status in September 2005, August 2005, July 2005, June 2005, May 2005, April 2005 (respectivey)\n                    -2= no consumption\n                    -1= pay duly\n                    1 = payment delay for one month\n                    2 = payment delay for two months\n                    ... \n                    8 = payment delay for eight months\n                    9 = payment delay for nine months and above","0641c5cc":"<a id='rf'><\/a>\n### Random Forest Classifier\nRandom forest classifier is comprised of multiple decision trees. It creates different random subset of decision trees from the training set as its predictors and selects the best solution by means of voting. As a result, the Random Forest model avoids overfitting problems. ","a0ebf4ae":"<a id='roc'><\/a>\n### ROC Curve\nReceiver Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. The AUC is the Area Under Curve. If the AUC is high, the model is better distinguishing between positive and negative class. The ROC curve is plotted with \u201cTrue Positive Rate\u201d or Recall (on the y-axis) against the \u201cFalse Positive Rate\u201d (on the x-axis). When the AUC is 0.5 means that the model has no discrimination capacity to distinguish between positive and negative class.\n\nThe Receiver operating characteristic (ROC) Curve with the respective area under the curve (AUC) are shown below for each model.","0a0b796c":"<a id='fe'><\/a>\n## Feature engineering\n\nThe data has been already encoded and cleaned. However, some categorical data have repeated categories. For instance, the variable \u2018education\u2019 has three categories with similar information:<br>\n4: others, 5: unknown, and 6: unknown<br>","7168b1f1":"The Random Forest Classifier has a high mean accuracy of 0.82, but also a higher variation (0.0096). However, accuracy does not consider the rate of false positives (non-default credits cards that were predicted as default) and false negatives (default credit cards that were incorrectly predicted as non-default). Both cases have negative impact on the bank, since false positives leads to unsatisfied customers and false negatives leads to financial loss. ","9f2a159c":"<a id='sum'><\/a>\n## Review of model performance\nWe evaluate the performance of the model using the following metrics: accuracy, precision, recall, f1-score, AUC ROC curve, and confusion matrix.","37c0a217":"<a id='ml'><\/a>\n## Machine Learning: Classification models\n\nThe classification models used for this analysis are: Logistic Regression, Decision Tree and Random Forest Classifier.<br>\n\nTo build machine learning models the original data was divided into features (X) and target (y) and then split into train (80%) and test (20%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm).\n\n<a id='sp'><\/a>\n### Spliting the data into train and test sets","7ed3f8bb":"The AUC for the Random Forest Classifier model is 0.77. This means there is 77% chance that the model will be able to distinguish between default class and non-default class.","9041b57f":"# Predicting Credit Card Default using ML algorithms via scikit-learn","423fa656":"<a id='fs'><\/a>\n### Feature Selection\n\n#### Recursive Feature Elimination\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","97f24414":"lbfgs, l2, none: 0.7848333333333334   fast convergence in 10000\nsaga, l2 0.7846666666666666     slow, no convergence after 1000"}}