{"cell_type":{"47ef5d8e":"code","7c92d653":"code","6780a88b":"code","a13183cc":"code","74fea7e6":"code","786cf7f6":"code","13cd5960":"code","3c55e5ea":"code","2d917323":"code","ff077179":"code","af4aec99":"code","a5994280":"code","30dde0fe":"markdown","8bf2ae07":"markdown","b266ad56":"markdown","47119c7c":"markdown","9f4c8a5a":"markdown","70a527d3":"markdown","4c27d8cf":"markdown","e26ff446":"markdown","09c02637":"markdown","54a8aeb5":"markdown"},"source":{"47ef5d8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c92d653":"dataset=pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\nprint(dataset.head())","6780a88b":"# to check skewness of GRE Score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(dataset['GRE Score'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"GRE Score\")\nax.set(title=\"GRE Score distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\n\nprint(\"skew value: \", skew(dataset['GRE Score']))","a13183cc":"# to check skewness of TOEFL Score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(dataset['TOEFL Score'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"TOEFL Score\")\nax.set(title=\"TOEFL Score distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(dataset['TOEFL Score']))","74fea7e6":"# to check skewness of CGPA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(dataset['CGPA'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"CGPA\")\nax.set(title=\"CGPA distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(dataset['CGPA']))","786cf7f6":"# to check skewness of target variable\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(dataset['Chance of Admit '], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Chance of Admit\")\nax.set(title=\"Chance of Admit distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(dataset['Chance of Admit ']))","13cd5960":"# we need to normalize the target variable\nfrom scipy import stats\n\ncrim_box=stats.boxcox(dataset['Chance of Admit '])[0]\nprint(skew(crim_box))\n\n\n# to check skewness of target variable\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(crim_box, color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Chance of Admit\")\nax.set(title=\"Chance of Admit distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","3c55e5ea":"dataset['Chance of Admit ']=crim_box\nprint(\"skew value: \", skew(dataset['Chance of Admit ']))","2d917323":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ny=dataset[dataset.columns[8]].values;\nX=dataset[dataset.columns[[1,2,3,4,5,6,7]]].values\n\nprint(X.shape)\nprint(y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint(X_train.shape)\nprint(X_test.shape)","ff077179":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nmodel=RandomForestRegressor()\nmodel.fit(X_train,y_train)\n\ny_pred=model.predict(X_test)\nprint(\"Accuracy: \", model.score(X_test,y_test))\n\n\n#print(y_pred)\n#print(y_test)\n#print(accuracy_score(y_pred,y_test))\n\nprint(\"MSE: \", mean_squared_error(y_pred,y_test))\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_pred,y_test)))","af4aec99":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X_train,y_train)\n\ny_pred=model.predict(X_test)\nprint(\"Accuracy: \", model.score(X_test,y_test))\n\n\n#print(y_pred)\n#print(y_test)\n#print(accuracy_score(y_pred,y_test))\n\nprint(\"MSE: \", mean_squared_error(y_pred,y_test))\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_pred,y_test)))","a5994280":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nmodel=xgb.XGBRegressor()\nmodel.fit(X_train,y_train)\n\ny_pred=model.predict(X_test)\nprint(\"Accuracy: \", model.score(X_test,y_test))\n\n\n#print(y_pred)\n#print(y_test)\n#print(accuracy_score(y_pred,y_test))\n\nprint(\"MSE: \", mean_squared_error(y_pred,y_test))\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_pred,y_test)))","30dde0fe":"## Importing Dataset","8bf2ae07":"## Box-Cox Method to approximate Normal Distribution of Data\n\nNow for the target variable skew is -0.28. Lets try to bring this value in range (-0.1,0.1)\n\nHere boxcox function from scipy.stats library is used to normalize the skewness in the Target Variable. \n\nMathematical form:\n![Mathematical form](https:\/\/drive.google.com\/uc?export=view&id=1nSCIZVNTbDWlGgDhbXPzMStnkcNp7AAK)\n\nThe lambda variable varies from -5 to 5 and the value is calculated for each parameter. Optimal value is considered which would give the best approximation of a normal distribution curve.\n\nReference links:\n* [Statistic How To Article](https:\/\/www.statisticshowto.com\/box-cox-transformation\/)\n* [Towards Data Science Article](https:\/\/towardsdatascience.com\/top-3-methods-for-handling-skewed-data-1334e0debf45)","b266ad56":"## Checking Skewness of some variables.\n\nVariables to check:\n* GRE Score\n* TOEFL Score\n* CGPA\n* Chance of Admit (Target Variable)\n\nData with less amount of skewness is more suitable for predictive models.","47119c7c":"## Splitting the Dataset","9f4c8a5a":"## Training and Testing Different Models","70a527d3":"### 1. Random Forest Regressor","4c27d8cf":"### 3. XGB Regression","e26ff446":"### 2. Linear Regression","09c02637":"## *In this notebook, I have tried to find the skewness of variables and normalize those which has high skewness. Explored and found the use of Box Cox function which I have explained a bit and also given the reference links which helped me understand it*","54a8aeb5":"### Linear Regression gives the best performance!\n\n#### * Accuracy:  0.8709283447564493\n#### * MSE:  0.0020274705647666605\n#### * RMSE:  0.045027442352044166"}}