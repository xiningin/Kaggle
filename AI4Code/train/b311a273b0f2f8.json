{"cell_type":{"7599d39c":"code","493a262b":"code","fb4c89db":"code","141f342a":"code","214d3379":"code","d2337f9a":"code","b72ed0c6":"code","68872510":"code","06e45221":"code","fec80c70":"code","2b899ae3":"code","a104c2ff":"code","86d45922":"code","c2518cc3":"code","0e036db3":"code","5600182b":"code","261f5c30":"code","874b770a":"code","f69786db":"code","efafb2c3":"code","e2277a0e":"code","1aa0fcd3":"code","d162fa43":"code","e9c9e359":"code","99b5898b":"code","2e212e61":"code","6e5aef46":"code","36dbcd98":"code","5c853832":"code","42b58905":"code","92f51c89":"code","e47c7903":"code","dafc588f":"markdown","698eded2":"markdown","8bb9d9ba":"markdown","0c94e56e":"markdown","2f3e09ea":"markdown","f8ca0464":"markdown","fe7374cb":"markdown","33fe1064":"markdown","03f0115f":"markdown","4c533484":"markdown","96421f1d":"markdown","b3dcfce0":"markdown","da9fffd4":"markdown"},"source":{"7599d39c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","493a262b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, Normalizer, QuantileTransformer, OrdinalEncoder\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor","fb4c89db":"pd.set_option('display.max_columns', 25)","141f342a":"df=pd.read_csv(\"\/kaggle\/input\/melbourne-housing-snapshot\/melb_data.csv\")","214d3379":"df.head()","d2337f9a":"df.drop(['Address','Date'],axis=1,inplace=True)","b72ed0c6":"df.hist(figsize=(8,8))\nplt.tight_layout()","68872510":"for feature in df.select_dtypes(include='object').columns:\n    df.groupby(feature)['Price'].mean()[:20].plot(kind='barh')\n    plt.show()","06e45221":"df.corr()['Price'].sort_values()","fec80c70":"df.drop(['Propertycount','Landsize','BuildingArea'],axis=1,inplace=True)","2b899ae3":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr(),annot=True)","a104c2ff":"df.drop('Bedroom2',axis=1,inplace=True)","86d45922":"si=SimpleImputer(strategy='most_frequent')\noe=OrdinalEncoder()\ndf_cat=df.select_dtypes(include='object').columns\ndf2=pd.DataFrame()\nfor col in df_cat:\n    imp_col=si.fit_transform(np.array(df[col]).reshape(-1,1))\n    df2[col]=oe.fit_transform(imp_col)[:,0]","c2518cc3":"df2.join(df.Price).corr()['Price'].sort_values()","0e036db3":"df.drop(['CouncilArea','SellerG','Method','Regionname'],axis=1,inplace=True)","5600182b":"df.info()","261f5c30":"df.describe()","874b770a":"df.Price.hist()","f69786db":"X,y=df.drop('Price',axis=1),df.Price","efafb2c3":"X.shape,y.shape","e2277a0e":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include=['int64','float64']).columns\n\nimputer=SimpleImputer(strategy='most_frequent')\nohe=OneHotEncoder(handle_unknown='ignore')\nscaler=MinMaxScaler()\n\nnum_trans=Pipeline([('imputer',imputer),('scaler',scaler)])\ncat_trans=Pipeline([('imputer',imputer),('ohe',ohe)])\n\nct=ColumnTransformer([('num',num_trans,X_num),('cat',cat_trans,X_cat)])\nsel = SelectFromModel(RandomForestRegressor(n_estimators = 100)) \npipe=Pipeline([('ct',ct),('sel',sel)])\npipe.fit(X,y)","1aa0fcd3":"support=pipe.named_steps.sel.get_support()\nsupport","d162fa43":"imp_cols=pipe.named_steps.sel.estimator_.feature_importances_\nimp_cols","e9c9e359":"cat_cols=pipe.named_steps.ct.named_transformers_.cat.named_steps.ohe.get_feature_names(X_cat)\ncat_cols","99b5898b":"X_cols=list(X_num)+list(cat_cols)\nX_cols","2e212e61":"pd.DataFrame(imp_cols,index=X_cols)[support].sort_values(by=0).plot(kind='barh',figsize=(8,8))","6e5aef46":"models = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet())) \nmodels.append(('KNN', KNeighborsRegressor())) \nmodels.append(('CART', DecisionTreeRegressor())) \nmodels.append(('SVR', SVR()))","36dbcd98":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include=['int64','float64']).columns\n\nimputer=SimpleImputer(strategy='most_frequent')\nohe=OneHotEncoder(handle_unknown='ignore')\nscaler=MinMaxScaler()\n\nnum_trans=Pipeline([('imputer',imputer),('scaler',scaler)])\ncat_trans=Pipeline([('imputer',imputer),('ohe',ohe)])\n\nct=ColumnTransformer([('num',num_trans,X_num),('cat',cat_trans,X_cat)])\n\nSFM = SelectFromModel(RandomForestRegressor(n_estimators = 100)) \npipe=Pipeline([('ct',ct),('SFM',SFM)])\npipe.fit(X,y)\nX_SFM=pipe.transform(X)","5c853832":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    cv_results = cross_val_score(model, X_SFM, y, cv=kfold, scoring='neg_mean_absolute_error')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","42b58905":"leaf_size = list(range(1,5))\nn_neighbors = list(range(9,20))\np=[1,2]\n\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n\nmodel = KNeighborsRegressor()\n\nclf = GridSearchCV(model, hyperparameters, cv=10, scoring='neg_mean_absolute_error')\n\nbest_model = clf.fit(X_SFM,y)\n\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\nprint('Best score:', best_model.best_score_)","92f51c89":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nensembles = []\nensembles.append(('AB',AdaBoostRegressor()))\nensembles.append((('GBM',GradientBoostingRegressor())))\nensembles.append((('RF',RandomForestRegressor(n_estimators=10))))\nensembles.append((('ET',ExtraTreesRegressor(n_estimators=10))))","e47c7903":"results = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    cv_results = cross_val_score(model, X_SFM, y, cv=kfold, scoring='neg_mean_absolute_error')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","dafc588f":"# Prepare feature matrix\n- impute missing values\n- scaling\n- one hot encoding\n- feature selection","698eded2":"# Visualization","8bb9d9ba":"## Histplot for numeric features","0c94e56e":"- We begin by visualizing the data and looking at correlations among features \n- Next unimportant features are dropped, the data is transformed, and Random Forests are used to select the most important features\n- The performance of several algorithms will be compared and most promising model will be tuned\n- The same process is repeated for ensembles\n- Next time an analysis incorporating the time feature could be explored","2f3e09ea":"# Correlation","f8ca0464":"## Numeric features","fe7374cb":"# Ensemble","33fe1064":"## Categorical features","03f0115f":"## Barplot for categorical features","4c533484":"# Compare Models","96421f1d":"# Select relevant features using RandomForestRegressor and SelectFromModel","b3dcfce0":"# Melbourne Housing\n## Outline","da9fffd4":"# Tune KNN"}}