{"cell_type":{"52240450":"code","2a2d41ea":"code","1cc4d7db":"code","0f4252d5":"code","bbf96324":"code","7c075863":"code","8ae266d0":"code","36972eaf":"code","1da13e01":"code","c324bfcb":"code","88b0efae":"code","fb81870a":"code","f0e193ed":"code","e2ea9d39":"code","626f53c4":"code","b11c9137":"code","24a2d6f9":"code","921f85f4":"code","ad2e295d":"code","99efc67a":"code","71a39f22":"code","6d16b4fd":"code","f8db1b02":"code","adf041b9":"code","e5c6c1da":"code","7a9aa6ff":"code","5fd72611":"markdown","19f85497":"markdown","d27287e6":"markdown","76ea2524":"markdown","a5911b38":"markdown","3a014125":"markdown","248c4654":"markdown","2b0a5439":"markdown","0f721f5c":"markdown","a6b931b5":"markdown","3716605b":"markdown","3233b58c":"markdown","1975c7ee":"markdown","f4293698":"markdown","e3dd6b40":"markdown","0828d2d6":"markdown","90eeafc9":"markdown","8afd9e76":"markdown","d63f1442":"markdown","f551d594":"markdown","10686fcd":"markdown","40560e07":"markdown","f55ea783":"markdown","3c78f653":"markdown","6563bc2e":"markdown","fa7bb803":"markdown","61ab768f":"markdown","295f7f15":"markdown","be079818":"markdown","c4002869":"markdown","ba32501f":"markdown","5a681f50":"markdown","713348d2":"markdown","3d102aec":"markdown","b11599a2":"markdown","1c8e1872":"markdown","5c587ced":"markdown","97516003":"markdown","3898cbcd":"markdown","0506c8d1":"markdown","b33d5abb":"markdown","aaeda4ec":"markdown","8b284722":"markdown","94455c50":"markdown","e3f7cb75":"markdown","204b0b6d":"markdown"},"source":{"52240450":"%%writefile random_identification.py\ndef random_identification(observation, configuration):\n    return 2","2a2d41ea":"%%writefile random_forest_random.py\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = RandomForestClassifier(n_estimators=25)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action ","1cc4d7db":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","0f4252d5":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","bbf96324":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","7c075863":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","8ae266d0":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","36972eaf":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","1da13e01":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","c324bfcb":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","88b0efae":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","fb81870a":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","f0e193ed":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","e2ea9d39":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 2 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","626f53c4":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","b11c9137":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","24a2d6f9":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","921f85f4":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","ad2e295d":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make, evaluate","99efc67a":"env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)","71a39f22":"env.run([\"random_identification.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=500, height=400)","6d16b4fd":"# raise SystemExit(\"Stop right there!\")","f8db1b02":"list_names = [\n    \"rock\", \n    \"paper\", \n    \"scissors\",\n    \"hit_the_last_own_action\",  \n    \"copy_opponent\", \n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\",\n    \"statistical_prediction\",\n    \"random_forest_random\",\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nscores = np.zeros((len(list_names), 1), dtype=int)","adf041b9":"env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)","e5c6c1da":"for ind_agent_1 in range(len(list_names)):\n    current_score = evaluate(\n        \"rps\", \n        [\"random_identification.py\", list_agents[ind_agent_1]], \n        configuration={\"episodeSteps\": 1000}\n    )\n    scores[ind_agent_1, 0] = current_score[0][0]","7a9aa6ff":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=[\"random_identification.py\"],\n)\n\n\nplt.figure(figsize=(2, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap=\"coolwarm\", linewidths=1, linecolor=\"black\", \n    fmt=\"d\", vmin=-500, vmax=500,\n)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(rotation=360, fontsize=15);","5fd72611":"Setup battlefield","19f85497":"<a id=\"6\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Reactionary<center><h2>\n","d27287e6":"Copy from kernel [Decision Tree Classifier](https:\/\/www.kaggle.com\/alexandersamarin\/decision-tree-classifier?scriptVersionId=46415861)","76ea2524":"<a id=\"7\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Counter Reactionary<center><h2>\n\n","a5911b38":"Copy from kernel [(Not so) Markov \u26d3\ufe0f](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov)","3a014125":"Copy from kernel [Rock, Paper, Scissors with Memory Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns)","248c4654":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)","2b0a5439":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns<center><h2>\n\n\n","0f721f5c":"Based on results, only a random agent or a very simple agent can have a result bellow a few hundred. The case of the standard counter action ('paper' in this case) is not a threat for this competition as any decent responsive strategy can beat it.\n\nSo, the `random identification` agent can be used as a validation of a submition and identify if an agent has some kind of `inteligence` to be valid for competition or if it is completely random.\nIf an agent fails to beat `random identification` agent with a score bellow a few hunded then the submition should be disqualified as it is too `random`","a6b931b5":"<a id=\"4\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Scissors<center><h2>","3716605b":"<a id=\"102\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Marathon: Random Forest Random against all agents<center><h2>\n","3233b58c":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Scissors action","1975c7ee":"<a id=\"12\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>","f4293698":"<a id=\"3\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Paper<center><h2>","e3dd6b40":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>","0828d2d6":"<a id=\"14\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Decision Tree Classifier<center><h2>\n\n","90eeafc9":"<a id=\"9\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>\n\n\n","8afd9e76":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","d63f1442":"<a id=\"13\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Opponent Transition Matrix<center><h2>\n","f551d594":"<a id=\"202\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Random Identfication<center><h2>","10686fcd":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","40560e07":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Paper action","f55ea783":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation","3c78f653":"<a id=\"5\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Copy Opponent<center><h2>","6563bc2e":"Copy from kernel [Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents)","fa7bb803":"We need to import the library for creating environments and simulating agent battles","61ab768f":"Create environnment without debug","295f7f15":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n## Random identification agent\n* [Agent: random_identification](#202)\n\n## Agents\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Markov Agent](#10)\n* [Agent: Memory Patterns](#11)\n* [Agent: Multi Armed Bandit](#12)\n* [Agent: Opponent Transition Matrix](#13)\n* [Agent: Decision Tree Classifier](#14)\n* [Agent: Statistical Prediction](#15)\n* [Agent: Random Forest Random](#201)\n \n\n### Results    \n* [Results](#103)\n* [Review](#104)\n","be079818":"<a id=\"201\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Random Forest Random<center><h2>","c4002869":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py    \n\nAlways uses Rock action","ba32501f":"<a id=\"100\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Setup and validation<center><h2>","5a681f50":"<a id=\"8\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical<center><h2>\n\n\n","713348d2":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nHit the last action of the opponent","3d102aec":"Copy from kernel [Rock Paper Scissors - Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction)","b11599a2":"<a id=\"103\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Results<center><h2>","1c8e1872":"Validation","5c587ced":"# Random Agent identification - Rock Paper Scissors\n\nScope of this notebook is to create an agent tha can identify if the opponent is random or not responsive at all into actions.\n\nBased on [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison), any agent that monitors opponents move can beat very easily agents `rock`, `paper` and `scissors` except `nash equilibrium` and themselves.\n\nBased on this observation we can build an agent that responds in a very predictive way and monitor the commulative score that it loses. If the score against this agent is very low then the opponent is not responding with an `inteligent` way and either it is random or responds with a standard sequence.\n\nIf an agent fails to beat `random identification` agent with a score bellow a few hunded then the submition should be disqualified as it is too `random` or too easy.","97516003":"<a id=\"104\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Review<center><h2>","3898cbcd":"<a id=\"2\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Rock<center><h2>","0506c8d1":"Run the simulation","b33d5abb":"<a id=\"15\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical Prediction<center><h2>\n\n","aaeda4ec":"<a id=\"10\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Markov Agent<center><h2>\n\n\n","8b284722":"Copy from kernel [RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix)","94455c50":"Copy from kernel [Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy)\n\nNash Equilibrium Strategy (always random)","e3f7cb75":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nCopy the last action of the opponent","204b0b6d":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22838\/logos\/header.png?t=2020-11-02-21-55-44)"}}