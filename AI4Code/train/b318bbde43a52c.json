{"cell_type":{"e112ebd2":"code","d3da14b2":"code","1ebaf05c":"code","d05d6a30":"code","90c875f5":"code","52496177":"code","2e47b04a":"code","5c590060":"code","b4b51f9f":"code","879ac159":"code","57f36f91":"code","ad72021d":"code","2120f78f":"code","08418efd":"code","3a96cf93":"code","0394093e":"code","f19bad60":"code","b04c69dd":"code","b124a5f6":"code","5a7a2fce":"code","e6f8ce62":"code","708769fa":"code","c8008a19":"code","db336dbe":"code","0fa134b9":"code","2f4dff99":"code","cec53385":"code","5cd0732c":"code","75cde8de":"code","f1f42df5":"code","29afa3cb":"code","83667a53":"code","197592dc":"code","a1d6691e":"code","a8cf40c6":"code","e619b60e":"code","3dbe063a":"code","78b7934a":"code","3a06afe9":"code","b27cd4e2":"code","20d829bd":"code","321ef75d":"code","5c2b96de":"code","c1cd419c":"code","419e7332":"code","53510307":"code","5563a51f":"code","9b28a6e6":"code","f70ef31b":"code","5e4e8f29":"code","8283e4f9":"code","f2de3797":"code","844a481e":"code","3cbb68e6":"code","54fb45f2":"code","348fd092":"code","cfc198ad":"code","04bbe1dd":"code","995ac2a9":"code","cb95f9af":"code","ea6c31f9":"code","a4d2d0fb":"code","63e49425":"code","5d33378c":"code","13931e30":"code","8589f2d4":"code","dc085bb4":"code","261a10a2":"code","c8143605":"code","7e088abf":"code","d401d3cf":"code","b7aedd3b":"code","4fcc9285":"code","66d910df":"code","0844ebf5":"code","57ae2a84":"code","39c5c000":"code","17144cfb":"code","57c2e083":"code","4d1e4a83":"code","b4506e7d":"code","fade0a33":"code","c9eb731d":"code","b1d22482":"code","276ccdd9":"code","62c32423":"code","6f822755":"code","7acf475c":"code","f9e76c37":"code","0aa994f2":"code","75c95165":"code","9cb7a8f0":"code","20f4a758":"code","a65a4c3c":"code","546b43b6":"code","6206494b":"code","4187484f":"code","052ebafa":"code","113410b5":"code","f039a6bc":"code","a81ee5dc":"code","1f0dcec8":"code","fadd03c4":"code","39d5e37d":"code","d92018ce":"code","07dc57ad":"code","29427baa":"code","251942b6":"code","97e550fd":"code","e89c6d76":"code","1bb87305":"code","31c06164":"code","70348159":"code","b88f9ad9":"code","743b99df":"code","19d2f338":"code","8d70d926":"code","8eb4d109":"code","28225eee":"code","d1f0d423":"code","e8ab66c9":"code","291e3e64":"code","2af0eb37":"code","3121b2b7":"code","4d79d6a8":"code","9966114a":"code","ce844610":"code","d8b0ed76":"code","f1dbfd4f":"code","c005c779":"code","0c1b6ac3":"code","f5726b01":"code","abc33187":"code","f829af4e":"code","03135f5a":"code","393c7933":"code","72ce4447":"code","7c7f9c06":"code","5f101431":"code","8d210bcb":"code","d3cbb43f":"code","a9ed5ed5":"code","4999b835":"code","2d4a5195":"code","2eb32472":"code","d75ff019":"code","aa185181":"code","460e1b25":"code","eea7be82":"code","7068b4f5":"code","de94c883":"code","f46b24bc":"code","68c01b7e":"code","d67efbe3":"code","34cf4436":"code","64838e6e":"code","1cb71057":"code","4b0f415a":"code","faa81610":"code","10d8be22":"code","0f5a604c":"code","7c491a0a":"code","efcabb58":"code","e51d575f":"code","f9ee21ae":"code","8c8be6dd":"code","ebe96118":"code","df99d313":"code","2d55abab":"code","2a7a3833":"code","400cfc03":"code","e8ada8dc":"code","63ed2175":"code","56c6ad65":"code","0592546e":"code","74f1e14f":"code","63a02acc":"code","495e12d6":"code","8e2dfc42":"code","3e0586ad":"code","3c7c45a5":"code","ae83aa02":"code","3ad36ee4":"code","e74327e9":"code","9d99a8ea":"code","ea127f94":"code","c3507ba4":"code","ee28f5fd":"code","a1bc0fda":"code","dd10ad31":"code","83a06130":"code","889cf96b":"code","d5698384":"code","10f99f33":"code","4e1ed213":"code","c321d710":"code","8f5e7e6c":"code","dfcdf060":"code","28ff9516":"code","182f7314":"code","e23292ed":"code","84f2ed6c":"code","ba486fba":"code","2b1cc307":"code","b26d3f01":"code","4cd5be16":"code","ff083997":"code","c877b91d":"code","3f614809":"code","1176580a":"code","44454c32":"code","6ecaa752":"code","6c6773e3":"code","d71e4270":"code","447b04bc":"code","74c191dc":"code","91d60408":"code","e85a240d":"code","446761f3":"code","40fd84ba":"code","92311668":"code","e85d4454":"code","3fda54d6":"code","a0937614":"code","cef56a5e":"code","62c181cc":"code","fb3cafaf":"code","50518522":"code","f0c34eb6":"code","9908b6ca":"code","552a036d":"code","a3771f9d":"code","e3262f08":"code","dec3c139":"code","307c75e8":"code","40d13ad7":"code","9d8fb833":"code","dd7ba638":"code","2481caa0":"code","210b5c74":"code","c48adada":"code","6077f46a":"code","adbf037c":"code","a042e956":"code","5410c40d":"code","aade2c43":"code","f0240017":"code","2854c5a6":"code","26348f15":"code","d938325f":"code","a6428910":"code","ae2f273f":"code","bf7c1c57":"code","bfe47fa4":"code","ec86ab4c":"code","5163f51a":"code","5afea125":"code","a80b5d1f":"code","36469baa":"code","60d1c92b":"code","6f13c0d0":"code","75ef21f8":"code","bd12831b":"code","b5d6fe19":"code","b54d1b50":"code","91f5bdaa":"code","1e188a9a":"markdown","16c5234d":"markdown","55c8eb28":"markdown","f97dc54b":"markdown","fad6e5b2":"markdown","630f9801":"markdown","5093769a":"markdown","ab08e0c2":"markdown","76039b22":"markdown","07f3bef3":"markdown","4e988b92":"markdown","e9dc45d4":"markdown","d059a3c7":"markdown","e7ca78f2":"markdown","19370c65":"markdown","f6ece58d":"markdown","5470557b":"markdown","a0c02f17":"markdown","1d3deddf":"markdown","2738bae4":"markdown","f4cd3e5f":"markdown","fc9ab264":"markdown","e3d37141":"markdown","9745c283":"markdown","2f92a3d0":"markdown","5b9d5aae":"markdown","5413de75":"markdown","dd38555d":"markdown","e6801d3e":"markdown","36894ea3":"markdown","92b8da6d":"markdown","87659a96":"markdown","d78ae6f1":"markdown","8317aa7e":"markdown","8db7e3a1":"markdown","baa25b80":"markdown","c8df5bdf":"markdown","22b3b2b6":"markdown","618cc613":"markdown","91eaec54":"markdown","a5c8be06":"markdown","c44dd6ab":"markdown","65b95fa9":"markdown","061cf005":"markdown","463a7300":"markdown","286e75ad":"markdown","0de2ffb6":"markdown","c57638b8":"markdown","f7b2c32b":"markdown","ff31bff0":"markdown","0751c1cb":"markdown","42f14020":"markdown","fa2780c1":"markdown","84e10697":"markdown","dd1c466e":"markdown","2c81f147":"markdown","fd6733df":"markdown","d894661e":"markdown","ae1e1e42":"markdown","a04d5b27":"markdown","5b1512fb":"markdown","81b54e40":"markdown","5abad776":"markdown","f8af50ab":"markdown","86424f4b":"markdown","565c1c9f":"markdown","919261d8":"markdown","20c94a6c":"markdown","e744a268":"markdown","94515e34":"markdown","1897f1c2":"markdown","3d5d557f":"markdown","5e681aa1":"markdown","85eaa247":"markdown","a8f3440c":"markdown","383c9f5f":"markdown","c7ec68bf":"markdown","bb9922ac":"markdown","09a4fec3":"markdown","900b795c":"markdown","c3e6530c":"markdown","aed289b5":"markdown","c4e57a0d":"markdown"},"source":{"e112ebd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3da14b2":"# !pip install pyforest\n# 1-Import Libraies\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nfrom sklearn.compose import make_column_transformer\n\n# Scaling\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n\n# Importing plotly and cufflinks in offline mode\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\nfrom termcolor import colored\n\nimport ipywidgets\nfrom ipywidgets import interact\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nfrom wordcloud import WordCloud ","1ebaf05c":"## Some Useful Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('-'*79, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    \n        \ndef multicolinearity_control(df):\n    feature =[]\n    collinear=[]\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs=['bold']), df.shape,'\\n',\n                                  colored('-'*79, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\"Duplicates were dropped!\"),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"There are no duplicates\"),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')     \n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary will drop related columns!', attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)       \n    print('New shape after missing value control:', df.shape)\n        \n###############################################################################\n\n# To view summary information about the column\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","d05d6a30":"# df = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")","90c875f5":"df = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")","52496177":"df.tail()","2e47b04a":"df.sample(5)","5c590060":"first_looking(df)\nduplicate_values(df)\ndrop_columns(df,[])\ndrop_null(df, 90)","b4b51f9f":"df.columns","879ac159":"df.drop(['unnamed:_0', 'clothing_id'], axis = 1, inplace=True)","57f36f91":"df.shape","ad72021d":"df = df.rename(columns = {'Review Text' : 'text', 'recommended_ind' : 'recommended', \n                          'positive_feedback_count' : 'feedback_count', 'division_name' : 'division', \n                          'department_name' : 'department', 'class_name' :'class'})","2120f78f":"df.describe().T","08418efd":"df.describe(include=object).T","3a96cf93":"sns.heatmap(df.corr(), annot=True);","0394093e":"df.columns","f19bad60":"first_look(\"age\")","b04c69dd":"df.age.describe().T","b124a5f6":"px.histogram(df, x = df.age)","5a7a2fce":"pd.crosstab(df.age, df.recommended).iplot(kind=\"bar\")","e6f8ce62":"first_look(\"rating\")","708769fa":"df.rating.describe().T","c8008a19":"px.histogram(df, x = df.rating)","db336dbe":"pd.crosstab(df.rating, df.recommended).iplot(kind=\"bar\")","0fa134b9":"first_look(\"recommended\")","2f4dff99":"df.recommended.describe().T","cec53385":"px.histogram(df, x = df.recommended)","5cd0732c":"first_look(\"feedback_count\")","75cde8de":"df.feedback_count.describe().T","f1f42df5":"px.histogram(df, x = df.feedback_count)","29afa3cb":"pd.crosstab(df.feedback_count, df.recommended).iplot(kind=\"bar\")","83667a53":"first_look(\"division\")","197592dc":"df.division.describe().T","a1d6691e":"px.histogram(df, x = df.division)","a8cf40c6":"pd.crosstab(df.division, df.recommended).iplot(kind=\"bar\")","e619b60e":"first_look(\"department\")","3dbe063a":"df.department.describe().T","78b7934a":"px.histogram(df, x = df.department)","3a06afe9":"pd.crosstab(df.department, df.recommended).iplot(kind=\"bar\")","b27cd4e2":"first_look(\"class\")","20d829bd":"df[\"class\"].describe().T","321ef75d":"px.histogram(df, x = df[\"class\"])","5c2b96de":"pd.crosstab(df[\"class\"], df.recommended).iplot(kind=\"bar\")","c1cd419c":"first_look(\"title\")","419e7332":"df.title.describe().T","53510307":"first_look(\"review_text\")","5563a51f":"df.review_text.describe().T","9b28a6e6":"df.columns","f70ef31b":"# recommended : \n# Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n\nprint(df.recommended.value_counts())\n\nplt.figure(figsize=(10,10))\n\nexplode = [0,0.1]\nplt.pie(df.recommended.value_counts(), explode=explode,autopct='%1.1f%%', shadow=True,startangle=140)\nplt.legend(labels=['1 : recommended','0 : not recommended'])\nplt.title('Recommendation Distribution')\nplt.axis('off');","5e4e8f29":"df.columns","8283e4f9":"df_ml = df.copy()\n\n# df_ml = df[['review_text', 'recommended']].copy()","f2de3797":"drop_columns = ['age', \n                'title', \n                'rating',\n                'feedback_count', \n                'division',\n                'department',\n                'class']","844a481e":"df_ml.drop(drop_columns, axis = 1, inplace = True)","3cbb68e6":"df_ml.info()","54fb45f2":"df_ml.rename(columns = {'review_text':'text', 'recommended':'recommend'}, inplace = True)","348fd092":"df_ml.columns","cfc198ad":"missing_values(df_ml)","04bbe1dd":"df_ml.isnull().melt(value_name=\"missing\")","995ac2a9":"plt.figure(figsize = (10, 5))\n\nsns.displot(\n    data = df_ml.isnull().melt(value_name = \"missing\"),\n    y = \"variable\",\n    hue = \"missing\",\n    multiple = \"fill\",\n    height = 9.25)\n\nplt.axvline(0.3, color = \"r\");","cb95f9af":"df_ml = df_ml.dropna()\n\n# df_ml = df_ml.dropna(subset=['text'], axis=0)\n# df_ml = df_ml.reset_index(drop=True)","ea6c31f9":"missing_values(df_ml)","a4d2d0fb":"df_ml.info()","63e49425":"df_ml[\"text\"].str.isspace().sum()\ndf_ml[df_ml[\"text\"].str.isspace() == True].index","5d33378c":"df_ml.head()","13931e30":"stop_words = stopwords.words('english')","8589f2d4":"def cleaning(data):\n    \n    #1. Tokenize\n    text_tokens = word_tokenize(data.replace(\"'\", \"\").lower()) \n        \n    #2. Remove Puncs\n    tokens_without_punc = [w for w in text_tokens if w.isalpha()]  \n    \n    #3. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #4. lemma\n    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n        \n    #joining\n    return \" \".join(text_cleaned)","dc085bb4":"df_ml[\"text\"] = df_ml[\"text\"].apply(cleaning)\ndf_ml[\"text\"].head()","261a10a2":"\" \".join(df_ml[\"text\"]).split()","c8143605":"rare_words = pd.Series(\" \".join(df_ml[\"text\"]).split()).value_counts()\nrare_words","7e088abf":"rare_words = rare_words[rare_words <= 2] ","d401d3cf":"rare_words.index","b7aedd3b":"df_ml[\"text\"] = df_ml[\"text\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\ndf_ml[\"text\"].head()","4fcc9285":"df_ml[df_ml[\"recommend\"] == 0].head(3)","66d910df":"df_ml[df_ml[\"recommend\"] == 1].head(3)","0844ebf5":"\" \".join(df_ml[\"text\"]).split()","57ae2a84":"positive_words =\" \".join(df_ml[df_ml[\"recommend\"] == 1].text).split()\npositive_words","39c5c000":"negative_words = \" \".join(df_ml[df_ml[\"recommend\"] == 0].text).split()\nnegative_words ","17144cfb":"len(positive_words)","57c2e083":"len(negative_words)","4d1e4a83":"review_text = df_ml[\"text\"]","b4506e7d":"all_words = \" \".join(review_text)","fade0a33":"all_words[:100]","c9eb731d":"wordcloud = WordCloud(width = 800, height = 400, background_color = \"white\", max_words = 250).generate(all_words)\n\nplt.figure(figsize = (13, 13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","b1d22482":"wordcloud = WordCloud(width = 800, height = 400, background_color = \"white\", max_words = 250).generate(str(positive_words))\n\nplt.figure(figsize = (13, 13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","276ccdd9":"wordcloud = WordCloud(width = 800, height = 400, background_color = \"white\", max_words = 250).generate(str(negative_words))\n\nplt.figure(figsize = (13, 13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","62c32423":"counter_all = Counter(word_tokenize(all_words))\ncounter_all.most_common(50)","6f822755":"X = df_ml[\"text\"].values\ny = df_ml[\"recommend\"].map({0:1, 1:0}).values","7acf475c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 101)","f9e76c37":"from sklearn.feature_extraction.text import CountVectorizer","0aa994f2":"vectorizer = CountVectorizer()\n\nX_train_count = vectorizer.fit_transform(X_train)\nX_test_count = vectorizer.transform(X_test)","75c95165":"X_train_count","9cb7a8f0":"X_test_count","20f4a758":"len(X_train_count.toarray())","a65a4c3c":"X_train_count.toarray()","546b43b6":"len(X_test_count.toarray())","6206494b":"X_test_count.toarray()","4187484f":"pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())","052ebafa":"X_train","113410b5":"from sklearn.feature_extraction.text import TfidfVectorizer","f039a6bc":"tf_idf_vectorizer = TfidfVectorizer()\n\nX_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\nX_test_tf_idf = tf_idf_vectorizer.transform(X_test)","a81ee5dc":"X_train_tf_idf.toarray()","1f0dcec8":"pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())","fadd03c4":"from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report, accuracy_score, f1_score, recall_score, precision_score, average_precision_score","39d5e37d":"def eval(model, X_train, X_test):\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    \n    print(confusion_matrix(y_test, y_pred))\n    print(\"Test_Set\")\n    print(classification_report(y_test,y_pred))\n    print(\"Train_Set\")\n    print(classification_report(y_train,y_pred_train))\n    plot_confusion_matrix(model, X_test, y_test, cmap=\"plasma\")","d92018ce":"from sklearn.linear_model import LogisticRegression\n\nlogreg_count = LogisticRegression(C = 0.1, max_iter = 1000, class_weight = 'balanced', random_state = 101)\nlogreg_count.fit(X_train_count,y_train)","07dc57ad":"print(\"LogReg_Count Model\")\nprint (\"------------------\")\neval(logreg_count, X_train_count, X_test_count)","29427baa":"import random\nimport pylab as pl\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc","251942b6":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      LogisticRegression(C = 0.1, max_iter = 1000, class_weight= \"balanced\", random_state=101),\n      classes = logreg_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","97e550fd":"y_pred = logreg_count.predict(X_test_count)\nlog_count_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nlog_count_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nlog_AP = viz.score_","e89c6d76":"print(\"viz.score_       : \", viz.score_)\nprint(\"LogReg_Count_rec : \", log_count_rec)\nprint(\"LogReg_Count_f1  : \", log_count_f1)\nprint(\"LogReg_Count_AP  : \", log_AP)","1bb87305":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\n\ncustom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = LogisticRegression(C = 0.1, max_iter = 1000, class_weight = \"balanced\", random_state = 101)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-0\":\n        log_count_rec = scores\n    elif i == \"f1-0\":\n        log_count_f1 = scores\n    print(f\" {i:20} score for LogReg_Count : {scores}\\n\")","31c06164":"import random\nimport pylab as pl\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc","70348159":"from yellowbrick.classifier import PrecisionRecallCurve\nviz = PrecisionRecallCurve(\n    LogisticRegression(C = 0.1, max_iter = 1000, class_weight = \"balanced\", random_state = 101),\n    classes = logreg_count.classes_,\n    per_class = True,\n    cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","b88f9ad9":"LogReg_Count_AP = viz.score_\nLogReg_Count_AP","743b99df":"logreg_tfidf = LogisticRegression(C = 1, max_iter = 1000, class_weight = \"balanced\", random_state = 101)\nlogreg_tfidf.fit(X_train_tf_idf,y_train)","19d2f338":"print(\"LogReg_TFIDF Model\")\nprint(\"------------------\")\neval(logreg_tfidf, X_train_tf_idf, X_test_tf_idf)","8d70d926":"from yellowbrick.classifier import PrecisionRecallCurve\nviz = PrecisionRecallCurve(\n    LogisticRegression(C = 1, max_iter = 1000, class_weight = \"balanced\", random_state=101),\n    classes = logreg_count.classes_,\n    per_class = True,\n    cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","8eb4d109":"y_pred = logreg_tfidf.predict(X_test_tf_idf)\nlog_tf_idf_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nlog_tf_idf_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nlog_tf_idf_AP = viz.score_","28225eee":"print(\"viz.score_       : \", viz.score_)\nprint(\"LogReg_TFIDF_rec : \", log_tf_idf_rec)\nprint(\"LogReg_TFIDF_f1  : \", log_tf_idf_f1)\nprint(\"LogReg_TFIDF_AP  : \", log_tf_idf_AP)","d1f0d423":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    LogisticRegression(C = 1, max_iter = 1000, random_state = 101, class_weight = \"balanced\")\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        log_tfidf_rec = scores\n    elif i == \"f1-1\":\n        log_tfidf_f1 = scores\n    print(f\" {i:20} score for LogReg_TFIDF : {scores}\\n\")","e8ab66c9":"viz = PrecisionRecallCurve(\n    LogisticRegression(C = 1, max_iter = 1000, random_state = 101, class_weight = \"balanced\"),\n    classes = logreg_tfidf.classes_,\n    per_class = True,\n    cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","291e3e64":"LogReg_TFIDF_AP = viz.score_\nLogReg_TFIDF_AP","2af0eb37":"from sklearn.naive_bayes import MultinomialNB","3121b2b7":"nbmulti_count = MultinomialNB()\nnbmulti_count.fit(X_train_count,y_train)","4d79d6a8":"print(\"NBMulti_Count Model\")\nprint(\"-------------------\")\neval(nbmulti_count, X_train_count, X_test_count)","9966114a":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      MultinomialNB(),\n      classes = nbmulti_count.classes_,  \n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","ce844610":"y_pred = nbmulti_count.predict(X_test_count)\nnb_multi_count_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nnb_multi_count_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nnb_multi_count_AP = viz.score_","d8b0ed76":"print(\"viz.score_         : \", viz.score_)\nprint(\"NBMulti_Count_rec : \", nb_multi_count_rec)\nprint(\"NBMulti_Count_f1  : \", nb_multi_count_f1)\nprint(\"NBMulti_Count_AP  : \", nb_multi_count_AP)","f1dbfd4f":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = MultinomialNB()\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        nbm_count_rec = scores\n    elif i == \"f1-1\":\n        nbm_count_f1 = scores\n    print(f\" {i:20} score for NBMulti_Count : {scores}\\n\")","c005c779":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n    MultinomialNB(),\n    classes = nbmulti_count.classes_,  \n    per_class = True,\n    cmap = \"Set1\"\n)\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","0c1b6ac3":"NBMulti_Count_AP = viz.score_\nNBMulti_Count_AP","f5726b01":"from sklearn.naive_bayes import BernoulliNB","abc33187":"nbberno_count = BernoulliNB()\nnbberno_count.fit(X_train_count,y_train)","f829af4e":"print(\"NBBerno_Count Model\")\nprint(\"-------------------\")\neval(nbberno_count, X_train_count, X_test_count)","03135f5a":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      BernoulliNB(),\n      classes = nbberno_count.classes_,  \n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","393c7933":"y_pred = nbberno_count.predict(X_test_count)\nnb_ber_count_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nnb_ber_count_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nnb_ber_count_AP = viz.score_","72ce4447":"print(\"viz.score_        : \", viz.score_)\nprint(\"NBBerno_Count_rec : \", nb_ber_count_rec)\nprint(\"NBBerno_Count_f1  : \", nb_ber_count_f1)\nprint(\"NBBerno_Count_AP  : \", nb_ber_count_AP)","7c7f9c06":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = BernoulliNB()\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        nbb_count_rec = scores\n    elif i == \"f1-1\":\n        nbb_count_f1 = scores\n    print(f\" {i:20} score for NBBerno_Count : {scores}\\n\")","5f101431":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      BernoulliNB(),\n      classes = nbberno_count.classes_,  \n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","8d210bcb":"NBBerno_Count_AP = viz.score_\nNBBerno_Count_AP","d3cbb43f":"nbmulti_tfidf = MultinomialNB()\nnbmulti_tfidf.fit(X_train_tf_idf,y_train)","a9ed5ed5":"print(\"NBMulti_TFIDF MODEL\")\nprint(\"-------------------\")\neval(nbmulti_tfidf, X_train_tf_idf, X_test_tf_idf)","4999b835":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      MultinomialNB(),\n      classes = nbmulti_tfidf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","2d4a5195":"y_pred = nbmulti_tfidf.predict(X_test_tf_idf)\nnb_multi_tf_idf_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nnb_multi_tf_idf_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nnb_multi_tf_idf_AP = viz.score_","2eb32472":"print(\"viz.score_        : \", viz.score_)\nprint(\"NBMulti_TFIDF_rec : \", nb_multi_tf_idf_rec)\nprint(\"NBMulti_TFIDF_f1  : \", nb_multi_tf_idf_f1)\nprint(\"NBMulti_TFIDF_AP  : \", nb_multi_tf_idf_AP)","d75ff019":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = MultinomialNB()\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        nbm_tfidf_rec = scores\n    elif i == \"f1-1\":\n        nbm_tfidf_f1 = scores\n    print(f\" {i:20} score for NBMulti_TFIDF : {scores}\\n\")","aa185181":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      MultinomialNB(),\n      classes = nbmulti_tfidf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","460e1b25":"NBMulti_TFIDF_AP = viz.score_\nNBMulti_TFIDF_AP","eea7be82":"nbberno_tfidf = BernoulliNB()\nnbberno_tfidf.fit(X_train_tf_idf,y_train)","7068b4f5":"print(\"NBBerno_TFIDF MODEL\")\nprint(\"-------------------\")\neval(nbberno_tfidf, X_train_tf_idf, X_test_tf_idf)","de94c883":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      BernoulliNB(),\n      classes = nbberno_tfidf.classes_, \n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","f46b24bc":"y_pred = nbberno_tfidf.predict(X_test_tf_idf)\nnb_ber_tf_idf_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nnb_ber_tf_idf_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nnb_ber_tf_idf_AP = viz.score_","68c01b7e":"print(\"viz.score_        : \", viz.score_)\nprint(\"NBBerno_TFIDF_rec : \", nb_multi_tf_idf_rec)\nprint(\"NBBerno_TFIDF_f1  : \", nb_multi_tf_idf_f1)\nprint(\"NBBerno_TFIDF_AP  : \", nb_multi_tf_idf_AP)","d67efbe3":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = BernoulliNB()\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        nbb_tfidf_rec = scores\n    elif i == \"f1-1\":\n        nbb_tfidf_f1 = scores\n    print(f\" {i:20} score for NBBerno_TFIDF : {scores}\\n\")","34cf4436":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n      BernoulliNB(),\n      classes = nbberno_tfidf.classes_, \n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","64838e6e":"NBBerno_TFIDF_AP = viz.score_\nNBBerno_TFIDF_AP","1cb71057":"from sklearn.svm import LinearSVC\nsvc_count = LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101)  \nsvc_count.fit(X_train_count,y_train)","4b0f415a":"print(\"SVC_Count Model\")\nprint(\"---------------\")\neval(svc_count, X_train_count, X_test_count)","faa81610":"viz = PrecisionRecallCurve(\n      LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101),\n      classes = svc_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","10d8be22":"y_pred = svc_count.predict(X_test_count)\nsvc_count_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nsvc_count_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nsvc_count_AP = viz.score_","0f5a604c":"print(\"viz.score_    : \", viz.score_)\nprint(\"SVC_Count_rec : \", svc_count_rec)\nprint(\"SVC_Count_f1  : \", svc_count_f1)\nprint(\"SVC_Count_AP  : \", svc_count_AP)","7c491a0a":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        svc_count_rec = scores\n    elif i == \"f1-1\":\n        svc_count_f1 = scores\n    print(f\" {i:20} score for SVC_Count : {scores}\\n\")","efcabb58":"viz = PrecisionRecallCurve(\n      LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101),\n      classes = svc_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","e51d575f":"SVC_Count_AP = viz.score_\nSVC_Count_AP","f9ee21ae":"svc_tf_idf = LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101)  \nsvc_tf_idf.fit(X_train_tf_idf,y_train)","8c8be6dd":"print(\"SVC_TFIDF Model\")\nprint(\"---------------\")\neval(svc_tf_idf, X_train_tf_idf, X_test_tf_idf)","ebe96118":"viz = PrecisionRecallCurve(\n      LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101),\n      classes = svc_tf_idf.classes_,\n      per_class = True,\n      cmap = \"Set1\"\n)\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","df99d313":"y_pred = svc_tf_idf.predict(X_test_tf_idf)\nsvc_tf_idf_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nsvc_tf_idf_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nsvc_tf_idf_AP = viz.score_","2d55abab":"print(\"viz.score_     : \", viz.score_)\nprint(\"SVC_TFIDF_rec   : \", svc_tf_idf_rec)\nprint(\"SVC_TFIDF_f1   : \", svc_tf_idf_f1)\nprint(\"SVC_TFIDF_AP   : \", svc_tf_idf_AP)","2a7a3833":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101)\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        svc_tfidf_rec = scores\n    elif i == \"f1-1\":\n        svc_tfidf_f1 = scores\n    print(f\" {i:20} score for SVC_TFIDF : {scores}\\n\")","400cfc03":"viz = PrecisionRecallCurve(\n      LinearSVC(C = 0.01, class_weight = \"balanced\", random_state = 101),\n      classes = svc_tf_idf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","e8ada8dc":"SVC_TFIDF_AP = viz.score_\nSVC_TFIDF_AP","63ed2175":"from sklearn.ensemble import RandomForestClassifier\n\nrf_count = RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1)\nrf_count.fit(X_train_count, y_train)","56c6ad65":"print(\"RF_Count Model\")\nprint(\"--------------\")\neval(rf_count, X_train_count, X_test_count)","0592546e":"viz = PrecisionRecallCurve(\n      RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1),\n      classes = rf_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","74f1e14f":"y_pred = rf_count.predict(X_test_count)\nrf_count_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nrf_count_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nrf_count_AP = viz.score_","63a02acc":"print(\"viz.score_   : \", viz.score_)\nprint(\"RF_Count_rec : \", rf_count_rec)\nprint(\"RF_Count_f1  : \", rf_count_f1)\nprint(\"RF_Count_AP  : \", rf_count_AP)","495e12d6":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        rf_count_rec = scores\n    elif i == \"f1-1\":\n        rf_count_f1 = scores\n    print(f\" {i:20} score for RF_Count : {scores}\\n\")","8e2dfc42":"viz = PrecisionRecallCurve(\n      RandomForestClassifier(200, max_depth = 12, random_state = 42, n_jobs = -1, class_weight=\"balanced\"),\n      classes = rf_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","3e0586ad":"RF_Count_AP = viz.score_\nRF_Count_AP","3c7c45a5":"rf_tf_idf = RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1)\nrf_tf_idf.fit(X_train_tf_idf, y_train)","ae83aa02":"print(\"RF_TFIDF Model\")\nprint(\"--------------\")\neval(rf_tf_idf, X_train_tf_idf, X_test_tf_idf)","3ad36ee4":"viz = PrecisionRecallCurve(\n      RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1),\n      classes = rf_tf_idf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","e74327e9":"y_pred = rf_tf_idf.predict(X_test_tf_idf)\nrf_tf_idf_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nrf_tf_idf_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nrf_tf_idf_AP = viz.score_","9d99a8ea":"print(\"viz.score_   : \", viz.score_)\nprint(\"RF_TFIDF_rec : \", rf_tf_idf_rec)\nprint(\"RF_TFIDF_f1  : \", rf_tf_idf_f1)\nprint(\"RF_TFIDF_AP  : \", rf_tf_idf_AP)","ea127f94":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1)\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        rf_tfidf_rec = scores\n    elif i == \"f1-1\":\n        rf_tfidf_f1 = scores\n    print(f\" {i:20} score for RF_TFIDF : {scores}\\n\")","c3507ba4":"viz = PrecisionRecallCurve(\n      RandomForestClassifier(n_estimators = 200, max_depth = 11, class_weight = \"balanced\", random_state = 101, n_jobs = -1),\n      classes = rf_tf_idf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","ee28f5fd":"RF_TFIDF_AP = viz.score_\nRF_TFIDF_AP","a1bc0fda":"from sklearn.ensemble import AdaBoostClassifier\n\nada_count = AdaBoostClassifier(n_estimators = 500, random_state = 101)\nada_count.fit(X_train_count, y_train)","dd10ad31":"print(\"Ada_Count Model\")\nprint(\"---------------\")\neval(ada_count, X_train_count, X_test_count)","83a06130":"viz = PrecisionRecallCurve(\n      AdaBoostClassifier(n_estimators = 500, random_state = 101),\n      classes = ada_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","889cf96b":"y_pred = ada_count.predict(X_test_count)\nada_count_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nada_count_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nada_count_AP = viz.score_","d5698384":"print(\"viz.score_    : \", viz.score_)\nprint(\"Ada_Count_rec : \", ada_count_rec)\nprint(\"Ada_Count_f1  : \", ada_count_f1)\nprint(\"Ada_Count_AP  : \", ada_count_AP)","10f99f33":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = AdaBoostClassifier(n_estimators = 500, random_state = 101)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        ada_count_rec = scores\n    elif i == \"f1-1\":\n        ada_count_f1 = scores\n    print(f\" {i:20} score for Ada_Count : {scores}\\n\")","4e1ed213":"viz = PrecisionRecallCurve(\n      AdaBoostClassifier(n_estimators = 500, random_state = 101),\n      classes = ada_count.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","c321d710":"Ada_Count_AP = viz.score_\nAda_Count_AP","8f5e7e6c":"ada_tf_idf = AdaBoostClassifier(n_estimators = 500, random_state = 101)\nada_tf_idf.fit(X_train_tf_idf, y_train)","dfcdf060":"print(\"Ada_TFIDF Model\")\nprint(\"---------------\")\neval(ada_tf_idf, X_train_tf_idf, X_test_tf_idf)","28ff9516":"viz = PrecisionRecallCurve(\n      AdaBoostClassifier(n_estimators = 500, random_state = 101),\n      classes = ada_tf_idf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","182f7314":"y_pred = ada_tf_idf.predict(X_test_tf_idf)\nada_tf_idf_rec = recall_score(y_test, y_pred, pos_label = 0, average = None)\nada_tf_idf_f1 = f1_score(y_test, y_pred, pos_label = 0, average = None)\nada_tf_idf_AP = viz.score_","e23292ed":"print(\"viz.score_    : \", viz.score_)\nprint(\"Ada_TFIDF_rec : \", ada_tf_idf_rec)\nprint(\"Ada_TFIDF_f1  : \", ada_tf_idf_f1)\nprint(\"Ada_TFIDF_AP  : \", ada_tf_idf_AP)","84f2ed6c":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label = 0),\n                 'recall-0': make_scorer(recall_score, pos_label = 0),\n                 'f1-0': make_scorer(f1_score, pos_label = 0),\n                 'precision-1': make_scorer(precision_score, pos_label = 1),\n                 'recall-1': make_scorer(recall_score, pos_label = 1),\n                 'f1-1': make_scorer(f1_score, pos_label = 1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = AdaBoostClassifier(n_estimators = 500, random_state = 101)\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        ada_tfidf_rec = scores\n    elif i == \"f1-1\":\n        ada_tfidf_f1 = scores\n    print(f\" {i:20} score for Ada_TFIDF : {scores}\\n\")","ba486fba":"viz = PrecisionRecallCurve(\n      AdaBoostClassifier(n_estimators = 500, random_state = 101),\n      classes = ada_tf_idf.classes_,\n      per_class = True,\n      cmap = \"Set1\")\n\nfig, ax = plt.subplots(figsize = (10, 6))\nax.set_facecolor('#eafff5')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","2b1cc307":"Ada_TFIDF_AP = viz.score_\nAda_TFIDF_AP","b26d3f01":"# df_dl = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n# df_dl.head()","4cd5be16":"df_dl = pd.read_csv('..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')","ff083997":"df_dl = df_dl[[\"Review Text\",\"Recommended IND\"]]\ndf_dl.head()","c877b91d":"df_dl.rename(columns = {'Review Text':'text', 'Recommended IND':'recommend'}, inplace = True)","3f614809":"df_dl.shape","1176580a":"df_dl.isnull().sum()","44454c32":"df_dl = df_dl.dropna(subset = ['text'], axis = 0)\ndf_dl = df_dl.reset_index(drop = True)","6ecaa752":"df_dl.isnull().sum()","6c6773e3":"df_dl.shape","d71e4270":"df_dl.head()","447b04bc":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU, Embedding, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping","74c191dc":"X = df_dl['text'].values\ny = df_dl['recommend'].map({0:1, 1:0}).values","91d60408":"num_words = 10000\ntokenizer = Tokenizer(num_words = num_words)","e85a240d":"tokenizer.fit_on_texts(X)","446761f3":"tokenizer.word_index","40fd84ba":"len(tokenizer.word_index)","92311668":"len(tokenizer.word_index.keys())","e85d4454":"X_num_tokens = tokenizer.texts_to_sequences(X)","3fda54d6":"num_tokens = [len(tokens) for tokens in X_num_tokens]\nnum_tokens = np.array(num_tokens)","a0937614":"X_num_tokens","cef56a5e":"num_tokens.max()","62c181cc":"num_tokens.mean()","fb3cafaf":"num_tokens.argmax()","50518522":"X[16263]","f0c34eb6":"num_tokens.argmin()","9908b6ca":"X[820]","552a036d":"max_tokens = 103","a3771f9d":"sum(num_tokens < max_tokens) \/ len(num_tokens)","e3262f08":"X_pad = pad_sequences(X_num_tokens, maxlen = max_tokens)","dec3c139":"X_pad.shape","307c75e8":"from sklearn.model_selection import train_test_split\nfrom keras.layers import Bidirectional","40d13ad7":"X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size = 0.2, stratify = y, random_state = 101)","9d8fb833":"model = Sequential()","dd7ba638":"embedding_size = 100","2481caa0":"model.add(Embedding(input_dim = num_words,\n                    output_dim = embedding_size,\n                    input_length = max_tokens,\n                    name = 'embedding_layer'))","210b5c74":"model.add(Bidirectional(GRU(units = 48, return_sequences = True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(GRU(units = 24, return_sequences = True)))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(GRU(units = 12)))\nmodel.add(Dense(1, activation = 'sigmoid'))","c48adada":"optimizer = Adam(lr = 0.004)","6077f46a":"model.compile(loss = 'binary_crossentropy',\n              optimizer = optimizer,\n              metrics = [\"Recall\"])","adbf037c":"model.summary() ","a042e956":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", \n                           verbose = 1, patience = 5, restore_best_weights = True)","5410c40d":"pd.Series(y_train).value_counts(normalize = True) ","aade2c43":"weights = {0:19, 1:81}","f0240017":"model.fit(X_train, y_train, epochs = 25, batch_size = 256, class_weight = weights,\n         validation_data = (X_test, y_test), callbacks = [early_stop])","2854c5a6":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","26348f15":"model_loss.plot()","d938325f":"model.evaluate(X_train, y_train)","a6428910":"model.evaluate(X_test, y_test) ","ae2f273f":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_auc_score\n\ny_train_pred = (model.predict(X_train) >= 0.5).astype(\"int32\")\n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"---------------------------------------------\")\nprint(classification_report(y_train, y_train_pred))","bf7c1c57":"y_pred = (model.predict(X_test) >= 0.5).astype(\"int32\")\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"--------------------------------------\")\nprint(classification_report(y_test, y_pred))","bfe47fa4":"from sklearn.metrics import precision_recall_curve, average_precision_score","ec86ab4c":"y_pred_proba = model.predict(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(precision, recall)\nplt.xlabel('precision')\nplt.ylabel('recall')\nplt.title('Precision Recall Curve')\nplt.show()","5163f51a":"DL_AP = average_precision_score(y_test, y_pred_proba)\nDL_f1 = f1_score(y_test, y_pred)\nDL_rec = recall_score(y_test, y_pred)","5afea125":"print(\"DL_AP   : \", DL_AP)\nprint(\"DL_f1   : \", DL_f1)\nprint(\"DL_rec  : \", DL_rec)","a80b5d1f":"compare = pd.DataFrame({\"Model\": [\"NaiveBayes(Multi)_Count\", \"NaiveBayes(Berno)_Count\", \"LogReg_Count\", \"SVM_Count\", \n                                  \"Random Forest_Count\", \"AdaBoost_Count\", \"NaiveBayes(Multi)_TFIDF\", \n                                  \"NaiveBayes(Berno)_TFIDF\", \"LogReg_TFIDF\", \"SVM_TFIDF\", \"Random Forest_TFIDF\", \n                                  \"AdaBoost_TFIDF\", \"DL\"],\n                        \n                        \"F1_Score\": [nbm_count_f1, nbb_count_f1, log_count_f1, svc_count_f1, rf_count_f1, ada_count_f1,\n                                    nbm_tfidf_f1, nbb_tfidf_f1, log_tfidf_f1, svc_tfidf_f1, rf_tfidf_f1, ada_tfidf_f1, DL_f1],\n                                                 \n                        \"Recall_Score\": [nbm_count_rec, nbb_count_rec, log_count_rec, svc_count_rec, rf_count_rec, \n                                         ada_count_rec, nbm_tfidf_rec, nbb_tfidf_rec, log_tfidf_rec, svc_tfidf_rec, \n                                         rf_tfidf_rec, ada_tfidf_rec, DL_rec],\n                        \n                        \"Average_Precision_Score\": [NBMulti_Count_AP, NBBerno_Count_AP, LogReg_Count_AP, SVC_Count_AP, \n                                                    RF_Count_AP, Ada_Count_AP, NBMulti_TFIDF_AP, NBBerno_TFIDF_AP,\n                                                    LogReg_TFIDF_AP, SVC_TFIDF_AP, RF_TFIDF_AP, Ada_TFIDF_AP, DL_AP]})\n  \n    \ncompare = compare.sort_values(by=\"Recall_Score\", ascending=True)\nfig = px.bar(compare, x = \"Recall_Score\", y = \"Model\", title = \"Recall_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"F1_Score\", ascending=True)\nfig = px.bar(compare, x = \"F1_Score\", y = \"Model\", title = \"F1_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Average_Precision_Score\", ascending=True)\nfig = px.bar(compare, x = \"Average_Precision_Score\", y = \"Model\", title = \"Average_Precision_Score\")\nfig.show()","36469baa":"review1 = \"I ordered this dress in 0p since i am 5ft. it fits great its not too short just above the knee.\"\nreview2 = \"Love the top design, not so much the fabric. i should've read the top was 100% polyester. returning and finding a better quality top.\"\nreview3 = \"Love this henely, the lace-up is cute. the shirt is comfy. so versatile. beach coverup or over jeans\"\nreview4 = \"Loved this dress online, but my small pettie stature made me looks like an orphan from the prairie.\"\nreview5 = \"I tried it in the store but was not the true size so i ordered online. i wore it only once so far and got lots of compliments. very unique design.\"\nreview6 = \"Nice shirt seems well made. good just not a great fit for me.\"\nreview7 = \"I tried on this dress in store and was amazed by the quality and simple structure of the dress. bought it with no hesitation.\"\nreview8 = \"This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.\"\nreview9 = \"Great quality top. i do wish it fit me...i'm really long waisted and this top was too short. had to return\"\nreview10 = \"A serious joke. i struggled with the buttons for a good 10 minutes and gave up after the 3rd button. i'm not sure what they were thinking.\"\n\nreviews = [review1, review2, review3, review4, review5, review6, review7, review8, review9, review10]","60d1c92b":"tokens = tokenizer.texts_to_sequences(reviews) ","6f13c0d0":"tokens_pad = pad_sequences(tokens, maxlen = max_tokens)\ntokens_pad.shape","75ef21f8":"mod_pred = model.predict(tokens_pad)","bd12831b":"mod_pred","b5d6fe19":"df_pred = pd.DataFrame(mod_pred, index = reviews)\ndf_pred.rename(columns = {0:'Pred_Proba'}, inplace = True)","b54d1b50":"df_pred[\"Predicted_Feedbaack\"] = df_pred[\"Pred_Proba\"].apply(lambda x: \"Not Recommended\" if x >= 0.5 else \"Recommended\")","91f5bdaa":"df_pred","1e188a9a":"- In this project we have used sentiment analysis to determine whether the product is recommended or not. We have built models with five different machine learning algorithms and also with deep learning algorithm and compare their performance. Thus, we have determined the algorithm that makes the most accurate emotion estimation by using the information obtained from the * Review Text * variable.\n\n- When the scores are examined in the Compare section, it is seen that the scores are generally close to each other, but the Logistic Regression model stands out. \n\n- Thank you in advance for your constructive and instructive comments.","16c5234d":"### Create Word Cloud (for most common words in recommended not recommended reviews separately)","55c8eb28":"## Modelling with Deep Learning","f97dc54b":"### Collect Words (positive and negative separately)","fad6e5b2":"### Modeling","630f9801":"***recommended*** \n- Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.","5093769a":"### Countvectorize ***BernoulliNB with Cross Validation***","ab08e0c2":"## Modelling with Machine Learning Models","76039b22":"### Feature Selection and Rename Column Name","07f3bef3":"To run machine learning algorithms we need to convert text files into numerical feature vectors. We will use bag of words model for our analysis.\n\nFirst we spliting the data into train and test sets:","4e988b92":"# Sentiment analysis of women's clothes reviews\n\n\nIn this study we used sentiment analysis to determined whether the product is recommended or not. We used different machine learning algorithms to get more accurate predictions. The following classification algorithms have been used: Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Random Forest and Ada Boosting. The dataset comes from Woman Clothing Review that can be find at (https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews. \n","e9dc45d4":"### Tokenization","d059a3c7":"### TF-IDF","e7ca78f2":"## Logistic Regression","19370c65":"### Some Useful User Defined Functions","f6ece58d":"### TFIDF ***With Cross Validation***","5470557b":"### TF-IDF ***BernoulliNB***","a0c02f17":"### TF-IDF","1d3deddf":"### Counting words","2738bae4":"### TF-IDF ***Cross Validation***","f4cd3e5f":"### Rare Words","fc9ab264":"---\n---\n","e3d37141":"### Missing Value Detection","9745c283":"### Countvectorize ***MultinomialNB with Cross Validation***","2f92a3d0":"### Tokenization, Noise Removal, Lexicon Normalization","5b9d5aae":"In the next step we create a numerical feature vector for each document:","5413de75":"### TF-IDF","dd38555d":"***feedback_count:***\n- Positive Integer documenting the number of other customers who found this review positive.","e6801d3e":"### Data Wrangling","36894ea3":"### CountVectorizer ***With Cross Validation***","92b8da6d":"## 3. Text Mining\n\nText is the most unstructured form of all the available data, therefore various types of noise are present in it. This means that the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as **text preprocessing**.\n\nThe three key steps of text preprocessing:\n\n- **Tokenization:**\nThis step is one of the top priorities when it comes to working on text mining. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n\n\n- **Noise Removal:**\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, upper and lower case differentiation, punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\n\n- **Lexicon Normalization:**\nAnother type of textual noise is about the multiple representations exhibited by single word.\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d. Though they mean different things, contextually they all are similar. This step converts all the disparities of a word into their normalized form (also known as lemma). \nThere are two methods of lexicon normalisation; **[Stemming or Lemmatization](https:\/\/www.guru99.com\/stemming-lemmatization-python-nltk.html)**. Lemmatization is recommended for this case, because Lemmatization as this will return the root form of each word (rather than just stripping suffixes, which is stemming).\n\nAs the first step change text to tokens and convertion all of the words to lower case.  Next remove punctuation, bad characters, numbers and stop words. The second step is aimed to normalization them throught the Lemmatization method. ","87659a96":"***class***\n- Categorical name of the product class name.","d78ae6f1":"### Train - Test Split","8317aa7e":"***department***\n- Categorical name of the product department name","8db7e3a1":"## Prediction","baa25b80":"### TF-IDF with Cross Validation","c8df5bdf":"### TF-IDF ***BernoulliNB with Cross Validation***","22b3b2b6":"### CountVectorizer with Cross Validation","618cc613":"### Countvectorize ***BernoulliNB***","91eaec54":"The target class variable is imbalanced, where \"Recommended\" values are more dominating then \"Not Recommended\".","a5c8be06":"### Compare Models F1 Scores, Recall Scores and Average Precision Score","c44dd6ab":"***rating*** \n- Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.","65b95fa9":"## 2. Feature Selection and Data Cleaning\n\nFrom now on, the DataFrame we will work with should contain two columns: **\"Review Text\"** and **\"Recommended IND\"**. We can do the missing value detection operations from now on. We can also rename the column names if we want.","061cf005":"- Let us have look at the columns remaining in the dataset.","463a7300":"## Random Forest\n\n### CountVectorizer","286e75ad":"### CountVectorizer  ***Cross Validation***","0de2ffb6":"### CountVectorizer ","c57638b8":"## 1. Exploratory Data Analysis","f7b2c32b":"## Ada Boosting\n\n### CountVectorizer","ff31bff0":"***division*** \n- Categorical name of the product high level division.","0751c1cb":"### Detect Reviews (positive and negative separately)","42f14020":"## Determines\n\nThe data is a collection of 22641 Rows and 10 column variables. Each row includes a written comment as well as additional customer information. \nAlso each row corresponds to a customer review, and includes the variables:\n\n\n**Feature Information:**\n\n**Clothing ID:** Integer Categorical variable that refers to the specific piece being reviewed.\n\n**Age:** Positive Integer variable of the reviewers age.\n\n**Title:** String variable for the title of the review.\n\n**Review Text:** String variable for the review body.\n\n**Rating:** Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n\n**Recommended IND:** Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n\n**Positive Feedback Count:** Positive Integer documenting the number of other customers who found this review positive.\n\n**Division Name:** Categorical name of the product high level division.\n\n**Department Name:** Categorical name of the product department name.\n\n**Class Name:** Categorical name of the product class name.\n\n---\n\nThe basic goal in this project is to predict whether customers recommend the product they purchased using the information in their *Review Text*.\nEspecially, it should be noted that the expectation in this project is to use only the \"Review Text\" variable and neglect the other ones. \nOf course, if we want, we can work on other variables individually.\n\nProject Structure is separated in five tasks: ***EDA, Feature Selection and Data Cleaning , Text Mining, Word Cloud*** and ***Sentiment Classification with Machine Learning***.\n\nClassically, we can start to know the data after doing the import and load operations. \nWe need to do missing value detection for Review Text, which is the only variable we need to care about. We can drop other variables.\n\nWe will need to apply ***noise removal*** and ***lexicon normalization*** processes by using the capabilities of the ***nltk*** library to the data set that is ready for text mining.\n\nAfterwards, we will implement ***Word Cloud*** as a visual analysis of word repetition.\n\nFinally, we will build models with five different algorithms and compare their performance. Thus, you will determine the algorithm that makes the most accurate emotion estimation by using the information obtained from the * Review Text * variable.\n\n\n\n\n","fa2780c1":"***title*** \n- String variable for the title of the review.","84e10697":"### Model evaluation","dd1c466e":"## 5. Sentiment Classification with Machine Learning and Deep Learning\n\nBefore moving on to modeling, as data preprocessing steps we will need to perform **[vectorization](https:\/\/machinelearningmastery.com\/prepare-text-data-machine-learning-scikit-learn\/)** and **train-test split**. \n\nMachine learning algorithms most often take numeric feature vectors as input. Thus, when working with text documents, we need a way to convert each document into a numeric vector. This process is known as text vectorization. Commonly used vectorization approach that we will use here is to represent each text as a vector of word counts.\n\nAt this moment, we have our review text column as a token (which has no punctuations and stopwords). We can use Scikit-learn\u2019s CountVectorizer to convert the text collection into a matrix of token counts. We can imagine this resulting matrix as a 2-D matrix, where each row is a unique word, and each column is a review.\n\nAfter performing data preprocessing, we will build our models using following classification algorithms:\n\n- Logistic Regression,\n- Naive Bayes,\n- Support Vector Machine,\n- Random Forest,\n- Ada Boosting\n- Deep Learning Model.","2c81f147":"## Support Vector Machine (SVM)\n\n### Countvectorizer","fd6733df":"Welcome to the \"***Sentiment Analysis and Classification***\" study.\n\nThis analysis will focus on using Natural Language techniques to find broad trends in the written thoughts of the customers. \nThe goal is to predict whether customers recommend the product they purchased using the information in their review text.\n\nOne of the challenges in this study is to extract useful information from the *Review Text* variable using text mining techniques. The other challenge is that we need to convert text files into numeric feature vectors to run machine learning algorithms.\n\nWe will build sentiment classification models using Machine Learning algorithms (***Logistic Regression, Naive Bayes, Support Vector Machine, Random Forest*** and ***Ada Boosting***) and **Deep Learning algorithms**.\n\nBefore diving into the project, let's take a look at the Determines and Tasks.","d894661e":"- Columns with ordinal information, although the \"rating\" and \"recommended_ind\" columns are encoded as numeric. ","ae1e1e42":"### TF-IDF ***MultinomialNB with Cross Validation***","a04d5b27":"***review_text*** \n- String variable for the review body.","5b1512fb":"---\n---","81b54e40":"---\n---","5abad776":"#### Check Proportion of Target Class Variable:","f8af50ab":"### Converting tokens to numeric","86424f4b":"- The \"Unnamed:_0\" column contains completely unique values and contains the same information as the index. Alsa \"clothing_id\" columns has unique values over 1200. I'm dropping this columns because they won't work for us as they stand.","565c1c9f":"## Tasks\n\n#### 1. Exploratory Data Analysis\n\n- Import Modules, Load Discover the Data\n\n#### 2. Feature Selection and Data Cleaning\n\n- Feature Selection and Rename Column Name\n- Missing Value Detection\n\n#### 3. Text Mining\n\n- Tokenization\n- Noise Removal\n- Lexicon Normalization\n\n#### 4. WordCloud - Repetition of Words\n\n- Detect Reviews\n- Collect Words \n- Create Word Cloud \n\n\n#### 5. Sentiment Classification with Machine Learning\n\n- Train - Test Split\n- Vectorization\n- TF-IDF\n- Logistic Regression\n- Naive Bayes\n- Support Vector Machine\n- Random Forest\n- AdaBoost\n- Model Comparison","919261d8":"### TF-IDF ***MultinomialNB***","20c94a6c":"___\n\n# WELCOME!\n\n___","e744a268":"### Conclusion","94515e34":"***age***\n- Positive Integer variable of the reviewers age.","1897f1c2":"### CountVectorizer with Cross Validation","3d5d557f":"---\n---\n","5e681aa1":"### Train Set Split","85eaa247":"## 4. WordCloud - Repetition of Words\n\nNow we'll create a Word Clouds for reviews, representing most common words in each target class.\n\nWord Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud.\n\nWe will create separate word clouds for positive and negative reviews. We can qualify a review as positive or negative, by looking at its recommended status.\n\nWe can follow the steps below:\n\n- Detect Reviews\n- Collect Words \n- Create Word Cloud \n","a8f3440c":"### TD-IDF","383c9f5f":"### Maximum number of tokens for all documents\u00b6","c7ec68bf":"### Import Libraries, Load and Discover the Data","bb9922ac":"### Fixing token counts of all documents (pad_sequences)","09a4fec3":"## Naive Bayes \n\n### Countvectorizer ***MultinomialNB***","900b795c":"### Count Vectorization","c3e6530c":"### TF-IDF with Cross Validation","aed289b5":"### Creating word index","c4e57a0d":"### TF-IDF "}}