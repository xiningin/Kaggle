{"cell_type":{"361770a4":"code","7d03272d":"code","152e295b":"code","77cd82cd":"code","71c8b1b1":"code","91000270":"code","0ebe55ac":"code","21e8e513":"code","d3f7d29a":"code","042894b9":"code","e5bcbe5c":"code","c888ea2b":"code","d084a889":"code","0b7e9ed0":"code","d1971c28":"code","228f124f":"code","b7b45f40":"code","c3e3be9d":"code","25e62167":"code","b8426b84":"code","32357d62":"code","843bdac0":"code","a5c01afe":"code","baa7fbfc":"code","b6b8592a":"code","a8cf3152":"code","bb066e07":"markdown","bd6b5bc1":"markdown","6dab389f":"markdown","4d99f6c6":"markdown","3a4d4673":"markdown","a600bea0":"markdown","9d9111c3":"markdown","25b15f6c":"markdown","a2862c00":"markdown","fd647f40":"markdown","dbe85f28":"markdown","898a3b8b":"markdown","c557e60a":"markdown"},"source":{"361770a4":"import pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7d03272d":"def understanding(df):\n    print(df.info())\n    print(df.describe())\n    for col in df.columns:\n        if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n            plt.hist(df[col])\n            plt.title(col)\n            plt.show()\n            plt.clf()\n        elif df[col].dtype == 'object':\n            sns.barplot(df[col].value_counts().index,df[col].value_counts()).set_title(col)\n            plt.show()\n            \ndef heat(df):\n    num_columns = []\n    for col in df.columns:\n        if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n            num_columns.append(col)\n    sns.heatmap(df[num_columns].corr(), annot=True)","152e295b":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","77cd82cd":"\n#understanding(training)","71c8b1b1":"print(training.Name.value_counts())\ntraining['Sur'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\ncommon_sur = ['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev']\ntraining.Sur = training.Sur.apply(lambda x: 'Other' if x not in common_sur else x)\ntraining.Sur.value_counts()","91000270":"test['Sur'] = test.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntest.Sur = test.Sur.apply(lambda x: 'Other' if x not in common_sur else x)","0ebe55ac":"training.Sex = training.Sex.apply(lambda x: 0 if x == 'male' else 1)\ntraining.Sex.value_counts()","21e8e513":"test.Sex = test.Sex.apply(lambda x: 0 if x == 'male' else 1)","d3f7d29a":"print(training.Age.value_counts())\nmedian = round((training.Age.median()), 2)\nmedian","042894b9":"training.SibSp.value_counts()","e5bcbe5c":"training.Parch.value_counts()","c888ea2b":"print(training.Ticket.value_counts())\ntraining['Ticket_grouped'] = training.Ticket.apply(lambda x: x.replace('.', '').replace('\/', '').split(' ')[0].lower() if len(x.replace('.', '').replace('\/', '').split(' ')) > 1 else '0')\ntraining.Ticket_grouped = training.Ticket_grouped.apply(lambda x: 0 if x == '0' else 1)\nprint(training.Ticket_grouped.value_counts())","d084a889":"test['Ticket_grouped'] = test.Ticket.apply(lambda x: x.replace('.', '').replace('\/', '').split(' ')[0].lower() if len(x.replace('.', '').replace('\/', '').split(' ')) > 1 else '0')\ntest.Ticket_grouped = test.Ticket_grouped.apply(lambda x: 0 if x == '0' else 1)","0b7e9ed0":"training.Fare.value_counts()","d1971c28":"training.Embarked.value_counts()","228f124f":"heat(training)","b7b45f40":"X_train = training.copy()\nX_test = test.copy()\n\ncols_to_drop = ['Cabin', 'Ticket', 'Name']\n\nX_test.drop(cols_to_drop, axis=1, inplace=True)\nX_train.drop(cols_to_drop, axis=1, inplace=True)","c3e3be9d":"X_train.dropna(subset=['Embarked'], inplace=True)\nX_test.dropna(subset=['Embarked'], inplace=True)\n\nX_train.Age = X_train.Age.fillna(median)\nX_test.Age = X_test.Age.fillna(median)\nX_test.Fare = X_test.Fare.fillna(X_test.Fare.median())\n","25e62167":"X_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n\n\ny_train = X_train.Survived\nX_train.drop(['Survived'], axis=1, inplace=True)\nX_test.drop(['Survived'], axis=1, inplace=True)\n","b8426b84":"print(X_train.isna().any())\n","32357d62":"X_test.isna().any()","843bdac0":"xgb = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state=0)\ncv = cross_val_score(xgb,X_train,y_train,cv=5)\nprint(cv.mean())","a5c01afe":"rf = RandomForestClassifier(random_state=0)\n\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv.mean())\n\nrf.fit(X_train, y_train)","baa7fbfc":"rf = RandomForestClassifier(random_state=0, n_estimators=90, max_depth=6)\n\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv.mean())\n\nrf.fit(X_train, y_train)","b6b8592a":"#from sklearn.model_selection import GridSearchCV\n#\n#\n#params = { 'max_depth': [3,6,10],\n#           'learning_rate': [0.01, 0.05, 0.1],\n#           'n_estimators': [100, 500, 1000],\n#           'colsample_bytree': [0.3, 0.7]}\n#xgbr = XGBClassifier(use_label_encoder=False, eval_metric='error', seed = 20)\n#clf = GridSearchCV(estimator=xgbr, \n#                    param_grid=params,\n#                    scoring='neg_mean_squared_error', \n#                    verbose=1)\n#clf.fit(X_train, y_train)\n#print(\"Best parameters:\", clf.best_params_)","a8cf3152":"xgb_final = XGBClassifier(use_label_encoder=False, eval_metric='error', colsample_bytree = 0.7, learning_rate = 0.01, max_depth = 6, n_estimators = 100)\nxgb_final.fit(X_train, y_train)\ncv = cross_val_score(xgb_final, X_train, y_train, cv=5)\nprint(cv.mean())\n\npred = xgb_final.predict(X_test).astype(int)\n\nfinal_data = {'PassengerId': test.PassengerId, 'Survived': pred}\nsubmission = pd.DataFrame(data=final_data)\n\nsubmission.to_csv('submission.csv', index=False)","bb066e07":"## Sex\n\nHere I simply changed the sex column into binary.","bd6b5bc1":"# Feature Engineering\n\n## Name\n\nHere we see that each passenger has a unique name. In order to still be able to use this information I reduced the cardinality by creating a `Sur` column to identify common sur names. I then kept the most common values and placed the others as `other`","6dab389f":"## Train Test Split\n\nHere I created the train test split to get the data ready for model building. \n- I dropped the columns that were replaced in the above section. \n- I also dropped `Cabin` since it was mostly null values.\n- Null values were replaced with the column median\n- Categorical columns were then one hot encoded","4d99f6c6":"# Results\n\nWhile my model building was pretty basic, when I submitted it I placed in the **top 10%** so I am satisfied with that. There are definitely areas I can improve upon.\n\n## Areas for Growth:\n\n- Perform normalization on the numerical data to open the kinds of models I can use\n- Do more intensive tuning and work on a system for doing so\n- Do the markdown as I work through the data instead of having to go back through after the model is complete","3a4d4673":"# EDA\n\nBelow I used a function from the python file to do some basic EDA. It prints `training.info()`, `training.describe()` and graphs histograms for numerical columns and bar graphs for categorical columns. It shows us the following about the data:\n\n- there are null values in 3 columns\n\n- `Cabin` is mostly null values\n\n- `Name` has a very high cardinality\n\n- The numerical columns are skewed right\n\n## Observations\n\n- There are categorical columns with very high cardinality\n\n- Much of the numerical data is skewed right\n\n- There are null values present","a600bea0":"# Basic Model Testing\n\nSince I know that the numerical columns are skewed, I chose models that deal well with non normalized data. I do not have as much experience with normalization, and since this is my first solo project, I wanted to keep my scope simple and tested with models that aren't effected by this.<br><br>\nFirst I used the basic models to test which one performed better and found that the **Random Forest Classifier** won out.","9d9111c3":"## Age\n\nThere are null values in the `Age` column so I created a median variable to fill those null values. I used median instead of mean since the data is skewed.","25b15f6c":"## Correlation\n\nLastly, I checked for high correlation between columns, none were were high enough to remove.","a2862c00":"# Model Tuning\n\nOnce I saw that the Random Forest Classifier did best, I tuned the number of estimators and max depth and kept those that produced the highest score. ","fd647f40":"## Ticket\n\nThis column also had unique values for each passenger. Here there simplest way to separate them was to identify if the ticket had only numerical values or letters too. \n\n- I created a `Ticket_grouped` column that removed punctuation. \n\n- Then the value was split and the lamda checked if it had more than one section and marked it zero if it was just numbers.\n\n- Lastly, I replaced the array values with `1` to indicate there were letters in the ticket number making the column binary","dbe85f28":"# Import Methods\n\nHere I imported the libraries I would need for my notebook.","898a3b8b":"# Titanic ML Project\n\nHello everyone! After learning Data Science for almost a year now I finally have done my first ML solo project. I tried to limit my scope since this is my first project, but even so I was able to score in the **top 10%**. Please take a look and feel free to leave a comment or suggestion on how to improve the code.\n\n## Overview:\n\n1) Import Methods\n\n2) Create DataFrames\n\n3) EDA\n\n4) Feature Engineering\n\n5) Basic Model Testing\n\n6) Model Tuning\n\n7) Results\n\n## Thanks to:\n\n- 30 Days of Kaggle for helping me get my ML started\n\n- Ken Jee for hi helpful walkthrough and youtube channel","c557e60a":"# Create DataFrames"}}