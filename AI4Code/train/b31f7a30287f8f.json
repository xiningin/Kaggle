{"cell_type":{"4990964c":"code","44226187":"code","0308517d":"code","10ebb747":"code","5f22f276":"code","598d86c0":"code","2def6a24":"code","87a8fb0c":"code","1d75cefa":"code","f8f15593":"code","36cfd30f":"code","7d1d01c2":"code","2df93b86":"code","c3bbf50c":"code","886a73e4":"code","a1976d5e":"code","3103e278":"code","fe52cc65":"code","707b8167":"code","978a2ea3":"code","f849452c":"code","abb2af32":"code","67705d06":"markdown","f88ff760":"markdown","f40f987a":"markdown","14b7216c":"markdown","9b3f5685":"markdown","2d8a1b4c":"markdown"},"source":{"4990964c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44226187":"#read data\nion_data = pd.read_csv('\/kaggle\/input\/ionosphere\/ionosphere.txt')","0308517d":"#shape of data\nion_data.shape","10ebb747":"ion_data.info()","5f22f276":"ion_data.describe()","598d86c0":"ion_data.head()","2def6a24":"#Assign column names\nion_data.columns = ['feature1','feature2','feature3','feature4','feature5','feature6','feature7'\n                   ,'feature8','feature9','feature10','feature11','feature12','feature13','feature14'\n                   ,'feature15','feature16','feature17','feature18','feature19','feature20','feature21'\n                   ,'feature22','feature23','feature24','feature25','feature26','feature27','feature28'\n                   ,'feature29','feature30','feature31','feature32','feature33','feature34','feature35']\nion_data.head()","87a8fb0c":"ion_data.shape","1d75cefa":"ion_data.info()","f8f15593":"#histogram of all feature\nimport matplotlib.pyplot as plt\nion_data.hist()\nplt.show()","36cfd30f":"# split into input (X) and output (Y) variables\nX = ion_data.values[:,:-1].astype(float)#include all rows of columns exclude last(feature35)\nY = ion_data.values[:,-1]#include all rows of last column\n\n# encode class values as integers\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nencoder.fit(Y)\nY = encoder.transform(Y)","7d1d01c2":"print(X.shape)\nprint(Y.shape)","2df93b86":"X.shape[1]","c3bbf50c":"from sklearn.model_selection import train_test_split\n# split into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=1)","886a73e4":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\n\n# determine the number of input features\nn_features = X.shape[1]\n#define a model\nmodel_1 = Sequential()\nmodel_1.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\nmodel_1.add(Dense(1, activation='sigmoid'))\n\n#compile a model\nmodel_1.compile(loss= 'binary_crossentropy' , optimizer='adam')\n\n# Fit the model\nhistory =model_1.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0, validation_data=(X_test,y_test))\n\n# predict test set\nyhat = model_1.predict_classes(X_test)\n\n# evaluate predictions\nscore = accuracy_score(y_test, yhat)\nmul_by_100 = score*100\nprint('Accuracy: {:,.2f}{}'.format(mul_by_100, \"%\"))","a1976d5e":"from matplotlib import pyplot\n# plot learning curves\npyplot.title('Learning Curves')\npyplot.xlabel('Epoch')\npyplot.ylabel('Cross Entropy')\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='val')\npyplot.legend()\npyplot.show()","3103e278":"#layers\nmodel_2 = Sequential()\nmodel_2.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\nmodel_2.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\nmodel_2.add(Dense(1, activation='sigmoid'))\n\n#compile a model\nmodel_2.compile(loss= 'binary_crossentropy' , optimizer='adam')\n\n# Fit the model\nhistory =model_2.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0, validation_data=(X_test,y_test))\n\n# predict test set\nyhat = model_2.predict_classes(X_test)\n\n# evaluate predictions\nscore_2 = accuracy_score(y_test, yhat)\nmul_by_100 = score_2*100\nprint('Accuracy: {:,.2f}{}'.format(mul_by_100, \"%\"))","fe52cc65":"#layers\nmodel_3 = Sequential()\nmodel_3.add(Dense(50, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\nmodel_3.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\nmodel_3.add(Dense(1, activation='sigmoid'))\n\n#compile a model\nmodel_3.compile(loss= 'binary_crossentropy' , optimizer='adam')\n\n# Fit the model\nhistory =model_3.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0, validation_data=(X_test,y_test))\n\n# predict test set\nyhat = model_3.predict_classes(X_test)\n\n# evaluate predictions\nscore_3 = accuracy_score(y_test, yhat)\nmul_by_100 = score_3*100\nprint('Accuracy: {:,.2f}{}'.format(mul_by_100, \"%\"))","707b8167":"from tensorflow.keras.layers import Dropout\n#layers\nmodel_4 = Sequential()\nmodel_4.add(Dense(50, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\nmodel_4.add(Dropout(0.4))\nmodel_4.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\nmodel_4.add(Dropout(0.4))\nmodel_4.add(Dense(1, activation='sigmoid'))\n\n#compile a model\nmodel_4.compile(loss= 'binary_crossentropy' , optimizer='adam')\n\n# Fit the model\nhistory =model_4.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0, validation_data=(X_test,y_test))\n\n# predict test set\nyhat = model_4.predict_classes(X_test)\n\n# evaluate predictions\nscore_4 = accuracy_score(y_test, yhat)\nmul_by_100 = score_4*100\nprint('Accuracy: {:,.2f}{}'.format(mul_by_100, \"%\"))","978a2ea3":"# define a row of new data\nrow = [1,0,0.02337,-0.00592,-0.09924,-0.11949,-0.00763,-0.11824,0.14706,0.06637,0.03786,-0.06302,0,0,-0.04572,-0.1554,-0.00343,-0.10196,-0.11575,-0.05414,0.01838,0.03669,0.01519,0.00888,0.03513,-0.01535,-0.0324,0.09223,-0.07859,0.00732,0,0,-0.00039,0.12011]\n# make prediction\nyhat = model_4.predict_classes([row])\n# invert transform to get label for class\nyhat = encoder.inverse_transform(yhat)\n# report prediction\nprint('Predicted: %s' % (yhat[0]))","f849452c":"from keras.optimizers import SGD\n# create model\nmodel_5 = Sequential()\nmodel_5.add(Dense(34, input_dim=34 , activation= 'relu' ))\nmodel_5.add(Dense(1,  activation= 'sigmoid' ))\n\n# Compile model\nepochs = 50\nlearning_rate = 0.1\ndecay_rate = learning_rate \/ epochs\nmomentum = 0.8\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\nmodel_5.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n\n# Fit the model\nmodel_5.fit(X_train, y_train, validation_split=0.33, epochs=epochs, batch_size=8, verbose=0)\n\n# predict test set\nyhat = model_4.predict_classes(X_test)\n\n# evaluate predictions\nscore_5 = accuracy_score(y_test, yhat)\nmul_by_100 = score_5*100\nprint('Accuracy: {:,.2f}{}'.format(mul_by_100, \"%\"))","abb2af32":"from keras.optimizers import SGD\n# create model\nmodel_6 = Sequential()\nmodel_6.add(Dense(50, input_dim=34 , activation= 'relu' ))\nmodel_6.add(Dense(8, activation= 'relu' ))\nmodel_6.add(Dense(1,  activation= 'sigmoid' ))\n\n# Compile model\nepochs = 50\nlearning_rate = 0.1\ndecay_rate = learning_rate \/ epochs\nmomentum = 0.8\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\nmodel_6.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n\n# Fit the model\nmodel_6.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=8, verbose=0)\n\n# evaluate the model\nscores_6 = model_6.evaluate(X, Y)\nprint(\"%s: %.2f%%\" % (model_6.metrics_names[1], scores_6[1]*100))","67705d06":"Modeling # 1","f88ff760":"Model # 5 with sgd","f40f987a":"Model # 4","14b7216c":"Model # 2\n#add second hidden layer with 8 nodes","9b3f5685":"Model # 3\nWe will increase the number of nodes in the first hidden layer from 10 to 50, and in the second hidden layer from 8 to 10.","2d8a1b4c":"Lets predict the label"}}