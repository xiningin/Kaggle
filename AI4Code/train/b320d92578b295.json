{"cell_type":{"83c37cfe":"code","43ab31eb":"code","a09c6f0e":"code","18a31673":"code","9fd532b2":"code","e9f62d7b":"code","6e6e9794":"code","c3f7cb1e":"code","dc949e87":"code","55d91027":"code","9079c5d5":"code","e07620bc":"code","d0fd933d":"code","4da6e554":"code","41d181bc":"code","dc856f4f":"code","2abbed10":"code","270319eb":"code","95cfdbf5":"code","85687060":"code","accfd46d":"code","b76dcd67":"code","9bfb051f":"code","477b42ff":"code","b6f6b6d3":"code","f915694b":"code","c5255eb0":"code","c8e9eef6":"code","e7487023":"code","bc4557f8":"code","c48e88cc":"code","501278af":"code","44967ad2":"code","d35b663c":"code","08bdaab2":"code","8b9b86eb":"code","73a2bcfe":"code","474243c9":"code","4e9a69af":"code","0c0804f3":"code","b5dc64d7":"code","935fe2cd":"code","93a3a740":"code","bbaf97c9":"code","0b2b9271":"code","b5e921ba":"code","1fe4f06e":"code","26ed6455":"code","087f625a":"code","abfa1ce4":"code","9019e256":"code","665b8617":"code","69323b9f":"code","b4451b74":"code","de74c4f1":"code","7cf1188f":"code","29ec2b28":"markdown","4b81ee31":"markdown","aaf85f72":"markdown","1d1721ce":"markdown","24efaa4b":"markdown","5107095b":"markdown","e315e74f":"markdown","824ac911":"markdown","26e87bd0":"markdown","fb28e2b1":"markdown","e843bf98":"markdown","9dba55c9":"markdown","a5a62c11":"markdown","935b3ec1":"markdown","c3013c19":"markdown","4cc15be6":"markdown","23231180":"markdown","6abf828f":"markdown","195d5db6":"markdown","ac2842df":"markdown"},"source":{"83c37cfe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom xgboost import XGBClassifier\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()\npd.set_option('display.Max_columns',None) # to see all the columns of the dataset","43ab31eb":"# importing train and test data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","a09c6f0e":"# getting info about the training data\ntrain.info()","18a31673":"# checking wether dataset is balanced or not\ntrain['Died'] = 1-train['Survived']\ntrain.Survived.value_counts().plot(kind='bar')\nplt.title('Survived')\nplt.show()","9fd532b2":"train.Ticket = train.Ticket.str[0]\ntest.Ticket = train.Ticket.str[0]","e9f62d7b":"train.groupby('Ticket')['Survived','Died'].sum().plot(kind='bar',stacked = True)\nplt.show()","6e6e9794":"ticket_map = {\n    '1':'1','2':'2','3':'3','4':'4','5':'4','6':'4','7':'4','8':'4','9':'4','P':'P','S':'S','C':'C','A':'A','W':'W','F':'F','L':'L'\n            }\ntrain.Ticket = train.Ticket.map(ticket_map)\ntest.Ticket = test.Ticket.map(ticket_map)","c3f7cb1e":"train.Ticket.value_counts(),test.Ticket.value_counts()","dc949e87":"sns.catplot(x='Age',data = train,kind= 'box')\nplt.show()\ntrain.Age.mean(),train.Age.median()","55d91027":"train.groupby('Sex')['Survived','Died'].sum().plot.bar()\nplt.show()","9079c5d5":"train['Age'][(train.Sex == 'male') & (train.Age.isnull())] = train.Age.median()\ntrain['Age'][(train.Sex == 'female') & (train.Age.isnull())] = train.Age.median()\ntest['Age'][(test.Sex == 'male') & (test.Age.isnull())] = test.Age.median()\ntest['Age'][(test.Sex == 'female') & (test.Age.isnull())] = test.Age.median()","e07620bc":"train.Embarked.value_counts()","d0fd933d":"# filling null value with most frequent class\ntrain.Embarked.fillna('S',inplace=True)","4da6e554":"train['Name_Title'] = train.Name.str.split(',',expand=True)[1].str.split('.',expand=True)[0].str.strip()\ntest['Name_Title'] = test.Name.str.split(',',expand=True)[1].str.split('.',expand=True)[0].str.strip()","41d181bc":"Title_map = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Other\",\n    \"Don\": \"Other\",\n    \"Sir\" : \"Other\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Other\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Other\",\n    'Dona' : \"Other\"\n}\ntrain.Name_Title = train.Name_Title.map(Title_map)\ntest.Name_Title = test.Name_Title.map(Title_map)","dc856f4f":"# Deleting the Name and PassengerID\ndel train['Name']\ndel test['Name']\ndel train['PassengerId']\ndel test['PassengerId']","2abbed10":"train.Ticket[train.Cabin.isnull()].value_counts()","270319eb":"# taking only first letter of cabin column's data \ntest['Cabin'] = test['Cabin'].str[0]\ntrain['Cabin'] = train['Cabin'].str[0]","95cfdbf5":"train[['Cabin','Ticket']].groupby('Cabin',dropna = False).describe(include = 'all')","85687060":"# This function which will replace the null value in cabin with the similar value as of other data according to their Ticket \ndef cabin(data):\n    df= data.copy()\n    for i in range(len(df.Cabin)):\n        if pd.isna(df['Cabin'][i]):\n            \n            cabin = train['Cabin'][train.Ticket == train['Ticket'][i]].mode()\n            if len(cabin) == 0:\n                cabin = 'M' # If there is no match then replace it with 'M' for missing class\n            else:\n                cabin = cabin[0]\n            df['Cabin'][i] = cabin\n    return df['Cabin']","accfd46d":"train['Cabin'] = cabin(train)\ntest['Cabin'] = cabin(test)","b76dcd67":"print(train.Cabin.value_counts())\nprint(test.Cabin.value_counts())","9bfb051f":"cabin_map = {'M':'M','C':'C','B':'B','D':'D','E':'E','A':'A','F':'F','G':'G','T':'G'}\ntrain.Cabin = train.Cabin.map(cabin_map)\ntest.Cabin = test.Cabin.map(cabin_map)\ntrain.Cabin = train.Cabin.astype('O')\ntest.Cabin = test.Cabin.astype('O')","477b42ff":"test.isnull().sum()","b6f6b6d3":"test[['Cabin','Fare']].groupby('Cabin',dropna = False).describe(include = 'all')","f915694b":"test.Fare.fillna(test['Fare'][test.Cabin==test.Cabin[test.Fare.isnull()].values[0]].median(),inplace = True)","c5255eb0":"print('TRAIN_DATA')\nprint('Pclass',len(train.Pclass.value_counts()))\nprint('Sex',len(train.Sex.value_counts()))\nprint('SibSp',len(train.SibSp.value_counts()))\nprint('Parch',len(train.Parch.value_counts()))\nprint('Cabin',len(train.Cabin.value_counts()))\nprint('Embarked',len(train.Embarked.value_counts()))\nprint('Name_label',len(train.Name_Title.value_counts()))\nprint('TEST_DATA')\nprint('Pclass',len(test.Pclass.value_counts()))\nprint('Sex',len(test.Sex.value_counts()))\nprint('SibSp',len(test.SibSp.value_counts()))\nprint('Parch',len(test.Parch.value_counts()))\nprint('Cabin',len(test.Cabin.value_counts()))\nprint('Embarked',len(test.Embarked.value_counts()))\nprint('Name_label',len(test.Name_Title.value_counts()))","c8e9eef6":"train['Family'] = train.Parch + train.SibSp + 1\ntest['Family'] = test.Parch + test.SibSp + 1","e7487023":"train['Solo_Travel'] = np.where(train.Family ==1,1,0)\ntrain['Family_Travel'] = np.where((train.Family>1)&(train.Family<=4),1,0)\ntrain['Group_Travel'] = np.where(train.Family>4,1,0)\ntest['Solo_Travel'] = np.where(test.Family ==1,1,0)\ntest['Family_Travel'] = np.where((test.Family>1)&(test.Family<=4),1,0)\ntest['Group_Travel'] = np.where(test.Family>4,1,0)","bc4557f8":"del train['Parch']\ndel train['SibSp']\ndel test['Parch']\ndel test['SibSp']","c48e88cc":"print(train.groupby('Pclass')['Survived'].value_counts())\ntrain.groupby('Pclass')['Survived','Died'].sum().plot.bar(stacked = True)\nplt.show()","501278af":"train.head()","44967ad2":"# ecoding the categorical variable\ntrain_final = pd.get_dummies(train,drop_first = True)\ntest_final = pd.get_dummies(test,drop_first = True)","d35b663c":"train_final.shape,test_final.shape","08bdaab2":"train_final['child'] = np.where(train_final['Age']<18,1,0)\ntest_final['child'] = np.where(test_final['Age']<18,1,0)\ntrain_final['senior_citizen'] = np.where(train_final['Age']>60,1,0)\ntest_final['senior_citizen'] = np.where(test_final['Age']>60,1,0)","8b9b86eb":"train_final.groupby('child')['Survived','Died'].sum().plot.bar(stacked = True)\ntrain_final.groupby('senior_citizen')['Survived','Died'].sum().plot.bar(stacked = True)\nplt.show()","73a2bcfe":"sns.boxplot(train.Fare)\nplt.show()","474243c9":"# to remove the outlier in Fare column, I have replaced all those with 100\ntrain_final.Fare[train_final['Fare']>100] = 100\ntest_final.Fare[test_final['Fare']>100] = 100","4e9a69af":"del train_final['Died']","0c0804f3":"# printing the correlation matrix\ncor = train_final.corr()\nplt.subplots(figsize=(20,20))\nsns.heatmap(cor,annot=True)\nplt.show()","b5dc64d7":"def correlation(corr_matrix,threshold):\n    col = set()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j]) > threshold:\n                col_name = corr_matrix.columns[i]\n                col.add(col_name)\n    return list(col)","935fe2cd":"print('columns with positive or negative correlation of more than 0.9 -> ',len(correlation(cor,0.9)))","93a3a740":"correlation(cor,0.9)","bbaf97c9":"# Removing the feature with high correlation\ntrain_final = train_final.drop(correlation(cor,0.9),axis = 1)\ntest_final = test_final.drop(correlation(cor,0.9),axis = 1)","0b2b9271":"# making the train test split\nip_train = train_final.drop('Survived',axis = 1)\nop_train = train_final['Survived']\nx_train,x_test,y_train,y_test = train_test_split(ip_train,op_train,random_state = 0,test_size = 0.4)","b5e921ba":"LR_alg = LogisticRegression()\nLR_alg.fit(x_train,y_train)\nlogistic_regression_accuracy = LR_alg.score(x_test,y_test)\n# printing the perfomance of classifier\nprint(logistic_regression_accuracy)\ny_pred_lr = LR_alg.predict(x_test)\nprint(confusion_matrix(y_test,y_pred_lr))\nprint(classification_report(y_test,y_pred_lr))","1fe4f06e":"RF_alg = RandomForestClassifier(\n                                n_estimators = 100,\n                                random_state = 9\n                                )\nRF_alg.fit(x_train,y_train)\nrandom_forest_accuracy = RF_alg.score(x_test,y_test)\n# printing the perfomance of classifier\nprint(random_forest_accuracy)\ny_pred_rf = RF_alg.predict(x_test)\nprint(confusion_matrix(y_test,y_pred_rf))\nprint(classification_report(y_test,y_pred_rf))","26ed6455":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 250, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 10, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in np.linspace(2,20,5)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in np.linspace(2,10,5)]\n# Method of selecting samples for training each tree\nbootstrap = [True]\ncriterion = ['gini','entropy']\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n               'criterion': criterion}\n\nrf = RandomForestClassifier()\nrf_random_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=9, n_jobs = -1)\nrf_random_search.fit(x_train,y_train)\n\nrf_random_search.best_params_\n\n# for fine tuning I have used GridsearchCV after RandomizedSearchCV\ngrid = {\n    'n_estimators' : [rf_random_search.best_params_['n_estimators'],rf_random_search.best_params_['n_estimators']+50,rf_random_search.best_params_['n_estimators']+100],\n    'min_samples_split' : [rf_random_search.best_params_['min_samples_split'],rf_random_search.best_params_['min_samples_split']-1,rf_random_search.best_params_['min_samples_split']+1],\n    'min_samples_leaf' : [rf_random_search.best_params_['min_samples_leaf'],rf_random_search.best_params_['min_samples_leaf']-1,rf_random_search.best_params_['min_samples_leaf']+1],\n    'max_depth' : [rf_random_search.best_params_['max_depth'],None],\n    'max_features' : [rf_random_search.best_params_['max_features']],\n    'criterion' : [rf_random_search.best_params_['criterion']],\n    'bootstrap' : [rf_random_search.best_params_['bootstrap']]\n}\nrf_clf = RandomForestClassifier()\nrf_grid_search = GridSearchCV(rf_clf,grid,n_jobs = -1,cv=5,verbose = 3)\nrf_grid_search.fit(x_train,y_train)\n\nrf_grid_search.best_params_","087f625a":"feat_imp = pd.Series(rf_grid_search.best_estimator_.feature_importances_,index = x_train.columns)\nplt.subplots(figsize =(16,6))\nfeat_imp.sort_values(ascending=False).plot.bar()\nplt.show()","abfa1ce4":"# printing the perfomance of classifier\nrandom_forest_accuracy_grid = rf_grid_search.score(x_test,y_test)\nprint(random_forest_accuracy_grid)\ny_pred_grid = rf_grid_search.predict(x_test)\nprint(confusion_matrix(y_test,y_pred_grid))\nprint(classification_report(y_test,y_pred_grid))","9019e256":"classifier = XGBClassifier()\nclassifier.fit(x_train,y_train)\n# printing the perfomance of classifier\nprint(classifier.score(x_test,y_test))\ny_pred_clf = classifier.predict(x_test)\nprint(confusion_matrix(y_test,y_pred_clf))\nprint(classification_report(y_test,y_pred_clf))","665b8617":"xgb_params = {\n    'objective' : ['binary:logistic'],\n    'booster' : ['gbtree'],\n    'eval_metric' : ['logloss'],\n    'max_depth': [int(i) for i in np.linspace(2,10,5)],\n    'min_child_weight':[int(i) for i in np.linspace(1,5,5)],\n    'gamma' : [i for i in np.linspace(0.0,1,6)],\n    'subsample' : [ i for i in np.linspace(0.6,1.0,9)],\n    'colsample_bytree' : [i for i in np.linspace(0.6,1.0,9)],\n    'learning_rate' : [0.01,0.05,0.1,0.15,0.2,0.25,0.3]\n}\nxgb_params\nxgb = XGBClassifier()\nxgb_hptune = RandomizedSearchCV(xgb,xgb_params,cv=5,random_state = 0,n_iter = 200,n_jobs = -1,verbose = 2)\nxgb_hptune.fit(x_train,y_train)\nprint(xgb_hptune.best_params_)","69323b9f":"# printing the perfomance of classifier\nprint(xgb_hptune.score(x_test,y_test))\ny_pred_xgb = xgb_hptune.predict(x_test)\nprint(confusion_matrix(y_test,y_pred_xgb))\nprint(classification_report(y_test,y_pred_xgb))","b4451b74":"# initialising the NN\nmodel = Sequential()\n\n# layers\nmodel.add(Dense(16, kernel_initializer = 'uniform', activation = 'relu', input_dim = ip_train.shape[1]))\nmodel.add(Dense(16, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# compiling the NN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n # filepath required for checkpoint\nfilepath = os.path.join('.\/',\"titanic.best.hdf5\")\n# making a checkpoint\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\n# fitting the training data to the model\nmodel_history = model.fit(x_train,y_train,validation_data = (x_test, y_test),epochs=200,verbose = 1, batch_size=32,callbacks=[checkpoint])\n\n# loading the best weights to the model and compiling it again\nmodel.load_weights(filepath)\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# printing the perfomance of classifier\ny_pred_nn = model.predict(x_test)\ny_pred_nn = np.where(y_pred_nn>0.5,1,0)\nprint(confusion_matrix(y_test,y_pred_nn))\nprint(classification_report(y_test,y_pred_nn))","de74c4f1":"model.load_weights(filepath)","7cf1188f":"# getting the final test prediction from all the models\ny_pred_xgb = xgb_hptune.predict(test_final) # XGBClassifier with hypertuning of parameters\ny_pred_grid = rf_grid_search.predict(test_final) # RandomForestClassifier with hypertuning of parameters\ny_pred_lr = LR_alg.predict(test_final) # LogisticRegression\ny_pred_clf = classifier.predict(test_final) # XGBClassifier without hypertuning of parameters\n# from neural network retriving the final prediciton\ny_pred_nn = model.predict(test_final)\ny_pred_nn = np.where(y_pred_nn > 0.5,1,0)\ny_pred_nnn = []\nfor i in y_pred_nn:\n    y_pred_nnn.append(i[0])\ny_pred_nnn = np.array(y_pred_nnn) \n# making final prediction as mean of all the predictions\nprediction = (y_pred_xgb + y_pred_grid + y_pred_nnn + y_pred_lr )\/4\nprediction = np.where(prediction > 0.5,1,0)\n\n# Saving the prediciton into the CSV file\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntitanic_submission = pd.DataFrame(test['PassengerId'])\ntitanic_submission['Survived'] = prediction\ntitanic_submission.to_csv('.\/titanic_dataset_prediction.csv',index=False)\n","29ec2b28":"Pclass v\/s Survived plot suggests that class 1 and 2 might be of the vips","4b81ee31":"### RandomForestClassifier","aaf85f72":"Extracting the title from the Name column","1d1721ce":"Age feature has some outlier but important thing is that the it looks like they might had a policy of ladies first so I have filled the null Age data according to their Sex","24efaa4b":"### Hypertuning the parameters of RandomForestClassifier","5107095b":"### importing required libraries","e315e74f":"### XGBoostClassifier ","824ac911":"#### Feature Importance","26e87bd0":"### feature engineering and feature selection","fb28e2b1":"### LogisticRegression","e843bf98":"### Final Prediction","9dba55c9":"Fare column in test data has one missing value. I have filled it with the median value acoording to the its cabin class","a5a62c11":"I have taken the mean of all the models that has been built for final prediction","935b3ec1":"In Embarked columns it has only two NaN values which can be filled by most frequent value","c3013c19":"for Ticket feature I have taked first letter of each data and the graph suggests tha it has something to do with survival","4cc15be6":"### Hypertuning the parameters of XGBoostClassifier","23231180":"handling the null values in Cabin column according to their Ticket class","6abf828f":"adding a new column which suggest if the passenger was child or senior citizon or not","195d5db6":"### NeuralNetwork","ac2842df":"I hace combined the all the number of family members and based on it I have classified into solo travel,family travel and group travel"}}