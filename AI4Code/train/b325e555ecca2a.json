{"cell_type":{"631b05f9":"code","45e891ec":"code","eae2ef82":"code","e9b196c6":"code","2bc20361":"code","580a84c2":"code","e4243350":"code","947f4826":"code","2ba0c248":"code","bb627487":"code","4befa226":"code","d05161d0":"code","624f4714":"code","b9191595":"code","17f5e11c":"code","bf9f15f0":"code","095043b9":"code","a592d3b6":"code","914b2d69":"code","8e09fe62":"code","b5fbe77c":"markdown","6cc5dd96":"markdown","57c7d2f3":"markdown","b5e0307e":"markdown"},"source":{"631b05f9":"from xgboost import XGBClassifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import RandomizedSearchCV\n#from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","45e891ec":"X_asm = pd.read_csv(\"..\/input\/asm_reduced_final.csv\")\ny = pd.read_csv(\"..\/input\/trainLabels.csv\")\nX_byte = pd.read_csv(\"..\/input\/final_byte_with_entropy.csv\")\nasm_file_size = pd.read_csv(\"..\/input\/asm_file_size.csv\")","eae2ef82":"X_asm = X_asm.merge(asm_file_size, on=\"Id\")","e9b196c6":"byte_entropy_filesize = X_byte[[\"Id\", \"entropy\", \"fsize\"]]\nbyte_entropy_filesize = byte_entropy_filesize.set_index(\"Id\")","2bc20361":"poly = PolynomialFeatures(3)\nx_byte_poly = poly.fit_transform(byte_entropy_filesize)","580a84c2":"byte_entropy_filesize.head()","e4243350":"x_byte_poly = x_byte_poly[:, 1:]","947f4826":"x_byte_df = pd.DataFrame(x_byte_poly, columns = [\"byte_fe\"+str(i) for i in range(x_byte_poly.shape[1])] )","2ba0c248":"byte_entropy_filesize = byte_entropy_filesize.reset_index()\nx_byte_df[\"Id\"] = byte_entropy_filesize[\"Id\"]","bb627487":"x_byte_df.head()","4befa226":"#Very important\n#data = X_asm.merge(y_asm, on=\"Id\")\n#data.head()\ndata_asm_byte_final = X_asm.merge(x_byte_df, on=\"Id\")\ndata_asm_byte_final.head()","d05161d0":"final_y = data_asm_byte_final[\"Class\"]\ndata_asm_byte_final = data_asm_byte_final.drop(\"Class\", axis=1)","624f4714":"data_asm_byte_final.head()","b9191595":"\n#Let's normalize the data.\ndef normalize(dataframe):\n    #print(\"Here\")\n    test = dataframe.copy()\n    for col in tqdm(test.columns):\n        if(col != \"Id\" and col !=\"Class\"):\n            max_val = max(dataframe[col])\n            min_val = min(dataframe[col])\n            test[col] = (dataframe[col] - min_val) \/ (max_val-min_val)\n    return test","17f5e11c":"data_asm_byte_final = normalize(data_asm_byte_final)","bf9f15f0":"data_asm_byte_final.head()","095043b9":"data_y = final_y\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nx_train, x_test, y_train, y_test = train_test_split(data_asm_byte_final.drop(['Id'], axis=1), data_y,stratify=data_y,test_size=0.20)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\nx_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train,stratify=y_train,test_size=0.20)","a592d3b6":"def perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv):\n    cv_log_error_array = []\n    for i in tqdm(list_of_hyperparam):\n        if (model_name == \"knn\"):\n            model =  KNeighborsClassifier(n_neighbors = i)\n            #model.fit(x_train, y_train)\n        elif(model_name == \"lr\"):\n            model = LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n            #model.fit(x_train, y_train)\n        elif(model_name == \"rf\"):\n            model = RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1, class_weight='balanced')\n            #model.fit(x_train, y_train)\n        elif(model_name == \"xgbc\"):\n            #weights = []\n            #a = y_train.value_counts().sort_index()\n            #sum_a = a.sum()\n            #for i in a:\n            #    weights.append(sum_a\/i)\n            model = XGBClassifier(n_estimators=i,nthread=-1)\n            #model.fit(x_train, y_train, sample_weight = weights)\n        model.fit(x_train, y_train)\n        caliberated_model = CalibratedClassifierCV(model, method = \"sigmoid\")\n        caliberated_model.fit(x_train, y_train)\n        predict_y = caliberated_model.predict_proba(x_cv)\n        cv_log_error_array.append(log_loss(y_cv, predict_y))\n    for i in range(len(cv_log_error_array)):\n        print ('log_loss for hyper_parameter = ',list_of_hyperparam[i],'is',cv_log_error_array[i])\n    return cv_log_error_array\n       \ndef get_best_hyperparam(list_of_hyperparam, cv_log_error_array):\n    index = np.argmin(cv_log_error_array)\n    best_hyperparameter = list_of_hyperparam[index]\n    return best_hyperparameter\n\n\ndef perform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test):\n    \n    if (model_name == \"knn\"):\n            model =  KNeighborsClassifier(n_neighbors = best_hyperparameter)\n    elif(model_name == \"lr\"):\n            model = LogisticRegression(penalty = 'l2',C = best_hyperparameter,class_weight = 'balanced')\n    elif(model_name == \"rf\"):\n            model = RandomForestClassifier(n_estimators = best_hyperparameter,random_state = 42,n_jobs = -1,  class_weight='balanced')\n    elif(model_name == \"xgbc\"):\n            model = XGBClassifier(n_estimators = best_hyperparameter,nthread=-1)\n            \n    model.fit(x_train, y_train)\n    \n    caliberated_model = CalibratedClassifierCV(model, method = \"sigmoid\")\n    caliberated_model.fit(x_train, y_train)\n\n    predicted_y = caliberated_model.predict_proba(x_train)\n    print(\"The training log-loss for best hyperparameter is\", log_loss(y_train, predicted_y))\n    predicted_y = caliberated_model.predict_proba(x_cv)\n    print(\"The cv log-loss for best hyperparameter is\", log_loss(y_cv, predicted_y))\n    predicted_y = caliberated_model.predict_proba(x_test)\n    print(\"The test log-loss for best hyperparameter is\", log_loss(y_test, predicted_y))\n\n    predicted_y = caliberated_model.predict(x_test)\n    #plot_confusion_matrix(y_test, predicted_y)\n    \n\ndef plot_cv_error(list_of_hyperparam, cv_log_error_array):\n    fig, ax = plt.subplots()\n    ax.plot(list_of_hyperparam, cv_log_error_array,c='g')\n    for i, txt in enumerate(np.round(cv_log_error_array,3)):\n        ax.annotate((list_of_hyperparam[i],np.round(txt,3)), (list_of_hyperparam[i],cv_log_error_array[i]))\n    plt.grid()\n    plt.title(\"Cross Validation Error for each hyperparameter\")\n    plt.xlabel(\"Hyperparameter\")\n    plt.ylabel(\"Error measure\")\n    plt.show()\n","914b2d69":"list_of_hyperparam = [10,50,100,500,1000,2000,3000]\nmodel_name = \"rf\"\ncv_log_error_array = perform_hyperparam_tuning(list_of_hyperparam, model_name,  x_train, y_train, x_cv, y_cv)","8e09fe62":"best_hyperparameter = get_best_hyperparam(list_of_hyperparam, cv_log_error_array)\nperform_on_best_hyperparam(model_name, best_hyperparameter, cv_log_error_array,x_train,y_train,x_cv,y_cv,x_test,y_test)","b5fbe77c":"# Byte file sizes","6cc5dd96":"# Apply polyfeatures here on byte files.","57c7d2f3":"# Random Forest","b5e0307e":"# ASM File features"}}