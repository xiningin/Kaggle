{"cell_type":{"e555efc8":"code","4511bee2":"code","e379467b":"code","6911e714":"code","c77a6cff":"code","9ad6060b":"code","54e5e42c":"code","d7dfd27f":"code","d334b5be":"code","22744570":"code","8e2a77ca":"code","231973b4":"code","32541423":"code","10687d62":"code","2c9416cb":"code","239cfe57":"code","739965b6":"code","db25b0c5":"code","4807aec0":"code","81cdec06":"code","48b3b16c":"code","a6f6bdca":"code","f9c5587f":"code","9bc7bfed":"code","2a07c160":"code","97fd40ab":"code","193deaeb":"code","2beef8fc":"code","32f16f5b":"code","0fe5e72a":"code","a86fe455":"markdown","656daf6b":"markdown","0461d27a":"markdown","13707d40":"markdown","7a33bc78":"markdown","1c9c81cd":"markdown","eb061129":"markdown","bdfaae60":"markdown","d5bfe3dc":"markdown","90cea879":"markdown","429d7130":"markdown","0860f835":"markdown","eea03afc":"markdown","04b8ae3e":"markdown","e981c2aa":"markdown","6e73b1fc":"markdown"},"source":{"e555efc8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport seaborn as sns\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\n# Import the 3 dimensionality reduction methods\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA,FastICA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.linalg import svd\n","4511bee2":"train = pd.read_csv('..\/input\/train.csv')\ntrain.head()","e379467b":"print(train.shape)","6911e714":"sample_size = train.shape[0]\ndimension = train.shape[1]","c77a6cff":"# save the labels to a Pandas series target\ntarget = train.label\n# Drop the label feature\ntrain = train.iloc[:,:-1]","9ad6060b":"train.shape","54e5e42c":"# Standardize the data\nfrom sklearn.preprocessing import StandardScaler\nX = train.values\nX_std = StandardScaler().fit_transform(X)","d7dfd27f":"# Calculating Eigenvectors and eigenvalues of Cov matirx\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)","d334b5be":"#check the shape of the eig_values\nprint(eig_vals.shape)\nprint(eig_vecs.shape)","22744570":"# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the eigenvalue, eigenvector pair from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)","8e2a77ca":"np.array(eig_pairs).shape","231973b4":"# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\nvar_exp = [(i\/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance","32541423":"# Invoke SKlearn's PCA method\nn_components = 50\npca = PCA(n_components=n_components).fit(train.values)\n\neigenvalues = pca.components_.reshape(n_components, 28, 28)\n\neigenvalues = pca.components_","10687d62":"#how much each component adds up\nprint(pca.explained_variance_ratio_)","2c9416cb":"#total explained variance ratio\nprint(np.sum(pca.explained_variance_ratio_))","239cfe57":"n_row = 5\nn_col = 6\n\n# Plot the first 8 eignenvalues\nplt.figure(figsize=(15,12))\nfor i in list(range(n_row * n_col)):\n    offset =0\n    plt.subplot(n_row, n_col, i + 1)\n    plt.imshow(eigenvalues[i].reshape(28,28), cmap='jet')\n    title_text = 'Eigenvalue ' + str(i + 1)\n    plt.title(title_text, size=10)\n    plt.xticks(())\n    plt.yticks(())\nplt.show()","739965b6":"# plot some of the numbers\nplt.figure(figsize=(14,12))\nfor digit_num in range(0,70):\n    plt.subplot(7,10,digit_num+1)\n    grid_data = train.iloc[digit_num].as_matrix().reshape(28,28)  # reshape from 1d to 2d pixel array\n    plt.imshow(grid_data, interpolation = \"none\", cmap = \"jet\")\n    plt.xticks([])\n    plt.yticks([])\nplt.tight_layout()","db25b0c5":"# Taking only the first N rows to speed things up\nX= train[:6000].values\n\n# Standardising the values\nX_std = StandardScaler().fit_transform(X)\n\n# Call the PCA method with an explained ratio of 90%. \npca = PCA(0.9)\npca.fit(X_std)\nX_5d = pca.transform(X_std)\n\n# For cluster coloring in our Plotly plots, remember to also restrict the target values \nTarget = target[:6000]","4807aec0":"X_5d.shape","81cdec06":"#how much each component adds up\nprint(pca.explained_variance_ratio_)\n#how many component do we have?\n#how much each component adds up\nprint(\"we have \"+str(len(pca.explained_variance_ratio_)) + \" PCA components\" )","48b3b16c":"#total explained variance ratio\nprint(np.sum(pca.explained_variance_ratio_))","a6f6bdca":"principalDf = pd.DataFrame(data = X_5d[:,:2]\n             , columns = ['principal component 1', 'principal component 2'])","f9c5587f":"finalDf = pd.concat([principalDf, Target], axis = 1)","9bc7bfed":"finalDf.head(5)","2a07c160":"finalDf['label'].unique()","97fd40ab":"fig = plt.figure(figsize = (18,10))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\n\ntargets = finalDf['label'].unique()\ncolors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'pink','cyan','magenta']\n\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['label'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               ,finalDf.loc[indicesToKeep, 'principal component 2']\n               ,c = color\n               ,s = 50)\nax.legend(targets)\nax.grid()","193deaeb":"from sklearn.cluster import KMeans # KMeans clustering \n# Set a KMeans clustering with 9 components ( 9 chosen sneakily ;) as hopefully we get back our 9 class labels)\nkmeans = KMeans(n_clusters=10)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(X_5d)","2beef8fc":"X_clustered[:10]","32f16f5b":"np.array(finalDf['label'])[:10]","0fe5e72a":"from sklearn.metrics import classification_report\n\nprint(classification_report(finalDf['label'],X_clustered))","a86fe455":"**Takeaway from the Plot**\n\nPCA is actually in fact an unsupervised method which does not depend on class labels. \nWe could use clustering techniques to label the data","656daf6b":"# 1. Principal Component Analysis (PCA)\n\nIn a nutshell, PCA is a linear transformation algorithm that seeks to project the original features of our data onto a smaller set of features ( or subspace ) while still retaining most of the information. To do this the algorithm tries to find the most appropriate directions\/angles ( which are the principal components ) that maximise the variance in the new subspace. Why maximise the variance though? \n\nTo answer the question, more context has to be given about the PCA method. One has to understand that the principal components are orthogonal to each other ( think right angle ). As such when generating the covariance matrix ( measure of how related 2 variables are to each other ) in our new subspace, the off-diagonal values of the covariance matrix will be zero and only the diagonals ( or eigenvalues) will be non-zero. It is these diagonal values that represent the *variances* of the principal components that we are talking about or information about the variability of our features. \n\nTherefore when PCA seeks to maximise this variance, the method is trying to find directions ( principal components ) that contain the largest spread\/subset of data points or information ( variance ) relative to all the data points present. For a brilliant and detailed description on this, check out this stackexchange thread: \n\n[PCA and proportion of variance explained][1] by amoeba\n\n  [1]: http:\/\/stats.stackexchange.com\/a\/140579\/3277","0461d27a":"# MNIST Dataset\n\nFor the purposes of this interactive guide, the MNIST (Mixed National Institute of Standards and Technology) computer vision digit dataset was chosen partly due to its simplicity and also surprisingly deep and informative research that can be done with the dataset. So let's load the training data and see what we have","13707d40":"# Introduction\n\n## Why is Dimensionality Reduction required?\n\n1. Space required to store the data is reduced as the number of dimensions comes down.\n2. Less dimensions lead to less computation\/training time.\n3. Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful.\n4. It takes care of multicollinearity by removing redundant features. For example, you have two variables \u2013 \u2018time spent on treadmill in minutes\u2019 and \u2018calories burnt\u2019. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n5. It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly\n\nI will use kinds of method to reduce the dimension of the data as shown below.\n\n 1. **Principal Component Analysis ( PCA )**  - Unsupervised, linear method\n 2. **Linear Discriminant Analysis (LDA)** - Supervised, linear method\n 3. **t-distributed Stochastic Neighbour Embedding (t-SNE)** - Nonlinear, probabilistic method\n 4. **Independent Component Analysis(ICA)** \n 5. **ISOMAP** Projection based\n 6. **Uniform Manifold Approximation and Projection (UMAP)** Projection based \n \nLets start with importing libraries.","7a33bc78":"### Calculating the Eigenvectors\n\nNow it may be informative to observe how the variances look like for the digits in the MNIST dataset. Therefore to achieve this, let us calculate the eigenvectors and eigenvalues of the covarience matrix as follows:","1c9c81cd":"**Visualising the MNIST Digit set on its own**\n\nNow just for the fun and curiosity of it, let's plot the actual MNIST digit set to see what the underlying dataset actually represents, rather than being caught up with just looking at 1 and 0's.","eb061129":"**Pearson Correlation Plot**\n\nSince we are still having the problem that our dataset consists of a relatively large number of features (columns),  it is perfect time to introduce Dimensionality Reduction methods. Before we start off, let's conduct some cleaning of the train data by saving the label feature and then removing it from the dataframe","bdfaae60":"### Interactive visualisations of PCA representation\n\nWhen it comes to these dimensionality reduction methods, scatter plots are most commonly implemented because they allow for great and convenient visualisations of clustering ( if any existed ) and this will be exactly what we will be doing as we plot the first 2 principal components as follows:","d5bfe3dc":"### PCA Implementation via Sklearn\n\nNow using the Sklearn toolkit, we implement the Principal Component Analysis algorithm as follows:","90cea879":"Phew, they are definitely digits all right. So let's proceed onto the main event.","429d7130":"### K-Means Clustering to identify possible classes\n\nWe can apply a clustering algorithm on our new PCA projection data and hopefully arrive at distinct clusters which would tell us something about the underlying class separation in the data. \n\nTo start off, we set up a KMeans clustering method with Sklearn's **KMeans** call and use the **fit_predict** method to compute cluster centers and predict cluster indices for the first and second PCA projections (to see if we can observe any appreciable clusters).","0860f835":"**Takeaway from the Plots**\n\nIt is obvious that more complicated directions or components are being generated in the search to maximise variance in the new feature subspace.","eea03afc":"At first, we normalise the data using Sklearn's convenient StandardScaler call.\n\nNext we invoke Sklearn's inbuilt PCA function by providing into its argument *n_components*, the number of components\/dimensions we would like to project the data onto. I have just decided to take a PCA on 10 components ( against perhaps taking 200 over components).\n\nFinally, I call both fit and transform methods which fits the PCA model with the standardised digit data set and then does a transformation by applying the dimensionality reduction on the data.","04b8ae3e":"**Visualizing the Eigenvalues**\n\nAs alluded to above, since the PCA method seeks to obtain the optimal directions (or eigenvectors) that captures the most variance ( spreads out the data points the most ). Therefore it may be informative ( and cool) to visualise these directions and their associated eigenvalues. For the purposes of this notebook and for speed, I will invoke PCA to only extract the top 10 eigenvalues ( using Sklearn's .components_ call) from the digit dataset and visually compare the top 5 eigenvalues to some of the other smaller ones to see if we can glean any insights as follows:","e981c2aa":"The MNIST set consists of 42,000 rows and 785 columns. There are 28 x 28 pixel images of digits  ( contributing to 784 columns) as well as one extra label column which is essentially a class label to state whether the row-wise contribution to each digit gives a 1 or a 9. Each row component contains a value between one and zero and this describes the intensity of each pixel. ","6e73b1fc":"**How can we interpret this result**\n\nVisually, the clusters generated by the KMeans algorithm appear to provide a clearer demarcation amongst clusters as compared to naively adding in class labels into our PCA projections. This should come as no surprise as PCA is meant to be an unsupervised method and therefore not optimised for separating different class labels. \n\nThis particular task however is accomplished by LDA at the following link."}}