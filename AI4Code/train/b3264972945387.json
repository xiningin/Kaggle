{"cell_type":{"98e0777a":"code","c3f4af2d":"code","aedac13d":"code","7b3ef99a":"code","836451f6":"code","c410c954":"code","46c664b5":"code","cc621929":"code","62201fe9":"code","a43b9e44":"code","579c2b92":"code","6a6b181e":"code","7138f74e":"code","52a9ac78":"code","4896e063":"code","d4663873":"code","9df36b82":"code","f26c5f91":"code","8d69ec2d":"code","7cce7f4c":"code","6ebbf037":"code","1e8e57b5":"code","0993ca5f":"code","3ecc47b0":"code","20a9eb4e":"code","85030a43":"code","212e7720":"code","a1762ba6":"code","f4ad5fe8":"code","eda818fa":"code","2bcd94aa":"code","ed0fc220":"code","4169b0da":"code","9abe5b12":"code","747eb9c5":"code","26dc6ea3":"code","cce87344":"code","d7eaf9d4":"code","514f7af5":"code","591937c2":"code","264c1c32":"code","80edb545":"code","b147ebd0":"code","81335849":"code","414f7112":"code","514995c3":"code","8c0268ae":"code","4c7b95dd":"code","b9ff5b46":"code","f47816be":"code","747a9d26":"code","f7379e3c":"code","f93208de":"code","21b02dd9":"code","71a7deab":"code","7cd285f7":"code","66747bdd":"code","6c6552ba":"code","52e8444f":"code","22091996":"code","9e88a630":"code","a0658e1e":"code","0fa88d92":"code","3395d0d1":"code","435ca354":"code","5041e819":"code","893b29e9":"code","e044db9e":"code","aeabfd7c":"code","1fac992d":"code","65c04ef5":"code","484ed564":"code","91161ed5":"code","e5227eb6":"code","101f52cb":"code","d9e93267":"markdown","dea05011":"markdown","4e5a952b":"markdown","76dac7ea":"markdown","6e232e8d":"markdown","fbd94641":"markdown","48dca41e":"markdown"},"source":{"98e0777a":"import re\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n%matplotlib inline\n\nimport warnings  \nwarnings.filterwarnings('ignore')","c3f4af2d":"# Function to reduce memory usage.  From: https:\/\/www.kaggle.com\/fabiendaniel\/detecting-malwares-with-lgbm\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","aedac13d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(train.shape)\ntrain.sample(2)","7b3ef99a":"# Revenues are not uniformally distributed\ntrain['revenue'].hist(bins=25)\nplt.show()","836451f6":"# When comparing the listed revenues with their actual values found online, \n# it's clear the values given here are not accurate.\ntrain.sort_values('revenue').sample(3)","c410c954":"# Budget is also skewed.\ntrain['budget'].hist(bins=25)\nplt.show()","46c664b5":"# $0 budget for some movies?\ntrain['budget'].describe()","cc621929":"print('Movies with 0$ Budget:', len(train[train['budget'] == 0]))\ntrain[train['budget'] == 0].head()","62201fe9":"# Create columns for year, month, and day of week\ntrain['release_date'] = pd.to_datetime(train['release_date'], infer_datetime_format=True)\ntrain['release_day'] = train['release_date'].apply(lambda t: t.day)\ntrain['release_weekday'] = train['release_date'].apply(lambda t: t.weekday())\ntrain['release_month'] = train['release_date'].apply(lambda t: t.month)\n\n# Year was being interpreted as future dates in some cases so I had to adjust some values\ntrain['release_year'] = train['release_date'].apply(lambda t: t.year if t.year < 2018 else t.year -100)","a43b9e44":"train['runtime'].hist(bins=25)\nplt.show()","579c2b92":"len(train[train['runtime'] == 0])","6a6b181e":"# I'll write a function that will map the average runtime for each year to movies with 0 runtie.\nfrom collections import defaultdict\ndef map_runtime(df):\n    df['runtime'].fillna(0)\n    \n    run = df[(df['runtime'].notnull()) & (df['runtime'] != 0)]\n    year_mean = run.groupby(['release_year'])['runtime'].agg('mean')\n    d = dict(year_mean)\n    \n    for i in df[df['runtime'] == 0]:\n        df['runtime'] = df.loc[:, 'release_year'].map(d)\n    \n    return df","7138f74e":"train = map_runtime(train)\ntrain.runtime.describe()","52a9ac78":"train['homepage'].head()","4896e063":"# For homepage, I'll change it to 0 for NaN and 1 if a page is listed.\ntrain['homepage'].fillna(0, inplace=True)\ntrain.loc[train['homepage'] != 0, 'homepage'] = 1","d4663873":"# For poster_path, I'll change it to 0 for NaN and 1 if a path is listed.\ntrain['poster_path'].fillna(0, inplace=True)\ntrain.loc[train['poster_path'] != 0, 'poster_path'] = 1","9df36b82":"# For genres, I'll fill Na values with drama (most common).  Likely a better approach available.\ntrain.genres = train.genres.fillna('18')","f26c5f91":"# To fill in zero budget data points, I'll try to use correlated values as predictors\nX = train[train['budget'] != 0]\nfor i in X.select_dtypes(include='number', exclude='datetime'):\n    print(i, stats.pearsonr(X.budget, X[i]))","8d69ec2d":"# release_year and popularity correlate most strongly with budget\ndef map_budget(df):\n    d = defaultdict()\n    #df['budget'] = df['budget'].fillna(0)\n    X = df[df['budget'] != 0]\n    \n    year_mean = pd.Series(X.groupby(['release_year'])['budget'].agg('mean'))\n    d = dict(year_mean)\n    \n    for i in df[df['budget'] == 0]:\n        df['budget'] = df.loc[:, 'release_year'].map(d)\n    \n    # In a few cases, there are only 1 or 2 movies provided from a given year and are filled with Na values\n    df.budget = df.sort_values(by='release_year').budget.fillna(method='ffill')\n    \n    return df","7cce7f4c":"train = map_budget(train)\ntrain.budget.describe()","6ebbf037":"train['belongs_to_collection'].head()","1e8e57b5":"# belongs_to_collection NaN values can be replaced with 'none'\ntrain['belongs_to_collection'] = train['belongs_to_collection'].fillna('none')","0993ca5f":"train['spoken_languages'].head()","3ecc47b0":"train.spoken_languages.value_counts(dropna=False)","20a9eb4e":"# For spoken_languages I'll fill Na values with [{'iso_639_1': 'en', 'name': 'English'}]\ntrain.spoken_languages = train.spoken_languages.fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")","85030a43":"train['overview'].head()","212e7720":"# For overview, I'll fill Na values with 'none'\ntrain.overview = train.overview.fillna('none')","a1762ba6":"train['Keywords'].head()","f4ad5fe8":"# For Keywords, I'll fill Na values with 'none'\ntrain.Keywords = train.Keywords.fillna('none')","eda818fa":"train.production_countries.describe()","2bcd94aa":"# For production_countries, I'll fill Na with the most common value\ntrain.production_countries = train.production_countries.fillna(\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")","ed0fc220":"train.production_companies.value_counts()","4169b0da":"# Create a columns for title length\ntitle_len = []\nfor i in train['title']:\n    title_len.append(len(i.split()))\ntitle_len = pd.Series(title_len, name='title_length')\ntrain = pd.concat([train,title_len], axis=1)\n\ntrain['title_length'].describe()","9abe5b12":"# For genres, I'll make a new column counting the number of listed genre types\n# This will strip out all characters except for numbers, and return this as an array\ngenre_ids = []\nfor i in train['genres']:\n    i = re.findall('\\d+', i)\n    genre_ids.append(i)\ngenre_ids = pd.Series(genre_ids, name='genre_ids').astype(str)\n\n# This will count the number of genres listed for each film\nnum_genre_types = []\nfor i in genre_ids:\n    num_genre_types.append(len(i.split()))\nnum_genre_types = pd.Series(num_genre_types, name='num_genre_types').astype(int)\ntrain = pd.concat([train, genre_ids, num_genre_types], axis=1)\n\ntrain['num_genre_types'].describe()","747eb9c5":"# Create column for sequels\nis_sequel = []\nfor i in train['Keywords']:\n    if 'sequel' in str(i):\n        is_sequel.append(1)\n    else:\n        is_sequel.append(0)\nis_sequel = pd.Series(is_sequel, name='is_sequel')\ntrain = pd.concat([train, is_sequel], axis=1)\n\ntrain['is_sequel'].describe()","26dc6ea3":"keyword_words = []\nfor i in train['Keywords']:\n    i = re.findall('[a-zA-Z \\t]+', i)\n    stopwords = ['id', 'name', ' ']\n    i = [word for word in i if word not in stopwords]\n    keyword_words.append(i)\nkeyword_words = pd.Series(keyword_words, name='keyword_words').astype(str)\ntrain = pd.concat([train, keyword_words], axis=1)\n\n# This will count the number of Keywords listed for each film\nnum_keywords = []\nfor i in keyword_words:\n    num_keywords.append(len(i.split(',')))\nnum_keywords = pd.Series(num_keywords, name='num_keywords').astype(int)\ntrain = pd.concat([train, num_keywords], axis=1)\n\ntrain['num_keywords'].describe()","cce87344":"# could use the numbers from the categories, sum them up, and then convert them to a category to target incode\nkeyword_ids = []\nfor i in train['Keywords']:\n    i = re.findall('[0-9]+', i)\n    keyword_ids.append(i)\nkeyword_ids = pd.Series(keyword_ids, name='keyword_ids')\ntrain = pd.concat([keyword_ids, train], axis=1)\ntrain.keyword_ids.head()","d7eaf9d4":"train.belongs_to_collection.head()","514f7af5":"# Extract number from belongs to collection\ncollection_id = []\nfor i in train['belongs_to_collection']:\n    i = re.findall('[0-9]+', i)\n    collection_id.append(i[:1])\ncollection_id = pd.Series(collection_id, name='collection_id').apply(lambda x: ''.join([str(i) for i in x]))\n\n# Fill in blank values with 'No Collection'\nfor i in collection_id[collection_id == ''].index:\n    collection_id.loc[i] = 'No Collection'\n\ntrain = pd.concat([train, collection_id], axis=1)\n\ntrain['collection_id'].describe()","591937c2":"# Add column with 1 for movies in a collection and 0 if not\nis_in_collection = []\nfor i in train['collection_id']:\n    if i != 'No Collection':\n        is_in_collection.append(1)\n    else:\n        is_in_collection.append(0)\n\nis_in_collection = pd.Series(is_in_collection, name='is_in_collection')\ntrain = pd.concat([train, is_in_collection], axis=1)\n\ntrain['is_in_collection'].describe()","264c1c32":"train['production_countries'].head()","80edb545":"# Create a column for production country (1 for US, 0 for rest of world)\n# It would be helpful if countries had different codes, but they all appear to be the same so it's difficult to work with\nUS_prod_country = []\nfor i in train['production_countries']:\n    if 'US' in str(i):\n        US_prod_country.append(1)\n    else:\n        US_prod_country.append(0)\nUS_prod_country = pd.Series(US_prod_country, name='US_prod_country')\ntrain = pd.concat([train, US_prod_country], axis=1)\n\ntrain['US_prod_country'].describe()","b147ebd0":"# Create column for number of production countries\nnum_production_countries = []\nfor i in train['production_countries']:\n    i = re.findall('[a-zA-Z \\t]+', str(i))\n    num_production_countries.append(str(i).count('name'))\nnum_production_countries = pd.Series(num_production_countries, name='num_production_countries')\ntrain = pd.concat([train, num_production_countries], axis=1)\n\ntrain['num_production_countries'].describe()","81335849":"# Create a column for each production company name and a column for the number of companies\nproduction_company_names = []\nnum_production_companies = []\nfor i in train['production_companies']:\n    i = re.findall('[a-zA-Z \\t]+', str(i))\n    stopwords = ['id', 'name', ' ']\n    production_company_names.append([word for word in i if word not in stopwords])\n    num_production_companies.append(str(i).count('name'))\n\nproduction_company_1 = []\nproduction_company_2 = []\nproduction_company_3 = []\nproduction_company_4 = []\nproduction_company_5 = []\nproduction_company_6 = []\nproduction_company_7 = []\nproduction_company_8 = []\n\nfor i in production_company_names:\n    try:\n        production_company_1.append(i[:][0:1])\n        production_company_2.append(i[:][1:2])\n        production_company_3.append(i[:][2:3])\n        production_company_4.append(i[:][3:4])\n        production_company_5.append(i[:][4:5])\n        production_company_6.append(i[:][5:6])\n        production_company_7.append(i[:][6:7])\n        production_company_8.append(i[:][7:8])\n    except:\n        production_company_1.append('none')\n        production_company_2.append('none')\n        production_company_3.append('none')\n        production_company_4.append('none')\n        production_company_5.append('none')\n        production_company_6.append('none')\n        production_company_7.append('none')\n        production_company_8.append('none')\n\nnum_production_companies = pd.Series(num_production_companies, name='num_production_companies')\nproduction_company_1 = pd.Series(production_company_1, name='production_company_1').apply(''.join)\nfor i in production_company_1[production_company_1 == ''].index:\n    production_company_1.iloc[i] = False\nproduction_company_2 = pd.Series(production_company_2, name='production_company_2').apply(''.join)\nfor i in production_company_2[production_company_2 == ''].index:\n    production_company_2.iloc[i] = False\nproduction_company_3 = pd.Series(production_company_3, name='production_company_3').apply(''.join)\nfor i in production_company_3[production_company_3 == ''].index:\n    production_company_3.iloc[i] = False\nproduction_company_4 = pd.Series(production_company_4, name='production_company_4').apply(''.join)\nfor i in production_company_4[production_company_4 == ''].index:\n    production_company_4.iloc[i] = False\nproduction_company_5 = pd.Series(production_company_5, name='production_company_5').apply(''.join)\nfor i in production_company_5[production_company_5 == ''].index:\n    production_company_5.iloc[i] = False\nproduction_company_6 = pd.Series(production_company_6, name='production_company_6').apply(''.join)\nfor i in production_company_6[production_company_6 == ''].index:\n    production_company_6.iloc[i] = False\nproduction_company_7 = pd.Series(production_company_7, name='production_company_7').apply(''.join)\nfor i in production_company_7[production_company_7 == ''].index:\n    production_company_7.iloc[i] = False\nproduction_company_8 = pd.Series(production_company_8, name='production_company_8').apply(''.join)\nfor i in production_company_8[production_company_8 == ''].index:\n    production_company_8.iloc[i] = False\ntrain = pd.concat([train, num_production_companies, production_company_1, production_company_2,\n              production_company_3, production_company_4, production_company_5, production_company_6,\n              production_company_7, production_company_8], axis=1)\n\ntrain.production_company_8.head()","414f7112":"# Create a column for number of spoken languages\nnum_spoken_languages = []\nfor i in train['spoken_languages']:\n    a = str(i).split()\n    num_spoken_languages.append(a.count(\"'name':\"))\nnum_spoken_languages = pd.Series(num_spoken_languages, name = 'num_spoken_languages')\ntrain = pd.concat([train, num_spoken_languages], axis=1)\n\ntrain['num_spoken_languages'].describe()","514995c3":"# Create column for release status\nstatus_is_released = []\nfor i in train['status']:\n    if i == 'Released':\n        status_is_released.append(1)\n    else:\n        status_is_released.append(0)\nstatus_is_released = pd.Series(status_is_released, name = 'status_is_released')\ntrain = pd.concat([train, status_is_released], axis=1)\ntrain['status_is_released'].describe()","8c0268ae":"def data_processing(df):\n    # Create columns for year, month, and day of week\n    df['release_date'] = df['release_date'].fillna(method='ffill')\n    df['release_date'] = pd.to_datetime(df['release_date'], infer_datetime_format=True)\n    df['release_day'] = df['release_date'].apply(lambda t: t.day)\n    df['release_weekday'] = df['release_date'].apply(lambda t: t.weekday())\n    df['release_month'] = df['release_date'].apply(lambda t: t.month)\n    # Year was being interpreted as future dates in some cases so I had to adjust some values\n    df['release_year'] = df['release_date'].apply(lambda t: t.year if t.year < 2018 else t.year -100)\n    \n    # Function that will map the average runtime for each year to movies with 0 runtie.\n    def map_runtime(df):\n        df['runtime'].fillna(0)\n    \n        run = df[(df['runtime'].notnull()) & (df['runtime'] != 0)]\n        year_mean = run.groupby(['release_year'])['runtime'].agg('mean')\n        d = dict(year_mean)\n    \n        for i in df[df['runtime'] == 0]:\n            df['runtime'] = df.loc[:, 'release_year'].map(d)\n        return df\n    df = map_runtime(df)\n    \n    # For homepage, I'll change it to 0 for NaN and 1 if a page is listed.\n    df['homepage'].fillna(0, inplace=True)\n    df.loc[df['homepage'] != 0, 'homepage'] = 1\n    \n    # For poster_path, I'll change it to 0 for NaN and 1 if a path is listed.\n    df['poster_path'].fillna(0, inplace=True)\n    df.loc[df['poster_path'] != 0, 'poster_path'] = 1\n    \n    # release_year correlates strongly with budget, so I'll use that to estimate the null values\n    def map_budget(df):\n        d = defaultdict()\n        X = df[df['budget'] != 0]\n        year_mean = pd.Series(X.groupby(['release_year'])['budget'].agg('mean'))\n        d = dict(year_mean)\n    \n        for i in df[df['budget'] == 0]:\n            df['budget'] = df.loc[:, 'release_year'].map(d)\n    \n        # In a few cases, there are only 1 or 2 movies provided from a given year and are filled with Na values\n        df.budget = df.sort_values(by='release_year').budget.fillna(method='ffill')\n        return df\n    df = map_budget(df)\n    \n    # Fill remaining Na values\n    df['belongs_to_collection'] = df['belongs_to_collection'].fillna('none')\n    df.spoken_languages = df.spoken_languages.fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")\n    df.overview = df.overview.fillna('none')\n    df.Keywords = df.Keywords.fillna('none')\n    df.production_countries = df.production_countries.fillna(\n        \"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")\n    df.genres = df.genres.fillna('18')\n    \n    ############ Feature Engineering ############\n    \n    # Create a columns for title length\n    title_len = []\n    for i in df['title']:\n        title_len.append(len(str(i).split()))\n    title_len = pd.Series(title_len, name='title_length')\n    df = pd.concat([df, title_len], axis=1)\n    \n    # Create columns for genres id's and for number of genres listed\n    genre_id = []\n    num_genre_types = []\n    for i in df['genres']:\n        i = re.findall('\\d+', str(i))\n        genre_id.append(i)\n    genre_id = pd.Series(genre_id, name='genre_id') #.apply(lambda x: ''.join([str(i) for i in x]))\n    \n    genre_id_1 = []\n    genre_id_2 = []\n    genre_id_3 = []\n    genre_id_4 = []\n    genre_id_5 = []\n    genre_id_6 = []\n    genre_id_7 = []\n\n    for i in genre_id:\n        try:\n            genre_id_1.append(i[:][0:1])\n            genre_id_2.append(i[:][1:2])\n            genre_id_3.append(i[:][2:3])\n            genre_id_4.append(i[:][3:4])\n            genre_id_5.append(i[:][4:5])\n            genre_id_6.append(i[:][5:6])\n            genre_id_7.append(i[:][6:7])\n        except:\n            genre_id_1.append('none')\n            genre_id_2.append('none')\n            genre_id_3.append('none')\n            genre_id_4.append('none')\n            genre_id_5.append('none')\n            genre_id_6.append('none')\n            genre_id_7.append('none')\n            \n    genre_id_1 = pd.Series(genre_id_1, name='genre_id_1').apply(''.join)\n    for i in genre_id_1[genre_id_1 == ''].index:\n        genre_id_1.iloc[i] = 'none'\n    genre_id_2 = pd.Series(genre_id_2, name='genre_id_2').apply(''.join)\n    for i in genre_id_2[genre_id_2 == ''].index:\n        genre_id_2.iloc[i] = 'none'\n    genre_id_3 = pd.Series(genre_id_3, name='genre_id_3').apply(''.join)\n    for i in genre_id_3[genre_id_3 == ''].index:\n        genre_id_3.iloc[i] = 'none'\n    genre_id_4 = pd.Series(genre_id_4, name='genre_id_4').apply(''.join)\n    for i in genre_id_4[genre_id_4 == ''].index:\n        genre_id_4.iloc[i] = 'none'\n    genre_id_5 = pd.Series(genre_id_5, name='genre_id_5').apply(''.join)\n    for i in genre_id_5[genre_id_5 == ''].index:\n        genre_id_5.iloc[i] = 'none'\n    genre_id_6 = pd.Series(genre_id_6, name='genre_id_6').apply(''.join)\n    for i in genre_id_6[genre_id_6 == ''].index:\n        genre_id_6.iloc[i] = 'none'\n    genre_id_7 = pd.Series(genre_id_7, name='genre_id_7').apply(''.join)\n    for i in genre_id_7[genre_id_7 == ''].index:\n        genre_id_7.iloc[i] = 'none'\n    \n    for i in genre_id.astype(str):\n        num_genre_types.append(len(i.split(',')))\n    num_genre_types = pd.Series(num_genre_types, name='num_genre_types').astype(int)\n    df = pd.concat([df, genre_id_1, genre_id_2, genre_id_3, genre_id_4, genre_id_5, \n                    genre_id_6, genre_id_7, num_genre_types], axis=1)\n    \n    # Create column for sequels\n    is_sequel = []\n    for i in df['Keywords']:\n        if 'sequel' in str(i):\n            is_sequel.append(1)\n        else:\n            is_sequel.append(0)\n    is_sequel = pd.Series(is_sequel, name='is_sequel')\n    df = pd.concat([df, is_sequel], axis=1)\n    \n    keyword_words = []\n    for i in df['Keywords']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        stopwords = ['id', 'name', ' ']\n        i = [word for word in i if word not in stopwords]\n        keyword_words.append(i)\n    keyword_words = pd.Series(keyword_words, name='keyword_words')\n    df = pd.concat([df, keyword_words], axis=1)\n\n    # This will count the number of Keywords listed for each film\n    num_keywords = []\n    for i in keyword_words:\n        num_keywords.append(len(str(i).split(',')))\n    num_keywords = pd.Series(num_keywords, name='num_keywords').astype(int)\n    df = pd.concat([df, num_keywords], axis=1)\n    \n    # Create column for Keyword Id numbers\n    keyword_ids = []\n    for i in df['Keywords']:\n        i = re.findall('[0-9]+', str(i))\n        keyword_ids.append(i)\n    keyword_ids = pd.Series(keyword_ids, name='keyword_ids')\n    #df = pd.concat([keyword_ids, df], axis=1)\n    \n    # Extract number from belongs to collection\n    collection_id = []\n    for i in df['belongs_to_collection']:\n        i = re.findall('[0-9]+', str(i))\n        collection_id.append(i[:1])\n    collection_id= pd.Series(collection_id, name='collection_id').apply(lambda x: ''.join([str(i) for i in x]))\n\n    # Fill in blank values with 'No Collection'\n    for i in collection_id[collection_id == ''].index:\n        collection_id.loc[i] = 'no collection'\n    collection_id = collection_id\n    df = pd.concat([df, collection_id], axis=1)\n    \n    # Add column with 1 for movies in a collection and 0 if not\n    is_in_collection = []\n    for i in df['collection_id']:\n        if i != 'no collection':\n            is_in_collection.append(1)\n        else:\n            is_in_collection.append(0)\n    is_in_collection = pd.Series(is_in_collection, name='is_in_collection').astype(int)\n    df = pd.concat([is_in_collection, df], axis=1)\n    \n    # Create a column for production country (1 for US, 0 for rest of world)\n    # It would be helpful if countries had different codes, but they all appear to be the same so it's difficult to work with\n    US_prod_country = []\n    for i in df['production_countries']:\n        if 'US' in str(i):\n            US_prod_country.append(1)\n        else:\n            US_prod_country.append(0)\n    US_prod_country = pd.Series(US_prod_country, name='US_prod_country')\n    df = pd.concat([df, US_prod_country], axis=1)\n    \n    # Create column for number of production countries\n    num_prod_countries = []\n    for i in df['production_countries']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        num_prod_countries.append(str(i).count('name'))\n    num_prod_countries = pd.Series(num_prod_countries, name='num_production_countries')\n    df = pd.concat([df, num_prod_countries], axis=1)\n    \n    # Create a column for each production company name and a column for the number of companies\n    production_company_names = []\n    num_production_companies = []\n    for i in df['production_companies']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        stopwords = ['id', 'name', ' ']\n        production_company_names.append([word for word in i if word not in stopwords])\n        num_production_companies.append(str(i).count('name'))\n\n    production_company_1 = []\n    production_company_2 = []\n    production_company_3 = []\n    production_company_4 = []\n    production_company_5 = []\n    production_company_6 = []\n    production_company_7 = []\n    production_company_8 = []\n\n    for i in production_company_names:\n        try:\n            production_company_1.append(i[:][0:1])\n            production_company_2.append(i[:][1:2])\n            production_company_3.append(i[:][2:3])\n            production_company_4.append(i[:][3:4])\n            production_company_5.append(i[:][4:5])\n            production_company_6.append(i[:][5:6])\n            production_company_7.append(i[:][6:7])\n            production_company_8.append(i[:][7:8])\n        except:\n            production_company_1.append('none')\n            production_company_2.append('none')\n            production_company_3.append('none')\n            production_company_4.append('none')\n            production_company_5.append('none')\n            production_company_6.append('none')\n            production_company_7.append('none')\n            production_company_8.append('none')\n\n    num_production_companies = pd.Series(num_production_companies, name='num_production_companies')\n    production_company_1 = pd.Series(production_company_1, name='production_company_1').apply(''.join)\n    for i in production_company_1[production_company_1 == ''].index:\n        production_company_1.iloc[i] = 'none'\n    production_company_2 = pd.Series(production_company_2, name='production_company_2').apply(''.join)\n    for i in production_company_2[production_company_2 == ''].index:\n        production_company_2.iloc[i] = 'none'\n    production_company_3 = pd.Series(production_company_3, name='production_company_3').apply(''.join)\n    for i in production_company_3[production_company_3 == ''].index:\n        production_company_3.iloc[i] = 'none'\n    production_company_4 = pd.Series(production_company_4, name='production_company_4').apply(''.join)\n    for i in production_company_4[production_company_4 == ''].index:\n        production_company_4.iloc[i] = 'none'\n    production_company_5 = pd.Series(production_company_5, name='production_company_5').apply(''.join)\n    for i in production_company_5[production_company_5 == ''].index:\n        production_company_5.iloc[i] = 'none'\n    production_company_6 = pd.Series(production_company_6, name='production_company_6').apply(''.join)\n    for i in production_company_6[production_company_6 == ''].index:\n        production_company_6.iloc[i] = 'none'\n    production_company_7 = pd.Series(production_company_7, name='production_company_7').apply(''.join)\n    for i in production_company_7[production_company_7 == ''].index:\n        production_company_7.iloc[i] = 'none'\n    production_company_8 = pd.Series(production_company_8, name='production_company_8').apply(''.join)\n    for i in production_company_8[production_company_8 == ''].index:\n        production_company_8.iloc[i] = 'none'\n    df = pd.concat([df, num_production_companies, production_company_1, production_company_2,\n              production_company_3, production_company_4, production_company_5, production_company_6,\n              production_company_7, production_company_8], axis=1)\n    \n    # Create a column for number of spoken languages\n    num_spoken_languages=[]\n    for i in df['spoken_languages']:\n        a = str(i).split()\n        num_spoken_languages.append(a.count(\"'name':\"))\n    num_spoken_languages = pd.Series(num_spoken_languages, name = 'num_spoken_languages')\n    df = pd.concat([df, num_spoken_languages], axis=1)\n        \n    # Create column for release status\n    status_is_released = []\n    for i in df['status']:\n        if i == 'Released':\n            status_is_released.append(1)\n        else:\n            status_is_released.append(0)\n    status_is_released = pd.Series(status_is_released, name = 'status_is_released')\n    df = pd.concat([df, status_is_released], axis=1)\n    \n    # Drop columns that have been engineered\n    df = df.drop(['belongs_to_collection', 'genres', 'Keywords', 'belongs_to_collection', 'homepage', 'imdb_id', \n                 'original_title', 'overview', 'poster_path', 'production_companies', 'production_countries',\n                 'release_date', 'spoken_languages', 'status', 'tagline', 'title', 'cast', 'crew'], axis=1)\n    # Drop 'keyword_words' column for now.  Can work with it later.\n    df = df.drop(['keyword_words'], axis=1)\n    return reduce_mem_usage(df)","4c7b95dd":"# Reload the data fresh and apply the processing function\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","b9ff5b46":"train = data_processing(train)\ntest = data_processing(test)","f47816be":"def train_target_encoded_year(df, cols):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the average revenue for each unique value from a given year.\"\"\"\n    for i in cols:\n        d = df.groupby(['release_year', i]).agg({'revenue':'mean'})\n        df = df.set_index(['release_year', i], drop=False)\n        df[i] = d.revenue\n        df = df.reset_index(drop=True)\n    return df","747a9d26":"def test_target_encoded_year(df_train, df_test, cols):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the unique average revenue per year generated from the training dataframe.\"\"\"\n    for i in cols:\n        d = df_train.groupby(['release_year', i]).agg({'revenue':'mean'})\n        df_test = df_test.set_index(['release_year', i], drop=False)\n        df_test[i] = d.revenue\n        df_test = df_test.reset_index(drop=True)\n    return df_test","f7379e3c":"def target_encode(df, target_feature, m = 300): \n    d = defaultdict()\n    target_mean = df[target_feature].mean()\n    \n    # Map values and create dictionary   \n    for cat_feature in df.select_dtypes(include='category'):\n        group_target_mean = df.groupby([cat_feature])[target_feature].agg('mean')\n        group_target_count = df.groupby([cat_feature])[target_feature].agg('count')\n        smooth = (group_target_count * group_target_mean + m * target_mean) \/ (group_target_count + m)\n        k = pd.Series(df[cat_feature])\n        v = df[cat_feature].map(smooth)\n        d[cat_feature] = dict(zip(k, v))\n        df[cat_feature] = df[cat_feature].map(smooth)\n        \n    return df, d\n","f93208de":"def test_target_encoded_year(df_train, df_test):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the unique average revenue per year generated from the training dataframe.\"\"\"\n    cols = df_test.select_dtypes(include='object').columns\n    for col in cols:\n        d = df_train.groupby(['release_year', col]).agg({'revenue':'mean'})\n        df_test = df_test.set_index(['release_year', col], drop=False)\n        df_test[col] = d.revenue\n        df_test = df_test.reset_index(drop=True)\n        \n    # There are a numerous missing values in the test set after processing so I'll fill them with the yearly avg.\n    for col in cols:\n        #d = defaultdict()\n        X = df_test[df_test[col].notnull()]\n        year_mean = pd.Series(X.groupby(['release_year'])[col].agg('mean'))\n        d = dict(year_mean)\n    \n        for i in df_test[df_test['budget'].isnull()]:\n            df_test[col] = df_test.loc[:, 'release_year'].map(d)\n    \n    return reduce_mem_usage(df_test)","21b02dd9":"# The numeric columns look okay, but budget may need normalization as the st. dev is quite large\ntrain.describe()","71a7deab":"from category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder","7cd285f7":"# Make complete list of genre ids\ngenre_ids = train['genre_id_1']\nfor i in train.loc[:, 'genre_id_2': 'genre_id_7'].columns:\n    genre_ids = pd.concat([genre_ids, train[i]], axis=0)\n\nle = LabelEncoder()\nlab_enc = le.fit_transform(genre_ids)\ngenre_ids_dict = dict(zip(genre_ids, lab_enc))\n\n# Map genre_ids_dict to genre_id columns\nfor i in train.loc[:, 'genre_id_1': 'genre_id_7'].columns:\n    train[i] = train[i].map(genre_ids_dict)","66747bdd":"train.loc[:, 'production_company_2': 'production_company_8'].head()","6c6552ba":"# Make complete list of production companies\nprod_companies = train['production_company_1']\nfor i in train.loc[:, 'production_company_2': 'production_company_8'].columns:\n    prod_companies = pd.concat([prod_companies, train[i]], axis=0)\n\nle = LabelEncoder()\nlab_enc = le.fit_transform(prod_companies)\nprod_companies_dict = dict(zip(prod_companies, lab_enc))\n\n# Map genre_ids_dict to genre_id columns\nfor i in train.loc[:, 'production_company_1': 'production_company_8'].columns:\n    train[i] = train[i].map(prod_companies_dict)","52e8444f":"le = LabelEncoder()\ntrain['collection_id'] = le.fit_transform(train['collection_id'])\ntrain['original_language'] = le.fit_transform(train['original_language'])","22091996":"from sklearn.preprocessing import LabelEncoder\ndef cat_encode(df):\n    le = LabelEncoder()\n    \n    # Make complete list of genre ids\n    genre_ids = df['genre_id_1']\n    for i in df.loc[:, 'genre_id_2': 'genre_id_7'].columns:\n        genre_ids = pd.concat([genre_ids, df[i]], axis=0)\n\n    lab_enc_genres = le.fit_transform(genre_ids)\n    genre_ids_dict = dict(zip(genre_ids, lab_enc_genres))\n\n    # Map genre_ids_dict to genre_id columns\n    for i in df.loc[:, 'genre_id_1': 'genre_id_7'].columns:\n        df[i] = df[i].map(genre_ids_dict)\n\n    # Make complete list of production companies\n    prod_companies = df['production_company_1']\n    for i in df.loc[:, 'production_company_2': 'production_company_8'].columns:\n        prod_companies = pd.concat([prod_companies, df[i]], axis=0)\n\n    lab_enc_comp = le.fit_transform(prod_companies)\n    prod_companies_dict = dict(zip(prod_companies, lab_enc_comp))\n\n    # Map genre_ids_dict to genre_id columns\n    for i in df.loc[:, 'production_company_1': 'production_company_8'].columns:\n        df[i] = df[i].map(prod_companies_dict)\n        \n    df['collection_id'] = le.fit_transform(df['collection_id'])\n    df['original_language'] = le.fit_transform(df['original_language'])\n    \n    return reduce_mem_usage(df)","9e88a630":"train = cat_encode(train)\ntest = cat_encode(test)","a0658e1e":"train.shape","0fa88d92":"# Get an idea of what correlates most strongly with revenue\nfor i in train.columns:\n    print(i, stats.pearsonr(train[i], train['revenue']))","3395d0d1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","435ca354":"X = train.drop(['id', 'revenue'], axis=1)\ny = train['revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","5041e819":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)","893b29e9":"sns.distplot((y_test-pred),bins=50)\nplt.show()","e044db9e":"def rmsle(y_true, y_pred):\n    return 'rmsle', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False","aeabfd7c":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint('RMSLE:', rmsle(y_test, pred))","1fac992d":"from lightgbm import LGBMRegressor","65c04ef5":"lr = LGBMRegressor(boosting_type='dart',num_leaves=20,max_depth=-1,min_data_in_leaf=20, learning_rate=0.2,n_estimators=500,subsample_for_bin=200000,\n                   class_weight=None,min_split_gain=0.0,min_child_weight=0.001,subsample=0.1,subsample_freq=0,colsample_bytree=0.75,reg_alpha=0.0,reg_lambda=0.0,\n                   random_state=101,n_jobs=-1)","484ed564":"lr.fit(X_train, y_train,eval_set=[(X_test, y_test)],eval_metric=rmsle)\npred = lr.predict(X_test, num_iteration=lr.best_iteration_)","91161ed5":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint('RMSLE:', rmsle(y_test, pred))","e5227eb6":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['revenue'] = lr.predict(test.drop('id', axis=1), num_iteration=lr.best_iteration_)","101f52cb":"submission.to_csv('TMDB_test_predictions.csv', index=False)","d9e93267":"I'll label encode the category columns using sklearn.","dea05011":"## EDA\nIt's likely features like budget, popularity, and release date will correlate strongly with revenue.  By contrast, features like poster path might not be helpful without extensive analysis.","4e5a952b":"I think it's useful to first use a basic linear regression model.  We can make a more complex model later.","76dac7ea":"I'll come back to budget and update the values using a linear regression approach.  But it will be helpful to have as much information as possible for other features like runtime as this might affect the total budget.","6e232e8d":"#### Dealing with categorical columns","fbd94641":"#### Model Building and Parameter Tuning","48dca41e":"## Feature Engineering"}}