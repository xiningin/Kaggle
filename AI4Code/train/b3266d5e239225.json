{"cell_type":{"229453f2":"code","42d0bb27":"code","51b6b0fc":"code","8d32661f":"code","3aeb1d35":"code","50fc60f2":"code","436b40d8":"code","fc11aee3":"code","0c805ccf":"code","a37bc54a":"code","b76287a9":"code","a2a33b70":"code","17f09d03":"code","596ad303":"code","7ed6beac":"code","e4d15630":"code","b22e26c0":"code","9b955bea":"markdown","5cf0f896":"markdown","f3a88e44":"markdown","e954ae74":"markdown","a6d4eeb9":"markdown","c2463701":"markdown","ddb83833":"markdown","9d3e4a66":"markdown","c1ec14a8":"markdown","4e799666":"markdown","18332bd1":"markdown","a0a1e2ac":"markdown","1b8fddf6":"markdown","0e805a72":"markdown","7aced71d":"markdown","1ae59692":"markdown","83e06bc7":"markdown","9c204511":"markdown","c07679c5":"markdown","8f72288f":"markdown","a90ea6d1":"markdown","74c0dc59":"markdown","aa7fd87e":"markdown","00a53abf":"markdown"},"source":{"229453f2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\n%matplotlib inline\n%config InlineBackend.figure_format='retina'","42d0bb27":"# Code to plot a function. Borrowed from fastai library.\ndef plot_func(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = np.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)","51b6b0fc":"red_bag = np.array(['Apple']*4 + ['Orange'])\nprint(f'Red Bag conatins: {Counter(red_bag)}') \n\ngreen_bag = np.array(['Orange']*9 + ['Apple'])\nprint(f'Green Bag conatins: {Counter(green_bag)}')\n\nwhite_bag = np.array(['Orange']*5 + ['Apple']*5)\nprint(f'White Bag conatins: {Counter(white_bag)}')","8d32661f":"# Define the probability of observing Appless & Oranges in red bag.\nred_ctr = Counter(red_bag)\n\nP_apple_red = red_ctr['Apple'] \/ len(red_bag)\nprint(f'P(Apple from Red Bag) : {round(P_apple_red,2)}')\n\nP_orange_red = red_ctr['Orange'] \/ len(red_bag)\nprint(f'P(Orange from Red Bag): {round(P_orange_red,2)}')","3aeb1d35":"# Define the probability of observing Appless & Oranges in green bag.\ngreen_ctr = Counter(green_bag)\n\nP_apple_green = green_ctr['Apple'] \/ len(green_bag)\nprint(f'P(Apple from Green Bag) : {round(P_apple_green,2)}')\n\nP_orange_green = green_ctr['Orange'] \/ len(green_bag)\nprint(f'P(Orange from Green Bag): {round(P_orange_green,2)}')","50fc60f2":"# Define the probability of observing Appless & Oranges in white bag.\nwhite_ctr = Counter(white_bag)\n\nP_apple_white = white_ctr['Apple'] \/ len(white_bag)\nprint(f'P(Apple from White Bag) : {round(P_apple_white,2)}')\n\nP_orange_white = white_ctr['Orange'] \/ len(white_bag)\nprint(f'P(Orange from White Bag): {round(P_orange_white,2)}')","436b40d8":"# Function to calculate surprise\ndef surprise(probability):\n    return np.log2(1\/probability)\n\n# Ploting the\nplot_func(surprise, tx='Probabity', ty='Surprise', title='Surprise vs Entropy', min=0, max=1)","fc11aee3":"# Defining the probability and getting the surprise values\nP_h = 0.9\nP_t = 0.1\n\nS_h = surprise(P_h); \nprint(f'Surprise of heads is: {round(S_h,2)}')\n\nS_t = surprise(P_t); \nprint(f'Surprise of tails is: {round(S_t,2)}')","0c805ccf":"print(f'Surprise by using the definition: {round(surprise(P_h * P_h * P_t),2)}')\nprint(f'Surprise by adding up the individual values: { round((S_h + S_h + S_t),2)} ')","a37bc54a":"table = pd.DataFrame({'Heads': [.15, .9], 'Tails': [3.32, .1]}, index=['S(x)','P(x)']); table","b76287a9":"# Entropy is the average surprise per iteration of the process\nEntropy =  table.loc['P(x)','Heads'] * table.loc['S(x)', 'Heads'] +  table.loc['P(x)', 'Tails'] * table.loc['S(x)', 'Heads']\nEntropy","a2a33b70":"# Entropy as a dot product of surprise and its associated probability.\nP_e = np.array([.9, .1])\nS_e = np.array([.15, 3.32])\n\nEntropy = np.dot(P_e, S_e)\nEntropy","17f09d03":"def entropy(arr):\n    ent = 0\n    probs = dict()\n    ctr = Counter(arr)\n    for e in ctr:\n        probs[f'P_{e}'] = ctr[e] \/ len(arr)\n    for p in probs:\n        ent += -1 * probs[p] * np.log2(probs[p])\n        return round(ent, 2)  ","596ad303":"def show_entropy(bag_type=None, bag_type_str=None):\n    ctr = Counter(bag_type)\n    print(f\"{bag_type_str} conatins: {Counter(bag_type)}\")\n    print(f\"P(Apple from {bag_type_str}) : { ctr['Apple'] \/ len(bag_type)}\")\n    print(f\"P(Orange from {bag_type_str}): {ctr['Orange'] \/ len(bag_type)}\")\n    print(f\"Entropy of the {bag_type_str} is: {entropy(bag_type)}\")","7ed6beac":"show_entropy(bag_type = red_bag, bag_type_str='Red Bag')","e4d15630":"show_entropy(bag_type = green_bag, bag_type_str='Green Bag')","b22e26c0":"show_entropy(bag_type = white_bag, bag_type_str='White Bag')","9b955bea":"The entropy for white bag is 1. In this case the probability and surprise have the same values for both the fruits.","5cf0f896":"Since, the white bag has equal number of apples and oranges, we would be equally surprised, irrespective of whether we pick up and apple or orange.","f3a88e44":"The green bag has a lot more oranges than apples. Hence we would not be very surprised if we picked up an orange from this bag. On the other hand, if we picked up a apple, that would be relatively surprising.","e954ae74":"Entropy is used for a lot of purposes in Data Science.\n* It can be used to build classiification trees.\n* It is the basis for Mutual Information which quantified the relation between two things.\n* It is the basis of Relative Entropy(aka the Kullback-Leibler Distance) and Cross Entropy.\n\nAll of the above use entropy or something derived from it to quantify similarities and differences. Let's learn how entropy quantifies similarities and differences","a6d4eeb9":"Let's calculate the surprise of getting 2 heads and 1 tail. It turns out that it is same adding the surprise of 2 heads and 1 tail as seen in the snippets below.","c2463701":"## Entropy as the expected value of surprise ","ddb83833":"## Entropy in Action \ud83c\udfac","9d3e4a66":"Let's assume that we have a biased coin with $P(H)$ = 0.9 and $P(T)$ = 0.1. We can calculate the surprise of heads and tails as follows","c1ec14a8":"The entropy for the red bag turns out to be 0.72. It is closer to the probability of picking an apple from the red bag because there are more apples in the red bag as compared to oranges.","4e799666":"From the above equation, it can be observed that the Entropy can also be represented as the dot product of the probability of observing certain events and their associated surprise.","18332bd1":"# Demystifying Entropy in ML \ud83d\udc68\u200d\ud83d\udcbb \n\n> In this post, I summarize my understing of what is entropy means in Machine Learning using some code examples. This post is a documentation of my understanding of the awesome [video](https:\/\/www.youtube.com\/watch?v=YtebGVx-Fxw) from [Statquest](https:\/\/twitter.com\/joshuastarmer).   ","a0a1e2ac":"## Summary\n\nWe use entropy to quantify the similarity or difference between two groups composed of a certain types of objects.Entropy of a group is highest when we have the same number of objects belonging to different classes in that group and it decreases as the difference between the objects of different classes increases.","1b8fddf6":"Since the number of apples in the red bag  is more, the probability of picking up an apples  from the red bag is higher. Hence it would not be very surprising, if we picked up an apples from the red bag. In contrast, if we picked up an orange from the red bag, we would be relatively more surprised.","0e805a72":"Looking at the above examples, we can observe the relationship between surprise and probability.\n* When the probability of picking an apple from the red bag is high, the surprise is low.\n* When the probability of picking an apple fronm the green bag is low, the surprise is high. \n\nHence we can conclude the Surprise is inversely related to probability of an event.","7aced71d":"Let's say we have three bags \ud83d\udcbc  that are red, green, and blue in color. All these bags contain Apples \ud83c\udf4e  and Oranges \ud83c\udf4a in different quantities. The task at hand is to quantify how similar or different these bags are from each other in terms of their composition using `Entropy` as a metric. ","1ae59692":"The entropy for the green bag turns out to be 0.47. This makes sense because green bag has a higher probability of picking a fruit with lower surprise.","83e06bc7":" $$Entropy = \\sum \\frac{1}{log(p(x))} \\ast p(x)$$","9c204511":"## Why are we discussing Entropy?","c07679c5":"## Introduction to Surprise \ud83d\ude32: Bags, Apples & Oranges \ud83d\udcbc \ud83c\udf4e \ud83c\udf4a ","8f72288f":"## Utility Code","a90ea6d1":"## Defining Surprise Mathematically \ud83e\uddee ","74c0dc59":"Due to the inverse relationship between surprise and probability, we define surprise as the log of the inverse of the probability. Based on this definition, the surprise is zero when the probability is 1 and surprise is undefined when the probability is zero, which makes sense. We can see in the plot that probability increases surprise decreases and becomes zero when probability equals one. On the other hand, as probability nears zero, surprise tends to large values.","aa7fd87e":"The surprise for tails is much larger than surprise of heads due to the inverse relationship.","00a53abf":"## Calculating surprise for a series of events "}}