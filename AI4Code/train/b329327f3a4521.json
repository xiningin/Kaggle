{"cell_type":{"c2ce6bf9":"code","0f87a570":"code","0d51f190":"code","396d21e5":"code","1ab4c1ef":"code","680c14ec":"code","07c2fdc1":"code","658b64a9":"code","0a62ab9e":"code","c1f53990":"code","0bea339e":"code","7d6ccd50":"code","0efaaba1":"code","eac3e852":"code","03fd91f5":"code","bd0e9761":"code","e72b0e93":"code","047413c7":"code","b9a6b712":"code","436153f7":"code","a71fbecf":"code","9ed79af2":"code","ada0f24c":"code","c31435a1":"code","25905e93":"code","ce5b1c64":"code","75bd2431":"markdown","962d9da2":"markdown","57f56ec9":"markdown","8173b18c":"markdown","79da92f7":"markdown","38d5d6b7":"markdown","58664a62":"markdown","e060fc18":"markdown","a7efc56b":"markdown","39f2f778":"markdown","6437cebc":"markdown","e32b6cdd":"markdown"},"source":{"c2ce6bf9":"# import the important libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","0f87a570":"# import the dataset\ndata=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","0d51f190":"# check the shape of the dataset\ndata.shape","396d21e5":"# we will check if there are any null values in the dataset\ndata.isnull().sum()","1ab4c1ef":"data.info()","680c14ec":"# we can check that how many values are present in the 'class' having values as 0 or 1\ndata['Class'].value_counts()","07c2fdc1":"# dividing the dataframe into fraud and non fraud data\nnon_fraud=data[data['Class']==0]\nfraud=data[data['Class']==1]","658b64a9":"non_fraud.shape, fraud.shape","0a62ab9e":"# now we are going to select the 492 non-fraud entries from the dataframe \nnon_fraud=non_fraud.sample(fraud.shape[0])\nnon_fraud.shape","c1f53990":"data=fraud.append(non_fraud, ignore_index=True)\ndata","0bea339e":"# now let us again check the value counts\ndata.Class.value_counts()","7d6ccd50":"# now dividing the dataframe into dependent and independent varaible\nX=data.drop(['Class'], axis=1)\ny=data.Class\n\n# check the shape\nX.shape, y.shape","0efaaba1":"# we will divide the dataset into training and testing dataset\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=99)\n\n# check the shape again\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","eac3e852":"X_train","03fd91f5":"# scaler\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","bd0e9761":"X_train,y_train","e72b0e93":"y_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","047413c7":"X_train.shape","b9a6b712":"X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n\n# check the shape again\nX_train.shape, X_test.shape","436153f7":"# import the libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Conv1D,BatchNormalization,Dropout","a71fbecf":"# import model\nmodel=Sequential()\n# layers\nmodel.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=X_train[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# build ANN\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='relu'))","9ed79af2":"# summary\nmodel.summary()","ada0f24c":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","c31435a1":"%%time\n# fitting the model\nhistory=model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))","25905e93":"# plot\ndef plot_learningcurve(history,epochs):\n  epoch=range(1,epochs+1)\n  # accuracy\n  plt.plot(epoch, history.history['accuracy'])\n  plt.plot(epoch, history.history['val_accuracy'])\n  plt.title('Model accuracy')\n  plt.xlabel('epoch')\n  plt.ylabel('accuracy')\n  plt.legend(['train','val'], loc='upper left')\n  plt.show()\n\n  # loss\n  plt.plot(epoch, history.history['loss'])\n  plt.plot(epoch, history.history['val_loss'])\n  plt.title('Model loss')\n  plt.xlabel('epoch')\n  plt.ylabel('loss')\n  plt.legend(['train','val'], loc='upper left')\n  plt.show()","ce5b1c64":"plot_learningcurve(history,20)","75bd2431":"Now we will build our model","962d9da2":"We can see that our data is highly varied, we need to scale our data.\n\nWe will scale our data using Standard Scaler","57f56ec9":"y_train,y_test are in series , we will convert the same into an array","8173b18c":"Now we can say that the data is balanced now and concated","79da92f7":"Here there are legitimate 284315 transactions and the transactions which are fraud are only 492.\n\nSo we can say that data is highly imbalanced, we will balance the dataset\n\n* Balancing the dataset ","38d5d6b7":"Here there are 31 columns and the last column is the target variable named Class","58664a62":"We can see that there is no null value in the dataset\n\nWe will check the datatype of the values in all columns","e060fc18":"from observing the dataset and from the info, we can see that our target column consists of values consisting 0 and 1.","a7efc56b":"Here we will have to change the shape so that our model can process the data","39f2f778":"We will perform CNN on Credit Card Dataset to check whether the transactions are fraud or not.","6437cebc":"Now that we have data that is balanced, we will merge the dataset of non_fraud and fraud transactions","e32b6cdd":"We are going to make the non_fraud data frame of the same size of the fraud.shape to match the entries, "}}