{"cell_type":{"df26cce9":"code","935b6f10":"code","c3d17bdd":"code","2f5d53c2":"code","7f069c5f":"code","f259bd91":"code","73c1fa3a":"code","37ba4fae":"code","e5ec1201":"code","b80c94fc":"code","31c3c8ab":"code","057811d7":"code","864f22f1":"code","974380e5":"code","7fa2c328":"code","142fba81":"code","a2a57df8":"code","1927676c":"code","bc0d36dd":"code","bab67869":"code","c0a3f2a3":"code","2433014f":"code","f5faa16d":"code","4f030228":"code","1f83f564":"code","f1b83f81":"code","e974cb80":"code","9b55f209":"code","e5ad56a8":"code","ab9229b1":"code","ee5b47fc":"code","1b310df6":"code","2e3baee5":"code","f599020a":"code","ef2073d5":"code","d3fff6a4":"code","dfc47fea":"code","85013d8a":"code","5cac2f9f":"code","d09d84bd":"code","adc75fd4":"code","f322e66a":"code","fdda4039":"code","c23d79a6":"code","5530a449":"code","01b63d04":"code","f47f9770":"code","36633824":"code","39d3a1f7":"code","a3c17e5e":"code","c1b607b6":"code","61030bb9":"code","ee4c5457":"code","16cb3b6e":"code","5ae9b4ce":"code","14a1d5d9":"code","21a83fab":"code","699caa67":"code","c9e13ff4":"code","d481e858":"code","9598c14b":"code","b0186c95":"code","c565e999":"code","a4584d3a":"code","27d1eca0":"code","53f2496c":"code","8262b98d":"markdown","9eef2386":"markdown","96e38152":"markdown","5c5585ab":"markdown","c652dba7":"markdown","e6890e2d":"markdown","dd5d9602":"markdown","d850af13":"markdown","7935e086":"markdown","757fd889":"markdown","bcfb58c1":"markdown","2c32deae":"markdown","94084df5":"markdown","4dfc95f5":"markdown","8fd2873f":"markdown","6d2fd316":"markdown","7d48337b":"markdown","828ed2c0":"markdown","1158eb54":"markdown","b1dc5f8f":"markdown","7ac00dd0":"markdown","9b2531ba":"markdown","46452772":"markdown","1914f800":"markdown","bfb7b046":"markdown","e047ecaa":"markdown","fa92da97":"markdown","8d82d261":"markdown","90d3c2e7":"markdown","ec4aab78":"markdown","19043fba":"markdown","c3920a44":"markdown","7317bea2":"markdown","2f46a3cf":"markdown","ac1de143":"markdown","97eb82cb":"markdown","8c142ea1":"markdown","b869094a":"markdown","934cda96":"markdown","961257ee":"markdown","312fc8a1":"markdown","5d8e9572":"markdown","5d40c4f0":"markdown","b6254839":"markdown","47de811b":"markdown","7a2eae2f":"markdown","80848ebb":"markdown"},"source":{"df26cce9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","935b6f10":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c3d17bdd":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","2f5d53c2":"train.head()","7f069c5f":"test.head()","f259bd91":"# numeric vars\nnum_cols = set(train._get_numeric_data().columns)","73c1fa3a":"# numeric vars\nnum_cols = set(train._get_numeric_data().columns)","37ba4fae":"# nominal vars\nnominal_vars = [\n    'MSZoning', 'LandContour', 'Utilities',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n    'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', \n    'GarageType', 'MiscFeature', \n    'SaleType', 'SaleCondition']","e5ec1201":"# ranking vars\nranking_vars = [\n    'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual',\n    'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'Street', 'Alley',\n    'LandSlope', 'Functional', 'GarageFinish', 'MoSold', 'YrSold', 'PavedDrive', \n    'CentralAir', 'LotShape', 'MSSubClass', ]","b80c94fc":"# continuous vars\ncontinue_vars = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n    'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n    'BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'BedroomAbvGr', 'KitchenAbvGr',\n    'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'GarageYrBlt', 'YearBuilt', 'YearRemodAdd',\n    'OverallQual']","31c3c8ab":"train[continue_vars].describe()","057811d7":"print(f'skew: {train.SalePrice.skew()}')\nprint(f'kurt: {train.SalePrice.kurt()}')\nsns.distplot(train.SalePrice, fit=norm)\nf = plt.figure()\nstats.probplot(train.SalePrice, plot=plt)\nplt.show()","864f22f1":"train.SalePrice = np.log1p(train.SalePrice)\n\nprint(f'skew: {train.SalePrice.skew()}')\nprint(f'kurt: {train.SalePrice.kurt()}')\nsns.distplot(train.SalePrice, fit=norm)\nf = plt.figure()\nstats.probplot(train.SalePrice, plot=plt)\nplt.show()","974380e5":"X_total = pd.concat((train, test)).drop(['SalePrice'], axis=1)","7fa2c328":"f, ax = plt.subplots(6, 6, figsize=(25, 20))\nfor i, c in enumerate(continue_vars):\n    sns.distplot(X_total[c], fit=stats.norm, ax=ax[i\/\/6, i%6])","142fba81":"f, ax = plt.subplots(5, 5, figsize=(25, 15))\nfor i, c in enumerate(nominal_vars):\n    g = sns.barplot(data=pd.DataFrame(X_total[c].value_counts()).reset_index(), x='index', y=c, ax=ax[i\/\/5, i%5])\n    g.set(xticks=[])\n    g.set(title=c)","a2a57df8":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = train.corr()\nsns.heatmap(corr)\nplt.show()","1927676c":"f, ax = plt.subplots(figsize=(8, 7))\ntop_corr = train[corr.SalePrice.sort_values(ascending=False)[:10].index].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","bc0d36dd":"sns.boxplot(data=train[['SalePrice', 'OverallQual']], x='OverallQual', y='SalePrice')\nplt.show()","bab67869":"train[['SalePrice', 'GrLivArea']].plot.scatter(x='SalePrice', y='GrLivArea')\nplt.show()","c0a3f2a3":"sns.pairplot(train[corr.SalePrice.sort_values(ascending=False)[:10].index])\nplt.show()","2433014f":"# delete ID\ntrain_id = train.Id\ntest_id = test.Id\ntest_idx = test.index\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","f5faa16d":"train.drop(train.GrLivArea.sort_values(ascending=False)[:2].index, axis=0, inplace=True)","4f030228":"train[['SalePrice', 'GrLivArea']].plot.scatter(x='SalePrice', y='GrLivArea')\nplt.show()","1f83f564":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train['SalePrice'].values\nall_data = pd.concat((train, test), axis=0).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","f1b83f81":"count = all_data.isnull().sum()\npercent = (count \/ all_data.isnull().count() * 100).sort_values(ascending=False)\nmissing_table = pd.DataFrame({'percent': percent})\nmissing_table.head(10)","e974cb80":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=percent.index[:30], y=percent[:30])\nplt.title('missing percent', fontsize=15)\nplt.xlabel('features', fontsize=15)\nplt.ylabel('percent', fontsize=15)\nplt.show()","9b55f209":"# specific value replacements\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","e5ad56a8":"# mean value replacements\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","ab9229b1":"all_data.drop(['Utilities'], axis=1, inplace=True)","ee5b47fc":"# Complete\nall_data.isnull().sum().sum()","1b310df6":"all_data.describe().T[(all_data.describe().T)['min'] == 0].index","2e3baee5":"all_data['HasMasVnr'] = all_data.MasVnrArea.apply(lambda x: 1 if x else 0)\nall_data['Has2ndFlrSF'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x else 0)\nall_data['HasGarageArea'] = all_data['GarageArea'].apply(lambda x: 1 if x else 0)\nall_data['HasWoodDeckSF'] = all_data['WoodDeckSF'].apply(lambda x: 1 if x else 0)\nall_data['HasOpenPorchSF'] = all_data['OpenPorchSF'].apply(lambda x: 1 if x else 0)\nall_data['HasEnclosedPorch'] = all_data['EnclosedPorch'].apply(lambda x: 1 if x else 0)\nall_data['Has3SsnPorch'] = all_data['3SsnPorch'].apply(lambda x: 1 if x else 0)\nall_data['HasScreenPorch'] = all_data['ScreenPorch'].apply(lambda x: 1 if x else 0)\nall_data['HasPoolArea'] = all_data['PoolArea'].apply(lambda x: 1 if x else 0)\nall_data['HasMiscVal'] = all_data['MiscVal'].apply(lambda x: 1 if x else 0)\n\nall_data['TotalBath'] = all_data['BsmtFullBath'] + all_data['BsmtHalfBath'] * 0.5\\\n+ all_data['FullBath'] + all_data['HalfBath'] * 0.5\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n#all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['2ndFlrSF']\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'] + all_data['BsmtFinSF1'] \\\n+ all_data['BsmtFinSF2']","f599020a":"nominal_vars = list(set(nominal_vars) - set(['Utilities']))","ef2073d5":"all_data[nominal_vars] = all_data[nominal_vars].astype(str)\nall_data[ranking_vars] = all_data[ranking_vars].astype(str)","d3fff6a4":"from sklearn.preprocessing import LabelEncoder\n\nfor c in ranking_vars:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","dfc47fea":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()","85013d8a":"skewness = skewness[abs(skewness) > 0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","5cac2f9f":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","d09d84bd":"X_train = all_data[:ntrain]\nX_test = all_data[ntrain:]","adc75fd4":"from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor as rfr, GradientBoostingRegressor as gbr\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport optuna\nfrom functools import partial","f322e66a":"n_folds = 5\n\ndef rmsle(model, x, y):\n    return np.sqrt(mean_squared_error(y, model.predict(x)))\n\ndef rmsle_cv(model, x, y):\n    #kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    kf = 5\n    rmse =np.sqrt(-cross_val_score(model, x.values, y, scoring='neg_mean_squared_error', cv=kf))\n    return rmse","fdda4039":"pipe_lasso = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', Lasso())\n])\npipe_enet = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', ElasticNet(max_iter=5000))\n])\npipe_krr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', KernelRidge())\n])\n\ngrid_param_lasso = [{\n    'model__alpha': 0.0001 * np.arange(1, 100)\n}]\ngrid_param_enet = [{\n    'model__alpha': 0.0001 * np.arange(1, 100),\n    'model__l1_ratio': 0.001 * np.arange(1, 10)\n}]\ngrid_param_krr = [{\n    'model__alpha': 0.0001 * np.arange(1, 100),\n    'model__degree': [1, 2, 3],\n    'model__alpha': [0.6],\n    'model__kernel': ['polynomial'],\n    'model__coef0': [2.5]\n}]","c23d79a6":"best_params = {\n    'Lasso': None,\n    'ElasticNet': None,\n    'Kernel Ridge': None\n}","5530a449":"search_lasso = GridSearchCV(pipe_lasso, grid_param_lasso, scoring='neg_mean_squared_error', n_jobs=-1).fit(X_train, y_train)\nbest_params['Lasso'] = search_lasso.best_params_","01b63d04":"search_enet = GridSearchCV(pipe_enet, grid_param_enet, scoring='neg_mean_squared_error', n_jobs=-1).fit(X_train, y_train)\nbest_params['ElasticNet'] = search_enet.best_params_","f47f9770":"search_krr = GridSearchCV(pipe_krr, grid_param_krr, scoring='neg_mean_squared_error', n_jobs=-1).fit(X_train, y_train)\nbest_params['Kernel Ridge'] = search_krr.best_params_","36633824":"best_params","39d3a1f7":"model_lasso = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', Lasso(alpha=0.0005))\n])\n\nmodel_enet = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', ElasticNet(alpha=0.0089, l1_ratio=0.009000000000000001, random_state=3))\n])\n\nmodel_krr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', KernelRidge(alpha=0.6,\n                        kernel='polynomial',\n                        degree=2,\n                        coef0=2.5))\n])","a3c17e5e":"def objective_xgbr(trial, X, y):\n    param = {\n        'n_estimators': 2000,\n        'max_depth': trial.suggest_int('max_depth', 3, 11),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.01),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 100),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 100),\n        'subsample': trial.suggest_categorical('subsample', list(np.arange(0.4, 1.1, 0.1))),\n        'colsample_bylevel': trial.suggest_categorical('colsample_bylevel', list(np.arange(0.4, 1.1, 0.1))),\n        'n_jobs': -1\n    }\n    model = XGBRegressor(**param)\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    train_scores, test_scores = [], []\n    for train_idx, test_idx in kf.split(X_train):\n        X_tmp_train, X_tmp_test = X_train.iloc[train_idx, :], X_train.iloc[test_idx, :]\n        y_tmp_train, y_tmp_test = y_train[train_idx], y_train[test_idx]\n        model.fit(X_tmp_train, y_tmp_train,\n                 eval_metric=['rmse'], eval_set=[(X_tmp_test, y_tmp_test)],\n                 early_stopping_rounds=30, verbose=0,\n                 callbacks=[optuna.integration.XGBoostPruningCallback(trial, observation_key='validation_0-rmse')])\n        train_score = np.sqrt(mse(y_tmp_train, model.predict(X_tmp_train)))\n        test_score = np.sqrt(mse(y_tmp_test, model.predict(X_tmp_test)))\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    train_score = np.array(train_scores).mean()\n    test_score = np.array(test_scores).mean()\n    print(f'train score: {train_score}')\n    print(f'test score: {test_score}')\n    return test_score","c1b607b6":"optimizer = partial(objective_xgbr, X=X_train, y=y_train)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(optimizer, n_trials=100)","61030bb9":"optuna.visualization.plot_optimization_history(study)","ee4c5457":"optuna.visualization.plot_slice(study)","16cb3b6e":"study.best_params","5ae9b4ce":"model_xgbr = XGBRegressor(colsample_bytree=0.4, learning_rate=0.00898718134841855, max_depth=8, \n                             n_estimators=2200, reg_alpha=0.036142628805195254, reg_lambda=0.03188665185506858,\n                             subsample=0.6, random_state =42)","14a1d5d9":"model_gbr = gbr(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5)","21a83fab":"model_lgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","699caa67":"stack_gen = StackingCVRegressor(regressors=(model_lgbm, model_lasso, model_enet, model_krr, model_gbr),\n                               meta_regressor=model_xgbr,\n                               use_features_in_secondary=True)","c9e13ff4":"models = [\n    model_lasso, model_enet, model_krr, model_gbr, model_xgbr, model_lgbm\n]\ncross_score = {\n    'Lasso': 0,\n    'ElasticNet': 0,\n    'Kernel Ridge': 0,\n    'GradientBoosting': 0,\n    'XGBoost': 0,\n    'LightGBM': 0,\n}\n\nfor idx, model in enumerate(models):\n    cross_score[list(cross_score.keys())[idx]] = rmsle_cv(model, X_train, y_train).mean()","d481e858":"cross_score","9598c14b":"for model in models:\n    model = model.fit(X_train, y_train)","b0186c95":"stack_gen = stack_gen.fit(X_train, y_train)","c565e999":"def blend(X):\n    return ((0.15 * model_lasso.predict(X)) + \\\n            (0.15 * model_enet.predict(X)) + \\\n            (0.05 * model_krr.predict(X)) + \\\n            (0.15 * model_xgbr.predict(X)) + \\\n            (0.15 * model_lgbm.predict(X)) + \\\n            (0.35 * stack_gen.predict(np.array(X))))","a4584d3a":"np.sqrt(mse(y_train, blend(X_train)))","27d1eca0":"np.sqrt(mse(y_train, stack_gen.predict(X_train)))","53f2496c":"sub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = score = np.expm1(blend(X_test))\nsub.to_csv('submission.csv',index=False)","8262b98d":"* I create a stacking model.","9eef2386":"* The above missing values are continuous variables and have been replaced with the poorest values.\n* Lot Frontage' will be similar to the value of nearby houses, so it has been replaced with a median value.","96e38152":"* You can also think of numeric variables as continuous variables and other variables as categorical variables.\n* However, there may be categorical variables among numeric variables.\n* I thought the most obvious way to distinguish them is to read the variable manual and distinguish them directly.","5c5585ab":"* All of the above missing values were replaced with specific values. The reason is that we found that their missing values mean Nan or some specific value.","c652dba7":"# 2) HyperParameter Tuning: Pipeline + GridSearch(+Optuna) + Cross Validation\n* Build pipelines for Laso, ElasticNet, and Kernel Ridge and parameters to use for grid search.","e6890e2d":"* Create a function that calculates the cross validation score.","dd5d9602":"# XGBoost HyperParameters","d850af13":"* Criteria for processing missing values include deletion and replacement.\n* 'Delete method' deletes the columns in which missing values exist in the simplest and cleanest way.\n* In this competition, Rows should not be deleted because missing values may exist in the test data.\n* The condition that a column can be deleted is that more than 15% of the column is missing or does not affect the analysis at all.\n* replacements is an option that should be considered before deletion.\n* Simple replacements are again specific value replacements and mean value replacements (average, median, mod, regression), and simple probability replacements (hot deck, cold deck).\n* 'specific value replacements' is possible when there is background knowledge that missing values have some meaning.\n* It can be applied mainly to categorical variables. ex) All other values are recorded, but only specific values are recorded.\n* In this case, these values usually correspond to special cases or negative meanings or it may be a value that may be reluctant to check.\n* In this case, the rate of missing values is usually abnormally high. Therefore, if you read the explanation of the variable and fall under the above situation, It replaces it with a specific value.\n* The mean value replacement is applied to continuous variables and is used when missing values can be inferred through values of other variables.","7935e086":"#  The final result","757fd889":"* 'Utilities', 'Condition2', 'RoofMatl', 'Heating' are biased variables.\n* They are removed because they are variables that did not significantly affect the analysis.\n* GarageArea, 1stFlrSF are also removed (High correlation coefficient).","bcfb58c1":"* I selected only the top 10 variables with the highest correlation with the dependent variable and conducted the correlation analysis again.\n* I selected only the top 10 variables with the highest correlation with the dependent variable and conducted the correlation analysis again.\n* \"OverallQual\" and \"GrLiv Area\" are the strongest variables.\n* Garage Cars and Garage Area are also highly correlated. High correlation between independent variables is not good because it causes multicollinearity.\n* Among them, I drew a box plot and scatterplot using 'OverallQual' and 'GrLiv Area', which show the highest correlation with the dependent variable.\n* I understand that they have a strong linear relationship with the dependent variable. So I had an expectation that I could find an outlier.","2c32deae":"* Let's draw a scatterplot again. It's clean.","94084df5":"* Grid search is conducted using each pipeline and parameter, and optimal hyperparameters for each model are stored.","4dfc95f5":"* Next, the two outliers identified in the scatter plot were removed.","8fd2873f":"# Grid search results are as follows.","6d2fd316":"* Among categorical variables, there are variables indicating order or ranking.\n* They perform label encoding instead of one hot encoding. Then, the meaning of ranking can be saved.\n* The following variables correspond to this.","7d48337b":"# Description\nThe table of contents is as follows.\n\nData search:\n\n1) Univariate search:\n\n2) Bivariate search:\n\nPre-processing.\n\n1) Cleansing - Outliers..\n\n2) Cleansing - Processing missing values.\n\n3) Derivative variable generation\n\n4) Change variables.\n\n5) Dummyization.\n\nModeling.\n\n1) Create a basic model.\n\n2) HyperParameter Tuning: Pipeline + GridSearch(+Optuna) + Cross Validation\n\n3) Model learning.\n\n4) Ensemble: Stacking and blending.\n\n5) The final result.","828ed2c0":"* \"skew\" means skewness and \"kurt\" means kurtosis.\n* Skewness represents the bias of the distribution, and kurtosis represents the sharpness of the distribution.\n* I thought about why to check these indicators and concluded that it was because it was an indicator that could judge the normality of the variable.\n* Variables, especially dependent variables, must follow normality to ensure that the model performs well. The absolute value of skewness is 0-3 and kurtosis is 1-8.\n* The histogram is a good chart to check the distribution of variables.\n* The qq plot is a good chart for determining normality, and the point must follow the red diagonal.\n* Currently, it is difficult to judge that the dependent variable is perfectly normal.","1158eb54":"* Create a model using grid search results.","b1dc5f8f":"# Checking Distributions\n* First, we checked the dependent variable 'SalePrice'.","7ac00dd0":"# 5) Dummyization\n* For all categorical types (excluding order variables), get_dummies, which automatically allow one-hot encoding, are applied.","9b2531ba":"* We can also observe the results for each hyperparameter.","46452772":"# 3) Derivative variable generation\n* Derivative variables are methods that can improve the quality of analysis. There are several ideas for how to generate derivative variables.\n* It is a continuous variable and means an observation of an object, and if this value is 0, it means that there is no object.\n* Therefore, it is possible to add categorical variables as to whether or not the object is present. Categorical variables with binary values can be stored as 0 and 1.\n* If you can express continuous variables of the same series in association, use a four-line operation.\n* New variables can be created. It is similar to calculating BMI with height and weight.\n* The above judgment can be used using min and max values after describe.","1914f800":"# 3. Modeling\n## 1) Create a basic model\n* Create basic models for analysis.\n* Use lasso, elasticNet, KernelRidge, GradientBoosting, XGBoost, LightGBM","bfb7b046":"# 4) Change variables\n### Categorical variables must proceed with 'dummy' work.\n* The function used at this time is 'get_dummies'.\n* This function should be applied to all categorical variables.\n* To do this, all categorical variables must be 'str'. Therefore, the types of numerical categorical variables are changed earlier.","e047ecaa":"## Reading DataSet","fa92da97":"# Import some necessary librairies","8d82d261":"* In this case, log conversion is a solution.\n* Log transformation allows variables to follow normality","90d3c2e7":"* If you look at the heat map, you can judge that it has a high correlation if it is close to white or black.\n* I paid attention to variables that have a high correlation with the dependent variable.\n* Our purpose is to match the value of the dependent variable, and I thought that variables that have a high correlation with the dependent variable would have important characteristics to solve the problem.","ec4aab78":"# Loading Data ","19043fba":"### The hyperparameters of tree models are diverse and have many combinations. Their optimization takes a lot of time.\n* So, I looked for XGBoost's hyperparameters using the Optuna package using early stopping and cross validation.\n* The hyperparameters of tree models are diverse and have many combinations. Their optimization takes a lot of time.\n* I looked for XGBoost's hyperparameters using the Optuna package using early stopping and cross validation.\n* Preparations: 'optuna', 'functions-partial', objective function\n\n#### Pre-understanding:\n* Optuna is a framework that helps optimize hyperparameters. An objective function is required.\n* Optuna's objective function selects a new hyperparameter combination of the model every trial.\n* Optuna's study object is an object that performs optimization. The optimization of the study object requires a partial object and the number of attempts.\n* The partial object is an object that binds X, y with the objective function to be used by optuna.\n* Study objects store results that meet the purpose for each trial. Finally, remember the most purposeful hyperparameter combination.\n* The trial factor of objective embeds the function of specifying the range and value of hyperparameters. It has a hyperparameter name, range or list as a factor in common.\n* Suggest_int: Select an integer value within the range.\n* Suggest_uniform: Select an equal distribution value within a range.\n* Suggest_discrete_uniform: Select a discrete uniform distribution value within the range.\n* Suggest_loguniform: Select a logarithmic function linear value within a range.\n* Suggest_category: Select a value in the list.","c3920a44":"* I weighted each model for the final result.\n* I previously gave a higher weight to the model with excellent cross-validation performance.","7317bea2":"# 3) Model learning.\n* Cross validation is conducted for all models.","2f46a3cf":"* I haven't done grid search for Gradient Boosting and LightGBM yet (I referred to other notebooks).","ac1de143":"# 2. Pre-processing\n* 1) Cleansing - Outliers\n* First, I removed the \"ID\" that is not an outlier but is not needed for analysis.","97eb82cb":"* This time, we checked basic statistics for continuous variables.\n* I found that the minimum value of some variables is zero.","8c142ea1":"# 1) Univariate search\nIt is very important to understand the meaning, type, and shape of each variable.\n\nSo I simply divided it into numeric variables and other variables.\n\nI used the '_get_numerical_data' function of numpy to find out the numeric variable.","b869094a":"# 4) Ensemble: Stacking and blending.\n* Learn the stacking model.","934cda96":"* Now, the final learning is conducted using the training data.","961257ee":"* We're going to change the boxcox for the variables selected above. Reduce skewness.","312fc8a1":"* Oh Let's look at the scatter plot with GrLiv Area.\n* We can see that the two points with the largest GrLiv Area are completely out of the linear relationship.\n* Draw pair plot!","5d8e9572":"# 2) Bivariate search\n* Now, we have explored the two variables.\n* First, a correlation analysis was conducted to confirm the linear relationship between variables.\n* The 'corr' function helps you do this.\n* Seaborn's heatmap is a chart that helps visualize the results of correlation analysis.","5d40c4f0":"# 2) Cleansing - Processing missing values\n* Log conversion of dependent variables and removal of outliers in training data are over.\n* There is no need to stay in the training data anymore.\n* Therefore, we separate the dependent variable and combine training X and test X.","b6254839":"* Some variables seem to be able to follow a normal distribution through log transformation or box cox transformation. Some variables have zero. It seems that +1 should be done when converting.\n* Draw a bar chart for categorical variables.\n* Some variables were extremely biased toward one value (0). So they don't seem to be very important variables.","47de811b":"* We used Optuna to find XGBoost's optimal hyperparameters.\n* We can check the RMSE by XGBoost's attempt.","7a2eae2f":"# Concating","80848ebb":"* It is advantageous for analysis that all continuous variables follow normality.\n* We select variables with high skewness values."}}