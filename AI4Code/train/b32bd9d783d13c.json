{"cell_type":{"d319d53c":"code","20412edf":"code","71be67c0":"code","bd363d50":"code","8763de4d":"code","8be447da":"code","ff493d76":"code","90c4595e":"code","1cd8fb36":"code","1cdd9cae":"code","a6524c73":"code","43d45fd8":"code","e8480421":"code","3c9bea57":"code","4f73ba19":"code","ab276c18":"code","a0e2a350":"code","be39e2da":"code","d578017b":"code","dc9a4e5e":"code","3ddea687":"code","e71c14af":"code","8a3a789f":"code","8dd69b08":"code","c491a85d":"code","38e0a29b":"code","8464e4ae":"code","3b76e365":"code","9d6f5860":"code","886425ab":"code","7a9a9421":"code","a0313883":"code","89a3c9cd":"code","6c20b4c8":"code","a0785c3d":"code","6680c620":"code","369c52dc":"code","74311a91":"code","d0cd3db0":"code","06375b95":"code","b4217be5":"code","643da736":"code","0cc6fa7d":"code","22a50ab5":"code","c0ee80ed":"markdown","d926d736":"markdown","c9e0afe9":"markdown","2db063db":"markdown","e4432556":"markdown","a04fb796":"markdown","bcd4bafe":"markdown","7dd03893":"markdown","27ed9ba2":"markdown","cb62409b":"markdown","7cce3932":"markdown","f90e00d5":"markdown","f0762852":"markdown","5a181c24":"markdown","becaa844":"markdown","144f7423":"markdown","11027245":"markdown","71154e81":"markdown","859565e7":"markdown","430274c6":"markdown","781d68f2":"markdown","702a5f86":"markdown"},"source":{"d319d53c":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error \nfrom keras.layers import LSTM, Dense\nfrom keras.models import Sequential\n\n","20412edf":"arno = pd.read_csv('..\/input\/acea-water-prediction\/River_Arno.csv')","71be67c0":"arno.describe()","bd363d50":"arno.info()","8763de4d":"arno.head()","8be447da":"print(\"Data shape :\" ,arno.shape)","ff493d76":"sns.heatmap(arno.isnull())","90c4595e":"arno.iloc[3474:4569+1,-9] = arno.iloc[3474:4569+1,-4]\narno.iloc[3474:4569+1,-10] = arno.iloc[3474:4569+1,-4]\narno.iloc[3474:4569+1,-3] = arno.iloc[3474:4569+1,-4]\narno.iloc[3474:6764+1,-5] = arno.iloc[3474:6764+1,-8]\narno.iloc[3474:6764+1,-6] = arno.iloc[3474:6764+1,-8]\narno.iloc[3839:6764+1,-7] = arno.iloc[3839:6764+1,-8]\narno.iloc[4569:6764+1,-3] = arno.iloc[4569:6764+1,-8]\narno.iloc[4569:6764+1,-4] = arno.iloc[4569:6764+1,-8]\narno.iloc[4569:6764+1,-9] = arno.iloc[4569:6764+1,-8]\narno.iloc[4569:6764+1,-10] = arno.iloc[4569:6764+1,-8]\narno.iloc[6474:6764+1,-11] = arno.iloc[6474:6764+1,-13]\n","1cd8fb36":"sns.heatmap(arno.isnull())","1cdd9cae":"# moving target data 1 day, this could be explored later\n# NN MSE for Number_days = 3 : 0.007\nNumber_days = 4\narno['Rainfall_Vernio'] = arno['Rainfall_Vernio'].shift (-Number_days)\narno['Rainfall_Mangona'] = arno['Rainfall_Mangona'].shift (-Number_days)\narno['Rainfall_S_Agata'] = arno['Rainfall_S_Agata'].shift (-Number_days)\narno['Rainfall_S_Piero'] = arno['Rainfall_S_Piero'].shift (-Number_days)\narno['Rainfall_Le_Croci'] = arno['Rainfall_Le_Croci'].shift (-Number_days)\narno['Rainfall_Cavallina'] = arno['Rainfall_Cavallina'].shift (-Number_days)\n\n# dropping all rows that contain a null value\narno_1 = arno.dropna(how='any',axis=0).copy()","a6524c73":"arno_1.info()","43d45fd8":"sns.heatmap(arno_1.isnull())","e8480421":"sns.heatmap(arno_1.corr());","3c9bea57":"arno_1['Day'] = arno_1['Date'].str.split('\/').str[0]\narno_1['Month'] = arno_1['Date'].str.split('\/').str[1]\narno_1['Year'] = arno_1['Date'].str.split('\/').str[2]","4f73ba19":"plt.figure(1,figsize = (10,5))\nplt.plot(arno_1.Hydrometry_Nave_di_Rosano)\nplt.title(\"Hydrometry_Nave_di_Rosano\")","ab276c18":"arno_1.Hydrometry_Nave_di_Rosano.replace(0, np.nan, inplace=True)\narno_1.Hydrometry_Nave_di_Rosano.interpolate(method ='pchip', limit_direction ='forward', inplace=True)","a0e2a350":"plt.figure(1,figsize = (10,5))\nplt.plot(arno_1.Hydrometry_Nave_di_Rosano)\nplt.title(\"Lake Level\")","be39e2da":"from sklearn.preprocessing import StandardScaler\n\n# Standardizing the features\narno_T = arno_1.T\nx = StandardScaler().fit_transform(arno_T.iloc[1:-5,:])","d578017b":"from sklearn.decomposition import PCA\n\npca = PCA()\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents)","dc9a4e5e":"principalDf.head()","3ddea687":"eigvals = pca.explained_variance_ratio_","e71c14af":"eigvals\/np.sum(eigvals) * 100","8a3a789f":"plt.figure(figsize = (8,8))\nplt.bar(list(range(14)),eigvals\/np.sum(eigvals) * 100)\nplt.show()","8dd69b08":"#pourcentage of the first two axes\n(eigvals[0]+eigvals[1])\/np.sum(eigvals) * 100 ","c491a85d":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\n\nfeatures = ['Rainfall_Le_Croci','Rainfall_Cavallina','Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero','Rainfall_Vernio','Rainfall_Stia','Rainfall_Consuma','Rainfall_Incisa','Rainfall_Montevarchi','Rainfall_S_Savino','Rainfall_Laterina','Rainfall_Bibbiena','Rainfall_Camaldoli']\n\nax.scatter(principalDf.loc[:, 0]\n            , principalDf.loc[:, 1]\n            , s = 50)\n\nfor i, txt in enumerate(features):\n    ax.annotate(txt, (principalDf.loc[i, 0], principalDf.loc[i, 1]))\n\nax.grid()","38e0a29b":"cols = ['Year','Date', 'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata', 'Rainfall_Mangona', 'Rainfall_S_Piero', 'Rainfall_Vernio', 'Rainfall_Stia', 'Rainfall_Consuma', 'Rainfall_Incisa', 'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina', 'Rainfall_Bibbiena', 'Rainfall_Camaldoli', 'Temperature_Firenze', 'Day', 'Month', 'Hydrometry_Nave_di_Rosano']\narno_1 = arno_1[cols]\narno_1.head()","8464e4ae":"from sklearn.ensemble import GradientBoostingRegressor","3b76e365":"# Train-Test split the data\narno_model = arno_1.iloc[:, 2:].values\n\nX, y = arno_model[:, :-1],  arno_model[:, -1]\nscaler = MinMaxScaler(feature_range=(0, 1))\nX = scaler.fit_transform(X)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.82, shuffle = True)\n\n\nX_train = np.asarray(X_train).astype('float32')\ny_train = np.asarray(y_train).astype('float32')\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor(learning_rate=0.01,n_estimators=500)\ngbr_model = gbr.fit(X_train,y_train)","9d6f5860":"y_pred = gbr_model.predict(X_test)","886425ab":"plt.figure(figsize = (10,5))\nplt.plot(y_test, 'r', label='True values')\nplt.plot(y_pred, label= 'Predicted values')\nplt.legend()\nplt.show()","7a9a9421":"mae = mean_absolute_error(y_test, y_pred)\nprint('MAE: %.3f' % mae)\n\nmse = mean_squared_error(y_test, y_pred)\nprint('MSE: %.3f' % mse)","a0313883":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(0.001), loss='mse')","89a3c9cd":"r = model.fit(X_train, y_train,batch_size=32,epochs=30)","6c20b4c8":"y_pred = model.predict(X_test)","a0785c3d":"plt.figure(figsize = (10,5))\nplt.plot(y_test, 'r', label='True values')\nplt.plot(y_pred, label= 'Predicted values')\nplt.legend()\nplt.show()","6680c620":"for i in range(15):\n    plt.figure(i)\n    plt.plot(X_test[:,i])\n    plt.title(i)","369c52dc":"mae = mean_absolute_error(y_test, y_pred)\nprint('MAE: %.3f' % mae)\n\nmse = mean_squared_error(y_test, y_pred)\nprint('MSE: %.3f' % mse)","74311a91":"# Train-Test split the data\n\narno_model = arno_1.iloc[:, [6,7,10,16,17,18,19]].values\n\nX, y = arno_model[:, :-1],  arno_model[:, -1]\nscaler = MinMaxScaler(feature_range=(0, 1))\nX = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.82, shuffle = False)\n\n# Cat Boost Regressor\n\ngbr = GradientBoostingRegressor()\ngbr_model = gbr.fit(X_train,y_train)","d0cd3db0":"y_pred = gbr_model.predict(X_test)","06375b95":"plt.figure(figsize = (10,5))\nplt.plot(y_test, 'r', label='True values')\nplt.plot(y_pred, label= 'Predicted values')\nplt.legend()\nplt.show()","b4217be5":"mae = mean_absolute_error(y_test, y_pred)\nprint('MAE: %.3f' % mae)\n\nmse = mean_squared_error(y_test, y_pred)\nprint('MSE: %.3f' % mse)","643da736":"# Using walk forward validation\n\narno_model = arno_1.iloc[:, 2:]\n\nmodel = XGBRegressor(objective='reg:squarederror', n_estimators=500)\n\nwindow_size = 30000\nfor i in range (400, arno_model.shape[0], window_size):\n    end = i + window_size\n    if end > arno_model.shape[0]:\n        end = arno_model.shape[0]\n    \n    window = arno_model.iloc[0:end ,:]\n    \n    # getting train and test values from window\n    data = window.values\n    X, y = data[:, :-1], data[:, -1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle = False)\n    \n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test)\n    print(\"values for\",i,\"th run\")\n    mae = mean_absolute_error(y_test, prediction)\n    print('MAE: %.3f' % mae)\n    \n    mse = mean_squared_error(y_test, prediction)\n    print('MSE: %.3f' % mse)\n    \n    plt.figure(figsize = (10,5))\n    plt.plot(y_test, 'r', label='True values')\n    plt.plot(prediction, label= 'Predicted values')\n    plt.legend()\n    plt.show()","0cc6fa7d":"#LSTM Dataset \n\ndef LSTM_dataset(dataset,lookback):\n    X = np.zeros(np.shape(dataset))\n    Y = np.zeros(np.shape(dataset)[0])\n    for i in range (len(dataset)-lookback-1):\n        row = dataset[i:(i+lookback),:]\n        X[i:(i+lookback),:] = row\n        Y[i] = dataset[i+lookback,-1]\n    Y = np.reshape(Y,(np.shape(Y)[0],1))\n    return np.concatenate((X,Y),axis=1)","22a50ab5":"\narno_model = arno_1.iloc[:, 2:].values\n# taking data for day n and predicting hydrometry of n+1 th day, you may look at different lookbacks, 7, 15, 30..\nlookback=1\n\narno_model = LSTM_dataset(arno_model,lookback)\n\nscaler = MinMaxScaler(feature_range=(0, 1))\n\nX, y = arno_model[:, :-1],  arno_model[:, -1]\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle = False)\n\n# reshape into [samples, timesteps, features]\nX_train =  X_train.reshape((X_train.shape[0],1,X_train.shape[1]))\nX_test =  X_test.reshape((X_test.shape[0],1,X_test.shape[1]))\n\nprint(np.shape(X_train))\n#X_test = np.reshape(X_test, (4, 106, X_test.shape[1]))\n#y_train = np.reshape(y_train, (4, 106))\n#print (X_train.shape)\n#print(y_train.shape)\n\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape= np.shape(X_train)[1:], name= \"lstm\"))\nmodel.add(Dense(1, name= \"output\"))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n#print(model.summary())\nmodel.fit(X_train, y_train,batch_size=32, epochs=15)\nprediction = model.predict(X_test)\n\n\nmae = mean_absolute_error(y_test, prediction)\nprint('MAE: %.3f' % mae)\n\nmse = mean_squared_error(y_test, prediction)\nprint('MSE: %.3f' % mse)\n\nplt.figure(figsize = (10,5))\nplt.plot(y_test, 'r', label='True values')\nplt.plot(prediction, label= 'Predicted values')\nplt.legend()\nplt.show()","c0ee80ed":"Before diving into the different models, let is first take a clsoer look to our dataset in order to understand how it behaves.","d926d736":"Some columns were added to the dataset : day, month and year. We'll use them to understand how the hydrometry varies over the year. ","c9e0afe9":"# River Arno\n## *Data Preprocessing*","2db063db":"# Introduction","e4432556":"# GradientBoostingRegressor","a04fb796":"Hello everyone, welcome to our notebook! \\\nIn the following notebook, we'll present the four models that we worked on that help predict the water level in different waterbodies, more specifically in : water springs, lakes, rivers, or aquifers. Each waterbody's behaviour is unique, therefore different model is used for each waterbody. We'll take a look into different models each time and compare them in order to pick the best one.\nGood reading !\n\n\n\n","bcd4bafe":"The previous graph shows some anomalies in the hydrometry values. For instance, the hydrometry can't be equal to 0 around the row 3500. To fix this, we'll replace these zeros by nan values so we can interpolate them afterwards. The method used for the interpolation is the \"pchip\" method.","7dd03893":"We're going to work on the river Arno first.","27ed9ba2":"The heat map shows the existence of many missing values ( Nan values). Droping all the rows containing missing values has been tried but the model performance wasn't good enough, a predicted result since the model dosen't have enough training examples to generalize well to the rest of the data. A different approach was taken, instead of dropping these rows, why not try to fil the missing values with approximative values. The map shows where measurements were taken, we'll use that to fill most of the missing values with rainfalls in closer regions.","cb62409b":"# Two groups of points :\n**1) Points located near Arno**\n* Stia\n* Camaldoli\n* Consuma\n* Incisa\n* Montevarchi\n* Laterina\n* Bibbiena\n* *S_Savino*\n\n**2) Points reaching the River Arno through the River Sieve**\n* Vernio\n* Magona\n* Cavallina\n* S_Agata\n* Le_Corci\n* S_peiro\n\n**remarks :**\n\n* (The Sieve is a river in Italy. It is a tributary of the Arno.)\n* (Lago di Bilancino is made with a dam on the river Sieve)","7cce3932":"To see if there are really different categories in our data set, we'll proceed to use the principal component analysis.\n","f90e00d5":"Let is first import the libraries needed. ","f0762852":"The results of principal component analysis show that we can indeed categorize the different points to two categories. \nNow, we'll start trying on different models and evaluate their performances.","5a181c24":"We also shifted some of the values in the dataset by a certain number of days, since for some of these rainfalls, it takes time to arrive to the river. We settled on a four days shift because it gave the best performance.","becaa844":"# Neural Network","144f7423":"![](https:\/\/media.discordapp.net\/attachments\/791649795617325061\/804851868571533332\/download.png)\n","11027245":"# Models using results of PCA","71154e81":"The map shown earlier helps catergorize the points to two categories : ","859565e7":"As we can see, there are a lot of missing values in the dataset. Some preprocessing is necessary for the model to learn the right weights. The heat map visualizes better the values that are missing.","430274c6":"# Models","781d68f2":"# LSTM","702a5f86":"Using the results of the PCA analysis, we're going to use fewer variales to build our models, we'll then see which one perform better."}}