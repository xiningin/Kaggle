{"cell_type":{"3ccb0202":"code","fb9e9c67":"code","5603d5bc":"code","be63c820":"code","144b76b8":"code","16eb811a":"code","89e6df6c":"code","8e31adb2":"code","6158b039":"code","4c802b1f":"code","cee9fe80":"code","0c47e8cd":"code","d4923d6c":"code","8ac99efa":"code","e9b3d8a8":"code","edfefde8":"code","10b37ba3":"code","79f024d2":"code","3af6ee6e":"code","f4b10b80":"markdown","9249eaa7":"markdown","5b11fc2f":"markdown","e3375a07":"markdown","e17f53a8":"markdown","8e8667ec":"markdown"},"source":{"3ccb0202":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Dense, Activation, Flatten, Dense,MaxPool2D, Dropout\nfrom keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\n","fb9e9c67":"size=150\ndata=[]\nassign_dict = {\"NORMAL\":0, \"PNEUMONIA\":1}\ndef load_data(directory):\n    for sub_directory in os.listdir(directory):\n        if sub_directory==\"NORMAL\":\n            inner_directory=os.path.join(directory,sub_directory)\n            for i in os.listdir(inner_directory):\n                img=cv2.imread(os.path.join(inner_directory,i),0)\n                img=cv2.resize(img,(size,size))\n                data.append([img,assign_dict[sub_directory]])\n                \n        if sub_directory==\"PNEUMONIA\":\n            inner_directory=os.path.join(directory,sub_directory)\n            for i in os.listdir(inner_directory):\n                img=cv2.imread(os.path.join(inner_directory,i),0)\n                img=cv2.resize(img,(size,size))\n                data.append([img,assign_dict[sub_directory]])        \n            \n    random.shuffle(data)\n    return np.array(data)\n            \n    ","5603d5bc":"train=load_data('..\/input\/chest-xray-pneumonia\/chest_xray\/train')\nval = load_data('..\/input\/chest-xray-pneumonia\/chest_xray\/val')\ntest = load_data('..\/input\/chest-xray-pneumonia\/chest_xray\/test')\n","be63c820":"train","144b76b8":"total_count=0\nnum_normal=0\nnum_pneumonia=0\nnormal=[]\npneumonia=[]\nfor i in train:\n    if (i[1]==0):\n        total_count=total_count+1\n        num_normal=num_normal+1\n        normal.append(i[0])\n    else:\n        total_count=total_count+1\n        num_pneumonia=num_pneumonia+1\n        pneumonia.append(i[0]) \nprint('Total img: '+str(total_count))        \nprint('Normal : '+str(num_normal))\nprint('PNEUMONIA: '+str(num_pneumonia))","16eb811a":"rows=2\ncols=4\nfig=plt.figure(figsize=(18,18))\nfig, ax = plt.subplots(2, 4)\nfor i in range(rows):\n    for j in range(cols):\n        if i<1:\n            ax[i,j].imshow(normal[j])\n            ax[i,j].set_title('NORMAL')\n        else:\n            ax[i,j].imshow(pneumonia[j])\n            ax[i,j].set_title('PNEUMONIA')\n        ","89e6df6c":"x_train=[]\ny_train=[]\nfor x,y in train:\n    x_train.append(x)\n    y_train.append(y)","8e31adb2":"x_val=[]\ny_val=[]\nfor x,y in val:\n    x_val.append(x)\n    y_val.append(y)","6158b039":"x_test=[]\ny_test=[]\nfor x,y in test:\n    x_test.append(x)\n    y_test.append(y)","4c802b1f":"x_train=np.array(x_train)\/255\nx_val=np.array(x_val)\/255\nx_test=np.array(x_test)\/255","cee9fe80":"x_train=x_train.reshape(-1,size,size,1)\ny_train=np.array(y_train)\nx_val=x_val.reshape(-1,size,size,1)\ny_val=np.array(y_val)\nx_test=x_test.reshape(-1,size,size,1)\ny_test=np.array(y_test)\n","0c47e8cd":"image_gen=ImageDataGenerator(rotation_range=30,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            featurewise_center=False,  \n                            samplewise_center=False, \n                            featurewise_std_normalization=False,  \n                            samplewise_std_normalization=False, \n                            zca_whitening=False,  \n                            zoom_range=0.2,\n                            horizontal_flip=True\n                            )\n","d4923d6c":"model = Sequential()\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = \"sgd\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","8ac99efa":"reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n                              patience = 2, \n                              verbose=1,\n                              factor=0.25, \n                              min_lr=0.000001)","e9b3d8a8":"history = model.fit(image_gen.flow(x_train,y_train, batch_size = 25),\n                                 epochs = 12 ,\n                                 validation_data = image_gen.flow(x_val, y_val),\n                                 callbacks = [reduce_lr]\n                   )","edfefde8":"pd.DataFrame(model.history.history).plot(figsize=(12,8))\n","10b37ba3":"prediction=model.predict_classes(x_test)","79f024d2":"print(classification_report(prediction, y_test))","3af6ee6e":"confusion_matrix(prediction, y_test)","f4b10b80":"<a id='4'><\/a>\n# Model Traning","9249eaa7":"* [Load and Check Data](#1)<br><br>\n* [Some Images](#2)\n* [Imaga Generator and Model](#3)\n* [Model Training](#4)\n* [Result](#5)","5b11fc2f":"<a id='3'><\/a>\n# ImageGenerator and CNN Model","e3375a07":"<a id='2'><\/a>\n# Some Images","e17f53a8":"<a id='1'><\/a>\n# Load and Check Data","8e8667ec":"<a id='5'><\/a>\n# Result"}}