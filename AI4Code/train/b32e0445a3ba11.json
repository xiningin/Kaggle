{"cell_type":{"a5025b89":"code","c5a19587":"code","5d01231c":"code","f3fc3f9e":"code","97c53976":"code","4ec5772f":"code","0da773c2":"markdown","54166578":"markdown","644427db":"markdown","d58c9ef0":"markdown","701ea39c":"markdown","6ae217ae":"markdown"},"source":{"a5025b89":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport random\nimport os\nimport math\nimport time\nfrom torch.utils.data import DataLoader, Dataset","c5a19587":"class SequenceLoader(Dataset):\n\n    def __init__(self, dataframe_path, signal_noise_cutoff, test_set=None):\n        super().__init__()\n        self.df = pd.read_json(dataframe_path)\n        deg_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\n        \n        self.is_test = test_set is not None or deg_cols[0] not in self.df.columns\n        if self.is_test:\n            self.df = self.df.query((\"seq_length == 107\" if test_set == 'public' else \"seq_length == 130\"))\n            self.y = None\n        else:\n            self.df = self.df[self.df.signal_to_noise >= signal_noise_cutoff]\n            self.y = np.stack([np.stack(self.df[col].values) for col in deg_cols], axis=-1)\n\n        self.sample_ids = self.df['id'].values\n        self.X = np.stack(self.df['train_tensor'].values)\n        self.id_to_bp_mat_map = self.load_bp_mats()\n\n    def __getitem__(self, index: int):\n        x = torch.tensor(self.X[index, :, :], dtype=torch.float32)\n        seq_adj = self.get_sequence_adjacency(x.size()[0])\n        bp_adj = self.get_base_pair_adjacency(self.sample_ids[index])\n\n        if self.is_test:\n            sample_id = self.sample_ids[index]\n            return sample_id, x, seq_adj, bp_adj\n\n        targets = torch.tensor(self.y[index, :, :], dtype=torch.float32)\n        return x, targets, seq_adj, bp_adj\n\n    @staticmethod\n    def get_sequence_adjacency(size):\n        r_shift = np.pad(np.identity(size), ((0, 0), (1, 0)), mode='constant')[:, :-1]\n        l_shift = np.pad(np.identity(size), ((0, 0), (0, 1)), mode='constant')[:, 1:]\n        return torch.tensor(r_shift + l_shift, dtype=torch.float32)\n\n    def get_base_pair_adjacency(self, sample_id):\n        return self.id_to_bp_mat_map[sample_id]\n\n    def load_bp_mats(self):\n        res = {}\n        for sid in self.sample_ids:\n            res[sid] = torch.tensor(np.load('..\/input\/stanford-covid-vaccine\/bpps\/' + sid + '.npy'), dtype=torch.float32)\n        return res\n    \n    def __len__(self) -> int:\n        return self.df.shape[0]\n\n\ndef dataset_loader(df_path, batch_size, signal_noise_cutoff=-99.0, test_set=None):\n    dataset = SequenceLoader(df_path, signal_noise_cutoff, test_set=test_set)\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=(test_set is None),\n        num_workers=4\n    )","5d01231c":"# Generates an encoding to capture a node's position (just like transformers).\n# Used as part of the GraphBlock.\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.2, max_len=300):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x, offset=None):\n        x = x + self.pe[:x.size(1), :]\n        return self.dropout(x)\n    \n\n# Provides info about the entire graph to every node.\nclass GraphBlock(nn.Module):\n\n    def __init__(self, mlp_size, dropout=0.2):\n        super(GraphBlock, self).__init__()\n        self.pos_encoder = PositionalEncoding(mlp_size)\n        self.graph_layer = nn.Linear(mlp_size, mlp_size)\n        self.layer_norm = torch.nn.LayerNorm(mlp_size)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x):\n        graph_node_emb = torch.tanh(self.graph_layer(x))\n        graph_node_emb = self.dropout(graph_node_emb)\n        graph_emb = torch.mean(graph_node_emb, 1)\n        graph_emb = graph_emb.reshape((graph_emb.size()[0], 1, graph_emb.size()[1]))\n\n        pos_enc = self.pos_encoder(x)\n        return self.dropout(self.layer_norm(pos_enc + graph_emb))\n\n\nclass ConvAttnBlock(nn.Module):\n\n    def __init__(self, mlp_size, adj_conv_channels=4, dropout=0.2):\n        super(ConvAttnBlock, self).__init__()\n        self.adj_conv_channels = adj_conv_channels\n        self.neighbor_layer = nn.Linear(mlp_size, mlp_size)\n        self.result_layer = nn.Linear(mlp_size, mlp_size)\n        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=adj_conv_channels, \n                                     kernel_size=(7), padding=3)\n        self.conv2 = torch.nn.Conv2d(in_channels=adj_conv_channels, \n                                     out_channels=adj_conv_channels, kernel_size=(17), padding=8)\n        self.layer_norm = torch.nn.LayerNorm(mlp_size)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x, bp_adj):\n        batch = bp_adj.size(0)\n        d1 = bp_adj.size(1)\n        d2 = bp_adj.size(2)\n\n        bp_adj = self.conv1(bp_adj.reshape((batch, 1, d1, d2)))\n        bp_adj = self.conv2(bp_adj)\n        bp_adj = torch.mean(bp_adj, 1)[:, :d1, :d2]\n        neighbor_emb = torch.tanh(self.neighbor_layer(x))\n        neighbor_emb = self.dropout(neighbor_emb)\n        neighbor_sum = torch.matmul(bp_adj, neighbor_emb)\n        cat = x + neighbor_sum\n        out = torch.tanh(self.result_layer(cat))\n        out = self.layer_norm(out)\n        return self.dropout(out), bp_adj\n\n\n# Combines 2 rounds of conv attention on the sequence and BPP matrices.\n# Includes data from 1 graph block and a skip connection.\nclass NeighborAttnStage(nn.Module):\n    def __init__(self, mlp_size, dropout=0.2):\n        super(NeighborAttnStage, self).__init__()\n        self.graph_block = GraphBlock(mlp_size)\n        self.sequence_block1 = ConvAttnBlock(mlp_size)\n        self.sequence_block2 = ConvAttnBlock(mlp_size)\n        self.base_pair_block1 = ConvAttnBlock(mlp_size)\n        self.base_pair_block2 = ConvAttnBlock(mlp_size)\n        \n    def forward(self, x_in, seq_adj, bp_adj):\n        x_bp1, _ = self.base_pair_block1(x_in, bp_adj)\n        x_seq1, _ = self.sequence_block1(x_in, seq_adj)\n        x = self.graph_block(x_in) + x_bp1 + x_seq1\n\n        x_bp2, _ = self.base_pair_block2(x, bp_adj)\n        x_seq2, _ = self.sequence_block2(x, seq_adj)\n        return x_bp2 + x_seq2 + x_in\n\n\nclass NeighborhoodAttentionModel(nn.Module):\n\n    def __init__(self, mlp_size, dropout=0.2):\n        super(NeighborhoodAttentionModel, self).__init__()\n        self.input_fc = nn.Linear(14, mlp_size)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv_attn_stage1 = NeighborAttnStage(mlp_size)\n        self.conv_attn_stage2 = NeighborAttnStage(mlp_size)\n        self.conv_attn_stage3 = NeighborAttnStage(mlp_size)\n        self.output_fc1 = nn.Linear(mlp_size, mlp_size)\n        self.output_fc2 = nn.Linear(mlp_size, 3)\n\n    def forward(self, x, seq_adj, bp_adj):\n        x = torch.tanh(self.input_fc(x))\n        x = self.dropout(x)\n\n        x = self.conv_attn_stage1(x, seq_adj, bp_adj)\n        x = self.conv_attn_stage2(x, seq_adj, bp_adj)\n        x = self.conv_attn_stage3(x, seq_adj, bp_adj)\n\n        x = self.dropout(x)\n        x = torch.tanh(self.output_fc1(x))\n        \n        x = self.dropout(x)\n        return self.output_fc2(x)","f3fc3f9e":"# Uninteresting code used to keep track of average loss over an epoch\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        self.start_time = time.time()\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    @property\n    def time(self):\n        return time.time() - self.start_time\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        self.start_time = time.time()\n\n\n# Pytorch MCRMSE Losss\n# [Link to Kernel]\nclass RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss\n\n\nclass MCRMSELoss(nn.Module):\n    def __init__(self, num_scored=3):\n        super().__init__()\n        self.rmse = RMSELoss()\n        self.num_scored = num_scored\n\n    def forward(self, yhat, y):\n        score = 0\n        for i in range(self.num_scored):\n            score += self.rmse(yhat[:, :, i], y[:, :, i]) \/ self.num_scored\n        return score\n\n","97c53976":"# Get Device (CPU or GPU)\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\n# Hyperparameters\nepochs = 100\nbatch_size = 8\nnode_emb_size = 512\nlr = 0.0001\nlr_drop_epochs = 45\nlr_gamma = 0.1\ncriterion = MCRMSELoss()\n\n# Setup Train\/Val Data Loaders\nprepd_train_data = '..\/input\/covidtrainvaldataset\/train_1.json'\nprepd_val_data = '..\/input\/covidtrainvaldataset\/val_1.json'\ntrain_loader = dataset_loader(prepd_train_data, batch_size=batch_size, signal_noise_cutoff=0.6)\nval_loader = dataset_loader(prepd_val_data, batch_size=batch_size, signal_noise_cutoff=1.0)\n\n# Model\nmodel = NeighborhoodAttentionModel(node_emb_size).to(device)\n\n# Optimizer & Scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_drop_epochs, gamma=lr_gamma)\n\n# Objects for keeping track of loss over epochs.\nepoch_loss_hist = Averager()\nval_loss_hist = Averager()\n\n\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss_hist.reset()\n    \n    # Training\n    for sequences, targets, seq_adj_matrix, bp_adj_matrix in train_loader:\n        optimizer.zero_grad()\n        pred = model(sequences.to(device), seq_adj_matrix.to(device), bp_adj_matrix.to(device))\n        loss = criterion(pred[:, :targets.size()[1], :], targets.to(device))\n        loss.backward()\n        optimizer.step()\n        epoch_loss_hist.send(loss.item())\n        \n    # Validation\n    with torch.no_grad():\n        model.eval()\n        val_loss_hist.reset()\n\n        for sequences, targets, seq_adj_matrix, bp_adj_matrix in val_loader:\n            pred = model(sequences.to(device), seq_adj_matrix.to(device), bp_adj_matrix.to(device))\n            loss = criterion(pred[:, :targets.size(1), :], targets.to(device))\n            val_loss_hist.send(loss.item())\n\n    print('Epoch:', epoch, 'Train Loss:', epoch_loss_hist.value, 'CV Loss:', val_loss_hist.value)\n    scheduler.step()\n","4ec5772f":"def build_submission_df(ids, pred_tensor):\n    if type(pred_tensor).__module__ != np.__name__:\n        pred_tensor = pred_tensor.cpu().detach().numpy()\n    res = []\n    for i, id in enumerate(ids):\n        for j, pred in enumerate(pred_tensor[i, :, :]):\n            res.append([id+'_'+str(j)] + list(pred))\n    return res\n\n\ndef make_pred_file(model, loaders, postfix=''):\n    res = []\n    for loader in loaders:\n        for ids, sequences, seq_adj_matrix, bp_adj_matrix in loader:\n            test_pred = model(sequences.to(device), seq_adj_matrix.to(device), bp_adj_matrix.to(device))\n            res += build_submission_df(ids, test_pred)\n\n    pred_df = pd.DataFrame(res, columns=['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_Mg_50C'])\n    pred_df['deg_pH10'] = 0\n    pred_df['deg_50C'] = 0\n    pred_df.to_csv('.\/submission'+postfix+'.csv', index=False)\n    \n\n\ntest_data_path = '..\/input\/covidtrainvaldataset\/test_1.json'\ntest_data_loader1 = dataset_loader(test_data_path, test_set='public', batch_size=batch_size)\ntest_data_loader2 = dataset_loader(test_data_path, test_set='private', batch_size=batch_size)\nmake_pred_file(model, [test_data_loader1, test_data_loader2])","0da773c2":"## Model","54166578":"# Ideas for Further Improvement\n\n* Deeper and\/or wider network. Increasing the node emb & mlp sizes or adding additional Neighborhood Attention Stages (NAS). \n* Swapping out one of the NAS blocks or adding a transformer layer. I tried swapping out the las NAS block for an self attention block and was able to push the training error much lower. However, it was much easier to overit with this setup. I didn't spend much time trying to tune the regularization with this setup so it's might still be worth exploring.\n* Inspired by multi-head attention, setting up multiple parallel graph attention blocks and averaging the resulting node values. \n* Weighting the examples using the signal-to-noise ratio and\/or weighting the individual node predictions based on the error metric.\n* Auto encoder pretraining or pretraining to predict loop type (excluding loop type from the inputs).\n* I didn't spend too much time tuning hyper params, there's likely a good bit of room from improvement.\n* Data augmentation has proven effective in other kernels. \n\n## Thanks for Reading!\n\n### Notes\n* I have zero domain knowledge here.\n* I wasn't entirely sure what to call some things. I'm very open to suggestions. \n* The LB score here is based on a single fold. \n* I know the competition is short and nearly over. Considering that there are 10+ other kernels with better public lb scores, this seems safe to release. ","644427db":"## Data Loader\nThe data prep is pretty basic, so I'll skip it and just load from prepared json files. The inputs are just 14 dimensional, 1-hot (err.. 3-hot) encodings of the sequence, structure and predicted loop types. \n\nThe loader loads the 14d encodings along with the base pair matrices. Additionally, it creates an appropriatly size \"sequence adjacency\" matrix that's later used to represent the sequence connections in the graph. It is also setup to work for both train and test sets.","d58c9ef0":"## Training","701ea39c":"## Inference","6ae217ae":"Hey everybody, it turns out I don't quite have the bandwidth to keep up with school and work and this competition. So, I wanted to share what I've been working on, in case it might be useful to other Kagglers.\n\nI came up with a graph-style architecture that incorporates the BPP matrices. It takes into account that base pair connections aren't certain and allows passing info from other nodes in a neighborhood (not just adjacent nodes). So in that sense it's also transformer\/attention inspired.\n\n## TL;DR What's the General Idea?\n\nI'm learning an attention matrix that selects a set of relevant graph neighbors. I do this by running 2D convolutions over the BPP matrices and then over a fixed matrix that represent the sequence structure. These attention matrices are then multiplied by the current \"node values\" and the result is added back to them. This way, for every node, a block's output is a weighted average of the node and it's relevant nearby neighbors.\n\n## Intuition\n\nBy assuming a fully connected graph, it seems that transformers potentially give the network a little bit too much freedom. On the other hand, by only passing info between sequential nodes, RNNs are potentially a little too restrictive. GCNs seems like a good fit and there are some great kernels using them. \n\n[Admittedly, there are also really high scoring kernels using transformers and RNNs, but that wasn't the case when I started on this. So, perhaps some of my initial intuition isn't quite right...] \n\nBut GCNs require that we know the structure of the graph and the base pair connections aren't certain in this case. It'd be nice if we allowed the network to consider alternative structures when making predictions. So we'll apply large convolutions to our adjacency matrices to learn a sort of attention mechanism over nearby nodes in the graph.\n\n## How it Works\n\nThe core building block is the **Convolutional Attention Block** pictured below:\n\n<img src=\"https:\/\/docs.google.com\/drawings\/d\/e\/2PACX-1vQPk1R-IXHqyoxyDS_ZxBrcxGmZXPOzLEH3yTjXjCdA-F1QQhf6A91zKeVV8H2mEg8uAN51HHZj5V6f\/pub?w=1193&amp;h=1116\" style=\"max-width:75%\">\n\nThe \"adjacency matrix\" is either the base pair probabilities matrix (BPPS) or a \"sequence matrix\" that represents the sequence connections. Think of the neigborhood attention matrix as selection the relevant nearby nodes for every node. Then the matrix multiplication returns a weighted average of the embeddings for those relevant nodes. Finally, we add the neighborhood representation to the nodes initial embedding.  \n\nWe then combine a few of these blocks (2 for each adjacency matrix) into a larger **Neighborhood Attention Stage:**\n\n<img src=\"https:\/\/docs.google.com\/drawings\/d\/e\/2PACX-1vSfOU6scS01XqkFl6yjTIF-WBcIKT64IaXnzwU7dz1BGv8ZcBzpVLuiS7iEkbSNZC6UnyZZjj6XfH6R\/pub?w=1417&amp;h=1169\"  style=\"max-width:75%\">\n\nThe **Graph Block** is just a positional encoding added to an average of all the node values in the graph. Considering that the private dataset contains longer sequences than any in the test dataset, you might consider dropping the positional encoding. I tried this a few times and it performs worse (-0.015) on CV and public lb.\n\nFinally, we stack a few of these stages together with some fully connected layers to generate the final network:\n\n<img src=\"https:\/\/docs.google.com\/drawings\/d\/e\/2PACX-1vQdPtdsGuWKTpp7iFaKOPgaIiQZ_NZN-9I7gi37eJ4J-yMBgW5KXG4eYL_XrzXKqe-SPBRLWDDPeZlu\/pub?w=928&amp;h=1057\"  style=\"max-width:75%\">\n\n# Code"}}