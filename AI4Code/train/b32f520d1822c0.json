{"cell_type":{"dbd9877f":"code","4b165122":"code","88ad5d16":"code","fc066faf":"code","319fcf01":"code","5f441ae5":"markdown","91dca49b":"markdown","aa2a7f5b":"markdown","4fd42b85":"markdown","1d54d26b":"markdown"},"source":{"dbd9877f":"!pip install tensorflow==1.13.1","4b165122":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport threading\nimport multiprocessing\nimport gym\nimport os\nimport shutil","88ad5d16":"game_name = 'CartPole-v0'\nlog_dir = '.\/tmp\/log'\noutput_graph = False\nn_workers = multiprocessing.cpu_count()\n\n_gamma = 0.9\n_epsilon = 1e-6\n_entropy_param = 0.001\n_max_global_episode = 3000\n_global_episode = 0\n_update_global_iter = 10\n_render_flag = False\n_global_reward_records = []\n_actor_lr = 0.001\n_critic_lr = 0.001\n_global_net_scope = 'Global_Net'\n\nenv = gym.make(game_name)\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.n\n\n\nclass A3CNet(object):\n    def __init__(self, scope, global_net=None):\n        if scope == _global_net_scope:   # global network\n            with tf.variable_scope(scope):\n                self.state = tf.placeholder(tf.float32, [None, num_state], 'state')\n                self.actor_variables, self.critic_variables = self._build_AC_net(scope)[-2:]\n        else:   # local net, calculate losses\n            with tf.variable_scope(scope):\n                self.state = tf.placeholder(tf.float32, [None, num_state], 'state')\n                self.action = tf.placeholder(tf.int32, [None, ], 'action')\n                self.target_v = tf.placeholder(tf.float32, [None, 1], 'target_v')\n\n                self.action_prob, self.v, self.actor_variables, self.critic_variables = self._build_AC_net(scope)\n\n                td_error = tf.subtract(self.target_v, self.v, name='td_error')\n                with tf.name_scope('critic_loss'):\n                    self.critic_loss = tf.reduce_mean(tf.square(td_error))\n\n                with tf.name_scope('actor_loss'):\n                    log_action_prob = tf.reduce_sum(tf.one_hot(self.action, num_action, dtype=tf.float32) * tf.log(self.action_prob + _epsilon), axis=1, keep_dims=True)\n                    entropy = -tf.reduce_sum(self.action_prob * tf.log(self.action_prob + _epsilon),\n                                             axis=1, keep_dims=True)  # for exploration\n                    self.actor_loss = tf.reduce_mean(-(log_action_prob * tf.stop_gradient(td_error) + _entropy_param * entropy))\n\n                with tf.name_scope('local_gradient'):\n                    self.actor_grads = tf.gradients(self.actor_loss, self.actor_variables)\n                    self.critic_grads = tf.gradients(self.critic_loss, self.critic_variables)\n\n            with tf.name_scope('sync'):\n                with tf.name_scope('pull'):\n                    self.pull_actor_variables_op = [l_p.assign(g_p) for l_p, g_p in zip(self.actor_variables, global_net.actor_variables)]\n                    self.pull_critic_variables_op = [l_p.assign(g_p) for l_p, g_p in zip(self.critic_variables, global_net.critic_variables)]\n                with tf.name_scope('push'):\n                    self.update_actor_op = actor_optimizer.apply_gradients(zip(self.actor_grads, global_net.actor_variables))\n                    self.update_critic_op = critic_optimizer.apply_gradients(zip(self.critic_grads, global_net.critic_variables))\n\n    def _build_AC_net(self, scope):\n        init_weights = tf.random_normal_initializer(0., .1)\n        with tf.variable_scope('actor'):\n            policy_fc_1 = tf.layers.dense(self.state, 200, tf.nn.relu6, kernel_initializer=init_weights, name='policy_fc1')\n            action_prob = tf.layers.dense(policy_fc_1, num_action, tf.nn.softmax, kernel_initializer=init_weights, name='action_prob')\n        with tf.variable_scope('critic'):\n            v_fc_1 = tf.layers.dense(self.state, 100, tf.nn.relu6, kernel_initializer=init_weights, name='v_fc1')\n            v = tf.layers.dense(v_fc_1, 1, kernel_initializer=init_weights, name='state_value')  # state value\n        actor_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '\/actor')\n        critic_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '\/critic')\n        return action_prob, v, actor_variables, critic_variables\n\n    def update_global_variables(self, feed_dict):  # run by a local\n        sess.run([self.update_actor_op, self.update_critic_op], feed_dict)  # local grads applies to global net\n\n    def pull_global_variables(self):  # run by a local\n        sess.run([self.pull_actor_variables_op, self.pull_critic_variables_op])\n\n    def act(self, state):  # run by a local\n        prob_distribution = sess.run(self.action_prob, feed_dict={self.state: state[np.newaxis, :]})\n        action = np.random.choice(range(prob_distribution.shape[1]),\n                                  p=prob_distribution.ravel())  # select action w.r.t the actions prob\n        return action","fc066faf":"class Worker(object):\n    def __init__(self, name, global_net):\n        self.env = gym.make(game_name).unwrapped\n        self.name = name\n        self.A3C = A3CNet(name, global_net)\n\n    def work(self):\n        global _global_episode, _global_reward_records\n        state_buffer, action_buffer, reward_buffer = [], [], []\n        while _global_episode < _max_global_episode and not coordinator.should_stop():\n            state = self.env.reset()\n            episode_reward = 0\n            step = 0\n            while True:\n                if _render_flag:\n                    if self.name == 'worker_0':   # visualization of worker 0 \n                        self.env.render()\n                action = self.A3C.act(state)\n                next_state, reward, terminal, info = self.env.step(action)\n                step += 1\n                if step == 200:\n                    terminal = True\n                state_buffer.append(state)\n                action_buffer.append(action)\n                reward_buffer.append(reward)\n                episode_reward += reward\n\n                if terminal or step % _update_global_iter == 0:   # update global and assign to local net\n                    if terminal:\n                        v_next_state = 0   # terminal\n                    else:\n                        v_next_state = sess.run(self.A3C.v, {self.A3C.state: next_state[np.newaxis, :]})[0, 0]\n                    target_v_buffer = []\n                    for reward in reward_buffer[::-1]:    # reverse buffer r\n                        v_next_state = reward + _gamma * v_next_state\n                        target_v_buffer.append(v_next_state)\n                    target_v_buffer.reverse()\n\n                    state_buffer, action_buffer, target_v_buffer = np.vstack(state_buffer), np.array(action_buffer), np.vstack(target_v_buffer)\n                    feed_dict = {\n                        self.A3C.state: state_buffer,\n                        self.A3C.action: action_buffer,\n                        self.A3C.target_v: target_v_buffer,\n                    }\n                    self.A3C.update_global_variables(feed_dict)\n\n                    state_buffer, action_buffer, reward_buffer = [], [], []\n                    self.A3C.pull_global_variables()\n\n                state = next_state\n                \n                if terminal:\n                    if len(_global_reward_records) == 0:  # record running episode reward\n                        _global_reward_records.append(episode_reward)\n                    else:\n                        _global_reward_records.append(0.99 * _global_reward_records[-1] + 0.01 * episode_reward)\n                    print(\n                        self.name,\n                        \"episode:\", _global_episode,\n                        \"| episode_reward: %i\" % _global_reward_records[-1],\n                          )\n                    _global_episode += 1\n                    break","319fcf01":"if __name__ == \"__main__\":\n    sess = tf.Session()\n    with tf.device(\"\/cpu:0\"):\n        actor_optimizer = tf.train.RMSPropOptimizer(_actor_lr, name='RMSPropActor')\n        critic_optimizer = tf.train.RMSPropOptimizer(_critic_lr, name='RMSPropCritic')\n        global_a3c_net = A3CNet(_global_net_scope)  # we only need its params\n        workers = []\n        # Create worker\n        for i in range(n_workers):\n            worker_name = 'worker_%i' % i   # worker name\n            workers.append(Worker(worker_name, global_a3c_net))\n    coordinator = tf.train.Coordinator()\n    sess.run(tf.global_variables_initializer())\n\n    if output_graph:\n        if os.path.exists(log_dir):\n            shutil.rmtree(log_dir)\n        tf.summary.FileWriter(log_dir, sess.graph)\n\n    worker_threads = []\n    for worker in workers:   # create threads for workers\n        job = lambda: worker.work()\n        thread = threading.Thread(target=job)\n        thread.start()\n        worker_threads.append(thread)\n    coordinator.join(worker_threads)\n    # plot results\n    plt.plot(np.arange(len(_global_reward_records)), _global_reward_records)\n    plt.xlabel('step')\n    plt.ylabel('weighted moving average reward')\n    plt.show()","5f441ae5":"## Worker ","91dca49b":"## High Level Strucutre of A3C\n- **Global Network**: consist of a neural network to process the input states, and the output layers consists of value and policy estimations.\n- **Local Network (worker)**: same structure as the global network.\n\n## How A3C works\n1. Each worker initializes its network parameters equal to the global network.\n2. Each worker interacts with its own environment and accumulates experience in the form of tuples (observation, action, reward, terminal, value) after every interaction.\n3. For every several iterations, we calculate the value and policy losses. Note that we also calculate an entropy of the policy to increase exploration. Then, we can calculate the gradients.\n4. We then use the gradients from to update the global network parameters. The global network is constantly updated by each worker as they interact with its own environment. Because that each worker has it's own environment and different exploartion policy, the overall experience for training is more diverse.\n5. This concludes one round-trip (episode) of training. Then it repeats (1\u20135).","aa2a7f5b":"## Reference\n* [Reinforcement Learning Methods and Tutorials](https:\/\/github.com\/MorvanZhou\/Reinforcement-learning-with-tensorflow)\n","4fd42b85":"## Create threads and Run the codes","1d54d26b":"# Asynchronous Advantage Actor-Critic (A3C)\n## Terminology in Today's Topic\n\n- **Actor-Critic**: a typical model-free reinforcement learning method. A3C is an asynchronous version of actor-critic.\n\n- **Asynchronous Workers**: multiple workers running in different threads in parallel (on a single machine). Each worker can use different exploration policies to make the samples more diverse. \n\n## Advantages of A3C over Actor-Critic\n- **Sample Diversity**: By running different exploration policies in different threads, the overall changes being made to the parameters by multiple actor-learners applying online updates in parallel are likely to be less correlated in time than a single agent applying online updates.\n\n- **Less Training Time**: the training will speed up by using multiple parallel workers. \n\n- **Stability**: A3C model is more robust to the choice of learning rate and random initilization.\n\nNow it's time to introduce A3C model, which is an asynchronous version of actor-critic algorithm. Following is an illustration for A3C framework.\n\n![A3C_framework.jpg](attachment:A3C_framework.jpg)\n\nTo learn more details, please refer to the [paper](https:\/\/arxiv.org\/pdf\/1602.01783.pdf). \n\nNext, We will implement A3C in the CartPole task.\n\n\n## OpenAI gym: CartPole\n\n![poster.jpg](attachment:poster.jpg)\n\nA pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n\n- **Observation**: a tuple which includes cart position, cart velocity, pole angle and pole velocity at tip.\n- **Action**: includes two discrete actions, push cart to the left or push cart to the right.\n- **Reward**: reward is 1 for every step taken, including the termination step.\n- **Starting state**: all observations are assigned a uniform random value between \u00b10.05.\n- **Episode termination**: \n   * pole Angle is more than \u00b112\u00b0\n   * Cart Position is more than \u00b12.4 (center of the cart reaches the edge of the display)\n   * Episode length is greater than 200\n\nTo see more details about this environment, please refer to [OpenAI CartPole](https:\/\/github.com\/openai\/gym\/wiki\/CartPole-v0).\n"}}