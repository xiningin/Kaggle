{"cell_type":{"84c458ec":"code","b514cc2c":"code","da480bf0":"code","1c1d5de2":"code","643bef31":"code","2471e7f8":"code","8f2c722b":"code","a5d9d923":"code","dbdda2f1":"code","c5ca269e":"code","4b774c33":"markdown","2031c1c6":"markdown","6057494b":"markdown","22081416":"markdown","38104acf":"markdown","7ce8d816":"markdown","e78febb6":"markdown","838bcf3c":"markdown","1e0a9b14":"markdown"},"source":{"84c458ec":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import Model\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\nfrom keras.constraints import max_norm\n\nsns.set(style=\"white\", context=\"talk\")","b514cc2c":"train = pd.read_csv(\"..\/input\/train.csv\")\n\nimage_length = 28\nimage_size = image_length**2\n\ny_train_full = train[\"label\"].values\ny_train_full = to_categorical(y_train_full, num_classes=10)\nX_train_full = train.drop(labels=\"label\", axis=\"columns\").values\nX_train_full = X_train_full.reshape((-1, image_length, image_length, 1))\n\ntest = pd.read_csv(\"..\/input\/test.csv\")\nX_test = test.values\nX_test = X_test.reshape((-1, image_length, image_length, 1))\n\ninput_shape = (image_length, image_length, 1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full,\n                                                  test_size=.1, random_state=0)","da480bf0":"datagen = ImageDataGenerator(rotation_range=10,\n                             width_shift_range=.1,\n                             height_shift_range=.1,\n                             shear_range=10,\n                             zoom_range=.1)\ndatagen.fit(X_train)","1c1d5de2":"convolution_constraint = max_norm(3, axis=[0, 1, 2])\ndense_constraint = max_norm(3)\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=64, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\", input_shape=input_shape))\n\nmodel.add(Conv2D(filters=64, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\"))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(.5))\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\"))\n\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\"))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(.5))\nmodel.add(Flatten())\nmodel.add(Dense(units=256, kernel_constraint=dense_constraint,\n                activation=\"relu\"))\nmodel.add(Dropout(.5))\nmodel.add(Dense(units=10, activation='softmax'))","643bef31":"model.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","2471e7f8":"epochs = 150\nbatch_size = 378\nhistory = model.fit_generator(\n    datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_val, y_val),\n    epochs=epochs, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n)","8f2c722b":"epochs = np.array(history.epoch) + 1\ntrain_accuracies = history.history[\"acc\"]\nvalidation_accuracies = history.history[\"val_acc\"]\n\n# Add a trend line for the validation accuracies based on the reciprocal of the epoch number.\nregression = LinearRegression()\nepoch_features = np.array([1\/epochs]).T\ntrend = regression.fit(epoch_features, validation_accuracies).predict(epoch_features)\nasymptote = regression.intercept_\n\nfig, ax = plt.subplots(figsize=(10, 8))\nax.plot(epochs, train_accuracies, label=\"Training\")\nax.axhline(asymptote, color=\"grey\", linestyle=\":\")\nax.plot(epochs, trend, color=\"grey\", linestyle=\"--\")\nax.plot(epochs, validation_accuracies, label=\"Validation\")\n\nax.legend(loc=\"lower right\")\nax.set_ylim([.96, 1])\nax.set_xlim([0, max(epochs)])\nax.set_title(\"Learning curve\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epoch\")\nplt.show()","a5d9d923":"final_train_accuracy = np.mean(train_accuracies[-10:])\nfinal_validation_accuracy = np.mean(validation_accuracies[-10:])\nprint(\"Final training accuracy:\\t{:.4f}\".format(final_train_accuracy))\nprint(\"Final validation accuracy:\\t{:.4f}\".format(final_validation_accuracy))","dbdda2f1":"model = Sequential()\nmodel.add(Conv2D(filters=64, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\", input_shape=input_shape))\n\nmodel.add(Conv2D(filters=64, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\"))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(.5))\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\"))\n\nmodel.add(Conv2D(filters=128, kernel_size=3, padding=\"same\",\n                 kernel_constraint=convolution_constraint,\n                 activation=\"relu\"))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(.5))\nmodel.add(Flatten())\nmodel.add(Dense(units=256, kernel_constraint=dense_constraint,\n                activation=\"relu\"))\nmodel.add(Dropout(.5))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","c5ca269e":"epochs = 150\nbatch_size = 420\nmodel.fit_generator(\n    datagen.flow(X_train_full, y_train_full, batch_size=batch_size),\n    epochs=epochs, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n)\n\npredictions = model.predict(X_test).argmax(axis=1)\n\npredictions_count = predictions.size\nsubmission = pd.DataFrame({\"Label\": predictions, \"ImageId\": range(1, predictions_count + 1)}, columns=[\"ImageId\", \"Label\"])\nsubmission.to_csv(\"submission.csv\", index=False)","4b774c33":"# Create model","2031c1c6":"# Introduction\n\nHow well can we classify MNIST with a neural network, without having to think about it very hard or spend a long time experimenting?\n\nIn this notebook, we will set up a typical convolutional net, and spend a while training it. We won't do any hyperparameter tuning.\n\nWe'll see that it performs competitively (> 99.5% accuracy), with no apparent danger of overfitting.","6057494b":"# Get the data and prepare it","22081416":"We will use a standard architecture for our model, following established rules of thumb:\n* Short sequences (just 2) of convolutional layers with small (size 3) kernels\n* Padding the convolutions so that the size of the image is not changed\n* Max pooling (size 2) after each sequence of convolutional layers\n* A substantial number (64) of kernels in the first layer, doubling after every pool\n* Once the image is quite small (7 by 7), finishing with a dense final layer with a large number of units (256)\n* Dropout after every pooling layer and after the dense final layer, with probability of 1\/2 of dropping out\n* Constrain every weight vector to have a max norm of 3\n* ReLU activations throughout, with a softmax for the output","38104acf":"The final validation accuracy is around 99.5% to 99.6%.\n\nNote that the training accuracy is lower than the validation accuracy because dropout is applied during training. I'm impressed that the net still does a good job in this situation, approaching a 99% accuracy when half of its neurons are randomly going missing! It must be learning some robust features.\n\nThe validation accuracy has plateaued and shows no sign of falling due to overfitting -- I experimented with training for a few hundred more epochs and this remained the case, even as the training loss levelled off to its minimum. I noticed that the validation accuracy was almost perfectly fit by a reciprocal trend line, which I've included on the graph as a dashed line, along with a dotted line indicating the asymptote of this trend line. This further corroborates the validation accuracy having plateaued at its final maximum value. It appears that dropout is doing an excellent job of preventing overfitting, and we don't need to worry about training the net too much.\n\nFinally, we retrain the net using the full training data set, before submitting our predictions. This should boost the final test accuracy a little. At this point we could also make an ensemble of several such nets, randomly initialised; this should boost the final accuracy a little more, approaching a value of 99.6% to 99.7%.","7ce8d816":"# Train model\n\nWe will use a standard loss function (categorical cross-entropy).\n\nWe will use the Adam optimiser because it performs well without fine-tuning.\n\nNote that I used a Kaggle GPU, which makes the process much faster.","e78febb6":"# Set up data augmentation","838bcf3c":"# Results","1e0a9b14":"We will use basic data augmentation, applying modest linear transformations."}}