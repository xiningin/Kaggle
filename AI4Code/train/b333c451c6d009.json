{"cell_type":{"537e7320":"code","7ee09be6":"code","38d75634":"code","11ec860a":"code","820b0bc1":"code","3e1a5911":"code","693eade0":"code","2e11226c":"code","3443bf8c":"code","d7f3afd9":"code","68b77549":"code","3d21ca70":"code","9aefca73":"code","e4396c4a":"code","b02a7649":"code","d4f0b842":"code","eba170c3":"code","d8b27184":"code","11cb63cf":"code","69fcdda3":"code","b9f05e9d":"code","0731a5b3":"code","61ce5a75":"code","5808714e":"code","ea246fb6":"code","423fea8a":"code","f0ea85b7":"code","093d4edb":"code","1d5cea91":"code","44480b7b":"code","0c0131e2":"code","69e53c62":"code","bfc869de":"code","a2733c4f":"code","ea8d684a":"code","1a98addd":"code","265db5c1":"code","e9912823":"code","fdf979ed":"code","edc730f3":"markdown","9c03afb5":"markdown","845505b5":"markdown","114e636c":"markdown","a8df5125":"markdown","302906db":"markdown","c2536a89":"markdown","ba761930":"markdown","5a4162bd":"markdown","78c93858":"markdown","942cb857":"markdown","18292125":"markdown","559f87e2":"markdown","f640f2b0":"markdown","ad357fc3":"markdown","ac239cad":"markdown","4ddd2384":"markdown","10ebadac":"markdown","11bdcdf5":"markdown","46fdfa7b":"markdown"},"source":{"537e7320":"# ALL imports\nimport warnings  \nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style; style.use('ggplot')\n\nimport re\nimport xgboost as xgb\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding\nfrom keras.utils import to_categorical\n","7ee09be6":"# Create dataframes train and test\ntrain = pd.read_csv('..\/input\/drugsComTrain_raw.csv')\ntest = pd.read_csv('..\/input\/drugsComTest_raw.csv')\nprint(len(train))\nprint(len(test))","38d75634":"# Sort train dataframe from most to least useful\nuseful_train = train.sort_values(by='usefulCount', ascending=False)\nuseful_train.iloc[:10]","11ec860a":"# Print top 3 most useful reviews\nprint(\"3 most useful reviews: \\n\")\nfor i in useful_train.review.iloc[:3]:\n    print(i, '\\n')","820b0bc1":"# Print 3 of the least useful reviews\nprint(\"3 of the least useful reviews: \\n\")\nfor i in useful_train.review.iloc[-3:]:\n    print(i, '\\n')","3e1a5911":"\nwc = WordCloud(stopwords=STOPWORDS).generate(train.review[1])\nplt.imshow(wc)","693eade0":"\"\"\"\n# Create a list of all drugs and their average ratings, cast to dataframe\nrate_ls = []\n\nfor i in train.drugName.unique():\n    \n    # Only consider drugs that have at least 10 ratings\n    if np.sum(train.drugName == i) >= 10:\n        rate_ls.append((i, np.sum(train[train.drugName == i].rating) \/ np.sum(train.drugName == i)))\n    \navg_rate = pd.DataFrame(rate_ls)\n\"\"\"","2e11226c":"# Sort drugs by their ratings, look at top 10 best and worst rated drugs\n#avg_rate = avg_rate.sort_values(by=[1], ascending=False).reset_index(drop=True)\n#avg_rate[:10]","3443bf8c":"#avg_rate[-10:]","d7f3afd9":"# 10 most common conditions\nconditions = train.condition.value_counts().sort_values(ascending=False)\nconditions[:10]","68b77549":"conditions[:10].plot(kind='bar')\nplt.title('Histogram of Review Counts for the 10 Most Common Conditions')\nplt.xlabel('Condition')\nplt.ylabel('Number of Reviews');","3d21ca70":"# Empirical Distribution of Ratings\ntrain.rating.hist(bins=10)\nplt.title('Distribution of Ratings')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.xticks([i for i in range(1, 11)]);","9aefca73":"rating_avgs = (train['rating'].groupby(train['drugName']).mean())\nrating_avgs.hist(color='skyblue')\nplt.title('Distribution of average drug ratings')\nplt.xlabel('Rating')\nplt.ylabel('Count')","e4396c4a":"rating_avgs = (train['rating'].groupby(train['condition']).mean())\nrating_avgs.hist(color='skyblue')\nplt.title('Averages of medication reviews for each disease')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.show()","b02a7649":"# Is rating correlated with usefulness of the review?\nplt.scatter(train.rating, train.usefulCount, c=train.rating.values, cmap='tab10')\nplt.title('Useful Count vs Rating')\nplt.xlabel('Rating')\nplt.ylabel('Useful Count')\nplt.xticks([i for i in range(1, 11)]);","d4f0b842":"# Create a list (cast into an array) containing the average usefulness for given ratings\nuse_ls = []\n\nfor i in range(1, 11):\n    use_ls.append([i, np.sum(train[train.rating == i].usefulCount) \/ np.sum([train.rating == i])])\n    \nuse_arr = np.asarray(use_ls)\n\nplt.scatter(use_arr[:, 0], use_arr[:, 1], c=use_arr[:, 0], cmap='tab10', s=200)\nplt.title('Average Useful Count vs Rating')\nplt.xlabel('Rating')\nplt.ylabel('Average Useful Count')\nplt.xticks([i for i in range(1, 11)]);","eba170c3":"train.review.head()","d8b27184":"# Input: s - a string\n# Output: a string with any enclosing quotation marks removed\ndef remove_enclosing_quotes(s):\n    if s[0] == '\"' and s[-1] == '\"':\n        return s[1:-1]\n    else:\n        return s\n    \ntrain.review = train.review.apply(remove_enclosing_quotes)\ntest.review = test.review.apply(remove_enclosing_quotes)\ntrain.head()","11cb63cf":"import re\ntrain.review = train.review.apply(lambda x: re.sub(r'&#\\d+;',r'', x))\ntest.review = test.review.apply(lambda x: re.sub(r'&#\\d+;',r'', x))","69fcdda3":"# Inputs:\n#    data_frame - a pandas data frame containing text columns\n#    text_cols - a list of column names in data_frame containing the text columns to be combined\n# Outputs:\n#    text_data - a dataframe containing a single column which joins all columns in the text_cols\n#                list into a single column separated by spaces.\n# Side-Effects: n\/a\ndef combine_text_columns(data_frame, text_cols):\n    \"\"\"\n    Converts all text in each row of data_frame to a single vector\n    \"\"\"\n    #col_list = list(set(text_cols) & set(data_frame.columns.tolist()))\n    text_data = data_frame[text_cols]\n    \n    text_data.fillna(\"\", inplace=True)\n    \n    return text_data.apply(lambda x: \" \".join(x), axis=1)","b9f05e9d":"# Create the text_vector combining all  text columns\ntext_cols = ['drugName', 'condition', 'review']\ntrain['text'] = combine_text_columns(train, text_cols)\ntest['text'] = combine_text_columns(test, text_cols)\ntrain[['text', 'rating', 'usefulCount']].head()","0731a5b3":"# The token pattern grouping alpha-numerically\nTOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n\n# Instantiation of CountVectorizer object\nvec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2), lowercase=True, stop_words='english', min_df=2, max_df=0.99) # added min_df param\n\n# Fit to training data\nX = vec_alphanumeric.fit_transform(train.text)\n\n# Create a column with binary rating indicating the polarity of a review\ntrain['binary_rating'] = train['rating'] > 5\n\ny = train.binary_rating\n\n# Print the total number of tokesn and first 15 tokens\nmsg = 'There are {} tokens in the review column if we split on non-alpha numeric.'\nprint(msg.format(len(vec_alphanumeric.get_feature_names())))\nprint(vec_alphanumeric.get_feature_names()[:30])","61ce5a75":"# Train test split\n# NB: We are using the data in the drugsComTest_raw.csv file as a holdout set for final performance evaluation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.1) # Default is 75%\/25% split\n\nclf = MultinomialNB().fit(X_train, y_train)\n\npred = clf.predict(X_test)\n\nprint(\"Accuracy on training set: {}\".format(clf.score(X_train, y_train)))\nprint(\"Accuracy on test set: {}\".format(clf.score(X_test, y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(pred, y_test))","5808714e":"\"\"\"\nalphas = np.linspace(0, 2, 10)\ntrain_accs = []\ntest_accs = []\n\nfor a in alphas:\n    clfa = MultinomialNB(alpha=a).fit(X_train, y_train)\n    pred = clfa.predict(X_test)\n    train_accs.append(clfa.score(X_train, y_train))\n    test_accs.append(clfa.score(X_test, y_test))\n    \nplt.plot(alphas, train_accs, 'ro')\nplt.plot(alphas, test_accs, 'go')\nplt.show()\n\"\"\"","ea246fb6":"# Projects hold-out set's text into the space formed by the test corpus\nX_validation = vec_alphanumeric.transform(test.text)\n\n# Binarizes ratings for the validation set\ntest['binary_rating'] = test['rating'] > 5\ny_validation = test.binary_rating\n\nclf = MultinomialNB(alpha=0.0).fit(X, y)\n\n#\nprint(\"Accuracy on test set: {}\".format(clf.score(X_validation, y_validation)))\npred_validation = clf.predict(X_validation)\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_validation, pred_validation))\nprint(classification_report(y_validation, pred_validation))","423fea8a":"# The token pattern grouping alpha-numerically\nTOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n\n# Instantiation of CountVectorizer object\nvec_alphanumeric = TfidfVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2), lowercase=True, stop_words='english', min_df=2, max_df=0.99) # added min_df param\n\n# Fit to training data\nX = vec_alphanumeric.fit_transform(train.text)\n\n# Create a column with binary rating indicating the polarity of a review\ntrain['binary_rating'] = train['rating'] > 5\n\ny = train.binary_rating\n\n# Print the total number of tokesn and first 15 tokens\nmsg = 'There are {} tokens in the review column if we split on non-alpha numeric.'\nprint(msg.format(len(vec_alphanumeric.get_feature_names())))\nprint(vec_alphanumeric.get_feature_names()[:30])","f0ea85b7":"# Train test split\n# NB: We are using the data in the drugsComTest_raw.csv file as a holdout set for final performance evaluation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.1) # Default is 75%\/25% split\n\nclf_lr = LogisticRegression(penalty='l2', C=100).fit(X_train, y_train)\n\npred = clf_lr.predict(X_test)\n\nprint(\"Accuracy on training set: {}\".format(clf_lr.score(X_train, y_train)))\nprint(\"Accuracy on test set: {}\".format(clf_lr.score(X_test, y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","093d4edb":"# Projects hold-out set's text into the space formed by the test corpus\nX_validation = vec_alphanumeric.transform(test.text)\n\n# Binarizes ratings for the validation set\ntest['binary_rating'] = test['rating'] > 5\ny_validation = test.binary_rating\n\nclf_lr = LogisticRegression(penalty='l2', C=100).fit(X, y)\n\n#\nprint(\"Accuracy on test set: {}\".format(clf_lr.score(X_validation, y_validation)))\npred_validation = clf_lr.predict(X_validation)\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_validation, pred_validation)\/len(y_validation))\nprint(classification_report(y_validation, pred_validation))","1d5cea91":"# roc curve\ny_pred_prob = clf_lr.predict_proba(X_validation)[:,1]\n\nfpr, tpr, thresholds = roc_curve(y_validation, y_pred_prob)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Logistic Regression Classifier\")\nplt.show()\n\nprint(\"AUC: {}\".format(roc_auc_score(y_validation, y_pred_prob)))","44480b7b":"# Train test split\n# NB: We are using the data in the drugsComTest_raw.csv file as a holdout set for final performance evaluation.\n\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y) # Default is 75%\/25% split\n\nclf_xg = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, max_depth=4).fit(X_train, y_train)\n\npred = clf_xg.predict(X_test)\n\nprint(\"Accuracy on training set: {}\".format(clf_xg.score(X_train, y_train)))\nprint(\"Accuracy on test set: {}\".format(clf_xg.score(X_test, y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(pred, y_test))\n\"\"\"","0c0131e2":"# Train test split\n# NB: We are using the data in the drugsComTest_raw.csv file as a holdout set for final performance evaluation.\n\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y) # Default is 75%\/25% split\n\nclf_rfc = RandomForestClassifier().fit(X_train, y_train)\n\npred = clf_rfc.predict(X_test)\n\nprint(\"Accuracy on training set: {}\".format(clf_rfc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {}\".format(clf_rfc.score(X_test, y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(pred, y_test))\n\"\"\"","69e53c62":"# Train test split\n# NB: We are using the data in the drugsComTest_raw.csv file as a holdout set for final performance evaluation.\n\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y) # Default is 75%\/25% split\n\nclf_svc = SVC().fit(X_train, y_train)\n\npred = clf_svc.predict(X_test)\n\nprint(\"Accuracy on training set: {}\".format(clf_svc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {}\".format(clf_svc.score(X_test, y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(pred, y_test))\n\"\"\"","bfc869de":"\"\"\"\n# Instantiation of CountVectorizer object\nvec_alphanumeric = TfidfVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2), lowercase=True, stop_words='english', max_df=0.99, min_df=2, max_features=200000) # added min_df param\n\n# Fit to training data\nX = vec_alphanumeric.fit_transform(train.text)\n\n# Create a column with binary rating indicating the polarity of a review\ntrain['binary_rating'] = train['rating'] > 5\n\ny = train.binary_rating\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.1) \n\nmodel = Sequential()\nmodel.add(Dense(units=4, activation='relu', input_dim=len(vec_alphanumeric.get_feature_names()))) # 4 nodes - 6 min\/epoch\nmodel.add(Dense(units=2, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nhistory = model.fit(X_train, y_train, epochs=4, batch_size=128, verbose=1, validation_data=(X_test, y_test)) # ~5 mins\/epoch\n\n# Projects hold-out set's text into the space formed by the test corpus\nX_validation = vec_alphanumeric.transform(test.text)\n\n# Binarizes ratings for the validation set\ntest['binary_rating'] = test['rating'] > 5\ny_validation = test.binary_rating\n\ny_validation = to_categorical(y_validation)\nscore = model.evaluate(X_validation, y_validation, batch_size=128)\n\nprint(score)\n\ny_cat = to_categorical(y)\nfinal = model.fit(X, y_cat, epochs=2, batch_size=128, verbose=1, validation_data=(X_validation, y_validation)) # ~6 mins\/epoch\n\"\"\"","a2733c4f":"#scaler = StandardScaler().fit(X)\n#X_standardized = scaler.transform(X)\n#standardized_X_test = scaler.transform()","ea8d684a":"print(\"The length of a tokenized vector is {}.\".format(X.shape[1]))\nprint(\"The number of records is {}.\".format(X.shape[0]))","1a98addd":"TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\nnum_features = 1000\n\npl = Pipeline([\n    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))),\n    #('dim_red', SelectKBest(chi2, num_features)),\n    ('clf', MultinomialNB())\n])\n\n#pl.fit(train.text, train.binary_rating)\n#pl.score(train.text, train.binary_rating)","265db5c1":"from itertools import combinations\n\nimport numpy as np\nfrom scipy import sparse\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n\n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n\n        spi = self._create_sparse_interactions(X)\n        return spi\n\n    def get_feature_names(self):\n        return self.feature_names\n\n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n\n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n\n                # get column multiplications value\n                out = X[:, col_ixs[0]]\n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)","e9912823":"# HashingVectorizer could potentially speed this kernel up\n# may also be useful if we want to try out more complex models\n\n\ntext_data = train.text\nTOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\nhashing_vect = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC, stop_words='english', n_features=200000, norm=None, binary=False, ngram_range=(1,2))\nhashed_text = hashing_vect.fit_transform(text_data)\nhashed_df = pd.DataFrame(hashed_text.data)\nhashed_text.shape","fdf979ed":"# Train test split\n# NB: We are using the data in the drugsComTest_raw.csv file as a holdout set for final performance evaluation.\nX_train, X_test, y_train, y_test = train_test_split(hashed_text, y, random_state=42, stratify=y) # Default is 75%\/25% split\n\nclf_lr = LogisticRegression(penalty='l1', solver='liblinear', C=3).fit(X_train, y_train)\n\npred = clf_lr.predict(X_test)\n\nprint(\"Accuracy on training set: {}\".format(clf_lr.score(X_train, y_train)))\nprint(\"Accuracy on test set: {}\".format(clf_lr.score(X_test, y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(pred, y_test))","edc730f3":"The disparity between the accuracy on the trainining versus test sets indicates that we may want to try hyperparameter tuning with our logistic regression model.","9c03afb5":"Notice that the reviews seem to be placed in quotes. Let's start by writing a simple script to remove these quotes.","845505b5":"We can explore how changing alpha affects the quality of our classifier.","114e636c":"Rather than using random forests (which typically perform poorly in terms of speed on vectorized text data due to the sparse nature of the data as the algoithm has to find the best split for each feature and select the one maximizing the information gain), we will try xgboost. Although this method suffers from the same scaling limitations as random forests, xgboost typically generalizes better due to the way the weaker classifying trees are adaptively boosted. ","a8df5125":"From the plot above, it appears we don't get significant variation in the test set accuracy for alpha between 0 and 1. As such, we'll continue to rely on the default setting for alpha (alpha=1). We can now evaluate our model on the hold-out set.","302906db":"## Preprocessing\n\nBefore we can begin trying different machine learning models, we need to prepare our data to be processed by these algorithms. A major challenge with this data set is the nearly 200,000 reviews and the size of the vectorized corpus. We will implement a few techniques for speeding up the model fitting and prediction process which will allow us to use models more advanced than Naive-Bayes for a data-set of this size.\n\nThe obvious starting question for such an approach is how can we convert the raw text of the review into a data representation that can be used by a numerical classifier. To this end, we will use the process of vectorization. By vectorizing the 'review' column, we can allow widely-varying lengths of text to be converted into a numerical format which can be processed by the classifier.\n\nThis process involves creating tokens (i.e. individual words or groups of words extracted from the text). Once the list of tokens is created, they are assigned an index integer identifier which allows them to be listed. We can then count the number of words in the document and normalize them in such a way that de-emphasizes words that appear frequently (like \"a\", \"the\", etc.). This creates what is known as a bag (multi-set) of words. Such a representation associates a real-valued vector to each review representing the importance of the tokens (words) in the review. This represents the entire corpus of reviews as a large matrix where each row of the matrix represents one of the reviews and each column repreents a token occurence.","c2536a89":"## Brief EDA","ba761930":"We won't repeat the exploratory data analysis used during the competition much in this notebook. We will only highlight a few insights that lead to thinking of methods to improve this kernel. The key observation came from exploring the most and least useful reviews.","5a4162bd":"We can possibly produce a better model by incorporating interaction terms. The function below was copied from [drivendataorg's github page](https:\/\/github.com\/drivendataorg\/box-plots-sklearn\/blob\/master\/src\/features\/SparseInteractions.py) to modify sklearn's PolynomialFeatures function to work with sparse matrices as input. \n\nAs we have already incorporated bigrams into our model, I did not find any significant gain in classification accuracy from experimenting with incorporating interaction terms. This makes sense because bigrams are probably sufficient to detect sentiment by phrase such as \"not good.\" It is harder to think of examples where interactions from words further away would be strongly predictive of sentiment in a way that paired words would not. As such, I did not spend much time exploring how to incorporate interaction terms although the problem is interesting statistically and computationally (due to the scaling issues). However, if we can get the hashing vectorizer to perform well, this may be another avenue to explore.","78c93858":"# KUC Winter 2018 Hackathon Revisited\n\nThis kernel was developed from our KUC Winter 2018 Hackathon submission for team NDL. The team consisted of  Izzidin Oakes, Neil Ashtekar, Suraj Dalsania, Ming Ju Li, and myself. Our original kernel is still available [here.](https:\/\/www.kaggle.com\/neilash\/team-ndl-algorithms-and-illnesses\/) Many parts of this kernel are lifted directly from that previous submission. I wanted to revise this by using a few tricks to improve the performance of our models. This kernel improves on the original submission by incorporating additional preprocessing techniques I've learned since that time.","942cb857":"To do:\n* use cross validation to set hyperparameters in logistic regression model and validate with auc_score rather than accuracy\n* group all preprocessing into pipeline\n* use logistic regression for identifying useful and negative reviews\n* experiment with hyperparameters in the vectorizer as a way of restricting the number of tokens\n* alternate tokenizations could improve performance\n* identifying useful feedback could be a more important problem than sentiment analysis","18292125":"The default implementation of CountVectorizer splits on white-space. For our specific problem, this gives us a larger number of tokens with little predictive value towards the sentiment of a review. For this reason, we choose to instead split by grouping continuous segments of alpha-numeric characters. We believe this will produce more semantically meaningful tokens and lead to better scalability and predictive power for our machine learning models.","559f87e2":"Halving the number of tokens only decreased test set accuracy by around 0.2%.\nHalving again only dropped the test accuracy by another 0.2%.","f640f2b0":"Another problem with the review text we noticed during our exploration of the data set is that when the data was scraped from the web, many punctuation characters were converted into their ascii representation. Since our alphanumeric tokenizer is splitting on punctuation, this  populates our vector with semantically meaningless strings corresponding to the ascii punctuation. We can use a regular expression to remove all occurences of this pattern from the review text which should further reduce the size of our tokens and eliminate more tokens with little predictive information.","ad357fc3":"If we want to try incorporating interaction terms, we need to reduce the size of our vectors. Considering all possible pairs of interaction terms adds 13008280456 (161297 choose 2). As such, we can probably take up to around 1500 of the best features to produce a matrix of comparable size. In order to perform feature selection, we will use the chi-squared test to choose our top features. I arbitrarily started with 300 features and then worked from there. The number of features we choose at this step is a hyperparameter which we could potentially optimize via cross validation in the future. At this point, our preprocessing is becoming sufficiently complex that it will be useful to incorporate a scikit-learn pipeline.","ac239cad":"This performance is about as good as the performance we achieved during the competition with more complex models (neural networks and random forest). This just goes to give yet another example of the power of effective preprocessing techniques. We can also try using logistic regression.","4ddd2384":"From the exploratory data analysis, we see that both the **drugName** and **condition** columns also have predictive value. For this reason, we will prepend each of these to the review and save the complete string as a single column called **text**.","10ebadac":"The above output indicates that the size of our vectorized reviews will be rather large which indicates that we'll need to do some tricks to make our learning algorithms (beyond Naive Bayes) perform well at this scale. Moreover, from the tokens above, it seems that we will need some form of preprocessing. If we think back to the data exploration performed earlier in the notebook, we can see that when the reviews were scraped from the web, there were certain formatting patterns which we could replace to get more meaningful tokens.","11bdcdf5":"Final performance worese than logistic regression with tfidf. On the next iteration, we will focus on hyperparameter tuning the logistic regression model and subsequent work with the neural network may be used to explore the hashing vectorizer.","46fdfa7b":"Increasing the number of estimators in the ensemble appears to improve accuracy; however, it appears we would need a very large number of estimators in the ensemble to get performance comparable to the other classifiers we've used. However, as we pointed it out previously, this type of method scales poorly for this sort of data. As such, we won't explore further xgboost models until we use the HashingVectorizer or one of the word vector representations available through the different NLP packages in Python.\n\nA random forest classifier will suffer the same drawbacks of xgboost in that the fitting process is inefficient on sparse data. However, we did experiment with it previously and were able to get decent accuracy."}}