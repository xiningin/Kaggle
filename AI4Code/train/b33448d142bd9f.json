{"cell_type":{"959b6422":"code","c65f31af":"code","48f63516":"code","c4c9fa74":"code","311ec4b3":"code","51c9d870":"code","51d2ba3c":"code","c09f310c":"code","1eeead2d":"code","fb486e13":"code","c16df960":"code","90e97073":"code","25aecad0":"code","18774bfe":"code","38395687":"code","26ff3ae0":"code","57d595ad":"code","94226afd":"code","d01f15d1":"code","8845a11d":"code","a26cbf3f":"code","e7cc8642":"code","c632fe7c":"code","4e94e815":"code","6c323a4d":"code","b09f9382":"code","ebe26993":"code","2cadbfbd":"code","89caa64f":"code","03cfb85a":"code","75d2eb07":"code","d6305c4a":"code","a7c9e90d":"code","4248af03":"code","6dc45747":"code","009362ca":"code","79e5b72f":"code","9a8c9155":"code","925f5a93":"code","054f06e7":"code","0b7ab259":"code","0d26cebf":"code","ea64c784":"code","5f9fa5b1":"code","db2ddfb6":"code","204e452b":"code","78c8ab9a":"code","2ec1db3d":"code","da8ddcd3":"code","8dc39b5f":"code","0776f993":"code","178e5356":"code","585517c6":"code","7c06a8a7":"code","7dc3bf83":"code","1771056b":"code","07b98ca2":"code","604c2fd1":"code","d804e157":"code","b06e6b3c":"code","7dcf9339":"code","e624be0f":"code","52d455b9":"code","67182a11":"code","bc4d42fb":"code","81458b69":"code","90e186bc":"code","34d944c3":"code","762728d2":"code","4f60177c":"markdown","2738c208":"markdown","2d7651f8":"markdown","aeba512e":"markdown","bd738c26":"markdown","d1433527":"markdown","f485fd2e":"markdown","c065068b":"markdown","37f6bc75":"markdown","425ddb53":"markdown","45352ec4":"markdown","13f08fa6":"markdown","82f3167a":"markdown","ee3852f0":"markdown","fb31e707":"markdown","a3b72994":"markdown","492d064d":"markdown","4194eaf6":"markdown","ddf0a9c2":"markdown","2b2c26f4":"markdown","ff0396d2":"markdown","44c6d8f9":"markdown","ea81ffd3":"markdown","9b689486":"markdown","053b1cbf":"markdown","257e7efb":"markdown","ab930fa4":"markdown","930e520b":"markdown","7ecf948c":"markdown","d74f2799":"markdown","1373b759":"markdown","fe19125d":"markdown","3ebafd47":"markdown","71a86f07":"markdown"},"source":{"959b6422":"from datetime import datetime, timedelta,date\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error\n\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.cluster import KMeans","c65f31af":"TelecomChurn = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n#first few rows\nTelecomChurn.head()","48f63516":"# data summary\nprint(\"Data dimension:\",TelecomChurn.shape)\nTelecomChurn.info()","c4c9fa74":"# encoding the churn variable into 0 and 1\nTelecomChurn['Churn'] = TelecomChurn['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\nTelecomChurn.head()","311ec4b3":"# changing TotalCharges column from object to float\nTelecomChurn['TotalCharges'] = TelecomChurn['TotalCharges'].apply(pd.to_numeric, downcast='float', errors='coerce')\nprint(TelecomChurn.dtypes)","51c9d870":"# check for null and total observations related to it\nnull_columns=TelecomChurn.columns[TelecomChurn.isnull().any()]\nTelecomChurn[null_columns].isnull().sum()","51d2ba3c":"# drop na variables\nTelecomChurn = TelecomChurn.dropna()\nTelecomChurn.shape","c09f310c":"# summary description of the numeric variables of the dataset\nTelecomChurn[['tenure', 'MonthlyCharges', 'TotalCharges']].describe()","1eeead2d":"# to check the number of unique values in each of the columns\nfor col in list(TelecomChurn.columns):\n    print(col, TelecomChurn[col].nunique())","fb486e13":"# calculate the proportion of churn vs non-churn\nTelecomChurn['Churn'].mean()","c16df960":"# calculate the proportion of churn by gender\nchurn_by_gender = TelecomChurn.groupby(by='gender')['Churn'].sum() \/ TelecomChurn.groupby(by='gender')['Churn'].count() * 100.0\nprint('Churn by gender:',churn_by_gender)","90e97073":"# calculate the proportion of churn by contract\nchurn_by_contract = TelecomChurn.groupby(by='Contract')['Churn'].sum() \/ TelecomChurn.groupby(by='Contract')['Churn'].count() * 100.0\nprint('Churn by contract:',churn_by_contract)","25aecad0":"# calculate the proportion of churn by payment method\nchurn_by_payment = TelecomChurn.groupby(by='PaymentMethod')['Churn'].sum() \/ TelecomChurn.groupby(by='PaymentMethod')['Churn'].count() * 100.0\nprint('Churn by payment method:',churn_by_payment)\npd.DataFrame(churn_by_payment)","18774bfe":"# figure\nax = churn_by_payment.plot(\n    kind='bar',\n    color='skyblue',\n    grid=False,\n    figsize=(10, 7),\n    title='Churn Rates by Payment Methods'\n)\n\nax.set_xlabel('Payment Methods')\nax.set_ylabel('Churn rate (%)')\n\nplt.show()","38395687":"# proportion of churn by gender and contract\nchurn_gendercontract = TelecomChurn.groupby(['gender', 'Contract'])['Churn'].sum()\/TelecomChurn.groupby(['gender', 'Contract'])['Churn'].count()*100\nchurn_gendercontract","26ff3ae0":"# keep gender in row and contract by column\nchurn_gendercontract1 = churn_gendercontract.unstack('Contract').fillna(0)\nchurn_gendercontract1 ","57d595ad":"# figure\nax = churn_gendercontract1.plot(\n    kind='bar', \n    grid= False,\n    figsize=(10,7)\n)\n\nax.set_title('Churn rates by Gender & Contract Status')\nax.set_xlabel('Gender')\nax.set_ylabel('Churn rate (%)')\n\nplt.show()","94226afd":"# observations by citizen type\nTelecomChurn['SeniorCitizen'].value_counts()","d01f15d1":"# Total observations by citizen type, contract and tech support \nTelecomChurn.groupby(['SeniorCitizen','Contract','TechSupport'])['Churn'].count()","8845a11d":"# proportion of churn by gender and contract\nchurn_citizentechcontract = TelecomChurn.groupby(['SeniorCitizen','Contract','TechSupport'])['Churn'].sum()\/TelecomChurn.groupby(['SeniorCitizen','Contract','TechSupport'])['Churn'].count()*100\nchurn_citizentechcontract","a26cbf3f":"# keep gender and payment method in row and contract by column\nchurn_citizentechcontract1 = churn_citizentechcontract.unstack(['TechSupport']).fillna(0)\nchurn_citizentechcontract1","e7cc8642":"# figure\nax = churn_citizentechcontract1.plot(\n    kind='bar', \n    grid= False,\n    figsize=(10,7)\n)\n\nax.set_title('Churn rates by Citizen Type, Tech Support & Contract Status')\nax.set_xlabel('Non-Senior:\"0\"   Senior:\"1\"')\nax.set_ylabel('Churn rate (%)')\n\nplt.xticks()\nplt.show()","c632fe7c":"# summary of tenure, monthly charges and total charges\nTelecomChurn[['tenure','MonthlyCharges','TotalCharges']].describe()","4e94e815":"# plot a histogram\nplt.hist(TelecomChurn['tenure'], bins= 100, alpha=0.5,)\nplt.title('Frequency Distribution by Tenure')\nplt.xlabel('Tenure')\nplt.ylabel('Frequency')\nplt.show()\n\n\nplt.hist(TelecomChurn['tenure'], cumulative=1, density =True, bins= 100)\nplt.title('Cumulative Frequency Distribution by Tenure')\nplt.xlabel('Tenure')\nplt.ylabel('Cumulative Frequency Distribution')\nplt.show()","6c323a4d":"# proportion of churn by tenure\nchurn_monthlycharges = TelecomChurn.groupby(by = 'tenure')['Churn'].mean().reset_index()\nchurn_monthlycharges\n\nplt.figure(figsize=(10,7))\nplt.scatter(churn_monthlycharges.tenure, churn_monthlycharges.Churn)\nplt.title('Churn Rate by Tenure')\nplt.xlabel('Tenure')\nplt.ylabel('Churn Rate')\nplt.show()","b09f9382":"# proportion of churn by MonthlyCharges\nchurn_monthlycharges = TelecomChurn.groupby(by = 'MonthlyCharges')['Churn'].mean().reset_index()\nplt.figure(figsize=(6,5))\nplt.scatter(churn_monthlycharges.MonthlyCharges, churn_monthlycharges.Churn)\nplt.title('Churn Rate by Monthly Charges')\nplt.xlabel('Monthly Charges')\nplt.ylabel('Churn Rate')\nplt.show()\n\n# proportion of churn by TotalCharges\nchurn_totalcharges = TelecomChurn.groupby(by = 'TotalCharges')['Churn'].mean().reset_index()\nplt.figure(figsize=(6,5))\nplt.scatter(churn_totalcharges.TotalCharges, churn_totalcharges.Churn)\nplt.title('Churn Rate by Total Charges')\nplt.xlabel('Total Charges')\nplt.ylabel('Churn Rate')\nplt.show()","ebe26993":"# finding correlations\ncorrdata = TelecomChurn[['Churn','tenure','MonthlyCharges','TotalCharges']]\ncorr = corrdata.corr()\n# plot the heatmap\nsns.heatmap(corr,cmap=\"coolwarm\",\n        xticklabels=corrdata.columns,\n        yticklabels=corrdata.columns,annot=True)","2cadbfbd":"# segmenting based on data type and pre-processing\n#customer id col\nId_col     = ['customerID']\n#Target column. y should be an array\ntarget_col = [\"Churn\"]\ny = (TelecomChurn[target_col]).values.ravel()\n# cluster column \ncluster_col = [\"tenure\"]\n#categorical columns with categories less than 6\ncat_cols   = TelecomChurn.nunique()[TelecomChurn.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\nprint(cat_cols)\n#Binary columns with 2 values\nbin_cols   = TelecomChurn.nunique()[TelecomChurn.nunique() == 2].keys().tolist()\nprint(bin_cols)\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\nprint(multi_cols)\n# continuous column\ncont_col = [\"tenure\",\"MonthlyCharges\"]\nprint(cont_col)\nprint(y)","89caa64f":"#Label encoding Binary columns\nle = LabelEncoder()\nbinary = TelecomChurn[bin_cols]\nprint(binary.shape) \nprint(binary.info())\nbinary.head()\nfor i in bin_cols :\n    binary[i] = le.fit_transform(binary[i])","03cfb85a":"# multi-label categorical columns\ndummy_vars = pd.get_dummies(TelecomChurn[multi_cols])\nprint(dummy_vars.shape)\nprint(dummy_vars.info())","75d2eb07":"#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(TelecomChurn[cont_col])\nscaled = pd.DataFrame(TelecomChurn,columns= cont_col)\nscaled.shape\nprint(scaled.info())","d6305c4a":"# creating a dataset to combine pre-processed variables\nX = pd.concat([binary,scaled,dummy_vars], axis = 1)\n# drop churn variable from the X dataset\nX = X.drop(['Churn'],axis=1)\nprint(X.shape)\nprint(X.info())","a7c9e90d":"# import machine learning libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nimport xgboost as xgb\n\n# creating the function\n# XGBoost and SVC functions are used while modeling and are thus not presented here\nlogreg = LogisticRegression(solver='lbfgs', max_iter = 10000)\nDT = DecisionTreeClassifier()\nrfc = RandomForestClassifier()\nknn = KNeighborsClassifier()","4248af03":"# recursive feature extraction for the top 15 features\nrfe = RFE(rfc, 10)\nrfe = rfe.fit(X, y)\nprint(rfe.support_)\nprint(rfe.ranking_)\n\n#identifying columns for RFE\nrfe_data = pd.DataFrame({\"rfe_support\" :rfe.support_,\n                       \"columns\" : [i for i in X.columns if i not in Id_col + target_col],\n                       \"ranking\" : rfe.ranking_,\n                      })\n\n# extract columns as a list\nrfe_var = rfe_data[rfe_data[\"rfe_support\"] == True][\"columns\"].tolist()\n\nrfe_data","6dc45747":"# select a subset of variables for the dataframe based on RFE method\nX1 = X[rfe_var]","009362ca":"# running a logistic regression\n# copy the dataset \nX2 = X1\n# manually add intercept\nX2['intercept'] = 1.0;\n#X2.head()\nlogit_model=sm.Logit(y,X2)\nresult=logit_model.fit()\nprint(result.summary2())","79e5b72f":"# create a train and test set with the new selected variables\nXtrain, Xtest, ytrain,ytest = train_test_split(X1,y,test_size = 0.2,random_state = 111)","9a8c9155":"print('Ratio of churn in the training sample:',ytrain.mean())\nprint('Ratio of churn in the training sample:',ytest.mean())","925f5a93":"parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(estimator=logreg, param_grid = parameters,cv = 10,scoring = 'accuracy')\ngrid.fit(Xtrain,ytrain)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","054f06e7":"def result(X_test,y_test):\n\n    y_test_pred = grid.predict(X_test)\n    print('Accuracy score:{:.2f}'.format(accuracy_score(y_test, y_test_pred)))\n    print(                                                                                )\n\n    confusionmat_data = pd.DataFrame({'y_Predicted': y_test_pred,'y_Actual': y_test},columns=['y_Actual','y_Predicted'])\n    confusion_matrix = pd.crosstab(confusionmat_data['y_Actual'], confusionmat_data['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n    print('Confusion Matrix:\\n {}\\n'.format(confusion_matrix))\n    print(                                                                               )\n\n    class_report = classification_report(y_test, y_test_pred)\n    print('Classification report:\\n {}\\n'.format(class_report))\n    print(                                                                               )\n\n    mse = mean_squared_error(y_test, y_test_pred)\n    rmse = np.sqrt(mse)\n    print('Mean-squared error:\\n {}\\n'.format(rmse))\n\n    # predict probabilities\n    #probs = grid.predict_proba(X_test)\n    #probs = grid.predict(X_test)\n\n    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n","0b7ab259":"# result\nresult(Xtest,ytest)","0d26cebf":"parameters = {'min_samples_split': [10,100,1000,10000],\n              'max_depth':[2,5,10,100,150,200,250]}\n\ngrid = GridSearchCV(estimator= DT, param_grid = parameters,cv = 10,scoring = 'accuracy')\ngrid.fit(Xtrain,ytrain)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","ea64c784":"grid.best_estimator_.feature_importances_\nfor name, importance in sorted(zip(grid.best_estimator_.feature_importances_,Xtrain.columns),reverse= True)[:5]:\n    print(name, importance)\n    \nfeatureimp_plot = pd.Series(grid.best_estimator_.feature_importances_, index = Xtrain.columns)\nfeatureimp_plot.nlargest(5).plot(kind='barh')  ","5f9fa5b1":"# result\nresult(Xtest,ytest)","db2ddfb6":"parameters = {'n_estimators': [1,5,10,100,200],'min_samples_split': [10,100,1000,10000],'max_depth':[2,5,10,100,150,200,250]}\n\ngrid = GridSearchCV(estimator= rfc, param_grid = parameters,cv = 10,scoring = 'accuracy')\ngrid.fit(Xtrain,ytrain)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","204e452b":"grid.best_estimator_.feature_importances_\nfor name, importance in sorted(zip(grid.best_estimator_.feature_importances_,Xtrain.columns),reverse= True)[:5]:\n    print(name, importance)\n    \nfeatureimp_plot = pd.Series(grid.best_estimator_.feature_importances_, index = Xtrain.columns)\nfeatureimp_plot.nlargest(5).plot(kind='barh')   ","78c8ab9a":"# result\nresult(Xtest,ytest)","2ec1db3d":"#building the model & printing the score\nparameter = {\n'max_depth': [1,5,10,15],\n'n_estimators': [50,100,150,300],\n'learning_rate': [0.01, 0.1, 0.3],\n}\n\ngrid = GridSearchCV(xgb.XGBClassifier(objective='binary:logistic'), param_grid = parameter, cv= 5, scoring='balanced_accuracy')\ngrid.fit(Xtrain,ytrain)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","da8ddcd3":"featureimp_plot = pd.Series(grid.best_estimator_.feature_importances_, index = Xtrain.columns)\nfeatureimp_plot.nlargest(5).plot(kind='barh') ","8dc39b5f":"# result\nresult(Xtest,ytest)","0776f993":"parameter = {'C': [5,10, 100]}\ngrid = GridSearchCV(svm.SVC(kernel='linear'), param_grid = parameter, cv= 4)\n\ngrid.fit(Xtrain,ytrain)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","178e5356":"# result\nresult(Xtest,ytest)","585517c6":"from imblearn.over_sampling import SMOTE","7c06a8a7":"#Split train and test data\nsmote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(X,y,\n                                                                         test_size = .25 ,\n                                                                         random_state = 111)\n\n#oversampling minority class using smote\nos = SMOTE(random_state = 0)\nos_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\nos_smote_X = pd.DataFrame(data = os_smote_X,columns= X.columns)\nos_smote_Y = pd.DataFrame(data = os_smote_Y,columns= ['Churn'])\n\nprint(os_smote_X.shape)\nprint(os_smote_Y.shape)","7dc3bf83":"rfe = RFE(rfc, 10)\nrfe = rfe.fit(X, y)\nprint(rfe.support_)\nprint(rfe.ranking_)\n\n#identified columns Recursive Feature Elimination\nrfe_data = pd.DataFrame({\"rfe_support\" :rfe.support_,\n                       \"columns\" : [i for i in X.columns if i not in Id_col + target_col],\n                       \"ranking\" : rfe.ranking_,\n                      })\nselected_cols = rfe_data[rfe_data[\"rfe_support\"] == True][\"columns\"].tolist()\n\nrfe_data\nprint(selected_cols)","1771056b":"# calculate the proportion of churn is now equal to the no churn\nos_smote_Y.mean()","07b98ca2":"#train and test data under SMOTE\ntrain_smoterfe_X = os_smote_X[selected_cols]\ntrain_smoterfe_Y = os_smote_Y.values.ravel()\ntest_smoterfe_X  = smote_test_X[selected_cols]\ntest_smoterfe_Y  = smote_test_Y","604c2fd1":"parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(estimator=logreg, param_grid = parameters,cv = 10,scoring = 'accuracy')\ngrid.fit(train_smoterfe_X,train_smoterfe_Y)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)\n","d804e157":"# result\nresult(test_smoterfe_X,test_smoterfe_Y)","b06e6b3c":"parameters = {'min_samples_split': [10,100,1000,10000],\n              'max_depth':[2,5,10,100,150,200,250]}\n\ngrid = GridSearchCV(estimator= DT, param_grid = parameters,cv = 10,scoring = 'accuracy')\ngrid.fit(train_smoterfe_X,train_smoterfe_Y)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_) \n\ngrid.best_estimator_.feature_importances_\nfor name, importance in sorted(zip(grid.best_estimator_.feature_importances_,train_smoterfe_X.columns),reverse= True)[:5]:\n    print(name, importance)\n    \nfeatureimp_plot = pd.Series(grid.best_estimator_.feature_importances_, index = train_smoterfe_X.columns)\nfeatureimp_plot.nlargest(5).plot(kind='barh')   ","7dcf9339":"# result\nresult(test_smoterfe_X,test_smoterfe_Y)","e624be0f":"parameters = {'n_estimators': [1,5,10,100,200],'min_samples_split': [10,100,1000,10000],'max_depth':[2,5,10,100,150,200,250]}\n\ngrid = GridSearchCV(estimator= rfc, param_grid = parameters,cv = 10,scoring = 'accuracy')\ngrid.fit(train_smoterfe_X,train_smoterfe_Y)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","52d455b9":"grid.best_estimator_.feature_importances_\nfor name, importance in sorted(zip(grid.best_estimator_.feature_importances_,train_smoterfe_X.columns),reverse= True)[:5]:\n    print(name, importance)\n    \nfeatureimp_plot = pd.Series(grid.best_estimator_.feature_importances_, index = train_smoterfe_X.columns)\nfeatureimp_plot.nlargest(5).plot(kind='barh')   ","67182a11":"# result\nresult(test_smoterfe_X,test_smoterfe_Y)","bc4d42fb":"#building the model & printing the score\nparameter = {\n'max_depth': [1,5,10,15],\n'n_estimators': [50,100,150,300],\n'learning_rate': [0.01, 0.1, 0.3],\n}\n\ngrid = GridSearchCV(xgb.XGBClassifier(objective='binary:logistic'), param_grid = parameter, cv= 5, scoring='balanced_accuracy')\ngrid.fit(train_smoterfe_X,train_smoterfe_Y)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","81458b69":"grid.best_estimator_.feature_importances_\nfor name, importance in sorted(zip(grid.best_estimator_.feature_importances_,train_smoterfe_X.columns),reverse= True)[:5]:\n    print(name, importance)\n    \nfeatureimp_plot = pd.Series(grid.best_estimator_.feature_importances_, index = train_smoterfe_X.columns)\nfeatureimp_plot.nlargest(5).plot(kind='barh')  ","90e186bc":"# result\nresult(test_smoterfe_X,test_smoterfe_Y)","34d944c3":"parameter = {'C': [1,5,10, 100]}\ngrid = GridSearchCV(svm.SVC(kernel='linear'), param_grid = parameter, cv= 4)\ngrid.fit(train_smoterfe_X,train_smoterfe_Y)\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters:\", grid.best_params_)","762728d2":"# result\nresult(test_smoterfe_X,test_smoterfe_Y)","4f60177c":"The variable 'Total Charges' is converted into a numeric format. Then check for any missing observations. Some observations for this variable is found to be missing. I'll be deleting the missing observations from the dataset.","2738c208":"#### XGBoost Classification Model","2d7651f8":"We can summarize the result as follows:\n\n**1.**: Variables such as contract type, status of online security, method of internet service provided, availability status of technical support and payment method are among the top 5 variables in the predictive analysis.\n\n**2.**: Precision is greater than recall. The f1-score ranges between 0.55 and 0.62 for the 'churn' class.It is the highest for decision tree model.\n\n**3.**: The AUC score ranges between 0.70 and 0.74. In terms of the AUC score, an optimal Decision Tree is the best model followed by SVM and XGBoost.\n\nAs I'm working with an unbalanced dataset, machine learning models can be bias in favor of the majority classi.e. no churn as opposed to the minority class i.e. churn. Given the objective of this study, understanding and predicting 'churn' becomes very important. But unbalanced dataset effect the prediction analysis for 'churn' with a lower recall value. In order to overcome this problem, I'm going to use SMOTE which is a type of an over-sampling method.\n\nSMOTE balances the class distribution between 'churn' and 'no churn'. Under this method new observations for 'churn' are created between the existing 'no churn' observations. It generates the training sample by the linear transformation of the existing observations of the 'churn' class.","aeba512e":"#### SVM with SMOTE","bd738c26":"## Customer Churn\n\nCustomer Churn is one of the important metrics to evaluate the growth potential of a business enterprise. It is measured as the percentage of customers that stopped using a company's product or services at the end of a time period by the number of customers it had at the beginning of the time period.\nCustomer churn analysis is used widely by telecommunication services, internet service providers, online streaming services, insuarance firms, etc. as the cost of retaining an existing customer is cheaper than acquiring a new one. Now, churn can be volunatry or  involuntary. Involutary churn can be the decision of a customer to switch to an alternative company or service providers. Involuntary churn includes relocation to other location, death, etc. In majority of the application, analysis is based on involuntary churn which can be primarily due to the customer's current product or service experience of a company or due to a better alternative options provided by a company's business competitors.\nIn this study, we use the data from a telecommunication company to perform churn analysis. Such an analysis will help in understanding the customer retention policies that are required to reduce the churn rate. The structure of this study are as follows:\n- **I**: Exploratory data analysis\n\n- **II**: Data pre-processing\n\n- **III**: Predictive modeling\n\n- **IV**: Conclusion","d1433527":"There is no presence of negative or missing observations in these three variables. Next, I check the dimension of the categorical variables by looking at their unique values. The unique values vary between 2 and 4 for the categorical variables.","f485fd2e":"> ## II. Data pre-processing:\n\nIn this section, I'm going to process the data which will be used for predictive modeling. I'll scale the 'tenure' and 'Monthly Charges', and pre-process the categorical variables.","c065068b":"The churn rate is around 27% in this dataset. Now, I'll be looking at few variables which can give us more information about the factors that can effect customer churn.","37f6bc75":"#### Logistic Regression (Base)","425ddb53":"Recursive Feature Elimination (RFE) is a method of selecting a subset of independent variables (features) that are relevant for predicting the target variable i.e. churn in this study. It uses the model accuracy to identify the attributes that contribute the most towards explaining the target variable. One can use logistic regression and tree-based models for feature extraction. I'm using random forest for feature extraction here. More information on RFE is provided in https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html","45352ec4":"#### SMOTE (Synthetic Minority Oversampling Technique)\n\nAs I'm working with an unbalanced dataset, machine learning models can be biased in favor of the majority class i.e. no churn as opposed to the minority class i.e. churn. Given the objective of this study, predicting 'churn' becomes very important. But unbalanced dataset effect the prediction analysis for 'churn' with a lower recall value. In order to overcome this problem, I'm going to use SMOTE which is a type of an over-sampling method.\n\nSMOTE balances the class distribution between 'churn' and 'no churn'. Under this method new observations for 'churn' are created between the existing 'no churn' observations. It generates the training sample by the linear transformation of the existing observations of the 'churn' class.","13f08fa6":"#### Decision Tree with Smote    ","82f3167a":"There is no such significant difference in churn rate by gender.","ee3852f0":"#### Random Forest with SMOTE","fb31e707":"I ran a logistic regression model to understand the effect of the independent variables on the target variable. Given the p-values, most of the variables are insignificant at 5% level of significance. Let's interpret the above result for some selected variables. For gender, male is denoted as 1 and female is denoted 0. The negative coefficient estimate before the 'gender' variable suggest that the log-odd of churn for male relative to female is lower. This is consistent with the data analysis done with gender earlier.\n\nIf the tenure length increases then the log-odds of churning i.e. the possibility of churn decreases. This statistical result confirms the result obtained earlier from the exploratory data analysis. In case of the tech support availability, if the tech support is not available then the possibility of churn increases. If there is a month-to-month contract then the chances of churn increases. An increase in monthly charges increases the churn rate. These statistical results are similar to the conclusions drawn from the exploratory data analysis.","a3b72994":"The customer churn data mainly consists of customer's social characteristics, type of service packages used by them and the method of payment. There are 7043 observations with 21 variables.","492d064d":"#### XGBoost with SMOTE","4194eaf6":"The above figure analyzed the churn rate when it is grouped by gender and contract type. Irrespective of the gender, we find that the churn rate is the highest for the month-to-month contract type. The conclusion of this analysis is similar to the conclusion derived from their individual analysis presented above.","ddf0a9c2":"#### Decision Tree Classification Model","2b2c26f4":"#### SVC Classification Model","ff0396d2":"> ## I. Exploratory data analysis:","44c6d8f9":"In the preceding section, I have split the dataset into train and test set. I'll be using 10-fold cross validation which is an useful method for model training when you have an unbalanced dataset. The data here is unbalanced because the ratio of churn to non-churn is 1:3. I'll also use grid search to derive the optimal model. To know more about GridSearchCV and its importance check this link: https:\/\/www.datacamp.com\/community\/tutorials\/parameter-optimization-machine-learning-models \n\nFirst objective of prediction modeling is to create a base model. In this case, a simple logistic regression is my base model. Then explore alternative machine learning models to find better models for improving the prediction capability. For comparison across different models, I'll look at precision, recall, f1-score and auc score.","ea81ffd3":"Churn rate is highest for the short-term contract compared to the long-term contract. For a month-to-month contract, the churn rate is 42% which is higher compared to the 'one-year' or the 'two-year' contract. The churn rates for the 'one-year' or the 'two-year' contract are 11% and 2% respectively.","9b689486":"> ## IV. Conclusion:\n\n**1.**: SMOTE has reduced the gap between precision and recall for the 'churn' class. Precision still remains higher than recall for the 'churn class' except for the Logistic Regression and linear SVM models. For these two models, recall becomes higher than precision. The recall value improved in majority of the models due to a lower False Negatives. The f1-score is between 0.60 and 0.65 for the 'churn' class. \n\n**2.**: The AUC score is now between 0.72 and 0.77. The AUC score decresed for the Decision Tree. Logistic Regression and linear SVM model has the highest AUC score of 0.77. In terms of AUC score, both these models are best suited for this study. I will prefer Logistic Regression because it is simple, intuitive and interpretable.","053b1cbf":"#### Logistic Regression","257e7efb":"#### Random Forest Classification Model","ab930fa4":"From the above analysis, I found that a significant share of the customers are either new with a tenure of less than 10 months or old customers with more than 70 months of tenure. From the cumulative frequency distribution plot, we find that around 20% of the customers have a tenure of less than 10 months. Around 70% of the customers have a tenure between 10 months and 60 months. And the remaining 20% have a tenure more than 60 months.\n\nFrom the graph presented below, the churn rate decreases as the tenure length increases. This indicates customer loyalty in terms of tenure length and its effect on the churn rate. I can conclude that the telecom service producer should focus on the new customers so that they avail its telecom services for a longer period of time.","930e520b":"There were 11 missing observations under the variable 'Total Charges' which will be removed from the dataset.","7ecf948c":"#### Logistic Regression with SMOTE","d74f2799":"Few interesting conclusions can be derived from the above exploratory data analysis. The above analysis is based on the citizen type, contract type and technical support. The senior citizen is denoted as 1 or 0 otherwise. Non-senior citizens are the majority in this dataset by around 5:1 ratio. For the senior citizen, there is no churn rate for the no internet service for the one-year and two-year contracts.\n\nThe availability of tech support helps in reducing the churn rate compared to those with no tech support. Though churn rate is even lower for those with no internet service compared to those with technical support. It is not clear if they have any other means of customer service available to them without an internet service or not. Nevertheless, improvement in the technical support can be useful in reducing the churn rate.","1373b759":"> ## III. Predictive Modeling    \n\nThe objective of this study is to predict the churn vs no-Churn situation from the telecom dataset.Inorder to predict the outcome, we need to predict whether a customer will churn or not i.e.(Churn\/NoChurn as (1\/0)).\n\nNow for each of the observations, four different events can occur when we try to predict:\n- case 1: predicted as 1 which are actually 1. Also known as True Positives (TP).\n- case 2: predicted as 1 which are actually 0. Also known as False Positives (FP).Also known as Type I error.\n- case 3: predicted as 0 which are actually 0. Also known as True Negatives (TN).\n- case 4: predicted as 0 which are actually 1. Also known as False Negatives (FN).Also known as Type II error.\n\nIn this study, predicting churn accurately is very important as it has a far-reaching impact on the future prospect of the business. This means a prediction analysis will be able to identify case 1 with greater accuracy. This means the objective of the study is to minimize case 4 or Type II error. There are certain criterias based on which prediction models will be judged. Let's look at those criteria.\n\nAccuracy,precision, recall, F1 and specificity are the different types of precision metrics used in evaluating the performance of the prediction models. These are also used to compare the performances between different alternative models. Accuracy measures the ratio of TP and TN given all the observations. Precision measures the ratio of the relevant class (i.e.Churn(1)) correctly predicted given the total number of predictions made for the relevant class. Recall also known as sensitivity measures the ratio of the relevant class given the actual number of observations of the relevant class. F1 score is the weighted average of precision and recall. It is particularly useful when we want to strike a balance between precision and recall. Sensitivity is the ratio of TN given the actual number of observations that belongs to the negative class. Another important metrics are Receiver Operating Curve (ROC) and Area under the Curve (AUC). A good discussion on these metrics are available at: https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5.\n\nAs mentioned earlier, one of the important objectives of the prediction model here is to reduce False Negatives (FN). This means that we have to consider improving the recall. But reducing the FN sometimes leads to an increase in FP which means precision may decrease. In this case, I'll look at the F1 scores along with the AUC values to decide on the best model for this study.","fe19125d":"The churn rate is the highest when the method of payment is through an electronic check. The other segments of the payment methods have a churn rate between 15% - 20%. The graph for this analysis is presented below.","3ebafd47":"From the above, correlation plot I find some interesting conclusions. Total Charges are highly correlated with tenure and monthly charges. Whereas, correlation between tenure and monthly charges is low.\n\nThere is a negative relationship between churn and tenure. There is a positive relationship between churn and tenure. Both these relationships make sense. But, I see a negative relationship between total charges and churn. This means that as the total charges increases, the churn rate goes down. This conclusion is absurd! Why a customer will be willing to pay a higher cost for the services? I'll be droping 'Total Charges' from the final dataset due to high collinearity with tenure and monthly charges, as well as a negative relationship with churn.","71a86f07":"The pattern for churn rate for monthly charges varies quite a lot between 0 and 1. Whereas, the churn rate is mostly either 0 or 1 for the total charges. It is hard to graphically derive additional insights about churn vs no-churn from these two variables."}}