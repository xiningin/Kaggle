{"cell_type":{"d55d9dbf":"code","2b1f9c61":"code","a6ff1b1a":"code","a39cbecd":"code","41552fa2":"code","8257402c":"code","7385dee3":"code","a157f550":"code","a2809ab7":"code","e17abe6d":"code","c3cc5b19":"code","80ca73b2":"code","bae3732b":"code","d634ac92":"code","7ecce710":"code","0b8d4df6":"code","0f4e2bba":"code","c78434dc":"code","e203e93e":"code","6656ab3b":"code","275c36e9":"code","66925a62":"code","f1354e31":"code","995aaa06":"code","e8db96b3":"code","46256bb1":"markdown","dfe51ec9":"markdown","ab8a59e9":"markdown","5b012eab":"markdown","be0afa65":"markdown","e246be90":"markdown","057af047":"markdown","5121bae4":"markdown","9cffba8a":"markdown","6be7002d":"markdown"},"source":{"d55d9dbf":"!git clone https:\/\/github.com\/tensorflow\/agents.git\n!mv agents\/tf_agents .\n!rm -rf agents","2b1f9c61":"!git clone https:\/\/github.com\/google\/gin-config\n!mv gin-config\/gin .\n!rm -rf gin-config","a6ff1b1a":"import numpy as np\n\nimport tensorflow as tf\nprint('tf.version:', tf.version.VERSION)\n\nimport tf_agents\n\nfrom kaggle_environments import make","a39cbecd":"import random\nimport os\ndef seed_everything(seed=42):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\nseed_everything()","41552fa2":"# for model\nfc_layer_params = (512, 256)\n# for training\nlearning_rate = 1e-5\nreplay_buffer_max_length = 10_000\nbatch_size = 64\nnum_eval_episodes = 10\n\nnum_iterations = 400_000\ncollect_steps_per_iteration = 1\neval_interval = 1_000","8257402c":"def get_board(obs, config, last_position):\n    \"\"\"Convert `obs` dict to 1D array.\n    Dim : 0=mine, 1=inhibit, 2=food\n    Returns:\n        numpy array size (7, 11, 3)\n    \"\"\"\n    rows, columns = config['rows'], config['columns']\n    n_cells = rows * columns\n    center = 0\n    current_position = None\n    X = np.zeros((n_cells, 3))\n    if last_position:\n        X[last_position, 1] = 1\n    for n in range(4):\n        geese = obs['geese'][n]\n        if n==obs.index: # mine\n            X[geese, 0] = 1\n            if len(geese) > 0:\n                current_position = geese[0]\n                center = n_cells\/\/2 + geese[0] + 1\n        else:\n            X[geese, 1] = 0.5\n            if len(geese) > 0:\n                X[geese[0], 1] = 1\n    X[obs['food'], 2] = 1\n    # centering board to my head\n    X = np.tile(X, (3, 1))[center:center+n_cells, :]\n    # reshape\n    X = X.reshape((rows, columns, 3))\n    X = np.array(X, dtype=np.float16)\n    return X, current_position","7385dee3":"from tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\n\nclass GeeseEnv(py_environment.PyEnvironment):\n    def __init__(self):\n        self.choices = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n        self._env = make(\"hungry_geese\", debug=False)\n        self.env = self._env.train([\"greedy\", \"greedy\", \"greedy\", None])\n        self.config = self._env.configuration\n        self.last_position = None\n        self._episode_ended = False\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n        \n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(self.config.rows, self.config.columns, 3), \n            dtype=np.float16, minimum=0, maximum=1,\n            name='observation')\n        \n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n    \n    def _reset(self):\n        obs = self.env.reset()\n        state, self.last_position = get_board(obs, self.config, self.last_position)\n        self._episode_ended = False\n        return ts.restart(state)\n    \n    def _step(self, action):\n        if self._episode_ended:\n            return self.reset()\n        obs, reward, done, info = self.env.step(self.choices[action])\n        state, self.last_position = get_board(obs, self.config, self.last_position)\n        if self._env.done:\n            self._episode_ended = True\n        if self._episode_ended:\n            return ts.termination(state, reward)\n        else:\n            return ts.transition(state, reward, discount=0.9)","a157f550":"# Check environment\nfrom tf_agents.environments import utils\n\nutils.validate_py_environment(GeeseEnv(), episodes=5)","a2809ab7":"# Convert python env to tensorflow env\nfrom tf_agents.environments import tf_py_environment\n\ntrain_env = tf_py_environment.TFPyEnvironment(GeeseEnv())\neval_env = tf_py_environment.TFPyEnvironment(GeeseEnv())","e17abe6d":"from tf_agents.networks import q_network\n\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    fc_layer_params=fc_layer_params,\n    conv_layer_params=[(32,3,1),(64,3,1)],\n)","c3cc5b19":"from tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.utils import common\n\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()","80ca73b2":"def compute_avg_return(environment, policy, num_episodes=10):\n\n    total_return = 0.0\n    for _ in range(num_episodes):\n\n        time_step = environment.reset()\n        episode_return = 0.0\n\n    while not time_step.is_last():\n        action_step = policy.action(time_step)\n        time_step = environment.step(action_step.action)\n        episode_return += time_step.reward\n    total_return += episode_return\n\n    avg_return = total_return \/ num_episodes\n    return avg_return.numpy()[0]","bae3732b":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_max_length)","d634ac92":"from tf_agents.drivers import dynamic_step_driver\n\ncollect_driver = dynamic_step_driver.DynamicStepDriver(\n    train_env,\n    agent.collect_policy,\n    observers=[replay_buffer.add_batch],\n    num_steps=collect_steps_per_iteration)\n\n# Initial data collection\n_ = collect_driver.run()","7ecce710":"dataset = replay_buffer.as_dataset(\n    num_parallel_calls=3, \n    sample_batch_size=batch_size, \n    num_steps=2).prefetch(3)\n\niterator = iter(dataset)","0b8d4df6":"# Save model to .\/model directory\nimport shutil\nfrom tf_agents.policies import policy_saver\n\nbest_return = 0\ndef save_model_if_best(agent, avg_return):\n    global best_return\n    if avg_return > best_return:\n        policy_dir = 'model'\n        shutil.rmtree(policy_dir, ignore_errors=True)\n        tf_policy_saver = policy_saver.PolicySaver(agent.policy, batch_size=None)\n        tf_policy_saver.save(policy_dir)\n        print(f'saved model, best return={avg_return:,.0f}')\n        best_return = avg_return","0f4e2bba":"%%time \n\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n\n# Reset the train step\nagent.train_step_counter.assign(0)\n\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\n\ndef train_one_iteration():\n\n    # Collect a few steps using collect_policy and save to the replay buffer.\n    collect_driver.run()\n\n    # Sample a batch of data from the buffer and update the agent's network.\n    experience, unused_info = next(iterator)\n    train_loss = agent.train(experience)\n\n    step = agent.train_step_counter.numpy()\n\n    if step % eval_interval == 0:\n        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n        print('step = {0}: Average Return = {1:,.0f}'.format(step, avg_return))\n        save_model_if_best(agent, avg_return)\n        returns.append(avg_return)\n\nfor _ in range(num_iterations):\n    train_one_iteration()","c78434dc":"import matplotlib.pyplot as plt\n\nsteps = range(0, num_iterations + 1, eval_interval)\nplt.ylabel('Average Return')\nplt.xlabel('Step')\nplt.plot(steps, returns)","e203e93e":"compute_avg_return(eval_env, agent.policy, num_eval_episodes)","6656ab3b":"%%writefile main.py\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\n\np = Path('\/kaggle_simulations\/agent\/')\nif p.exists():\n    sys.path.append(str(p))\nelse:\n    p = Path('__file__').resolve().parent\n    \n# tf_agents\nfrom tf_agents.networks import q_network\nfrom tf_agents.trajectories import time_step as ts\n","275c36e9":"# Save get_board function\nimport inspect\n\npath = 'main.py'\nwith open(path, 'a') as f:\n    s = inspect.getsource(get_board)\n    f.write(s)","66925a62":"%%writefile -a main.py\n\nlast_position = None\nsaved_policy = tf.compat.v2.saved_model.load(str(p\/'model'))\npolicy_state = saved_policy.get_initial_state(batch_size=1)\n\ndef main(obs, config):\n    global last_position\n    state, last_position = get_board(obs, config, last_position)\n    time_step = ts.TimeStep([0], [0], [0], [state])\n    action = saved_policy.action(time_step, policy_state)\n    action = ['NORTH', 'SOUTH', 'WEST', 'EAST'][int(action.action)]\n    return action","f1354e31":"env = make(\"hungry_geese\", debug=True)\nenv.reset()\nsteps = env.run(['greedy', 'greedy', 'greedy', 'main.py'])\n[res.reward for res in steps[-1]]","995aaa06":"# make submission.tar.gz\n!tar -czf submission.tar.gz model main.py gin tf_agents","e8db96b3":"# clean up\n!rm -rf gin tf_agents model main.py\n!ls","46256bb1":"# git clone libraries","dfe51ec9":"# Training","ab8a59e9":"To submit to kaggle using TF-Agent ..\n\n1. Create a python file that describes the agent\n2. git cloned TF-Agent directory\n3. git cloned gin directory\n4. Learned weights file\n\nI found that I should make submiton.tar.gz by putting the four of them together with tar.\n\nBelow is a brief example.\n\n---\n\n- V1: init release\n- V2: changed params\n- V3: added conv_layer_param(CNN) ","5b012eab":"# Parameters","be0afa65":"# Custom environment","e246be90":"# Agent","057af047":"# Import","5121bae4":"# Data Collection\nhttps:\/\/www.tensorflow.org\/agents\/tutorials\/10_checkpointer_policysaver_tutorial?hl=en#data_collection","9cffba8a":"# Make submission file","6be7002d":"# Mod observation function"}}