{"cell_type":{"c89c14f4":"code","0e678439":"code","7f9a1564":"code","a0753890":"code","68af41fa":"code","c82ffecd":"code","824fc711":"code","1d4c6d4b":"code","a300230d":"code","57d882dc":"code","a973367f":"code","91c5b0fe":"code","6b393c09":"code","51971b7b":"code","5a58b009":"code","7eeb99fb":"code","c0161cdf":"code","55ff6278":"code","73a7678b":"code","de88c5a5":"code","cb8e4151":"code","ec9e7978":"code","f3c82d85":"code","36675fc9":"code","e826af12":"code","9b52f342":"code","3b9921f2":"code","2e1eae41":"code","53dcdb60":"code","60884057":"code","7bf152be":"code","34c6bb24":"code","d6933e2b":"code","7050fff6":"markdown","e0ab130d":"markdown","fbf7d0d2":"markdown","c3904420":"markdown","780d3a65":"markdown","7ddcba35":"markdown","30285702":"markdown","a77d357d":"markdown","5c4622d4":"markdown","bf59a2c8":"markdown","244431b7":"markdown"},"source":{"c89c14f4":"import pandas as pd\nimport numpy as np\n\n# eps for making value a bit greater than 0 later on\neps = np.finfo(float).eps\n\nfrom numpy import log2 as log","0e678439":"dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}","7f9a1564":"df = pd.DataFrame(dataset,columns=['Taste','Temperature','Texture','Eat'])\ndf","a0753890":"def find_entropy(df):\n    '''\n    Function to calculate the entropy of the label i.e. Eat.\n    '''\n    Class = df.keys()[-1] \n    entropy = 0\n    values = df[Class].unique()\n    for value in values:\n        fraction = df[Class].value_counts()[value]\/len(df[Class])\n        entropy += -fraction*np.log2(fraction)\n    return entropy","68af41fa":"def find_entropy_attribute(df,attribute):\n    '''\n    Function to calculate the entropy of all features.\n    '''\n    Class = df.keys()[-1]   \n    target_variables = df[Class].unique()  \n    variables = df[attribute].unique()\n    entropy2 = 0\n    for variable in variables:\n        entropy = 0\n        for target_variable in target_variables:\n                num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n                den = len(df[attribute][df[attribute]==variable])\n                fraction = num\/(den+eps)\n                entropy += -fraction*log(fraction+eps)\n        fraction2 = den\/len(df)\n        entropy2 += -fraction2*entropy\n    return abs(entropy2)","c82ffecd":"def find_winner(df):\n    '''\n    Function to find the feature with the highest information gain.\n    '''\n    Entropy_att = []\n    IG = []\n    for key in df.keys()[:-1]:\n#         Entropy_att.append(find_entropy_attribute(df,key))\n        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n    return df.keys()[:-1][np.argmax(IG)]","824fc711":"def get_subtable(df, node, value):\n    '''\n    Function to get a subtable of met conditions.\n    \n    node: Column name\n    value: Unique value of the column\n    '''\n    return df[df[node] == value].reset_index(drop=True)","1d4c6d4b":"def buildTree(df,tree=None): \n    '''\n    Function to build the ID3 Decision Tree.\n    '''\n    Class = df.keys()[-1]  \n    #Here we build our decision tree\n\n    #Get attribute with maximum information gain\n    node = find_winner(df)\n    \n    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n    attValue = np.unique(df[node])\n    \n    #Create an empty dictionary to create tree    \n    if tree is None:                    \n        tree={}\n        tree[node] = {}\n    \n   #We make loop to construct a tree by calling this function recursively. \n    #In this we check if the subset is pure and stops if it is pure. \n\n    for value in attValue:\n        \n        subtable = get_subtable(df,node,value)\n        clValue,counts = np.unique(subtable['Eat'],return_counts=True)                        \n        \n        if len(counts)==1:#Checking purity of subset\n            tree[node][value] = clValue[0]                                                    \n        else:        \n            tree[node][value] = buildTree(subtable) #Calling the function recursively \n                   \n    return tree","a300230d":"tree = buildTree(df)","57d882dc":"import pprint\npprint.pprint(tree)","a973367f":"def predict(inst,tree):\n    '''\n    Function to predict for any input variable.\n    '''\n    #Recursively we go through the tree that we built earlier\n\n    for nodes in tree.keys():        \n        \n        value = inst[nodes]\n        tree = tree[nodes][value]\n        prediction = 0\n            \n        if type(tree) is dict:\n            prediction = predict(inst, tree)\n        else:\n            prediction = tree\n            break;                            \n        \n    return prediction","91c5b0fe":"data = {'Taste':'Salty','Temperature':'Cold','Texture':'Hard'}","6b393c09":"inst = pd.Series(data)","51971b7b":"prediction = predict(inst,tree)\nprediction","5a58b009":"# The input values\nx1 = [0, 1, 1, 2, 2, 2,3,2,1]\nx2 = [0, 0, 1, 1, 1, 0,2,2,1]\n# The class\ny = np.array([0, 0, 0, 1, 1, 0,1,0,1])","7eeb99fb":"def partition(a):\n    return {c: (a==c).nonzero()[0] for c in np.unique(a)}","c0161cdf":"def entropy(s):\n    res = 0\n    val, counts = np.unique(s, return_counts=True)\n    freqs = counts.astype('float')\/len(s)\n    for p in freqs:\n        if p != 0.0:\n            res -= p * np.log2(p)\n    return res","55ff6278":"def mutual_information(y, x):\n\n    res = entropy(y)\n\n    # We partition x, according to attribute values x_i\n    val, counts = np.unique(x, return_counts=True)\n    freqs = counts.astype('float')\/len(x)\n\n    # We calculate a weighted average of the entropy\n    for p, v in zip(freqs, val):\n        res -= p * entropy(y[x == v])\n\n    return res","73a7678b":"def mutual_information(y, x):\n\n    res = entropy(y)\n\n    # We partition x, according to attribute values x_i\n    val, counts = np.unique(x, return_counts=True)\n    freqs = counts.astype('float')\/len(x)\n\n    # We calculate a weighted average of the entropy\n    for p, v in zip(freqs, val):\n        res -= p * entropy(y[x == v])\n\n    return res","de88c5a5":"from pprint import pprint\n\ndef is_pure(s):\n    return len(set(s)) == 1\n\ndef recursive_split(x, y):\n    # If there could be no split, just return the original set\n    if is_pure(y) or len(y) == 0:\n        return y\n\n    # We get attribute that gives the highest mutual information\n    gain = np.array([mutual_information(y, x_attr) for x_attr in x.T])\n    selected_attr = np.argmax(gain)\n\n    # If there's no gain at all, nothing has to be done, just return the original set\n    if np.all(gain < 1e-6):\n        return y\n\n\n    # We split using the selected attribute\n    sets = partition(x[:, selected_attr])\n\n    res = {}\n    for k, v in sets.items():\n        y_subset = y.take(v, axis=0)\n        x_subset = x.take(v, axis=0)\n\n        res[\"x_%d = %d\" % (selected_attr, k)] = recursive_split(x_subset, y_subset)\n\n    return res\n\nX = np.array([x1, x2]).T","cb8e4151":"c4_5 = recursive_split(X, y)","ec9e7978":"def pretty(d, indent=0):\n    for key, value in d.items():\n        print('\\t' * indent + str(key))\n\n        if isinstance(value, dict):\n            pretty(value, indent+1)\n        else:\n            print('\\t' * (indent+1) + str(value))","f3c82d85":"pretty(c4_5)","36675fc9":"import time","e826af12":"import numpy as np\nfrom scipy.linalg import norm\nfrom scipy.spatial.distance import euclidean\n \n_SQRT2 = np.sqrt(2)     # sqrt(2) with default precision np.float64\n\ndef hellinger_dist(p, q):\n    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) \/ _SQRT2","9b52f342":"def hellinger_dist(p, q):\n    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) \/ _SQRT2","3b9921f2":"# The input values\nx1 = [0, 1, 1, 2, 2, 2, 1]\nx2 = [0, 0, 1, 1, 1, 0, 1]\n\n# The class\ny = np.array([0, 0, 0, 1, 1, 0, 0])","2e1eae41":"# repeat = 1\n\n# p = np.array([.05, .05, .1, .1, .2, .2, .3] * repeat)\n# q = np.array([  0,   0,  0, .1, .3, .3 ,.3] * repeat)\n\n# p \/= p.sum()\n# q \/= q.sum()\n\n# hellinger_dist(p=p, q=q)","53dcdb60":"# The input values\nx1 = [0, 1, 1, 2, 2, 2, 1]\nx2 = [0, 0, 1, 1, 1, 0, 1]\n\n# The class\ny = np.array([0, 0, 0, 1, 1, 0, 0])","60884057":"def partition(a):\n    return {c: (a==c).nonzero()[0] for c in np.unique(a)}","7bf152be":"from pprint import pprint\n\ndef is_pure(s):\n    return len(set(s)) == 1\n\ndef recursive_split(x, y):\n    \n    # If there could be no split, just return the original set\n    if is_pure(y) or len(y) == 0:\n        return y\n\n    # We get attribute that gives the smallest distance\n    distance = np.array([hellinger_dist(y\/y.sum(), x_attr\/x_attr.sum()) for x_attr in x.T])\n    selected_attr = np.argmin(distance)\n\n    # If the distance is very less, nothing has to be done, just return the original set\n    if np.all(distance < 1e-6):\n        return y\n\n    # We split using the selected attribute\n    sets = partition(x[:, selected_attr])\n\n    res = {}\n    for k, v in sets.items():\n        y_subset = y.take(v, axis=0)\n        x_subset = x.take(v, axis=0)\n\n        res[\"x_%d = %d\" % (selected_attr, k)] = recursive_split(x_subset, y_subset)\n\n    return res\n\nX = np.array([x1, x2]).T\nhellinger = recursive_split(X, y)","34c6bb24":"def pretty_print(d, indent=0):\n    for key, value in d.items():\n        print('\\t' * indent + str(key))\n        if isinstance(value, dict):\n            pretty(value, indent+1)\n        else:\n            print('\\t' * (indent+1) + str(value))","d6933e2b":"pretty_print(hellinger)","7050fff6":"Creating a dataset,","e0ab130d":"Three ways of computing the Hellinger distance between two discrete\nprobability distributions using NumPy and SciPy.","fbf7d0d2":"# Hellinger Distance","c3904420":"Now, for prediction we go through each node of the tree to find the output.","780d3a65":"In the ID3 algorithm, decision trees are calculated using the concept of entropy and information gain.","7ddcba35":"http:\/\/www.pythonexample.com\/code\/hellinger-distance-decision-tree\/\n\nhttps:\/\/gist.github.com\/larsmans\/3116927\n\nhttp:\/\/videolectures.net\/ecmlpkdd08_cieslak_ldtf\/?q=hellinger%20distance","30285702":"The tree splits are as follows,","a77d357d":"http:\/\/gabrielelanaro.github.io\/blog\/2016\/03\/03\/decision-trees.html","5c4622d4":"# ID3 Algorithm","bf59a2c8":"# C4.5","244431b7":"#### Entropy can be defined as:\n\\begin{align}\nH(S)=-\\sum_{i=1}^{N}p_{i}log_{2}p_{i}\n\\end{align}"}}