{"cell_type":{"49e5e7a0":"code","4befb7c5":"code","0a995043":"code","966909f0":"code","b7058f70":"code","143c313a":"code","09847737":"code","9481e3ee":"code","3b5a5346":"code","0ba80813":"code","98517cc8":"code","877ae6a1":"code","e4a622c8":"code","5f949f80":"code","8021c9c1":"code","bbccdf35":"code","549bfc1f":"code","fb78918d":"code","c16d56ce":"code","55364199":"code","c1cb1131":"code","710f1250":"code","627c6dc0":"code","aacd17af":"code","18ba2e9c":"code","db9652bb":"code","0df25919":"code","d3022227":"code","a32b7c7e":"code","29915b0c":"code","a9d0201b":"code","0299f8de":"code","5fe3ebb2":"code","7143e438":"code","b3066770":"code","063c18ff":"code","be03efbe":"code","97ee9c88":"code","eadd8324":"code","f37410cc":"code","41e976eb":"code","e18814d2":"code","f5d86a87":"code","bb3f6786":"code","75dc4266":"code","cc08a2eb":"code","cad2c5b6":"code","01c7bd59":"code","bb345525":"code","f224bcc9":"code","48e70f2d":"code","49905c45":"code","e393826b":"code","7d0ba93c":"code","8ddafff5":"code","aa1bd597":"code","431f581b":"code","e9354c1d":"code","6fc82129":"code","de2b95f4":"code","1594b8a7":"code","a9d14f69":"code","fbb3d765":"markdown","022ee548":"markdown","e95bcee0":"markdown","2e6a8ed6":"markdown","94127834":"markdown","191990e0":"markdown","4157b493":"markdown","4e6ed134":"markdown","ae742547":"markdown","b0fcddd3":"markdown","9a0f22a5":"markdown","d92fb86f":"markdown","7d9b1ab5":"markdown","57d7b8cb":"markdown","7d71c000":"markdown","dc836fbf":"markdown","3f1b20ca":"markdown","a7cead97":"markdown","1798aac2":"markdown","bb9bb6db":"markdown","b36b3657":"markdown","ae5929b0":"markdown","cb710a8c":"markdown","62084c55":"markdown","08d87f46":"markdown","715e0cdf":"markdown","a36c7ac4":"markdown","91be8d72":"markdown","b604f7ea":"markdown","a6c72807":"markdown","2e627c27":"markdown","6d1c6cae":"markdown","9cfc13aa":"markdown","f9fd2116":"markdown","7697b6af":"markdown","cc9175d7":"markdown","3df52ca9":"markdown","5063a6d2":"markdown","276d3f47":"markdown","24a5a11e":"markdown"},"source":{"49e5e7a0":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os                                                                                                            \nimport matplotlib as mpl                                                                                             \nif os.environ.get('DISPLAY','') == '':                                                                               \n    print('no display found. Using non-interactive Agg backend')                                                     \n    mpl.use('Agg')                                                                    \n        \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas_profiling as pp\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import pyplot\nimport zipfile\n\nimport tensorflow as tf","4befb7c5":"colab = os.environ.get('COLAB_GPU', '10')\nif (int(colab) == 0):\n    from google.colab import drive\n    drive.mount('\/content\/drive')  \nelse:\n    print(\"\")","0a995043":"# Check if Google Colab path exists\nif os.path.exists(\"\/content\/drive\/My Drive\/MyDSNotebooks\/Imbalanced_data\/input\/creditcardzip\") :\n    # Change the current working Directory    \n    os.chdir(\"\/content\/drive\/My Drive\/MyDSNotebooks\/Imbalanced_data\/input\/creditcardzip\")\n# else check if Kaggle\/local path exists\nelif os.path.exists(\"..\/input\/creditcardzip\") :\n    # Change the current working Directory    \n    os.chdir(\"..\/input\/creditcardzip\")\nelse:\n    print(\"Can't change the Current Working Directory\") \nprint(\"Current Working Directory \" , os.getcwd())","966909f0":"# Load the Data Set\n# df = pd.read_csv('https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/creditcard.csv') \ndf = pd.read_csv('creditcard.csv')","b7058f70":"# Check the data, make sure it loaded okay\nprint(df.head())","143c313a":"# Check the datatypes of the Data set \ndf.info()","09847737":"# Check the Uniqueness\ndf.nunique()","9481e3ee":"# Check for missing data\ndf.isnull().sum()","3b5a5346":"# Check basic Statistics\ndf.describe(include ='all')","0ba80813":"# Check the Class Imbalance of the Data \ndf['Class'].value_counts()","98517cc8":"# Histograms of the features\n# most of the data has a quasi-normal\/gaussian distribution\ndf.hist(bins=20, figsize=(20,15))\nplt.show()","877ae6a1":"# divide full data into features and label\nspl1 = 0.3\nspl2 = 0.3\nX = df.loc[:, df.columns != 'Class']\ny = df.loc[:, df.columns == 'Class']\n# create train, test and validate datasets\n\n# first split original into Train and Test+Val\nX_train, X_test1, y_train, y_test1 = train_test_split(X,y, test_size = spl1, random_state = None, shuffle=True)\n# then split Test+Val into Test and Validate\n# Validate will only be used in the 2 Model system (explained below)\nX_test, X_val, y_test, y_val = train_test_split(X_test1,y_test1, test_size = spl2, random_state = None, shuffle=True)\n\nclass_names=[0,1] # name  of classes 1=fraudulent transaction\n\ny_val['Class'].value_counts()","e4a622c8":"# find the number of minority (value=1) samples in our train set so we can down-sample our majority to it\nyes = len(y_train[y_train['Class'] ==1])\n\n# retrieve the indices of the minority and majority samples \nyes_ind = y_train[y_train['Class'] == 1].index\nno_ind = y_train[y_train['Class'] == 0].index\n\n# random sample the majority indices based on the amount of \n# minority samples\nnew_no_ind = np.random.choice(no_ind, yes, replace = False)\n\n# merge the two indices together\nundersample_ind = np.concatenate([new_no_ind, yes_ind])\n\n# get undersampled dataframe from the merged indices of the train dataset\nX_train = X_train.loc[undersample_ind]\ny_train = y_train.loc[undersample_ind]\n\ny_train = np.array(y_train).flatten()","5f949f80":"def visualize(Actual, Pred, Algo):\n    #Confusion Matrix\n    cnf_matrix=metrics.confusion_matrix(Actual, Pred) #\n\n    #Visualize confusion matrix using heat map\n\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n\n    # create heatmap\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix: '+Algo, y=1.1) \n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","8021c9c1":"def display_metrics(model_name, train_features, test_features, train_label, test_label, pred, algo):\n    model_probs = model_name.predict_proba(test_features)\n    n = model_name.predict_proba(test_features).shape[1]-1\n    model_probs = model_probs[:, n]\n    try:\n        print(model_name.score(test_features, test_label)) \n        print(\"Accuracy score (training): {0:.3f}\".format(model_name.score(train_features, train_label))) \n        print(\"Accuracy score (validation): {0:.3f}\".format(model_name.score(test_features, test_label))) \n    except Exception as e:\n        print(\"error\")  \n    try:\n        print(pd.Series(model_name.feature_importances_, index=train_features.columns[:]).nlargest(10).plot(kind='barh')) \n    except Exception as e:\n        print(\"error\") \n    print(\"Confusion Matrix:\")\n    tn, fp, fn, tp = confusion_matrix(test_label, pred).ravel()\n    total = tn+ fp+ fn+ tp \n    print(\"false positive pct:\",(fp\/total)*100) \n    print(\"tn\", \" fp\", \" fn\", \" tp\") \n    print(tn, fp, fn, tp) \n    print(confusion_matrix(test_label, pred)) \n    print(\"Classification Report\") \n    print(classification_report(test_label, pred))\n    print(\"Specificity =\", tn\/(tn+fp))\n    print(\"Sensitivity =\", tp\/(tp+fn))\n    y=np.reshape(test_label.to_numpy(), -1)\n    fpr, tpr, thresholds = metrics.roc_curve(y, model_probs, pos_label=1)\n    cm_results.append([algo, tn, fp, fn, tp])\n    cr_results.append([algo, classification_report(test_label, pred)])\n    roc.append([algo, fpr, tpr, thresholds])\n    # AUC score should be (Sensitivity+Specificity)\/2\n    print(algo + ':TEST | AUC Score: ' + str( round(metrics.auc(fpr, tpr),3 )))\n    return tn, fp, fn, tp","bbccdf35":"def auc_roc_metrics(model, test_features, test_labels, algo): # model object, features, actual labels, name of algorithm\n    # useful for imbalanced data\n    ns_probs = [0 for _ in range(len(test_labels))]\n    # predict probabilities\n    model_probs = model.predict_proba(test_features)\n    # keep probabilities for the positive outcome only\n    n = model.predict_proba(test_features).shape[1]-1\n    model_probs = model_probs[:, n]  \n    model_auc = auc_roc_metrics_plots(model_probs, ns_probs, test_labels, algo) \n    return model_auc","549bfc1f":"def auc_roc_metrics_plots(model_probs, ns_probs, test_labels, algo):\n    \n    # calculate scores\n    ns_auc = roc_auc_score(test_labels, ns_probs) # no skill\n    model_auc = round(roc_auc_score(test_labels, model_probs), 3)\n\n    # summarize scores\n    print('%10s : ROC AUC=%.3f' % ('No Skill',ns_auc))\n    print('%10s : ROC AUC=%.3f' % (algo,model_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(test_labels, ns_probs)\n    # NameError: name 'ns_probs' is not defined\n    model_fpr, model_tpr, _ = roc_curve(test_labels, model_probs)\n    # plot the roc curve for the model\n    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    pyplot.plot(model_fpr, model_tpr, marker='.', label='%s (area = %0.2f)' % (algo, model_auc))\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    # show the legend\n    pyplot.legend()\n    pyplot.title('Receiver Operating Characteristic curve')\n    # show the plot\n    pyplot.show()\n    return model_auc","fb78918d":"# Define our custom loss function\ndef focal_loss(y_true, y_pred):\n    gamma = 2.0\n    alpha = 0.25\n    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n","c16d56ce":"def prediction_cutoff(model, test_features, cutoff):\n    model.predict_proba(test_features)\n    # to get the probability in each class, \n    # for example, first column is probability of y=0 and second column is probability of y=1.\n\n    # the probability of being y=1\n    prob1=model.predict_proba(test_features)[:,1]\n    predicted=[1 if i > cutoff else 0 for i in prob1]\n    return predicted","55364199":"metrics_results = {}\nroc = []\ncm_results = []\ncr_results = []","c1cb1131":"lr = LogisticRegression()\n\nlr.fit(X_train, y_train)\n# use custom prediction cutoff function to show how it works\nlr_Pred = prediction_cutoff(lr, X_test, 0.5) # 0.5 is the default cutoff for a logistic regression test","710f1250":"print(metrics.accuracy_score(y_test, lr_Pred))\ntn, fp, fn, tp = display_metrics(lr, X_train, X_test, y_train, y_test, lr_Pred, 'LR')\nvisualize(y_test, lr_Pred, 'LR') # actual labels vs predicted labels\nlr_auc = auc_roc_metrics(lr, X_test, y_test, 'LR')\nmetrics_results['lr'] = lr_auc","627c6dc0":"# useful for unbalanced data, maybe include later in metrics summary for all models\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_Pred)\nlr_f1, lr_auc = f1_score(y_test, lr_Pred), auc(lr_recall, lr_precision)\n# summarize scores\nprint('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\npyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\npyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n# axis labels\npyplot.xlabel('Recall')\npyplot.ylabel('Precision')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","aacd17af":"rf = RandomForestClassifier(n_estimators = 1000)\nrf.fit(X_train, y_train, sample_weight=np.where(y_train == 1,1.0,1.0).flatten())\nrf_Pred=rf.predict(X_test)","18ba2e9c":"print(classification_report(y_test, rf_Pred))\ntn, fp, fn, tp = display_metrics(rf, X_train, X_test, y_train, y_test, rf_Pred, 'RF')\nvisualize(y_test, rf_Pred, 'RF')\nrf_auc = auc_roc_metrics(rf, X_test, y_test, 'RF')\nmetrics_results['rf'] = rf_auc","db9652bb":"#setup model parameters, change some of the defaults based on benchmarking\ngb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_features=5, \n                                    max_depth=3, random_state=None, subsample = 0.5, criterion='mse', \n                                    min_samples_split = 10, min_samples_leaf = 10)\n\n#since a false negative is much more likely than a false positive, we should weight them accordingly\n# no weights gives worse false positive counts\ngb_clf.fit( X_train, y_train, sample_weight=np.where(y_train == 1,1.0,1.0) ) #  fn = 12 and fp = 1057\n\n#use model to predict validation dataset\npredictions = gb_clf.predict(X_test)\n","0df25919":"tn, fp, fn, tp = display_metrics(gb_clf, X_train, X_test, y_train, y_test, predictions, 'GB')\nvisualize(y_test, predictions, 'GB')\ngb_auc = auc_roc_metrics(gb_clf, X_test, y_test, 'GB')\nmetrics_results['gb'] = gb_auc","d3022227":"#setup model parameters, change some of the defaults based on benchmarking\ngb_clf1 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_features=5, \n                                    max_depth=3, random_state=None, subsample = 1.0, criterion='mse', \n                                    min_samples_split = 10, min_samples_leaf = 10)\n\n#since a false negative is much more likely than a false positive, we should weight them accordingly. \n#IE Finding a true one is more important, also more rare\ngb_clf1.fit( X_train, y_train, sample_weight=np.where(y_train == 1,3.6,1.4) ) # was 5.0\n\n#use model to predict validation dataset\npredictions = gb_clf1.predict(X_test) ","a32b7c7e":"algo = 'GB1 Train **'\ntn1, fp1, fn1, tp1 = display_metrics(gb_clf1, X_train, X_test, y_train, y_test, predictions, algo)\nvisualize(y_test, predictions, algo)\ngb1_auc = auc_roc_metrics(gb_clf1, X_test, y_test, algo)\nmetrics_results['gb1_train'] = gb1_auc","29915b0c":"X_test['Prediction'] = predictions","a9d0201b":"yes_ind = X_test[X_test['Prediction'] == 1].index","0299f8de":"X2_test = X_test.loc[yes_ind]\ny2_test = y_test.loc[yes_ind]","5fe3ebb2":"X_test = X_test.drop(['Prediction'], axis=1)","7143e438":"X2_test = X2_test.drop(['Prediction'], axis=1)","b3066770":"proba = gb_clf1.predict_proba(X2_test) \npred = gb_clf1.predict(X2_test) \ndf = pd.DataFrame(data=proba[:,0], columns=[\"preda_1\"])\ndf.hist(bins=20, figsize=(10,5))\nplt.show()","063c18ff":"algo = 'PredictedPositives'\ntest_labels = y2_test\nns_probs = [0 for _ in range(len(test_labels))]\nauc_roc_metrics_plots(proba[:,1], ns_probs, test_labels, algo)","be03efbe":"#setup model parameters, change some of the defaults based on benchmarking\ngb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_features=10, \n                                    max_depth=3, random_state=None, subsample = 1.0, criterion='mse', \n                                    min_samples_split = 10, min_samples_leaf = 10)\n\n#since a false negative is much more likely than a false positive, we should weight them accordingly. \n#IE Finding a true one is more important\n# note that the weights in the 2nd model are the inverse of the weights in the 1st model\ngb_clf2.fit( X_train, y_train, sample_weight=np.where(y_train == 1,3.6,1.4) ) # was 0.1 but should be > 1 to work correctly\n\n#use model to predict validation dataset\npredictions = gb_clf2.predict(X2_test) \n\nalgo = 'GB2 Train **'\ntn, fp, fn, tp = display_metrics(gb_clf2, X_train, X2_test, y_train, y2_test, predictions, algo)\n\nvisualize(y2_test, predictions, algo)\n\ngb2_auc = auc_roc_metrics(gb_clf2, X2_test, y2_test, algo)\nmetrics_results['gb2_train'] = gb2_auc\n\nprint(\"2 Step Final Confusion Matrix:\")\nprint(tn+tn1, fp) \nprint(fn+fn1, tp) \n\nfig, ax = plt.subplots() \ntick_marks = np.arange(len(class_names)) \nplt.xticks(tick_marks, class_names) \nplt.yticks(tick_marks, class_names)\n\n#create heatmap with combined data from both models\nsns.heatmap(pd.DataFrame([[tn+tn1,fp],[fn+fn1,tp]]), annot=True, cmap=\"YlGnBu\" ,fmt='g') \nax.xaxis.set_label_position(\"top\") \nplt.tight_layout() \nplt.title('2 Step Final Confusion matrix (Test)', y=1.1) \nplt.ylabel('Actual label') \nplt.xlabel('Predicted label')","97ee9c88":"# run the validate dataset through the first model\nalgo = '2-Step'\npredictions1 = gb_clf1.predict(X_val)\npredictions_proba1 = gb_clf1.predict_proba(X_val)\nX1_val_final = X_val.copy()\nX1_val_final=X1_val_final.join(y_val)\nX1_val_final['Proba_1'] = predictions_proba1[:,1]","eadd8324":"# use both models to predict final validation dataset\nalgo = 'GB1 Validate **'\ntn1, fp1, fn1, tp1 = display_metrics(gb_clf1, X_test, X_val, y_test, y_val, predictions1, algo) \nvisualize(y_val, predictions1, algo)\ngb1_auc = auc_roc_metrics(gb_clf1, X_val, y_val, algo)\nmetrics_results['gb1_validate'] = gb1_auc\n","f37410cc":"\nX_val['Prediction'] = predictions1\n\nyes_ind = X_val[X_val['Prediction'] == 1].index\n\nX2_val = X_val.loc[yes_ind]\ny2_val = y_val.loc[yes_ind]\nX2_val = X2_val.drop(['Prediction'], axis=1)\n# run the validate dataset through the second model\npredictions2 = gb_clf2.predict(X2_val)\n\nX2_val_final = X2_val.copy()\nX2_val_final.join(y2_val)\npredictions_proba2 = gb_clf2.predict_proba(X2_val)\n# validate the join!!\nX2_val_final['Proba_2'] = predictions_proba2[:,1]\nX2_val_final\n\ncols_to_use = X2_val_final.columns.difference(X1_val_final.columns)\nX_val_final = X1_val_final.join(X2_val_final[cols_to_use], how='left', lsuffix='_1', rsuffix='_2')\n# rowwise action (axis=1)\nX_val_final.loc[X_val_final['Proba_2'].isnull(),'Proba_2'] = X_val_final['Proba_1']\n#X_val_final['Proba_2'].fillna(df['Proba_1'])\n#X_val_final.query(\"Proba_1 != Proba_2\")","41e976eb":"algo = 'GB2 Validate **'\ntn, fp, fn, tp = display_metrics(gb_clf2, X_train, X2_val, y_train, y2_val, predictions2, algo) \nvisualize(y2_val, predictions2, algo)\ngb2_auc = auc_roc_metrics(gb_clf2, X2_val, y2_val, algo)\nmetrics_results['gb2_validate'] = gb2_auc\n\nprint(\"2 Step Final Confusion Matrix:\")\nprint(tn+tn1, fp) \nprint(fn+fn1, tp) \n\nfig, ax = plt.subplots() \ntick_marks = np.arange(len(class_names)) \nplt.xticks(tick_marks, class_names) \nplt.yticks(tick_marks, class_names)\n\n#create heatmap with combined data from both models\nsns.heatmap(pd.DataFrame([[tn+tn1,fp],[fn+fn1,tp]]), annot=True, cmap=\"YlGnBu\" ,fmt='g') \nax.xaxis.set_label_position(\"top\") \nplt.tight_layout() \nplt.title('2 Step Final Confusion matrix (Validate)', y=1.1) \nplt.ylabel('Actual label') \nplt.xlabel('Predicted label')\n\nalgo = '2-Step'\nSpecificity = (tn+tn1)\/(tn+tn1+fp)\nSensitivity = tp\/(tp+fn+fn1)\n\nprint(\"Specificity =\", Specificity)\nprint(\"Sensitivity =\", Sensitivity)\n\nprint('2 Step Algorithm' + ':TEST | AUC Score: ' + str( round( (Specificity+Sensitivity)\/2,3 )))\n\ncm_results.append([algo, (tn+tn1), fp, (fn+fn1), tp])","e18814d2":"# try to combine the 2 models into one AUC score, however not sure that the proba values from 2 different models can be combined \n\ntest_labels = X_val_final['Class']\nns_probs = [0 for _ in range(len(test_labels))]\nmodel_probs = X_val_final['Proba_2']\nmodel_pred=[1 if i > 0.50 else 0 for i in model_probs]\n\ntwo_step_auc = auc_roc_metrics_plots(model_probs, ns_probs, test_labels, algo)\n\nmetrics_results['2-step'] = two_step_auc\n\ncr_results.append([algo, classification_report(test_labels, model_pred)])","f5d86a87":"y=np.reshape(test_labels.to_numpy(), -1)\nfpr, tpr, thresholds = metrics.roc_curve(y, model_probs, pos_label=1)\nroc.append([algo, fpr, tpr, thresholds])","bb3f6786":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\n#from keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.layers import Conv1D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPool1D\nfrom keras.layers import Flatten\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation","75dc4266":"# create new activation function\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))","cc08a2eb":"# add this function to the list of Activation functions\nget_custom_objects().update({'swish': Activation(swish)})","cad2c5b6":"# prepare data for model\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","01c7bd59":"def create_dnn():\n    # input_dim must equal number of features in X_train and X_test dataset\n    clf1 = Sequential([\n        Dense(units=16, kernel_initializer='uniform', input_dim=30, activation='relu'),\n        Dense(units=18, kernel_initializer='uniform', activation='relu'),\n        Dropout(0.25),\n        Dense(20, kernel_initializer='uniform', activation='relu'),\n        Dense(24, kernel_initializer='uniform', activation='relu'),\n        Dense(1, kernel_initializer='uniform', activation='sigmoid')\n    ])\n    return clf1","bb345525":"def create_simple_dnn():\n    # input_dim must equal number of features in X_train and X_test dataset\n    clf1 = Sequential([\n        Dense(units=16, kernel_initializer='uniform', input_dim=30, activation='relu'),\n        Dense(units=18, kernel_initializer='uniform', activation='relu'),\n        Dense(1, kernel_initializer='uniform', activation='sigmoid')\n    ])\n    return clf1","f224bcc9":"def create_complex_dnn():\n    # input_dim must equal number of features in X_train and X_test dataset\n    clf1 = Sequential([\n        Dense(units=16, kernel_initializer='uniform', input_dim=30, activation='relu'),\n        Dense(units=18, kernel_initializer='uniform', activation='relu'),\n        Dropout(0.10),\n        Dense(units=30, kernel_initializer='uniform', activation='relu'),\n        Dense(units=28, kernel_initializer='uniform', activation='relu'),\n        Dropout(0.10),\n        Dense(units=30, kernel_initializer='uniform', activation='relu'),\n        Dense(units=28, kernel_initializer='uniform', activation='relu'),\n        Dropout(0.10),\n        Dense(units=20, kernel_initializer='uniform', activation='relu'),\n        Dense(units=24, kernel_initializer='uniform', activation='relu'),\n        Dense(units=1, kernel_initializer='uniform', activation='sigmoid')\n    ])\n    return clf1","48e70f2d":"def create_cnn():\n    model = Sequential()\n    model.add(Conv1D(filters=32, kernel_size=10, strides=1, activation='relu', padding='valid', input_shape=input_shape ))\n    model.add(BatchNormalization())\n    model.add(MaxPool1D(2))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64, 2, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool1D(2))\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    return model","49905c45":"input_shape = (X_train.shape[1], 1)\nclf = create_cnn()\n\n# reshape data for CNN expected input\nnrows, ncols = X_train.shape # (602,30)\nX_train_arr = X_train.copy()\ny_train_arr = y_train.copy()\nX_train_arr = X_train_arr.reshape(nrows, ncols, 1)\n\nnrows, ncols = X_test.shape # (602,30)\nX_test_arr = X_test.copy()\ny_test_arr = y_test.copy()\nX_test_arr = X_test_arr.reshape(nrows, ncols, 1)\n\nclf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nclf.summary()\nclf.fit(X_train_arr, y_train_arr, epochs=200, verbose=0, sample_weight=np.where(y_train_arr == 1,1.0,1.0).flatten())\n# check model metrics\nscore = clf.evaluate(X_train_arr, y_train_arr, batch_size=128)\nprint('\\nAnd the Train Score is ', score[1] * 100, '%')\nscore = clf.evaluate(X_test_arr, y_test_arr, batch_size=128)\nprint('\\nAnd the Test Score is ', score[1] * 100, '%')\n# predict probabilities for test set\nyhat_probs = clf.predict(X_test_arr, verbose=0)\n# predict crisp classes for test set\nyhat_classes = clf.predict_classes(X_test_arr, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\nprint(\"Classification Report (CNN)\") \nprint(classification_report(y_test_arr, yhat_classes))\n\ntn, fp, fn, tp = display_metrics(clf, X_train_arr, X_test_arr, y_train_arr, y_test_arr, yhat_classes, 'CNN')\nvisualize(y_test_arr, yhat_classes, 'CNN')\ncnn_auc = auc_roc_metrics(clf, X_test_arr, y_test_arr, 'CNN')\nmetrics_results['cnn'] = cnn_auc","e393826b":"clf = create_dnn()\nclf.summary()\nclf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n#adam = keras.optimizers.Adam(learning_rate=0.001)\n# try using focal_loss to give heavier weight to examples that are difficult to classify\n# seems to improve the metrics slightly\n#clf.compile(optimizer=adam, loss=[focal_loss], metrics=['accuracy'])\n\n# create\/fit model on the training dataset\n#clf.fit(X_train, y_train, batch_size=16, epochs=32, sample_weight=np.where(y_train == 1,0.2,1.0).flatten())\nclf.fit(X_train, y_train, batch_size=16, epochs=20, verbose=0, sample_weight=np.where(y_train == 1,1.0,1.0).flatten())\n\n# check model metrics\nscore = clf.evaluate(X_train, y_train, batch_size=128)\nprint('\\nAnd the Train Score is ', score[1] * 100, '%')\nscore = clf.evaluate(X_test, y_test, batch_size=128)\nprint('\\nAnd the Test Score is ', score[1] * 100, '%')\n# predict probabilities for test set\nyhat_probs = clf.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = clf.predict_classes(X_test, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\nprint(\"Classification Report (DNN)\") \nprint(classification_report(y_test, yhat_classes))\n\ntn, fp, fn, tp = display_metrics(clf, X_train, X_test, y_train, y_test, yhat_classes, 'DNN')\nvisualize(y_test, yhat_classes, 'DNN')\ndnn_auc = auc_roc_metrics(clf, X_test, y_test, 'DNN')\nmetrics_results['dnn'] = dnn_auc","7d0ba93c":"clf = create_simple_dnn()\nclf.summary()\nclf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# create\/fit model on the training dataset\nclf.fit(X_train, y_train, batch_size=32, epochs=32, verbose=0, sample_weight=np.where(y_train == 1,1.0,1.0).flatten())\n\n# check model metrics\nscore = clf.evaluate(X_train, y_train, batch_size=128)\nprint('\\nAnd the Train Score is ', score[1] * 100, '%')\nscore = clf.evaluate(X_test, y_test, batch_size=128)\nprint('\\nAnd the Test Score is ', score[1] * 100, '%')\n# predict probabilities for test set\nyhat_probs = clf.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = clf.predict_classes(X_test, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\nprint(\"Classification Report (DNN Simple)\") \nprint(classification_report(y_test, yhat_classes))\ntn, fp, fn, tp = display_metrics(clf, X_train, X_test, y_train, y_test, yhat_classes, 'DNN Simple')\nvisualize(y_test, yhat_classes, 'DNN Simple')\ndnn_simple_auc = auc_roc_metrics(clf, X_test, y_test, 'DNN-Simple')\nmetrics_results['dnn_simple'] = dnn_simple_auc","8ddafff5":"clf = create_complex_dnn()\nclf.summary()\nclf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# create\/fit model on the training dataset\nclf.fit(X_train, y_train, batch_size=16, epochs=32, verbose=0, sample_weight=np.where(y_train == 1,4.0,1.0).flatten())\n\n# check model metrics\nscore = clf.evaluate(X_train, y_train, batch_size=128)\nprint('\\nAnd the Train Score is ', score[1] * 100, '%')\nscore = clf.evaluate(X_test, y_test, batch_size=128)\nprint('\\nAnd the Test Score is ', score[1] * 100, '%')\n# predict probabilities for test set\nyhat_probs = clf.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = clf.predict_classes(X_test, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\nprint(\"Classification Report (DNN complex)\") \nprint(classification_report(y_test, yhat_classes))\ntn, fp, fn, tp = display_metrics(clf, X_train, X_test, y_train, y_test, yhat_classes, 'DNN Complex')\nvisualize(y_test, yhat_classes, 'DNN Complex')\ndnn_complex_auc = auc_roc_metrics(clf, X_test, y_test, 'DNN-Complex')\nmetrics_results['dnn_complex'] = dnn_complex_auc","aa1bd597":"def create_autoencoder():\n    # input_dim must equal number of features in X_train and X_test dataset\n    clf1 = Sequential([\n        Dense(units=15, kernel_initializer='uniform', input_dim=30, activation='tanh', activity_regularizer=regularizers.l1(10e-5)),\n        Dense(units=7, kernel_initializer='uniform', activation='relu'),\n        Dense(units=7, kernel_initializer='uniform', activation='tanh'),\n        Dense(units=31, kernel_initializer='uniform', activation='relu'),\n        Dense(units=1, kernel_initializer='uniform', activation='sigmoid')\n    ])\n    return clf1","431f581b":"clf = create_autoencoder()\nclf.summary()\nclf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# create\/fit model on the training dataset\nclf.fit(X_train, y_train, batch_size=16, epochs=32, verbose=0, sample_weight=np.where(y_train == 1,2.0,1.0).flatten())\n\n# check model metrics\nscore = clf.evaluate(X_train, y_train, batch_size=32)\nprint('\\nAnd the Train Score is ', score[1] * 100, '%')\nscore = clf.evaluate(X_test, y_test, batch_size=32)\nprint('\\nAnd the Test Score is ', score[1] * 100, '%')\n# predict probabilities for test set\nyhat_probs = clf.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = clf.predict_classes(X_test, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\nprint(\"Classification Report (AutoEncoder)\") \nprint(classification_report(y_test, yhat_classes))\ntn, fp, fn, tp = display_metrics(clf, X_train, X_test, y_train, y_test, yhat_classes, 'AutoEncoder')\nvisualize(y_test, yhat_classes, 'AutoEncoder')\nautoencoder_auc = auc_roc_metrics(clf, X_test, y_test, 'AutoEncoder')\nmetrics_results['autoencoder'] = autoencoder_auc","e9354c1d":"print(\"AUC comparisons\")\nprint(metrics_results)","6fc82129":"plt.figure(figsize=(14,10),dpi=640)\n\nfor i in range(0,len(roc)):\n    auc1 = auc(roc[i][1],roc[i][2])\n    plt.plot(roc[i][1],roc[i][2], label=\"AUC {0}:{1}\".format(roc[i][0], auc1), linewidth=2)\n    \nplt.plot([0, 1], [0, 1], 'k--', lw=1) \nplt.xlim([0.0, 1.0]) \nplt.ylim([0.0, 1.05])\n\nplt.xlabel('False Positive Rate')  \nplt.ylabel('True Positive Rate') \nplt.title('ROC') \nplt.grid(True)\nplt.legend(loc=\"lower right\")\nplt.show()","de2b95f4":"y_val['Class'].value_counts()","1594b8a7":"y_test['Class'].value_counts()","a9d14f69":"final_results = pd.DataFrame(cm_results, columns=('algo','TN','FP','FN','TP')) \nfinal_results['SP'] = round(final_results['TN']\/(final_results['TN'] + final_results['FP']), 3)\nfinal_results['SE'] = round(final_results['TP']\/(final_results['TP'] + final_results['FN']), 3)\nfinal_results['Avg'] = (final_results['SP'] + final_results['SE'])\/2\nprint('test, val, split settings')\nprint(spl1,spl2)\nprint('test, val, split sizes')\nprint( (spl1-spl1*spl2), (spl1*spl2) )\nfiltered = final_results[~final_results.algo.str.contains('a', regex= True, na=False)]\nsort = filtered.sort_values(filtered.columns[7], ascending = False)\nprint(sort)\n#sort.to_csv('results.csv', sep=',', mode='a', encoding='utf-8', header=True)","fbb3d765":"Create 2nd train dataset from 1st dataset where the prediction was 1","022ee548":"Here are the final results in tabular form. ","e95bcee0":"Look at simpler and more complex examples of a DNN for comparison","2e6a8ed6":"number of Actual 0 and 1 in the final test dataset for all other models\n\"1\" total should match the FN + TP","94127834":"Create the models to be used layer, using Sequential()","191990e0":"Final confusion matrix results comparing the different algorithms. The items marked with ** are interim results for the 2 step process, and are not for comparison, only shown for reference. As you can see, both the FP and FN values are best for the 2 step process. This process is the most efficient at finding fraudulent transactions, and has the least amount of noise (FP).","4157b493":"Results from Deep NN are better than 1 step\/model examples, but overall not quite as good as the 2 step\/model process. I can get the sensitivity to be as good, but in that case, the specificity is much lower. As more data is added or processed through this DNN, the results should improve, maybe eventually beating the 2 step model. However, it seems that increasing the number of epochs will weight the model to higher false negatives, similar to using sample weights for the GBM model:\n\n**sample_weight=np.where(y_train == 1,0.1,1.0)**\n\n**giving a 1 in the training data 10 times the weight or inflence of a 0**\n\nFor now, we will keep the number of epochs at 5.\nWeighting has the same effect on this DNN as it had on the GBM. Best all around result with \n\nsample_weight=np.where(y_train == 1,0.1,1.0).flatten()","4e6ed134":"2nd step takes all the Predicted Positives (the misclassified FP from upper right (~ 14000) plus the TP (since we won't use the actual value until the validation step)) and reprocesses these using a different model. The other 2 squares (Predicted 0's) are not included in the 2nd model, since we already have a low False negative result, so the initial predicted 0s don't change. Will need to add those back into the final results at the end.","ae742547":"Create some calculation and visualization functions to show the results","b0fcddd3":"Next will try a few Neural Networks","9a0f22a5":"1st step\n\nbuild the 1st model to be used later on the validate dataset","d92fb86f":"Display the results","7d9b1ab5":"<h1 align='center' style='color:purple'>Credit Card Fraud - Imbalanced Data Set<\/h1>","57d7b8cb":"Look at the prediction values from the first model (preda_1) for the rows with a predicted label of 0","7d71c000":"<pre>\nAUC comparisons between all the models:\n\n{'lr': 0.965, 'rf': 0.975, 'gb': 0.975, 'gb1_train': 0.979, 'gb2_train': 0.967, 'gb1_validate': 0.99, 'gb2_validate': 0.974, '2-step': 0.941, 'dnn': 0.964, 'dnn_simple': 0.978, 'dnn_complex': 0.939, 'autoencoder': 0.956}\n{'lr': 0.968, 'rf': 0.979, 'gb': 0.976, 'gb1_train': 0.975, 'gb2_train': 0.968, 'gb1_validate': 0.991, 'gb2_validate': 0.978, '2-step': 0.957, 'dnn': 0.983, 'dnn_simple': 0.981, 'dnn_complex': 0.961, 'autoencoder': 0.952}\n\nSide by Side comparisions of all models\n\n\n    LR\n    [[64954  3280]\n     [   15   104]]\n\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.95      0.98     68234\n               1       0.03      0.87      0.06       119\n\n        accuracy                           0.95     68353\n       macro avg       0.52      0.91      0.52     68353\n    weighted avg       1.00      0.95      0.97     68353\n\n\n    RF\n    [[66254  1980]\n     [   13   106]]\n\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.97      0.99     68234\n               1       0.05      0.89      0.10       119\n\n        accuracy                           0.97     68353\n       macro avg       0.53      0.93      0.54     68353\n    weighted avg       1.00      0.97      0.98     68353\n\n\n    GB\n    [[66732  1502]\n     [   16   103]]\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.98      0.99     68234\n               1       0.06      0.87      0.12       119\n\n        accuracy                           0.98     68353\n       macro avg       0.53      0.92      0.55     68353\n    weighted avg       1.00      0.98      0.99     68353\n\n\n    2Step\n    [[45336  162]\n     [   5    67]]\n\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.99      0.99     45377\n               1       0.36      0.93      0.52        72\n        accuracy                           0.??     45449\n       macro avg       0.57      0.96      0.71     45449\n    weighted avg       1.00      0.99      0.99     45449\n\n\n    DNN\n    [[64991  3243]\n     [    9   110]]\n\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.95      0.98     68234\n               1       0.03      0.92      0.06       119\n\n        accuracy                           0.95     68353\n       macro avg       0.52      0.94      0.52     68353\n    weighted avg       1.00      0.95      0.97     68353\n\n\n    DNN Simple\n    [[63011  5223]\n     [    6   113]]\n\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.92      0.96     68234\n               1       0.02      0.95      0.04       119\n\n        accuracy                           0.92     68353\n       macro avg       0.51      0.94      0.50     68353\n    weighted avg       1.00      0.92      0.96     68353\n\n\n    AutoEncoder\n    60277 7957 7 112\n    [[60277  7957]\n     [    7   112]]\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.88      0.94     68234\n               1       0.01      0.94      0.03       119\n\n        accuracy                           0.88     68353\n       macro avg       0.51      0.91      0.48     68353\n    weighted avg       1.00      0.88      0.94     68353\n    \n    \n    CNN\n    [[64761  3473]\n     [   14   105]]\n    Classification Report\n                  precision    recall  f1-score   support\n\n               0       1.00      0.95      0.97     68234\n               1       0.03      0.88      0.06       119\n\n        accuracy                           0.95     68353\n       macro avg       0.51      0.92      0.52     68353\n    weighted avg       1.00      0.95      0.97     68353\n\n\n","dc836fbf":"run the CNN model","3f1b20ca":"Then we look at the ROC curve","a7cead97":"There is some variability in the results from run to run, due to random sampling and imbalanced data. This time, LogisticRegression has better prediction capability, the RandomForestClassifier test has a lot more mistakes in the False Positive category, and even a few more mistakes in the False Negative category.","1798aac2":"clean up the X_test dataset for future modeling, means remove the Prediction column","bb9bb6db":"Now run the basic DNN (Deep Neural Network)","b36b3657":"After tweaking the parameters, i can get a decent result from GradientBoostingClassifier. Changing the weights has a very large influence on the number of errors (FN and FP). Since this data is mostly 0 values, decreasing the weight of a true value vs a false value will decrease the FN, doing the opposite will decrease the FP. For one example run:  the sample_weight=np.where(y_train == 1,0.37,1.0) gives 13 FN and 795 FP. sample_weight=np.where(y_train == 1,0.1,1.0) gives 17 FN and 217 FP","ae5929b0":"run Logistic Regression model first","cb710a8c":"Add 1st model prediction column to X_test for filtering","62084c55":"The 2 step process has the highest sensitivity (and specificity) between the models. The 2 step process also improves the overall model prediction of positives by a large amount (FP\/TP ratio from above 10x to below 2x). I don't think we could get this high of precision and recall together with a single model. The best I could do with a single model was 10x FP\/TP ratio.","08d87f46":"Next we build the 2nd model to be used model later on the validate dataset and look at the output","715e0cdf":"**Use Case:** Credit Card Fraud Detection\n\n    Compare different common algorithms, develop and optimize a new 2 sequential\/consecutive model algorithm to see if this can give better results\n    \n**Author:** Donald Stierman - Senior Data Scientist\n\n**Details:** Imbalanced data can cause issues with most Machine Learning Algorithms and Neural Networks. To alleviate this, I choose to down-sample the training data to use as the input dataset. After creating the down-sampled dataset, I ran this through several different common model algorithms, including a new modeling technique I developed specifically for imbalanced data. I got this idea after reading about some highly effective Healthcare screening solutions currently in use. I.E. Breast Cancer detection in women (see comments: below). If a mammogram comes back positive, we already know that there will be a lot of false positives (benign tumors, scars, etc). Usually the doctor will follow up with a 2nd test, such as biopsy. This will screen out the false positives leaving mostly true positives (cancerous tissue). This same idea can possibly be applied to credit care fraud. We want to catch all true cases of fraud (fraud prevention), to be compliant with government regulations, and additionally not create a huge workload of false cases to be investigated (cost control).\n\ncomments:\n\n**Here are some different ways to explain the methodology used in the Healthcare use case:**\n\n*1st test (high specificity) -> 2nd test (high sensitivity) -> Only treat cancerous tissue\n\n*TN\/(TN + FP) is high ~ 1    -> TP\/(TP + FN) is high ~ 1    -> Find all Positive cases\n\n*catch all possible cases\/remove healthy patients -> remove all false flags -> high confidence in Positive result\/few missed positives\n\n\nThis same methodology can be applied to Credit Card Fraud detection\n\nLink to code repo at Github:\n\nhttps:\/\/github.com\/donaldstierman\/imbalanced_data\n\n**Models used:**<pre>\n    Logistic Regression\n    Random Forest\n    Gradient Boosted Decision Trees\n    Customized 2 Step Gradient Boosted Decision Trees\n    Deep Neural Network\n    1D Convolutional Neural Network\n    AutoEncoder\n<\/pre>\n**Goal:** \nFor this example, I chose 2 metrics to optimize, ROC\/AUC and best \"macro avg recall\". I chose these because in the health care example, it is better to catch all cancer patients, even if it means more tests are performed. To compare the results, first objective is to find the best overall model (lowest mislabelled predictions), second is to find the model that has a low number of false negatives (faudulent transactions that are missed) without having too many false positives (genuine transactions that are needlessly investigated)\n<pre>\n    1) Compare the AUC to find the most robust model of the single step models. However, the value of this metric cannot be calculated directly on the 2 step model, so we need to use #2 below for final comparison\n    2) Maximize the Sensitivity (higher priority) or reduce the number of False Negatives (FN\/TP ratio) and maximize the Specificity (lower priority) to control the number of tests performed in the 2nd step. I.E. catch all the fraudulent transaction even if there are false flags (false positives).\n<\/pre>\n**Results:** The Customized 2 Step model has the best results overall, by only a slight margin. \n                          \n                                AUC    Specificity\/Sensitivity\n                          \n    Logistic Regression         .967    .95\/.87\n    Random Forest               .977    .97\/.89  **best AUC**\n    Gradient Boosted Tree       .976    .99\/.84\n    Customized 2 Step GB Trees  NA      .99\/.93  **best overall**\n    Deep Neural Network         .973    .95\/.92  **2nd best overall**\n    AutoEncoder                 .954    .88\/.93    \n    ","a36c7ac4":"Now lets try a GradientBoosting Algorithm","91be8d72":"Public Credit Card Dataset. This is financial data, and is considered to be sensitive so it is \"encrypted\" through the use of PCA to protect privacy. Only the Time and Dollar columns are intact after the \"encryption\"\n\nDoing some initial data exploration","b604f7ea":"I am using undersampling as i think this is the best approach for this type of data","a6c72807":"This DNN is successful at reducing the FP\/TP ratio. This is expected as a Neural Network can decide on its own rules to include based on the input data. Below I try other more and less complex methods, but so far the results are not as good.","2e627c27":"Now that we have built the 2 models from the test dataset, run the untouched validate dataset through both of them to get an unbiased result to compare against","6d1c6cae":"Show the results of this model","9cfc13aa":"Next try the Random Forest model","f9fd2116":"Show the results of this model","7697b6af":"number of Actual 0 and 1 in the final validation dataset for 2-test model\n\"1\" total should match the FN + TP","cc9175d7":"My next idea is to run 2 consecutive models consecutively. 1st model should have low false negatives to catch (almost) all the actual positives, even if the number of false positives is high. Then only take these records with a predicted 1 value (should only be a few thousand), as the input for the next model. 2nd test should have low false positives to weed out the actual negatives. Will use the Validate dataset on the 2 models created from the Train and Test datasets\n\nHere are some details on the new model:\n\nCurrent:\nFull Dataset -> Train -> Build M1(Train) -> Run M1(Test) -> Filter(Predicted 1's from Test) -> Build M2 -> run M2(Filtered Test)\n                Test\n                \nTo Do:               \nFull Dataset -> Train -> Build M1(Train) -> Run M1(Test) -> Filter(Predicted 1's from Test) -> Build M2 -> run M1 and M2(Validate)\n                Test\n                Validate\n\nCan also try the inverse, but think that option will have less chance of success.","3df52ca9":"Divide the dataset into features and labels and then into Train, Test and Validate datasets","5063a6d2":"Adding swish activation function code for possible use later, can compare to relu, etc","276d3f47":"select rows with prediction of 1","24a5a11e":"Setup to run on Google Colab platform"}}