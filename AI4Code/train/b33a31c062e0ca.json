{"cell_type":{"40b55d8e":"code","a3833861":"code","dd7f0cf4":"code","b1f23a7c":"code","44790dc6":"code","3ad0f96b":"code","42247b82":"code","9fc22b14":"code","184257e3":"code","ebf68ce7":"code","34bf4aff":"code","6274c570":"code","761fe369":"code","902d8b9a":"code","a41c2ac7":"code","34ce15d2":"code","35a11218":"code","09e1668c":"code","40892132":"code","01f289fa":"code","94526625":"code","8c0e7cac":"code","643d6f15":"code","5e26d49c":"code","88eaf5ee":"code","42fbed5f":"code","db964e94":"code","2d8c8586":"code","6ac15d1d":"code","7d29020f":"code","347adf04":"code","57c1b89d":"code","a73cdf85":"code","d5785bb1":"code","c2bcc882":"code","04d55a12":"code","6383c811":"code","64526b9f":"code","020fb7d6":"code","6019593d":"code","2372c895":"code","633fd45d":"code","db6a400d":"code","26e8cd62":"code","6a49929f":"code","73d065e9":"code","5a1d7403":"code","f62e56d1":"code","512dfecd":"code","4d289c1b":"code","81fab177":"code","75f3d501":"code","fbd8c2eb":"code","fb64d9a8":"code","db258457":"code","6a26d53d":"code","f4299fd4":"code","a683b65e":"code","896e2f13":"code","a9b7723f":"code","88f8fd39":"code","c7a3641f":"code","6004fd05":"code","486e55f3":"code","da71e099":"code","3b3e5ae2":"code","6f8e3e24":"code","d32ca4b1":"code","21c8803b":"code","690cbfc3":"code","498e2974":"code","92c40ef8":"markdown","68a92247":"markdown","16e7a8e2":"markdown","a9943b4a":"markdown","29d96c0a":"markdown","4f213494":"markdown","b868906c":"markdown","b3beeed7":"markdown","a6986a49":"markdown","a849b617":"markdown","3a9cbfc8":"markdown","7d6b8291":"markdown","9a84804a":"markdown","53e6bda7":"markdown","ee325637":"markdown","6d4fbe3c":"markdown","b56adad5":"markdown","702dc4bf":"markdown","35692fec":"markdown","aa7dd0b1":"markdown","5bb4033c":"markdown","99f47e1a":"markdown","5dcb7bb5":"markdown","d8de62bd":"markdown","46458671":"markdown","4650d5c4":"markdown","43af2fec":"markdown","0027b5f6":"markdown","e5852129":"markdown","666a7329":"markdown","e8c9c235":"markdown","309c14a1":"markdown","c3098760":"markdown","9d2f50ba":"markdown","eac15695":"markdown","c77bffd6":"markdown","6e7cb37d":"markdown"},"source":{"40b55d8e":"#Important libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n# Pretty display for notebooks\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler","a3833861":"#Read_Data\ndata = pd.read_csv('..\/input\/customer-segmentaion\/customers.csv' , sep = ',' , encoding ='utf8')","dd7f0cf4":"#Show first 5 row from data\ndata.head()","b1f23a7c":"data.info()","44790dc6":"data.head()","3ad0f96b":"#Show data nulls\ndata.isnull().sum().sum()\n#No Null Data","42247b82":"#Data about Data\ndata.describe().style.background_gradient(cmap='Purples')","9fc22b14":"#Show outliers with boxplot\nplt.figure(figsize = (15,8))\ncol_names = [\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicatessen\"]\nfor i in range(6):\n    plt.subplot(3,2,i+1)#3 number of row #2 number of columns\n    sns.boxplot(x=data[col_names[i]], linewidth=2.5)\nplt.show()","184257e3":"#Show outliers with histogram\nplt.figure(figsize = (15,8))\ncol_names = [\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicatessen\"]\nfor i in range(6):\n    plt.subplot(3,2,i+1)#3 number of row #2 number of columns\n    sns.histplot(data[col_names[i]])\nplt.show()","ebf68ce7":"data.head()","34bf4aff":"log_data = data.copy()","6274c570":"data.head()","761fe369":"log_data.head()","902d8b9a":"log_data['fresh']  = np.log(log_data['Fresh'])\nlog_data['milk']   = np.log(log_data['Milk'])\nlog_data['grocery']= np.log(log_data['Grocery'])\nlog_data[\"frozen\"] = np.log(log_data[\"Frozen\"])\nlog_data[\"detergents_Paper\"] = np.log(log_data[\"Detergents_Paper\"])\nlog_data[\"delicatessen\"]     = np.log(log_data[\"Delicatessen\"])\n \nplt.figure(figsize = (13,8))\ncol_names_log= [\"Fresh\", \"fresh\", \"Milk\", \"milk\", \"Grocery\", \"grocery\",\n                \"Frozen\", \"frozen\", \"Detergents_Paper\", \"detergents_Paper\",\n                \"Delicatessen\", \"delicatessen\"]\nfor i in range(12):\n    plt.subplot(6,2,i+1)\n    sns.distplot(log_data[col_names_log[i]])\n    plt.title(col_names_log[i])\nplt.show()","a41c2ac7":"data.head()","34ce15d2":"log_data.head()","35a11218":"#Drop Columns after log transform\nlog_data.drop(['Fresh' , 'Milk','Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen'] \n          ,axis=1 ,inplace = True)","09e1668c":"#show data after log\nlog_data.head()","40892132":"log_data.describe().style.background_gradient(cmap='Purples')","01f289fa":"log_data.corr().style.background_gradient(cmap='inferno')","94526625":"data.head()","8c0e7cac":"# Display the correlation heatmap for data before and after log\nlog_corr = log_data.corr()\ncorr  = data.corr()\nf = plt.figure(figsize = (16,8))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax1 = sns.heatmap(corr, annot=True, mask=mask, cbar_kws={'label': 'Before Log Normalization'})\n\nmask2 = np.zeros_like(corr)\nmask2[np.tril_indices_from(mask2)] = True\nwith sns.axes_style(\"white\"):\n    ax2 = sns.heatmap(log_corr, annot=True, mask=mask2, cmap=\"YlGnBu\", cbar_kws={'label': 'After Log Normalization'})","643d6f15":"log_data['Region'].value_counts()","5e26d49c":"log_data['Channel'].value_counts()","88eaf5ee":"#convert to object to be more readable\n#Channel:{Hotel\/Restaurant\/Cafe - 1, Retail - 2}\nlog_data['Channel'].replace({1:\"h-r-c\" , 2:\"Retail\"} , inplace=True)\n#Region:{Lisbon - 1, Oporto - 2, or Other - 3} \nlog_data['Region'].replace({1:\"Lisbon\" , 2:\"Oporto\" , 3:\"Other Zone\"}, inplace=True)    \n","42fbed5f":"log_data['Region'].value_counts()","db964e94":"plt.figure(figsize=(18,6))\nsns.countplot(data = log_data, x=log_data[\"Region\"], hue=log_data[\"Channel\"], palette=\"muted\",\n              order=[\"Lisbon\",\"Oporto\", \"Other Zone\"])\nplt.title(\"REG\u0130ON\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.show()","2d8c8586":"sns.lmplot('detergents_Paper' , 'grocery' , data=log_data , hue='Region' , fit_reg=False , height=4)\nplt.show()","6ac15d1d":"sns.lmplot('detergents_Paper' , 'grocery' , data=log_data , hue='Channel' , fit_reg=False , height=5)\nplt.show()","7d29020f":"sns.lmplot('milk' , 'grocery' , data=log_data , hue='Region' , fit_reg=False , height=4)\nplt.show()","347adf04":"sns.lmplot('milk' , 'grocery' , data=log_data , hue='Channel' , fit_reg=False , height=4)\nplt.show()","57c1b89d":"plt.figure(figsize = (20,8))\nsns.barplot(data=log_data, palette=\"Set1\")","a73cdf85":"sns.pairplot(log_data , size=5) ","d5785bb1":"log_data.info()","c2bcc882":"log_data.head()","04d55a12":"#log_data.drop(['Region' , 'Channel'] \n #         ,axis=1 ,inplace = True)","6383c811":"#We have transformed categorical columns to dummy.\nlog_data= pd.concat([log_data, pd.get_dummies(log_data[\"Channel\"], drop_first=True),\n                     pd.get_dummies(log_data[\"Region\"])], axis=1)\nlog_data.drop(columns=[\"Channel\", \"Region\"], axis=1, inplace=True)\nlog_data.head()  \n#drop for Retail columns as the same column of h-r-c","64526b9f":"#show last 5 row from data\nlog_data.tail()","020fb7d6":"plt.figure(figsize=(15,10))\nsns.boxplot(x='variable', y='value', data=log_data.melt())\nplt.show()","6019593d":" outliers_list = []\n# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n    \n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data[feature], 25)\n    \n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data[feature], 75)\n    \n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    outliers = list(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))].index.values)\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    print('-=-=-=-=-=-=-=-=-=-=-=-=-------------------------------=-=-=-=-=-=-=-=-=-=-=-=')\n    outliers_list.extend(outliers)\n    \nprint(\"List of Outliers -> \\n :{}\".format(outliers_list))","2372c895":"duplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\nduplicate_outliers_list.sort()\nprint(\"\\nList of Common Outliers -> {}\".format(duplicate_outliers_list))","633fd45d":"# Remove the outliers\noutliers  = duplicate_outliers_list\n\nnew_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)","db6a400d":"new_data.info()","26e8cd62":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15,10))\nsns.boxplot(x='variable', y='value', data=new_data.melt())\nplt.show()","6a49929f":"new_data.head()","73d065e9":"#Before clustering, we transform features from original version to standardize version\n#as after dummy for two columns has zero and ones \n#and another columns has data by milliones \n\nscaler= StandardScaler()\nstd_data= scaler.fit_transform(new_data)","5a1d7403":"mean_vec = np.mean(std_data, axis=0)\ncov_mat = (std_data - mean_vec).T.dot((std_data - mean_vec)) \/ (std_data.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","f62e56d1":"print('NumPy covariance matrix: \\n%s' %np.cov(std_data.T))","512dfecd":"cov_mat = np.cov(std_data.T)\n\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors %s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","4d289c1b":"u,s,v = np.linalg.svd(std_data.T)\nu","81fab177":"cor_mat = np.corrcoef(std_data.T)\n\neig_vals, eig_vecs = np.linalg.eig(cor_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","75f3d501":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","fbd8c2eb":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\ncum_var_exp","fb64d9a8":"new_data.head()","db258457":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(9, 6))\n\n    plt.bar(range(10), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    #lamda\n    plt.step(range(10), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    #ratio \n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","6a26d53d":"# Apply PCA by fitting the new data with the same number of dimensions as features\nfrom sklearn.decomposition import PCA\n#svd_solver auto , full , arpack\n#n_component is number of new features\n#n_component is 4 as important component\npca = PCA(n_components=4, copy=True , svd_solver='full' , random_state=0 ,\n        iterated_power='auto' ,whiten = False)\nreduced_data = pca.fit_transform(std_data)","f4299fd4":"reduced_data","a683b65e":"# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2',\n                                                     'Dimension 3', 'Dimension 4'])\n                                                    ","896e2f13":"#show final data after preprocessing\nreduced_data.head()","a9b7723f":"#show another information \nprint(\"Explained Variance  => {}\\n\".format(pca.explained_variance_))\nprint(\"Explained Variance Ratio => {}\\n\".format(pca.explained_variance_ratio_))\nprint(\"Explained Variance Ratio cumsum => {}\\n\".format(pca.explained_variance_ratio_.cumsum()))\nprint(\"components_ => {}\\n\".format(pca.components_))","88f8fd39":"from sklearn.cluster import KMeans \nilist = [] #list of inertias #sum of distance between data point and center of cluster\nn=25 #number of clusters \nfor i in range (1,n):\n    KMeanModel = KMeans(n_clusters=i , init='k-means++' , random_state=33 , algorithm='auto')\n    KMeanModel.fit(reduced_data)#Fitting Model\n    ilist.append(KMeanModel.inertia_)","c7a3641f":"ilist","6004fd05":"plt.plot(range(1,n) , ilist)\nplt.title('Elbow Graph')\nplt.xlabel('Clusters')\nplt.ylabel('Inertias')\nplt.show()","486e55f3":"from sklearn.metrics import silhouette_score\nscore = []\nfor n in range(2,11):\n    KMean = KMeans(n_clusters=n , init='k-means++' , random_state=33 , algorithm='auto')\n    KMean.fit(reduced_data)\n    result = KMean.labels_\n    print(n ,\"    \" , silhouette_score(reduced_data , result))\n    score.append(silhouette_score(reduced_data , result))","da71e099":"plt.style.use(\"fivethirtyeight\")\nplt.plot(range(2,11) , score)\nplt.title('*Elbow for # of cluster with  silhouette_score*')\nplt.xlabel('Cluster')\nplt.ylabel('silhouette_score')\nplt.show()","3b3e5ae2":"KMeanModel = KMeans(n_clusters= 3, init='k-means++' , random_state=33 , algorithm='auto')\n#algorithm is auto , full or elkan\n#Fitting Model\nKMeanModel.fit(reduced_data)\ny_predict=KMeanModel.predict(reduced_data)\ncenters = KMeanModel.cluster_centers_\nlabels  = KMeanModel.labels_\ninertial= KMeanModel.inertia_\niteration=KMeanModel.n_iter_  ","6f8e3e24":"silhouette_Score = silhouette_score(reduced_data , labels)\nprint('Silutescore Score for KMean :: ',silhouette_Score)","d32ca4b1":"print('\\n Centers of 3 clusters :: \\n' , centers)\nprint('\\n Labels is :: \\n',labels)\nprint('\\n Y_Predictions :: \\n' , y_predict)\nprint('\\n Inertial is :: ',inertial)\nprint('\\n Iteration is :: ',iteration)","21c8803b":"#plot cluster size\nplt.hist(y_predict)\nplt.title(\"Sales Per Cluster\")\nplt.xlabel(\"Clusters\")\nplt.ylabel(\"Sales\")\nplt.show()","690cbfc3":"#convert data fram to np.array to avoid error\nreduced_data = np.array(reduced_data) #that all\n# Visualising the clusters for 2 dimantion\nplt.scatter(reduced_data[y_predict == 0, 0], reduced_data[y_predict == 0, 1], s = 20, c = 'red', label = 'Cluster 1')\nplt.scatter(reduced_data[y_predict == 1, 0], reduced_data[y_predict == 1, 1], s = 20, c = 'blue', label = 'Cluster 2')\nplt.scatter(reduced_data[y_predict == 2, 0], reduced_data[y_predict == 2, 1], s = 20, c = 'green', label = 'Cluster 3')\n#plt.scatter(reduced_data[y_predict == 3, 0], reduced_data[y_predict == 3, 1], s = 20, c = 'cyan', label = 'Cluster 4')\n#plt.scatter(reduced_data[y_predict == 4, 0], reduced_data[y_predict == 4, 1], s = 30, c = 'magenta', label = 'Cluster 5')\n#plt.scatter(reduced_data[y_predict == 5, 0], reduced_data[y_predict == 5, 1], s = 30, c = 'k', label = 'Cluster 6')\n\nplt.scatter(KMeanModel.cluster_centers_[:, 0], KMeanModel.cluster_centers_[:, 1], s = 100, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of Customers')\n\nplt.legend()\nplt.show()\n","498e2974":"#reduced_data['Cluster'] = y_predict\n#reduced_data.head()\n# Now we can use supervised learning","92c40ef8":"#-we found variation in data as standard deviation is different\n\n#-mean is bigger than median for all features so data has a lot of outliers\n\n#-Maximum is much bigger than mean ","68a92247":"# Feature Engineering","16e7a8e2":"from Elbow show that  silhouette_score is low with cluster =2 or 6\n\nand High with 4\n\nThen will choice #of cluster is 4 or 3 and show accuracy","a9943b4a":"PCA\nwe will find which compound combinations of features best describe customers.","29d96c0a":"From Histogram the distribution of each feature appears not normal","4f213494":"**General Information Of Data**\n* The customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal.\n\n **Features**\n* Fresh: annual spending (m.u.) on fresh products (Continuous);\n* Milk: annual spending (m.u.) on milk products (Continuous);\n* Grocery: annual spending (m.u.) on grocery products (Continuous);\n* Frozen: annual spending (m.u.) on frozen products (Continuous);\n* Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous);\n* Delicatessen: annual spending (m.u.) on and delicatessen products (Continuous);\n* Channel: {Hotel\/Restaurant\/Cafe - 1, Retail - 2} (Nominal)\n* Region: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)","b868906c":"The plot above clearly shows that most of the variance (36.54882177% of the variance to be precise) can be explained by the first principal component alone.\n\nThe second principal component still bears some information (18%) \n\nthe third principal components still bears some information(17%)\n\nthe fourth principal components still bears some information(12%)\n\nthe Fifth principal components still bears some information(11%)\n\nWhile can be safely dropped another principal components without losing much information.\n\n**Together, the first  principal components contain 94.5% of the information.**","b3beeed7":"The more verbose way above was  used for explaining , we could have used the numpy cov function:","a6986a49":"scatter between milk & paper, milk & grocery , and paper & grocery semi linearity\n\nand high correlation between them","a849b617":"The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \u201ccore\u201d of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.","3a9cbfc8":"eigendecomposition on the covariance matrix","7d6b8291":"**Outliers**","9a84804a":"when clusters is 1 the inertias is high \n\nfrom cluster =8 to 25 the change of inertias is small \n\nso will choise number of cluster is (4,6) ","53e6bda7":"The classic approach to PCA is to perform the eigendecomposition on the covariance matrix \u03a3, which is a d\u00d7d matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n\n\n\u03c3jk=1n\u22121\u2211i=1n(xij\u2212x\u00afj)(xik\u2212x\u00afk).\n\nWe can summarize the calculation of the covariance matrix via the following matrix equation:\n\n\u03a3=1n\u22121((X\u2212x\u00af)T(X\u2212x\u00af))\n\nwhere x\u00af is the mean vector x\u00af=1n\u2211i=1nxi.\n\nThe mean vector is a d-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.\n\n","ee325637":"the common approach is to rank the eigenvalues from highest to lowest in order choose the top k eigenvectors.","6d4fbe3c":"# Standardizing","b56adad5":"# - Eigendecomposition - Computing Eigenvectors and Eigenvalues","702dc4bf":"distribution for data more normality after applying log","35692fec":"**Correlation Matrix**","aa7dd0b1":"# Correlation Matrix","5bb4033c":"After applying a natural logarithm scaling to the data, the distribution of each feature appears much more normal","99f47e1a":"Whether to standardize the data prior to a PCA on the covariance matrix depends on the measurement scales of the original features. \n\nSince PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data.","5dcb7bb5":"All data is integer data types contain 440 row and 8 columns as a Features","d8de62bd":"-corr between grocery and milk 0.73 before log and 0.76 after log\n\n-corr between detergents_paper and milk 0.66 before log and 0.68 after log","46458671":"# Clustering Model","4650d5c4":"# Exploratory Data Analaysis","43af2fec":"# Sorting Eigenpairs","0027b5f6":"because having a row show up as multiple outliers can add to our confidence that it is truly an outlier.","e5852129":"Notice that wholesale is bigger than retail in all Region","666a7329":"boxplot show outlier just for data from  fresh  to delicatessen as another data is descrete","e8c9c235":"**Implementation: PCA**","309c14a1":"logarithmic transformation","c3098760":"# Covariance Matrix","9d2f50ba":"Eigendecomposition of the standardized data based on the correlation matrix","eac15695":"# Explained Variance\nAfter sorting the eigenpairs,  \u201chow many principal components are we going to choose for our new feature subspace?\u201d A useful measure is the so-called \u201cexplained variance,\u201d which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.","c77bffd6":"**There were 13 data points ([65, 66, 75, 128, 154, 203, 218, 233, 264, 304, 305, 325, 338])\nthat were considered outliers for more than one feature. So, instead of removing all outliers (which would result in us losing a lot of information),\nonly outliers that occur for more than one feature should be removed.**","6e7cb37d":"\n# Visualising the Clusters"}}