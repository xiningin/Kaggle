{"cell_type":{"bd6eb211":"code","c3d92fd3":"code","4f9b40d7":"code","cdd271ec":"code","7e18fd9d":"code","0adabd0a":"code","c3c9a477":"code","89d475f8":"code","9c085b77":"code","01f2d6ab":"code","0ecce450":"code","b8804128":"code","530ceaaa":"code","5e99fc83":"code","48560fbc":"code","599396a6":"code","1d823f55":"code","d2570dfc":"markdown","8dfa4fb6":"markdown","7c5b7ad7":"markdown","4f9a0da4":"markdown","b62f4ee8":"markdown","bd333072":"markdown"},"source":{"bd6eb211":"from lightgbm import LGBMClassifier \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\nplt.style.use('ggplot')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c3d92fd3":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nsub_df.head()","4f9b40d7":"display(train_df.head())\ntrain_df.shape, train_df.columns","cdd271ec":"train_df.target.value_counts().plot.pie()\nplt.show()","7e18fd9d":"def get_cross_feature(train_df, test_df):\n    # Feature cross \n    # f5: p_:0.07643\n    # f36: p_:0.08091\n    # f47: p_:0.04571\n    cross_a = ['f5', 'f36', 'f47']\n    cross_top = ['f34', 'f55', 'f43']\n    for ct in tqdm(cross_top):\n        for ca in cross_a:\n            train_df[f'{ct}_{ca}'] = train_df[ct] *  train_df[ca] \n            test_df[f'{ct}_{ca}'] = test_df[ct] *  test_df[ca] \n\n    print('Cross feature Done')\n    return train_df, test_df\n\ntrain_df, test_df = get_cross_feature(train_df, test_df)\ndrop2 = ['f47_freq', 'f55_freq', 'f5_freq', 'f43_freq', 'f34_freq',\n       'f43_f47', 'f34_f36', 'f34_f47', 'f43_f5', 'f43_f36']\n\ntrain_cols = [i for i in train_df if  i not in ['id', 'target'] + drop2]\nprint('Reflashed train_cols', len(train_cols))","0adabd0a":"import math\nimport typing as ty\nimport numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\ndef reglu(x: Tensor) -> Tensor:\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n\n\ndef geglu(x: Tensor) -> Tensor:\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n\n\ndef get_activation_fn(name: str) -> ty.Callable[[Tensor], Tensor]:\n    return (\n        reglu\n        if name == 'reglu'\n        else geglu\n        if name == 'geglu'\n        else t.sigmoid\n        if name == 'sigmoid'\n        else getattr(F, name)\n    )\n\n\nclass swish(nn.Module):\n    def __init__(self):\n        super(swish, self).__init__()\n \n    def forward(self, x):\n        x = x * t.sigmoid(x)\n        return x\n\ndef get_nonglu_activation_fn(name: str) -> ty.Callable[[Tensor], Tensor]:\n    return (\n        F.relu\n        if name == 'reglu'\n        else F.gelu\n        if name == 'geglu'\n        else get_activation_fn(name)\n    )\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        *,\n        d_numerical: int,\n        d: int,\n        d_hidden_factor: float,\n        n_layers: int,\n        activation: str,\n        normalization: str,\n        hidden_dropout: float,\n        residual_dropout: float,\n        d_out: int,\n    ) -> None:\n        super().__init__()\n\n        def make_normalization():\n            return {'batchnorm': nn.BatchNorm1d, 'layernorm': nn.LayerNorm}[\n                normalization\n            ](d)\n        if activation == 'swish':\n            swish_act = swish()\n            self.main_activation = swish_act \n            self.last_activation = t.sigmoid \n        else:\n            self.main_activation = get_activation_fn(activation)\n            self.last_activation = get_nonglu_activation_fn(activation)\n        \n        self.residual_dropout = residual_dropout\n        self.hidden_dropout = hidden_dropout\n\n        d_in = d_numerical\n        d_hidden = int(d * d_hidden_factor)\n\n        self.first_layer = nn.Linear(d_in, d)\n        self.layers = nn.ModuleList(\n            [\n                nn.ModuleDict(\n                    {\n                        'norm': make_normalization(),\n                        'linear0': nn.Linear(\n                            d, d_hidden * (2 if activation.endswith('glu') else 1)\n                        ),\n                        'linear1': nn.Linear(d_hidden, d),\n                    }\n                )\n                for _ in range(n_layers)\n            ]\n        )\n        self.last_normalization = make_normalization()\n        self.head = nn.Linear(d, d_out)\n\n    def forward(self, x_num: Tensor) -> Tensor:\n        x = []\n        if x_num is not None:\n            x.append(x_num)\n        x = t.cat(x, dim=-1)\n        x = self.first_layer(x)\n        for layer in self.layers:\n            layer = ty.cast(ty.Dict[str, nn.Module], layer)\n            z = x\n            z = layer['norm'](z)\n            z = layer['linear0'](z)\n            z = self.main_activation(z)\n            if self.hidden_dropout:\n                z = F.dropout(z, self.hidden_dropout, self.training)\n            z = layer['linear1'](z)\n            if self.residual_dropout:\n                z = F.dropout(z, self.residual_dropout, self.training)\n            x = x + z\n        x = self.last_normalization(x)\n        x = self.last_activation(x)\n        x = self.head(x)\n        x = x.squeeze(-1)\n        return x","c3c9a477":"from datetime import datetime\ndef train(model, data_loader, epochs, lr=0.001, cuda_flag=False):\n    model.train()\n    cost_func = F.binary_cross_entropy_with_logits\n    if cuda_flag:\n        model = model.to(t.device('cuda'))\n#     opt = t.optim.Adam(model.parameters(), lr=lr)\n    opt = t.optim.AdamW(model.parameters(), lr=lr)\n    total_st = datetime.now()\n    acc_his = []\n    for ep_i in range(epochs):\n        ep_st = datetime.now()\n        print('--'*20, f'[epoch {ep_i}]', '--'*20)\n        print(f'Start {ep_st}: .....')\n        ep_true_cnt = 0\n        ep_total_cnt = 0\n        for idx, (tmp_x, tmp_y) in enumerate(data_loader):\n            if cuda_flag:\n                tmp_x = tmp_x.to(t.device('cuda')).float()\n                tmp_y = tmp_y.to(t.device('cuda'))\n            opt.zero_grad()\n            p = model(tmp_x)\n            loss = cost_func(p, tmp_y)\n            loss.backward()\n            opt.step()\n            if idx % 200 == 0:\n                pr_loss = loss.item()\n                idx_p = str(idx).zfill(3)\n                ture_cnt = t.sum((p > 0.5) == tmp_y)\n                ep_true_cnt += ture_cnt\n                ep_total_cnt += tmp_y.size()[0]\n                auc_ = ture_cnt \/ tmp_y.size()[0]\n                print(f'[{ep_i}- {idx_p}] loss: {pr_loss:.5f} auc:{auc_:.5f}')\n        ep_cost = datetime.now() - ep_st\n        auc_ = ep_true_cnt \/ ep_total_cnt\n        acc_his.append(auc_)\n        if auc_ > 0.755:\n            print(f'[ {ep_i} ] epoch-auc: {auc_:.5f} cost:{ep_cost}')\n            break\n        print(f'[ {ep_i} ] epoch-auc: {auc_:.5f} cost:{ep_cost}')\n    \n    t_cost = datetime.now() - total_st\n    print(f'Done! Train cost {t_cost}')\n    return model, acc_his\n\n\ndef validation(model, te_dataloader, cuda_flag=False):\n    model.eval()\n    cost_func = F.binary_cross_entropy_with_logits\n    pred_out = []\n    total_st = datetime.now()\n    ep_true_cnt = 0\n    ep_total_cnt = 0\n    for idx, (tmp_x, tmp_y) in enumerate(te_dataloader):\n        if cuda_flag:\n            tmp_x = tmp_x.to(t.device('cuda')).float()\n            tmp_y = tmp_y.to(t.device('cuda'))\n        p = model(tmp_x)\n        pred_out.append(np.floor(p.cpu().detach().flatten()))\n        loss = cost_func(p, tmp_y)\n        if idx % 50 == 0:\n            pr_loss = loss.item()\n            idx_p = str(idx).zfill(3)\n            ture_cnt = t.sum((p > 0.5) == tmp_y)\n            ep_true_cnt += ture_cnt\n            ep_total_cnt += tmp_y.size()[0]\n            auc_ = ture_cnt \/ tmp_y.size()[0]\n            print(f'[{idx_p}] loss: {pr_loss:.5f} auc:{auc_:.5f}')\n    ep_cost = datetime.now() - total_st\n    auc_ = ep_true_cnt \/ ep_total_cnt\n    print(f'val-auc: {auc_:.5f} cost:{ep_cost}')\n    return pred_out\n\ndef model_predict(model, te_dataloader, cuda_flag=False):\n    model.eval()\n    cost_func = F.binary_cross_entropy_with_logits\n    pred_out = []\n    total_st = datetime.now()\n    for idx, tmp_x in enumerate(te_dataloader):\n        tmp_x = tmp_x[0]\n        if cuda_flag:\n            tmp_x = tmp_x.to(t.device('cuda')).float()\n        p = model(tmp_x)\n        pred_out.append(np.floor(p.cpu().detach().flatten().numpy()))\n\n    ep_cost = datetime.now() - total_st\n    print(f'Done cost:{ep_cost}')\n    return pred_out","89d475f8":"## data prepare\nfrom torch.utils.data import DataLoader, TensorDataset\nlen_ = train_df.shape[0]\ntr_x, tr_y = train_df.loc[:int(len_*0.85), train_cols].values, train_df.loc[:int(len_*0.85), 'target'].values\nte_x, te_y = train_df.loc[int(len_*0.85):, train_cols].values, train_df.loc[int(len_*0.85):, 'target'].values\ntr_ts = TensorDataset(t.tensor(tr_x).float(),\n                      t.tensor(tr_y).float())\ntr_loader = DataLoader(dataset = tr_ts, batch_size=512, shuffle=True)\nte_ts = TensorDataset(t.tensor(te_x).float(),\n                      t.tensor(te_y).float())\nte_loader = DataLoader(te_ts, batch_size=512)\n\n\ntest_ts = TensorDataset(t.tensor(test_df[train_cols].values).float())\ntest_loader = DataLoader(test_ts, batch_size=512)","9c085b77":"# model train\ndevice = t.device('cuda')\nprint(device)\nmodel = ResNet(\n    d_numerical=len(train_cols),\n    d_out=1,\n    d_hidden_factor=32, # 64\n    n_layers=6, # 4\n    activation='reglu', #'reglu', 'swish'\n    normalization='layernorm',\n    d=16, # 8\n    hidden_dropout=0.2, # 0.2\n    residual_dropout=0.1\n)\n# print(model)\nmodel, acc_his = train(model, tr_loader, epochs=100, lr=0.001, cuda_flag=True)","01f2d6ab":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20, 4))\nplt.plot([i.cpu().numpy() for i in acc_his])\nplt.title('acc trend by epoches')\nplt.show()","0ecce450":"_pred_out = validation(model, te_loader, cuda_flag=True)\nprint('\\n')\npred_out = model_predict(model, test_loader, cuda_flag=True)","b8804128":"pred_out_ =  np.concatenate(pred_out)\nnp.max(pred_out_), np.min(pred_out_) ,np.mean(pred_out_)","530ceaaa":"predict_out_df = test_df[['id', 'f0']].copy(deep=True)\npredict_out_df.columns = ['id', 'target']","5e99fc83":"predict_out_df['target'] = pred_out_","48560fbc":"sub_df_final = sub_df[['id']].merge(predict_out_df, on='id', how='left')\nsub_df_final.to_csv('submission.csv', index=False)\nprint(os.getcwd())","599396a6":"!mkdir ~\/.kaggle\n!echo '{\"username\":\"scchuy\",\"key\":\"03dc93223547d8ce9f34aefbb1d89ee2\"}' > ~\/.kaggle\/kaggle.json\n!chmod 600 ~\/.kaggle\/kaggle.json","1d823f55":"!kaggle competitions submit -c tabular-playground-series-nov-2021 -f submission.csv -m \"submit.2021-11-30-4\"","d2570dfc":"## 3.1 model-train","8dfa4fb6":"# 1 quick view","7c5b7ad7":"# 2- feature generator","4f9a0da4":"# submit","b62f4ee8":"## 3.1 Train & eval & predict function ","bd333072":"# 3 Restnet\n- inference: https:\/\/openreview.net\/forum?id=i_Q1yrOegLY\n- inference: https:\/\/github.com\/yandex-research\/rtdl\/tree\/main\/bin    \n\n\n**Revisiting Deep Learning Models for Tabular Data**  \nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko  \n22 May 2021 (modified: 09 Nov 2021)NeurIPS 2021 PosterReaders:  EveryoneShow Bibtex  \nKeywords: tabular data, architecture, DNN  \nTL;DR: Compared many DL models for tabular data, identified a strong baseline (ResNet) and proposed a powerful Transformer-based model.\nAbstract: The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  \n\nIn this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution. The source code is available at https:\/\/github.com\/yandex-research\/rtdl.  \n\nCode Of Conduct: I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.  \nCode: https:\/\/github.com\/yandex-research\/rtdl"}}