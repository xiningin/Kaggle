{"cell_type":{"f3d53d07":"code","092fdd8e":"code","bbea9de7":"code","d69b81dd":"code","21764136":"code","e7f7ec96":"code","c44f401f":"code","443dc6e8":"code","e262f1c5":"code","61b027f1":"code","414f3b5b":"code","fb14e5df":"code","541d1ed4":"code","a9fa4ac8":"code","61e1783e":"code","fd4af9f6":"code","aca9bd22":"code","f49508cd":"code","6936e6f6":"code","d0f3d510":"code","87db7bb1":"code","9aa7994c":"code","6a3cfc75":"code","fd7f00b1":"code","e5880ad9":"code","26195ff0":"code","cbc21884":"code","3c58acc8":"code","3d65518a":"code","efa2471f":"code","ceeebef3":"code","3aa5ba5e":"code","f908b696":"code","b74c1a20":"code","f801d716":"code","f87632e6":"code","c111c2b3":"code","b519aad3":"code","9c19a64a":"code","02d2a99d":"code","a0818932":"code","81b6d692":"code","e745f852":"code","28e2966b":"code","e5b1ecdc":"code","cfe51249":"code","90ac253f":"code","4e029f00":"code","3fcc4cff":"markdown","90355fe1":"markdown","1f96f7b3":"markdown","df2c5235":"markdown","2f2a7f05":"markdown","a22d19ff":"markdown","4a794c3f":"markdown","0781c25b":"markdown","4b68c0ff":"markdown","ac81ef93":"markdown","7fadb3f7":"markdown","a5ebfe71":"markdown","69042bef":"markdown","4be09854":"markdown","9ee03275":"markdown","7f5d46b6":"markdown","f12e1e20":"markdown","0cb6e2a4":"markdown","fd8a8c15":"markdown","0d0baff7":"markdown","bcddb556":"markdown","e8ee7a6b":"markdown","a8a5e06c":"markdown","105c06d4":"markdown","a9a224e1":"markdown","cf948abc":"markdown"},"source":{"f3d53d07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport warnings  \nwarnings.filterwarnings('ignore')","092fdd8e":"dataset = pd.read_csv('..\/input\/malicious-and-benign-websites\/dataset.csv')\ndataset.describe(include='all')","bbea9de7":"dataset.head()","d69b81dd":"dataset.drop('URL', axis =1, inplace=True)","21764136":"# Look for null values \nprint(dataset.isnull().sum())","e7f7ec96":"dataset.drop('CONTENT_LENGTH', axis =1, inplace=True)\ndataset.dropna(inplace=True)\nprint(dataset.isnull().sum())","c44f401f":"dataset.drop(['CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE', ], axis =1, inplace=True)","443dc6e8":"corr = dataset.corr()\ncorr.style.background_gradient(cmap='coolwarm')","e262f1c5":"dataset.drop(['TCP_CONVERSATION_EXCHANGE','URL_LENGTH','APP_BYTES','SOURCE_APP_PACKETS','REMOTE_APP_PACKETS','SOURCE_APP_BYTES','REMOTE_APP_BYTES'], axis = 1, inplace=True)\ncorr = dataset.corr()\ncorr.style.background_gradient(cmap='coolwarm')","61b027f1":"import seaborn as sns\nsns.distplot(dataset.loc[dataset['Type'] == 1]['NUMBER_SPECIAL_CHARACTERS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['NUMBER_SPECIAL_CHARACTERS'], bins = 50, color='blue')","414f3b5b":"sns.distplot(dataset.loc[dataset['Type'] == 1]['DIST_REMOTE_TCP_PORT'], bins = 50, color='red')\n","fb14e5df":"sns.distplot(dataset.loc[dataset['Type'] == 0]['DIST_REMOTE_TCP_PORT'], bins = 50, color='blue')","541d1ed4":"print(dataset.loc[dataset['Type'] == 0]['DIST_REMOTE_TCP_PORT'].value_counts())","a9fa4ac8":"print(dataset.loc[dataset['Type'] == 1]['DIST_REMOTE_TCP_PORT'].value_counts())","61e1783e":"sns.distplot(dataset.loc[dataset['Type'] == 1]['REMOTE_IPS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['REMOTE_IPS'], bins = 50, color='blue')","fd4af9f6":"sns.distplot(dataset.loc[dataset['Type'] == 1]['APP_PACKETS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['APP_PACKETS'], bins = 50, color='blue')","aca9bd22":"sns.boxplot(dataset['APP_PACKETS'])","f49508cd":"dataset = dataset[((dataset.APP_PACKETS - dataset.APP_PACKETS.mean()) \/ dataset.APP_PACKETS.std()).abs() < 3]","6936e6f6":"sns.boxplot(dataset['APP_PACKETS'])","d0f3d510":"sns.distplot(dataset.loc[dataset['Type'] == 1]['APP_PACKETS'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['APP_PACKETS'], bins = 50, color='blue')","87db7bb1":"print(dataset.loc[dataset['Type'] == 1]['DIST_REMOTE_TCP_PORT'].value_counts())","9aa7994c":"print(dataset.loc[dataset['Type'] == 0]['DIST_REMOTE_TCP_PORT'].value_counts())","6a3cfc75":"sns.distplot(dataset.loc[dataset['Type'] == 1]['DNS_QUERY_TIMES'], bins = 50, color='red')\nsns.distplot(dataset.loc[dataset['Type'] == 0]['DNS_QUERY_TIMES'], bins = 50, color='blue')","fd7f00b1":"print(dataset['DNS_QUERY_TIMES'].value_counts())","e5880ad9":"# Scale data then split\nfrom sklearn import preprocessing\n# Separate into train and test as well as features and predictor\nX = dataset.drop('Type',axis=1) #Predictors\ny = dataset['Type']\nX = preprocessing.scale(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11)","26195ff0":"# Method for evaluating results\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\ndef calculateScores(y_test, predictions):\n    accuracy = 100*accuracy_score(y_test, predictions)\n    precision = 100*precision_score(y_test, predictions)\n    recall = 100*recall_score(y_test, predictions)\n    f1 = 100*f1_score(y_test, predictions)\n    print (' Accuracy  %.2f%%' % accuracy)\n    print (' Precision %.2f%%'% precision)\n    print (' Recall    %.2f%%'% recall)\n    print (' F1        %.2f%%'% f1)\n    print('Confusion Matrix')\n    print(confusion_matrix(y_test,predictions))\n    return {'Accuracy':accuracy, 'F1': f1}","cbc21884":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression(solver='lbfgs')\nmodel = reg.fit(X_train, y_train)\npredictions = model.predict(X_test)\nscores = calculateScores(y_test, predictions)","3c58acc8":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(random_state=1)\nmlp.fit(X_train, y_train)\npredictions = mlp.predict(X_test)\nscores = calculateScores(y_test, predictions)","3d65518a":"def predict( X_train, y_train, **kwargs):\n    mlp = MLPClassifier(**kwargs, random_state=1)\n    mlp.fit(X_train, y_train)\n    return mlp.predict(X_test)","efa2471f":"def calculateScoresNoOutput(y_test, predictions):\n    accuracy = 100*accuracy_score(y_test, predictions)\n    precision = 100*precision_score(y_test, predictions)\n    recall = 100*recall_score(y_test, predictions)\n    f1 = 100*f1_score(y_test, predictions)\n    return {'Accuracy':accuracy, 'F1': f1}","ceeebef3":"# Let's try the different solvers\nsolvers = ['lbfgs', 'sgd', 'adam']\nresults = []\nfor solver in solvers:\n    result_dict = calculateScoresNoOutput(y_test, predict(X_train, y_train, solver=solver))\n    result_dict['Solver'] = solver\n    results.append(result_dict)\ndf = pd.DataFrame(results, columns = ['Solver','Accuracy', 'F1'])\ndf","3aa5ba5e":"# Generalise attempting different values\ndef try_different_values(values, column_name, X_train, y_train, **kwargs):\n    results = []\n    for value in values:\n        kwargs[column_name] = value\n        result_dict = calculateScoresNoOutput(y_test, predict(X_train, y_train, **kwargs))\n        result_dict[column_name] = value\n        results.append(result_dict)\n    df = pd.DataFrame(results, columns = [column_name,'Accuracy', 'F1'])\n    return df","f908b696":"activations = ['identity', 'logistic', 'tanh', 'relu']\ntry_different_values(activations, 'activation', X_train, y_train, solver='lbfgs')","b74c1a20":"alphas = []\nfor i in range(5,40):\n     alphas.append(1\/(2**i))\nalpha_df = try_different_values(alphas, 'alpha', X_train, y_train, solver='lbfgs', activation='logistic')","f801d716":"alpha_df.set_index('alpha', inplace=True)\nalpha_df.plot()","f87632e6":"print(alpha_df.loc[alpha_df['Accuracy'].idxmax()])\nprint(alpha_df.loc[alpha_df['F1'].idxmax()])","c111c2b3":"# Store alpha\nalpha= 4.76837158203125e-07","b519aad3":"batch_sizes = [2 ** e for e in range(10)]\nbatch_df = try_different_values(batch_sizes, 'batch_size', X_train, y_train, solver='lbfgs', activation='logistic', alpha=alpha)","9c19a64a":"batch_df.set_index('batch_size', inplace=True)\nbatch_df.plot()","02d2a99d":"layers = []\nfor i in range (1,25):\n    layers+= [(i)]\nlayers_df = try_different_values(layers, 'hidden_layer_sizes', X_train, y_train, solver='lbfgs', activation='logistic', alpha=alpha)","a0818932":"layers_df.set_index('hidden_layer_sizes', inplace=True)\nlayers_df.plot()","81b6d692":"print(layers_df.loc[layers_df['Accuracy'].idxmax()])\nprint(layers_df.loc[layers_df['F1'].idxmax()])","e745f852":"layers = []\nfor i in range (1,15):\n    for j in range(1,15):\n        layers+= [(i,j)]\nlayers_df = try_different_values(layers, 'hidden_layer_sizes', X_train, y_train, solver='lbfgs', activation='logistic', alpha=alpha)","28e2966b":"layers_df.set_index('hidden_layer_sizes', inplace=True)\nlayers_df.plot()","e5b1ecdc":"#layers_df = layers_df.reset_index()\nlayers_df = layers_df.reset_index()\nprint(layers_df.iloc[[layers_df['Accuracy'].idxmax()]])\nprint(layers_df.iloc[[layers_df['F1'].idxmax()]])","cfe51249":"layers = []\nfor i in range (1,10):\n    for j in range(1,10):\n        for k in range (1,10):\n            layers+= [(i,j,k)]\nlayers_3_df = try_different_values(layers, 'hidden_layer_sizes', X_train, y_train, solver='lbfgs', activation='logistic', alpha=alpha)","90ac253f":"layers_3_df.set_index('hidden_layer_sizes', inplace=True)\nlayers_3_df.plot()","4e029f00":"layers_3_df = layers_3_df.reset_index()\nprint(layers_3_df.iloc[[layers_3_df['Accuracy'].idxmax()]])\nprint(layers_3_df.iloc[[layers_3_df['F1'].idxmax()]])","3fcc4cff":"Looking at the data there are several categorical features (WHOIS_COUNTRY, SERVER etc.). For simplicity we'll ignore these features in our network.","90355fe1":"There are null values each for the DNS_QUERY_TIMES and SERVER columns, so we could easily drop these records \/ place a dummy value instead without affecting the data too much. The CONTENT_LENGTH column is a bit more concerning, we can't afford to drop that many records (almost half the dataset) and interpolating might distort the data somewhat. Given that there are plenty of other features, I'm choosing to drop the column.","1f96f7b3":"REMOTE_IPS is described as 'this variable has the total number of IPs connected to the honeypot'. Looks like malicious websites have a slightly lower grouping of remote IPs connected than benign.","df2c5235":"Looks like malicious websites generally have don't have a port associated (0 isn't a valid port?) not sure if we can infer anything from this or nor but we'll leave it in for now. It may be worth removing it later and observing the affect on the model.","2f2a7f05":"The regularisation parameter doesn't seem to change much. Let's save it and try the batch size.","a22d19ff":"## Data Analysis and Preparation\n\nLet's begin by having a look at the data - cleaning it up where appropriate. ","4a794c3f":"The graph is slightly misleading - there aren't actually any negative values (thankfully). Again nothing too obvious but we'll leave it our model. ","0781c25b":"Let's have a look at the correlations of the remaining features.","4b68c0ff":"That looks a lote better. Let's have a look at the histogram again.","ac81ef93":"The URL column is a unique identifier so we may as well remove that. ","7fadb3f7":"Batch size doesn't seem to be affecting the accuracy - let's move on to the hidden layers. ","a5ebfe71":"So our base model has 86% accueracy, but pretty poor precision and recall. From the confusion matrix we can see that while it's pretty good at predicting benign websites, it's poor at predicting the malicious ones, which makes it pretty useless if we wanted to use it in the real world. \n\nLet's see if we can improve this with a neural network. We'll use the standard SciKit learn class, starting off by using all the default values, before attempting to optimise it by adjusting its parameters.","69042bef":"Nothing too obvious. Again the spike at 0, these might correlate with the 0 on remote IPs. ","4be09854":"Looks like the lbfgs solver is the best, with highest accuray and F1 scores. We'll use that from now on. ","9ee03275":"Looks like there are some highly correlated features in there. A lot of these make sense - for example longer URLs will probably contain more special characters. Let's remove some of the more highly correlated features. ","7f5d46b6":"## The Model\n\nGiven our reduced dataset, let's start trying to create a model. We'll start by scaling the data then splitting it into a test and train set. We'll then use a simple logistic regression model to baseline the accuracy then see if we can perform better using a nerual network.","f12e1e20":"There's a definite advatage at 5 hidden layers. What if we add a second layer. ","0cb6e2a4":"Another improvement using the logistic activation. Let's try adjusting the regularisation","fd8a8c15":"Given there's a couple of values definitely way out of the normal range we'll remove these. ","0d0baff7":"So the best result with another hidden layer isn't better than a single hidden layer of size. We'll try a third just to see.","bcddb556":"Hard to infer much from this, looks like there might be an outlier. APP_PACKETS is '...the total number of IP packets generated during the communication between the honeypot and the server'. So a large number of these could be a technical error.","e8ee7a6b":"That looks a lot better, let's examine the data to see if it's suitable to use in the model.","a8a5e06c":"## Conclusion\nLooking at the final results. We've got up to 91% accuracy, with an F1 score of 65%. This is qutie good considering the rather naive neural network implementation and lazy data preparation. Whilst not the most complete implementation it does show that a neural network is a viable model for this data. \n### Future Work\nA more considered approad to implementing the neural network could be considered i.e. is the lbfgs solver still the best with a three layer hidden network? The data could be better prepared and the categorical data should be included to see if it adds any value. The other issue is that there is not a large amont of data, specifically malicious websites. If more data could be collected it could help the model, and help stop any problems of overfitting.\n\n","105c06d4":"# Overview\nThe goal of this notebook is to see if a simple neural network can be trained to classify a website as malicious or benign. ","a9a224e1":"The red bars are the malicious websites - there are some definite odd spikes there so this looks promising.","cf948abc":"Straight away that's a small improvement on the simple model, it is better at classifying the malicious websites, as seen in the confusion matrix and the F1 score.\n\nLet's have a play with the parameters to see if this can be improved."}}