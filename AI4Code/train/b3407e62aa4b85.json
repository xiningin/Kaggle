{"cell_type":{"6e22dda1":"code","80fc1d9a":"code","093eb188":"code","99e476ce":"code","47ce5828":"code","9a50d7d3":"code","348ca317":"code","2c387ba8":"code","8eab5992":"code","f897d724":"code","cdcd3003":"code","1be05e90":"code","730f949d":"code","02fdc29f":"code","d7e48055":"code","049a51f5":"code","c894abbf":"markdown","3f7ddc9b":"markdown","3e46ba50":"markdown","983fbb21":"markdown","70c498a1":"markdown","27c8c1ad":"markdown","2d42404d":"markdown","cd435218":"markdown","3a62be2b":"markdown","d0a1ff4f":"markdown","4ad68f37":"markdown","ec93a2ea":"markdown","d3919ff4":"markdown"},"source":{"6e22dda1":"!pip install pydotplus","80fc1d9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import Ridge, ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score, mean_absolute_error as MAE\nimport pydotplus as pdot\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport scipy.stats\nimport matplotlib as mlp\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","093eb188":"## importing Dataset\n\ndata = pd.read_csv('\/kaggle\/input\/Data_Oct2018_v2.csv', index_col=0, parse_dates=True)[['World Equities',\n                                                                                       'US Treasuries',\n                                                                                       'Bond Risk Premium',\n                                                                                       'Inflation Protection',\n                                                                                       'Currency Protection',\n                                                                                       'Real Estate']]\n\ntrain = data.iloc[:int(0.85 * data.shape[0]), :]\ntest = data.iloc[int(0.85 * data.shape[0]):, :]\ntrain.head()","99e476ce":"test.shape","47ce5828":"def skewness(r):\n    \"\"\"\n    Alternative to scipy.stats.skew()\n    Computes the skewness of the supplied Series or DataFrame\n    Returns a float or a Series\n    \"\"\"\n    demeaned_r = r - r.mean()\n    # use the population standard deviation, so set dof=0\n    sigma_r = r.std(ddof=0)\n    exp = (demeaned_r**3).mean()\n    return exp\/sigma_r**3\n\n\ndef kurtosis(r):\n    \"\"\"\n    Alternative to scipy.stats.kurtosis()\n    Computes the kurtosis of the supplied Series or DataFrame\n    Returns a float or a Series\n    \"\"\"\n    demeaned_r = r - r.mean()\n    # use the population standard deviation, so set dof=0\n    sigma_r = r.std(ddof=0)\n    exp = (demeaned_r**4).mean()\n    return exp\/sigma_r**4\n\n\ndef compound(r):\n    \"\"\"\n    returns the result of compounding the set of returns in r\n    \"\"\"\n    return np.expm1(np.log1p(r).sum())\n\n                         \ndef annualize_rets(r, periods_per_year):\n    \"\"\"\n    Annualizes a set of returns\n    We should infer the periods per year\n    but that is currently left as an exercise\n    to the reader :-)\n    \"\"\"\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year\/n_periods)-1\n\n\ndef annualize_vol(r, periods_per_year):\n    \"\"\"\n    Annualizes the vol of a set of returns\n    We should infer the periods per year\n    but that is currently left as an exercise\n    to the reader :-)\n    \"\"\"\n    return r.std()*(periods_per_year**0.5)\n\n\ndef sharpe_ratio(r, riskfree_rate, periods_per_year):\n    \"\"\"\n    Computes the annualized sharpe ratio of a set of returns\n    \"\"\"\n    # convert the annual riskfree rate to per period\n    rf_per_period = (1+riskfree_rate)**(1\/periods_per_year)-1\n    excess_ret = r - rf_per_period\n    ann_ex_ret = annualize_rets(excess_ret, periods_per_year)\n    ann_vol = annualize_vol(r, periods_per_year)\n    return ann_ex_ret\/ann_vol\n\n\ndef is_normal(r, level=0.01):\n    \"\"\"\n    Applies the Jarque-Bera test to determine if a Series is normal or not\n    Test is applied at the 1% level by default\n    Returns True if the hypothesis of normality is accepted, False otherwise\n    \"\"\"\n    if isinstance(r, pd.DataFrame):\n        return r.aggregate(is_normal)\n    else:\n        statistic, p_value = scipy.stats.jarque_bera(r)\n        return p_value > level\n\n\ndef drawdown(return_series: pd.Series):\n    \"\"\"Takes a time series of asset returns.\n       returns a DataFrame with columns for\n       the wealth index, \n       the previous peaks, and \n       the percentage drawdown\n    \"\"\"\n    wealth_index = 1000*(1+return_series).cumprod()\n    previous_peaks = wealth_index.cummax()\n    drawdowns = (wealth_index - previous_peaks)\/previous_peaks\n    return pd.DataFrame({\"Wealth\": wealth_index, \n                         \"Previous Peak\": previous_peaks, \n                         \"Drawdown\": drawdowns})\n\n\ndef semideviation(r):\n    \"\"\"\n    Returns the semideviation aka negative semideviation of r\n    r must be a Series or a DataFrame, else raises a TypeError\n    \"\"\"\n    if isinstance(r, pd.Series):\n        is_negative = r < 0\n        return r[is_negative].std(ddof=0)\n    elif isinstance(r, pd.DataFrame):\n        return r.aggregate(semideviation)\n    else:\n        raise TypeError(\"Expected r to be a Series or DataFrame\")\n\n\ndef var_historic(r, level=5):\n    \"\"\"\n    Returns the historic Value at Risk at a specified level\n    i.e. returns the number such that \"level\" percent of the returns\n    fall below that number, and the (100-level) percent are above\n    \"\"\"\n    if isinstance(r, pd.DataFrame):\n        return r.aggregate(var_historic, level=level)\n    elif isinstance(r, pd.Series):\n        return -np.percentile(r, level)\n    else:\n        raise TypeError(\"Expected r to be a Series or DataFrame\")\n\n\ndef cvar_historic(r, level=5):\n    \"\"\"\n    Computes the Conditional VaR of Series or DataFrame\n    \"\"\"\n    if isinstance(r, pd.Series):\n        is_beyond = r <= -var_historic(r, level=level)\n        return -r[is_beyond].mean()\n    elif isinstance(r, pd.DataFrame):\n        return r.aggregate(cvar_historic, level=level)\n    else:\n        raise TypeError(\"Expected r to be a Series or DataFrame\")\n\n\ndef var_gaussian(r, level=5, modified=False):\n    \"\"\"\n    Returns the Parametric Gauusian VaR of a Series or DataFrame\n    If \"modified\" is True, then the modified VaR is returned,\n    using the Cornish-Fisher modification\n    \"\"\"\n    # compute the Z score assuming it was Gaussian\n    z = norm.ppf(level\/100)\n    if modified:\n        # modify the Z score based on observed skewness and kurtosis\n        s = skewness(r)\n        k = kurtosis(r)\n        z = (z +\n                (z**2 - 1)*s\/6 +\n                (z**3 -3*z)*(k-3)\/24 -\n                (2*z**3 - 5*z)*(s**2)\/36\n            )\n    return -(r.mean() + z*r.std(ddof=0))\n\n\ndef summary_stats(r, riskfree_rate=0.03):\n    \"\"\"\n    Return a DataFrame that contains aggregated summary stats for the returns in the columns of r\n    \"\"\"\n    ann_r = r.aggregate(annualize_rets, periods_per_year=12)\n    ann_vol = r.aggregate(annualize_vol, periods_per_year=12)\n    ann_sr = r.aggregate(sharpe_ratio, riskfree_rate=riskfree_rate, periods_per_year=12)\n    dd = r.aggregate(lambda r: drawdown(r).Drawdown.min())\n    skew = r.aggregate(skewness)\n    kurt = r.aggregate(kurtosis)\n    cf_var5 = r.aggregate(var_gaussian, modified=True)\n    hist_cvar5 = r.aggregate(cvar_historic)\n    return pd.DataFrame({\n        \"Annualized Return\": ann_r,\n        \"Annualized Vol\": ann_vol,\n        \"Skewness\": skew,\n        \"Kurtosis\": kurt,\n        \"Cornish-Fisher VaR (5%)\": cf_var5,\n        \"Historic CVaR (5%)\": hist_cvar5,\n        \"Sharpe Ratio\": ann_sr,\n        \"Max Drawdown\": dd\n    })","9a50d7d3":"summary_stats(data)","348ca317":"data.aggregate(lambda r: drawdown(r).Drawdown.idxmin())","2c387ba8":"plt.style.use('seaborn-whitegrid')\n(1 + data).cumprod().plot(figsize=(9, 8))\nplt.show()","8eab5992":"class linear_factor:\n    \"\"\"\nlinear_factor is used to create object for linear modelling of factors. Three types of models are accepted:\n1. Lasso\n2. Ridge\n3. ElasticNet\n    \"\"\"\n    def __init__(self, param, model_name):\n        self.model_name = model_name\n        self.param = param\n    \n    def get_model(self):\n        if self.model_name == 'Lasso':\n            return Lasso()\n        elif self.model_name == 'ElasticNet':\n            return ElasticNet()\n        else:\n            return Ridge()\n    \n    def param_tune(self, X, Y):\n        \"\"\"\n        Tunes the hyperparameter of the given linear model, and saves the tuned model as object attribute. \n        Also, it returns the results of hyperparameter tuning.\n        \"\"\"\n        model = self.get_model()\n        grid_model = RandomizedSearchCV(model, n_iter = 10, param_distributions=self.param, scoring=['r2', 'neg_mean_squared_error'],\n                                        cv=6, random_state=42, return_train_score=True, refit='r2', verbose = False).fit(X.to_numpy(), Y.to_numpy())\n        self.columns = X.columns\n        self.fit_model = grid_model.best_estimator_\n        return pd.DataFrame(grid_model.cv_results_)[['param_alpha', 'mean_test_r2', 'mean_train_r2',\n                                     'mean_test_neg_mean_squared_error', 'mean_train_neg_mean_squared_error']].sort_values(by = ['mean_test_r2'], axis = 0, ascending = False)\n    \n    def predict(self, X_test):\n        self.y_pred = self.fit_model.predict(X_test.to_numpy())\n        \n    def summary(self, y_test):\n        \"\"\"\n        Summarizes the results of the model and it's predictions.\n        \"\"\"\n        y_test_n = y_test.to_numpy()\n        mae = MAE(y_test_n, self.y_pred)\n        mse = MSE(y_test_n, self.y_pred)\n        r2 = r2_score(y_test_n, self.y_pred)\n        print(self.model_name + '\\'s Test Results:\\nMSE = ' + str(mse) + ', MAE = ' + str(mae) + ', R-Square = ' + str(r2) + ',')\n        print('Value to Fund manager:', round(self.fit_model.intercept_ * 100, 3), '%')\n        \n        coef = pd.DataFrame({'Factor Loadings' : self.fit_model.coef_}, index = self.columns)\n        plt.style.use('Solarize_Light2')\n        ax = coef.plot(kind = 'bar', figsize=(12, 10), color='green', legend = False)\n        ax.set_title('Betas of Factors', y = 1.04)\n        \n        # set individual bar lables using above list\n        for i in ax.patches:\n            # get_x pulls left or right; get_height pushes up or down\n            ax.text(i.get_x()+0.05, i.get_height()+.0025, \\\n                str(round(i.get_height()*100, 2))+'%', fontsize=15,\n                    color='red')","f897d724":"param = {'alpha' : [0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.75, 0.00009]}","cdcd3003":"lasso = linear_factor(param, model_name = 'Lasso')\nresults = lasso.param_tune(train.drop('Real Estate', axis = 1), train['Real Estate'])\nprint('Validation Results:\\nTrain MSE =', results['mean_train_neg_mean_squared_error'].mean(),\n     ', Test MSE =', results['mean_test_neg_mean_squared_error'].mean())\nresults","1be05e90":"lasso.predict(test.drop('Real Estate', axis = 1))\nlasso.summary(test['Real Estate'])","730f949d":"ridge = linear_factor(param, model_name='Ridge')\nresults = ridge.param_tune(train.drop('Real Estate', axis = 1), train['Real Estate'])\nprint('Validation Results:\\nTrain MSE =', results['mean_train_neg_mean_squared_error'].mean(),\n     ', Test MSE =', results['mean_test_neg_mean_squared_error'].mean())  \nresults","02fdc29f":"ridge.predict(test.drop('Real Estate', axis = 1))\nridge.summary(test['Real Estate'])","d7e48055":"param2 = {'alpha' : param['alpha'],\n         'l1_ratio' : np.arange(0.01, 0.99, 0.01)}\nelastic = linear_factor(param2, model_name='ElasticNet')\nresults = elastic.param_tune(train.drop('Real Estate', axis = 1), train['Real Estate'])\nprint('Validation Results:\\nTrain MSE =', results['mean_train_neg_mean_squared_error'].mean(),\n     ', Test MSE =', results['mean_test_neg_mean_squared_error'].mean())  \nresults","049a51f5":"elastic.predict(test.drop('Real Estate', axis = 1))\nelastic.summary(test['Real Estate'])","c894abbf":"The above output gives the dates of occurence of maximum loss of wealth of investors corresponding to each asset. Both World Equities (i.e. financial markets all around the world) and Real Estate had their maximum drawdown during the Global financial crisis of 2007-09. Bond Risk Premium seems to go down with the onset of the crisis; but stabilizes over the period.","3f7ddc9b":"The Real Estate returns are largely influenced by Bond Risk Premium an US Treasuries. The Lasso also tells that Inflation Protection and Currency Protection do not influence returns of Real Estate significantly.","3e46ba50":"# **Factor Investing:**\n\nFactor Models serve mainly two purposes:\n1. To reduce the complexity of modeling asset price movements\n2. To understand the factor loadings of the individual assets in order to estimate the covariance of our returns\n\nFactor models can also be used for hedging.\n\nA linear factor model can be expressed in the following equation\n\n\\begin{equation*}\n    y_t = {X_t}^T \\beta + \\epsilon_t\n\\end{equation*}\n\n$t$ is used to index each observation. ${\\bf y} = \\{y_t\\}$ is called the dependent variable for observation $t$ and the vector ${X_t}^T = (X_t^1, X_t^2, \\dots, X_t^m)$ is called the set of factors (statisticians would call them independent or explanatory variables) for observation $t$. \n\n${\\bf \\beta} = (\\beta_1,\\dots,\\beta_m)'$ are called the factor loadings.\n\nBeta can obtained using Ordinary Least Square Method.\n\nHowever, in this notebook, we will explore variations of Linear Factor Models which are:\n1. Lasso\n2. Ridge\n3. ElasticNet\n\nThe above variations are used to eliminate any factor that does not contribute much to an asset's returns.\n\n# **Dataset Description:**\n\nThe dataset used for modelling has been taken from the paper written Blyth et. al.[2]. It contains returns of World Equities, US Treasury Bills, Bond Risk Premium, Inflation Protection and Currency Protections as factors for linear models. Further, For modelling, in the dataset, returns of Real Estate are taken as dependent variable $y$ which we want to model.\n\nReference:<br>\n[1] [Ang Andrew.  Asset Management: A Systematic Approach to Factor Investing. Oxford University Press, 2014](https:\/\/books.google.co.in\/books\/about\/Asset_Management.html?id=rWW8AwAAQBAJ&redir_esc=y)<br>\n[2] [Blyth Stephen, Szigety Mark, C., and Xia Jake. Flexible indeterminate factor-based asset allocation. Journal of Portfolio Management, 42(5):79\u201393, 2016.](https:\/\/dash.harvard.edu\/bitstream\/handle\/1\/27716504\/47611405.pdf?sequence=1)<br>","983fbb21":"According to ridge, most of the returns have come from World Equities and Bond Risk Premium. But given the R - Square and other metrics, Lasso is better than Ridge for this dataset.","70c498a1":"## **3. ElasticNet:**\n\n[ElasticNet](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#elastic-net) is a linear regression model which is trained based on both the L1 and L2 regularization of Betas of factors. The objective function to minimize in this model is:\n\n<center>$min_{w}$$\\frac{1}{2n_{samples}}$$||Xw - y||_{2}^{2}$$+\\alpha\\rho||w||_{1}$$+\\frac{\\alpha(1-\\rho)}{2}||w||_{1}^2$<\/center><br>\n\nHere, \n\n1. $\\alpha$ = Constant used to control the amount of shrinkage i.e. Hyperparameter of Ridge Regression model.\n2. $\\rho$ = l1_ratio\n\nElasticNet is genearally used when features are correlated with each other.\n\n*Reference*:<br>\n\n[1] [Zou Hui and Hastie Trevor. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B, 67(2):301\u2013320, 2005](https:\/\/web.stanford.edu\/~hastie\/Papers\/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf)<br>\n\n[2] [S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky. An Interior-Point Method for Large-Scale L1-Regularized Least Squares. IEEE Journal of Selected Topics in Signal Processing, 2007](https:\/\/web.stanford.edu\/~boyd\/papers\/pdf\/l1_ls.pdf)\n","27c8c1ad":"Though results of ElasticNet are almost same as Lasso, still some weightage has been given to Inflation Protection. Also, According to R-Square and other metrics, ElasticNet is better than Ridge but still lacks in comparison to Lasso.","2d42404d":"I have defined a common class for all the three variations of Linear model. Let's see how it works:","cd435218":"# **Exploratory Analysis**:\n\nWe will explore the data by first calculating basic stats.","3a62be2b":"World Equities, US Treasuries and Real Estate have upward trend. Both World Equities and Real Estate are somewhat in sync with each other too.","d0a1ff4f":"# **<u>Problem Statement<\/u>:**\n\n\n![Asset Management](https:\/\/e3zine.com\/wp-content\/uploads\/2017\/09\/IT_asset_management_shutterstock_420469072web.jpg)\n\n    Exploring Machine Learning Techniques to model Asset Price Movements in order to obtain an optimal portfolio","4ad68f37":"Bond Risk Premium, Inflation Protection and Currency Protection assets are giving negative risk adjusted returns (Sharpe Ratio) i.e. they are in loss. Maximum loss in wealth of investors have occurred in Real Estate","ec93a2ea":"## **2. Ridge:**\n\nThe Ridge regression too is used for eliminating non-significant factors. But it imposes a penalty on the size of Betas. It is also called L2 Regularization. The mathematical model for Ridge is:\n\n<center>$min_{w}$$\\frac{1}{2n_{samples}}$$||Xw - y||_{2}^{2}$$+\\alpha||w||_{2}^2$<\/center><br>\n\nHere, $\\alpha$ = Constant used to control the amount of shrinkage i.e. Hyperparameter of Ridge Regression model.\n\n*Reference*:<br>\n\n[1]  [Rifkin, R. M. and Lippert, R. A. Notes on Regularized Least Squares. Computer Science and Artificial Intelligence Laboratory Technical Report, May 1, 2007](http:\/\/cbcl.mit.edu\/publications\/ps\/MIT-CSAIL-TR-2007-025.pdf)<br>\n\n[2] [Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern optimization lens. Ann. Statist., 44(2):813\u2013852, 04 2016.](https:\/\/projecteuclid.org\/download\/pdfview_1\/euclid.aos\/1458245736)","d3919ff4":"## **1. Lasso:**\n\n[Lasso](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#lasso) is a linear model that computes sparse betas or factor loadings i.e. it eliminates any asset returns taken into consideration as factors that have zero betas. This helps in shrinkage of covariances of asset returns to be estimated. Lasso is also called L1 Regularization model.\n\nThe Mathematical model for Lasso is given by:\n\n<center>$min_{w}$$\\frac{1}{2n_{samples}}$$||Xw - y||_{2}^{2}$$+\\alpha||w||_{1}$<\/center><br>\n\nHere, $\\alpha$ = Constant used to tune the linear regression models i.e. Hyperparameter of Linear Regression model\n\n*Reference*:<br>\n\n[1] [Tibshirani Robert. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58(1):267\u2013288, 1996.](http:\/\/www-personal.umich.edu\/~jizhu\/jizhu\/wuke\/Tibs-JRSSB96.pdf)<br>\n\n[2] [Jerome Freidman, Trevor Hastie and Rob Tribshirani. Regularization Path For Generalized linear Models by Coordinate Descent. Journal of Statistical Software, Volume 33, Issue 1, January 2010.](https:\/\/www.jstatsoft.org\/article\/view\/v033i01\/v33i01.pdf)<br>\n\n[3] [S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky. An Interior-Point Method for Large-Scale L1-Regularized Least Squares. IEEE Journal of Selected Topics in Signal Processing, 2007.](https:\/\/web.stanford.edu\/~boyd\/papers\/pdf\/l1_ls.pdf)"}}