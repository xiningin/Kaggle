{"cell_type":{"96e32cc6":"code","6777c05b":"code","0e9674a1":"code","a46861c8":"code","a7d1e5dc":"code","18aa0c6b":"code","1a1a8a77":"code","fc560667":"code","c034dab1":"code","fbc21e74":"code","1b7eb101":"code","2bcf7f23":"code","7068f102":"code","91019c70":"code","a44cc813":"code","32c9e89c":"code","5488a91b":"code","154add39":"code","c9281f6c":"markdown","0a424324":"markdown","9fa9213b":"markdown","10a1ef95":"markdown","a38fb4a9":"markdown","ad2e8027":"markdown","b5ce5f17":"markdown","8cd30ef2":"markdown","7d2828b1":"markdown","57a371a6":"markdown","9b1800a5":"markdown","e4cc4979":"markdown","b895ef6d":"markdown","8d4250de":"markdown","54825559":"markdown","cd2a13c0":"markdown"},"source":{"96e32cc6":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","6777c05b":"file_path = '..\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv'\ndf = pd.read_csv(file_path)\nprint(df.shape)\ndf.head()","0e9674a1":"# Remove rows with missing target, separate target from predictors\ndf.dropna(axis=0, subset=['Survived'], inplace=True)\ny = df['Survived']\nX = df.drop(['Survived'], axis=1)\n\n\n# Break off validation set from training data.\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=1)","a46861c8":"# Number of missing values in each column of training data\nmissing_val_count_by_column = (X_train_full.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","a7d1e5dc":"# All categorical columns\nobject_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train_full[col]) == set(X_valid_full[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that could be dropped from the dataset:', bad_label_cols)","18aa0c6b":"X_train = X_train_full.drop(['Firstname', 'Lastname'], axis=1)\nX_valid = X_valid_full.drop(['Firstname', 'Lastname'], axis=1)","1a1a8a77":"training_countries = X_train['Country'].unique()\nvalid_countries = X_valid['Country'].unique()\ndiff_countries = list(set(training_countries) - set(valid_countries))\nprint(\"Unique countries in training set:\")\nprint(training_countries)\nprint(\"Unique countries in validation set:\")\nprint(valid_countries)\nprint(\"Countries in one and not in the other:\")\nprint(diff_countries)","fc560667":"categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n\nX_train.drop(['PassengerId'], axis=1, inplace=True)\nX_valid.drop(['PassengerId'], axis=1, inplace=True)","c034dab1":"y_baseline = np.zeros(len(y_valid))\nprint('Baseline MAE:', mean_absolute_error(y_valid, y_baseline))","fbc21e74":"# Preprocessing for categorical data. \n# Set handle_unknown='ignore' so that new categories are set to zeros.\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nrf_model_1 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nrf_pipeline_1 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', rf_model_1)\n                     ])\n\n# Preprocessing of training data, fit model \nrf_pipeline_1.fit(X_train, y_train.values.ravel())\n\n# Preprocessing of validation data, get predictions\npreds_1 = rf_pipeline_1.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds_1))","1b7eb101":"X.head()\nX_proc = X.drop(['Firstname','Lastname'], axis=1)","2bcf7f23":"# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(rf_pipeline_1, X_proc, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score:\", scores.mean())","7068f102":"def get_score_rf(n_estimators):\n    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n    # Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n    my_pipeline = Pipeline(steps=[\n                    ('preprocessor', preprocessor),\n                    ('model', RandomForestClassifier(n_estimators=n_estimators, random_state=0))\n                    ])\n    scores = -1 * cross_val_score(my_pipeline, X_proc, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    return scores.mean()","91019c70":"results = {}\nfor i in range(1,9):\n    results[i*50] = get_score_rf(i*50)\nprint(results)","a44cc813":"rf_model_final = RandomForestClassifier(n_estimators=50, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nrf_pipeline_final = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', rf_model_final)\n                     ])\n\n# Preprocessing of training data, fit model \nrf_pipeline_final.fit(X_train, y_train.values.ravel())\n\n# Preprocessing of validation data, get predictions\npreds_rf_final = rf_pipeline_final.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds_rf_final))","32c9e89c":"# # Preprocessing for categorical data. \n# # Set handle_unknown='ignore' so that new categories are set to zeros.\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Bundle preprocessing for numerical and categorical data\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Define model\n# xgb_model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=3)\n\n# # Bundle preprocessing and modeling code in a pipeline\n# xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                       ('model', xgb_model)\n#                      ])\n\n# # Preprocessing of training data, fit model \n# xgb_pipeline.fit(X_train, y_train.values.ravel(), model__early_stopping_rounds=5, \n#                  model__eval_set=[(X_valid, y_valid)], model__verbose=False)\n\n# # Preprocessing of validation data, get predictions\n# preds_xgb = xgb_pipeline.predict(X_valid)\n\n# print('MAE:', mean_absolute_error(y_valid, preds_xgb))\n\n\n#xgb_model.fit(X_train, y_train, \n#             early_stopping_rounds=5, \n#             eval_set=[(X_valid, y_valid)], \n#             verbose=False)","5488a91b":"# One-hot encode the data\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n\n# Fill any NaN columns (should only happen with country) with 0s\nX_train.fillna(0, inplace=True)\nX_valid.fillna(0, inplace=True)","154add39":"# Define the model\nxgb_model = xgb.XGBClassifier(random_state=1, learning_rate = 0.05, n_estimators=1000, n_jobs=3)\n\n# Fit the model\nxgb_model.fit(X_train,y_train, early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)])\n\n# Get predictions\npreds_xgb = xgb_model.predict(X_valid) # Your code here\n\n# Calculate MAE\nmae_xgb = mean_absolute_error(y_valid, preds_xgb) # Your code here\n\nprint(\"Mean Absolute Error:\" , mae_xgb)\n","c9281f6c":"Import the data set and take a peak.","0a424324":"## Random Forest","9fa9213b":"Import libraries.","10a1ef95":"Looks like n_estimators=50, 100, 150 are equally the best.We'll choose 50 for efficiency.  \nHere's the final model and predictions below for Random Forest.  \nEither way we're still worse than baseline.","a38fb4a9":"Finally, let's establish a baseline with the naive assumption that everyone died.","ad2e8027":"A little better, oddly enough. Maybe the single random training and validation sets were a little harder for the model than average over 5.  \nThis model still performs worse than baseline, however.  \nLet's see if we can find a better n_estimators parameter.  \nFirst we'll need to preprocess X without separating into test and validation sets.","b5ce5f17":"Hey, now we're better than baseline!","8cd30ef2":"Looks like performance is slightly worse than baseline with the current test and validation set.  \nNow let's see performance with 5-fold cross-validation (this agrees with the proportion of train-test split above).  \nFirst, we'll need to preprocess the entirety of X.","7d2828b1":"We can safely drop Firstname and Lastname columns, but let's dig into Country before dropping it.","57a371a6":"Looks like there are some countries in the validation set that aren't in the training set.  \nWe can handle this in our pipeline.","9b1800a5":"Looks like there aren't any!  \nLet's investigate some of the columns.","e4cc4979":"First, check for missing values in the training set.","b895ef6d":"Seems like XGBoost model doesn't play nicely with a sci-kit learn pipeline.  \nWe'll have to set up categorical columns manually.","8d4250de":"# Estonia Disaster Passenger Survival\nAttempting to predict passanger survival Random Forest and XGBoost models.","54825559":"Do a little pre-processing.","cd2a13c0":"## XGBoost"}}