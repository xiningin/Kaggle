{"cell_type":{"6bc278da":"code","6348a1fe":"code","2d8400a3":"code","d76289c2":"code","e5beb48f":"code","819bbcae":"code","106803b7":"code","6cdf62b9":"code","68466b0b":"code","b01ffcdc":"code","eb011243":"code","b107c77d":"code","26d298b1":"code","d8045c06":"code","1c93901b":"code","016d6d16":"code","c094b035":"code","d9dd620b":"code","cdba577e":"code","e248b07d":"code","75a523e1":"code","c4ce8764":"code","c4ef6db6":"code","1a0f2f05":"code","d1c0f4aa":"code","f8eb23ad":"code","08580f3e":"code","e1e157f6":"code","7adaf713":"code","f0264c73":"code","e30e3ecc":"code","9a415da6":"code","558e8592":"code","a6c9c2e2":"code","0be2aecf":"code","ac83ccc1":"code","b63925fb":"code","d3627766":"code","71ffb46a":"code","d815ff77":"code","fb8a10f4":"code","198787e5":"code","c74482e3":"code","b60a766f":"code","ebb633ec":"code","d9d6e9a7":"code","7115123d":"code","51a36f3c":"code","2ed6199c":"code","32c4fa00":"code","4f391d38":"code","a42eabbf":"code","e112d0c1":"code","91598186":"code","daa9bfa9":"code","cf0d124d":"code","51e0fcf5":"code","0e601b60":"code","0abbf394":"code","8988119f":"code","c109d389":"code","e57c7577":"code","159291fc":"code","c6a3f325":"code","053cdbff":"code","ceca5a57":"code","96b1a881":"code","413f6b72":"code","8eee1aca":"markdown","5aa2544f":"markdown","6250b4e0":"markdown","1cabdc52":"markdown","a2b4b6ce":"markdown","a7b2ec29":"markdown","8c0e562a":"markdown","d8dc3fd3":"markdown","8494e4fc":"markdown","72f568be":"markdown","e09559ff":"markdown","d8040e5f":"markdown","0d8d29c1":"markdown"},"source":{"6bc278da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6348a1fe":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","2d8400a3":"train.head()","d76289c2":"train.target.value_counts()","e5beb48f":"train.loc[train.target==0][:10]","819bbcae":"train.loc[train.target==1][:10]","106803b7":"train[\"target\"].replace(0, value = \"positive\", inplace = True)\ntrain[\"target\"].replace(1, value = \"negative\", inplace = True)","6cdf62b9":"train.head()","68466b0b":"train.groupby(\"target\").count()","b01ffcdc":"df = pd.DataFrame()\ndf[\"text\"] = train[\"text\"]\ndf[\"label\"] = train[\"target\"]","eb011243":"df.head()","b107c77d":"#uppercase-lowercase transformation\ndf['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#change symbols\ndf['text'] = df['text'].str.replace('[^\\w\\s]','')\n#numbers\ndf['text'] = df['text'].str.replace('\\d','')\n#stopwords\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n\n#delete\ndlt = pd.Series(' '.join(df['text']).split()).value_counts()[-1000:]\ndf['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in dlt))\n#lemmi\nfrom textblob import Word\n#nltk.download('wordnet')\ndf['text'] = df['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) ","26d298b1":"df.head()","d8045c06":"df.head()","1c93901b":"df.iloc[0]","016d6d16":"train_x, test_x, train_y, test_y = model_selection.train_test_split(df[\"text\"],\n                                                                   df[\"label\"], \n                                                                    random_state = 1)","c094b035":"train_y[0:5]","d9dd620b":"encoder = preprocessing.LabelEncoder()","cdba577e":"train_y = encoder.fit_transform(train_y)\ntest_y = encoder.fit_transform(test_y)","e248b07d":"train_y[0:5]","75a523e1":"test_y[0:5]","c4ce8764":"vectorizer = CountVectorizer()\nvectorizer.fit(train_x)","c4ef6db6":"x_train_count = vectorizer.transform(train_x)\nx_test_count = vectorizer.transform(test_x)","1a0f2f05":"x_train_count","d1c0f4aa":"vectorizer.get_feature_names()[0:5]","f8eb23ad":"x_train_count.toarray()","08580f3e":"#wordlevel","e1e157f6":"tf_idf_word_vectorizer = TfidfVectorizer()\ntf_idf_word_vectorizer.fit(train_x)","7adaf713":"x_train_tf_idf_word = tf_idf_word_vectorizer.transform(train_x)\nx_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)","f0264c73":"tf_idf_word_vectorizer.get_feature_names()[0:5]","e30e3ecc":"x_train_tf_idf_word.toarray()","9a415da6":"# ngram level tf-idf","558e8592":"tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range = (2,3))\ntf_idf_ngram_vectorizer.fit(train_x)","a6c9c2e2":"x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(train_x)\nx_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)","0be2aecf":"# characters level tf-idf","ac83ccc1":"tf_idf_chars_vectorizer = TfidfVectorizer(analyzer = \"char\", ngram_range = (2,3))\ntf_idf_chars_vectorizer.fit(train_x)","b63925fb":"x_train_tf_idf_chars = tf_idf_chars_vectorizer.transform(train_x)\nx_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)","d3627766":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(x_train_count, train_y)\naccuracy = model_selection.cross_val_score(loj_model, \n                                           x_test_count, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Count Vectors Accuracy:\", accuracy)","71ffb46a":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(x_train_tf_idf_word,train_y)\naccuracy = model_selection.cross_val_score(loj_model, \n                                           x_test_tf_idf_word, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Word-Level TF-IDF Accuracy:\", accuracy)","d815ff77":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(x_train_tf_idf_ngram,train_y)\naccuracy = model_selection.cross_val_score(loj_model, \n                                           x_test_tf_idf_ngram, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"N-GRAM TF-IDF Accuracy:\", accuracy)","fb8a10f4":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(x_train_tf_idf_chars,train_y)\naccuracy = model_selection.cross_val_score(loj_model, \n                                           x_test_tf_idf_chars, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"CHARLEVEL Accuracy:\", accuracy)","198787e5":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_count,train_y)\naccuracy = model_selection.cross_val_score(nb_model, \n                                           x_test_count, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Count Vectors Accuracy:\", accuracy)","c74482e3":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_tf_idf_word,train_y)\naccuracy = model_selection.cross_val_score(nb_model, \n                                           x_test_tf_idf_word, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Word-Level TF-IDF Accuracy:\", accuracy)","b60a766f":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_tf_idf_ngram,train_y)\naccuracy = model_selection.cross_val_score(nb_model, \n                                           x_test_tf_idf_ngram, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"N-GRAM TF-IDF Accuracy:\", accuracy)","ebb633ec":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_tf_idf_chars,train_y)\naccuracy = model_selection.cross_val_score(nb_model, \n                                           x_test_tf_idf_chars, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"CHARLEVEL Accuracy:\", accuracy)","d9d6e9a7":"rf = ensemble.RandomForestClassifier()\nrf_model = rf.fit(x_train_count,train_y)\naccuracy = model_selection.cross_val_score(rf_model, \n                                           x_test_count, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Count Vectors Accuracy:\", accuracy)","7115123d":"rf = ensemble.RandomForestClassifier()\nrf_model = rf.fit(x_train_tf_idf_word,train_y)\naccuracy = model_selection.cross_val_score(rf_model, \n                                           x_test_tf_idf_word, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Word-Level TF-IDF Accuracy:\", accuracy)","51a36f3c":"rf = ensemble.RandomForestClassifier()\nrf_model = loj.fit(x_train_tf_idf_ngram,train_y)\naccuracy = model_selection.cross_val_score(rf_model, \n                                           x_test_tf_idf_ngram, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"N-GRAM TF-IDF Accuracy:\", accuracy)","2ed6199c":"rf = ensemble.RandomForestClassifier()\nrf_model = loj.fit(x_train_tf_idf_chars,train_y)\naccuracy = model_selection.cross_val_score(rf_model, \n                                           x_test_tf_idf_chars, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"CHARLEVEL Accuracy:\", accuracy)","32c4fa00":"xgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(x_train_count,train_y)\naccuracy = model_selection.cross_val_score(xgb_model, \n                                           x_test_count, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Count Vectors Accuracy:\", accuracy)","4f391d38":"xgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(x_train_tf_idf_word,train_y)\naccuracy = model_selection.cross_val_score(xgb_model, \n                                           x_test_tf_idf_word, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Word-Level TF-IDF Accuracy:\", accuracy)","a42eabbf":"xgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(x_train_tf_idf_ngram,train_y)\naccuracy = model_selection.cross_val_score(xgb_model, \n                                           x_test_tf_idf_ngram, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"N-GRAM TF-IDF Accuracy:\", accuracy)","e112d0c1":"xgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(x_train_tf_idf_chars,train_y)\naccuracy = model_selection.cross_val_score(xgb_model, \n                                           x_test_tf_idf_chars, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"CHARLEVEL Accuracy:\", accuracy)","91598186":"test","daa9bfa9":"train","cf0d124d":"df","51e0fcf5":"y_pred= pd.Series(test.text)","0e601b60":"v = CountVectorizer()\nv.fit(train_x)\ny_pred = v.transform(y_pred)","0abbf394":"y_pred","8988119f":"y_pred","c109d389":"predictions=pd.DataFrame(loj_model.predict(y_pred))","e57c7577":"predictions['target']=predictions[0]","159291fc":"del predictions[0]","c6a3f325":"predictions","053cdbff":"submission['target']=predictions.target","ceca5a57":"submission","96b1a881":"test.head()","413f6b72":"submission.to_csv('model_submission.csv', index=False)\nsubmission.describe()","8eee1aca":"### TF-IDF","5aa2544f":"Count Vectors\nTF-IDF Vectors (words, characters, n-grams)\nWord Embeddings\n\nTF(t) = (sample frequency for t word) \/ (total number of words)\n\nIDF(t) = log_e (total documents \/ total document includes t word)","6250b4e0":"# Sentiment Classification with Machine Learning","1cabdc52":"## Lojistik Regression","a2b4b6ce":"### Feature Engineering","a7b2ec29":"### Count Vectors","8c0e562a":"## Random Forests","d8dc3fd3":"### checking sentiment","8494e4fc":"### Zeros: Positive Sentiment","72f568be":"## Naive Bayes","e09559ff":"## XGBoost","d8040e5f":"### Ones: Negative Sentiment","0d8d29c1":"### Test-Train"}}