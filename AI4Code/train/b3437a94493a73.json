{"cell_type":{"a6b7f2b8":"code","264730b5":"code","49930759":"code","e6b870a6":"code","8fe01210":"code","b94b27cb":"code","9b2af5bd":"code","6a744cd7":"code","6fe514b5":"code","d44bce6c":"code","e031b8fb":"code","f2b34913":"code","bd9baa71":"code","65430c73":"code","122fefe8":"code","3c4d6f5e":"code","e3d9a962":"code","a0170c9e":"code","71264485":"code","24ad1e12":"code","7ebf9c2d":"code","2e1e5513":"markdown","7442cb4f":"markdown","c7813a50":"markdown","c12a6814":"markdown","f207b178":"markdown","1d54a562":"markdown","a922c683":"markdown","593b611f":"markdown","6645660e":"markdown","3aeb8cb6":"markdown","c07a8e0d":"markdown","ec5e0db8":"markdown","b57bf62d":"markdown","2b5ae6de":"markdown","354e297c":"markdown","69ba4a8b":"markdown","20407745":"markdown","d7732657":"markdown","728bea86":"markdown","45484fa6":"markdown","9f3f1e9c":"markdown"},"source":{"a6b7f2b8":"import os\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np \nprint('Number of CPUs in the system: {}'.format(os.cpu_count()))","264730b5":"X1, y1 = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=123)\nX, X_test,y,y_test=train_test_split(X1,y1,test_size=0.2)\nsplitter = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\nfolds = list(splitter.split(X, y))\nprint(len(folds))\nprint(len(folds[0]),len(folds[1]))","49930759":"class CrossValidationParallelism(BaseEstimator, RegressorMixin, TransformerMixin):\n    \"\"\"\n    model : model to train in our folds \n    Author : Chaka Abderrazak \n    X : array\n    y : array\n        \"\"\"\n    def __init__(self, model,X ,y,test):\n        self.model = model\n        #self.split = split\n        self.X = X\n        self.y = y\n        self.test =test\n    def cross_validation(self, split,num_fold):\n            \"\"\"\n            Helper function to crossvalidate \n          \n            split : tuple\n             Training and test indices (train_idx, test_idx)\n            \"\"\"\n            X_train, y_train = X[split[0],:], y[split[0]]\n            X_val, y_val   = X[split[1],:], y[split[1]]\n            # Clone the model \n            self.meta_model_ = clone(self.model)\n            results = {}\n            # Train the model\n            self.meta_model_.fit(X_train, y_train)\n            # Make predictions on the val data\n            results['pred_val'] = self.meta_model_.predict_proba(X_val)[:,1]\n            # Make predictions on the test data\n            results['test_pred']= self.meta_model_.predict_proba(X_test)[:,1]\n            # Evaluate the model\n            results['auc']= roc_auc_score(y_val, results['pred_val'])\n            print(f\" num of fold : {num_fold}| AUC: {results['auc']}\")\n            results['num_fold'] =num_fold\n            return results\n        \n        \ndef benchmark_models( split):\n    \"\"\"\n    Helper function to benchmark models\n    X : array\n    y : array\n    split : tuple\n     Training and test indices (train_idx, test_idx)\n    \"\"\"\n    X_train, y_train = X[split[0],:], y[split[0]]\n    X_test, y_test   = X[split[1],:], y[split[1]]\n    \n    \n    model_library = {}\n    # One candidate model\n    model_library[\"logit\"] = LogisticRegression(solver='liblinear')\n    # Another candidate model\n    model_library[\"rf\"] = RandomForestClassifier(n_estimators=100, min_samples_leaf=20)\n\n    results = {}\n    for model_name, model in model_library.items():\n        # Train the model\n        model.fit(X_train, y_train)\n        # Make predictions on the test data\n        pred_test = model.predict_proba(X_test)[:,1]\n        # Evaluate the model\n        results[model_name] = roc_auc_score(y_test, pred_test)\n    \n    return pd.DataFrame(results, index = [\"ROC-AUC\"])\n        \n      ","e6b870a6":"model =RandomForestClassifier(n_estimators=100, min_samples_leaf=20)\ncrossvald = CrossValidationParallelism(model,X ,y,X_test)\nresults= crossvald.cross_validation(split=folds[0],num_fold=0)\nresults['auc']","8fe01210":"r =benchmark_models(split=folds[0])\nr.head()","b94b27cb":"import multiprocessing as mp\n#pool = mp.Pool(3)\n# Python can count the available cores for you in most cases\npool = mp.Pool(mp.cpu_count()-1)\n","9b2af5bd":"%%time\nresults = []\ndef log_result(x):\n    results.append(x)\n    \nfor fold in folds:\n    pool.apply_async(benchmark_models, args=(fold,), callback = log_result)\n    #results.append(pool.apply_async(benchmark_models, args=(X, y, fold)).get())","6a744cd7":"# Close the pool for new tasks\npool.close()\n# Wait for all tasks to complete at this point\npool.join()\n","6fe514b5":"# After collecting the results, we can work with the data as usual.\nresult = pd.concat(results, axis=0, sort=True)\nresult.head()","d44bce6c":"result.index.name = \"metric\"\nresult.reset_index()\naverage = result.groupby(['metric']).mean()\naverage.head()","e031b8fb":"%%time\nfrom joblib import Parallel, delayed\ncrossvald = CrossValidationParallelism(model,X ,y,X_test)\nresults_joblib  = Parallel(n_jobs=3, verbose=100)(\ndelayed(crossvald.cross_validation)(fold,num ) for num,  fold in enumerate(folds) )\n# After collecting the results, we can work with the data as usual.\nfor i in results_joblib : \n    print(i['auc'] ,i['num_fold'])","f2b34913":"for i in results_joblib :\n    reuslt = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in i.items() ]))\n    # final = pd.concat(reuslt)\n    print(reuslt.head(2))","bd9baa71":"np.hstack([ i['pred_val'] for i in results_joblib])[0:3]","65430c73":"import numpy as np  \nnp.column_stack([ i['test_pred'] for i in results_joblib])[0:10]","122fefe8":"np.column_stack([ i['test_pred'] for i in results_joblib]).mean(axis=1)[0:3]","3c4d6f5e":"np.column_stack([ i['auc'] for i in results_joblib]).mean(axis=1)","e3d9a962":"%%time\nimport numpy as np\nimport ray\nimport time\nray.shutdown()\n# Start Ray.\nray.init()\n# Define our Class\nclass CrossValidationParallelism(BaseEstimator, RegressorMixin, TransformerMixin):\n    \"\"\"\n    model : model to train in our folds \n\n    X : array\n    y : array\n        \"\"\"\n    def __init__(self, model,X ,y,test):\n        self.model = model\n        #self.split = split\n        self.X = X\n        self.y = y\n        self.test =test\n    def cross_validation(self, split,num_fold):\n            \"\"\"\n            Helper function to crossvalidate \n          \n            split : tuple\n             Training and test indices (train_idx, test_idx)\n            \"\"\"\n            X_train, y_train = X[split[0],:], y[split[0]]\n            X_val, y_val   = X[split[1],:], y[split[1]]\n            # Clone the model \n            self.meta_model_ = clone(self.model)\n            results = {}\n            # Train the model\n            self.meta_model_.fit(X_train, y_train)\n            # Make predictions on the val data\n            results['pred_val'] = self.meta_model_.predict_proba(X_val)[:,1]\n            # Make predictions on the test data\n            results['test_pred']= self.meta_model_.predict_proba(X_test)[:,1]\n            # Evaluate the model\n            results['auc']= roc_auc_score(y_val, results['pred_val'])\n            print(f\" num of fold : {num_fold}| AUC: {results['auc']}\")\n            results['num_fold'] =num_fold\n            return results\n\n# Start 3 tasks in parallel.\n@ray.remote\ndef cross_validation_helper(fold,num):\n    crossvald = CrossValidationParallelism(model,X ,y,X_test)\n    results = crossvald.cross_validation(fold,num)\n    return results \n\nresult_ids = [cross_validation_helper.remote(fold,num) for  num,  fold in enumerate(folds)]","a0170c9e":"values = ray.get(result_ids)","71264485":" np.column_stack([ i['auc'] for i in values]).mean(axis=1)\n","24ad1e12":"import numpy as np\nfrom joblib import parallel_backend # added line.\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC      \n\nfrom ray.util.joblib import register_ray # added line.\nregister_ray() # added line.\n\nparam_space = {\n    'C': np.logspace(-6, 6, 10),\n    'gamma': np.logspace(-8, 8, 10),\n    'tol': np.logspace(-4, -1, 10),\n    'class_weight': [None, 'balanced'],\n}\n\nmodel = SVC(kernel='rbf')\nsearch = RandomizedSearchCV(model, param_space, cv=3, n_iter=300,verbose=1)\ndigits = load_digits()\n#ray.init(address=\u201dauto\u201d)\nwith parallel_backend('ray'): # added line.\n    search.fit(digits.data, digits.target)","7ebf9c2d":"search.best_params_ ","2e1e5513":"Python does include a native way to run a Python workload across multiple CPUs. The multiprocessing module spins up multiple copies of the Python interpreter, each on a separate core, and provides primitives for splitting tasks across cores. But sometimes even multiprocessing isn\u2019t enough.\n\n\n\n\n<a id=2><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Joblib<\/center><\/h3>\n\n\nJoblib is a set of tools to provide lightweight pipelining in Python. In particular:\n\n    transparent disk-caching of functions and lazy re-evaluation (memoize pattern)\n    easy simple parallel computing\n\nJoblib is optimized to be fast and robust on large data in particular and has specific optimizations for numpy arrays. It is BSD-licensed.\n\nExample :\n\n        >>> from math import modf\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i\/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res","7442cb4f":"In the Parallel constructor you use the delayed argument to designate the function you want to run in parallel. delayed returns a new function that wraps your function. You can then call the newly wrapped function with arguments that'll be passed to your original function.","c7813a50":"The lines that were added to the original code are #2,#7,#8, and #21. Note that so far, this code will run on multiple cores but only on a single node, because we haven\u2019t specified how to connect to a Ray cluster yet.\n\nTo run it on a Ray cluster add ray.init(address=\u201dauto\u201d) or ray.init(address=\u201d<address:port>\u201d) before calling\nwith parallel_backend(\u201cray\u201d) as shown in line #20. \n","c12a6814":"# Complete result \n\n![image.png](attachment:image.png)","f207b178":"**Out_of_fold_predictions**\n","1d54a562":"<a id=1><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Multiprocessing Lib <\/center><\/h3>\n\nThe multiprocessing library is not particularly geared towards statistics, like scikit-learn. It was difficult for me to figure out which of its functionality is useful for our problem of parallelizing cross validation. To sum up our requirements, we want to:\n\n    Run multiple tasks in parallel\n    Pass several arguments to the function\n    \nWe first specify how many processes we want to run in parallel. This number is restricted by the number of cores available. I like to use all but one core on my machine (to avoid programs freezing).\n    \nOn a shared computation server, make sure you understand the policy and be polite by leaving resources for others.","a922c683":"**A cross validation helper Class \/Benchmark Models function**","593b611f":"After assigning each task the program moves on without waiting for the result from the worker. That was convenient when we assigned the tasks and didn\u2019t want to wait for the first result before assigning the second task. But we ususally want to wait for all results before moving on with script and, for example, average the results.\nWe tell the program to wait for all workers to complete their tasks using the method join(). Before we do so, we are required to make sure that no new tasks are assigned, which we do by using close() on the pool.\n\n","6645660e":"**Experimental Results**\nThose results comes from this  article : \n    \nhttps:\/\/medium.com\/distributed-computing-with-ray\/easy-distributed-scikit-learn-training-with-ray-54ff8b643b33\n    \nTo evaluate the benefits of distributing scikit-learn with Ray, we run hyperparameter tuning with random and grid search on SVM classifiers and random forests using the scikit-learn digits dataset. The code to reproduce the results is available here. We start with a single m5.8xlarge node with 32 cores on AWS. Then we increase the number of nodes to five and then to ten. We compare the Ray backend to the Loky, Multiprocessing, and Dask backends.\n\nThe following figure shows the performance comparison on a random forest benchmark as well as two SVM hyperparameter tuning benchmarks. For random forests, we use 45,000 trees. For hyperparameter tuning, we use 1,500 configurations for random search and 20,000 for grid search. The performance in terms of execution time is normalized to the performance of the scikit-learn default Loky backend (in the plots, higher is better).\n\n![image.png](attachment:image.png)\n\nOn multiple nodes, Ray outperforms the other backends. Ray performs significantly better in the random forest benchmark. Ray shines in this workload due to the large number of tree estimators used, which results in 45,000 tasks being submitted (compared to 1,500 tasks in hyperparameter tuning with random search and 20,000 in grid search). Ray\u2019s high-throughput decentralized scheduler along with its use of shared memory allow Ray to scale this workload efficiently to multiple nodes. The performance improvement as we add more nodes increases but is bottlenecked mainly by the serial part of the program (Amdahl\u2019s law). Adding more hyperparameters to tune can further improve the parallelism and result in improved scalability.\n\n\n**Conclusion**\n\nIn this Notebook , we showed how you can scale your scikit-learn applications to a cluster with Ray\u2019s implementation of joblib\u2019s backend, by adding four lines of code.\n\nThis is only one of many powerful libraries built to scale using Ray, including Tune, a scalable hyperparameter tuning library and RLlib, a scalable reinforcement learning library.\n\n\n **Upvote if  you  find it Useful**","3aeb8cb6":"# Reference : \n\nhttps:\/\/docs.ray.io\/en\/latest\/joblib.html\n\nhttps:\/\/cosmiccoding.com.au\/tutorials\/multiprocessing\n\nhttps:\/\/joblib.readthedocs.io\/en\/latest\/parallel.html\n\ndifference between ray and spark : \nhttps:\/\/www.youtube.com\/watch?v=yLKHHiT2nWw&list=PLmetp36hFxeyc9qO_5tPNMW-YD3tZfCFN&index=6\n\nhttps:\/\/towardsdatascience.com\/parallelizing-python-code-3eb3c8e5f9cd?fbclid=IwAR2iuV1PBMa6Zp46hFG4R-b_Vr0AYOXefpvEbRRnqRn8xdA_HSOorj0zZ98\n\nhttps:\/\/johaupt.github.io\/python\/parallel%20processing\/cross-validation\/multiprocessing_cross_validation.html","c07a8e0d":"**Test Heleper Fucntion for benchmarking models**","ec5e0db8":"**Test prediction**","b57bf62d":" \n <a id=3><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Ray<\/center><\/h3>\n\n\nScikit-learn parallelizes training on a single node using the joblib parallel backends. Joblib instantiates jobs that run on multiple CPU cores. The parallelism of these jobs is limited by the number of CPU cores available on that node. The current implementation of joblib is optimized for a single node, but why not go further and distribute it on multiple nodes?\n\nRunning distributed applications on multiple nodes introduces a host of new complexities like scheduling tasks across multiple machines, transferring data efficiently, and recovering from machine failures. Ray handles all of these details while keeping things simple.\n\n\nI'll describe how to use Ray to easily build applications that can scale from your laptop to a large cluster.\n\n**Why Ray?**\n\nMany tutorials explain how to use Python\u2019s multiprocessing module. Unfortunately the multiprocessing module is severely limited in its ability to handle the requirements of modern applications. These requirements include the following:\n\n    Running the same code on more than one machine.\n    \n    Building microservices and actors that have state and can communicate.\n    \n    Gracefully handling machine failures.\n    \n    Efficiently handling large objects and numerical data.\n\nRay addresses all of these points, makes simple things simple, and makes complex behavior possible.\n\n\n**Necessary Concepts**\n\n\nTraditional programming relies on two core concepts: functions and classes. Using these building blocks, programming languages allow us to build countless applications.\n\nHowever, when we migrate our applications to the distributed setting, the concepts typically change.\n\nOn one end of the spectrum, we have tools like OpenMPI, Python multiprocessing, and ZeroMQ, which provide low-level primitives for sending and receiving messages. These tools are very powerful, but they provide a different abstraction and so single-threaded applications must be rewritten from scratch to use them.\n\nOn the other end of the spectrum, we have domain-specific tools like TensorFlow for model training, Spark for data processing and SQL, and Flink for stream processing. These tools provide higher-level abstractions like neural networks, datasets, and streams. However, because they differ from the abstractions used for serial programming, applications again must be rewritten from scratch to leverage them.\n\n\nRay occupies a unique middle ground. Instead of introducing new concepts. Ray takes the existing concepts of functions and classes and translates them to the distributed setting as tasks and actors. This API choice allows serial applications to be parallelized without major modifications.\n    \n**Starting Ray**\n\nThe ray.init() command starts all of the relevant Ray processes. On a cluster, this is the only line that needs to change (we need to pass in the cluster address). These processes include the following:\n\n    A number of worker processes for executing Python functions in parallel (roughly one worker per CPU core).\n    A scheduler process for assigning \u201ctasks\u201d to workers (and to other machines). A task is the unit of work scheduled by Ray and corresponds to one function invocation or method invocation.\n    A shared-memory object store for sharing objects efficiently between workers (without creating copies).\n    An in-memory database for storing metadata needed to rerun tasks in the event of machine failures.\n\nRay workers are separate processes as opposed to threads because support for multi-threading in Python is very limited due to the global interpreter lock.\n\n**Parallelism with Tasks**\n\nTo turn a Python function f into a \u201cremote function\u201d (a function that can be executed remotely and asynchronously), we declare the function with the @ray.remote decorator. Then function invocations via f.remote() will immediately return futures (a future is a reference to the eventual output), and the actual function execution will take place in the background (we refer to this execution as a task).\n\n Example : \n    \n            import ray\n            import time\n            ray.shutdown()\n            # Start Ray.\n            ray.init()\n\n            @ray.remote\n            def f(x):\n                time.sleep(1)\n                return x\n\n            # Start 4 tasks in parallel.\n            result_ids = []\n            for i in range(4):\n                result_ids.append(f.remote(i))\n\n            # Wait for the tasks to complete and retrieve the     results.\n            # With at least 4 cores, this will take 1 second.\n            results = ray.get(result_ids)  # [0, 1, 2, 3]\n\nBecause the call to f.remote(i) returns immediately, four copies of f can be executed in parallel simply by running that line four times.\n","2b5ae6de":"**Overall predictions :**","354e297c":"For cross validation, we would usually average the results over all splits and then compare our models.","69ba4a8b":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center> Modern Parallel and Distributed Python<\/center><\/h3>\n\n* [Process-Based Parallelism :multiprocessing ](#1)\n* [Joblib](#2)\n* [Ray](#3)\n\nPython is long on convenience and programmer-friendliness, but it isn\u2019t the fastest programming language around. Some of its speed limitations are due to its default implementation, cPython, being single-threaded. That is, cPython doesn\u2019t use more than one hardware thread at a time.\n\nAnd while you can use the threading module built into Python to speed things up, threading only gives you concurrency, not parallelism. It\u2019s good for running multiple tasks that aren\u2019t CPU-dependent, but does nothing to speed up multiple tasks that each require a full CPU. \n\n\nParallel and distributed computing are a staple of modern applications. We need to leverage multiple cores or multiple machines to speed up applications or to run them at a large scale. The infrastructure for crawling the web and responding to search queries are not single-threaded programs running on someone\u2019s laptop but rather collections of services that communicate and interact with one another.\n\nThis Notebook  reviews some common options for parallelizing Python code, including process-based parallelism, specialized libraries, ipython parallel,.... and Ray in order  to speed our code for :\n    \n **Cross_Validtion\/Select Best models**\n\n\n**Ray**\nDeveloped by a team of researchers at the University of California, Berkeley, Ray underpins a number of distributed machine learning libraries. But Ray isn\u2019t limited to machine learning tasks alone, even if that was its original use case. Any Python tasks can be broken up and distributed across systems with Ray.\n\nRay\u2019s syntax is minimal, so you don\u2019t need to rework existing apps extensively to parallelize them. The @ray.remote decorator distributes that function across any available nodes in a Ray cluster, with optionally specified parameters for how many CPUs or GPUs to use. The results of each distributed function are returned as Python objects, so they\u2019re easy to manage and store, and the amount of copying across or within nodes is kept to a minimum. This last feature comes in handy when dealing with NumPy arrays, for instance.\n\nRay even includes its own built-in cluster manager, which can automatically spin up nodes as needed on local hardware or popular cloud computing platforms.\n\n\n**Dask**\n\nFrom the outside, Dask looks a lot like Ray. It, too, is a library for distributed parallel computing in Python, with its own task scheduling system, awareness of Python data frameworks like NumPy, and the ability to scale from one machine to many.\n\nDask works in two basic ways. The first is by way of parallelized data structures \u2014 essentially, Dask\u2019s own versions of NumPy arrays, lists, or Pandas DataFrames. Swap in the Dask versions of those constructions for their defaults, and Dask will automatically spread their execution across your cluster. This typically involves little more than changing the name of an import, but may sometimes require rewriting to work completely.\n\nThe second way is through Dask\u2019s low-level parallelization mechanisms, including function decorators, that parcel out jobs across nodes and return results synchronously (\u201cimmediate\u201d mode) or asynchronously (\u201clazy\u201d). Both modes can be mixed as needed, too.\n\nOne key difference between Dask and Ray is the scheduling mechanism. Dask uses a centralized scheduler that handles all tasks for a cluster. Ray is decentralized, meaning each machine runs its own scheduler, so any issues with a scheduled task are handled at the level of the individual machine, not the whole cluster.\n\nDask also offers an advanced and still experimental feature called \u201cactors.\u201d An actor is an object that points to a job on another Dask node. This way, a job that requires a lot of local state can run in-place and be called remotely by other nodes, so the state for the job doesn\u2019t have to be replicated. Ray lacks anything like Dask\u2019s actor model to support more sophisticated job distribution.\n\n**Joblib**\n\nJoblib has two major goals: run jobs in parallel and don\u2019t recompute results if nothing has changed. These efficiencies make Joblib well-suited for scientific computing, where reproducible results are sacrosanct. Joblib\u2019s documentation provides plenty of examples for how to use all its features.\n\nJoblib syntax for parallelizing work is simple enough\u2014it amounts to a decorator that can be used to split jobs across processors, or to cache results. Parallel jobs can use threads or processes.\nJoblib includes a transparent disk cache for Python objects created by compute jobs. This cache not only helps Joblib avoid repeating work, as noted above, but can also be used to suspend and resume long-running jobs, or pick up where a job left off after a crash. The cache is also intelligently optimized for large objects like NumPy arrays. Regions of data can be shared in-memory between processes on the same system by using numpy.memmap.\n\nOne thing Joblib does not offer is a way to distribute jobs across multiple separate computers. In theory it\u2019s possible to use Joblib\u2019s pipeline to do this, but it\u2019s probably easier to use another framework that supports it natively.\n\n**The problem**\n\nSome common data science tasks take a long time to run, but are embarrassingly parallel. Embarrassingly parallel means that they do not depend on each other and could therefore easily be done at the same time. The best examples are training different models and cross validation. In cross validation, training the model on k-1 folds before testing it on the remaining fold and training the model on k-1 different folds before testing it on a different remaining fold are two tasks that are not connected. Because they are not connected, we can handle them to different workers and process them in parallel.\n\n**Process-Based Parallelism :multiprocessing**\n\nThe first approach is to use process-based parallelism. With this approach, it is possible to start several processes at the same time (concurrently). This way, they can concurrently perform calculations.\n\n \nScikit-learn has parallization implemented using its n_jobs option, but we don\u2019t need to rely on its ecosystem to parallelize model selection. Instead, we will use the multiprocessing library directly.\nStarting from Python 3, the multiprocessing package is preinstalled and gives us a convenient syntax for launching concurrent processes. It provides the Pool object, which automatically divides input into subsets and distributes them among many processes.\n\nHere is an example of how to use a Pool object to launch ten processes:\n    \n            import multiprocessing as mp\n\n            num_workers = mp.cpu_count()  \n\n            pool = mp.Pool(num_workers)\n            for task in tasks:\n                pool.apply_async(func, args = (task,))\n\n            pool.close()\n            pool.join()\n        \n**Select Best Model \/ Cross Validation Example  :**\n    \nWe can use scikit-learn to conveniently generate the indices for the training and test data for a number of cross-validation folds","20407745":"**Test the Class \/Function  for cross validation only one model :**","d7732657":"**n_jobs=-1: use all available cores**","728bea86":" **Test final prediction :**","45484fa6":"Function apply_async can be used to send function calls including additional arguments to one of the processes in the pool. In contrast to apply (without the _async), the program will not wait for each call to be completed before moving on. We can therefore assign the first cross-validation iteration and immidiately assign the second iteration before the first iteration is completed.\nThere is a drawback to apply_async in that it does not return the result after the call complete. Instead, it returns another object with a get() method. A more convenient solution is a callback. The callback function will be called on the result once the function call is completed. So we\u2019ll specify a list for the results and a callback to save each result into that list.\n\nAn important intuition with apply and apply_async is that we assign a single function call to a worker when we call the function. In contrast, the map functionality would assign a list of tasks to available workers at once. apply_async calls the workers into your office one by one to explain their task to them.\nIMPORTANT: The results will not come back in the same order as we assigned the tasks. If we want to match the results to each fold, then we should pass an identifier to the function.\n\n**More Explanation from stackoverflow :**\n\nNotice, unlike **pool.map**, the order of the results may not correspond to the order in which the **pool.apply_async** calls were made.\n\nSo, if you need to run a function in a separate process, but want the current process to block until that function returns, use Pool.apply. Like P**ool.apply, Pool.map **blocks until the complete result is returned.\n\nIf you want the Pool of worker processes to perform many function calls asynchronously, use Pool.apply_async. The order of the results is not guaranteed to be the same as the order of the calls to **Pool.apply_async**.\n\nNotice also that you could call a number of different functions with **Pool.apply_async** (not all calls need to use the same function).\n\nIn contrast, Pool.map applies the same function to many arguments. However, unlike **Pool.apply_async**, the results are returned in an order corresponding to the order of the arguments.","9f3f1e9c":"**Distributed Scikit-Learn with Ray**\n\nScikit-learn parallelizes training on a single node using the joblib parallel backends. Joblib instantiates jobs that run on multiple CPU cores. The parallelism of these jobs is limited by the number of CPU cores available on that node. The current implementation of joblib is optimized for a single node, but why not go further and distribute it on multiple nodes?\n\nRunning distributed applications on multiple nodes introduces a host of new complexities like scheduling tasks across multiple machines, transferring data efficiently, and recovering from machine failures. Ray handles all of these details while keeping things simple.\n\nRay is a fast and simple framework for building and running distributed applications. Ray also provides many libraries for accelerating machine learning workloads. If your scikit-learn code takes too long to run and has a high degree of parallelism, using the Ray joblib backend could help you to seamlessly speed up your code from your laptop to a remote cluster by adding four lines of code that register and specify the Ray backend.\n\n\n**A More Complete Example**\n\nHere\u2019s a more complete example that does hyperparameter tuning of an SVM with cross-validation using random search."}}