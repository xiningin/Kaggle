{"cell_type":{"bb682649":"code","88dae215":"code","5ea8a295":"code","887bbdfc":"code","1d0bc980":"code","0e69371b":"code","889cb57d":"code","2f89793a":"code","f21380fb":"code","b17b09aa":"code","efedb318":"code","24ed5156":"code","05ac526b":"code","d5def782":"code","cf564d0e":"code","c40e44e2":"code","a9a87b3a":"code","57557cab":"code","9c5547ae":"code","a2328cc6":"code","00fbedfb":"code","7ab1de99":"code","b2a684ad":"code","992b23d0":"code","8ffccf76":"code","1e046ccf":"code","841313fb":"code","d672027c":"code","544ec955":"code","3fd137ab":"code","eb5cdf30":"code","adc933eb":"code","03b8cc00":"code","d981bdec":"code","1f252ef1":"code","3c3f2d79":"code","c7048b4f":"code","5fb0fb95":"code","369a970c":"code","dcb8f2bf":"code","67058843":"code","c5a11207":"code","1e9597be":"code","712787e5":"code","73ecc0e5":"code","3ff515e9":"code","a19ec6ee":"code","5dba4c8d":"code","215b1baf":"code","2bc9c66f":"code","89fe7f39":"code","887b0672":"code","d37ecafc":"code","2e0be8a0":"code","63dfc8cd":"code","acc413cf":"code","f0d0ec78":"code","3f10569d":"markdown","f22386f5":"markdown","9b3e1187":"markdown","2486b24d":"markdown","a6f8eb9c":"markdown","146ac495":"markdown","92ba06c7":"markdown","f97dc3ee":"markdown","b42f9a44":"markdown","939c5b28":"markdown","af8930ad":"markdown","5f9d005d":"markdown","8f671099":"markdown","30e0ea66":"markdown","d899018d":"markdown","0a149746":"markdown","59a82350":"markdown","67133589":"markdown","1f2a42eb":"markdown","861b2498":"markdown","7ba11316":"markdown","4761b224":"markdown","2561b5ba":"markdown","e79dd52a":"markdown","8e739193":"markdown","d0f089ba":"markdown","f8e8444b":"markdown","4785fb3b":"markdown","3ebedba0":"markdown","690b22c9":"markdown","e7dedfaa":"markdown","096dc833":"markdown","ce07636a":"markdown","dd2c5678":"markdown","a2c00152":"markdown","d0ca4fae":"markdown","78491d87":"markdown","00177c6e":"markdown","40d01221":"markdown","361d7ed5":"markdown","8934d675":"markdown","68dd4001":"markdown","4270f4b1":"markdown","c7a5eff1":"markdown"},"source":{"bb682649":"import pandas as pd\nimport numpy as np\nimport sklearn as skl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sc\nfrom sklearn.metrics import roc_auc_score\nimport gc #importing garbage collector\nimport time\nfrom scipy import signal\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline  \n\nSEED = 42\n#Pandas - Displaying more rorws and columns\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","88dae215":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5ea8a295":"timesteps = 14\nstartDay = 0","887bbdfc":"df_train = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv')\ndf_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sell_prices.csv')\ndf_days = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/calendar.csv')\n\ndf_train = reduce_mem_usage(df_train)\ndf_prices = reduce_mem_usage(df_prices)\ndf_days = reduce_mem_usage(df_days)","1d0bc980":"series_cols = df_train.columns[df_train.columns.str.contains(\"d_\")].values\nlevel_cols = df_train.columns[df_train.columns.str.contains(\"d_\")==False].values","0e69371b":"df_train.head(1)","889cb57d":"sns.set_palette(\"colorblind\")\n\nfig, ax = plt.subplots(5,1,figsize=(20,28))\ndf_train[series_cols].sum().plot(ax=ax[0])\nax[0].set_title(\"Top-Level-1: Summed product sales of all stores and states\")\nax[0].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"state_id\")[series_cols].sum().transpose().plot(ax=ax[1])\nax[1].set_title(\"Level-2: Summed product sales of all stores per state\");\nax[1].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"store_id\")[series_cols].sum().transpose().plot(ax=ax[2])\nax[2].set_title(\"Level-3: Summed product sales per store\")\nax[2].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"cat_id\")[series_cols].sum().transpose().plot(ax=ax[3])\nax[3].set_title(\"Level-4: Summed product sales per category\")\nax[3].set_ylabel(\"Unit sales of all products\");\ndf_train.groupby(\"dept_id\")[series_cols].sum().transpose().plot(ax=ax[4])\nax[4].set_title(\"Level-4: Summed product sales per product department\")\nax[4].set_ylabel(\"Unit sales of all products\");","2f89793a":"submission_sample =pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sample_submission.csv')\nsubmission_sample.head(10)","f21380fb":"# total number of series * number of quartiles * 2 (validation & evaluation)\n42840*9*2","b17b09aa":"submission_sample.shape","efedb318":"temp_series = df_train\nplt.figure(figsize=(12,8))\npeak_days = []\nx = np.count_nonzero(temp_series==0, axis=0)\npeaks, _ = sc.signal.find_peaks(x, height=np.quantile(x,0.75), threshold=max(x)\/25)\npeak_d = temp_series.columns[peaks]\npeak_days=peak_d\nplt.plot(x)\nplt.plot(peaks, x[peaks], \"x\", color='red')\n    \nplt.title('Number of Zero Sales per Day')\nplt.ylabel('Number of Zero Sales')\nplt.xlabel('Days')","24ed5156":"peak_days","05ac526b":"df_days[df_days['d'].isin(peak_days)]","d5def782":"peak_days_before=[]\npeak_days_after=[]\n\nfor i, days in enumerate(peak_days):\n    peak_days_before.append('d_'+str(np.int(peak_days[i][2:])-1))\n    peak_days_after.append('d_'+str(np.int(peak_days[i][2:])+1))","cf564d0e":"df_train_no_outlier = df_train.copy().T[1:]\ndf_train_no_outlier.columns = df_train.T.iloc[0]\n\nfor x,y,z in zip(peak_days,peak_days_before,peak_days_after):\n        df_train_no_outlier[df_train_no_outlier.index==x] = np.reshape([pd.concat([df_train_no_outlier[df_train_no_outlier.index==y],df_train_no_outlier[df_train_no_outlier.index==z]],axis=0).mean()],(1,30490))\n\ndf_train_no_outlier = df_train_no_outlier.T.reset_index()","c40e44e2":"df_train_no_outlier = pd.concat([df_train_no_outlier[level_cols],df_train_no_outlier[series_cols].apply(pd.to_numeric,downcast='float')],axis=1)\ndf_train_no_outlier = reduce_mem_usage(df_train_no_outlier)","a9a87b3a":"df_train_no_outlier.info()","57557cab":"temp_series = df_train_no_outlier\nplt.figure(figsize=(12,8))\nx = np.count_nonzero(temp_series==0, axis=0)\nplt.plot(x)\n    \nplt.title('Number of Zero Sales per Day')\nplt.ylabel('Number of Zero Sales')\nplt.xlabel('Days')\nplt.ylim(0,30000)","9c5547ae":"del temp_series, peak_days_before, peak_days_after, peak_d, peak_days, peaks","a2328cc6":"del df_train","00fbedfb":"df_train_no_outlier.head()","7ab1de99":"series_cols = df_train_no_outlier.columns[df_train_no_outlier.columns.str.contains(\"d_\")].values\nlevel_cols = df_train_no_outlier.columns[df_train_no_outlier.columns.str.contains(\"d_\")==False].values","b2a684ad":"Level1 = pd.DataFrame(df_train_no_outlier[series_cols].sum(),columns={'Total'}).T\nLevel2 = df_train_no_outlier.groupby(\"state_id\")[series_cols].sum()\nLevel3 = df_train_no_outlier.groupby(\"store_id\")[series_cols].sum()\nLevel4 = df_train_no_outlier.groupby(\"cat_id\")[series_cols].sum()\nLevel5 = df_train_no_outlier.groupby(\"dept_id\")[series_cols].sum()\n\nLevel6 = df_train_no_outlier.groupby([\"state_id\",'cat_id'])[series_cols].sum().reset_index()\nLevel6['index']=''\nfor row in range(len(Level6)):\n    Level6['index'][row]=str(Level6['state_id'][row])+'_'+str(Level6['cat_id'][row])\nLevel6.set_index(Level6['index'],inplace=True)\nLevel6.drop(['state_id','cat_id','index'],axis=1,inplace=True)\n\nLevel7 = df_train_no_outlier.groupby([\"state_id\",'dept_id'])[series_cols].sum().reset_index()\nLevel7['index']=''\nfor row in range(len(Level7)):\n    Level7['index'][row]=str(Level7['state_id'][row])+'_'+str(Level7['dept_id'][row])\nLevel7.set_index(Level7['index'],inplace=True)\nLevel7.drop(['state_id','dept_id','index'],axis=1,inplace=True)\n\nLevel8 = df_train_no_outlier.groupby([\"store_id\",'cat_id'])[series_cols].sum().reset_index()\nLevel8['index']=''\nfor row in range(len(Level8)):\n    Level8['index'][row]=str(Level8['store_id'][row])+'_'+str(Level8['cat_id'][row])\nLevel8.set_index(Level8['index'],inplace=True)\nLevel8.drop(['store_id','cat_id','index'],axis=1,inplace=True)\n\nLevel9 = df_train_no_outlier.groupby([\"store_id\",'dept_id'])[series_cols].sum().reset_index()\nLevel9['index']=''\nfor row in range(len(Level9)):\n    Level9['index'][row]=str(Level9['store_id'][row])+'_'+str(Level9['dept_id'][row])\nLevel9.set_index(Level9['index'],inplace=True)\nLevel9.drop(['store_id','dept_id','index'],axis=1,inplace=True)\n\nLevel10= df_train_no_outlier.groupby([\"item_id\"])[series_cols].sum()\n\n\nLevel11= df_train_no_outlier.groupby([\"item_id\",'state_id'])[series_cols].sum().reset_index()\nLevel11['index']=''\nfor row in range(len(Level11)):\n    Level11['index'][row]=str(Level11['item_id'][row])+'_'+str(Level11['state_id'][row])\nLevel11.set_index(Level11['index'],inplace=True)\nLevel11.drop(['item_id','state_id','index'],axis=1,inplace=True)\n\n\nLevel12= df_train_no_outlier.copy()\nLevel12.set_index(Level12['id'],inplace=True, drop =True)\nLevel12.drop(level_cols,axis=1,inplace=True)\n\ndf=pd.concat([Level1,Level2,Level3,Level4,Level5,Level6,Level7,Level8,Level9,Level10,Level11,Level12])\n\ndel Level1,Level2,Level3,Level4,Level5,Level6,Level7,Level8,Level9,Level10,Level11,Level12","992b23d0":"test = pd.concat([df.reset_index()['index'],submission_sample.reset_index().id[:42840]],axis=1)\ntest\ntest['index'].replace('_validation','',regex=True,inplace=True)","8ffccf76":"test['proof'] = ''\nfor row in range(len(test)):\n    if test['index'][row] in test['id'][row]:\n        test['proof'][row]=True\ntest[test['proof']==False]","1e046ccf":"del test","841313fb":"df_days[\"date\"] = pd.to_datetime(df_days['date'])\ndf_days.set_index('date', inplace=True)\n\ndf_days['is_event_day'] = [1 if x ==False else 0 for x in df_days['event_name_1'].isnull()] \ndf_days['is_event_day'] = df_days['is_event_day'].astype(np.int8)\n\nday_before_event = df_days[df_days['is_event_day']==1].index.shift(-1,freq='D')\ndf_days['is_event_day_before'] = 0\ndf_days['is_event_day_before'][df_days.index.isin(day_before_event)] = 1\ndf_days['is_event_day_before'] = df_days['is_event_day_before'].astype(np.int8)\n\ndel day_before_event\n\ndaysBeforeEventTest = df_days['is_event_day_before'][1913:1941]\ndaysBeforeEvent = df_days['is_event_day_before'][startDay:1913]\ndaysBeforeEvent.index = df_train_no_outlier.index[startDay:1913]","d672027c":"df_final = pd.concat([df.T.reset_index(drop=True), daysBeforeEvent.reset_index(drop=True)], axis = 1)\ndf_final = df_final[startDay:]","544ec955":"df_days","3fd137ab":"features = df_days[['is_event_day_before','wday','snap_CA','snap_TX','snap_WI']]\nfeatures.head()","eb5cdf30":"# adding 'id' column as well as 'cat_id', 'dept_id' and 'state_id', then changing the type to 'categorical'\ndf_prices.loc[:, \"id\"] = df_prices.loc[:, \"item_id\"] + \"_\" + df_prices.loc[:, \"store_id\"] + \"_validation\"\ndf_prices['state_id'] = df_prices['store_id'].str.split('_',expand=True)[0]\ndf_prices = pd.concat([df_prices, df_prices[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\ndf_prices = df_prices.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\ndf_prices[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\", 'state_id']] = df_prices[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\", 'state_id']].astype(\"category\")\ndf_prices = df_prices.drop(columns=2)","adc933eb":"price_features = pd.DataFrame(df_prices.groupby(['wm_yr_wk','store_id','cat_id'])['sell_price'].mean().reset_index())\nprice_features['sell_price'] = price_features['sell_price'].astype('float32')","03b8cc00":"price_features['store_cat'] = 0\n\nfor row in range(len(price_features)):\n     price_features['store_cat'][row]=str(price_features['store_id'][row])+'_'+str(price_features['cat_id'][row])","d981bdec":"price_features= price_features.pivot(index='store_cat',columns='wm_yr_wk',values='sell_price').T\nprice_features.head()","1f252ef1":"features = df_days[['wm_yr_wk','is_event_day_before','wday','snap_CA','snap_TX','snap_WI']]\nfeatures.head()\nfeatures = pd.merge(features.reset_index(),price_features,how='left', left_on='wm_yr_wk', right_on='wm_yr_wk').set_index('date')\nfeatures.drop('wm_yr_wk', axis=1, inplace=True)\nfeatures.head()","3c3f2d79":"features_test = features.iloc[1913:1941,:]\nfeatures_train = features.iloc[startDay:1913,:]\ndf_final_more = pd.concat([df.T.reset_index(drop=True), features_train.reset_index(drop=True)], axis = 1)\ndf_final = df_final_more.copy()","c7048b4f":"del df_final_more, features","5fb0fb95":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ndt_scaled = sc.fit_transform(df_final)","369a970c":"gc.collect()","dcb8f2bf":"X_train = []\ny_train = []\nfor i in range(timesteps, 1913 - startDay):\n    X_train.append(dt_scaled[i-timesteps:i])\n    y_train.append(dt_scaled[i][0:42840]) \n    \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nprint('Shape of X_train :'+str(X_train.shape))\nprint('Shape of y_train :'+str(y_train.shape))","67058843":"inputs = df_final[-timesteps:]\ninputs = sc.transform(inputs)","c5a11207":"%who","1e9597be":"del df_train_no_outlier, df_prices, df_days, df, df_final, dt_scaled, price_features","712787e5":"gc.collect()","73ecc0e5":"def tilted_loss(q, y, f):\n    e = (y - f)\n    return keras.backend.mean(keras.backend.maximum(q * e, (q - 1) * e), \n                              axis=-1)","3ff515e9":"QUANTILES = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]","a19ec6ee":"EPOCHS = 32 # going through the dataset 32 times\nBATCH_SIZE = 32 # with each training step the model sees 32 examples","5dba4c8d":"# Importing the Keras libraries and packages\nimport tensorflow_probability as tfp\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nimport tensorflow as tf\nimport keras\n\ndef run_model(X_train, y_train, q):\n\n    model = Sequential()\n\n    # Adding the first LSTM layer and some Dropout regularisation\n    layer_1_units=40\n    model.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n    model.add(Dropout(0.2))\n\n    # Adding a second LSTM layer and some Dropout regularisation\n    layer_2_units=300\n    model.add(LSTM(units = layer_2_units, return_sequences = True))\n    model.add(Dropout(0.2))\n\n    # Adding a third LSTM layer and some Dropout regularisation\n    layer_3_units=300\n    model.add(LSTM(units = layer_3_units))\n    model.add(Dropout(0.2))\n\n    # Adding the output layer\n    model.add(Dense(units = y_train.shape[1]))\n\n    # Compiling the RNN\n    model.compile(optimizer = 'adam',loss=lambda y, f: tilted_loss(q, y, f))\n    \n    # To follow at which quantile we are predicting right now  \n    print('Running the model for Quantil: '+str(q)+':')\n\n    # Fitting the RNN to the Training set\n    fit = model.fit(X_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose=2)\n    \n    X_test = []\n    X_test.append(inputs[0:timesteps])\n    X_test = np.array(X_test)\n    prediction = []\n     \n    for j in range(timesteps,timesteps + 28):\n        predicted_volume = model.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 42875)) #incl. features\n        testInput = np.column_stack((np.array(predicted_volume), np.array(features_test.iloc[j-timesteps,:]).reshape(1,35))) #here no of features is 5\n        X_test = np.append(X_test, testInput).reshape(1,j + 1,42875) #incl. features\n        predicted_volume = sc.inverse_transform(testInput)[:,0:42840] #without features\n        prediction.append(predicted_volume)\n    \n    prediction = pd.DataFrame(data=np.array(prediction).reshape(28,42840)).T\n    return prediction","215b1baf":"# We run the model for all the quantiles mentioned above. \n# Combining all quantile predictions one after another to a large dataset.\npredictions = pd.concat(\n    [run_model(X_train, y_train, q) \n     for q in QUANTILES]) ","2bc9c66f":"gc.collect()","89fe7f39":"predictions.shape","887b0672":"predictions.shape[0]*2","d37ecafc":"predictions.to_pickle('Uncertainty_Predictions.pkl')","2e0be8a0":"submission = pd.concat((predictions, predictions), ignore_index=True)\nidColumn = submission_sample[[\"id\"]]    \nsubmission[[\"id\"]] = idColumn  \n\n#re-arranging collumns\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n#\ncolsname = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\nsubmission.columns = colsname\n\nsubmission.to_csv(\"submission.csv\", index=False)","63dfc8cd":"temp_series = submission[171360:171360+42840]\n\nborder = [1,3,10,3,7,9,21,30,70,3049,9147,30490]\nsumi = 0\nlevels =[]\nfor i in border:\n    sumi += i\n    levels.append(pd.DataFrame(temp_series[sumi-i:sumi]))\n\n\nfor i,level in enumerate(levels):\n    levels[i] = levels[i].sum()","acc413cf":"plt.figure(figsize=(20, 8))\n\nfor i in range(12):\n    plt.plot(levels[i][1:],label='level'+str(i+1))\n\nplt.legend()","f0d0ec78":"fig,ax = plt.subplots(figsize=(20, 8))\n\nfor i in range(6):\n    ax = ax.twinx()\n    ax.plot(levels[i][1:],label='level'+str(i+1))\n    plt.yticks([])","3f10569d":"## Aggregation levels <a class=\"anchor\" id=\"sub_aggregation_levels\"><\/a>","f22386f5":"Finally, let's create a submission file, using the ids of the sample submission. As we have the validation and evaluation data, we need to stack the submission file on top of itself.","9b3e1187":"![grafik.png](attachment:grafik.png)","2486b24d":"Let's take a look at it graphically:","a6f8eb9c":"## Loading data <a class=\"anchor\" id=\"data\"><\/a>","146ac495":"In the M5 we have to project different aggregation levels at certain quantiles. All that changes in comparison to the [baseline lstm](https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7#Future-Improvements) is the loss function. The following few lines defines the loss function defined in the section above.","92ba06c7":"At almost every outlier day, there is a special vent like thanksgiving or christmas.","f97dc3ee":"Now, we need to test, whether the combination of the levels is right and the rows contain the same input. We are going to do this by comparing the row names.","b42f9a44":"# Preparing to start <a class=\"anchor\" id=\"prepare\"><\/a>\n\n## Loading packages <a class=\"anchor\" id=\"packages\"><\/a>","939c5b28":"Let's take a look if this worked","af8930ad":"![grafik.png](attachment:grafik.png)","5f9d005d":"## Looking at the hierachy <a class=\"anchor\" id=\"hierarchy_ts\"><\/a>\n\nIn the competition guideline we can find that the hierarchy consits of 12 levels. Let's try to reconstruct some of them:\n\n1. The top is given by the unit sales of all products, aggregated for all stores\/states. \n2. Unit sales of all products, aggregated for each state.\n3. Unit sales of all products, aggregated for each store.\n4. Unit sales of all products, aggregated for each category.\n5. Unit sales of all products, aggregated for each department.  \n...","8f671099":"Note for the start: *I was a bit shocked that here on kaggle only a few notebooks exist, daring to predict the M5 with uncertainty. Most of the notebooks simply translate the results of the accuracy competition.*\n\nI just recently started writing in Python and tried applying a Neural Network to the problem. I am just a beginner in this field, therefore please be patient :)","30e0ea66":"What if we add prices for the products? Though we only have them on a weekly level, they could increase the model.\n\nEven, if we take prices per product per week, we have to take into account 3049 products * 282 weeks leading to 859818 additional columns.\n\nHowever, if we group by store and category, we receive 10 (stores) * 3 (categories),therefore, only 30 additional columns.","d899018d":"Level 12, 11 and 10 (the most detailled ones) have the lowest total sums. Level12 is approx 2\/3 the sum of level 1","0a149746":"Please feel free to share any ideas for improvement as a comment and we can discuss more in detail","59a82350":"## Creating the Submission File <a class=\"anchor\" id=\"submission\"><\/a>","67133589":"Let's plot them at different levels to see if the curves move similarly.","1f2a42eb":"We can see that our shape matches the requested outcome. Multiplying by two (for the validation and evaluation data).","861b2498":"## Generating Train and Test Data <a class=\"anchor\" id=\"traintest\"><\/a>","7ba11316":"## Outlier <a class=\"anchor\" id=\"outlier\"><\/a>\n\nAt certain days, sales dropped significantly (e.g. christmas).\n\nHere, we take a look at peak days (i.e. peaks in terms of zero sales) on an overall levelm:","4761b224":"## More Features <a class=\"anchor\" id=\"morefeat\"><\/a>\nNext, we want to increase our number of features a little.","2561b5ba":"# Sources and guidelines <a class=\"anchor\" id=\"sources\"><\/a>\n\nAknowledgements\nAs a starting point I mainly used these notebooks:  \n* [baseline LSTM of Accuracy Prediction](https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-8#Future-Improvements)    \n* [Quantile regression, from linear models to trees to deep learning](https:\/\/towardsdatascience.com\/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3)\n* [Deep Quantiel Regression](https:\/\/towardsdatascience.com\/deep-quantile-regression-c85481548b5a)\n* [M5 Uncertainty Notebook by Allunia](https:\/\/www.kaggle.com\/allunia\/m5-uncertainty)\n\nMy other M5 notebook for Accuracy can be seen here: (will be uploaded soon)","e79dd52a":"As seen in the table above, we are going to create the 12 Levels one after another through grouping statements ","8e739193":"## Prediction intervals and quantiles <a class=\"anchor\" id=\"PIs\"><\/a>\n\nGiven that forecasters will be asked to provide the median, and the 50%, 67%, 95%, and 99% PIs, u is set to u1=0.005, u2=0.025, u3=0.165, u4=0.25, u5=0.5, u6=0.75, u7=0.835, u8=0.975, and u9=0.995, therefore leading to the following quartiles:\n\n* 99% PI - $u_{1} = 0.005$ and $u_{9} = 0.995$\n* 95% PI - $u_{2} = 0.025$ and $u_{8} = 0.975$\n* 67% PI - $u_{3} = 0.165$ and $u_{7} = 0.835$\n* 50% PI - $u_{4} = 0.25$ and $u_{6} = 0.75$\n* median - $u_{5} = 0.5$","d0f089ba":"In the next step, let's create X_train and y_train by creating different dataframes with 14 days of projection. For y_train we only use sales values for predictions. As we only predict sales, only 0:42840 columns are choosen.","f8e8444b":"* In the first submission row we are asked to make precitions for the top level 1 (unit sales of all products, aggregated for all stores\/states)\n* The next 3 rows represent level 2.\n* Followed by level 3 and so on\n* Some rows contain aggregations at different levels. An X indicates the absence of an second aggregration level.\n* The prediction interval can be validation (related to the public leaderboard) or evaluation (related to the private leaderboard).\n\nAn overview of the different levels is given in the Competitors Guide as follows:","4785fb3b":"Every combination is fine and in the right order.","3ebedba0":"Let's take a look at one of the predicted datasets (here we take the median with quantile = 0.5)\n\nWe want to test, whether the sum of our daily predictions on the different levels equal to level1 (Total levels).","690b22c9":"# M5 Competition - Uncertainty - LSTM Neural Network\n\n1. [Introduction and sources](#sources)\n\n\n2. [Preparing to start](#prepare)\n    * [Loading packages](#packages)\n    * [Loading data](#data)\n    * [Looking at the hierachy](#hierarchy_ts)\n    \n    \n3. [The submission format](#submission)\n    * [Intro](#intro)\n    * [Prediction intervals and quartiles](#PIs)\n    * [Outlier](#outlier)\n    * [Aggregation levels](#sub_aggregation_levels)\n    \n    \n4. [Model Preparation](#Feature_Creation)\n    * [Limited Features](#limfeat)\n    * [More Features](#morefeat)\n    * [Pricing Feature](#pricefeat)\n    * [Feature Scaling](#featscale)\n    * [Train and Test Data Creation](#traintest)\n    \n    \n5. [LSTM Modeling](#Modeling)\n    * [Loss Function](#lossfct)\n    * [Running the Model](#runmodel)\n    * [Creating the submission file](#submission)\n\nThe M5 competition ran from 2 March to 30 June 2020. Basis of the competition is to predict sales forecasts for walm,art stores. For that, we use hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories, stores in three geographical areas of the US: California, Texas, and Wisconsin.\n\nEach row contains an id that is a concatenation of an item_id and a store_id, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard). \n\nWe are predicting 28 forecast days (F1-F28) of items sold for each row. For the **validation rows**, this corresponds to d_1914 - d_1941, and for the **evaluation rows**, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)\n\nDetailed Information can be found [at the university website or](https:\/\/mofc.unic.ac.cy\/m5-competition\/) and [the competition website on kaggle](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/data).\n\nAn overview of the data given, can be seen here:\n\nData exists in three files:\n1. File 1: \u201ccalendar.csv\u201d: Contains information about the dates the products are sold.\n    * date: The date in a \u201cy-m-d\u201d format.\n    * wm_yr_wk: The id of the week the date belongs to.\n    * weekday: The type of the day (Saturday, Sunday, \u2026, Friday).\n    * wday: The id of the weekday, starting from Saturday.\n    * month: The month of the date.\n    * year: The year of the date.\n    * event_name_1: If the date includes an event, the name of this event.\n    * event_type_1: If the date includes an event, the type of this event.\n    * event_name_2: If the date includes a second event, the name of this event.\n    * event_type_2: If the date includes a second event, the type of this event.\n    * snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP  purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n\n\n2. File 2: \u201csell_prices.csv\u201d: Contains information about the price of the products sold per store and date.\n    * store_id: The id of the store where the product is sold. \n    * item_id: The id of the product.\n    * wm_yr_wk: The id of the week.\n    * sell_price: The price of the product for the given week\/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).  \n\n\n3. File 3: \u201csales_train.csv\u201d: Contains the historical daily unit sales data per product and store.\n    * item_id: The id of the product.\n    * dept_id: The id of the department the product belongs to.\n    * cat_id: The id of the category the product belongs to.\n    * store_id: The id of the store where the product is sold.\n    * state_id: The State where the store is located.\n    * d_1, d_2, \u2026, d_i, \u2026 d_1941: The number of units sold at day i, starting from 2011-01-29. \n","e7dedfaa":"![grafik.png](attachment:grafik.png)","096dc833":"### Pricing Feature <a class=\"anchor\" id=\"pricefeat\"><\/a>","ce07636a":"At least, they all have similar ups and downs!","dd2c5678":"When creating X_test, we are using the last 14 days in order to predict day 1915 sales. Therefore, in order to predict 1916th day, 13 days from our input data and 1 day from our prediction are used. After that we slide the window one by one, i.e.:\n\n* 12 days from input data + 2 days from our prediction to predict 1917th day\n* 11 days from input data + 3 days from our prediction to predict 1918th day\n* .....\n* 14 days our prediction to predict last 1941th day sales.\n\n","a2c00152":"### Insights\n\n* It has become much clearer how these levels are aggregated by performing groupby- and summing up the sales.\n* We can already observe nice periodic patterns. ","d0ca4fae":"![grafik.png](attachment:grafik.png)","78491d87":"# Feature Creation  <a class=\"anchor\" id=\"Feature_Creation\"><\/a>\n\nIn the next part we have to decide how many features we want to take for test and training datasets. ","00177c6e":"| Level id|\tAggregation Level|\tNumber of series|\n|:----|:----|:----|\n|1|Unit sales of all products, aggregated for all stores\/states|\t1|\n|2|Unit sales of all products, aggregated for each State|\t3|\n|3|Unit sales of all products, aggregated for each store| \t10|\n|4|Unit sales of all products, aggregated for each category|\t3|\n|5|Unit sales of all products, aggregated for each department|\t7|\n|6|Unit sales of all products, aggregated for each State and category|\t9|\n|7|Unit sales of all products, aggregated for each State and department|\t21|\n|8|Unit sales of all products, aggregated for each store and category|\t30|\n|9|Unit sales of all products, aggregated for each store and department|\t70|\n|10|Unit sales of product x, aggregated for all stores\/states|\t3,049|\n|11|Unit sales of product x, aggregated for each State|\t9,147|\n|12|Unit sales of product x, aggregated for each store|\t30,490|\n| |**Total**|**42,840**|","40d01221":"## Running the Model <a class=\"anchor\" id=\"runmodel\"><\/a>","361d7ed5":"## Limited Features <a class=\"anchor\" id=\"limfeat\"><\/a>\n\nIn the first part we are taking only one extra feature (i.e. limited features).","8934d675":"# LSTM Modeling <a class=\"anchor\" id=\"Modeling\"><\/a>\n\nNext, we start our modelling. We will use LSTM Neural Networks with different layers. \n\nIn general, neural networks are easily described by the following picture:\n1. The neural network model starts with random weights and tries to find the best weights for the different layers, predicting outcomes and comparing them with the true target outcomes. For this it uses the loss function. \n2. The loss function measures the quality of  the network\u2019s output\n3. Then, the loss score is used as a feedback signal to adjust the weights.","68dd4001":"# The submission format <a class=\"anchor\" id=\"submission\"><\/a>\n\n## Intro <a class=\"anchor\" id=\"intro\"><\/a>\n\n* We have 28 F-columns as we are predicting daily sales for the next 28 days. \n* We are asked to make uncertainty estimates for these days.","4270f4b1":"## Feature Scaling <a class=\"anchor\" id=\"featscale\"><\/a>\nFor better modeling, we are scaling features using min-max scaler in range 0-1.","c7a5eff1":"## Loss Function <a class=\"anchor\" id=\"lossfct\"><\/a>"}}