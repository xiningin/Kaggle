{"cell_type":{"bb7f5ef9":"code","524f5029":"code","60d89173":"code","51b4057a":"code","2690d70e":"code","fcb9c111":"code","b0840bc9":"code","1d466cfd":"code","fb71ceb1":"code","b1cd124c":"code","3b077eae":"code","0f610b3d":"code","951ebb62":"code","6a8f7e2c":"code","c6885350":"code","4f5bfbec":"code","ecfa6a4a":"code","71ad8e51":"code","74ea1c84":"markdown","e2e63885":"markdown","8ce4f646":"markdown","ae9a9f53":"markdown","9312f029":"markdown","90c5288d":"markdown"},"source":{"bb7f5ef9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')","524f5029":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","60d89173":"print(f\"train data: Rows={train.shape[0]}, Columns={train.shape[1]}\")\nprint(f\"test data : Rows={test.shape[0]}, Columns={test.shape[1]}\")","51b4057a":"train.head()","2690d70e":"print(train.isnull().sum())","fcb9c111":"print(test.isnull().sum())","b0840bc9":"cfg = {\n    'TARGET' : 'target',\n    'N_FOLDS' : 5,\n    'RANDOM_STATE': 529,\n    'N_ESTIMATORS' : 50_000,\n    'LEARNING_RATE': 0.1\n}\n\ntrain_passes = train['PassengerId'].unique()","1d466cfd":"kf = KFold(n_splits=cfg['N_FOLDS'],\n           shuffle=True,\n           random_state=cfg['RANDOM_STATE'])\n\n# Create Folds\nfold = 1\nfor tr_idx, val_idx in kf.split(train_passes):\n    fold_passes = train_passes[val_idx]\n    train.loc[train['PassengerId'].isin(fold_passes), 'fold'] = fold\n    fold += 1\ntrain['fold'] = train['fold'].astype('int')","fb71ceb1":"train['fold'].value_counts()","b1cd124c":"def prepare_features(df, train=True):\n    #df.dropna(subset=['Age'])\n    df['Pclass'] = df['Pclass'].astype('category')\n    df['SibSp'] = df['SibSp'].astype('category')\n    df['Age'] = df['Age'].fillna(df['Age'].mean())\n    \n    df = pd.get_dummies(df, columns=[\"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"])\n    \n    return df","3b077eae":"train['isTrain'] = True\ntest['isTrain'] = False\nalldata = pd.concat([train, test]).reset_index(drop=True).copy()\nalldata = alldata.drop('Name', axis=1)\n\nalldata = prepare_features(alldata)","0f610b3d":"train_feats = alldata.query('isTrain').reset_index(drop=True).copy()\ntest_feats = alldata.query('isTrain == False').reset_index(drop=True).copy()","951ebb62":"train_feats.head()","6a8f7e2c":"X_test = test_feats.drop('Survived', axis=1)\n\noof = train_feats[['PassengerId','Survived','fold']].reset_index(drop=True).copy()\nsubmission_df = test[['PassengerId']].copy()\n\nFEATURES = X_test.columns.values\nTARGET = ['Survived']","c6885350":"regs = []\nfis = []\n# Example Fold 1\nfor fold in range(1, 6):\n    print(f'===== Running for fold {fold} =====')\n    # Split train \/ val\n    X_tr = train_feats.query('fold != @fold')[FEATURES]\n    y_tr = train_feats.query('fold != @fold')[TARGET]\n    X_val = train_feats.query('fold == @fold')[FEATURES]\n    y_val = train_feats.query('fold == @fold')[TARGET]\n    print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape)\n\n    # Create our model\n    reg = lgb.LGBMClassifier(n_estimators=cfg['N_ESTIMATORS'],\n                            learning_rate=cfg['LEARNING_RATE'],\n                            objective='binary',\n                            metric=['binary_logloss'],\n                            importance_type='gain'\n                            #importance_type='split'\n                           )\n    # Fit our model\n    reg.fit(X_tr, y_tr,\n            eval_set=(X_val, y_val),\n            early_stopping_rounds=500,\n            verbose=200,\n           )\n\n    # Predicting on validation set\n    fold_preds = reg.predict(X_val,\n                             num_iteration=reg.best_iteration_)\n    oof.loc[oof['fold'] == fold, 'preds'] = fold_preds\n    # Score validation set\n    fold_score = mean_absolute_error(\n        oof.query('fold == 1')['Survived'],\n            oof.query('fold == 1')['preds']\n    )\n\n    # Creating a feature importance dataframe\n    fi = pd.DataFrame(index=reg.feature_name_,\n                 data=reg.feature_importances_,\n                 columns=[f'{fold}_importance'])\n\n    # Predicting on test\n    fold_test_pred = reg.predict(X_test,\n                num_iteration=reg.best_iteration_)\n    submission_df[f'pred_{fold}'] = fold_test_pred.astype(int)\n    print(f'Score of this fold is {fold_score:0.6f}')\n    regs.append(reg)\n    fis.append(fi)","4f5bfbec":"oof_score = mean_absolute_error(oof['Survived'], oof['preds'])\nprint(f'Out of fold score {oof_score:0.6f}')","ecfa6a4a":"submission_df.set_index('PassengerId')\nsubmission_df","71ad8e51":"pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\nsubmission_df['Survived'] = submission_df[pred_cols].mode(axis=1)\n\nsubmission_df[['PassengerId','Survived']].to_csv('submission.csv', index=False)","74ea1c84":"# Simple LGBM Classification and K-Fold model\n\nI create this note for personal study.\nThanks to all the kagglers who shared their helpful wisdom and information.","e2e63885":"# Checking the data","8ce4f646":"# Preparing Features","ae9a9f53":"# LGBM Classification","9312f029":"# Make Submission","90c5288d":"# Create Fold"}}