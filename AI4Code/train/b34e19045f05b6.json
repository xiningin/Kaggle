{"cell_type":{"29d3cf31":"code","dc1a62b8":"code","795e25e3":"code","34b6210c":"code","1aaae170":"code","9fd21131":"code","34deb70b":"code","ea7c5598":"code","1f1f09f9":"code","c0760cac":"code","e311502c":"code","b0ea1aea":"code","a5d01883":"code","4ef8d890":"code","e85ad2b3":"code","ad05ae7a":"code","919d7603":"code","ac455d11":"code","b7b0cfca":"code","6b3c2bf5":"code","097b6f2f":"code","0ee740bf":"code","0adc30b2":"code","c27e7278":"code","a42e3f73":"code","6ed7bdd3":"code","b83c5ef1":"code","f705fde6":"code","5cdaf6c4":"code","19987a47":"code","c35c10a1":"code","5e1fa966":"code","2e0e505e":"code","c968d439":"code","27de26c4":"code","cb197e1c":"code","abd2c1a1":"code","10448a35":"code","278e8767":"code","382af38a":"markdown","49781f2a":"markdown"},"source":{"29d3cf31":"import numpy as np\nimport os\nfrom PIL import Image\nimport h5py    \nimport pandas as pd\nimport io\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt","dc1a62b8":"hdf5_fpath=\"..\/input\/flowershd5dataset\/flowers-hd5\/data\/flowers\/flowers.hdf5\"\nf = h5py.File(hdf5_fpath)","795e25e3":"# list(f['train'])","34b6210c":"(Image.open(io.BytesIO(bytes(np.array(f['train']['image_00009_3']['img'])))).resize((256,256)))","1aaae170":"class Train_dataset:\n    def __init__(self):\n        \n        if os.path.exists(\"..\/input\/t2i-synthesis-with-generative-adversarial\/data.npy\"):\n            data=np.load(\"..\/input\/t2i-synthesis-with-generative-adversarial\/data.npy\",allow_pickle=True).item()\n            self.data=data['data']\n            self.word_to_index=data[\"word_to_index\"]\n            self.max_sequence_length=data[\"max_sequence_length\"]\n            \n        else:\n            self.word_to_index={}\n            self.data=[]\n            self.max_sequence_length=0\n            \n            unique=[]\n            for img_name in tqdm(f['train']):\n                \n                image=np.array(Image.open(io.BytesIO(bytes(np.array(f['train'][img_name]['img'])))).resize((256,256)))\n                \n                text=np.array(f['train'][img_name]['txt']).item().strip()\n#                 print(text)\n                if text not in unique:\n#                     print(type(text) ,'\\t\\t')\n                    unique.append(text)\n#                     unique[text]=img_name\n                    words=text.split()\n                    self.max_sequence_length=max(self.max_sequence_length,len(words))\n                    for i,word in enumerate(words):\n                        if word not in self.word_to_index:\n                            self.word_to_index[word]=len(self.word_to_index)+1\n                        words[i]=self.word_to_index[word]\n                    self.data.append({\"image\":image,\"text\":words})\n                        \n        np.save('data.npy',{'data':self.data,\"word_to_index\":self.word_to_index,\"max_sequence_length\":self.max_sequence_length})\n#         self.data=np.random.choice(self.data,50000)\n        self.transforms=transforms.ToTensor()\n    def __len__(self):\n#         return 1000\n        return len(self.data)\n    def __getitem__(self,idx):\n        data=self.data[idx].copy()\n        image=data[\"image\"]\n        image=self.transforms(Image.fromarray(image))\n        right_text=data[\"text\"]\n        right_text=np.array([0]*(self.max_sequence_length-len(right_text))+right_text)\n        \n        wrong_idx=np.random.choice([(i) for i in range(len(self.data)) if i != idx])\n        wrong_data=self.data[wrong_idx].copy()\n        wrong_text=wrong_data.copy()['text']\n        wrong_text=np.array([0]*(self.max_sequence_length-len(wrong_text))+wrong_text)\n        wrong_img=wrong_data[\"image\"]\n        wrong_img=self.transforms(Image.fromarray(wrong_img))\n        return (image,wrong_img,right_text,wrong_text)\n        \n        ","9fd21131":"%%time\nd_set=Train_dataset()","34deb70b":"len(d_set)","ea7c5598":"data_loader=torch.utils.data.DataLoader(d_set, batch_size=16)","1f1f09f9":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(356, 64*8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(64*8),\n            nn.ReLU(),\n            nn.Conv2d(64*8, 64*8, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64*8),\n            nn.ReLU())\n\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(64*8, 64*8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64*8),\n            nn.ReLU(),\n            nn.Conv2d(64*8, 64*8, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64*8),\n            nn.ReLU())\n\n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(64*8, 64*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64*4),\n            nn.ReLU(),\n            nn.Conv2d(64*4, 64*4, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64*4),\n            nn.ReLU())\n\n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(64*4, 64*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64*4),\n            nn.ReLU(),\n            nn.Conv2d(64*4, 64*4, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64*4),\n            nn.ReLU())\n\n        self.layer5 = nn.Sequential(\n            nn.ConvTranspose2d(64*4, 64*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64*2),\n            nn.ReLU(),\n            nn.Conv2d(64*2, 64*2, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64*2),\n            nn.ReLU())\n\n        self.layer6 = nn.Sequential(\n            nn.ConvTranspose2d(64*2, 64*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64*2),\n            nn.ReLU(),\n            nn.Conv2d(64*2, 64*2, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64*2),\n            nn.ReLU())\n\n        self.layer7= nn.Sequential(\n            torch.nn.ConvTranspose2d(64*2, 3, 4, 2, 1, bias=False),\n            torch.nn.Tanh())\n\n    def forward(self,noise,encoded_text):\n        x = torch.cat([noise,encoded_text],dim=1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = self.layer7(x)\n        return x","c0760cac":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=(7,7), padding=(3,3)),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2))\n\n        self.layer2 =  nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2))\n\n        self.layer3 =  nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=(3,3),padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=(3,3), padding=(1,1)),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=(3,3), padding=(1,1)),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2))\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=(1,1)),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n\n        self.layer6 = nn.Sequential(\n            nn.Flatten(start_dim=1),\n            nn.Linear(512, 256),\n            nn.Linear(256, 2))\n\n    def forward(self, x, encoded_text):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = torch.cat([x, encoded_text], dim=1)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        return x","e311502c":"# class Text_encoder(nn.Module):\n#     def __init__(self,num_word,embeding_size,hidden_size):\n#         super().__init__()\n#         self.hidden_size=hidden_size\n#         self.embedding=nn.Embedding(num_word+1,embeding_size,padding_idx=0)  #+1 for padding idx\n#         self.rnn=nn.RNN(embeding_size,hidden_size,batch_first=True)\n#     def forward(self,text):\n#         text=self.embedding(text)\n#         text,hidden=self.rnn(text)\n#         hidden=hidden.view(-1,self.hidden_size,1,1)\n        \n#         return hidden","b0ea1aea":"class TextEncoder(nn.Module):\n    def __init__(self, voc_size, embed_size=128, hidden_size=256, n_layers=1, bidirectional=True, use_attention=True):\n        super().__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.use_attention = use_attention\n        self.embedding = nn.Embedding(voc_size, embed_size, padding_idx=0)\n        self.gru = nn.GRU(embed_size, hidden_size, n_layers, bidirectional=True, batch_first=True)\n        if self.use_attention:\n            self.W1 = nn.Linear(hidden_size, hidden_size)\n            self.W2 = nn.Linear(hidden_size, hidden_size)\n            self.V = nn.Linear(hidden_size, 1)\n\n    def attention(self, enc_outputs, hidden):\n        # score: batch_size, max_length, hidden_dim\n        score = torch.tanh(self.W1(enc_outputs)+self.W2(hidden))\n        # attention_weights: batch_size, max_length, 1\n        # we get 1 at the last axis because we are applying score to self.V\n        attn_weights = torch.softmax(self.V(score), dim=1)\n        # context_vector shape after sum == (batch_size, hidden_dim)\n        context_vector = torch.sum(attn_weights*enc_outputs, dim=1)\n        return context_vector\n\n    def forward(self,input_seqs, hidden=None):\n        input_lens = input_seqs.ne(0).sum(dim=1).cpu()\n        input_lens, sort_idx = input_lens.sort(dim=0, descending=True)\n        input_seqs = input_seqs[sort_idx]\n        embedded = self.embedding(input_seqs)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lens,  batch_first=True)\n        outputs, hidden = self.gru(packed,hidden)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        _, unsort_idx = sort_idx.sort(dim=0)\n        outputs = outputs[unsort_idx].contiguous()\n        outputs = outputs[..., :self.hidden_size] + outputs[..., self.hidden_size:]\n        hidden = hidden.sum(dim=0).unsqueeze(dim=1)\n        context_vector = self.attention(outputs, hidden) if self.use_attention else hidden\n        return context_vector.view(-1, self.hidden_size, 1, 1)","a5d01883":"device = 'cuda'\nif os.path.exists(\"..\/input\/text-to-image-gpu\/checkpoint.pth\"):\n    check_point=torch.load(\"..\/input\/text-to-image-gpu\/checkpoint.pth\",map_location=device)\nelse:\n    check_point={} \n","4ef8d890":"check_point['train_folds']","e85ad2b3":"check_point['train_data'][0]","ad05ae7a":"check_point['models'][0].keys()","919d7603":"gen=Generator().to(device)\n# gen=Generator()\ngen.load_state_dict(check_point['models'][0].get('generator',gen.state_dict()))\ndisc=Discriminator().to(device)\n# disc=Dicriminator()\ndisc.load_state_dict(check_point['models'][0].get(\"discriminator\",disc.state_dict()))\ntext_encod=TextEncoder(len(d_set.word_to_index)+1,128,256).to(device)\n# text_encod=TextEncoder(len(d_set.word_to_index),128,256)\ntext_encod.load_state_dict(check_point['models'][0].get(\"text_encoder\",text_encod.state_dict()))","ac455d11":"optimizer_G = torch.optim.Adam(gen.parameters(), lr = 0.0001)\noptimizer_G.load_state_dict(check_point['optimizers'][0]['generator'].get(\"optimizer_G\",optimizer_G.state_dict()))\noptimizer_D = torch.optim.Adam(disc.parameters(), lr = 0.0001)\noptimizer_D.load_state_dict(check_point['optimizers'][0]['discriminator'].get(\"optimizer_D\",optimizer_D.state_dict()))\noptimizer_T_E = torch.optim.Adam(text_encod.parameters(), lr = 0.0001)\noptimizer_T_E.load_state_dict(check_point['optimizers'][0]['text_encoder'].get(\"optimizer_T_E\",optimizer_T_E.state_dict()))\ncriterion = nn.CrossEntropyLoss()","b7b0cfca":"scheduler_G=torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=[lambda epoch:1e-4])\nscheduler_D=torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=[lambda epoch:1e-4])\nscheduler_T_E=torch.optim.lr_scheduler.LambdaLR(optimizer_T_E, lr_lambda=[lambda epoch:1e-4])","6b3c2bf5":"def Train(epoch):\n    total_loss_G=0\n    total_loss_D=0\n    \n#     total_loss_T_E=0\n    for right_img,wrong_img,right_text,wrong_text in data_loader:\n        torch.cuda.empty_cache()\n        right_img=right_img.to(device)\n        wrong_img=wrong_img.to(device)\n        right_text=right_text.to(device)\n        wrong_text=wrong_text.to(device)\n\n        \n        Enc_right_text=text_encod(right_text )\n        Enc_wrong_text=text_encod(wrong_text)\n        \n        ones=torch.ones((right_img.shape[0]),dtype=torch.long,device = device)\n        zeros=torch.zeros((right_img.shape[0]),dtype=torch.long,device = device)\n        noise=torch.randn((len(right_img),100,1,1),device = device)\n        \n        fake_img=gen(noise,Enc_right_text.detach())\n        \n#         D_real=disc(img,Enc_right_text)\n#         D_wrong=disc(img,Enc_wrong_text)\n#         D_wrong_img=disc(wrong_img,Enc_right_text)\n        \n#         loss_D_real=criterion(D_real,ones)\n#         loss_D_wrong=criterion(D_wrong,zeros)\n#         loss_D_wrong_img=criterion(D_wrong_img,zeros)\n        \n#         loss_T_E = loss_D_real + (loss_D_wrong+loss_D_wrong_img)\/2\n#         optimizer_T_E.zero_grad()\n#         loss_T_E.backward()\n#         optimizer_T_E.step()\n        \n#         print(fake_image.shape)\n\n        D_right=disc(right_img,Enc_right_text)\n        D_wrong_text=disc(right_img,Enc_wrong_text)\n        D_wrong_img=disc(wrong_img,Enc_right_text)\n        D_fake=disc(fake_img.detach(),Enc_right_text.detach())\n        \n        \n        loss_D_right = criterion(D_right,ones)\n        loss_D_wrong_text = criterion(D_wrong_text,zeros)\n        loss_D_fake = criterion(D_fake,zeros)\n        loss_D_wrong_img = criterion(D_wrong_image,zeros)\n        \n        loss_D = loss_D_right + 0.25*loss_D_wrong_text + 0.25*loss_D_wrong_img + 0.5*loss_D_fake\n        \n        optimizer_D.zero_grad()\n        optimizer_T_E.zero_grad()\n        loss_D.backward()\n        optimizer_D.step()\n        optimizer_T_E.step()\n        \n        D_fake = disc(fake_image,Enc_right_text.detach())\n        \n        loss_G = criterion(D_fake,ones)\n        optimizer_G.zero_grad()\n        loss_G.backward()\n        optimizer_G.step()\n        \n#         total_loss_T_E += loss_T_E.item()\n        total_loss_G+=loss_G.item()\n        total_loss_D+=loss_D.item()\n        \n#         print(f'loss_D :{loss_D}, loss_G:{loss_G}')\n        \n        del right_img,\n        del right_text,\n        del wrong_text,\n        del Enc_right_text, \n        del Enc_wrong_text,\n        del noise,\n        del fake_img,\n        del D_right,\n        del D_wrong_img,\n        del D_wrong_text,\n        del D_fake,\n        del ones,\n        del zeros,\n        del loss_D_real,\n        del loss_D_wrong_img,\n        del loss_D_wrong_text,\n        del loss_D_fake,\n        del loss_D,\n        del loss_G\n        \n        torch.cuda.empty_cache()\n    scheduler_G.step()\n    scheduler_D.step()\n#     scheduler_T_E.step()\n     \n#     avg_total_loss_T_E = total_loss_T_E\/dataloader_len\n    avg_loss_G=total_loss_G\/dataloader_len\n    avg_loss_D=total_loss_D\/dataloader_len\n    print(f'epoch :{epoch},loss_D :{avg_loss_D},loss_G :{avg_loss_G}')\n    return avg_loss_D,avg_loss_G\n","097b6f2f":"for i in range(1):\n    loss_D, loss_G=Train(i+1)\n    loss_d_hist.append(loss_D)\n    loss_g_hist.append(loss_G)\n#     loss_T_E_hist.append(loss_T_E)\n    check_point={\n        \"text_encoder\":text_encod.state_dict() ,\n        \"discriminator\" :disc.state_dict() ,\n        \"generator\":gen.state_dict() ,\n        \"optimizer_D\":optimizer_D.state_dict() ,\n        \"optimizer_G\":optimizer_G.state_dict() ,\n        \"optimizer_T_E\":optimizer_T_E.state_dict(),\n        \"train_data\": train_data,\n        \"loss_d_hist\":loss_d_hist,\n        \"loss_g_hist\":loss_g_hist,\n        \"loss_T_E_hist\":loss_T_E_hist,\n        \"epoch_data\": epoch_data\n    }\n    torch.save(check_point,\"check_point.pth\")","0ee740bf":"train_data = check_point.get('train_data', {})\nepoch_data = check_point.get('epoch_data', [])\nfig, axs = plt.subplots(1, 1, figsize=(10, 5))\nax = axs\nax.clear();\nax.set_title('Trining loss over epoch')\nax.set_xlabel(\"Epoch\")\nax.set_ylabel(\"Loss\")\n\nax.plot(epoch_data, train_data[0][:, 0], label = f\" discriminator loss {train_data[0][-1][0]:.4f}\")\nax.plot(epoch_data, train_data[0][:, 2], label = f\" generetor loss {train_data[0][-1][2]:.4f}\")\nax.legend()\nplt.show()","0adc30b2":"check_point['models'][0].keys()","c27e7278":"def visualize(image, text, word_to_index, ax = None, title = None):\n    if ax is None: _, ax = plt.subplots(1, 1, figsize = (10, 10))\n    if title: ax.set_title(title)\n    index_to_word = dict(map(reversed, word_to_index.items()))\n    ax.imshow(image.permute(1, 2, 0))\n    ax.set_xlabel(\"|\".join([index_to_word[word.item()] for word in text if word.item() in index_to_word]))","a42e3f73":"idx = np.random.randint(len(d_set))\n\nimage, _, right_text, _ = d_set[idx]\nright_text = torch.tensor(right_text, device=device)\n# print(right_text[None].shape)\nenc_text = text_encod(right_text[None])\n# enc_text = check_point['models'][\"text_encoder\"](right_text[None])\n# print(enc_text.shape)\nnoise = torch.randn((1, 100, 1, 1), device=device)\n\ntext_encod.eval()\ngen.eval()\n\n\ngen_image = gen(noise, enc_text).detach().squeeze().cpu()\n# gen_image = models[fold][\"generator\"](noise, enc_text).detach().squeeze().cpu()\n# print(gen_image.shape)\n\n_, axs = plt.subplots(1, 2, figsize=(10*2, 10))\n\nvisualize(image, right_text, d_set.word_to_index, axs[0], \"Right\")\nvisualize(gen_image, right_text, d_set.word_to_index, axs[1], \"Generated\")\nplt.show()","6ed7bdd3":"models = {}\nfor fold in tqdm(range(0)):\n    models[fold] = {\n        \"text_encoder\": TextEncoder(len(d_set.word_to_index)+1, use_attention=True).to(device),\n        \"generator\": Generator().to(device),\n        \"discriminator\": Discriminator().to(device)\n    }\n# for key in models[0]:\n#     models[0][key].eval()","b83c5ef1":"loss_d_hist=check_point.get(\"loss_d_hist\",[])\nloss_g_hist=check_point.get(\"loss_g_hist\",[])\nloss_T_E_hist = check_point.get(\"loss_T_E_hist\",[])\n# del check_point\ntorch.cuda.empty_cache()\n\nfig,axs=plt.subplots(1,3,figsize=(15,5))\naxs[0].plot(range(len(loss_d_hist)),loss_d_hist,label=\"dis_loss\")\naxs[1].plot(range(len(loss_g_hist)),loss_g_hist,label=\"gen_loss\")\naxs[2].plot(range(len(loss_T_E_hist)),loss_T_E_hist,label=\"T_E_loss\")\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nplt.show()","f705fde6":"image","5cdaf6c4":"gen_image","19987a47":"fig,axs=plt.subplots(1,1,figsize=(10,5))\naxs.imshow(gen_image.permute(1,2,0).detach().cpu())","c35c10a1":"img,_,right_text,_ = d_set[1890]\n\nprint(right_text.shape)\nright_text=torch.tensor(right_text,device=device)[None]\nprint(right_text.shape)\n\ntext_encod.eval()\ngen.eval()\n\nEnc_right_text=text_encod(right_text)\nprint(Enc_right_text.shape)\nnoise=torch.randn((1,100,1,1),device = device)\ngen_img=gen(noise,Enc_right_text)\n\nfig,axs=plt.subplots(1,2,figsize=(10,5))\naxs[0].imshow(gen_img[0].permute(1,2,0).detach().cpu())\naxs[1].imshow(img.permute(1,2,0).detach().cpu())\n\n","5e1fa966":"# img=img[Nun].to(device)\n# right_text.shape\n# right_text=torch.tensor(right_text,device=device)[None]\n# right_text.shape\n# Enc_right_text=text_encod(right_text)\n\n# noise=torch.randn((1,100,1,1),device = device)\n# gen_img=gen(noise,Enc_right_text)\n# transforms.ToPILImage()(gen_img.squeeze())\n","2e0e505e":"transforms.ToPILImage()(img)","c968d439":"# gen.state_dict()","27de26c4":"# for img,right_text,_ in d_set:\n#     text1=(wrong_text).to(device)\n#     break","cb197e1c":"# image=gen(torch.randn(32,100,1,1).to(device),text_encod(text1))","abd2c1a1":"# transforms.ToPILImage()(image[26].squeeze())","10448a35":"# Total_data=np.load(\"..\/input\/t2i-synthesis-with-generative-adversarial\/data.npy\",allow_pickle=True).item()","278e8767":"# test_sentence_numeric=[]\n# test_sentence=input().split()\n\n# for word in test_sentence:\n#     if word in Total_data['word_to_index']:\n#         test_sentence_numeric.append(Total_data['word_to_index'][word])\n# print(test_sentence_numeric)\n# test_sentence_numeric=([0]*(Total_data['max_sequence_length']-len(test_sentence_numeric)))+test_sentence_numeric\n# test_sentence_numeric=torch.tensor(test_sentence_numeric,device=device)[None]\n# print(test_sentence_numeric.shape)\n\n# text_encod.eval()\n# gen.eval()\n\n# Enc_test_sentence_numeric=text_encod(test_sentence_numeric)\n# print(Enc_test_sentence_numeric.shape)\n\n# noise=torch.randn((1,100,1,1),device = device)\n# generated_img=gen(noise,Enc_test_sentence_numeric)\n# transforms.ToPILImage()(gen_img.squeeze())","382af38a":" # hand written sentence to generatation","49781f2a":"**Please note! This kernel is for practice purposes only. The references used are mentioned below.**\n\n**Reference links:**\n- Paper - \n- Git repo - \n- "}}