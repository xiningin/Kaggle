{"cell_type":{"65d312bf":"code","396949c3":"code","4b02097c":"code","98031d80":"code","9d94a721":"code","ecd2ca2c":"code","8fefc2a6":"code","371e3ae2":"code","d01d24d0":"code","b4e85ddb":"code","e254db24":"code","171529f2":"code","8cb62f1d":"code","5d407ad3":"code","7c89fa7f":"code","2c2942dd":"code","ca61fe1a":"code","2b06ca69":"code","b3067cfb":"code","0a4d1bc2":"code","89302942":"code","f328c347":"code","115a87c9":"code","60fd8dbd":"code","877357e9":"code","41938df7":"code","59d1bc0d":"code","533283b6":"code","1f3bb337":"code","ee9441f7":"code","8a7df374":"code","12483652":"markdown","c5de815d":"markdown","d3951317":"markdown","5d8edbc4":"markdown","a2e7838b":"markdown","4ead36c5":"markdown","c4a2040b":"markdown","00ed688e":"markdown","b5c3e89c":"markdown","891c7506":"markdown","e74f9fa9":"markdown","5d26355f":"markdown","e7ce4b9e":"markdown","5ccce33e":"markdown","2820403c":"markdown","0cc3a918":"markdown","766f1fb8":"markdown","724863d1":"markdown","c990c86b":"markdown"},"source":{"65d312bf":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.model_selection import train_test_split\n#from sklearn.cross_validation import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nimport os\nprint(os.listdir(\"..\/input\"))\nnp.random.seed(42)","396949c3":"def get_samples_bagging(data,target,size_col):\n    features = np.random.choice(data.columns, size_col, replace=True)\n    data = data[features]\n    indices = list(set(np.random.randint(data.shape[0], size=data.shape[0])))\n    data = data.iloc[indices,:]\n    targ = target.iloc[indices,:]\n    return data, targ,features","4b02097c":"train = pd.read_csv(\"..\/input\/train.csv\",index_col='PassengerId')\ntest = pd.read_csv(\"..\/input\/test.csv\",index_col='PassengerId')#","98031d80":"train.shape,test.shape","9d94a721":"train.head()","ecd2ca2c":"def replaceGen(sex):\n    gen =0\n    if sex=='male':\n        gen=0\n    elif sex=='female':\n        gen=1\n    return gen\n    ","8fefc2a6":"train['Sex'] = train['Sex'].apply(replaceGen)\ntest['Sex'] = test['Sex'].apply(replaceGen)","371e3ae2":"train['Age'].hist(figsize=(10, 4));","d01d24d0":"train['Age'].fillna(train['Age'].mean(), inplace=True)\ntest['Age'].fillna(test['Age'].mean(), inplace=True)","b4e85ddb":"test[test['Fare'].isna()]","e254db24":"Age_mean = train[(train['Pclass']==3) & (train['Embarked']=='S') & (train['Age']>55) & (train['Sex']==0)]['Fare'].mean()","171529f2":"test['Fare'].fillna(Age_mean, inplace=True)","8cb62f1d":"X =train.drop(['Survived','Name','Ticket','Cabin','Embarked'],axis=1)\ny =pd.DataFrame(train['Survived'])\ntest_f =test.drop(['Name','Ticket','Cabin','Embarked'],axis=1)","5d407ad3":"X.shape,y.shape","7c89fa7f":"X.head()","2c2942dd":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","ca61fe1a":"x_bag1,y_bag1,feat = get_samples_bagging(x_train,y_train,6)\nprint(x_bag1.head(5))\nprint(x_bag1.shape)\nx_bag1,y_bag1,feat = get_samples_bagging(x_train,y_train,6)\nprint(x_bag1.head(5))\nprint(x_bag1.shape)\nx_bag1,y_bag1,feat = get_samples_bagging(x_train,y_train,6)\nprint(x_bag1.head(5))\nprint(x_bag1.shape)","2b06ca69":"np.random.seed(2)\nx_bag1,y_bag1,feact = get_samples_bagging(x_train,y_train,6)\ntmodel1 = DecisionTreeClassifier()\ntmodel1.fit(x_bag1, y_bag1)\npd1 =tmodel1.predict_proba(x_test[feact])\npd.DataFrame(tmodel1.predict_proba(x_test[feact])).head(5)","b3067cfb":"np.random.seed(1767)\nx_bag1,y_bag1,feact = get_samples_bagging(x_train,y_train,6)\ntmodel2 = DecisionTreeClassifier()\ntmodel2.fit(x_bag1, y_bag1)\npd2 =tmodel2.predict_proba(x_test[feact])\npd.DataFrame(tmodel2.predict_proba(x_test[feact])).head(5)","0a4d1bc2":"probs = []\nprobs.append(pd1)\nprobs.append(pd2)\nmeand_target = np.mean(probs, axis=0)\npd.DataFrame(np.mean(probs, axis=0)).head()","89302942":"pd.DataFrame(np.argmax(meand_target, axis=1)).head()","f328c347":"probs = []\nmodel = []\n\nfor i in range(10):\n    np.random.seed(19+i)\n    x_bag1,y_bag1,feact = get_samples_bagging(x_train,y_train,6)\n    model.append(DecisionTreeClassifier())\n    model[i].fit(x_bag1, y_bag1)\n    probs.append(model[i].predict_proba(x_test[feact]))\n\nt = np.mean(probs, axis=0)\ny_target =np.argmax(t, axis=1).reshape(-1,1)\naccuracy_score(y_test, y_target)","115a87c9":"probs = []\nmodel = []\nfor i in range(100):\n    np.random.seed(19+i)\n    x_bag1,y_bag1,feact = get_samples_bagging(x_train,y_train,6)\n    model.append(DecisionTreeClassifier())\n    model[i].fit(x_bag1, y_bag1)\n    probs.append(model[i].predict_proba(x_test[feact]))\n\nt = np.mean(probs, axis=0)\ny_target =np.argmax(t, axis=1).reshape(-1,1)\naccuracy_score(y_test, y_target)","60fd8dbd":"probs = []\nmodel = []\nfor i in range(1000):\n    np.random.seed(19+i)\n    x_bag1,y_bag1,feact = get_samples_bagging(x_train,y_train,6)\n    model.append(DecisionTreeClassifier())\n    model[i].fit(x_bag1, y_bag1)\n    probs.append(model[i].predict_proba(x_test[feact]))\n\nt = np.mean(probs, axis=0)\ny_target =np.argmax(t, axis=1).reshape(-1,1)\naccuracy_score(y_test, y_target)","877357e9":"probs = []\nmodel = []\nfor i in range(10):\n    x_bag1,y_bag1,feact = get_samples_bagging(x_train,y_train,6)\n    model.append(DecisionTreeClassifier())\n    model[i].fit(x_bag1, y_bag1)\n    probs.append(model[i].predict_proba(test_f[feact]))\n\nt = np.mean(probs, axis=0)","41938df7":"pd.DataFrame(t).head()","59d1bc0d":"y_target =np.argmax(t, axis=1).reshape(-1,1)","533283b6":"d_ytarget = pd.DataFrame(y_target)","1f3bb337":"test_f_salida = pd.DataFrame( { 'PassengerId': test_f.index , 'Survived': d_ytarget[0]} )","ee9441f7":"#Show Output\ntest_f_salida.head(20)","8a7df374":"test_f_salida.to_csv( 'titanic_pred.csv' , index = False )","12483652":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1400\/1*Ei3eNxEKrPm7qpcDXsW_MA.png\" alt=\"Drawing\" style=\"width: 600px;\"\/>","c5de815d":"We select the column with major probability","d3951317":"<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/05\/Screenshot-from-2018-05-08-13-11-49-768x580.png\" alt=\"Drawing\" style=\"width: 600px;\"\/>","5d8edbc4":"## How does it work?, \n\nsee this step for step\n\nI Generated the first model and using predict_proba for get probability for column","a2e7838b":"Run model with all Dataset","4ead36c5":"----------------------------------------------------------------------","c4a2040b":"Run with 100 samples","00ed688e":"Show my Dataset","b5c3e89c":"Bootstrap\n\nBootstrap is a method using in ensemble for create DataSet with features random, but each features could repeat with the same probability","891c7506":"## Advanced Ensemble Techniques Bagging\nthis Kernel is built in base to this documents: https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/ , https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7\nIf I help you please upvote","e74f9fa9":"We take each de the Dataset built,  get in our model and obtain combinate prediction","5d26355f":"Finally combination probabilities with mean","e7ce4b9e":"Upload dataset","5ccce33e":"Run with 1000 samples","2820403c":"This Kernel es a basic example of implementation about Stacking, this technique is very import for understand techniques more avanzed ","0cc3a918":"I Generated the seconf model and using predict_proba for get probability for column","766f1fb8":"## Engineer Features\nWe work in the features Dataset, this example is about Stacking, and it's not important to deep in the feature","724863d1":"i count data missing and count dataset","c990c86b":"Run with 10 samples"}}