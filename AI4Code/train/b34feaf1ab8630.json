{"cell_type":{"efe671e0":"code","50cfa3c7":"code","3d50f709":"code","52fa01dd":"code","29c39b29":"code","0dfd1d16":"code","8aa8be7c":"code","13a295b7":"code","4359063c":"code","0bf9ee32":"code","efd9c165":"code","28ab6bbf":"code","992c9b81":"code","e4e41c81":"markdown","275967ae":"markdown","5540c8eb":"markdown","394d32e0":"markdown","b3c68c82":"markdown","14191d50":"markdown","5f8007ca":"markdown","7d255f99":"markdown","15b25a5c":"markdown","fb39f40c":"markdown","5957e7e5":"markdown","e9b33928":"markdown","53bc1953":"markdown","78083ac9":"markdown","0483574f":"markdown","36aa4b4e":"markdown","cb6c716e":"markdown","f58cdcdf":"markdown","a2070dd4":"markdown","cb2a3528":"markdown","ac4e88da":"markdown","41b27f27":"markdown","92df84c1":"markdown","ba2b2b1f":"markdown"},"source":{"efe671e0":"import pandas as pd\nimport numpy as np\nfrom category_encoders.cat_boost import CatBoostEncoder\n\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","50cfa3c7":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\npseud_label = pd.read_csv('\/kaggle\/input\/best-sub\/best_sub.csv')","3d50f709":"test['Survived'] = pseud_label['Survived']","52fa01dd":"def converter(x):\n    c, n = '', ''\n    x = str(x).replace('.', '').replace('\/','').replace(' ', '')\n    for i in x:\n        if i.isnumeric():\n            n += i\n        else :\n            c += i \n    if n != '':\n        return c, int(n)\n    return c, np.nan","29c39b29":"def create_extra_features(data):\n    data['Ticket_type'] = data['Ticket'].map(lambda x: converter(x)[0])\n    data['Ticket_number'] = data['Ticket'].map(lambda x: converter(x)[1])\n    \n    data['Cabin_type'] = data['Cabin'].map(lambda x: converter(x)[0])\n    data['Cabin_number'] = data['Cabin'].map(lambda x: converter(x)[1])\n    data['Name1'] = data['Name'].map(lambda x: x.split(', ')[0])    \n    data['Name2'] = data['Name'].map(lambda x: x.split(', ')[1])\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    data['isAlone'] = data['FamilySize'].apply(lambda x : 1 if x == 1 else 0)\n    \n    # Sex\n    data['Sex'] = data['Sex'].map({'male':0, 'female':1})\n    \n    # Age\n    age_map = train[['Age', 'Pclass']].dropna().groupby('Pclass').median().to_dict()['Age']\n    data.loc[train['Age'].isnull(), 'Age'] = data.loc[train['Age'].isnull(), 'Pclass'].map(age_map)\n\n    # Embarked\n    data['Embarked'] = data['Embarked'].fillna('X')\n    return data\n\ntrain = create_extra_features(train)\ntest = create_extra_features(test)","0dfd1d16":"ce = CatBoostEncoder()\n\ncolumn_name = ['Ticket_type', 'Embarked', 'Cabin_type', 'Name1', 'Name2']\ntrain[column_name] = ce.fit_transform(train[column_name], train['Survived'])\ntest[column_name] = ce.transform(test[column_name])","8aa8be7c":"train.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","13a295b7":"all_data = pd.concat([train, test]).reset_index(drop = True)\nall_data = all_data.fillna(all_data.median())\n_, test_data = all_data[:len(train)], all_data[len(test):]\n","4359063c":"all_data.head()","0bf9ee32":"X_train, X_test, y_train, y_test = train_test_split(all_data.drop('Survived', axis=1), all_data.Survived, test_size=0.2, random_state=0, stratify=all_data.Survived)","efd9c165":"# When running XGBoost builtin in SageMaker it implies the first column is the target\n# It also expects a train set and validation set\n# Prepare datasets\ndf_train = X_train.copy()\ndf_valid = X_test.copy()\n\ndf_train['Survived'] = y_train\ndf_valid['Survived'] = y_test\n\ncols = df_train.columns.tolist()\ncols.insert(0, cols.pop(cols.index('Survived')))\n\ndf_train = df_train.reindex(columns= cols)\ndf_valid = df_valid.reindex(columns= cols)\ndf_train.head()\n","28ab6bbf":"my_sub = pd.read_csv('\/kaggle\/input\/mysubsagemakertunning\/tunningjob_submission.csv')","992c9b81":"sample_submission['Survived'] = my_sub['Survived']\nsample_submission.to_csv('tunningjob_submission.csv', index=False)","e4e41c81":"### initialize hyperparameters\n```python\nhyperparameters = {\n        \"max_depth\":\"5\",\n        \"eta\":\"0.2\",\n        \"gamma\":\"4\",\n        \"min_child_weight\":\"6\",\n        \"subsample\":\"0.7\",\n        \"objective\":\"binary:logistic\",\n        \"num_round\":\"100\"}\n\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'titanic-xgb-built-in-algo')\n\nxgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, 'latest') \n\n### construct a SageMaker estimator that calls the xgboost-container\nestimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                                          hyperparameters=hyperparameters, \n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          sagemaker_session=session, \n                                          output_path=output_path)\nobjective_metric_name = 'validation:auc'\n```","275967ae":"## What is SageMaker  \n(from their website) Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML.  \n\nSageMaker has lots of features for every step of the ML pipeline, like AutoPilot for AutoML, DataWrangler to automate preprocessing steps using GUI etc.  \n\n**In this notebook I used only the Hyperparameter Tuning feature**","5540c8eb":"## Setup_Hyperparameter_Tuning\nNote, with the default setting below, the hyperparameter tuning job can take about 30 minutes to complete.\n\nNow that we have prepared the dataset, we are ready to train models. Before we do that, one thing to note is there are algorithm settings which are called \"hyperparameters\" that can dramtically affect the performance of the trained models. For example, XGBoost algorithm has dozens of hyperparameters and we need to pick the right values for those hyperparameters in order to achieve the desired model training results. Since which hyperparameter setting can lead to the best result depends on the dataset as well, it is almost impossible to pick the best hyperparameter setting without searching for it, and a good search algorithm can search for the best hyperparameter setting in an automated and effective way.\n\nWe will use SageMaker hyperparameter tuning to automate the searching process effectively. Specifically, we specify a range, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune. SageMaker hyperparameter tuning will automatically launch multiple training jobs with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will give it a budget (max number of training jobs) and it will complete once that many training jobs have been executed.\n\nIn this example, we are using SageMaker Python SDK to set up and manage the hyperparameter tuning job. We first configure the training jobs the hyperparameter tuning job will launch by initiating an estimator, which includes:\n\nThe container image for the algorithm (XGBoost)  \nConfiguration for the output of the training jobs  \nThe values of static algorithm hyperparameters, those that are not specified will be given default values  \nThe type and number of instances to use for the training jobs \n\n[https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/hyperparameter_tuning\/xgboost_direct_marketing\/hpo_xgboost_direct_marketing_sagemaker_python_sdk.ipynb](http:\/\/)","394d32e0":"%%time\n```python\nfrom sagemaker.serializers import CSVSerializer\nxgb_predictor = tuner.deploy(\n    initial_instance_count = 1,\n    instance_type = 'ml.m5.4xlarge',\n    serializer = CSVSerializer())\n```","b3c68c82":"```python\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker import image_uris\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker import get_execution_role\nimport boto3\n\nregion = boto3.Session().region_name\nsession = sagemaker.Session()\nrole = get_execution_role()\nbucket = session.default_bucket()\n```","14191d50":"## IMPORTANT. \n\nI didn't run the sageMaker cells here because it needed my credentials.  \nI had run this on SageMaker Jupyter Notebooks, there I didn't need to configure anything, all the libraries were installed out of the box.  \n**The main point is to explain how to use sageMaker tunning jobs.**","5f8007ca":"### Creating the inputs\n```python\ncontent_type = \"csv\"\n\ntrain_input = TrainingInput(train_data_s3_path, content_type=content_type)\nvalidation_input = TrainingInput(test_data_s3_path, content_type=content_type)\n\ntuner = HyperparameterTuner(\n    estimator,\n    objective_metric_name,\n    hyperparameter_ranges,\n    max_jobs=20,\n    max_parallel_jobs=10\n)\n```","7d255f99":"## The result of prediction below","15b25a5c":"### Creating a predictor (a deployed server with this model ready)","fb39f40c":"### Adding Hyperparameter Tuning\n```python\nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nhyperparameter_ranges = {\n    'max_depth': IntegerParameter(1, 20, scaling_type=\"Linear\"),\n    'eta': ContinuousParameter(0.01, 0.3, scaling_type=\"Logarithmic\"),\n    'min_child_weight': IntegerParameter(1,10, scaling_type=\"Linear\"),\n    'gamma': ContinuousParameter(0.01, 0.5, scaling_type=\"Logarithmic\"),\n    'colsample_bytree': ContinuousParameter(0.8, 0.95, scaling_type=\"Logarithmic\"),\n    'subsample': ContinuousParameter(0.6, 0.9, scaling_type=\"Logarithmic\")\n}\n```","5957e7e5":"```python\nsage_client.delete_endpoint(EndpointName=xgb_predictor.endpoint_name)\n```","e9b33928":"### Creating the Input that SageMaker can work with (from s3) and Launching","53bc1953":"## Importing libraries","78083ac9":"The Feature engineering steps I got from this kernel: [https:\/\/www.kaggle.com\/subinium\/how-to-use-pycaret-with-feature-engineering](http:\/\/)  \nFor more information on the steps of FE, please check his kernel, its great.  \n\nHere I'll focus only on the Hyperparameter Optimization part\n\nAs for training and the idea of  pseudolabelling the test data I got from this kernel: [https:\/\/www.kaggle.com\/alexryzhkov\/n3-tps-april-21-lightautoml-starter](http:\/\/)  \n\nSo thanks a lot **subinium** and **alexryzhkov** for sharing","0483574f":"### I'll use the sagemaker session to send the data to S3 so SageMaker can read from there.","36aa4b4e":"### Setting up Hyperparameter Ranges","cb6c716e":"%%time\n```python\ntuner.fit({'train': train_input, 'validation': validation_input}, include_cls_metadata=False)\n```","f58cdcdf":"### Predict\n```python\npredictions = xgb_predictor.predict(test_data.values).decode('utf-8')\npredictions = np.fromstring(predictions, sep=',')\npredictions = predictions>0.5\npredictions.astype(int)\naccuracy_score(y_test, predictions) #0.89\n```","a2070dd4":"### Sending data to s3\n```python\nprefix = 'sagemaker\/tps-titanic\/xgboost'\ntrain_file = 'df_train.csv';\ndf_train.to_csv(train_file, index=False, header=False)\ntrain_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"\/train\")\nprint('Train data uploaded to: ' + train_data_s3_path)\n\ntest_file = 'df_valid.csv';\ndf_valid.to_csv(test_file, index=False, header=False)\ntest_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"\/test\")\nprint('Test data uploaded to: ' + test_data_s3_path)\n```","cb2a3528":"### Deleting the endpoint (deployed server with the model) to stop incurring charges.","ac4e88da":"### Analysing results\n```python\nfrom pprint import pprint\nsage_client = boto3.Session().client('sagemaker')\ntuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)\nif tuning_job_result.get('BestTrainingJob',None):\n    print(\"Best model found so far:\")\n    pprint(tuning_job_result['BestTrainingJob'])\nelse:\n    print(\"No training jobs have reported results yet.\")\n    \n```","41b27f27":"## SageMaker  \n\nI've runned the following inside a SageMaker Jupyter Notebook, so I didn't need to install any of the following libraries","92df84c1":"### Output from this step:  \n\nBest model found so far:  \n{'CreationTime': datetime.datetime(2021, 4, 10, 12, 55, tzinfo=tzlocal()),  \n 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:auc',  \n                                                 'Value': 0.9596620202064514},  \n 'ObjectiveStatus': 'Succeeded',  \n 'TrainingEndTime': datetime.datetime(2021, 4, 10, 12, 58, 21, tzinfo=tzlocal()),  \n 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:475414269301:training-job\/xgboost-210410-1254-006-a1aa1de8',  \n 'TrainingJobName': '**xgboost**-210410-1254-006-a1aa1de8',  \n 'TrainingJobStatus': 'Completed',  \n 'TrainingStartTime': datetime.datetime(2021, 4, 10, 12, 57, 19, tzinfo=tzlocal()),  \n 'TunedHyperParameters': **{'colsample_bytree': '0.8777596567529345',  \n                          'eta': '0.08153822251994182',  \n                          'gamma': '0.03797776943376543',  \n                          'max_depth': '10',  \n                          'min_child_weight': '13',  \n                          'subsample': '0.8543920787438437'}**}  \n","ba2b2b1f":"### Analysing Results and best parameters"}}