{"cell_type":{"8b149f37":"code","86feaeab":"code","71ec4842":"code","c28fbc26":"code","595bb738":"code","0edcfebf":"code","859c4218":"code","f9d6cd48":"code","c30cb74d":"code","533ef987":"code","4541b281":"code","74bd2872":"code","8e1b0c78":"code","fdb059ad":"code","6bb37456":"code","92237455":"code","2d9ba6fb":"code","88bad518":"code","03372609":"code","e39dd439":"code","64cfea3d":"code","2a6425b7":"code","ec4ed32e":"code","7ef197d7":"code","22f45211":"code","f58f6f82":"code","76e83485":"code","b0a83726":"code","248f31f3":"code","9b4e37b9":"markdown","25461ce6":"markdown","0a0b76e6":"markdown","65e8d67e":"markdown","a499a3b3":"markdown","a537be35":"markdown","89534038":"markdown","04f5707f":"markdown","2efb23c3":"markdown","c0221797":"markdown","a9f3b32b":"markdown","249186c2":"markdown","c8bfb990":"markdown","dd3ed61c":"markdown","81fb1e08":"markdown","ec3bcb92":"markdown","9959e0bf":"markdown","af9f55b2":"markdown","cfed3c05":"markdown","7a087d35":"markdown","c2bfd116":"markdown","570da846":"markdown","3cf602b9":"markdown","174fedf0":"markdown","7810d944":"markdown","4fdc268b":"markdown","c117c27b":"markdown","bafe9fda":"markdown","860c7956":"markdown","9fc37d82":"markdown"},"source":{"8b149f37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86feaeab":"# import os\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","71ec4842":"#\u0110\u1ecdc file train v\u00e0 file test\ntrain_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","c28fbc26":"train_df.info()","595bb738":"test_df.info()","0edcfebf":"cnt_srs = train_df['target'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n","859c4218":"print(\"T\u1ec9 l\u1ec7 ph\u1ea7n tr\u0103m s\u1ed1 c\u00e2u h\u1ecfi Insincere l\u00e0:\", (len(train_df.loc[train_df.target==1])) \/ (len(train_df.loc[train_df.target == 0])) * 100)","f9d6cd48":"words = train_df['question_text'].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain_df['words'] = words\nwords = train_df.loc[train_df['words']<200]['words']\nsns.distplot(words, color='g')\nplt.show()","c30cb74d":"print('S\u1ed1 t\u1eeb trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file train l\u00e0 {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 t\u1eeb trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file test l\u00e0 {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x.split())))))","533ef987":"print('S\u1ed1 t\u1eeb l\u1edbn nh\u1ea5t c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file train l\u00e0 {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 t\u1eeb l\u1edbn nh\u1ea5t c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file test l\u00e0 {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x.split())))))","4541b281":"print('S\u1ed1 k\u00fd t\u1ef1 trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file train l\u00e0 {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\nprint('S\u1ed1 k\u00fd t\u1ef1 trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file test l\u00e0 {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x)))))","74bd2872":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"question_text\"], title=\"Word Cloud of Questions\")","8e1b0c78":"from collections import defaultdict\ntrain1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"top c\u00e1c t\u1eeb c\u00f3 nhi\u1ec1u trong c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh\", \n                                          \"top c\u00e1c t\u1eeb c\u00f3 nhi\u1ec1u trong c\u00e2u h\u1ecfi ko ch\u00e2n th\u00e0nh\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","fdb059ad":"from gensim.utils import simple_preprocess \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english')) \nwordnet_lemmatizer = WordNetLemmatizer()\ndef preprocessing(corpus):\n    res = []\n    for doc in corpus:\n        words = []\n        for word in simple_preprocess(doc):\n            if word not in stop_words:\n                word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n                word2 = wordnet_lemmatizer.lemmatize(word1,pos = \"v\")\n                word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n                words.append(word3)\n                pass\n            pass\n        res.append(' '.join(words))        \n        pass\n    return res","6bb37456":"train = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")","92237455":"# \u00e1p d\u1ee5ng cho t\u1eadp train v\u00e0 test \ntrain['question_text'] = preprocessing(train['question_text'])\ntest['question_text'] = preprocessing(test['question_text'])","2d9ba6fb":"vector = TfidfVectorizer( ngram_range = (1,2))\ntrain_feature_matrics = vector.fit_transform(train['question_text'].values.astype('U'))\ntest_feature_matrics = vector.transform(test['question_text'].values.astype('U'))","88bad518":"train_feature_matrics","03372609":"# Khai b\u00e1o th\u00eam c\u00e1c th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_feature_matrics, train['target'], test_size=0.25, shuffle=False)\nC = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]","e39dd439":"for c in C:\n    model = LogisticRegression(solver='liblinear', penalty='l2', C=c)\n    model.fit(train_x, train_y)\n    prediction = model.predict(valid_x)\n    f1 = f1_score(valid_y, prediction)\n    acc = accuracy_score(valid_y, prediction)\n    print(\"Regularization: \", c)\n    print(\"F1 score: \",f1)\n    print(\"Acc score: \",acc)","64cfea3d":"thresholds = [0.5, 0.1, 0.14, 0.15, 0.16, 0.17, 0.25, 0.3, 0.4, 0.55, 0.7]\nc = 30\nmodel = LogisticRegression(solver='liblinear', penalty='l2', C=c)\nmodel.fit(train_x, train_y)\npredict = model.predict_proba(valid_x)[:,1]\nfor t in thresholds:\n    predict_t = np.where(predict > t, 1, 0)\n    f1 = f1_score(valid_y, predict_t)\n    print(\"Threshold: \", t)\n    print(\"F1 score: \", f1)","2a6425b7":"t= 0.15\npredict = model.predict_proba(valid_x)[:,1]\npredict = np.where(predict > t, 1, 0)","ec4ed32e":"confusion_matrix(valid_y, predict)","7ef197d7":"class_weight = {0: 1., 1: 14.}\nthresholds = [0.1, 0.25, 0.3, 0.35, 0.4, 0.55, 0.6, 0.7, 0.8, 0.85, 0.9]\nc = 30\nmodel = LogisticRegression(solver='liblinear', penalty='l2', C=c, class_weight = class_weight)\nmodel.fit(train_x, train_y)\npredict = model.predict_proba(valid_x)[:,1]\nfor t in thresholds:\n    predict_t = np.where(predict > t, 1, 0)\n    f1 = f1_score(valid_y, predict_t)\n    print(\"Threshold: \", t)\n    print(\"F1 score: \", f1)","22f45211":"# Ki\u1ec3m tra d\u1eef li\u1ec7u c\u00f3 trong t\u1eadp test\ntest.head(20)","f58f6f82":"model = LogisticRegression(solver='liblinear', penalty='l2', C=30)\nmodel.fit(train_feature_matrics, train['target'])\npredict = model.predict_proba(test_feature_matrics)[:,1]\npredict = np.where(predict > 0.17, 1, 0)","76e83485":"test['prediction'] = predict","b0a83726":"test.head(20)","248f31f3":"results = test[['qid', 'prediction']]\nresults.to_csv('submission.csv', index=False)","9b4e37b9":"\u0110\u00e3 xong qu\u00e1 tr\u00ecnh ti\u1ec1n x\u1eed l\u00fd , l\u00fac n\u00e0y d\u1eef li\u1ec7u c\u01a1 b\u1ea3n \u0111\u00e3 \u0111\u01b0\u1ee3c l\u00e0m s\u1ea1ch , b\u00e2y gi\u1edd ta s\u1ebd ch\u1ecdn l\u1ef1a model \u0111\u1ec3 th\u1ef1c hi\u1ec7n qu\u00e1 tr\u00ecnh train","25461ce6":"T\u00ednh c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong c\u00e1c c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh v\u00e0 kh\u00f4ng ch\u00e2n th\u00e0nh","0a0b76e6":"# 1. Gi\u1edbi thi\u1ec7u b\u00e0i to\u00e1n:\n## Quora Insincere Questions Classification\nQuora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi l\u1eabn nhau. Tr\u00ean Quora, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00e1c, nh\u1eefng ng\u01b0\u1eddi \u0111\u00f3ng g\u00f3p th\u00f4ng tin chi ti\u1ebft \u0111\u1ed9c \u0111\u00e1o v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng. M\u1ed9t th\u00e1ch th\u1ee9c quan tr\u1ecdng l\u00e0 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh - nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra d\u1ef1a tr\u00ean nh\u1eefng ti\u1ec1n \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch.\n* Quora l\u00e0 n\u1ec1n t\u1ea3ng \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 h\u1ecdc h\u1ecfi l\u1eabn nhau b\u1eb1ng c\u00e1ch \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 tr\u1ea3 l\u1eddi \u0111\u1ec3 chia s\u1ebb ki\u1ebfn th\u1ee9c. M\u1ee5c \u0111\u00edch c\u1ee7a b\u00e0i to\u00e1n n\u00e0y l\u00e0 \u0111\u1ec3 ph\u00e2n lo\u1ea1i c\u00e1c c\u00e2u h\u1ecfi \u0111\u1eb7t ra l\u00e0 thu\u1ed9c lo\u1ea1i c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh hay kh\u00f4ng ch\u00e2n th\u00e0nh.\n* Nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh th\u01b0\u1eddng l\u00e0 nh\u1eefng c\u00e2u \u0111\u01b0a ra tuy\u00ean b\u1ed1 quan \u0111i\u1ec3m c\u1ee7a m\u00ecnh h\u01a1n l\u00e0 \u0111\u1ec3 t\u00ecm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi c\u00f3 \u00edch: kh\u00f4ng trung l\u1eadp, khi\u00eau kh\u00edch ho\u1eb7c ch\u00ea bai, kh\u00f4ng c\u00f3 c\u0103n c\u1ee9 th\u1ef1c t\u1ebf ho\u1eb7c ch\u1ee9a n\u1ed9i dung khi\u00eau d\u00e2m.\n* Input: C\u00e2u h\u1ecfi d\u1ea1ng text.\n* Output: 0\/1 (Sincere\/ Insincere). \n","65e8d67e":"**Nh\u1eadn x\u00e9t**: Model n\u00e0y cho k\u1ebft qu\u1ea3 F1-score kh\u00f4ng t\u1ed1t b\u1eb1ng model tr\u01b0\u1edbc , v\u00ec v\u1eady ta s\u1ebd d\u00f9ng k\u1ebft qu\u1ea3 c\u1ee7a model tr\u01b0\u1edbc \u0111\u1ec3 d\u1ef1 to\u00e1n k\u1ebft qu\u1ea3 cho t\u1eadp test ","a499a3b3":"# 3.Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","a537be35":"# T\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng\n\nL\u1ef1a ch\u1ecdn **TF-IDF** l\u00e0m \u0111\u1eb7c tr\u01b0ng:\n\n**TF-IDF l\u00e0 g\u00ec?**\n\n**TF-IDF** (Term Frequency \u2013 Inverse Document Frequency) l\u00e0 1 k\u0129 thu\u1eadt s\u1eed d\u1ee5ng trong khai ph\u00e1 d\u1eef li\u1ec7u v\u0103n b\u1ea3n. Tr\u1ecdng s\u1ed1 n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb trong m\u1ed9t v\u0103n b\u1ea3n. Gi\u00e1 tr\u1ecb cao th\u1ec3 hi\u1ec7n \u0111\u1ed9 quan tr\u1ecdng cao v\u00e0 n\u00f3 ph\u1ee5 thu\u1ed9c v\u00e0o s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n nh\u01b0ng b\u00f9 l\u1ea1i b\u1edfi t\u1ea7n su\u1ea5t c\u1ee7a t\u1eeb \u0111\u00f3 trong t\u1eadp d\u1eef li\u1ec7u. M\u1ed9t v\u00e0i bi\u1ebfn th\u1ec3 c\u1ee7a **TF-IDF** th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong c\u00e1c h\u1ec7 th\u1ed1ng t\u00ecm ki\u1ebfm nh\u01b0 m\u1ed9t c\u00f4ng c\u1ee5 ch\u00ednh \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 v\u00e0 s\u1eafp x\u1ebfp v\u0103n b\u1ea3n d\u1ef1a v\u00e0o truy v\u1ea5n c\u1ee7a ng\u01b0\u1eddi d\u00f9ng. **TF-IDF** c\u0169ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 l\u1ecdc nh\u1eefng t\u1eeb stopwords trong c\u00e1c b\u00e0i to\u00e1n nh\u01b0 t\u00f3m t\u1eaft v\u0103n b\u1ea3n v\u00e0 ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n.\n\n**TF l\u00e0 g\u00ec?**\n**TF**: Term Frequency(T\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb) l\u00e0 s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n. V\u00ec c\u00e1c v\u0103n b\u1ea3n c\u00f3 th\u1ec3 c\u00f3 \u0111\u1ed9 d\u00e0i ng\u1eafn kh\u00e1c nhau n\u00ean m\u1ed9t s\u1ed1 t\u1eeb c\u00f3 th\u1ec3 xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u1ea7n trong m\u1ed9t v\u0103n b\u1ea3n d\u00e0i h\u01a1n l\u00e0 m\u1ed9t v\u0103n b\u1ea3n ng\u1eafn. Nh\u01b0 v\u1eady, term frequency th\u01b0\u1eddng \u0111\u01b0\u1ee3c chia cho \u0111\u1ed9 d\u00e0i v\u0103n b\u1ea3n( t\u1ed5ng s\u1ed1 t\u1eeb trong m\u1ed9t v\u0103n b\u1ea3n).\n\n![image.png](attachment:aaaff1a8-fd14-414f-8b90-6748c32c69e1.png)\n\nTrong \u0111\u00f3:\n\n> tf(t, d): t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb t trong v\u0103n b\u1ea3n d\n\n> f(t, d): S\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb t trong v\u0103n b\u1ea3n d\n\n> max({f(w, d) : w \u2208 d}): S\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb c\u00f3 s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong v\u0103n b\u1ea3n d\n\n**IDF l\u00e0 g\u00ec?**\n\n**IDF**: Inverse Document Frequency(Ngh\u1ecbch \u0111\u1ea3o t\u1ea7n su\u1ea5t c\u1ee7a v\u0103n b\u1ea3n), gi\u00fap \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb . Khi t\u00ednh to\u00e1n TF , t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb \u0111\u01b0\u1ee3c coi nh\u01b0 c\u00f3 \u0111\u1ed9 quan tr\u1ecdng b\u1eb1ng nhau. Nh\u01b0ng  m\u1ed9t s\u1ed1 t\u1eeb nh\u01b0 \u201cis\u201d, \u201cof\u201d v\u00e0 \u201cthat\u201d th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n r\u1ea5t nhi\u1ec1u l\u1ea7n nh\u01b0ng \u0111\u1ed9 quan tr\u1ecdng l\u00e0 kh\u00f4ng cao. Nh\u01b0 th\u1ebf ch\u00fang ta c\u1ea7n gi\u1ea3m \u0111\u1ed9 quan tr\u1ecdng c\u1ee7a nh\u1eefng t\u1eeb n\u00e0y xu\u1ed1ng.\n\n![image.png](attachment:b77d3007-95af-4ee4-9311-3261bef33b3a.png)\n\nTrong \u0111\u00f3:\n\n> idf(t, D): gi\u00e1 tr\u1ecb idf c\u1ee7a t\u1eeb t trong t\u1eadp v\u0103n b\u1ea3n\n\n> |D|: T\u1ed5ng s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D\n\n> |{d \u2208 D : t \u2208 d}|: th\u1ec3 hi\u1ec7n s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D c\u00f3 ch\u1ee9a t\u1eeb t.\n\nC\u01a1 s\u1ed1 logarit trong c\u00f4ng th\u1ee9c n\u00e0y kh\u00f4ng thay \u0111\u1ed5i gi\u00e1 tr\u1ecb idf c\u1ee7a t\u1eeb m\u00e0 ch\u1ec9 thu h\u1eb9p kho\u1ea3ng gi\u00e1 tr\u1ecb c\u1ee7a t\u1eeb \u0111\u00f3. V\u00ec thay \u0111\u1ed5i c\u01a1 s\u1ed1 s\u1ebd d\u1eabn \u0111\u1ebfn vi\u1ec7c gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c t\u1eeb thay \u0111\u1ed5i b\u1edfi m\u1ed9t s\u1ed1 nh\u1ea5t \u0111\u1ecbnh v\u00e0 t\u1ef7 l\u1ec7 gi\u1eefa c\u00e1c tr\u1ecdng l\u01b0\u1ee3ng v\u1edbi nhau s\u1ebd kh\u00f4ng thay \u0111\u1ed5i. (n\u00f3i c\u00e1ch kh\u00e1c, thay \u0111\u1ed5i c\u01a1 s\u1ed1 s\u1ebd kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn t\u1ef7 l\u1ec7 gi\u1eefa c\u00e1c gi\u00e1 tr\u1ecb **IDF**). Vi\u1ec7c s\u1eed d\u1ee5ng logarit nh\u1eb1m gi\u00fap gi\u00e1 tr\u1ecb **tf-idf** c\u1ee7a m\u1ed9t t\u1eeb nh\u1ecf h\u01a1n, do ch\u00fang ta c\u00f3 c\u00f4ng th\u1ee9c t\u00ednh tf-idf c\u1ee7a m\u1ed9t t\u1eeb trong 1 v\u0103n b\u1ea3n l\u00e0 t\u00edch c\u1ee7a **tf** v\u00e0 **idf** c\u1ee7a t\u1eeb \u0111\u00f3.\n\n**C\u1ee5 th\u1ec3, ch\u00fang ta c\u00f3 c\u00f4ng th\u1ee9c t\u00ednh TF-IDF ho\u00e0n ch\u1ec9nh nh\u01b0 sau:** \n\n> tfidf(t, d, D) = tf(t, d) x idf(t, D)\n\nKhi \u0111\u00f3:\n\nNh\u1eefng t\u1eeb c\u00f3 gi\u00e1 tr\u1ecb **TF-IDF** cao l\u00e0 nh\u1eefng t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong v\u0103n b\u1ea3n n\u00e0y, v\u00e0 xu\u1ea5t hi\u1ec7n \u00edt trong c\u00e1c v\u0103n b\u1ea3n kh\u00e1c. Vi\u1ec7c n\u00e0y gi\u00fap l\u1ecdc ra nh\u1eefng t\u1eeb ph\u1ed5 bi\u1ebfn v\u00e0 gi\u1eef l\u1ea1i nh\u1eefng t\u1eeb c\u00f3 gi\u00e1 tr\u1ecb cao (t\u1eeb kho\u00e1 c\u1ee7a v\u0103n b\u1ea3n \u0111\u00f3).","89534038":"T\u00ednh t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb trong t\u1eadp d\u1eef li\u1ec7u b\u1eb1ng c\u00e1ch t\u1ea1o m\u1ed9t '***word cloud***' tr\u00ean c\u1ed9t '***question_text***'","04f5707f":"**Cu\u1ed1i c\u00f9ng**: t\u1ea1o file submission .csv ch\u1ee9a k\u1ebft qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh. ","2efb23c3":"**Nh\u1eadn x\u00e9t**: C\u00f3 th\u1ec3 th\u1ea5y , gi\u00e1 tr\u1ecb Regularization t\u1ed1t nh\u1ea5t l\u00e0 30,  V\u00ec h\u00e0m predict s\u1ebd tr\u1ea3 v\u1ec1 m\u1ed9t sample l\u00e0 sincere hay kh\u00f4ng b\u1eb1ng c\u00e1ch so s\u00e1nh x\u00e1c su\u1ea5t \u0111\u1ea7u ra v\u1edbi m\u1ed9t gi\u00e1 tr\u1ecb m\u1eb7c \u0111\u1ecbnh threshold = 0.5 (decision bound = 0), ta s\u1ebd th\u1eed thay \u0111\u1ed5i m\u1ee9c threshold n\u00e0y.","c0221797":"# 5.L\u1ef1a ch\u1ecdn model\n\u0110\u1ed1i v\u1edbi b\u00e0i to\u00e1n ph\u00e2n l\u1edbp n\u00e0y , v\u1edbi \u0111\u1eb7c \u0111i\u1ec3m c\u1ee7a b\u1ed9 d\u1eef li\u1ec7u l\u00e0 b\u1ecb ch\u00eanh l\u1ec7ch qu\u00e1 l\u1edbn , v\u00ec v\u1eady model ph\u00f9 h\u1ee3p nh\u1ea5t \u0111\u01b0\u1ee3c l\u1ef1a ch\u1ecdn l\u00e0 **F1- score**. **F1- score** l\u00e0 c\u00e1ch \u0111\u00e1nh gi\u00e1 h\u1ed7 tr\u1ee3 r\u1ea5t t\u1ed1t cho b\u00e0i to\u00e1n ph\u00e2n l\u1edbp c\u00f3 2 l\u1edbp d\u1eef li\u1ec7u \n\n# a. T\u00ecm hi\u1ec3u v\u1ec1 F1-score\n\n* \u0110\u1ea7u ti\u00ean, **Precision** \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m **positive** m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang tr\u00ean t\u1ed5ng s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n l\u00e0 **Positive**. **Recall** \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m **positive** m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang tr\u00ean t\u1ed5ng s\u1ed1 \u0111i\u1ec3m th\u1eadt s\u1ef1 l\u00e0 **Positive** (hay t\u1ed5ng s\u1ed1 \u0111i\u1ec3m \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n l\u00e0 **positive** ban \u0111\u1ea7u).\n\n* **Precision** c\u00e0ng cao, t\u1ee9c l\u00e0 s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n l\u00e0 **positive** \u0111\u1ec1u l\u00e0 **positive** c\u00e0ng nhi\u1ec1u. **Precision** = 1, t\u1ee9c l\u00e0 t\u1ea5t c\u1ea3 s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 do\u00e1n l\u00e0 **Positive** \u0111\u1ec1u \u0111\u00fang, hay kh\u00f4ng c\u00f3 \u0111i\u1ec3m n\u00e0o c\u00f3 nh\u00e3n l\u00e0 **Negative** m\u00e0 m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n nh\u1ea7m l\u00e0 **Positive**.\n\n* **Recall** c\u00e0ng cao, t\u1ee9c l\u00e0 s\u1ed1 \u0111i\u1ec3m l\u00e0 **positive** b\u1ecb b\u1ecf s\u00f3t c\u00e0ng \u00edt. **Recall** = 1, t\u1ee9c l\u00e0 t\u1ea5t c\u1ea3 s\u1ed1 \u0111i\u1ec3m c\u00f3 nh\u00e3n l\u00e0 **Positive** \u0111\u1ec1u \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh nh\u1eadn ra.\n\n* Ch\u1ec9 d\u00f9ng **Precision**, m\u00f4 h\u00ecnh ch\u1ec9 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n cho m\u1ed9t \u0111i\u1ec3m m\u00e0 n\u00f3 ch\u1eafc ch\u1eafn nh\u1ea5t. Khi \u0111\u00f3 **Precision** = 1, tuy nhi\u00ean ta kh\u00f4ng th\u1ec3 n\u00f3i l\u00e0 m\u00f4 h\u00ecnh n\u00e0y t\u1ed1t.Ch\u1ec9 d\u00f9ng **Recall**, n\u1ebfu m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m \u0111\u1ec1u l\u00e0 **Positive**. Khi \u0111\u00f3 **Recall** = 1, tuy nhi\u00ean ta c\u0169ng kh\u00f4ng th\u1ec3 n\u00f3i \u0111\u00e2y l\u00e0 m\u00f4 h\u00ecnh t\u1ed1t.\nKhi \u0111\u00f3 **F1-score** \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng. **F1-score** l\u00e0 trung b\u00ecnh \u0111i\u1ec1u h\u00f2a (**harmonic mean**) c\u1ee7a **Precision** v\u00e0 **Recall** (gi\u1ea3 s\u1eed hai \u0111\u1ea1i l\u01b0\u1ee3ng n\u00e0y kh\u00e1c 0). **F1-score** \u0111\u01b0\u1ee3c tinh theo c\u00f4ng th\u1ee9c:\n\n> ![image.png](attachment:73ba45a9-aea9-4ed4-8977-2740c6b421b1.png)\n\n* **F1-score** c\u00f3 gi\u00e1 tr\u1ecb n\u1eb1m trong n\u1eeda kho\u1ea3ng (0,1].**F1** c\u00e0ng cao, b\u1ed9 ph\u00e2n l\u1edbp c\u00e0ng t\u1ed1t. Khi c\u1ea3 **Recall** v\u00e0 **Precision** \u0111\u1ec1u b\u1eb1ng 1 (t\u1ed1t nh\u1ea5t c\u00f3 th\u1ec3), **F1** = 1. Khi c\u1ea3 **Recall** v\u00e0 **Precision** \u0111\u1ec1u th\u1ea5p, v\u00ed d\u1ee5 b\u1eb1ng 0.1, **F1** = 0.1.\n\n# b.L\u1ef1a ch\u1ecdn m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n: m\u00f4 h\u00ecnh Logistic Regression\n- Gi\u1edbi thi\u1ec7u qua v\u1ec1 Logistic Regression \n> **Logistic Regression** l\u00e0 1 thu\u1eadt to\u00e1n ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 g\u00e1n c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng cho 1 t\u1eadp h\u1ee3p gi\u00e1 tr\u1ecb r\u1eddi r\u1ea1c (nh\u01b0 0, 1, 2, ...). M\u1ed9t v\u00ed d\u1ee5 \u0111i\u1ec3n h\u00ecnh l\u00e0 ph\u00e2n lo\u1ea1i Email, g\u1ed3m c\u00f3 email c\u00f4ng vi\u1ec7c, email gia \u0111\u00ecnh, email spam, ... Giao d\u1ecbch tr\u1ef1c tuy\u1ebfn c\u00f3 l\u00e0 an to\u00e0n hay kh\u00f4ng an to\u00e0n, kh\u1ed1i u l\u00e0nh t\u00ednh hay \u00e1c t\u00ecnh. Thu\u1eadt to\u00e1n tr\u00ean d\u00f9ng h\u00e0m sigmoid logistic \u0111\u1ec3 \u0111\u01b0a ra \u0111\u00e1nh gi\u00e1 theo x\u00e1c su\u1ea5t. \n\n- H\u1ecdc **tf-idf** \u0111\u00e3 **extract** \u1edf tr\u00ean b\u1eb1ng m\u00f4 h\u00ecnh Logistic regression. Th\u1eed tunning tham s\u1ed1 regularize C.","a9f3b32b":"**Nh\u1eadn x\u00e9t:** D\u1eef li\u1ec7u file train kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null.","249186c2":"**Nh\u1eadn x\u00e9t:** D\u1eef li\u1ec7u file test kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null.","c8bfb990":"**\u0110\u1ecdc l\u1ea1i file train.csv v\u00e0 test.csv \u0111\u01b0a v\u1ec1 d\u1ea1ng t\u1ec7p 'train' v\u00e0 'test'**","dd3ed61c":"**K\u1ebft lu\u1eadn**: M\u00f4 h\u00ecnh th\u1eed nghi\u1ec7m tr\u00ean t\u1eadp test \u0111\u00e3 cho ra k\u1ebft qu\u1ea3 ph\u00e2n bi\u1ec7t \u0111\u01b0\u1ee3c \u0111\u00e2u l\u00e0 c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh (**prediction** = 0) v\u00e0 \u0111\u00e2u l\u00e0 c\u00e2u h\u1ecfi kh\u00f4ng tr\u00e2n th\u00e0nh (**prediction** = 1).\n\n","81fb1e08":"**\u0110\u1ecdc c\u00e1c d\u1eef li\u1ec7u file train v\u00e0 file test**","ec3bcb92":"**Nh\u1eadn x\u00e9t:** S\u1ed1 c\u00e2u h\u1ecfi \"insincere\" ch\u1ec9 chi\u1ebfm kho\u1ea3ng 6-7% trong t\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi. D\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng kh\u00e1 l\u1edbn, do \u0111\u00f3 \u0111\u1ed9 \u0111o F1 s\u1ebd th\u00edch h\u1ee3p cho nh\u1eefng tr\u01b0\u1eddng h\u1ee3p nh\u01b0 n\u00e0y","9959e0bf":" **PH\u00c2N T\u00cdCH T\u1eeaNG C\u00c2U H\u1eceI**","af9f55b2":"# 4. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u\n\n**\u0110\u1ea7u ti\u00ean ta s\u1ebd x\u1eed l\u00fd data:** B\u1ecf c\u00e1c t\u1eeb kh\u00f4ng mang ngh\u0129a v\u00e0 d\u1ea5u (stopword, punctual), \u0111\u1ed5i ch\u1eef hoa th\u00e0nh ch\u1eef th\u01b0\u1eddng, lemmatize (\u0111\u01b0a t\u1ea5t c\u1ea3 c\u00e1c ch\u1eef v\u1ec1 m\u1ed9t d\u1ea1ng th\u1ed1ng nh\u1ea5t)","cfed3c05":"**Nh\u1eadn x\u00e9t**: V\u00ec s\u1ed1 sample c\u00f3 nh\u00e3n 1 ch\u1ec9 chi\u1ebfm 6% t\u1ed5ng s\u1ed1 sample, ta th\u1eed s\u1eed d\u1ee5ng class **weight** \u0111\u1ec3 c\u1ea3i ti\u1ebfn model. Coi 1 sample c\u00f3 label 1 nh\u01b0 14 samples c\u00f3 label 0.","7a087d35":"# 6. K\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n trong t\u1eadp test","c2bfd116":"**Nh\u1eadn x\u00e9t:** C\u00f3 th\u1ec3 th\u1ea5y \u0111\u1ed9 d\u00e0i trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong t\u1eadp d\u1eef li\u1ec7u file train v\u00e0 file test l\u00e0 gi\u1ed1ng nhau, tuy nhi\u00ean c\u00f3 nh\u1eefng c\u00e2u h\u1ecfi kh\u00e1 d\u00e0i trong t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n.","570da846":"**S\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u**","3cf602b9":"**Import th\u01b0 vi\u1ec7n:**","174fedf0":"**Nh\u1eadn x\u00e9t**: T\u1ea1i Threshold = 0.15 , gi\u00e1 tr\u1ecb F1 score l\u00e0 t\u1ed1t nh\u1ea5t cho t\u1eadp val","7810d944":"# 2. Import c\u00e1c th\u01b0 vi\u1ec7n v\u00e0 c\u00e1c file d\u1eef li\u1ec7u c\u1ea7n thi\u1ebft","4fdc268b":"* B\u00e2y gi\u1edd , b\u1eaft \u0111\u1ea7u \u0111i v\u00e0o qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n","c117c27b":"# B\u00c1O C\u00c1O B\u00c0I T\u1eacP L\u1edaN M\u00d4N H\u1eccC M\u00c1Y\n\n**Gi\u1ea3ng vi\u00ean:** Tr\u1ea7n Qu\u1ed1c Long\n\n**L\u1edbp m\u00f4n h\u1ecdc:** 2122I_INT3405_1\n\n**Sinh vi\u00ean:** \u0110\u00e0o Anh Tu\u1ea5n\n\n**MSSV:** 18021372","bafe9fda":"**Nh\u1eadn x\u00e9t:** Nh\u00ecn v\u00e0o bi\u1ec3u \u0111\u1ed3 ta th\u1ea5y , d\u1eef li\u1ec7u \u0111ang b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng , s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi c\u00f3 label = 0  chi\u1ebfm \u0111a s\u1ed1, trong khi s\u1ed1 l\u01b0\u1ee3ng c\u00e1c  c\u00e2u h\u1ecfi c\u00f3 label = 1 l\u1ea1i r\u1ea5t \u00edt. \u0110i\u1ec1u n\u00e0y s\u1ebd g\u00e2y \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn k\u1ebft qu\u1ea3 n\u1ebfu \u0111\u01b0a lu\u00f4n d\u1eef li\u1ec7u v\u00e0o \u0111\u1ec3 train, v\u00ec th\u1ec3 ch\u00fang ta c\u1ea7n ph\u1ea3i c\u00f3 b\u01b0\u1edbc ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u tr\u01b0\u1edbc khi \u0111\u01b0a v\u00e0o m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n.","860c7956":"**NH\u1eacN X\u00c9T:**\n* M\u1ed9t v\u00e0i t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u \u1edf c\u1ea3 hai l\u1edbp nh\u01b0 people, think, many...\n* NH\u1eefng top words xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t \u1edf l\u1edbp c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh l\u00e0 : best, will, people...\n* NH\u1eefng top words xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t \u1edf l\u1edbp c\u00e2u h\u1ecfi ko ch\u00e2n th\u00e0nh l\u00e0 : people, women, will...","9fc37d82":"**Ti\u1ebfp theo , v\u1ebd bi\u1ec1u \u0111\u1ed3 th\u1ec3 hi\u1ec7n s\u1ef1 ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u trong t\u1eadp train**"}}