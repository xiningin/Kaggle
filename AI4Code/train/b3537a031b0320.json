{"cell_type":{"2113f9cf":"code","7f6ee3b4":"code","b3c33419":"code","63899ef2":"code","de11a685":"code","4b258648":"code","30158955":"code","e636397d":"code","6de47e64":"code","7f78ff93":"code","4f32dbb8":"code","b19b6c07":"code","4b72219a":"code","91e9d7c5":"code","63b3253a":"code","e8f81020":"code","ea09ad15":"code","30e4d95e":"code","6d0c7206":"code","17f2c546":"code","f3a70234":"code","33a86dd5":"code","d2cbf15d":"code","72cd8c18":"code","fdc6284d":"code","95c09a1d":"code","36e8eb9e":"code","d89e8bdb":"code","7efb20d1":"code","8af0d103":"code","fd59bdd3":"code","04619c78":"code","56c3ca3e":"code","28dcdb43":"code","cf263f70":"code","d54dc72b":"code","e8ace820":"code","23c61b58":"code","d340d92a":"code","a68f7014":"code","b025ff50":"code","35ab954d":"code","bab0b48a":"code","85f76f8e":"code","3fdad36e":"code","76b2f239":"code","c0a3017a":"code","12529c17":"markdown","a185c7c9":"markdown","84bb0f14":"markdown","451b8e2c":"markdown","7d938305":"markdown","5ede451b":"markdown","a954682f":"markdown","7fe54ba0":"markdown","008c709a":"markdown","3862c050":"markdown","b94173c8":"markdown","b5246da7":"markdown","ea032c0b":"markdown","742a55aa":"markdown","d10b182d":"markdown","a4f478a6":"markdown","672ecd8d":"markdown","758f095a":"markdown","36bf8414":"markdown","b8d4491a":"markdown","13bd93c5":"markdown","376a6e7c":"markdown","e7682b1a":"markdown","7a52e2ca":"markdown","612d4de5":"markdown","79ebc66d":"markdown","e317f8be":"markdown","171bc345":"markdown","9c791414":"markdown","7d81326d":"markdown","4d88747d":"markdown","79d6701c":"markdown"},"source":{"2113f9cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f6ee3b4":"import warnings  \nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b3c33419":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata","63899ef2":"data.isnull().sum()","de11a685":"sns.countplot(data['Pregnancies'])","4b258648":"sns.distplot(data['Glucose'])","30158955":"sns.distplot(data['BloodPressure'])","e636397d":"sns.distplot(data['SkinThickness'])","6de47e64":"sns.distplot(data['Insulin'])","7f78ff93":"sns.distplot(data['BMI'])","4f32dbb8":"sns.distplot(data['DiabetesPedigreeFunction'])","b19b6c07":"sns.distplot(data['Age'])","4b72219a":"sns.countplot(data['Outcome'])","91e9d7c5":"sns.swarmplot(x=\"Outcome\", y=\"Pregnancies\", data=data)","63b3253a":"sns.swarmplot(x=\"Outcome\", y=\"Age\", data=data)","e8f81020":"sns.lmplot(x='Insulin',y='Glucose', hue = 'Outcome',data = data)","ea09ad15":"sns.lmplot(x='DiabetesPedigreeFunction',y='Glucose', hue = 'Outcome',data = data)","30e4d95e":"sns.lmplot(x='Pregnancies',y='Glucose', hue = 'Outcome',data = data)","6d0c7206":"sns.lmplot(x='BloodPressure',y='Glucose',hue = 'Outcome',data = data)","17f2c546":"sns.lmplot(x='SkinThickness',y='BMI',hue = 'Outcome',data = data)","f3a70234":"sns.lmplot(x='SkinThickness',y='Insulin',hue = 'Outcome',data = data)","33a86dd5":"plt.figure(figsize=(16,10))\nsns.heatmap(data.corr(method='pearson'), annot=True)","d2cbf15d":"y=data['Outcome']\nX=data.drop(columns=['Outcome'], inplace=True)","72cd8c18":"features = ['Age', 'Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction']\nX=data[features]","fdc6284d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, shuffle=True)","95c09a1d":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)","36e8eb9e":"y_pred = log_reg.predict(X_test)\ny_pred","d89e8bdb":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, y_pred)\nprint(classification_report)","7efb20d1":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nx_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nx_test_smote, y_test_smote = smote.fit_resample(X_test, y_test)","8af0d103":"X_train = x_train_smote\nX_test = x_test_smote\ny_train = y_train_smote\ny_test = y_test_smote","fd59bdd3":"pred1=log_reg.predict(X_test)\npred1","04619c78":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, pred1)\nprint(classification_report)","56c3ca3e":"cm = confusion_matrix(y_test, pred1)\ncm","28dcdb43":"tn = cm[0,0]\nfp = cm[0,1]\ntp = cm[1,1]\nfn = cm[1,0]\naccuracy  = (tp + tn) \/ (tp + fp + tn + fn)\nprecision = tp \/ (tp + fp)\nrecall    = tp \/ (tp + fn)\nf1score  = 2 * precision * recall \/ (precision + recall)\nprint(f1score)","cf263f70":"predicted_probab = log_reg.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='Logistic Regression')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","d54dc72b":"import xgboost as xgb\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train,y_train)\ny_pred1 = model.predict(X_test)\n","e8ace820":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, y_pred1)\nprint(classification_report)","23c61b58":"roc_auc_score(y_test, y_pred1)","d340d92a":"predicted_probab = model.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='XGB')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","a68f7014":"from sklearn.tree import DecisionTreeClassifier\ndt_clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0, criterion='entropy')\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)","b025ff50":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, dt_pred)\nprint(classification_report)","35ab954d":"auc = roc_auc_score(y_test, dt_pred)\nauc","bab0b48a":"\npredicted_probab = dt_clf.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='Decision Tree')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","85f76f8e":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)","3fdad36e":"rfc_predict = rfc.predict(X_test)\nroc_auc_score(y_test, rfc_predict)","76b2f239":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, rfc_predict)\nprint(classification_report)","c0a3017a":"#RF\npredicted_probab = rfc.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='Random Forest')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","12529c17":"**Splitting into features and labels**","a185c7c9":"# What does the relationships tell us?\n1. High levels of Insulin in blood post 2 hrs indiacte that glucose levels are high and can lead to diabetes\n2. Presence of family history does not necessarily lead to diabetes but person can be at risk\n3. High Systolic blood pressure in long term can also lead to increase in glucose level as we can observe it from graph\n4. Skin thickness can increase BMI and in turn becomes tolerant to insulin action due to which insulin effect on glucose level can decrease. This in long term can lead to diabetes and therefore it is said that BMI should be maintained.\n5. Insulin levels are increased in cases where skin thickness is more****","84bb0f14":"# What we will see in this notebook:\n1. analysis on the dataset to understand each independent variable and its relationship with the target variable\n2. necessary preprocessing of data\n3. Application of  algorithms and select the best among them\n","451b8e2c":"# Building models","7d938305":"**Splitting of data into train and test**","5ede451b":"**skin thickness is more if insulin content is more**","a954682f":"**Here we see imbalance in the Target variable and we need to treat it before applying any algorithm**","7fe54ba0":"**Long term High systolic blood pressure can increase glucose level**","008c709a":"**The performance of models can further be improved by Hyperparameter Tuning and we might get other model which can perform better than Logistic Regression**\n\n**Work in progress....**","3862c050":"**This column also seems to be skewed**","b94173c8":"**ROC curve**","b5246da7":"**Again predicting results after treatment of imbalance**","ea032c0b":"**This model also does not perform well as compared to logistic regression**","742a55aa":"**Since target variable is imbalanced, Applying smote for its treatment**","d10b182d":"**Insulin column seems to be skewed. We need to check it further..**","a4f478a6":"**Logistic Regression (Baseline Model)**","672ecd8d":"**Decision Tree Classifier**","758f095a":"# load libraries and read data","36bf8414":"\n**From the above application of models we see that Random forest performs the best as its f1 score is the best among all the models.\n**For this type of dataset, we need our model to perform well to identify positive case and thus accuracy becomes secondary metric of secondary importance**","b8d4491a":"# Univariate Analysis of Data","13bd93c5":"**If you find the notebook useful then provide feedback and do suggest for any sort of improvements which can be made. I am a beginner and would like to get any suggestions for further enhancement of my knowledge**","376a6e7c":"**Check for null values**","e7682b1a":"**If normal levels of Insulin are maintained in blood then glucose level is also low**","7a52e2ca":"**This model is not performing well as compared to logistic regression**","612d4de5":"**Random Forest**","79ebc66d":"**Though accuracy from the baseline model is little less but precision has improved very much. In this type of dataset where we need to be more precise about the early detection of positive case, accuracy becomes less important. Thus we need to understand what our model building purpose is.**","e317f8be":"**Most Observations in Glucose column lies within the limits of 100-150**","171bc345":"**Systolic Blood Pressure is also Normal**","9c791414":"**Presence of Family history of diabetes can lead to rise inglucose levels but not necessarily. This can be seen in the above plot**","7d81326d":"**XGBoost classifier**","4d88747d":"**so we get an accuracy of around 76% with precision of positive case as 72%. let's see if we can improve it**","79d6701c":"# Relationship between variables"}}