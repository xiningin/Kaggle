{"cell_type":{"33e31a7e":"code","1d537b7f":"code","f40504fd":"code","25b88813":"code","ccf7adc1":"code","eec1bfeb":"code","b04e5097":"code","b9638790":"code","d9f66278":"code","4ab5f50a":"code","f23ab3c1":"code","ebfb61ac":"code","b439b966":"code","2620702b":"code","b807af42":"code","2c154705":"code","02c0fad9":"code","92240625":"code","91bdb6ed":"code","3c72f0cc":"code","4a825f0e":"code","ce14289b":"code","2e1b3496":"code","98c4f786":"code","de6719db":"code","339b7b71":"code","96aea503":"code","5d6ef499":"code","820c8dd5":"code","4a9e8d33":"code","7dd917f0":"code","3987a8bb":"code","0aeca3e0":"code","85b67efd":"code","1183e7eb":"code","af149377":"code","b28bc562":"markdown","41adf353":"markdown","e379337b":"markdown","b4f7df99":"markdown","f8fbb6c1":"markdown","99d21594":"markdown","a456cf7c":"markdown","291e3bb3":"markdown","62cb7595":"markdown","f385ed65":"markdown","603cb8f2":"markdown","cf82e2a7":"markdown","5f76c529":"markdown","34076435":"markdown","e3d53fbe":"markdown","7a2c7fbd":"markdown","3fee7201":"markdown","38745ce2":"markdown","31bb8a9a":"markdown","259d30ee":"markdown","21628beb":"markdown","483d83dd":"markdown","faaf6885":"markdown","95541bdc":"markdown","6f021ad5":"markdown","1502f489":"markdown","e544ced5":"markdown","1cf7fc30":"markdown","03a5c729":"markdown","2a662d30":"markdown","da23f252":"markdown","81da7d90":"markdown","3603c98a":"markdown","939471c3":"markdown","ab3d8227":"markdown"},"source":{"33e31a7e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1d537b7f":"data = pd.read_csv('\/kaggle\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv')\ndata.head(3)","f40504fd":"data.isnull().sum()","25b88813":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nnonan_data = data.dropna().reset_index(drop=True)\n\nhouse_specs = ['Rooms', 'Type', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt']\ndata_hs = nonan_data[house_specs]\n\nfig, ax = plt.subplots(2, 2, figsize=(17, 12))\nsns.boxplot(data=data_hs.drop(['Landsize', 'BuildingArea', 'YearBuilt'], axis=1), ax=ax[0, 0])\nsns.boxplot(data=data_hs['Landsize'], ax=ax[0, 1])\nax[0, 1].set_title('The Landsize Boxplot')\nsns.boxplot(data=data_hs['BuildingArea'], ax=ax[1, 0])\nax[1, 0].set_title('The Building Area Boxplot')\nsns.boxplot(data=data_hs['YearBuilt'], ax=ax[1, 1])\nax[1, 1].set_title('The Year Built Boxplot')\nplt.tight_layout","ccf7adc1":"data.columns","eec1bfeb":"col_train = ['Suburb', 'Rooms', 'Method', 'Date', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n             'YearBuilt', 'Lattitude', 'Longtitude', 'Distance', 'Price']\ndf_train = data[col_train].dropna().reset_index(drop=True)\ndf_train['Date'] = pd.to_datetime(df_train['Date'])\n\ndf_train['year'] = [x.year for x in df_train['Date']]\ndf_train['month'] = [x.month for x in df_train['Date']]","b04e5097":"df_train.head(3)","b9638790":"suburbs = list(df_train.Suburb.value_counts().index)\nrep_cols = col_train\nfor r in ['Method', 'Date', 'Suburb']:\n    rep_cols.remove(r) \nstat_cols = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\nend_cols = [1, 1, 1, 1, 1, 0, 0, 0, 1, 1]","d9f66278":"def search_vals(dx, x, z):\n    dx_mean = dx.mean()\n    dx_mode = dx.mode()[0]\n    \n    q1 = dx.quantile(0.25)\n    q3 = dx.quantile(0.75)\n    iqr = q3 - q1\n    \n    upper = q3 + 1.5 * iqr\n    lower = q1 - 1.5 * iqr\n    \n    if x == 0:\n        y = dx_mean\n    else:\n        y = dx_mode\n        \n    if (z == 1) & (lower < 0):\n        lower = 0\n        \n    if z == 1:\n        y = int(y)\n        upper = int(upper)\n        lower = int(lower)\n    \n    return round(y, 2), upper, lower","4ab5f50a":"replacement = {}\nfor s in suburbs:\n    app = {}\n    for c, i, j in zip(rep_cols, stat_cols, end_cols):\n        app[c] = list(search_vals(df_train[df_train['Suburb'] == s][c], i, j))\n    replacement[s] = app","f23ab3c1":"for i in range(4):\n    print(suburbs[i], '\\n', replacement[suburbs[i]], '\\n')","ebfb61ac":"dx_suburb = df_train['Suburb']\nfor c in rep_cols:\n    changed = []\n    for i, s in enumerate(dx_suburb):\n        v = df_train.loc[i, c]\n        u = replacement[s][c][1]\n        l = replacement[s][c][2]\n        if v > u:\n            v = u\n        elif v < l:\n            v = l\n        changed.append(v)\n    df_train[c] = changed","b439b966":"df_train.head(3)","2620702b":"from sklearn.preprocessing import MinMaxScaler\n\nx_cols = ['Rooms', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt',\n          'Lattitude', 'Longtitude', 'Distance', 'year', 'month']\ntrain_x = df_train[x_cols]\ntrain_y = df_train['Price']\n\nscaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n\nnew_train_x = scaler_x.fit_transform(train_x)\nnew_train_y = scaler_y.fit_transform(np.array(train_y).reshape(-1, 1))","b807af42":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nx_train, x_valid, y_train, y_valid = train_test_split(new_train_x, new_train_y, test_size=0.2)\n\nrf_model = RandomForestRegressor()\nrf_model.fit(x_train, y_train)\n\ny_pred = rf_model.predict(x_valid) \nerror = np.sqrt(mean_squared_error(y_pred, y_valid))\nerror","2c154705":"data.head(3)","02c0fad9":"da_suburb = data['Suburb'].value_counts().index\nnot_in_the_list = []\nfor d in da_suburb:\n    if d not in suburbs:\n        not_in_the_list.append(d)\nprint('The suburb that is not in the list :')\nfor i, s in enumerate(not_in_the_list):\n    print((i+1), s)","92240625":"len(data[data.Suburb.isin(not_in_the_list)])","91bdb6ed":"drop_ind = data[data.Suburb.isin(not_in_the_list)].index\ndata = data.drop(drop_ind)\ndata = data.reset_index(drop=True)\ndata = data.drop(['Bedroom2'], axis=1)","3c72f0cc":"dx_suburb = data['Suburb']\nfor c in rep_cols:\n    changed = []\n    for i, s in enumerate(dx_suburb):\n        v = data.loc[i, c]\n        u = replacement[s][c][1]\n        l = replacement[s][c][2]\n        if v > u:\n            v = u\n        elif v < l:\n            v = l\n        changed.append(v)\n    data[c] = changed","4a825f0e":"dfs = data['Suburb']\nrep_cols.remove('Price')\n\nfor c in rep_cols:\n    dfc = data[c]\n    null = dfc.isnull()\n    changed = []\n    for i, d, s in zip(null, dfc, dfs):\n        if  i == True:\n            d = replacement[s][c][0] \n        changed.append(d)\n    data[c] = changed","ce14289b":"data.isnull().sum()","2e1b3496":"data['Date'] = pd.to_datetime(data['Date'])\ndata['year'] = [x.year for x in data['Date']]\ndata['month'] = [x.month for x in data['Date']]","98c4f786":"from tqdm.auto import tqdm\n\nprice_null = data['Price'].isnull()\nx_ = scaler_x.transform(data[x_cols])\nnew_ = []","de6719db":"progress_bar = tqdm(range(len(data)))\n\nfor n, i, p in zip(price_null, x_, data['Price']):\n    if n == True:\n        p = rf_model.predict(i.reshape(1, -1))\n        p = scaler_y.inverse_transform(p.reshape(-1, 1))[0, 0]\n    new_.append(p)\n    progress_bar.update(1)\n    \ndata['Price'] = new_","339b7b71":"data.isnull().sum()","96aea503":"data = data.dropna()\ndata.head(3)","5d6ef499":"xr = []\nfor i in data['Type']:\n    if i == 'h':\n        xr.append('House')\n    elif i == 't':\n        xr.append('Townhouse')\n    elif i == 'u':\n        xr.append('Duplex')\ndata['Type'] = xr","820c8dd5":"da = data[['Type', 'Date', 'Price']]\ndag = da.groupby(by=['Type', 'Date']).mean()\n\nnew_dag_type = []\nnew_dag_date = []\nnew_dag_price = []\n\nfor i, j in zip(dag.index, dag.Price):\n    a, b = i\n    new_dag_type.append(a)\n    new_dag_date.append(b)\n    new_dag_price.append(j)\n    \nnew_dag = pd.DataFrame({'type':new_dag_type, 'date':new_dag_date, 'price':new_dag_price})","4a9e8d33":"plt.figure(figsize=(13, 7))\nsns.lineplot(data=new_dag, x='date', y='price', hue='type')\nplt.tight_layout()","7dd917f0":"dl = data[['Regionname', 'Suburb', 'Price', 'Lattitude', 'Longtitude']].groupby(by=['Regionname', 'Suburb']).mean()\ndl_region = [i for i, j in dl.index]\ndl_suburb = [j for i, j in dl.index]\n\ndl['Region'] = dl_region\ndl['Suburb'] = dl_suburb\ndl = dl.reset_index(drop=True)\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 16))\nsns.scatterplot(data=data, x='Lattitude', y='Longtitude', hue='Type', size='Rooms', ax=ax[0])\nax[0].set_title('The House Location')\nsns.scatterplot(data=dl, x='Lattitude', y='Longtitude', hue='Region', ax=ax[1])\nax[1].set_title('The Suburb Location')\nplt.tight_layout()","3987a8bb":"data.Regionname.value_counts()","0aeca3e0":"da = data[['Regionname', 'Date', 'Price']].groupby(by=['Regionname', 'Date']).mean()\nda_region = []\nda_date = []\nda_price = []\n\nfor i, j in zip(da.index, da['Price']):\n    a, b = i\n    da_region.append(a)\n    da_date.append(b)\n    da_price.append(j)\n    \nnew_da = pd.DataFrame({'region':da_region, 'date':da_date, 'price':da_price})","85b67efd":"metropolitan = ['Southern Metropolitan', 'Northern Metropolitan', 'Western Metropolitan',\n                'Eastern Metropolitan', 'South-Eastern Metropolitan']\nvictoria = ['Eastern Victoria', 'Northern Victoria', 'Western Victoria']\n\nfig, ax = plt.subplots(2, 1, figsize=(11, 15))\nsns.lineplot(data=new_da[new_da.region.isin(metropolitan)], x='date', y='price', hue='region', ax=ax[0])\nax[0].set_title('Price Fluctuation in Metropolitan Area')\nsns.lineplot(data=new_da[new_da.region.isin(victoria)], x='date', y='price', hue='region', ax=ax[1])\nax[1].set_title('Price Fluctuation in Victoria Area')","1183e7eb":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import ExtraTreesRegressor\nimport scipy\n\nohe = OneHotEncoder()\ntypes = ohe.fit_transform(np.array(data['Type']).reshape(-1, 1))\ntypes_np = scipy.sparse.csr_matrix.toarray(types)\ntype_cats = [x.split('_')[1] for x in list(ohe.get_feature_names())]\ntype_df = pd.DataFrame(types_np, columns=type_cats)\n\nmmf_scaler = MinMaxScaler()\ndf_f = data[rep_cols]\ndf_f = pd.DataFrame(mmf_scaler.fit_transform(df_f), columns=rep_cols)\ndf_f = pd.concat([df_f, type_df], axis=1)\n\nmmf_ = MinMaxScaler()\nprice_scaled = mmf_.fit_transform(np.array(data['Price']).reshape(-1, 1))\n\nthe_tree = ExtraTreesRegressor()\nthe_tree.fit(df_f, price_scaled)","af149377":"plt.figure(figsize=(10, 6))\nsns.barplot(x=df_f.columns, y=the_tree.feature_importances_, palette=\"Set2\")\nplt.xticks(rotation=45)\nplt.title('Attributes of House and Its Contribution to Its Price')\nplt.tight_layout()","b28bc562":"The Square root of MSE is 0.047 so the prediction doesn't look bad. We can use this model to fill price values with NaN","41adf353":"## Most Important Attributes","e379337b":"Now we will fill the NaN value in price using **Random Forest Model** that we already made before. ","b4f7df99":"As we can see above, location is very important to a price. **The closer a house to the center of the city, the more costly the house would be**. **After its location, the next attribute that is almost that important is its Building area**.","f8fbb6c1":"These are some takeaways from this notebook : \n\n1. **The most important factor that contributes to the price of a house are its distance to the city center and its building area.**\n2. **The price of the house always decline around January and February.**\n3. **If you are looking for a cheap house located close to Melbourne City Business District, Western Metropolitan is the cheapest region. But if you are really looking for the cheapest house in Melbourne and doesn't really care about its distance to CBD, then Western Victoria is the place for you my friend.**","99d21594":"## Filling NaN Price","a456cf7c":"## Making Dataframe for replacement Values","291e3bb3":"## Train Machine Learning Model to Fill NaN Price","62cb7595":"# **I. Import Data & Libraries**","f385ed65":"If you wanted to look closely, at the Victoria area, ther are only a few numbers of houses with the type of Duplex or Townhouse. Most of the type sold here are House type.","603cb8f2":"## Price Fluctuation in Each Region","cf82e2a7":"*You can also see my visualization on the final data (the data that I already edited) using Tableau Public here.\nhttps:\/\/public.tableau.com\/views\/MelbourneHousing_16306641330900\/Dashboard2?:language=en-US&:display_count=n&:origin=viz_share_link*","5f76c529":"There are only 179 data. We can delete all this data.","34076435":"Now we will fill all the missing values using the replacement values","e3d53fbe":"At the **Metropolitan area**, the list of region with the most expensive to the cheapest houses are :\n1. Southern Metropolitan - spike at October\/ November 2017\n2. Eastern Metropolitan\n3. South-Eastern Metropolitan - Became same as 4 and 5 around June\/July 2017\n4. Northern Metropolitan\n5. Western Metropolitan\n\nThere are two things that we can note here:\n1. The price of the house at **Southern Metropolitan** spiked around October - November 2017. Buying property around this time isn't really a good idea.\n2. The price of the house at **South-Eastern Metropolitan** got lower and became the same as Northern and Western Metropolitan started from June - July 2017\n\nAt the Victoria area, the list of region with the most expensive to the cheapest houses are :\n1. Eastern Victoria\n2. Northern Victoria\n3. Western Victoria\n\nAll of these three regions have houses that are sold with lower price than Western Metropolitan. ","7a2c7fbd":"Now, we will see how the price fluctuated in regards of their regions. We can classify the regions to two big groups, Metropolitan and Victoria. **Metropolitan** is the region that is located closer to city center compared to **Victoria**, as you can see at the graph from previous section. ","3fee7201":"There are 36 suburbs that we haven't found in training dataset. That's quite many. Let's see how many houses sold in this suburbs exist in our data. ","38745ce2":"In this data, we will prepare the data for training step of **Random Forest Model**.","31bb8a9a":"## Editing Outlier on Train Data","259d30ee":"In this step, we will find the replacement values, which is the value that we will use to replace the NaN or outlier data using *close approximation*. This replacement values are *upper extreme, lower extreme* and *mode* or *mean*. ","21628beb":"## Making Train Data","483d83dd":"## Finding the Amount of NaN & Outlier","faaf6885":"## Editing All Outlier Data","95541bdc":"## Price Fluctuation of Each Type of House","6f021ad5":"# **IV. Conclusions**","1502f489":"## Filling NaN Values Except Price","e544ced5":"# **II. Cleaning the Data**","1cf7fc30":"## House Locations","03a5c729":"### Replacement Values Example","2a662d30":"For your information, Region (represented by Regionname on the data) is the bigger than Council. Council is a part of Region, and Suburb is the part of Council. Suburb data itself, doesn't have NaN values at all so it's perfect to use this data for *close approximation*.","da23f252":"# **III. Data Analysis**","81da7d90":"Now, we will see how each factor contributes to the price of a house. The factors that we will be considering are : \n1. Rooms\n2. Bathroom\n3. Parking car\n4. Land size\n5. Building Area\n6. Location\n7. Year Built\n8. Distance to the city center\n9. House type\n\nWe will use **Extra Trees** to determine how each of these 9 factors impact the price.","3603c98a":"# **Table of Contents**\n**I. Import Data & Libraries**\n\n**II. Cleaning the Data**\n* Finding the Amount of NaN & Outliers\n* Making Train Data\n* Making Dataframe for replacement Values\n* Editing Outlier on Train Data\n* Train Machine Learning Model to Fill NaN Price\n* Editing All Outlier Data\n* Filling NaN Values Except Price\n* Filling NaN Price\n\n**III. Data Analysis**\n* Price Fluctuation of Each Type of House\n* House Locations\n* Price Fluctuation in Each Region\n* Most Important Attributes\n\n**IV. Conclusions**","939471c3":"Well, we all know that cleaning the data is tricky. That's why it's important to make a plan before doing that. Sometimes, even with a good planning, you just throw away the plan out of the window because of the fact that your data is too messy. But, there's nothing wrong with trying isn't it.\n\nWhat I wanna do first here is finding how many NaN values that we have. I will fill this NaN values with what I call as a *close approximation*, which basically just a data from different rows but has similarity in some feature. In this case, If there's a missing data in the number of bedroom, I will look up for what suburb that house is located then I will fill the NaN with the mode of bedroom in that suburb.\n\nThe same thing is also applied for outlier. I will use *close approximation* for outlier and replace them with upper extreme if the value is higher or lower extreme the value is lower.\n\nThis fixation on NaN and outlier values are applied to all columns, except price. We will predict the price with Random Forest because of the accuracy that this algorithm has. Also, The data that we have is also not many, the training time wouldn't be any problem.","ab3d8227":"As you can see above, House is the most expensive one, then Townhouse, then Duplex. The price of both Duplex and Townhouse was quite stable and consistent. \n\n**You can say that along 2017 the price is quite unpredictable. Then, at every beginning of the year, the price of the houses declined.**"}}