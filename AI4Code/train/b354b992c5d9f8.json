{"cell_type":{"6ed95d41":"code","b5328f68":"code","dbfd7a0f":"code","11533c98":"code","e750ddcf":"code","62668f93":"code","acb2cefa":"code","d34401b6":"code","3acd70a0":"markdown","6d64b539":"markdown","f9ff6166":"markdown","ec6ac961":"markdown","6a45cd58":"markdown","e5218898":"markdown","9c368fc3":"markdown","dbde7120":"markdown"},"source":{"6ed95d41":"import numpy as np  \nimport pandas as pd  \n\nfrom keras.layers import Dense,Input,Bidirectional,Conv1D,GRU\nfrom keras.layers import Embedding,GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD\n\nfrom sklearn.model_selection import train_test_split \n\nimport spacy","b5328f68":"glove_file = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\ntrain_file = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntest_file = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\n\nX_train = train_file[\"comment_text\"].str.lower()\nX_test = test_file[\"comment_text\"].str.lower()\n\ny_train = train_file[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values","dbfd7a0f":"max_features=150000\nmaxlen=200\nembed_size=300\n\ntok=text.Tokenizer(num_words=max_features,lower=True)\ntok.fit_on_texts(list(X_train)+list(X_test))\nX_train=tok.texts_to_sequences(X_train)\nX_test=tok.texts_to_sequences(X_test)\nx_train=sequence.pad_sequences(X_train,maxlen=maxlen)\nx_test=sequence.pad_sequences(X_test,maxlen=maxlen)","11533c98":"embeddings_index = {}\ncounter = 0\nwith open(glove_file,encoding='utf8') as f:\n    for line in f:\n        counter += 1\n        if (counter%1000000)==0:\n            print(counter)\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","e750ddcf":"nlp = spacy.load('en');\n\nword_index = tok.word_index\n\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.random.randn(num_words, embed_size)\/4\nkk = 0\nmoo = 0\nfor word, i in word_index.items(): \n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        kk += 1\n    else: \n        for x in nlp(word,disable=['parser', 'ner']):\n            embedding_vector = embeddings_index.get(x.lemma_)\n            if embedding_vector is not None: \n                embedding_matrix[i] = embedding_vector \n                kk += 1\n                break\n","62668f93":"model_input = Input(shape=(maxlen, )) \nx = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(model_input)\nx = SpatialDropout1D(0.1)(x)\nx = Bidirectional(GRU(200, return_sequences=True,dropout=0.25,recurrent_dropout=0.25,implementation=1))(x)\nx = Conv1D(128, kernel_size = 3)(x)   \navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool])   \npreds = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(model_input, preds)  \nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=2e-4),metrics=['accuracy'])\n","acb2cefa":"model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1)","d34401b6":"y_pred = model.predict(x_test,batch_size=1024,verbose=1)\n\nsubmission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('submission.csv', index=False)","3acd70a0":"We will use the pre-trained GLOVE embedding, so load that in along with the test and training data.","6d64b539":"Finally, we fit the model to the entire test set, and run for 10 epochs. Each epoch, on a Kaggle CPU, takes around half an hour.","f9ff6166":"We next make our prediction and save the output.","ec6ac961":"We next map each of the tokens to the embedding vector. If a word is not found then we use the Spacy library to lemmatize the word, and see if we can find that instead. If we still can't find it, then we replace it with a random vector.","6a45cd58":"We assemble the embedding matrix by looping through GLOVE and adding the vector to the dictionary (2.2M words).","e5218898":"We give a brief notebook below for classifying toxic wikipedia comments (mislabelled above as Twitter comments).\n\nWe first load in some appropriate libraries, using _Keras_ for the heavy lifting.","9c368fc3":"We then tokenize each of the test and training entries.\n\nThe embedding is 300 dimensional, but we have options for the maximum length of comments, and the max number of features we shall include. Here we use 150k features and cap the length to 200 words.","dbde7120":"Now we define the model. We use the following:\n\n* Glove embedding layer\n* Dropout layer\n* Bidirectional GRU layer\n* 1D Convolution layer \n* Average & Max pooling layer\n* Dense layer \n* Six category output, using binary cross entropy\n\nSome experimentation has gone into parameterizing the model, but due to a lack of resources it can be likely parameterized further."}}