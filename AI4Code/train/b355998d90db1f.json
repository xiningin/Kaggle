{"cell_type":{"7708ecec":"code","8b9788dc":"code","fd0d4215":"code","967310ca":"code","32307854":"code","bd1d7615":"code","104e557c":"code","7edc1bbd":"code","6fda1d66":"code","547e19dc":"code","226f1802":"code","bec8e5f8":"markdown","e10e4580":"markdown","b0bde29e":"markdown","9115c7f8":"markdown","1cf6acc8":"markdown","f57b6623":"markdown","2b310b87":"markdown","03cb179f":"markdown","641251fa":"markdown","4a9d2645":"markdown","c8143029":"markdown","db4771e8":"markdown"},"source":{"7708ecec":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n","8b9788dc":"from sklearn.datasets import make_classification\n\n\n\nX, y = make_classification(n_samples=50000,n_features=8,\n\n                           n_informative=8,n_redundant=0,\n\n                           n_clusters_per_class=2, random_state = 26)","fd0d4215":"print(type(X), type(y))\nprint(len(X[1]), X[1], y[1])\nprint(\"Shape : {}, {}\".format(X.shape, y.shape))","967310ca":"import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ncolors=['black','yellow']\n\ncmap = matplotlib.colors.ListedColormap(colors)\n\n#Plot the figure\n\nplt.figure()\n\nplt.title('Non-linearly separable classes')\n\nplt.scatter(X[:,0], X[:,1], c=y,\n\n            marker= '*', s=50, cmap=cmap, alpha = 0.5 )\n\nplt.savefig('fig1.png', bbox_inches='tight')","32307854":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.25,random_state = 25)\n\nprint(\"shape of X_train:{} shape 0f Y_train:{}\".format(X_train.shape, Y_train.shape))  \nprint(\"shape of X_test:{} shape 0f Y_test:{}\".format(X_test.shape, Y_test.shape))  ","bd1d7615":"X_train = X_train.T\n\nY_train = Y_train.reshape(1, len(Y_train))\n\nX_test = X_test.T\n\nY_test = Y_test.reshape(1, len(Y_test))\n\nprint(\"shape of X_train:{} shape 0f Y_train:{} after transformation\".format(X_train.shape, Y_train.shape))\n","104e557c":"import tensorflow as tf\n\ndef placeholders(num_features):\n\n  A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n\n  Y = tf.placeholder(dtype = tf.float64, shape = ([1,None]))\n\n  return A_0,Y","7edc1bbd":"def initialiseParameters(num_features, num_nodes):\n\n  W1 = tf.Variable(initial_value=tf.random_normal([num_nodes,num_features], dtype = tf.float64) * 0.01)\n\n  b1 = tf.Variable(initial_value=tf.zeros([num_nodes,1], dtype=tf.float64))\n\n  W2 = tf.Variable(initial_value=tf.random_normal([1,num_nodes], dtype=tf.float64) * 0.01)\n\n  b2 = tf.Variable(initial_value=tf.zeros([1,1], dtype=tf.float64))\n\n  return {\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}  ","6fda1d66":"def forward_propagation(A_0,parameters):\n\n  Z1 = tf.matmul(parameters[\"W1\"],A_0) + parameters[\"b1\"]\n\n  A1 = tf.nn.relu(Z1)\n\n  Z2 = tf.matmul(parameters[\"W2\"],A1) + parameters[\"b2\"]\n\n  return Z2","547e19dc":"def shallow_model(X_train,Y_train,X_test,Y_test, num_nodes, learning_rate, num_iter):\n\n  num_features = X_train.shape[0]\n\n  A_0, Y = placeholders(num_features)\n\n  parameters = initialiseParameters(num_features, num_nodes)\n\n  Z2 = forward_propagation(A_0, parameters)\n\n  cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2,labels=Y))\n\n  train_net = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n  init = tf.global_variables_initializer()\n\n  \n\n  with tf.Session() as sess:\n\n    sess.run(init)\n\n    for i in range(num_iter):\n\n      _,c = sess.run([train_net, cost], feed_dict={A_0: X_train, Y: Y_train})\n\n      if i % 1000 == 0:\n\n        print(c)\n\n    correct_prediction = tf.equal(tf.round(tf.sigmoid(Z2)), Y)\n\n    # Calculate accuracy\n\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n    print(\"Accuracy on test set:\", accuracy.eval({A_0: X_test, Y: Y_test}))","226f1802":"shallow_model(X_train, Y_train, X_test, Y_test, 8, 0.2, 10000)\n","bec8e5f8":"## Program Flow\nInitialize the placeholders and parameters using the function defined.\n\nCall forward propagation function and assign the final output to Z2.\n\nUsing Z2 and actual output Y, calculate cost function using sigmoid_cross_entropy function.\n\nCall GradientDescetOptimizer to update parameters such that it minimizes cost.\n\nInitialise TensorFlow session.\n\nRun the session by passing the values that are to be evaluated and returned. Fill the place holders with actual train data.\n\nRun the above step for number of times till there is no significant change in the cost value.\n\nThe .eval() function inside the session uses update parameters of W and b to calculate the accuracy of the model against test data.\n\n***\n\n## Implementing the Model\nThe model below creates a network with one hidden layer with 8 nodes. Let the learning rate be 0.2 and number of iterations be 10000.\n\nAt every 1000 iterating you can see that cost value is approaching close to zero.\n\nThe model is able to classify the test data set with 95% accuracy.\n\n","e10e4580":"## Defining the Model\nNow it's time to define the model which runs the tensorflow session which trains the network using the train data and calculates the accuracy agains the test data.","b0bde29e":"### Getting Dimensions Right\nIn order to feed the neural network, the shape of the data has to be in the form (num_features, num_samples).\n\nYou can transpose the data as shown in the below code snippet to get the right dimensions.\n\nSince the Y data is in the form, we use a list - reshape() function to convert it into a row vector.","9115c7f8":"## Initializing Parameters\n* You now need to initialize parameters, weights, and bias for hidden layer as well as output layer.\n\n* Before training, the parameters are initialized to some random values.\n\n* Usually weights are initialized with values that are normally distributed.\n\n* To initialize weights use tf.random_normal() and pass appropriate dimensions.\n\n* bias can be initialized to zero using tf.zeros() function.\n\n* the parameter num_nodes in below code represents number of parameters","1cf6acc8":"we will be using the TensorFlow framework to implement a neural network. \nThe best part of this framework is that it provides an implementation for backpropagation.   \nThere is no need to explicitly calculate derivatives.   \nIn this code snippet, you will see how to use TensorFlow to implement and train a shallow neural network.","f57b6623":"## Applying Gradient Descent\nOnce the cost function is defined, its time to find the derivatives of loss with respect to parameters W and b.\n\nThis is done in one step in TensorFlow as:\n* This performs a single step of gradient descent that internally calculates the derivatives and updates the parameters and attempts to minimize the cost.  \n`` \ntrain_net = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) ``  \n\n","2b310b87":"## Visualizing the Data\n\n* You can visualize the data using the below code snippet. Here only first two features are used for visualization since it is a 2D plot.\n\n* Here zeros are classified as black spots and ones are classified as yellow spots","03cb179f":"## Forward Propagation\nNow we have place holders as well as parameters in place.\n\nThe forward propagation is implemented as follows:\n\nA1 is the output of hidden layer, here relu activation function is used for hidden layer. You can also use tanh function using tf.nn.tanh() function available in TensorFlow.\n\nYou can see that activation function is not applied for the final output. The activation function will be internally applied in cost function which you will see in backpropagation.\n\n","641251fa":"## Defining Placeholders\n\n* Placeholders are meant to pass external data into the network; it may be either train or test data.\n* The placeholders are defined as shown in the code below.\n* placeholders must be of same shape as the input data. None is passed when you are not aware of number of samples in advance.","4a9d2645":"## About the Data\nAs you have seen from the plot, the data is not linearly separable. i.e., simple logistic regression may not classify this data accurately as no clear linear boundary separates the data.\n\nLet's see how accurately the shallow neural network classifies this data.\n***\n## Train Test Split\nIn order to test the accuracy of your model, you need to split the data into train and test sets.\n\nYou use the train set to train the model and test the set to test the accuracy of a trained model.","c8143029":"## Generating Data\nWe can generate our own data using scikit-learn make_classification function.  \nThe below code snippet generates random records each having eight features.  \nEach record is mapped either to 0 or 1.  \nIn total let's generate 50000 records.  ","db4771e8":"## Defining Cost Function\nYou now know the definition of loss function L(a,y).\n\nTensorFlow has inbuilt cost function called sigmoid_cross_entropy that provides the same implementation.\n\nThe only difference is that we are passing nonactivated output from final layer as Z2 that internally gets activated by Sigmoid function.\n\nLabels Y is the actual output as per the data\n\nFinally, the mean value of the loss is calculated using tf.reduce_mean().  \n\n`` cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2,labels=Y)) ``"}}