{"cell_type":{"cb80fd78":"code","19df6b95":"code","ca20f235":"code","4fd7e73e":"code","708450b7":"code","73dc1afd":"code","a0c53072":"code","cf8c949b":"code","24adf5bc":"code","26e5ffec":"code","49cee687":"code","2b7082e8":"code","ae42752e":"code","184ea0f1":"code","e4810060":"code","ac30e37b":"code","605f428f":"code","4f18e3be":"code","978b72ab":"markdown"},"source":{"cb80fd78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19df6b95":"# re-arrange the dataset\n!mkdir \/kaggle\/working\/testset\nfl = open('\/kaggle\/input\/food41\/meta\/meta\/classes.txt')\ncls = fl.readline()\nwhile(cls):\n    !mkdir \/kaggle\/working\/testset\/{cls}\n    cls = fl.readline()\n# Moving test files to testset\/, train files will be left.\ntestfile = open('\/kaggle\/input\/food41\/meta\/meta\/test.txt')\nimg = testfile.readline().strip()\nwhile(img):\n    cls = img.split('\/')[0]\n    !mv \/kaggle\/input\/food41\/images\/{img}.jpg \/kaggle\/working\/testset\/{cls}\/\n    print(f'\\r{img}',end='')\n    img = testfile.readline().strip()","ca20f235":"!nvidia-smi","4fd7e73e":"!pip install torch-lr-finder","708450b7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.models.resnet import resnet50","73dc1afd":"train_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.ColorJitter(brightness=0.1,contrast=0.1,saturation=0.1),\n        torchvision.transforms.RandomAffine(15),\n        torchvision.transforms.RandomHorizontalFlip(),\n        torchvision.transforms.RandomRotation(15),\n        torchvision.transforms.Resize((224,224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\nvalid_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((224,224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n])","a0c53072":"train_dataset = torchvision.datasets.ImageFolder('food-101\/images\/',transform=train_transforms)\nvalid_dataset = torchvision.datasets.ImageFolder('food-101\/testset\/',transform=valid_transforms)","cf8c949b":"batch_size = 128\ntrain_loader = torch.utils.data.DataLoader(train_dataset,batch_size,shuffle=True,num_workers=4,pin_memory=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size,shuffle=False,num_workers=4,pin_memory=True)","24adf5bc":"def visualize_images(dataloader):\n    mean=np.array([0.485, 0.456, 0.406])\n    std=np.array([0.229, 0.224, 0.225])\n    figure, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 14))\n    classes = list(dataloader.dataset.class_to_idx.keys())\n    img_no = 0\n    for images,labels in dataloader:\n        for i in range(3):\n            for j in range(3):\n                img = np.array(images[img_no]).transpose(1,2,0)\n                lbl = labels[img_no]\n\n                ax[i,j].imshow((img*std) + mean)\n                ax[i,j].set_title(classes[lbl])\n                ax[i,j].set_axis_off()\n                img_no+=1\n        break","26e5ffec":"visualize_images(train_loader)","49cee687":"visualize_images(valid_loader)","2b7082e8":"model = resnet50(pretrained=True)","ae42752e":"model","184ea0f1":"# Freeze first few layers. You can try different values instead of 100\nfor i,param in enumerate(model.parameters()):\n    if i<100:\n        param.requires_grad=False","e4810060":"model.fc = torch.nn.Sequential(\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(2048,101)\n)","ac30e37b":"from torch_lr_finder import LRFinder\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\nlr_finder.range_test(train_loader, end_lr=0.001, num_iter=25)\nlr_finder.plot()\nlr_finder.reset()","605f428f":"%reload_ext tensorboard\n%tensorboard --logdir runs","4f18e3be":"cuda = True\nepochs = 10\n# model_name = '\/content\/drive\/My Drive\/resnet50.pt'\nmodel_name = '\/content\/drive\/MyDrive\/resnet50_new.pt'\noptimizer = torch.optim.Adam(model.parameters(),lr=4e-5,weight_decay=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=1,verbose=True)\n\nwriter = SummaryWriter() # For Tensorboard\nearly_stop_count=0\nES_patience=5\nbest = 0.0\nif cuda:\n    model.cuda()\n\nfor epoch in range(epochs):\n    \n    # Training\n    model.train()\n    correct = 0\n    train_loss = 0.0\n    tbar = tqdm(train_loader, desc = 'Training', position=0, leave=True)\n    for i,(inp,lbl) in enumerate(tbar):\n        optimizer.zero_grad()\n        if cuda:\n            inp,lbl = inp.cuda(),lbl.cuda()\n        out = model(inp)\n        loss = criterion(out,lbl)\n        train_loss += loss\n        out = out.argmax(dim=1)\n        correct += (out == lbl).sum().item()\n        loss.backward()\n        optimizer.step()\n        tbar.set_description(f\"Epoch: {epoch+1}, loss: {loss.item():.5f}, acc: {100.0*correct\/((i+1)*train_loader.batch_size):.4f}%\")\n    train_acc = 100.0*correct\/len(train_loader.dataset)\n    train_loss \/= (len(train_loader.dataset)\/batch_size)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        val_loss = 0.0\n        vbar = tqdm(valid_loader, desc = 'Validation', position=0, leave=True)\n        for i,(inp,lbl) in enumerate(vbar):\n            if cuda:\n                inp,lbl = inp.cuda(),lbl.cuda()\n            out = model(inp)\n            val_loss += criterion(out,lbl)\n            out = out.argmax(dim=1)\n            correct += (out == lbl).sum().item()\n        val_acc = 100.0*correct\/len(valid_loader.dataset)\n        val_loss \/= (len(valid_loader.dataset)\/batch_size)\n    print(f'\\nEpoch: {epoch+1}\/{epochs}')\n    print(f'Train loss: {train_loss}, Train Accuracy: {train_acc}')\n    print(f'Validation loss: {val_loss}, Validation Accuracy: {val_acc}\\n')\n\n    scheduler.step(val_loss)\n\n    # write to tensorboard\n    writer.add_scalar(\"Loss\/train\", train_loss, epoch)\n    writer.add_scalar(\"Loss\/val\", val_loss, epoch)\n    writer.add_scalar(\"Accuracy\/train\", train_acc, epoch)\n    writer.add_scalar(\"Accuracy\/val\", val_acc, epoch)\n\n    if val_acc>best:\n        best=val_acc\n        torch.save(model,model_name)\n        early_stop_count=0\n        print('Accuracy Improved, model saved.\\n')\n    else:\n        early_stop_count+=1\n\n    if early_stop_count==ES_patience:\n        print('Early Stopping Initiated...')\n        print(f'Best Accuracy achieved: {best:.2f}% at epoch:{epoch-ES_patience}')\n        print(f'Model saved as {model_name}')\n        break\n    writer.flush()\n# writer.close()","978b72ab":"\n# Note:\n\nThis Code was written on google colab and has not been tested on kaggle. There could be minor bugs in it BUT you can get decent results if you can make it run!\n"}}