{"cell_type":{"f3fc0442":"code","b13524a8":"code","5950d018":"code","cf032a0c":"code","c135aa36":"code","0ef9ab86":"code","6e1d1d92":"code","8b18198d":"code","918e2749":"code","ea2f6f08":"code","12adac87":"code","6a8ee04c":"code","a9f6f169":"code","6c5d4203":"code","4bda7b94":"code","9b11d27f":"code","cd499234":"code","c670f446":"code","66a8e484":"code","62c390a1":"code","e4b1f7e5":"code","e2a595c9":"code","d1d51ce4":"code","7406ff8b":"code","730b66cc":"code","be27cbc6":"code","6028ba4b":"code","55836737":"code","608c98f7":"code","af0ae881":"code","ae0db14d":"code","a832533b":"code","0c346e52":"code","042a2776":"code","e18f70a9":"code","29e04354":"code","47ceb9ff":"markdown","3edbb636":"markdown","e493a701":"markdown","2fb27449":"markdown","7c106d8b":"markdown","bf2a5825":"markdown","39738835":"markdown","a9b9dc6c":"markdown","c4ff51ad":"markdown","8c3a3a8b":"markdown","66039fea":"markdown","66217f96":"markdown","c00caf27":"markdown","a09729e2":"markdown","1c8cc035":"markdown","a8878df1":"markdown"},"source":{"f3fc0442":"#importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n\nimport sklearn as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n","b13524a8":"originaldata = pd.read_csv('\/kaggle\/input\/suicide-rates-overview-1985-to-2016\/master.csv')\n\noriginaldata.columns = ['country', 'year', 'sex', 'age', 'suicides_no', 'population','suicidesper100k',\n                      'country-year', 'yearlyHDI', 'GDPpyear', 'GDPpcapita', 'generation']\n\noriginaldata.head()","5950d018":"#fixing and cleaning the original data  \noriginaldata['GDPpyear'] = originaldata.apply(lambda x: float(x['GDPpyear'].replace(',', '')), axis=1)\noriginaldata.sex.astype('category')","cf032a0c":"extra_data = pd.read_csv('\/kaggle\/input\/widandsuicide\/suicidedataextrafestures.csv')\n\nextra_data.columns = [\n    'country', 'year', 'sex', 'age', 'suicides_no', 'population','suicidesper100k', 'country-year', 'yearlyHDI',\n    'GDPpyear', 'GDPpcapita', 'generation', 'suicide%', 'Internetusers', 'Expenses', 'employeecompensation',\n    'Unemployment', 'Physiciansp1000', 'Legalrights', 'Laborforcetotal', 'Lifeexpectancy', 'Mobilesubscriptionsp100',\n    'Refugees', 'Selfemployed', 'electricityacess', 'secondarycompletion']","c135aa36":"extra_data.head()","0ef9ab86":"countrynames = [\n    'Argentina',\n    'Armenia',\n    'Australia',\n    'Austria',\n    'Belgium',\n    'Brazil',\n    'Bulgaria',\n    'Canada',\n    'Chile',\n    'Colombia',\n    'Croatia',\n    'Cuba',\n    'Czech Republic',\n    'Denmark',\n    'Finland',\n    'France',\n    'Germany',\n    'Greece',\n    'Hungary',\n    'Iceland',\n    'Ireland',\n    'Israel',\n    'Italy',\n    'Japan',\n    'Mexico',\n    'Netherlands',\n    'New Zealand',\n    'Norway',\n    'Poland',\n    'Portugal',\n    'Romania',\n    'Russian Federation',\n    'South Africa',\n    'Spain',\n    'Sweden',\n    'Switzerland',\n    'Thailand', \n    'Turkmenistan',\n    'Ukraine',\n    'United Kingdom', \n    'United States']","6e1d1d92":"df1 = extra_data.copy()\ndf = df1.iloc[np.where(df1.country == countrynames[0])]\nfor i, x in enumerate(countrynames[1:]):\n    df = df.append(df1.iloc[np.where(df1.country == x)])\n\ndf = df[df.year >= 1995]\ndf = df[df.year <= 2013]","8b18198d":"col = plt.cm.Spectral(np.linspace(0, 1, 20))\n\nplt.figure(figsize=(8, 6))\n\nagedistf = pd.DataFrame(df.groupby('sex').get_group('female').groupby('age').suicides_no.sum())\n\nagedistm = pd.DataFrame(df.groupby('sex').get_group('male').groupby('age').suicides_no.sum())\n\nplt.bar(agedistm.index, agedistm.suicides_no, color=col[18])\nplt.bar(agedistf.index, agedistf.suicides_no, color=col[7])\nplt.legend(['male', 'female'], fontsize=16)\nplt.ylabel('Count', fontsize=14)\nplt.xlabel('Suicides per 100K', fontsize=14)","918e2749":"plt.figure(figsize=(12, 15))\n\n\nplt.subplot(211)\ndf.groupby(['country']).suicidesper100k.mean().nlargest(10).plot(kind='barh', color=col)\nplt.xlabel('Average Suicides\/100k', size=20)\nplt.ylabel('Country', fontsize=20)\nplt.title('Top 10 countries', fontsize=30)\n\nplt.subplot(212)\ndf.groupby(['country']).suicides_no.mean().nlargest(10).plot(kind='barh', color=col)\nplt.xlabel('Average Suicides_no', size=20)\nplt.ylabel('Country', fontsize=20);","ea2f6f08":"plt.figure(figsize=(10, 16))\n\nplt.subplot(311)\n\nsns.barplot(x='sex', y='population', hue='age', data=df, palette=\"Blues\")\nplt.xticks(ha='right', fontsize=20)\nplt.ylabel('Population', fontsize=20)\nplt.xlabel('Sex', fontsize=20)\nplt.legend(fontsize=14, loc='best')\n\nplt.subplot(313)\n\nsns.barplot(x='sex', y='suicidesper100k', hue='age', data=df,palette=\"Blues\")\nplt.xticks(ha='right', fontsize=20);\nplt.ylabel('suicidesper100k',fontsize=20);\nplt.xlabel('Sex',fontsize=20);\nplt.legend(fontsize=14);\n\nplt.subplot(312)\nsns.barplot(x='sex', y='suicides_no', hue='age', data=df, palette=\"Blues\")\nplt.xticks(ha='right', fontsize=20)\nplt.ylabel('suicides incidences', fontsize=20)\nplt.xlabel('Sex', fontsize=20)\nplt.legend(fontsize=14)","12adac87":"year = originaldata.groupby('year').year.unique()\n\nplt.figure(figsize=(6, 5))\n\ntotalpyear = pd.DataFrame(originaldata.groupby('year').suicides_no.sum())\n\nplt.plot(year.index[0:31], totalpyear[0:31], color=col[18])\nplt.xlabel('year', fontsize=14)\nplt.ylabel('Total number of suicides in the world', fontsize=14)","6a8ee04c":"plt.figure(figsize=(20, 8))\nplt.subplot(121)\nplt.hist(df.suicidesper100k, bins=30, color=col[18])\nplt.xlabel('Suicides per 100K of population', fontsize=14)\nplt.ylabel('count', fontsize=14)\n\nplt.subplot(122)\nplt.hist(df.GDPpcapita, bins=30, color=col[7])\nplt.xlabel('GDP', fontsize=14)\nplt.ylabel('count', fontsize=14)","a9f6f169":"features = ['country', 'year', 'GDPpyear', 'GDPpcapita', 'employeecompensation', 'Unemployment',\n            'Lifeexpectancy', 'Refugees', 'Selfemployed', 'Internetusers']\n\ntotal = df[features].groupby('country').get_group(countrynames[0]).groupby('year').mean()\ntotal['Suicides'] = df[['country', 'year', 'suicidesper100k']].groupby('country').get_group(countrynames[0]).groupby('year').sum()\ntotal['population'] = df[['country', 'year', 'population']].groupby('country').get_group(countrynames[0]).groupby('year').sum()\n\ntotal['country'] = countrynames[0]\n\nfor i, x in enumerate(countrynames[1:]):\n    suicides = df[features].groupby('country').get_group(x).groupby('year').mean()\n    suicides['Suicides'] = df[['country', 'year', 'suicidesper100k']].groupby('country').get_group(x).groupby('year').sum()\n    total['population'] = df[['country', 'year', 'population']].groupby('country').get_group(x).groupby('year').sum()\n  \n    suicides['country'] = x\n    total = total.append(suicides)\n\ntotal.reset_index(inplace=True)\nsort = True","6c5d4203":"totalfeatures = ['country', 'year', 'GDPpyear', 'GDPpcapita', 'employeecompensation', 'Unemployment',\n                 'Lifeexpectancy', 'Refugees', 'Selfemployed', 'Internetusers', 'population']","4bda7b94":"plt.figure(figsize=(20, 8))\nplt.subplot(121)\nsns.distplot(total.Suicides, bins=15)\nplt.xlabel('total Suicides (summed over sex and age group) per 100K of population', fontsize=14)\n\nplt.subplot(122)\nplt.hist(total.GDPpcapita, bins=30, color=col[7])\nplt.xlabel('GDP', fontsize=14)","9b11d27f":"plt.figure(figsize=(8, 5))\n\nsuicides = df[['year', 'GDPpyear', 'Selfemployed', 'Unemployment', 'Lifeexpectancy']].groupby('year').mean()\nsuicides['Suicides'] = df[['country', 'year', 'suicidesper100k']].groupby('year').sum()\n\nplt.plot(suicides.index, suicides.GDPpyear\/suicides.GDPpyear.max(), color=col[1])\nplt.plot(suicides.index, suicides.Unemployment\/suicides.Unemployment.max(), color=col[7])\nplt.plot(suicides.index, suicides.Lifeexpectancy\/suicides.Lifeexpectancy.max(), color=col[15])\nplt.plot(suicides.index, suicides.Suicides\/suicides.Suicides.max(), color=col[17])\nplt.legend(['global average GDPpyear', 'global average Unemployment', 'global average Life expectancy', 'Total suicides per 100k'], fontsize=14, loc='best')\nplt.ylabel('Normalized', fontsize=14)\nplt.xlabel('year', fontsize=14)","cd499234":"corr = total.corr()\n\n# Generate a mask for the upper triangle\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(8, 6))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n            square=True, linewidths=0.2, cbar_kws={\"shrink\": 0.8});","c670f446":"# Cleaning the data, replacing null values with appropriate replacements\n\ntotal.Internetusers.fillna(total.Internetusers. min(), inplace=True)\ntotal.Refugees.fillna(8, inplace=True)\ntotal.employeecompensation.fillna(total.employeecompensation.mean(), inplace=True)\ntotal.population.fillna(total.population.mean(), inplace=True)","66a8e484":"total['risk'] = total.Suicides.copy()\n\ntotal['risk'] = np.where(total.risk < total.Suicides.mean(), 0, 1)","62c390a1":"plt.figure(figsize=(16, 5))\nplt.subplot(121)\nplt.hist(total.risk, color=col[6])\nplt.ylabel('counts', fontsize=14)\nplt.xlabel('Suicide risk', fontsize=14)\n\nplt.subplot(122)\nsns.distplot(total.Suicides[total.risk == 0], bins=10)\nsns.distplot(total.Suicides[total.risk == 1], bins=20)  \nplt.xlabel('Suicides', fontsize=14)","e4b1f7e5":"# Label encoding countries\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ntotal.country = le.fit_transform(total.country)  # Alphabetic order [0:40]\ntotal.country.unique()","e2a595c9":"# Preparing data for modeling\n\nX = np.asarray(total[totalfeatures])\ny = np.asarray(total['risk'])\n\n\n# Applying standard scaler on data, since ML algorithms work with the assumption that the data is normally distributed\n\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)","d1d51ce4":"# Train-test split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=4)\n\nprint('Train set:', X_train.shape, y_train.shape)\nprint('Test set:', X_test.shape, y_test.shape)","7406ff8b":"ax1 = total[total['risk'] == 1][0:200].plot(kind='scatter', x='GDPpcapita', y='employeecompensation', color='DarkRed',\n                                            label='high risk', figsize=(6, 5), fontsize=12)\ntotal[total['risk'] == 0][0:200].plot(kind='scatter', x='GDPpcapita', y='employeecompensation', color='LightGreen',\n                                      label='low risk', ax=ax1)\n\nplt.ylabel('employeecompensation', fontsize=16)\nplt.xlabel('GDP per capita', fontsize=16)\nplt.legend(fontsize=14)\n\n\nax1 = total[total['risk'] == 1][0:200].plot(kind='scatter', x='Lifeexpectancy', y='Selfemployed', color='DarkRed',\n                                            label='high risk', figsize=(6, 5), fontsize=12)\ntotal[total['risk'] == 0][0:200].plot(kind='scatter', x='Lifeexpectancy', y='Selfemployed', color='LightGreen',\n                                      label='low risk', ax=ax1);\n\nplt.ylabel('Selfemployed', fontsize=16)\nplt.xlabel('Lifeexpectancy', fontsize=16)\nplt.legend(fontsize=14)\n\n\nax1 = total[total['risk'] == 1][0:200].plot(kind='scatter', x='GDPpcapita', y='Unemployment', color='DarkRed',\n                                            label='high risk', figsize=(6, 5), fontsize=12);\ntotal[total['risk'] == 0][0:200].plot(kind='scatter', x='GDPpcapita', y='Unemployment', color='LightGreen',\n                                     label='low risk', ax=ax1);\n\nplt.ylabel('Unemployment', fontsize=16)\nplt.xlabel('GDP per capita', fontsize=16);\nplt.legend(fontsize=14);","730b66cc":"fig = plt.figure(figsize=(30, 30))\n\nj = 0\nfor i, x in enumerate(total.columns[0:11]):\n    plt.subplot(4, 3, j+1)\n    j += 1\n    sns.distplot(total[x][total.risk == 0], label='low risk')\n    sns.distplot(total[x][total.risk == 1], label='high risk')       \n    plt.legend(loc='best', fontsize=18)  \n    plt.xlabel(x, fontsize=18)","be27cbc6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nLR = LogisticRegression(C=0.001, solver='liblinear').fit(X_train, y_train)\n\nyLRhat = LR.predict(X_test)\n\nyLRhat_prob = LR.predict_proba(X_test)\n\n\nprint('precision_recall_fscore_support', precision_recall_fscore_support(y_test, yLRhat, average='weighted'))\n\ncm = confusion_matrix(y_test, yLRhat)\nprint('\\n confusion matrix \\n', cm)\n\nprint('classification report for Logistic Regression\\n', classification_report(y_test, yLRhat))","6028ba4b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nDT = DecisionTreeClassifier(criterion=\"entropy\", max_depth=7, max_leaf_nodes=30)\nDT = DT.fit(X_train, y_train)\nydthat = DT.predict(X_test)\n\nprint('******************Decision Tree classifier**************')\n\nprint('Accuracy =', DT.score(X_test, y_test))\nprint('Train Accuracy=', DT.score(X_train, y_train))\nprint('CM\\n', confusion_matrix(y_test, ydthat))\nprint('classification report for decision tree\\n', classification_report(y_test, ydthat))\nprint('# of leaves', DT.get_n_leaves(), '\\n Depth', DT.get_depth())\n\n\nDTfeat_importance = DT.feature_importances_\nDTfeat_importance = pd.DataFrame([totalfeatures, DT.feature_importances_]).T\n\n\nprint(DTfeat_importance.sort_values(by=1, ascending=False))\nprint('\\n# of features= ', DT.n_features_)","55836737":"# USing Area under curve of ROC curve as the metric. This shows how much our classification is better than just\n# randomly chosen classes\n\nfrom sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, ydthat)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","608c98f7":"max_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor max_depth in max_depths: \n    dt = DecisionTreeClassifier(max_depth=max_depth)\n    dt.fit(X_train, y_train)\n    train_pred = dt.predict(X_train)\n    \n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\n    \nplt.figure(figsize=(6, 5))\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'DarkRed', label='Train AUC')\nline2, = plt.plot(max_depths, test_results, 'DarkBlue', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}, fontsize=14)\nplt.ylabel('AUC score', fontsize=14)\nplt.xlabel('Tree depth', fontsize=14)\nplt.show()","af0ae881":"max_leaf_nodes = np.linspace(3, 33, 31, endpoint=True).astype(int)\ntrain_results = []\ntest_results = []\n\nfor max_leaf_nodes in max_leaf_nodes: \n    dt2 = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, max_depth=7)\n    dt2.fit(X_train, y_train)\n    train_pred = dt2.predict(X_train)\n\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n\n    y_pred = dt2.predict(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\n    \nplt.figure(figsize=(6, 5))\n    \nline1, = plt.plot(np.linspace(3, 33, 31, endpoint=True), train_results, 'DarkRed', label='Train AUC')\nline2, = plt.plot(np.linspace(3, 33, 31, endpoint=True), test_results, 'DarkBlue', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}, fontsize=14)\nplt.ylabel('AUC score', fontsize=14)\nplt.xlabel('Max leaf nodes', fontsize=14)\nplt.show()","ae0db14d":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=20, max_depth=10, min_samples_split=2, min_samples_leaf=5,\n                                       max_leaf_nodes=20, max_features=len(totalfeatures)) \n\nrandom_forest.fit(X_train, y_train)\n\nyrfhat = random_forest.predict(X_test)\nfeat_importance = random_forest.feature_importances_\nrffeat_importance = pd.DataFrame([totalfeatures, random_forest.feature_importances_]).T\n\nprint('******************Random forest classifier**************')\nprint('Accuracy on training data', random_forest.score(X_train, y_train))\nprint('Accuracy on test data', random_forest.score(X_test, y_test))\nprint('CM\\n', confusion_matrix(y_test, yrfhat))\nprint('Classification report for random forest\\n', classification_report(y_test, yrfhat))\nprint(rffeat_importance.sort_values(by=1, ascending=False))","a832533b":"from sklearn.neural_network import MLPClassifier\n\nNN = MLPClassifier(solver='lbfgs', max_iter=1000, alpha=1e-4,\n                   hidden_layer_sizes=(4, 4), random_state=4)\n\nNN.fit(X_train, y_train)\ny_predict = NN.predict(X_test)\ncmMLP = confusion_matrix(y_test, y_predict)\n\n\nprint(\"Training set score:\", NN.score(X_train, y_train))\nprint(\"Test set score:\", NN.score(X_test, y_test))\nprint('minimum loss achived=', NN.loss_)\nprint('confusion matrix for MLPclassifier from scikit learn\\n', cmMLP)\nprint('classification reportfor MLPclassifier from scikit learn\\n', classification_report(y_test, y_predict))","0c346e52":"import sklearn\nfrom eli5.sklearn import PermutationImportance\nfrom eli5.permutation_importance import get_score_importances\nsorted(sklearn.metrics.SCORERS.keys())\n\n\ndef score(X, y):\n    \n    y_pred = NN.predict(X)\n \n    return accuracy_score(y, y_pred)\n\nbase_score, score_decreases = get_score_importances(score, X_test, y_test)\nfeature_importances = np.mean(score_decreases, axis=0)\n\nNNfeatureimportance = pd.DataFrame(totalfeatures, feature_importances)\nNNfeatureimportance.reset_index(inplace=True)\nNNfeatureimportance.columns = ['importance', 'feature']\nNNfeatureimportance.sort_values(by='importance', ascending=False)","042a2776":"models = [LR, NN, DT, random_forest]\nmodelnames = ['Logistic regression', 'MLP classifier NN', 'Random Forest', 'Decison tree']\n\n\nfor i, x in enumerate(models):\n    \n    y_true = y_test\n    y_probas = x.predict_proba(X_test)\n    ax1 = skplt.metrics.plot_roc(y_true, y_probas, plot_micro=False, plot_macro=True, classes_to_plot=[], figsize=(5, 5))\n    plt.axis((-0.01, 1, 0, 1.1))\n    plt.legend([modelnames[i]], loc='best')","e18f70a9":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nrfscores = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='accuracy')\n\nrfpredictions = cross_val_predict(random_forest, X_train, y_train)\n\nprint('CM for random forest with cross validation\\n', confusion_matrix(y_train, rfpredictions))\nprint('classification report for random forest with CV \\n', classification_report(y_train, rfpredictions))\n\ndtscores = cross_val_score(DT, X_train, y_train, cv=5, scoring='accuracy')\nDTpredictions = cross_val_predict(DT, X_train, y_train)\n\nprint('CM for Decision tree with cross validation\\n', confusion_matrix(y_train, DTpredictions))\nprint('classification report for Decision tree with CV \\n', classification_report(y_train, DTpredictions))\n\nnnscores = cross_val_score(NN, X_train, y_train, cv=5, scoring='accuracy')\nNNpredictions = cross_val_predict(NN, X_train, y_train)\n\nprint('CM for MLP classifier  with cross validation\\n', confusion_matrix(y_train, NNpredictions))\nprint('classification report for MLP classifier with CV \\n', classification_report(y_train, NNpredictions))","29e04354":"print('Feature importance results for the three best models')\nprint('random forest accuracy score (5-fold cross validation)=', rfscores.mean(), '+\/-', rfscores.std()*2)\n\nplt.figure(figsize=(23, 4))\nplt.bar(rffeat_importance[0], rffeat_importance[1], color=col[2], width=0.4,)\nplt.xticks(ha='right', rotation=30, fontsize=15)\nplt.legend(['Random Forest classifier acc = % f'% rfscores.mean()], fontsize=14)\n\nprint('Decison Tree accuracy score (5-fold cross validation)=', dtscores.mean(), '+\/-', dtscores.std()*2)\nplt.figure(figsize=(23, 4))\nplt.bar(DTfeat_importance[0], DTfeat_importance[1], color=col[14], width=0.4)\nplt.legend(['Desicion Tree cassifier acc = % f'% dtscores.mean()], fontsize=14)\nplt.xticks(ha='right', rotation=30, fontsize=15)\n\nprint('MLP classifier accuracy score (5-fold cross validation)=', nnscores.mean(), '+\/-', nnscores.std()*2)\nplt.figure(figsize=(23, 4))\nplt.bar(NNfeatureimportance['feature'], NNfeatureimportance['importance'], color=col[18], width=0.4)\nplt.legend(['MLP classifier Neural net acc = % f'% nnscores.mean()], fontsize=14);\nplt.xticks(ha='right', rotation=30, fontsize=15);\n","47ceb9ff":"From all the extra features added from the WDI database, I'm using 11 features for exploring. These are basically the features with fewer null values.\n\ntotalfeatures: \n'country', 'year','GDPpyear','GDPpcapita','employeecompensation','Unemployment',\n 'Lifeexpectancy','Refugees','Selfemployed','Internetusers','population'.\n      \n I'm aslo lifting the fine subsample level of sex and age group. I'm creating a new pandas data frame \"total\", with a new column \"suicides\" as the total value of Suicides per 100k of population per country per year, calculated by summing over both sexes and age groups.","3edbb636":"# A classification analysis on suicide data\n\n For this exploration, I used the suicide rate data set from Kaggle https:\/\/www.kaggle.com\/russellyates88\/suicide-rates-overview-1985-to-2016. \n\n \"This compiled dataset pulled from four other datasets linked by time and place, and was built to find signals correlated to increased suicide rates among different cohorts globally, across the socio-economic spectrum.\"\n\n This data set (I call it the \"originaldata\") contains a combination of numerical and categorical features:\n\n **categorical**\n * Country\n * Year: 1985 to 2016\n * Sex: Male\/female \n * Age: Five age groups \n * Generation\n\n **Numerical**\n * Population\n * Number of Suicides: Suicide incidences \n * Suicides per 100k people:normalized version of suicide incidences\n * GDP for year: Gross Domestic Product(a measure of economic development)\n * GDP per capita for year: GDP\/population\n * HDI for year : Human Development Index\n\n I was interested in exploring the possible causes that might increase the risk of suicide in societies, using different machine learning algorithms. Therefore, I decided to enrich the data set by adding more features from the World Development Indicator database to the original suicide data (\"extra_data\")\n","e493a701":"2- Importing the suicide data set with extra features added from WDI database","2fb27449":"# Correlations\n# Calculating the correlation matrix for all chosen features:","7c106d8b":"# Feature importance results summary\nThe bar plot summarizes the feature importances from  different classification results. But what are the true causes? There are some overlaps in feature importances between different classification methods, and there are also some disagreements. This is why one can not trust the results of a single model. As scientists, we should learn not to panic when encountering controversies. Controversies are indeed built in to the scientific method. Instead, we should try to explore other resources and feed more insight into the picture we are exploring. The truth is all models are wrong, but some are useful!","bf2a5825":"# Exploratory Data Analysis\n","39738835":"1- Importing the original suicide data set and renaming the columns","a9b9dc6c":"# Model performance optimization\n# shown for the decision tree classifier only.","c4ff51ad":"# Countries\n The original data set contains data from more than 100 countries during 1985-2016. I've decided to limit the number of countries to make the analysis more insightful. I've chosen 40 countries from different parts of the world that I believe are a good sample of the different regions.","8c3a3a8b":"# Classification performance comparison\/ROC plot","66039fea":"# Data preprocessing \n Cleaning the data, replacing null values with appropriate replacements\n","66217f96":"# k-fold cross validation to minimize overfitting","c00caf27":"The feature \"Internetusers\" is strongly correlated with GDP features, as expected. \"Lifeexpectancy\" also shows strong corrlaiton with GDP per capital. This should be noted when evaluating feature importances from model outputs. Other features do not show strong correlations.","a09729e2":"# Binary classification for suicide risk\n I decided to perform a binary classification on the suicide data, by assigning risk classes based on the suicide incidences per 100k of the population as high\/low suicide risk. Adding an extra column to the \"total\" data frame as 'risk'. \n * Suicides<mean(Suicides) --> low risk --> class 0\n *   Suicides>mean(Suicides) --> high risk --> class 1","1c8cc035":"Also, the extra data from the WDI database was only available for 1995-2013, so I decided to limit the data to this time period.","a8878df1":"# Model development \n I'm using four different classification methods, optimize each, compare the performance of models and extract the feature importance for each model. Based on the EDA results, I expect nonlinear classification to perform better on this data. \n\nmodels explored in this work are:\n 1. Logistic regression\n 2. Decision tree\n 3. Random forest\n 4. MLP classifiere (Neural Network)"}}