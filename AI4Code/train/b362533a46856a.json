{"cell_type":{"f5497f20":"code","e3c1a1a0":"code","cbdcf29d":"code","22071472":"code","66431ccd":"code","52b69d58":"code","ebfb979a":"code","9776ec1d":"code","66e42ddc":"code","c8c10f63":"code","9d843f72":"code","7234756b":"code","02b3447e":"code","a8eca6a6":"code","a8a04ca9":"code","921dc84d":"code","f5964e92":"code","34401d79":"code","98c71323":"code","4d5f59a7":"code","a2284b3c":"code","9397abc6":"code","2f38a808":"code","2b4a09be":"code","0b9ef98a":"code","db150d5c":"code","d5458c68":"code","89c35b0f":"code","a6086cf6":"code","75ab2f59":"code","e240f6bf":"code","daa9af85":"code","9a6dcfd0":"code","7d56029d":"code","763ea369":"code","4e55828e":"code","01fbe14b":"code","38126920":"code","129ef3c9":"code","5be0dab4":"code","03b5bb9f":"code","d16769e1":"code","5986ddb8":"code","9650aed3":"code","716a48de":"code","4c61ec5a":"code","9709fe69":"code","16c45589":"code","289b3434":"code","f3469264":"code","b457fb99":"code","165b711f":"code","b27fce21":"code","4f7cd6a4":"code","38e33023":"code","22b045b9":"code","29cbc1db":"code","25abd360":"code","93bb477a":"code","c1493b3c":"code","c2395108":"code","0fb25180":"code","af81292f":"code","45164442":"code","8a8b41df":"code","3aae4352":"code","94d08b4e":"code","5e2c26b3":"code","d91d5d40":"code","94a3805b":"code","f7ec213a":"code","c61c6e46":"code","c12e76d8":"markdown","168584aa":"markdown","796f581a":"markdown","09020599":"markdown","274b0477":"markdown","b26864fa":"markdown","f15aa7e6":"markdown","05a560db":"markdown","5ae3bf98":"markdown","97ad8b28":"markdown","3644f907":"markdown","4f873cd2":"markdown","0e504ad0":"markdown","030d7e7d":"markdown","218ddf34":"markdown","8f623518":"markdown","96046797":"markdown","bca12ad6":"markdown","d7242007":"markdown","455a40c1":"markdown","50975fb6":"markdown","e5213ae1":"markdown","74817b43":"markdown","c872ab64":"markdown","9cf16b9a":"markdown","42b2e7b3":"markdown","995ce7cc":"markdown","83e8d1d5":"markdown","4b4a474e":"markdown","becbebc9":"markdown","a11ce536":"markdown","e419006f":"markdown","b9303d44":"markdown","243bc64e":"markdown","cc7b4d3b":"markdown","433d0c3d":"markdown","6829e3b4":"markdown","82df38cb":"markdown","74f7bc2d":"markdown","77e00124":"markdown","7e06748a":"markdown","21ad95f4":"markdown","59aaaaec":"markdown","022b3846":"markdown","a6cdeeb1":"markdown"},"source":{"f5497f20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3c1a1a0":"import pandas as pd\nimport numpy as np\nimport os\nimport dask\nimport dask.dataframe as dd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n# # Display all cell outputs\n# from IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = 'all'\n\n\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.graph_objs as go\n# import chart_studio.plotly as py\n\ninit_notebook_mode(connected=True)\ncf.go_offline(connected=True)\n\n# Set global theme\ncf.set_config_file(world_readable=True, theme='pearl')","cbdcf29d":"# from google.colab import drive\n# drive.mount('\/content\/gdrive')","22071472":"# import os\n# os.environ['KAGGLE_CONFIG_DIR'] = \"\/content\/gdrive\/My Drive\/Kaggle\"\n# # \/content\/gdrive\/My Drive\/Kaggle is the path where kaggle.json is present in the Google Drive","66431ccd":"# #changing the working directory\n# %cd \/content\/gdrive\/My Drive\/Kaggle\n# #Check the present working directory using pwd command ","52b69d58":"!kaggle datasets download -d mkechinov\/ecommerce-events-history-in-cosmetics-shop","ebfb979a":"# #unzipping the zip files and deleting the zip files\n# !unzip \\2019-Dec.csv.zip  && rm *.zip","9776ec1d":"# from glob import glob\n# df1 = dd.read_csv(os.path.join('\/content\/gdrive\/My Drive\/Kaggle\/2019-Dec.csv'))\n\ndf1 = dd.read_csv(\"\/kaggle\/input\/ecommerce-events-history-in-cosmetics-shop\/2019-Dec.csv\")","66e42ddc":"df1.head()","c8c10f63":"df1.visualize()","9d843f72":"df1.npartitions","7234756b":"# Using verbose displays full info\n# df1.info(verbose=True)","02b3447e":"df1.dtypes","a8eca6a6":"sample_df = df1.sample(frac = 0.3, random_state=101)","a8a04ca9":"sample_df.info(verbose=True)","921dc84d":"\n# Create null cols and non-null columns\nnull_cols = sample_df.columns[sample_df.isnull().sum().compute()>0]\n# Non nulls are those with same lenght as df\nnot_null_cols = sample_df.columns[sample_df.notnull().sum().compute()==20692840]\n# Create null df and non-null df\nnull_df= sample_df[null_cols]\nnot_null_df= sample_df[not_null_cols]\n","f5964e92":"df_null_sum = sample_df.isnull().sum().compute()\ndf_is_null = sample_df.isnull().compute()","34401d79":"#sample_null = df_is_null.sample(100000, random_state=101)\nsample_null_sort = df_is_null.sort_values('user_id')","98c71323":"# Using sample to prevent kernel crash\nsns.heatmap(sample_null_sort,cbar=False, cmap='magma', yticklabels=False,)\nplt.title('Missing values in sample dataset')","4d5f59a7":"# sample_df['category_code'].value_counts().compute()","a2284b3c":"new_df = sample_df.drop(columns = 'category_code', axis = 1).copy() #drop the category_code column","9397abc6":"# (new_df.brand.isnull().sum()\/len(new_df)).compute() #Check the proportion of null values in brand column","2f38a808":"new_df.brand = new_df.brand.replace(np.nan,'Not Available') #replace Nan values to 'Not available' text","2b4a09be":"# new_df.brand.value_counts().compute() ","0b9ef98a":"new_df['price'].describe().compute() # check statistic state of price column","db150d5c":"# len(new_df['price'][new_df['price']<0])\/len(new_df['price']) \n# % of column with negative price comparing with total amount of values in price column","d5458c68":"new_df = new_df[new_df['price']>= 0] # take only rows with positive values in price column","89c35b0f":"# (new_df.user_session.isnull().sum()\/len(new_df)).compute()","a6086cf6":"new_df.user = new_df.user_session.replace(np.nan, 'Not Available') #replace Nan values with 'Not Available' text","75ab2f59":"pd_df = new_df.compute()","e240f6bf":"pd_df.event_time[:1].str.slice(-3)","daa9af85":"# date = pd_df.event_time.str.slice(0,10)\nyr=  pd_df.event_time.str.slice(0,4)\nmo = pd_df.event_time.str.slice(5,7)\nda = pd_df.event_time.str.slice(8,10)\ntime = pd_df.event_time.str.slice(10,-3).str.strip()\ntime_zone = pd_df.event_time.str.slice(-3).str.strip()\n\n","9a6dcfd0":"hr = time.str.slice(0,2).str.strip()\n\nmin = time.str.slice(3,5).str.strip()\n\nsec= time.str.slice(6).str.strip()\n","7d56029d":"mo = mo.str.zfill(2)\nda =da.str.zfill(2)","763ea369":"hr = hr.str.zfill(2)\nmin=min.str.zfill(2)\nsec= sec.str.zfill(2)","4e55828e":"date_df = pd.DataFrame({'yr':yr,'mo':mo,'da':da})\ndate = date_df.astype(str).apply(\"-\".join,axis=1)\n","01fbe14b":"time_df = pd.DataFrame({'hr':hr,'min':min,'sec':sec})\ntime = time_df.astype(str).apply(\":\".join,axis=1)\n","38126920":"pd_df['Date'] = date \n","129ef3c9":"pd_df['time'] = time","5be0dab4":"pd_df['Date'] = pd.to_datetime(pd_df['Date'], format=\"%Y-%m-%d\")","03b5bb9f":"pd_df['hr'] = pd.to_datetime(pd_df['time'], format=\"%H:%M:%S\").dt.hour\n# pd_df['hr'] = pd.to_datetime(pd_df['time'], format=\"%H:%M:%S\").dt.time","d16769e1":"# Set index and drop\npd_df.set_index('Date', inplace=True, drop=True)","5986ddb8":"# drop event_type columns and time for now \npd_df.drop(['event_time','time'], axis=1, inplace=True)","9650aed3":"# pd_df.head()","716a48de":"# Just incase if we need dask df\n\nnew_df = dd.from_pandas(pd_df, npartitions=7)","4c61ec5a":"# Check customer behavior on the site\ncustmoer_behavior_share =new_df.event_type.value_counts().compute()\/len(new_df)*100","9709fe69":"labels = custmoer_behavior_share.index.tolist()\nvalues = custmoer_behavior_share.values.tolist()\n","16c45589":"fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict( line=dict(color='#000000', width=2)),)\nfig.update_layout(title=\"Customer Beahviour\",\n                 \n                    font=dict(\n                        family=\"Courier New, monospace\",\n                              size=18,\n                              color=\"#7f7f7f\"))\n# fig.show(renderer=\"colab\")\nfig.show(renderer=\"kaggle\")","289b3434":"# How many visitors every day at the site\nvisitor_by_date = pd_df.groupby(pd_df.index)['user_id'].nunique()","f3469264":"# days are numbered fromo 0-7 starting from monday.\n# so 0-4 gives monday-friday\n#5 and 6 gives sat and sunday\nweekends_df = pd_df[pd_df.index.dayofweek>4]\nweekdays_df = pd_df[pd_df.index.dayofweek<=4]","b457fb99":"# Separate visitord by weekdays and weekends\nweekends_visitors = weekends_df.groupby(weekends_df.index)['user_id'].nunique()\nweekdays_visitors = weekdays_df.groupby(weekdays_df.index)['user_id'].nunique()","165b711f":"# Experiment of gaining size value but with same denominator, this is done to gain different markers  size in scatter plot.\nprint(weekdays_visitors.values\/\/200)\nprint(weekends_visitors.values\/\/200)","b27fce21":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n            x=weekends_visitors.index.tolist(), y=weekends_visitors.values.tolist(),mode=\"lines+markers\", name=\"Weekends\",marker=dict(size=weekends_visitors.values\/\/200)))\nfig.add_trace(go.Scatter(\n            x=weekdays_visitors.index.tolist(), y=weekdays_visitors.values.tolist(),mode=\"markers\",name=\"Weekdays\",marker=dict(size=weekdays_visitors.values\/\/200)))\nfig.update_layout(title=\"Number of Visits Everyday\", xaxis_title=\"December 1 - 31\",yaxis_title=\"Frequency\")\nfig.show(renderer=\"kaggle\")","4f7cd6a4":"event_by_date = pd_df.groupby(pd_df.index)['event_type']\ny = pd.DataFrame(event_by_date.value_counts().unstack())\ny_d = np.array(y[['view','cart','remove_from_cart','purchase']])","38e33023":"title = 'Visitor actions on Page'\nlabels = ['View', 'Cart', 'Remove from cart', 'Purchase']\ncolors = ['rgb(67,67,67)', 'rgb(115,115,115)', 'rgb(49,130,189)', 'rgb(189,189,189)']\n\nmode_size = [8, 8, 12, 8]\nline_size = [2, 2, 4, 2]\n\nx_data = y.index\ny_data = y_d\n\nfig = go.Figure()\nfor i in range(0,4):\n    fig.add_trace(go.Scatter(x=x_data, y = y_d[:,i], mode='lines',\n        name=labels[i],\n        line=dict(color=colors[i], width=line_size[i]),\n        connectgaps=True,\n    ))\n\nfig.update_layout(\n    xaxis=dict(\n        showline=True,\n        showgrid=False,\n        showticklabels=True,\n        linecolor='rgb(204, 204, 204)',\n        linewidth=2,\n        ticks='outside',\n        tickfont=dict(\n            family='Arial',\n            size=12,\n            color='rgb(82, 82, 82)',\n        ),\n    ),\n    yaxis=dict(\n        showgrid=False,\n        zeroline=False,\n        showline=False,\n        showticklabels=False,\n    ),\n    autosize=False,\n    margin=dict(\n        autoexpand=False,\n        l=100,\n        r=20,\n        t=110,\n    ),\n    showlegend=False,\n    plot_bgcolor='white'\n)\n\n\nannotations = []\n\n# Adding labels\nfor y_trace, label, color in zip(y_data, labels, colors):\n    # labeling the left_side of the plot\n    annotations.append(dict(xref='paper', x=0.08, y=y_trace[0],\n                                  xanchor='right', yanchor='middle',\n                                  text=label + ' {}'.format(y_trace[0]),\n                                  font=dict(family='Arial',\n                                            size=10),\n                                  showarrow=False))\n    \n# Title\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Dec 2019',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n# Source\nannotations.append(dict(xref='paper', yref='paper', x=0.5, y=-0.1,\n                              xanchor='center', yanchor='top',\n                              text='Visitor behavior on Page',\n                              font=dict(family='Arial',\n                                        size=12,\n                                        color='rgb(150,150,150)'),\n                              showarrow=False))\n\nfig.update_layout(annotations=annotations)\n\nfig.show(renderer=\"kaggle\")","22b045b9":"brand = new_df['brand'].value_counts().compute()\nprint(brand)","29cbc1db":"fig = go.Figure(data=[go.Histogram(x=brand[1:],nbinsx=10, histnorm='probability')])\nfig.update_layout(\n    title_text='Brand Frequency', # title of plot\n    xaxis_title_text='Brand Appearance Frequency', # xaxis label\n    yaxis_title_text='Percentage', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    #bargroupgap=0.1 # gap between bars of the same location coordinates\n)\n# fig.show(renderer=\"colab\")\nfig.show(renderer=\"kaggle\")","25abd360":"# create a list of brands (different from Not available) mentioned more than 10000 times.\nbrand_list = brand[1:][brand >= 10000].index \n# filter out a list of rows with brands in top 6.1%\nbest_brands = new_df[new_df['brand'].isin(brand_list)]","93bb477a":"len(brand_list) # count top 6.1% brands","c1493b3c":"j = new_df['event_type'].value_counts().compute()\nj = j.drop('remove_from_cart')\nj ","c2395108":"fig = go.Figure()\nfor i in range(len(brand_list)):\n    name = brand_list[i]\n    j = best_brands[best_brands['brand']==name]['event_type'].value_counts().compute()\n    j = j.drop('remove_from_cart')\n\n    fig.add_trace(go.Funnel(\n        name = name,\n        y = j.index,\n        x = j,\n        orientation = \"h\",\n        textposition = \"inside\",\n        textinfo = \"value+percent initial\"))\n\nfig.update_layout(\n    title_text='Customre behavior statistic for 15 most-popular brand', # title of plot\n    yaxis_title_text='Customer behavior', # xaxis label\n    xaxis_title_text='Brand performance', # yaxis label\n    )\n\n# fig.show(renderer=\"colab\")\nfig.show(renderer= \"kaggle\")","0fb25180":"fig = go.Figure() \n\ncus_act = ['view','cart','remove_from_cart','purchase']\nc = ['greenyellow', 'blue', 'violet', 'tomato']\nfor i in range(len(cus_act)):\n    fig.add_trace(go.Scatter(x=best_brands['hr'][best_brands['event_type'] == cus_act[i]],\n                             y=best_brands['price'][best_brands['event_type'] == cus_act[i]],\n                             mode='markers', \n                             marker_color=c[i],\n                             marker_size = 3, \n                             name=cus_act[i]))\n\nfig.update_layout(\n    title_text='Customre behavior statistic in by daily hour', # title of plot\n    xaxis_title_text='Day hour', # xaxis label\n    yaxis_title_text='Product price', # yaxis label\n    xaxis={'categoryorder':'category ascending'},\n    # bargap=0.2, # gap between bars of adjacent location coordinates\n    #bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show(renderer=\"kaggle\")","af81292f":"grp_by_hr_event_type = pd_df.groupby(['hr','event_type']).count()","45164442":"layout= dict(title=\"Hourly Store Traffic\", xaxis_title=\"Time of Day\", yaxis_title=\"Number of  Users\")\ngrp_by_hr_event_type['user_id'].unstack(1).iplot(kind=\"bar\", layout=layout)","8a8b41df":"grp_id_event = pd_df.groupby(['user_id','event_type']).hr.max() - pd_df.groupby(['user_id','event_type']).hr.min()\n","3aae4352":"grp_hr_id_event = grp_id_event.unstack()","94d08b4e":"\nmean_per_2000_user_cart = [grp_hr_id_event.cart[0+x:2000+x].mean() for x in range(0,grp_hr_id_event.shape[0],2000) ]\nmean_per_2000_user_purchase = [grp_hr_id_event.purchase[0+x:2000+x].mean() for x in range(0,grp_hr_id_event.shape[0],2000) ]\nmean_per_2000_user_remove_from_cart = [grp_hr_id_event.remove_from_cart[0+x:1000+x].mean() for x in range(0,grp_hr_id_event.shape[0],2000) ]\nmean_per_2000_user_view = [grp_hr_id_event.view[0+x:2000+x].mean() for x in range(0,grp_hr_id_event.shape[0],2000) ]\nindex = [x+1000 for x in range(0,grp_hr_id_event.shape[0],2000)]","5e2c26b3":"# Create df makes plotting easier\nmean_per_2000_user = pd.DataFrame({'cart':mean_per_2000_user_cart,\n                                   'purchase':mean_per_2000_user_purchase,\n                                   \"remove_from_cart\":mean_per_2000_user_remove_from_cart,\n                                   \"view\":mean_per_2000_user_view                                  \n                                  }, index=index)","d91d5d40":"layout= dict(title=\"Avg Hourly Activity per 2000 user\", xaxis_title=\"Number of Users\", yaxis_title=\"Avg Hours Spent By Users\")\nmean_per_2000_user.iplot(kind=\"scatter\", layout=layout)","94a3805b":"event_type_purchase = pd_df[pd_df['event_type']==\"purchase\"]","f7ec213a":"event_type_purchase.price.describe()","c61c6e46":"event_type_purchase.price.iplot(kind=\"hist\", title=\"Amount Spent on Purchase Distribution\", xaxis_title=\"Money\", yaxis_title=\"Frequency\")","c12e76d8":"### Brands analysis","168584aa":"The graph suggests that, the behaviour of users gets more negative for product as time increases. As shown in graph user, removes the product more as the time progresses. However, as we go from left to right in graph it the gap in dropping item and purchasing in closing. It suggest, user is more likely to buy product if he\/she spends less time browsing the product.","796f581a":"Extract Avg hour spent by users. Calculate mean per 2 thousand user to make plot more clear and gain access on behaviour on average. Calculate for each event type","09020599":"In figure,there is a significant decrease in the amount of visitors on as we approach the end of the month. This may be becuase of the holiday season which is typically from 24th of December. People may travel and celebrate instead of shopping online.\nThe week # 2 has most visitors, with Monday Dec 9 gaining 10.98k visitors, while last week sees the least customers with 31st December having 3.3k visitors which is the least amount.\n\nWhats more theres always decrease in the customer number as the weekend approaches and rises again from Monday.","274b0477":"#### Extraction\n","b26864fa":"#### Join\nalso change to df","f15aa7e6":"93.9% of mentioned brands are refered under 10000 times while the brand with top apprearance is mentioned up to 80000 times, almost 8 times more than regular brand. As a result, we will do more analysis how 6.1% (15 brands) of all brands make a diffirent to be mentioned\/considered more during purchase decision process. ","05a560db":"Grouping by hour and then event_type and couting users gives the traffic by hour per event_type","5ae3bf98":"As expected late nights and early mornings are traffic free. The lunch rush hours starts from 9 until 14. While the peak hour is 19:00 hrs.\n    ","97ad8b28":"#### Padding","3644f907":"# 2. Import data","4f873cd2":"Notes: link on how to fecth kaggle data to google colab \\\n https:\/\/medium.com\/analytics-vidhya\/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a","0e504ad0":"# 1. Libraries","030d7e7d":"As illustrusted in above graph, 15 most popular brand has performance rate (add-to-cart\/view and purchase\/view) better than average dataset performance (add-to-cart\/view = 53.6% and purchase\/view = 12.8%). \n\nAmong the top 15 most-popular brands, some brands are not performing as good as average dataset with purchase\/view rate lower than average are grattol (9.9%), estel (8.9%), kapous(7.3%), jessnail (5.4%) and concept (7.6%). ","218ddf34":"There are three null columns: *category_code, user_session  and brand*. They all are nominal-qualitative variables. Due to lack of correlation measure for nominal variables we'll skip the correlation measurement steps.\n","8f623518":"## Data sampling","96046797":"## Missing values in \"category_code\" column","bca12ad6":"Create new pandas dataframe to compute faster\n","d7242007":"# 4. Data Analysis\n\nQuestions to answers:\n1. What are regular customer actions? What is the amount of visitors by dates in the ecommerce store?\nPercentage of each action and time stamp for those? ==> conclusion on activities of customers during day hours ==> conclusion for advises helping marketing team tailored based on customer activities.\n2. Products most seen by customers (event_type = cart)? What are characteristics of these products (price, brand, category) and their seasonalities? ==> product portfolio for business development\n","455a40c1":"# 3. Data Cleaning","50975fb6":"## 4.1 What are regular customer actions? Percentage of each action and time stamp for those? ==> conclusion on activities of customers during day hours ==> conclusion for advises helping marketing team tailored based on customer activities.\n","e5213ae1":"To prevent from running multiple types create variables will null count sum ","74817b43":"## Remove negative values in \"price\" column","c872ab64":"## Replace missing values in \"user_session\" column","9cf16b9a":"To reaseach money spent first only extract those with purchase event_type","42b2e7b3":"## 4.2 Product portfolio analysis","995ce7cc":"The describe method and the histogram suggests that most of the purchases were less than 5.5(3rd quartile is 5ish)","83e8d1d5":"### Null Columns ","4b4a474e":"### Product price Vs. Day hour analysis","becbebc9":"### Missing data\n\n\n\n","a11ce536":"Since there are many missing values in the category_code columns and there is category_id column which is closely related with category_code. We can drop the category_code column","e419006f":"#### Avg Money Spent ","b9303d44":"## Replace missing values in \"brand\" column","243bc64e":"From above tabletable of sample dataset, total of items added to cart is about 53.6% of total item viewed, however, only 22.9% of added-to-cart items was purchased, equivalent to 12.8% of viewed items of those 15 most-popular brands.\n\nFunel graph below will show propotion of each brands in this process.","cc7b4d3b":"As percentage of rows with negative price values is so small in sample set, all these rows will be removed from sample dataset.","433d0c3d":"#### Assign, Change and Reset\n","6829e3b4":"# About the file\nThe data for this project can be downloaded for this [kaggle](https:\/\/www.kaggle.com\/mkechinov\/ecommerce-events-history-in-cosmetics-shop). This file contains behavior data for a one month (December 2019) from a large multi-category online store. \n\n\nEach row in the file represents an event. All events are related to products and users. There are different types of events.\n\n*File structure*\n- event_time: Time when event happened at (in UTC).\n\n- event_type : can be:\nview - a user viewed a product\ncart - a user added a product to shopping cart\nremovefromcart - a user removed a product from shopping cart\npurchase - a user purchased a product\nTypical funnel: view => cart => purchase.\n\n- product_id: ID of a product\n\n- category_id: Product's category ID\n\n- category_code\n\n- Product's category taxonomy (code name) if it was possible to make it. Usually present for meaningful categories and skipped for different kinds of accessories.\n\n- brand: Downcased string of brand name. Can be missed.\n\n- price: Float price of a product. Present.\n\n- user_id: Permanent user ID.\n\n- user_session: Temporary user's session ID. Same for each user's session. Is changed every time user come back to online store from a long pause.\n\n- Multiple purchases per session: A session can have multiple purchase events. It's ok, because it's a single order.","82df38cb":"#### Check the peak hrs","74f7bc2d":"Products attract different customer behaviors at fours particular price range: under 50, 50-100, 100-150 and over 150.\n\nProducts with price range under 50 is the most welcomed all day-long with particularly high rate of purchase comparing to products in other price range. This is also product range with highest remove-from-cart rate of customer behavior.\n\nProducts in price range 50-100 seems added to cart more than removed from card, particularly from 8:30-21:00 daily. However, purchase\/view rate for this product range is quite low.\n\nProducts with price range over 100 is hardly purchased by customer. They are usually viewed throughout the day and put into cart for consideration during 7:00-17:00.","77e00124":"Customer behavior on Page by days:","7e06748a":"#### Study Length of Time Spent Vs Behaviour","21ad95f4":" Make a grouped dataframe by user_id and event type. Extract total hr spent by $max-min$ because hr is time not length.","59aaaaec":"In this section, columns event_time, event_type and price will be used as main analysis input for the analysis. \n\nColumn brand will be used partly in this but not in co-operation with the other 3 columns as more than 42.6% of values in brand column is missing. Any conclusion drawed from this column will be for reference only.","022b3846":"### Reset Date into index\n\n\nDirect reset from event_time gives error. It must be due to padding. i.e month and day are of one length instead of 2, for eg  month =1 instead of month = 01.\nSo, extract each section from yr, month, day, hr, timezone and so on.  Create two time series one for date and another for time.\n\n- Pad the month and day using zfill. \n- Again join yr,month day\n- Change to date time \n- Assign to df\n- Reset index \n- Drop unnecessary columns\n","a6cdeeb1":"\nObservations:\n- Customer purchased only 6% of items while viewd about 48% of items in online store. \n- The items added to cart were around 26% and removed are arpund 18%. \n- It is noticeable that almost 69% of the products added in cart was removed (3 979 679\/5 768 333)\n\nIn ecommerce, the conversion rate represents the percentage of visitors that make a purchase. In this case it is 6.21% which is really high compare to the average conversion rate in general in ecommerce\n"}}