{"cell_type":{"e4b9e9dc":"code","a19651c5":"code","589d384b":"code","a2e99ca0":"code","6e363196":"code","0a982e6e":"code","1ddee85e":"code","1762e9e1":"code","38542739":"code","c7014906":"code","277671d4":"code","7fe0cf7b":"code","9d941903":"code","63bd12e2":"code","369b0473":"code","29681c24":"code","5fa95a1e":"code","ee95f567":"code","deaac4df":"code","c5187671":"code","69556339":"code","2ebeb195":"code","42aa221f":"code","f70ca1bb":"code","1551106b":"code","92c26d58":"code","9c4a9d8b":"code","e953b810":"code","f84a4e71":"code","80e2565f":"code","804b99b0":"code","ec86769d":"code","d7afbd9f":"code","ec5c664a":"code","5afdea29":"code","26bab34e":"code","0fc2fdbb":"code","de9039c1":"code","c08fdb91":"code","4266a58c":"code","700cbb8b":"code","e744d86f":"code","81f9792d":"code","7a701839":"code","6b2bc5ca":"code","3e8a740b":"code","1fb23fd9":"code","8fbb2e88":"code","6539573c":"code","144ef448":"code","b4314bf0":"code","0d49c50f":"code","e91630ea":"code","d8db21b6":"code","3333ca92":"code","48949173":"code","d30eef8e":"code","a986dd1a":"code","d13e1cca":"code","aaa978a8":"code","5438a51d":"code","dd983521":"code","1ce898d1":"code","214d3f98":"code","67e05305":"code","f7db9e7d":"code","249a8240":"code","12188c0b":"markdown","a878ae08":"markdown","58788b26":"markdown","612be597":"markdown","7e7ac3ab":"markdown","70131bee":"markdown","ab79eed3":"markdown","d2e8e565":"markdown","20a39c16":"markdown","f97c2142":"markdown","c7b18b57":"markdown","d909724e":"markdown","e6f2aa0d":"markdown","36296502":"markdown","d96cae89":"markdown","3f75b94b":"markdown","e18cc8c1":"markdown","6ecb0e1d":"markdown","b0d2c41a":"markdown","7addcf7b":"markdown","50964453":"markdown","c6951104":"markdown","5d85ab4e":"markdown","b5848ed0":"markdown","83ba319e":"markdown","34cdc024":"markdown","7820b7f8":"markdown","36c135ff":"markdown","ce91ef13":"markdown","662bde6c":"markdown","4667aad3":"markdown","06a7e451":"markdown"},"source":{"e4b9e9dc":"import time\nnotebookstart = time.time()","a19651c5":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","589d384b":"import platform\nimport sys\nimport importlib\nimport multiprocessing\nimport random","a2e99ca0":"import numpy as np\nimport pandas as pd\n\nrandom.seed(321)\nnp.random.seed(321)\n\npd.options.display.max_columns = 9999","6e363196":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.colors as mcolors\n\n%matplotlib inline\n\nmpl.rc('figure', figsize=(15, 12))\nplt.figure(figsize=(15, 12))\nplt.rcParams['figure.facecolor'] = 'azure'\nmpl.style.use('seaborn')\nplt.style.use('seaborn')\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')","0a982e6e":"import seaborn as sns\n\nsns.set(rc={'figure.figsize': (15, 12)})\nsns.set(context='notebook', style='darkgrid', font='sans-serif',\n        font_scale=1.1, rc={'figure.facecolor': 'azure',\n        'axes.facecolor': 'azure', 'grid.color': 'steelblue'})\nsns.color_palette(mcolors.TABLEAU_COLORS);","1ddee85e":"import missingno as msno","1762e9e1":"import scikitplot as skplt","38542739":"import sklearn\n\nfrom sklearn.model_selection import train_test_split, \\\n    cross_val_predict, cross_val_score\nfrom sklearn.model_selection import StratifiedShuffleSplit, \\\n    StratifiedKFold, GridSearchCV\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, \\\n    RobustScaler, MaxAbsScaler, Normalizer\nfrom sklearn.preprocessing import LabelBinarizer, label_binarize\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, \\\n    classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.metrics import average_precision_score, \\\n    precision_recall_fscore_support\n\nfrom sklearn.utils import shuffle, resample\nfrom sklearn.base import BaseEstimator, ClassifierMixin","c7014906":"from sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier","277671d4":"from scipy import stats","7fe0cf7b":"import xgboost as xgb\nfrom xgboost import XGBClassifier, plot_tree, plot_importance","9d941903":"import lightgbm as lgbm\nfrom lightgbm import LGBMClassifier","63bd12e2":"import catboost\nfrom catboost import CatBoostClassifier","369b0473":"import skopt\nfrom skopt import BayesSearchCV","29681c24":"import imblearn\nfrom imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\nfrom imblearn.combine import SMOTEENN, SMOTETomek","5fa95a1e":"print('Operating system version........', platform.platform())\nprint('Python version is............... %s.%s.%s' % sys.version_info[:3])\nprint('scikit-learn version is.........', sklearn.__version__)\nprint('pandas version is...............', pd.__version__)\nprint('numpy version is................', np.__version__)\nprint('matplotlib version is...........', mpl.__version__)\nprint('seaborn version is..............', sns.__version__)\nprint('scikit-plot version is..........', skplt.__version__)\nprint('missingno version is............', msno.__version__)\nprint('xgboost version is..............', xgb.__version__)\nprint('catboost version is.............', catboost.__version__)\nprint('lightgbm version is.............', lgbm.__version__)\nprint('scikit-optimize version is......', skopt.__version__)\nprint('imblearn version is.............', imblearn.__version__)","ee95f567":"def getDatasetInformation(csv_filepath, is_corr_required=True):\n    \"\"\"\n    Read CSV (comma-separated) file into DataFrame\n    \n    Returns,\n    - DataFrame\n    - DataFrame's shape\n    - DataFrame's data types\n    - DataFrame's describe\n    - DataFrame's sorted unique value count\n    - DataFrame's missing or NULL value count\n    - DataFrame's correlation between numerical columns\n    \"\"\"\n\n    dataset_tmp = pd.read_csv(csv_filepath)\n\n    dataset_tmp_shape = pd.DataFrame(list(dataset_tmp.shape),\n            index=['No of Rows', 'No of Columns'], columns=['Total'])\n    dataset_tmp_shape = dataset_tmp_shape.reset_index()\n\n    dataset_tmp_dtypes = dataset_tmp.dtypes.reset_index()\n    dataset_tmp_dtypes.columns = ['Column Names', 'Column Data Types']\n\n    dataset_tmp_desc = pd.DataFrame(dataset_tmp.describe())\n    dataset_tmp_desc = dataset_tmp_desc.transpose()\n\n    dataset_tmp_unique = dataset_tmp.nunique().reset_index()\n    dataset_tmp_unique.columns = ['Column Name', 'Unique Value(s) Count'\n                                  ]\n\n    dataset_tmp_missing = dataset_tmp.isnull().sum(axis=0).reset_index()\n    dataset_tmp_missing.columns = ['Column Names',\n                                   'NULL value count per Column']\n    dataset_tmp_missing = \\\n        dataset_tmp_missing.sort_values(by='NULL value count per Column'\n            , ascending=False)\n\n    if is_corr_required:\n        dataset_tmp_corr = dataset_tmp.corr(method='spearman')\n    else:\n        dataset_tmp_corr = pd.DataFrame()\n\n    return [\n        dataset_tmp,\n        dataset_tmp_shape,\n        dataset_tmp_dtypes,\n        dataset_tmp_desc,\n        dataset_tmp_unique,\n        dataset_tmp_missing,\n        dataset_tmp_corr,\n        ]","deaac4df":"def getHighlyCorrelatedColumns(dataset, NoOfCols=6):\n    df_corr = dataset.corr()\n\n    # set the correlations on the diagonal or lower triangle to zero,\n    # so they will not be reported as the highest ones\n\n    df_corr *= np.tri(k=-1, *df_corr.values.shape).T\n    df_corr = df_corr.stack()\n    df_corr = \\\n        df_corr.reindex(df_corr.abs().sort_values(ascending=False).index).reset_index()\n    return df_corr.head(NoOfCols)","c5187671":"def createFeatureEngineeredColumns(dataset):\n    dataset_tmp = pd.DataFrame()\n\n    dataset_tmp['CountOfZeroValues'] = (dataset == 0).sum(axis=1)\n    dataset_tmp['CountOfNonZeroValues'] = (dataset != 0).sum(axis=1)\n\n    weight = ((dataset != 0).sum() \/ len(dataset)).values\n    dataset_tmp['WeightedCount'] = (dataset * weight).sum(axis=1)\n\n    dataset_tmp['SumOfValues'] = dataset.sum(axis=1)\n\n    dataset_tmp['VarianceOfValues'] = dataset.var(axis=1)\n    dataset_tmp['MedianOfValues'] = dataset.median(axis=1)\n    dataset_tmp['MeanOfValues'] = dataset.mean(axis=1)\n    dataset_tmp['StandardDeviationOfValues'] = dataset.std(axis=1)\n\n    # dataset_tmp['ModeOfValues'] = dataset.mode(axis=1)\n\n    dataset_tmp['SkewOfValues'] = dataset.skew(axis=1)\n    dataset_tmp['KurtosisOfValues'] = dataset.kurtosis(axis=1)\n\n    dataset_tmp['MaxOfValues'] = dataset.max(axis=1)\n    dataset_tmp['MinOfValues'] = dataset.min(axis=1)\n    dataset_tmp['DiffOfMinMaxOfValues'] = \\\n        np.subtract(dataset_tmp['MaxOfValues'],\n                    dataset_tmp['MinOfValues'])\n\n    dataset_tmp['QuantilePointFiveOfValues'] = dataset[dataset\n            > 0].quantile(0.5, axis=1)\n    dataset_tmp['MovingAverage'] = dataset.apply(lambda x: \\\n            np.ma.average(x), axis=1)\n\n    percentileList = [ 1, 2, 5, 10, 25, 50, 60, 75, 80, 85, 95, 99, ]\n    for i in percentileList:\n        dataset_tmp['Percentile' + str(i)] = dataset.apply(lambda x: \\\n                np.percentile(x, i), axis=1)\n\n    for column in dataset.columns:\n        (hist, bin_edges) = np.histogram(dataset[column], bins=1000,\n                density=True)\n        dataset_tmp[column + '_histval'] = \\\n            [hist[np.searchsorted(bin_edges, ele) - 1] for ele in\n             dataset[column]]\n\n    for column in dataset.columns:\n        dataset_tmp[column + '_log'] = np.log(dataset[column])\n\n    for column in dataset.columns:\n        dataset_tmp[column + '_square'] = np.square(dataset[column])\n\n    dataset_tmp = dataset_tmp.fillna(0)\n    dataset = pd.concat([dataset, dataset_tmp], axis=1)\n\n    return dataset","69556339":"def getZeroStdColumns(dataset):\n    columnsWithZeroStd = dataset.columns[dataset.std() == 0].tolist()\n    return columnsWithZeroStd","2ebeb195":"def getUniqueValueColumns(dataset, valueToCheck=0):\n    columnsWithUniqueValue = dataset.columns[dataset.nunique()\n            == valueToCheck].tolist()\n    return columnsWithUniqueValue","42aa221f":"def plotCategoricalVariableDistributionGraph(target_value, title='', xticksrotation=0):\n    tmp_count = target_value.value_counts()\n    \n    fig=plt.figure()\n    fig.suptitle(title, fontsize=18)\n    \n    ax1=fig.add_subplot(221)\n    sns.pointplot(x=tmp_count.index, y=tmp_count, ax=ax1)\n    ax1.set_title('Distribution Graph')\n    plt.xticks(rotation=xticksrotation)\n    \n    ax2=fig.add_subplot(222)\n    sns.barplot(x=tmp_count.index, y=tmp_count, ax=ax2)\n    ax2.set_title('Distribution Graph - Bar')\n    plt.xticks(rotation=xticksrotation)\n    \n    #ax3=fig.add_subplot(212)\n    ax3=fig.add_subplot(223)\n    ax3.pie(tmp_count, labels=tmp_count.index, autopct=\"%1.1f%%\", shadow=True, startangle=195)\n    ax3.axis('equal')\n    ax3.set_title('Distribution Graph - Pie')\n    \n    ax4=fig.add_subplot(224)\n    stats.probplot(target_value, plot=ax4)\n    ax4.set_title('Distribution - Probability Plot')\n    \n    fig.tight_layout()\n    fig.subplots_adjust(top=0.90)\n    plt.show()","f70ca1bb":"def plot_distplot(dataset):\n    colors = mcolors.TABLEAU_COLORS\n\n    dataset_fordist = dataset.select_dtypes([np.int, np.float])\n    number_of_subplots = len(dataset_fordist.columns)\n    number_of_columns = 3\n\n    number_of_rows = number_of_subplots \/\/ number_of_columns\n    number_of_rows += number_of_subplots % number_of_columns\n\n    postion = range(1, number_of_subplots + 1)\n\n    fig = plt.figure(1)\n    for k in range(number_of_subplots):\n        ax = fig.add_subplot(number_of_rows, number_of_columns,\n                             postion[k])\n        sns.distplot(dataset_fordist.iloc[:, k],\n                     color=random.choice(list(colors.keys())), ax=ax)\n    fig.tight_layout()\n    plt.show()","1551106b":"def convertIntFloatToInt(dictObj):\n    for (k, v) in dictObj.items():\n        if float('Inf') == v:\n            pass\n        elif int(v) == v and isinstance(v, float):\n            dictObj[k] = int(v)\n    return dictObj","92c26d58":"(\n    dataset_sctp_train,\n    df_train_shape,\n    df_train_dtypes,\n    df_train_describe,\n    df_train_unique,\n    df_train_missing,\n    df_train_corr,\n    ) = getDatasetInformation('..\/input\/train.csv', False)\n\n(\n    dataset_sctp_test,\n    df_test_shape,\n    df_test_dtypes,\n    df_test_describe,\n    df_test_unique,\n    df_test_missing,\n    df_test_corr,\n    ) = getDatasetInformation('..\/input\/test.csv', False)","9c4a9d8b":"dataset_sctp_train.head()","e953b810":"df_train_shape","f84a4e71":"df_train_dtypes","80e2565f":"df_train_describe","804b99b0":"df_train_unique","ec86769d":"df_train_missing","d7afbd9f":"msno.matrix(dataset_sctp_train, color=(33 \/ 255, 102 \/ 255, 172 \/ 255));","ec5c664a":"dataset_sctp_test.head()","5afdea29":"df_test_shape","26bab34e":"df_test_dtypes","0fc2fdbb":"df_test_describe","de9039c1":"df_test_unique","c08fdb91":"df_test_missing","4266a58c":"msno.matrix(dataset_sctp_test, color=(33 \/ 255, 102 \/ 255, 172 \/ 255));","700cbb8b":"del(df_train_shape, df_train_dtypes, df_train_describe, df_train_unique, df_train_missing, df_train_corr)\ndel(df_test_shape, df_test_dtypes, df_test_describe, df_test_unique, df_test_missing, df_test_corr)","e744d86f":"plot_distplot(dataset_sctp_train.iloc[:, 2:29])","81f9792d":"plot_distplot(dataset_sctp_train.iloc[:, 29:56])","7a701839":"plot_distplot(dataset_sctp_train.iloc[:, 56:83])","6b2bc5ca":"plot_distplot(dataset_sctp_train.iloc[:, 83:110])","3e8a740b":"plot_distplot(dataset_sctp_train.iloc[:, 110:137])","1fb23fd9":"plot_distplot(dataset_sctp_train.iloc[:, 137:164])","8fbb2e88":"plot_distplot(dataset_sctp_train.iloc[:, 164:191])","6539573c":"plot_distplot(dataset_sctp_train.iloc[:, 191:204])","144ef448":"dataset_sctp_train.target.unique()","b4314bf0":"dataset_sctp_train.target.value_counts()","0d49c50f":"plotCategoricalVariableDistributionGraph(dataset_sctp_train.target, 'Target (feature) - Distribution', xticksrotation=90)","e91630ea":"y = dataset_sctp_train['target']\nX = dataset_sctp_train.drop(['ID_code', 'target'], axis=1)\nX.astype('float32')\n\ndataset_sctp_test_ID_code = dataset_sctp_test['ID_code']\nZ = dataset_sctp_test.drop(['ID_code'], axis=1)\nZ.astype('float32')\n\n(X.shape, y.shape, Z.shape)","d8db21b6":"getHighlyCorrelatedColumns(X, 10)","3333ca92":"X=createFeatureEngineeredColumns(X)\nZ=createFeatureEngineeredColumns(Z)\n\n(X.shape, y.shape, Z.shape)","48949173":"columnsWithZeroStdToRemove = getZeroStdColumns(X)\nprint(f'Columns with Zero STD to drop from Train and Test dataset(s) are {columnsWithZeroStdToRemove}.')\n\nX.drop(columnsWithZeroStdToRemove, axis=1, inplace=True)\nZ.drop(columnsWithZeroStdToRemove, axis=1, inplace=True)\n\n(X.shape, Z.shape)","d30eef8e":"X_columns_one_unique_value = getUniqueValueColumns(X, 1)\nprint(f'Columns with only 1 as value to drop from Train and Test datasets are {X_columns_one_unique_value}.')\n\nX.drop(X_columns_one_unique_value, axis=1, inplace=True)\nZ.drop(X_columns_one_unique_value, axis=1, inplace=True)\n\n(X.shape, Z.shape)","a986dd1a":"X_columns_zero_unique_value = getUniqueValueColumns(X, 0)\nprint(f'Columns with only 0 as value to drop from Train and Test datasets are {X_columns_zero_unique_value}.')\n\nX.drop(X_columns_zero_unique_value, axis=1, inplace=True)\nZ.drop(X_columns_zero_unique_value, axis=1, inplace=True)\n\n(X.shape, Z.shape)","d13e1cca":"n_cpus_avaliable = multiprocessing.cpu_count()\n\nprint(f'We\\'ve got {n_cpus_avaliable} cpus to work with.')","aaa978a8":"def augment(x, y, t=2):\n    (xs, xn) = ([], [])\n    for i in range(t):\n        mask = y > 0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:, c] = x1[ids][:, c]\n        xs.append(x1)\n\n    for i in range(t \/\/ 2):\n        mask = y == 0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:, c] = x1[ids][:, c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x, xs, xn])\n    y = np.concatenate([y, ys, yn])\n    return (x, y)","5438a51d":"lgbmparams = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    \"tree_learner\": \"serial\",\n    'n_jobs': n_cpus_avaliable,\n}\n\n\nn_splits = 7\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True)\nsctp_predictions_lgbm_pp = np.zeros(len(Z))\n\n\nfor (index, (train_indices, val_indices)) in enumerate(skf.split(X, y)):\n    print('Training on fold ' + str(index + 1) + '\/' + str(n_splits) + '...')\n\n    # Generate batches from indices\n    (xtrain, xval) = (X.iloc[train_indices], X.iloc[val_indices])\n    (ytrain, yval) = (y.iloc[train_indices], y.iloc[val_indices])\n\n    (xtrain_aug, ytrain_aug) = augment(xtrain.values, ytrain.values,\n            t=2)\n    xtrain_aug = pd.DataFrame(xtrain_aug, columns=X.columns)\n    ytrain_aug = [int(value) for value in ytrain_aug]\n    ytrain_aug = pd.Series(ytrain_aug)\n\n    train_set_lgbm = lgbm.Dataset(data=xtrain_aug, label=ytrain_aug)\n    val_set_lgbm = lgbm.Dataset(data=xval, label=yval)\n    watchlist = [train_set_lgbm, val_set_lgbm]\n\n    evaluation_results_lgbm = {}\n\n    lgbm_twoclass_model = lgbm.train(\n        lgbmparams,\n        train_set=train_set_lgbm,\n        num_boost_round=10000,\n        valid_sets=watchlist,\n        early_stopping_rounds=1000,\n        evals_result=evaluation_results_lgbm,\n        verbose_eval=1000,\n        )\n    sctp_predictions_lgbm_pp += lgbm_twoclass_model.predict(Z,\n            num_iteration=lgbm_twoclass_model.best_iteration) \\\n        \/ skf.n_splits","dd983521":"ax = lgbm.plot_metric(evaluation_results_lgbm)\nplt.show()","1ce898d1":"ax = lgbm.plot_importance(lgbm_twoclass_model, max_num_features=50, color=mcolors.TABLEAU_COLORS)\nplt.show()","214d3f98":"sctp_predictions_lgbm = lgbm_twoclass_model.predict(Z)\nsctp_predictions_lgbm = [round(value) for value in sctp_predictions_lgbm]\nsctp_predictions_lgbm = list(map(int, sctp_predictions_lgbm))\nsctp_predictions_lgbm[:20]","67e05305":"dataset_submission = pd.DataFrame()\ndataset_submission['ID_code'] = dataset_sctp_test_ID_code\ndataset_submission['target'] = sctp_predictions_lgbm\ndataset_submission.to_csv('lgbm_twoclass_model_submission.csv', index=False)","f7db9e7d":"dataset_submission = pd.DataFrame()\ndataset_submission['ID_code'] = dataset_sctp_test_ID_code\ndataset_submission['target'] = sctp_predictions_lgbm_pp\ndataset_submission.to_csv('lgbm_pp_twoclass_model_submission.csv', index=False)","249a8240":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","12188c0b":"upper = df_train_corr.where(np.triu(np.ones(df_train_corr.shape), k=1).astype(np.bool))\n\nto_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n\nprint(f'Columns to drop from Train and Test datasets are {to_drop}.')\n\nX.drop(columns=to_drop, axis=1, inplace=True)\nZ.drop(columns=to_drop, axis=1, inplace=True)\n\n(X.shape, Z.shape)","a878ae08":"del(df_train_corr)","58788b26":"## INTRODUCTION\n\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.\n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.","612be597":"# FEATURE ENGINEERING","7e7ac3ab":"# PREDICTIONS OF TEST DATA","70131bee":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#SANTANDER-CUSTOMER-TRANSACTION-PREDICTION\" data-toc-modified-id=\"SANTANDER-CUSTOMER-TRANSACTION-PREDICTION-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>SANTANDER CUSTOMER TRANSACTION PREDICTION<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#INTRODUCTION\" data-toc-modified-id=\"INTRODUCTION-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>INTRODUCTION<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#LOAD-PACKAGES\" data-toc-modified-id=\"LOAD-PACKAGES-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>LOAD PACKAGES<\/a><\/span><\/li><li><span><a href=\"#DEFINE-GENERIC-COMMON-METHODS\" data-toc-modified-id=\"DEFINE-GENERIC-COMMON-METHODS-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>DEFINE GENERIC COMMON METHODS<\/a><\/span><\/li><li><span><a href=\"#LOAD-DATA-AND-PERFORM-ANALYSIS\" data-toc-modified-id=\"LOAD-DATA-AND-PERFORM-ANALYSIS-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>LOAD DATA AND PERFORM ANALYSIS<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#ANALYSE-TRAIN-DATA\" data-toc-modified-id=\"ANALYSE-TRAIN-DATA-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>ANALYSE TRAIN DATA<\/a><\/span><\/li><li><span><a href=\"#ANALYSE-TEST-DATA\" data-toc-modified-id=\"ANALYSE-TEST-DATA-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>ANALYSE TEST DATA<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#PERFORM-EXPLORATORY-DATA-ANALYSIS-(EDA)\" data-toc-modified-id=\"PERFORM-EXPLORATORY-DATA-ANALYSIS-(EDA)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>PERFORM EXPLORATORY DATA ANALYSIS (EDA)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#UNDERSTANDING-DISTRIBUTION-OF-FEATURE(S)\" data-toc-modified-id=\"UNDERSTANDING-DISTRIBUTION-OF-FEATURE(S)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>UNDERSTANDING DISTRIBUTION OF FEATURE(S)<\/a><\/span><\/li><li><span><a href=\"#UNDERSTANDING-TARGET-VARIABLE\" data-toc-modified-id=\"UNDERSTANDING-TARGET-VARIABLE-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>UNDERSTANDING TARGET VARIABLE<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#FEATURE-ENGINEERING\" data-toc-modified-id=\"FEATURE-ENGINEERING-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>FEATURE ENGINEERING<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#SEPERATE-DATA-AND-TARGET-FEATURES\" data-toc-modified-id=\"SEPERATE-DATA-AND-TARGET-FEATURES-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>SEPERATE DATA AND TARGET FEATURES<\/a><\/span><\/li><li><span><a href=\"#CORRELATION-MATRIX\" data-toc-modified-id=\"CORRELATION-MATRIX-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>CORRELATION MATRIX<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#MODELING\" data-toc-modified-id=\"MODELING-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>MODELING<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#BINOMIAL-CLASSIFICATION:-Using-LGBMClassifier-with-StratifiedKFold-and-Manual-Data-Augmentation\" data-toc-modified-id=\"BINOMIAL-CLASSIFICATION:-Using-LGBMClassifier-with-StratifiedKFold-and-Manual-Data-Augmentation-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>BINOMIAL CLASSIFICATION: Using <strong>LGBMClassifier<\/strong> with <strong>StratifiedKFold<\/strong> and <strong>Manual Data Augmentation<\/strong><\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#PERFORM-CLASSIFICATION\" data-toc-modified-id=\"PERFORM-CLASSIFICATION-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;<\/span>PERFORM CLASSIFICATION<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#BINOMIAL-CLASSIFICATION:-Using-XGBClassifier-with-StratifiedKFold-and-Manual-Data-Augmentation\" data-toc-modified-id=\"BINOMIAL-CLASSIFICATION:-Using-XGBClassifier-with-StratifiedKFold-and-Manual-Data-Augmentation-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>BINOMIAL CLASSIFICATION: Using <strong>XGBClassifier<\/strong> with <strong>StratifiedKFold<\/strong> and <strong>Manual Data Augmentation<\/strong><\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#PERFORM-CLASSIFICATION\" data-toc-modified-id=\"PERFORM-CLASSIFICATION-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;<\/span>PERFORM CLASSIFICATION<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#PREDICTIONS-OF-TEST-DATA\" data-toc-modified-id=\"PREDICTIONS-OF-TEST-DATA-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>PREDICTIONS OF TEST DATA<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#PREDICT-with-LIGHTGBM\" data-toc-modified-id=\"PREDICT-with-LIGHTGBM-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>PREDICT with LIGHTGBM<\/a><\/span><\/li><li><span><a href=\"#PREDICT-with-XGBOOST\" data-toc-modified-id=\"PREDICT-with-XGBOOST-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>PREDICT with XGBOOST<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","ab79eed3":"### PERFORM CLASSIFICATION","d2e8e565":"# MODELING","20a39c16":"## BINOMIAL CLASSIFICATION: Using **LGBMClassifier** with **StratifiedKFold** and **Manual Data Augmentation**","f97c2142":"# PERFORM EXPLORATORY DATA ANALYSIS (EDA)","c7b18b57":"## SEPERATE DATA AND TARGET FEATURES","d909724e":"## PREDICT with XGBOOST","e6f2aa0d":"# DEFINE GENERIC COMMON METHODS","36296502":"## BINOMIAL CLASSIFICATION: Using **XGBClassifier** with **StratifiedKFold** and **Manual Data Augmentation**","d96cae89":"dataset_submission = pd.DataFrame()\ndataset_submission['ID_code'] = dataset_sctp_test_ID_code\ndataset_submission['target'] = sctp_predictions_xgb\ndataset_submission.to_csv('xgb_twoclass_model_submission.csv', index=False)","3f75b94b":"# SANTANDER CUSTOMER TRANSACTION PREDICTION\n\n**BHAVESHKUMAR THAKER**","e18cc8c1":"# LOAD PACKAGES","6ecb0e1d":"sctp_predictions_xgb = xgb_twoclass_model.predict(xgb.DMatrix(Z))\nsctp_predictions_xgb = [round(value) for value in sctp_predictions_xgb]\nsctp_predictions_xgb = list(map(int, sctp_predictions_xgb))\nsctp_predictions_xgb[:20]","b0d2c41a":"#mask = np.zeros_like(df_train_corr, dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(\n    df_train_corr,\n    cmap='rainbow',\n    annot=False,\n    fmt='.2f',\n    center=0,\n    square=False,\n    linewidths=.75,\n    #mask=mask,\n    );\nplt.title('Correlation Matrix', fontsize=18)\nplt.show()","7addcf7b":"## ANALYSE TEST DATA","50964453":"## CORRELATION MATRIX","c6951104":"plot_importance(xgb_twoclass_model, max_num_features=50, color=mcolors.TABLEAU_COLORS);","5d85ab4e":"n_splits = 2\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True)\nsctp_predictions_xgb_pp = np.zeros(len(Z))\n\n\nfor (index, (train_indices, val_indices)) in enumerate(skf.split(X, y)):\n    print('Training on fold ' + str(index + 1) + '\/' + str(n_splits) + '...')\n    \n    (xtrain, xval) = (X.iloc[train_indices], X.iloc[val_indices])\n    (ytrain, yval) = (y.iloc[train_indices], y.iloc[val_indices])\n\n    (xtrain_aug, ytrain_aug) = augment(xtrain.values, ytrain.values,\n            t=2)\n    xtrain_aug = pd.DataFrame(xtrain_aug, columns=X.columns)\n    ytrain_aug = [int(value) for value in ytrain_aug]\n    ytrain_aug = pd.Series(ytrain_aug)\n\n    train_set_xgb = xgb.DMatrix(data=xtrain_aug, label=ytrain_aug)\n    val_set_xgb = xgb.DMatrix(data=xval, label=yval)\n    watchlist = [(train_set_xgb, 'train'), (val_set_xgb, 'valid')]\n\n    evaluation_results_xgb = {}\n\n    xgb_twoclass_model = xgb.train(\n        xgbparams,\n        train_set_xgb,\n        10000,\n        watchlist,\n        verbose_eval=1000,\n        early_stopping_rounds=1000,\n        evals_result=evaluation_results_xgb,\n        )\n    sctp_predictions_xgb_pp += xgb_twoclass_model.predict(xgb.DMatrix(Z)) \\\n        \/ skf.n_splits","b5848ed0":"## UNDERSTANDING DISTRIBUTION OF FEATURE(S)","83ba319e":"xgbparams = {\n    'booster': 'gbtree',\n    'colsample_bylevel': 0.48973134715974026,\n    'colsample_bytree': 0.46131821596707184,\n    'device': 'cpu',\n    'eval_metric': 'auc',\n    'gamma': 8.0762512124703e-06,\n    'learning_rate': 0.054722068464825926,\n    'max_delta_step': 3,\n    'max_depth': 29,\n    'min_child_weight': 14,\n    'missing': 'None',\n    'n_jobs': n_cpus_avaliable,\n    'objective': 'binary:logistic',\n    'reg_lambda': 2.1021696940800796,\n    'scale_pos_weight': 43.8858626195784,\n    'subsample': 0.8113392946402368,\n    'tree_method': 'approx',\n    }","34cdc024":"## ANALYSE TRAIN DATA","7820b7f8":"# LOAD DATA AND PERFORM ANALYSIS","36c135ff":"## PREDICT with LIGHTGBM","ce91ef13":"### PERFORM CLASSIFICATION","662bde6c":"## UNDERSTANDING TARGET VARIABLE","4667aad3":"df_train_corr = X.corr(method='spearman')\ndf_train_corr","06a7e451":"dataset_submission = pd.DataFrame()\ndataset_submission['ID_code'] = dataset_sctp_test_ID_code\ndataset_submission['target'] = sctp_predictions_xgb_pp\ndataset_submission.to_csv('xgb_pp_twoclass_model_submission.csv', index=False)"}}