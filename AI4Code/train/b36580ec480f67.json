{"cell_type":{"ee4f4d60":"code","d126396d":"code","90d91a1f":"code","087a799a":"code","d7fd3740":"code","169823a3":"code","983dea41":"code","16ebfc98":"code","982ff163":"code","886ba0eb":"code","c71c3ab4":"code","52e412e6":"code","f35c3ecf":"code","3f452516":"code","adf7993a":"code","6923ad7a":"code","a411cdd5":"code","d6b653eb":"code","4e968fd3":"code","da6e09c7":"code","849f5ece":"code","fa6e21c9":"code","bf35011a":"code","31928397":"code","fa708d45":"code","70070b7d":"code","f8ed2ebf":"code","0216156d":"code","a8d25277":"code","6c49f48e":"code","63f20b9e":"code","1fc459d7":"code","884597ae":"code","a8cbe37f":"markdown","5a5e8de4":"markdown","af3a39f3":"markdown","ef9d809a":"markdown","d59b830c":"markdown","fc9e9a61":"markdown","a76cc0c9":"markdown","2fc2167b":"markdown","291de2be":"markdown","b325205f":"markdown","a8b6100b":"markdown","bde0eff0":"markdown","80c39584":"markdown","127d033c":"markdown","5806ce02":"markdown","f354ddcb":"markdown","5b3b2238":"markdown","434efd12":"markdown","c24dd323":"markdown","fd4a2afc":"markdown","6abcb1f6":"markdown","7bec920e":"markdown","a2d8ebd9":"markdown","d5a4ce4d":"markdown","d52636a3":"markdown","1cc935ae":"markdown","5cab5cad":"markdown","5aad94be":"markdown","d15a6139":"markdown","a09e03fa":"markdown","84b81483":"markdown","65e65949":"markdown","d70895f0":"markdown"},"source":{"ee4f4d60":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nfrom scipy.stats import probplot\nfrom scipy.stats import skew\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\n%matplotlib inline\nrcParams['figure.figsize'] = 8, 6\nsb.set()","d126396d":"# Read in data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint(train.shape)\nprint(test.shape)\n\ntrain.head()","90d91a1f":"corr = train.corr()\nplt.figure(figsize = (16, 12))\nsb.heatmap(corr, linewidths = 0.5, fmt = '.2f', center = 1)\nplt.show()","087a799a":"sb.pairplot(train[['SalePrice','OverallQual', 'TotalBsmtSF', 'GrLivArea', 'FullBath', 'GarageCars']])\nplt.show()","d7fd3740":"y_train = train['SalePrice']\nsb.distplot(y_train)\nplt.xlabel('Sale Price')\nplt.ylabel('Count')\nplt.title('Distribution of Sale Price')\nplt.show()\n\nprobplot(y_train, plot = plt)\nplt.show()","169823a3":"train['SalePrice'] = np.log1p(train['SalePrice'])\ny_train = train['SalePrice']\nsb.distplot(y_train)\nplt.xlabel('Sale Price')\nplt.ylabel('Count')\nplt.title('Distribution of Sale Price')\nplt.show()\n\nprobplot(y_train, plot = plt)\nplt.show()","983dea41":"plt.scatter(train['OverallQual'], train['SalePrice'])\nplt.title('OveralQual vs. SalePrice')\nplt.show()\n\nplt.scatter(train['TotalBsmtSF'], train['SalePrice'])\nplt.title('TotalBsmtSF vs. SalePrice')\n\nplt.show()\n\nplt.scatter(train['GrLivArea'], train['SalePrice'])\nplt.title('GrLivArea vs. SalePrice')\nplt.show()\n\nplt.scatter(train['FullBath'], train['SalePrice'])\nplt.title('FullBath vs. SalePrice')\nplt.show()\n\nplt.scatter(train['GarageCars'], train['SalePrice'])\nplt.title('GarageCars vs. SalePrice')\nplt.show()","16ebfc98":"train_features = train.drop(['SalePrice'], axis = 1)\ntest_features = test\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop = True)\nfeatures.shape","982ff163":"# Selecting numeric variables with skewness > 0.5\nnumvars = features.select_dtypes(include = ['int64', 'float64', 'int32']).columns\nnumvars_skew = pd.DataFrame(features[numvars].skew(), columns = ['Skew'])\nnumvars_skew = numvars_skew[numvars_skew['Skew'] > 0.5]\n\n# Applying log transformation to skewed variables\nskewed = features[numvars_skew.index]\nunskewed = np.log1p(skewed)\n\n# Replacing in dataset\nfeatures[skewed.columns] = unskewed","886ba0eb":"fig = plt.figure(figsize = (16, 12))\nax = fig.add_subplot()\n\nsb.boxplot(data=features[skewed.columns] , orient=\"h\")\nplt.show()","c71c3ab4":"# Sum of missing values\nprint(\"There are {} missing values in the features dataset.\".format(features.isnull().sum().sum()))","52e412e6":"# Summmary of columns with missing values\nmissing = features.columns[features.isnull().any()]\nfeatures[missing].isnull().sum().to_frame()","f35c3ecf":"# Remove columns with more than 33 percent missing values: > 1000\nremove_cols = features.columns[features.isnull().sum() > 1000]\nprint('The following list contains columns in the train dataset which have more than 33 percent missing values: \\n{}.'.format(remove_cols))\n\n# Drop those columns from features\nfeatures = features.drop(remove_cols, axis = 1)\n\nprint('I will remove those columns. \\nThere are now {} missing values in the features dataset.'.format(features.isnull().sum().sum()))","3f452516":"# List of train columns that are objects\nobjs = features.select_dtypes(include = ['object']).columns\n\n# List of train columns that are objects with missing values\nmissing = features[objs].columns[features[objs].isnull().any()]\nfeatures[missing].isnull().sum().to_frame()","adf7993a":"# Fill train columns that are objects that have missing values with mode\nfeatures[missing] = features[missing].fillna(features.mode().iloc[0])\nprint(\"There are now {} missing values among the categorical variables.\".format(features[objs].isnull().any().sum()))","6923ad7a":"features[objs] = features[objs].apply(LabelEncoder().fit_transform)\nfeatures.info()","a411cdd5":"missing = features.columns[features.isnull().any()]\nfeatures[missing].isnull().sum().to_frame()","d6b653eb":"imputer = SimpleImputer()\nimputed_features = pd.DataFrame(imputer.fit_transform(features))\nimputed_features.columns = features.columns\nfeatures = imputed_features\n\n# Drop the ID columns\nfeatures = features.drop('Id', axis = 1)\n\nfeatures.head()","4e968fd3":"print(\"There are now {} missing values in the features dataset.\".format(features.isnull().any().sum()))","da6e09c7":"features.shape","849f5ece":"y_train = train['SalePrice']\nX_train = features.iloc[:len(y_train), :]\nX_test = features.iloc[len(y_train):, :]\n\nX_train.shape, y_train.shape, X_test.shape","fa6e21c9":"from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\n# ignore warnings\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) ","bf35011a":"seed = 1\n\nlinear = LinearRegression()\nelastic = ElasticNet(random_state = seed)\nridge = Ridge(random_state = seed)\nlasso = Lasso(random_state = seed)\nkernel = KernelRidge()\nr_forest = RandomForestRegressor(random_state = seed)\ng_boost = GradientBoostingRegressor(random_state = seed)\nsvr = SVR()\nknn = KNeighborsRegressor()\nlgbm = LGBMRegressor(random_state = seed)\nxgb = XGBRegressor(random_state = seed)","31928397":"kfold = KFold(n_splits = 10, shuffle = True, random_state = seed)\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = kfold))\n    rmse = np.round(rmse, 6)\n    return(rmse)","fa708d45":"scores = {}\nscores['linear'] = cv_rmse(linear).mean()\nscores['elastic'] = cv_rmse(elastic).mean()\nscores['ridge'] = cv_rmse(ridge).mean()\nscores['lasso'] = cv_rmse(lasso).mean()\nscores['kernel'] = cv_rmse(kernel).mean()\nscores['r_forest'] = cv_rmse(r_forest).mean()\nscores['g_boost'] = cv_rmse(g_boost).mean()\nscores['svr'] = cv_rmse(svr).mean()\nscores['knn'] = cv_rmse(knn).mean()\nscores['lgbm'] = cv_rmse(lgbm).mean()\nscores['xgb'] = cv_rmse(xgb).mean()","70070b7d":"plt.scatter(scores.keys(), scores.values())\nplt.xticks(rotation = 45)\nplt.title('10-Fold Cross Validation Scores')\nplt.ylabel('RMSE')\nplt.xlabel('Model')\nplt.show()","f8ed2ebf":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","0216156d":"linear_model = linear.fit(X_train, y_train)\nlinear_pred = linear_model.predict(X_train)\nlinear_error = rmsle(y_train, linear_pred)\n\nelastic_model = elastic.fit(X_train, y_train)\nelastic_pred = elastic_model.predict(X_train)\nelastic_error = rmsle(y_train, elastic_pred)\n\nridge_model = ridge.fit(X_train, y_train)\nridge_pred = ridge_model.predict(X_train)\nridge_error = rmsle(y_train, ridge_pred)\n\nlasso_model = lasso.fit(X_train, y_train)\nlasso_pred = lasso_model.predict(X_train)\nlasso_error = rmsle(y_train, lasso_pred)\n\nkernel_model = kernel.fit(X_train, y_train)\nkernel_pred = kernel_model.predict(X_train)\nkernel_error = rmsle(y_train, kernel_pred)\n\nr_forest_model = r_forest.fit(X_train, y_train)\nr_forest_pred = r_forest_model.predict(X_train)\nr_forest_error = rmsle(y_train, r_forest_pred)\n\ng_boost_model = g_boost.fit(X_train, y_train)\ng_boost_pred = g_boost_model.predict(X_train)\ng_boost_error = rmsle(y_train, g_boost_pred)\n\nsvr_model = svr.fit(X_train, y_train)\nsvr_pred = svr_model.predict(X_train)\nsvr_error = rmsle(y_train, svr_pred)\n\nknn_model = knn.fit(X_train, y_train)\nknn_pred = knn_model.predict(X_train)\nknn_error = rmsle(y_train, knn_pred)\n\nlgbm_model = lgbm.fit(X_train, y_train)\nlgbm_pred = lgbm_model.predict(X_train)\nlgbm_error = rmsle(y_train, lgbm_pred)\n\nxgb_model = xgb.fit(X_train, y_train)\nxgb_pred = xgb_model.predict(X_train)\nxgb_error = rmsle(y_train, xgb_pred)","a8d25277":"print(\"Linear Model RMSLE:\", linear_error)\nprint(\"Elastic Net Model RMSLE:\", elastic_error)\nprint(\"Ridge Model RMSLE:\", ridge_error)\nprint(\"LASSO Model RMSLE:\", lasso_error)\nprint(\"Kernel Ridge Model RMSLE:\", kernel_error)\nprint(\"Random Forest Model RMSLE:\", r_forest_error)\nprint(\"Gradient Boosting Model RMSLE:\", g_boost_error)\nprint(\"Suppor Vector Regression Model RMSLE:\", svr_error)\nprint(\"K-Nearest Neighbors Model RMSLE:\", knn_error)\nprint(\"LightGBM RMSLE:\", lgbm_error)\nprint(\"XGBoost Model RMSLE:\", xgb_error)\n\nmodel_names = ['linear', 'elastic', 'ridge', 'lasso', 'kernel', 'r_forest', 'g_boost', 'svr', 'knn', 'lgbm', 'xgb']\nrmsle_scores = [linear_error, elastic_error, ridge_error, lasso_error, kernel_error, r_forest_error, \n                              g_boost_error, svr_error, knn_error, lgbm_error, xgb_error]\n\nplt.scatter(model_names, rmsle_scores)\nplt.xticks(rotation = 45)\nplt.title('Fitted Model RMSLE Scores')\nplt.ylabel('RMSLE')\nplt.xlabel('Model')\nplt.show()","6c49f48e":"blended_pred = (r_forest_pred*0.25 + g_boost_pred*0.15 + svr_pred*0.15 + lgbm_pred*0.30 + xgb_pred*0.15)\nblended_error = rmsle(y_train, blended_pred)\nprint(\"Blended Model RMSLE:\", blended_error)","63f20b9e":"model_names = ['linear', 'elastic', 'ridge', 'lasso', 'kernel', 'r_forest', \n               'g_boost', 'svr', 'knn', 'lgbm', 'xgb', 'blended']\nrmsle_scores = [linear_error, elastic_error, ridge_error, lasso_error, kernel_error, r_forest_error, \n                              g_boost_error, svr_error, knn_error, lgbm_error, xgb_error, blended_error]\nplt.scatter(model_names, rmsle_scores)\nplt.xticks(rotation = 45)\nplt.title('Fitted Model RMSLE Scores')\nplt.ylabel('RMSLE')\nplt.xlabel('Model')\nplt.show()","1fc459d7":"r_forest_pred = r_forest_model.predict(X_test)\ng_boost_pred = r_forest_model.predict(X_test)\nsvr_pred = r_forest_model.predict(X_test)\nlgbm_pred = r_forest_model.predict(X_test)\nxgb_pred = r_forest_model.predict(X_test)\n\nblended_pred = (r_forest_pred*0.25 + g_boost_pred*0.15 + svr_pred*0.15 + lgbm_pred*0.30 + xgb_pred*0.15)\n\npredictions = np.expm1(blended_pred)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('submission.csv', index = False)","884597ae":"submission.head()","a8cbe37f":"We now have our categorical variables as integers and with no missing values. Let's see how many integer missing values are left over.","5a5e8de4":"Linear relationships are exhibited, but let's consider the distribution of SalePrice and then check back in.","af3a39f3":"I will do a mode imputation on these null values, filling them with the highest frequency value from the column.","ef9d809a":"Look at correlation with SalePrice: OveralQual, TotalBsmtSF, GrLivArea, FullBath, and GarageCars stand out with the strongest correlations.\n\n### Check for Linear Relationship","d59b830c":"### Initialize Models\n\nWe will be looking at a multitude of models and then examining their scores. From there, we can blend the models.\n\nI will initialize all models with default values, except the seed.","fc9e9a61":"Now let's see if SalePrice and our chosen variables exhibit linearity again.","a76cc0c9":"### Models\n\n__Linear Regression:__ A linear approach to modeling the relationship between target and one or more predictors. It attempts to minimize the sum of error squared.\n\n__Ridge Regression:__ Decreases the model complexity by shrinking variable effects. Uses L2 regularization to add a penalty to the ordinary least squares (OLS) equation. Predictors with minor contribution have their ceofficients shrunk close to 0. This leads to lower variance and lower error value.\n\n__LASSO Regression:__ Shrinks regression coefficients toward 0 by penalizing the model with the L1 norm. Forces some coefficient estimates to equal 0, removing some predictors from the model and reducing complexity.\n\n__Elastic Net Regression:__ Regression model penalized with both the L1 and L2 norm. Shrinks some coefficients and sets some to 0.\n\n__Kernel Ridge Regression:__ Combines Ridge Regression with kernel trick. \n\n__Random Forest Regression:__ Ensemble of different regression trees. Constructs many decision trees at training time and outputs the class that is the mean prediction of the individual trees.\n\n__Gradient Boosting Regression:__ Boosting is an ensemble technique in which he predictors are made sequentially. Gradient boosting produces a prediction model in the form of an ensemble of weak prediction models, like decision trees.\n\n__Decision Tree Regression:__ Uses binary rules to calculate a target value. Breaks down dataset into smaller and smaller subsets that contain instances with similar values, finally resulting in a tree wtih decision nodes and leaf nodes. \n\n__Super Vector Regression:__ Principles of SVM applied to regression. Minimizes error, individualizing a hyperplane which maximizes the margin, and tolerating a part of error.\n\n__K-Nearest Neighbors Regression:__ Principles of KNN applied to regression. Uses the average of nearest data points to predict target.","2fc2167b":"### Handling Missing Values","291de2be":"## Pre-Processing Data\n\nCombine the train and test datasets' features to universally apply any transformations.","b325205f":"### Transforming Skewed Variables\n\nWe should check the normality of all the numeric variables.","a8b6100b":"### Is Sale Price Normally Distributed?","bde0eff0":"### Fitting Models and Evaluating Predictions\n\nLet's go ahead and train the models. \n\nWe will use Root Mean Squared Log Error as an evaluation metric. RMSLE is more robust to the effect of outliers. We also use it because RMSLE incurs a larger penalty from the underestimation of the value. If we think about this relative to the business standpoint of assessing house prices, underestimating a house price is less acceptable than overestimating a house price -- whether that be because we hypothetically work for a real estate company selling these houses, or some entity looking to acquire these houses. It is better to overestimate.","80c39584":"For our final predictions, I am going to use our blended model.\n\n## Submission","127d033c":"## Reading in Data","5806ce02":"It would be better to optimize the various parameters for the models we used. I might revisit that and some feature engineering at a later date. As of now, these steps have placed us in the top 67% of the competition.\n\nHope this was an enjoyable read.\n\nThank you!","f354ddcb":"A lower RMSE means the model has performed better. ","5b3b2238":"# Predicting Housing Prices","434efd12":"We can continue with our encoding process. I will just do label encoding because there are quite a few categorical values to manage, and one-hot encoding will get haphazard.","c24dd323":"Seems like we did a pretty solid job getting rid of many of the missing values. Let's do imputation on the rest now.\n\n### Imputation\n\nWe will replace the missing values with the mean value along each column. Since the encoded categorical variables no longer have missing values, we don't have to worry about subsetting that.","fd4a2afc":"SalePrice is quite right-skewed and does not reflect a normal distribution. In the normal probability plot, the values do not follow the diagonal line. In its current state, SalePrice does not satisfy the requirements for linear regression. We can apply a log transformation to fix this.","6abcb1f6":"We can see the RMSLE scores for the models.","7bec920e":"## Appendix","a2d8ebd9":"## Loading Libraries","d5a4ce4d":"### Categorical Variables\n\nNow, let's do a simple encoding of the categorical variables. I want to check the NA values here, too.","d52636a3":"I will later perform imputation some of these columns, but I don't want to keep columns with more than 33 percent of the data missing, which is ~1000 rows. Taking a look at those columns, it seems they are also not so important that I should argue to keep them.","1cc935ae":"### Load Libraries","5cab5cad":"## Modelling\n\nLet's look at the data we are working with and properly assign X_train, y_train, X_test.","5aad94be":"## Examine the Available Feautres\n\nFirst let's see what variables are best correlated with Sale Price,","d15a6139":"Let's get a summary of all the columns that have missing values, and how many there are.","a09e03fa":"Linearity checks out.","84b81483":"### K-Fold Cross Validation\n\nLet's use k-fold cross validation to evaluate model performance. We will show the cross validation root mean squared error.","65e65949":"Here we have all our models. Let's blend a few.","d70895f0":"Fixed well enough so that variables have more normal distributions."}}