{"cell_type":{"451418ad":"code","f7276e71":"code","54f0354e":"code","82e732bc":"code","c08d6762":"code","e645e893":"code","c503a680":"code","2add1f75":"code","d65f973a":"code","c9732cfd":"code","96019160":"code","04982b04":"code","52535bcb":"code","f352006d":"code","cd0b80a0":"code","a8d901d8":"code","6d186571":"code","1da4e622":"code","329483b1":"code","3ae8d3d7":"code","cfc8e720":"markdown","7041b957":"markdown","a9a27392":"markdown","23d807f0":"markdown","288a6859":"markdown","0a73838c":"markdown","acdf232f":"markdown","52effebe":"markdown","53c719cf":"markdown","4f990431":"markdown","d396d87c":"markdown","86719f52":"markdown","2111167c":"markdown","d9fffb79":"markdown"},"source":{"451418ad":"import os\nimport re\n\n# load train data\ntrain_file = '\/kaggle\/input\/tweets\/train.tsv' \ntrain_file = open(train_file,'r').readlines()\ntrain_file = [i.strip().split('\\t') for i in train_file]\ntrain_file = sorted(train_file, key=lambda x: x[0])\n\ntweet_ids = [i[0] for i in train_file]\nuser_ids = [i[1] for i in train_file]\ntweets = [i[2] for i in train_file]\nlabels = [i[3] for i in train_file]\n\n# load test data\ntest_file = '\/kaggle\/input\/tweets\/valid.tsv' \ntest_file = open(test_file,'r').readlines()\ntest_file = [i.strip().split('\\t') for i in test_file]\ntest_file = sorted(test_file, key=lambda x: x[0])\n\ntweet_ids_test = [i[0] for i in test_file]\nuser_ids_test = [i[1] for i in test_file]\ntweets_test = [i[2] for i in test_file]\nlabels_test = [i[3] for i in test_file]","f7276e71":"# removes column names\ndel tweet_ids_test[-1]\ndel user_ids_test[-1]\ndel tweets_test[-1]\ndel labels_test[-1]","54f0354e":"tweets[3:9]","82e732bc":"labels[3:9]","c08d6762":"# cleaning\ntweets = [tweet.lower() for tweet in tweets] # lowercase\ntweets = [re.sub(r'http\\S+', '', tweet) for tweet in tweets] # remove urls\ntweets = [re.sub('@[A-Za-z0-9]+', '', tweet) for tweet in tweets] # remove mentions\ntweets = [re.sub('[^A-Za-z0-9 ]+', '', tweet) for tweet in tweets] # remove non-alphanumeric characters","e645e893":"tweets[0]","c503a680":"N = len(labels)\n_80 = int(N*.8)\n\ntweets_train = tweets[:_80]\nlabels_train = labels[:_80]\n\ntweets_valid = tweets[_80:]\nlabels_valid= labels[_80:]","2add1f75":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport itertools\nimport numpy as np\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n        \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","d65f973a":"# importing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report","c9732cfd":"knn_pipe = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english')),\n        (\"classifier\", KNeighborsClassifier(n_neighbors=40, weights='distance')),\n    ]\n)\n\nknn_pipe.fit(tweets_train, labels_train)\nknn_pipe = knn_pipe.predict(tweets_valid)\n\ncr = classification_report(labels_valid, knn_pipe)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, knn_pipe, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='KNN')","96019160":"#NB Model\n#getting an error that this is sparse matrix, so tried to use DenseTransformer but didnt look good\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.base import TransformerMixin\n\nclass DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n    \nnb_pipe = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english')),\n        ('to_dense', DenseTransformer()),\n        (\"classifier\", GaussianNB()),\n    ]\n)\n\nnb_pipe.fit(tweets_train, labels_train)\npred_nb = nb_pipe.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_nb)\nprint(cr)\n\n#classes=list(set(labels_valid))\n#cm = confusion_matrix(labels_valid, pred_nb, labels=classes)\n#plot_confusion_matrix(cm, classes=classes, title='NB')","04982b04":"#Random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nrf_pipe = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english')), \n         # min_df=3, analyzer='char_wb', ngram_range = (1,5), max_df = 4500))\n        (\"classifier\", RandomForestClassifier())\n    ]\n)\n         \n#n_estimators=10, max_depth=None, min_samples_split=2, random_state=0\n\nrf_pipe.fit(tweets_train, labels_train)\npred_rf = rf_pipe.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_rf)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, pred_rf, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='Random forests')","52535bcb":"#Extra Trees model\n#best performing out of RF, ET, and DT (without parameter tuning)\net_pipe = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english', min_df=4, ngram_range = (1,4))),\n        (\"classifier\", ExtraTreesClassifier())\n    ]\n)\n         \n\net_pipe.fit(tweets_train, labels_train)\npred_et = et_pipe.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_et)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, pred_et, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='Extra Trees')","f352006d":"#Decision Tree model \n#worst performing model out of RF, ET, and DT\n\ndt_pipe = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english')),\n        (\"classifier\", DecisionTreeClassifier())\n    ]\n)\n         \n\ndt_pipe.fit(tweets_train, labels_train)\npred_dt = dt_pipe.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_dt)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, pred_dt, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='Decision Trees')","cd0b80a0":"#SVM model\nfrom sklearn.linear_model import SGDClassifier\n\nsvm_pipe = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english')),\n        (\"classifier\", SGDClassifier())\n    ]\n)\n         \n\nsvm_pipe.fit(tweets_train, labels_train)\npred_svm = svm_pipe.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_svm)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, pred_svm, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='SVM')","a8d901d8":"# EDIT\nrf_pipeline = Pipeline(\n    steps=[\n        (\"tfidf\", TfidfVectorizer(stop_words='english',\n                                 max_df=0.5,ngram_range=(1,2))), \n         # min_df=3, analyzer='char_wb', ngram_range = (1,5), max_df = 4500))\n        (\"classifier\", RandomForestClassifier(max_depth=None, min_samples_split=3,\n                                             n_estimators=250))\n    ]\n)\n         \n#n_estimators=10, max_depth=None, min_samples_split=2, random_state=0\n\nrf_pipeline.fit(tweets_train, labels_train)\npred_rf = rf_pipeline.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_rf)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, pred_rf, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='Random forests')","6d186571":"svm_pipeline = Pipeline(\n    [\n    ('vect', CountVectorizer(max_df = 0.5, max_features = None, ngram_range = (1,3))),\n    ('tfidf', TfidfTransformer(norm = 'l2', use_idf=True)),\n    ('clf', SGDClassifier(class_weight='balanced', eta0 = 0.0, fit_intercept = True, max_iter = 1500))\n    ]\n)\n\nsvm_pipeline.fit(tweets_train, labels_train)\npred_svm = svm_pipeline.predict(tweets_valid)\n\ncr = classification_report(labels_valid, pred_svm)\nprint(cr)\n\nclasses=list(set(labels_valid))\ncm = confusion_matrix(labels_valid, pred_svm, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='SVM')","1da4e622":"pred_test = rf_pipe.predict(tweets_test)\n\ncr = classification_report(labels_test, pred_test)\nprint(cr)\n\nclasses=list(set(labels_test))\ncm = confusion_matrix(labels_test, pred_test, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='Random Forest')","329483b1":"pred_test = svm_pipeline.predict(tweets_test)\n\ncr = classification_report(labels_test, pred_test)\nprint(cr)\n\nclasses=list(set(labels_test))\ncm = confusion_matrix(labels_test, pred_test, labels=classes)\nplot_confusion_matrix(cm, classes=classes, title='SVM')","3ae8d3d7":"#RANDOM FOREST GRIDSEARCH\nfrom pprint import pprint\nfrom time import time\nimport logging\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n\n# #############################################################################\ncategories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndata = fetch_20newsgroups(subset='train', categories=categories)\nprint(\"%d documents\" % len(data.filenames))\nprint(\"%d categories\" % len(data.target_names))\nprint()\n\n# #############################################################################\n# Define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', RandomForestClassifier()),\n])\n\n# uncommenting more parameters will give better exploring power but will\n# increase processing time in a combinatorial way\nparameters = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    # 'vect__max_features': (None, 5000, 10000, 50000),\n    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    # 'tfidf__use_idf': (True, False),\n    # 'tfidf__norm': ('l1', 'l2'),\n    'clf__n_estimators': (10, 100, 150, 200, 250, 300),\n    'clf__max_depth': (None, 1, 2, 5),\n    'clf__min_samples_split': (None, 1, 2, 3, 4),\n    # 'clf__max_iter': (10, 50, 80),\n}\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    pprint(parameters)\n    t0 = time()\n    grid_search.fit(tweets_train, labels_train)\n    print(\"done in %0.3fs\" % (time() - t0))\n    print()\n\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\n#SVM GRIDSEARCH\n\nfrom sklearn.linear_model import SGDClassifier\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n\n# #############################################################################\ncategories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndata = fetch_20newsgroups(subset='train', categories=categories)\nprint(\"%d documents\" % len(data.filenames))\nprint(\"%d categories\" % len(data.target_names))\nprint()\n\n# #############################################################################\n# Define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\n\n# uncommenting more parameters will give better exploring power but will\n# increase processing time in a combinatorial way\nparameters = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    'vect__max_features': (None, 5000, 10000, 50000),\n    'vect__ngram_range': ((1, 1), (1, 2), (1,3)),  # unigrams or bigrams\n    'tfidf__use_idf': (True, False),\n    'tfidf__norm': ('l1', 'l2'),\n    'clf__max_iter': (1000, 1250, 1500),\n    'clf__fit_intercept': (True, False),\n    'clf__class_weight': (None, 'balanced'),\n    'clf__eta0': (0.0, 'double')\n}\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    pprint(parameters)\n    t0 = time()\n    grid_search.fit(tweets_train, labels_train)\n    print(\"done in %0.3fs\" % (time() - t0))\n    print()\n\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","cfc8e720":"## Random Forest Model","7041b957":"## Decision Trees Model","a9a27392":"# Clean data","23d807f0":"## KNN Model","288a6859":"## Parameter tuning w Grid Search (SVM)","0a73838c":"## Grid Search","acdf232f":"## Test","52effebe":"## SVM Model","53c719cf":"# Models","4f990431":"## Parameter tuning w Grid Search (Random Forest)","d396d87c":"## NB Model","86719f52":"## Extra Trees Model","2111167c":"## Split Data","d9fffb79":"# Load in data"}}