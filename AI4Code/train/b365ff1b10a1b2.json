{"cell_type":{"5f431ab7":"code","ad3cbc19":"code","47beafaf":"code","ba698971":"code","661ca99f":"code","03019822":"code","16e8f68b":"code","6befccab":"code","2ffe615a":"code","76e7b512":"code","ce0ac15c":"code","c428cfdc":"code","f0c700f0":"code","fbffda84":"code","3f11eb7d":"code","92b14ed0":"code","26d3e7cc":"code","98ebeb45":"code","a79ea25f":"code","2aff9c70":"markdown","49f41eae":"markdown","15b9362e":"markdown","878b34ba":"markdown","19ef1325":"markdown","83546a97":"markdown","a2e40ff0":"markdown","3360ad42":"markdown","fda8dccc":"markdown","05612ef6":"markdown","5afec6be":"markdown","89fba53c":"markdown","11515e3e":"markdown","3c8791b7":"markdown","a1a52c38":"markdown","2fad4fb5":"markdown","cd26a38e":"markdown","54da4965":"markdown","bb520a96":"markdown","6c400fb9":"markdown","a545b83b":"markdown","c22677d1":"markdown","ed2735b9":"markdown","4ccf3ccc":"markdown"},"source":{"5f431ab7":"#Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","ad3cbc19":"orig = pd.read_csv('..\/input\/mushrooms.csv')","47beafaf":"#Shuffles the orig DataFrame\n#orig = orig.sample(frac=1)","ba698971":"orig.head()","661ca99f":"X = orig.drop(['class'], axis=1)\ny = orig['class']","03019822":"for attr in X.columns:\n    print('\\n*', attr, '*')\n    print(X[attr].value_counts())","16e8f68b":"X.drop(['veil-type'], axis=1, inplace=True)","6befccab":"for attr in X.columns:\n    fig, ax =plt.subplots(1,2)\n    sns.countplot(X[X['stalk-root']=='?'][attr], ax=ax[0]).set_title('stalk-root = ?')\n    sns.countplot(X[X['stalk-root']!='?'][attr], ax=ax[1]).set_title('stalk-root != ?')\n    fig.show()","2ffe615a":"#For columns with only two values\nfor col in X.columns:\n    if len(X[col].value_counts()) == 2:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col])","76e7b512":"X.head()","ce0ac15c":"X = pd.get_dummies(X)","c428cfdc":"X.head()","f0c700f0":"#New\n#train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.0)","fbffda84":"#New (used to show train_X is indeed the same as X, albeit, shuffled)\n#print(len(X))\n#print(len(train_X), len(train_y))\n#print(len(val_X), len(val_y))","3f11eb7d":"kmeans = KMeans(n_clusters=2, random_state=None)\n\n#Old\nkmeans.fit(X)\n\n#New\n#kmeans.fit(train_X)","92b14ed0":"#Old\nclusters = kmeans.predict(X)\n\n#New\n#clusters = kmeans.predict(train_X)","26d3e7cc":"clusters","98ebeb45":"cluster_df = pd.DataFrame()\ncluster_df['cluster'] = clusters\n\n#Old\ncluster_df['class'] = y\n\n#New\n#cluster_df['class'] = train_y","a79ea25f":"sns.factorplot(col='cluster', y=None, x='class', data=cluster_df, kind='count')","2aff9c70":"Read the original data.","49f41eae":"And now we 'one-hot-encode' the rest of the variables:","15b9362e":"# And once again, both clusters have a majority of edible mushrooms...\n# The problem seems to have to do with shuffling the DataFrame, but even that seems strange given how nice the original clustering result was (with most of the edibles in one cluster, and most of the poison edibles in the other).\n# Let me know what you think!","878b34ba":"We create a DataFrame to show how each cluster holds different shares of the poisonous mushrooms.","19ef1325":"# Encoding:","83546a97":"# Classification","a2e40ff0":"# Clustering: Problem begins here. I'd recommend not commenting anything out until you reach the end of the clustering section so that you can see the inital clustering result. Then after, read the block below for instructions to see how the bug arises. ","3360ad42":"Since the distributions are very different among the variables (meaning that the '?' values may not be irrelevant), we will not impute the '?' values for 'stalk-root', rather, we will encode them for our learning algorithms.","fda8dccc":"# Exploration","05612ef6":"Disregard the below statements.","5afec6be":"# **In this notebook we will compare using an unsupervised learning algorithm (K-Means Clustering) and a simple supervised learning algorithm (Logistic Regression) for binary classification on the mushroom data set. To compare our algorithms, we will use scikit-learn's test_train_split() method.**","89fba53c":"We'll use a binary encoding for variables that hold only 2 possible values, and a one-hot-encoding for variables that hold 3 or more possible values.","11515e3e":"We will cluster the data into two clusters, one will hold most of the 'poison' mushrooms and the other one will hold most of the 'edible' mushrooms.\n\nKeep in mind that the dataset, 'X' does not specify which mushrooms are 'edible' or 'poisonous', and so the clustering algorithm is ignorant of this too!","3c8791b7":"# End of clustering\n\n# As we can see from above, one cluster has clustered most of the' edible' mushrooms, while the other cluster has clustered most of the 'poisonous' mushrooms. Beautiful. \n# Now, to see the bug, run again starting at the Clustering header, but un-comment the '#New' statements and comment-in the '#Old' statements in the code-blocks to see the bug. And after that read the block below this one.","a1a52c38":"Second, the 'stalk-root' variable has a '?' value for it's missing values. Rather than impute this missing value, I will divide the dataset into two sections: where 'stalk-root'=='?' and where 'stalk-root'!='?', and analyze the distribution of each variable within those two data sets. ","2fad4fb5":"# And now both clusters hold a majority of edible mushrooms.... Weird, eh?\n\n# Next, I'd recommend recommenting-out the '#New' statements, uncommenting the old statements, then re-running this kernel, but with the 'orig' data frame shuffled. I left a piece of commented code at the top of the kernel that does this, that you should comment out. \n# So, to recap, you should re-run the kernel, but with the shuffle statement uncommented, the '#Old' statements uncommented, the '#New' statements commented-in, and observe the resulting factor plot for the clusters. After that, read the block below: ","cd26a38e":"Will get started here after clustering problem is figured out.","54da4965":"Before we encode each of our categorical variables in X & y with numbers so that algorithm can work with them, we will first take a look at the values contained within each of X's attributes..","bb520a96":"First, the 'veil-type' variable has only one value, 'p'. And since there is only one possible value, it gives us little information - so we can drop this column.","6c400fb9":"Two things to note here: ","a545b83b":"Now we will build our training data set so that the machine can work on it.","c22677d1":"# (Don't uncomment this next statement until instructed to below.)","ed2735b9":"We will use the 'class' attribute as our dependent variable (variable to be predicted) and all the other attributes as our independent variables (variables to be used for prediction).","4ccf3ccc":"From the above observation, we see that our clustering algorithm could potentially be used to classify mushrooms as either 'poisonous' or 'edible'.\n\nFor example, if we used our clustering model to place a mushroom in cluster 0, we could predict it to be edible, and if it placed the mushroom into cluster 1, we could predict it to be 'poisonous'. \n\nThis is what we'll do in the next section, and, just for fun, we will compare it's performance to sklearn's LogisticRegression."}}