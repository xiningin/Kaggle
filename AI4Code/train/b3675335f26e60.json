{"cell_type":{"832e6373":"code","95a4233b":"code","07121068":"code","66441c48":"code","87142abe":"code","c14a4b9f":"code","b07fb719":"code","0975b2fa":"code","a997b2fe":"markdown","1d0735ac":"markdown","9733c056":"markdown","c7a28fd2":"markdown","b44ce172":"markdown","05c126fb":"markdown","54e4c1ce":"markdown","52f05b74":"markdown","a66be123":"markdown","453d41db":"markdown"},"source":{"832e6373":"import os\nos.chdir('\/kaggle\/working\/')\nimport pandas as pd\n\n# for case 2\ndef half_images(df):\n    # create new empty dfs for each half\n    half_1 = pd.DataFrame(columns=df.columns)\n    half_2 = pd.DataFrame(columns=df.columns)\n\n    # remove half of training images for each label and add to half_1\n    for digit in range(10):\n        half_1 = pd.concat([half_1,\n                df[df['label'] == digit].sample(frac=0.5)],\n                ignore_index=True)\n\n    # add removed rows to half_2\n    half_2 = pd.concat([df, half_1, half_1]).drop_duplicates(keep=False)\n    return half_1, half_2\n\n# print('splitting dataset into half by images for case 2...')\n# half_1, half_2 = half_images(pd.read_csv('\/kaggle\/data\/case1\/train.csv'))\n# # save halfs to csv\n# half_1.to_csv('\/kaggle\/data\/case2\/train_halfImages.csv', index=False)\n# half_2.to_csv('\/kaggle\/data\/case2\/test_halfImages.csv', index=False)\n# print('finished splitting dataset into half by images for case 2')\n\n\n\n\n# for case 3\ndef half_digits(df):\n    # create new empty dfs for each half\n    half_1 = pd.DataFrame(columns=df.columns)\n    half_2 = pd.DataFrame(columns=df.columns)\n\n    # remove digits [0, 4] and add to half_1\n    for digit in range(5):\n        half_1 = pd.concat([half_1,\n                df.loc[df['label'] == digit]],\n                ignore_index=True)\n        # print(half_1)\n\n    # add removed rows to half_2\n    half_2 = pd.concat([df, half_1, half_1]).drop_duplicates(keep=False)\n    return half_1, half_2\n\n\n# print('splitting dataset into half by digits for case 3...')\n# half_1, half_2 = half_digits(pd.read_csv('\/kaggle\/data\/case1\/train.csv'))\n# # save halfs to csv\n# half_1.to_csv('\/kaggle\/data\/case3\/train_halfDigits.csv', index=False)\n# half_2.to_csv('\/kaggle\/data\/case3\/test_halfDigits.csv', index=False)\n# print('finished splitting dataset into half by images for case 3')","95a4233b":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nclass MNISTDataset(Dataset):\n    def __init__(self, file_path, transform=transforms.ToTensor()):\n        self.data = pd.read_csv(file_path)\n        self.transform=transform\n\n        # training data\n        if len(self.data.columns) == 785:\n            # select all columns except first (first column is labels)\n            self.X = self.data.iloc[:, 1:].values\n            # reshape X to 28x28\n            self.X = self.X.reshape((-1, 28, 28))\n            # convert X to numpy array and add axis\n            self.X = np.asarray(self.X, np.uint8)[:,:,:,None]\n\n            # select first column (labels) as y data\n            self.y = self.data.iloc[:, 0].values\n            # convert to tensor without compying (share memory)\n            self.y = torch.from_numpy(self.y)\n        # test data\n        else:\n            # reshape X to 28x28\n            self.X = self.data.values.reshape((-1, 28, 28))\n            # convert X to numpy array and add axis\n            self.X = np.asarray(self.X, np.uint8)[:,:,:,None]\n\n            # no labels for test set\n            self.y = None\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # training (return both x and y)\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        # testing (return only x)\n        else:\n            return self.transform(self.X[idx])","07121068":"import torch\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader\n\nimport torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nbatch_size = 16\n\n# transformations to apply on images\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,)) # mean and std of mnist dataset\n])\n# todo: add rotations transforms to trainset for data augmentation\n\ndata_set = MNISTDataset('\/kaggle\/input\/digitrecognizerdatafiles\/train.csv', transform)\n\n# split dataset into training and validation sets\n# use random split in 85\/15 ratio\n# better approach would be to use stratified sampling so that each digit class\n# gets sampled equally so validation set is balanced\n# see: https:\/\/stackoverflow.com\/a\/52284619\ntrain_set_len = int(len(data_set) * 0.85)\nvalid_set_len = len(data_set) - train_set_len\ntrain_set, valid_set = random_split(data_set, (train_set_len, valid_set_len))\n\ntrain_loader = DataLoader(train_set, batch_size, shuffle=True)\nvalid_loader = DataLoader(train_set, batch_size, shuffle=True)\n\n# create test set and test loader\ntest_set = MNISTDataset('\/kaggle\/input\/digitrecognizerdatafiles\/test.csv', transform)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","66441c48":"# network architecture\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.dropout = nn.Sequential(\n            nn.Flatten(),\n            # dimensions of flat layer using formula from\n            # https:\/\/stackoverflow.com\/a\/67790132\n            nn.Linear(32 * 7 * 7, 1024),\n            nn.Dropout(p=0.3),\n            nn.ReLU()\n        )\n        self.output = nn.Sequential(\n            nn.Linear(1024, 10),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.dropout(x)\n        x = self.output(x)\n        return x\n\nmodel = CNN()\n# put device on gpu if available\nmodel.to(device)\nprint('model architecture:')\nprint(model)","87142abe":"learning_rate = 1e-3\nn_epochs = 30\n\n# define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\nprint('loss function:', criterion)\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\nprint('optimizer: ', optimizer)","c14a4b9f":"# train network\ndef train(model, data_loader, valid_loader, criterion, optimizer,  n_epochs=20):\n    model.train()\n    for epoch in range(n_epochs):\n        running_train_loss = 0.0\n        running_valid_loss = 0.0\n        running_accuracy = 0.0\n        total = 0\n        # training loop\n        # for batch, (image, label) in enumerate(dataloader):\n        for (image, label) in train_loader:\n            # use GPU to train if available\n            image, label = image.to(device), label.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # compute prediction and loss\n            pred = model(image)\n            train_loss = criterion(pred, label)\n\n            # backpropogation\n            train_loss.backward()\n            optimizer.step()\n\n\n            running_train_loss += train_loss.item()\n\n        train_loss = running_train_loss \/ len(train_loader)\n\n        # validation loop\n        with torch.no_grad():\n            model.eval()\n            for (image, label) in valid_loader:\n                image, label = image.to(device), label.to(device)\n                pred = model(image)\n                valid_loss = criterion(pred, label)\n\n                # use max value to make prediction\n                _, predicted = torch.max(pred, 1)\n                running_valid_loss += valid_loss.item()\n                total += label.size(0)\n                running_accuracy += (predicted == label).sum().item()\n        val_loss = running_valid_loss \/ len (valid_loader)\n\n        accuracy = (100 * running_accuracy \/ total)\n\n        # log epoch training stats\n        print(f'completed epoch {(epoch + 1):02} of {n_epochs}, training loss: {train_loss:.4f}, validation loss: {valid_loss:.4f}, accuracy: {accuracy:.4f}')\n\nprint('training...')\ntrain(model, train_loader, valid_loader, criterion, optimizer, n_epochs)\nprint('finished training')","b07fb719":"def predict(model, data_loader):\n    with torch.no_grad():\n        model.eval()\n        predictions = torch.LongTensor()\n\n        for image_batch in data_loader:\n            image_batch = image_batch.to(device)\n\n            output_batch = model(image_batch)\n\n            # pred = output.cpu().data.max(1, keepdim=True)[1]\n            _, pred_batch = torch.max(output_batch, dim=1)\n            # test_pred = torch.cat((test_pred, pred), dim=0)\n            predictions = torch.cat((predictions.to(device), pred_batch.to(device)), dim=0)\n\n    return predictions\n\n\n# predict on test data\nprint('testing...')\npredictions_tenosr = predict(model, test_loader)\nprint(predictions_tenosr)","0975b2fa":"# first conver pytorch tensor to np array\npredictions_np = predictions_tenosr.cpu().numpy()\n# then create pandas df from np array with index column and column labels\nindicies = np.arange(1, len(test_set)+1)\npredictions_df = pd.DataFrame(np.c_[indicies[:, None], predictions_np], columns=['ImageId', 'Label'])\nprint(predictions_df.head())\n\n# convert dataframe to csv\npredictions_df.to_csv('\/kaggle\/working\/abdullahzr_cnn_digit_recognizer_test_submission.csv', index=False)","a997b2fe":"### custom PyTorch dataset class for MNIST handwritten digits\nstores samples and corresponding labels\ninherits from torch.utils.data.Dataset\noverrides __len__ and __getitem__\n\nsources used:\n\nhttps:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\n\nhttps:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html\n\nhttps:\/\/www.kaggle.com\/c\/digit-recognizer\/discussion\/79691\n\nhttps:\/\/www.kaggle.com\/tonysun94\/pytorch-1-0-1-on-mnist-acc-99-8","1d0735ac":"### Create train, validation, and test datasets and dataloaders","9733c056":"Abdullah Rehman (abdullahzr)","c7a28fd2":"### Train Model","b44ce172":"### Use Trained Model to Predict Unlabeled Test Data","05c126fb":"### Define HyperParameters, Loss Function, and Optimizer","54e4c1ce":"### Define Network Architecture","52f05b74":"### Code to split dataset by images (case 2) or split dataset by digits (case 2)\n\n- Case 2) - Training using only half the training set such that for each label, remove half of the training images. Name this file (train_halfImages.csv). Test on the other half of the images. Name that file (test_halfImages.csv).\n\n\n- Case 3) - Training using only half the training set such that only keep images for digits 0,1,2,3, and 4. Name this file (train_halfDigits.csv). Test on the other half of the images. Name that file (test_halfDigits.csv)\n\nsources used:\n\nhttps:\/\/stackoverflow.com\/questions\/41170971\/how-to-delete-fraction-of-rows-that-has-specific-attribute-value-from-pandas-dat\n\nhttps:\/\/stackoverflow.com\/questions\/34296292\/pandas-dropna-store-dropped-rows\n\nhttps:\/\/stackoverflow.com\/questions\/12850345\/how-do-i-combine-two-dataframes\n\nhttps:\/\/www.kaggle.com\/c\/digit-recognizer\/data?select=train.csv\n\nhttps:\/\/stackoverflow.com\/questions\/18180763\/set-difference-for-pandas","a66be123":"### Convert Predictions Tesnor to CSV with Column Titles for Submission on Kaggle","453d41db":"# Assignment 1\n## Getting Your Feet Wet With Machine Learning\n\nhttps:\/\/www.kaggle.com\/c\/digit-recognizer\/overview"}}