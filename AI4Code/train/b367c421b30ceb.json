{"cell_type":{"47f8b40f":"code","b9396a0b":"code","5276d77c":"code","fe9f0b44":"code","a6b6194f":"code","177024ad":"code","7ef746b5":"code","00cb2c18":"code","bbc0fb4d":"code","1aa5ba98":"code","93666b5a":"code","b6ee3fa9":"code","0309cbb6":"code","7d7f82dd":"code","e78ba5dc":"code","f8af7309":"code","76f915b4":"code","6eb4eff1":"code","d9500962":"code","a1203718":"code","5eac93a0":"code","eac968df":"code","a578c4bf":"code","0fb03b7e":"code","44357d1f":"code","38eb2c6a":"code","6dbc2625":"code","c56c3b50":"code","aab24998":"code","fe8f5ba8":"code","7161b01b":"code","059b4d89":"code","7ea15467":"code","00856cf7":"code","34480660":"code","6ab977a4":"code","72479163":"code","34363884":"code","c029caa4":"code","c209ab40":"code","5fa90c2d":"code","31174885":"code","dbd26d75":"code","c0f8ffbe":"code","8ab36544":"code","a258995d":"code","b4bb0fcf":"code","c0d9dfc4":"code","ad49659f":"code","b2e8efb6":"markdown","527f027a":"markdown","b4e6ba63":"markdown","b406093e":"markdown","9477d0b4":"markdown","146339f1":"markdown","da37fc00":"markdown","0cf2ab2b":"markdown","7302adb4":"markdown","24be25c5":"markdown","c4d63b95":"markdown","e3c3a898":"markdown","139865e3":"markdown","c7e92a1e":"markdown","0bc0df4e":"markdown","6d42f424":"markdown","20bf9708":"markdown","b3402ba5":"markdown","965d0973":"markdown","4478a5fa":"markdown","f3110282":"markdown","e7e6ef42":"markdown","61a49378":"markdown","78b5c0c6":"markdown"},"source":{"47f8b40f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#data wrangling\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9396a0b":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\n# .head shows the first 5 data rows of the data\n# here you can see an overview of all the features","5276d77c":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()\n# here the survived column is missing","fe9f0b44":"train_data['train_test'] = 1\ntest_data['train_test'] = 0\ntest_data['Survived'] = np.NaN\nall_data = pd.concat([train_data,test_data])\n%matplotlib inline\nall_data.columns\nprint(all_data.shape)","a6b6194f":"print(\"Training set shape\", train_data.shape)\nprint(\"Test set shape\", test_data.shape)","177024ad":"submission_data = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsubmission_data.head()","7ef746b5":"print(\"Submission set shape\", submission_data.shape)","00cb2c18":"# null count and data types for test data\ntest_data.info()","bbc0fb4d":"test_data.describe()","1aa5ba98":"# null count and data types for training data\ntrain_data.info()","93666b5a":"train_data.describe()","b6ee3fa9":"# missing data in training data set\n# missingno.matrix(training_data)","0309cbb6":"# quick way to separate numeric columns\ntrain_data.describe().columns","7d7f82dd":"# separate numeric and categorical values seperately\ndf_num = train_data [[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]]\ndf_cat = train_data [[\"Survived\",\"Pclass\",\"Sex\",\"Ticket\",\"Cabin\",\"Embarked\"]]","e78ba5dc":"# distributions for all numeric values\nfor i in  df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","f8af7309":"# outlier detection with bug\n#from collections import Counter\n#def detect_outliers(df, n, features):\n#    outlier_indices = []\n#    for col in features:\n#       Q1 = np.percentile(df[col], 25)\n#        Q3 = np.percentile(df[col], 75)\n#       IQR = Q3 - Q1\n#        outlier_step = 1.5 * IQR\n#        outlier_list_col = df[(df[col]) < Q1 - outlier_step | (df[col] > Q3  + outlier_step)].index\n#        outlier_indices.extend(outlier_list_col)\n#    outlier_indices = Counter(outlier_indices)\n#    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n)\n#    return mutliple_outliers\n\n#outliers_to_drop = detect_outliers(train_data, 2,[\"Age\",\"Sibsp\",\"Parch\",\"Fare\"])\n#print(\"We will drop these {} indices: \").format(len(outliers_to_drop)), outliers_to_drop)\n\n#train.loc[outliers_to_drop, :]\n\n\n","76f915b4":"print(df_num.corr())\nsns.heatmap(df_num.corr(), annot = True, fmt = \".2f\", cmap = \"coolwarm\")","6eb4eff1":"# compare survival rate across age, SibSp, Parch, and Fare\npd.pivot_table(train_data, index = \"Survived\", values = [\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","d9500962":"# visualize categorical variables\nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()\\\n    ","a1203718":"# comparing survival and eaach of these categorical variables\nprint(pd.pivot_table(train_data, index = \"Survived\", columns = \"Pclass\", values = \"Ticket\", aggfunc = \"count\"))\nprint()\nprint(pd.pivot_table(train_data, index = \"Survived\", columns = \"Sex\", values = \"Ticket\", aggfunc = \"count\"))\nprint()\nprint(pd.pivot_table(train_data, index = \"Survived\", columns = \"Embarked\", values = \"Ticket\", aggfunc = \"count\"))\nprint()","5eac93a0":"# how many women have survived?\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","eac968df":"# how many men have survived?\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","a578c4bf":"sns.barplot(x = \"Pclass\", y = \"Survived\", data = train_data)\nplt.ylabel(\"Survival Probability\")\nplt.xlabel(\"Survival Probability by Passanger Class\")","0fb03b7e":"# survival by gender and passenger class\n\ng = sns.factorplot(x = \"Pclass\", y = \"Survived\", hue = \"Sex\", data = train_data, kind = \"bar\")\ng.despine(left = True)\nplt.ylabel(\"Survival Probability\")\nplt.xlabel(\"Survival Probability by Sex and Passanger Class\")","44357d1f":"sns.barplot(x = \"Embarked\", y = \"Survived\", data = train_data)\nplt.ylabel(\"Survival Probability\")\nplt.xlabel(\"Survival Probability by Point of Embarkation\")","38eb2c6a":"sns.factorplot(\"Pclass\", col = \"Embarked\", data = train_data, kind = \"count\")","6dbc2625":"# RegEx with split on spaces\ndf_cat.Cabin\ntrain_data[\"cabin_multiple\"] = train_data.Cabin.apply(lambda x:0 if pd.isna(x) else len(x.split(\" \")))\ntrain_data[\"cabin_multiple\"].value_counts()","c56c3b50":"# How does the survival rate look like across these?\npd.pivot_table(train_data, index = \"Survived\", columns = \"cabin_multiple\", values = \"Ticket\", aggfunc = \"count\")","aab24998":"# Hypothesis: Cabins with the same letter might be in similar areas of the ship. Could that be a determining factor for survival?\n# create categories based on letters\n\ntrain_data[\"cabin_letter\"] = train_data.Cabin.apply(lambda x: str(x)[0])\n\n# comparing survival rate with cabin letter\nprint(train_data.cabin_letter.value_counts())\n\npd.pivot_table(train_data, index=\"Survived\", columns=\"cabin_letter\", values=\"Name\", aggfunc=\"count\")","fe8f5ba8":"# What about the name titles? \n# What kind of titles are used and do they have a better probability for survival?\ntrain_data.Name.head(25)\n","7161b01b":"# feature engineering persons title with RegEx\ntrain_data[\"name_title\"] = train_data.Name.apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\ntrain_data[\"name_title\"].value_counts()","059b4d89":"#create all categorical variables that we did above for both training and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(train_data.Age.mean())\nall_data.Age = all_data.Age.fillna(train_data.Age.median())\n#all_data.Fare = all_data.Fare.fillna(train_data.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(train_data.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape\nX_train.head()\n","7ea15467":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived\n","00856cf7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","34480660":"#I usually use Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","6ab977a4":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","72479163":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","34363884":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c029caa4":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c209ab40":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","5fa90c2d":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","31174885":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","dbd26d75":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c0f8ffbe":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","8ab36544":"#Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n#A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc)], voting = 'soft') ","a258995d":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","b4bb0fcf":"submission_data.head()","c0d9dfc4":"submission_data.shape","ad49659f":"# create submission data frame\nsubmit = pd.DataFrame(\"PassengerID\": test[PassengerID],\"Survived\": Y)","b2e8efb6":"We want our submission data frame to have 418 rows and 2 columns. PassengerId and Survived.","527f027a":"Finding: Missing values for age & cabin, and embarked","b4e6ba63":"**Findings**\nn stands for na and therefore nearly 3\/4 of the data were missing values but we could still use it as a categorical value. We can thereofre use them as a feature.\n\n1. Many people in the 0 column did not survive\n2. But people with clear defined cabin numbers had higher survival rates especially in decks B to E. Why is that?\n* S = sun deck\n* A = upper promenade deck\n* B = promenade deck glass enclosed\n* C = upper deck\n* D = salon deck\n* E = main deck\n* F = middle deck\n* G = lower deck\n\n![Cabin Classes](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png)","b406093e":"**Findings**:\n24 people had two ore more cabins\n180 had one cabin\n687 had no cabin at all","9477d0b4":"**Findings:**\nIs a higher title like royal, Countess, etc. better for survival and what about the Captain?\n\n![Captain Edward Smith](https:\/\/i2-prod.hampshirelive.news\/incoming\/article5184571.ece\/ALTERNATES\/s1200c\/0_Captain-Edward-Smith-Titanic-captain.jpg)\n","146339f1":"#### 3.3) Data preprocessing\n\n1. drop null values from \"embarked\"\n2. Induce only relevant variables\nexclude: PassengerID and prenames and surnames\n3. Do categorical transformation on all data. We can ensure that the training data and test data have the same columns. Note: not recommended for practical data science projects\n4. Inpute date with mean\/median for fare and age\n5. Normalizing fare using log to give more semblance of a normal distribution\n6. Scale data 0-1 with standard scaler","da37fc00":"** Findings **\n1. Younger people higher probability of surviving?\n2. People who paid a higher fare?\n3. People with parents on board?\n4. Fewer Siblings?","0cf2ab2b":"### ad 1) Business Understanding\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n![RMS Titanic](https:\/\/image.brigitte.de\/11048874\/t\/zu\/v4\/w1440\/r1.5\/-\/titanic-untergang.jpg)\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","7302adb4":"#### Ad 3.2) Feature engineering\nTicket and Cabin showed far too many columns why we would like to simplify this.\n1. Do passengers have multiple cabins?","24be25c5":"# MCI Titanic ML Project Example Walk Through\nIn this notebook, I try to show my students how a data scientist would go about working through a problem.\n\nGoal: Use Titanic passenger data to predict who will survive and who will die.\n\nNote: This is a binary classification prediction problem. Yes they will survive, or no they will not survive.\n\nOverview\nWe will use CRISP-DM to properly document all necessary stepts for this project:\n\n### 1) Business Understanding\n\n### 2) Data Understanding\n\n2.1) Understand data types & natur of the data .info() .describe()\n2.2) Histogram & Boxplots for numeric\n2.3) Value counts for categoric\n2.4) Correlation between metrics\n\n### 3) Data Preperation\n\n3.1) missing data remove or impuke\n3.2) Explore interesting topics\nDid the wealthy survive?\nBy location?\nAge scatterplot with ticket price\nyoung & wealthy?\ntotal spent?\n\n### 4) Data Modelling\n4.1) Feature engineering\n\n### 5) Evaluation\n\n### 6) Deployment","c4d63b95":"**Finding:** Missing values for age & cabin, and 2 for embarked","e3c3a898":"### 4) Data Modelling\nwe use cross validation.\n\n1. Random sample from training data\n2. Run model on some of training data and test model\n\n* Naive Bayes (72.6%)\n* Logistic Regression (82.1%)\n* Decision Tree (77.6%)\n* K Nearest Neighbor (80.5%)\n* Random Forest (80.6%)\n* **Support Vector Classifier (83.2%)**\n* Soft Voting Classifier - All Models (82.8%)","139865e3":"**Findings**\n1. A lot more people from 1st class survived. Might be relevant in model.\n2. More women have survived. Women and children first applied in scenario?\n3. If they got on board in Cherbourg they have a slightly higher chance of survival\n4. 1st class has highest probability of survival\n","c7e92a1e":"Hypothesis: Was the highest number of 1st class embarkments in Cherbourg?","0bc0df4e":"**Findings**: \n1. Families tend to travel together. Parents and siblings correlate.\n2. Age and number of siblings has a negative correlation\nNote: important for regression because we would like to avoid multicollinearity. Violation of assumption of interdependend variables.\n","6d42f424":"**Finding:**\nThere is a clear separation between prenam, title and Surname.","20bf9708":"**Finding:** Missing values for age & cabin, only 1 for fare","b3402ba5":"**Findings**:\nAge nearly normally distrubuted. All others are not. Perhaps we should take the non-normal distributions and consider normalizing them?\n","965d0973":"Our hypothesis seems to be true","4478a5fa":"**Findings**\n1. People with 4 cabins survived\n2. People with 3 cabins had same chances\n3. people with 2 cabins slightly higher survival rate\n4. people with 1 cabin much higher survival rate\n5. people with no cabin with mich lower survival rate","f3110282":"### 6) Deployment\nSubmit selected model","e7e6ef42":"Note that the test set has one columen less than training set, the Survived column. This is because \"Survived\" is a response column, ar also called a target varibale. Our job is to analyze the data in the training set and predict the survival of the passengers in the test set.","61a49378":"Here we import the data. For this analysis, we will be excluslively working with the Training set. We will be validating based in data from the training set as well. For our final submissions, we will make predictions based on the test set. ","78b5c0c6":"### ad 2) Data Understanding\n#### 2.1. Data Description\nWhat are the definitions of the data set as mentioned under the data tab on top of the page.\n* Survived: 0 = Did not survive, 1 = survived\n* Pclass: Ticket class where 1 = 1st class, 2 = 2nd clss, 3 = 3rd class. This can also been seen as a proxy for socio-economic status\n* Sex: Male or female\n* Sibsp: Number of siblings or spouses onboard the titanic\n* Parch: Number of partents or children onboard the titanic\n* Ticket: Passanger ticket number\n* Fare: Passanger fare\n* Cabin: Cabin number\n* Embarked: point of embarkment where C = Cherbourg, Q=Queenstown, S=Southhampton\n\n![Titanic Route](https:\/\/lh3.googleusercontent.com\/proxy\/sIGe-RxbPtS-kpkqvyvlaUUmKxEi8Px_HOc-eL6q9NW6BkBHr6NREhx1dAC14IgXcQdxRkcL1IU3ozbCDjTHdxPw53ihgw)\n\n#### 2.2 Exploratory Data Analysis (EDA)\n##### 1) For numeric data\n* check data types and null values\n* made histograms to understand distributions\n* corrplot\n* Pivot table comparing survival rate across numeric values\n\n##### 2) For categorical data\n* made barcharts to understand balance of classes\n* made pivot tables to understand the relationship with survival"}}