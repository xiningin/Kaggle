{"cell_type":{"da46da5c":"code","fea04ffa":"code","26315d0e":"code","5d7e071b":"code","c6780f56":"code","0b29c926":"code","0066eef3":"code","17f69cfd":"code","6a893271":"code","967e728c":"code","6cf3b174":"code","e190b522":"code","11501e2b":"code","80d2df38":"code","fbbf979f":"code","8bb0479e":"code","89417b8d":"code","e9422343":"code","98379103":"code","ee0d8aa6":"code","353c5c79":"code","cc287f2e":"code","52cbe1ba":"code","3e43b96a":"code","3daa0a81":"code","be75796d":"code","04834bf5":"markdown","92247feb":"markdown","83cd2baa":"markdown","f4839fcc":"markdown","5ccbc8b0":"markdown","b6084006":"markdown","f4556512":"markdown","e4c81b9d":"markdown","2362da2e":"markdown","9d724848":"markdown","8bb73d2d":"markdown","e768afe2":"markdown","32f0ba8d":"markdown","75717dc6":"markdown","502aa872":"markdown","63c7941b":"markdown"},"source":{"da46da5c":"# Importing Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","fea04ffa":"dataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\nX = dataset.iloc[:, 2:4].values\ny = dataset.iloc[:, 1].values\ndataset.head()","26315d0e":"# As y contains text, we need to encode it.\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","5d7e071b":"# Splitting dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","c6780f56":"# Applying Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","0b29c926":"# Trainig the Model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","0066eef3":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","17f69cfd":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('Logistic Regression')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","6a893271":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 35, metric = 'minkowski', p = 2)\n# I increased the value of K (Number of neighbours) as the model was overfitting with less number of neighbours.\nclassifier.fit(X_train, y_train)","967e728c":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","6cf3b174":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('K-Nearest Neighbor')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","e190b522":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","11501e2b":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","80d2df38":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('Support Vector Machine')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","fbbf979f":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0, C=0.5, gamma=0.5)\nclassifier.fit(X_train, y_train)","8bb0479e":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","89417b8d":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('Kernel SVM')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","e9422343":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","98379103":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","ee0d8aa6":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('Naive Bayes')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","353c5c79":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0, max_depth=3, min_samples_split=0.8)\nclassifier.fit(X_train, y_train)","cc287f2e":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","52cbe1ba":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('Decision Tree Classification')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","3e43b96a":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 0, max_depth=5)\nclassifier.fit(X_train, y_train)","3daa0a81":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint('Accuracy = '+str(accuracy_score(y_test, y_pred)))\n\nimport seaborn as sns\nplt.subplots(figsize=(5,5))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax);\n\nax.set_xlabel('Prediction');ax.set_ylabel('Label'); \nax.set_title('Confusion Matrix'); ","be75796d":"def Label(val):\n    if val==0:\n        return 'Malignant'\n    else:\n        return 'Benign'\nfrom matplotlib.colors import ListedColormap\nplt.style.use('fivethirtyeight')\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.15),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.3, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = Label(j), alpha = 0.7, s = 50)\nplt.title('Random Forest Classification')\nplt.xlabel('Mean-Texture')\nplt.ylabel('Mean-Radius')\nplt.axis([5,25,0,50])\nplt.legend(loc = 'upper left')\nplt.show()","04834bf5":"# [Random Forest Classification](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**Random decision trees or random forest are an ensemble learning method for classification, regression, etc. It operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes or classification or mean prediction(regression) of the individual trees.**","92247feb":"## [Lets Start with Understanding what is Classification?](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. A classification model attempts to draw some conclusion from observed values. Given one or more inputs a classification model will try to predict the value of one or more outcomes.\nFor example:- When filtering emails \u201cspam\u201d or \u201cnot spam\u201d, when looking at transaction data, \u201cfraudulent\u201d, or \u201cauthorized\u201d.**","83cd2baa":"# [Naive Bayes](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**It is a classification algorithm based on Bayes\u2019s theorem which gives an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Even if the features depend on each other, all of these properties contribute to the probability independently. Naive Bayes model is easy to make and is particularly useful for comparatively large data sets.**","f4839fcc":"## **[Algorithms that we will consider:-](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)**\n* Logistic Regression\n* K-Nearest Neighbour\n* Support Vector Machine\n* Kernel SVM\n* Naive Bayes\n* Decision Tree Classification\n* Random Forest Classification","5ccbc8b0":"## [Preparing Dataset](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)","b6084006":"# Thank you very much for your attention to my work. I wish you great datasets for research!!..\n![](https:\/\/i.pinimg.com\/originals\/4f\/92\/fe\/4f92fe4ee07e79bc3495e41bb5ae1bd3.gif)","f4556512":"# [Logistic Regression](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\nIt is a classification algorithm in machine learning that uses one or more independent variables to determine an outcome. The outcome is measured with a dichotomous variable meaning it will have only two possible outcomes. It was derives by equating the Linear Regresson function with the Sigmoid function:\n* **y = a0 + a1X1 + a2X2 \u2026. + anXn - Linear Regression Function**\n* **p = 1 \/ (1 + e^(-y)) - Sigmoid Function**\n\nOn equating the above 2, We get:-\n* **p = 1 \/ 1 + e^(-(a0 + a1X1 + a2X2 \u2026. + anXn)) - Logistic Regression Function**\n","e4c81b9d":"## [About Dataset:](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\nI am using Breast-Cancer dataset for this kernel as it is one of the most popular and easy to understand dataset. I will be predict a tumour as malignant or benign on the basis of mean-texture and mean-radius (To show the working of classifying algorithms).","2362da2e":"## [Table of Contents:](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n* **Understanding Classification**\n* **How does Classification Work?**\n* **Types of Algorithms**\n* **Testing of Algorithms**","9d724848":"# [K-Nearest Neighbor](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**Classification is computed from a simple majority vote of the k nearest neighbors of each point. It is supervised and takes a bunch of labeled points and uses them to label other points. To label a new point, it looks at the labeled points closest to that new point also known as its nearest neighbors. It has those neighbors vote, so whichever label the most of the neighbors have is the label for the new point. The \u201ck\u201d is the number of neighbors it checks.**","8bb73d2d":"# [Support Vector Machine](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**The support vector machine is a classifier that represents the training data as points in space separated into categories by a gap as wide as possible. New points are then added to space by predicting which category they fall into and which space they will belong to.**","e768afe2":"# [Decision Tree Classification](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**The decision tree algorithm builds the classification model in the form of a tree structure. It utilizes the if-then rules which are equally exhaustive and mutually exclusive in classification. The process goes on with breaking down the data into smaller structures and eventually associating it with an incremental decision tree. The final structure looks like a tree with nodes and leaves. The rules are learned sequentially using the training data one at a time. Each time a rule is learned, the tuples covering the rules are removed. The process continues on the training set until the termination point is met.**","32f0ba8d":"# [Kernel SVM](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**The support vector machine is a classifier that represents the training data as points in space separated into categories by a gap as wide as possible. New points are then added to space by predicting which category they fall into and which space they will belong to.**","75717dc6":"## [We can apply machine learning model by following six steps:-](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n* Indentifying Problem\n* Analysing Data\n* Preparing Data\n* Evaluating Algorithm\n* Improving Results\n* Presenting Results","502aa872":"## [How does Classification Work?](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)\n**The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which class\/category the new data will fall into.**\n![](https:\/\/www.comodo.com\/images\/best-free-spam-removal-software.png)\n**Spam mail detection can be identified as a classification problem, this is a binary classification since there can be only two classes i.e mail is spam or not. The classifier, in this case, needs training data to understand how the given input variables are related to the class. And once the classifier is trained accurately, it can be used to detect whether a particular mail is spam or not.**","63c7941b":"# **[Beginners Guide to Classification Analysis and Plot Intrepretation](https:\/\/www.kaggle.com\/kshitijmohan\/lstm-stock-prediction)**\n![](https:\/\/expertsystem.com\/wp-content\/uploads\/2017\/03\/machine-learning-definition.jpeg)\n### Earlier I made a notebook on Regression([Open Here](https:\/\/www.kaggle.com\/kshitijmohan\/regression-complete-analysis)) and this time We'll be focussing on Classification."}}