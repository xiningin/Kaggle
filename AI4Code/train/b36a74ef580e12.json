{"cell_type":{"d5f82f9a":"code","afda0830":"code","9f3a8039":"code","01b6ed1a":"code","3464451f":"code","9b18ac31":"code","a27fa6b3":"code","6618fc70":"code","61f44a7c":"code","afc9fc29":"code","e8d2f989":"code","e2fca8d4":"code","44631762":"code","8e5c0d66":"code","4c1eb491":"code","0311ddcb":"code","fa1dc794":"code","18d5a82a":"code","130fb5f7":"code","7f33bf11":"code","7febdff2":"code","d5682f74":"code","516d3c92":"code","621f1bbd":"code","ac6656db":"code","217546fd":"code","af976090":"code","e7f81777":"code","c455b540":"code","96515c2a":"code","4a54042b":"code","519a904b":"code","c63c20a7":"code","a1579aaa":"code","25fea6c1":"code","7b0ae079":"code","549531d4":"code","38f0c54d":"code","e7fa5f94":"code","bbbc5019":"code","44c6ba18":"code","dcdf03ae":"code","ad37c2f6":"code","24510691":"code","2b3c7da1":"code","140ab5ad":"code","c01cfc50":"code","491c1ffc":"code","f3ad1b6f":"code","e57acdb9":"code","9129c6b7":"code","c374199e":"code","e915e5e1":"code","2ba03592":"code","552ee57b":"code","50c0eb25":"code","156aacbb":"code","8eb61353":"code","4d801a61":"code","68b92a71":"code","aac23586":"code","c5dbb153":"code","b73f72d2":"code","8621cd2e":"code","49237be4":"code","2d1e12a9":"code","7f278311":"markdown","f0d88c3d":"markdown","ab493932":"markdown","8c6b8a2e":"markdown","12c6c8fc":"markdown","6a18e759":"markdown","0162f082":"markdown","9e3caea1":"markdown","7093ba69":"markdown","2b34820e":"markdown","f6f31453":"markdown","fc959772":"markdown","f55d3640":"markdown","b075e5cb":"markdown","9bd3f601":"markdown","8793089b":"markdown","d748ae7e":"markdown","a8ccc640":"markdown","08443aaa":"markdown","2c41b6fb":"markdown","5309d82c":"markdown","c8a338a7":"markdown","3a3dfc08":"markdown"},"source":{"d5f82f9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# DEEP LEARNING PACKAGES\n\n\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport time, gc\nimport random\nimport cv2\n\nfrom tensorflow import keras\nimport matplotlib.image as mpimg\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.models import clone_model\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n\n# set the matplotlib backend so figures can be saved in the background\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\n# import the necessary keras and sklearn packages\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n","afda0830":"class_map = pd.read_csv(\"..\/input\/bengaliai-cv19\/class_map.csv\")\nsample_submission = pd.read_csv(\"..\/input\/bengaliai-cv19\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/bengaliai-cv19\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/bengaliai-cv19\/train.csv\")","9f3a8039":"train.head()","01b6ed1a":"train.tail()","3464451f":"print(len(train))\nprint(len(test))","9b18ac31":"train.describe()","a27fa6b3":"class_map.head()\n","6618fc70":"class_map.describe()","61f44a7c":"len(class_map.label)\n","afc9fc29":"len(class_map.label.unique())","e8d2f989":"class_map.component_type.value_counts()","e2fca8d4":"print(f'Size of training data: {train.shape}')\nprint(f'Size of test data: {test.shape}')\nprint(f'Size of class map: {class_map.shape}')","44631762":"train_df_groot = train.groupby(['grapheme_root']).size().reset_index()\ntrain_df_groot=train_df_groot.rename(columns={0:'count'})","8e5c0d66":"class_map_df_groot = class_map[class_map.component_type=='grapheme_root']\ngroot_merged = pd.merge(train_df_groot,class_map_df_groot[['label','component']],left_on='grapheme_root',right_on='label',how='inner')\ngroot_merged.sort_values(by=\"count\",ascending=False)[:10]","4c1eb491":"groot_merged.head()","0311ddcb":"train_df_vd = train.groupby(['vowel_diacritic']).size().reset_index()\ntrain_df_vd=train_df_vd.rename(columns={0:'count'})\nclass_map_df_vd = class_map[class_map.component_type=='vowel_diacritic']\nvd_merged = pd.merge(train_df_vd,class_map_df_vd[['label','component']],left_on='vowel_diacritic',right_on='label',how='inner')\nvd_merged.sort_values(by=\"count\",ascending=False)[:10]","fa1dc794":"train_df_cd = train.groupby(['consonant_diacritic']).size().reset_index()\ntrain_df_cd=train_df_cd.rename(columns={0:'count'})\nclass_map_df_cd = class_map[class_map.component_type=='consonant_diacritic']\ncd_merged = pd.merge(train_df_cd,class_map_df_cd[['label','component']],left_on='consonant_diacritic',right_on='label',how='inner')\ncd_merged.sort_values(by=\"count\",ascending=False)[:5]","18d5a82a":"train.head()","130fb5f7":"train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')","7f33bf11":"print(f\"Train: unique grapheme roots: {train.grapheme_root.nunique()}\")\nprint(f\"Train: unique vowel diacritics: {train.vowel_diacritic.nunique()}\")\nprint(f\"Train: unique consonant diacritics: {train.consonant_diacritic.nunique()}\")\nprint(f\"Train: total unique elements: {train.grapheme_root.nunique() + train.vowel_diacritic.nunique() + train.consonant_diacritic.nunique()}\")\nprint(f\"Class map: unique elements: \\n{class_map.component_type.value_counts()}\")\nprint(f\"Total combinations: {pd.DataFrame(train.groupby(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'])).shape[0]}\")","7febdff2":"def plot_count_heatmap(feature1, feature2, df, size=1):  \n \n    tmp = train.groupby([feature1, feature2])['grapheme'].count()\n    df = tmp.reset_index()\n    df\n    df_m = df.pivot(feature1, feature2, \"grapheme\")\n    f, ax = plt.subplots(figsize=(9, size * 4))\n    sns.heatmap(df_m, annot=True, fmt='3.0f', linewidths=.5, ax=ax)","d5682f74":"plot_count_heatmap('vowel_diacritic','consonant_diacritic', train)","516d3c92":"train = train.drop(['grapheme'], axis=1, inplace=False)","621f1bbd":"train.head()","ac6656db":"img_0=pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/train_image_data_0.parquet')\nimg_0.iloc[:,1:]=img_0.iloc[:,1:].astype('uint8')","217546fd":"train_df_0 = pd.merge(train,img_0,on='image_id',how='inner')","af976090":"train_df_0.head()","e7f81777":"train_df_0.shape","c455b540":"train_df_0.vowel_diacritic.value_counts()","96515c2a":"train_df_0.consonant_diacritic.value_counts()","4a54042b":"train_df_0.grapheme_root.value_counts()","519a904b":"img = img_0.iloc[0,1:]\nimg=img.astype(int)\nimg = np.array(img).reshape(137,236)\nplt.imshow(img);","c63c20a7":"img = img_0.iloc[10,1:]\nimg=img.astype('float32')\nimg = np.array(img).reshape(137,236)\n# Construct image object from array, needed for resizing\nimg = Image.fromarray(img)\nplt.imshow(img);","a1579aaa":"img_resized = img.resize((96,96))\nplt.imshow(img_resized);","25fea6c1":"img_resized=np.array(img_resized).reshape(96,96,1)","7b0ae079":"class BengaliNet:\n    @staticmethod\n    def build_grapheme_branch(inputs, numGraphemes,finalAct=\"sigmoid\", chanDim=-1):\n \n        x = Conv2D(32, (3, 3), padding=\"same\")(inputs)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(3, 3))(x)\n        x = Dropout(0.25)(x)\n        \n        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(128, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = Conv2D(128, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(256)(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(numGraphemes)(x)\n        x = Activation(finalAct, name=\"grapheme_output\")(x)\n \n        # return the Grapheme prediction sub-network\n        return x\n    \n    @staticmethod\n    def build_vowel_branch(inputs, numVowels, finalAct=\"sigmoid\",chanDim=-1):\n\n        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(3, 3))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(128)(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(numVowels)(x)\n        x = Activation(finalAct, name=\"vowel_output\")(x)\n\n        # return the vowel prediction sub-network\n        return x\n    \n    @staticmethod\n    def build_consonant_branch(inputs, numConsonants, finalAct=\"sigmoid\",chanDim=-1):\n\n        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(3, 3))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = Conv2D(64, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(128)(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(numConsonants)(x)\n        x = Activation(finalAct, name=\"consonant_output\")(x)\n\n        # return the consonant prediction sub-network\n        return x\n    \n    @staticmethod\n    def build(width, height, numGraphemes, numVowels, numConsonants, finalAct=\"sigmoid\"):\n        # initialize the input shape and channel dimension (this code\n        # assumes you are using TensorFlow which utilizes channels\n        # last ordering)\n        inputShape = (height, width,1)\n        chanDim = -1\n\n        # construct both the \"grapheme\" , \"vowel\", and \"consonant\" sub-networks\n        inputs = Input(shape=inputShape)\n        graphemeBranch = BengaliNet.build_grapheme_branch(inputs,\n            numGraphemes, finalAct=finalAct, chanDim=chanDim)\n        vowelBranch = BengaliNet.build_vowel_branch(inputs,\n            numVowels, finalAct=finalAct, chanDim=chanDim)\n        consonantBranch = BengaliNet.build_consonant_branch(inputs,\n            numConsonants, finalAct=finalAct, chanDim=chanDim)\n\n        # create the model using our input (the batch of images) and\n        # three separate outputs -- one for the grapheme\n        # branch, the vowel branch, and consonant branch respectively\n        model = Model(\n            inputs=inputs,\n            outputs=[graphemeBranch, vowelBranch, consonantBranch],\n            name=\"Bengalinet\")\n\n        # return the constructed network architecture\n        return model","549531d4":"EPOCHS = 50\nINIT_LR = 1e-3\nBS = 32\nIMAGE_DIMS = (96, 96, 1)","38f0c54d":"data = []\ngraphemeLabels = []\nvowelLabels = []\nconsonantLabels = []","e7fa5f94":"del train\ngc.collect()","bbbc5019":"train_df_0.head()","44c6ba18":"graphemeLabels = train_df_0.grapheme_root\ntrain_df_0=train_df_0.drop(\"grapheme_root\",axis=1)","dcdf03ae":"vowelLabels = train_df_0.vowel_diacritic\ntrain_df_0=train_df_0.drop(\"vowel_diacritic\",axis=1)","ad37c2f6":"train_df_0.head()","24510691":"consonantLabels = train_df_0.consonant_diacritic\ntrain_df_0=train_df_0.drop(\"consonant_diacritic\",axis=1)","2b3c7da1":"train_df_0.head()","140ab5ad":"for i in range(len(train_df_0)):\n    img = train_df_0.iloc[i,1:]\n    img=img.astype('float32')\n    img = np.array(img).reshape(137,236)\n    img = Image.fromarray(img)\n    img_resized = img.resize((96,96))\n    img_resized = np.array(img_resized).reshape(96,96,1)\n    data.append(img_resized)","c01cfc50":"graphemeLabels[:10]","491c1ffc":"data[0].shape","f3ad1b6f":"import sklearn\nfrom sklearn import preprocessing\nsklearn.preprocessing.label_binarize\n\n\ndata = np.array(data, dtype=\"float\") \/ 255.0\n \n# convert the label lists to NumPy arrays prior to binarization\ngraphemeLabels = np.array(graphemeLabels)\nvowelLabels = np.array(vowelLabels)\nconsonantLabels = np.array(consonantLabels)\n \n# binarize all three sets of labels\nprint(\"[INFO] binarizing labels...\")\ngraphemeLB = preprocessing.LabelBinarizer()\nvowelLB = preprocessing.LabelBinarizer()\nconsonantLB = preprocessing.LabelBinarizer()\ngraphemeLabels = graphemeLB.fit_transform(graphemeLabels)\nvowelLabels = vowelLB.fit_transform(vowelLabels)\nconsonantLabels = consonantLB.fit_transform(consonantLabels)\n \nprint(graphemeLabels.shape)\nprint(vowelLabels.shape)\nprint(consonantLabels.shape)","e57acdb9":"train_df_0=train_df_0.drop('image_id',axis=1)","9129c6b7":"train_df_0.head()","c374199e":"def preprocess(image):\n    resized_image = tf.image.resize(image,[96,96])\n    final_image = tf.keras.applications.xception.preprocess_input(resized_image)\n    return final_image\n","e915e5e1":"# partition the data into training and testing splits using 90% of\n# the data for training and the remaining 10% for testing\n(trainX, testX, trainGraphemeY, testGraphemeY,trainVowelY, testVowelY,trainConsonantY,testConsonantY) = train_test_split(train_df_0, graphemeLabels, vowelLabels,consonantLabels,test_size=0.1, random_state=42)","2ba03592":"trainX=np.array(trainX).reshape(trainX.shape[0],137,236,1)\n#trainX=tf.convert_to_tensor(trainX)\ntrainX.shape","552ee57b":"testX=np.array(testX).reshape(testX.shape[0],137,236,1)\n#testX=tf.convert_to_tensor(testX)\ntestX.shape","50c0eb25":"EPOCHS = 1\nINIT_LR = 1e-3\nBS = 32\nIMAGE_DIMS = (96, 96, 1)","156aacbb":"import tensorflow as tf","8eb61353":"train_set =  tf.data.Dataset.from_tensor_slices((trainX))\ntrain_set=train_set.map(preprocess).batch(BS).prefetch(1)\nval_set =  tf.data.Dataset.from_tensor_slices((testX))\nval_set=val_set.map(preprocess).batch(BS).prefetch(1)","4d801a61":"train_Y=tf.data.Dataset.from_tensor_slices((trainGraphemeY, trainVowelY, trainConsonantY))\ntrain_Y = train_Y.batch(BS).prefetch(1)","68b92a71":"test_Y=tf.data.Dataset.from_tensor_slices((testGraphemeY, testVowelY, testConsonantY))\ntest_Y = test_Y.batch(BS).prefetch(1)","aac23586":"dataset  = tf.data.Dataset.zip((train_set, train_Y))","c5dbb153":"val_dataset = tf.data.Dataset.zip((val_set, test_Y))","b73f72d2":"import keras\nclass MultiOutputDataGenerator(tf.keras.preprocessing.image.ImageDataGenerator):\n\n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n\n\n        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n                                         shuffle=shuffle):\n            target_dict = {}\n            i = 0\n            for output in ordered_outputs:\n                target_length = target_lengths[output]\n                target_dict[output] = flowy[:, i: i + target_length]\n                i += target_length\n\n            yield flowx, target_dict","8621cd2e":"# Data augmentation for creating more training data\ndatagen = MultiOutputDataGenerator(\n    featurewise_center=False,  # set input mean to 0 over the dataset\n    samplewise_center=False,  # set each sample mean to 0\n    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n    samplewise_std_normalization=False,  # divide each input by its std\n    zca_whitening=False,  # apply ZCA whitening\n    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n    zoom_range = 0.20, # Randomly zoom image \n    width_shift_range=0.20,  # randomly shift images horizontally (fraction of total width)\n    height_shift_range=0.20,  # randomly shift images vertically (fraction of total height)\n    horizontal_flip=False,  # randomly flip images\n    vertical_flip=False)  # randomly flip images\n#datagen.fit(trainX)","49237be4":"from keras.layers import Activation, Dense","2d1e12a9":"#strategy = tf.distribute.MirroredStrategy()\n\n#with strategy.scope():\n# initialize our BengaliNet multi-output network\nmodel = BengaliNet.build(96, 96,numGraphemes=len(graphemeLB.classes_),numVowels=len(vowelLB.classes_),numConsonants=len(consonantLB.classes_),finalAct=\"softmax\")\n\n# define two dictionaries: one that specifies the loss method for\n# each output of the network along with a second dictionary that\n# specifies the weight per loss\nlosses = {\n    \"grapheme_output\": \"categorical_crossentropy\",\n    \"vowel_output\": \"categorical_crossentropy\",\n    \"consonant_output\": \"categorical_crossentropy\"\n}\nlossWeights = {\"grapheme_output\": 1.0, \"vowel_output\": 1.0, \"consonant_output\":1.0}\n\n# initialize the optimizer and compile the model\nprint(\"[INFO] compiling model...\")\nopt = tf.keras.optimizers.Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(optimizer=opt, loss=losses, loss_weights=lossWeights,metrics=[\"accuracy\"])","7f278311":"### Create the X and y from the Images and labels (Just taking 50 values to check the process)","f0d88c3d":"Define Multi Channel CNN, one channel each for Graphemes, vowels and consonants","ab493932":"# Binarize the Labels","8c6b8a2e":"> Create TensorFlow Dataset from images, after resizing\u00b6","12c6c8fc":"Unique values\nWe look here to the distribution of grapheme roots, vowel diacritics and consonant diacritics.","6a18e759":"Read the image file data from the first parquet file","0162f082":"# Import the required Packages","9e3caea1":"Build and train the model","7093ba69":"Top 10 Vowel Diacritic in taining data (There are only 11)","2b34820e":"del train_df_0\ngc.collect()","f6f31453":"> <font color = blue> **Bengali is my mother tounge and I am really interested and having fun to work and improve Bengali Handwritten Grapheme. \n**","fc959772":"# Heatmap\n\n**Heatmap showing the distribution of couple of features**","f55d3640":"Merge the images and labels to create train dataset and validation dataset","b075e5cb":"![maxresdefault.jpg](attachment:maxresdefault.jpg)","9bd3f601":"Top 10 Grapheme Roots","8793089b":"Basically I am inspired from some other kernal and notebooks and learning so far.","d748ae7e":"Top 5 Consonant Diacritic in training data","a8ccc640":"# Bengali.AI Handwritten Grapheme Classification\n\nBengali is the 5th most spoken language in the world with hundreds of million of speakers. It\u2019s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there\u2019s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.\n\n\n\nOptical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English\u2019s 250 graphemic units).\n","08443aaa":"Create TensorFlow Dataset from labels","2c41b6fb":"# *Resize an image using openCV and check (resize to (96,96) input shape for CNN)*","5309d82c":"### Calling the dataframe","c8a338a7":"Following code is a function to create a Heatmap.\nThanks to Gabriel Preda\n\n- feature1 - ex: vowel_diacritic\n- feature2 - ex: consonant_diacritic","3a3dfc08":"# <font color = RED**>  EDA"}}