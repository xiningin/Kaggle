{"cell_type":{"e6ccc9b4":"code","10af5fec":"code","2ada69ae":"code","dc16f913":"code","942e3c9e":"code","54bc5995":"code","e22def9d":"code","c1384131":"code","fce29768":"code","c9290f27":"code","7b6f2a83":"code","8e5eed1e":"code","20ecbe11":"code","561101f2":"code","6320e3c2":"code","3d05ef62":"code","1da9fd81":"code","07d42b0b":"code","0d0798b9":"code","5b9c7c74":"code","9471153b":"code","d715e294":"code","964043bd":"code","6d6a712e":"code","f9d586df":"code","76eba860":"code","1e6ed184":"code","32f02a72":"code","ff4f6ca8":"code","1abec51a":"code","f4c388b7":"code","70b7f6c0":"code","12f8dff9":"code","9925bed1":"code","171dee98":"code","4c07b3ab":"code","643262c2":"code","85d26827":"code","bed3559f":"code","db93bddb":"code","98f1466f":"code","d963763c":"code","6005e8aa":"code","e8ed577c":"code","6ec7aa1e":"code","b9b7e473":"code","5ede0b6e":"code","90e20fbf":"code","1b8c7883":"code","5fccc23d":"code","9aeaac46":"code","aaaf87fe":"code","fecac76a":"markdown"},"source":{"e6ccc9b4":"!pip install --upgrade xverse scikit-optimize catboost --user\n","10af5fec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2ada69ae":"from xverse.transformer import WOE\nfrom xverse.graph import BarCharts\nfrom xverse.ensemble import VotingSelector\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier, Pool, cv, CatBoost\nfrom catboost.utils import get_roc_curve\nfrom catboost.utils import get_fpr_curve\nfrom catboost.utils import get_fnr_curve\nimport pprint\n# Metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, accuracy_score, matthews_corrcoef\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\nfrom skopt.space import Real, Categorical, Integer\n\nfrom time import time\nimport shap\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport warnings\nimport numexpr as ne\nimport psutil\nimport random","dc16f913":"\nprint(psutil.cpu_count(logical=False))\nprint( ne.detect_number_of_cores())","942e3c9e":"\nRANDOM_STATE = 1242\nrandom.seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_STATE)\n\nNUM_THREADS = 2\nNUM_THREADS_FILE = 2\nNUM_THREADS_PRED = 2","54bc5995":"pd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)","e22def9d":"input_path = '\/kaggle\/input\/hr-analytics-case-study\/'\noutput_path = '\/kaggle\/working\/'","c1384131":"target_col = 'Attrition'\nindex_col = 'EmployeeID'","fce29768":"\nclass DataPrepare(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Process data\n    Parameters:\n    -----------\n    cat_encode: encode categorical and apply WOE binning\n    na_fill: fill NA\n\n\n    Attributes:\n    -----------\n\n\n    \"\"\"\n    def __init__(self,\n                 cat_encode: bool = False,\n                 na_fill: bool = False,\n                 target_col: str = 'Attrition',\n                 index_col: str = 'Id'\n                 ) -> None:\n        self.cat_encode = cat_encode\n        self.na_fill = cat_encode\n        self.target_col = target_col\n        self.index_col = index_col\n        self.copy = True\n        self.binner = WOE()\n        self.feat_sel = VotingSelector(minimum_votes=3)\n        self.clf = WOE()\n        self.columns = list()\n        self.medians = dict()\n        self.means = dict()\n\n    def _copy(self, X: pd.DataFrame) -> pd.DataFrame:\n        return X.copy() if self.copy else X\n\n    def _create_dummies(self, X: pd.DataFrame) -> pd.DataFrame:\n        obj_col = [col for col in X.select_dtypes(include=['object']).columns if\n                   col not in [self.target_col, self.index_col]]\n        # generate binary values using get_dummies\n        dum_df = pd.get_dummies(X, columns=obj_col, prefix=obj_col)\n        remove_dummies = [col for col in dum_df.columns if col in ['BusinessTravel_Travel_Rarely',\n                                                                   'Department_Research & Development',\n                                                                   'EducationField_Life Sciences', 'Gender_Male',\n                                                                   'JobRole_Sales Executive',\n                                                                   'MaritalStatus_Married']]\n        dum_df.drop(columns=remove_dummies, inplace=True)\n        return dum_df\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fitting X\n\n        Parameters:\n        -----------\n        X: pandas.DataFrame\n\n        Returns:\n        --------\n        self\n        \"\"\"\n        if self.cat_encode:\n            dum_df = self._create_dummies(X)\n            self.feat_sel.fit(dum_df, y)\n            self.binner.fit(self.feat_sel.transform(dum_df), y)\n            output_woe_bins = self.binner.woe_bins  # future transformation\n            output_mono_bins = self.binner.mono_custom_binning\n            self.clf = WOE(woe_bins=output_woe_bins, mono_custom_binning=output_mono_bins)  # output_bins was created earlier\n        else:\n            self.columns = X.columns.tolist()\n        self.medians['NumCompaniesWorked'] = X['NumCompaniesWorked'].median()\n        self.means['EnvironmentSatisfaction'] = int(X['EnvironmentSatisfaction'].mean())\n        self.means['JobSatisfaction'] = int(X['JobSatisfaction'].mean())\n        self.means['WorkLifeBalance'] = int(X['WorkLifeBalance'].mean())\n\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"\n        Transform X.\n\n        Parameters:\n        -----------\n        X: pandas.DataFrame\n            \n\n        Returns:\n        --------\n        X_transformed: pandas.DataFrame\n        \"\"\"\n        x_transformed = self._copy(X)\n        if self.cat_encode:\n            dum_df = self._create_dummies(x_transformed)\n            x_transformed = self.clf.transform(self.feat_sel.transform(dum_df))\n        else:\n            x_transformed = x_transformed[self.columns]\n        if self.na_fill:\n            x_transformed['TotalWorkingYears'].fillna(x_transformed['YearsAtCompany'])\n            for key, value in self.medians.items():\n                x_transformed[key].fillna(value)\n            for key, value in self.means.items():\n                x_transformed[key].fillna(value)\n\n        return x_transformed\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        fit X, transform X\n        Eqivalent to fit(X).transform(X)\n\n        Parameters:\n        -----------\n        X: pandas.DataFrame\n            \n\n        Returns:\n        --------\n        X_transformed: pandas.DataFrame\n        \"\"\"\n        if y is None:\n            # fit method of arity 1 (unsupervised transformation)\n            return self.fit(X).transform(X)\n        else:\n            # fit method of arity 2 (supervised transformation)\n            return self.fit(X, y).transform(X, y)\n","c9290f27":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title, callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optimizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    if callbacks:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                  len(optimizer.cv_results_['params']),\n                                  best_score,\n                                  3*best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","7b6f2a83":"res_df = pd.read_csv(input_path + 'general_data.csv')\nms = pd.read_csv(input_path + 'manager_survey_data.csv')\nes = pd.read_csv(input_path + 'employee_survey_data.csv')","8e5eed1e":"res_df = res_df.merge(ms, on='EmployeeID')\nres_df = res_df.merge(es, on='EmployeeID')\ndel ms, es","20ecbe11":"print(res_df.info())","561101f2":"res_df.describe(include='all').T","6320e3c2":"res_df.nunique()","3d05ef62":"res_df.drop(columns=['Over18', 'StandardHours', 'EmployeeCount'], inplace=True)","1da9fd81":"res_df['Attrition'].replace({'Yes': 1, 'No': 0}, inplace=True)","07d42b0b":"for col in res_df.select_dtypes('object').columns:\n    print(res_df[col].value_counts())","0d0798b9":"columns_to_drop = [col for col in res_df.columns if col in [target_col, 'EmployeeCount', 'Over18', 'StandardHours', index_col]]\nX = res_df.drop(columns=columns_to_drop)\ny = res_df[target_col]","5b9c7c74":"X_train, X_holdout, y_train, y_holdout = train_test_split(\n    X, y,\n    stratify = y,\n    test_size=0.25, random_state=RANDOM_STATE)\nprint(X_train.shape, y_train.shape, X_holdout.shape, y_holdout.shape)","9471153b":"%matplotlib inline\nnum_col = [col for col in X_train.select_dtypes(include=['number']).columns if col not in [target_col, index_col]]\n\nX_train[num_col].hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);","d715e294":"corr = X_train.corr()\nplt.figure(figsize=(12, 10))\nplt.style.use('bmh')\n\nsns.heatmap(corr[abs(corr)>0.49], # (corr >= 0.5) | (corr <= -0.4)\n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","964043bd":"binner = WOE()\nbinner.fit(X_train, y_train)","6d6a712e":"%matplotlib inline\nplt.figure.max_open_warning=30\nwoe_df = binner.woe_df\ncharts = BarCharts(bar_type='v')\ncharts.plot(woe_df);","f9d586df":"prep = DataPrepare(cat_encode=True, na_fill=True)\ntrain_x = prep.fit_transform(X_train, y_train)\ntrain_y = y_train.copy()\ntrain_x_h = prep.transform(X_holdout)","76eba860":"feat_sel = VotingSelector(minimum_votes=3)\nfeat_sel.fit(train_x, train_y)\nprint(feat_sel.available_techniques)\nfeat_sel.feature_importances_","1e6ed184":"feat_sel.feature_votes_","32f02a72":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nlogistic = LogisticRegression(class_weight='balanced', dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=200, multi_class='ovr', n_jobs=NUM_THREADS,\n#           solver='liblinear',\n          tol=0.0001, verbose=0, warm_start=False, random_state=RANDOM_STATE)\n\ndistributions = dict(C=Real(1e-10, 1e2, 'log-uniform'),\n                     penalty=Categorical(['l2', 'l1']))\n\nopt_lr = BayesSearchCV(logistic,\n                    distributions,\n                    scoring=\"roc_auc\",\n                    cv=skf,\n                    n_iter=100,\n                    n_jobs=2,\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=RANDOM_STATE)\n","ff4f6ca8":"%%time\nbest_params = report_perf(opt_lr, train_x, train_y, 'LogReg', \n                          callbacks=[VerboseCallback(100), \n                                     DeadlineStopper(60*10)]) #DeadlineStopper(60*60*15)])#15 hours","1abec51a":"pd.DataFrame(opt_lr.cv_results_).sort_values(by=['param_C', 'mean_test_score']).plot(\n    x='param_C', y='mean_test_score', logx=True,\n                                   sort_columns=True, figsize=(12,8))","f4c388b7":"print(\"val. score: %s\" % opt_lr.best_score_)\nprint(\"test score: %s\" % opt_lr.score(train_x_h, y_holdout))\n\ny_proba = opt_lr.best_estimator_.predict_proba(train_x_h)\ny_pred = opt_lr.best_estimator_.predict(train_x_h)\n\nprint('Accuracy:', accuracy_score(y_holdout,y_pred))\nprint('MCC:', matthews_corrcoef(y_holdout,y_pred))\nprint(confusion_matrix(y_holdout,y_pred))\nprint(classification_report(y_holdout,y_pred))\n","70b7f6c0":"prep_cb = DataPrepare()\ntrain_x = prep_cb.fit_transform(X_train, y_train)\ntrain_y = y_train.copy()\ntrain_x_h = prep_cb.transform(X_holdout)","12f8dff9":"cat_features = [col for col in train_x.select_dtypes(include=['object']).columns if col not in [target_col, index_col]]\ncat_features","9925bed1":"#usual catboost\n\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nparams = {'iterations': 500,\n          'thread_count': NUM_THREADS,\n          'learning_rate': 0.05,\n          'loss_function': 'Logloss',\n          'depth': 6,\n          'metadata': {'model_dtypes': str(train_x.dtypes.to_dict())},\n          'name': 'cb_sample',\n          'random_state': RANDOM_STATE,\n          'cat_features': cat_features,\n          'custom_metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n          'eval_metric': 'AUC:hints=skip_train~false',\n          'early_stopping_rounds': 100,\n          'border_count': 30,\n#           'boost_from_average': True,\n          'metric_period': 25,\n#           'task_type': 'GPU',\n          'verbose': False}\n\nclf = CatBoostClassifier(**params)\n\n# Defining your search space\nsearch_spaces = {#'iterations': Integer(10, 1000),\n                'depth': Integer(2, 6),\n                'random_strength': Real(1e-2, 1e3, 'log-uniform'),\n                'bagging_temperature': Real(0.5, 1.5),\n                'border_count': Integer(5, 50),\n                'one_hot_max_size': Integer(2, 15),\n                'l2_leaf_reg': Integer(1, 100, 'log-uniform'),\n                'max_ctr_complexity': Integer(2, 4),\n                }\n\nopt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=\"roc_auc\",\n                    cv=skf,\n                    n_iter=100,\n                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n                    return_train_score=True,\n                    iid=False,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=RANDOM_STATE)","171dee98":"%%time\nbest_params = report_perf(opt, train_x, train_y, 'CatBoost', \n                          callbacks=[VerboseCallback(100), \n                                     DeadlineStopper(60*10)]) #DeadlineStopper(60*60*15)])#15 hours","4c07b3ab":"best_params","643262c2":"# pd.DataFrame(opt.cv_results_).sort_values(by=['param_l2_leaf_reg', 'mean_test_score']).plot(\n#     x='param_l2_leaf_reg', y='mean_test_score', logx=True,\n#                                    sort_columns=True, figsize=(12,8))\ncol_to_plot='param_l2_leaf_reg'\npd.DataFrame(opt.cv_results_).groupby([col_to_plot])['mean_test_score'].mean().reset_index().plot(\n    x=col_to_plot, y='mean_test_score', logx=True,\n                                   sort_columns=True, figsize=(12,8))","85d26827":"col_to_plot='param_depth'\npd.DataFrame(opt.cv_results_).groupby([col_to_plot])['mean_test_score'].mean().reset_index().plot(\n    x=col_to_plot, y='mean_test_score',\n                                   sort_columns=True, figsize=(12,8))","bed3559f":"print(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(train_x_h, y_holdout))\n\ny_proba = opt.best_estimator_.predict_proba(train_x_h)\ny_pred = opt.best_estimator_.predict(train_x_h)\n\nprint('Accuracy:', accuracy_score(y_holdout,y_pred))\nprint('MCC:', matthews_corrcoef(y_holdout,y_pred))\nprint(confusion_matrix(y_holdout,y_pred))\nprint(classification_report(y_holdout,y_pred))","db93bddb":"opt.best_estimator_.get_all_params()","98f1466f":"cv_dataset = Pool(data=train_x_h,\n                  label=y_holdout,\n                  cat_features=cat_features,\n                  feature_names=train_x_h.columns.tolist(),\n                  thread_count=NUM_THREADS_FILE)\nshap_values=opt.best_estimator_.get_feature_importance(cv_dataset, type='ShapValues', thread_count = NUM_THREADS,\n                                                   verbose = cv_dataset.num_row()\/\/5)\nshap_expected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\n\nshap.summary_plot(shap_values,train_x_h, max_display=30, auto_size_plot=True)\n\nshap.summary_plot(shap_values,train_x_h, max_display=30, plot_type='bar')","d963763c":"#catboost with SGLB functionality\n\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nparams = {'iterations': 500,\n          'thread_count': NUM_THREADS,\n          'learning_rate': 0.05,\n          'loss_function': 'Logloss',\n          'depth': 6,\n          'metadata': {'model_dtypes': str(train_x.dtypes.to_dict())},\n          'name': 'cb_sglb',\n          'random_state': RANDOM_STATE,\n          'cat_features': cat_features,\n          'custom_metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n          'eval_metric': 'AUC:hints=skip_train~false',\n          'early_stopping_rounds': 100,\n          'border_count': 30,\n          'langevin': True,# option and tune diffusion_temperature and model_shrink_rate\n#           'boost_from_average': True,\n          'metric_period': 25,\n#           'task_type': 'GPU',\n          'verbose': False}\n\nclf_sglb = CatBoostClassifier(**params)\n# Defining your search space\nsearch_spaces_sglb = {#'iterations': Integer(10, 1000),\n                'depth': Integer(2, 6),\n                'random_strength': Real(1e-2, 1e3, 'log-uniform'),\n                'bagging_temperature': Real(0.5, 1.5),\n                'border_count': Integer(5, 50),\n                'one_hot_max_size': Integer(2, 15),\n                'l2_leaf_reg': Integer(1, 100, 'log-uniform'),\n                'max_ctr_complexity': Integer(2, 4),\n                'diffusion_temperature': Real(0.01, 10.0, 'log-uniform'),\n#                 'model_shrink_rate': Real(0.1, 3.0),\n                }\n\nopt_sglb = BayesSearchCV(clf_sglb,\n                    search_spaces_sglb,\n                    scoring=\"roc_auc\",\n                    cv=skf,\n                    n_iter=100,\n                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n                    return_train_score=True,\n                    iid=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=RANDOM_STATE)","6005e8aa":"%%time\nbest_params = report_perf(opt_sglb, train_x, train_y, 'CatBoostSGLB', \n                          callbacks=[VerboseCallback(100), \n                                     DeadlineStopper(60*10)]) #DeadlineStopper(60*60*15)])#15 hours","e8ed577c":"best_params","6ec7aa1e":"# pd.DataFrame(opt_sglb.cv_results_).sort_values(by=['param_diffusion_temperature', 'mean_test_score']).plot(\n#     x='param_diffusion_temperature', y='mean_test_score',\n#                                    sort_columns=True, figsize=(12,8))\ncol_to_plot='param_diffusion_temperature'\npd.DataFrame(opt_sglb.cv_results_).groupby([col_to_plot])['mean_test_score'].mean().reset_index().plot(\n    x=col_to_plot, y='mean_test_score',\n                                   sort_columns=True, figsize=(12,8))","b9b7e473":"# pd.DataFrame(opt_sglb.cv_results_).sort_values(by=['param_depth', 'mean_test_score']).plot(\n#     x='param_depth', y='mean_test_score',\n#                                    sort_columns=True, figsize=(12,8))\ncol_to_plot='param_depth'\npd.DataFrame(opt_sglb.cv_results_).groupby([col_to_plot])['mean_test_score'].mean().reset_index().plot(\n    x=col_to_plot, y='mean_test_score',\n                                   sort_columns=True, figsize=(12,8))","5ede0b6e":"col_to_plot='param_l2_leaf_reg'\npd.DataFrame(opt_sglb.cv_results_).groupby([col_to_plot])['mean_test_score'].mean().reset_index().plot(\n    x=col_to_plot, y='mean_test_score',# logx=True,\n                                   sort_columns=True, figsize=(12,8))","90e20fbf":"col_to_plot='param_max_ctr_complexity'\npd.DataFrame(opt_sglb.cv_results_).groupby([col_to_plot])['mean_test_score'].mean().reset_index().plot(\n    x=col_to_plot, y='mean_test_score',\n                                   sort_columns=True, figsize=(12,8))","1b8c7883":"print(\"val. score: %s\" % opt_sglb.best_score_)\nprint(\"test score: %s\" % opt_sglb.score(train_x_h, y_holdout))\n\ny_proba = opt_sglb.best_estimator_.predict_proba(train_x_h)\ny_pred = opt_sglb.best_estimator_.predict(train_x_h)\n\nprint('Accuracy:', accuracy_score(y_holdout,y_pred))\nprint('MCC:', matthews_corrcoef(y_holdout,y_pred))\nprint(confusion_matrix(y_holdout,y_pred))\nprint(classification_report(y_holdout,y_pred))","5fccc23d":"opt_sglb.best_estimator_.get_all_params()","9aeaac46":"pd.DataFrame(opt_sglb.cv_results_).sort_values(by=['mean_test_score', 'mean_train_score', 'mean_fit_time'], ascending=[False, True, True])","aaaf87fe":"# Seems to fail in new mode with\n# CatBoostError: catboost\/libs\/fstr\/shap_values.cpp:810: Cannot calc shap values, model contains non zero approx for zero-weight leaf\ncv_dataset = Pool(data=train_x_h,\n                  label=y_holdout,\n                  cat_features=cat_features,\n                  feature_names=train_x_h.columns.tolist(),\n                  thread_count=NUM_THREADS_FILE)\nshap_values=opt_sglb.best_estimator_.get_feature_importance(cv_dataset, type='ShapValues', thread_count = NUM_THREADS,\n                                                   verbose = cv_dataset.num_row()\/\/5)\nshap_expected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\n\nshap.summary_plot(shap_values,train_x_h, max_display=30, auto_size_plot=True)\n\nshap.summary_plot(shap_values,train_x_h, max_display=30, plot_type='bar')","fecac76a":"## Catboost in Stochastic Gradient Langevin Boosting (SGLB) mode\nRecently Catboost team presented new feature. Let's test it.\n\nhttps:\/\/arxiv.org\/abs\/2001.07248\n> In this paper, we introduce Stochastic Gradient\n> Langevin Boosting (SGLB) \u2014 a powerful and efficient machine learning framework, which may\n> deal with a wide range of loss functions and has\n> provable generalization guarantees. The method\n> is based on a special form of Langevin Diffusion\n> equation specifically designed for gradient boosting. This allows us to **guarantee the global convergence**, while standard gradient boosting algorithms can guarantee only local optima, which is\n> a problem for multimodal loss functions. To illustrate the advantages of SGLB, we apply it to a\n> classification task with 0-1 loss function, which\n> is known to be multimodal, and to a standard Logistic regression task that is convex. The **algorithm is implemented as a part of the CatBoost**\n> gradient boosting library and **outperforms classic**\n> gradient boosting methods.\n\nhttps:\/\/github.com\/catboost\/catboost\/releases\/tag\/v0.21\n\n> The main feature of this release is the Stochastic Gradient Langevin Boosting (SGLB) mode that can improve quality of your models with **non-convex loss functions**. To use it specify langevin option and tune diffusion_temperature and model_shrink_rate. "}}