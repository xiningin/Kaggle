{"cell_type":{"7015f71b":"code","d2507004":"code","f8a14961":"code","bdeb6df7":"code","319b0661":"code","c5250689":"code","2bdb5a35":"code","bde7602d":"code","40739a3d":"code","1f76a4ae":"code","6a879bb5":"code","60fc1df5":"code","a55adea3":"code","f772fb71":"code","4ac05215":"code","3705fed2":"code","899c50f6":"code","e9f8ec9a":"code","58cc9db5":"code","e6b15fec":"code","5e937d3e":"code","87531404":"code","23bce9b9":"code","13d5adf0":"code","feaf22c7":"code","85ecb827":"code","69c101a0":"code","5532eed0":"code","e4e02761":"code","8bd4a253":"code","a33d5364":"code","4b86892d":"code","836ea9bb":"code","c8d635e5":"code","13ba166b":"code","afff6c21":"code","54a9b296":"code","abfe0bd7":"code","a81cf286":"code","3632bbd9":"code","43546d94":"code","e61f2f7b":"code","94675341":"code","fe375568":"code","b67d2eda":"code","823ed7e1":"code","a91dc1c5":"code","77debebe":"code","c4a0425e":"code","54d666a1":"code","9d18f4cc":"code","f168295c":"markdown","b5ca7daa":"markdown","c276abc5":"markdown","b4cd93f7":"markdown","acedcf67":"markdown","a051d554":"markdown","3fb5a1c5":"markdown","6c2e6c23":"markdown","3ce4390b":"markdown","cb0ed9fa":"markdown"},"source":{"7015f71b":"import numpy as np # linear algebra\nimport random\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nimport cv2\nimport shutil\nfrom glob import glob\n# Helper libraries\nimport matplotlib.pyplot as plt\nimport math\n%matplotlib inline\nprint(tf.__version__)","d2507004":"data_root='\/kaggle\/input\/covidct\/'\npath_positive_cases = os.path.join('\/kaggle\/input\/covidct\/CT_COVID\/')\npath_negative_cases = os.path.join('\/kaggle\/input\/covidct\/CT_NonCOVID\/')","f8a14961":"# jpg and png files\npositive_images_ls = glob(os.path.join(path_positive_cases,\"*.png\"))\n\nnegative_images_ls = glob(os.path.join(path_negative_cases,\"*.png\"))\nnegative_images_ls.extend(glob(os.path.join(path_negative_cases,\"*.jpg\")))","bdeb6df7":"covid = {'class': 'CT_COVID',\n         'path': path_positive_cases,\n         'images': positive_images_ls}\n\nnon_covid = {'class': 'CT_NonCOVID',\n             'path': path_negative_cases,\n             'images': negative_images_ls}","319b0661":"df = pd.DataFrame(columns=['height', 'width'])\nliste = []\n\nfor img_name in os.listdir(path_positive_cases):\n    img = cv2.imread(os.path.join(path_positive_cases, img_name))\n    liste.append(img.shape[:-1])\n\nfor img_name in os.listdir(path_negative_cases):\n    img = cv2.imread(os.path.join(path_negative_cases, img_name))\n    liste.append(img.shape[:-1])\n\ndf['height'] = list(zip(*liste))[0]\ndf['width'] = list(zip(*liste))[1]","c5250689":"df.describe()","2bdb5a35":"fig = plt.figure(1, figsize=(20, 3))\nplt.subplot(121)\n# plot the image height\nplt.hist(df['height'])\nplt.title('Distribution of Image Height')\nplt.xlabel('Image Height')\nplt.ylabel('Count')\nplt.subplot(122)\nplt.hist(df['width'])\nplt.title('Distribution of Image Width')\nplt.xlabel('Image Width')\nplt.ylabel('Count')\n\nplt.show()","bde7602d":"total_positive_covid = len(positive_images_ls)\ntotal_negative_covid = len(negative_images_ls)\nprint(\"Total Positive Cases Covid19 images: {}\".format(total_positive_covid))\nprint(\"Total Negative Cases Covid19 images: {}\".format(total_negative_covid))","40739a3d":"image_positive = cv2.imread(os.path.join(positive_images_ls[1]))\nimage_negative = cv2.imread(os.path.join(negative_images_ls[5]))\n\nf = plt.figure(figsize=(8, 8))\nf.add_subplot(1, 2, 1)\nplt.imshow(image_negative)\nf.add_subplot(1,2, 2)\nplt.imshow(image_positive)","1f76a4ae":"print(\"Image COVID Shape {}\".format(image_positive.shape))\nprint(\"Image Non COVID Shape {}\".format(image_negative.shape))","6a879bb5":"# Create Train-Test Directory\nsubdirs  = ['train\/', 'test\/']\nfor subdir in subdirs:\n    labeldirs = ['CT_COVID', 'CT_NonCOVID']\n    for labldir in labeldirs:\n        newdir = subdir + labldir\n        os.makedirs(newdir, exist_ok=True)","60fc1df5":"# Copy Images to test set\n\n# seed random number generator\nrandom.seed(123)\n# define ratio of pictures used for testing \ntest_ratio = 0.15\n\n\nfor cases in [covid, non_covid]:\n    total_cases = len(cases['images']) #number of total images\n    num_to_select = int(test_ratio * total_cases) #number of images to copy to test set\n    \n    print(cases['class'], num_to_select)\n    \n    list_of_random_files = random.sample(cases['images'], num_to_select) #random files selected\n\n    for files in list_of_random_files:\n        shutil.copy2(files, 'test\/' + cases['class'])","a55adea3":"# Copy Images to train set\nfor cases in [covid, non_covid]:\n    image_test_files = os.listdir('test\/' + cases['class']) # list test files \n    for images in cases['images']:\n        if images.split('\/')[-1] not in (image_test_files): #exclude test files from shutil.copy\n            shutil.copy2(images, 'train\/' + cases['class'])","f772fb71":"total_train_covid = len(os.listdir('\/kaggle\/working\/train\/CT_COVID'))\ntotal_train_noncovid = len(os.listdir('\/kaggle\/working\/train\/CT_NonCOVID'))\ntotal_test_covid = len(os.listdir('\/kaggle\/working\/test\/CT_COVID'))\ntotal_test_noncovid = len(os.listdir('\/kaggle\/working\/test\/CT_NonCOVID'))\n\nprint(\"Train sets images COVID: {}\".format(total_train_covid))\nprint(\"Train sets images Non COVID: {}\".format(total_train_noncovid))\nprint(\"Test sets images COVID: {}\".format(total_test_covid))\nprint(\"Test sets images Non COVID: {}\".format(total_test_noncovid))","4ac05215":"batch_size = 256\nepochs = 250\nIMG_HEIGHT = 300\nIMG_WIDTH = 300","3705fed2":"\"\"\"train_image_generator = ImageDataGenerator(\n        rescale=1.\/255,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest') \"\"\"\n\ntrain_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our training data\ntest_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our validation data","899c50f6":"train_dir = os.path.join('\/kaggle\/working\/train')\ntest_dir = os.path.join('\/kaggle\/working\/test')\n\n\ntotal_train = total_train_covid + total_train_noncovid\ntotal_test = total_test_covid + total_test_noncovid","e9f8ec9a":"train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=train_dir,\n                                                           shuffle=True,\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                           class_mode='binary')","58cc9db5":"test_data_gen = test_image_generator.flow_from_directory(batch_size=batch_size,\n                                                              directory=test_dir,\n                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                              class_mode='binary')","e6b15fec":"model = Sequential([\n    Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dense(1)\n])","5e937d3e":"model.compile(optimizer='RMSprop',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","87531404":"model.summary()","23bce9b9":"early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\ncheck_point = ModelCheckpoint(\".\/my_model.h5\", monitor=\"val_loss\", save_best_only=True)\nreduce = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6)\n\ncallbacks_list = [early, check_point, reduce]\n\nhistory = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train \/\/ batch_size,\n    epochs=epochs,\n    validation_data=test_data_gen,\n    validation_steps=total_test \/\/ batch_size,\n    callbacks=callbacks_list\n)","13d5adf0":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(len(loss))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","feaf22c7":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","85ecb827":"with open(\"model.json\", \"r\") as json_file:\n    json = json_file.read()\n\n# load the json file\n# here i use json as loaded content of json file\nmodel = keras.models.model_from_json(json)\nmodel.load_weights(\"my_model.h5\")","69c101a0":"layer_outputs = [layer.output for layer in model.layers[:6]]\n# Creates a model that will return these outputs, given the model input:\nactivation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)","5532eed0":"from keras.preprocessing import image\n\n\nimg = image.load_img(negative_images_ls[6], target_size=(IMG_HEIGHT, IMG_WIDTH))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\n# Remember that the model was trained on inputs\n# that were preprocessed in the following way:\nimg_tensor \/= 255.\n\n# Its shape is (1, 150, 150, 3)\nprint(img_tensor.shape)","e4e02761":"# This will return a list of 5 Numpy arrays:\n# one array per layer activation\nactivations = activation_model.predict(img_tensor)\n\nfirst_layer_activation = activations[0]\nprint(first_layer_activation.shape)","8bd4a253":"import matplotlib.pyplot as plt\n\nplt.matshow(first_layer_activation[0, :, :, 31], cmap='viridis')\nplt.show()","a33d5364":"# These are the names of the layers, so can have them as part of our plot\nlayer_names = []\nfor layer in model.layers[:8]:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n# Now let's display our feature maps\nfor layer_name, layer_activation in zip(layer_names, activations):\n    # This is the number of features in the feature map\n    n_features = layer_activation.shape[-1]\n\n    # The feature map has shape (1, size, size, n_features)\n    size = layer_activation.shape[1]\n\n    # We will tile the activation channels in this matrix\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    # We'll tile each filter into this big horizontal grid\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            # Post-process the feature to make it visually palatable\n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n    # Display the grid\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    \nplt.show()","4b86892d":"def get_activations(modelx, i,input_image):\n\n    out_layer = modelx.layers[i]\n    model = tf.keras.models.Model(inputs = modelx.inputs, outputs = out_layer.output)\n    return model.predict(input_image)\n\n\ndef postprocess_activations(activations):\n\n  #using the approach in https:\/\/arxiv.org\/abs\/1612.03928\n    output = np.abs(activations)\n    output = np.sum(output, axis = -1).squeeze()\n\n  #resize and convert to image \n    output = cv2.resize(output, (imagesize, imagesize))\n    output \/= output.max()\n    output *= 255\n    return 255 - output.astype('uint8')\ndef apply_heatmap(weights):\n  #generate heat maps \n    heatmap = cv2.applyColorMap(weights, cv2.COLORMAP_JET)\n    heatmap = cv2.addWeighted(heatmap, 0.7, img, 0.3, 0)\n    return heatmap\n\ndef plot_heatmaps(modelx, rng):\n    level_maps = None\n  \n \n    for i in range(rng):\n        activations = get_activations(modelx, i,input_image)\n \n        weights = postprocess_activations(activations)\n        heatmap = apply_heatmap(weights)\n        if level_maps is None:\n            level_maps = heatmap\n        else:\n            level_maps = np.concatenate([level_maps, heatmap], axis = 1)\n    plt.figure(figsize=(15, 15))\n    plt.axis('off')\n    ax = plt.imshow(level_maps)\n\n","836ea9bb":"import cv2\nimagesize = IMG_HEIGHT\nimg = cv2.imread(negative_images_ls[2])[:,:,::-1]\nimg = cv2.resize(img, (imagesize, imagesize))\nx = image.img_to_array(img)\ninput_image = np.expand_dims(x, axis=0)\n\n\nplot_heatmaps(model,6)","c8d635e5":"plt.imshow(img)","13ba166b":"# show the confusion matrix of our predictions\n\n# compute predictions\n#predictions = model.predict_generator(generator=test_data_gen)\n# y_pred = [np.argmax(probas) for probas in predictions]\ny_pred =  model.predict_classes(test_data_gen, batch_size=None).flatten()\n\ny_test = test_data_gen.classes\nclass_names = test_data_gen.class_indices.keys()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"black\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()","afff6c21":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_test, y_pred)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test, y_pred)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_test, y_pred)\nprint('F1 score: %f' % f1)\n\nkappa = cohen_kappa_score(y_test, y_pred)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, y_pred)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, y_pred)\nprint(matrix)","54a9b296":"import numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras import applications\n\n\ntop_model_weights_path = 'bottleneck_fc_model.h5'\nnb_train_samples = 576\nnb_validation_samples = 96\nbatch_size=32","abfe0bd7":"# But let's take a look at how we record the bottleneck features using image data generators:\n\ndatagen = ImageDataGenerator(rescale=1. \/ 255)\n\n# build the VGG16 network\nmodel_vgg = applications.VGG16(include_top=False, weights='imagenet')\n\ngenerator = datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=batch_size,\n    class_mode=None,\n    shuffle=False)\nbottleneck_features_train = model_vgg.predict_generator(generator, nb_train_samples \/\/ batch_size)\n#bottleneck_features_train = model_vgg.predict_generator(generator)\n\nnp.save('bottleneck_features_train.npy', bottleneck_features_train)\n\ngenerator = datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=batch_size,\n    class_mode=None,\n    shuffle=False)\nbottleneck_features_validation = model_vgg.predict_generator(generator, nb_validation_samples \/\/ batch_size)\n#bottleneck_features_validation = model_vgg.predict_generator(generator)\n\nnp.save('bottleneck_features_validation.npy', bottleneck_features_validation)","a81cf286":"# We can then load our saved data and train a small fully-connected model:\n\ntrain_data = np.load('bottleneck_features_train.npy')\ntrain_labels = np.array([0] * int((nb_train_samples \/ 2)) + [1] * int((nb_train_samples \/ 2)))\n\nvalidation_data = np.load('bottleneck_features_validation.npy')\nvalidation_labels = np.array([0] * int((nb_validation_samples \/ 2)) + [1] * int((nb_validation_samples \/ 2)))\n\nmodelVgg = Sequential()\nmodelVgg.add(Flatten(input_shape=train_data.shape[1:]))\nmodelVgg.add(Dense(256, activation='relu'))\nmodelVgg.add(Dropout(0.5))\nmodelVgg.add(Dense(1, activation='sigmoid'))\n\nmodelVgg.compile(optimizer='rmsprop',\n              loss='binary_crossentropy', metrics=['accuracy'])\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\ncheck_point = ModelCheckpoint(\".\/my_model_vgg.h5\", monitor=\"val_loss\", save_best_only=True)\nreduce = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6)\n\ncallbacks_list = [early, check_point, reduce]\n\nmodelVgg.fit(train_data, train_labels,\n          epochs=epochs,\n          batch_size=batch_size,\n          validation_data=(validation_data, validation_labels),\n          callbacks=callbacks_list)\nmodelVgg.save_weights(top_model_weights_path)","3632bbd9":"# serialize model to JSON\nmodel_json = modelVgg.to_json()\nwith open(\"modelVgg.json\", \"w\") as json_file:\n    json_file.write(model_json)","43546d94":"with open(\"modelVgg.json\", \"r\") as json_file:\n    json = json_file.read()\n\n# load the json file\n# here i use json as loaded content of json file\nmodelVgg = keras.models.model_from_json(json)\nmodelVgg.load_weights(\"my_model_vgg.h5\")","e61f2f7b":"# show the confusion matrix of our predictions\n\n# compute predictions\n#predictions = model.predict_generator(generator=test_data_gen)\n# y_pred = [np.argmax(probas) for probas in predictions]\ny_pred =  modelVgg.predict_classes(validation_data, batch_size=None).flatten()\ny_test = validation_labels\nclass_names = test_data_gen.class_indices.keys()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"black\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()","94675341":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_test, y_pred)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test, y_pred)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_test, y_pred)\nprint('F1 score: %f' % f1)\n\nkappa = cohen_kappa_score(y_test, y_pred)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, y_pred)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, y_pred)\nprint(matrix)","fe375568":"!wget --no-check-certificate 'https:\/\/drive.google.com\/file\/d\/0Bz7KyqmuGsilT0J5dmRCM0ROVHc\/' -O 'vgg16_weights.h5'","b67d2eda":"from keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Model\n\n# path to the model weights files.\nweights_path = 'vgg16_weights.h5'\ntop_model_weights_path = 'bottleneck_fc_model.h5'\n# dimensions of our images.\nimg_width, img_height = IMG_WIDTH, IMG_HEIGHT\n\n#nb_train_samples = 2000\n#nb_validation_samples = 800\n#epochs = 50\n#batch_size = 16","823ed7e1":"# build the VGG16 network\n\nbase_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\nprint('Model loaded.')\n\n# build a classifier model to put on top of the convolutional model\n\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=base_model.output_shape[1:]))\ntop_model.add(Dense(256, activation='relu'))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(1, activation='sigmoid'))\n\n# note that it is necessary to start with a fully-trained\n# classifier, including the top classifier,\n# in order to successfully do fine-tuning\ntop_model.load_weights(top_model_weights_path)","a91dc1c5":"# add the model on top of the convolutional base\n# model.add(top_model)\nmodelFtunning = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n\n# set the first 25 layers (up to the last conv block)\n# to non-trainable (weights will not be updated)\nfor layer in modelFtunning.layers[:15]:\n    layer.trainable = False\n\n# compile the model with a SGD\/momentum optimizer\n# and a very slow learning rate.\nmodelFtunning.compile(loss='binary_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n# prepare data augmentation configuration\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary')\n\nmodelFtunning.summary()\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\ncheck_point = ModelCheckpoint(\".\/my_model_ftunning.h5\", monitor=\"val_loss\", save_best_only=True)\nreduce = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6)\n\ncallbacks_list = [early, check_point, reduce]\n\n# fine-tune the model\nmodelFtunning.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples \/\/ batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples \/\/ batch_size,\n    verbose=2,\n    callbacks=callbacks_list)","77debebe":"# serialize model to JSON\nmodel_json = modelFtunning.to_json()\nwith open(\"modelFtunning.json\", \"w\") as json_file:\n    json_file.write(model_json)","c4a0425e":"with open(\"modelFtunning.json\", \"r\") as json_file:\n    json = json_file.read()\n\n# load the json file\n# here i use json as loaded content of json file\nmodelFtunning = keras.models.model_from_json(json)\nmodelFtunning.load_weights(\"my_model_ftunning.h5\")","54d666a1":"# show the confusion matrix of our predictions\n\n# compute predictions\n#predictions = model.predict_generator(generator=test_data_gen)\n# y_pred = [np.argmax(probas) for probas in predictions]\ny_pred =  modelFtunning.predict(test_data_gen, batch_size=None).flatten()\ny_pred = y_pred.round()\ny_test = test_data_gen.classes\nclass_names = test_data_gen.class_indices.keys()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"black\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()","9d18f4cc":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_test, y_pred)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test, y_pred)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_test, y_pred)\nprint('F1 score: %f' % f1)\n\nkappa = cohen_kappa_score(y_test, y_pred)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, y_pred)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, y_pred)\nprint(matrix)","f168295c":"### Attention Map","b5ca7daa":"### Create Train-Test Directory ","c276abc5":"# Using the bottleneck features of a pre-trained network - VGG16\n\nA more refined approach would be to leverage a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.We will use the VGG16 architecture, pre-trained on the ImageNet dataset --a model previously featured on this blog. The method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n\n> **Turkish translation:** Daha eleyici bir yakla\u015f\u0131m, b\u00fcy\u00fck bir veri k\u00fcmesi \u00fczerinde \u00f6nceden e\u011fitilmi\u015f bir a\u011fdan yararlanmak olacakt\u0131r. B\u00f6yle bir a\u011f, \u00e7o\u011fu bilgisayar g\u00f6rme problemi i\u00e7in yararl\u0131 olan \u00f6zellikleri zaten \u00f6\u011frenmi\u015f olacak ve bu \u00f6zelliklerin kullan\u0131lmas\u0131, sadece mevcut verilere dayanan herhangi bir y\u00f6ntemden daha iyi bir do\u011frulu\u011fa ula\u015fmam\u0131z\u0131 sa\u011flayacakt\u0131r. Bu blogda daha \u00f6nce yer alan bir model olan ImageNet veri k\u00fcmesinde \u00f6nceden e\u011fitilmi\u015f VGG16 mimarisini kullanaca\u011f\u0131z.Burada sundu\u011fumuz y\u00f6ntemin ImageNet'te bulunmayan s\u0131n\u0131flar\u0131 i\u00e7eren sorunlar da dahil olmak \u00fczere daha geni\u015f bir yelpazedeki sorunlara genelleme olas\u0131l\u0131\u011f\u0131 daha y\u00fcksektir.\n\n![image.png](attachment:image.png)\n\nOur strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we will train a small fully-connected model on top of the stored features. The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation.\n\n> **Turkish translation:** Stratejimiz a\u015fa\u011f\u0131daki gibi olacakt\u0131r: sadece modelin evri\u015fimsel k\u0131sm\u0131n\u0131, tam ba\u011flant\u0131l\u0131 katmanlara kadar her \u015feyi somutla\u015ft\u0131raca\u011f\u0131z. Daha sonra bu modeli e\u011fitim ve do\u011frulama verilerimizde bir kez \u00e7al\u0131\u015ft\u0131rarak iki numpy dizisinde \u00e7\u0131kt\u0131y\u0131 (VGG16 modelinden \"darbo\u011faz \u00f6zellikleri\": tam ba\u011fl\u0131 katmanlardan \u00f6nceki son aktivasyon haritalar\u0131) kaydedece\u011fiz. Ard\u0131ndan, depolanan \u00f6zelliklerin \u00fcst\u00fcnde tam olarak ba\u011fl\u0131 k\u00fc\u00e7\u00fck bir model e\u011fitece\u011fiz. Tam ba\u011flant\u0131l\u0131 modelimizi do\u011frudan donmu\u015f bir evri\u015fimsel taban\u0131n \u00fcst\u00fcne eklemek ve her \u015feyi \u00e7al\u0131\u015ft\u0131rmak yerine \u00e7evrimd\u0131\u015f\u0131 olarak saklamam\u0131z\u0131n nedeni, hesaplama verimlili\u011fidir. VGG16 kullanmak pahal\u0131, \u00f6zellikle CPU \u00fczerinde \u00e7al\u0131\u015f\u0131yorsan\u0131z ve biz sadece bir kez yapmak istiyoruz. Bunun veri art\u0131rmay\u0131 kullanmam\u0131z\u0131 engelledi\u011fini unutmay\u0131n.\n\nYou can find the full code for this experiment [here](https:\/\/gist.github.com\/fchollet\/f35fbc80e066a49d65f1688a7e99f069). You can get the weights file from [Github](https:\/\/gist.github.com\/baraldilorenzo\/07d7802847aaad0a35d3). We won't review how the model is built and loaded --this is covered in multiple Keras examples already. But let's take a look at how we record the bottleneck features using image data generators:\n\n","b4cd93f7":"# Training a small network from scratch (as a baseline)","acedcf67":"# Fine-tuning the top layers of a pre-trained network\n\nTo further improve our previous result, we can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n\n* instantiate the convolutional base of VGG16 and load its weights\n* add our previously defined fully-connected model on top, and load its weights\n* freeze the layers of the VGG16 model up to the last convolutional block\n\n\n\n![image.png](attachment:image.png)\n\nNote that:\n\n* in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n* we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n* fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features.\n\n\nYou can find the full code for this experiment [here](https:\/\/gist.github.com\/fchollet\/7eb39b44eb9e16e59632d25fb3119975).\nAfter instantiating the VGG base and loading its weights, we add our previously trained fully-connected classifier on top:","a051d554":"### Load Best Model","3fb5a1c5":"### Class Map","6c2e6c23":"### Simple CNN Model\n[Tensorflow Tutorial](https:\/\/www.tensorflow.org\/tutorials\/images\/classification)","3ce4390b":"### Building powerful image classification models using very little data\n\nIn this tutorial, we will present a few simple yet effective methods that you can use to build a powerful image classifier, using only very few training examples --just a few hundred or thousand pictures from each class you want to be able to recognize.\n\nWe will go over the following options:\n\n* training a small network from scratch (as a baseline)\n* using the bottleneck features of a pre-trained network\n* fine-tuning the top layers of a pre-trained network\n\nThis will lead us to cover the following Keras features:\n\nfit_generator for training Keras a model using Python data generators\nImageDataGenerator for real-time data augmentation\nlayer freezing and model fine-tuning\n...and more.\n\nhttps:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html\n\n#### Note: You will need Keras version 2.0.0 or higher to run them.\n\nThat is very few examples to learn from, for a classification problem that is far from simple. So this is a challenging machine learning problem, but it is also a realistic one: in a lot of real-world use cases, even small-scale data collection can be extremely expensive or sometimes near-impossible (e.g. in medical imaging). Being able to make the most out of very little data is a key skill of a competent data scientist.","cb0ed9fa":"### Datasets Overview "}}