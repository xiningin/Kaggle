{"cell_type":{"851e49b1":"code","6f4b8649":"code","bbed637a":"code","6e85aaff":"code","ce6d2dad":"markdown","cf37f705":"markdown","9ac13ead":"markdown","bad32165":"markdown"},"source":{"851e49b1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport joblib\nimport pickle\nimport lightgbm as lgb\nimport xgboost as xg\nimport gc\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom tqdm import tqdm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom datetime import datetime, timedelta\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(seed=42)","6f4b8649":"def additional_features(df):\n\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['hour'] = df['date'].dt.hour\n    df['day'] = df['date'].dt.day\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['week'] = df['date'].dt.week\n\n    df['month_dummy'] = 0\n    df.loc[df['month'] > 10, 'month_dummy'] = 1\n    df.loc[df['month'] < 3, 'month_dummy'] = 1\n\n    df['hour_dummy'] = 0\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 19) & (df['hour'] < 23), 'hour_dummy'] = 1\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 5) & (df['hour'] < 9), 'hour_dummy'] = 1\n\n    df['day_dummy'] = 0\n    df.loc[(df['hour'] > 9) & (df['hour'] < 19), 'day_dummy'] = 1\n\n    return df\n\n\ndef get_data(ROOT):\n\n    CAL_DTYPES = {\"latitude\": \"float64\", \"longitude\": \"float64\", \"type\": \"category\",\n                  \"source\": \"category\", \"station\": \"category\", 'aqi': 'float64'}\n\n    train = pd.read_csv(ROOT + \"\/pm_train.csv\", dtype=CAL_DTYPES)\n    test = pd.read_csv(ROOT + \"\/pm_test.csv\", dtype=CAL_DTYPES)\n    weather = pd.read_csv(ROOT + \"\/weather.csv\")\n    sub = pd.read_csv(ROOT + \"\/sample_submission.csv\")\n\n    return train, test, weather, sub\n\ndef preprocess(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', cat_feat = [], rolling_means=[], shifts=[], diffs = []):\n\n    cont_vars = ['temperature','apparentTemperature',\n                 'dewPoint', 'humidity', 'windSpeed',\n                'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n    \n    cat_feat = [\"type\", \"source\", \"station\", \"hour\", \"year\", \"dayofyear\",\n           \"dayofweek\"]\n\n    train, test, weather, sub = get_data(ROOT=ROOT)\n\n    df = pd.concat([train, test], axis=0)\n    df['date'] = pd.to_datetime(df['date'])\n    weather['date'] = pd.to_datetime(weather['date'])\n    weather = weather.sort_values(by='date')\n\n    weather_df = pd.DataFrame()\n    weather_df['date'] = pd.date_range(start=weather.iloc[0]['date'], end=weather.iloc[-1]['date'], freq='H')\n    weather_df = pd.merge(weather_df, weather.iloc[:, 1:], on=['date'], how='left')\n\n    del weather\n\n    weather_df['year'] =  weather_df['date'].dt.year\n    weather_df['dayofyear'] = weather_df['date'].dt.dayofyear\n    weather_df['hour'] = weather_df['date'].dt.hour\n\n    df = additional_features(df)\n    \n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n\n    for col in ['summary', 'icon', \"year\", \"dayofyear\", \"hour\"]:\n        weather_df[col] = weather_df[col].fillna(method=\"ffill\")\n        weather_df[col] = weather_df[col].fillna(method=\"bfill\")\n        le = preprocessing.LabelEncoder()\n        weather_df[col] = le.fit_transform(weather_df[col])\n   \n    for col in weather_df.columns[3:]:\n        weather_df[col] = weather_df[col].fillna(weather_df[col].rolling(4, min_periods=1).mean())\n        weather_df[col] = weather_df[col].fillna(weather_df.groupby([\"dayofyear\", \"hour\"])[col].transform('mean'))\n    \n    for col in weather_df.columns[3:-2]:\n        for lag in rolling_means:\n            weather_df['%s_rolling_%s_mean' % (col, lag)] = weather_df[col].rolling(lag).mean()\n\n    for col in weather_df.drop(['date', 'summary', 'icon', 'dayofyear',\n                         'year'], axis=1).columns:\n        for lag in [2, 4]:\n            weather_df['%s_shift_%s' % (col, lag)] = weather_df[col].shift(lag)\n\n    for col in cont_vars:\n        for lag in diffs:\n            weather_df['%s_diff_%s' % (col, lag)] = weather_df[col].diff(lag)\n\n    weather_df = weather_df.dropna()\n\n    df = pd.merge(df, weather_df.iloc[:, 1:], on=[\"year\", \"dayofyear\", \"hour\"], how='left')\n\n#     for col in cat_feat[2:]:\n#         le = preprocessing.LabelEncoder()\n#         df[col] = le.fit_transform(df[col])\n\n#     icols = [['type', 'station', 'month', 'hour'],\n#             ['type', 'station', 'week', 'hour'],\n#             ['type', 'station', 'dayofyear', 'hour'],\n#              ['type', 'station', 'icon', 'month', 'hour'],\n#              ['type', 'station', 'summary', 'month', 'hour']]\n    \n   \n#     temp_df = df.copy()\n#     temp_df = temp_df.loc[temp_df['date']<\"2018-10-31\"].dropna()\n#     col_fill = '_' + '_'.join(icols[0]) + '_'\n#     for col in icols:\n#         col_name = '_' + '_'.join(col) + '_'\n#         temp = temp_df[col + ['aqi']].groupby(col, as_index=False)['aqi'].mean()\n#         temp = temp.rename(columns={'aqi': 'enc%smean' % col_name})\n#         df = df.merge(temp, on=col, copy=False, how='left')\n#         df['enc%smean' % col_name] = df['enc%smean' % col_name]\n#         df['enc%smean' % col_name] = df['enc%smean' % col_name].fillna(df['enc%smean' % col_fill])\n\n    return df, weather_df, sub","bbed637a":"##################### LightGBM ####################################\ndef train_lightGBM(train, test, sub_light, target, feat, cat_feat,\n                   top_n_features, Nfolds, params_k, MODEL_ROOT):\n\n    sub_light[target] = 0\n\n    train = train.reset_index(drop=True)\n    val_set = test.loc[test[target].dropna().index]\n\n    if top_n_features!=-1:\n\n        if os.path.isfile('lgb_feature_importance.pickle'):\n            with open('lgb_feature_importance.pickle', 'rb') as handle:\n                all_features = pickle.load(handle)\n                feat = list(all_features[0:top_n_features]['feature'])\n\n        else:\n\n            train_data = lgb.Dataset(data=train[feat],\n                                     label=train[target],\n                                     categorical_feature=cat_feat,\n                                     free_raw_data=False)\n\n            valid_data = lgb.Dataset(data=val_set[feat],\n                                     label=val_set[target],\n                                     categorical_feature=cat_feat,\n                                     free_raw_data=False)\n\n            model_gbm = lgb.train(params_k, train_data, valid_sets=[train_data, valid_data],\n                                  num_boost_round=1500, early_stopping_rounds=50,\n                                  verbose_eval=100)\n\n            pred_val = model_gbm.predict(val_set[feat], num_iteration=model_gbm.best_iteration)\n            print(\"CV score:\", np.sqrt(mean_squared_error(val_set[target].values, pred_val)))\n\n            feature_importance_df = pd.DataFrame()\n            feature_importance_df[\"feature\"] = feat\n            feature_importance_df[\"importance\"] = model_gbm.feature_importance()\n\n            all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)\n            all_features.reset_index(inplace=True)\n\n            with open('lgb_feature_importance.pickle', 'wb') as handle:\n                pickle.dump(all_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            feat = list(all_features[0:top_n_features]['feature'])\n\n        temp = []\n        for col in cat_feat:\n            if col in feat:\n                temp.append(col)\n\n        cat_feat = temp\n    else:\n        print('No feature selection')\n\n    kf = KFold(n_splits=Nfolds, shuffle=True)\n\n    scores = []\n    scores_fold = []\n    fold_ = 0\n    for train_index, val_index in kf.split(train.index):\n        train_set = train.loc[train_index]\n        val_set2 = train.loc[val_index]\n        #val_set_full = pd.concat([val_set2, val_set], axis=0)\n        val_set_full = val_set\n\n        train_data = lgb.Dataset(data=train_set[feat],\n                                 label=train_set[target],\n                                 categorical_feature=cat_feat,\n                                 free_raw_data=False)\n\n        valid_data = lgb.Dataset(data=val_set_full[feat],\n                                 label=val_set_full[target],\n                                 categorical_feature=cat_feat,\n                                 free_raw_data=False)\n\n        model_gbm = lgb.train(params_k, train_data, valid_sets=[train_data, valid_data],\n                              num_boost_round=1500, early_stopping_rounds=50,\n                              verbose_eval=100)\n\n        if not os.path.exists(MODEL_ROOT):\n            os.makedirs(MODEL_ROOT)\n\n        with open(MODEL_ROOT + '\/model_lgb_%s_%s.pkl' % (top_n_features, fold_), 'wb') as fout:\n            pickle.dump(model_gbm, fout)\n\n        pred_val = model_gbm.predict(val_set[feat])\n        rmse = np.sqrt(mean_squared_error(val_set[target].values, pred_val))\n        scores.append(rmse)\n        print(\"CV score:\", rmse)\n\n\n        val_set2[\"pred\"] = model_gbm.predict(val_set2[feat])\n        rmse_fold = np.sqrt(mean_squared_error(val_set2[target].values, val_set2[\"pred\"].values))\n        scores_fold.append(rmse_fold)\n        print(\"CV score fold:\", rmse_fold)\n\n        pred = model_gbm.predict(test[feat].fillna(0))\n        sub_light[target] = sub_light[target] + pred \/ Nfolds\n\n        fold_ = fold_ + 1\n\n    return sub_light, scores, scores_fold\n\n\n##################### CatBoost ####################################\ndef training_CatBoost(train, test, sub_cb, target, feat, cat_feat,\n                   top_n_features, Nfolds, params_k, MODEL_ROOT):\n\n    sub_cb[target] = 0\n\n    train = train.reset_index(drop=True)\n    val_set = test.loc[test[target].dropna().index]\n\n    for col in cat_feat:\n        train[col] = train[col].fillna(0).astype(np.int)\n        val_set[col] = val_set[col].fillna(0).astype(np.int)\n        test[col] = test[col].fillna(0).astype(np.int)\n\n    if top_n_features != -1:\n        if os.path.isfile('cat_boost_feature_importance.pickle'):\n            with open('cat_boost_feature_importance.pickle', 'rb') as handle:\n                all_features = pickle.load(handle)\n                feat = list(all_features[0:top_n_features]['feature'])\n        else:\n            if params_k==True:\n                model_cb = CatBoostRegressor(\n                    n_estimators=2000,\n                    learning_rate=0.05,\n                    loss_function='MAE',\n                    eval_metric='RMSE',\n                    cat_features=cat_feat,\n                    max_bin=50,\n                    subsample=0.9,\n                    colsample_bylevel=0.5,\n                    verbose=100)\n            else:\n                model_cb = CatBoostRegressor(\n                    n_estimators=2000,\n                    loss_function='MAE',\n                    eval_metric='RMSE',\n                    cat_features=cat_feat,\n                    learning_rate=0.05,\n                    verbose=100)\n\n            model_cb.fit(train[feat], train[target], use_best_model=True, eval_set=(val_set[feat], val_set[target]),\n                         early_stopping_rounds=25)\n\n            feature_importance_df = pd.DataFrame()\n            feature_importance_df[\"feature\"] = feat\n            feature_importance_df[\"importance\"] = model_cb.get_feature_importance()\n\n            all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)\n            all_features.reset_index(inplace=True)\n\n            with open('cat_boost_feature_importance.pickle', 'wb') as handle:\n                pickle.dump(all_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            feat = list(all_features[0:top_n_features]['feature'])\n\n        temp = []\n        for col in cat_feat:\n            if col in feat:\n                temp.append(col)\n\n        cat_feat = temp\n    else:\n        print('No feature selection')\n\n    kf = KFold(n_splits=Nfolds, shuffle=True)\n\n    fold_ = 0\n    scores = []\n    scores_fold = []\n    for train_index, val_index in kf.split(train.index):\n        train_set = train.loc[train_index]\n        val_set2 = train.loc[val_index]\n        #val_set_full = pd.concat([val_set2, val_set], axis=0)\n        val_set_full = val_set\n\n        if params_k == True:\n            model_cb = CatBoostRegressor(\n                n_estimators=2000,\n                learning_rate=0.05,\n                loss_function='MAE',\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                max_bin=50,\n                subsample=0.9,\n                colsample_bylevel=0.5,\n                verbose=100)\n        else:\n            model_cb = CatBoostRegressor(\n                n_estimators=2000,\n                loss_function='MAE',\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                learning_rate=0.05,\n                verbose=100)\n\n        model_cb.fit(train_set[feat], train_set[target], use_best_model=True,\n                     eval_set=(val_set_full[feat], val_set_full[target]),\n                     early_stopping_rounds=25)\n\n        if not os.path.exists(MODEL_ROOT):\n            os.makedirs(MODEL_ROOT)\n\n        with open(MODEL_ROOT + '\/model_cb_%s_%s.pkl' % (top_n_features, fold_), 'wb') as fout:\n            pickle.dump(model_cb, fout)\n\n        pred_val = model_cb.predict(val_set[feat])\n        rmse = np.sqrt(mean_squared_error(val_set[target].values, pred_val))\n        scores.append(rmse)\n        print(\"CV score:\", rmse)\n\n        val_set2[\"pred\"] = model_cb.predict(val_set2[feat])\n        rmse_fold = np.sqrt(mean_squared_error(val_set2[target].values, val_set2[\"pred\"].values))\n        scores_fold.append(rmse_fold)\n        print(\"CV score fold:\", rmse_fold)\n\n        pred = model_cb.predict(test[feat].fillna(0))\n        sub_cb[target] = sub_cb[target] + pred \/ Nfolds\n\n        fold_ = fold_ + 1\n\n    return sub_cb, scores, scores_fold\n\n##################### XGBoost ####################################\ndef training_XGboost(train, test, sub_xg, target, feat, cat_feat,\n                   top_n_features, Nfolds, params_k, MODEL_ROOT):\n\n    sub_xg[target] = 0\n\n    train = train.reset_index(drop=True)\n    val_set = test.loc[test[target].dropna().index]\n\n    for col in cat_feat:\n        train[col] = train[col].fillna(0).astype(np.int)\n        val_set[col] = val_set[col].fillna(0).astype(np.int)\n        test[col] = test[col].fillna(0).astype(np.int)\n\n    if top_n_features != -1:\n        if os.path.isfile('cat_boost_feature_importance.pickle'):\n            with open('cat_boost_feature_importance.pickle', 'rb') as handle:\n                all_features = pickle.load(handle)\n                feat = list(all_features[0:top_n_features]['feature'])\n        else:\n            if params_k==True:\n                model_xg = xg.XGBRegressor(\n                    n_estimators=2000,\n                    learning_rate=0.05,\n                    eval_metric='RMSE',\n                    cat_features=cat_feat,\n                    max_bin=50,\n                    subsample=0.75,\n                    colsample_bylevel=0.75,\n                    verbose=50)\n            else:\n                model_xg = xg.XGBRegressor(\n                    n_estimators=2000,\n                    eval_metric='RMSE',\n                    cat_features=cat_feat,\n                    learning_rate=0.05,\n                    verbose=50)\n\n            model_xg.fit(train[feat], train[target], eval_set=[(val_set[feat], val_set[target])],\n                         eval_metric='rmse', early_stopping_rounds=25)\n\n\n            feature_importance_df = pd.DataFrame()\n            feature_importance_df[\"feature\"] = feat\n            feature_importance_df[\"importance\"] = model_xg.feature_importances_\n\n            all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)\n            all_features.reset_index(inplace=True)\n\n            with open(MODEL_ROOT+'\/xgboost_feature_importance.pickle', 'wb') as handle:\n                pickle.dump(all_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            feat = list(all_features[0:top_n_features]['feature'])\n\n        temp = []\n        for col in cat_feat:\n            if col in feat:\n                temp.append(col)\n\n        cat_feat = temp\n    else:\n        print('No feature selection')\n\n    kf = KFold(n_splits=Nfolds, shuffle=True)\n\n    fold_ = 0\n    scores = []\n    scores_fold = []\n    for train_index, val_index in kf.split(train.index):\n        train_set = train.loc[train_index]\n        val_set2 = train.loc[val_index]\n        #val_set_full = pd.concat([val_set2, val_set], axis=0)\n        val_set_full = val_set\n\n        if params_k == True:\n            model_xg = xg.XGBRegressor(\n                n_estimators=2000,\n                learning_rate=0.05,\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                max_bin=50,\n                subsample=0.75,\n                colsample_bylevel=0.75,\n                verbose=50)\n        else:\n            model_xg = xg.XGBRegressor(\n                n_estimators=2000,\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                learning_rate=0.05,\n                verbose=50)\n\n        model_xg.fit(train[feat], train[target], eval_set=[(val_set[feat], val_set[target])],\n                         eval_metric='rmse', early_stopping_rounds=25)\n            \n        if not os.path.exists(MODEL_ROOT):\n            os.makedirs(MODEL_ROOT)\n\n        with open(MODEL_ROOT + '\/model_xg_%s_%s.pkl' % (top_n_features, fold_), 'wb') as fout:\n            pickle.dump(model_xg, fout)\n\n        pred_val = model_xg.predict(val_set[feat])\n        rmse = np.sqrt(mean_squared_error(val_set[target].values, pred_val))\n        scores.append(rmse)\n        print(\"CV score:\", rmse)\n\n        val_set2[\"pred\"] = model_xg.predict(val_set2[feat])\n        rmse_fold = np.sqrt(mean_squared_error(val_set2[target].values, val_set2[\"pred\"].values))\n        scores_fold.append(rmse_fold)\n        print(\"CV score fold:\", rmse_fold)\n\n        pred = model_xg.predict(test[feat])\n        sub_xg[target] = sub_xg[target] + pred \/ Nfolds\n\n        fold_ = fold_ + 1\n\n    return sub_xg, scores, scores_fold","6e85aaff":"#### Hyperparameters ############\ncat_feats = ['type', 'source', 'station', 'dayofyear',\n            'hour',  'month', 'year', 'dayofweek', 'month_dummy',\n            'hour_dummy', 'day_dummy', 'summary', 'icon', 'week']\n\nuseless_columns = ['ID', 'date']\ntarget = 'aqi'\n\nparams_lgb = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.9,\n            'subsample_freq': 1,\n            'learning_rate': 0.03,\n            'num_leaves': 2**8-1,\n            'min_data_in_leaf': 2**9-1,\n            'feature_fraction': 0.5,\n            'max_bin': 50,\n            'n_estimators': 2000,\n            'boost_from_average': False,\n            \"random_seed\":42,\n            }\n\n\ndf, weather, sub = preprocess(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', \n                              cat_feat = cat_feats, rolling_means=[4, 8, 12], shifts=[2, 4], diffs=[2, 4])\n   \nsub[target] = 0\ntrain = df.loc[df['date']<'2019-01-01']\ntest = df.loc[df['date']>='2019-01-01']\n\nprint(train.shape)\nfor col in train.columns:\n    print(col, train[col].isnull().sum(axis=0), test[col].isnull().sum(axis=0))\n\n#train = train.dropna()\nfeat = list(train.drop(useless_columns + [target], axis=1))\n\n\nfor top_n_features in [100]:\n    print(\"*\"*25, \"top_n_features\", top_n_features, \"*\"*25,)\n    sub_light, scores, scores_fold = train_lightGBM(train=train.copy(), test=test.copy(), sub_light=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=params_lgb, MODEL_ROOT='models\/lgb')\n\n    print('LighGBM CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('LighGBM CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_light.to_csv(\"sub_lgb_%s.csv\" %top_n_features, index=False)\n        \n    print(\"*\" * 25, \"top_n_features\", top_n_features, \"*\" * 25, )\n\n    sub_cb, scores, scores_fold = training_CatBoost(train=train.copy(), test=test.copy(), sub_cb=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=True, MODEL_ROOT='models\/cb')\n\n    print('CatBoost CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('CatBoost CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_cb.to_csv(\"sub_cb_%s.csv\" %top_n_features, index=False)\n    \n    sub_xg, scores, scores_fold = training_XGboost(train=train.copy(), test=test.copy(), sub_xg=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=True, MODEL_ROOT='models\/xg')\n\n    print('XGBoost CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('XGBoost CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_xg.to_csv(\"sub_xg_%s.csv\" %top_n_features, index=False)","ce6d2dad":"# Data import and preprocess","cf37f705":"# Import libraries","9ac13ead":"# Running phase","bad32165":"# Model Training"}}