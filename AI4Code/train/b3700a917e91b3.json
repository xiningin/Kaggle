{"cell_type":{"a502d8d4":"code","1783a988":"code","de46d2cd":"code","653a91f6":"code","d8048d3c":"code","15053648":"code","e9c469cf":"code","3fa13517":"code","b22ad99a":"code","8af82086":"code","31516353":"code","a92469ca":"code","8f56d9b0":"code","2d0883cd":"code","9f28f001":"code","3977bdbb":"code","9260cee9":"code","a09ec9e9":"code","6d1e232c":"markdown","e344244c":"markdown","2f9d3472":"markdown","cac9a5a6":"markdown","47f5f29a":"markdown","889d2dad":"markdown","52336704":"markdown","ba6a73fd":"markdown","20b54b53":"markdown","314380c4":"markdown","833567e0":"markdown","dfa37e2d":"markdown","dbbb0caf":"markdown","f25c0838":"markdown","09d870d0":"markdown"},"source":{"a502d8d4":"import pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport datetime\nimport os\nimport time\nimport gc\n%matplotlib inline","1783a988":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test  = pd.read_csv('..\/input\/test.csv')\n\ndf_train_sample = df_train.copy()\ndel df_train_sample\ngc.collect()","de46d2cd":"df_train.head()","653a91f6":"df_train.tail()","d8048d3c":"df_train.shape","15053648":"dtypes = {\n        'Id'                : 'uint32',\n        'groupId'           : 'uint32',\n        'matchId'           : 'uint16',\n        'assists'           : 'uint8',\n        'boosts'            : 'uint8',\n        'damageDealt'       : 'float16',\n        'DBNOs'             : 'uint8',\n        'headshotKills'     : 'uint8', \n        'heals'             : 'uint8',    \n        'killPlace'         : 'uint8',    \n        'killPoints'        : 'uint8',    \n        'kills'             : 'uint8',    \n        'killStreaks'       : 'uint8',    \n        'longestKill'       : 'float16',    \n        'maxPlace'          : 'uint8',    \n        'numGroups'         : 'uint8',    \n        'revives'           : 'uint8',    \n        'rideDistance'      : 'float16',    \n        'roadKills'         : 'uint8',    \n        'swimDistance'      : 'float16',    \n        'teamKills'         : 'uint8',    \n        'vehicleDestroys'   : 'uint8',    \n        'walkDistance'      : 'float16',    \n        'weaponsAcquired'   : 'uint8',    \n        'winPoints'         : 'uint8', \n        'winPlacePerc'      : 'float16' \n}","e9c469cf":"train_dtypes = pd.read_csv('..\/input\/train.csv', dtype=dtypes)\ndf_train = pd.read_csv('..\/input\/train.csv')\n\n#check datatypes:\ntrain_dtypes.info()","3fa13517":"#check datatypes:\ndf_train.info()","b22ad99a":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","8af82086":"df_train = reduce_mem_usage(df_train)\ndf_train.info()","31516353":"train_dtypes = pd.read_csv('..\/input\/train.csv',nrows=10000 , dtype=dtypes)","a92469ca":"train_dtypes.head()","8f56d9b0":"train = pd.read_csv('..\/input\/train.csv', skiprows=range(1, 3000000), nrows=10000, dtype=dtypes)","2d0883cd":"train.head()","9f28f001":"del train; del train_dtypes;\ngc.collect()","3977bdbb":"columns = ['Id', 'groupId', 'matchId','killPlace','killPoints','kills','killStreaks','longestKill','winPlacePerc']\n\ndtypes = {\n        'Id'                : 'uint32',\n        'groupId'           : 'uint32',\n        'matchId'           : 'uint16',   \n        'killPlace'         : 'uint8',    \n        'killPoints'        : 'uint8',    \n        'kills'             : 'uint8',    \n        'killStreaks'       : 'uint8',    \n        'longestKill'       : 'float16',    \n        'winPlacePerc'      : 'float16' \n}\nexample = pd.read_csv('..\/input\/train.csv', usecols=columns, dtype=dtypes)","9260cee9":"example.head()","a09ec9e9":"debug = True\nif debug:\n    df_train = pd.read_csv('..\/input\/train.csv',nrows=10000 , dtype=dtypes)\n    df_test  = pd.read_csv('..\/input\/test.csv', dtype=dtypes)\nelse:\n    df_train = pd.read_csv('..\/input\/train.csv', dtype=dtypes)\n    df_test  = pd.read_csv('..\/input\/test.csv', dtype=dtypes)","6d1e232c":"### 4. Importing just selected columns  \nIf you want to analyze just some specific feature, you can import just the selected columns. ","e344244c":"### 5. Using debug mode\nMany people try to make feature engineering and predict pipelines. However, if the size of the data is large, it takes too long to create a variable or training a model. In this case, we can save time and effort by drawing a sample in advance as metioned above.","2f9d3472":"### Results\n- Using these methods, you can reduce ram spikes and time effort, memory. \n- If you have a good tip or method that you know, please ask me for a comment.","cac9a5a6":"You saved almost five times the memory.\n**864.3 MB -> 162.1 MB**","47f5f29a":"### 1. Deleting unused variables and gc.collect()\n\nUnlike other languages, Python does not efficiently utilize memory. Variables that we do not use, or that we use or discard, also occupy memory. So we have to keep in mind two things.\n\n1. Unused variables are deleted using del.\n\n2. After del deleting it, it is surely removed from memory through the command gc.collect()","889d2dad":"b) Simple row skip","52336704":"### 2. Presetting the datatypes \n\nPython automatically reads the data type, which causes a lot of memory waste. So if we know in advance the memory we will set up, we can use it much more effectively.","ba6a73fd":"You saved almost five times the memory. **864.3 MB -> 178.7  MB** . almost similiar","20b54b53":"### 3. Importing selected rows of the a file. \nIf the size of the data is large as in this competition, you can try sampling. If you check code working well, use selected rows not all rows. ( it is called debug )","314380c4":"a) Select number of rows to import","833567e0":"***source***\n- https:\/\/www.kaggle.com\/yuliagm\/how-to-work-with-big-datasets-on-16g-ram-dask\n- https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n- https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/53773\n\nFor reference, the materials here are tailored to PBUG based on the material at https:\/\/www.kaggle.com\/yuliagm\/how-to-work-with-big-datasets-on-16g-ram-dask. So if you do upvote, please upvote the article above.","dfa37e2d":"***Background***\n\n- As you know that, The size of the data is large and it takes a long time and memory error occurs. \n- So I share a study of how to save time and how to reduce memory. \n- This is my first field of study. So if there is a mistake or something to add, please add it as a comment.","dbbb0caf":"### 6. Lightgbm: prevent RAM spike (explode) at the init training\n\nI've been looking for a discussion on a similar contest called TalkingdataADtracking. \nThis competition also had a lot of memory errors due to the large data size. \nThe relevant document is as below but I have not confirmed it yet.\n\n- [LightGBM Faster Training](https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/56158)\n- [Lightgbm: prevent RAM spike (explode) at the init training\n](https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/53773)\n- [Reducing Lightgbm RAM spike using 1 difference](https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/55325)\n\n- I have not tested this part yet, so I will check the results and rewrite it.","f25c0838":"***OUTLINE***\n\n- Deleting unused variables and gc.collect()  \n- Presetting the datatypes  \n- Importing selected rows of the a file. \n- Importing just selected columns  \n- Using debug mode \n- Lightgbm: prevent RAM spike (explode) at the init training","09d870d0":"If you do not want to do the above, it's a good idea to use kaggler's code."}}