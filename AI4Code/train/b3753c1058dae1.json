{"cell_type":{"41ff46d5":"code","2f3f18cd":"code","0eae3665":"code","9b6cbf4b":"code","b3464772":"code","80068371":"code","9e2d6d90":"code","7f3a3501":"code","26121fac":"code","bb37e659":"code","877453f4":"code","dc35874e":"code","6765486a":"code","1c88fb5b":"code","08de3b7e":"code","a1438db3":"code","e14e2500":"code","65fd8d91":"code","40738cfd":"code","c84fcf11":"code","00ba4977":"code","2bc86702":"code","bc16b69e":"code","f5b0dba5":"code","3c1d8ed3":"code","1692d4a0":"code","d606d70b":"code","76ffc3c7":"code","9dd45d24":"code","421d36ee":"code","56241a77":"code","9fd8ec5a":"code","f684fbce":"code","1a483780":"code","80116652":"code","2e507b98":"code","5beeb94d":"code","2ee7b7e1":"code","670eff69":"code","38d49c7d":"code","b8b3aaea":"code","c2b19fbc":"code","b61fb4bf":"code","067e5622":"code","95f1ed11":"code","77d14c79":"code","8e7700de":"code","e436e1b2":"code","b991cff7":"code","85bd2aac":"code","d48ff64d":"code","c10a0223":"code","044a93dd":"code","a00a1909":"code","29ae9cfc":"code","6aab6112":"code","783858ee":"code","32da3c10":"code","fda5a989":"code","4315f0d6":"markdown","9c487382":"markdown","3f5cd35c":"markdown","8c7134b0":"markdown","5cf58d1a":"markdown","f983cf74":"markdown","f13b2ba9":"markdown","9993f353":"markdown","22a1eca2":"markdown","3dc14044":"markdown","c2212d15":"markdown","9da37208":"markdown","97720d3f":"markdown","0b0dd22e":"markdown","1aa40814":"markdown","8443737d":"markdown","aafdba64":"markdown","4c313180":"markdown","32c24fd8":"markdown","b1a35166":"markdown","9ffd3564":"markdown","c643f39f":"markdown","5c8a2050":"markdown","ec438b90":"markdown","e0f18fd5":"markdown","0c80db58":"markdown","1f66cf39":"markdown","e7bd3b5e":"markdown","50d52bf9":"markdown","4479b6cc":"markdown","da1c191c":"markdown","02dc6711":"markdown","c90b3b43":"markdown","968eb8c4":"markdown","5434915b":"markdown","8bd18ad0":"markdown","f4ea1923":"markdown","ec6cf7b6":"markdown","4caa9e56":"markdown","24433f59":"markdown","ccc7ebcf":"markdown","970e07d3":"markdown","1f2e6317":"markdown","6c6bdb83":"markdown","99960689":"markdown","5ebdec43":"markdown","2bb00d4d":"markdown"},"source":{"41ff46d5":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split #(divide into train\/test)\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import metrics #(vital for notebook)\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\n","2f3f18cd":"diamonds = pd.read_csv('..\/input\/diamondscsv\/diamonds.csv')\n\ndiamonds # display the first and last 5 rows","0eae3665":"diamonds.shape","9b6cbf4b":"diamonds.info()","b3464772":"sns.distplot(diamonds['price'], axlabel=\"Price\",  kde=True); # The following graph shows the price distribution of the diamonds","80068371":"diamonds['price'].describe()","9e2d6d90":"diamonds.isnull().sum() # if we have null values, we would like to drop them","7f3a3501":"diamonds.duplicated().sum() # if we have duplicates values, we would like to drop them","26121fac":"diamonds.describe()","bb37e659":"#Dropping dimentionless diamonds\ndiamonds = diamonds.drop(diamonds[diamonds[\"x\"]==0].index)\ndiamonds = diamonds.drop(diamonds[diamonds[\"y\"]==0].index)\ndiamonds = diamonds.drop(diamonds[diamonds[\"z\"]==0].index)\ndiamonds.shape","877453f4":"diamonds.drop('Unnamed: 0', axis = 1, inplace = True) # its just index\n\ndiamonds","dc35874e":"diamonds['cut'].value_counts()","6765486a":"plt.figure(figsize = (10,5))\nsns.countplot(x = diamonds['cut']); # we will plot it for better illustration","1c88fb5b":"diamonds['color'].value_counts()","08de3b7e":"plt.figure(figsize = (10,5))\nsns.countplot(x = diamonds['color']); # we will plot it for better illustration","a1438db3":"diamonds['clarity'].value_counts()","e14e2500":"plt.figure(figsize = (10,5))\nsns.countplot(x = diamonds['clarity']); # we will plot it for better illustration","65fd8d91":"#prepertion - categorical variables\n\ndiamonds['cut'] = diamonds['cut'].map({\n    'Ideal'        :5,\n    'Premium'      :4,\n    'Very Good'    :3,\n    'Good'         :2,\n    'Fair'         :1\n})\n\ndiamonds['color'] = diamonds['color'].map({\n    'D'            :7,\n    'E'            :6,\n    'F'            :5,\n    'G'            :4,\n    'H'            :3,\n    'I'            :2,\n    'J'            :1\n})\ndiamonds['clarity'] = diamonds['clarity'].map({\n    'FL'            :11,\n    'IF'            :10,\n    'VVS1'          :9,\n    'VVS2'          :8,\n    'VS1'           :7,\n    'VS2'           :6,\n    'SI1'           :5,\n    'SI2'           :4,\n    'I1'            :3,\n    'I2'            :2,\n    'I13'           :1\n})\n\ndiamonds","40738cfd":"# I want to know the correalation between each feature\nplt.figure(figsize=(20, 10))\nsns.heatmap(diamonds.corr(), annot=True);","c84fcf11":"sns.scatterplot(x=diamonds.price, y=diamonds.x, color='red')\nplt.title(\"Price VS 'x'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.y, color='blue')\nplt.title(\"Price VS 'y'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.z, color='green')\nplt.title(\"Price VS 'z'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.cut, color='orange')\nplt.title(\"Price VS 'cut'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.table, color='purple')\nplt.title(\"Price VS 'table'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.color, color='cyan')\nplt.title(\"Price VS 'color'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.clarity, color='brown')\nplt.title(\"Price VS 'clarity'\")\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.depth, color='pink')\nplt.title(\"Price VS 'depth'\")\nplt.show()","00ba4977":"ax = sns.regplot(x=\"price\", y=\"x\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs 'X'\", color=\"red\", loc='left');\n# it looks ok to me","2bc86702":"ax = sns.regplot(x=\"price\", y=\"y\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs 'Y'\", color=\"red\", loc='left');\n# the outliers are unusually prominent (n<30)","bc16b69e":"ax= sns.regplot(x=\"price\", y=\"z\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs 'Z'\", color=\"red\", loc='left');\n# the outliers are unusually prominent (2<n<30)","f5b0dba5":"ax= sns.regplot(x=\"price\", y=\"carat\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs Carat\", color=\"red\", loc='left');\n# it looks ok to me","3c1d8ed3":"ax=sns.regplot(x=\"price\", y=\"table\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs Table\", color=\"red\");\n# the outliers are unusually prominent (45<X<75)","1692d4a0":"ax=sns.regplot(x=\"price\", y=\"depth\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs Depth\", color=\"red\");\n# the outliers are unusually prominent (45<X<75)","d606d70b":"# Due to the conclusions from the graphs, we will drop the outliers\ndiamonds = diamonds[(diamonds[\"y\"]<30)]\ndiamonds = diamonds[(diamonds[\"z\"]<30)&(diamonds[\"z\"]>2)]\n# diamonds = diamonds[(diamonds[\"carat\"]<4.2)] it made my model worse\ndiamonds = diamonds[(diamonds[\"table\"]<75)&(diamonds[\"table\"]>40)]\ndiamonds = diamonds[(diamonds[\"depth\"]<75)&(diamonds[\"depth\"]>50)]\n\ndiamonds.shape","76ffc3c7":"sns.scatterplot(x=diamonds.price, y=diamonds.x, color='red')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.y, color='blue')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.z, color='green')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.cut, color='orange')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.table, color='purple')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.color, color='cyan')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.clarity, color='brown')\nplt.show()\nsns.scatterplot(x=diamonds.price, y=diamonds.depth, color='pink')\nplt.show()","9dd45d24":"ax = sns.regplot(x=\"price\", y=\"y\", data=diamonds, fit_reg=True, scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\nax.set_title(\"Regression Line on Price vs 'z'\", color=\"red\");","421d36ee":"plt.figure(figsize=(6,6))\nsns.scatterplot(diamonds.x, diamonds.z, hue=diamonds.price)\nplt.title(\"'x' vs 'z' with price\")\nplt.show()\nplt.figure(figsize=(6,6))\nsns.scatterplot(diamonds.x, diamonds.y, hue=diamonds.price)\nplt.title(\"'x' vs 'y' with price\")\nplt.show()\nsns.scatterplot(diamonds.x, diamonds.y, hue=diamonds.price)\nplt.title(\"'y' vs 'z' with price\")\nplt.show()","56241a77":"plt.figure(figsize=(6,6))\nsns.scatterplot(diamonds.carat, diamonds.x, hue=diamonds.price)\nplt.title(\"carat vs 'x' with price\")\nplt.show()\nplt.figure(figsize=(6,6))\nsns.scatterplot(diamonds.carat, diamonds.y, hue=diamonds.price)\nplt.title(\"carat vs 'y' with price\")\nplt.show()\nsns.scatterplot(diamonds.carat, diamonds.y, hue=diamonds.price)\nplt.title(\"carat vs 'z' with price\")\nplt.show()","9fd8ec5a":"diamonds.describe()","f684fbce":"# the updated heatmap:\n\nplt.figure(figsize=(16, 8))\nsns.heatmap(diamonds.corr(), annot=True);","1a483780":"volume = np.ones(len(diamonds)) # the new column\ndiamonds_vol = diamonds.copy() # copy of diamonds dataset\ndiamonds_vol['volume'] = volume # adding the new column\n\ndiamonds_vol['volume'] = diamonds_vol['x'] * diamonds_vol['y'] * diamonds_vol['z'] # volume = x*y*z\n\n# dropping the x,y,z columns:\ndiamonds_vol.drop('x', axis = 1, inplace = True) \ndiamonds_vol.drop('y', axis = 1, inplace = True)\ndiamonds_vol.drop('z', axis = 1, inplace = True)\n\ndiamonds_vol.head(3)","80116652":"features = ['carat','cut','color','clarity', 'depth', 'table', 'x','y','z']\nss = StandardScaler()\ndf = diamonds\ndf[features] = ss.fit_transform(df[features])\ndf.head(3)","2e507b98":"# Assigning the featurs as X and trarget as y\ny = df[\"price\"]\nX = df.drop([\"price\"],axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","5beeb94d":"# equivalent command to do this in one line\n#X = df[['carat', 'x', 'y', 'z']]\n\n# print the first 5 rows\nX.head()","2ee7b7e1":"# check the type and shape of X\nprint(type(X))\nprint(X.shape)","670eff69":"# select a Series from the DataFrame\ny = df['price']\n\n# equivalent command that works if there are no spaces in the column name\ny = df.price\n\n# print the first 5 values\ny.head()","38d49c7d":"# check the type and shape of y\nprint(type(y))\nprint(y.shape)","b8b3aaea":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # train\/test split","c2b19fbc":"# default split is 75% for training and 25% for testing\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b61fb4bf":"dummy_regr_mean = DummyRegressor(strategy=\"mean\")\ndummy_regr_mean.fit(X_train, y_train)\ny_pred = dummy_regr_mean.predict(X_test)\nerr_dummy_mean = mean_squared_error(y_test,dummy_regr_mean.predict(X_test), squared=False)\ninitial_prediction_dm_mean = dummy_regr_mean.predict(X_test)\n\nprint(\"R2 Score:\",metrics.r2_score(y_test, y_pred))\n#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint (\"Mean Square ERR:\",mean_squared_error(y_test,dummy_regr_mean.predict(X_test), squared=False))","067e5622":"results = {}\nresults[\"Predicted\"] = initial_prediction_dm_mean.round(2)\nresults[\"Reality\"] = y_test\n\npd.DataFrame.from_dict(results)","95f1ed11":"plt.plot(y_test, y_pred, 'o')\nln, b = np.polyfit(y_test, y_pred,1)\nplt.plot(y_test,ln*y_test+b);","77d14c79":"dummy_regr_median = DummyRegressor(strategy=\"median\")\ndummy_regr_median.fit(X_train, y_train)\ny_pred = dummy_regr_median.predict(X_test)\nerr_dummy_meadian = mean_squared_error(y_test,dummy_regr_median.predict(X_test), squared=False)\ninitial_prediction_dm_median = dummy_regr_median.predict(X_test)\n\nprint(\"R2 Score:\",metrics.r2_score(y_test, y_pred))\n#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint (\"Mean Square ERR:\",mean_squared_error(y_test,dummy_regr_median.predict(X_test), squared=False))","8e7700de":"results = {}\nresults[\"Predicted\"] = initial_prediction_dm_median.round(2)\nresults[\"Reality\"] = y_test\n\npd.DataFrame.from_dict(results)","e436e1b2":"plt.plot(y_test, y_pred, 'o')\nln, b = np.polyfit(y_test, y_pred,1)\nplt.plot(y_test,ln*y_test+b);","b991cff7":"knn = KNeighborsRegressor()\nknn.fit(X_train, y_train)\ninitial_prediction_knn = knn.predict(X_test)\ny_pred = knn.predict(X_test)\n\nprint(\"R2 Score:\",metrics.r2_score(y_test, y_pred))\nprint (\"Mean Square ERR:\",mean_squared_error(y_test,initial_prediction_knn, squared=False))","85bd2aac":"for k in range(3,21):  \n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    initial_prediction_knn = knn.predict(X_test)\n    y_pred = knn.predict(X_test)\n    print (\"Mean Square ERR for k =\",k,\":\",mean_squared_error(y_test,initial_prediction_knn, squared=False))","d48ff64d":"knn = KNeighborsRegressor(n_neighbors=10)\nknn.fit(X_train, y_train)\ninitial_prediction_knn = knn.predict(X_test)\ny_pred = knn.predict(X_test)\n\nprint(\"R2 Score:\",metrics.r2_score(y_test, y_pred))\nprint (\"Mean Square ERR:\",mean_squared_error(y_test,initial_prediction_knn, squared=False))","c10a0223":"knn_reg_scores = cross_val_score(KNeighborsRegressor(n_neighbors=10),X_train, y_train, cv=10, scoring=\"neg_mean_squared_error\")\nknn_reg_scores_r2 = cross_val_score(KNeighborsRegressor(n_neighbors=10),X_train, y_train, cv=10, scoring=\"r2\")\n\n#print(knn_reg_scores)\n#print(knn_reg_scores_r2)\n\nprint(\"After cross validation:\\n\")\nprint(\"R2 Score:\",knn_reg_scores_r2.mean())\nprint (\"Mean Square ERR:\",np.sqrt(-knn_reg_scores.mean()))","044a93dd":"results = {}\nresults[\"Predicted\"] = initial_prediction_knn.round(2)\nresults[\"Reality\"] = y_test\n\npd.DataFrame.from_dict(results)","a00a1909":"plt.plot(y_test, y_pred, 'o')\nln, b = np.polyfit(y_test, y_pred,1)\nplt.plot(y_test,ln*y_test+b);","29ae9cfc":"linear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\ninitial_prediction_lr = linear_reg.predict(X_test)\n\nprint(\"R2 Score:\",linear_reg.score(X_test,y_test))\nprint(\"Mean Square ERR:\",mean_squared_error(y_test,initial_prediction_lr, squared=False))","6aab6112":"linear_reg_scores = cross_val_score(LinearRegression(),X_train, y_train,cv=5,scoring=\"neg_mean_squared_error\", )\nlinear_reg_scores_r2 = cross_val_score(LinearRegression(),X_train, y_train,cv=5,scoring=\"r2\")\n\n#print(linear_reg_scores)\n#print(linear_reg_scores_r2)\n\nprint(\"After cross validation:\\n\")\nprint(\"R2 Score:\",linear_reg_scores_r2.mean())\nprint (\"Mean Square ERR:\",np.sqrt(-linear_reg_scores.mean()))","783858ee":"results = {}\nresults[\"Predicted\"] = initial_prediction_lr.round(2)\nresults[\"Reality\"] = y_test\n\npd.DataFrame.from_dict(results)","32da3c10":"plt.plot(y_test, y_pred, 'o')\nln, b = np.polyfit(y_test, y_pred,1)\nplt.plot(y_test,ln*y_test+b);","fda5a989":"print(\"The score for the models:\\n\")\nprint(\"Dummy Regressor:     \",round(err_dummy_mean,5),\"  Mean Square ERR.\")\nprint(\"KNN Regressor:       \",round(np.sqrt(-knn_reg_scores.mean()),5),\"    Mean Square ERR.\")\nprint(\"Linear Regression:   \",round(np.sqrt(-linear_reg_scores.mean()),5),\"  Mean Square ERR.\")","4315f0d6":"# KNN Regressor","9c487382":"There are 10 attributes included in the dataset including the target ie. price.\n\n###### Feature description:\n\n- **Price:** price in US dollars ($\\$$326 - $\\$$18,823) This is the target column containing tags for the features. \n\n###### The 4 Cs of Diamonds:\n\n- **Carat:** (0.2--5.01) The carat is the diamond\u2019s physical weight measured in metric carats.  One carat equals 1\/5 gram and is subdivided into 100 points. Carat weight is the most objective grade of the 4Cs. \n\n- **Cut:** (Fair$\\rightarrow$Good$\\rightarrow$Very Good$\\rightarrow$Premium$\\rightarrow$Ideal) In determining the quality of the cut, the diamond grader evaluates the cutter\u2019s skill in the fashioning of the diamond. The more precise the diamond is cut, the more captivating the diamond is to the eye.  \n\n- **Color:** from J (worst) to D (best). The colour of gem-quality diamonds occurs in many hues. In the range from colourless to light yellow or light brown. Colourless diamonds are the rarest. Other natural colours (blue, red, pink for example) are known as \"fancy,\u201d and their colour grading is different than from white colorless diamonds.  \n\n- **Clarity:** (I1 (worst)$\\rightarrow$SI2$\\rightarrow$SI1$\\rightarrow$VS2$\\rightarrow$VS1$\\rightarrow$VVS2$\\rightarrow$VVS1$\\rightarrow$IF (best)). Diamonds can have internal characteristics known as inclusions or external characteristics known as blemishes. Diamonds without inclusions or blemishes are rare; however, most characteristics can only be seen with magnification.  \n\n###### Dimensions:\n\n- **Depth:** The height of a diamond, measured from the culet to the table, divided by its average girdle diameter.\n\n- **Table:** The width of the diamond's table expressed as a percentage of its average diameter.\n\n- **X:** Length in mm (0-10.74).\n\n- **Y:** Width in mm (0-58.9).\n\n- **Z:** Depth in mm (0-31.8).\n\n###### What else do we know?\n- Because the response variable is continuous, this is a **regression** problem. ","3f5cd35c":"# Outliers","8c7134b0":"# SCALING","5cf58d1a":"#### A linear approach to modelling the relationship between a scalar response and one or more explanatory variables.","f983cf74":"You can see that the grapghs look much more better now, no outliers in our data.","f13b2ba9":"### Quick review:\n##### Link to the dataset: https:\/\/www.kaggle.com\/braitonnepomucen\/diamondscsv\nThis classic dataset contains the prices and other attributes of almost 54,000 diamonds.\nI would like to create a model which will predict the price of a diamond, according to it's data.\nThe things that we going to find out:\n- What is the biggest factor on diamond's price?\n- Do people are likely to pay a high price on a diamond because of it's look, size or it's quality?\n\nFirstly, we will upload the dataset:","9993f353":"#### The output is the property value for the object. This value is the average of the values of k nearest neighbors.","22a1eca2":"# Dummy Regressor","3dc14044":"As you can see, the dataset has no null data, it also has no duplicated rows.","c2212d15":"# Linear Regression","9da37208":"# Diamond Price Prediction\n#####  @ Haim Goldfisher","97720d3f":"We will find the best number of neighbors (k) for better results:","0b0dd22e":"### Cross validation:","1aa40814":"### By median:","8443737d":"### Cross validation:","aafdba64":"It can be seen that there is a gap between values that have correlation and those that do not. Moreover, points in the graph that are abnormal are noticeable. We would like to examine this. I chose to explore the features which are NOT ordinal. Because outliers cannot be found in ordinal categories.\n","4c313180":"According to the explantionfor, the order is: Fair, Good, Very Good, Premium, Ideal.\n\nSo that, we would like to set the order:\n\n1 - Fair\n\n2 - Good\n\n3 - Very Good\n\n4 - Premium\n\n5 - Ideal","32c24fd8":"As we saw, cut, color aand clarity are ordinal data. We would like to make them numerical so 1 will present the worst and the biggest number will present the best.\nFirstly, we would like to understand what do those columns icluded.","b1a35166":"Reminder: We had 53,920 diamonds, now we have 53,905. It means that we \"lost\" only 15 diamonds.\n\nNow that we have removed regression outliers, let us have a look at the pair plot of data in our hand.","9ffd3564":"![1_f7qq7QZeyQzYF0RTxzPhkg.jpeg](attachment:1_f7qq7QZeyQzYF0RTxzPhkg.jpeg)","c643f39f":"Let's take a look at the x,y,z columns. In the min's row, we have some illegal values. Those diamonds are dimensionless, or lost one of the dimensions. We must filter out those as it clearly faulty data points","5c8a2050":"I decided to remove some unnecessary columns.\nIt seems that the first column present the index of the row. We will drop it:","ec438b90":"![image.png](attachment:image.png)","e0f18fd5":"According to the explantionfor, D is the best and J is the worst.\n\nSo that, we would like to set the order:\n\n1 - J\n\n2 - I\n\n3 - H\n\n4 - G\n\n5 - F\n\n6 - E\n\n7 - D","0c80db58":"## It can be concluded that KNN-Regressor is the most accurated model for our dataset, with 711.47978 Mean Square ERR.","1f66cf39":"After we have finished cleaning the data, we can start with the most significant part, the models.","e7bd3b5e":"According to the explantionfor, the order from best to worst: FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3.\n\nSo that, we would like to set the order:\n\n1 - I3 (level 3 inclusions)\n\n2 - I2\n\n3 - I1\n\n4 - SI2\n\n5 - SI1\n\n6 - VS2\n\n7 - VS1\n\n8 - VVS2\n\n9 - VVS1\n\n10 - IF\n\n11 - FL (flawless)","50d52bf9":"More graphs to illustrate the direct connection between 'x','y','z' and the carat.","4479b6cc":"### By mean:","da1c191c":"We had at the start 53,940 diamonds, now we have 53,920. It means that we \"lost\" only 20 diamonds.","02dc6711":"![image.png](attachment:image.png)","c90b3b43":"$Var(mean)$ : The sum of the squared differences from the mean line of our data. The black line in the above graph represents the mean line.","968eb8c4":"# Conclusion","5434915b":"![image.png](attachment:image.png)","8bd18ad0":"$Var(line)$ : The sum of the squared differences from our model line. The blue line in the above graph represents the model line.","f4ea1923":"Some graphs to illustrate the symmetry of a diamond.","ec6cf7b6":"One last thing before we start with the model part. While I was trying to make my model better, I played with the data a little bit. I thought that maybe we can calculate the volume of the diamond and drop the x,y,z columns. 'volume' had 0.92 value of correlation with 'price'. However, the results were worse than my currect results. You can see by yourself the new dataset I tried to test:","4caa9e56":"K = 10 gave us the best results","24433f59":"# $R^2$ Score","ccc7ebcf":"## Model Building","970e07d3":"**Evolution:** As you can see, our work paid off. The correlation between the price to \"carat\", and x,y,z is stronger now:\n\n- **carat -** bofore: 0.92. after: 0.92.\n- **'x' -** bofore: 0.89. after: 0.89.\n- **'y' -** bofore: 0.87. after: 0.89.\n- **'z' -** bofore: 0.87. after: 0.88.","1f2e6317":"A few points to notice in these plots:\n\nThere are some features with datapoint that are far from the rest of dataset which will affect the outcome of our regression model. We call them \"outliers\".\n\n- \"x\" has no dimensional outlies in our dataset.\n- \"y\" and \"z\" have some dimensional outlies in our dataset that need to be eliminated.\n- The \"depth\" should be capped but we must examine the regression line to be sure.\n- The \"table\", \"carat\" features should be capped too.","6c6bdb83":"We will use $R^2$ Score as our accurucy metric. The formula of $R^2$ is:\n\n$$R^2 = \\frac{Var(mean)-Var(line)}{Var(mean)}$$ $*(Var = Variation)$","99960689":"The heatmap tells us the biggest story. We can infer that:\n- The carat is the biggest factor to determine the price of a diamond. It obviously that X,Y and Z have a direct impact on the carat weight. The other features have a small value compare to it. It makes me think what would happen if we didn't have the size of the diamond. However, we can understand that the size means a lot when it comes to diamonds.\n- If the carat has direct impact from the x,y,z values, maybe those columns are useless for the model. Because all those columns tell the same story. Moreover, maybe that fact can make our model unstable (to much weight to the size).\n- \"depth\", \"cut\" and \"table\" show low correlation. We could consider dropping it.\n- Diamonds are symetric. That fact has no connection to our model, but we can see that there is a huge correlation between x,y and z.\n\n\nLet's have a look at regression plots to get a close look at the data:","5ebdec43":"## Data Preprocessing\n","2bb00d4d":"![image.png](attachment:image.png)"}}