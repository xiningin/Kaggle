{"cell_type":{"58291315":"code","da82bc2a":"code","a7b057c8":"code","26a2621d":"code","b4e52528":"code","06457095":"code","889dc678":"code","ae7e9428":"code","b5317ed6":"code","bb5e2aca":"code","49fa619f":"code","36606e85":"code","9efbc060":"code","82dd8058":"code","71307b4b":"code","69114252":"code","baad1a14":"code","af7d7e3b":"code","4fe5558e":"code","ece7af4e":"code","e968ff62":"markdown","44589bb0":"markdown","57a524a0":"markdown","51b6e1d9":"markdown","dc86dfa7":"markdown","df0105a5":"markdown","3ce017f4":"markdown","76bba9ac":"markdown","729e9e66":"markdown","e78361ea":"markdown","ee1c8c4c":"markdown","c965b97f":"markdown","d4f65ee8":"markdown","61751cfa":"markdown","05e0a203":"markdown","b419213b":"markdown","2735a645":"markdown","e19bfdc5":"markdown","da4ca271":"markdown","ad389c4e":"markdown","753d7dac":"markdown","15cd0571":"markdown","2c41385b":"markdown","a2f6b88e":"markdown"},"source":{"58291315":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE \n\n","da82bc2a":"X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)","a7b057c8":"counter=Counter(y)\ncounter","26a2621d":"for label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","b4e52528":"oversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","06457095":"counter = Counter(y)\ncounter","889dc678":"for label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","ae7e9428":"oversample1 = SMOTE(sampling_strategy=0.3)","b5317ed6":"X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n","bb5e2aca":"for label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","49fa619f":"# transform the dataset\nX, y = oversample1.fit_resample(X, y)","36606e85":"for label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","9efbc060":"# Random Forest evaluated on imbalanced dataset\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n\n# define model\nmodel = RandomForestClassifier()\n\n# evaluate \ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\naccuracy = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(accuracy))","82dd8058":"# Random forest evaluated on imbalanced dataset with SMOTE oversampling\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n\n# apply SMOTE\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\n# define model\nmodel = RandomForestClassifier()\n\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\naccuracy = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(accuracy))\n","71307b4b":"# borderline-SMOTE for imbalanced dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom matplotlib import pyplot\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n# transform the dataset\noversample = BorderlineSMOTE()\nX, y = oversample.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","69114252":"# borderline-SMOTE with SVM for imbalanced dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SVMSMOTE\nfrom matplotlib import pyplot\nimport numpy as np\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n# transform the dataset\noversample = SVMSMOTE()\nX, y = oversample.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","baad1a14":"# define model\nmodel = RandomForestClassifier()\n\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\naccuracy = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(accuracy))\n","af7d7e3b":"# Oversample and plot imbalanced dataset with ADASYN\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import ADASYN\nfrom matplotlib import pyplot\nimport numpy as np\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n\n# transform the dataset\noversample = ADASYN()\nX, y = oversample.fit_resample(X, y)\n\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","4fe5558e":"# Oversample and plot imbalanced dataset with SMOTE + Tomek\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.combine import SMOTETomek\nfrom matplotlib import pyplot\nimport numpy as np\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n\n# transform the dataset\noversample = SMOTETomek()\nX, y = oversample.fit_resample(X, y)\n\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","ece7af4e":"# Oversample and plot imbalanced dataset with SMOTE + ENN\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.combine import SMOTEENN\nfrom matplotlib import pyplot\nimport numpy as np\n\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                           n_clusters_per_class=1, weights=[0.975], flip_y=0, random_state=1)\n\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n\n# transform the dataset\noversample = SMOTEENN()\nX, y = oversample.fit_resample(X, y)\n\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\nplt.legend()\nplt.show()","e968ff62":"> ## \"*Balance is Key in Life*\"","44589bb0":"# Visualizing different SMOTE based resampling techniques\n\n<font size='3'>The below given picture shows how a imbalanced dataset looks once we apply SMOTE and\/or SMOTE variation resampling techniques.<\/font>","57a524a0":"# Types of SMOTE","51b6e1d9":"### Adaptive Synthetic Sampling (ADASYN)\n\n<font size='3'>ADASYN is a form of the SMOTE algorithm which also aims to oversample the minority class by generating synthetic instances. ADASYN generates more synthetic instances where the density of minority instances is low and it generates few new synthetic instances where density is high.\n<\/font>","dc86dfa7":"## ***Can we deal with Imbalanced Data?***\n<font size=\"3\">Yes, there are three general ways to deal with the problem of Imbalanced dataset \u2013\n\n1. Under-sampling: To make class distribution nearly uniform or balanced, undersampling down-sizes the majority class by removing observations until the dataset is balanced.\n\n2. Over-sampling: More often used sampling technique to balance class distribution is over-sampling, which over-sizes the minority class by adding observations. In other words, it generates observation from minority class to balance the distribution.\n\n3. Hybrid techniques: Hybrid techniques are the combination of both oversampling and under-sampling techniques.\nMost of the data scientist and Machine learning enthusiast uses over-sampling in most cases over undersampling techniques because under-sampling removes observations from data that may be carrying some important information.\n\nHere we will be specifically focusing on SMOTE (Synthetic Minority Oversampling Technique) oversampling technique.<\/font>","df0105a5":"### **Balanced distribution Post SMOTE:**\n![image.png](attachment:image.png)","3ce017f4":"### SMOTE + ENN\n\n<font size='3'>SOMTE + ENN (Edited Nearest Neighbour) is another very popular hybrid method. SMOTE + ENN by default uses k=3 nearest neighbors to locate minority or majority class instances that overlap the space of other classes or are misclassified in a dataset. Later such instances are removed or dropped from the dataset. <\/font>","76bba9ac":"<font size=\"10\">I<\/font><font size=\"3\">n today's Machine Learning and Data Science driven world we often come across Imbalanced data distribution, where observations in one of the classes are much higher or lower than the other classes. For example when dealing with problems related to churn analysis, analysis related to a rare disease, Fraud Detection, Facial recognition, etc. where you have a limited number of observations in a particular class.\n\nThe challenge of working with such imbalanced datasets is that algorithms like Random Forest, Decision Tree, Artificial neural networks, etc will get impacted and will have poor performance.\n\nLet's assume you are working on a Credit card-based fraud detection problem. Such problems are bound to have imbalanced datasets. Out of 1000 data points, we can have 975 data points where credit card transactions were non-fraudulent and 25 are fraudulent. Clearly, in this example, class distribution is skewed. Now if we would have implemented standard Machine Learning algorithms such as Decision Tree and Logistic Regression or advanced algorithms like ANN then they will have a bias towards the majority class and tend to ignore the minority class. The prediction will be based on the majority class. Hence, if we have imbalanced data distribution in our dataset then our model is not reliable. <\/font>","729e9e66":"### **How SMOTE creates new synthetic instances:**\n\n<font size=\"3\">SMOTE choses random instance from minority class and finds k nearest neighbour and creates a new synthetic instance along the line (depicted as red colour dot in below image)<\/font>\n\n![image.png](attachment:image.png)\n\n","e78361ea":"### Borderline-SMOTE SVM\n\n<font size='3'>Borderline-SMOTE SVM is an alternative of Borderline-SMOTE where instead of KNN to find the nearest neighbor, SVM algorithm is used to define instances of minority class which is close to support vectors.<\/font>","ee1c8c4c":"<font size='3'>Visulizing balanced dataset post SMOTE:<\/font>","c965b97f":"## ***Can we modify sampling ratio?***\n\n<font size='3'>Ideally, SMOTE creates an equal number of new synthetic instances for minority classes to balance the class distribution but we can choose the ratio or proportion based on which we want a new synthetic instance for the minority class. We can update the above example to first oversample the minority class to have 30 percent of the new synthetic instances.<\/font>","d4f65ee8":"# Conclusion\n\n<font size='3'>TThere are many more extensions to SMOTE which can be used to deal with the issue of class imbalance in the case of binary and multi-class classification problems. Therefore, it is important to apply the specific techniques to the dataset which helps the models to perform better and gives most of the accurate predictions.<\/font>","61751cfa":"# Synthetic data to solve Imbalanced Classification","05e0a203":"<font size='3'>Here we will implement the Random Forest algorithm on an imbalanced dataset where we have 1000 instances out of which the majority class has 975 instances and the minority class has 25 instances. Of course, the below dataset is synthetic and made up to explain how accuracy changes, when a particular algorithm is applied to an imbalanced dataset pre and post-SMOTE.<\/font>","b419213b":"### SMOTE + Tomek Resampling\n<font size='3'>SMOTE + TOMEK is a hybrid technique that involves both undersampling and oversampling. This optimizes the performance of classifier models. Firstly, SMOTE oversampling is done and class distribution may overlap on each other, this could lead to overfitting. To address this issue Tomek links will be applied on both original and synthetic instances. It removes the class observation of both classes which overlaps each other's space. Hence, Tomek links instead of removing the observations only from the majority class, it removes both the class observations. This combination could yield higher recall though at the cost of precision.<\/font>","2735a645":"<font size='3'>We can use the Counter object to summarize the number of samples in each class to confirm class distribution post SMOTE.<\/font>","e19bfdc5":"### Borderline-SMOTE\n\n<font size='3'>Borderline SMOTE classifies an instance of minority class as a noise point and ignores such instances while creating synthetic instances. It often gives more attention to extreme observations which sometimes can be an issue.<\/font>","da4ca271":"<font size=\"3\">We can use the Counter object to summarize the number of samples in each class to confirm class distribution pre SMOTE.<\/font>","ad389c4e":"<font size='3'>Visualizing original dataset: <\/font>","753d7dac":"![image.png](attachment:image.png)","15cd0571":"## Practical implementation of SMOTE","2c41385b":"<font size=\"3\">Lets create a synthetic binary classification dataset with 1,000 samples using make_classification() function from scikit-learn. Distribution will have 25 samples in minority class and 975 smaples in majority class. Hence, a skewed and imbalanced dataset.<\/font>","a2f6b88e":"## *What is SMOTE and How it works ?*\n\n<font size=\"5\">**S**<\/font><font size=\"3\">*ynthetic*<\/font> <font size=\"5\">**M**<\/font><font size=\"3\">*inority*<\/font> <font size=\"5\">**O**<\/font><font size=\"3\">*verSampling*<\/font> <font size=\"5\">**TE**<\/font><font size=\"3\">*chnique* carries out an oversampling approach to create synthetic samples for the minority class to rebalance the training set. The idea is to focus on feature space to generate new instances by interpolation between several positive instances that lie together.\n\nA random instance from the minority is selected, then finds k nearest minority class neighbors, and a line is drawn between the two instances in the feature space and a new synthetic instance is created along that line.\n\nConsider the below image as imbalanced data set where instances in orange color are minority class.<\/font>\n\n### **Pre SMOTE:**\n![image.png](attachment:image.png)\n\n\n"}}