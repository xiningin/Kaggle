{"cell_type":{"a5c78ef7":"code","b663638d":"code","889220a8":"code","2a105bb9":"code","21f87c87":"code","21b13a4b":"code","2d5105e7":"code","8cf1c6a5":"code","fd8bcf7f":"code","95e3e06f":"code","2d89b79a":"code","12654664":"code","a3b15b5d":"code","fcd65152":"code","0ad95f44":"code","22dd48f4":"code","425b2f46":"code","f9c173da":"code","95af85be":"code","647da1ab":"code","a01c1cd2":"code","32bdb785":"code","43a278d5":"code","5a8304bd":"code","85ba1ae2":"code","a9e3b579":"code","8d9be062":"code","76ca059f":"code","fd234c51":"code","1ec6e303":"code","63b51264":"code","875bccc9":"code","5113ed17":"code","ef1b5af0":"markdown","8407354e":"markdown","727c1d27":"markdown","bba63c10":"markdown","dd471291":"markdown","b0e8ceab":"markdown","2d836309":"markdown","5ff18be9":"markdown","7e44f459":"markdown","4b1d9cee":"markdown","496be26a":"markdown","c1dba957":"markdown","b27979ad":"markdown","be4932ca":"markdown","075e2b37":"markdown","6cf90152":"markdown","6244a112":"markdown","01a0b1d3":"markdown","0ccdbfa1":"markdown","15b7fb1f":"markdown","6956e096":"markdown","b3edb43d":"markdown","29e5d66f":"markdown"},"source":{"a5c78ef7":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport ast\nimport os\nimport json\nimport pandas as pd\nimport torch\nimport importlib\nimport cv2 \n\nfrom shutil import copyfile\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import GroupKFold\nfrom PIL import Image\nfrom string import Template\nfrom IPython.display import display\n\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'","b663638d":"# check Torch and CUDA version\nprint(f\"Torch: {torch.__version__}\")\n!nvcc --version","889220a8":"!git clone https:\/\/github.com\/Megvii-BaseDetection\/YOLOX -q\n\n%cd YOLOX\n!pip install -U pip && pip install -r requirements.txt\n!pip install -v -e . ","2a105bb9":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","21f87c87":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row","21b13a4b":"df = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf.head(5)","2d5105e7":"# Taken only annotated photos\ndf[\"num_bbox\"] = df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf_train = df[df[\"num_bbox\"]>0]\n\n#Annotations ast.literal_eval \u6587\u5b57\u5217\u3092Python\u306e\u30ea\u30c6\u30e9\u30eb\u3068\u3057\u3066\u898b\u3066\u3001\u30ea\u30b9\u30c8\u3084\u8f9e\u66f8\u306b\u5909\u63db\u3059\u308b\ndf_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\n\n#Images resolution\ndf_train[\"width\"] = 1280\ndf_train[\"height\"] = 720\n\n#Path of images\ndf_train = df_train.progress_apply(get_path, axis=1)","8cf1c6a5":"kf = GroupKFold(n_splits = 5) \ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\n\ndf_train.head(5)","fd8bcf7f":"HOME_DIR = '\/kaggle\/working\/' \nDATASET_PATH = 'dataset\/images'\n\n!mkdir {HOME_DIR}dataset\n!mkdir {HOME_DIR}{DATASET_PATH}\n!mkdir {HOME_DIR}{DATASET_PATH}\/train2017\n!mkdir {HOME_DIR}{DATASET_PATH}\/val2017\n!mkdir {HOME_DIR}{DATASET_PATH}\/annotations","95e3e06f":"SELECTED_FOLD = 4\n\nfor i in tqdm(range(len(df_train))): #\u300c\u9032\u6357\u72b6\u6cc1\u3084\u51e6\u7406\u72b6\u6cc1\u3092\u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\uff08\u30b9\u30c6\u30fc\u30bf\u30b9\u30d0\u30fc\uff09\u3068\u3057\u3066\u8868\u793a\u300d\u3059\u308b\u6a5f\u80fd\n    row = df_train.loc[i]\n    if row.fold != SELECTED_FOLD:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/train2017\/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/val2017\/{row.image_id}.jpg') ","2d89b79a":"print(f'Number of training files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}\/train2017\/\"))}')\nprint(f'Number of validation files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}\/val2017\/\"))}')","12654664":"def save_annot_json(json_annotation, filename):\n    with open(filename, 'w') as f:\n        output_json = json.dumps(json_annotation)\n        f.write(output_json)","a3b15b5d":"annotion_id = 0","fcd65152":"def dataset2coco(df, dest_path):\n    \n    global annotion_id\n    \n    annotations_json = {\n        \"info\": [],\n        \"licenses\": [],\n        \"categories\": [],\n        \"images\": [],\n        \"annotations\": []\n    }\n    \n    info = {\n        \"year\": \"2021\",\n        \"version\": \"1\",\n        \"description\": \"COTS dataset - COCO format\",\n        \"contributor\": \"\",\n        \"url\": \"https:\/\/kaggle.com\",\n        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n    }\n    annotations_json[\"info\"].append(info)\n    \n    lic = {\n            \"id\": 1,\n            \"url\": \"\",\n            \"name\": \"Unknown\"\n        }\n    annotations_json[\"licenses\"].append(lic)\n\n    classes = {\"id\": 0, \"name\": \"starfish\", \"supercategory\": \"none\"}\n\n    annotations_json[\"categories\"].append(classes)\n\n    \n    for ann_row in df.itertuples():\n            \n        images = {\n            \"id\": ann_row[0],\n            \"license\": 1,\n            \"file_name\": ann_row.image_id + '.jpg',\n            \"height\": ann_row.height,\n            \"width\": ann_row.width,\n            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n        }\n        \n        annotations_json[\"images\"].append(images)\n        \n        bbox_list = ann_row.bboxes\n        \n        for bbox in bbox_list:\n            b_width = bbox[2]\n            b_height = bbox[3]\n            \n            # some boxes in COTS are outside the image height and width\n            if (bbox[0] + bbox[2] > 1280):\n                b_width = bbox[0] - 1280 \n            if (bbox[1] + bbox[3] > 720):\n                b_height = bbox[1] - 720 \n                \n            image_annotations = {\n                \"id\": annotion_id,\n                \"image_id\": ann_row[0],\n                \"category_id\": 0,\n                \"bbox\": [bbox[0], bbox[1], b_width, b_height],\n                \"area\": bbox[2] * bbox[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            }\n            \n            annotion_id += 1\n            annotations_json[\"annotations\"].append(image_annotations)\n        \n        \n    print(f\"Dataset COTS annotation to COCO json format completed! Files: {len(df)}\")\n    return annotations_json","0ad95f44":"# Convert COTS dataset to JSON COCO\ntrain_annot_json = dataset2coco(df_train[df_train.fold != SELECTED_FOLD], f\"{HOME_DIR}{DATASET_PATH}\/train2017\/\")\nval_annot_json = dataset2coco(df_train[df_train.fold == SELECTED_FOLD], f\"{HOME_DIR}{DATASET_PATH}\/val2017\/\")\n\n# Save converted annotations\nsave_annot_json(train_annot_json, f\"{HOME_DIR}{DATASET_PATH}\/annotations\/train.json\")\nsave_annot_json(val_annot_json, f\"{HOME_DIR}{DATASET_PATH}\/annotations\/valid.json\")","22dd48f4":"# Choose model for your experiments NANO or YOLOX-S (you can adapt for other model type)\n\nNANO = False","425b2f46":"config_file_template = '''\n\n#!\/usr\/bin\/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.depth = 0.33\n        self.width = 0.50\n        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n        \n        # Define yourself dataset path\n        self.data_dir = \"\/kaggle\/working\/dataset\/images\"\n        self.train_ann = \"train.json\"\n        self.val_ann = \"valid.json\"\n\n        self.num_classes = 1\n\n        self.max_epoch = $max_epoch\n        self.data_num_workers = 2\n        self.eval_interval = 1\n        \n        self.mosaic_prob = 1.0\n        self.mixup_prob = 1.0\n        self.hsv_prob = 1.0\n        self.flip_prob = 0.5\n        self.no_aug_epochs = 2\n        \n        self.input_size = (960, 960)\n        self.mosaic_scale = (0.5, 1.5)\n        self.random_size = (10, 20)\n        self.test_size = (960, 960)\n'''","f9c173da":"if NANO:\n    config_file_template = '''\n\n#!\/usr\/bin\/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nimport torch.nn as nn\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.depth = 0.33\n        self.width = 0.25\n        self.input_size = (416, 416)\n        self.mosaic_scale = (0.5, 1.5)\n        self.random_size = (10, 20)\n        self.test_size = (416, 416)\n        self.exp_name = os.path.split(\n            os.path.realpath(__file__))[1].split(\".\")[0]\n        self.enable_mixup = False\n\n        # Define yourself dataset path\n        self.data_dir = \"\/kaggle\/working\/dataset\/images\"\n        self.train_ann = \"train.json\"\n        self.val_ann = \"valid.json\"\n\n        self.num_classes = 1\n\n        self.max_epoch = $max_epoch\n        self.data_num_workers = 2\n        self.eval_interval = 1\n\n    def get_model(self, sublinear=False):\n        def init_yolo(M):\n            for m in M.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-3\n                    m.momentum = 0.03\n\n        if \"model\" not in self.__dict__:\n            from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n            in_channels = [256, 512, 1024]\n            # NANO model use depthwise = True, which is main difference.\n            backbone = YOLOPAFPN(self.depth,\n                                 self.width,\n                                 in_channels=in_channels,\n                                 depthwise=True)\n            head = YOLOXHead(self.num_classes,\n                             self.width,\n                             in_channels=in_channels,\n                             depthwise=True)\n            self.model = YOLOX(backbone, head)\n\n        self.model.apply(init_yolo)\n        self.model.head.initialize_biases(1e-2)\n        return self.model\n\n'''","95af85be":"PIPELINE_CONFIG_PATH='cots_config.py'\n\npipeline = Template(config_file_template).substitute(max_epoch = 5)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","647da1ab":"# .\/yolox\/data\/datasets\/voc_classes.py\n\nvoc_cls = '''\nVOC_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('.\/yolox\/data\/datasets\/voc_classes.py', 'w') as f:\n    f.write(voc_cls)\n\n# .\/yolox\/data\/datasets\/coco_classes.py\n\ncoco_cls = '''\nCOCO_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('.\/yolox\/data\/datasets\/coco_classes.py', 'w') as f:\n    f.write(coco_cls)\n\n# check if everything is ok    \n!more .\/yolox\/data\/datasets\/coco_classes.py","a01c1cd2":"sh = 'wget https:\/\/github.com\/Megvii-BaseDetection\/storage\/releases\/download\/0.0.1\/yolox_s.pth'\nMODEL_FILE = 'yolox_s.pth'\n\nif NANO:\n    sh = '''\n    wget https:\/\/github.com\/Megvii-BaseDetection\/storage\/releases\/download\/0.0.1\/yolox_nano.pth\n    '''\n    MODEL_FILE = 'yolox_nano.pth'\n\nwith open('script.sh', 'w') as file:\n  file.write(sh)\n\n!bash script.sh","32bdb785":"!cp .\/tools\/train.py .\/","43a278d5":"!python train.py \\\n    -f cots_config.py \\\n    -d 1 \\\n    -b 32 \\\n    --fp16 \\\n    -o \\\n    -c {MODEL_FILE}   # Remember to chenge this line if you take different model eg. yolo_nano.pth, yolox_s.pth or yolox_m.pth","5a8304bd":"# I have to fix demo.py file because it:\n# - raises error in Kaggle (cvWaitKey does not work) \n# - saves result files in time named directory eg. \/2021_11_29_22_51_08\/ which is difficult then to automatically show results\n\n%cp ..\/..\/input\/yolox-kaggle-fix-for-demo-inference\/demo.py tools\/demo.py","85ba1ae2":"TEST_IMAGE_PATH = \"\/kaggle\/working\/dataset\/images\/val2017\/0-4614.jpg\"\nMODEL_PATH = \".\/YOLOX_outputs\/cots_config\/best_ckpt.pth\"\n\n!python tools\/demo.py image \\\n    -f cots_config.py \\\n    -c {MODEL_PATH} \\\n    --path {TEST_IMAGE_PATH} \\\n    --conf 0.1 \\\n    --nms 0.45 \\\n    --tsize 960 \\\n    --save_result \\\n    --device gpu","a9e3b579":"OUTPUT_IMAGE_PATH = \".\/YOLOX_outputs\/cots_config\/vis_res\/0-4614.jpg\" \nImage.open(OUTPUT_IMAGE_PATH)","8d9be062":"from yolox.utils import postprocess\nfrom yolox.data.data_augment import ValTransform\n\nCOCO_CLASSES = (\n  \"starfish\",\n)\n\n# get YOLOX experiment\ncurrent_exp = importlib.import_module('cots_config')\nexp = current_exp.Exp()\n\n# set inference parameters\ntest_size = (960, 960)\nnum_classes = 1\nconfthre = 0.1\nnmsthre = 0.45\n\n\n# get YOLOX model\nmodel = exp.get_model()\nmodel.cuda()\nmodel.eval()\n\n# get custom trained checkpoint\nckpt_file = \".\/YOLOX_outputs\/cots_config\/best_ckpt.pth\"\nckpt = torch.load(ckpt_file, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"])","76ca059f":"def yolox_inference(img, model, test_size): \n    bboxes = []\n    bbclasses = []\n    scores = []\n    \n    preproc = ValTransform(legacy = False)\n\n    tensor_img, _ = preproc(img, None, test_size)\n    tensor_img = torch.from_numpy(tensor_img).unsqueeze(0)\n    tensor_img = tensor_img.float()\n    tensor_img = tensor_img.cuda()\n\n    with torch.no_grad():\n        outputs = model(tensor_img)\n        outputs = postprocess(\n                    outputs, num_classes, confthre,\n                    nmsthre, class_agnostic=True\n                )\n\n    if outputs[0] is None:\n        return [], [], []\n    \n    outputs = outputs[0].cpu()\n    bboxes = outputs[:, 0:4]\n\n    bboxes \/= min(test_size[0] \/ img.shape[0], test_size[1] \/ img.shape[1])\n    bbclasses = outputs[:, 6]\n    scores = outputs[:, 4] * outputs[:, 5]\n    \n    return bboxes, bbclasses, scores","fd234c51":"def draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, classes_dict):\n    for i in range(len(bboxes)):\n            box = bboxes[i]\n            cls_id = int(bbclasses[i])\n            score = scores[i]\n            if score < confthre:\n                continue\n            x0 = int(box[0])\n            y0 = int(box[1])\n            x1 = int(box[2])\n            y1 = int(box[3])\n\n            cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n            cv2.putText(img, '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), (x0, y0 - 3), cv2.FONT_HERSHEY_PLAIN, 0.8, (0,255,0), thickness = 1)\n    return img","1ec6e303":"TEST_IMAGE_PATH = \"\/kaggle\/working\/dataset\/images\/val2017\/0-4614.jpg\"\nimg = cv2.imread(TEST_IMAGE_PATH)\n\n# Get predictions\nbboxes, bbclasses, scores = yolox_inference(img, model, test_size)\n\n# Draw predictions\nout_image = draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, COCO_CLASSES)\n\n# Since we load image using OpenCV we have to convert it \nout_image = cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB)\ndisplay(Image.fromarray(out_image))","63b51264":"import greatbarrierreef\n\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()  ","875bccc9":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n \n    bboxes, bbclasses, scores = yolox_inference(image_np, model, test_size)\n    \n    predictions = []\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        cls_id = int(bbclasses[i])\n        score = scores[i]\n        if score < confthre:\n            continue\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","5113ed17":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","ef1b5af0":"List of pretrained models:\n* YOLOX-s\n* YOLOX-m\n* YOLOX-nano for inference speed (!)\n* etc.","8407354e":"### 6B.2 INFERENCE BBOXES","727c1d27":"# 6. RUN INFERENCE\n\n## 6A. INFERENCE USING YOLOX TOOL","bba63c10":"# 4. DOWNLOAD PRETRAINED WEIGHTS","dd471291":"## 6B. INFERENCE USING CUSTOM SCRIPT (IT WOULD BE USED FOR COTS INFERENCE PART)\n\n### 6B.1 SETUP MODEL","b0e8ceab":"# 7. SUBMIT TO COTS COMPETITION AND EVALUATE","2d836309":"<div class=\"alert alert-success\" role=\"alert\">\n\u3053\u306e\u4f5c\u696d\u306f2\u3064\u306e\u90e8\u5206\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002  \n    <ul>\n        <li> PART 1 - \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30ab\u30b9\u30bf\u30e0\u30e2\u30c7\u30eb\uff08COTS\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u7528\uff09-> COTS\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u7528\u306eYoloX\u30d5\u30eb\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3->\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af<\/li>\n        <li> PART 2 - COTS\u7528\u306eKaggle\u306eYOLOX\u304c\u5229\u7528\u53ef\u80fd\u3067\u3059-> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots\">COTS\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u884c\u308f\u308c\u305fYOLOX\u691c\u51fa\u306e\u63d0\u51fa (PART 2 - \u691c\u51fa)<\/a><\/li>\n    <\/ul>\n    \n<\/div>","5ff18be9":"# 1. INSTALL YOLOX","7e44f459":"**\u753b\u50cf\u66f8\u304d\u8fbc\u307f**\n* `\/kaggle\/input`\u306b\u306f**YOLOv5**\u306b\u5fc5\u8981\u306a\u66f8\u304d\u8fbc\u307f\u30a2\u30af\u30bb\u30b9\u6a29\u304c\u306a\u3044\u305f\u3081\u3001\u30a4\u30e1\u30fc\u30b8\u3092\u73fe\u5728\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(`\/kaggle\/working`)\u306b\u30b3\u30d4\u30fc\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n* **\u4e26\u5217\u8a08\u7b97**\u3092\u4f7f\u7528\u3059\u308b**Joblib**\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u9ad8\u901f\u5316\u3067\u304d\u307e\u3059\u3002","4b1d9cee":"<div class=\"alert alert-success\" role=\"alert\">\n    Find this notebook helpful? :) Please give me a vote ;) Thank you\n <\/div>","496be26a":"## 3A. YOLOX-S EXPERIMENT CONFIGURATION FILE\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u3001\u5b9f\u9a13\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3067\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002 YOLOX-s\u3068nano\u306e\u30ab\u30b9\u30bf\u30e0\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 \u5143\u306egithub\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u72ec\u81ea\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002","c1dba957":"# 5. TRAIN MODEL","b27979ad":"# Train YOLOX on COTS dataset (PART 1 - TRAINING)\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u3001Kaggle\u3067\u30ab\u30b9\u30bf\u30e0\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u691c\u51fa\u30e2\u30c7\u30eb\uff08COTS(\u30d2\u30c8\u30c7)\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff09\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002 YOLOX\u691c\u51fa\u5668\u306b\u57fa\u3065\u3044\u3066\u72ec\u81ea\u306e\u30ab\u30b9\u30bf\u30e0\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306e\u826f\u3044\u51fa\u767a\u70b9\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n\nYOLO\uff08You Look Only Onse \u201cYou Only Live Once\u201c\u3092\u3082\u3058\u3063\u305f\u3082\u306e\uff09\u306e\u65e5\u672c\u8a9e\u8aac\u660e\u306f[\u3053\u3053](https:\/\/www.renom.jp\/ja\/notebooks\/tutorial\/image_processing\/yolo\/notebook.html)\u304c\u826f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u3053\u3067\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u5b8c\u5168\u306agithub\u30ea\u30dd\u30b8\u30c8\u30ea - [YOLOX](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX)\n\n<div align = 'center'><img src='https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/raw\/main\/assets\/logo.png'\/><\/div>\n\n**\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u30ab\u30d0\u30fc\u3055\u308c\u3066\u3044\u308b\u624b\u9806\uff1a**\n* Install YOLOX \n* Prepare COTS dataset for YOLOX object detection training\n* Download Pre-Trained Weights for YOLOX\n* Prepare configuration files\n* YOLOX training\n* Run YOLOX inference on test images\n* Export YOLOX weights for Tensorflow inference (soon)\n\n\u4eca\u3001\u79c1\u306fYOLOX\u3067\u5b66\u7fd2\u3068\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\u306e\u305f\u3081\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 \u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u3001\u3088\u308a\u826f\u3044\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3067\u3059\uff08YOLOX\u5b9f\u9a13\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u904a\u3093\u3067\u304f\u3060\u3055\u3044\uff09\u3002","be4932ca":"### 6B.4 ALL PUZZLES TOGETHER","075e2b37":"**\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8**\n* \u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b **python time-series API**\u3092\u4f7f\u7528\u3057\u3066\u4e88\u6e2c\u3092\u9001\u4fe1\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u3053\u306e\u30b3\u30f3\u30c6\u30b9\u30c8\u306f\u4ee5\u524d\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u691c\u51fa\u30b3\u30f3\u30c6\u30b9\u30c8\u3068\u306f\u7570\u306a\u308a\u307e\u3059\u3002\n* \u5404\u4e88\u6e2c\u884c\u306b\u306f\u3001\u753b\u50cf\u306e\u3059\u3079\u3066\u306e\u5883\u754c\u30dc\u30c3\u30af\u30b9\u3092\u542b\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \u63d0\u51fa\u306f\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3082**COCO**\u306e\u3088\u3046\u3067\u3059\u3002\u3053\u308c\u306f `[x_min, y_min, width, height]`\u3000\u3067\u3059\u3002\n* \u30b3\u30f3\u30da\u30e1\u30c8\u30ea\u30c3\u30af `F2`\u306f\u3001\u30d2\u30c8\u30c7\u306e\u898b\u9003\u3057\u3092\u6700\u5c0f\u9650\u306b\u6291\u3048\u308b\u305f\u3081\u306b\u3001\u3044\u304f\u3064\u304b\u306e\u8aa4\u691c\u77e5\uff08FP\uff09\u3092\u8a31\u5bb9\u3057\u307e\u3059\u3002 \u3064\u307e\u308a\u3001**\u507d\u9670\u6027\uff08FN\uff09**\u306b\u53d6\u308a\u7d44\u3080\u3053\u3068\u306f\u3001\u507d\u967d\u6027\uff08FP\uff09\u3088\u308a\u3082\u91cd\u8981\u3067\u3059\u3002\n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","6cf90152":"# 2. PREPARE COTS DATASET FOR YOLOX\n\u3053\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u306f\u3001Awsaf\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u305f\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u304b\u3089\u629c\u7c8b\u3057\u305f\u3082\u306e\u3067\u3059\u3002 [Great-Barrier-Reef: YOLOv5 train](https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-train)\n\n## A. PREPARE DATASET AND ANNOTATIONS\n* \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001 **bboxed-images** (`~5k`)\u306e\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u96fb\u8eca\u306b\u306f\u7d0423K\u306e\u753b\u50cf\u3092\u3059\u3079\u3066\u4f7f\u7528\u3067\u304d\u307e\u3059\u304c\u3001\u307b\u3068\u3093\u3069\u306e\u753b\u50cf\u306b\u306f\u30e9\u30d9\u30eb\u304c\u3042\u308a\u307e\u305b\u3093\u3002 \u3057\u305f\u304c\u3063\u3066\u3001**bboxed images**\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066\u5b9f\u9a13\u3092\u5b9f\u884c\u3059\u308b\u65b9\u304c\u7c21\u5358\u3067\u3059\u3002","6244a112":"<div class=\"alert alert-warning\">\n<strong> \u30e2\u30c7\u30eb\u30925EPOCHS\u306e\u307f\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3057\u305f....\u3053\u308c\u306f\u30c7\u30e2\u76ee\u7684\u306e\u307f\u3067\u3059\u3002 <\/strong> \n<\/div>","01a0b1d3":"## B. CREATE COCO ANNOTATION FILES\n\n**\u30e9\u30d9\u30eb\u4f5c\u6210**\n\n\u30e9\u30d9\u30eb\u3092**YOLO**\u5f62\u5f0f\u306b\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u753b\u50cf\u6bce\u306b1\u3064\u306e`*.txt`\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u307e\u3059\uff08\u753b\u50cf\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304c\u306a\u3044\u5834\u5408\u306f`*.txt`\u30d5\u30a1\u30a4\u30eb\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\uff09\u3002\u4ed5\u69d8\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\u3002\n\n* \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3054\u3068\u306b1\u884c\n* \u5404\u884c\u306f`[x_center, y_center, width, height]`\u5f62\u5f0f\n* Box\u306f**\u6b63\u898f\u5316**\u3055\u308c\u305f`xywh`\u5f62\u5f0f (`0 - 1`)\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 Boxes\u304c\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u306e\u5834\u5408\u306f\u3001`x_center`\u3068`width`\u3092`image width`\u3067\u5272\u308a\u3001`y_center`\u3068`height`\u3092`image height`\u3067\u5272\u308a\u307e\u3059\u3002\n* \u30af\u30e9\u30b9\u756a\u53f7\u306f**zero-indexed**\u3067\u3059\uff080\u304b\u3089\u59cb\u307e\u308a\u307e\u3059\uff09\u3002\n\n> \u30b3\u30f3\u30da\u306ebbox\u306f**COCO**\u5f62\u5f0f\u3067\u3042\u308b\u305f\u3081`[x_min, y_min, width, height]`\u3067\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001**COCO**\u5f62\u5f0f\u3092**YOLO** \u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n","0ccdbfa1":"Help Protect the Great Barrier Reef\u306e[\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30ce\u30fc\u30c8](https:\/\/www.kaggle.com\/remekkinas\/yolox-training-pipeline-cots-dataset-lb-0-507)\u3092\u65e5\u672c\u8a9e\u3067\u89e3\u8aac\u3057\u307e\u3059\u3002\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u691c\u51fa\u306f\u8208\u5473\u6df1\u3044\u30c8\u30d4\u30c3\u30af\u3067\u3042\u308a\u3001\u6700\u5148\u7aef\u306e\u6280\u8853\u3092\u4f7f\u3063\u305f[\u3053\u306e\u30ce\u30fc\u30c8](https:\/\/www.kaggle.com\/remekkinas\/yolox-training-pipeline-cots-dataset-lb-0-507)\u3082\u3068\u3066\u3082\u8208\u5473\u6df1\u3044\u306e\u3067\u3059\u304c\u3001\u521d\u5b66\u8005\u306b\u306f\u7406\u89e3\u306e\u96e3\u3057\u3044\u3082\u306e\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u80cc\u666f\u7b49\u306e\u8aac\u660e\u3082\u52a0\u3048\u3066\u3001\u3053\u308c\u3092\u8aad\u3080\u3060\u3051\u3067\u4e00\u5b9a\u306e\u7406\u89e3\u306b\u9054\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u307e\u3059\u3002\n\n\u53c2\u8003\u306b\u306a\u3063\u305f\u3089\u6295\u7968\u304a\u9858\u3044\u3057\u307e\u3059\uff01","15b7fb1f":"# 3. PREPARE CONFIGURATION FILE\n\nYoloxno\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb:\n- [YOLOX-nano](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/blob\/main\/exps\/default\/nano.py)\n- [YOLOX-s](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/blob\/main\/exps\/default\/yolox_s.py)\n- [YOLOX-m](https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/blob\/main\/exps\/default\/yolox_m.py)\n\n\u4ee5\u4e0b\u306b\u3001COTS\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u7528\u306e2\u3064\u306e\uff08yolox-s\u3068yolox-nano\uff09\u69cb\u6210\u30d5\u30a1\u30a4\u30eb\u3092\u793a\u3057\u307e\u3059\u3002\n\n<div align=\"center\"><img  width=\"800\" src=\"https:\/\/github.com\/Megvii-BaseDetection\/YOLOX\/raw\/main\/assets\/git_fig.png\"\/><\/div>","6956e096":"### 6B.3 DRAW RESULT","b3edb43d":"<div class=\"alert alert-warning\">\n<strong> For YOLOX_s I use input size 960x960 but you can change it for your experiments.<\/strong> \n<\/div>","29e5d66f":"## 3B. YOLOX-NANO CONFIG FILE\n<div class=\"alert alert-warning\">\n<strong> For YOLOX_nano I use input size 460x460 but you can change it for your experiments.<\/strong> \n<\/div"}}