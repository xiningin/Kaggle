{"cell_type":{"2acdf5b8":"code","f3806d50":"code","6ee00e94":"code","bb9a5895":"code","d4ede2ec":"code","480cbc6f":"code","82fada50":"code","f2dc4996":"code","aa424047":"code","25863625":"code","a17b1037":"code","700169f1":"code","34c621c0":"code","0a341437":"code","b7af6c27":"code","78c97747":"code","df9822f5":"code","8e0c25d7":"markdown","95bbf4b3":"markdown","fce9f5c4":"markdown","af437442":"markdown","f008727b":"markdown","2d15fff7":"markdown","b2fc879d":"markdown","c76dec47":"markdown","abec907e":"markdown","cb001540":"markdown","e018e748":"markdown","82a7b9a9":"markdown","41905ab4":"markdown","291b9f18":"markdown","a442c22b":"markdown","b6fb2907":"markdown","58cacb4f":"markdown","329dcf31":"markdown","e9bf9e75":"markdown","c0dc2343":"markdown","30319574":"markdown","7649bd95":"markdown","7ad2b2ed":"markdown"},"source":{"2acdf5b8":"import numpy as np\r\nimport pandas as pd\r\nimport plotly.express as px\r\nimport plotly.graph_objects as go\r\nimport plotly.figure_factory as ff\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.model_selection import train_test_split","f3806d50":"mean_01=np.array([-0.2,-0.2])\r\ncov_01=np.array([[0.05,-0.035],[-0.035,0.05]])\r\nmean_02=np.array([0.2,0.38])\r\ncov_02=np.array([[0.05,0],[0,0.005]])\r\nnp.random.seed(42)\r\ndata_01=np.random.multivariate_normal(mean_01,cov_01,200, check_valid= \"warn\")\r\ndata_02=np.random.multivariate_normal(mean_02,cov_02,200, check_valid= \"warn\")\r\ndata = np.vstack((data_01,data_02))\r\ndf_train = pd.DataFrame(data, columns = [\"Feature_1\", \"Feature_2\"])\r\ndf_train[\"class\"] = [-1]*200 + [1]*200 ","6ee00e94":"## For Contributing, refer to expectedoutput1.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Feature_1\")\r\nplt.ylabel(\"Feature_2\")\r\nplt.scatter(df_train[\"Feature_1\"][:200],df_train[\"Feature_2\"][:200], label=\"Class A\", edgecolor = \"b\", s = 60, alpha = 0.8)\r\nplt.scatter(df_train[\"Feature_1\"][200:],df_train[\"Feature_2\"][200:], label=\"Class B\", marker=\"^\", edgecolor = \"r\", s =60, alpha = 0.8)\r\nplt.legend()\r\nplt.show()","bb9a5895":"X = df_train[[\"Feature_1\",\"Feature_2\"]].to_numpy()\r\nY = df_train[[\"class\"]].to_numpy()","d4ede2ec":"class SVM:\r\n    def __init__(self,C=1.0):\r\n        self.C=C\r\n        self.W=0\r\n        self.b=0\r\n        \r\n    def hinge_loss(self,W,b,X,Y):\r\n        temp=0\r\n        for i in range(X.shape[0]):\r\n            ti=Y[i]*(np.dot(W,X[i]) + b)\r\n            temp+=max(0,1-ti)\r\n        loss=0.5*(np.dot(W.T,W)) + self.C * temp\r\n        return(loss[0][0])\r\n    \r\n    def fit(self,X,Y,batch_size=100,learning_rate=0.001):\r\n        n_features,n_examples,n,c=X.shape[1],X.shape[0],learning_rate,self.C\r\n        print(n_examples)\r\n        W=np.zeros((1,n_features))\r\n        b=0\r\n        loss_list=[]\r\n        for i in range(300):\r\n            loss_list.append(self.hinge_loss(W,b,X,Y))\r\n            indices=np.arange(n_examples)\r\n            np.random.shuffle(indices)\r\n            indices=indices[:batch_size]\r\n            gradW,gradb=0,0\r\n            for i in indices:\r\n                ti=Y[i]*(np.dot(W,X[i]) + b)\r\n                if ti>=1:\r\n                    gradW +=0 \r\n                    gradb +=0\r\n                else:\r\n                    gradW += c*(Y[i]*X[i])\r\n                    gradb += c*Y[i]\r\n            W = W - n*W + n*gradW\r\n            b = b + n*gradb\r\n            self.W=W\r\n            self.b=b\r\n        return(W,b,loss_list)","480cbc6f":"sv=SVM(0.5)\r\nW,b,loss_list=sv.fit(X, Y)\r\nW=W.reshape((-1,))","82fada50":"## For Contributing, refer to expectedoutput2.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Iteration\")\r\nplt.ylabel(\"Loss\")\r\nplt.plot(loss_list,label=\"Low Penalty\",c='g')\r\nplt.legend()\r\nplt.show()\r\n","f2dc4996":"## For Contributing, refer to expectedoutput3.html in the expected outputs folder.\r\n","aa424047":"## For Contributing, refer to expectedoutput4.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Feature_1\")\r\nplt.ylabel(\"Feature_2\")\r\nplt.scatter(df_train[\"Feature_1\"][:200],df_train[\"Feature_2\"][:200], label=\"Class A\", edgecolor = \"b\", s = 60, alpha = 0.8)\r\nplt.scatter(df_train[\"Feature_1\"][200:],df_train[\"Feature_2\"][200:], label=\"Class B\", marker=\"^\", edgecolor = \"r\", s =60, alpha = 0.8)\r\nxt=np.linspace(-1,1,10)\r\ny= -1 * (b+W[0]*xt)\/W[1]\r\ny1= (1-b-W[0]*xt)\/W[1]\r\ny2= (-1-b-W[0]*xt)\/W[1]\r\nplt.plot(xt,y, c = 'black', label=\"Separating Hyperplane\")\r\nplt.plot(xt,y2,c='r',linestyle=\"--\",label=\"Negative Hyperplane\")\r\nplt.plot(xt,y1,c='g',linestyle=\"--\",label=\"Positive Hyperplane\")\r\nplt.legend()\r\nplt.show()","25863625":"sv=SVM(10)\r\nW,b,loss_list=sv.fit(X, Y)\r\nW=W.reshape((-1,))","a17b1037":"## For Contributing, refer to expectedoutput5.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Iteration\")\r\nplt.ylabel(\"Loss\")\r\nplt.plot(loss_list,label=\"Mid Penalty\",c='y')\r\nplt.legend()\r\nplt.show()","700169f1":"## For Contributing, refer to expectedoutput6.html in the expected outputs folder.\r\n","34c621c0":"## For Contributing, refer to expectedoutput7.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Feature_1\")\r\nplt.ylabel(\"Feature_2\")\r\nplt.scatter(df_train[\"Feature_1\"][:200],df_train[\"Feature_2\"][:200], label=\"Class A\", edgecolor = \"b\", s = 60, alpha = 0.8)\r\nplt.scatter(df_train[\"Feature_1\"][200:],df_train[\"Feature_2\"][200:], label=\"Class B\", marker=\"^\", edgecolor = \"r\", s =60, alpha = 0.8)\r\nxt=np.linspace(-1,1,10)\r\ny= -1 * (b+W[0]*xt)\/W[1]\r\ny1= (1-b-W[0]*xt)\/W[1]\r\ny2= (-1-b-W[0]*xt)\/W[1]\r\nplt.plot(xt,y, c = 'black', label=\"Separating Hyperplane\")\r\nplt.plot(xt,y2,c='r',linestyle=\"--\",label=\"Negative Hyperplane\")\r\nplt.plot(xt,y1,c='g',linestyle=\"--\",label=\"Positive Hyperplane\")\r\nplt.legend()\r\nplt.show()","0a341437":"sv=SVM(1000)\r\nW,b,loss_list=sv.fit(X, Y)\r\nW=W.reshape((-1,))","b7af6c27":"## For Contributing, refer to expectedoutput8.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Iteration\")\r\nplt.ylabel(\"Loss\")\r\nplt.plot(loss_list,label=\"High Penalty\",c='r')\r\nplt.legend()\r\nplt.show()","78c97747":"## For Contributing, refer to expectedoutput9.html in the expected outputs folder.\r\n","df9822f5":"## For Contributing, refer to expectedoutput10.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Feature_1\")\r\nplt.ylabel(\"Feature_2\")\r\nplt.scatter(df_train[\"Feature_1\"][:200],df_train[\"Feature_2\"][:200], label=\"Class A\", edgecolor = \"b\", s = 60, alpha = 0.8)\r\nplt.scatter(df_train[\"Feature_1\"][200:],df_train[\"Feature_2\"][200:], label=\"Class B\", marker=\"^\", edgecolor = \"r\", s =60, alpha = 0.8)\r\nxt=np.linspace(-1,1,10)\r\ny= -1 * (b+W[0]*xt)\/W[1]\r\ny1= (1-b-W[0]*xt)\/W[1]\r\ny2= (-1-b-W[0]*xt)\/W[1]\r\nplt.plot(xt,y, c = 'black', label=\"Separating Hyperplane\")\r\nplt.plot(xt,y2,c='r',linestyle=\"--\",label=\"Negative Hyperplane\")\r\nplt.plot(xt,y1,c='g',linestyle=\"--\",label=\"Positive Hyperplane\")\r\nplt.legend()\r\nplt.show()","8e0c25d7":"# Machine Learning Model","95bbf4b3":"## High penelty SVM","fce9f5c4":"### Training the model","af437442":"For demonstration purposes, let us take a 2 dimensional dataset with tow features (Feature_1 and Feature_2) and consisting of two classes (Class A and Class B) having a distribution specifications as follows:\r\n\r\n**Class A:** The Class A is centred around the mean of (-0.2,-0.2) and has the covariance matrix [[0.05,-0.035],[-0.035,0.05]]\r\n\r\n**Class B:** The Class B is centred around the mean of (0.2,0.38) and has the covariance matrix [[0.05,0],[0,0.005]]\r\n\r\nDefintions: \r\n\r\n**Mean:** A Class with centre (x1, x2) as mean denotes that the average value along \"Feature_1\" is x1 and the average value along \"Feature_2\" is x2\r\n\r\nP.S: Since we would like to ensure that the outputs corrosponds to the desired output, we will also add the seed value of 42 while generating these distributions.\r\n\r\n","f008727b":"## Low penalty SVM","2d15fff7":"### Visualising the decision boundry over iterations","b2fc879d":"## Defining the model","c76dec47":"# Loading the dataset","abec907e":"### Training the model","cb001540":"### Visualising the decision boundry over iterations","e018e748":"### Visualising the error ","82a7b9a9":"### Plotting the decision boundry","41905ab4":"### Plotting the decision boundry","291b9f18":"## Mid penelty SVM","a442c22b":"## Preparing training and test sets","b6fb2907":"# Visualising the dataset","58cacb4f":"# Importing important libraries","329dcf31":"This notebook is a part of the OneML_ContriHub the link to which can be found [here](https:\/\/github.com\/ContriHUB\/OneML_ContriHub)","e9bf9e75":"### Visualising the error ","c0dc2343":"### Plotting the decision boundry","30319574":"### Visualising the error ","7649bd95":"### Visualising the decision boundry over iterations","7ad2b2ed":"### Training the model"}}