{"cell_type":{"b02d6977":"code","70585e98":"code","7c9605f3":"code","4cbab899":"code","28062aad":"code","9297b51d":"code","ef8c5808":"code","1eca6542":"code","d62f6f36":"code","21028fa5":"code","02f049ab":"code","6a38aca2":"code","96188072":"code","a5a569df":"code","f84ad106":"code","94299726":"code","aca3a887":"code","e3f7f910":"code","2e950c0e":"code","c99becc6":"code","5fb46677":"code","d89b6ceb":"code","e305251c":"code","677e4772":"code","e053a54d":"code","15f976e2":"code","a922226b":"code","900143e4":"code","d6d37566":"code","101a765d":"code","2a0eb8e3":"code","a3a4e92d":"code","7640b329":"code","0aaf3991":"code","58f85689":"code","989ebc02":"code","d65232d0":"code","4d0efbe3":"code","fed500bd":"code","3c36475c":"code","b781dd87":"code","0ad92e19":"code","02eee8ca":"code","cc8c78db":"code","dcd64bbc":"code","56228689":"code","39d7c271":"code","c69bff88":"code","56680f19":"code","278413ff":"code","28774736":"code","37b40d0b":"code","f13adce9":"code","ede611c0":"code","15006cf8":"code","ee121ff5":"code","1abc5af3":"code","e472b8aa":"code","30ef9b64":"code","1ace71e6":"code","4813ab3d":"code","f96d97c6":"code","8a822a6f":"code","7f72c5d3":"code","f2c006a8":"code","3df3c66a":"code","91904f3d":"code","6ffe2a9a":"markdown","71893685":"markdown","4b90bd60":"markdown","fb519809":"markdown","e339a2fb":"markdown","54ef85c1":"markdown","7dd75df0":"markdown","36bf4cdb":"markdown","e1a46248":"markdown","5a3affd0":"markdown","cf7b7f51":"markdown","9469e8b0":"markdown","30a9f77e":"markdown","f68b47b5":"markdown","80cb1818":"markdown","cd4494f8":"markdown","d0647a7b":"markdown","dbc49642":"markdown","b59cd998":"markdown","da790c54":"markdown","19bd81fd":"markdown"},"source":{"b02d6977":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ntrain = pd.read_csv(\"..\/input\/tatinn\/titanic_train1.csv\")\ntest = pd.read_csv(\"..\/input\/test-dataset-for-titanic-competition\/titanic_test.csv\")\n","70585e98":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")","7c9605f3":"train.isnull().sum()\nprint(\"Train Shape:\",train.shape)\ntest.isnull().sum()\nprint(\"Test Shape:\",test.shape)","4cbab899":"train.info()","28062aad":"test.info()","9297b51d":"train.head(10)","ef8c5808":"train.describe()","1eca6542":"test.describe()","d62f6f36":"train.isnull().sum()","21028fa5":"test.isnull().sum()\ntest[\"Survived\"] = \"\"\ntest.head()","02f049ab":"train['Survived'].value_counts()","6a38aca2":"import matplotlib.pyplot as plt # Plot the graphes\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots","96188072":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","a5a569df":"bar_chart('Sex')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Sex'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Sex'].value_counts())","f84ad106":"bar_chart('Pclass')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Pclass'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Pclass'].value_counts())","94299726":"bar_chart('SibSp')\nprint(\"Survived :\\n\",train[train['Survived']==1]['SibSp'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['SibSp'].value_counts())","aca3a887":"bar_chart('Parch')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Parch'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Parch'].value_counts())","e3f7f910":"bar_chart('Embarked')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Embarked'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Embarked'].value_counts())","2e950c0e":"train.head()","c99becc6":"Image(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/t\/5090b249e4b047ba54dfd258\/1351660113175\/TItanic-Survival-Infographic.jpg?format=1500w\")","5fb46677":"train.head(10)","d89b6ceb":"train_test_data = [train,test] # combine dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","e305251c":"train['Title'].value_counts()","677e4772":"test['Title'].value_counts()","e053a54d":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset[\"Title\"].map(title_mapping)","15f976e2":"dataset.head()","a922226b":"test.head()","900143e4":"bar_chart('Title')","d6d37566":"# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","101a765d":"train.head()","2a0eb8e3":"sex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","a3a4e92d":"bar_chart('Sex')","7640b329":"test.head()","0aaf3991":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace= True)\ntest[\"Age\"].fillna(test.groupby('Title')['Age'].transform(\"median\"), inplace= True)","58f85689":"train.head(30)\n#train.groupby(\"Title\")[\"Age\"].transform(\"median\")","989ebc02":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.show()\n\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.xlim(10,50)","d65232d0":"train.info()\ntest.info()","4d0efbe3":"for dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4\n# for dataset in train_test_data:\n#     dataset.loc[]\n#train[train['Age'].isin([23])]","fed500bd":"train.head()\nbar_chart('Age')","3c36475c":"Pclass1 = train[train['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass'] == 3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st Class','2nd Class','3rd Class']\ndf.plot(kind = 'bar', stacked =  True, figsize=(10,5))\nplt.show()\nprint(\"Pclass1:\\n\",Pclass1)\nprint(\"Pclass2:\\n\",Pclass2)\nprint(\"Pclass3:\\n\",Pclass3)","b781dd87":"for dataset in train_test_data:\n    dataset['Embarked'] =  dataset['Embarked'].fillna('S')","0ad92e19":"train.head()","02eee8ca":"embarked_mapping = {'S':0,'C':1,'Q':2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","cc8c78db":"# train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"])\n# train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n# test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n# train.head(50)\n\n\n# fill missing Fare with median fare for each Pclass\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain.head(50)","dcd64bbc":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4 )\nfacet.map(sns.kdeplot, 'Fare', shade = True)\nfacet.set(xlim = (0, train['Fare'].max()))\nfacet.add_legend()\nplt.show()","56228689":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","39d7c271":"for dataset in train_test_data:\n    dataset.loc[dataset['Fare'] <= 17, 'Fare'] = 0,\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1,\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2,\n    dataset.loc[dataset['Fare'] >= 100, 'Fare'] = 3","c69bff88":"train.head()","56680f19":"train.Cabin.value_counts()","278413ff":"for dataset in train_test_data:\n    dataset['Cabin'] =  dataset['Cabin'].str[:1]","28774736":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","37b40d0b":"cabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","f13adce9":"# fill missing Fare with median fare for each Pclass\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","ede611c0":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n","15006cf8":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","ee121ff5":"family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","1abc5af3":"train.head()","e472b8aa":"features_drop = ['Ticket','SibSp','Parch']\ntrain = train.drop(features_drop, axis = 1)\ntest = test.drop(features_drop,axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","30ef9b64":"train_data = train.drop('Survived', axis = 1)\ntarget = train['Survived']\ntrain_data.shape, target.shape","1ace71e6":"train_data.head(10)","4813ab3d":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport numpy as np","f96d97c6":"train.info()","8a822a6f":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","7f72c5d3":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","f2c006a8":"#learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nclf = [KNeighborsClassifier(n_neighbors = 13),DecisionTreeClassifier(),\n       RandomForestClassifier(n_estimators=13),GaussianNB(),SVC(),ExtraTreeClassifier(),\n      GradientBoostingClassifier(n_estimators=10, learning_rate=1,max_features=3, max_depth =3, random_state = 10),AdaBoostClassifier(),ExtraTreesClassifier()]\ndef model_fit():\n    scoring = 'accuracy'\n    for i in range(len(clf)):\n        score = cross_val_score(clf[i], train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\n        print(\"Score of Model\",i,\":\",round(np.mean(score)*100,2))\n#     round(np.mean(score)*100,2)\n#     print(\"Score of :\\n\",score)\nmodel_fit()","3df3c66a":"clf1 = SVC()\nclf1.fit(train_data, target)\ntest\ntest_data = test.drop(columns = ['Survived','PassengerId'])\nprediction = clf1.predict(test_data)\n# test_data\n","91904f3d":"test_data['Survived'] = prediction\nsubmission = pd.DataFrame(test['PassengerId'],test_data['Survived'])\nsubmission.to_csv(\"Submission.csv\",index=False)","6ffe2a9a":"### Read files","71893685":"# 5. Modelling","4b90bd60":"This notebook is a companion to the book **Titanic Passanger Survival Analysis**.\n\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.","fb519809":"## family Size","e339a2fb":"Feature engineering is the process of using domain knowledge of the data to create features (**feature vectors**) that make machine learning algorithms work.\n\nfeature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis.","54ef85c1":"The Chart confirms **1st class** more likely survivied than other classes.\n\nThe Chart confirms **3rd class more** likely dead than other classes","7dd75df0":"#### 4.1 how titanic sank?","36bf4cdb":"Those who were **20 to 30 years old** were more **dead and more survived**.","e1a46248":"# Workflow stages\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\n\n* We may combine mulitple workflow stages. We may analyze by visualizing data.\n* Perform a stage earlier than indicated. We may analyze data before and after wrangling.\n* Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\n* Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.","5a3affd0":"The Chart confirms a **person aboarded with more than 2 siblings or spouse** more likely survived.\n \nThe Chart confirms a ** person aboarded without siblings or spouse** more likely dead","cf7b7f51":"The Chart confirms **Women more likely survivied than Men**.","9469e8b0":"# 4. Feature engineering","30a9f77e":"#### Title Map\nMr : 0\n\nMiss : 1\n\nMrs: 2\n\nOthers: 3","f68b47b5":"The Chart confirms a **person aboarded from C slightly more likely survived**.\n\nThe Chart confirms a **person aboarded from Q more likely dead.**\n\nThe Chart confirms a **person aboarded from S more likely dead.**","80cb1818":"The Chart confirms a person aboarded with more **than 2 parents or children more likely survived.**\n\nThe Chart confirms a person **aboarded alone more likely dead**","cd4494f8":"# Titanic Passanger Survival Analysis","d0647a7b":"more than 50 % of 1st class are from S embark.\n\nmore than 50 % of 2st class are from S embark.\n\nmore than 50 % of 3st class are from S embark.\n\n## fill out missing embark with S embark","dbc49642":"# Data Visualization using Matplotlib and Seaborn packages.","b59cd998":"# 6.Cross Validation(k-fold)","da790c54":"### Bar Chart for Categorical Features\n* Pclass\n* Sex\n* SibSp ( # of siblings and spouse)\n* Parch ( # of parents and children)\n* Embarked\n* Cabin","19bd81fd":"## Data Dictionary\nSurvived: 0 = No, 1 = Yes\n* pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n* sibsp: # of siblings \/ spouses aboard the Titanic\n* parch: # of parents \/ children aboard the Titanic\n* ticket: Ticket number\n* cabin: Cabin number\n* embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n## Total rows and columns\n\nWe can see that there are 891 rows and 12 columns in our training dataset."}}