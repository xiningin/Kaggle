{"cell_type":{"f57021e1":"code","a39e3ba4":"code","d2c0d927":"code","7c8c2fad":"code","2d049225":"code","d30b9f89":"code","c415a6bc":"code","30d8cca7":"code","64444f39":"code","6272d2ed":"code","f7a2e3e9":"code","280d803d":"code","ab43c863":"code","83fbe6ca":"code","39b1ec69":"code","cf7f04fa":"code","d42d35ef":"code","4188b846":"code","0cd6d530":"code","d1acf1ee":"code","590adf42":"code","2d61964b":"code","9a13c0c1":"code","c36f1e96":"code","5af56325":"code","f1868dae":"code","fa1cc0ab":"code","a255628c":"code","dac51dc0":"code","238db28a":"code","968bcc3f":"code","db7c69c1":"code","0318b3cf":"code","197380f0":"code","60863134":"code","130a4959":"code","83ab68d7":"code","2cfa34cf":"code","634abb60":"code","910524dc":"code","ba3ea102":"code","15a1d4ad":"code","92843e56":"code","291b362e":"code","535ef787":"code","185c30fc":"code","24c4ca42":"code","946eb08f":"markdown","90253a90":"markdown","af4f0f0a":"markdown","8a6531b4":"markdown","8fe6b72d":"markdown","8c04c4f9":"markdown","0a1fbe22":"markdown","33368d57":"markdown","1152e9af":"markdown","b7d5de5d":"markdown","ff7af63a":"markdown","a2c6543e":"markdown","d23ee3d5":"markdown","a2182c3a":"markdown","6f84edcc":"markdown","ce354887":"markdown","5f5e9a55":"markdown","bcd9668a":"markdown","75f8d8aa":"markdown","c96411a6":"markdown","5fef242b":"markdown","63d8f14e":"markdown","a69f7b92":"markdown","c3a32502":"markdown","3099f2dc":"markdown","e056ea42":"markdown","b0b768cc":"markdown","f95bbf44":"markdown","1b5ffa25":"markdown","c2664898":"markdown","0ceb8761":"markdown","de9ea497":"markdown","4d513f3c":"markdown","343fef14":"markdown","72d352fd":"markdown","de32ac24":"markdown","9e9b90e6":"markdown"},"source":{"f57021e1":"# importing all the essential packages \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport missingno\nimport pycountry\nimport operator\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom ggplot import *\n!pip install venndata\nfrom venndata import venn\nfrom plotly.subplots import make_subplots\n\nimport os\n# listing the content of the dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a39e3ba4":"df = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')\n\nprint(f'Length of the dataframe is {len(df)}')","d2c0d927":"df.head()","7c8c2fad":"# Checking the NaN s in the data\nmissingno.matrix(df, labels=True, figsize=(100, 10))\nplt.show()","2d049225":"'''\nClearly the first row of the data-frame corresponds to questions \nand thus should not be used for analysis. So let's take it out.\n'''\nquestions = df.iloc(0)[0]\ndf.drop(0, axis=0, inplace=True)","d30b9f89":"# Q1 corresponds to age lets visualize the age distribution of the data-scientists\nfig, ax = plt.subplots(figsize=(20, 10))\nsns.countplot(x='Q1', data=df, ax=ax,\n             order=np.unique(df['Q1'].values.tolist()),\n             color = 'black',\n             alpha=0.7)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+0.25, height+50, 'n=%.0f'%(height), fontsize=25, rotation=45)\nage_grps, counts = np.unique(df['Q1'].values.tolist(), return_counts=True)\nax.plot(age_grps, counts, linestyle='--', color='k', linewidth=5, alpha=0.5)\nax.set_title('Age of data-scientists and machine-learning engineers', fontsize=50)\nax.set_ylabel('Counts', fontsize = 30)\nax.set_xlabel('Age group', fontsize= 30)\nax.set_ylim([0, max(counts)+1000])\nplt.show()","c415a6bc":"# A function to properly round floating numbers\ndef proper_round(num, dec=0):\n    '''\n    num: floating number\n    dec: number of points after decimal (by default: 0)\n    '''\n    num = str(num)[:str(num).index('.')+dec+2]\n    if num[-1]>='5':\n        return float(num[:-2-(not dec)]+str(int(num[-2-(not dec)])+1))\n    return float(num[:-1])","30d8cca7":"plt.figure(figsize = (50,20))\nplt.subplot(2, 1, 1)\nax1 = sns.distplot([float(duration)\/60 for duration in\n                  df['Time from Start to Finish (seconds)'].values.tolist()\n                 if int(duration)<(180*60)],\n                bins= 180)\nax1.set_xlabel('Duration minutes distribution for 3 hr (mins)', fontsize=30)\nax1.set_title('Histogram of Duration for people who filled it within 3hr', fontsize=50)\n\nplt.subplot(2, 1, 2)\nax2 = sns.countplot([int(proper_round(float(duration)\/(60*60))) for duration in \n                     df['Time from Start to Finish (seconds)'].values.tolist()\n                    if int(proper_round(float(duration)\/(60*60))) > 3],\n                   color='black', alpha=0.7)\nax2.set_xlabel('Duration hr spread', fontsize=30)\nax2.set_ylabel('Count', fontsize=30)\nax2.set_title('Count distribution for Hr taken (> 3hr) to fill the survery', fontsize=50)\n\nmax_hr = max([int(proper_round(float(duration)\/(60*60))) for duration in \n                     df['Time from Start to Finish (seconds)'].values.tolist()])\nx_label = np.linspace(4, max_hr, num=max_hr-3)\nx_label= [f'{int(x)} hr' for x in x_label]\nax2.set_xticklabels(x_label, rotation=45, fontsize=10)\nplt.show()","64444f39":"plt.figure(figsize = (20, 20))\nplt.subplot(2, 1, 1)\nax1 = sns.boxplot(x=df['Q1'].values.tolist(), y=[float(duration)\/60 for duration in\n                  df['Time from Start to Finish (seconds)'].values.tolist()],\n                  order=np.unique(df['Q1'].values.tolist()),\n                  color='grey')\nax1.set_xlabel('Age Group', fontsize=30)\nax1.set_ylabel('Spread of average minutes taken', fontsize=30)\nax1.set_title('Boxplot for time taken by a age group in minutes', fontsize=50)\n\nplt.subplot(2, 1, 2)\nax2 = sns.boxplot(x=df['Q1'].values.tolist(), y=[float(duration)\/60 for duration in\n                  df['Time from Start to Finish (seconds)'].values.tolist()],\n                  order=np.unique(df['Q1'].values.tolist()),\n                  color='grey')\nax2.set_xlabel('Age Group', fontsize=30)\nax2.set_ylabel('Spread of average hour taken', fontsize=30)\nax2.set_ylim([-1, 60])\nax2.set_title('Boxplot for time taken by a age group in minutes (Zoomed in)', fontsize=50)\nplt.show()","6272d2ed":"countries, counts = np.unique(df['Q3'].values.tolist(), return_counts=True)\nfig = px.pie(names=countries, values=counts,\n            width=1500, height=1000,\n            title='Survey counts from different countries')\nfig.show()","f7a2e3e9":"genders, counts = np.unique(df['Q2'].values.tolist(), return_counts=True)\nfig = px.pie(names=genders, values=counts,\n            width=700, height=500,\n            title='Gender distribution')\nfig.show()","280d803d":"# Lets visualize the sex distribution among different age-groups\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2,1,1)\nax1 = sns.countplot(x='Q1', hue='Q2', data=df,\n                    order=np.unique(df['Q1'].values.tolist()), \n                    alpha=0.7)\nage_grps, counts = np.unique(df['Q1'].values.tolist(), return_counts=True)    \nfor p in ax1.patches:\n    height = p.get_height()\n    try:\n        txt=f'n={int(height)}'\n    except:\n        txt='n=0'\n    ax1.text(p.get_x()+0.01, height+50, txt,\n             fontsize=15, rotation=90)\n\nax1.set_title('Gender distribution among different age-groups', fontsize=50)\nax1.set_ylabel('Counts', fontsize = 30)\nax1.set_xlabel('Age group', fontsize= 30)\nax1.set_ylim([0, max(counts)+500])\nax1.tick_params(axis='x', rotation=45)\n\nplt.subplot(2,1,2)\ndf1 = df.groupby('Q1')['Q2'].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\nax2 = sns.barplot(x='Q1', y='percent', hue='Q2', data=df1,\n                  order=np.unique(df1['Q1'].values.tolist()),\n                  alpha=0.7)\n\nfor p in ax2.patches:\n    height = p.get_height()\n    ax2.text(p.get_x()+0.01, height+0.5, f'n={round(height, 2)} %',\n             fontsize=15, rotation=90)  \n    \nax2.set_title('Percentage gender distribution among different age-groups', fontsize=50)\nax2.set_ylabel('Counts', fontsize = 30)\nax2.set_xlabel('Age group', fontsize= 30)\nax2.set_ylim([0, 120])\nax2.tick_params(axis='x', rotation=45)\nplt.show()","ab43c863":"# A function to get count matrix from a datafrma columns\ndef cm_two_columns(df, col1, col2, norm=False, no_nan=True):\n    '''\n    df: dataframe\n    col1: column 1 -> y\n    col2: column 2 -> x\n    norm: optional-argument to normalize the count-matrix\n    no_nan: To remove NaNs\n    '''\n    uq_cols1 = np.unique(df[col1].values.tolist())\n    uq_cols2 = np.unique(df[col2].values.tolist())\n    if no_nan:\n        if col1!='Q24' and col2!='Q24':\n            uq_cols1 = [uq_col1 for uq_col1 in uq_cols1 if 'nan'!=uq_col1]\n            uq_cols2 = [uq_col2 for uq_col2 in uq_cols2 if 'nan'!=uq_col2]\n        elif col1=='Q24':\n            uq_cols1 = [uq_col1 for uq_col1 in uq_cols1 if 'nan'!=uq_col1]\n            uq_cols2 = [uq_col2 for uq_col2 in uq_cols2 if 'nan'!=uq_col2]\n            #sort-col1\n            ordered_compensations = [float(x.split('-')[0].split('> ')[-1].replace('$', '').replace(',', '')) for x in uq_cols1]\n            uq_cols1 = [x for _, x in sorted(zip(ordered_compensations, uq_cols1))]\n\n        else:\n            uq_cols1 = [uq_col1 for uq_col1 in uq_cols1 if 'nan'!=uq_col1]\n            uq_cols2 = [uq_col2 for uq_col2 in uq_cols2 if 'nan'!=uq_col2]\n            #sort-col2\n            ordered_compensations = [float(x.split('-')[0].split('> ')[-1].replace('$', '').replace(',', '')) for x in uq_cols2]\n            uq_cols2 = [x for _, x in sorted(zip(ordered_compensations, uq_cols2))]\n            \n    cm = []\n    for uq_col1 in uq_cols1:\n        df_uq_col1 = df[df[col1]==uq_col1]\n        col1_count = []\n        for uq_col2 in uq_cols2:\n            col1_count.append(len(df_uq_col1[df_uq_col1[col2]==uq_col2]))\n        cm.append(col1_count)\n    cm = np.array(cm)\n    if norm:\n        # (row)normalize-the matrix and return the normalized matrix\n        total_counts = cm.sum(axis=1)\n        norm_cm = cm\/total_counts[:, np.newaxis]\n        return norm_cm, uq_cols1, uq_cols2\n    \n    return cm, uq_cols1, uq_cols2","83fbe6ca":"# Generating heatmap for different genders in different country\ncm, countries, genders = cm_two_columns(df, 'Q3', 'Q2')\nplt.figure(figsize=(20, 20))\nsns.heatmap(pd.DataFrame(cm, index = countries, columns=genders),\n            annot=True, cmap='gist_gray', fmt='g')\nplt.title('Count heatmap of gender distribution in different countries', fontsize=30)\nplt.xlabel('Genders', fontsize=20)\nplt.ylabel('Country', fontsize=20)\nplt.show()","39b1ec69":"# seeing the sex-ratio in each country\nnorm_cm, countries, genders = cm_two_columns(df, 'Q3', 'Q2', norm=True)\nplt.figure(figsize=(20, 20))\nsns.heatmap(pd.DataFrame(norm_cm, index = countries, columns=genders),\n            annot=True, cmap='gist_gray', fmt='g')\nplt.title('gender-ratio in different countries', fontsize=30)\nplt.xlabel('Genders', fontsize=20)\nplt.ylabel('Country', fontsize=20)\nplt.show()","cf7f04fa":"countries, counts = np.unique(df['Q3'].values.tolist(), return_counts=True)\ncountries_counts = {}\nfor country, count in zip(countries, counts):\n    countries_counts[country] = count\n\ncountries_counts = sorted(countries_counts.items(), key=operator.itemgetter(1), reverse=True)\nplt.figure(figsize=(80, 40))\n\nfor i, (country, _) in enumerate(countries_counts):\n    if i < 6:\n        df_country = df[df['Q3']==country]\n        explode_vals=[]\n        for x in df_country.Q2.unique():\n            if x == 'Man':\n                explode_vals.append(0.2)\n            else:\n                explode_vals.append(0)    \n        plt.subplot(2, 3, i+1)\n        percentages = ((df_country.Q2.value_counts()\/sum(df_country.Q2.value_counts()))*100).values.tolist()\n        plt.pie(x=percentages,\n                explode=explode_vals,\n                labels=[f'{round(i, 2)} %' for i in percentages])\n        if country=='United States of America':\n            plt.title(f'Gender split in USA', fontsize=30)\n        else:\n            plt.title(f'Gender split in {country}', fontsize=30)\n        plt.legend(labels = [f'{con} - {round(i, 2)} %' for con, i in zip(df_country.Q2.unique(), percentages)],\n                   fontsize=30, loc='best')\nplt.show()","d42d35ef":"cm, education_lvl, positions = cm_two_columns(df, 'Q4', 'Q5')\nplt.figure(figsize=(20, 10))\nsns.heatmap(pd.DataFrame(cm, index = education_lvl, columns=positions),\n            annot=True, cmap='gist_gray', fmt='g')\nplt.title('Count heatmap of education level vs different titles', fontsize=30)\nplt.xlabel('Position Title', fontsize=20)\nplt.ylabel('Education Level', fontsize=20)\nplt.show()","4188b846":"df_no_nan_salary = df[df['Q24'].notna()].copy(deep=True)\n\n# Consider salary > 100,000 USD\nmt_100k = ['100,000-124,999', '125,000-149,999', '150,000-199,999', '200,000-249,999',\n          '250,000-299,999', '300,000-500,000', '> $500,000']\n\nin_bw_40k_100k = ['40,000-49,999', '50,000-59,999', '60,000-69,999', \n                  '70,000-79,999', '80,000-89,999', '90,000-99,999']\n\nsal_mt_100k = []\nsal_bw_40k_100k = []\nfor sal in df_no_nan_salary['Q24'].values.tolist():\n    if sal in mt_100k:\n        sal_mt_100k.append(1)\n    else:\n        sal_mt_100k.append(0)\n    if sal in in_bw_40k_100k:\n        sal_bw_40k_100k.append(1)\n    else:\n        sal_bw_40k_100k.append(0)\n        \ndf_no_nan_salary['sal_mt_100k'] = sal_mt_100k\ndf_no_nan_salary['sal_bw_40k_100k'] = sal_bw_40k_100k    ","0cd6d530":"countries_w_codes = {}\nfor country in pycountry.countries:\n    countries_w_codes[country.name] = country.alpha_3\n\ncodes = [countries_w_codes.get(country, f'{country}') for country in countries]","d1acf1ee":"'''\nThere are some countries with no code in the list setting them manually:\n\n'Iran, Islamic Republic of...' -> 'Iran, Islamic Republic of': 'IRN',\n'Republic of Korea' ->  \"Korea, Democratic People's Republic of\"-> 'PRK'\n'Russia' -> 'Russian Federation': 'RUS',\n'South Korea' -> 'Korea, Republic of' -> 'KOR'\n'Taiwan' -> 'Taiwan, Province of China': 'TWN'\n'United Kingdom of Great Britain and Northern Ireland' -> 'United Kingdom': 'GBR'\n'United States of America' -> 'United States': 'USA'\n'''\n\ncorrected_codes = {'Iran, Islamic Republic of...': 'IRN', 'Republic of Korea': 'PRK',\n                   'Russia': 'RUS', 'South Korea': 'KOR', 'Taiwan': 'TWN',\n                   'United Kingdom of Great Britain and Northern Ireland': 'GBR',\n                   'United States of America': 'USA'}\n\ncodes_crr = []\nfor code in codes:\n    if code in corrected_codes.keys():\n        codes_crr.append(corrected_codes[code])\n    else:\n        codes_crr.append(code)\n        \ncountry_codes ={}\nfor country, code in zip(countries, codes_crr):\n    country_codes[country] = code\n","590adf42":"countries = []\niso_codes = []\ncounts_mt_100k = []\ncounts_bw_40k_100k = []\nfor country in country_codes.keys():\n    countries.append(country)\n    df_selected = df_no_nan_salary[df_no_nan_salary['Q3']==country]\n    iso_codes.append(country_codes[country])\n    counts_mt_100k.append(len(df_selected[df_selected['sal_mt_100k'] == 1]))\n    counts_bw_40k_100k.append(len(df_selected[df_selected['sal_bw_40k_100k'] == 1]))\n    \ndf_country_info = pd.DataFrame()\ndf_country_info['country'] = countries\ndf_country_info['counts of compensation > 100k$'] = counts_mt_100k\ndf_country_info['counts of compensation between 40k-100k$'] = counts_bw_40k_100k\ndf_country_info['iso_codes'] = iso_codes\n\nfig = px.choropleth(data_frame=df_country_info,\n                    locations='iso_codes',\n                    color='counts of compensation > 100k$',\n                    color_continuous_scale=\"Viridis\",\n                    range_color=(0, max(counts_mt_100k)),\n                    hover_name='country', # column to add to hover information\n                    title='Heatmap of salary more than 100k')\nfig.show()","2d61964b":"# just out of interest lets see the spread of somewhat average-pays 40k-100k\nfig = px.choropleth(data_frame=df_country_info,\n                    locations='iso_codes',\n                    color='counts of compensation between 40k-100k$',\n                    color_continuous_scale=\"Viridis\",\n                    range_color=(0, max(counts_mt_100k)),\n                    hover_name='country', # column to add to hover information\n                    title='Heatmap of salary more than 100k')\nfig.show()","9a13c0c1":"counts_lt_40k = []\nfor count_mt_100k, count_bw_40k_100k in zip(counts_mt_100k, counts_bw_40k_100k):\n    if count_bw_40k_100k == 0 and count_mt_100k == 0:\n        counts_lt_40k=1\n    else:\n        counts_lt_40k=0\n\ndf_country_info['counts of compensation < 40k$'] = counts_lt_40k\n\n# just out of interest lets see the spread of lower pays < 40k\nfig = px.choropleth(data_frame=df_country_info,\n                    locations='iso_codes',\n                    color='counts of compensation < 40k$',\n                    color_continuous_scale=\"Viridis\",\n                    range_color=(0, max(counts_mt_100k)),\n                    hover_name='country', # column to add to hover information\n                    title='Heatmap of salary more than 100k')\nfig.show()","c36f1e96":"cm, education_lvl, compensations = cm_two_columns(df_no_nan_salary, 'Q4', 'Q24')\nplt.figure(figsize=(30, 10))\nsns.heatmap(pd.DataFrame(cm, index = education_lvl, columns=compensations),\n            annot=True, cmap='gist_gray', fmt='g')\nplt.title('Count heatmap of education level vs compensation', fontsize=30)\nplt.xlabel('Compensation', fontsize=20)\nplt.ylabel('Education Level', fontsize=20)\nplt.show()","5af56325":"norm_cm, education_lvl, compensations = cm_two_columns(df_no_nan_salary, 'Q4', 'Q24', norm=True)\nplt.figure(figsize=(30, 10))\nsns.heatmap(pd.DataFrame(norm_cm, index = education_lvl, columns=compensations),\n            annot=True, cmap='gist_gray', fmt='.2g')\nplt.title('Normalized heatmap of education level vs compensation', fontsize=30)\nplt.xlabel('Compensation', fontsize=20)\nplt.ylabel('Education Level', fontsize=20)\nplt.show()","f1868dae":"plt.figure(figsize=(30, 10))\ndf1 = df_no_nan_salary.groupby('Q4')['Q24'].value_counts(normalize=True)\ndf1 = df1.rename('norm_count').reset_index()\ncompensations = list(np.unique(df1['Q24'].values.tolist()))\n\nordered_compensations = [float(x.split('-')[0].split('> ')[-1].replace('$', '').replace(',', '')) for x in compensations]\ncompensations = [x for _, x in sorted(zip(ordered_compensations, compensations))]\n\n\nax = sns.barplot(x='Q24', y='norm_count', hue='Q4', data=df1,\n                 order=compensations, alpha=0.7)\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x(), height+0.01, f'n={round(height, 2)} %',\n             fontsize=8, rotation=90)  \n\nax2 = plt.axes([0.53, 0.45, .35, .4], facecolor='whitesmoke')\nsns.barplot(x='Q24', y='norm_count', hue='Q4', data=df1,\n            ax=ax2, order=compensations)  \n    \nax2.set_title('zoom')\nax2.set_xlim([12.5,24])\nax2.set_ylim([0, 0.1])\nax2.get_legend().remove()\nax2.tick_params(axis='x', rotation=45)\n\nax.set_title('Normalized Compensation per education level', fontsize=50)\nax.set_ylabel('Counts', fontsize = 30)\nax.set_xlabel('Age group', fontsize= 30)\nax.set_ylim([0, 0.6])\nax.tick_params(axis='x', rotation=45)\nax.legend(loc='upper left')\nplt.show()","fa1cc0ab":"'''\n'What programming languages do you use on a regular basis? (Select all that apply)''\n- Selected Choice - \nPart-1: 'Python',\nPart-2: 'R',\nPart-3: 'SQL',\nPart-4: 'C',\nPart-5: 'C++',\nPart-6: 'Java',\nPart-7: 'Javascript',\nPart-8: 'Julia',\nPart-9: 'Swift',\nPart-10: 'Bash',\nPart-11: 'MATLAB',\nPart-12: 'None',\nOTHER: 'Other',\n'''\n# Dictonary with index of different people who use a language\nlangs_survery_idx = {'Python': [], 'R': [], 'SQL': [], 'C': [],\n                   'C++': [], 'Java': [], 'Javascript': [], 'Julia': [],\n                   'Swift': [], 'Bash': [], 'MATLAB': [], 'None': [],\n                   'Other': []}\n\n# Looking at Q7\ndf_Q7 = df[[col for col in df.columns.values.tolist() if 'Q7' in col]].copy(deep=True)\ndf_Q7.fillna(0, inplace=True)\n\nfor idx, row in df_Q7.iterrows():\n    for key in langs_survery_idx.keys():\n        if key in row.values.tolist():\n            langs_survery_idx[key].append(idx)","a255628c":"# Generating counts of different combinations that people are using\npair_counts = []\nfor lang1 in langs_survery_idx.keys():\n    lang1_count = []\n    for lang2 in langs_survery_idx.keys():\n        lang1_count.append(len([x for x in langs_survery_idx[lang1] \n                                if x in langs_survery_idx[lang2]]))\n    pair_counts.append(lang1_count)\n\nplt.figure(figsize=(20, 20))\nmask = np.zeros_like(np.array(pair_counts), dtype=np.bool)\n# get the indices of the upper traingle\nmask[np.triu_indices_from(mask, k=1)] = True\nsns.heatmap(pd.DataFrame(pair_counts, index=langs_survery_idx.keys(),\n                         columns=langs_survery_idx.keys()),\n            mask=mask, annot=True, cmap='gist_gray', linewidth=0.5, fmt='g')\nplt.title('Normalized heatmap of education level vs compensation', fontsize=30)\nplt.xlabel('Compensation', fontsize=20)\nplt.ylabel('Education Level', fontsize=20)\nplt.show()","dac51dc0":"labels, radii, actualOverlaps, disjointOverlaps = venn.df2areas(\n    df_Q7.replace(list(langs_survery_idx.keys()), 1),\n    fineTune=False)\nfig, ax = venn.venn(radii, actualOverlaps, disjointOverlaps,\n                    labels=list(langs_survery_idx.keys()),\n                    labelsize=10, cmap='Blues', fineTune=False)\nfig.set_size_inches(15, 15)\nplt.show()","238db28a":"# removing the NaNs and seeing the recommendations\nlangs, counts = np.unique([x for x in df['Q8'].values.tolist() if x==x], return_counts=True)\nfig = px.pie(names=langs, values=counts,\n            width=700, height=500,\n            title='Recomended language distribution')\nfig.show()","968bcc3f":"'''\n'Which of the following integrated development environments (IDE's) do you use on a regular basis?'\n(Select all that apply) - Selected Choice - \nPart_1: \"Jupyter (JupyterLab, Jupyter Notebooks, etc) \",\nPart_2: \"RStudio \",\nPart_3: \"Visual Studio \/ Visual Studio Code\",\nPart_4: \"Click to write Choice 13\",\nPart_5: \"PyCharm\",\nPart_6: \"Spyder\",\nPart_7: \"Notepad++\",\nPart_8: \"Sublime Text\",\nPart_9: \"Vim \/ Emacs\",\nPart_10: \"MATLAB\",\nPart_11: \"None\",\nPart_OTHER: \"Other\",\n'''\nque_to_IDE = {'Part_1': 'Jupyter', 'Part_2': 'RStudio', 'Part_3': 'Visual Studio', 'Part_4': 'Visual Studio Code (VSCode)',\n             'Part_5': 'PyCharm', 'Part_6': 'Spyder', 'Part_7': 'Notepad ++', 'Part_8': 'Sublime Text',\n             'Part_9': 'Vim', 'Part_10': 'MATLAB', 'Part_11': 'None', 'OTHER': 'Other'}\nIDE_count = {}\nfor col in [col for col in df.columns if 'Q9' in col]:\n    IDE_count[que_to_IDE[col.replace('Q9_', '')]] = len([x for x in df[col] if x==x])\n\nfig = px.pie(names=IDE_count.keys(), values=[IDE_count[key] for key in IDE_count.keys()],\n            width=700, height=500,\n            title='IDE usage distribution')\nfig.show()","db7c69c1":"'''\n'Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -\n'Part_1': 'Kaggle Notebooks',\n'Part_2': 'Colab Notebooks',\n'Part_3': 'Azure Notebooks',\n'Part_4': 'Paperspace \/ Gradient',\n'Part_5': 'Binder \/ JupyterHub',\n'Part_6': 'Code Ocean',\n'Part_7': 'IBM Watson Studio',\n'Part_8': 'Amazon Sagemaker Studio',\n'Part_9': 'Amazon EMR Notebooks',\n'Part_10': 'Google Cloud AI Platform Notebooks',\n'Part_11': 'Google Cloud Datalab Notebooks',\n'Part_12': 'Databricks Collaborative Notebooks',\n'Part_13': 'None',\n'OTHER': 'Other',\n'''\n\nque_to_hosted_nb = {'Part_1': 'Kaggle Notebooks', 'Part_2': 'Colab Notebooks',\n                    'Part_3': 'Azure Notebooks', 'Part_4': 'Paperspace \/ Gradient',\n                    'Part_5': 'Binder \/ JupyterHub', 'Part_6': 'Code Ocean',\n                    'Part_7': 'IBM Watson Studio', 'Part_8': 'Amazon Sagemaker Studio',\n                    'Part_9': 'Amazon EMR Notebooks', \n                    'Part_10': 'Google Cloud AI Platform Notebooks',\n                    'Part_11': 'Google Cloud Datalab Notebooks',\n                    'Part_12': 'Databricks Collaborative Notebooks ',\n                    'Part_13': 'None', 'OTHER': 'Other'}\nhosted_nb_count = {}\n\n\nfor col in [col for col in df.columns if 'Q10' in col]:\n    hosted_nb_count[que_to_hosted_nb[col.replace('Q10_', '')]] = len([x for x in df[col] if x==x])\n\nfig = px.pie(names=hosted_nb_count.keys(),\n             values=[hosted_nb_count[key] for key in hosted_nb_count.keys()],\n             width=800, height=500,\n             title='Hosted Notebooks usage distribution')\nfig.show()","0318b3cf":"platforms, counts = np.unique(df['Q11'].dropna().values.tolist(), return_counts=True)\nfig = px.pie(names=platforms, values=counts,\n            width=1000, height=700,\n            title='Distribution of differnt platform usage')\nfig.show()","197380f0":"platforms_no_students, counts_no_students = np.unique(df[df['Q5'] != 'Student']['Q11'].dropna().values.tolist(),\n                                                      return_counts=True)\n\nplatforms_students, counts_students = np.unique(df[df['Q5'] == 'Student']['Q11'].dropna().values.tolist(),\n                                                      return_counts=True)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=platforms_no_students, values=counts_no_students,\n                     name=\"Professionals\"), 1, 1)\nfig.add_trace(go.Pie(labels=platforms_students, values=counts_students,\n                     name=\"Studetns\"), 1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    height=700, width=1200,\n    title_text=\"Students vs Professionals platform usage\",\n    annotations=[dict(text='professionals', x=0.15, y=0.5, font_size=15, showarrow=False),\n                 dict(text='students', x=0.84, y=0.5, font_size=15, showarrow=False)])\nfig.show()","60863134":"'''\n'Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice -\n'Part_1': 'GPU',\n'Part_2': 'TPU',\n'Part_3': 'None',\n'OTHER': 'Other',\n'''\n\nque_to_spec_hardware = {'Part_1': 'GPU', 'Part_2': 'TPU',\n                        'Part_3': 'None', 'OTHER': 'Other'}\nspec_hardware_count = {}\n\n\nfor col in [col for col in df.columns if 'Q12' in col]:\n    spec_hardware_count[que_to_spec_hardware[col.replace('Q12_', '')]] = len([x for x in df[col] if x==x])\n\nfig, ax = plt.subplots(figsize=(10,6))\nax.vlines(x=list(spec_hardware_count.keys()), ymin=0,\n          ymax=[spec_hardware_count[k] for k in spec_hardware_count.keys()],\n          color='firebrick', alpha=0.7, linewidth=2)\nax.scatter(x=list(spec_hardware_count.keys()),\n           y=[spec_hardware_count[k] for k in spec_hardware_count.keys()],\n           s=75, color='firebrick', alpha=0.7)\n\n# Title, Label, Ticks and Ylim\nax.set_title('Specialized hardware usage', fontdict={'size':22})\nax.set_ylabel('Counts')\nax.set_xlabel('Hardware')\nax.set_ylim(0, 9000)\n\n# Annotate\nfor row in spec_hardware_count.keys():\n    ax.text(row, spec_hardware_count[row]+50,\n            s=round(spec_hardware_count[row], 2),\n            horizontalalignment= 'center',\n            verticalalignment='bottom', fontsize=14)\n\nplt.show()","130a4959":"df_Q12 = df[[col for col in df.columns.values.tolist() if 'Q12' in col]].copy(deep=True)\ndf_Q12.fillna(0, inplace=True)\n\nhardware_idx = {'GPUs': [], 'TPUs':[], 'None':[], 'Other':[]}\nfor idx, row in df_Q12.iterrows():\n    for key in hardware_idx.keys():\n        if key in row.values.tolist():\n            hardware_idx[key].append(idx)\n            \nlabels, radii, actualOverlaps, disjointOverlaps = venn.df2areas(df_Q12.replace(list(hardware_idx.keys()), 1),\n                                                                fineTune=True)\nfig, ax = venn.venn(radii, actualOverlaps, disjointOverlaps,\n                    labels=list(hardware_idx.keys()),\n                    labelsize=10, cmap='Greys', fineTune=True)\nfig.set_size_inches(15, 15)\nplt.title('Hardware usage overlap')\nplt.show()","83ab68d7":"fig, ax = plt.subplots(figsize=(20, 10))\nsns.countplot(x='Q13', data=df, ax=ax,\n             order=['Never', 'Once', '2-5 times', '6-25 times', 'More than 25 times'],\n             color = 'black',\n             alpha=0.7)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+0.25, height+50, 'n=%.0f'%(height), fontsize=25, rotation=45)\nax.set_title('Frequency of TPU usage', fontsize=50)\nax.set_ylabel('Counts', fontsize = 30)\nax.set_xlabel('Times', fontsize= 30)\nax.set_ylim([0, max(counts)+1500])\nplt.show()","2cfa34cf":"# seeing the sex-ratio in each country\ncm, job_titles, TPU_usage = cm_two_columns(df, 'Q5', 'Q13')\n\nplt.figure(figsize=(10, 20))\nsns.heatmap(pd.DataFrame(cm, index = job_titles, columns=TPU_usage),\n            annot=True, cmap='gist_gray', fmt='g', square=True)\nplt.title('Unormalized(count) heatmap for TPU usage acrros different job-titles', fontsize=30)\nplt.xlabel('TPU usage', fontsize=20)\nplt.ylabel('Job Title', fontsize=20)\nplt.show()","634abb60":"norm_cm, job_titles, TPU_usage = cm_two_columns(df, 'Q5', 'Q13', norm=True)\n\nplt.figure(figsize=(10, 20))\nsns.heatmap(pd.DataFrame(norm_cm, index = job_titles, columns=TPU_usage),\n            annot=True, cmap='gist_gray', fmt='.3g', square=True)\nplt.title('Normalized heatmap for TPU usage acrros different job-titles', fontsize=30)\nplt.xlabel('TPU usage', fontsize=20)\nplt.ylabel('Job Title', fontsize=20)\nplt.show()","910524dc":"'''\n'What data visualization libraries or tools do you use on a regular basis?  (Select all that apply) - Selected Choice -\n'Part_1': 'Matplotlib',\n'Part_2': 'Seaborn',\n'Part_3': 'Plotly \/ Plotly Express',\n'Part_4': 'Ggplot \/ ggplot2',\n'Part_5': 'Shiny',\n'Part_6': 'D3 js',\n'Part_7': 'Altair',\n'Part_8': 'Bokeh',\n'Part_9': 'Geoplotlib',\n'Part_10': 'Leaflet \/ Folium',\n'Part_11': 'None',\n'OTHER': 'Other',\n'''\n\nque_to_vis_pack = {'Part_1': 'Matplotlib', 'Part_2': 'Seaborn', \n                   'Part_3': 'Plotly \/ Plotly Express', 'Part_4': 'Ggplot \/ ggplot2', \n                   'Part_5': 'Shiny', 'Part_6': 'D3 js', 'Part_7': 'Altair',\n                   'Part_8': 'Bokeh', 'Part_9': 'Geoplotlib', 'Part_10': 'Leaflet \/ Folium',\n                   'Part_11': 'None', 'OTHER': 'Other'}\nvis_pack_count = {}\n\n\nfor col in [col for col in df.columns if 'Q14' in col]:\n    vis_pack_count[que_to_vis_pack[col.replace('Q14_', '')]] = len([x for x in df[col] if x==x])\n\n    \ndf_vis_pack = pd.DataFrame()\ndf_vis_pack['packages'] = list(vis_pack_count.keys())\ndf_vis_pack['counts'] = [vis_pack_count[k] for k in vis_pack_count.keys()]\n\nfig = px.treemap(df_vis_pack, path = ['packages'], values='counts')\nfig.update_layout(\n    height=600, width=800,\n    title_text=\"Different visualization package usage\")\nfig.show()","ba3ea102":"# Looking at Q14\ndf_Q14 = df[[col for col in df.columns.values.tolist() if 'Q14' in col]].copy(deep=True)\ndf_Q14.fillna(int(0), inplace=True)\n\nlabels, radii, actualOverlaps, disjointOverlaps = venn.df2areas(\n    df_Q14.replace([' Altair ', ' Bokeh ', ' D3 js ', ' Geoplotlib ',\n                    ' Ggplot \/ ggplot2 ', ' Leaflet \/ Folium ', ' Matplotlib ',\n                    ' Plotly \/ Plotly Express ', ' Seaborn ', ' Shiny ', 'None',\n                    'Other'], 1),\n    fineTune=False)\nfig, ax = venn.venn(radii, actualOverlaps, disjointOverlaps,\n                    labels=list(vis_pack_count.keys()),\n                    labelsize=10, cmap='Blues', fineTune=False)\nfig.set_size_inches(15, 15)\nplt.title(\"Venn-diagram of different visualization packages\")\nplt.show()","15a1d4ad":"# seeing the sex-ratio in each country\ncm, exp, comp = cm_two_columns(df, 'Q15', 'Q24')\nplt.figure(figsize=(20, 10))\nsns.heatmap(pd.DataFrame(pd.DataFrame(cm, index=exp, columns=comp),\n                         index=[ 'I do not use machine learning methods', 'Under 1 year','1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-10 years', '10-20 years', '20 or more years']),\n            annot=True, cmap='gist_gray', fmt='g')\nplt.title('count heatmap of ML experience vs Compensation', fontsize=30)\nplt.xlabel('ML-experience', fontsize=20)\nplt.ylabel('Compensation', fontsize=20)\nplt.show()","92843e56":"df_Q15_Q24 = df[['Q15', 'Q24']].copy(deep=True)\n# Dropping NaNs\ndf_Q15_Q24.dropna(inplace=True)\n\nencoding_exp = {'I do not use machine learning methods': 0,\n                'Under 1 year': 1, '1-2 years': 2, '2-3 years': 3,\n                '3-4 years': 4, '4-5 years': 5, '5-10 years': 6,\n                '10-20 years': 7, '20 or more years': 8}\nencoding_comp = {'$0-999': 0, '1,000-1,999': 1, '2,000-2,999': 2, '3,000-3,999': 3,\n                 '4,000-4,999': 4, '5,000-7,499': 5, '7,500-9,999': 6, \n                 '10,000-14,999': 7, '15,000-19,999': 8, '20,000-24,999': 9,\n                 '25,000-29,999': 10, '30,000-39,999': 11, '40,000-49,999': 12,\n                 '50,000-59,999': 13, '60,000-69,999': 14, '70,000-79,999': 15,\n                 '80,000-89,999': 16, '90,000-99,999': 17, '100,000-124,999': 18,\n                 '125,000-149,999': 19, '150,000-199,999': 20, '200,000-249,999': 21,\n                 '250,000-299,999': 22, '300,000-500,000': 23, '> $500,000': 24}\n\ndf_Q15_Q24.replace({'Q15': encoding_exp, 'Q24': encoding_comp}, inplace=True)\ncorr = df_Q15_Q24['Q15'].corr(df_Q15_Q24['Q24'])\n\nprint(f'Correlation between the ML experience with compensation is {round(corr, 3)}')","291b362e":"'''\n'Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice -'\n'Part_1': 'Scikit-learn',\n'Part_2': 'Tensorflow',\n'Part_3': 'Keras',\n'Part_4': 'PyTorch',\n'Part_5': 'Fast.ai',\n'Part_6': 'MXNet',\n'Part_7': 'Xgboost',\n'Part_8': 'LightGBM',\n'Part_9': 'CatBoost',\n'Part_10': 'Prophet',\n'Part_11': 'H2O 3',\n'Part_12': 'Caret',\n'Part_13': 'Tidymodels',\n'Part_14': 'JAX',\n'Part_15': 'None',\n'OTHER': 'Other',\n'''\n\nque_to_ml_framework = {'Part_1': 'Scikit-learn', 'Part_2': 'Tensorflow',\n                       'Part_3': 'Keras', 'Part_4': 'PyTorch', 'Part_5': 'Fast.ai',\n                       'Part_6': 'MXNet', 'Part_7': 'Xgboost', 'Part_8': 'LightGBM',\n                       'Part_9': 'CatBoost', 'Part_10': 'Prophet', 'Part_11': 'H2O 3',\n                       'Part_12': 'Caret', 'Part_13': 'Tidymodels', 'Part_14': 'JAX',\n                       'Part_15': 'None', 'OTHER': 'Other'}\nml_framework_count = {}\n\nfor col in [col for col in df.columns if 'Q16' in col]:\n    ml_framework_count[que_to_ml_framework[col.replace('Q16_', '')]] = len([x for x in df[col] if x==x])\n\nfig = px.pie(names=ml_framework_count.keys(),\n             values=[ml_framework_count[key] for key in ml_framework_count.keys()],\n             width=800, height=500,\n             title='ML framework usage distribution')\nfig.show()","535ef787":"df_professionals = df[df['Q5'] != 'Student']\npro_ml_framework_count = {}\n\nfor col in [col for col in df_professionals.columns if 'Q16' in col]:\n    pro_ml_framework_count[que_to_ml_framework[col.replace('Q16_', '')]] = len([x for x in df_professionals[col] if x==x])\n\ndf_students = df[df['Q5'] == 'Student']\nstudents_ml_framework_count = {}\n\nfor col in [col for col in df_students.columns if 'Q16' in col]:\n    students_ml_framework_count[que_to_ml_framework[col.replace('Q16_', '')]] = len([x for x in df_students[col] if x==x])\n\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=list(pro_ml_framework_count.keys()),\n                     values=[pro_ml_framework_count[k] for k in \n                             pro_ml_framework_count.keys()],\n                     name=\"Professionals\"), 1, 1)\nfig.add_trace(go.Pie(labels=list(students_ml_framework_count.keys()),\n                     values=[students_ml_framework_count[k] for k in \n                             students_ml_framework_count.keys()],\n                     name=\"Studetns\"), 1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    height=500, width=800,\n    title_text=\"Students vs Professionals ML-framework usage\",\n    annotations=[dict(text='professionals', x=0.14, y=0.5, font_size=15, showarrow=False),\n                 dict(text='students', x=0.84, y=0.5, font_size=15, showarrow=False)])\nfig.show()","185c30fc":"'''\n'Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice -'\n'Part_1': 'Linear or Logistic Regression',\n'Part_2': 'Decision Trees or Random Forests',\n'Part_3': 'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n'Part_4': 'Bayesian Approaches',\n'Part_5': 'Evolutionary Approaches',\n'Part_6': 'Dense Neural Networks (MLPs, etc)',\n'Part_7': 'Convolutional Neural Networks',\n'Part_8': 'Generative Adversarial Networks',\n'Part_9': 'Recurrent Neural Networks',\n'Part_10': 'Transformer Networks (BERT, gpt-3, etc)',\n'Part_11': 'None',\n'OTHER': 'Other',\n'''\n\nque_to_ml_algo = {'Part_1': 'Linear or Logistic Regression', \n                  'Part_2': 'Decision Trees or Random Forests',\n                  'Part_3': 'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n                  'Part_4': 'Bayesian Approaches', 'Part_5': 'Evolutionary Approaches',\n                  'Part_6': 'Dense Neural Networks (MLPs, etc)',\n                  'Part_7': 'Convolutional Neural Networks',\n                  'Part_8': 'Generative Adversarial Networks',\n                  'Part_9': 'Recurrent Neural Networks',\n                  'Part_10': 'Transformer Networks (BERT, gpt-3, etc)',\n                  'Part_11': 'None', 'OTHER': 'Other'}\nml_algo_count = {}\n\nfor col in [col for col in df.columns if 'Q17' in col]:\n    ml_algo_count[que_to_ml_algo[col.replace('Q17_', '')]] = len([x for x in df[col] if x==x])\n\nplt.figure(figsize=(30, 15))\nplt.barh([i for i in range(len(ml_algo_count.keys()))],\n         [ml_algo_count[k] for k in ml_algo_count.keys()],\n         align='center', alpha=0.4)\n# Annotate\ntotal_x = np.sum(np.array([ml_algo_count[k] for k in ml_algo_count.keys()]))\nfor y, x in zip([i for i in range(len(ml_algo_count.keys()))],\n                [ml_algo_count[k] for k in ml_algo_count.keys()]):\n    plt.text(x+800, y,\n            s=f'{x} => {round((x\/total_x)*100, 2)}%',\n            horizontalalignment= 'center',\n            verticalalignment='bottom', fontsize=24)\n    \nplt.yticks([i for i in range(len(ml_algo_count.keys()))], list(ml_algo_count))\nplt.xlabel('Counts per algorithm', fontsize=30)\nplt.ylabel('Different ML algorithm', fontsize=30)\nplt.title('Algorithm Usage Count', fontsize=30)\nplt.xlim((0, max([ml_algo_count[k] for k in ml_algo_count.keys()])+1800))\nplt.show()","24c4ca42":"# Looking at Q17\ndf_Q17 = df[[col for col in df.columns.values.tolist() if 'Q17' in col]].copy(deep=True)\ndf_Q17.fillna(int(0), inplace=True)\n\nlabels, radii, actualOverlaps, disjointOverlaps = venn.df2areas(\n    df_Q17.replace(['Bayesian Approaches', 'Convolutional Neural Networks',\n                    'Decision Trees or Random Forests',\n                    'Dense Neural Networks (MLPs, etc)', 'Evolutionary Approaches',\n                    'Generative Adversarial Networks',\n                    'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n                    'Linear or Logistic Regression', 'None', 'Other',\n                    'Recurrent Neural Networks',\n                    'Transformer Networks (BERT, gpt-3, etc)'], 1),\n    fineTune=False)\nfig, ax = venn.venn(radii, actualOverlaps, disjointOverlaps,\n                    labels=list(ml_algo_count.keys()),\n                    labelsize=10, cmap='Blues', fineTune=False)\nfig.set_size_inches(15, 15)\nplt.title(\"Venn-diagram of different ML algorithms usage\")\nplt.show()","946eb08f":"aaeee !! look at that there are a lot of python coders out there. No wonder there is so much help for everything in python. More importantly lets look at the combination (Python & SQL) and (C & SQL) are quite a popular combination. I personally use (Python and SQL) for web development and AWS automation. Seems like I am normal.\n\nLets look at the **Venn-Diagram** to see if there is any other significant overlap","90253a90":"Looks like India, USA, Brazil,and Japan have a lot of data-scientists.\n\n## Gender-ratio is another interesting topic which we can look into Q2\n\nLets look at overall gender-ratio in the field.","af4f0f0a":"People are still using personal PCs I though the usage of AWS would be much more anyways this might be the case because there are a lot of students in the survey let's see what working professionals are doing.","8a6531b4":"It is true then `Matplotlib` and `Seaborn` are the dominant visualization packages and as a novice I should aim to master using these two packages.\n\n\n## Now lets look at Q15\n\nExperience is something that has to be directly correlated to the compensation. Its logical right the more experienced you get the higher position and more responsitibility you are given and it should correspond to higher compensation. Let us see if this is actually the case.\n","8fe6b72d":"Peaks of master's and doctoral degree occurs towards the higher salary range. Thus it seems that chances of getting positions with higher compensation is correlated to the education level. **Go do PhD if you havent** :P lol. \n\n\n## Now then lets look at Q7\n\nWhat programming language do the people in community regularly use, this will give an indicator to what is the most in-demand programming language in the field. It is well known that a professional will need more than one language in there day to day job because different languages often used for differnt tasks. Like I myself use SQL, Python, Bash and Batch on a regular basis. But I also use HTML, CSS, JS when I do some web-development. This analysis is surely not an indicator of which language is more useful its more an indicator of which of the languages are more often used in the field. Having mentioned this let's try and analyze the response to this question to see which combination or a language is more in use in the field.","8c04c4f9":"Looks like men are not just dominating overall but are also dominating the field in every country. A gender-ratio for each country will be a better indicator for this. ","0a1fbe22":"The visualziation indeed confirms this and basic approaches like `LR` and `DT` are almost used by everyone in the field. Therefore mastering them should be novice's priority and thery are the basic algorithm which should be easy to learn for anyone.\n\n\nThat brings an end to this basic analysis of the kaggle-survey-2020. I have tried analysing the questions I felt were useful for me. I did learn quite a few new things analysing this survey. \n\nIt's only my first notebook and pardon me for its length. I have tried to be eloborative. In future I will try and stick to the story mode of a notebook answering a very specific question for a survery like this because those notebooks seem to be doing well in kaggle-competetions. Lets see where this will land me.\n\n\n![funfact.png](attachment:funfact.png)\n\n# <center> THE END <\/center>","33368d57":"There you have it, why people want to go to North-America. United States of America, Canada and India has some of the highest paid jobs in the field. Obviosuly the cost of living is also high in USA and Canada but so is the compensation.","1152e9af":"More or less the same. Lets look at the low-wage range i.e. salaries below 40k.","b7d5de5d":"\nLooks like all the Swift, Java and Javascript users also use python on a regular basis. On the other hand its very easy to see that python overlaps with almost all the languages because python is extremley easy to use and the community is extremly helpful.\n\n## Lets now look at what people suggest\n\nLooking at what people use in there daily life Python and SQL should dominate the recommendations.","ff7af63a":"Seems like Data scientists, Machine Learning Engineers and Data Engineers are the ones who the TPU more, nothing extremly conclusive is evident though by looking at these hatmaps. But this is expected because with these roles the reducing the training-time is crucial.\n\n## Q14 which visualization packages are more in use\n\nSince this question is about visualization lets try and visualize the response to this question in an appealing manner. ","a2c6543e":"Looks more even spread.\n\n## Now then lets see if higher education results in higher  compensations.\n\nIs it worth doing a PhD ? How should we see this, Heatmaps or histogram should look good.","d23ee3d5":"ta-da!! does look like younder people finished the survery slightly quicker. Not saying that they are quick. :P .... lol\nNow then lets look at the gender distribution among people working in AI, ML and DS. \n\n\n## Now then lets look at Q3\n\ncountry where the majority of Data scientists are. Intresting to deicde where to start your career as it can be related to number of opportunities or population.\n\n","a2182c3a":"The two column `Q15` and `Q24` inputs are categorical but they have sense of order in them. So to be able to see correlation between these two columns I should first encode these classes to ordered class and then look for correlation between these two columns.","6f84edcc":"Ahaan !! There is quite significant **(0.356)** correlation between these two columns.\n\n# Q16\n\nAllow it I am going to use pie-charts again to save some time.\nWhat all machine learning platform should I focus on to excel in the field.","ce354887":"Tadaaah !!! Kaggle Notebooks and Google Colab Notebooks rock.\n\n## Now lets dwell into Q11\n\nWhat  type of computing platforms do people using, I regularly use AWS, Jetson and my PC. Each option has their own pros I prefer AWS because you can mess it up and just terminate the instance. Jetson is expensive whereas PC only has so much processing power. ","5f5e9a55":"Ahan !! quite a lot of young people are interested on AI, ML and DS. This just shows the recent expansion of the field and the increasing demand of data-scientists in recent years leading to more and more younger people getting pulled towards the field.\n\nLets see if duration people spent to fill the form and this duration has any correlation with the age-group. To see the distribution ignoring the duration more than 3 hours because the only way a survey would have taken over 3 hours is if someone lef the survey incomplete and completed it later.","bcd9668a":"Looks like the younger generation has higher percentage of female employees. I hope this equality means keeps on increases as newer generations enter the field. Now lets see the gender ratio in different countries.","75f8d8aa":"Ahan !! clearly the **Research Scientists** postion is highle occupied by PhD and masters students. For **Data Scientists** there is quite an even spred among PhD, Masters and Bachelors degree holders, but it the count is not hugely supporting PhD degree requirements, But for **Data Analysts** it seems that Master's and Bachelor's degree is sufficient.\n\nThis is quite an expected behavoiour as more research-intensive the position is more important becomes a doctorate degree. Thus if an individual wants to work in a research-field he\/she\/zhe should probably consider doing a PhD.\n\nAnother factor which might influence a person's decision regrading higher studies is weather or not the higher level of education results in higher compensation.\n\n## Lets look at Q24\nYearly Compensation, lets look at where are the high paid jobs are located.\n\n**Droping** the rows where yearly compensation is *NaN* -> There are **10,729** rows after droping all the rows where compensation is *NaN*","c96411a6":"It does look like master's and PhD degree are either reuired for or lead to higher compensation positions. Lets try and validate this conclusion via normalized histograms.","5fef242b":"It is obvious that easy and useful algorithms like `linear and logistic regression` must be use by almost every individual becuase of their ease of implementation and its usefulness. Whereas more complicated and specific algoriths like `GAN`s and `RNN`s must be in use by very few individual lets visualize this hypothesis to check if this is the case.","63d8f14e":"Students are still majorly relying on personal PCs and AWS beacuse they are cheap and AWS offers free services is well (`t2.micro` EC2 instance for example). Seems like I have a lot of pie-charts, I will try and reduce pie-chart's usage from here onwards.\n\n## Much awaited Q12\n\nwhat type of specialized hardware are people using these days. Are people using TPUs and GPUs to train models\n","a69f7b92":"I suspect if all the people who use GPUs are the ones who use TPUs is well whereas the ones (would like to see the overlap between TPU and GPU).","c3a32502":"Looks like Malaysia, Phillippines, Tunisia, Ierland, 'Iran, Islamic Republic of ...' has the best sex ratio amonf all the countries. Whereas Japan, Italy and Repulic of Korea can do with a better gender-ratio.\n\nIt can clearly be seen that South-American countries does not have very good gender ratio. Lets plot a pie-chart visualization of top six countries based on survey count.","3099f2dc":"### Now lets plot see the heatmap of where the high salary jobs are","e056ea42":"Jupyter, VSCode and Pycharm are clearly the favourites of the developers in the field.\n\n## Now lets see Q10 what are peoples favourite hosted notebooks\n\nI personal love GoogeColab beacuse it can easily be lined to GoogleDrive for large dataset without loosing the data for the next time you try to work on the project. Other than that I am only starting to use kaggle notebooks. Other than these I have used JupyterHub and paperspace. ","b0b768cc":"# An attempt to analyze kaggle-survey-2020\n\nAuthor: Anshul Verma\n\nDate: $05^{th}$. January. 2021\n\nRevision: 2.00","f95bbf44":"WoW! people are recommending R. It's good that I know R and its used more for statistical-analysis and working on forecasting and time-series models is quite easy in R. In my opinion it's used more by *staticians* and *researchers*. The languages which are recommended to begineers are without a question Python, R and SQL because they are easy to learn are very highly used in the field. Whereas C and C++ comes into picture when extremely high implementation speed is needed. MATLAB is not free but is used widely in a lot of research laboratories and has a lot of packadges specific to different fields which makes it a good choice if you want to do a PhD in a field like **bio-chemistry**. I have used it to simulate some control systems SIMULINK is extremly useful and easy to use.\n\n## Now lets see what IDEs people use.\n\nI personaly use PyCharm and VisualStudio it's very useful to maintain and work on a project. I also use SublimeText when the project is not too big because its extremly quick and Jupter for the notebooks. Let us see what people use","1b5ffa25":"The survey consists of 39-Questions and certain specific answers to a particular question leads to an additional (earlier-choice specific or an extra) question. \n\n## Lets analyze the easy questions first Q1\n\nage-group of people in the field","c2664898":"Ahaan !! It does look like a lot of people who use TPU also use GPU. Whereas for Other specialized hardwares it seems like the must have advantage on some thing that GPU and TPU do not help with. I recently started learning about `OpenVino` and I started using it to compress my models and enhance their speed in CPUs not a hardware but I think its pretty cool. \n\n## Now then lets look at Q13\n\nHow many times have people used TPUs, Is there any particular position which require extensive usage of TPU ?\n","0ceb8761":"Clearly `Matplotlib`, `Seaborn`, `Plotly` and `ggplot` are faviourates of people in the field. `Matplotlib` is the most commonly used but the users were allowed to enter multiple answers *(select all that apply)*, therefore its very much possible that `Matplotlib` is used even by users who use other vissualization packages. ","de9ea497":"Seems like students use `scikit-learn`, `tensorflow` and `keras` more than professionals. Professionals have higher percentage usage of `XGBoost` and `LightGBM` than students. Nothing too drastic and we should focus our attention to top-5\/6 packadges, to be able to take the next step in our career. \n\n# Q17\n\nNow lets look at the next question from the survey which can help us guide towards our journey to becoming a master. ML algorithms that we should focus in-depth attention towards.","4d513f3c":"## Now then lets look at Q4 and Q5\n\nLets try and see if there is a particular education level prefered for a job-title in the field. A very intriguing question to decide if someone wants to do higher studies.","343fef14":"Looks like data has lot of NaNs, quite expected of a data coming out of a survey.","72d352fd":"We should focus on learning `scikit-learn`, `tensorflow`, `PyTorch`, `keras` and `XGBoost` because a lot of people already use it which means that there will be a lot of help availabel for different functions of these packades. It also must be easy to use given that a majoruty of people are using it. `XGBoost` and `LightGBM` are packages which offer gradient-boosting frameworks are more in use these days.\n\nLets see if professionals and students have slightly different preferences. Dropping all the rows with *NaN* as an input for **Q5**.","de32ac24":"## Checking how the dataframe looks like","9e9b90e6":"Looks like men are dominating the field.\n\nNow then lets look at the gender distribution among different age groups and in different countries. "}}