{"cell_type":{"46309ebe":"code","0cf75447":"code","c8c02ceb":"code","143173ac":"code","8b6ba0ed":"code","c701d88f":"code","e9b8cbe0":"code","dca60812":"code","7001c6b9":"code","156cfbfb":"code","eded2dde":"code","c19092a6":"code","6ad80562":"code","31333d6b":"code","a1367501":"code","56bbf358":"code","c981fb8e":"code","a752e5f2":"code","927dcccd":"code","2156841f":"code","a7cf9237":"code","e2531527":"code","d52e3b2e":"code","57d51e1a":"code","e196f0b6":"code","9b35b262":"markdown","04a4b6ee":"markdown","93e1d93b":"markdown","c50df558":"markdown","0ee26be7":"markdown","25ed011e":"markdown","3bfca0ff":"markdown","6fc3527b":"markdown","a957cbcb":"markdown","a275b564":"markdown","0015437a":"markdown","ddc62d97":"markdown","58d82df8":"markdown","775d45aa":"markdown","e0052047":"markdown","05765c43":"markdown","2d8e314e":"markdown","219a49db":"markdown","204a9fdc":"markdown","6d8ad3d4":"markdown","dc4f8344":"markdown","921b1976":"markdown","12530457":"markdown","0a56d97d":"markdown","98e40f6a":"markdown","b1071fae":"markdown","939c4a8a":"markdown","6dcb443a":"markdown","d0b2ab68":"markdown","dcbcfd66":"markdown","5e69096f":"markdown","1e201b88":"markdown","f7fdcd99":"markdown"},"source":{"46309ebe":"### General libraries ###\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz \nfrom graphviz import Source\nfrom IPython.display import SVG\n\n##################################\n\n### ML Models ###\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\n##################################\n\n### Metrics ###\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score","0cf75447":"# Load the data.\ndata=pd.read_csv('..\/input\/default of credit card clients.csv')\n\n# Information\ndata.info()","c8c02ceb":"# Drop \"ID\" column.\ndata=data.drop(['ID'], axis=1)","143173ac":"# Check for duplicate rows.\nprint(f\"There are {data.duplicated().sum()} duplicate rows in the data set.\")\n\n# Remove duplicate rows.\ndata=data.drop_duplicates()\nprint(\"The duplicate rows were removed.\")","8b6ba0ed":"# Check for null values.\nprint(f\"There are {data.isna().any().sum()} cells with null values in the data set.\")","c701d88f":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),annot=True, cmap='rainbow',linewidth=0.5, fmt='.2f');","e9b8cbe0":"# Distinguish attribute columns and class column.\nX=data[data.columns[:-1]]\ny=data['dpnm']","dca60812":"# Split to train and test sets. \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)","7001c6b9":"# Initialize a decision tree estimator.\ntr = tree.DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=25)\n\n# Train the estimator.\ntr.fit(X_train, y_train)","156cfbfb":"# Plot the tree.\ndot_data = tree.export_graphviz(tr, out_file=None, feature_names=X.columns, filled=True, rounded=True, special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","eded2dde":"# Make predictions.\ntr_pred=tr.predict(X_test)\n\n# CV score\n#tr_cv=cross_val_score(tr, X_train, y_train, cv=10).mean()","c19092a6":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % tr.score(X_test, y_test))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, tr_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, tr_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, tr_pred))","6ad80562":"# Plot confusion matrix for Decision tree.\ntr_matrix = confusion_matrix(y_test,tr_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Decision tree');","31333d6b":"# Predict propabilities for the test data.\ntr_probs = tr.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ntr_probs = tr_probs[:, 1]\n\n# Compute the AUC Score.\nauc_tr = roc_auc_score(y_test, tr_probs)\nprint('AUC: %.2f' % auc_tr)","a1367501":"!pip install pydotplus","56bbf358":"from io import StringIO\nimport pydotplus\nfrom IPython.display import Image\nfrom sklearn.ensemble import RandomForestClassifier","c981fb8e":"MAX_DEPTH = 2\nMAX_GINI_TODISPLAY = 0.2\nMIN_PERCENT_SAMPLES_TODISPLAY = 0.02\nmin_nb_samples_todisplay = int(MIN_PERCENT_SAMPLES_TODISPLAY * X_train.shape[0])","a752e5f2":"model_simple = RandomForestClassifier(random_state=42, bootstrap=False, max_depth=MAX_DEPTH, max_features=3, n_estimators=30000, min_impurity_decrease=0)\n\nmodel_simple.fit(X_train, y_train)","927dcccd":"for tree_todisplay in model_simple.estimators_:\n    # Get gini indices of last leaves\n    # node[0] and node[1] are gini indices of children (-1 if last leaf: this is what we want to get gini indices of last leaves)\n    \n    end_node_gini_indices_and_nb_samples = [ [node[4], node[5]] for node in tree_todisplay.tree_.__getstate__()['nodes'] if ((node[0] == -1) and (node[1] == -1))] \n    \n    display_current_tree = 0\n    for (end_node_gini_indice, end_node_gini_nb_samples) in end_node_gini_indices_and_nb_samples:        \n        if ((end_node_gini_indice < MAX_GINI_TODISPLAY) and (end_node_gini_nb_samples > min_nb_samples_todisplay)):            \n            display_current_tree = 1\n        \n    if (display_current_tree == 1):\n        dot_data = StringIO()\n\n        tree.export_graphviz(\n            tree_todisplay,\n            out_file=dot_data,\n            feature_names=X.columns,\n\n            class_names = sorted(np.unique(y_train.astype(str))),\n\n            max_depth=3,\n            filled=True,\n        )\n        g = pydotplus.graph_from_dot_data(\n            dot_data.getvalue()\n        )    \n        g.set_size('\"5,5!\"')\n\n        display(Image(g.create_png()))\n        ","2156841f":"df_test = pd.concat([pd.DataFrame(X_test, columns=X.columns).reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)","a7cf9237":"df_criteria = df_test[(df_test['BILL_AMT1'] <= 54416) & (df_test['PAY_AMT2'] > 4553)]\ndf_criteria['dpnm'].value_counts()","e2531527":"df_criteria['dpnm'].value_counts()[0] \/ df_criteria.shape[0]","d52e3b2e":"df_criteria = df_test[(df_test['BILL_AMT1'] <= 55000) & (df_test['PAY_AMT2'] > 4500)]\ndf_criteria['dpnm'].value_counts()[0] \/ df_criteria.shape[0]","57d51e1a":"df_criteria = df_test[  ((df_test['BILL_AMT1'] <= 55000) & (df_test['PAY_AMT2'] > 4500))\n                       | ((df_test['BILL_AMT1'] <= 54416) & (df_test['PAY_AMT1'] > 4553))\n                       | ((df_test['BILL_AMT2'] <= 4959.5) & (df_test['PAY_AMT1'] > 4053.5))\n                       | ((df_test['BILL_AMT3'] <= 49144.5) & (df_test['PAY_AMT2'] > 4522.0))                     \n                       | ((df_test['BILL_AMT1'] <= 54416) & (df_test['PAY_AMT5'] > 4317.5))                                           \n                    ]\n\ndf_criteria['dpnm'].value_counts()[0] \/ df_criteria.shape[0]","e196f0b6":"df_criteria['dpnm'].value_counts()[0] \/ df_test.shape[0]","9b35b262":"Precision of our prediction in test set, for first rule","04a4b6ee":"## Part 3: Modeling\n\nIn this section we build and try 3 models:\n\n - Decision tree (classical)\n - Simplified decision trees : that's my custom code for generating simple tree-based rules\n","93e1d93b":"## Train the random forest","c50df558":"## Part 1: Load and clean the data\n\nIn this section we will load the data from the csv file and check for any \"impurities\", such as null values or duplicate rows. If any of these will appear, we will remove them from the data set. We will also plot the correlations of the class column with all the other columns.","0ee26be7":"## Decision tree","25ed011e":"## Define parameters","3bfca0ff":"# Simplified decision trees generation for rule-based inference, with CART algorithm","6fc3527b":"Not bad ! With a set of 5 simple rules, we can predict about **20%** of test set data with **88%** precision  \nSo we can predict 20% of decisions without any machine learning deployment.","a957cbcb":"Precision of our prediction in test set if we change rule to the following, more human readable rule that could be determined with business people :  \nIF BILL_AMT1 <= 55000 AND PAY_AMT2 > 4500 : THEN class 0 is most probable one with gini impurity of 0.186","a275b564":"## Attribute Information\n\nBelow there are the description of the attributes that will be used in our model for better understanding of the data:\n\n- `LIMIT_BAL`: Amount of the given credit (NT dollar). It includes both the individual consumer credit and his\/her family (supplementary) credit.\n- `SEX`: Gender (1 = male; 2 = female).\n- `EDUCATION`: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n- `MARRIAGE`: Marital status (1 = married; 2 = single; 3 = others).\n- `AGE`: Age (year).\n- `PAY_1`: the repayment status in September, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_2`: the repayment status in August, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_3`: the repayment status in July, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_4`: the repayment status in June, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_5`: the repayment status in May, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_6`: the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `BILL_AMT1`: Amount of bill statement (NT dollar). Amount of bill statement in September, 2005.\n- `BILL_AMT2`: Amount of bill statement (NT dollar). Amount of bill statement in August, 2005.\n- `BILL_AMT3`: Amount of bill statement (NT dollar). Amount of bill statement in July, 2005.\n- `BILL_AMT4`: Amount of bill statement (NT dollar). Amount of bill statement in June, 2005.\n- `BILL_AMT5`: Amount of bill statement (NT dollar). Amount of bill statement in May, 2005.\n- `BILL_AMT6`: Amount of bill statement (NT dollar). Amount of bill statement in April, 2005.\n- `PAY_AMT1`: Amount of previous payment (NT dollar). Amount paid in September, 2005.\n- `PAY_AMT2`: Amount of previous payment (NT dollar). Amount paid in August, 2005.\n- `PAY_AMT3`: Amount of previous payment (NT dollar). Amount paid in July, 2005.\n- `PAY_AMT4`: Amount of previous payment (NT dollar). Amount paid in June, 2005.\n- `PAY_AMT5`: Amount of previous payment (NT dollar). Amount paid in May, 2005.\n- `PAY_AMT6`: Amount of previous payment (NT dollar). Amount paid in June, 2005.\n- `dpnm`: Default payment next month.(Yes = 1, No = 0)","0015437a":"# Conclusion","ddc62d97":"## Metrics for Decision tree","58d82df8":"We also check for null values.","775d45aa":"Below is the plot of the correlation matrix for the data set.","e0052047":"Note that this code is probably not optimal and could be improved in order to directly print out only rules path that lead to the low gini leave, in each tree","05765c43":"Decision tree based methods are a very powerful machine learning technique, particularly for tabular data.  \nThey can generate decision trees for you, based on your data, in an iterative fashion.  \nFor tabular data (meaning not images, not time series,not language... Only plain regular excel-like tables with columns, every line being independant from each other and containing several predictor variables), decision tree based machine learning algorithms are in fact performing at state of the art level (with algorithms such as XGBoost, LGBM).  \nThis scikit-learn page is interesting, to understand tree-based models : https:\/\/scikit-learn.org\/stable\/modules\/tree.html\n\nHowever, the most performing decision tree algorithms generate very complex trees that can be very, very deep, and you can have multiple trees based on residual errors of former ones. It is impossible for a human to understand such trees in a global way.  \n\nIn some cases, it is useful to have some human-interpretable models, in order to :\n- Check if you can trust the algorithm\n- Be able to justify your choices to regulators (typically financial regulators in credit risk problems)\n- Discover business rules from the data and use them to better approach your customers  \n- Decrease maintenance efforts by using simpler (but less performing), close to business ruled-based models that will be more robust to new kind of data than more complex (though more performing) machine learning models\n\nThere are many techniques for human-interpretability \/ explainability of machine learning models, including LIME or SHAPLEY : those techniques are interesting because they allow you to maintain your model complexity (which means keep your level of performance) and still explain what's happening.\nBut even with LIME or SHAPLEY it can still sometimes be difficult to explain some decisions, and you still have model maintenance \/ retraining issue.\n\nIn this notebook, we'll see a much more simple technique, based on DecisionTreeClassifier from scikit-learn (CART algorithm) + some additionnal code we wrote, in order to generate simple, human-understandable rules from data. To generate those rules though, we'll have to sacrifice performance (more precisely this means that our simple rule-based model will only be able to predict a subsample of instances, not all of them).\nWe want those rules to be simple (not more than 2 criterias), precise (as in good precision score). Exhaustivity and recall will not at all be our priority.\n\n\n","2d8e314e":"## Import libraries\/packages ","219a49db":"## Display only trees that respect simplicity constraints we defined in our parameters","204a9fdc":"Percentage of values in the set we predicted :","6d8ad3d4":"Now, we can visually inspect generated trees above : we'll see that each tree has at least one leave with a gini coefficient that is not higher than the maximum one we defined.  \nOur rules are all the set of rules that go from the top of a tree to its low-gini leave  \n\nFor example, on the first tree you have this rule :\n- IF  BILL_AMT1 <= 54416 AND PAY_AMT2 > 4553 :  THEN class 0 (no payment default) is most probable one with gini impurity of 0.184\n\nAnd on next trees, you have :\n- IF  BILL_AMT1 <= 54416 AND PAY_AMT1 > 4553 :  THEN class 0 is most probable one with gini impurity of 0.186\n- IF  BILL_AMT2 <= 4959.5 AND PAY_AMT1 > 4053.5 :  THEN class 0 is most probable one with gini impurity of 0.189\n- IF  BILL_AMT3 <= 49144.5 AND PAY_AMT1 > 4522.0 : THEN class 0 is mot probable one with gini impurity of 0.184\n- IF  BILL_AMT1 <= 54416 AND PAY_AMT5 > 4317.5 : THEN class 0 is most probable one with gini impurity of 0.199\n\nYou will see that many trees are redundant between themselves, with parameters changing only slightly (for example, for the first rule, you also have a tree with PAY_AMT2 > 4692.5 instead of 4553 : and indeed, why 4692.5 and why not 4553 ? \nThat's where decision trees sometimes provide over complex solutions where you could have less trees with same performance.  \nYou can talk to the business and come up with a figure that makes sense and then check how y is distributed in the test set.  \n\n","dc4f8344":"=> We see that it's mostly equivalent (even slightly better)","921b1976":"## Part 2: Pre-processing\n\nIn this part we prepare our data for our models. This means that we choose the columns that will be our independed variables and which column the class that we want to predict. Once we are done with that, we split our data into train and test sets and perfom a standardization upon them.","12530457":"## Visually inspect resulting trees and extract interesting rules","0a56d97d":"=> Note the precision of 0.671 with this technique : meaning that when the model predicts something, it's right 67% of the time.  \n=> That means it's wrong 33% of the time : is it acceptable ? You need to defined acceptable threshold with the business.","98e40f6a":"Since the `ID` column is for indexing purposes only, we remove it from the data set.","b1071fae":"Now let's build a simple model with all the rules we have determined so far :\n\nIF  (BILL_AMT1 <= 55000 AND PAY_AMT2 > 4500 )  \n   OR (BILL_AMT1 <= 54416 AND PAY_AMT1 > 4553)  \n   OR (BILL_AMT2 <= 4959.5 AND PAY_AMT1 > 4053.5)  \n   OR (BILL_AMT3 <= 49144.5 AND PAY_AMT1 > 4522.0)  \n   OR (BILL_AMT1 <= 54416 AND PAY_AMT5 > 4317.5)  \n   \nTHEN we predicted class 0  (= no payment default)  ","939c4a8a":"# Simplified decision trees","6dcb443a":"Thanks to this original notebook from which I took data preparation, description and DecisionTree calling code : https:\/\/www.kaggle.com\/mariosfish\/default-of-credit-card-clients-lr-dt-nn\n\nI just added my simple tree-based rules generation code (see \u00a7 Simplified decision trees)\n","d0b2ab68":"Distribution of y on test set for first rule","dcbcfd66":"## Models\n\nWe will create 2 models :\n- Decision tree  (classic one)\n- Simplified rules (that's my custom code that generates those rules) ","5e69096f":"Now we check for duplicate rows. If any, we remove them from the data set, since they provide only reduntant information.","1e201b88":"This was just a simple demonstration of how you can extract simple rules from data, using machine learning for modelisation, but without any machine learning for deployment.\nIn some cases, those techniques can be very useful to increase human explainability, and it will be much easier to justify those rules to regulators. It will also save ML model maintenance time.  \n\nNote that other powerful techniques can be used for interpretability such as LIME and SHAPLEY.  \nYou could still train a machine learning model on remaining 80% non-predicted decisions, with an hybrid approach.","f7fdcd99":"Now we'll use random forest to generate multiple, simple trees, each one of which will give us a set of rules to predict payment default.  \n\nBut we'll constrain random forest with particular hyper parameters :\n\n- A very short maximum depth to keep it simple\n- No bootstrap in order to search the whole dataset for each tree\n\nThen we'll have some code to explore all resulted trees, and retain only the folling ones :\n- A maximum gini impurity coefficient (https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#Gini_impurity) of tree leaves : the lowest it is, the lowest risk you'll have of an incorrect classification. Here you can try several MAX_GINI_TODISPLAY parameters and see the lowest possible you can get while still having results.\n- A minimum percentage of samples concerned by the tree : the idea is that you want rules that can apply to a minimum percentage of instances, in order to be useful"}}