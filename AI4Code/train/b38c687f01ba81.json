{"cell_type":{"d46b8f17":"code","f0f5856c":"code","7c24f84e":"code","66b064d6":"code","ac3e5be2":"code","da901eae":"code","b75ee116":"code","1d5234ac":"code","32161064":"code","fe6f0167":"code","55b85a1f":"code","49e5d947":"code","18e1eb45":"code","02141ab5":"code","f80abde7":"markdown"},"source":{"d46b8f17":"# ==================\n# Library\n# ==================\nimport warnings\nwarnings.simplefilter('ignore')\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport sys\nimport pickle\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom torch.optim import lr_scheduler\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn import TransformerEncoder","f0f5856c":"# ==========================\n# Constant\n# ==========================\nTRAIN_PATH = \"..\/input\/data-science-spring-osaka-2021\/train.csv\"\nTEST_PATH = \"..\/input\/data-science-spring-osaka-2021\/test.csv\"\nACTION_PATH = \"..\/input\/data-science-spring-osaka-2021\/actions.csv\"\nSUB_PATH =\"..\/input\/data-science-spring-osaka-2021\/sample_submission.csv\"\nTRAIN_SEQ_PATH = \"..\/input\/fe001-make-feature\/fe001_train_seq.npy\"\nTEST_SEQ_PATH = \"..\/input\/fe001-make-feature\/fe001_test_seq.npy\"","7c24f84e":"# ==========================\n# Settings\n# ==========================\nex = \"002\"\nSEED = 0\nN_SPLITS = 5\nSHUFFLE = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 32\nEPOCH = 20","66b064d6":"# ==========================\n# Function\n# ==========================\n# ====================\n# Function\n# ====================\ndef process_data(data_seq):\n    # attention\u306emask\n    mask = data_seq[:,0] == 0\n    return {\n        'input_data_seq': data_seq,\n        \"mask\":mask\n    }\n\nclass DSPO_Dataset(Dataset):\n    \n    def __init__(self, data_seq, train = True, y = None):\n        self.data_seq = data_seq\n        self.train = train\n        self.y = y\n    \n    def __len__(self):\n        return len(self.data_seq)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.data_seq[item],\n            \n        )\n\n        # Return the processed data where the lists are converted to `torch.tensor`s\n        if self.train : \n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              'mask': torch.tensor(data[\"mask\"], dtype=torch.bool),  \n              \"y\":torch.tensor(self.y[item], dtype=torch.float32)\n               }\n        else:\n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              'mask': torch.tensor(data[\"mask\"], dtype=torch.bool), \n               }\n        \nclass Transformer_model(nn.Module):\n    def __init__(\n        self, dropout=0.2, con_size = 20, linear_emb1 = 240, dim_feedforward = 720, linear_emb2 = 100, nhead=4):\n        super(Transformer_model, self).__init__()\n        self.linear1 = nn.Sequential(\n            nn.Linear(con_size , linear_emb1),\n            nn.LayerNorm(linear_emb1 ),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.transformer_encoder = TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=linear_emb1, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n                                                      num_layers=2)\n        \n        # dense\n        self.linear2 = nn.Sequential(\n            nn.Linear(linear_emb1, linear_emb2),\n            nn.LayerNorm(linear_emb2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.linear_final = nn.Linear(linear_emb2,12)\n\n    def forward(self, data_seq, mask):\n        data_seq = self.linear1(data_seq)\n        # pytorch\u306etransformer encoder\u306einput\u306eshape\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\n        data_seq = data_seq.permute(1, 0, 2).contiguous()\n        output = self.transformer_encoder(data_seq, src_key_padding_mask=mask)\n        output = output.permute(1, 0, 2).contiguous()\n        output = torch.mean(output, 1)\n        output = self.linear2(output)\n        output = self.linear_final(output)\n        return output\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef sigmoid(value):\n    return 1 \/ (1 + np.exp(-value))","ac3e5be2":"# ==========================\n# Main\n# ==========================\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\ntrain_seq = np.load(TRAIN_SEQ_PATH)\ntest_seq = np.load(TEST_SEQ_PATH)\naction = pd.read_csv(ACTION_PATH)\nsub = pd.read_csv(SUB_PATH)","da901eae":"action_to_num_dict = {}\nnum_to_action_dict = {}\n\nfor n, i in enumerate(action[\"action_seq\"]):\n    action_to_num_dict[i] = n\n    num_to_action_dict[n] = i","b75ee116":"train[\"action_seq_num\"] = train[\"action_seq\"].map(action_to_num_dict)","1d5234ac":"train[\"action_seq_num\"].value_counts().sort_index()","32161064":"target = train[\"action_seq_num\"].values\ntarget = np.eye(len(action))[target]","fe6f0167":"y_oof = np.empty([len(train),12])\ntest_preds = np.empty([len(test),12])\ntest_ = DSPO_Dataset(test_seq, train = False, y = None)\ntest_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\nkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=SHUFFLE,random_state=SEED)\nseed_everything(SEED)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_seq,y = train[\"action_seq_num\"])):\n    x_train_seq = train_seq[train_idx]\n    y_train = target[train_idx]\n    x_val_seq = train_seq[valid_idx]\n    y_val = target[valid_idx]\n    train_ = DSPO_Dataset(x_train_seq, train = True, y = y_train)\n    val_ = DSPO_Dataset(x_val_seq,train = True, y = y_val)\n    train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True , num_workers=2)\n    val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n    model = Transformer_model()\n    model = model.to(device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,\n                      lr=1e-3,\n                      weight_decay=0.1,\n                      )\n    num_train_optimization_steps = int(len(train_loader) * EPOCH)\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=5,\n                                                num_training_steps=num_train_optimization_steps)\n    \n    criterion = nn.BCEWithLogitsLoss()\n    best_val = None\n    for epoch in tqdm(range(EPOCH)):\n        model.train() \n        train_losses_batch = []\n        val_losses_batch = []\n        epoch_loss = 0\n\n        # ==========================\n        # train\n        # ==========================\n        for d in train_loader:\n\n            # =========================\n            # data loader\n            # =========================\n\n            input_data_seq = d['input_data_seq']\n            mask = d[\"mask\"]\n            y = d[\"y\"]\n            input_data_seq = input_data_seq.to(device)\n            mask = mask.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n\n            output = model(input_data_seq,mask)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses_batch.append(loss.item())\n\n        train_loss = np.mean(train_losses_batch)\n        \n        # ==========================\n        # eval\n        # ==========================\n        model.eval()  # switch model to the evaluation mode\n        val_preds = np.ndarray((0,12))\n        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n            # Predicting on validation set\n            for d in val_loader:\n                # =========================\n                # data loader\n                # =========================\n                input_data_seq = d['input_data_seq']\n                mask = d[\"mask\"]\n                y = d[\"y\"]\n                input_data_seq = input_data_seq.to(device)\n                mask = mask.to(device)\n                y = y.to(device)\n                output = model(input_data_seq,mask)\n\n                loss = criterion(output, y)\n                val_preds = np.concatenate([val_preds, output.detach().cpu().numpy()], axis=0)\n                val_losses_batch.append(loss.item())\n\n\n        val_loss = np.mean(val_losses_batch)\n        acc = accuracy_score(np.argmax(y_val,axis=1), np.argmax(val_preds,axis=1))\n        print(epoch, \"train loss:\", train_loss, \"val loss:\",val_loss, \"val acc:\",acc)\n        \n        if not best_val:\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            continue\n\n        if val_loss <= best_val:\n            best_epoch = epoch\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            \n    print(f\"{fold}_best_poch:{best_epoch},best_val:{best_val}, best_acc:{best_acc}\")\n    \n    # ===================================\n    # test\n    # ===================================\n    model = Transformer_model()\n    model.load_state_dict(torch.load(f\"ex{ex}_{fold}.pth\"))\n    model.to(device)\n    model.eval()\n    test_preds_ = np.ndarray((0,12))\n    with torch.no_grad():  # Do not calculate gradient since we are only predicting\n        # Predicting on test set\n        for d in test_loader:\n            # =========================\n            # data loader\n            # =========================\n            input_data_seq = d['input_data_seq']\n            mask = d[\"mask\"]\n            input_data_seq = input_data_seq.to(device)\n            mask = mask.to(device)\n            output = model(input_data_seq,mask)\n\n            test_preds_ = np.concatenate([test_preds_, output.detach().cpu().numpy()], axis=0)\n\n    #torch.save(best_model.state_dict(), f\"..\/ex\/ex{ex}\/ex{ex}_{b}_{fold}.pth\")  # Saving the model\n    test_preds += test_preds_ \/  N_SPLITS\nacc = accuracy_score(train[\"action_seq_num\"].values,np.argmax(y_oof,axis=1))\nprint(f\"cv:{acc}\")\nnp.save(f\"ex{ex}_test_pred.npy\",test_preds)\nnp.save(f\"ex{ex}_oof.npy\",y_oof)","55b85a1f":"test_preds_max = sigmoid(np.sort(test_preds, axis=1)[:, -1:])\ntest_preds_label = np.argmax(test_preds, axis=1)","49e5d947":"test_preds_label_list = []\nfor l,m in zip(test_preds_label,test_preds_max):\n    if m > 0.5:\n        test_preds_label_list.append(num_to_action_dict[l])\n    else:\n        # unseen\n        test_preds_label_list.append(num_to_action_dict[6])","18e1eb45":"sub[\"action_seq\"] = test_preds_label_list\nsub.to_csv(f\"ex{ex}.csv\",index=False)","02141ab5":"sub[\"action_seq\"].value_counts()","f80abde7":"### Transformer\u3092\u3064\u304b\u3063\u305fBaseLine\u3067\u3059\u3002\u53c2\u8003\u307e\u3067\u306b\u5171\u6709\u3057\u307e\u3059\u3002"}}