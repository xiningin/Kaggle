{"cell_type":{"01648a80":"code","505a843d":"code","ba175498":"code","1143d47c":"code","357d2db6":"code","e271fca0":"code","06958f89":"code","90226bd2":"code","0bef6784":"code","25bd31a4":"code","debf7adb":"code","790d6bac":"code","0703eea4":"code","345a5477":"code","3d625bfe":"code","72bb8976":"code","42617c7c":"code","4bfcd1e1":"code","9ae48b53":"code","3a7aacf3":"code","c2830488":"code","cdc2463d":"code","5d58259b":"code","37c457ec":"code","ef35c454":"code","848a1c3c":"code","82a89b77":"code","8be164d8":"code","08279c09":"markdown","18367410":"markdown","4dc3ac94":"markdown","dfce68b2":"markdown","b6c8e86a":"markdown","b9994e6c":"markdown","6e19a775":"markdown","91a27287":"markdown","6c3cc3e5":"markdown","68088f7a":"markdown","3ee95188":"markdown","b84e9cb8":"markdown","ee323802":"markdown"},"source":{"01648a80":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n                    \nimport os\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score, precision_score, recall_score\n\n\nfrom keras.layers import Input\nfrom keras import Model\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nstemmer=SnowballStemmer('english')\nlemma=WordNetLemmatizer()\nfrom string import punctuation\n\nimport re\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt","505a843d":"print(os.listdir(\"..\/input\"))","ba175498":"df = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","1143d47c":"print(\"Columns :\", df.columns)\nprint(\"Row 0 :\")\nprint(\"qid :\", df.iloc[0]['qid'])\nprint(\"question_text : \", df.iloc[0]['question_text'])\nprint(\"target :\", df.iloc[0]['target'])","357d2db6":"n_rows = len(df)\nn_insincere = sum(df['target'])\nprint(n_rows)\nprint(n_insincere)\n\nlabel_repart = pd.DataFrame(data={\"\" :[n_rows - n_insincere, n_insincere]}, index = [str(n_rows - n_insincere) + ' sincere questions', str(n_insincere) + ' insincere question'])\nlabel_repart.plot(kind='pie', title='Labels repartition, ratio ' + str(round(n_insincere \/ n_rows,2)*100) + \"%\", subplots=True, figsize=(8,8))","e271fca0":"insincere_question = df[df['target'] == 1]['question_text'].values\nfor i in range(10):\n    print(insincere_question[i])","06958f89":"def clean_review(review_col):\n    review_corpus=[]\n    stops = set(stopwords.words(\"english\"))\n    for i in range(0,len(review_col)):\n        review=str(review_col[i])\n        review=re.sub('[^a-zA-Z]',' ',review)\n        word_token = word_tokenize(str(review).lower())\n        #review = [word for word in word_token if word not in stops]\n        #review=' '.join(review)\n        review=[lemma.lemmatize(w) for w in word_token if w not in stops]\n        review=' '.join(review)\n        review_corpus.append(review)\n        #if i % 1000 == 0:\n           #print(i\/len(review_col)) \n    return review_corpus","90226bd2":"df['clean_question']=clean_review(df['question_text'].values)","0bef6784":"df_test['clean_question']=clean_review(df_test['question_text'].values)","25bd31a4":"#all_words=' '.join(df['clean_question'].values)\n#all_words=word_tokenize(all_words)\n#dist=FreqDist(all_words)\n#num_unique_word=len(dist)\nnum_unique_word = 166289","debf7adb":"df.head()","790d6bac":"#r_len=[]\n#for text in df['clean_question'].values:\n#    word=word_tokenize(text)\n#    l=len(word)\n#    r_len.append(l)    \n#MAX_QUESTION_LEN=np.max(r_len)\nMAX_QUESTION_LEN=125","0703eea4":"MAX_FEATURES = num_unique_word\nMAX_WORDS = MAX_QUESTION_LEN","345a5477":"y_train = df['target'].values\nX_train_text = df['clean_question'].values\nX_test_text = df_test['clean_question'].values\nprint(X_train_text.shape,y_train.shape)\nprint(X_test_text.shape)","3d625bfe":"X_train_text, X_val_text, y_train, y_val = train_test_split(X_train_text, y_train, test_size=0.1)","72bb8976":"tokenizer = Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(list(X_train_text))\nX_train = tokenizer.texts_to_sequences(X_train_text)\nX_val = tokenizer.texts_to_sequences(X_val_text)\nX_test = tokenizer.texts_to_sequences(X_test_text)","42617c7c":"X_train = sequence.pad_sequences(X_train, maxlen=MAX_WORDS)\nX_val = sequence.pad_sequences(X_val, maxlen=MAX_WORDS)\nX_test = sequence.pad_sequences(X_test, maxlen=MAX_WORDS)\nprint(X_train.shape,X_val.shape)","4bfcd1e1":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n    \ndef get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n    # word vectors\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    # embedding matrix\n    word_index = tokenizer.word_index\n    num_words = min(max_features, len(word_index) + 1)\n    all_embs = np.stack(embeddings_index.values()) #for random init\n    embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    max_features = embedding_matrix.shape[0]\n    \n    return embedding_matrix\n","9ae48b53":"gloveEmbed = get_embed_mat('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt', MAX_FEATURES, 300)","3a7aacf3":"word_index = tokenizer.word_index\nembedding_layer = Embedding(len(word_index) + 1,\n                            300,\n                            weights=[gloveEmbed],\n                            input_length=MAX_WORDS,\n                            trainable=False)","c2830488":"def line_search_f1_score(y_score, y_test):\n    max_f1_score = 0\n    opt_threshold = 0\n    for threshold in [i*0.01 for i in range(100)]:\n        y_preds = y_score > threshold\n        score = f1_score(y_preds, y_test)\n        if max_f1_score < score:\n            max_f1_score = score\n            opt_threshold = threshold\n    return max_f1_score, opt_threshold","cdc2463d":"class Metrics(Callback):\n    def __init__(self):\n        self.best_threshold = 0.5\n        self.best_f1_score = 0\n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n        self.best_f1_score = 0\n    def on_epoch_end(self, epoch, logs={}):\n         idx = np.random.randint(0,self.validation_data[0].shape[0],1000)\n         val_predict = (np.asarray(self.model.predict(self.validation_data[0][idx], verbose=1))).round()\n         val_targ = self.validation_data[1][idx]\n         #_val_f1 = f1_score(val_targ, val_predict)\n         _val_f1, threshold = line_search_f1_score(val_targ, val_predict)\n         if _val_f1 > self.best_f1_score:\n                self.best_f1_score = _val_f1\n         self.best_threshold = threshold\n         _val_recall = recall_score(val_targ, val_predict)\n         _val_precision = precision_score(val_targ, val_predict)\n         self.val_f1s.append(_val_f1)\n         self.val_recalls.append(_val_recall)\n         self.val_precisions.append(_val_precision)\n         print(\" \u2014 val_f1: %f \u2014 val_precision: %f \u2014 val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n         return\n \nmetric = Metrics()","5d58259b":"lstm_out = 200\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","37c457ec":"model.fit(X_train, y_train, validation_data=(X_val, y_val),\n          epochs=2, batch_size=1024, verbose=1,callbacks=[metric])","ef35c454":"y_score_test = model.predict(X_val, verbose=1)\nmax_f1_score, threshold = line_search_f1_score(y_score_test, y_val)","848a1c3c":"y_sub = model.predict(X_test, verbose = 1)","82a89b77":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.prediction = np.array(y_sub > threshold, dtype=int) \nsub.to_csv(\"submission.csv\", index=False)","8be164d8":"# Best f1_score on validation dataset :\nprint(threshold)\nprint(max_f1_score)","08279c09":"# Submission","18367410":"# Dependencies","4dc3ac94":"## Questions exploration","dfce68b2":"The dataset contains only 3 variables : \n1. an id to identify the question\n2. a question\n3. a label telling whether the question is insincere or not.","b6c8e86a":"## Model : LSTM","b9994e6c":"## Text Preprocessing","6e19a775":"## Preparing input data","91a27287":"# Loading data","6c3cc3e5":"# Directory content","68088f7a":"**Good news** the dataset is not so imbalanced","3ee95188":"<center><h1> Simple LSTM that does the job<\/h1><\/center> ","b84e9cb8":"# Features Insights","ee323802":"## GloVe embedding layers\n\nGloVe is an algorithm that enables to represent word as vector of semantic features. Refer to https:\/\/nlp.stanford.edu\/projects\/glove\/ for more information."}}