{"cell_type":{"65944c73":"code","45a431cd":"code","2b9b985f":"code","a47c7c54":"code","1c7028aa":"code","cffdbdb6":"code","5da5fb11":"code","dfcb4a3b":"code","a7980269":"code","31620d8a":"code","6010f091":"code","72d8405c":"code","d46447ec":"code","97a25010":"code","75e078c6":"code","62a2b11d":"code","59a9b659":"code","994ac4bf":"code","f54800f9":"code","aba9f5fa":"code","d27a3162":"code","93016491":"code","0452bbb9":"code","854db048":"code","2649cdc7":"code","a61e0a8a":"code","fcbb521f":"code","f2ab143a":"code","234a4b65":"code","e561bf81":"code","536ad5da":"code","687a97a5":"code","a1865972":"code","b0bc2c32":"code","3ffbf4de":"code","833fc557":"code","c044ca71":"code","06b34094":"code","26bc0db5":"code","b098b0c4":"code","c5e4b1b1":"code","c4108741":"code","bbe26d47":"code","f5b1d29e":"code","0c0a5b4a":"code","1416e7c5":"code","811312bb":"code","f7acf652":"code","01d54cb5":"code","39b8b8c2":"code","8f88d364":"code","d8b60172":"code","ebc7c201":"code","f23cdf0d":"code","d39a77a7":"code","3670d81f":"code","c14b1535":"code","4cc7d895":"code","1201e627":"code","03bfafa8":"code","b3b45102":"code","558850bf":"markdown","7705d96d":"markdown","7db69381":"markdown","4d1528c1":"markdown","c1e3733f":"markdown","8af3245e":"markdown","e5f00426":"markdown","01653cb6":"markdown","735747b1":"markdown","0be8fc60":"markdown"},"source":{"65944c73":"# Importing basic data manipulation libraies\n\nimport numpy as np\nimport pandas as pd","45a431cd":"# Filtering warning so that they do not appear on the notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')","2b9b985f":"# Data Paths\n\ntrain_data_path = '..\/input\/tabular-playground-series-jan-2022\/train.csv'\ntest_data_path = '..\/input\/tabular-playground-series-jan-2022\/test.csv'","a47c7c54":"# Loading Train Data\n\ntrain_df = pd.read_csv(train_data_path)\ntrain_df.head()","1c7028aa":"# Loading Test Data\n\ntest_df = pd.read_csv(test_data_path)\ntest_df.head()","cffdbdb6":"# splitting the feature into understandable format\n\ntrain_df['year'] = train_df['date'].apply(lambda x : int(x.split('-')[0]))\ntrain_df['month'] = train_df['date'].apply(lambda x : int(x.split('-')[1]))\ntrain_df['day'] = train_df['date'].apply(lambda x : int(x.split('-')[2]))\ntrain_df.drop('date', 1, inplace = True)\ntrain_df.head()","5da5fb11":"test_df['year'] = test_df['date'].apply(lambda x : int(x.split('-')[0]))\ntest_df['month'] = test_df['date'].apply(lambda x : int(x.split('-')[1]))\ntest_df['day'] = test_df['date'].apply(lambda x : int(x.split('-')[2]))\ntest_df.drop('date', 1, inplace = True)\ntest_df.head()","dfcb4a3b":"train_df.info()","a7980269":"test_df.head()","31620d8a":"train_df.country.value_counts()","6010f091":"test_df.country.value_counts()","72d8405c":"train_df.store.value_counts()","d46447ec":"test_df.store.value_counts()","97a25010":"train_df.year.value_counts()","75e078c6":"test_df.year.value_counts()","62a2b11d":"train_df.month.value_counts()","59a9b659":"test_df.month.value_counts()","994ac4bf":"train_df.day.value_counts()","f54800f9":"test_df.day.value_counts()","aba9f5fa":"train_df['product'].value_counts()","d27a3162":"test_df['product'].value_counts()","93016491":"import matplotlib.pyplot as plt\nimport seaborn as sns","0452bbb9":"# Checking the number of sold products by their respective year.\n\nsns.lineplot(x = 'year', y = 'num_sold', hue = 'product', data = train_df)\nplt.show()","854db048":"# Checking the number of sold products by their respective month.\n\nsns.lineplot(x = 'month', y = 'num_sold', hue = 'year', data = train_df)\nplt.show()","2649cdc7":"# Checking the number of sold products per month through years.\n\nplt.figure(figsize = (20, 6))\nsns.barplot(x = 'month', y = 'num_sold', hue = 'year', data = train_df)\nplt.show()","a61e0a8a":"# Checking the number of sold products per mdate through years.\n\nplt.figure(figsize = (20, 6))\nsns.barplot(x = 'day', y = 'num_sold', hue = 'year', data = train_df)\nplt.show()","fcbb521f":"plt.figure(figsize = (20, 6))\nsns.lineplot(x = 'year', y = 'num_sold', hue = 'day', data = train_df)\nplt.show()","f2ab143a":"train_df.head()","234a4b65":"from sklearn.preprocessing import LabelEncoder","e561bf81":"# One Hot Encoding\n\ntrain = train_df.copy()\ntest = test_df.copy()\n\n\nfor feature in test.columns:\n    if test[feature].dtype == 'object':\n        train_feature_data = train[feature]\n        train_encoded_feature = pd.get_dummies(train_feature_data)\n        train.drop(feature, 1, inplace = True)\n        train = pd.concat([train, train_encoded_feature], axis = 1)\n        \n        test_feature_data = test[feature]\n        test_encoded_feature = pd.get_dummies(test_feature_data)\n        test.drop(feature, 1, inplace = True)\n        test = pd.concat([test, test_encoded_feature], axis = 1)","536ad5da":"train.head()","687a97a5":"test.head()","a1865972":"train.year = train.year.apply(lambda x : x - 2015)\ntest.year = test.year.apply(lambda x : x - 2015)","b0bc2c32":"train.head()","3ffbf4de":"test.head()","833fc557":"from xgboost import XGBRegressor as xgbr","c044ca71":"baseline_model = xgbr()\nprint(baseline_model)","06b34094":"target = train.num_sold\ntrain.drop('num_sold', 1 , inplace = True)\ntrain.head()","26bc0db5":"plt.hist(target)\nplt.show()","b098b0c4":"from sklearn.model_selection import train_test_split","c5e4b1b1":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state = 42)","c4108741":"X_train.head()","bbe26d47":"X_val.head()","f5b1d29e":"%%time\nbaseline_model.fit(X_train, y_train)","0c0a5b4a":"baseline_model.score(X_train,y_train)","1416e7c5":"baseline_model.score(X_val, y_val)","811312bb":"submission_format = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\nsubmission_format.head()","f7acf652":"submission_data = submission_format\nsubmission_data.head()","01d54cb5":"from sklearn.ensemble import RandomForestRegressor as rfr","39b8b8c2":"rfr_model = rfr(n_estimators = 80)\nprint(rfr_model)","8f88d364":"%%time\nrfr_model.fit(X_train, y_train)","d8b60172":"rfr_model.score(X_train, y_train)","ebc7c201":"rfr_model.score(X_val, y_val)","f23cdf0d":"plt.bar(X_train.columns, rfr_model.feature_importances_)\nplt.xticks(rotation = 60)\nplt.show()","d39a77a7":"X_train.drop(['row_id', 'year', 'Finland', \"Sweden\"], axis = 1, inplace = True)\nX_val.drop(['row_id', 'year', 'Finland', \"Sweden\"], axis = 1, inplace = True)\ntest.drop(['row_id', 'year', 'Finland', \"Sweden\"], axis = 1, inplace = True)","3670d81f":"revised_xgb_model = xgbr()\nrevised_rfr_model = rfr(n_estimators = 80)","c14b1535":"%%time\nrevised_xgb_model.fit(X_train, y_train)\nrevised_rfr_model.fit(X_train, y_train)","4cc7d895":"print(revised_xgb_model.score(X_train, y_train))\nprint(revised_xgb_model.score(X_val, y_val))\nxgb_pred = revised_xgb_model.predict(test)\nprint(revised_rfr_model.score(X_train, y_train))\nprint(revised_rfr_model.score(X_val, y_val))\nrfr_pred = revised_rfr_model.predict(test)","1201e627":"submission_data.num_sold = rfr_pred\n# Pushing the data into a csv file for submission.\nsubmission_data.to_csv('rfr.csv', index = False)","03bfafa8":"submission_data.num_sold = xgb_pred\n# Pushing the data into a csv file for submission.\nsubmission_data.to_csv('xgb.csv', index = False)","b3b45102":"submission_data.num_sold = rfr_pred * 0.5 + xgb_pred * 0.5\n# Pushing the data into a csv file for submission.\nsubmission_data.to_csv('blend.csv', index = False)","558850bf":"As we've checked the feature, now it's time to encode the categorical features. In this sceanrio, the categorical features have a uniform distribution of less common unqiue labels, that's why I've used one hot encoding which can be a great tool to prepare the data more accurately.","7705d96d":"# Thanks for walking through this notebok :)\n\n## You can find other cool works of me on [kaggle](https:\/\/kaggle.com\/sagnik1511) or on [github](https:\/\/github.com\/sagnik1511)\n\n# Thank You :)","7db69381":"#### Checking the data splitt through various category \/ features.","4d1528c1":"We can see num_sold increase slightly with year.","c1e3733f":"# Fast Walkthrough : TPS JAN 2k22\n\nThis notebook holds a short and brief approach in submitting solutions for TPS competitions.\n\nDo **upvote** if you like it.","8af3245e":"We also have to prepare the train and the validation set so that we can understand the overfit quality of the model and wheter it should be discarded or not.","e5f00426":"Now, scaling the year as if not it may give this feature too much importance.","01653cb6":"### EDA : Exploratory Data Analysis\n\nNow checking through some inter-feature covariance .","735747b1":"Now we'll blend the outputs from both the models and use that as our target prediction.","0be8fc60":"Now, in this scenario we'll use 2 models which will be Regression models from XGB and RandomForest. and we'll blend the outputs of them to find the answers or the predictions."}}