{"cell_type":{"65ac61e2":"code","986c341e":"code","1cec5b53":"code","9aae80c7":"code","8a7ec48c":"code","4c07209b":"code","e43c2e73":"code","0fac9d2d":"code","2755e153":"code","db9a49a2":"code","3ace74f1":"code","4622efe8":"code","7f63ab2b":"code","b42ff761":"code","9eea9c21":"code","f7d0d58f":"code","c1d0a19d":"code","3dada35d":"markdown","13a46f07":"markdown","1911c112":"markdown","f29cedd1":"markdown","302e0bba":"markdown","12dd7c50":"markdown","e266851d":"markdown","11937582":"markdown"},"source":{"65ac61e2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","986c341e":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","1cec5b53":"import missingno as msno\n\nmsno.matrix(train)","9aae80c7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns='SalePrice'), train.SalePrice)","8a7ec48c":"msno.matrix(X_train.select_dtypes(np.number))","4c07209b":"msno.matrix(X_train.select_dtypes(exclude=np.number))","e43c2e73":"# For numeric variables\n# LotFrontage: 0\n# GarageYrBlt: 0 (remember this can change to -1 depending on the perfomance)\n# MasVnrArea: 0\n\n# For category variables\n# Alley: No\n# MasVnrType: No\n# BsmtQual: No\n# BsmtCond: No\n# BsmtExposure: Unf\n# BsmtFinType1: No\n# BsmtFinSF1: No\n# BsmtFinType2: No\n# BsmtFinSF2: No\n# BstmUnfSF: No\n# TotalBsmtSF: No\n# BsmtFullBath: No\n# BsmtHalfBath: No\n# Electrical: Dropping the full record\n# FireplaceQU: No\n# GarageType: No\n# GarageFinish: No\n# GarageCars: No\n# GarageArea: No\n# GarageQual: No\n# GarageCond: No\n# PoolQC: No (with a posibility of dropping the column)\n# Fence: No (with a posibility of droping the column)\n# MiscFeature: No\n\nX_train = X_train.assign(\n    LotFrontage = lambda df: df.LotFrontage.fillna(0),\n    GarageYrBlt = lambda df: df.GarageYrBlt.fillna(0),\n#     GarageYrBlt_alt = lambda df: df.GarageYrBlt.fillna(-1), # this is my alternative strategy\n    MasVnrArea = lambda df: df.MasVnrArea.fillna(0),\n    Alley = lambda df: df.Alley.fillna('No'),\n    MasVnrType = lambda df: df.MasVnrType.fillna('No'),\n    BsmtQual = lambda df: df.BsmtQual.fillna('No'),\n    BsmtCond = lambda df: df.BsmtCond.fillna('No'),\n    BsmtExposure = lambda df: df.apply(lambda ts: 'Unf' if pd.notnull(ts.BsmtQual) else 'No',axis=1), # Remember there is a missing value that may confuse the model, so this needs an special treatment\n    BsmtFinType1 = lambda df: df.BsmtFinType1.fillna('No'),\n#     BsmtFinSF1 =   lambda df: df.BsmtFinSF1.fillna('No'),\n    BsmtFinType2 = lambda df: df.BsmtFinType2.fillna('No'),\n#     BsmtFinSF2 =   lambda df: df.BsmtFinSF2.fillna('No'),\n#     BsmtUnfSF =    lambda df: df.BsmtUnfSF.fillna('No'),\n#     TotalBsmtSF =  lambda df: df.TotalBsmtSF.fillna('No'),\n#     BsmtFullBath = lambda df: df.BsmtFullBath.fillna('No'),\n#     BsmtHalfBath = lambda df: df.BsmtHalfBath.fillna('No'),\n    Electrical = lambda df: df.Electrical.fillna('-1'), # Droping it\n    FireplaceQu = lambda df: df.FireplaceQu.fillna('No'),\n    GarageType   = lambda df: df.GarageType.fillna('No'),\n    GarageFinish = lambda df: df.GarageFinish.fillna('No'),\n#     GarageCars   = lambda df: df.GarageCars.fillna('No'),\n#     GarageArea   = lambda df: df.GarageArea.fillna('No'),\n    GarageQual   = lambda df: df.GarageQual.fillna('No'),\n    GarageCond   = lambda df: df.GarageCond.fillna('No'),\n    PoolQC = lambda df: df.PoolQC.fillna('No'), # with possiblity of dropping\n    Fence = lambda df: df.Fence.fillna('No'), # with possiblity of dropping\n    MiscFeature = lambda df: df.MiscFeature.fillna('No')\n)","0fac9d2d":"# Exclude the empty `Electrical` record\nX_train = X_train.query(\"Electrical != '-1'\")","2755e153":"_,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(X_train.select_dtypes(np.number).corr(), ax=ax)","db9a49a2":"!pip install feature_engine","3ace74f1":"from feature_engine.encoding import MeanEncoder\n\n_,ax = plt.subplots(figsize=(10,10))\n\ncategorical = X_train.select_dtypes(exclude=np.number).columns.tolist()\nnumeric = X_train.select_dtypes(np.number).columns.tolist()\n\nmean_encoder = MeanEncoder(variables=X_train.select_dtypes(exclude=np.number).columns.tolist(), ignore_format=True).fit(X_train, y_train)\nsns.heatmap(mean_encoder.transform(X_train).filter(categorical).corr(), ax=ax)","4622efe8":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n_,ax = plt.subplots(2,2,figsize=(10,10))\n\npipe1 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('scaler',StandardScaler()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0))\n])\n\npipe2 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('scaler',Normalizer()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0))\n])\n\npipe3 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('scaler',MinMaxScaler()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0))\n])\n\npipe4 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('scaler',PowerTransformer()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0))\n])\n\npipes = {'standard-scaler':pipe1,'normalizer':pipe2,'minmax':pipe3,'power-transformer':pipe4}\n\ndiscretizer = KBinsDiscretizer(\n    n_bins=5, \n    encode='ordinal', \n    strategy='kmeans'\n)\n\ny_train2 = discretizer.fit_transform(\n    y_train[X_train.index].values.reshape(-1,1)\n)\n\ni,j=0,0\nfor key, pipe in pipes.items():\n    xpca = pipe.fit_transform(X_train, y_train)\n    sns.scatterplot(x=xpca[:,0], y=xpca[:,1], hue=y_train2[:,0], ax=ax[i,j])\n    ax[i,j].set_title(key)\n    i+=1\n    if i == 2:\n        j+=1\n        i = 0","7f63ab2b":"X_test = X_test.assign(\n    LotFrontage = lambda df: df.LotFrontage.fillna(0),\n    GarageYrBlt = lambda df: df.GarageYrBlt.fillna(0),\n#     GarageYrBlt_alt = lambda df: df.GarageYrBlt.fillna(-1), # this is my alternative strategy\n    MasVnrArea = lambda df: df.MasVnrArea.fillna(0),\n    Alley = lambda df: df.Alley.fillna('No'),\n    MasVnrType = lambda df: df.MasVnrType.fillna('No'),\n    BsmtQual = lambda df: df.BsmtQual.fillna('No'),\n    BsmtCond = lambda df: df.BsmtCond.fillna('No'),\n    BsmtExposure = lambda df: df.apply(lambda ts: 'Unf' if pd.notnull(ts.BsmtQual) else 'No',axis=1), # Remember there is a missing value that may confuse the model, so this needs an special treatment\n    BsmtFinType1 = lambda df: df.BsmtFinType1.fillna('No'),\n    BsmtFinSF1 =   lambda df: df.BsmtFinSF1.fillna('No'),\n    BsmtFinType2 = lambda df: df.BsmtFinType2.fillna('No'),\n    BsmtFinSF2 =   lambda df: df.BsmtFinSF2.fillna('No'),\n    BsmtUnfSF =    lambda df: df.BsmtUnfSF.fillna('No'),\n    TotalBsmtSF =  lambda df: df.TotalBsmtSF.fillna('No'),\n    BsmtFullBath = lambda df: df.BsmtFullBath.fillna('No'),\n    BsmtHalfBath = lambda df: df.BsmtHalfBath.fillna('No'),\n    Electrical = lambda df: df.Electrical.fillna('-1'), # Droping it\n    FireplaceQu = lambda df: df.FireplaceQu.fillna('No'),\n    GarageType   = lambda df: df.GarageType.fillna('No'),\n    GarageFinish = lambda df: df.GarageFinish.fillna('No'),\n    GarageCars   = lambda df: df.GarageCars.fillna('No'),\n    GarageArea   = lambda df: df.GarageArea.fillna('No'),\n    GarageQual   = lambda df: df.GarageQual.fillna('No'),\n    GarageCond   = lambda df: df.GarageCond.fillna('No'),\n    PoolQC = lambda df: df.PoolQC.fillna('No'), # with possiblity of dropping\n    Fence = lambda df: df.Fence.fillna('No'), # with possiblity of dropping\n    MiscFeature = lambda df: df.MiscFeature.fillna('No')\n)","b42ff761":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nimport warnings\n\nwarnings.simplefilter('ignore')\n\nX_train = X_train.drop(columns=['Id'])\nX_test = X_test.drop(columns=['Id'])\n\ncategorical = X_train.select_dtypes(exclude=np.number).columns.tolist()\nnumeric = X_train.select_dtypes(np.number).columns.tolist()\n\npipe1 = Pipeline([\n    ('numeric-imputer', SklearnTransformerWrapper(SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"), categorical)),\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n    ('scaler',StandardScaler()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0)),\n    ('regressor', RandomForestRegressor(n_estimators=100, max_depth=2, random_state=0))\n])\n\npipe2 = Pipeline([\n    ('numeric-imputer', SklearnTransformerWrapper(SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"), categorical)),\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n    ('scaler',MinMaxScaler()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0)),\n    ('regressor', RandomForestRegressor(n_estimators=100, max_depth=2, random_state=0))\n])\n\npipe3 = Pipeline([\n    ('numeric-imputer', SklearnTransformerWrapper(SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"), categorical)),\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n    ('scaler',PowerTransformer()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0)),\n    ('regressor', RandomForestRegressor(n_estimators=100, max_depth=2, random_state=0))\n])\n\npipes = {'standard-scaler':pipe1,'minmax':pipe2,'power-transformer':pipe3}\n\nfor key, pipe in pipes.items():\n    pipe.fit(X_train, y_train[X_train.index])\n    train_score = pipe.score(X_train, y_train[X_train.index])\n    test_score = pipe.score(X_test, y_test[X_test.index])\n    print(f\"Training score {key}: {train_score}\")\n    print(f\"Test score {key}: {test_score}\")","9eea9c21":"from sklearn.linear_model import SGDRegressor\n\npipe4 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n    ('scaler',StandardScaler()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0)),\n    ('regressor', SGDRegressor(loss='huber', max_iter=1000, tol=1e-3, random_state=0))\n])\n\npipe5 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n    ('scaler',MinMaxScaler()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0)),\n    ('regressor', SGDRegressor(loss='huber', max_iter=1000, tol=1e-3, random_state=0))\n])\n\npipe6 = Pipeline([\n    ('categorical-encoding',MeanEncoder(variables=categorical, ignore_format=True)),\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n    ('scaler',PowerTransformer()),\n    ('pca',PCA(n_components=2, svd_solver='randomized', random_state=0)),\n    ('regressor', SGDRegressor(loss='huber', max_iter=1000, tol=1e-3, random_state=0))\n])\n\npipes = {'standard-scaler':pipe4,'minmax':pipe5,'power-transformer':pipe6}\n\nfor key, pipe in pipes.items():\n    pipe.fit(X_train, y_train[X_train.index])\n    train_score = pipe.score(X_train, y_train[X_train.index])\n    test_score = pipe.score(X_test, y_test[X_test.index])\n    print(f\"Training score {key}: {train_score}\")\n    print(f\"Test score {key}: {test_score}\")","f7d0d58f":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest = test.assign(\n    LotFrontage = lambda df: df.LotFrontage.fillna(0),\n    GarageYrBlt = lambda df: df.GarageYrBlt.fillna(0),\n#     GarageYrBlt_alt = lambda df: df.GarageYrBlt.fillna(-1), # this is my alternative strategy\n    MasVnrArea = lambda df: df.MasVnrArea.fillna(0),\n    Alley = lambda df: df.Alley.fillna('No'),\n    MasVnrType = lambda df: df.MasVnrType.fillna('No'),\n    BsmtQual = lambda df: df.BsmtQual.fillna('No'),\n    BsmtCond = lambda df: df.BsmtCond.fillna('No'),\n    BsmtExposure = lambda df: df.apply(lambda ts: 'Unf' if pd.notnull(ts.BsmtQual) else 'No',axis=1), # Remember there is a missing value that may confuse the model, so this needs an special treatment\n    BsmtFinType1 = lambda df: df.BsmtFinType1.fillna('No'),\n#     BsmtFinSF1 =   lambda df: df.BsmtFinSF1.fillna('No'),\n    BsmtFinType2 = lambda df: df.BsmtFinType2.fillna('No'),\n#     BsmtFinSF2 =   lambda df: df.BsmtFinSF2.fillna('No'),\n#     BsmtUnfSF =    lambda df: df.BsmtUnfSF.fillna('No'),\n#     TotalBsmtSF =  lambda df: df.TotalBsmtSF.fillna('No'),\n#     BsmtFullBath = lambda df: df.BsmtFullBath.fillna('No'),\n#     BsmtHalfBath = lambda df: df.BsmtHalfBath.fillna('No'),\n#     Electrical = lambda df: df.Electrical.fillna('-1'), # Droping it\n    FireplaceQu = lambda df: df.FireplaceQu.fillna('No'),\n    GarageType   = lambda df: df.GarageType.fillna('No'),\n    GarageFinish = lambda df: df.GarageFinish.fillna('No'),\n#     GarageCars   = lambda df: df.GarageCars.fillna('No'),\n#     GarageArea   = lambda df: df.GarageArea.fillna('No'),\n    GarageQual   = lambda df: df.GarageQual.fillna('No'),\n    GarageCond   = lambda df: df.GarageCond.fillna('No'),\n    PoolQC = lambda df: df.PoolQC.fillna('No'), # with possiblity of dropping\n    Fence = lambda df: df.Fence.fillna('No'), # with possiblity of dropping\n    MiscFeature = lambda df: df.MiscFeature.fillna('No'),\n    \n#     MSZoning = lambda df: df.MSZoning.fillna('nan'),\n#     Utilities = lambda df: df.Utilities.fillna('nan'),\n#     Exterior1st = lambda df: df.Exterior1st.fillna('nan'),\n#     Exterior2nd = lambda df: df.Exterior2nd.fillna('nan'),\n#     KitchenQual = lambda df: df.KitchenQual.fillna('nan'),\n#     Functional = lambda df: df.Functional.fillna('nan'),\n#     SaleType = lambda df: df.SaleType.fillna('nan')\n)","c1d0a19d":"pd.concat(\n    [test[[\"Id\"]], pd.DataFrame(pipe1.predict(test.drop(columns=\"Id\")), \n                                columns=['SalePrice'])], \n    axis=1\n).set_index(\n    \"Id\"\n).to_csv(\n    \"\/kaggle\/working\/submission.csv\"\n)","3dada35d":"# EDA\n\n## Filling the blanks\n\n### Missing values for numeric variables\n\n#### LotFrontage\n\n`LotFrontage` have a considerable amount of missing values, according to the data definition:\n\n>LotFrontage: Linear feet of street connected to property\n\nUnder my understanding, I'm supposing the minimum feets of connection must be 0, so I'm replacing `NaN`s with 0.\n\n#### MasVnrArea\n\n>MasVnrArea: Masonry veneer area in square feet\n\nThere is just one missing value (under the train dataset), and I see the minimum value is 0, I let it be a 0, then.\n\n#### GarageYrtBlt\n\n>GarageYrBlt: Year garage was built\n\nMaybe, there are not garages at all in this spaces, to be able to perform a proper imputation strategy, the use of `GarageType`  and `GarageFinish` will be required. This provide a bit of context to know if there is or there isn't a garage.\n\nFiltering the values that have `GarageYrBlt` empty, we can see that the other two variable are empty also. Meaning we can set a default value for no garage instead of `NaN` (remember, `NaN`s are not easy to be interpretable by a machine).\n\nI'll we testing two options and see which performs the best\n\n1. Set as 0\n2. Set as -1\n\n### Missing values for categorical variables\n\n#### Alley\n\n>Alley: Type of alley access to property\n\n`NaN`s mean there are no alley access, so a default values in this case will be required.\n\nI'm setting `No` as default\n\n#### MasVnrType\n\n>MasVnrType: Masonry veneer type\n\nThere is a specific value for no masonry, as previously said, I'm defining a default value. `No` will be enough :)\n\n#### Bsmt-like variables\n\n>Bsmt variables refer to the basement characteristics\n\nAs almost all categorical variables, `None` means no existance. Again, a `No` will be sufficient... But wait! Looking at the matrix of missingness, just one record do not match with the rest (you can see all white lines match each other, except by one in BsmtExposure). \n\nThe definition said that:\n\n>BsmtExposure: Refers to walkout or garden level walls\n\n>No: No Exposure\n\n>NA: No Basement\n\nIf we use the all-known `No` options, but it will have two meanings: no exposure or no basement. In the other hand, the data said that the missing value is because it's unfinished, so I'll leave `Unf` in this kind of cases.\n\n#### Electrical\n\n>Electrical: Electrical system\n\nOne record... again!\n\nDocumentation said nothing for this cases, so I'm dropping it by now.\n\n#### FireplaceQu\n\n>FireplaceQu: Fireplace quality\n\nIn this case, the definition said `NaN` stand for `No Fireplace`, I'm leaving the ol'reliable `No`.\n\n#### Garage-like variables\n\n>Garage-like variable refer to garage characteristics\n\nI can't see a case like the basement, so a `No` will be enough.\n\n#### PoolQC\n\n>PoolQC: Pool quality\n\nThere are a lot of missing values because not all of us have a pool in our house. Dropping this variable depends on how well this variable performs agains the target variable.\n\n#### Fence\n\n>Fence: Fence quality\n\nSurely, there are more fences than pools, but again, not all of us have a fence. The dropping or inputing strategy depends on how this variable performs.\n\n#### MiscFeature\n\n>MiscFeature: Miscellaneous feature not covered in other categories\n\nSurely lot of house cover their characteristics in the categories, so the `NaN`s will be imputed by a `No`. Then we can evaluate leaving it or not depending on their performance.\n\n\nFew! Lot of work and we still not approach a machine learning model, not even close!!!\n\nDon't worry, this is part of the job (I'm motivating myself, by the way)","13a46f07":"# Imputation strategies","1911c112":"## Model evaluation\n\nIn this section we evaluate `RandomForestRegressor` and `SGDRegressor` using 3 different pipelines.\n\nThe results show that `RandomForestRegressor` with `StandardScalar` returns the best score in training (0.778) and testing (0.767), so we will work further in this line to improve this score.\n\nMeanwhile, I submite results and the score is ...","f29cedd1":"# Splitting data\n\nSplit train and test sets","302e0bba":"# Evaluating model in ","12dd7c50":"## Variables interaction\n\n### Numerical interactions\n\nLet's see first how the independent variables interact each other, if you're a veteran on data analysis you may guest why, if you're not let me explain myself.\n\nWhen we perform variable analysis, we used to look for details that affects our model during predictions. One of many problems is indepedent variable correlation or `Multicollinearity` the problem with this is very well explained by Yannis Vassiliadis in [this post](https:\/\/stats.stackexchange.com\/q\/288250). But put it in a nutshell\n\n>The problem is that the model doesn't know where to focused on, and the importance of the variables are hard to determine\n\n    This is too simplistic but it works to explain the problem, if you're still curious please check the  post and give it an upvote.\n\nNow, the way to determine if we have a multicollinearity problem, just perform a pearson correlation test betweent the independent variables. In this case we have the following correlations:\n\n1. `1stFlrSF` (First Floor square feet) and `TotalBsmtSF` (Total square feet of basement area) (0.830635): Seems that the houses with basement measure the same than the first floor.\n2. `TotRmsAbvGrd` (Total rooms above grade) and `GrLivArea` (Above grade (ground) living area square feet) (0.820784): There might be a proportion betweent the number of rooms above the ground and the total area of the living.\n3. `GarageArea` (Size of garage in square feet) and `GarageCars` (Size of garage in car capacity) (0.880344): Another proportion, this time betweent the number of cars and the area of the garage.\n4. `GarageYrBlt` and `GarageYrBlt_alt` (1.0): Remember we're testing the best value\n\nThis problem is solved in two possible ways:\n\n- Drop one of those columns causing redundancy\n- Standarize\n\nThere is also [authors](https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/) that considers multicollinearity non a problem if you don't need to give an interpretation of the coefficients in a linear regression problema and suggests not to remove it but standarize it.\n\nWe will also measure the impact on the regression using the two approaches and give a conclusion about it.","e266851d":"# First model iteration\n\nAt this point, we just explore all variables (a bit) to determine best strategy for imputations and transformation. Now it's time to start working with variable selection and model evaluation.\n\nI'll be doing both at the same time, giving a brief space to explaing what we are doing.\n\n## Variables visualization\n\nLet's start by performing a dimensionality reduction to see how variables look like in two dimension, using `SalePrice` as size. The pipeline will be the following:\n\n1. Do `MeanEncoder` over categorical variables.\n2. Standarize all variables: In this step, we'll be exploring four different types of algorithms\n    - StandardScaler\n    - Normalizer\n    - MinMaxScaler\n    - PowerTransformer, method yeo-johnson\n3. Fit a PCA model.\n\nIn order to keep a clean code, this step is going to be done inside a sklearn pipeline.\n\nThe results show `PowerTransformer`, `MinMaxScaler` and `StandardScaler` perform well for visualizing data. Selecting the best pipeline will be done by evaluating a regression model.","11937582":"### Categorical interactions\n\nIn comparision from numeric variables, categoricals need a numeric transformation. In this case there are many possible ways to approch this problem.\n\nOne of them would be doing a one hot encoding, the downside: if there are a lot of unique the complexity problem will increase drastically.\n\nAnother alternative is doing a frequency convertion, like frequency encoding, or mean encoding, the downside: model could overfit easier.\n\n> Under my experience, I suggests using a frequency like encoding. One hot is ok when you have few categorical columns, not the mayority of the data.\n\nTo perform the mean encoding I'll be using [feature_engine](https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html) library.\n\nAs results shows, there are the following correlations:\n\n1. `BstmExposure` with all variables: The `BstmExposure` said it is the walkout or garden level basement walls, I don't guess why is has a strong correlation with all other variables.\n2. `Exterior2nd` with `Exterior1st`: Description said this is referred to the material covering the house's exterior. Maybe, in some cases, the first material must need a specific second material.\n3. `GarageQual` with `GarageCond`: Looks like the garage condition and quality are redundant, there are the same number of features and the exact same unique values.\n4. `SaleType` with `SaleCondition`: More redundancy (?) contrary with the previous comparision, in this case there are not the exact same values, but at least they are correlated.\n\nAnyway, we will be applying the two techniques describes above:\n\n1. Dropping\n2. Standarizing\n\nJust to see how well both perform."}}