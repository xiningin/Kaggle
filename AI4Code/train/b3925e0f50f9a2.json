{"cell_type":{"1af9d9e5":"code","dade2fff":"code","aa02803c":"code","f79b27c2":"code","ce33162a":"code","9c6c08ab":"code","8d64374e":"code","da7f4063":"code","4148161c":"code","f3907fd1":"code","85a32d39":"code","c6180260":"code","61062e89":"code","9051f8cd":"code","6baee6d5":"code","d24a0759":"code","0cb4dcb0":"code","f7cae47e":"code","463fd3d6":"code","4793319c":"code","b8333ffd":"code","6e27ced0":"code","2326e571":"code","4e9bab9a":"code","0429bb07":"code","d1cee7ae":"code","e468728a":"code","a0ef28f2":"code","3704f4a5":"code","827be91a":"code","609f9b77":"code","5bdc9151":"code","47a01576":"code","29583ea0":"code","fd2c885d":"markdown","ef9cee2b":"markdown","c9ecaf9e":"markdown","abbb57a3":"markdown","8f212569":"markdown","bfa2be24":"markdown","4f2a333e":"markdown","4da6bc40":"markdown","7b92e612":"markdown","e0566641":"markdown","54732b12":"markdown","8f1b1f39":"markdown","90dfde0c":"markdown","2983235a":"markdown","5d953ba4":"markdown","7c800f45":"markdown","a62ec3a2":"markdown","48504cee":"markdown","315af12f":"markdown","68a8fadf":"markdown","6b998045":"markdown","336a443c":"markdown","509c43c5":"markdown"},"source":{"1af9d9e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dade2fff":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set()\n","aa02803c":"df=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","f79b27c2":"df.head()","ce33162a":"df.info()","9c6c08ab":"df.describe()","8d64374e":"df.isnull().sum()","da7f4063":"dfnew=df\ndfnew['Age'].fillna(dfnew['Age'].mean(), inplace = True)\ndfnew.isnull().sum()","4148161c":"dfnew4=dfnew.drop(['PassengerId','Survived','Name','Ticket','Cabin'],axis=1)\ny=dfnew.Survived\nprint(dfnew4)","f3907fd1":"test.head()","85a32d39":"test.info()","c6180260":"test.isnull().sum()","61062e89":"test1=test\ntestnew=test1['Age'].fillna(test1['Age'].mean(), inplace = True)\ntestnew=test1['Fare'].fillna(test1['Fare'].mean(), inplace = True)\ntest1.isnull().sum()","9051f8cd":"testfix=test1.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\ntestfix","6baee6d5":"print(dfnew4.Embarked.unique())\nprint(testfix.Embarked.unique())","d24a0759":"train_objs_num = len(dfnew4)\ndataset = pd.concat(objs=[dfnew4, testfix], axis=0)\ndataset = pd.get_dummies(dataset,drop_first=True)\ntrain1 = dataset[:train_objs_num]\ntest1 = dataset[train_objs_num:]\nprint(train1.columns)\nprint(test1.columns)","0cb4dcb0":"sns.countplot(x=dfnew.Survived,hue=\"Pclass\",data=dfnew)\nplt.title(\"no of passenger survived\")","f7cae47e":"sns.countplot(x=dfnew.Survived,hue=\"Sex\",data=dfnew)","463fd3d6":"a=dfnew[dfnew.Sex=='female']['Survived']\nfemale_survive_percent=sum(a==1)*100\/len(a)\nfemale_survive_percent","4793319c":"b=dfnew[dfnew.Sex=='male']['Survived']\nmale_survive_percent=sum(b==1)*100\/len(b)\nmale_survive_percent","b8333ffd":"a=dfnew[dfnew.Age<=18]\na\nsns.countplot(x=a.Survived,hue=\"Sex\",data=a)","6e27ced0":"dfnew.groupby('Pclass')['Age'].mean()","2326e571":"a=dfnew.corr()\nsns.heatmap(a,annot=True,cmap=\"RdYlGn\")","4e9bab9a":"from sklearn.ensemble import ExtraTreesRegressor\nmodel=ExtraTreesRegressor()\nmodel.fit(train1,y)","0429bb07":"model.feature_importances_\ncheck=pd.Series(model.feature_importances_,index=train1.columns)\ncheck.nlargest(6).plot(kind='barh')\nplt.show()","d1cee7ae":"from sklearn.model_selection import train_test_split\n\nX_train, y_train, X_test ,y_test = train_test_split(train1,y,test_size=0.2,random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","e468728a":"def models(X_train,X_test):\n    from sklearn import preprocessing\n    from sklearn import utils\n    lab_enc = preprocessing.LabelEncoder()\n    training_scores_encoded = lab_enc.fit_transform(X_test)\n    print(training_scores_encoded)\n    print(utils.multiclass.type_of_target(X_test))\n    print(utils.multiclass.type_of_target(X_test.astype('int')))\n    print(utils.multiclass.type_of_target(training_scores_encoded))\n    from sklearn.linear_model import LogisticRegression\n    log=LogisticRegression(random_state=0)\n    log.fit(X_train,training_scores_encoded)\n    \n    from sklearn.neighbors import KNeighborsClassifier\n    knn=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n    knn.fit(X_train,training_scores_encoded)\n    \n    from sklearn.svm import SVC\n    svc_lin=SVC(kernel='linear',random_state=0)\n    svc_lin.fit(X_train,training_scores_encoded)\n    \n    from sklearn.svm import SVC\n    svc_rbf=SVC(kernel='rbf',random_state=0)\n    svc_rbf.fit(X_train,training_scores_encoded)\n    \n    from sklearn.naive_bayes import GaussianNB\n    gauss=GaussianNB()\n    gauss.fit(X_train,training_scores_encoded)\n    \n    from sklearn.tree import DecisionTreeClassifier\n    tree=DecisionTreeClassifier(criterion='entropy',random_state=0)\n    tree.fit(X_train,training_scores_encoded)\n    \n    from sklearn.ensemble import RandomForestClassifier\n    forest=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\n    forest.fit(X_train,training_scores_encoded)\n    \n    print('[0] logistic regression training accuracy:  ', log.score(X_train,training_scores_encoded))\n    print('[1] k regression training accuracy:  ', knn.score(X_train,training_scores_encoded))\n    print('[2] svc linear regression training accuracy:  ', svc_lin.score(X_train,training_scores_encoded))\n    print('[3] svc rbf regression training accuracy:  ', svc_rbf.score(X_train,training_scores_encoded))\n    print('[4] gaussian regression training accuracy:  ', gauss.score(X_train,training_scores_encoded))\n    print('[5] decision regression training accuracy:  ', tree.score(X_train,training_scores_encoded))\n    print('[6] randomforest regression training accuracy:  ', forest.score(X_train,training_scores_encoded))\n    \n    \n    return log,knn,svc_lin,svc_rbf,gauss,tree,forest\n\n\nmodel=models(X_train,X_test)","a0ef28f2":"import numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nfor i in range(len(model)):\n    cm=confusion_matrix(y_test,model[i].predict(y_train))\n    TN,FP,FN,TP=confusion_matrix(y_test,model[i].predict(y_train)).ravel()\n    test_score=(TP+TN)\/(TP+TN+FN+FP)\n    print(cm)\n    print('Model[{}]  testing accuracy=\"{}\"' .format(i,test_score))\n    \n    \n    print()\n    ","3704f4a5":"precision=TP\/(TP+FP)\nprecision","827be91a":"recall=TP\/(TP+FN)\nrecall","609f9b77":"F1_score=2*precision*recall\/(precision+recall)\nF1_score","5bdc9151":"from sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nparameters = {\n                  'n_estimators' : [400,500,600,700,800,900,1000],\n                  'max_depth'    : [5,5.5,6,6.5,7,7.5,8],\n                'criterion':['gini', 'entropy'],\n                 'max_features':['auto', 'sqrt', 'log2']\n                 }\ngrid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1,verbose=1)\ngrid.fit(train1,y)\n\n# Results from Grid Search\nprint(\"\\n========================================================\")\nprint(\" Results from Grid Search \" )\nprint(\"========================================================\")\nprint(\"\\n The best estimator across ALL searched params:\\n\",\n          grid.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\",\n          grid.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\",\n          grid.best_params_)\nprint(\"\\n ========================================================\")","47a01576":"forest=RandomForestClassifier(criterion= 'entropy', max_depth= 7, max_features= 'sqrt', n_estimators= 1000)\na=forest.fit(train1,y)\nb=a.predict(test1)\nb","29583ea0":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': b})\noutput.to_csv('submission1234.csv', index=False)","fd2c885d":" Now we will read the datasets(train and test) in the csv format...\n ","ef9cee2b":"NOW lets get into data analysis","c9ecaf9e":"so over here decision tree algorithm is showing highest accuracy......but lets check with repect to y_test","abbb57a3":"Naturally after reading dataset each one is keen to see the dataset ,its feature,the types of features like series etc....so we would like to go with the same ...and also will check if any NAN values are found and vl have a glimpse at the statistical values of dataset ","8f212569":"\"THE BAD NEWS IS TIME FLIES .THE GOOD NEWS IS YOU ARE THE PILOT\"-Michael altshuler....same is the DATA,if we use in right way it can lead us to next level where every decision is logical and for a good purpose at large.\n\nSO LET's BEGIN WITH OUR \"TITANIC DATASET\"\n\n1. BRIEF INTRODUCTION\n>  THIS is a kernel where machine learning has been put to play along with bit of feature engineering,data analysis,model creation .THE data provided is in 2 datasets -1st dataset(train.csv) contains the input varibles and also the output variable,whereas the 2nd dataset(test.csv) contains only input varibles for which the output(0 or 1 in this case,where 1 represents survived and 0 represents sadly no more ..) is to be predicted ,which we have in the form of our submission.csv file \n\nSO what's the FLOW?\n\nimporting all libraries---->reading data ---->feature information and handling missing values--->data analysis---->feature importance---->model selection---->model creation ---->predicting for test datatest\n\nWe will first start the code by importing all the standard libraries which are needed.\n","bfa2be24":"There seems a declining trend","4f2a333e":"SO now we have a small challenge to compete with, i.e if u have a look at the unique values of Embarked column i train and test dataset,u will find that train dataset contains a additional unique values .so if we simply individually put one hot encoding to both we may have or rather will have column mismatch .so what we will do is concat the 2 dataset ,encode it and then separate it","4da6bc40":"NAN values are present in 3 features i.e Age,Cabin,Embarked","7b92e612":"SO with repect to y_test we get accuracy highest for logistic regression and with X_test we get highest for decision tree....so in such a situtation i think Randomforest will give better trade off between underfitting  and overfitting  .....BUT IT DOES NOT WORK ALWAYS","e0566641":" This clearly tells us that among those who lost their lifes Pclass 3 was the most underpriveleged  to get help and amongst those who survived ,pclass1 people were in majority as compared to others..","54732b12":"This is a collinear graph which tells us the relation between different features ","8f1b1f39":"     Over here we get to know....................#####   AS PER THIS DATASET(train.csv) \n        -less than 50% people survived  \n        -people in titanic-891.......\n        -class were-1,2,3.\n        -In case of age right from below 1 yr child  to  80 yr grandfather or grandmother were travelling \n        -talking about Fare it ranged from  free to 512 ","90dfde0c":"OFFcourse i can just wipe out the rows which contain NAN values in train dataset ,but it would hamper my model accuracy ...","2983235a":"So in case of those below 18 also the pattern was same as preference was given to female \n\n\nNext just for intuition we will see the mean age of passengers with respect to each Pclass","5d953ba4":"NOW ...WE COME TO 1 OF THE MOST IMPORTANT QUESTION i.e WHICH MODEL TO CHOOSE.....so lets get into it and check all models ","7c800f45":"NOW lets split the data with setting some of the parameters externally instead of by default settings","a62ec3a2":"so now we have wiped out the Age NAN value by replacing it with mean value of Age and  have further droped unwanted columns\n....\n\nNOW LET's do the same process for test.csv(2 nd dataset)","48504cee":"SO we have removed the unwanted parameters or features and have turned the Sex feature which was of male and female value to 1 and 0 respectively...\n\n\nNOW lets move to feature importance","315af12f":"SO the values are pretty descent enough ...so we will choose the model as RandomForestClassifier.So in order to increase the accuracy we will tune the hyperparameters of the model ...then with this we will move on to our end goal of predicting values for 2 nd dataset ","68a8fadf":"GRAPH speaks all what is required..","6b998045":"so now we have wiped out the Age NAN value by replacing it with mean value of Age and Fare and further we will drop the unwanted columns","336a443c":"This gives us intuition that there was a sex bias where more help was provided to females as in the ones who survived females have a majority, but in one's who lost lifes male are in majority.....\n\nNow this was graphical model to interpret the bias ,lets calculate statistically ","509c43c5":"SO where every 1 female lost her life ,almost equal to 4 male lost lifes...as can be clearly seen from statistics where % of women survived is 74.20 and at the same time % of male survived were just 18.89 \n     \n Heyy what if there is difference in pattern in survival with respect to sex  amongst those who survived below 18 yrs"}}