{"cell_type":{"203f739d":"code","98d8fc6a":"code","dda84614":"code","32c70a42":"code","9375c4c7":"code","ca56c8cd":"code","e81d95ee":"code","86f1841a":"code","4579d1bd":"code","2c9de75f":"code","158fd15c":"code","1b223fa0":"code","0209ab27":"code","c97b34c8":"code","619671c7":"code","710d9ec0":"code","72b5b4db":"code","f60a809d":"code","7bfbb3ea":"code","67b914c2":"code","d11ab358":"code","97c89cca":"code","9ba28b8b":"code","dc8a5ce2":"code","de129d47":"code","1cd39a73":"code","9cde5efe":"code","947f9934":"markdown","c6d01063":"markdown","25ec8971":"markdown","5853e490":"markdown","f86c5557":"markdown","c55aa984":"markdown","ce738952":"markdown","ef1666c0":"markdown"},"source":{"203f739d":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom catboost import Pool, CatBoostRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","98d8fc6a":"DATA_DIR='\/kaggle\/input\/ulaanbaatar-city-air-pollution-prediction\/'\ntrain=pd.read_csv(f'{DATA_DIR}\/pm_train.csv')\ntest=pd.read_csv(f'{DATA_DIR}\/pm_test.csv')\nweather=pd.read_csv(f'{DATA_DIR}\/weather.csv',index_col=0)\nsub=pd.read_csv(f'{DATA_DIR}\/sample_submission.csv')","dda84614":"train.head(3)","32c70a42":"# \u0426\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u043c\u044d\u0434\u044d\u044d\u043b\u044d\u043b \u0431\u0430\u0439\u0445\u0433\u04af\u0439 \u0431\u0430\u0439\u0433\u0430\u0430 \u0442\u0443\u043b \u0434\u0443\u0442\u0443\u0443 \u0431\u0430\u0439\u0433\u0430\u0430 \u043e\u0433\u043d\u043e\u043e\u043d\u0443\u0443\u0434\u044b\u0433 \u043d\u044d\u043c\u044d\u0445\ndef weather_hour_add(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df= df.set_index('date').asfreq('1H').reset_index()\n    cols = df.columns\n    df[cols] = df[cols]\n    df['date']=df['date'].astype('str')\n    return df","9375c4c7":"# \u0426\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u043c\u044d\u0434\u044d\u044d\u043b\u044d\u043b \u0431\u0430\u0439\u0445\u0433\u04af\u0439 \u0445\u044d\u0433\u0438\u0439\u0433 \u043d\u04e9\u0445\u04e9\u0445 \u0431\u043e\u043b\u043e\u043d \u043e\u0433\u043d\u043e\u043e\u043d\u044b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0443\u0443\u0434\u044b\u0433 \u043d\u044d\u043c\u044d\u0445\ndef make_fill_date(df):\n    \n    for col in tqdm(df.select_dtypes(exclude='object').columns):\n        df['roll_mean'] = df[col].rolling(6).mean().shift(1)\n        df[col]= df[col].fillna(df['roll_mean'])  \n        \n    df['date1']=pd.to_datetime(df['date'])\n    df['dayofyear'] = df['date1'].dt.dayofyear\n    df['year']=df['date1'].dt.year\n    df['month'] = df['date1'].dt.month\n    df['hour'] = df['date1'].dt.hour\n    df['summary']= df['summary'].transform(lambda v: v.ffill())\n    df['icon']= df['icon'].transform(lambda v: v.bfill())\n    del df['roll_mean']\n    del df['date1']\n    del df['precipIntensity'],df['precipProbability'] # feature importance=0\n    print(df.shape)\n    df=df.fillna(0)\n    return df","ca56c8cd":"# \u041b\u0430\u0433 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0443\u0443\u0434\u044b\u0433 \u043d\u044d\u043c\u044d\u0445\ndef make_lag(df):\n    \n    df['date1']=pd.to_datetime(df['date'])\n    df['hour_shift2']=df['date1'].dt.hour.shift(2)\n    df['hour_shift4']=df['date1'].dt.hour.shift(4)  \n    df['windBearing_shift2']=df['windBearing'].shift(2)\n    df['temperature_shift2']=df['temperature'].shift(2)\n    del df['date1']\n    print(df.shape)\n    return df","e81d95ee":"def cyclic_encode(df):\n    df['hour_sin'] = np.sin(df.hour*(2.*np.pi\/24))\n    df['hour_cos'] = np.cos(df.hour*(2.*np.pi\/24))\n    df['month_sin'] = np.sin((df.month-1)*(2.*np.pi\/12))\n    df['month_cos'] = np.cos((df.month-1)*(2.*np.pi\/12))\n    print(df.shape)\n    return df","86f1841a":"#\u0426\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u043c\u044d\u0434\u044d\u044d\u044d\u043b\u043b\u0438\u0439\u043d \u0448\u0438\u043b\u0436\u0438\u0445 \u0434\u0443\u043d\u0434\u0430\u0436 \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u043d\u044d\u043c\u044d\u0445 \ndef make_rolling_mean(df, cols, window):\n    for col in tqdm(cols):\n        df[f'{col}_rmean_{window}'] = df[f'{col}'].rolling(window).mean()\n    return df","4579d1bd":"#\u0426\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u043c\u044d\u0434\u044d\u044d\u044d\u043b\u043b\u0438\u0439\u043d \u0448\u0438\u043b\u0436\u0438\u0445 \u0434\u0443\u043d\u0434\u0430\u0436\u0438\u0439\u043d \u043b\u0430\u0433 \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u043d\u044d\u043c\u044d\u0445 \ndef make_rolling_mean_lag(df,cols,mean,lag):\n    for col in tqdm(cols):\n        df[f'{col}_rmean_{mean}_lag_{lag}'] = df[f'{col}'].rolling(mean).mean().shift(lag)\n    return df","2c9de75f":"weather = weather_hour_add(weather)\nweather = make_fill_date(weather)\nweather = make_lag(weather)\nweather = cyclic_encode(weather)","158fd15c":"weather=make_rolling_mean(weather ,cols=['windSpeed','windBearing','visibility'],window=4)\nweather=make_rolling_mean(weather ,cols=['windSpeed','windBearing','visibility','humidity','dayofyear','uvIndex'],window=8)\nweather=make_rolling_mean(weather ,cols=['windSpeed','windBearing','visibility','humidity','uvIndex'],window=12)","1b223fa0":"weather=make_rolling_mean_lag(weather,cols=['windSpeed','windBearing','uvIndex','visibility','temperature','humidity'], mean=4, lag=2) #ma4lag2                                 #ma4lag4\nweather=make_rolling_mean_lag(weather,cols=['uvIndex','windSpeed','humidity'], mean=6, lag=3)                                          #ma6lag3\nweather=make_rolling_mean_lag(weather,cols=['uvIndex','humidity'],mean=8,lag=4)                                                        #ma8lag4\nweather=make_rolling_mean_lag(weather,cols=['windSpeed','dayofyear','humidity'],mean=12,lag=2)                                         #ma12lag2\nweather=make_rolling_mean_lag(weather,cols=['uvIndex','visibility','humidity','cloudCover'],mean=12,lag=6)                             #ma12lag6\nweather=weather.fillna(0)","0209ab27":"train = train.merge(weather,on=['date'], how='left')\ntest = test.merge(weather, on=['date'], how='left')\ndf=train.append(test)\ndf.shape","c97b34c8":"#aqi-\u044b\u043d \u043b\u0430\u0433 \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u043d\u044d\u043c\u044d\u0445\ndef lag_aqi(df):\n    for i in tqdm([1]):\n        df['date']=pd.to_datetime(df['date'])\n        df['lag_date']= df['date']+ pd.DateOffset(years=i)\n        data=df[['lag_date','type','station','aqi']]\n        data = data.rename(columns={'lag_date': 'date','aqi': \"aqi_lag{}\".format(i)})\n        data = data.drop_duplicates(subset=['date','type','station'], keep=\"first\")\n        df=df.merge(data, on=['date','type','station'],how='left')\n        del df['lag_date']\n        del data\n    return df","619671c7":"#aqi-\u044b\u043d \u043b\u0430\u0433-1 \u0434\u044d\u044d\u0440\u0445 \u0448\u0438\u043b\u0436\u0438\u0445 \u0434\u0443\u043d\u0434\u0430\u0436 \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u043e\u0440\u0443\u0443\u043b\u0430\u0445\ndef aqi_roll_mean(df):\n    for i in tqdm([4,12]):      \n        list=[]\n        df['date']=pd.to_datetime(df['date'])\n        time=3600*i\n        list=df.groupby(['station','type']).rolling('{}s'.format(time), on='date').aqi.mean()\n        roll_mean=pd.DataFrame(list)\n        roll_mean.reset_index(inplace=True)\n        roll_mean=roll_mean.rename(columns={'aqi': 'aqi_lag_rmean{}'.format(i)})\n        roll_mean['date']=roll_mean['date']+ pd.DateOffset(years=1)\n        roll_mean = roll_mean.drop_duplicates(subset=['date','type','station'], keep=\"first\")\n        df=df.merge(roll_mean,on=['station','type','date'],how='left')\n        del roll_mean\n        del list\n    return df","710d9ec0":"df=lag_aqi(df)\ndf=aqi_roll_mean(df)","72b5b4db":"le = preprocessing.LabelEncoder()\ncols=['type', 'source', 'station', 'summary', 'icon']\ndf[cols] = df[cols].apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')","f60a809d":"df=df.drop(['latitude', 'longitude','summary', 'icon','source','uvIndex','station','month'],1)\ntrain = df[:train.shape[0]]\ntest = df[-test.shape[0]:]","7bfbb3ea":"#Valid set-\u0438\u0439\u0433 \u0442\u0435\u0441\u0442 \u0434\u0430\u0442\u0430 \u0434\u044d\u044d\u0440 'Aqi'\u0443\u0442\u0433\u04e9 \u04e9\u0433\u04e9\u0434\u0441\u04e9\u043d 1053 \u0442\u04af\u04af\u0432\u0440\u044d\u044d\u0440 \u0442\u04e9\u043b\u04e9\u04e9\u043b\u04af\u04af\u043b\u044d\u043d \u0430\u0432\u0430\u0432\nvalid=test.query('aqi>0')\nsub_add=valid[['ID','aqi']]\nvalid.shape","67b914c2":"#Aqi-\u044b\u043d \u0442\u0430\u0440\u0445\u0430\u043b\u0442\u044b\u0433 \u0445\u0430\u0440\u0445\u0430\u0434 \u041f\u043e\u0439\u0441\u0441\u043e\u043d \u0442\u0430\u0440\u0445\u0430\u043b\u0442\u0442\u0430\u0439 \u043e\u0439\u0440\u043e\u043b\u0446\u043e\u043e \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b Lightgbm ,Xgboost \u0437\u0430\u0433\u0432\u0430\u0440 \u0434\u044d\u044d\u0440 'objective' \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0439\u0433 \u041f\u043e\u0439\u0441\u0441\u043e\u043d \u0431\u0430\u0439\u0434\u043b\u0430\u0430\u0440 \u043e\u0440\u0443\u0443\u043b\u0436 \u04e9\u0433\u04e9\u0432 \nlgb_params= {\n            'boosting_type': 'gbdt',\n            'objective' : 'poisson',\n            'metric': 'rmse',\n            'subsample': 0.7,\n            'subsample_freq': 1,\n            'learning_rate': 0.01,\n            'num_leaves': 2**8-1,\n            'min_data_in_leaf': 2**9-1,\n            'feature_fraction': 0.9,\n            'max_bin': 100,\n            'n_estimators': 1500,\n            'boost_from_average': True,\n            \"random_seed\":42,\n            'early_stopping_rounds':50\n            }\ncat_params = {\n             'n_estimators' : 2000,\n             'loss_function':'RMSE',\n             'eval_metric' : 'RMSE',\n             'early_stopping_rounds': 50,\n             'task_type':'CPU'\n             }\nxgb_params = { \n             'objective': 'count:poisson',\n             'min_child_weight': 1, \n             'eta': 0.01, \n             'colsample_bytree': 0.9, \n             'max_depth': 6,\n             'subsample': 0.9,\n             'lambda': 1., \n             'nthread': -1, \n             'booster' : 'gbtree', \n             'silent': 1,\n             'eval_metric': 'rmse',\n             'seed':100,\n             'silent':None,\n             'tree_method': 'gpu_hist',\n             'gpu_id': 0\n            }","d11ab358":"class ML_Model():\n    def __init__(self, train, valid, test, train_label,valid_label):\n        self.target_col ='aqi'\n\n        self.train_set = train.drop(['ID','date', self.target_col] , 1)\n        self.valid_set = valid.drop(['ID','date', self.target_col] , 1)\n        self.train_label = train_label\n        self.valid_label = valid_label\n        self.test_set  = test.drop(['ID','date', self.target_col] , 1)\n\n    # LGBM MODEL\n    def lgbm_model(self, lgb_params, len_seeds=1, k=5):\n  \n        final_test = np.zeros((self.test_set.shape[0], ))\n\n        oof_train = np.zeros((self.train_set.shape[0], ))\n        oof_test = np.zeros((self.test_set.shape[0], ))\n\n        kfolds = StratifiedKFold(n_splits=k, random_state=42, shuffle=True)\n\n        # Train model per each fold\n        for  i , ( trn_ind , val_ind ) in  tqdm ( enumerate ( kfolds.split ( X = self.train_set , \n                                                                             y = self.train_label ))):\n            print('[Fold %d\/%d]' % (i + 1, k))\n            print('=' * 60)\n            # Split Train\/Valid\n            X_train, y_train = self.train_set.loc[trn_ind], self.train_label[trn_ind]\n            X_valid, y_valid = self.valid_set, self.valid_label\n\n            # LGBM-Dataset\n            dtrain = lgb.Dataset(X_train, y_train)\n            dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\n\n            # Train Model\n            model = lgb.train(lgb_params, dtrain, lgb_params.get('n_estimators'),\n                                   valid_sets=(dtrain, dvalid),\n                                   valid_names=('train', 'valid'),verbose_eval=200)\n            \n            valid_pred = model.predict(X_valid)\n            test_pred = model.predict(self.test_set)\n            final_test+=  test_pred  \/  k\n            \n        return final_test\n\n    #Catboost model       \n    def catboost_model(self, cat_params,  k=5):\n\n        final_test = np.zeros((self.test_set.shape[0], ))\n\n        kfolds = StratifiedKFold(n_splits=k, random_state=42, shuffle=True)\n\n        # Train model per each fold\n        for  i , ( trn_ind , val_ind ) in  tqdm ( enumerate ( kfolds.split ( X = self.train_set , \n                                                                             y = self.train_label ))):\n            print('[Fold %d\/%d]' % (i + 1, k))\n            print('=' * 60)\n            # Split Train\/Valid\n            X_train, y_train = self.train_set.loc[trn_ind], self.train_label[trn_ind]\n            X_valid, y_valid = self.valid_set, self.valid_label\n            \n            # Catboost-Dataset\n            dtrain = Pool(X_train, y_train)\n            dvalid = Pool(X_valid, y_valid)\n           \n            model = CatBoostRegressor(**cat_params)\n            # Train Model\n            model.fit(dtrain,eval_set=dvalid,verbose=200 )\n            valid_pred = model.predict(X_valid)\n            test_pred = model.predict(self.test_set)\n            final_test  +=  test_pred \/ k\n            \n        return final_test\n    \n    #Xgboost model       \n    def xgboost_model(self, xgb_params,  k=5):\n        \n        final_test = np.zeros((self.test_set.shape[0], ))\n\n        kfolds = StratifiedKFold(n_splits=k, random_state=42, shuffle=True)\n\n        # Train model per each fold\n        for  i , ( trn_ind , val_ind ) in  tqdm ( enumerate ( kfolds.split ( X = self.train_set , \n                                                                             y = self.train_label ))):\n            print('[Fold %d\/%d]' % (i + 1, k))\n            print('=' * 60)\n            # Split Train\/Valid\n            X_train, y_train = self.train_set.loc[trn_ind], self.train_label[trn_ind]\n            X_valid, y_valid = self.valid_set, self.valid_label\n            \n            # Xgboost-Dataset\n            dtrain = xgb.DMatrix(X_train, y_train)\n            dvalid = xgb.DMatrix(X_valid, y_valid)\n           \n            watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n            # Train Model\n            model = xgb.train(xgb_params, dtrain, 1500, watchlist, early_stopping_rounds=50, verbose_eval=200)\n            test_pred = model.predict(xgb.DMatrix(self.test_set))\n            final_test  +=  test_pred \/ k  \n            \n        return final_test","97c89cca":"ml_model = ML_Model(train,valid, test, train['aqi'], valid['aqi'])","9ba28b8b":"lgb_pred = ml_model. lgbm_model(lgb_params)\ncat_pred = ml_model.catboost_model(cat_params)\nxgb_pred = ml_model.xgboost_model(xgb_params)","dc8a5ce2":"test=pd.read_csv(f'{DATA_DIR}\/pm_test.csv')\nsub['lgb']=lgb_pred\nsub['cat']=cat_pred\nsub['xgb']=xgb_pred\nsub['aqi']=sub['lgb']*0.5+sub['cat']*0.3+sub['xgb']*0.2","de129d47":"#1. feature importance -\u0430\u0447 \u0445\u043e\u043b\u0431\u043e\u0433\u0434\u043b\u044b\u043d \u0437\u044d\u0440\u0433\u0438\u0439\u0433 \u0430\u0432\u0447 \u04af\u0437\u04af\u04af\u043b 'station' \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u0434\u044d\u044d\u0440 \u0430\u0447 \u0445\u043e\u043b\u0431\u043e\u0433\u0434\u043b\u044b\u043d \u0437\u044d\u0440\u044d\u0433 \u0431\u0430\u0433\u0430 \u0431\u0430\u0439\u0441\u0430\u043d\n#2. CV-g station \u0431\u04af\u0440\u044d\u044d\u0440 \u0433\u0430\u0440\u0433\u0430\u0436 \u0445\u0430\u0440\u0430\u0445\u0430\u0434 \u0434\u0430\u0440\u0430\u0430\u0445 CV \u0445\u0430\u0440\u044c\u0446\u0430\u043d\u0433\u0443\u0439 \u04e9\u043d\u0434\u04e9\u0440 \u0431\u0430\u0439\u0441\u0430\u043d 'station' \u0434\u044d\u044d\u0440 postprocess \u0445\u0438\u0439\u0432\n#3. CV-g 2 \u043e\u043d\u043e\u043e\u0440 \u0441\u0430\u043b\u0433\u0430\u0436 \u0445\u0430\u0440\u0430\u0445\u0430\u0434 2020 \u043e\u043d\u044b\u0445 \u04e9\u043d\u0434\u04e9\u0440 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b 'year' \u0434\u044d\u044d\u0440 postprocess \u0445\u0438\u0439\u0432","1cd39a73":"test['aqi']= sub['aqi']\ntest['date'] = pd.to_datetime(test['date'])\ntest['year']= test['date'].dt.year\ntest['month']= test['date'].dt.month\ntest.loc[(test['year']==2019) & (test['month']>9), 'aqi'] = test['aqi']*0.96\ntest.loc[test['year'] ==2020, 'aqi'] = test['aqi']*0.88\ntest.loc[test['station'] ==\"\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c\", 'aqi'] = test['aqi']*1.15\ntest.loc[test['station'] ==\"\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443\", 'aqi'] = test['aqi']*1.15","9cde5efe":"sub_result = sub_add.set_index('ID').combine_first(test.set_index('ID'))\nsub_result=sub_result.reset_index()\nsub['aqi']=sub_result['aqi']\ndel sub['lgb'], sub['cat'],sub['xgb']\nsub.to_csv('sub_blend_v3.csv',index=False)\nsub.head(4)","947f9934":"## Load the data","c6d01063":"## Model training and validation","25ec8971":"## Post processing","5853e490":"## Ensemble","f86c5557":"## Training models","c55aa984":"## DataPreproccesing","ce738952":"## Import libraries","ef1666c0":"## Submission"}}