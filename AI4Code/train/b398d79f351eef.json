{"cell_type":{"44346a14":"code","a85b3e6e":"code","e5c52f44":"code","196b5e87":"code","59d90c52":"code","e426b199":"code","ef6a23dc":"code","7348d236":"code","a58ee2b6":"code","6e9d0275":"code","fffb9f9f":"code","c3edadc8":"code","c2c0dcba":"code","7bd9a596":"code","299786d0":"code","96040ab3":"code","2b0d8c87":"code","7272b2b9":"code","bf8a6e6c":"code","638dd9da":"code","f717b379":"code","34c7730a":"code","80018d30":"code","ad3d7624":"markdown","87c1ff0a":"markdown","38de3209":"markdown","042d1c4e":"markdown","9a054091":"markdown","7c8e5d4e":"markdown"},"source":{"44346a14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nprint(tf.__version__)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a85b3e6e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ndf['target'].value_counts().plot.bar(title='Target')\nplt.show()","e5c52f44":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef summarize_data(corpus):\n    \"\"\"\n    print statements and visualizations to summarize the corpus\n    \"\"\"\n    \n    # get the documents size\n    df_doc_size = pd.Series([len(str(doc).split(\" \")) for doc in corpus])\n    \n    # get the tokens in the corpus\n    df_tokens = pd.Series([token for doc in corpus for token in str(doc).split(\" \")])\n    \n    print(\"---------------------------\")\n    print(\"num docs\", len(corpus))\n    print(\"median tokens\", df_doc_size.median())\n    print(\"num tokens\", len(df_tokens))\n    print(\"unique tokens\", len(df_tokens.value_counts()))\n    print(\"---------------------------\")\n    \n    # make plots\n    fig = plt.figure(figsize=(14,6))\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    \n    df_doc_size.plot.hist(ax=ax1, title='Document Sizes')\n    df_tokens.value_counts().plot.hist(ax=ax2, title='Tokens Counts')\n    \nsummarize_data(df.question_text.values.tolist())","196b5e87":"import shutil\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, valid_set = train_test_split(df, test_size=0.2, stratify=df.target)\n\nprint(train_set.shape)\nprint(valid_set.shape)","59d90c52":"# delete temp dir\nif os.path.exists('\/kaggle\/temp\/'):\n    shutil.rmtree('\/kaggle\/temp\/')\n\nos.mkdir(\"\/kaggle\/temp\/\")\n\ntrain_path = \"\/kaggle\/temp\/train.csv\"\nvalid_path = \"\/kaggle\/temp\/valid.csv\" \n\ntrain_set.to_csv(train_path, index=False)\nvalid_set.to_csv(valid_path, index=False)","e426b199":"train_sentences = train_set.question_text.values.tolist()\ntrain_labels = train_set.target\n\nvalid_sentences = valid_set.question_text.values.tolist()\nvalid_labels = valid_set.target\n\ntrain_sentences[:5]","ef6a23dc":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsequence_length = 50\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\nvocab_size = 100000\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=sequence_length, padding=padding_type, truncating=trunc_type)\n\nvalid_sequences = tokenizer.texts_to_sequences(valid_sentences)\nvalid_padded = pad_sequences(valid_sequences, maxlen=sequence_length, padding=padding_type, truncating=trunc_type)\n\nprint(train_sentences[:4])\nprint(train_padded[:4])","7348d236":"from collections import Counter\ntoken_sentences = tokenizer.sequences_to_texts(train_sequences)\nvocabulary = Counter()\n\nfor sentence in token_sentences:\n    vocabulary.update(sentence.split())","a58ee2b6":"vocab = [word for word, count in vocabulary.most_common()]\nlen(vocab)","6e9d0275":"import tensorflow as tf\n\ntrain_ds = tf.data.experimental.CsvDataset(train_path, record_defaults=[\"\"] + [tf.constant([], dtype=tf.int32)], select_cols=[1, 2], header=True)\ntrain_ds = train_ds.shuffle(10000).batch(512).prefetch(1)\n\nvalid_ds = tf.data.experimental.CsvDataset(valid_path, record_defaults=[\"\"] + [tf.constant([], dtype=tf.int32)], select_cols=[1, 2], header=True)\nvalid_ds = valid_ds.batch(512).prefetch(1)\n\nfor X, y in train_ds.take(5):\n    print(X[0], y[0])","fffb9f9f":"vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\ntext_ds = train_ds.map(lambda x, y: x)\nvectorize_layer.adapt(text_ds)\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nmodel.add(vectorize_layer)\n\nfor X in text_ds.take(1):\n    print(X[0].numpy())\n    print(model.predict(X)[0])","c3edadc8":"print(len(vectorize_layer.get_vocabulary()))\nprint(vectorize_layer.get_vocabulary()[:10])","c2c0dcba":"import zipfile\nlocal_zip = \"\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip\"\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/kaggle\/temp\/')\nzip_ref.close()","7bd9a596":"# load word embeddings\nembeddings_index = {}\nwith open('\/kaggle\/temp\/glove.840B.300d\/glove.840B.300d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[-300:], dtype='float32')\n        embeddings_index[word] = coefs","299786d0":"embed_matrix = np.zeros((vocab_size, 300))\nfor idx, word in enumerate(vectorize_layer.get_vocabulary()):\n    embed_vector = embeddings_index.get(word)\n    if embed_vector is not None:\n        embed_matrix[idx] = embed_vector\n        \nembed_matrix.shape","96040ab3":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(1,), dtype=tf.string),\n    vectorize_layer,\n    tf.keras.layers.Embedding(vocab_size, 300, input_length=sequence_length, weights=[embed_matrix], trainable=False, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","2b0d8c87":"# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\n\nneg = train_set.target.value_counts().loc[0]\npos = train_set.target.value_counts().loc[1]\ntotal = train_set.shape[0]\n\nweight_for_0 = (1 \/ neg) * (total \/ 2.0)\nweight_for_1 = (1 \/ pos) * (total \/ 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\nclass_weight","7272b2b9":"K = tf.keras.backend\nK.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nMETRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc')\n]\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=METRICS, )\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\n\nhistory = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[early_stopping_cb], class_weight=class_weight)","bf8a6e6c":"from sklearn.metrics import confusion_matrix\n\ndef plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    \nlabels = valid_set.target.values\npredictions = model.predict(valid_ds.map(lambda x, y: x))\nplot_cm(labels, predictions)","638dd9da":"train_auc=history.history['auc']\nvalid_auc=history.history['val_auc']\ntrain_loss=history.history['loss']\nvalid_loss=history.history['val_loss']\n\nepochs=range(len(train_auc)) # Get number of epochs\n\nplt.plot(epochs, train_auc, 'r')\nplt.plot(epochs, valid_auc, 'b')\nplt.title('Training and validation AUC')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"AUC\")\nplt.legend([\"Training AUC\", \"Validation AUC\"])\n\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, train_loss, 'r')\nplt.plot(epochs, valid_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\nplt.figure()\n\n\n# Expected Output\n# A chart where the validation loss does not increase sharply!","f717b379":"test_path = '\/kaggle\/input\/quora-insincere-questions-classification\/test.csv'\ntest_ds = tf.data.experimental.CsvDataset(test_path, record_defaults=[\"\"], select_cols=[1], header=True).batch(512).prefetch(1)\ny_pred = model.predict(test_ds)","34c7730a":"test_set = pd.read_csv(test_path)\ntest_set['prediction'] = np.where(y_pred >= 0.5, 1, 0)\ntest_set[['qid', 'prediction']].to_csv('submission.csv', index=False)","80018d30":"pd.read_csv('submission.csv').head()","ad3d7624":"# Make Predictions","87c1ff0a":"# keras Text preprocessing with Tokenizer","38de3209":"### Calculate class weights","042d1c4e":"# Dataset Exploration","9a054091":"# keras Data API and TextVectorization Layer","7c8e5d4e":"# Create a classification model with pretrained Embeddings"}}