{"cell_type":{"4298a4ae":"code","b73e1390":"code","634ff5c6":"code","9839a78d":"code","53474946":"code","faf4fd5c":"code","46b7ad4e":"code","5a08be78":"code","1f9d401b":"code","45062ec0":"code","6cb02b7d":"code","bb4bd060":"code","7765e3a7":"code","98a12275":"code","a677f72f":"code","d25ecce1":"code","78375f8b":"code","eadf21d8":"code","ebe40996":"code","36714216":"code","48b6e47f":"code","06111f43":"markdown","6c4ed2b0":"markdown","ba8da267":"markdown","a194517b":"markdown","93296472":"markdown","c0013b85":"markdown","9238c1a3":"markdown","bc63fa9a":"markdown","7dbcf39c":"markdown","40d19dd3":"markdown","2f0579de":"markdown","a913ea10":"markdown"},"source":{"4298a4ae":"#basics\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport math\n\n#modeling\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import *\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import train_test_split\nimport transformers\n\n#image preprocessing\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom PIL import Image\nimport cv2\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","b73e1390":"train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nsub = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')","634ff5c6":"train.info()","9839a78d":"count_by_label = train.groupby('label_group').size().sort_values(ascending = False).reset_index()\nsns.kdeplot(data=count_by_label, x=0)","53474946":"train_jpg_directory = '..\/input\/shopee-product-matching\/train_images'\ntest_jpg_directory = '..\/input\/shopee-product-matching\/test_images'\n\ntrain_image_path = []\nfor img in train.image:\n    train_image_path.append(os.path.join(train_jpg_directory, img))\ntrain['img_path'] = train_image_path\n\ntest_image_path = []\nfor img in test.image:\n    test_image_path.append(os.path.join(test_jpg_directory, img))\ntest['img_path'] = test_image_path","faf4fd5c":"count_by_label","46b7ad4e":"def display_multiple_img(images_paths, rows, cols):\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","5a08be78":"display_multiple_img(train.loc[train.label_group == 994676122, 'img_path'][:20], 4,5)","1f9d401b":"display_multiple_img(train.loc[train.label_group == 2141883596, 'img_path'], 1,2)","45062ec0":"#drop duplicated images\ntrain = train.drop_duplicates('image')","6cb02b7d":"#mapping label_group into integer values\nlabel_mapper = dict(zip(train['label_group'].unique(), np.arange(len(train['label_group'].unique()))))\nlabel_mapper_inv = dict(zip(np.arange(len(train['label_group'].unique())), train['label_group'].unique()))\ntrain['label_group'] = train['label_group'].map(label_mapper)","bb4bd060":"#train val split\ntr, val = train_test_split(train, random_state = SEED, shuffle = True, test_size = 0.1)\ntr.reset_index(drop = True, inplace = True)\nval.reset_index(drop = True, inplace = True)","7765e3a7":"AUTO = tf.data.experimental.AUTOTUNE\nIMAGE_SIZE = [384, 384]\nBATCH_SIZE = 8\n\ndef create_batch(train, batch_size=16):\n    x_anchors = np.zeros((batch_size, 384, 384, 3))\n    x_positives = np.zeros((batch_size, 384, 384, 3))\n    x_negatives = np.zeros((batch_size, 384, 384, 3))\n    \n    for i in range(0, batch_size):\n        # We need to find an anchor, a positive example and a negative example\n        random_index = np.random.randint(0, train.shape[0] - 1)\n        x_anchor = train.img_path[random_index]\n        x_anchor = tf.io.read_file(x_anchor)\n        x_anchor = tf.image.decode_jpeg(x_anchor, channels=3)\n        x_anchor = tf.image.resize(x_anchor, IMAGE_SIZE)\n        x_anchor = x_anchor \/ 255\n        #identifying label of x_anchor\n        y = train.label_group[random_index]\n        #identifying possible index of x_positive and x_negative\n        indices_for_pos = np.where(train.label_group == y)[0]\n        indices_for_neg = np.where(train.label_group != y)[0]\n        \n        #selecting \n        if len(indices_for_pos)==1:\n            x_positive = train.img_path[indices_for_pos[0]]\n        else:\n            x_positive = train.img_path[indices_for_pos[np.random.randint(0, len(indices_for_pos) - 1)]]\n        x_positive = tf.io.read_file(x_positive)\n        x_positive = tf.image.decode_jpeg(x_positive, channels=3)\n        x_positive = tf.image.resize(x_positive, IMAGE_SIZE)\n        x_positive = x_positive\/255\n        \n        x_negative = train.img_path[indices_for_neg[np.random.randint(0, len(indices_for_neg) - 1)]]\n        x_negative = tf.io.read_file(x_negative)\n        x_negative = tf.image.decode_jpeg(x_negative, channels=3)\n        x_negative = tf.image.resize(x_negative, IMAGE_SIZE)\n        x_negative = x_negative\/255\n        \n        x_anchors[i] = x_anchor\n        x_positives[i] = x_positive\n        x_negatives[i] = x_negative\n        \n    return [x_anchors, x_positives, x_negatives]","98a12275":"#example\nbatch = create_batch(tr)\nfig = plt.figure(figsize = (12, 4))\nplt.subplot(131)\nplt.imshow(batch[0][0])\nplt.title('anchor')\nplt.subplot(132)\nplt.imshow(batch[1][0])\nplt.title('positive')\nplt.subplot(133)\nplt.imshow(batch[2][0])\nplt.title('negative')","a677f72f":"IMAGE_SIZE = [512, 512, 3]\nBATCH_SIZE = 8\n\n#src : https:\/\/www.kaggle.com\/akensert\/glret-triplet-semi-hard-loss-with-distributed-tf\/notebook\n#Data generator for tfa.losses.TripletSemiHardLoss\ndef _get_transform_matrix(rotation, shear, hzoom, wzoom, hshift, wshift):\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n\n    # convert degrees to radians\n    rotation = math.pi * rotation \/ 360.\n    shear    = math.pi * shear    \/ 360.\n\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    rot_mat = get_3x3_mat([c1,    s1,   zero ,\n                           -s1,   c1,   zero ,\n                           zero,  zero, one ])\n\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_mat = get_3x3_mat([one,  s2,   zero ,\n                             zero, c2,   zero ,\n                             zero, zero, one ])\n\n    zoom_mat = get_3x3_mat([one\/hzoom, zero,      zero,\n                            zero,      one\/wzoom, zero,\n                            zero,      zero,      one])\n\n    shift_mat = get_3x3_mat([one,  zero, hshift,\n                             zero, one,  wshift,\n                             zero, zero, one   ])\n    return tf.matmul(\n        tf.matmul(rot_mat, shear_mat),\n        tf.matmul(zoom_mat, shift_mat)\n        )\n# zoom and shift images\ndef _spatial_transform(image,\n                       rotation=3.0,\n                       shear=2.0,\n                       hzoom=8.0,\n                       wzoom=8.0,\n                       hshift=8.0,\n                       wshift=8.0):\n\n    ydim = tf.gather(tf.shape(image), 0)\n    xdim = tf.gather(tf.shape(image), 1)\n    xxdim = xdim % 2\n    yxdim = ydim % 2\n\n    # random rotation, shear, zoom and shift\n    rotation = rotation * tf.random.normal([1], dtype='float32')\n    shear = shear * tf.random.normal([1], dtype='float32')\n    hzoom = 1.0 + tf.random.normal([1], dtype='float32') \/ hzoom\n    wzoom = 1.0 + tf.random.normal([1], dtype='float32') \/ wzoom\n    hshift = hshift * tf.random.normal([1], dtype='float32')\n    wshift = wshift * tf.random.normal([1], dtype='float32')\n\n    m = _get_transform_matrix(\n        rotation, shear, hzoom, wzoom, hshift, wshift)\n\n    # origin pixels\n    y = tf.repeat(tf.range(ydim\/\/2, -ydim\/\/2,-1), xdim)\n    x = tf.tile(tf.range(-xdim\/\/2, xdim\/\/2), [ydim])\n    z = tf.ones([ydim*xdim], dtype='int32')\n    idx = tf.stack([y, x, z])\n    \n    # destination pixels\n    idx2 = tf.matmul(m, tf.cast(idx, dtype='float32'))\n    idx2 = tf.cast(idx2, dtype='int32')\n    # clip to origin pixels range\n    idx2y = tf.clip_by_value(idx2[0,], -ydim\/\/2+yxdim+1, ydim\/\/2)\n    idx2x = tf.clip_by_value(idx2[1,], -xdim\/\/2+xxdim+1, xdim\/\/2)\n    idx2 = tf.stack([idx2y, idx2x, idx2[2,]])\n\n    # apply destinations pixels to image\n    idx3 = tf.stack([ydim\/\/2-idx2[0,], xdim\/\/2-1+idx2[1,]])\n    d = tf.gather_nd(image, tf.transpose(idx3))\n    image = tf.reshape(d, [ydim, xdim, 3])\n    return image\n\n#changing brightness of image\ndef _pixel_transform(image,\n                     saturation_delta=0.3,\n                     contrast_delta=0.1,\n                     brightness_delta=0.2):\n    image = tf.image.random_saturation(\n        image, 1-saturation_delta, 1+saturation_delta)\n    image = tf.image.random_contrast(\n        image, 1-contrast_delta, 1+contrast_delta)\n    image = tf.image.random_brightness(\n        image, brightness_delta)\n    return image\n\n#flipping images\ndef _random_fliplr(image, p=0.25):\n    r = tf.random.uniform(())\n    mirror_cond = tf.math.less(r, p)\n    image = tf.cond(\n        mirror_cond,\n        lambda: tf.reverse(image, [1]),\n        lambda: image\n    )\n    return image\n###########################################Triplet Data Generator##################################\ndef preprocess_input(image, target_size, augment=True):\n    \n    image = tf.image.resize(\n        image, target_size, method='bilinear')\n\n    image = tf.cast(image, tf.uint8)\n    if augment:\n        image = _spatial_transform(image)\n        image = _random_fliplr(image)\n        image = _pixel_transform(image)\n    image = tf.cast(image, tf.float32)\n    image \/= 255.\n    return image\n\ndef create_triplet_dataset(df, training, batch_size, input_size, K):\n    '''\n    df: dataset containing label and img_path\n    K : number of images which have same label\n    '''\n    def sample_input(image_paths, label, K):\n        image_paths = tf.strings.split(image_paths, sep=',')\n        labels = tf.tile([label], [K,])\n        if K-len(image_paths) > 0:\n            image_paths = tf.random.shuffle(image_paths)\n            for i in tf.range(K-len(image_paths)):\n                image_paths = tf.concat(\n                    [image_paths, [tf.gather(image_paths, i)]], axis=0)\n            return image_paths, labels\n        idx = tf.argsort(tf.random.uniform(tf.shape(image_paths)))\n        idx = tf.gather(idx, tf.range(K))\n        image_paths = tf.gather(image_paths, idx)\n        return image_paths, labels\n\n    def read_image(image_path):\n        image = tf.io.read_file(image_path)\n        return tf.image.decode_jpeg(image, channels=3)\n\n    def reshape(x, y):\n        x = tf.reshape(x, (-1, *input_size))\n        y = tf.reshape(y, (-1,))\n        return x, y\n    @tf.autograph.experimental.do_not_convert # to silence warning\n    def nested(x, y):\n        return (tf.data.Dataset.from_tensor_slices((x, y))\n                .map(lambda x, y: (read_image(x), y),\n                    tf.data.experimental.AUTOTUNE)\n                .map(lambda x, y: (preprocess_input(\n                        x, input_size[:2], True), y),\n                     tf.data.experimental.AUTOTUNE)\n                .batch(K))\n\n    image_paths, labels = df.img_path, df.label_group\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    if training:\n        dataset = dataset.shuffle(10_000)\n    dataset = dataset.map(\n        lambda x, y: sample_input(x, y, K), tf.data.experimental.AUTOTUNE)\n    dataset = dataset.flat_map(nested)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(lambda x, y: reshape(x, y), tf.data.experimental.AUTOTUNE)\n    return dataset","d25ecce1":"train_img_data = create_triplet_dataset(tr[['img_path', 'label_group']],\n                                    training = True,\n                                    batch_size = BATCH_SIZE,\n                                    input_size = IMAGE_SIZE,\n                                    K = 4)\n\nval_img_data = create_triplet_dataset(val[['img_path', 'label_group']],\n                                    training = True,\n                                    batch_size = 16,\n                                    input_size = IMAGE_SIZE,\n                                    K = 4)","78375f8b":"def triplet_net():\n    input = Input(shape = IMAGE_SIZE, dtype = tf.float32) #input image\n    base_model = keras.applications.Xception\n    base_model = base_model(include_top = False, weights = 'imagenet', input_shape = IMAGE_SIZE)\n    \n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    x = base_model(input)\n    pooled = Concatenate(axis = -1)([GlobalMaxPool2D()(x), GlobalAvgPool2D()(x)])\n    \n    output = Dense(300, name = 'linear_transformation')(pooled) \n    \n    model = Model(input, output)\n    model.compile(loss = tfa.losses.TripletSemiHardLoss(margin=5.0, distance_metric='L2'),\n                  optimizer = keras.optimizers.Adam(learning_rate=0.0001),\n                  metrics = tfa.losses.TripletSemiHardLoss(margin=5.0, distance_metric='L2'))\n    \n    return model","eadf21d8":"image_model = triplet_net()\nimage_model.summary()","ebe40996":"earlystop = keras.callbacks.EarlyStopping(patience=5)\nmodel_check = keras.callbacks.ModelCheckpoint(    \n        filepath=\".\/image_model.h5\", \n        monitor='val_loss',  \n        save_best_only=True)\n\n\nhistory = image_model.fit(train_img_data,\n                          verbose=1,\n                          steps_per_epoch = len(tr)\/\/BATCH_SIZE,\n                          validation_data = val_img_data,\n                          validation_steps = len(val)\/\/16,\n                          epochs=20,\n                          callbacks = [earlystop, model_check])","36714216":"#unfreeze EfficientNet and Fine Tuning\nimage_model.layers[3].trainable = True\n\n#recompile model\nimage_model.compile(loss = tfa.losses.TripletSemiHardLoss(margin=5.0, distance_metric='L2'),\n                  optimizer = keras.optimizers.Adam(learning_rate=0.00002),\n                  metrics = tfa.losses.TripletSemiHardLoss(margin=5.0, distance_metric='L2'))\n    \n    \nearlystop = keras.callbacks.EarlyStopping(patience=5)\n\nmodel_check = keras.callbacks.ModelCheckpoint(    \n        filepath=\".\/image_model_finetune.h5\", \n        monitor='val_loss',  \n        save_best_only=True)\n\n\nhistory = image_model.fit(train_img_data,\n                          verbose=1,\n                          steps_per_epoch = len(tr)\/\/BATCH_SIZE,\n                          validation_data = val_img_data,\n                          validation_steps = 10,\n                          epochs=20,\n                          callbacks = [earlystop, model_check])","48b6e47f":"image_model.save('image_encoder_Xception.h5')","06111f43":"I implemented create_batch function which creates batch of Image Triplets, for your understanding toward Input Image Triplet.","6c4ed2b0":"## 1. Image Data Preprocessing","ba8da267":"# 1. Explaratory Data Analysis","a194517b":"There are at least 2 duplicates of each label. therefore we can form image, text triplet","93296472":"### Data Generator for Semi-Hard Batch mining","c0013b85":"<b>This is Training Notebook for Image Encoder<\/b>","9238c1a3":"* 1. Data Generator above randomly select 16 images from different Categories.\n* 2. And then It augments K images for each category. therefore, there are K * BATCH_SIZE images in a batch.\n* 3. TripletSemiHardLoss class automatically conducts semi hard batch mining and selects Image triplets.","bc63fa9a":"# 3. Saving Image Encoder for Inference","7dbcf39c":"all rows are non-null","40d19dd3":"# 2. Image Embedding Encoder","2f0579de":"Most of the products have 1~15 duplicates","a913ea10":"## 2. Triplet EfficientNet for fine-tuning with Triplet Loss Objective"}}