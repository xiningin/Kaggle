{"cell_type":{"023ef182":"code","5e3eea9f":"code","cf7da584":"code","98d3f0b0":"code","461d71f0":"code","5ca4fb46":"code","a44e66ff":"code","fa51557c":"code","5e440084":"code","3856eff8":"code","8bebdf65":"code","53d4aed5":"code","bc0b5f25":"code","ca6acde0":"code","63a8a357":"markdown","e715c66b":"markdown","217c8c45":"markdown","821592b0":"markdown","a9c403df":"markdown","0c2dc924":"markdown","4e926f42":"markdown"},"source":{"023ef182":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e3eea9f":"df=pd.read_csv(\"\/kaggle\/input\/youtube-spam-collections\/Youtube Spam Collections\/Youtube02-KatyPerry.csv\")\npd.set_option('display.max_columns', None)\ndf.tail()","cf7da584":"import tensorflow as tf\nimport sklearn\nfrom tqdm import tqdm","98d3f0b0":"# Loading the BERT Classifier and Tokenizer along with Input module\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\n\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","461d71f0":"# But first see BERT tokenizer exmaples and other required stuff!\n\nexample='who is going to reach the billion first : Katy. Analysis using BERT with Huggingface'\ntokens=tokenizer.tokenize(example)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(tokens)\nprint(token_ids)","5ca4fb46":"train = df[:45000]\ntest = df[45000:]","a44e66ff":"def convert_data_to_examples(train, test, AUTHOR, CLASS): \n    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[AUTHOR], \n                                                          label = x[CLASS]), axis = 1)\n\n    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n                                                          text_a = x[AUTHOR], \n                                                          label = x[CLASS]), axis = 1,)\n  \n    return train_InputExamples, validation_InputExamples\n\ntrain_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'AUTHOR',  'CLASS')","fa51557c":"def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    features = [] # -> will hold InputFeatures to be converted later\n\n    for e in tqdm(examples):\n        input_dict = tokenizer.encode_plus(\n            e.text_a,\n            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n            max_length=max_length,    # truncates if len(s) > max_length\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n            truncation=True\n        )\n\n        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n\n    def gen():\n        for f in features:\n            yield (\n                {\n                    \"input_ids\": f.input_ids,\n                    \"attention_mask\": f.attention_mask,\n                    \"token_type_ids\": f.token_type_ids,\n                },\n                f.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n        (\n            {\n                \"input_ids\": tf.TensorShape([None]),\n                \"attention_mask\": tf.TensorShape([None]),\n                \"token_type_ids\": tf.TensorShape([None]),\n            },\n            tf.TensorShape([]),\n        ),\n    )\n\n\nDATA_COLUMN = 'AUTHOR'\nLABEL_COLUMN = 'CLASS'","5e440084":"train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\ntrain_data = train_data.shuffle(100).batch(32).repeat(2)","3856eff8":"validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\nvalidation_data = validation_data.batch(32)","8bebdf65":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\nmodel.fit(train_data, epochs=2, validation_data=validation_data)","53d4aed5":"pred_sentences = ['I kissed a girl. I liked it', 'KATY PERRY, I AM THE \"D\u00c9CIO CABELO\", \"DECIO HAIR']","bc0b5f25":"#tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n#tf_outputs = model(tf_batch)                                  \n#tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\nlabels = ['Positive', 'Negative']\n#label = tf.argmax(tf_predictions, axis=1)\n#label = label.numpy()\n#for i in range(len(pred_sentences)):\n    # print(pred_sentences[i], \": \", labels[label[i]])\nprint(pred_sentences, \": \", labels) ","ca6acde0":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n     \n    \ndhtml('Failure with Katy and BERT without losing the enthusiasm. - Really Churchill? It is not You that have failed.' )","63a8a357":"#\"I Kissed a Girl\"\n\n\"I Kissed a Girl\" is the debut single by American singer Katy Perry for her second studio album, One of the Boys (2008). It was released on April 28, 2008, by Capitol Records as the lead single from the record. Perry co-wrote the song with Max Martin, Cathy Dennis, and its producer Dr. Luke, with additional production from Benny Blanco. \"I Kissed a Girl\" is a pop-rock, electropop, and disco song with elements of a new wave. Perry stated its lyrics are \"about the magical beauty of a woman.\" The song sparked controversy for its handling of homosexual themes, but in retrospect has been viewed as the beginning of LGBT awareness in pop music.\nhttps:\/\/en.wikipedia.org\/wiki\/I_Kissed_a_Girl","e715c66b":"#Assuming that Class is Sentiment I didn't change positive\/negative to 1 and 0. \n\n#Better not Assume anything with data or anything else.","217c8c45":"<iframe width=\"717\" height=\"538\" src=\"https:\/\/www.youtube.com\/embed\/NX3FYucCh20\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>","821592b0":"#NameError: name 'validation_data' is not defined  ","a9c403df":"#Script by Satyam Prasad Tiwari https:\/\/www.kaggle.com\/satyampd\/imdb-sentiment-analysis-using-bert-w-huggingface\/notebook","0c2dc924":"#Amy and Penny kiss on Big Bang Theory","4e926f42":"#OH NO! I was almost there. AttributeError: 'str' object has no attribute 'text_a'\n\n#Maybe kissing a girl is better than BERT. We should ask Dr. Amy Farrah Fowler and Penny."}}