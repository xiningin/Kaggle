{"cell_type":{"b722756a":"code","ea3d308e":"code","19c4078b":"code","a999a0ee":"code","4b704a1e":"code","9fd5804b":"code","088810b3":"code","256ae3a1":"code","28c109ab":"code","58aee443":"code","dffe13f9":"code","9159726c":"code","229098d4":"code","93c3f249":"code","dd815802":"code","4007e3f0":"code","24598cfe":"code","3326b457":"code","4832c583":"code","e429cff4":"code","0a29d47a":"code","421c1a0e":"code","baa167de":"code","741b4ba9":"code","a69f4990":"code","2ea46a49":"code","a7104119":"code","ac947b91":"code","589c056e":"code","031375a6":"code","b14039df":"code","4c3da6e8":"code","ab463439":"code","c9aa5933":"code","7663c34b":"code","1892078b":"code","ea9be9a4":"code","d495a132":"code","5aef4852":"code","0b5c6818":"code","75372cc0":"code","6b40a0bc":"code","ea691712":"code","39bb5762":"code","b663a2aa":"code","9e1b4f32":"code","f3122036":"code","1f6068ec":"markdown","d153b9f8":"markdown","20946ea4":"markdown","ccb8aec9":"markdown","d149b424":"markdown"},"source":{"b722756a":"! pip install -q efficientnet","ea3d308e":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom glob import glob\nfrom PIL import Image\nimport pickle\n\nimport time\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tensorflow.keras import Model, layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Dropout, GaussianNoise\nfrom tensorflow.keras.applications import ResNet50\nimport tensorflow.keras.backend as K\n\nimport efficientnet.keras as efn","19c4078b":"traindf=pd.read_csv('..\/input\/flickr-image-dataset\/flickr30k_images\/results.csv',sep = '|') ","a999a0ee":"traindf=traindf.rename(columns=lambda x: x.strip())","4b704a1e":"traindf.columns","9fd5804b":"traindf=traindf.drop(['comment_number'], axis = 1)\ntraindf.head()","088810b3":"traindf.iloc[np.random.permutation(len(traindf))]\ntraindf=traindf[:9000]","256ae3a1":"traindf.shape","28c109ab":"traindf['comment'] = \"<start> \"+traindf['comment']+\" <end>\"","58aee443":"train_captions = []\nimg_name_vector = []\n\nfor row,data in traindf.iterrows():\n    image_path=str(\"..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/\"+data['image_name'])\n    train_captions.append(str(data['comment']).replace(\".\",\"\").strip())\n    img_name_vector.append(image_path)","dffe13f9":"print(train_captions[0])\nImage.open(img_name_vector[0])","9159726c":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","229098d4":"image_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","93c3f249":"# with strategy.scope():\n# image_model = tf.keras.applications.InceptionV3(include_top=False,\n#                                                 weights='imagenet')\ntarget_size = (299, 299,3)\nimage_model = efn.EfficientNetB3(\n        weights='noisy-student', # Choose between imagenet and 'noisy-student'\n#         weights='imagenet', \n        input_shape=target_size, include_top=False)\n","dd815802":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    #img = efn.preprocess_input(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","4007e3f0":"AUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","24598cfe":"from tqdm import tqdm\n# Get unique images\nencode_train = sorted(set(img_name_vector))\nfeature_dict = {}\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(\n  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    feature_dict[path_of_feature] =  bf.numpy()","3326b457":"from gensim.models import KeyedVectors\ndef build_matrix(word_index, embedding_index, vec_dim):\n    emb_mean, emb_std = -0.0033470048, 0.109855264\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len(word_index) + 1,vec_dim))\n#     embedding_matrix = np.zeros((len(word_index) + 1, vec_dim))\n    for word, i in word_index.items():\n        for candidate in [word, word.lower(), word.upper(), word.capitalize(), \n                          ps.stem(word), lc.stem(word), sb.stem(word), correction(word) ]:\n            if candidate in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate]\n                break\n    return embedding_matrix","4832c583":"# Find the maximum length of any caption in our dataset\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)","e429cff4":"# Choose the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)","0a29d47a":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n# make list from dict\ntokenizer.index2word = [tokenizer.index_word[ii] for ii in range(len(tokenizer.word_index)) ] \ntokenizer.index2word[:20] # see top-20 most frequent words","421c1a0e":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)","baa167de":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs)\nlenx = np.array([len(x) for x in cap_vector])\nprint(lenx.min(), lenx.mean())","741b4ba9":"# Create training and validation sets using an 80-20 split\nimg_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n                                                                    cap_vector,\n                                                                    test_size=0.2,\n                                                                    random_state=0)\nlen(img_name_train), len(cap_train), len(img_name_val), len(cap_val)","a69f4990":"\nBATCH_SIZE = 128\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = top_k + 1\nnum_steps = len(img_name_train) \/\/ BATCH_SIZE\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048","2ea46a49":"attention_features_shape = bf.shape[0] # 64 for InceptionV3, 100 for B1","a7104119":"# Load the numpy files\ndef map_func(img_name, cap):\n  img_tensor = feature_dict[img_name.decode('utf-8')] # np.load(img_name.decode('utf-8')+'.npy')\n  return img_tensor, cap","ac947b91":"img_name_train[:5]","589c056e":"dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# Shuffle and batch\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","031375a6":"val_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\n\n# Use map to load the numpy files in parallel\nval_dataset = val_dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# No Shuffle for Val and batch\n# val_dataset = val_dataset.shuffle(BUFFER_SIZE)\nval_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","b14039df":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # score shape == (batch_size, 64, hidden_size)\n    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n\n    # attention_weights shape == (batch_size, 64, 1)\n    # you get 1 at the last axis because you are applying score to self.V\n    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","4c3da6e8":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x\n    \nclass RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))","ab463439":"# with strategy.scope():\n# tf.keras.backend.clear_session()\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)","c9aa5933":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","7663c34b":"checkpoint_path = \".\/checkpoints\/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","1892078b":"start_epoch = 0","ea9be9a4":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss \/ int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, trainable_variables)\n\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","d495a132":"@tf.function # Non-teacher-forcing val_loss is too complicated at the moment\ndef val_step(img_tensor, target, teacher_forcing=True):\n  loss = 0\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n#   print(dec_input.shape) # (BATCH_SIZE, 1)\n  features = encoder(img_tensor)\n#   print(features.shape) # (BATCH_SIZE, IMG_FEAT_LEN, ENCODER_HID) = 64 100 256\n  for i in range(1, target.shape[1]):\n    predictions, hidden, _ = decoder(dec_input, features, hidden)\n    loss += loss_function(target[:, i], predictions)\n\n    # using teacher forcing\n    dec_input = tf.expand_dims(target[:, i], 1)\n\n  avg_loss = (loss \/ int(target.shape[1]))\n  return loss, avg_loss\n\ndef cal_val_loss(val_dataset):\n  # target.shape = (64,49) = (BATCH_SIZE, SEQ_LEN)\n  val_num_steps = len(img_name_val) \/\/ BATCH_SIZE\n\n  total_loss = 0\n  for (batch, (img_tensor, target)) in enumerate(val_dataset):\n    batch_loss, t_loss = val_step(img_tensor, target)\n    \n    total_loss += t_loss\n  print ('Valid Loss {:.6f}'.format(total_loss\/val_num_steps))\n  return total_loss\/val_num_steps","5aef4852":"import gc\n# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []\nval_loss_plot = []\ngc.collect()","0b5c6818":"EPOCHS = 20\n\nbest_val_loss = 100\nfor epoch in tqdm(range(start_epoch, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n    loss_plot.append(total_loss \/ num_steps)    \n    val_loss = cal_val_loss(val_dataset)\n    val_loss_plot.append(val_loss)\n    \n    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n                                         total_loss\/num_steps))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if val_loss < best_val_loss:\n      print('update best val loss from %.4f to %.4f' % (best_val_loss, val_loss))\n      best_val_loss = val_loss\n      ckpt_manager.save()\n","75372cc0":"plt.plot(loss_plot)\nplt.plot(val_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","6b40a0bc":"encoder.save_weights('encoder.h5')\ndecoder.save_weights('decoder.h5')\n!ls -sh","ea691712":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    print(img_tensor_val.shape)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","39bb5762":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (8, 8))\n        ax = fig.add_subplot(len_result\/\/2, len_result\/\/2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","b663a2aa":"rid = np.random.randint(0, len(img_name_train))\nimage = img_name_train[rid]\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_train[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image, result, attention_plot)","9e1b4f32":"for ii in range(10):\n    rid = np.random.randint(0, len(img_name_val))\n    image = img_name_val[rid]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n    result, attention_plot = evaluate(image)\n\n    print ('Real Caption:', real_caption)\n    print ('Prediction Caption:', ' '.join(result))\n    plot_attention(image, result, attention_plot)","f3122036":"image_url = '..\/input\/temporary-image\/docusign-XMQHdgirB0U-unsplash.jpg'\n\nimage_extension = image_url[-4:]\nimage_path = image_url\n\nresult, attention_plot = evaluate(image_path)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image_path, result, attention_plot)\n# opening the image\nImage.open(image_url)","1f6068ec":"### Upvote if u like it","d153b9f8":"Image courtesy - https:\/\/unsplash.com\/","20946ea4":"Resizing the image to 299px by 299px","ccb8aec9":"**Reference -** https:\/\/arxiv.org\/pdf\/1502.03044.pdf","d149b424":"Split the data into train and test"}}