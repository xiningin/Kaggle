{"cell_type":{"8fc87f15":"code","3ec5775e":"code","da4c3f4d":"code","5b5d5e3e":"code","16b8df0d":"code","30d2c486":"code","d1c8e5b5":"code","4c3914a5":"code","c6c182eb":"code","27daa692":"code","12254a85":"code","88b343b4":"code","47225dda":"code","148bd99e":"code","b9090ae3":"code","c2930c24":"code","f45242e0":"code","534d8987":"code","7b8ed35d":"code","83122c3d":"code","bf4c1007":"code","af732164":"code","28205620":"code","d0fa51b9":"code","07d44181":"code","1f90d3ab":"code","d8addd13":"code","5f7c3df8":"code","fc2f5d68":"code","8d058874":"code","13c4f3b9":"code","0502997a":"code","b7650175":"code","77400d74":"code","595d93c1":"markdown","4b3ad917":"markdown","e27d8511":"markdown","2288294c":"markdown","cc204ea9":"markdown","8359b064":"markdown","c880b9ea":"markdown","95cc6c30":"markdown","00fbf35f":"markdown","add71ee8":"markdown","c73728d8":"markdown","067fdf2b":"markdown","11f9bc83":"markdown","c10b8125":"markdown","82a5f3c6":"markdown","0a93cade":"markdown","2ec7d4f9":"markdown","e790bd2c":"markdown","dcadcabd":"markdown","a439f624":"markdown","5da1c377":"markdown","92b40dfe":"markdown","a398f789":"markdown","8e4cdb66":"markdown","c8f922c9":"markdown","26e06d28":"markdown","0a4de525":"markdown","0ea0dc0f":"markdown","7233a2eb":"markdown","cd23fb87":"markdown","a6361487":"markdown","57302da8":"markdown","025f0515":"markdown","c43350ee":"markdown","a1ca678f":"markdown","759105b8":"markdown","364804fe":"markdown"},"source":{"8fc87f15":"!pip install -U sentence-transformers","3ec5775e":"import pandas as pd \nimport numpy as np\nfrom tqdm import tqdm \nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport collections\nimport wordcloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport spacy\nimport gensim.corpora as corpora\nfrom gensim.models import LdaMulticore, CoherenceModel, Phrases, LdaModel\nimport time\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.plotting import figure,show,output_notebook\nimport pyLDAvis\nimport pyLDAvis.gensim_models\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nimport pprint\nimport tensorflow as tf\n\npd.set_option('display.max_columns',None)\nwarnings.filterwarnings('ignore')\n%matplotlib inline","da4c3f4d":"data = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv',\n                  usecols=['cord_uid','source_x','title','license','publish_time',\n                           'abstract','authors','journal','url'])\ndata.head()","5b5d5e3e":"data = data[data.publish_time=='2021']\nprint(f'There are {len(data)} articles published in 2021')","16b8df0d":"len1 = len(data)\ndata.dropna(axis=0,how='any',subset=['abstract'],inplace=True)\nprint(f'Dropped {len1-len(data)} articles with missing abstract')","30d2c486":"len1 = len(data)\ndata.drop_duplicates(subset=['abstract'],keep='first',inplace=True)\nprint(f'Dropped {len1-len(data)} articles with duplicate abstract')","d1c8e5b5":"data['abstract'] = data['abstract'].apply(lambda x: x.strip().lower())\nstopwords = set(stopwords.words('english'))\ndata['abstract_tokens'] = data['abstract'].apply(lambda x: word_tokenize(x))\n\n## Remove non-English words\ndata['abstract_tokens'] = data['abstract_tokens'].apply(lambda x: [w for w in x if w.isalpha()])\n\n## Remove Stopwords\ndata['abstract_tokens'] = data['abstract_tokens'].apply(lambda x: [w for w in x if w not in stopwords])\n\n## Remove words with single characters\ndata['abstract_tokens'] = data['abstract_tokens'].apply(lambda x: [w for w in x if len(w)>1])\n#data['abstract_cleaned'] = data['abstract_tokens'].apply(lambda x: ' '.join(x))\ndata.head()","4c3914a5":"data.reset_index(drop=True,inplace=True)","c6c182eb":"inverted_idx = {}\nfor idx,sentence in enumerate(list(data['abstract_tokens'])):\n    for token in set(sentence):\n        if token not in inverted_idx:\n            inverted_idx[token] = [idx]\n        else:\n            inverted_idx[token].append(idx)\n\nprint(f'There are {len(inverted_idx)} unique words in inverted index')","27daa692":"keywords_risk = ['risk','risky','risks','risked','risking']\nrisk_idx = []\nfor word in keywords_risk:\n    try:\n        risk_idx += inverted_idx[word]\n    except:\n        pass\nrisk_idx = list(set(risk_idx))\nprint(f'Found {len(risk_idx)} articles related to risk')","12254a85":"risk_article = data.iloc[risk_idx,:]\nrisk_article.head()","88b343b4":"def lemmatization(texts, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n    texts_out = []\n    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        ##'-PRON-' is used as the lemma for all pronouns such as their, you, me, and I\n        texts_out.append([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' \n                          for token in doc if token.pos_ in allowed_postags])\n    return texts_out   ","47225dda":"risk_article['Lemma_Tokens'] = lemmatization(list(risk_article['abstract_tokens']))\n## convert tokens that frequently occur together as bigram and trigram\nbigram = Phrases(list(risk_article['Lemma_Tokens']), min_count=3)\nrisk_article['Phrase_Lemma_Tokens'] = risk_article['Lemma_Tokens'].apply(lambda x: bigram[x])\n#trigram = Phrases(list(risk_article['Phrase_Lemma_Tokens']), min_count=5)\n#risk_article['Phrase_Lemma_Tokens'] = risk_article['Phrase_Lemma_Tokens'].apply(lambda x: trigram[x])\n\nrisk_abstract_tokens = []\nfor tokens in list(risk_article['Phrase_Lemma_Tokens']):\n    risk_abstract_tokens += tokens\nrisk_wordcnt_dict = collections.Counter(risk_abstract_tokens)\n\nwcloud = wordcloud.WordCloud(background_color='white')\nwcloud.generate_from_frequencies(risk_wordcnt_dict)\nfig,ax = plt.subplots(figsize=(10,8))\nax.imshow(wcloud)\nfor pos in ['left','right','top','bottom']:\n    ax.spines[pos].set_color(None)\nax.set_xticks([])\nax.set_yticks([])\nax.set_title('Word Cloud: Articles Related to Risk Factor' ,fontsize=14)","148bd99e":"#vectorizer = CountVectorizer()\n#risk_article_bow = vectorizer.fit_transform(\n#    [' '.join(tokens) for tokens in list(risk_article['Lemma_Tokens'])])\n\n#search_params = {'n_components': range(1,20)}\n#LDA_model = LatentDirichletAllocation(max_iter=10,learning_method='batch', \n#                                      random_state=98, n_jobs=-1)\n#model = GridSearchCV(LDA_model, param_grid=search_params)\n#LDA_model.fit_transform(risk_article_bow)\n#model.fit(risk_article_bow)\n\n#print(\"Best Model's Params: \", model.best_params_)\n#print(\"Best Log Likelihood Score: \", model.best_score_)\n\n#LDA_model = LatentDirichletAllocation(**model.best_params_)\n#LDA_model.fit(risk_article_bow)\n#LDA_model.perplexity(risk_article_bow)","b9090ae3":"##https:\/\/towardsdatascience.com\/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\nid2token_risk_article = corpora.Dictionary(list(risk_article['Phrase_Lemma_Tokens']))\n##eliminate rare and frequent tokens\n#id2token_risk_article.filter_extremes(no_below=2, no_above=0.8, \n#                                       keep_tokens=['smoke','smoking','smoker'])\ncorpus_risk_article = [id2token_risk_article.doc2bow(tokens) \n                        for tokens in list(risk_article['Phrase_Lemma_Tokens'])]\n\nbegin_time = time.time()\ncoherence_scores = []\nfor nb_topics in tqdm(range(3,11)):\n    lda_model = LdaMulticore(corpus=corpus_risk_article,\n                            id2word=id2token_risk_article,\n                            num_topics=nb_topics, \n                            random_state=98,\n                            chunksize=100,\n                            passes=10,\n                            iterations=50,\n                            decay=0.5,\n                            per_word_topics=True,)\n    #cm = CoherenceModel(model=lda_model, corpus=corpus_smoke_article, coherence='u_mass')\n    cm = CoherenceModel(model=lda_model, \n                        texts=list(risk_article['Phrase_Lemma_Tokens']),\n                        dictionary=id2token_risk_article,\n                        coherence='c_v',\n                        topn=20)\n    coherence_score = cm.get_coherence()\n    coherence_scores.append(coherence_score)\n    \nprint(f'Topic number selection took {time.time()-begin_time} seconds')\n\nfig,ax = plt.subplots(figsize=(8,5))\nax.plot(range(3,11),coherence_scores,color='black')\nax.scatter(range(3,11),coherence_scores,marker='o',color='blue')\nax.set_xticks(range(3,11,1))\nax.set_title('c_v Coherence Score w.r.t. Topic Number',fontsize=14)\n#ax.set_xlabel('Number of Topics',fontsize=14)\nfor pos in ['top','right']:\n    ax.spines[pos].set_color(None)","c2930c24":"lda_model = LdaMulticore(corpus=corpus_risk_article,\n                        id2word=id2token_risk_article,\n                        num_topics=6, \n                        random_state=98,\n                        chunksize=100,\n                        passes=10,\n                        iterations=50,\n                        decay=0.5,\n                        per_word_topics=True,)\n\nprint('Perplexity: ', lda_model.log_perplexity(corpus_risk_article))","f45242e0":"word_topic_scores = lda_model.show_topics(num_words=15,formatted=False)\n\nfig = plt.figure(figsize=(16,15))\nfor i,topic in enumerate(word_topic_scores):\n    word_score_pairs = topic[1]\n    #word_score_dict = dict(topic[1])\n    words = [word_score_pair[0] for word_score_pair in word_score_pairs]\n    probs = [word_score_pair[1] for word_score_pair in word_score_pairs]\n    #fig,ax = plt.subplots(figsize=(7,5))\n    ax = fig.add_subplot(3,2,i+1)\n    ax.barh(y=words,width=probs,height=0.6,color=sns.color_palette()[i])\n    ax.set_yticklabels(labels=words, fontsize=14)\n    ax.set_xticks([])\n    ax.set_title(f'Topic {i+1}',fontsize=15)\n    for pos in ['left','right','bottom','top']:\n        ax.spines[pos].set_color(None)","534d8987":"%%time\npyLDAvis.enable_notebook()\npyLDAvis.gensim_models.prepare(lda_model, corpus_risk_article, id2token_risk_article)","7b8ed35d":"document_topic_distribution = []\nnum_topic_document = []\ntopic_frequency = {}\nfor i in range(len(corpus_risk_article)):\n    document_topic = lda_model.get_document_topics(corpus_risk_article[i])\n    document_topic_distribution.append(document_topic)\n    num_topic_document.append(len(document_topic))\n    for topic_idx,prob in document_topic:\n        if topic_idx not in topic_frequency:\n            topic_frequency[topic_idx] = 1\n        else:\n            topic_frequency[topic_idx] += 1\n\ntopic_frequency = dict(sorted(topic_frequency.items()))","83122c3d":"#num_topic_document_counter = collections.Counter(num_topic_document)\nnum_topic_document_counter = (pd.DataFrame(num_topic_document)).value_counts(sort=False)\ncounter_idx = [str(idx[0]) for idx in num_topic_document_counter.index]\ncounter_values = num_topic_document_counter.values\nfig,axes = plt.subplots(figsize=(16,5),nrows=1,ncols=2)\naxes[0].bar(counter_idx, counter_values,width=0.6,alpha=0.5)\naxes[0].set_xlabel('Number of topics in an Article',fontsize=14)\n\ndf_topic_frequency = pd.DataFrame.from_dict(topic_frequency,orient='index')\ntopic_idxs = [str(idx+1) for idx in df_topic_frequency.index]\naxes[1].bar(topic_idxs, df_topic_frequency.iloc[:,0], width=0.6,color='red',alpha=0.5)\naxes[1].set_xlabel('Topic Index',fontsize=14)\n\nfor ax in axes:\n    for pos in ['top','right']:\n        ax.spines[pos].set_color(None)\n    ax.set_xticklabels(['1','2','3','4','5','6'],fontsize=13)","bf4c1007":"## Generate document topic matrix\ndocument_topic_matrix = np.zeros((len(corpus_risk_article),6))\nfor idx,document_topic in enumerate(document_topic_distribution):\n    for topic_idx,prob in document_topic:\n        document_topic_matrix[idx][topic_idx] = prob\npd.DataFrame(document_topic_matrix).head()","af732164":"tSNE = TSNE(n_components=2,init='pca',random_state=98)\nembedding_matrix = tSNE.fit_transform(document_topic_matrix)\ndf_embedding = pd.DataFrame(embedding_matrix, columns=['X_TSNE','Y_TSNE'])\ncluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', \n                  3: 'red', 4: 'skyblue', 5:'salmon'}\ndf_embedding['dominant_topic_idx'] = np.argmax(document_topic_matrix,axis=1)\ndf_embedding['color'] = df_embedding['dominant_topic_idx'].map(cluster_colors)\ncluster_labels = {0: 'Treatment Methods', 1: 'Mental Health and Behavior', 2: 'Severity of Disease', \n                  3: 'Transmission of Virus', 4: 'Public and Social Service', 5:'Death'}\ndf_embedding['label'] = df_embedding['dominant_topic_idx'].map(cluster_labels)\ndf_embedding['article_title'] = list(risk_article['title'])\n\noutput_notebook()\n\nsource = ColumnDataSource(\n    dict(x = df_embedding['X_TSNE'],\n         y = df_embedding['Y_TSNE'],\n         color=df_embedding['color'],\n         label=df_embedding['label'],\n         topic_key= df_embedding['dominant_topic_idx'],\n         title= df_embedding['article_title'],\n         #content = \n        ))\n\nlda_plot = figure(title='T-SNE Clustering of LDA Topics', \n              plot_width=800, plot_height=600,\n              tools='pan,wheel_zoom,box_zoom,reset,hover', \n              x_axis_type=None, y_axis_type=None)\nlda_plot.scatter(x='x', y='y', source=source, \n                 legend='label', color='color',alpha=0.7, size=10)\nhover = lda_plot.select(dict(type=HoverTool))\nhover.tooltips = {'content':'Title: @title'}\nlda_plot.legend.location = 'bottom_right'\nshow(lda_plot)","28205620":"sbert_model = SentenceTransformer('stsb-distilroberta-base-v2',device='cuda')\nrisk_sentences = list(risk_article['Lemma_Tokens'].apply(lambda x: ' '.join(x)))\nsentence_embeddings = sbert_model.encode(risk_sentences, \n                                         convert_to_numpy=True,\n                                         show_progress_bar=True)","d0fa51b9":"print(f'The shape of each embedding sentence: {sentence_embeddings[0].shape}')","07d44181":"num_clusters = 6\nclustering_model = KMeans(n_clusters=num_clusters,\n                          n_init=10,\n                          max_iter=300,\n                          tol=1e-4,\n                          random_state=98)\nclustering_model.fit(sentence_embeddings)\ncluster_assignment = clustering_model.labels_","1f90d3ab":"df_docs = pd.DataFrame(risk_sentences,columns=['Doc'])\ndf_docs['Topic'] = cluster_assignment\ndf_docs['Doc_ID'] = range(len(df_docs))\ndocs_per_topic = df_docs.groupby(['Topic'],as_index=False).agg({'Doc':' '.join})\ndocs_per_topic","d8addd13":"def c_TF_IDF(documents, m, ngram_range=(1, 2), min_df=3):\n    bow = CountVectorizer(ngram_range=ngram_range, \n                          stop_words=\"english\",\n                          min_df = min_df,)\n    bow.fit(documents)\n    bow_matrix = bow.transform(documents).toarray()\n    w = np.sum(bow_matrix,axis=1)\n    tf = np.divide(bow_matrix.T, w)\n    sum_t = np.sum(bow_matrix,axis=0)\n    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n    tf_idf = np.multiply(tf, idf)\n    \n    return tf_idf, bow\n\nc_tf_idf, bow = c_TF_IDF(docs_per_topic.Doc.values,m=len(risk_sentences))\nprint(f'The shape of cluster tf-idf matrix is {c_tf_idf.shape}')","5f7c3df8":"def extract_top_n_words_per_topic(tf_idf, bow, docs_per_topic, n=20):\n    words = bow.get_feature_names()\n    labels = list(docs_per_topic.Topic)\n    tf_idf_transposed = tf_idf.T\n    indices = tf_idf_transposed.argsort()[:, -n:]\n    top_n_words = {}\n    for i,label in enumerate(labels):\n        for j in indices[i]:\n            if label not in top_n_words:\n                top_n_words[label] = [(words[j],tf_idf_transposed[i][j])]\n            else:\n                top_n_words[label].append((words[j],tf_idf_transposed[i][j]))\n        top_n_words[label] = top_n_words[label][::-1]\n    \n    return top_n_words\n\ntop_n_words = extract_top_n_words_per_topic(c_tf_idf,bow,docs_per_topic)\n#print('Top ten words in topic 1 are:')\n#pprint.pprint(top_n_words[0][:20])","fc2f5d68":"fig = plt.figure(figsize=(16,15))\nfor label,word_prob_pairs in top_n_words.items():\n    top_15_words_prob_pairs = dict(word_prob_pairs[:15])\n    #probs = [word_score_pair[1] for word_score_pair in word_score_pairs]\n    ax= fig.add_subplot(3,2,label+1)\n    wcloud = wordcloud.WordCloud(background_color='white')\n    wcloud.generate_from_frequencies(top_15_words_prob_pairs)\n    ax.imshow(wcloud)\n    for pos in ['left','right','top','bottom']:\n        ax.spines[pos].set_color(None)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f'Topic {label+1}' ,fontsize=14)","8d058874":"ENCODING_DIM = 32\ninput_embedding = tf.keras.layers.Input(shape=(sentence_embeddings.shape[1],))\nencoded = tf.keras.layers.Dense(128,activation='relu')(input_embedding)\nencoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\nencoded = tf.keras.layers.Dense(ENCODING_DIM,activation='relu')(encoded)\n\ndecoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\ndecoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\ndecoded = tf.keras.layers.Dense(sentence_embeddings.shape[1], activation='linear')(decoded)\n\nautoencoder = tf.keras.models.Model(input_embedding, decoded)\nautoencoder.compile(optimizer='adam', loss='mse')","13c4f3b9":"autoencoder.fit(sentence_embeddings, \n                sentence_embeddings, \n                epochs=500, \n                batch_size=128, \n                shuffle=True,)","0502997a":"encoder = tf.keras.models.Model(input_embedding,encoded)\nencoded_sentence_embeddings = encoder.predict(sentence_embeddings)","b7650175":"tSNE = TSNE(n_components=2,init='pca',random_state=98)\n#embedding_matrix = tSNE.fit_transform(encoded_sentence_embeddings)\nembedding_matrix = tSNE.fit_transform(sentence_embeddings)\n\ndf_embedding = pd.DataFrame(embedding_matrix, columns=['X_TSNE','Y_TSNE'])\ncluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', \n                  3: 'red', 4: 'skyblue', 5:'salmon'}\ndf_embedding['topic_idx'] =  cluster_assignment\ndf_embedding['color'] = df_embedding['topic_idx'].map(cluster_colors)\ncluster_labels = {0: 'Transmission of Virus', 1: 'Death', 2: 'Severe Respiratory Symdrone', \n                  3: 'Healthcare Service', 4: 'Mental Health', 5:'Treatment methods'}\ndf_embedding['label'] = df_embedding['topic_idx'].map(cluster_labels)\ndf_embedding['article_title'] = list(risk_article['title'])\n\noutput_notebook()\n\nsource = ColumnDataSource(\n    dict(x = df_embedding['X_TSNE'],\n         y = df_embedding['Y_TSNE'],\n         color=df_embedding['color'],\n         label=df_embedding['label'],\n         topic_key= df_embedding['topic_idx'],\n         title= df_embedding['article_title'],\n         #content = \n        ))\n\nlda_plot = figure(title='T-SNE Clustering of Documents: Raw Embedding', \n              plot_width=800, plot_height=600,\n              tools='pan,wheel_zoom,box_zoom,reset,hover', \n              x_axis_type=None, y_axis_type=None)\nlda_plot.scatter(x='x', y='y', source=source, \n                 legend='label', color='color',alpha=0.7, size=10)\nhover = lda_plot.select(dict(type=HoverTool))\nhover.tooltips = {'content':'Title: @title'}\nlda_plot.legend.location = 'bottom_right'\nshow(lda_plot)","77400d74":"tSNE = TSNE(n_components=2,init='pca',random_state=98)\n#embedding_matrix = tSNE.fit_transform(encoded_sentence_embeddings)\nembedding_matrix = tSNE.fit_transform(encoded_sentence_embeddings)\n\ndf_embedding = pd.DataFrame(embedding_matrix, columns=['X_TSNE','Y_TSNE'])\ncluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', \n                  3: 'red', 4: 'skyblue', 5:'salmon'}\ndf_embedding['topic_idx'] =  cluster_assignment\ndf_embedding['color'] = df_embedding['topic_idx'].map(cluster_colors)\ncluster_labels = {0: 'Transmission of Virus', 1: 'Death', 2: 'Severe Respiratory Symdrone', \n                  3: 'Healthcare Service', 4: 'Mental Health', 5:'Treatment methods'}\ndf_embedding['label'] = df_embedding['topic_idx'].map(cluster_labels)\ndf_embedding['article_title'] = list(risk_article['title'])\n\noutput_notebook()\n\nsource = ColumnDataSource(\n    dict(x = df_embedding['X_TSNE'],\n         y = df_embedding['Y_TSNE'],\n         color=df_embedding['color'],\n         label=df_embedding['label'],\n         topic_key= df_embedding['topic_idx'],\n         title= df_embedding['article_title'],\n         #content = \n        ))\n\nlda_plot = figure(title='T-SNE Clustering of Documents: Embeddings after Dimension Reduction', \n              plot_width=800, plot_height=600,\n              tools='pan,wheel_zoom,box_zoom,reset,hover', \n              x_axis_type=None, y_axis_type=None)\nlda_plot.scatter(x='x', y='y', source=source, \n                 legend='label', color='color',alpha=0.7, size=10)\nhover = lda_plot.select(dict(type=HoverTool))\nhover.tooltips = {'content':'Title: @title'}\nlda_plot.legend.location = 'bottom_right'\nshow(lda_plot)","595d93c1":"1. The first topic is similar with topic 4 generated by LDA, talking about the transmission dynamic of the virus. \n2. The second topic is similar with topic 6 generated by LDA, talking about the death\n3. The third topic is similar with topic 3 generated by LDA, but seems more specific here, talking about one of the severe symptoms: respiratory .\n4. The fourth topic is similar with topic 5 generated by LDA, but seems more specific here, talking about one of the public service: healthcare service.\n5. The fifth topic is similar with topic 2 generated by LDA, but seems more specific here, talking about negative emotions like anxiety and distree.\n6. The sixth topic is probably similar with topic 1 generated by LDA, talking about the treatment methods.","4b3ad917":"**Why Choose T-SNE?**\n\n'T-SNE gives the impression that it has classified the data by bringing it to two-dimensions but in reality, it doesn\u2019t reduce the dimensions. It is a visualizer, which tells how each class is distributed and is there any overlap between them'. An intuition of T-SNE from the article on Medium: [PCA vs LDA vs T-SNE \u2014 Let\u2019s Understand the difference between them](https:\/\/medium.com\/analytics-vidhya\/pca-vs-lda-vs-t-sne-lets-understand-the-difference-between-them-22fa6b9be9d0)","e27d8511":"As we can see from the interactive figure above, topic 4 and 5 (1 and 4 in the figure) are very close. The other topics are separated appropriately.","2288294c":"## Dimension Reduction with AutoEncoder","cc204ea9":"# Data Preprocessing","8359b064":"For topic modeling in python, LDA model are available and fast to implement with the help of either two libraries (sklearn and genism). ","c880b9ea":"# Topic Extraction I: LDA","95cc6c30":"sklearn LDA pipeline: CounterVectorizer + LatentDirichletAllocation + GridSearchCV","00fbf35f":"## Embedding Texts with SentenceBert","add71ee8":"Warning: Although the articles with duplicated abstracts are dropped, there still exist some articles with meaningless abstracts. The cleaning strategy here only weakens the effects of those meaningless abstracts.  ","c73728d8":"To sum up, the topics extracted from these two methods are similar, but the topics from the second method are more specific.","067fdf2b":"I use this [blog](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html) as a guidance to build the autoencoder","11f9bc83":"# Conclusion","c10b8125":"## Topic per Document","82a5f3c6":"Here I create inverted index. Inverted index is a fundamental technology used commonly in search engine .  Inverted index assigns each word with a list of document id and it helps retrieve the articles with keyword matching fastly. For example {'patient':[1,3,6]} means that the word patient occurs in the second,fourth and sixth document in the corpus.","0a93cade":"## Word per Topic","2ec7d4f9":"**What's next?**\n\n1. Combine LDA with Bert embedding for better clustering results.\n2. Try larger topic numbers","e790bd2c":"Words or phrases like 'risk', 'result', 'patient', 'infection', 'disease', 'study' occur frequently in this corpus.","dcadcabd":"The label of each article is decided by the topic with highest probability. According to the potential risk factors that Task 2 mentions and the topic inference acquired from the word-per-topic part, I lable each topic with some 'real ideas'.\n\nAs we can see from the figure, there is not too much overlap between documents of different clusters(topics) but the clusters are very close with each other. ","a439f624":"LDA is always a traditional and go-to algorithm for topic analysis. You can extract real ideas from topics and cluster documents based on topics easily with this algorithm. The advantage of LDA is that you can get relatively clear boundaries when clustering the documents while the disadvantage of LDA is that the real ideas from topics are somewhat ambiguous because it is not easy to tune the hyparameters  of this unsupervised learning algorithm. ","5da1c377":"# Article Matching and Deep Cleaning","92b40dfe":"1. The first topic is probably talking about treatment methods. We can see the word like 'vaccine','drug','treatment','procedure'. \n2. The second topic is probably talking about mental health of people during panademic. We can see the word like 'mental_health','panademic'.\n3. The third topic is probably talking about the severity of disease . We can see the word like 'severe','patient','infection'.\n4. The fourth topic is probably talking about the transmission dynamics of the virus. We can see the word like 'transmission','country','population'.\n5. The fifth topic is probably talking about the public service and work during panademic. We can see the word like 'care','service','support','work'.\n6. The sixth topic is probably talking about the death. We can see the word like 'mortality','death','conclusion'.","a398f789":"Bert and its modified versions nowadays outperform everything and achieve state-of-art results in all kinds of NLP tasks. Hence, SentenceBert can give very good representation of the documents. The advantage of the second method is that the real ideas from topics are more meaningful and interpretable while the disadvantage of it is that the decision boundary of documents clustering is not clear.","8e4cdb66":"gensim pipeline: Dictionary + doc2bow + LdaModel + CoherenceModel","c8f922c9":"# Fundamental Ideas Preview","26e06d28":"For topic modeling and visualization, lemmatization is needed. POS tagging technique is also used to filter meaningful words. ","0a4de525":"## Clustering Documents with K-Means Clustering","0ea0dc0f":"The boundaries between documents of different topics are not very clear, in both raw embeddings or embeddings after dimension reduction.","7233a2eb":"In this notebook, I use two methods to do topic analysis on the CORD-19 articles in 2021. Hope you enjoy it!","cd23fb87":"The higher the c_v coherence score is, the more suitable the topic number should be. However, the coherence score will vary if we run the iteration different times. Hence, I choose 6 as the topic number for analysis.","a6361487":"Thanks to this amazing [articles](https:\/\/towardsdatascience.com\/topic-modeling-with-bert-779f7db187e6) from Medium . The author of it invented a modified TF-IDF alogirhtm called 'class-based variant of TF-IDF' to help extract the topics from clustered documents. If you find it interesting, do not forget to clap for that article :)","57302da8":"## Document Clustering Visualization: Raw Embedding vs Dimension Reduction","025f0515":"# Topic Extraction II: SentenceBert + K-Means Clustering + TF-IDF","c43350ee":"**1. Text Cleaning and Preprocessing**\n\n**2. Inverted Index**\n\n**3. Latent Dirichlet Allocation**\n\n**4. T-SNE**\n\n**5. Bert**\n\n**6. K-Means Clustering**\n\n**7. TF-IDF**\n\n**8. AutoEncoder**","a1ca678f":"## Extract Important Words in Topics with c-TF-IDF","759105b8":"First, let's see how the word cloud of this corpus looks like.","364804fe":"1. The left figure shows that there are very few articles cover all the six topics or only one topic. \n2. The right figure shows that occurence frequencies of topics are very close ."}}