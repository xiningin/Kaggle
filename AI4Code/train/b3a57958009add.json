{"cell_type":{"9f432c7d":"code","8588c9ba":"code","eb72bd26":"code","e534e185":"code","28060f14":"code","785bbbf6":"code","672988cf":"code","162edda5":"code","5cdfd581":"code","2ca02489":"code","56e6ff2b":"code","664a5b86":"code","975b2ac6":"code","595ca5f9":"code","c08bea6f":"code","88318026":"code","59a0bb37":"code","6a9afb74":"code","e9fcf347":"code","bee7b5ff":"code","62dea4bd":"code","80a17b6b":"code","86151e5b":"code","61c79dc7":"code","31b49f8f":"code","fd555123":"code","75250458":"code","553d9d98":"code","74bb0201":"code","a0b2ab48":"code","0269c0ac":"code","af44862a":"code","cd0135b5":"code","95a989a8":"markdown","69933a15":"markdown"},"source":{"9f432c7d":"from contextlib import contextmanager\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom pathlib import Path\nfrom scipy import stats\nimport pandas_profiling \nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport os, gc, re\nimport warnings\nimport logging\nimport random\nimport time\nimport cuml\nimport sys\n\n\n\nseeds = [2021, 42]\nwarnings.simplefilter('ignore')\nsns.set()\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline ","8588c9ba":"def apply_pallete(colors):\n    customPalette = sns.set_palette(sns.color_palette(colors))\n    sns.palplot(sns.color_palette(colors),size=0.5)\n    plt.tick_params(axis='both', labelsize=0, length=0)\npallete = [\"#6930c3\",\"#ff7f51\",\"#aa4465\",\"#ffa3a5\", \"#5e60ce\",\"#ff9b54\",\"#dd2d4a\",\"#f49cbb\", \"#0096c7\",\"#ffbf69\",\"#f26a8d\",\"#ffcbf2\",\"#48cae4\",\"#e9b827\",\"#f49cbb\",\"#e2afff\",\"#ade8f4\",\"#f9e576\",\"#ff86c8\"]\napply_pallete(pallete)","eb72bd26":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\nprint(f'train: {train.shape}, test: {test.shape}')","e534e185":"profile = train.profile_report(progress_bar=False,\n                               missing_diagrams=None,\n                               correlations={\n                                   'pearson': {'calculate': True},\n                                   'spearman': {'calculate': False},\n                                   'kendall': {'calculate': False},\n                                   'phi_k': {'calculate': True},\n                                   'cramers': {'calculate': True},\n                               }\n                              )\nprofile.set_variable('html.style',\n    {\n      'primary_color': '#5e60ce',\n      'full_width': True,\n    }\n)","28060f14":"profile","785bbbf6":"plt.figure(figsize=(5, 7.5))\ntrain.target.value_counts().add_prefix('target_').plot(kind='bar')\nplt.title('Target Count')\nplt.show()","672988cf":"plt.figure(figsize=(5, 7.5))\ntrain.drop(['id', 'target'], axis=1).dtypes.value_counts().add_suffix('_dtype').plot(kind='bar')\nplt.title('DataType Count')\nplt.show()","162edda5":"dense_features = [f for f in train.columns if 'cont' in f]\nlow_cardinality_sparse_features = [f for f in train.columns if 'cat' in f and train[f].nunique()<17]\nhigh_cardinality_sparse_features = [f for f in train.columns if 'cat' in f and train[f].nunique()>=17]\nsparse_features = [f for f in train.columns if 'cat' in f]\ntarget = ['target']\n","5cdfd581":"def autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2.,\n                1.05*height,\n                '%d' % int(height),\n                ha='center',\n                va='bottom',\n                size=14)\n","2ca02489":"fig, ax = plt.subplots(figsize=(12,6))\nbarp = ax.bar(sparse_features, train[sparse_features].nunique())\nax.set_ylabel('Number of unique categories')\nax.set_xlabel('Variables')\nax.set_title('Train Cardinality')\nax.set_ylim([0,350])\nax.tick_params(axis='x', rotation=45)\nautolabel(barp)\n","56e6ff2b":"fig, ax = plt.subplots(figsize=(12,6))\nbarp = ax.bar(sparse_features, test[sparse_features].nunique())\nax.set_ylabel('Number of unique categories')\nax.set_xlabel('Variables')\nax.set_title('Test Cardinality')\nax.set_ylim([0,350])\nax.tick_params(axis='x', rotation=45)\nautolabel(barp)","664a5b86":"plt.figure(figsize=(20,8))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(train[dense_features]), palette=pallete)\nplt.title(f'Train Continous')\nplt.show()","975b2ac6":"plt.figure(figsize=(20,8))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(test[dense_features]), palette=pallete)\nplt.title(f'Test Continous')\nplt.show()","595ca5f9":"vertical_offset_median = 0.025\nvertical_offset_max = 0.0125 \nvertical_offset_min = -0.025 \nplt.figure()\nfig, ax = plt.subplots(3, 4,figsize=(16, 30))\nfor i, c in enumerate(dense_features, start=1):\n    plt.subplot(3, 4, i)\n    data = pd.melt(pd.concat([train[[c]].rename(columns={c:f'{c}_tr'}),test[[c]].rename(columns={c:f'{c}_te'})]))\n    medians = data.groupby(['variable'])['value'].median().round(4)\n    mins = data.groupby(['variable'])['value'].min().round(4)\n    maxs = data.groupby(['variable'])['value'].max().round(4)\n    box_plot = sns.boxplot(x=\"variable\", y=\"value\", data=data, palette=pallete)\n    for xtick in box_plot.get_xticks():\n        box_plot.text(xtick, medians.max() + vertical_offset_median, medians[xtick], \n                horizontalalignment='center', size='large', color='black', weight='semibold')\n        box_plot.text(xtick, maxs.max() + vertical_offset_max, maxs[xtick], \n                horizontalalignment='center', size='large', color='black', weight='semibold')\n        box_plot.text(xtick, mins.min() + vertical_offset_min, mins[xtick], \n                horizontalalignment='center', size='large', color='black', weight='semibold')\n    plt.xlabel(c, fontsize=15)\n    plt.ylabel(None)\nplt.show()","c08bea6f":"for c in low_cardinality_sparse_features:\n    melt_df = train[dense_features+[c]].melt(id_vars=c, value_name='Value', var_name='Numeric Feaures')\n    fig, ax = plt.subplots(figsize=(20,8))\n    sns.boxplot(data=melt_df, x='Numeric Feaures',  y='Value', hue=c, ax=ax, palette=pallete)\n    ax.legend(loc=\"lower left\", ncol = len(ax.lines))\n    ax.set_ylabel(None)\n    ax.set_title(f'{c}')\n    frame = plt.gca()\n    frame.axes.get_xaxis().set_label_text(None)\n","88318026":"for cat in low_cardinality_sparse_features:\n    df = train.groupby(cat).agg(\n        freq = ('target',lambda x: x.count()\/len(train)),\n        mean_target = ('target',lambda x:x.mean())\n            ).plot(kind='bar', figsize=(5+train[cat].nunique(),7.5))\n    plt.title(f'{cat}')\n    plt.ylabel('Frequency & Target Mean')\n\n","59a0bb37":"for c in dense_features:\n    train[f'q_{c}'], bins_ = pd.qcut(train[c], 10, retbins=True, labels=[i for i in range(10)])\n    melt_df = train[target+[f'q_{c}']].melt(id_vars=f'q_{c}', value_name='Target', var_name='Value')\n    fig, ax = plt.subplots(figsize=(20,8))\n    sns.barplot(data=melt_df, x='Value',  y='Target', hue=f'q_{c}', ax=ax, palette=pallete)\n    ax.legend(loc=\"lower left\", ncol = len(ax.lines))\n    ax.set_ylabel('Mean Target', fontsize=15)\n    ax.set_xlabel(None)\n    ax.set_title(f'Quantized {c}')\n    frame = plt.gca()\n    frame.axes.get_xaxis().set_visible(False)\n","6a9afb74":"train_ = train.sample(frac=0.05, random_state=seeds[0])\nplt.figure()\nfig, ax = plt.subplots(4, 3,figsize=(12, 16))\nplt.suptitle('Continous Train Features', fontsize=18)\nfor i, col in enumerate(dense_features, start=1):\n    plt.subplot(4, 3, i)\n    sns.distplot(train_[col],color=\"darkblue\", kde=True, bins=60, label=col)\n    plt.xlabel(col, fontsize=9)\nplt.show();","e9fcf347":"test_ = test.sample(frac=0.05, random_state=seeds[0])\nplt.figure()\nfig, ax = plt.subplots(4, 3,figsize=(12, 16))\nplt.suptitle('Continous Test Features', fontsize=18)\nfor i, col in enumerate(dense_features, start=1):\n    plt.subplot(4, 3, i)\n    sns.distplot(test_[col],color=\"darkcyan\", kde=True, bins=60, label=col)\n    plt.xlabel(col, fontsize=9)\nplt.show();","bee7b5ff":"benc_features = []\nfor col in sparse_features:\n    benc = ce.BinaryEncoder(cols=col, drop_invariant=True)\n    benc.fit(train[col])\n    train_be = benc.transform(train[col])\n    be_features = train_be.columns.tolist()\n    train[be_features] = train_be.values\n    benc_features.extend(be_features)\n","62dea4bd":"features = benc_features + dense_features\ntrain_ = train[features + target].sample(frac=0.05, random_state=seeds[0])\n","80a17b6b":"model = cuml.cluster.KMeans(init=\"k-means||\",\n                            n_clusters=4,\n                            oversampling_factor=40,\n                            random_state=seeds[0])\ny_pred = model.fit_predict(train_[features].astype(np.float32))\n","86151e5b":"pca = cuml.decomposition.PCA(random_state=seeds[0], n_components=2, whiten=False)\npca_tr = pca.fit_transform(train_[features].astype(np.float32))\ntrain_['pca_1'] = pca_tr[:,0]\ntrain_['pca_2'] = pca_tr[:,1]","61c79dc7":"\ncmap = plt.cm.get_cmap(\"Accent\")\nplt.figure(figsize=(15,15))\nplt.scatter(train_.pca_1, train_.pca_2, c=y_pred, s=10, cmap=cmap)\nplt.legend()\nplt.title('PCA-KMEANS')\nplt.show()","31b49f8f":"cmap = plt.cm.get_cmap(\"flag\")\nplt.figure(figsize=(15,15))\nplt.scatter(train_.pca_1, train_.pca_2, c=train_.target, s=10, cmap=cmap)\nplt.legend()\nplt.title('PCA-TARGET')\nplt.show()","fd555123":"model = cuml.TSNE(n_components = 2, perplexity=20.0, random_state=seeds[0])\ntsne_embed  = model.fit_transform(train_[features].astype(np.float32))\ntrain_['tsne_1'] = tsne_embed[:,0]\ntrain_['tsne_2'] = tsne_embed[:,1]","75250458":"plt.figure(figsize=(16,16))\ndf1 = train_.loc[train_.target==1]\nplt.scatter(df1.tsne_1, df1.tsne_2, color='blue', s=10, label='Positive Target')\ndf2 = train_.loc[train_.target==0]\nplt.scatter(df2.tsne_1, df2.tsne_2, color='orange', s=10, label='Negative Target')\nplt.legend()\nplt.title('TSNE-TARGET')\nplt.show()","553d9d98":"mapper = cuml.manifold.UMAP(n_neighbors=20, n_components=2, n_epochs=500, learning_rate=0.5, random_state=seeds[0]).fit(train_[features])\numap_embed = mapper.transform(train_[features])\ntrain_['umap_1'] = umap_embed[:,0]\ntrain_['umap_2'] = umap_embed[:,1]","74bb0201":"cmap = plt.cm.get_cmap(\"Accent\")\nplt.figure(figsize=(15,15))\nplt.scatter(train_.umap_1, train_.umap_2, c=y_pred, s=10, cmap=cmap)\nplt.legend()\nplt.title('UMAP-KMEANS')\nplt.show()","a0b2ab48":"cmap = plt.cm.get_cmap(\"flag\")\nplt.figure(figsize=(15,15))\nplt.scatter(train_.umap_1, train_.umap_2, c=train_.target, s=10, cmap=cmap)\nplt.legend()\nplt.title('UMAP-TARGET')\nplt.show()","0269c0ac":"!pip install --no-warn-conflicts -q --upgrade xgboost","af44862a":"from sklearn.model_selection import StratifiedKFold\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nfrom lightgbm import LGBMClassifier\nfrom IPython.display import display\nfrom scipy.stats import rankdata\nfrom xgboost import DMatrix\nimport lightgbm as lgbm\nimport xgboost as xgb \nimport pandas as pd\nimport numpy as np\n\nseeds = [2021, 0, 42]\nn_splits = 10\nshuffle=True\niterations = 50000\nearly_stopping_rounds = 400\nverbose_eval = 0\nbaseline_rounds = 1\ncb_learning_rate = 0.006\nxgb_learning_rate = 0.01\nn_bags = 1\nn_models = 3\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\n\ncat_cols = [feature for feature in train.columns if 'cat' in feature]\ncont_cols = [feature for feature in train.columns if 'con' in feature]\n\nfor col in cat_cols:\n    train_only = list(set(train[col].unique()) - set(test[col].unique()))\n    test_only = list(set(test[col].unique()) - set(train[col].unique()))\n    both = list(set(test[col].unique()).union(set(train[col].unique())))\n    train.loc[train[col].isin(train_only), col] = np.nan\n    test.loc[test[col].isin(test_only), col] = np.nan\n    mode = train[col].mode().values[0]\n    train[col] = train[col].fillna(mode)\n    test[col] = test[col].fillna(mode)\n    \nX_train = train.drop(['id', 'target'], axis=1)\nX_test_ = test.drop(['id'], axis=1)\ny_train = train.target\ntest_id = test.id\nltrn = len(train)\nlte = len(test)\n\nfor col in cat_cols:\n    label_encoder = LabelEncoder().fit(X_train[col])\n    X_train[col] = label_encoder.transform(X_train[col])\n    X_test_[col] = label_encoder.transform(X_test_[col])\n    X_train[col] = X_train[col].astype('category')\n    X_test_[col] = X_test_[col].astype('category')\n    \nleave_one_out_cols = []\nfor col in cat_cols:\n    leave_one_out_encoder = LeaveOneOutEncoder().fit(X_train[col].astype('str'), y_train)\n    new_col = f'{col}_leave_one_out'\n    X_train[new_col] = leave_one_out_encoder.transform(X_train[col].astype('str'))\n    X_test_[new_col] = leave_one_out_encoder.transform(X_test_[col].astype('str'))\n    leave_one_out_cols.append(new_col)\n    \ndefault_cols = cat_cols + cont_cols\nreplaced_cat_cols_with_loo = leave_one_out_cols + cont_cols\n\n\nX_test = X_test_[default_cols]\nX_test_loo = X_test_[replaced_cat_cols_with_loo]\nbags_oof = np.zeros((ltrn, n_bags*n_models))\nbags_preds = np.zeros((lte, n_bags*n_models))\n\nxgb_params= defaultdict(seed=seeds[0], objective= 'binary:logistic',max_depth= 14,eta= xgb_learning_rate,colsample_bytree= 0.4,subsample= 0.8, alpha=7.5,gamma=0.75 ,min_child_weight= 1.5, max_bin=333,n_jobs= 2,eval_metric='logloss',tree_method= 'gpu_hist',gpu_id= 0, predictor= 'gpu_predictor',)\ncb_params = defaultdict(random_seed=seeds[0], iterations=iterations, learning_rate=cb_learning_rate, depth=7, bootstrap_type='Bernoulli',random_strength=1, min_data_in_leaf=2, l2_leaf_reg=9, loss_function='Logloss', eval_metric='AUC', grow_policy='Depthwise',max_bin=1024, model_size_reg= 0,task_type= 'GPU',od_type='IncToDec',od_wait=100,verbose=verbose_eval,subsample=0.8,od_pval=1e-10,max_ctr_complexity= 8,has_time= False,simple_ctr = 'FeatureFreq',combinations_ctr= 'FeatureFreq')\nlgbm_params = defaultdict(seed=seeds[0], max_depth= 75, subsample= 0.85, colsample_bytree= 0.25, learning_rate= 0.01,reg_lambda= 7.5,reg_alpha= 0.75,min_child_samples= 200,num_leaves= 200, max_bin= 777,cat_smooth= 80,cat_l2= 5,metric= 'auc',n_jobs= -1,verbose=-1)\n   \nfor bag in range(n_bags):\n    oof_cb = np.zeros((ltrn))\n    oof_xgb = np.zeros((ltrn))\n    oof_cbx = np.zeros((ltrn))\n    oof_xgbx = np.zeros((ltrn))\n    oof_lgb = np.zeros((ltrn))\n    for fold, (train_idx, val_idx) in enumerate(StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=seeds[bag]).split(X_train, y_train)):\n        print(f'Bag {bag+1} Fold {fold+1}')\n        X_tr = X_train[default_cols].iloc[train_idx]\n        X_val = X_train[default_cols].iloc[val_idx]\n        X_tr_loo = X_train[replaced_cat_cols_with_loo].iloc[train_idx]\n        X_val_loo = X_train[replaced_cat_cols_with_loo].iloc[val_idx]\n        y_tr = y_train.iloc[train_idx]\n        y_val = y_train.iloc[val_idx]\n        ltr = len(X_tr)\n        lv = len(X_val)\n        ptrain = Pool(data=X_tr, label=y_tr, cat_features=[x for x in range(len(cat_cols))])\n        pvalid = Pool(data=X_val, label=y_val, cat_features=[x for x in range(len(cat_cols))])\n        ptest = Pool(data=X_test, cat_features=[x for x in range(len(cat_cols))])\n        CModel = CatBoostClassifier(**cb_params)\n        CModel.fit(ptrain,\n                   eval_set=pvalid,\n                   use_best_model=True,\n                   early_stopping_rounds=early_stopping_rounds)\n        temp_fold_preds = rankdata(CModel.predict_proba(pvalid)[:,1])\/lv\n        oof_cb[val_idx] = temp_fold_preds\n        first_cb_auc = roc_auc_score(y_val, temp_fold_preds)\n        print(f'AUC of CB model is {first_cb_auc}')\n        baseline_preds_tr_cb = rankdata(CModel.predict_proba(ptrain)[:,1])\/ltr\n        baseline_preds_vl_cb = temp_fold_preds\n        test_preds_cb = rankdata(CModel.predict_proba(ptest)[:,1])\/lte  \n        \n        xtrain = DMatrix(data=X_tr, label=y_tr, nthread=2, enable_categorical=True)\n        xvalid = DMatrix(data=X_val, label=y_val, nthread=2, enable_categorical=True)\n        xtest = DMatrix(data=X_test, nthread=2, enable_categorical=True)\n        XModel = xgb.train(xgb_params, xtrain,\n                           evals=[(xvalid,'validation')],\n                           verbose_eval=verbose_eval,\n                           early_stopping_rounds=early_stopping_rounds,\n                           xgb_model=None,\n                           num_boost_round=iterations)\n        temp_fold_preds = rankdata(XModel.predict(xvalid))\/lv\n        oof_xgb[val_idx] = temp_fold_preds\n        first_xgb_auc = roc_auc_score(y_val, temp_fold_preds)\n        print(f'AUC of XGB model with categorical data is {first_xgb_auc}')\n        baseline_preds_tr_xgb = rankdata(XModel.predict(xtrain))\/ltr\n        baseline_preds_vl_xgb = temp_fold_preds\n        test_preds_xgb = rankdata(XModel.predict(xtest))\/lte\n        \n        ltrain = lgbm.Dataset(X_tr, label=y_tr, init_score=None, categorical_feature=cat_cols)\n        lvalid = lgbm.Dataset(X_val, label=y_val, init_score=None, categorical_feature=cat_cols)\n        ltest =  lgbm.Dataset(X_test, label=y_val, init_score=None, categorical_feature=cat_cols)\n        LModel = lgbm.train(lgbm_params,\n                        train_set=ltrain,\n                        num_boost_round=iterations,\n                        valid_sets=lvalid, \n                        init_model=None,\n                        early_stopping_rounds=early_stopping_rounds,\n                        categorical_feature=cat_cols,\n\n                        verbose_eval=False)           \n        temp_fold_preds = rankdata(LModel.predict(X_val))\/lv\n        oof_lgb[val_idx] = temp_fold_preds\n        test_preds_lgb = rankdata(LModel.predict(X_test))\/lte\n        bags_oof[val_idx,bag*n_models] = temp_fold_preds\n        bags_preds[:,bag*n_models] = test_preds_lgb\n        first_lgb_auc = roc_auc_score(y_val, temp_fold_preds)\n        print(f'AUC of LGBM model is {first_lgb_auc}')    \n        baseline_preds_tr_lgb = rankdata(LModel.predict(X_tr))\/ltr\n        baseline_preds_vl_lgb = temp_fold_preds\n        test_preds_lgb = test_preds_lgb\n    \n        baseline_train = (baseline_preds_tr_xgb+baseline_preds_tr_lgb+baseline_preds_tr_cb)\/3\n        baseline_valid = (baseline_preds_vl_xgb+baseline_preds_vl_lgb+baseline_preds_vl_cb)\/3\n        baseline_test = (test_preds_xgb+test_preds_lgb+test_preds_cb)\/3\n    \n        for baseline in range(baseline_rounds):\n            ptrain = Pool(data=X_tr, label=y_tr, cat_features=[x for x in range(len(cat_cols))], baseline=baseline_train)\n            pvalid = Pool(data=X_val, label=y_val, cat_features=[x for x in range(len(cat_cols))], baseline=baseline_valid)\n            ptest = Pool(data=X_test, cat_features=[x for x in range(len(cat_cols))], baseline=baseline_test)\n            cb_params_ = cb_params.copy()\n            cb_params_.update({'learning_rate': cb_learning_rate*(1\/(baseline+1))})\n            CModel = CatBoostClassifier(**cb_params_)\n            CModel.fit(ptrain, \n                       eval_set=pvalid,\n                       use_best_model=True,\n                       early_stopping_rounds=early_stopping_rounds)\n            temp_fold_preds = rankdata(CModel.predict_proba(pvalid)[:,1])\/lv\n            oof_cbx[val_idx] = temp_fold_preds\n            second_cb_auc = roc_auc_score(y_val, temp_fold_preds)\n            print(f'AUC of CB model with baseline round is {baseline+1} {second_cb_auc}')   \n            baseline_train = rankdata(CModel.predict_proba(ptrain)[:,1])\/ltr\n            baseline_valid = temp_fold_preds\n            baseline_test = rankdata(CModel.predict_proba(ptest)[:,1])\/lte\n            if baseline == baseline_rounds - 1:\n                bags_oof[val_idx,bag*n_models+1] = temp_fold_preds\n                bags_preds[:,bag*n_models+1] = baseline_test\n        \n            xtrain = DMatrix(data=X_tr_loo, label=y_tr, enable_categorical=False, base_margin=baseline_train)\n            xvalid = DMatrix(data=X_val_loo, label=y_val, enable_categorical=False, base_margin=baseline_valid)\n            xtest =  DMatrix(data=X_test_loo, enable_categorical=False, base_margin=baseline_test)\n            xgb_params_ = xgb_params.copy()\n            xgb_params_.update({'learning_rate': xgb_learning_rate*(1\/(baseline+1))})\n            XModel = xgb.train(xgb_params_, xtrain,\n                               evals=[(xvalid,'validation')],\n                               verbose_eval=verbose_eval,\n                               early_stopping_rounds=early_stopping_rounds,\n                               xgb_model=None,\n                               num_boost_round=iterations)\n            temp_fold_preds = rankdata(XModel.predict(xvalid))\/lv\n            oof_xgbx[val_idx] = temp_fold_preds\n            baseline_train = rankdata(XModel.predict(xtrain))\/ltr\n            baseline_valid = temp_fold_preds\n            baseline_test = rankdata(XModel.predict(xtest))\/lte\n            if baseline == baseline_rounds - 1:\n                bags_oof[val_idx,bag*n_models+2] = temp_fold_preds\n                bags_preds[:,bag*n_models+2] = baseline_test\n            second_xgb_auc = roc_auc_score(y_val, temp_fold_preds)\n            print(f'AUC of XGB model with baseline round is {baseline+1} {second_xgb_auc}')\n            print('*' * 75)\n    bag_auc = roc_auc_score(y_train, np.mean(bags_oof[:,bag*n_models:(bag+1)*n_models], axis=1))\n    print(f'AUC of average bag {bag+1} models is {bag_auc}')\n    print('#' * 80)\n","cd0135b5":"final_auc = roc_auc_score(y_train, np.mean(bags_oof, axis=1))\nprint(f'AUC of average of bags is {final_auc}')\nsubmission = pd.DataFrame({'id':test_id,'target':np.mean(bags_preds, axis=1)})\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head(3))\nnp.save('bags_oof', bags_oof)\nnp.save('bags_preds', bags_preds)","95a989a8":"A Simple Blend of **XGB-CB-LGBM**","69933a15":"A minimalistic **EDA**  "}}