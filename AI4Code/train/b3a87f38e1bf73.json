{"cell_type":{"099d88e2":"code","e56d69b9":"code","bbf64aa4":"code","f5af74d9":"code","cf9a22f0":"code","23f9be4d":"code","08c79a5e":"code","1271675f":"code","0dc19ecd":"code","4d322298":"code","9b9aff26":"code","16d55c44":"code","530bca25":"code","d82e251c":"code","7f6e663d":"code","b191e220":"code","f9083ffb":"code","58f99263":"code","2c7dfaff":"code","8f545f29":"code","0750774b":"code","9b5d5f63":"code","b2c8e4f7":"markdown","ff49d02f":"markdown","f7fc4206":"markdown","68d499c6":"markdown","9f1ad9b8":"markdown","43fc45b6":"markdown","f9249de3":"markdown","c688f966":"markdown","8029c612":"markdown","d89eb637":"markdown","74e1c4a9":"markdown","001eba31":"markdown","8b460f03":"markdown","42d4a968":"markdown","a163a5a6":"markdown","7acfa5c3":"markdown","ce236e2d":"markdown","5d647277":"markdown","ea0828c0":"markdown","eb2e11e9":"markdown"},"source":{"099d88e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score,train_test_split,RepeatedKFold,KFold,GridSearchCV\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,mean_squared_error,classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e56d69b9":"ss=StandardScaler()\nsvm=SVC()\nknn=KNeighborsClassifier()\nrf=RandomForestClassifier()\nad=AdaBoostClassifier()\nxg=XGBClassifier()","bbf64aa4":"dt=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nx=dt.iloc[:,:-1]\ny=dt.iloc[:,-1]\n\n# Splitting the dataset into train and test set in 80\/20 ration for evaluating the model performance \nX_train, X_test, y_train, y_test=train_test_split(x,y,test_size=0.2,random_state=42)\nX_test.reset_index(drop=True,inplace=True)\ny_test.reset_index(drop=True,inplace=True)\nX_train.reset_index(drop=True,inplace=True)\ny_train.reset_index(drop=True,inplace=True)\n\n\nfinal_X_test=X_test.to_numpy()\nfinal_y_test=y_test.to_numpy()\nx=X_train\ny=y_train\n","f5af74d9":"\nkf=KFold(n_splits=10)\n\nfinal_error={}\nindices={}\n\nk=x.shape[1]\nfor i in range(1,k+1):\n    sk=SelectKBest(chi2, k=i)\n    data_temp=sk.fit_transform(x, y)\n    indices[i]=(list(sk.get_support(True)))\n\n    error=list()\n    for train_index,test_index in kf.split(data_temp):\n        X_train,y_train=data_temp[train_index],y[train_index]\n        X_test,y_test=data_temp[test_index],y[test_index]\n        X_train=ss.fit_transform(X_train)\n        X_test=ss.transform(X_test)\n        knn.fit(X_train,y_train)\n        error.append(mean_squared_error(y_test,knn.predict(X_test)))\n    final_error[i]=error[0]","cf9a22f0":"print('Optimal features are: ',pd.DataFrame(indices.values(),index=final_error.values()).sort_index().iloc[0,:].values)\nplt.plot(np.array(list(final_error.keys())),final_error.values())","23f9be4d":"temp=pd.DataFrame(indices.values(),index=final_error.values()).sort_index().iloc[0,:]\nind=list(temp[temp.notnull()].values.astype(int))\n","08c79a5e":"xcopy=x.iloc[:,ind]\nxcopy.columns=list(range(xcopy.shape[1]))\nxcopy=xcopy.to_numpy()\nfinal_X_test=final_X_test[:,ind]","1271675f":"models=[svm,knn,rf,ad]\nrkf=RepeatedKFold(n_splits=10,n_repeats=15)\nresult=[]\nfor train_index,test_index in rkf.split(xcopy):\n    X_TRAIN,y_TRAIN=xcopy[train_index],y[train_index]\n    X_TEST,y_TEST=xcopy[test_index],y[test_index]\n    X_TRAIN=ss.fit_transform(X_TRAIN)\n    X_TEST=ss.transform(X_TEST)\n    temp=list()\n    for model in models:\n        model.fit(X_TRAIN,y_TRAIN)\n        temp.append(accuracy_score(y_TEST,model.predict(X_TEST)))\n    result.append(temp)\n        ","0dc19ecd":"result_frame=pd.DataFrame(result)\n\nresult=[]\nfor i in range(0,result_frame.shape[0],10):\n    result.append(result_frame.iloc[i:i+10,:].mean())\n\nresult=pd.DataFrame(result)\nresult.columns=['svm','knn','rf','ad']\nresult\n# print('\\n')\n#","4d322298":" print(\"The mean result of the 15 iterations are: \",result.mean())\n","9b9aff26":"x_transformed=ss.fit_transform(xcopy)\ntransformed_final_X_test=ss.transform(final_X_test)\nfor model in models:\n    model.fit(x_transformed,y)\n    target=['class 0', 'class 2']\n    print('for {} the results are: '.format(model))\n    print('\\n')\n    print(classification_report(final_y_test,model.predict(transformed_final_X_test),target_names=target))\n    print('\\n')\n    print('\\n')\n\n","16d55c44":"parameters = {'kernel':['rbf','poly','linear'], 'C':[0.1,0.5,1,5,10,100],'gamma':('scale','auto')}\nclf = GridSearchCV(svm, parameters, refit = True, verbose = 1)\nclf.fit(x_transformed,y)\n\nsorted(clf.cv_results_.keys())\n","530bca25":"svm_param=clf.best_params_\nsvm_param","d82e251c":"n_estimators = [int(x) for x in np.linspace(start = 5, stop = 50, num = 10)]\nmax_depth = [int(x) for x in np.linspace(1, 50, num = 10)]\n# max_depth.append(None)\nmin_samples_split = [2, 5,8]\nmin_samples_leaf = [1, 2,6]\nparameters = {'n_estimators': n_estimators,\n               \n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\n\n\nclf = GridSearchCV(rf, parameters, refit = True, verbose = 1)\nclf.fit(x_transformed,y)\n\nsorted(clf.cv_results_.keys())\n","7f6e663d":"rf_param=clf.best_params_\nrf_param","b191e220":"knn=KNeighborsClassifier()\nleaf_size = list(range(1,40,4))\nn_neighbors = list(range(1,30,3))\np=[1,2,4]\nweights=['uniform', 'distance']\nparameters={'leaf_size':leaf_size,'n_neighbors':n_neighbors,'p':p}\n\nclf=GridSearchCV(knn,param_grid=parameters,refit=True,verbose=1)\nclf.fit(x_transformed,y)\n\nsorted(clf.cv_results_.keys())\n\n","f9083ffb":"knn_param=clf.best_params_\nknn_param","58f99263":"# AdaBoostClassifier()\nn_estimators=list(range(1,40,5))\nlearning_rate=[0.01,0.1,0.6,1]\nparameters={'n_estimators':n_estimators,'learning_rate':learning_rate}\nclf=GridSearchCV(ad,param_grid=parameters,refit=True,verbose=1)\nclf.fit(x_transformed,y)\n\nsorted(clf.cv_results_.keys())\n","2c7dfaff":"ad_param=clf.best_params_\nad_param","8f545f29":"svm=SVC(C= svm_param['C'], gamma= svm_param['gamma'], kernel= svm_param['kernel'])\nknn=KNeighborsClassifier(leaf_size= knn_param['leaf_size'], n_neighbors= knn_param['n_neighbors'], p=knn_param['p'])\nrf=RandomForestClassifier(max_depth= rf_param['max_depth'],min_samples_leaf= rf_param['min_samples_leaf'], min_samples_split= rf_param['min_samples_split'],n_estimators= rf_param['n_estimators'])\nad=AdaBoostClassifier(learning_rate= ad_param['learning_rate'], n_estimators= ad_param['n_estimators'])\n\nmodels=[svm,knn,rf,ad]\n\nfor model in models:\n    model.fit(x_transformed,y)\n    target=['class 0', 'class 2']\n    print('for {} the results are: '.format(model))\n    print('\\n')\n    print(classification_report(final_y_test,model.predict(transformed_final_X_test),target_names=target))\n    print('\\n')\n    print('\\n')\n","0750774b":"\nrkf=RepeatedKFold(n_splits=10,n_repeats=15)\nresult=[]\nfor train_index,test_index in rkf.split(xcopy):\n    X_TRAIN,y_TRAIN=xcopy[train_index],y[train_index]\n    X_TEST,y_TEST=xcopy[test_index],y[test_index]\n    X_TRAIN=ss.fit_transform(X_TRAIN)\n    X_TEST=ss.transform(X_TEST)\n    temp=list()\n    for model in models:\n        model.fit(X_TRAIN,y_TRAIN)\n        temp.append(accuracy_score(y_TEST,model.predict(X_TEST)))\n    result.append(temp)\nresult_frame=pd.DataFrame(result)","9b5d5f63":"result=[]\nfor i in range(0,result_frame.shape[0],10):\n    result.append(result_frame.iloc[i:i+10,:].mean())\n\nresult=pd.DataFrame(result)\nresult.columns=['svm','knn','rf','ad']\nresult.mean()","b2c8e4f7":"## Now we are training different models like SVM, KNearestNeighbour, RandomForest, AdaBoost with the selected features in the training set only. We are using RepeatedKfold class and splitting the training set into 10 folds and we are repeating all evaluations for 15 iterations. After splitting we transformed the data using standard scalar and finally we are using accuracy as the performance metrics to evaluate our models performances    ","ff49d02f":"## Loading the required libraries","f7fc4206":"## Here,we are splitting the dataset into two sets i.e. train and test set.","68d499c6":"## The obtianed accuracy with differnt models in each of the 15 iterations are shown below in the Dataframe:","9f1ad9b8":"## Now i am just checking the model performance on the test set that we created above. The result may be biased because it may happens that the distribution of test set is not as same as the train set or this test set is easier or difficult for the model to recognize. I am just doing it to avoid any Data leakage and just for getting the whole classification report on a test set.","43fc45b6":"## Here i am again doing tests on the Train set with 10 fold validation to cross check the results.","f9249de3":"## Now we will do some hyperparameters tuning of the models that we are using to check how much it can impact the performance of our models. I am using GridSearchCV for this. You can use other methods for this also.","c688f966":"## Creating the objects of the classes we require further","8029c612":"## 3. KNN\n","d89eb637":"## 2. Random Forest  ","74e1c4a9":"1. ### age      >>   age\n1. ### sex      >>   sex\n1. ### cp       >>   chest pain type (4 values)\n1. ### trestbps >>   resting blood pressure\n1. ### chol     >>   serum cholestoral in mg\/dl\n1. ### fbs      >>   fasting blood sugar > 120 mg\/dl\n1. ### restecg  >>   resting electrocardiographic results (values 0,1,2)\n1. ### thalach  >>   maximum heart rate achieved\n1. ### exang    >>   exercise induced angina\n1. ### oldpeak  >>   ST depression induced by exercise relative to rest\n1. ### slope    >>   the slope of the peak exercise ST segment\n1. ### ca       >>   number of major vessels (0-3) colored by flourosopy\n1. ### thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n1. ### target   >>   0 indicates healthy and 1 indicates illness","001eba31":"## Now using these hyperparamters, i will again test the models on the test set as i did above with to check whether performance imroved or not. ","8b460f03":"## Just vary the random state value while splitting the dataset into train\/test set and see the results before and after tuning.","42d4a968":"## Above are the classification reports of each classifier. We don't need to use accuracy_score, precision, recall, f1 score separately. We can do it in just one go using the above used class. Isn't this COOL!.","a163a5a6":"## Using only the selected columns from the given dataset. ","7acfa5c3":"## Next, we will perform feature selection on the given dataset to find the best features to do our classification task based on their score using SelectKBest class of sklearn. To decide the value of k we will plot the mean squared error curve at different values of k. We are performing this operation on only the train set with 10 fold validation and here we are using KNearestNeighbour as our base model for getting the final scores","ce236e2d":"## 4. AdaBoost","5d647277":"## Above shown arre the classification reports of each classifier trained on the transformed train set and tested on the test set. ","ea0828c0":"### 1. SVM","eb2e11e9":"## Finally, we can see that for the test set evaluation case, the accuracy of all classiefiers besides SVM increased but in the case of 10 fold validation on training data, the accuracy only in case of AdaBoost classifier increased."}}