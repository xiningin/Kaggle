{"cell_type":{"f46faa00":"code","b581974a":"code","d51b9717":"code","d15b328f":"code","a900147c":"code","e7a9f166":"code","c55a0cae":"code","f773d876":"code","7f812c7e":"markdown","1a63a70e":"markdown","0ededa6c":"markdown","760a305d":"markdown","feaa253a":"markdown","6c4665ad":"markdown","5d23c44e":"markdown","e839e882":"markdown","8f5da1d9":"markdown"},"source":{"f46faa00":"import os\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping","b581974a":"PATH = r'..\/input\/hand-sign-recognition\/original_images\/original_images'\n\nclasses_wo_digits = os.listdir(PATH)\n\nclasses_wo_digits.sort()\n\nclasses_wo_digits = classes_wo_digits[10:]\n\nprint(classes_wo_digits)","d51b9717":"datagen = keras.preprocessing.image.ImageDataGenerator(\n                                            rescale=1.\/255, \n                                            validation_split=0.2\n                                                       )\n\ntrain_generator = datagen.flow_from_directory(\n                                        PATH,\n                                        target_size=(256, 256),\n                                        seed=51,\n                                        subset='training', \n                                        classes=classes_wo_digits\n                                              )\n\nvalidation_generator = datagen.flow_from_directory(\n                                        PATH,\n                                        target_size=(256, 256), \n                                        seed=51,\n                                        subset='validation', \n                                        shuffle=False, \n                                        classes=classes_wo_digits\n                                                   )","d15b328f":"fig, axes = plt.subplots(4, 8, sharex=True, figsize=(10, 10))\n\nplt.subplots_adjust(bottom=0.1, left=0.01, right=0.99,\n                    top=0.9, hspace=0.35)\n\nfor i in range(4):\n    for j in range(8):\n        plt.gray()\n        axes[i, j].imshow(train_generator[0][0][8*i + j])\n        axes[i, j].set_title(\n          classes_wo_digits[\n              train_generator[0][1][8*i + j].argmax()\n                            ],\n          weight='bold', size=16)\n        axes[i, j].set_xticks([])\n        axes[i, j].set_yticks([])\n\nplt.show()","a900147c":"xception = Xception(include_top=False, input_shape=(256, 256, 3))\n\nx = xception.output\nx = layers.GlobalMaxPooling2D()(x)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dense(512, activation='relu')(x)\noutput = layers.Dense(31, activation='softmax')(x)\n\nmodel = Model(xception.input, output)\n\n# Freezing all the Imported Layers\nfor layers in xception.layers:\n    layers.trainable = False\n    \nmodel.compile(optimizer='adam', loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nearlystop = EarlyStopping(monitor='val_accuracy', patience=20,\n                          verbose=1)","e7a9f166":"hist = model.fit(train_generator, \n                 validation_data=validation_generator,\n                 steps_per_epoch=100, validation_steps=10,\n                 epochs=50, callbacks=[earlystop])","c55a0cae":"plt.figure(figsize=(8,6))\nplt.plot(hist.history[\"accuracy\"])\nplt.plot(hist.history['val_accuracy'])\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title(\"Model Metrics\", weight='bold', size=18)\nplt.ylabel(\"Accuracy \/ Loss\", size=14)\nplt.xlabel(\"Epoch\", size=14)\nplt.legend([\"Training Accuracy\", \"Validation Accuracy\",\n            \"Training Loss\", \"Validation Loss\"])\nplt.show()","f773d876":"result = model.evaluate(validation_generator, steps=280)","7f812c7e":"**The model predict correctly allmost 100% of the valdiation data!**","1a63a70e":"# Training:","0ededa6c":"Here is an example of the first batch:","760a305d":"# Preprocessing:\nThe dataset contains 41 different signs represented as folders, with 1600 images for each sign - 26 alphabets, 10 digits and 5 extra symbols which are used to perform ancillary operations. For this work, I didn't use the digits because some of them are identical to letters (for example 0 and o, 5 and z) and the only way to distinguish them is by context (not in scope of this work).","feaa253a":"# Evaluating:","6c4665ad":"I split the data to \"train\" (80%) and \"validation\" (20%) using Keras's ImageDataGenerator:","5d23c44e":"In this notebook I will build a model to detect and recognize American Manual Alphabet (AMA). A model that can understand sign language can help improve the lives of many people and recognizing the Alphabet is a good start for it.\nI will use the Xception model that trained on ImageNet dataset and it will demonstrate the power of Transfer Learning. Xception is a deep convolutional neural network architecture developed by Google researchers. It's an efficient architecture that relies on two main points:\n- Depthwise Separable Convolution.\n- Shortcuts between Convolution blocks as in ResNet.\n\nXception model got an extraordinary results on ImageNet dataset with less then 20% parameters than VGG16. With my Xception based model I got an accuracy result of allmost 100% on the validation data!\n","e839e882":"**The model finished training after only half of the epochs!**","8f5da1d9":"# Building The Model:\nI imported the Xception model from keras with weights pre-trained on ImageNet. I didn't use the original top of the model and instead added 3 fully connected (FC) layers (the last one with units as the number of classes). The only trainable weights are the ones of the FC layers."}}