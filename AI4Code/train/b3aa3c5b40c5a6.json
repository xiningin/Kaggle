{"cell_type":{"ffccc42e":"code","4f3d906b":"code","412bbff0":"code","9b70fc8a":"code","dedfc113":"code","7b0e11aa":"code","c0de1f7e":"code","6de2476d":"code","ee2a04c4":"code","efd7da53":"code","f3ae3c6c":"code","0b0c6e1c":"code","d1b9fc47":"code","6416dae5":"code","0f90754e":"code","5c89558b":"code","0c8ee804":"code","0c8f5dd6":"code","8c5f0248":"code","450ff21f":"code","b241f504":"code","6c1eae5f":"code","79aadf0f":"code","c502e364":"code","4f38f0b5":"code","cf068f13":"code","b9d00d84":"markdown","3d4e7c54":"markdown","dd9485f5":"markdown","6d7ae0aa":"markdown","4d410808":"markdown","ed066165":"markdown","f810f079":"markdown","1766b0c7":"markdown","19aafc9e":"markdown","a1b129cf":"markdown","ded02a6f":"markdown","4d4f489d":"markdown","86d01e69":"markdown","86736e09":"markdown","3a8a31b2":"markdown","caae5ab7":"markdown","df542145":"markdown","f1c809a1":"markdown","c9d5f87c":"markdown","716c453e":"markdown","5977717a":"markdown","8f4ce108":"markdown","af6a5683":"markdown"},"source":{"ffccc42e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns   \n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.impute import SimpleImputer as Imputer\nfrom sklearn.preprocessing import LabelBinarizer # OneHot Encoder\nfrom sklearn.base import BaseEstimator, TransformerMixin # Custom Transformer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","4f3d906b":"cwd = os.getcwd()\nprint(cwd)","412bbff0":"\nhousing = pd.read_csv(\"..\/input\/california-house-price-prediction\/housing.csv\")\r\nprint(housing.shape)\r\nhousing.head()","9b70fc8a":"housing.info()","dedfc113":"housing['ocean_proximity'].value_counts()","7b0e11aa":"housing.describe()","c0de1f7e":"housing.hist(bins=50, figsize=(20,15))","6de2476d":"housing['income_cat'] = np.ceil(housing['median_income']\/1.5)\r\nhousing['income_cat'].where(housing['income_cat'] < 5, 5.0, inplace=True) # Replace values where the condition is False.\r\nsns.displot(housing['income_cat'])","ee2a04c4":"split_tuple = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\r\nfor (train_index, test_index) in split_tuple.split(housing, housing['income_cat']):\r\n    housing_test = housing.loc[test_index]\r\n    housing_train = housing.loc[train_index]\r\n\r\nstrat_check_housing = housing['income_cat'].value_counts() \/ len(housing)\r\nstrat_check_test = housing_test['income_cat'].value_counts() \/ len(housing_test)\r\nprint(strat_check_housing, strat_check_test)","efd7da53":"housing_test.drop(['income_cat'], axis=1, inplace=True)\r\nhousing_train.drop(['income_cat'], axis=1, inplace=True)","f3ae3c6c":"housing_train_1 = housing_train.copy()\r\nhousing_train_1.plot.scatter(x='longitude', y='latitude', alpha = 0.4, s=housing_train_1['population']\/100,\r\n                                            c='median_house_value', cmap=plt.get_cmap(\"jet\"), colorbar=True)","0b0c6e1c":"corr_matrix = housing_train_1.corr()\r\nprint(corr_matrix['median_house_value'].sort_values(ascending=False))\r\nhm = sns.heatmap(corr_matrix, vmax=0.8, annot = True, cmap = 'RdYlGn')","d1b9fc47":"pair_plot = housing_train_1[[\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]]\r\nsns.pairplot(pair_plot)","6416dae5":"housing_train_1.plot.scatter(x='median_income', y='median_house_value', alpha=0.1)","0f90754e":"housing_train_1.columns","5c89558b":"housing_train_1['rooms_per_houshold'] = housing_train_1['total_rooms']\/housing_train_1['households']\r\nhousing_train_1['bedrooms_per_room'] = housing_train_1['total_bedrooms']\/housing_train_1['total_rooms']\r\nhousing_train_1['population_per_household'] = housing_train_1['population']\/housing_train_1['households']\r\ncorr_matrix = housing_train_1.corr()\r\ncorr_matrix['median_house_value'].sort_values(ascending=False)","0c8ee804":"train_set = housing_train.drop('median_house_value', axis=1)\r\ntrain_set_labels = housing_train['median_house_value'].copy()","0c8f5dd6":"train_set_num = train_set.drop('ocean_proximity', axis=1)\r\n\r\nimputer = Imputer(strategy=\"median\") # Creating an Imputer instance of the class SimpleImputer\r\nimputer.fit(train_set_num) # Call the fit method of Imputer class; This calculates the median of all the attributes\r\nX = imputer.transform(train_set_num) # All the missing values in the train set are replaced by their corresponding medians and returns a plain numpy array\r\n\r\ntrain_set_num_tr = pd.DataFrame(X, columns=train_set_num.columns)\r\n\r\ntrain_set_num_tr.columns","8c5f0248":"rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\r\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\r\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\r\n        self.add_bedrooms_per_room = add_bedrooms_per_room\r\n    def fit(self, X, y=None):\r\n        return self # nothing else to do\r\n    def transform(self, X, y=None):\r\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\r\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\r\n        if self.add_bedrooms_per_room:\r\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\r\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\r\n        else:\r\n            return np.c_[X, rooms_per_household, population_per_household]\r\n\r\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\r\nhousing_extra_attribs = attr_adder.transform(train_set_num_tr.values)\r\ntrain_set_num_xAttr = pd.DataFrame(housing_extra_attribs, columns=['longitude', 'latitude', 'housing_median_age', \r\n                            'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income',\r\n                            'rooms_per_household', 'population_per_household'])\r\n","450ff21f":"train_set_num_xAttr_scaled = StandardScaler().fit_transform(train_set_num_xAttr)\r\ntrain_set_num_xAttr_scaled_df = pd.DataFrame(train_set_num_xAttr_scaled, columns=['longitude', 'latitude', 'housing_median_age', \r\n                            'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income',\r\n                            'rooms_per_household', 'population_per_household'])\r\ntrain_set_num_xAttr_scaled_df","b241f504":"train_set_cat = train_set['ocean_proximity'].copy()\r\nencoder = LabelBinarizer()\r\ntrain_set_cat_1H = encoder.fit_transform(train_set_cat)\r\ntrain_set_cat_1H_df = pd.DataFrame(train_set_cat_1H, columns=['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND'])\r\ntrain_set_cat_1H_df","6c1eae5f":"train_set_tr = pd.concat([train_set_num_xAttr_scaled_df, train_set_cat_1H_df], axis=1)\r\ntrain_set_tr","79aadf0f":"lr = LinearRegression()\r\nlr.fit(train_set_tr, train_set_labels)","c502e364":"print(\"Predictions:\\t\", lr.predict(train_set_tr.iloc[:5]))\r\nprint(\"Labels:\\t\\t\", list(train_set_labels.iloc[:5]))","4f38f0b5":"housing_predictions = lr.predict(train_set_tr)\r\nlin_mse = mean_squared_error(train_set_labels, housing_predictions)\r\nlin_rmse = np.sqrt(lin_mse)\r\nprint(\"Coefficients: \\t\", lr.coef_)\r\nprint(\"RMSE\\t\", lin_rmse)","cf068f13":"lr_CV = cross_val_score(lr, train_set_tr, train_set_labels, scoring='neg_mean_squared_error', cv=10)\r\n\r\nlr_CV_rmse = np.sqrt(-lr_CV)\r\nprint(\"CV Scores:\\t\", lr_CV_rmse)\r\nprint(\"Mean\\t\", lr_CV_rmse.mean())\r\nprint(\"STD\\t\", lr_CV_rmse.std())\r\n","b9d00d84":"For further understanding the distribution of the numerical features, let's plot the histogram for the entire dataset.","3d4e7c54":"## Data Cleaning: Numerical Attributes\r\nWe saw that total bedrooms attribute had missing values. Hence we try to overcome this issue by filling the missing values with the median of the attribute.\r\n\r\nAs mean can be computed for only numerical attributes, other categorical attributes (here: ocean_proximity) must be dropped","dd9485f5":"We can see that there are several levels in the median income data; as shown by the series of straight line dots on 500,000, 350,000 and other lower levels too.\r\n\r\nOther than median income data some attributes like total rooms, total bedrooms and population seems to have no correlation as it makes no sense in determining correlation between district level cumulative data v\/s median house value. Hence let's introduce new derived features:","6d7ae0aa":"The root mean squared error comes out to 68,911 which is very high. Let's crossvalidate the training set with k = 10.","4d410808":"After importing all the required packages, let's load the dataset from the directory","ed066165":"Combining the transformed numerical and categorical attributes:","f810f079":"The Lattitude v\/s Longitude scatter plot resembles the map of California. In particular, areas like Bay Area and San Diego are highly dense. It is clear that the price of houses also depend on the closeness to sea and population density.\r\n\r\n## Correlation with other variables\r\n### Computing the standard Pearson's Correlation Coefficient","1766b0c7":"# California Housing Prices Dataset \r\n## This was based on data from the 1990 California census.","19aafc9e":"Pearson's coefficient shows that only median income is strongly correlated with house value, but this measures only the linear correlations. In order examine non-linear correlations, lets plot each attribute against each other.","a1b129cf":"As the median income attribute looks to have a promising relationship with the median housing value, lets plot it separately","ded02a6f":"The housing dataset has 10 features and 20640 observations (instances of data). Each observation represents one district. Here, a district is the smallest block set by the US Department of Census. Now, lets try to understand the dataset more deeper:","4d4f489d":"Let's start exploring further into the data through visualisations. Most importantly, make sure you have made a copy of the training set.","86d01e69":"Let's try checking the actual and predicted median house values","86736e09":"As we saw earlier that almost all the numerical attributes are have a huge range; Ex; total number of rooms range from 6 to 39,320 while the median income only range from 0 to 15. We use StandardScalar to scale these values from 0 to 1.","3a8a31b2":"It's clear that there are 9 numerical (float type) features and 1 categorical (object type) features. Note that 'total_bedrooms' has only 20433 non-null values, i.e., 207 districts doesnot have this feature. This is needed to be taken care of. Now, let's understand the categorical variable (ocean_proximity) a bit deeper:","caae5ab7":"The Linear Resgression model has underfit the data because: (1) the parameters do not describe the target to a great extent (2) the LR model is not powerful enough","df542145":"Now that we have stratified sampling based on the income category. We can can use Scikit-Learn\u2019s StratifiedShuffleSplit class:","f1c809a1":"As we donot have separate test set, we divide the current set into 2 portions: training set and test set in the ratio 80:20.","c9d5f87c":"Rooms per houshold and bedrooms per room tend to have more correlation than rooms and bedrooms as such. \r\n\r\n## Preparing the data for the ML model\r\nSeparate the training set into data and target:","716c453e":"## Training the Model - Linear Regression","5977717a":"One of the advantages of using stratified sampling is that the test set retains the same proportion from each strata. Here the dataset is split on the basis of income category and the proportion remains the same as shown above. Now, let's remove the income_cat attribute in both the training and test sets to get back the original data.","8f4ce108":"# Data Cleaning: Categorical Attributes\r\nEncoding the labels of the categorical attributes to convert it to numbers: OneHot Encoder","af6a5683":"The feature 'ocean_proximity' has 5 categories, namely: <1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND.\r\nNow, let's understand the other numerical features:"}}