{"cell_type":{"495f16d1":"code","afcfb448":"code","9258cd60":"code","7dea63ea":"code","4788c751":"code","f99be8ef":"code","11f20f28":"code","97cd18a2":"code","98933efa":"code","efc8bd3a":"code","9a521bf9":"code","e523a7dc":"code","922d1655":"code","fe827af4":"code","dd7e5ca9":"code","5f675bb0":"code","231b7356":"code","ac52b0d9":"code","12673f27":"code","558c7ed5":"code","9263f7d1":"code","c8fa96b7":"code","55ac31fa":"code","e93a3a0a":"code","67a0a643":"code","c1f0f341":"code","b4edc5f8":"code","042f0a0d":"code","f9288de5":"code","d33c8328":"code","006464a7":"code","9451d53c":"code","89701906":"code","aadf06df":"code","c3c29825":"code","0001998d":"code","3c9935f0":"code","27ec8270":"code","bd4aae4a":"code","1e0c489b":"code","9904ca01":"code","57189b0f":"code","2594a511":"code","066a8288":"code","fd3ccbf2":"code","6b6d5e66":"code","b5a76b95":"code","b63ee65d":"code","f2915a28":"code","35bb290d":"markdown","199ffbf1":"markdown","921c1377":"markdown","8a91026d":"markdown","cd6637fa":"markdown","df6bbcb9":"markdown","63611f81":"markdown","a340772d":"markdown","bc06f21c":"markdown","12eb9061":"markdown","33de2fcc":"markdown","8cfd3f62":"markdown","77c8b4b3":"markdown","dbd73c3a":"markdown","82014406":"markdown","45766cc8":"markdown","de6a06e5":"markdown","7876c4b1":"markdown","828b0139":"markdown","0de00fd9":"markdown","cb958c3d":"markdown","0fd71671":"markdown","2b117176":"markdown","e47b2b92":"markdown","c3734383":"markdown","f73b7a0a":"markdown","7d7acc38":"markdown","25838d07":"markdown","62349f95":"markdown","21d1f9b4":"markdown","9cd20db3":"markdown","21f5dcba":"markdown","6583d1c9":"markdown","5537e5bd":"markdown","e92f5767":"markdown","a1c810b1":"markdown","c81c0b58":"markdown","851349a2":"markdown","d774db01":"markdown","a0eca8dc":"markdown"},"source":{"495f16d1":"#Importing general libraries\nimport numpy as np\nimport pandas as pd\nimport os\n\n#Importing visualization libraries and setting the parameters\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams.update({'font.size': 16})\n\n#Importing machine learning libraries\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","afcfb448":"#Taking a look at the training set\ndata_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_train.info()","9258cd60":"number_missing = data_train.isna().sum()\nmissing_columns = number_missing[number_missing > 0].sort_values(ascending = False)\nprint('the columns with missing values are: {}'.format(missing_columns.index.tolist()))\n_ = missing_columns.plot(kind = 'bar',figsize=(15,5))","7dea63ea":"(missing_columns['GarageYrBlt'] == missing_columns['GarageType'] == \\\nmissing_columns['GarageFinish'] == missing_columns['GarageQual'] == missing_columns['GarageCond']) ","4788c751":"missing_columns[missing_columns < 10]","f99be8ef":"data_train[missing_columns.index.tolist()].info()","11f20f28":"data_train_cleaned = data_train.copy().drop(columns='Id')\ndata_train_cleaned['Electrical'] = data_train_cleaned['Electrical'].fillna(data_train_cleaned['Electrical'].mode()[0])\ndata_train_cleaned[['GarageYrBlt','MasVnrArea']] = data_train_cleaned[['GarageYrBlt','MasVnrArea']].fillna(0)\ndata_train_cleaned.loc[:, data_train_cleaned.columns != 'LotFrontage'] = data_train_cleaned.loc[:, data_train_cleaned.columns != 'LotFrontage'].fillna('NA')\nprint('There are {} missing values in the dataset now.'.format(data_train_cleaned.isna().sum().sum()))","97cd18a2":"mean_stds = {}\nfor col in data_train_cleaned.columns:\n    if data_train_cleaned[col].dtype == object:\n        grouped_data = data_train_cleaned.groupby(col)['LotFrontage']\n        mean_stds[col] = (grouped_data.count()*grouped_data.std()).sum()\/data_train_cleaned.shape[0]\n\ncol_min_LotFrontage_std = min(mean_stds,key=mean_stds.get)\nprint('The best column to group the data and impute the LotFrontage column is \"{}\".'.format(col_min_LotFrontage_std))\n\ndata_train_cleaned['LotFrontage'] = data_train_cleaned.groupby(col_min_LotFrontage_std)['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nprint('After this final imputation, there are {} missing values in the dataset.'.format(data_train_cleaned.isna().sum().sum()))","98933efa":"no_pool_area = data_train_cleaned[data_train_cleaned['PoolQC'] == 'NA']['PoolArea'].mean()\nno_masonry_area = data_train_cleaned[data_train_cleaned['MasVnrType'] == 'NA']['MasVnrArea'].mean()\nprint('Mean pool area when there is no pool in the house: {}. \\nMean masonry area when there is no masonry in the house: {}.'.format(no_pool_area,no_masonry_area))","efc8bd3a":"cat_cols = ['MSSubClass']\nnum_cols = []\nfor col in data_train_cleaned.columns:\n    if data_train_cleaned[col].dtype == object:\n              cat_cols = cat_cols + [col]\n    else:\n        num_cols = num_cols + [col]\n        \nnum_cols.remove('MSSubClass')\nnum_cols.remove('SalePrice')\n            \nget_cat_data = FunctionTransformer(lambda x: x[cat_cols].values, validate=False)\nget_num_data = FunctionTransformer(lambda x: x[num_cols].values, validate=False)\n\npreprocessing_cat = Pipeline([('selector', get_cat_data),('encoder',OneHotEncoder(handle_unknown='ignore'))])\npreprocessing_num = Pipeline([('selector', get_num_data),('scaler',StandardScaler())])\n\npreprocessing_all = FeatureUnion(transformer_list = [('cat_features',preprocessing_cat),('num_features',preprocessing_num)])\n                             \nmodel_ridge = Pipeline([('preprocessing',preprocessing_all),('reg',Ridge())])\nmodel_lasso = Pipeline([('preprocessing',preprocessing_all),('reg',Lasso())])","9a521bf9":"y = data_train_cleaned['SalePrice'].values\nalpha = [0.1,0.3,1,10,30,100,300]\nparam_grid = dict(reg__alpha=alpha)\n\ngrid_ridge = GridSearchCV(model_ridge, param_grid=param_grid, cv=5,scoring='neg_root_mean_squared_error')\ngrid_lasso = GridSearchCV(model_lasso, param_grid=param_grid, cv=5,scoring='neg_root_mean_squared_error')\n\ngrid_ridge_result = grid_ridge.fit(data_train_cleaned,y)\ngrid_lasso_result = grid_lasso.fit(data_train_cleaned,y)","e523a7dc":"best_score_ridge, best_params_ridge = -grid_ridge_result.best_score_,grid_ridge_result.best_params_\nbest_score_lasso, best_params_lasso = -grid_lasso_result.best_score_,grid_lasso_result.best_params_\nprint(\"Ridge - RMSE: %f using %s\" % (best_score_ridge, best_params_ridge))\nprint(\"Lasso - RMSE: %f using %s\" % (best_score_lasso, best_params_lasso))","922d1655":"data_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nnumber_missing = data_test.isna().sum()\nmissing_columns = number_missing[number_missing > 0].sort_values(ascending = False)\nprint('List of columns with missing values:\\n\\n ')\ndata_test[missing_columns.index.tolist()].info()","fe827af4":"data_test_cleaned = data_test.copy().drop(columns='Id')\nfor col in data_test_cleaned.columns:\n    if data_test_cleaned[col].dtype == object:\n              data_test_cleaned[col] = data_test_cleaned[col].fillna('NA')\n    elif col == 'LotFrontage':\n        data_test_cleaned[col] = data_test_cleaned.groupby(col_min_LotFrontage_std)[col].transform(lambda x: x.fillna(x.median()))\n    else:\n        data_test_cleaned[col] = data_test_cleaned[col].fillna(0)\n\nprint('There are {} missing values in the test dataset now.'.format(data_test_cleaned.isna().sum().sum()))","dd7e5ca9":"#Predicting the labels of the test set and saving the submission files\npredictions_ridge = grid_ridge_result.predict(data_test_cleaned)\npredictions_lasso = grid_lasso_result.predict(data_test_cleaned)\noutput_ridge = pd.DataFrame({'Id': data_test.Id, 'SalePrice': predictions_ridge})\noutput_lasso = pd.DataFrame({'Id': data_test.Id, 'SalePrice': predictions_lasso})\noutput_ridge.to_csv('simple_ridge_submission.csv', index=False)\noutput_lasso.to_csv('simple_lasso_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","5f675bb0":"#Taking a look on one of the submission files\npd.read_csv('simple_ridge_submission.csv').head()","231b7356":"_ = data_train_cleaned['SalePrice'].hist(bins=30)\n_ = plt.xlabel('Sale Price ($)')\n_ = plt.ylabel('# of houses')","ac52b0d9":"_ = np.log1p(data_train_cleaned['SalePrice']).hist(bins=30)\n_ = plt.xlabel('log(Sale Price)')\n_ = plt.ylabel('# of houses')","12673f27":"cat_indexes = np.array([])\ncategories = grid_ridge_result.best_estimator_['preprocessing'].transformer_list[0][1]['encoder'].categories_\n\nfor i in range(len(categories)):\n    feat_cat = np.array([cat_cols[i] + '_' + str(categories[i][j]) for j in range(len(categories[i])) ])\n    cat_indexes = np.append(cat_indexes,feat_cat)\n    \ncat_indexes = np.append(cat_indexes,num_cols)\n\nridge_feat_importance_series = pd.Series(grid_ridge_result.best_estimator_['reg'].coef_,index = cat_indexes)\nneg_coefs = -ridge_feat_importance_series.sort_values(ascending=False).tail(10)\npos_coefs = ridge_feat_importance_series.sort_values().tail(10)\n\nfig,ax = plt.subplots(1,2,figsize=(16,5))\nneg_coefs.plot(kind = \"barh\",color='r',ax=ax[0])\npos_coefs.plot(kind = \"barh\",color='b',ax=ax[1])\nax[0].set_title('Negatively correlated')\nax[0].set_xlabel('Coef')\nax[1].set_title('Positively correlated')\nax[1].set_xlabel('Coef')\nax[1].yaxis.tick_right()","558c7ed5":"lasso_feat_importance_series = pd.Series(grid_lasso_result.best_estimator_['reg'].coef_,index = cat_indexes)\nnon_zero_features = np.abs(lasso_feat_importance_series[np.abs(lasso_feat_importance_series) != 0]).sort_values(ascending=False)\nprint('{} features were selected out of {}\\n'.format(non_zero_features.shape[0],lasso_feat_importance_series.shape[0]))\nprint('The most important features are: \\n')\nprint(non_zero_features.head(10))","9263f7d1":"cat_list = np.abs(lasso_feat_importance_series[np.abs(lasso_feat_importance_series) > 1]).index.tolist()\nfeat_list = set([cat_list[i].split('_')[0] for i in range(len(cat_list))])\nzero_features = list(set(data_train.drop(columns=['Id','SalePrice']).columns.tolist()) - feat_list) \nprint('The model discarted the following columns from the dataset: \\n {}'.format(zero_features))","c8fa96b7":"fig,ax = plt.subplots(2,2,figsize=(14,10))\nfig.tight_layout(pad=5)\n_ = sns.regplot(x='1stFlrSF',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[0,0])\n_ = sns.regplot(x='BsmtUnfSF',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[0,1])\n_ = sns.regplot(x='EnclosedPorch',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[1,0])\n_ = sns.regplot(x='OpenPorchSF',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[1,1])","55ac31fa":"fig,ax = plt.subplots(1,3,figsize=(18,5))\nfig.tight_layout(pad=3)\n\n_ = sns.regplot(x='BsmtFinSF1',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[0])\n_ = sns.regplot(x='BsmtFinSF2',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[1])\n_ = sns.regplot(x='TotalBsmtSF',y='SalePrice',data=data_train_cleaned,scatter_kws={'alpha':0.3},ax=ax[2])","e93a3a0a":"data_train_eng = data_train_cleaned.copy()\ndata_train_eng = data_train_eng.drop(columns=['1stFlrSF','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','LowQualFinSF'])","67a0a643":"print('Enclosed Porch: {}'.format(data_train_cleaned[data_train_cleaned['EnclosedPorch']==0].shape[0]))\nprint('Open Porch: {}'.format(data_train_cleaned[data_train_cleaned['OpenPorchSF']==0].shape[0]))\nprint('Three season porch: {}'.format(data_train_cleaned[data_train_cleaned['3SsnPorch']==0].shape[0]))\nprint('Screen porch: {}'.format(data_train_cleaned[data_train_cleaned['ScreenPorch']==0].shape[0]))","c1f0f341":"data_train_eng['HasPorch'] = ~((data_train_eng['EnclosedPorch']==0) & (data_train_eng['ScreenPorch']==0) & \\\n(data_train_eng['3SsnPorch']==0) & (data_train_eng['OpenPorchSF']==0))\nprint('There are {} houses with a porch'.format(data_train_eng['HasPorch'].sum()))\n_ = sns.barplot(x='HasPorch',y='SalePrice',data=data_train_eng)","b4edc5f8":"data_train_eng['PorchSF'] = data_train_eng['EnclosedPorch'] + data_train_eng['ScreenPorch'] + \\\n                            data_train_eng['3SsnPorch'] + data_train_eng['OpenPorchSF']\n\nfig,ax = plt.subplots(1,3,figsize=(18,5))\nfig.tight_layout(pad=3)\n\n_ = sns.regplot(x='ScreenPorch',y='SalePrice',data=data_train_eng,scatter_kws={'alpha':0.3},ax=ax[0])\n_ = sns.regplot(x='3SsnPorch',y='SalePrice',data=data_train_eng,scatter_kws={'alpha':0.3},ax=ax[1])\n_ = sns.regplot(x='PorchSF',y='SalePrice',data=data_train_eng,scatter_kws={'alpha':0.3},ax=ax[2])","042f0a0d":"data_train_eng = data_train_eng.drop(columns=['PorchSF'])\n\nzero_features_cat = zero_features.copy()\nzero_features_cat.remove('1stFlrSF')\nzero_features_cat.remove('BsmtUnfSF')\nzero_features_cat.remove('EnclosedPorch')\nzero_features_cat.remove('OpenPorchSF')\n\nfig,ax = plt.subplots(3,3,figsize=(18,15))\nfig.tight_layout(pad=3)\nk=0\nfor i in range(3):\n    for j in range(3):\n        sns.boxplot(x=zero_features_cat[k],y='SalePrice',data=data_train_eng,ax=ax[i,j])\n        k += 1\n\n        \n","f9288de5":"print('Number of non-null entries in Utilities: {}\\n'.format(data_train_eng[data_train_cleaned['Utilities'] == 'NoSeWa'].shape[0]))\nprint(data_train_cleaned['MiscFeature'].value_counts())\nprint(data_train_cleaned['Alley'].value_counts())","d33c8328":"data_train_eng = data_train_eng.drop(columns=['Utilities','MiscFeature','MiscVal','Alley'])","006464a7":"_ = sns.boxplot(x='GarageQual',y='SalePrice',data=data_train_eng)","9451d53c":"fig,ax = plt.subplots(1,2,figsize=(15,5))\nfig.tight_layout(pad=3)\nprint('Lasso importance of YrSold: {}\\n'.format(lasso_feat_importance_series['YrSold']))\nprint('Lasso importance of MoSold: {}'.format(lasso_feat_importance_series['MoSold']))\n_ = sns.barplot(x='YrSold',y='SalePrice',data=data_train_eng,ax = ax[0])\n_ = sns.barplot(x='MoSold',y='SalePrice',data=data_train_eng,ax = ax[1])","89701906":"data_train_eng = data_train_eng.drop(columns=['MoSold'])","aadf06df":"fig,ax = plt.subplots(2,2,figsize=(15,10))\nfig.tight_layout(pad=3)\nprint('Lasso importance of HalfBath: {}\\n'.format(lasso_feat_importance_series['HalfBath']))\nprint('Lasso importance of FullBath: {}\\n'.format(lasso_feat_importance_series['FullBath']))\nprint('Lasso importance of BsmtHalfBath: {}\\n'.format(lasso_feat_importance_series['BsmtHalfBath']))\nprint('Lasso importance of BsmtFullBath: {}\\n'.format(lasso_feat_importance_series['BsmtFullBath']))\n_ = sns.barplot(x='HalfBath',y='SalePrice',data=data_train_eng,ax=ax[0,0])\n_ = sns.barplot(x='FullBath',y='SalePrice',data=data_train_eng,ax=ax[0,1])\n_ = sns.barplot(x='BsmtHalfBath',y='SalePrice',data=data_train_eng,ax=ax[1,0])\n_ = sns.barplot(x='BsmtFullBath',y='SalePrice',data=data_train_eng,ax=ax[1,1])","c3c29825":"data_train_eng['InsideBath'] = data_train_eng['FullBath'] + data_train_eng['HalfBath']\ndata_train_eng['BsmtBath'] =  data_train_eng['BsmtFullBath'] + data_train_eng['BsmtHalfBath']\n\nfig,ax = plt.subplots(1,2,figsize=(16,5))\n_ = sns.boxplot(x='InsideBath',y='SalePrice',data=data_train_eng,ax = ax[0])\n_ = sns.boxplot(x='BsmtBath',y='SalePrice',data=data_train_eng, ax = ax[1])\n","0001998d":"data_train_eng = data_train_eng.drop(columns=['HalfBath','FullBath','BsmtHalfBath','BsmtFullBath'])","3c9935f0":"cat_cols = ['MSSubClass','YrSold','HasPorch']\nnum_cols = []\nfor col in data_train_eng.columns:\n    if data_train_eng[col].dtype == object:\n              cat_cols = cat_cols + [col]\n    else:\n        num_cols = num_cols + [col]\n        \nnum_cols.remove('MSSubClass')\nnum_cols.remove('YrSold')\nnum_cols.remove('HasPorch')\nnum_cols.remove('SalePrice')\n            \nget_cat_data = FunctionTransformer(lambda x: x[cat_cols].values, validate=False)\nget_num_data = FunctionTransformer(lambda x: x[num_cols].values, validate=False)\n\npreprocessing_cat = Pipeline([('selector', get_cat_data),('encoder',OneHotEncoder(handle_unknown='ignore'))])\npreprocessing_num = Pipeline([('selector', get_num_data),('scaler',StandardScaler())])\n\npreprocessing_all = FeatureUnion(transformer_list = [('cat_features',preprocessing_cat),('num_features',preprocessing_num)])\n                             \nmodel_ridge = Pipeline([('preprocessing',preprocessing_all),('reg',Ridge())])\nmodel_lasso = Pipeline([('preprocessing',preprocessing_all),('reg',Lasso())])","27ec8270":"#Taking the log of the sale prices\ny = np.log1p(data_train_eng['SalePrice'].values)\nalpha = [0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1,10,30,100]\nparam_grid = dict(reg__alpha=alpha)\n\ngrid_ridge = GridSearchCV(model_ridge, param_grid=param_grid, cv=5,scoring='neg_root_mean_squared_error')\ngrid_lasso = GridSearchCV(model_lasso, param_grid=param_grid, cv=5,scoring='neg_root_mean_squared_error')\n\ngrid_ridge_result = grid_ridge.fit(data_train_eng,y)\ngrid_lasso_result = grid_lasso.fit(data_train_eng,y)","bd4aae4a":"best_score_ridge, best_params_ridge = -grid_ridge_result.best_score_,grid_ridge_result.best_params_\nbest_score_lasso, best_params_lasso = -grid_lasso_result.best_score_,grid_lasso_result.best_params_\nprint(\"Ridge - RMSLE: %f using %s\" % (best_score_ridge, best_params_ridge))\nprint(\"Lasso - RMSLE: %f using %s\" % (best_score_lasso, best_params_lasso))","1e0c489b":"data_test_eng = data_test_cleaned.copy()\n\ndata_test_eng['HasPorch'] = ~((data_test_eng['EnclosedPorch']==0) & (data_test_eng['ScreenPorch']==0) & \\\n(data_test_eng['3SsnPorch']==0) & (data_test_eng['OpenPorchSF']==0))\ndata_test_eng['InsideBath'] = data_test_eng['FullBath'] + data_test_eng['HalfBath']\ndata_test_eng['BsmtBath'] =  data_test_eng['BsmtFullBath'] + data_test_eng['BsmtHalfBath']\n\ncols_todrop = ['1stFlrSF','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','LowQualFinSF','Utilities',\\\n               'MiscFeature','Alley','MoSold','HalfBath','FullBath','BsmtHalfBath','BsmtFullBath','MiscVal']\ndata_test_eng = data_test_eng.drop(columns=cols_todrop)","9904ca01":"#Predicting the labels of the test set and saving the submission files\n#The function np.expm1 is the inverse of np.logp1\npredictions_ridge = np.expm1(grid_ridge_result.predict(data_test_eng))\npredictions_lasso = np.expm1(grid_lasso_result.predict(data_test_eng))\noutput_ridge = pd.DataFrame({'Id': data_test.Id, 'SalePrice': predictions_ridge})\noutput_lasso = pd.DataFrame({'Id': data_test.Id, 'SalePrice': predictions_lasso})\noutput_ridge.to_csv('ridge_submission.csv', index=False)\noutput_lasso.to_csv('lasso_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","57189b0f":"#Taking a look on one of the submission files\npd.read_csv('ridge_submission.csv').head()","2594a511":"data_train_eng['logSalePrice'] = np.log1p(data_train_eng['SalePrice'])\ndata_train_eng = data_train_eng.drop(columns='SalePrice')","066a8288":"n = len(num_cols)\nfig, ax = plt.subplots(n,2,figsize=(15,n*6))\nfor i in range(n):\n    sns.residplot(x=num_cols[i], y='logSalePrice', data=data_train_eng, ax=ax[i,0])\n    sns.residplot(np.log1p(data_train_eng[num_cols[i]]),data_train_eng['logSalePrice'], ax=ax[i,1])","fd3ccbf2":"\nn = len(num_cols)\nfig, ax = plt.subplots(n,2,figsize=(15,n*6))\nfor i in range(n):\n    sns.distplot(data_train_eng[num_cols[i]], ax=ax[i,0],kde=False,bins=50)\n    sns.distplot(np.log1p(data_train_eng[num_cols[i]]), ax=ax[i,1],kde=False,bins=50)","6b6d5e66":"data_train_eng['LotFrontage'] = np.log1p(data_train_eng['LotFrontage'])\ndata_train_eng['LotArea'] = np.log1p(data_train_eng['LotArea'])\ndata_train_eng['MasVnrArea'] = np.log1p(data_train_eng['MasVnrArea'])\ndata_train_eng['GrLivArea'] = np.log1p(data_train_eng['GrLivArea'])\ndata_train_eng['OpenPorchSF'] = np.log1p(data_train_eng['OpenPorchSF'])\n\ndata_train_eng['Age'] = data_train_eng['YrSold'] - data_train_eng['YearBuilt']","b5a76b95":"num_cols.append('Age')\n            \nget_cat_data = FunctionTransformer(lambda x: x[cat_cols].values, validate=False)\nget_num_data = FunctionTransformer(lambda x: x[num_cols].values, validate=False)\n\npreprocessing_cat = Pipeline([('selector', get_cat_data),('encoder',OneHotEncoder(handle_unknown='ignore'))])\npreprocessing_num = Pipeline([('selector', get_num_data),('scaler',StandardScaler())])\n\npreprocessing_all = FeatureUnion(transformer_list = [('cat_features',preprocessing_cat),('num_features',preprocessing_num)])\n                             \nmodel_ridge = Pipeline([('preprocessing',preprocessing_all),('reg',Ridge(tol=0.002,max_iter=3000))])\nmodel_lasso = Pipeline([('preprocessing',preprocessing_all),('reg',Lasso(tol=0.002,max_iter=3000))])\n\ny = data_train_eng['logSalePrice'].values\nalpha = [0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1,10,30]\nparam_grid = dict(reg__alpha=alpha)\n\ngrid_ridge = GridSearchCV(model_ridge, param_grid=param_grid, cv=5,scoring='neg_root_mean_squared_error')\ngrid_lasso = GridSearchCV(model_lasso, param_grid=param_grid, cv=5,scoring='neg_root_mean_squared_error')\n\ngrid_ridge_result = grid_ridge.fit(data_train_eng,y)\ngrid_lasso_result = grid_lasso.fit(data_train_eng,y)\n\nbest_score_ridge, best_params_ridge = -grid_ridge_result.best_score_,grid_ridge_result.best_params_\nbest_score_lasso, best_params_lasso = -grid_lasso_result.best_score_,grid_lasso_result.best_params_\nprint(\"Ridge - RMSLE: %f using %s\" % (best_score_ridge, best_params_ridge))\nprint(\"Lasso - RMSLE: %f using %s\" % (best_score_lasso, best_params_lasso))","b63ee65d":"data_test_eng['LotFrontage'] = np.log1p(data_test_eng['LotFrontage'])\ndata_test_eng['LotArea'] = np.log1p(data_test_eng['LotArea'])\ndata_test_eng['MasVnrArea'] = np.log1p(data_test_eng['MasVnrArea'])\ndata_test_eng['GrLivArea'] = np.log1p(data_test_eng['GrLivArea'])\ndata_test_eng['OpenPorchSF'] = np.log1p(data_test_eng['OpenPorchSF'])\n\ndata_test_eng['Age'] = data_test_eng['YrSold'] - data_test_eng['YearBuilt']","f2915a28":"predictions_ridge = np.expm1(grid_ridge_result.predict(data_test_eng))\npredictions_lasso = np.expm1(grid_lasso_result.predict(data_test_eng))\noutput_ridge = pd.DataFrame({'Id': data_test.Id, 'SalePrice': predictions_ridge})\noutput_lasso = pd.DataFrame({'Id': data_test.Id, 'SalePrice': predictions_lasso})\noutput_ridge.to_csv('adjusted_ridge_submission.csv', index=False)\noutput_lasso.to_csv('adjusted_lasso_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","35bb290d":"Now our score is better than before. I could also try to do more feature engineering and remove the outliers to improve the model even more, but for now that's enough.","199ffbf1":"Is there a real reason for discarding these features? Let's find out! We will start by analyzing the numerical features and then the categorical ones.","921c1377":"The Ridge and Lasso regressors have one main hyperparameter, the regularization alpha. We shall tune this hyperparameter using GridSearchCV with RMSE scoring.","8a91026d":"Now, let's see how well this simple model is able to perform on the test set. But first, we need to prepare the test set in order to guarantee that there is no missing data or other problems (impute).","cd6637fa":"# 3 - Transforming the test set to feed it to the model","df6bbcb9":"Here I define the same pipeline as before, but now I will apply to the transformed dataframe, data_train_eng. I will also predict the log of the sale price instead of the sale price, then apply the inverse transformation to obtain the predicted sale prices.","63611f81":"Before diving into feature engineering and other sophisticated methods to treat the data, it is good to see how well a machine learning model can perform on the raw data. In order to do that we will just one-hot encode the categorical features and feed the data into a regression model (we will test ridge and lasso). We first notice that OneHotEncoder treat all int values as categories, so that we will need to specify the columns which we want to do the transformation. This can be done using FunctionTransformer together with the FeatureUnion functionality of scikit-learn. Also, the 'MSSubClass' column is categorical, despite being of type int64, hence, we need to tell OneHotEncoder that this column should be treated as a category as well. So, let's create our model. The pipeline is the following: first we separate numerical and categorical features to treat them independently. We perform one-hot encoding to deal with the categorical features, then we apply the StandardScaler to the numerical features in order to properly normalize the data. We choose the StandardScaler due to its robustness to outliers. Finally, we perform a regression to the data. There is one pipeline for each regression model, but the preprocessing is the same for them all.","a340772d":"In other notebooks, people impute 'LotFrontage' with the median within the 'Neighborhood' column. Of course this makes sense, but it seemed a little arbitrary to me. I think that a good idea to impute 'LotFrontage' (or any other continuous variable) is to find the feature where its standard deviation is the lowest for each category, so that replacing the null values by the median within each category is a good estimation of the real value of 'LotFrontage'. To do that, we compute the weighted mean of the standard deviation across all columns of the dataframe and then pick the column with the lowest one to do the imputation.","bc06f21c":"The missing values appearing in the special features such as Pool quality and Miscellaneous features are a consequence of the absence of something (a pool, a fence or a garage, for example). This is evident by the fact that the number of missing features in all garage-related (or basement-related) columns are the same","12eb9061":"This dataset has 80 columnns. The meaning of each one is in the description file. For example the \"MSZoning\" column tells us what kind of zone the house is located (Agricultural, Commercial, Industrial, ...). It is really hard to get some intuition about the data by just looking at the feature descriptions (I realized it in the hard way), and it is even harder to know what are the most important features and if we can discard or transform some of them to obtain better results. So, first, let's start by understanding the missing values and trying to decide how to impute them.","33de2fcc":"### Introduction","8cfd3f62":"That does not seem a good idea. For now, we will keep all the porch area features the way it is. Let's analyze the categorical features discarted by the Lasso model.","77c8b4b3":"It seems that the neighborhood of the house is an excellent indicator of the house price. It is also interesting to see that a poor roof material or a non-flat land contour heavily push down the sale price. We also did a lasso regression on the dataset. Let's see what were the most important features in this model and what features the regressor discarted.","dbd73c3a":"Hence, all the missing values can be imputed with the category 'NA' (for categorical columns), which may be a good information for predicting the saleprice of a house. The only exception is the Electrical column, which has only one missing value, and can be imputed with the mode of the column. The numeric columns are more tricky to handle. The missing values in 'MasVnrArea' are due to non-existing masonries, so, it makes sense to impute these values with 0. The 'GarageYrBlt' column has missing values for houses with no garages and there is no obvious way of imputation, so, let's assume that a really old garage is more or less the same as a non-existing garage and also fill the nulls with zeros. We will discuss the imputation of 'LotFrontage' later.","82014406":"Notice that there are many columns with missing values, more than on the training set. In fact, in the real world we need to be prepared to deal with data that have more problems and missing information than in our training set. So, let's try to use the knowledge gained by inspecting the training set to decide how to impute the test set. Our strategy will be to impute categorical features with 'NA' and numeric features with 0, with exception of the 'LotFrontage' column, that will be imputed in the same way that we did on the training set. I know that there are some columns that could be imputed in a better way, but I will not worry about it now.","45766cc8":"Nice! This method told us that the best column to impute 'LotFrontage' with the median is the 'Neighborhood' column. Finally, let's see if our imputation is consistent with pre-existing data","de6a06e5":"The features 'MiscFeature' and 'Alley' have very poor statistics, so it is no surprise that they were discarted by the model. We can do the same manually.","7876c4b1":"# 6 - Final adjustments","828b0139":"Why the bathroom features are not important to the model? If we look at the feature description text file, we will find that the information about the bathrooms are pretty scattered across different features. We have half and full bath information that are not linearly correlated to the sale price, as we can see below.","0de00fd9":"# 5 - Modelling after more cleaning and feature engineering","cb958c3d":"At first glance, it seems weird that the model discarted the area of the first floor as an important feature. However, the features 'GrLivArea' and '2ndFlrSF', which give us the total area above ground and the area of the second floor, respectively, are pretty important in the model. In fact, the three features together are redundant and there would be no problem in discarding one of them. The unfinished basement area was also discarted, but there is no strong correlation between this feature and the sale price of the house. Perhaps a better feature is just the total area of the basement area. ","0fd71671":"Hi! My name is Yuri and I'm a beginner in the field of data science and machine learning. After taking some courses on this subject, I decided to get my hands dirty here. This getting started competition seemed very intimidating to me at first glance due to the high number of features in the dataset, which is very different from what I was used from my MOOC experience. So I decided to approach the problem in a very slow paced way, writing down everything that came to my mind and was important to my decisions. I think this notebook will be very useful to me in future projects given the fact that everything is explained and I did not deleted steps that turned out to be useless in the end (I know that it is common to test many things under the hood but just show the final thoughts and answers). \n\nOf course I read many other notebooks from different users. Just to give credits to a few of them, I really benefited from \n\nhttps:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\nhttps:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\n\nhttps:\/\/www.kaggle.com\/erikbruin\/house-prices-lasso-xgboost-and-a-detailed-eda\n\n**Finally, I'm still learning from this and other competitions, so, any comments or constructive criticism will be helpful :)**","2b117176":"Nice! We were able to improve our model! Now the RMSLE score is 0.12556 for ridge and 0.12500 for Lasso, an improvement of about 8% over the simple model of section 2. Of course we can try to improve it even more with more feature engineering, and maybe by applying other regressors and techniques to the dataset.","e47b2b92":"After submission, the Lasso model performed better than the ridge model, with Root mean squared logarithmic errors equal to 0.13529 (Lasso) and 0.14583 (Ridge).","c3734383":"After reading other notebooks with more care, I noticed that one important step to achieve a better score on the test set is to lognormalize some of the features that are right skewed. I struggled to understand why this kind of transformation may help since a log-log relation is totally different from a linear or log-linear relation. One reason for a log-transformation being so helpful is explained in one [post in Stack Exchange](https:\/\/stats.stackexchange.com\/questions\/107610\/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution). Other reasons, that are more discussed in other notebooks, involve the assumptions of the linear regression model, which are very well explained in this youtube video https:\/\/www.youtube.com\/watch?v=0MFpOQRY0rw. Below, I will generate some residual plots and histograms to visually understand which variables have the highest potential to improve our linear regression model from a log transformation.","f73b7a0a":"The two discarted porch related features are not strongly correlated to the sale price. It is also clear that there are many houses with none of these porches. There are two more porch-related features. Let's see how many of them are zero.","7d7acc38":"The reason is not so obvious for other features. Maybe GarageCond and GarageType are somehow correlated to GarageQual, but it is not easy to identify it at first glance. We shall let the model handle it.\n\nWhat about features that were not discarted but are not very important as well? The two date-related features, 'YrSold' and 'MoSold' may be uncorrelated to the sale price. Let's investigate that.","25838d07":"That's it of data exploration and feature engineering for now. It is better to make another model and see how well it can perform with respect to the previous one. If necessary, we can always try to improve it again with more exploration, feature engineering etc.","62349f95":"Indeed, the range of values of the logarithm of the target variable is much smaller than the range of Sale prices. The distribution of values is also more similar to a normal distribution. I read in many notebooks that we need to do a log transformation in y because the linear regression model requires the target variable to be normally distributed. However, the real assumption of linear regression is that the **error** of the target variable is normally distributed. This means that if we repeat the measure for a given fixed set of features we shall see deviations in y from its mean value following a normal distribution. A short discussion on that can be found at https:\/\/stats.stackexchange.com\/questions\/342759\/where-does-the-misconception-that-y-must-be-normally-distributed-come-from. I think the real reason that performing a log transformation is a good idea is because predictions of expensive houses will be as good as of cheaper ones.\n\nSo, now let's explore the features and their relations to the sale price. Since we already did a linear regression on the dataset, a good start is to see what features were more correlated to the saleprice in our model. Let's do that.","21d1f9b4":"Indeed, houses with a porch are more expensive than those without one. The same idea that we applied to the basement area we may try to apply to the porch area, namely","9cd20db3":"# Predicting house prices step by step","21f5dcba":"Maybe a feature that indicates if a house has a porch is a good idea.","6583d1c9":"Now it is time to do some data analysis and visualization. We will use the result of our simple model to gain further insights over the dataset. Let's first look at the target variable distribution.","5537e5bd":"These features are totally noisy! Fortunatelly, this can be solved by defining a total bathroom feature, as done in other notebooks. Here, I will rather separate the inside bathrooms from the basement bathrooms. You can see below that both features are linearly correlated to the sale price.","e92f5767":"We can see that the target variable is skewed over smaller prices, since the vast majority of people cannot afford very expensive houses. This is why the submission metric is choosen to be the Root Mean Squared Logarithmic Error (RMSLE). As stated in the description of the competition, \"taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally\".","a1c810b1":"Despite the higher importance of the 'MoSold' feature, the second plot shows that there is no linear relation between month and sale price. The relation seems noisy, so we will drop this column and keep only the year instead (which also does not seem linear related to the price, but at least is less noisy). We may also treat this feature as a category.","c81c0b58":"# 4 - Exploratory data analysis and feature engineering","851349a2":"It seems that the feature 'TotalBsmtSF' is more correlated to the sale price than the others. Hence, we will keep this feature and discard the others. In order to do that and to play with other possible features that may be important, let's define another dataframe, data_train_eng.","d774db01":"# 1 - Cleaning the dataset","a0eca8dc":"# 2 - Simple model (no feature engineering and other transformations)"}}