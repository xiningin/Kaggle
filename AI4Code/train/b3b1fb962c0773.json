{"cell_type":{"759e102a":"code","cf1a326d":"code","59c7a0ce":"code","4459a4ef":"markdown"},"source":{"759e102a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve \nimport pickle\nimport time","cf1a326d":"class Simple_ML():\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def train(self, clf, standardize = False):\n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size = 0.2)\n        \n        self.standardize = standardize\n        if standardize:\n            self.StandardScaler1 = StandardScaler()\n            self.StandardScaler1.fit(X_train)\n            X_train = self.StandardScaler1.transform(X_train)\n            X_test = self.StandardScaler1.transform(X_test)\n            \n        #X_train.shape, X_test.shape, y_train.shape, y_test.shape\n        clf.fit(X_train, y_train)\n\n        score = clf.score(X_test, y_test)\n        self.clf= clf\n        \n        return score\n        \n    def save(self, model_file_name='model.pickle'):\n        with open(model_file_name, 'wb') as f:\n            pickle.dump(clf, f)\n        return model_file_name\n        \n    def load(self, model_file_name='model.pickle'):\n        with open(model_file_name, 'rb') as f:\n            return pickle.load(f)\n        \n    def predict(self, X):\n        if self.standardize:\n            X = self.StandardScaler1.transform(X)\n        return self.clf.predict(X)\n\n    # https:\/\/jonathonbechtel.com\/blog\/2018\/02\/06\/wines\/\n    def plot_learning_curve(self, estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.6, 1.0, 5)):\n        \"\"\"\n        Generate a simple plot of the test and traning learning curve.\n\n        Parameters\n        ----------\n        estimator : object type that implements the \"fit\" and \"predict\" methods\n            An object of that type which is cloned for each validation.\n\n        title : string\n            Title for the chart.\n\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples) or (n_samples, n_features), optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        ylim : tuple, shape (ymin, ymax), optional\n            Defines minimum and maximum yvalues plotted.\n\n        cv : integer, cross-validation generator, optional\n            If an integer is passed, it is the number of folds (defaults to 3).\n            Specific cross-validation objects can be passed, see\n            sklearn.cross_validation module for the list of possible objects\n\n        n_jobs : integer, optional\n            Number of jobs to run in parallel (default 1).\n        \"\"\"\n        plt.figure()\n        plt.title(title)\n        if ylim is not None:\n            plt.ylim(*ylim)\n        plt.xlabel(\"Training examples\")\n        plt.ylabel(\"Score\")\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        plt.grid()\n\n        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n\n        plt.legend(loc=\"best\")\n        return plt\n\n    def evaluate(self, classifiers_dict, plot=False, verbose = True):\n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size = 0.2)\n        \n        num_classifiers = len(classifiers_dict.keys())\n        df_results = pd.DataFrame(\n            data=np.zeros(shape=(num_classifiers,4)),\n            columns = ['classifier',\n                       'train_score', \n                       'test_score',\n                       'training_time'])\n        count = 0\n        plt_list=[]\n        for key, classifier in classifiers_dict.items():\n            if verbose:\n                print(f\"start training {key} ...\")\n            t_start = time.clock()\n            grid = GridSearchCV(classifier['classifier'], \n                          classifier['params'],\n                          refit=True,\n                            cv = 10, # 9+1\n                            scoring = 'accuracy', # scoring metric\n                            n_jobs = -1\n                            )\n            estimator = grid.fit(X_train,\n                                 y_train)\n            t_end = time.clock()\n            t_diff = t_end - t_start\n            train_score = estimator.score(X_train,\n                                          y_train)\n            test_score = estimator.score(X_test,\n                                         y_test)\n            df_results.loc[count,'classifier'] = key\n            df_results.loc[count,'train_score'] = train_score\n            df_results.loc[count,'test_score'] = test_score\n            df_results.loc[count,'training_time'] = t_diff\n            if verbose:\n                print(\"trained {c} in {f:.2f} s\".format(c=key,\n                                                        f=t_diff))\n            count+=1\n            if plot:\n                plt1 = self.plot_learning_curve(estimator, \n                                      \"{}\".format(key),\n                                      X_train,\n                                      y_train,\n                                      ylim=(0.75,1.0),\n                                      cv=10)\n                plt_list.append(plt1)\n        return df_results, plt_list","59c7a0ce":"if __name__ == \"__main__\":\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n\n    ds = datasets.load_wine()\n    print(ds.DESCR)\n    \n    print('get data')\n    X = ds.data\n    df = pd.DataFrame(ds.data, columns=ds.feature_names)\n    print(df.head())\n    print('')\n    \n    y = ds.target\n    print(f'y={y[:10]}')\n    print('')\n    \n    print('Missing values:')\n    print(df.isna().sum())\n    print('')\n    \n    # Simple_ML test\n    ml_process = Simple_ML(X, y)\n    clf = KNeighborsClassifier(n_neighbors=3)\n    score = ml_process.train(clf, True)\n    print(f'score = {score}')\n    \n    ml_process.save()\n    model = ml_process.load()\n    \n    print(f'predict = {model.predict(df.sample(2))}')\n    \n    # evaluate test\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.svm import SVC, LinearSVC\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import tree\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.ensemble import GradientBoostingClassifier\n    from sklearn.gaussian_process.kernels import RBF\n    from sklearn.ensemble import RandomForestClassifier\n\n    classifiers_dict = {\n        \"Logistic Regression\": \n                {'classifier': LogisticRegression(solver='liblinear'),\n                    'params' : [\n                                {\n                                 'max_iter': [1000],\n                                 'penalty': ['l1','l2'],\n                                 'C': [0.01,0.1,1,10,100,1000]\n                                }\n                               ]\n                },\n        \"Nearest Neighbors\": \n                {'classifier': KNeighborsClassifier(),\n                     'params': [\n                                {\n                                'n_neighbors': [1, 3, 5, 10],\n                                'leaf_size': [3, 30]\n                                }\n                               ]\n                },\n                 \n        \"Linear SVM\": \n                {'classifier': SVC(),\n                     'params': [\n                                {\n                                 'C': [1, 10, 100, 1000],\n                                 'gamma': [0.001, 0.0001],\n                                 'kernel': ['linear']\n                                }\n                               ]\n                },\n        # \"Gradient Boosting Classifier\": \n                # {'classifier': GradientBoostingClassifier(),\n                     # 'params': [\n                                # {\n                                 # 'learning_rate': [0.05, 0.1],\n                                 # 'n_estimators' :[50, 100, 200],\n                                 # 'max_depth':[3,None]\n                                # }\n                               # ]\n                # },\n        \"Decision Tree\":\n                {'classifier': tree.DecisionTreeClassifier(),\n                     'params': [\n                                {\n                                 'max_depth':[3,None]\n                                }\n                                 ]\n                },\n        \"Random Forest\": \n                {'classifier': RandomForestClassifier(),\n                     'params': {}\n                },\n        \"Naive Bayes\": \n                {'classifier': GaussianNB(),\n                     'params': {}\n                }\n    }\n    df_results, chart_list = ml_process.evaluate(classifiers_dict, plot=True)\n    from IPython.display import display, HTML\n    print('')\n    display(df_results.sort_values(by='test_score', ascending=False))\n    \n    for chart1 in chart_list:\n        chart1.show()\n","4459a4ef":"## Introduction\nIn this set, we will try to simplify the process of comparing the accuracy by constructing different models. We will simplify the complex process by using one class. "}}