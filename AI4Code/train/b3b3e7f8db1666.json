{"cell_type":{"47e7b653":"code","0f49d885":"code","453269e0":"code","f817e625":"code","04c64b13":"code","dcd5528a":"code","3f552ba8":"code","b7ab42ed":"code","6c0c591e":"code","ee0cb2e3":"code","e78b0c70":"code","ca799bde":"markdown","f4dd0ef2":"markdown"},"source":{"47e7b653":"from __future__ import print_function, division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nimport torch.utils.data\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy","0f49d885":"\nclass TransferLearning(nn.Module):\n    def __init__(self):\n        super(TransferLearning, self).__init__()\n        self.model = models.vgg16_bn(pretrained=True)\n\n\n        num_features = self.model.classifier[6].in_features\n        features = list(self.model.classifier.children())[:-1]  # Remove last layer\n        features.extend([nn.Linear(num_features, 2)])  # Add our layer with 4 outputs\n        self.model.classifier = nn.Sequential(*features)  # Replace the model classifier\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n        print(\"Current device is: \",self.device)\n        state_dic = torch.load('..\/input\/trainedmodel\/TrainedModels\/VGG16_bn-2_class.pt', map_location=torch.device(self.device))\n        self.model.load_state_dict(state_dic)\n\n        \n    #Loading and applying traonsformations to the dataset    \n    def load_data(self):\n        data_transforms = {\n            'DataTransform': transforms.Compose([\n            transforms.Resize(248),\n            transforms.RandomCrop(size=(224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.RandomApply(torch.nn.ModuleList([\n            transforms.ColorJitter(brightness=.5, hue=.3),\n            transforms.GaussianBlur(kernel_size = (5.9), sigma=(0.1, 2.0))\n            ]), p=0.2),  \n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n         ])\n      }\n        image_datasets = datasets.ImageFolder(os.path.join('..\/input\/yawning\/Dataset\/train'),\n                                            data_transforms['DataTransform'])\n        print(image_datasets.classes)\n        \n        size = image_datasets.__len__()\n        print(\"Size of dataset: \", image_datasets.__len__())\n        self.test_size = int(size * 0.2)\n        self.train_size = int((size - self.test_size) * 0.8)\n        self.dev = size - self.train_size - self.test_size\n        train_set, dev_set, test_set = torch.utils.data.random_split(image_datasets, [self.train_size, self.dev, self.test_size])\n        train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n        dev_dataloader = torch.utils.data.DataLoader(dev_set, batch_size=4, shuffle=True)\n        test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=True)\n        class_names = image_datasets.classes\n        print(\"Train data size\", self.train_size)\n        print(\"Dev data size\", self.dev)\n        print(\"test data size\", self.test_size)\n\n        return [train_dataloader, dev_dataloader,test_dataloader]\n\n    #Training the network\n    def retrain(self,num_of_epochs = 25,learning_rate = 0.001):\n            self.model.to(self.device)\n            criterion = nn.CrossEntropyLoss()\n            self.train_acc = []\n            self.dev_acc = []\n          # Observe that all parameters are being optimized\n            optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,momentum = 0.9)\n\n          # Decay LR by a factor of 0.1 every 7 epochs\n            scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n            dataloadersets = self.load_data()\n            since = time.time()\n            best_model_wts = copy.deepcopy(self.model.state_dict())\n            best_acc = 0.0\n            dataset_size = 0\n            for epoch in range(num_of_epochs):\n                print('Epoch {}\/{}'.format(epoch, num_of_epochs - 1))\n                print('-' * 10)\n                self.model.train()\n                running_loss = 0.0\n                running_corrects = 0\n                for inputs,labels in dataloadersets[0]:\n                    inputs = inputs.to(self.device)\n                    labels = labels.to(self.device)\n                    dataset_size += 1\n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(True):\n                        outputs = self.model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n                        loss.backward()\n                        optimizer.step()\n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n                    scheduler.step()\n                epoch_loss = running_loss \/ self.train_size\n                epoch_acc = running_corrects.double() \/ self.train_size\n                self.train_acc.append(epoch_acc)\n                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                    'Train', epoch_loss, epoch_acc))\n                dataset_size = 0\n                self.model.eval()\n                running_loss = 0.0\n                running_corrects = 0\n                for inputs,labels in dataloadersets[1]:\n                    inputs = inputs.to(self.device)\n                    labels = labels.to(self.device)\n                    dataset_size += 1\n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(False):\n                        outputs = self.model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n                        \n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n                epoch_loss = running_loss \/ self.dev\n                epoch_acc = running_corrects.double() \/ self.dev\n                self.dev_acc.append(epoch_acc)\n            \n                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                'Dev', epoch_loss, epoch_acc))\n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = copy.deepcopy(self.model.state_dict())\n\n            time_elapsed = time.time() - since\n            print('Training complete in {:.0f}m {:.0f}s'.format(\n             time_elapsed \/\/ 60, time_elapsed % 60))\n            print('Best val Acc: {:4f}'.format(best_acc))\n            self.model.load_state_dict(best_model_wts)\n            \n        \n\n\n\n\n    def calc_confusion(self,tp,tn,fp,fn):\n        accuarcy = (tp+tn)\/(tp+tn+fp+fn)\n        precision = tp\/(tp+fp)\n        sensitivity =tp\/(tp+fn)\n        print()\n        print(\"Accuarcy is: \",accuarcy)\n        print(\"Precision is: \",precision)\n        print(\"Sensitivity is: \",sensitivity)\n    #Testing the network\n    def test(self,criterion = nn.CrossEntropyLoss()):\n        print(self.device)\n        self.model.to(self.device)\n        a = time.time()\n        DataLoaders = self.load_data()\n        b = time.time() -a\n        print(b \/\/ 60,\"min\", b % 60,\"second\")\n        tp, tn, fp, fn = 0,0,0,0\n        since = time.time()\n        avg_loss = 0\n        avg_acc = 0\n        loss_test = 0\n        acc_test = 0\n\n        test_batches = len(DataLoaders[2])\n        print(\"Evaluating model\")\n        print('-' * 10)\n\n        for i, data in enumerate(DataLoaders[2]):\n            if i % 100 == 0:\n                print(\"\\rTest batch {}\/{}\".format(i, test_batches), end='', flush=True)\n\n            self.model.train(False)\n            self.model.eval()\n            inputs, labels = data\n\n            inputs = inputs.to(self.device)\n            labels = labels.to(self.device)\n\n            outputs = self.model(inputs)\n\n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n\n            loss_test += loss.item()\n            acc_test += torch.sum(preds == labels.data)\n            for i in range(len(preds)):\n                if preds[i] == 1 and labels.data[i] == preds[i]:\n                    tp += 1\n                elif preds[i] == 0 and labels.data[i] == preds[i]:\n                    tn += 1\n                elif preds[i] == 1 and labels.data[i] != preds[i]:\n                    fp +=1\n                else:\n                    fn += 1\n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n\n        avg_loss = loss_test \/ self.test_size\n        avg_acc = acc_test \/ self.test_size\n\n        elapsed_time = time.time() - since\n        self.calc_confusion(tp,tn,fp,fn)\n        print()\n        print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time \/\/ 60, elapsed_time % 60))\n        print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n        print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n        print('-' * 10)","453269e0":"Trained_model = TransferLearning()\nprint(Trained_model.model.__class__)\n\n\nTrained_model.test()","f817e625":"num_epoch = 30\nTrained_model.retrain(num_of_epochs = num_epoch)","04c64b13":"x  = np.arange(0, num_epoch)\nplt.title(\"Training Accuarcy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.plot(x, Trained_model.train_acc, color =\"green\")\nplt.show()","dcd5528a":"plt.title(\"Dev Accuarcy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.plot(x, Trained_model.dev_acc, color =\"red\")\nplt.show()","3f552ba8":"Trained_model.test()","b7ab42ed":"torch.save(Trained_model.model.state_dict(), 'VGG16_bn-2_class(Version2)4.pt')","6c0c591e":"def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\ndef show_databatch(inputs, classes):\n    out = torchvision.utils.make_grid(inputs)\n    imshow(out, title=[class_names[x] for x in classes])\nclass_names = ['no_yawn','yawn']    \ninputs, classes = next(iter(Trained_model.load_data()[2]))\nshow_databatch(inputs, classes)    ","ee0cb2e3":"def visualize_model(vgg, num_images=6):\n    was_training = vgg.training\n    datasets = Trained_model.load_data()\n    # Set model for evaluation\n    vgg.train(False)\n    vgg.eval() \n    \n    images_so_far = 0\n\n    for i, data in enumerate(datasets[2]):\n        inputs, labels = data\n        size = inputs.size()[0]\n        \n        inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n      \n        \n        outputs = vgg(inputs)\n        \n        _, preds = torch.max(outputs.data, 1)\n        predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n        \n        print(\"Ground truth:\")\n        show_databatch(inputs.data.cpu(), labels.data.cpu())\n        print(\"Prediction:\")\n        show_databatch(inputs.data.cpu(), predicted_labels)\n        \n        del inputs, labels, outputs, preds, predicted_labels\n        torch.cuda.empty_cache()\n        \n        images_so_far += size\n        if images_so_far >= num_images:\n            break\n        \n    vgg.train(mode=was_training) # Revert model back to original training state","e78b0c70":"visualize_model(Trained_model.model,num_images = 16)","ca799bde":"<a href=\".\/VGG16_bn-2_class(Version2)4.pt\"> Download File <\/a>","f4dd0ef2":"# **Imports**"}}