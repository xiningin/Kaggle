{"cell_type":{"b497ba15":"code","61f572b3":"code","c5a94fb7":"code","3a537fcd":"code","16730e27":"code","2a3d1d8e":"code","ed97c294":"code","3e262cc1":"code","d903a1b2":"code","963a5dc4":"code","3646edb7":"code","65045638":"code","0085e0cd":"code","1e936575":"code","fe13e5d7":"code","f9acb3f1":"code","94beadae":"code","3a4a08bd":"code","23b8e9f3":"code","b37dad3e":"code","8a2990ad":"code","714dcbf3":"code","051c73b7":"code","2405e5dd":"code","b73f14cb":"code","a1701995":"code","ef4f2c87":"code","0e6b34dd":"code","e0a8e99e":"code","a09115fe":"code","ae03bb3c":"code","49229427":"code","25a7ec3c":"code","f7856891":"code","73fe7b99":"code","a1ab296d":"code","5328f2ab":"code","8d893f34":"code","1276138b":"code","9871b936":"code","4b7de06a":"code","ae6edb4a":"code","90fe7817":"code","c421fc90":"code","e8af9f4a":"code","64aa18bd":"code","294fae80":"code","508124dc":"code","42371e2c":"code","6906af9d":"code","1f0627e5":"code","f97cc401":"code","e9fa7a38":"code","081cfcc8":"code","1b29969e":"code","99d464ff":"code","dd2a888a":"code","19ea9f36":"code","890841ba":"code","1f7c511e":"code","fed741ab":"code","2df91b9f":"code","ba58bde2":"code","116a04fd":"code","74832418":"code","0f94beba":"code","fabe931e":"code","8559be5a":"markdown","c7807477":"markdown","b7cf1594":"markdown","b788cdca":"markdown","cc87483d":"markdown","95a413df":"markdown","5cd5b399":"markdown","33fb5296":"markdown","daa58ede":"markdown","013ddef2":"markdown","279cb2fa":"markdown","3d4f1dc1":"markdown","e25a5ac6":"markdown","5cadff07":"markdown","b8a47c97":"markdown","e3cd3995":"markdown","a30bd131":"markdown","5f0bbe15":"markdown","d9e980ac":"markdown","61745605":"markdown","424c244d":"markdown","5268b0a3":"markdown"},"source":{"b497ba15":"import json \nimport numpy as np\nimport pandas as pd\nimport re, nltk, spacy, string\nimport en_core_web_sm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom plotly.offline import plot\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom pprint import pprint\n\n%matplotlib inline\n\nnlp = en_core_web_sm.load()","61f572b3":"# Opening JSON file \nfile_path = open('..\/input\/automatic-ticket-classification\/complaints-2021-05-14_08_16.json') # Write the path to your data file and load it \n  \n# returns JSON object as  \n# a dictionary \ndata = json.load(file_path)\ndf = pd.json_normalize(data)","c5a94fb7":"# Inspect the dataframe to understand the given data.\npd.set_option('max_columns', None)\ndf.head()","3a537fcd":"df.info()","16730e27":"# Print the column names\ndef print_column_names():\n    i = 1\n    for col in df.columns:\n        print(str(\"{:02d}\".format(i)) +\" Column:\\033[1m\", col, '\\033[0m')\n        i += 1\n        \nprint_column_names()","2a3d1d8e":"# Assign new column names\nnew_columns = {}\nfor col in df.columns:\n    new_columns[col] = re.sub(\"^_\", \"\", col).replace(\"source.\", \"\")\n\ndf.rename(columns = new_columns, inplace = True)\n\nprint_column_names()","ed97c294":"# Get the shape and size of the dataset\nprint('No of rows:\\033[1m', df.shape[0], '\\033[0m')\nprint('No of cols:\\033[1m', df.shape[1], '\\033[0m')","3e262cc1":"# Assign nan in place of blanks in the complaints column\ndf.complaint_what_happened.replace(r'', np.nan, regex=True, inplace=True)","d903a1b2":"# Remove all rows where complaints column is nan\ndf = df[df.complaint_what_happened.notna()]\nprint('Remaining Rows:\\033[1m', len(df), '\\033[0m')","963a5dc4":"# Write your function here to clean the text and remove all the unnecessary elements.\ndef cleaned_text(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(\"[\\[].*?[\\]]\", \"\", sentence)\n    sentence = re.sub(r'[^\\w\\s]','', sentence) \n    sentence = re.sub(r'[0-9]', '', sentence, re.I|re.A)\n    return sentence","3646edb7":"# Write your function to Lemmatize the texts\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\ndef lemmatized_text(sentence):\n    # tokenize into words\n    words = nltk.word_tokenize(sentence)\n    \n    # remove stop words (this is optional but generally done)\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n    \n    # tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(words)\n    \n    # tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            # if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:        \n            # else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)","65045638":"from tqdm import tqdm\n\n# Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints \ndf_clean = pd.DataFrame()\n\nfor x in tqdm(df.complaint_what_happened):\n    df_clean = df_clean.append({'complaint_what_happened':x, 'lemmatized_complaints':lemmatized_text(cleaned_text(x))}, ignore_index=True)","0085e0cd":"df_clean","1e936575":"from textblob import TextBlob\n\ntqdm.pandas()\n\n# Write your function to extract the POS tags \ndef extract_pos_tag(sentence):\n    # Create a textblob object\n    blob = TextBlob(sentence)\n    \n    # extract words with tags 'NN', join them and return\n    return ' '.join([ word for (word,tag) in blob.tags if tag == \"NN\"])\n    \ndf_clean[\"complaint_POS_removed\"] = df_clean.progress_apply(lambda x: extract_pos_tag(x.lemmatized_complaints), axis=1)\ndf_clean[\"length\"] = df_clean.progress_apply(lambda x: len(x.complaint_POS_removed), axis=1)","fe13e5d7":"# The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\ndf_clean","f9acb3f1":"# Write your code here to visualise the data according to the 'Complaint' character length\nplt.figure(figsize=(18,5))\nplt.hist(df_clean.length, bins=50)\nplt.xlabel(\"Complaint Character Length\")\nplt.show()","94beadae":"# Zoomed to see the figure upto 4000\nplt.figure(figsize=(18,5))\nplt.hist([l for l in df_clean.length if l < 4000], bins=50)\nplt.xlabel(\"Complaint Character Length\")\nplt.show()","3a4a08bd":"# Using a word cloud find the top 40 words by frequency among all the articles after processing the text\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(\n                          background_color='lemonchiffon',\n                          stopwords=stop,\n                          max_words=40,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(df_clean['complaint_POS_removed']))\n\nfig = plt.figure(figsize=(20,15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","23b8e9f3":"# Removing -PRON- from the text corpus\ndf_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')","b37dad3e":"# Write your code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean). \ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n# Top 30 unigrams by frequency among all the complaints\ncommon_words = get_top_n_words(df_clean['complaint_POS_removed'].values.astype('U'), 30)\ndf2 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n\n# Plot the top 30 unigrams\nplt.figure(figsize=(15,6))\nsns.barplot(x='unigram', y='count', data=df2, palette=\"coolwarm\")\nplt.xticks(rotation=90)\nplt.title(\"Top 30 unigrams in the Complaint text after removing stop words and lemmatization\", fontsize=20)\nplt.show()","8a2990ad":"# Print the top 10 words in the unigram frequency\ndf2.head(10)","714dcbf3":"# Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean). \ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n# Top 30 bigrams by frequency among all the complaints\ncommon_words = get_top_n_bigram(df_clean['Complaint_clean'].values.astype('U'), 30)\ndf3 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n\n# Plot the top 30 bigrams\nplt.figure(figsize=(15,6))\nsns.barplot(x='bigram', y='count', data=df3, palette=\"coolwarm\")\nplt.xticks(rotation=90)\nplt.title(\"Top 30 bigrams in the Complaint text after removing stop words and lemmatization\", fontsize=20)\nplt.show()","051c73b7":"# Print the top 10 words in the bigram frequency\ndf3.head(10)","2405e5dd":"# Write your code here to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean). \ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n# Top 30 trigrams by frequency among all the complaints\ncommon_words = get_top_n_trigram(df_clean['Complaint_clean'].values.astype('U'), 30)\ndf4 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n\n# Plot the top 30 unigrams\nplt.figure(figsize=(15,6))\nsns.barplot(x='trigram', y='count', data=df4, palette=\"coolwarm\")\nplt.xticks(rotation=90)\nplt.title(\"Top 30 trigrams in the Complaint text after removing stop words and lemmatization\", fontsize=20)\nplt.show()","b73f14cb":"# Print the top 10 words in the trigram frequency\ndf4.head(10)","a1701995":"df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')","ef4f2c87":"# All masked texts has been removed\ndf_clean","0e6b34dd":"# Write your code here to initialise the TfidfVectorizer \ntfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")","e0a8e99e":"# Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\ndtm = tfidf.fit_transform(df_clean.Complaint_clean)","a09115fe":"from sklearn.decomposition import NMF","ae03bb3c":"# Load your nmf_model with the n_components i.e 5\nnum_topics = 5 # write the value you want to test out\n\n# keep the random_state = 40\nnmf_model = NMF(n_components=num_topics, random_state=40) # write your code here","49229427":"nmf_model.fit(dtm)\nlen(tfidf.get_feature_names())","25a7ec3c":"# Print the Top15 words for each of the topics\nfor index,topic in enumerate(nmf_model.components_):\n    print(f'\\033[1mTHE TOP 15 WORDS FOR TOPIC #{index + 1}', '\\033[0m')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","f7856891":"# Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\ntopic_results = nmf_model.transform(dtm)\ntopic_results[0].round(2)\ntopic_results[0].argmax()\ntopic_results.argmax(axis=1)","73fe7b99":"#Assign the best topic to each of the cmplaints in Topic Column\ndf_clean['Topic'] = topic_results.argmax(axis = 1) # write your code to assign topics to each rows.","a1ab296d":"df_clean.head()","5328f2ab":"# Print the first 5 Complaint for each of the Topics\ndf_clean5 = df_clean.groupby('Topic').head(5)\ndf_clean5.sort_values('Topic')","8d893f34":"import warnings as warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Create the dictionary of Topic names and Topics\nTopic_names = {0:\"Bank Account services\",\n               1:\"Credit card or prepaid card\", \n               2:\"Others\",\n               3:\"Theft\/Dispute Reporting\",\n               4:\"Mortgage\/Loan\"}\n\n# Replace Topics with Topic Names\ndf_clean['Topic'] = df_clean['Topic'].map(Topic_names)","1276138b":"df_clean","9871b936":"# Create the dictionary again of Topic names and Topics\nTopic_names = {\"Bank Account services\":0,\n               \"Credit card or prepaid card\":1,\n               \"Others\":2,\n               \"Theft\/Dispute Reporting\":3,\n               \"Mortgage\/Loan\":4}\n\n# Replace Topics with Topic Names\ndf_clean['Topic'] = df_clean['Topic'].map(Topic_names)","4b7de06a":"df_clean","ae6edb4a":"# Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\ntraining_data = df_clean[[\"complaint_what_happened\",\"Topic\"]]","90fe7817":"training_data","c421fc90":"# Write your code to get the Vector count\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(training_data.complaint_what_happened)\n\n# Write your code here to transform the word vector to tf-idf\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)","e8af9f4a":"from sklearn.model_selection import train_test_split\n\n# Performing Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=0.25, random_state=42)\n\nprint(f\"Shape of X_train:\\033[1m {X_train.shape}\", '\\033[0m')\nprint(f\"Shape of y_train:\\033[1m {y_train.shape}\", '\\033[0m')\nprint(f\"Shape of X_test:\\033[1m {X_test.shape}\", '\\033[0m')\nprint(f\"Shape of y_test:\\033[1m {y_test.shape}\", '\\033[0m')","64aa18bd":"# Create a function to evaluate models\ndef eval_model(y_test, y_pred, model_name):\n    # print classification report of classifier\n    print(f\"\\033[1mCLASSIFICATION REPORT for {model_name}\\n\", '\\033[0m')\n    print(classification_report(y_test, y_pred, target_names=[\"Bank Account services\", \"Credit card or prepaid card\", \"Others\", \"Theft\/Dispute Reporting\",\"Mortgage\/Loan\"]))\n    \n    # plot confusion matrix of the classifier\n    plt.figure(figsize=(10,6))\n    plt.title(f\"CONFUSION MATRIX for {model_name}\\n\")\n    matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap(matrix, annot=True, cbar=None, cmap=\"Blues\", fmt='d', xticklabels=[\"Bank Account services\", \"Credit card or prepaid card\", \"Others\", \"Theft\/Dispute Reporting\",\"Mortgage\/Loan\"], yticklabels=[\"Bank Account services\", \"Credit card or prepaid card\", \"Others\", \"Theft\/Dispute Reporting\",\"Mortgage\/Loan\"])\n    plt.show()\n    \n    return","294fae80":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\n\n# Run the Logistic Regression model\nmodel_name = 'LOGISTIC REGRESSION'\nclf_lr = LogisticRegression(solver='liblinear')\n%time \nclf_lr.fit(X_train, y_train)\ny_pred_lr = clf_lr.predict(X_test)","508124dc":"# Calculate F1 Score using weighted average method\nf1_lr = f1_score(y_test, y_pred_lr, average=\"weighted\")\nf1_lr","42371e2c":"# Run Logistic Regression on tuned hyperparameters\nclf_lr_tuned = LogisticRegression(C=1, penalty='l1',solver='saga')\n%time \nclf_lr_tuned.fit(X_train, y_train)\ny_pred_lr_tuned = clf_lr_tuned.predict(X_test)","6906af9d":"# Calculate F1 Score of tuned model using weighted average method\nf1_lr_tuned = f1_score(y_test, y_pred_lr_tuned, average=\"weighted\")\nf1_lr_tuned","1f0627e5":"# Evaluate the tuned Logistic Regression classifier\neval_model(y_test, y_pred_lr_tuned, model_name)","f97cc401":"# Create a dataframe to store F1 Scores of all models we will build\nsummary = pd.DataFrame([{'Model': 'Logistic Regression','F1 Score (untuned)': round(f1_lr, 2), 'F1 Score (tuned)': round(f1_lr_tuned, 2)}])\nsummary","e9fa7a38":"from sklearn.tree import DecisionTreeClassifier\n\n# Run Decision Tree on default hyperparameters\nmodel_name = 'DECISION TREE'\nclf_dt = DecisionTreeClassifier()\n%time \nclf_dt.fit(X_train, y_train)\ny_pred_dt = clf_dt.predict(X_test)","081cfcc8":"# Calculate F1 Score using weighted average method\nf1_dt = f1_score(y_test, y_pred_dt, average=\"weighted\")\nf1_dt","1b29969e":"# Run Decision Tree on tuned hyperparameters\nclf_dt_tuned = DecisionTreeClassifier(criterion='gini', max_depth=30, min_samples_leaf=15, max_features=None)\n%time \nclf_dt_tuned.fit(X_train, y_train)\ny_pred_dt_tuned = clf_dt_tuned.predict(X_test)","99d464ff":"# Calculate F1 Score of tuned model using weighted average method\nf1_dt_tuned = f1_score(y_test, y_pred_dt_tuned, average=\"weighted\")\nf1_dt_tuned","dd2a888a":"# Evaluate the tuned Decision Tree classifier\neval_model(y_test, y_pred_dt_tuned, model_name)","19ea9f36":"# Update the summary table\nsummary.loc[len(summary.index)] = ['Decision Tree', round(f1_dt, 2), round(f1_dt_tuned, 2)]\nsummary","890841ba":"from sklearn.ensemble import RandomForestClassifier\n\n# Run the Random Forest model on default hyperparameters\nmodel_name = 'RANDOM FOREST'\nclf_rf = RandomForestClassifier()\n%time \nclf_rf.fit(X_train, y_train)\ny_pred_rf = clf_rf.predict(X_test)","1f7c511e":"# Calculate F1 Score using weighted average method\nf1_rf = f1_score(y_test, y_pred_rf, average=\"weighted\")\nf1_rf","fed741ab":"# Run Random Forest on tuned hyperparameters\nclf_rf_tuned = RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=5, max_features=None, max_depth=30, criterion='gini')\n%time \nclf_rf_tuned.fit(X_train, y_train)\ny_pred_rf_tuned = clf_rf_tuned.predict(X_test)","2df91b9f":"# Calculate F1 Score of tuned model using weighted average method\nf1_rf_tuned = f1_score(y_test, y_pred_rf_tuned, average=\"weighted\")\nf1_rf_tuned","ba58bde2":"# Evaluate the tuned Random Forest classifier\neval_model(y_test, y_pred_rf_tuned, model_name)","116a04fd":"# Update the summary table\nsummary.loc[len(summary.index)] = ['Random Forest', round(f1_rf, 2), round(f1_rf_tuned, 2)]\nsummary","74832418":"# Function to predict a topic for given text\ndef predict_topic(text):\n    \n    target_names = [\"Bank Account services\", \"Credit card or prepaid card\", \"Others\", \"Theft\/Dispute Reporting\", \"Mortgage\/Loan\"]\n\n    X_new_counts = count_vect.transform(text)\n    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n    predicted = clf_lr_tuned.predict(X_new_tfidf)\n\n    return target_names[predicted[0]]","0f94beba":"# Create a dataframe of some sample customer complaints\ndf_new = pd.DataFrame({'complaints': [\"I can not get from chase who services my mortgage, who owns it and who has original loan docs\", \n                                  \"The bill amount of my credit card was debited twice. Please look into the matter and resolve at the earliest.\",\n                                  \"I want to open a salary account at your downtown branch. Please provide me the procedure.\",\n                                  \"Yesterday, I received a fraudulent email regarding renewal of my services.\",\n                                  \"What is the procedure to know my CIBIL score?\",\n                                  \"I need to know the number of bank branches and their locations in the city of Dubai\"]})\ndf_new","fabe931e":"# Create a new column of predicted topics of each complaint, predicted using the tuned Logistic Regression model\ndf_new['predicted topic'] = df_new['complaints'].apply(lambda x: predict_topic([x]))\ndf_new","8559be5a":"#### Find the top 40 words by frequency among all the articles after processing the text.","c7807477":"## Importing the necessary libraries","b7cf1594":"## Exploratory data analysis to get familiar with the data.\n\nWrite the code in this task to perform the following:\n\n*   Visualise the data according to the 'Complaint' character length.\n*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text.\n*   Find the top unigrams, bigrams and trigrams by frequency among all the complaints after processing the text.","b788cdca":"#### Create a document term matrix using fit_transform\n\nThe contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\nThe tuples that are not there have a tf-idf score of 0","cc87483d":"You have to try atleast 3 models on the train & test data from these options:\n* Logistic regression\n* Decision Tree\n* Random Forest\n* Naive Bayes (optional)\n\n**Using the required evaluation metrics judge the tried models and select the ones performing the best**","95a413df":"#### Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text.","5cd5b399":"### Model 1: Logistic Regression","33fb5296":"### Model 2: Decision Tree","daa58ede":"## Topic Modelling using NMF\n\nNon-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n\nIn this task you have to perform the following:\n\n* Find the best number of clusters \n* Apply the best number to create word clusters\n* Inspect & validate the correction of each cluster wrt the complaints \n* Correct the labels if needed \n* Map the clusters to topics\/cluster names","013ddef2":"## Feature Extraction\nConvert the raw texts to a matrix of TF-IDF features\n\n**max_df** is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\nmax_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n\n**min_df** is used for removing terms that appear too infrequently\nmin_df = 2 means \"ignore terms that appear in less than 2 complaints\"","279cb2fa":"## Prepare the text for topic modeling\n\nOnce you have removed all the blank complaints, you need to:\n\n* Make the text lowercase\n* Remove text in square brackets\n* Remove punctuation\n* Remove words containing numbers\n\n\nOnce you have done these cleaning operations you need to perform the following:\n* Lemmatize the texts\n* Use POS tags to get relevant words from the texts.\n","3d4f1dc1":"## Loading the data\n\nThe data is in JSON format and we need to convert it to a dataframe.","e25a5ac6":"## Pipelines that needs to be performed:\n\nYou need to perform the following eight major tasks to complete the assignment:\n\n1.  Data loading\n\n2. Text preprocessing\n\n3. Exploratory data analysis (EDA)\n\n4. Feature extraction\n\n5. Topic modelling\u00a0\n\n6. Model building using supervised learning\n\n7. Model training and evaluation\n\n8. Model inference","5cadff07":"#### From the above summary table, we observe that the `tuned Logistic Regression` performs the best among all that we tried.","b8a47c97":"### Model 3: Random Forest","e3cd3995":"## Data preparation","a30bd131":"## Problem Statement \n\nYou need to build a model that is able to classify customer complaints based on the products\/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n\nYou will be doing topic modelling on the <b>.json<\/b> data provided by the company. Since this data is not labelled, you need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products\/services:\n\n* Credit card \/ Prepaid card\n\n* Bank account services\n\n* Theft\/Dispute reporting\n\n* Mortgages\/loans\n\n* Others\u00a0\n\n\nWith the help of topic modelling, you will be able to map each ticket onto its respective department\/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department.","5f0bbe15":"#### Apply the supervised models on the training data created. In this process, you have to do the following:\n* Create the vector counts using Count Vectoriser\n* Transform the word vecotr to tf-idf\n* Create the train & test data using the train_test_split on the tf-idf & topics\n","d9e980ac":"## The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as this will be of no use for our analysis","61745605":"## Supervised model to predict any new complaints to the relevant Topics.\n\nYou have now build the model to create the topics for each complaints.Now in the below section you will use them to classify any new complaints.\n\nSince you will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)","424c244d":"## Manual Topic Modeling\nYou need to do take the trial & error approach to find the best num of topics for your NMF model.\n\nThe only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good your final topics are.","5268b0a3":"#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n* Bank Account services\n* Credit card or prepaid card\n* Theft\/Dispute Reporting\n* Mortgage\/Loan\n* Others"}}