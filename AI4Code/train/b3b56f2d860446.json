{"cell_type":{"51238e80":"code","41d562e8":"code","6ab436b5":"code","b61ea51d":"code","db5975e0":"code","8c69af9b":"code","e454d6cc":"code","1860f60f":"code","dd75cc7b":"code","5be91fc2":"code","59a25fb2":"code","27023c5e":"code","cb547427":"code","a217e414":"code","18036e5b":"code","8e859b7b":"code","dac68748":"code","7e02bbec":"code","bffebf1b":"code","19df6679":"code","02afd821":"code","f5a1c379":"code","bb67d498":"code","1b13f27d":"code","23cb91e9":"code","8d274c1c":"code","ac557fce":"code","6f483717":"code","bd2332fb":"code","5db86d18":"markdown","4e760599":"markdown","2591ee11":"markdown","cb77288b":"markdown","b0aed646":"markdown","b3100f36":"markdown","5bd83583":"markdown","5f248f21":"markdown","17b4c69c":"markdown","c436e47f":"markdown","40fc7a60":"markdown","540a6e36":"markdown","d9a19720":"markdown","ab0c02cb":"markdown"},"source":{"51238e80":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","41d562e8":"data = pd.read_csv('\/kaggle\/input\/mbti-type\/mbti_1.csv')","6ab436b5":"def get1(x):\n    if x[0:1] == 'I':\n        return 1\n    else:\n        return 0\ndef get2(x):\n    if x[1:2] == 'N':\n        return 1\n    else:\n        return 0\ndef get3(x):\n    if x[2:3] == 'T':\n        return 1\n    else:\n        return 0\ndef get4(x):\n    if x[3:4] == 'J':\n        return 1\n    else:\n        return 0\ndata['I'] = data['type'].apply(get1)\ndata['N'] = data['type'].apply(get2)\ndata['T'] = data['type'].apply(get3)\ndata['J'] = data['type'].apply(get4)","b61ea51d":"data.head()","db5975e0":"from sklearn.utils import shuffle\nX = data['posts']\ny = data['I']\nsmallest_val = 0\nif y.value_counts()[0] < y.value_counts()[1]:\n    small = y.value_counts()[0]\n    big = y.value_counts()[1]\n    s_data = shuffle(data[data['I']==0][['posts','I']])\n    b_data = shuffle(data[data['I']==1][['posts','I']])\nelse:\n    small = y.value_counts()[1]\n    big = y.values_counts()[0]\n    s_data = shuffle(data[data['I']==1][['posts','I']])\n    b_data = shuffle(data[data['I']==0][['posts','I']])\nb_data = shuffle(b_data[:small])\nfor i in range(999):\n    i_data = shuffle(pd.concat([b_data,s_data]))","8c69af9b":"col = 'N'\n\nX = data['posts']\ny = data[col]\nsmallest_val = 0\nif y.value_counts()[0] < y.value_counts()[1]:\n    small = y.value_counts()[0]\n    big = y.value_counts()[1]\n    s_data = shuffle(data[data[col]==0][['posts',col]])\n    b_data = shuffle(data[data[col]==1][['posts',col]])\nelse:\n    small = y.value_counts()[1]\n    big = y.values_counts()[0]\n    s_data = shuffle(data[data[col]==1][['posts',col]])\n    b_data = shuffle(data[data[col]==0][['posts',col]])\nb_data = shuffle(b_data[:small])\nfor i in range(999):\n    n_data = shuffle(pd.concat([b_data,s_data]))","e454d6cc":"col = 'T'\n\nX = data['posts']\ny = data[col]\nsmallest_val = 0\nif y.value_counts()[0] < y.value_counts()[1]:\n    small = y.value_counts()[0]\n    big = y.value_counts()[1]\n    s_data = shuffle(data[data[col]==0][['posts',col]])\n    b_data = shuffle(data[data[col]==1][['posts',col]])\nelse:\n    small = y.value_counts()[1]\n    big = y.value_counts()[0]\n    s_data = shuffle(data[data[col]==1][['posts',col]])\n    b_data = shuffle(data[data[col]==0][['posts',col]])\nb_data = shuffle(b_data[:small])\nfor i in range(999):\n    t_data = shuffle(pd.concat([b_data,s_data]))","1860f60f":"col = 'J'\n\nX = data['posts']\ny = data[col]\nsmallest_val = 0\nif y.value_counts()[0] < y.value_counts()[1]:\n    small = y.value_counts()[0]\n    big = y.value_counts()[1]\n    s_data = shuffle(data[data[col]==0][['posts',col]])\n    b_data = shuffle(data[data[col]==1][['posts',col]])\nelse:\n    small = y.value_counts()[1]\n    big = y.value_counts()[0]\n    s_data = shuffle(data[data[col]==1][['posts',col]])\n    b_data = shuffle(data[data[col]==0][['posts',col]])\nb_data = shuffle(b_data[:small])\nfor i in range(999):\n    j_data = shuffle(pd.concat([b_data,s_data]))","dd75cc7b":"i_data.head()","5be91fc2":"n_data.head()","59a25fb2":"t_data.head()","27023c5e":"j_data.head()","cb547427":"i_data['I'].value_counts()","a217e414":"n_data['N'].value_counts()","18036e5b":"t_data['T'].value_counts()","8e859b7b":"j_data['J'].value_counts()","dac68748":"data['I'].value_counts()","7e02bbec":"i_data = data[['I','posts']]\nn_data = data[['N','posts']]\nt_data = data[['T','posts']]\nj_data = data[['J','posts']]","bffebf1b":"import random\nimport progressbar as pb\nfor i in pb.progressbar(range(4680)):\n    i_data = i_data.append(data[data['I']==0][['I','posts']].iloc[random.randint(0,len(data[data['I']==0][['I','posts']])-1)])\nfor i in pb.progressbar(range(6281)):\n    n_data = n_data.append(data[data['N']==0][['N','posts']].iloc[random.randint(0,len(data[data['N']==0][['N','posts']])-1)])\nfor i in pb.progressbar(range(713)):\n    t_data = t_data.append(data[data['T']==1][['T','posts']].iloc[random.randint(0,len(data[data['T']==1][['T','posts']])-1)])\nfor i in pb.progressbar(range(1807)):\n    j_data = j_data.append(data[data['J']==1][['J','posts']].iloc[random.randint(0,len(data[data['J']==1][['J','posts']])-1)])","19df6679":"print(i_data['I'].value_counts())\nprint(n_data['N'].value_counts())\nprint(t_data['T'].value_counts())\nprint(j_data['J'].value_counts())","02afd821":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')","f5a1c379":"from bs4 import BeautifulSoup\nimport re\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    return text\ni_data['cleaned'] = i_data['posts'].apply(cleanText)\nn_data['cleaned'] = n_data['posts'].apply(cleanText)\nt_data['cleaned'] = t_data['posts'].apply(cleanText)\nj_data['cleaned'] = j_data['posts'].apply(cleanText)","bb67d498":"i_data = i_data.reset_index().drop('index',axis=1)\nn_data = n_data.reset_index().drop('index',axis=1)\nt_data = t_data.reset_index().drop('index',axis=1)\nj_data = j_data.reset_index().drop('index',axis=1)","1b13f27d":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nnp.random.seed(1)\n\nscoring = {'acc': 'accuracy','neg_log_loss': 'neg_log_loss','f1_micro': 'f1_micro'}\ntfidf = CountVectorizer(ngram_range=(1, 1),stop_words='english',lowercase = True,max_features = 5000)","23cb91e9":"i_model = Pipeline([('tfidf', tfidf), ('model', DecisionTreeClassifier())])\nn_model = Pipeline([('tfidf', tfidf), ('model', DecisionTreeClassifier())])\nt_model = Pipeline([('tfidf', tfidf), ('model', DecisionTreeClassifier())])\nj_model = Pipeline([('tfidf', tfidf), ('model', DecisionTreeClassifier())])\ni_model.fit(i_data['cleaned'],i_data['I'])\nn_model.fit(n_data['cleaned'],n_data['N'])\nt_model.fit(t_data['cleaned'],t_data['T'])\nj_model.fit(j_data['cleaned'],j_data['J'])","8d274c1c":"print(\"- - - RESULTS\")\nprint(\"I Component Model Performance\")\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nresults_nb = cross_validate(i_model, i_data['cleaned'], i_data['I'], cv=kfolds,scoring=scoring, n_jobs=-1)\nprint(\"Accuracy:\",np.mean(results_nb['test_acc']))\nprint(\"F1 Score:\",np.mean(results_nb['test_f1_micro']))\nprint(\"Log Loss:\",np.mean(-1*results_nb['test_neg_log_loss']))\n\nprint(\"\\nN Component Model Performance\")\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nresults_nb = cross_validate(n_model, n_data['cleaned'], n_data['N'], cv=kfolds,scoring=scoring, n_jobs=-1)\nprint(\"Accuracy:\",np.mean(results_nb['test_acc']))\nprint(\"F1 Score:\",np.mean(results_nb['test_f1_micro']))\nprint(\"Log Loss:\",np.mean(-1*results_nb['test_neg_log_loss']))\n\nprint(\"\\nT Component Model Performance\")\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nresults_nb = cross_validate(t_model, t_data['cleaned'], t_data['T'], cv=kfolds,scoring=scoring, n_jobs=-1)\nprint(\"Accuracy:\",np.mean(results_nb['test_acc']))\nprint(\"F1 Score:\",np.mean(results_nb['test_f1_micro']))\nprint(\"Log Loss:\",np.mean(-1*results_nb['test_neg_log_loss']))\n\nprint(\"\\nJ Component Model Performance\")\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nresults_nb = cross_validate(j_model, j_data['cleaned'], j_data['J'], cv=kfolds,scoring=scoring, n_jobs=-1)\nprint(\"Accuracy:\",np.mean(results_nb['test_acc']))\nprint(\"F1 Score:\",np.mean(results_nb['test_f1_micro']))\nprint(\"Log Loss:\",np.mean(-1*results_nb['test_neg_log_loss']))","ac557fce":"probab[0][0][0]","6f483717":"import time\nprint(\"> > Predict your personality based on an excerpt!\")\nprint(\"Submit a rather long (preferably personal) piece of writing written by you.\")\nprint(\"For example, an aggregation of all of your tweets, or a personal essay.\")\nprint(\"Enter 'ex' for an example. Otherwise, paste your writing here.\")\nprint(\"\/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/  \")\nprint(\"Writing Sample Entry:\")\ntime.sleep(1)\nentry = pd.DataFrame()\nwhile True:\n    input_thing = input(\" \")\n    if input_thing == 'ex':\n        print(\"\/\/ Example: \")\n        print(data['posts'][13])\n        print(\"\")\n    else:\n        break\n        \nentry['input'] = [input_thing]\nentry['cleaned'] = entry['input'].apply(cleanText)\ni = int(i_model.predict(entry['cleaned']))\nn = int(n_model.predict(entry['cleaned']))\nt = int(t_model.predict(entry['cleaned']))\nj = int(j_model.predict(entry['cleaned']))\n\nprobab = []\n\nprobab.append(i_model.predict_proba(entry['cleaned']))\nprobab.append(n_model.predict_proba(entry['cleaned']))\nprobab.append(t_model.predict_proba(entry['cleaned']))\nprobab.append(j_model.predict_proba(entry['cleaned']))\n\ncomplete = []\ncomp_init = []\nif i == 1:\n    complete.append('Introversion')\n    comp_init.append('I')\nelse:\n    complete.append('Extroversion')\n    comp_init.append('E')\nif n == 1:\n    complete.append('Intuition')\n    comp_init.append('N')\nelse:\n    complete.append('Sensing')\n    comp_init.append('S')\nif t == 1:\n    complete.append('Thinking')\n    comp_init.append('T')\nelse:\n    complete.append('Feeling')\n    comp_init.append('F')\nif j == 1:\n    complete.append('Judging')\n    comp_init.append('J')\nelse:\n    complete.append('Percieving')\n    comp_init.append('P')\n    \nperson_string = ''\nfor i in comp_init:\n    person_string += i\n\nprint(\"\\n\/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \/ \\n\")\nprint(\"MBTI Personality: {}\".format(person_string))\n\nprobab_simp = []\nfor i in range(4):\n    if probab[i][0][0]>probab[i][0][1]:\n        append_value = probab[i][0][0]\n    else:\n        append_value = probab[i][0][1]\n    probab_simp.append(append_value)\n\n'''\nresults = pd.DataFrame({'Characteristic':complete,'Probability':probab_simp})\n\nprint(\"Your MBTI characteristics are: {}\".format(person_string_comp))\nresults'''\n    \nresults = pd.DataFrame({'Characteristic':complete,'Abbreviation':comp_init})\nresults","bd2332fb":"results = pd.DataFrame({'Characteristic':complete,'Abbreviation':comp_init})\nresults","5db86d18":"### Re-Stratify Components","4e760599":"### Stratify I data","2591ee11":"### View Data","cb77288b":"### Stratify J","b0aed646":"### Stratify N","b3100f36":"### We'll be attempting to predict your personality based on a sample of your messages\/posts!\nThis is based on the MBTI personality type scale, a 4-letter combination in which:\n* Introversion (I) \u2013 Extroversion (E)\n* Intuition (N) \u2013 Sensing (S)\n* Thinking (T) \u2013 Feeling (F)\n* Judging (J) \u2013 Perceiving (P)\n\nFor example, one personality might be \"INTJ\", indicating that you are introverted over extroverted, intuitive over sensitive, prefer thinking over feeling, and judging over percieving.\n\nUnlike other kernels, I've tried to use a different way to predict the personality. Many kernels try to predict the 4-letter code using one model. I've decided to train 4 models, each trained only on one of the 4 aspects of the MBTI personality scale. Then, we'll be able to create an interface that predicts your personality based on a tweet\/message!","5bd83583":"# Training Models","5f248f21":"# Create Interface","17b4c69c":"This performs a lot better than previous performance when we left out information for stratification, and it works better, if not on par, with unstratified data.","c436e47f":"### Stratify T","40fc7a60":"# Loading Data","540a6e36":"# Cleaning Text","d9a19720":"**Leftover data as a result of stratification:**\n\n| Component | Num of 0 Data | Num of 1 Data | Num of Excess Data |\n| --- | --- | --- | --- |\n| I | 1999 | 6676 | 4677 |\n| N | 1197 | 7478 | 6281 |\n| T | 4694 | 3981 | 713 |\n| J | 5241 | 3434 | 1807 |","ab0c02cb":"There's a lot of data that we are leaving out. Can we find a way to utilize the excess data?\nFor example, in component I, we are leaving out 4680 training examples for Introverted data.\nOne option is to reuse the existing Extroverted data to match the Introverted data in terms of number. While there is no new information, let's try it to see if it works."}}