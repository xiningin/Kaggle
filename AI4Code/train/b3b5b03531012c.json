{"cell_type":{"b2c0d64e":"code","a240d668":"code","e81d8e3b":"code","2c917c8f":"code","b782e587":"code","be7d1a2a":"code","db8a542c":"code","4d990555":"code","576d51cc":"code","a7c17931":"code","d151884b":"code","5ecdca6b":"code","2f0f2002":"code","0f1728e5":"code","b4484772":"code","d8bd97ed":"code","8af2c980":"code","04da1d77":"code","86232671":"code","853e29a4":"code","085cb064":"code","9fac721d":"code","97e9c2e4":"code","c6f0341e":"code","6958be43":"code","5b88b7cf":"code","f516de26":"markdown","bb67bddb":"markdown","a1af4e07":"markdown","f276835a":"markdown","0e162a97":"markdown","33fc8468":"markdown","1f3d4e15":"markdown","71ca342d":"markdown","0065388b":"markdown","806de033":"markdown","80233c01":"markdown","97b33136":"markdown"},"source":{"b2c0d64e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport os\nimport gc","a240d668":"from sklearn.preprocessing import LabelEncoder","e81d8e3b":"base_dir = '\/kaggle\/input\/m5-forecasting-accuracy\/'\ntrain_dir = os.path.join(base_dir, 'sales_train_evaluation.csv')\ntest_dir = os.path.join(base_dir, 'sample_submission.csv')\ncalendar_dir = os.path.join(base_dir, 'calendar.csv')\nprice_dir = os.path.join(base_dir, 'sell_prices.csv')\nsub_dir = os.path.join(base_dir, 'sample_submission.csv')","2c917c8f":"df_train = pd.read_csv(train_dir)\ndf_test = pd.read_csv(test_dir)\ndf_calendar = pd.read_csv(calendar_dir)\ndf_price = pd.read_csv(price_dir)\ndf_sub = pd.read_csv(sub_dir)","b782e587":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","be7d1a2a":"def making_train_data(df_train):\n    print(\"processing train data\")\n    df_train_after = pd.melt(df_train, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='days', value_name='demand')\n    df_train_after['days'] = df_train_after['days'].map(lambda x: int(x[2:]))\n    df_train_after = df_train_after.drop(['id'], axis=1)\n    df_train_after = reduce_mem_usage(df_train_after)\n    gc.collect()\n    return df_train_after","db8a542c":"def making_test_data(df_test):\n    print(\"processing test data\")\n    df_test['item_id'] = df_test['id'].map(lambda x: x[:-16])\n    df_test['dept_id'] = df_test['item_id'].map(lambda x: x[:-4])\n    df_test['cat_id'] = df_test['dept_id'].map(lambda x: x[:-2])\n    df_test['store_id'] = df_test['id'].map(lambda x: x[-15:-11])\n    df_test['state_id'] = df_test['store_id'].map(lambda x: x[:-2])\n    df_test['va_or_ev'] = df_test['id'].map(lambda x: x[-10:])\n    df_test_val = df_test.loc[df_test['va_or_ev'] == 'validation', :]\n    df_test_ev = df_test.loc[df_test['va_or_ev'] == 'evaluation', :]\n    df_test_val_after = pd.melt(df_test_val, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'va_or_ev'], var_name='days', value_name='demand')\n    df_test_ev_after = pd.melt(df_test_ev, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'va_or_ev'], var_name='days', value_name='demand')\n    df_test_after = pd.concat([df_test_val_after, df_test_ev_after])\n    df_test_after['days'] = df_test_after['days'].map(lambda x: int(x[1:]))\n    df_test_after.loc[df_test_after['va_or_ev']=='evaluation', ['days']] += 28\n    df_test_after['days'] += 1913\n    df_test_after = df_test_after.drop(['va_or_ev'], axis=1)\n    df_test_after = df_test_after.drop(['id'], axis=1)\n    df_test_after = reduce_mem_usage(df_test_after)\n    return df_test_after","4d990555":"def making_train_test_data(df_train ,df_test):\n    df_train = making_train_data(df_train)\n    df_test = making_test_data(df_test)\n    print(\"processing train test data\")\n    max_train_days = df_train['days'].max()\n    min_test_days = df_test['days'].min()\n    shift_data = 6\n    df_test = pd.concat([df_train.loc[max_train_days - 28 * shift_data <= df_train['days'], :], df_test.loc[df_test['days'] > max_train_days, :]]).reset_index(drop=True)\n    \n#     shift_days_set = [28, 29, 30]\n#     for i in shift_days_set:\n#         df_train['demand_{}day_ago'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(i))\n#         df_test['demand_{}day_ago'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(i))\n#         gc.collect()\n        \n    rolling_days_set = [2, 3, 5, 7, 14, 28, 56, 140]\n    for i in rolling_days_set:\n        df_train['demand_{}day_mean'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).mean())\n#         df_train['demand_{}day_max'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).max())\n        \n        df_test['demand_{}day_mean'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).mean())\n#         df_test['demand_{}day_max'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).max())\n        df_train = reduce_mem_usage(df_train)\n        df_test = reduce_mem_usage(df_test)\n        gc.collect()\n    \n    df_test = df_test.loc[df_test['days'] >= min_test_days, :]\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    \n    return df_train, df_test","576d51cc":"def making_calendar_data(df_calendar):\n    df_calendar = reduce_mem_usage(df_calendar)\n    gc.collect()\n    print(\"processing calendar data\")\n    df_calendar['days'] = df_calendar['d'].map(lambda x: int(x[2:]))\n    event_type = {np.nan: 1, 'Sporting': 2, 'Cultural': 3, 'National': 5, 'Religious': 7}\n    df_calendar['event_type_1'] = df_calendar['event_type_1'].map(event_type)\n    df_calendar['event_type_2'] = df_calendar['event_type_2'].map(event_type)\n    df_calendar['event_type'] = df_calendar['event_type_1'] * df_calendar['event_type_2']\n    le = LabelEncoder()\n    le.fit(df_calendar['event_type'])\n    df_calendar['event_type'] = le.transform(df_calendar['event_type'])\n    df_calendar = df_calendar.drop(['event_type_1', 'event_type_2', 'event_name_1', 'event_name_2', 'd', 'weekday', 'date', 'year'], axis=1)\n#     df_calendar['event_type_1day_ago'] = df_calendar['event_type'].shift(1)\n#     df_calendar['event_type_1day_after'] = df_calendar['event_type'].shift(-1)\n    df_calendar = reduce_mem_usage(df_calendar)\n    gc.collect()\n    return df_calendar","a7c17931":"def making_price_data(df_price):\n    df_price = reduce_mem_usage(df_price)\n    gc.collect()\n    print(\"processing price data\")\n#     shift_days_set = [28, 35, 42]\n#     for i in shift_days_set:\n#         df_price['price_{}day_ago'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(i))\n#     gc.collect()\n    \n    rolling_days_set = [28, 140]\n    for i in rolling_days_set:\n        df_price['price_{}day_mean'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).mean())\n        df_price['price_{}day_max'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).max())\n        df_price['price_{}day_min'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).min())\n        df_price = reduce_mem_usage(df_price)\n        gc.collect()\n    return df_price","d151884b":"def concat_data(df_train, df_test, df_calendar, df_price):\n    df_train, df_test = making_train_test_data(df_train ,df_test)\n    df_calendar = making_calendar_data(df_calendar)\n    df_price = making_price_data(df_price)\n    print(\"concat data\")\n    df_train = pd.merge(df_train, df_calendar, on='days', how='left')\n    df_test = pd.merge(df_test, df_calendar, on='days', how='left')\n    df_train = pd.merge(df_train, df_price, on=['wm_yr_wk', 'store_id', 'item_id'], how='left')\n    df_test = pd.merge(df_test, df_price, on=['wm_yr_wk', 'store_id', 'item_id'], how='left')\n    df_train = df_train.drop(['wm_yr_wk'], axis=1)\n    df_test = df_test.drop(['wm_yr_wk'], axis=1)\n    del df_calendar, df_price\n    gc.collect()\n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    return df_train, df_test","5ecdca6b":"def labeling_data(df_train, df_test, df_calendar, df_price):\n    df_train, df_test = concat_data(df_train, df_test, df_calendar, df_price)\n    print(\"labeling data\")\n    label_columns = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    for c in label_columns:\n        le  = LabelEncoder()\n        le.fit(df_train[c])\n        df_train[c] = le.transform(df_train[c])\n        df_test[c] = le.transform(df_test[c])\n        if c != 'item_id':\n            print(le.classes_)\n    \n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    \n    return df_train, df_test","2f0f2002":"df_train, df_test = labeling_data(df_train, df_test, df_calendar, df_price)","0f1728e5":"for c in df_train.columns:\n    print(c)","b4484772":"df_train","d8bd97ed":"df_test","8af2c980":"from sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\n\ndef metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)\/np.sum(y_true, axis=0))\n\ndef get_feature_importances(data, store_id, y_valid, y_valid_pred, used_features):\n    train_features = used_features\n    imp_df = pd.DataFrame()\n    imp_df[\"importance_gain_{}\".format(store_id)] = lgb.feature_importance(importance_type='gain')\n    imp_df[\"importance_split_{}\".format(store_id)] = lgb.feature_importance(importance_type='split')\n    imp_df[\"valid_rmse_{}\".format(store_id)] = mean_squared_error(y_valid, y_valid_pred, squared=False)\n    imp_df[\"valid_wrmse_{}\".format(store_id)] = metric(y_valid, y_valid_pred)\n    return imp_df\n\ntotal_imp_df = pd.DataFrame()\ndf_sub_ensemble = df_test.loc[:, ['item_id', 'store_id', 'days', 'demand']]\ndf_sub_ensemble['demand_model'] = 0","04da1d77":"df_train['snap'] = 0\ndf_test['snap'] = 0\nused_features = [c for c in df_train.columns if c not in ['demand', 'item_id', 'store_id', 'state_id', 'days', 'snap_CA', 'snap_TX', 'snap_WI']]\ntotal_imp_df[\"feature\"] = used_features\nstore_id_list = df_train['store_id'].unique()\nfor store_id in store_id_list:\n    print('store_id {}\/10'.format(store_id + 1))\n    \n    df_train['snap'] = 0\n    df_test['snap'] = 0\n    if 0 <= store_id <= 3:\n        df_train['snap'] = df_train['snap_CA']\n        df_test['snap'] = df_test['snap_CA']\n    elif 4 <= store_id <= 6:\n        df_train['snap'] = df_train['snap_TX']\n        df_test['snap'] = df_test['snap_TX']\n    else:\n        df_train['snap'] = df_train['snap_WI']\n        df_test['snap'] = df_test['snap_WI']\n        \n    train_index = (df_train['days'] < 1913 - 28) & (df_train['store_id'] == store_id)\n    valid_index = (1913 - 28 <= df_train['days']) & (df_train['store_id'] == store_id)\n    test_index = (df_test['store_id'] == store_id)\n    \n    X_train = df_train.loc[train_index, used_features].values\n    y_train = df_train.loc[train_index, 'demand'].values\n\n    X_valid = df_train.loc[valid_index, used_features].values\n    y_valid = df_train.loc[valid_index, 'demand'].values\n\n    X_test = df_test.loc[test_index, used_features].values\n\n    lgb_params = {\n        'objective': 'poisson',\n        'num_iterations' : 2000,\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.075,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10,\n        'colsample_bytree': 0.75\n                  }\n\n    train_data = lgbm.Dataset(X_train, y_train)\n    valid_data = lgbm.Dataset(X_valid, y_valid)\n\n    lgb = lgbm.train(lgb_params, train_data, valid_sets=[train_data, valid_data], early_stopping_rounds=10, verbose_eval=20)\n    \n    y_valid_pred = lgb.predict(X_valid, num_iteration=lgb.best_iteration)\n    y_test_pred = lgb.predict(X_test, num_iteration=lgb.best_iteration)\n    df_sub_ensemble.loc[test_index, ['demand_model']] = y_test_pred\n    \n    print(metric(y_valid, y_valid_pred))\n    \n    imp_df = get_feature_importances(df_train, store_id, y_valid, y_valid_pred, used_features)\n    total_imp_df = pd.concat([total_imp_df, imp_df], axis=1, sort=False)\n    \n    del X_train, X_valid, X_test, y_train, y_valid, lgb\n    gc.collect()","86232671":"df_imp = pd.DataFrame(columns=['features', 'importance_gain', 'importance_split', 'valid_rmse', 'valid_wrmse'])\ndf_imp[\"features\"] = used_features\ndf_imp['importance_gain'] = 0\ndf_imp['importance_split'] = 0\ndf_imp['valid_rmse'] = 0\ndf_imp['valid_wrmse'] = 0\nn_stores = len(store_id_list)\nfor store_id in store_id_list:\n    df_imp['importance_gain'] += total_imp_df['importance_gain_{}'.format(store_id)].values \/ n_stores\n    df_imp['importance_split'] += total_imp_df['importance_split_{}'.format(store_id)].values \/ n_stores\n    df_imp['valid_rmse'] += total_imp_df['valid_rmse_{}'.format(store_id)].values \/ n_stores\n    df_imp['valid_wrmse'] += total_imp_df['valid_wrmse_{}'.format(store_id)].values \/ n_stores","853e29a4":"df_imp.sort_values(by='importance_gain', ascending=False)","085cb064":"df_sub_before = df_test.loc[:, ['days', 'demand']]","9fac721d":"df_sub_before['demand'] = df_sub_ensemble['demand_model']","97e9c2e4":"df_sub = pd.read_csv(sub_dir)\ndf_sub_base = pd.read_csv(sub_dir)","c6f0341e":"def making_submission(df_sub, df_sub_before, df_sub_base):\n    df_sub['va_or_ev'] = df_sub['id'].map(lambda x: x[-10:])\n    df_sub_val = df_sub.loc[df_sub['va_or_ev'] == 'validation', :]\n    df_sub_ev = df_sub.loc[df_sub['va_or_ev'] == 'evaluation', :]\n    df_sub_val = df_sub_val.melt(id_vars=['id', 'va_or_ev'], var_name='days', value_name='demand').drop(['va_or_ev'], axis=1)\n    df_sub_ev = df_sub_ev.melt(id_vars=['id', 'va_or_ev'], var_name='days', value_name='demand').drop(['va_or_ev'], axis=1)\n    num_va = df_sub_val.shape[0]\n    num_ev = df_sub_ev.shape[0]\n    df_sub_val['demand'] = df_sub_before['demand'][:num_va].values\n    df_sub_ev['demand'] = df_sub_before['demand'][num_va:].values\n    df_sub_val = df_sub_val.pivot(index='id', columns='days', values='demand').reset_index()\n    df_sub_ev = df_sub_ev.pivot(index='id', columns='days', values='demand').reset_index()\n    df_sub_after = pd.concat([df_sub_val, df_sub_ev])\n    df_sub_columns = ['id'] + ['F{}'.format(i+1) for i in range(28)]\n    df_sub = df_sub_after.loc[:, df_sub_columns]\n    df_sub.columns = df_sub_columns\n    df_sub = pd.merge(df_sub_base['id'], df_sub, on='id', how='left')\n    return df_sub","6958be43":"df_sub = making_submission(df_sub, df_sub_before, df_sub_base)\ndf_sub.to_csv('.\/my_submission.csv', index=False)","5b88b7cf":"df_sub","f516de26":"# Import Modules","bb67bddb":"# Making Ensemble Submission","a1af4e07":"In Making Data Section, I made features using fundamental method such as lag features, date features, rolling mean and etc.\n\n\u30e9\u30b0\u7279\u5fb4\u91cf\u3001\u65e5\u4ed8\u3001\u79fb\u52d5\u5e73\u5747\u306a\u3069\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002","f276835a":"# Making Submission","0e162a97":"# Modeling","33fc8468":"In this Section, I remake original submission format.\n\n\u6700\u7d42\u7684\u306a\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u5143\u3005\u306e\u30b5\u30d6\u30df\u30c3\u30b7\u30e7\u30f3\u306e\u5f62\u5f0f\u3068\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u4f5c\u308a\u76f4\u3057\u3066\u3044\u307e\u3059\u3002","1f3d4e15":"# Introduction","71ca342d":"# Making Data","0065388b":"I'm sorry for not being good at English.\n\nThis is my first public notebook and I am beginner. Surly, it includes many defects. Thank you for your feedback!.\n\nKaggle\u521d\u5fc3\u8005\u3067\u3059\u304c\u3001\u521d\u3081\u3066\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u516c\u958b\u3057\u307e\u3059\u3002\u6570\u591a\u3044\u6b20\u9665\u304c\u3042\u308b\u3068\u601d\u308f\u308c\u307e\u3059\u304c\u3001\u3054\u4e86\u627f\u304f\u3060\u3055\u3044\u3002\u00a0\u30b3\u30e1\u30f3\u30c8\u306a\u3069\u3067\u6539\u5584\u70b9\u306a\u3069\u3042\u308c\u3070\u66f8\u3044\u3066\u304f\u308c\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\uff01\n\n\nI decide to publish notebook because I don't have much time to spend this competition to prepare for regular examination, and I'm happy that this notebook helps someone.\n\nM5\u30b3\u30f3\u30da\u30c6\u30a3\u30b7\u30e7\u30f3\u306b\u639b\u3051\u308c\u308b\u6642\u9593\u304c\u30c6\u30b9\u30c8\u52c9\u5f37\u306b\u53d6\u3089\u308c\u3001\u307b\u3068\u3093\u3069\u3067\u304d\u306a\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u3068\u601d\u3063\u305f\u306e\u3067\u3001\u8ab0\u304b\u306e\u304a\u5f79\u306b\u7acb\u3066\u308c\u3070\u3068\u601d\u3044\u3001\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u516c\u958b\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\n\n\nIn this notebook, I predict demand using lightgbm model for every store.\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u5e97\u8217\u6bce\u306bLightGBM\u306e\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u9700\u8981\u3092\u4e88\u6e2c\u3057\u3066\u3044\u307e\u3059\u3002\n\nI validate score using out of hold, not cross validation.\n\n\u691c\u8a3c\u3068\u3057\u3066\u3001\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u306f\u306a\u304f\u3001\u30a2\u30a6\u30c8\u30aa\u30d6\u30db\u30fc\u30eb\u30c9\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002(\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u691c\u8a3c\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3068\u3001\u8a08\u7b97\u6642\u9593\u3092\u8003\u616e\u3057\u305f\u7d50\u679c)","806de033":"In this Section, I import necessary modules.\n\n\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u3066\u3044\u304d\u307e\u3059\u3002","80233c01":"I predict demand using lightgbm model for every store.\n\n\u5e97\u8217\u6bce\u306bLightGBM\u306e\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u9700\u8981\u3092\u4e88\u6e2c\u3057\u3066\u3044\u307e\u3059\u3002","97b33136":"In this Section, I used to predict test data using many models.\u3000Now, It isn't necessary.\n\n\u3053\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u7d50\u679c\u3092\u4e00\u3064\u306b\u307e\u3068\u3081\u3066\u3044\u307e\u3057\u305f\u3002\u305d\u306e\u540d\u6b8b\u3067\u3059\u3002"}}