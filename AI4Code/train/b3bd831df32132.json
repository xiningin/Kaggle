{"cell_type":{"8dc1512c":"code","bb0cf73f":"code","1a0f0d81":"code","d5396e10":"code","3e4cba4c":"code","554615e3":"code","d4b14a5e":"code","910ba460":"code","e7e85976":"code","56b32a64":"code","5ebab890":"code","ebf1bdaf":"code","2850cca5":"code","3487228c":"code","2d754470":"code","23da1b4e":"code","e47944a1":"code","e5b496e6":"code","ae6dc5a6":"code","e461aa84":"code","2b5e3f15":"code","4a4d9f23":"code","bd2cbd8c":"code","9098682f":"code","18134e04":"code","4f099384":"code","afa1dc02":"code","aab9091b":"code","fb58c1ac":"code","aca51cce":"code","709982d7":"code","20a6ab70":"code","8eec1996":"code","11666f19":"markdown","a84cf0d2":"markdown","4966fd56":"markdown","c7341056":"markdown","5b5c7df1":"markdown","078b642a":"markdown","924b4b8a":"markdown","f4e054e9":"markdown","4d711da9":"markdown","8d9b18de":"markdown","5c8dad63":"markdown","8b38170b":"markdown","aef6166d":"markdown","1cd56aca":"markdown","b7a20c32":"markdown","4f41ceca":"markdown","b7ab70b7":"markdown","ae5ea5d0":"markdown","f9d274e9":"markdown","e773ab26":"markdown","8446fea1":"markdown","292e7a5a":"markdown","7f70448c":"markdown","623980fe":"markdown"},"source":{"8dc1512c":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (8, 5)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", 150)","bb0cf73f":"# input data files are available in the read-only \"..\/input\/\" directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1a0f0d81":"df_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ndf_all = pd.concat([df_train, df_test], ignore_index=True)\ndf_all.drop(\"SalePrice\", axis=1, inplace=True)","d5396e10":"df_train.head()","3e4cba4c":"df_train.shape, df_test.shape","554615e3":"num_cols = df_train.select_dtypes(include=\"number\").columns.to_list()\ncat_cols = df_train.select_dtypes(exclude=\"number\").columns.to_list()","d4b14a5e":"# from scipy.stats import probplot\n\n# sns.histplot(data=df_train[\"SalePrice\"], kde=True);\n\n# plt.figure()\n# probplot(df_train[\"SalePrice\"], plot=plt);","910ba460":"# skew = df_train[\"SalePrice\"].skew()\n# kurt = df_train[\"SalePrice\"].kurt()\n# print(f\"Skew of SalePrice: {skew:.2f},\\nKurtosis of SalePrice: {kurt:.2f}\")","e7e85976":"# df_train[\"SalePrice\"] = np.log(df_train[\"SalePrice\"])","56b32a64":"# sns.histplot(data=df_train[\"SalePrice\"], kde=True);\n\n# plt.figure()\n# probplot(df_train[\"SalePrice\"], plot=plt);","5ebab890":"# skew = df_train[\"SalePrice\"].skew()\n# kurt = df_train[\"SalePrice\"].kurt()\n# print(f\"Skew of SalePrice: {skew:.2f},\\nKurtosis of SalePrice: {kurt:.2f}\")","ebf1bdaf":"fig, ax = plt.subplots(6, 7, figsize=(25, 20))\nax = ax.flatten()\n\nfor i, col in enumerate(num_cols):\n#     sns.histplot(data=df_train[col], kde=True, ax=ax[i])\n    sns.stripplot(x=df_train[col], ax=ax[i])","2850cca5":"fig, ax = plt.subplots(6, 8, figsize=(25, 25))\nax = ax.flatten()\n\nfor i, col in enumerate(cat_cols):\n    sns.countplot(x=col, data=df_train, ax=ax[i])","3487228c":"for col in cat_cols[:3]:  # I just do the first 3 columns here to avoir cluttering the notebook\n    print(col)\n    print(df_train[col].value_counts(dropna=False))\n    print()","2d754470":"df_all.shape","23da1b4e":"missing_values = df_all.isna().sum()[df_all.isna().sum() > 0].sort_values(ascending=False).to_frame(\"count\")\nmissing_values[\"frequency\"] = missing_values[\"count\"] \/ df_all.shape[0]\nmissing_values","e47944a1":"# NaN means absence of object\nfor col in [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n            \"BsmtExposure\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n            \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\"]:\n    df_all[col] = df_all[col].fillna(\"None\")\n\n    \n# NaN means value is actually missing: replace by the mode\nfor col in [\"GarageYrBlt\", \"MasVnrArea\", \"MasVnrType\", \"BsmtFullBath\",\"BsmtHalfBath\",\n            \"GarageArea\", \"GarageCars\", \"TotalBsmtSF\",\"BsmtFinSF1\", \"BsmtFinSF2\",\n            \"BsmtFinSF2\", \"MSZoning\", \"Functional\", \"Utilities\", \"Electrical\",\n            \"KitchenQual\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"BsmtUnfSF\"]:\n    df_all[col] = df_all[col].fillna(df_all[col].mode()[0])\n\n# NaN means value is actually missing: replace by the median of similar values in Neighborhood\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","e5b496e6":"df_all.isna().sum().sum()","ae6dc5a6":"corr = df_train.corr()\ncorr[\"SalePrice\"].abs().sort_values(ascending=False).head(12)","e461aa84":"corr_cols = corr.loc[corr[\"SalePrice\"] > .48, \"SalePrice\"].sort_values(ascending=False).index\ncorr2 = df_train[corr_cols].corr()\nplt.figure(figsize=(10, 7))\nsns.heatmap(corr2, annot=True);","2b5e3f15":"corr_cols = corr_cols.drop(\"SalePrice\")  # delete the target variable first\n\nX_all = df_all[corr_cols]\nX_train = X_all[:df_train.shape[0]]\nX_test = X_all[df_train.shape[0]:]\n\ny_train = df_train[\"SalePrice\"]","4a4d9f23":"df_all.shape, df_train.shape, df_test.shape","bd2cbd8c":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n\npipe = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=0))\n\ncv = KFold(n_splits=5, shuffle=False)\n\nsfs = SequentialFeatureSelector(pipe, k_features=X_all.shape[1], forward=True, scoring=\"neg_root_mean_squared_error\", cv=cv)\nsfs.fit(X_train, y_train)\nsfs.subsets_","9098682f":"sfs.subsets_[9].get(\"feature_names\")","18134e04":"corr = X_train[list(sfs.subsets_[9].get(\"feature_names\"))].corr()\nsns.heatmap(corr, annot=True)","4f099384":"features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'YearRemodAdd']","afa1dc02":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","aab9091b":"X = X_train[features]\ny = y_train\n\nmodels = [LinearRegression(), Ridge(), ElasticNet(), RandomForestRegressor(), GradientBoostingRegressor(), Lasso(), LGBMRegressor()]\nrmses, maes, r2s = [], [], []\nfor model in models:\n    pipe = make_pipeline(StandardScaler(), model)\n    \n    y_preds = cross_val_predict(pipe, X, y, cv=cv, n_jobs=-1)\n    \n    rmse = np.sqrt(mean_squared_error(y_train, y_preds))\n    mae = mean_absolute_error(y_train, y_preds)\n    r2 = r2_score(y_train, y_preds)\n    \n    rmses.append(rmse)\n    maes.append(mae)\n    r2s.append(r2)\n    \npd.DataFrame({\"RMSE\": rmses,\"MAE\": maes,\"R2\": r2s}, index=models).sort_values(by=\"R2\", ascending=False)","fb58c1ac":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nparameters = {\n    \"gradientboostingregressor__learning_rate\": [1, 0.5, 0.25, 0.1, 0.05, 0.01],\n    \"gradientboostingregressor__n_estimators\": [1, 2, 4, 8, 16, 32, 64, 100, 200, 500],\n    \"gradientboostingregressor__max_depth\": np.arange(1, 33),\n}\n\npipe = make_pipeline(StandardScaler(), GradientBoostingRegressor())\nrand_grid = RandomizedSearchCV(pipe, parameters, n_iter=100, scoring=\"r2\", n_jobs=-1, cv=3, verbose=1)\nrand_grid.fit(X_train, y_train)\n\nprint()\nprint(f\"Best parameters: {rand_grid.best_params_}\")\nprint(f\"Best parameters: {rand_grid.best_score_:.4f}\")","aca51cce":"parameters = {\n    \"gradientboostingregressor__learning_rate\": [0.03, 0.04, 0.05, 0.06, 0.07],\n    \"gradientboostingregressor__n_estimators\": [180, 190, 200, 210, 220],\n    \"gradientboostingregressor__max_depth\": [1, 2, 3, 4],\n}\n\ngrid = GridSearchCV(pipe, parameters, scoring=\"r2\", n_jobs=-1, cv=3, verbose=1)\ngrid.fit(X_train, y_train)\n\nprint()\nprint(f\"Best parameters: {grid.best_params_}\")\nprint(f\"Best parameters: {grid.best_score_:.4f}\")","709982d7":"learning_rate = grid.best_params_['gradientboostingregressor__learning_rate']\nmax_depth = grid.best_params_['gradientboostingregressor__max_depth']\nn_estimators = grid.best_params_['gradientboostingregressor__n_estimators']\n\npipe = make_pipeline(StandardScaler(), GradientBoostingRegressor(learning_rate=learning_rate, max_depth=max_depth, n_estimators=n_estimators))\n\npipe.fit(X_train, y_train)\ny_preds = pipe.predict(X_test)\ny_preds","20a6ab70":"predictions = pd.DataFrame({\"Id\": df_test[\"Id\"], \"SalePrice\": y_preds})\npredictions.head()","8eec1996":"predictions.to_csv(\"output.csv\", index=False)","11666f19":"Let's see the correlation matrix of the selected 9 features","a84cf0d2":"Let's use these parameters to make the final predictions.","4966fd56":"I looked every single categorical columns. It seems there is not weird values\/categories within each column.  \nHowever there are a lot of missing values!","c7341056":"# Handle missing values\n(within df_all (train + test))","5b5c7df1":"# Correlation matrix","078b642a":"## Categorical columns","924b4b8a":"Some columns contains a lot of missing values (more than 99% for PoolQC for instance). But if you look at the data_description.txt, you'll notice that some NaN values actually mean an absence of something. For exemple NA in PoolQC means \"No Pool\". It means that we should not remove those NaN values.","f4e054e9":"GarageCars & GarageArea are highly correlated, let's discard GarageArea <br>\nGarageYrBlt & YearBlt are highly correlated, let's discard GarageYrBlt","4d711da9":"# Modeling","8d9b18de":"Get our train & test dataframes back","5c8dad63":"# Final predictions & submission","8b38170b":"It's hard to see if there are outliers right now. Let's use `value_counts()` for each column to see if weird values appear ","aef6166d":"Do a sequential feature selection to find the best features to use among those 11","1cd56aca":"There a lot of variables, let's apply a first selection to keep only the ones that are well correlated with the target variable \"SalePrice\"","b7a20c32":"9 features seems to be optimal (lowest RMSE)","4f41ceca":"# Visualize the data\n(for outliers detection)","b7ab70b7":"# Set up","ae5ea5d0":"Let's now use GridSearchCV to see if we can improve the results.","f9d274e9":"# Feature selection","e773ab26":"Let's start with the 11 more correlated variables (r > .48) for feature selection","8446fea1":"## Numeric columns","292e7a5a":"Some values are a bit off from the others like the one in BsmtFinSF1, but to me they are not outliers, they are still possible values. I'll keep them","7f70448c":"According to RMSE, MAE et R2 scores, GradientBoostingRegressor() seems to be the best regressor here. Let's see if we can tune its parameters.","623980fe":"# Hyperparameters tuning"}}