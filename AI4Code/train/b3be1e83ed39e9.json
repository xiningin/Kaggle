{"cell_type":{"093fb252":"code","563d3884":"code","ecf182d1":"code","49dd215e":"code","f4b2ec54":"code","10d93352":"code","176cf868":"code","3ad9f74d":"code","7eadebb8":"code","d088ea5e":"code","c830b04a":"code","dc6dc568":"code","4e69fec8":"code","6e4684e2":"code","b8ff7efb":"code","22532149":"code","7aef7531":"code","4ea6f873":"code","cb34b3f4":"code","7b012cfc":"markdown","adb448ad":"markdown","bbe3ab44":"markdown","667b255b":"markdown","e3d256fb":"markdown","6b06ed11":"markdown","7589f408":"markdown","130af55b":"markdown","d23b68c3":"markdown","8c80e032":"markdown","9b3c5991":"markdown","a45a70f2":"markdown","ce926dfa":"markdown","d469414f":"markdown","093df148":"markdown","b67d3b11":"markdown","66a103f4":"markdown"},"source":{"093fb252":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder","563d3884":"titanic = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic.head()","ecf182d1":"corr = titanic.corr()\nsns.heatmap(corr)","49dd215e":"ax = sns.countplot(x=\"Survived\", hue=\"Sex\", data=titanic)","f4b2ec54":"fig, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.barplot(ax=axes[0], x='Pclass', y='Survived', data=titanic)\nsns.barplot(ax=axes[1], x='Embarked', y='Survived', data=titanic)\nfig.show()","10d93352":"titanic.Embarked.value_counts()","176cf868":"fig, axes = plt.subplots(2, 2, figsize=(10,5))\nsns.histplot(ax=axes[0, 0], x=\"Fare\", hue=\"Survived\", data=titanic, \n             binrange=[0,150])  # quite long tail, but the more the fare, \n                                # the greater the cance of survival\nsns.histplot(ax=axes[0, 1], x=\"Age\", hue=\"Survived\", data=titanic,\n            binrange=[0,40])\nsns.histplot(ax=axes[1, 0], x=\"SibSp\", hue=\"Survived\", data=titanic)\nsns.histplot(ax=axes[1, 1], x=\"Parch\", hue=\"Survived\", data=titanic)\naxes[0,0].set_yscale('log')\naxes[1,0].set_yscale('log')\naxes[1,1].set_yscale('log')\nplt.subplots_adjust(bottom=0.1, right=0.8, top=1.5)\n\n\nfig.show()","3ad9f74d":"titanic.info()","7eadebb8":"par_child = titanic[(titanic.Age.isnull()) &\n                    ((titanic.SibSp != 0) |\n                    (titanic.Parch != 0))]\npar_child[par_child.Survived == 1].sort_values('Name', ascending=False).head()","d088ea5e":"no_parents_child = titanic[(titanic.Age.isnull()) &\n        (titanic.SibSp == 0) &\n        (titanic.Parch == 0)]\nprint(f\"Single: {len(no_parents_child)}, with family: {len(par_child)}\")","c830b04a":"titanic_cleaned = titanic.copy()\n\n\ndef set_children_ages(df):\n    df = df.copy()\n    children = df[(df.Age.isnull()) &\n                  (df.Parch != 0) &\n                  (df.SibSp != 0)]\n    children.Age = 3.0\n    df.drop(children.index, inplace=True)\n    df = pd.concat([df, children])\n    \n    return df\n\ndef set_median(column, df):\n    df = df.copy()\n    median = np.median(df[column].dropna())\n    df[column].fillna(median, inplace=True)\n    \n    return df\n\ndef set_average(column, df):\n    df = df.copy()\n    average = np.average(df[column].dropna())\n    df[column].fillna(average, inplace=True)\n    \n    return df\n\ndef is_minor(df):\n    df = df.copy()\n    minor = list()\n    for age in df.Age:\n        if age < 18.0:\n            minor.append(1)\n        else:\n            minor.append(0)\n    df['Minor'] = minor\n    \n    return df\n\ndef high_fare(df):\n    df = df.copy()\n    high_fare = list()\n    for fare in df.Fare:\n        if fare > 50.0:\n            high_fare.append(1)\n        else:\n            high_fare.append(0)\n    df['High_Fare'] = high_fare\n    \n    return df\n\ndef set_port(df):\n    df = df.copy()\n    df.Embarked = df.Embarked.fillna('S')\n    return df\n\ntitanic_cleaned = set_children_ages(titanic_cleaned)\ntitanic_cleaned = set_median('Age', titanic_cleaned)\ntitanic_cleaned = is_minor(titanic_cleaned)\ntitanic_cleaned = high_fare(titanic_cleaned)\ntitanic_cleaned = set_port(titanic_cleaned)\ntitanic_cleaned.Embarked.value_counts()","dc6dc568":"titanic_cleaned = pd.get_dummies(titanic_cleaned, columns=['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked'])\ntitanic_cleaned.head()","4e69fec8":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfeatures = ['Minor', 'High_Fare', 'Sex_female', 'Sex_male', \n            'Pclass_1', 'Pclass_2', 'Pclass_3', 'SibSp_0',\n            'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4', \n            'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1', \n            'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', \n            'Parch_6', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nX = titanic_cleaned[features]\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\ny = titanic_cleaned['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)","6e4684e2":"from sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\n\nmodels = [\n    ('LogReg', LogisticRegression()),\n    ('DT', tree.DecisionTreeClassifier()),\n    ('RF', RandomForestClassifier()),\n    ('KNN', KNeighborsClassifier()),\n    ('SVM', SVC(probability=True)),\n    ('GNB', GaussianNB()),\n    ('XGB', XGBClassifier(verbosity=0, use_label_encoder=False))\n]\n\ntarget_names = ['Not-Survived', 'Survived']\n\nscoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\nresults = []\nnames = []\nmodel_cv_results = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=1986)\n    cv_result = model_selection.cross_validate(model, X, y, cv=kfold, scoring=scoring)\n \n    results.append(cv_result)\n    names.append(name)\n    \n    df = pd.DataFrame(cv_result)\n    df['model'] = name\n    model_cv_results.append(df)\n    \nmodel_cv_results = pd.concat(model_cv_results, ignore_index=True)\nmodel_cv_results.groupby(['model']).mean()","b8ff7efb":"# model = RandomForestClassifier()\nmodel = XGBClassifier(verbosity=0, use_label_encoder=False)\nmodel.fit(X, y)","22532149":"titanic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_test.info()","7aef7531":"titanic_test = set_children_ages(titanic_test)\ntitanic_test = set_median('Age', titanic_test)\ntitanic_test = is_minor(titanic_test)\ntitanic_test = set_average('Fare', titanic_test)\ntitanic_test = high_fare(titanic_test)","4ea6f873":"titanic_test_features = pd.get_dummies(titanic_test, columns=['Sex', 'Pclass', 'SibSp', \n                                                              'Parch', 'Embarked'])\nX_test = titanic_test_features[features]\n# apply MinMaxScaler\nX_test = scaler.fit_transform(X_test)","cb34b3f4":"y_test = model.predict(X_test)\n# All female survived as in base model\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = titanic_test['PassengerId']\nsubmission['Survived'] = y_test\nsubmission.to_csv('submission.csv', index=False)","7b012cfc":"- There's a soft negative correlation between class and Fare, as expected. 1st class is 1 and 3rd 3.\n- There's a soft positive correlation between SibSp and Patch, expected also. This might indicate some covariance between parameters.","adb448ad":"# 5. Training data\n## 5.1 Cleaning\nThe training data have some missing values which will be filled","bbe3ab44":"Female group have a greater survival rate","667b255b":"## 5.3 Predictions\nFinally, we'll make the predictions","e3d256fb":"# Setup\n\nRegular libraries needed for Data Analysis \/ EDA will be imported first.","6b06ed11":"# 3. Feature Engineering\nThe following paragraphs will have the dataset transformation in order to increase the number of predictors\n\n## Set One-Hot Encoding\nTODO: Apprently adding the Embarking feature doesn't add to the prediction:\n- w\/o embarking 0.77272 (with Random Forest)\n- with embarking 0.76076 (with XGBoost, similar with RF)","7589f408":"## 4.2 Optimize the best candidate\n**TODO**: Implement a K-Fold RandomizedSearch to optimize parameters in the best 2 algorithms.\n\n```\nRandomizedSearchCV(estimator = rf_base, param_distributions = rf_grid, \n                               n_iter = 200, cv = 3, verbose = 2, random_state = 42, \n                               n_jobs = -1)\n```\n[sklearn randomized search cv](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html)\n\n[blog post](https:\/\/towardsdatascience.com\/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d)","130af55b":"After this simple analysis, the 177 people without age will be divided by the following criteria:\n- The vast majority (133) have no family links on board, we'll use the median age for them\n- There are many females who have siblings but no parents nor children, those are treated as adults and will be filled with the median age too.\n- For those survivals who are children, since they have Parent\/Children and Siblings, they'll be treated as median age children 3 yrs.\n- The rest who didn't survive, will be filled with median Age. Since they're the most representative sub-group, they don't impact in the net number.","d23b68c3":"However, what is most important are those who survived (whom class has fewer observations) so we'll take care of them. From the SibSp & Parch values, they seem to be childs.\n\nThe vast majority has values equal to 0, hence we'll use median value for them","8c80e032":"# 4. Model Selection\n\n## 4.1 Train Several Candidates\n- We'll train some models to see how they predict\n- At the same time we'll use k-folding to see how it predict across the whole dataset","9b3c5991":"- Survival rate descends almost linearly with classes.\n- Ports have different survival rates.\n- We can treat Embarked missing values as if they were embarked in S, since the vast majority did it there. In this way we don't alter the data group distribution.","a45a70f2":"## 5.2 Transform\nWe'll transform the dataset to later predict with it.\n\nPredicting all women survived, and prediction on the rest doesn't increase performance: 0.74880.","ce926dfa":"## 2.1 Cleaning Functions\n\nWe'll define some cleaning functions in order to apply the same algorithms to the testing dataset if it was neccesary ","d469414f":"# 1. Exploratory Data Analysis - EDA\n\nLet's do a quick EDA over the most important features to select the predictors which help most to the model generalization\/prediction.","093df148":"There are some missing values in the predictors used that will be filled\/cleaned","b67d3b11":"- Seems like higher fares tend to have a higher survival rate.\n- There's a clear bias in babies and childs survival rate.\n- Single people tend to have higher mortality, based on the bottom plots.","66a103f4":"# 2. Dataset Cleaning\nLet's take a look at the missing age values. Particularly, the ones linked to families, which has SibSp and Parch values different from 0."}}