{"cell_type":{"fd9c8b85":"code","3ed37d9f":"code","b93fda62":"code","148b76cd":"code","4b032866":"code","c6ab8866":"code","b30b7bfb":"code","c9a84a68":"code","9f42e553":"code","199bfbb5":"code","c3378337":"code","4d842d1c":"code","89973f62":"code","c5ce6a91":"code","4ae497d4":"code","5d50911d":"code","34d5d230":"markdown","116fd981":"markdown","6325699d":"markdown","5d73ed5b":"markdown","151c2dc8":"markdown","fe38262c":"markdown","9bacab77":"markdown","1f3791dc":"markdown"},"source":{"fd9c8b85":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor","3ed37d9f":"base_path = '\/kaggle\/input\/tabular-playground-series-jan-2021\/'\n\ntrain_df = pd.read_csv(base_path+'train.csv', index_col='id').assign(_set='train')\ntest_df = pd.read_csv(base_path+'test.csv', index_col='id').assign(_set='test')","b93fda62":"train_df.head()","148b76cd":"fig, ax = plt.subplots(figsize=(10,5))\nsns.histplot(data=train_df, x='target', kde=True, ax=ax)","4b032866":"features = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n       'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\n\nn=len(features)\n\nfig, axs = plt.subplots(n,1, figsize=(10,5*n))\n\nfor f, ax in zip(features, axs):\n    sns.histplot(data=train_df, x=f, stat='probability', kde=True, ax=ax, alpha=0.5, label='train')\n    sns.histplot(data=test_df, x=f, stat='probability', kde=True, ax=ax, color='green', alpha=0.5, label='test')\n    ax.legend()","c6ab8866":"corr = train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(\n    data=corr,\n    mask=mask,\n    annot=True,\n    fmt='.2f',\n    linewidths=1,\n    square=True,\n    ax=ax\n)","b30b7bfb":"num_features = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n       'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\ntarget = 'target'\n\nX = train_df[num_features]\ny = train_df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)","c9a84a68":"predict_train = model.predict(X_train)\npredict_test = model.predict(X_test)\n\nprint(\n    f\"RMSE Train: {(mean_squared_error(y_train, predict_train))**(.5)}\\n\",\n    f\"RMSE Test: {(mean_squared_error(y_test, predict_test))**(.5)}\"\n)","9f42e553":"model = xgb.XGBRegressor()\nmodel.fit(X_train, y_train)","199bfbb5":"predict_train = model.predict(X_train)\npredict_test = model.predict(X_test)\n\nprint(\n    f\"RMSE Train: {(mean_squared_error(y_train, predict_train))**(.5)}\\n\",\n    f\"RMSE Test: {(mean_squared_error(y_test, predict_test))**(.5)}\"\n)","c3378337":"params = {\n    'objective': 'reg:squarederror',\n    'n_estimators': 1000,\n    'lambda': 7.610705234008646, \n    'alpha': 0.0019377246932580476, \n    'colsample_bytree': 0.5, \n    'subsample': 0.7, \n    'learning_rate': 0.012, \n    'max_depth': 20, \n    'random_state': 24, \n    'min_child_weight': 229,\n    'random_state':42\n}\n\n\nreg = XGBRegressor(**params)\n\nmodel = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reg', reg)\n])\n\nmodel.fit(X_train, y_train)","4d842d1c":"predict_train = model.predict(X_train)\npredict_test = model.predict(X_test)\n\nprint(\n    f\"RMSE Train: {(mean_squared_error(y_train, predict_train))**(.5)}\\n\",\n    f\"RMSE Test: {(mean_squared_error(y_test, predict_test))**(.5)}\"\n)","89973f62":"test_df['target'] = model.predict(test_df[num_features])\ntest_df.head()","c5ce6a91":"test_df[['target']].to_csv('.\/final_kaggle.csv')","4ae497d4":"import shap\nshap.initjs()\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.TreeExplainer(model['reg'])\nshap_values = explainer.shap_values(X)","5d50911d":"shap.summary_plot(shap_values, X)","34d5d230":"# Imports","116fd981":"# Output","6325699d":"# Model","5d73ed5b":"## XGBoost","151c2dc8":"## Simple Model (Linear Regression)","fe38262c":"# Shap","9bacab77":"# Load Data","1f3791dc":"# EDA"}}