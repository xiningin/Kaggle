{"cell_type":{"4baf60fd":"code","b9d5a3f0":"code","bfa0bfcc":"code","2c7a4766":"code","bc100850":"code","44b3f100":"code","8eb16c5d":"code","1dad45f4":"code","e7d9aa4f":"code","c1881017":"code","2dd3f74a":"code","75063fb9":"code","7de6e5ee":"code","9df523d6":"code","e5152a52":"code","6b22bad7":"code","21c6af0f":"code","f767b811":"code","a04fe17f":"code","47d3b904":"code","eda632c4":"code","5a3653ab":"code","0ea32222":"code","6ab08f15":"code","d045307c":"code","8bd669fb":"code","c9731c6c":"code","d76a175b":"code","c86f7e9a":"code","b69bdb49":"code","cc7b8577":"code","fbba8f24":"code","c6d2a988":"code","ee8fce91":"code","b680a354":"code","36bfd61d":"code","b4e28a08":"code","ef75a870":"code","214eb0fc":"code","bb91f411":"code","91b49575":"code","09cab79a":"code","4254df0b":"code","c11acce6":"code","87a8b29e":"code","26bd7d0e":"code","32f61e17":"code","45df4f3c":"code","17ed2f0c":"code","7ae7064a":"code","4fc6739d":"code","43c3580f":"code","b10ac5a4":"code","4f526809":"code","b15219ae":"code","750dcebc":"code","e6c6b3e0":"code","08b176cf":"code","7db51652":"code","d68bf3c3":"markdown","f1e10220":"markdown","bada06ab":"markdown","148c9654":"markdown","81fba045":"markdown","66c5b835":"markdown","a6d9035a":"markdown","7490614c":"markdown","57d4a70b":"markdown"},"source":{"4baf60fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport random\nimport string\nimport nltk\nfrom gensim import corpora, models\nfrom wordsegment import load, segment\nfrom nltk.tokenize.casual import TweetTokenizer\nfrom itertools import chain, groupby\nimport scikitplot as skplt\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nimport re\nfrom nltk import SnowballStemmer\nfrom nltk.corpus import words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9d5a3f0":"df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv') #reading the data","bfa0bfcc":"print(\"Columns in the data: \", list(df.columns))\nprint(\"Size of the data set: \", df.shape[0]) #df.shape = tuple(rows, columns)","2c7a4766":"print(\"Taking a gist of the first few rows:\\n--------------------------------------\\n\")\ndf.head() #give the first 5 rows in the data","bc100850":"#counts of the predicted variable 'sentiment'\nsentiment_counts = dict(df['sentiment'].value_counts()) #.value_counts() provides count of each unique value. dict() transforms\n#it into {unique_value: counts} form\n\n#making a barplot\nfig = px.bar(\n    x=list(sentiment_counts.keys()),\n    y=list(sentiment_counts.values()),\n)\nfig.show()\n\n#Neutral dominates. Positive and Negative are kinda equal in number. No harsh class imbalance though","44b3f100":"#check if we have any empty rows\nnull_counts = df.isna().sum()\nprint(\"Null values counts column-wise\\n------------------------------\\n\")\nprint(null_counts)\n#There is one.let's remove that one row","8eb16c5d":"df.dropna(inplace=True) #drops the row with empty columns. Inplace=True modifies df. if False, do df=df.dropna(inplace=False)","1dad45f4":"#Now check again if we have any empty rows. Just to make sure\nnull_counts = df.isna().sum()\nprint(\"Null values counts column-wise\\n------------------------------\\n\")\nprint(null_counts)\nprint(\"New number of rows: \", df.shape[0])","e7d9aa4f":"#also, we understand that the 'selected_text' column is a phrase from the original sentence. Let's just verify it\nfor i in range(df.shape[0]):\n    if i in df.index: #to avoid indexes of dropped row\n        assert df.loc[i, 'selected_text'] in df.loc[i, 'text'] #checks if selected column is verbatim taken from text column","c1881017":"#take a look at few sample sequences\n\ndef random_text(df, sentiment, n_samples):\n    sub_df = df[df['sentiment']==sentiment]\n    indexes = random.sample(list(sub_df.index), n_samples)\n    print(df['text'][indexes])","2dd3f74a":"#neutral sentiment samples\nrandom_text(df, 'neutral', 5)","75063fb9":"#negative sentiment samples\nrandom_text(df, 'negative', 5)","7de6e5ee":"#positive sentiment samples\nrandom_text(df, 'positive', 5)","9df523d6":"def filter_df(df, column, condition):\n    '''\n    returns list of values of a column based on condition\n    example - df[df['some_column']==some_value]['column']\n    \n    params:\n        - df: dataframe\n        - column: column name in df (str)\n        - condition: series of Boolean values\n        \n    returns:\n        - list\n    \n    '''\n    return list(df[condition][column])\n\ndef decontracted(phrase):\n    '''\n    Preprocessing step to replace 'won't' with 'will not' and such\n    \n    params:\n        - phrase: a string\n        \n    returns:\n        - transformed phrase (str)\n    \n    '''\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ndef preprocess(sentence, tokenizer, lemmatizer):\n    '''\n    Perform elementary preprocessing steps\n    \n    params:\n        - sentence: string\n        - tokenizer: a callable that takes a string and returns a list of tokens\n        - lemmatizer: a callable that takes a word (string) and returns lemmatized word\n        \n    returns:\n        -tokens: list of strings\n    '''\n    stopwords_en = stopwords.words('english')\n    stopwords_en.remove('not')\n    stopwords_en.remove('nor')\n\n    sentence = sentence.lower()\n    sentence = re.sub(r'http\\S+',\" \", sentence)\n    sentence = re.sub(r'www\\S+',\" \", sentence)\n    sentence = decontracted(sentence)\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n    tokens = tokenizer(sentence)\n    tokens = [token for token in tokens if token not in stopwords_en]\n    return tokens\n\n\ndef generate_vocab(text_list, tokenizer, lemmatizer):\n    '''\n    Generates vocabulary based on a list of sentences\n    \n    params:\n        - text_list: list of strings\n        - tokenizer: a callable that takes a string and returns a list of tokens\n        - lemmatizer: a callable that takes a word (string) and returns lemmatized word\n    \n    returns:\n        vocab_counts: a dictionary with keys as words in the vocabulary and counts as number of times they occur in the text_list\n    \n    '''\n    tokens_list = map(lambda x: preprocess(x, tokenizer, lemmatizer), text_list)\n    vocab_counts = Counter(chain.from_iterable(tokens_list))\n    return vocab_counts\n","e5152a52":"#example\ntokenizer = TweetTokenizer().tokenize #a callable\nlemmatizer = WordNetLemmatizer().lemmatize # a callable\n\nsentence = \"I am telling you I really can't do that anymore!!\"\nprint(preprocess(sentence, tokenizer, lemmatizer))","6b22bad7":"positive_text = filter_df(df, 'text', df['sentiment'] == 'positive')\npostive_vocab_counts = generate_vocab(positive_text, tokenizer, lemmatizer)#vocab for positive sentiment labelled text\n\nnegative_text = filter_df(df, 'text', df['sentiment'] == 'negative')\nnegative_vocab_counts = generate_vocab(negative_text, tokenizer, lemmatizer) #vocab for negative sentiment labelled text\n\nneutral_text = filter_df(df, 'text', df['sentiment'] == 'neutral')\nneutral_vocab_counts = generate_vocab(neutral_text, tokenizer, lemmatizer)#vocab for neutral sentiment labelled text","21c6af0f":"def top_n(count_dict, n=20):\n    '''\n    Takes a count dictionary and returns n top words (keys) by count\n    \n    params:\n        -count_dict: dict with str keys and int values\n        \n    returns:\n        -None\n    \n    '''\n    count_dict = sorted(count_dict.items(), key=lambda x:x[1], reverse=True)\n    print(\"Word -----> Counts\\n------------------------\\n\")\n    for i in range(n):\n        print(count_dict[i][0], \"----->\", count_dict[i][1])","f767b811":"top_n(postive_vocab_counts)","a04fe17f":"top_n(neutral_vocab_counts)","47d3b904":"top_n(negative_vocab_counts)","eda632c4":"#Is there any distinction among sentiment text with respect to number of words\nsub_df = df[['text', 'sentiment']]\nsub_df['lengths'] = sub_df['text'].apply(lambda x: len(tokenizer(x)))\n\nfig = px.histogram(\n        sub_df, \n        x=\"lengths\", \n        color=\"sentiment\",\n        opacity=0.8,\n        marginal=\"box\",\n        barmode=\"overlay\"\n      )\n\nfig.show()\n    \n# distribution of sentences for each sentiment. Not anything useful here","5a3653ab":"#how many words in our data are actual english words (present in dictionary)?\nword_set = set([lemmatizer(word) for word in words.words()])\ndef is_english(word_set, word):\n    '''\n    Checks if a word is in word_set\n    \n    params:\n        -word_set - set() of strings\n        -word -string\n        \n    returns:\n        -bool\n    '''\n    return lemmatizer(word) in word_set\n\npostive_en_vocab = [is_english(word_set, word.lower()) for word in postive_vocab_counts.keys()]\nprint(\"Proportion of non-english words in positive vocab: \", round(sum(postive_en_vocab)\/len(postive_en_vocab),2))\n\nnegative_en_vocab = [is_english(word_set, word.lower()) for word in negative_vocab_counts.keys()]\nprint(\"Proportion of non-english words in negative vocab: \", round(sum(negative_en_vocab)\/len(negative_en_vocab),2))\n\nneutral_en_vocab = [is_english(word_set, word.lower()) for word in neutral_vocab_counts.keys()]\nprint(\"Proportion of non-english words in neutral vocab: \", round(sum(neutral_en_vocab)\/len(neutral_en_vocab),2))","0ea32222":"#Perform topic modelling to see what the data is talking about\ntokens = list(map(lambda x: preprocess(x, tokenizer, lemmatizer), list(df['text'])))","6ab08f15":"dictionary_LDA = corpora.Dictionary(tokens)\ndictionary_LDA.filter_extremes(no_below=3, no_above=1)\nprint(\"Size of dictionary:\", len(dictionary_LDA.itervalues()))","d045307c":"NUM_TOPICS = 10\ncorpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in tokens]\nlda_model = models.LdaModel(\n                    corpus, \n                    num_topics=NUM_TOPICS,\n                    id2word=dictionary_LDA,\n                    passes=4, alpha='auto',\n                    eta='auto'\n                )","8bd669fb":"#Looking at topic distribution for positive text\nscore_dict = {i:0 for i in range(NUM_TOPICS)}\n\nindexes = list(df[df['sentiment'] == 'positive'].index)\nsub_corpus = [corpus[i] for i in indexes if i<len(corpus)]\nfor seq in sub_corpus:\n    scores = lda_model[seq]\n    for topic, prob in scores:\n        score_dict[topic] += prob\/len(indexes)\n        \nfig = px.bar(\n    x=list(score_dict.keys()),\n    y=list(score_dict.values()),\n)\nfig.show()\n    ","c9731c6c":"#Looking at topic distribution for negative text\nscore_dict = {i:0 for i in range(NUM_TOPICS)}\n\nindexes = list(df[df['sentiment'] == 'negative'].index)\nsub_corpus = [corpus[i] for i in indexes if i<len(corpus)]\nfor seq in sub_corpus:\n    scores = lda_model[seq]\n    for topic, prob in scores:\n        score_dict[topic] += prob\/len(indexes)\n        \nfig = px.bar(\n    x=list(score_dict.keys()),\n    y=list(score_dict.values()),\n)\nfig.show()\n    \n","d76a175b":"#Looking at topic distribution for neutral text\nscore_dict = {i:0 for i in range(NUM_TOPICS)}\n\nindexes = list(df[df['sentiment'] == 'neutral'].index)\nsub_corpus = [corpus[i] for i in indexes if i<len(corpus)]\nfor seq in sub_corpus:\n    scores = lda_model[seq]\n    for topic, prob in scores:\n        score_dict[topic] += prob\/len(indexes)\n        \nfig = px.bar(\n    x=list(score_dict.keys()),\n    y=list(score_dict.values()),\n)\nfig.show()\n    ","c86f7e9a":"lda_model.print_topics(num_words=10)","b69bdb49":"print(\"Model log perplexity is: \", lda_model.log_perplexity(corpus))","cc7b8577":"#How does my selection length vary with my text length\ntags_text = list(map(lambda x: nltk.pos_tag(tokenizer(x)), list(df['text'])))\ntags_selection = list(map(lambda x: nltk.pos_tag(tokenizer(x)), list(df['selected_text'])))","fbba8f24":"lengths_text = [len(seq) for seq in tags_text]\nlengths_selection = [len(seq) for seq in tags_selection]","c6d2a988":"fig = px.scatter(x=lengths_text, y=lengths_selection)\nfig.show()","ee8fce91":"#I'm curious about examples where the whole text is the selected text\ncount=0\nindex = []\nfor i, (text, selection) in enumerate(zip(tags_text, tags_selection)):\n    if i>100:\n        break\n    if len(text) == len(selection):\n        index.append(i)\n        count+=1\n        print(len(text))\n        print(\" \".join([word for word,_ in text]))\n        print(\" \".join([word for word,_ in text]))\n        print(\"------------------------------------------\\n\")\n        ","b680a354":"print(\"Proportion of such pairs is: \", round(count\/len(tags_text),2))","36bfd61d":"sentiment_dict = dict(Counter(df.iloc[index,:]['sentiment']))\n\nfig = px.bar(\n    x=list(sentiment_dict.keys()),\n    y=list(sentiment_dict.values()),\n)\nfig.show()\n\n\n#Around 10k of the 11k neutral text have the same text and selected text.\n# Majority positive and negative data have nothing of that sort. Only 1\/4th data have exactly same text and selected text","b4e28a08":"pos_tags_sel = []\npos_tags_text = []\nfor i, (text, selection) in enumerate(zip(tags_text, tags_selection)):\n    pos_tags_sel.extend([tag[0] for _, tag in selection if tag[0].isalnum()])\n    pos_tags_text.extend([tag[0] for _, tag in text if tag[0].isalnum()])","ef75a870":"#refer https:\/\/www.learntek.org\/blog\/categorizing-pos-tagging-nltk-python\/ for POS tags and their meanings\npos_tag_dict = dict(Counter(pos_tags_text))\n\nfig = px.bar(\n    x=list(pos_tag_dict.keys()),\n    y=list(pos_tag_dict.values()),\n)\nfig.show()\n\n#","214eb0fc":"pos_select_dict = dict(Counter(pos_tags_sel))\n\npos_prop = {}\n\nfor key in pos_select_dict:\n    pos_prop[key] = pos_select_dict[key]\/pos_tag_dict[key]\n    \n\nfig = px.bar(\n    x=list(pos_prop.keys()),\n    y=list(pos_prop.values()),\n)\nfig.show()","bb91f411":"#start from scratch\n\ndf = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\n\n\ndf.dropna(inplace=True)\ntext = list(df['text'])\n","91b49575":"#same as functions above\ndef filter_df(df, column, condition):\n    return list(df[condition][column])\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ndef remove_trails(word):\n    '''\n    Process words like wowwwwwwww to wow\n    '''\n    groups = groupby(word)\n    result = [(label, sum(1 for _ in group)) for label, group in groups]\n    new_string = \"\"\n    for char, count in result:\n        if count<3:\n            new_string+=char*count\n        else:\n            new_string+=char\n    return new_string\n\n\ndef preprocess(sentence, tokenizer, lemmatizer, stemmer):\n    stopwords_en = stopwords.words('english')\n    stopwords_en.remove('not')\n    stopwords_en.remove('nor')\n    sentence = sentence.lower()\n    sentence = re.sub(r'http\\S+',\" \", sentence)\n    sentence = re.sub(r'www\\S+',\" \", sentence)\n    sentence = decontracted(sentence)\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n    tokens = tokenizer(sentence)\n    tokens = [remove_trails(stemmer(lemmatizer(token))) for token in tokens if token not in stopwords_en]\n    \n    return tokens\n\n\ntokenizer = TweetTokenizer().tokenize #a callable\nlemmatizer = WordNetLemmatizer().lemmatize # a callable\nstemmer = SnowballStemmer('english').stem","09cab79a":"tokens = list(map(lambda x: preprocess(x, tokenizer, lemmatizer, stemmer), text))","4254df0b":"vocabulary_counts = Counter(chain.from_iterable(tokens))\n","c11acce6":"count_lt3 = [key for key,_ in vocabulary_counts.items() if _<3]\nprint(\"Proportion of words with count < 3:\", round(len(count_lt3)\/len(vocabulary_counts),3))\nprint(\"------------------------------------------\")\nprint(count_lt3[:100]) #remove [:100] to view all\n\n#too many words with trailing sequence of letters like eeeeeee or oooooo","87a8b29e":"count_gt3 = [key for key,_ in vocabulary_counts.items() if _>=3]\nprint(\"Proportion of words with count > 3:\", round(len(count_gt3)\/len(vocabulary_counts),3))\nprint(\"------------------------------------------\")\nprint(count_gt3[:100]) #remove [:100] to view all\n","26bd7d0e":"#total counts by rare words and common words\ntotal = sum(vocabulary_counts.values())\n\nlt3 = sum(count for _, count in vocabulary_counts.items() if count<3)\ngt3 = sum(count for _, count in vocabulary_counts.items() if count>=3)\n\nprint(\"Total counts for --\")\nprint(\"Less than 3:\", lt3)\nprint(\"Greater than 3:\", gt3)\nprint(\"Total: \", total)","32f61e17":"vocabulary_counts = Counter(chain.from_iterable(tokens))\ncount_lt3 = [key for key,_ in vocabulary_counts.items() if _<3]\nprint(\"Proportion of words with count < 3:\", round(len(count_gt3)\/len(vocabulary_counts),3))\nprint(\"------------------------------------------\")\nprint(count_lt3[:100]) #remove [:100] to view all\n\n","45df4f3c":"MAX_FEATURES=int(0.27*len(vocabulary_counts)) #since 27% have counts greater than 3. Leaving rest of the vocab loses like 10% of total occurences\n\nvectorizer = TfidfVectorizer(\n        max_features=MAX_FEATURES\n    )\n\n\nsentences = [\" \".join(token_list) for token_list in tokens]\ntransformed = vectorizer.fit_transform(sentences)\nX = transformed.toarray()","17ed2f0c":"print(\"X shape: \", X.shape)\n# number of features ~= number of data point. Not very good","7ae7064a":"y = list(df['sentiment']) #encoding predicted valriable\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)","4fc6739d":"#using pd.DataFrame to get indexes of data in train and test. Helps lookup the text column for test examples from the original df\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(pd.DataFrame(X), pd.DataFrame(y),test_size=0.2, shuffle=True)\n\nX_train = X_train_df.to_numpy()\ny_train = y_train_df.to_numpy()\nX_test = X_test_df.to_numpy()\ny_test = y_test_df.to_numpy()","43c3580f":"model = LogisticRegression(max_iter=2000, solver='saga', penalty='l1', C=1)\nmodel.fit(X_train,y_train)","b10ac5a4":"y_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n#test results","4f526809":"y_pred = model.predict(X_train)\nprint(classification_report(y_train, y_pred))\n#train predict","b15219ae":"import matplotlib.pyplot as plt\n\ny_probas = model.predict_proba(X_train)# predicted probabilities generated by sklearn classifier\nskplt.metrics.plot_roc_curve(y_train, y_probas)\nplt.show()\n\n#negative --> 0\n#neutral --> 1\n#positive --> 2","750dcebc":"#Let's look at some missclassified cases\n\ny_pred = model.predict(X_test)\nincorrect = np.where(y_pred!=y_test.ravel())[0]","e6c6b3e0":"plot_confusion_matrix(model, X_test, y_test, display_labels=list(encoder.classes_))\n#most of the missclassifications are between negative and neutral","08b176cf":"#printing missclassified examples with their actual and predicted label\nPRINT = 30\ncount=0\nfor i in incorrect:\n    if count>PRINT:\n        break\n    count+=1\n    index = int(X_test_df.index[i])\n    print(\"-------------------------------------------\")\n    print(text[index], \"----> Actual:\", encoder.classes_[int(y_test[i])], \"----> Predicted:\", encoder.classes_[int(y_pred[i])])","7db51652":"\n#Fin.","d68bf3c3":"## EDA Notes\nWe have a good idea about our data. Let's summarize\n\n1. The sentiments are not associated to any topics. One cannot guess the sentiment based on the informtation about the topic it talks about\n2. The selected text for neutral sentiment text is the whole text itself. Just by classifying a text as neutral, we can score well on the jaccard similarity between the text and selected text\n\n## Next Steps\n\n1. Create a baseline for classification task","f1e10220":"## 3.0 Modelling","bada06ab":"## What do we know?\n\n1. All the three sentiments have similar distribution for lengths\n2. It's hard to differentiate topics based on sentiments. All sentiments discuss similar topics\n3. Around 40-50% of the vocabulary is not even in English. Possibly slang words\n\n\n## Next Steps\n\n1. Analyze selection and text together","148c9654":"### Question: What words occur more commonly in each sentiment than the rest?","81fba045":"## 1.2 What do we know so far?\n1. Our data has three classes - Neutral, Positive and Negative.\n2. Neutral is the majority, although the imbalance is not too harsh\n3. There was just one empty row, which is removed\n4. The selected text is part of the main text body\n<br>\n\n## 1.3 Next steps?\n1. Analyze the text vocabulary and sentences for all the three classes\n2. Analyze the selection and the main text. How these two are related apart from being subset of the other","66c5b835":"## 1.1 Super high-level summary","a6d9035a":"# Notebook Contents and Expectations\n\nThe notebook is aimed at understanding EDA, pre-processing, creating a baseline for sentiment classification. We'll understand where did the model go wrong and why. This is for people who know how to do fit() and predict(), but want to move beyond that.\n<br><br>\nPlease follow the comments for code explanations and insights explanation\n\nIf you want to skip EDA, please jumpy to section 3.0-Modelling","7490614c":"# 1.0 Reading the Data","57d4a70b":"# 2.0 EDA\n\n## 2.1 text column"}}