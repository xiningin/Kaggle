{"cell_type":{"05745115":"code","0468669a":"code","e66338c1":"code","2a1a9996":"code","1c5cf799":"code","236b68ad":"code","a8edc360":"code","8edd704e":"code","ea9b5704":"code","436db90b":"code","07bb7db6":"code","1b8a0566":"code","95455291":"code","27d4fbc7":"code","6512e400":"code","255165d7":"code","db9ae39d":"code","479354f4":"code","7273fc4e":"code","4ecea915":"code","a1a492c3":"code","55e56489":"code","69b6e5a0":"code","2f8621c5":"code","e5d75cf4":"code","6f7c9628":"code","b866f14c":"code","8f2e490d":"code","9ed973f8":"code","9609d762":"code","32317dd3":"markdown","5bab6316":"markdown","640ecb1d":"markdown","dc309fc9":"markdown","3cc7ee23":"markdown","f37c8838":"markdown","99555563":"markdown","f425e819":"markdown","d89d59d2":"markdown","297eecd0":"markdown","1dfaa3c1":"markdown","316fda6c":"markdown","d2ac6497":"markdown","f0b23700":"markdown","69a290e4":"markdown","4a90bb1a":"markdown","4425822c":"markdown","cbfd5644":"markdown","901800e1":"markdown","d55b3404":"markdown","b0cd9e97":"markdown","45bb1327":"markdown","0622aed8":"markdown","79f53150":"markdown","7193ff9b":"markdown","bd4c38ef":"markdown","922511fb":"markdown","55d77f78":"markdown","4d8cbe00":"markdown","9c71b39e":"markdown","f94f6ee4":"markdown","bab3b4b7":"markdown","7e093500":"markdown","604e0ebe":"markdown","8ba2e108":"markdown","4db5c9fa":"markdown","5e33384f":"markdown","664403a9":"markdown","04dcbc12":"markdown","1258c298":"markdown","a3b11878":"markdown","df30b6d7":"markdown","d4378b92":"markdown","c1552d1e":"markdown","7b4da5d6":"markdown","91ce1fc3":"markdown","f9c57bbc":"markdown","b1f84f4c":"markdown","b5c7905d":"markdown","b8663b26":"markdown","41dd79c3":"markdown","3b61dd9c":"markdown","7871cb46":"markdown","a223d9dd":"markdown","a15d89f5":"markdown","97eadfc6":"markdown","fbdbb4d1":"markdown","ac4bc351":"markdown","6352b64d":"markdown","02b88675":"markdown","16fb8523":"markdown","dfa1e6a4":"markdown","debe208a":"markdown"},"source":{"05745115":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport math\nimport warnings\nfrom matplotlib.lines import Line2D\nfrom bokeh.layouts import row, column\nfrom bokeh.transform import cumsum, transform\nfrom bokeh.transform import dodge, factor_cmap\nfrom bokeh.plotting import figure, show, gridplot\nfrom bokeh.io import output_notebook\nfrom bokeh.core.properties import value\nfrom bokeh.palettes import d3, brewer, plasma, Plasma256\nfrom bokeh.models import LabelSet, ColumnDataSource, LinearColorMapper, ColorBar, BasicTicker, FactorRange\n\nwarnings.filterwarnings('ignore')","0468669a":"output_notebook()","e66338c1":"multiplechoice_beg = pd.read_csv('..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv', low_memory=False, encoding='ISO-8859-1')\nmultiplechoice_old = pd.read_csv('..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv', low_memory= False)\nmultiplechoice_new = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv', low_memory= False)\n\nmultiplechoice_old.columns = [x.split('_')[0] for x in list(multiplechoice_old.columns)]\nmultiplechoice_new.columns = [x.split('_')[0] for x in list(multiplechoice_new.columns)]\n\nmultiplechoice_old.columns = multiplechoice_old.columns + '_' + multiplechoice_old.iloc[0]\nmultiplechoice_new.columns = multiplechoice_new.columns + '_' + multiplechoice_new.iloc[0]\nmultiplechoice_old = multiplechoice_old.drop([0])\nmultiplechoice_new = multiplechoice_new.drop([0])\n\nmultiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'].astype('float')\nmultiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'].astype('float')\n\nmultiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'].apply(lambda x:x\/3600)\nmultiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'].apply(lambda x:x\/3600)\n\ntime_old = str(round(multiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'].median()*60, 1)) + ' min'\ntime_new = str(round(multiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'].median()*60, 2)) + ' min'\n\nTOOLS=\"pan,wheel_zoom,zoom_in,zoom_out,undo,redo,reset,tap,save\"","2a1a9996":"survey_data = pd.DataFrame({'Number of Respondents' : [len(multiplechoice_beg), len(multiplechoice_old), len(multiplechoice_new)],\n                            'Number of Questions' :  ['64', '50', '34'],\n                            'Median Response Time' : ['16.4 min', time_old, time_new]},\n                            index = ['2017', '2018', '2019'])\nsurvey_data","1c5cf799":"p0 = figure(x_range = survey_data.index.values, y_range = (0,26000), plot_width = 400, plot_height = 500, tools = TOOLS, title = \"Number Of Respondents in Survey\")\nsource0 = ColumnDataSource(dict(x=survey_data.index.values, y=survey_data.iloc[:,0].values.reshape(len(survey_data))))\nlabels0 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-22, y_offset=0, source=source0, render_mode='canvas')\n\np0.vbar(survey_data.index.values, width = 0.9, top = survey_data.iloc[:,0].values.reshape(len(survey_data)), color='mediumseagreen')\np0.xaxis.axis_label = 'Year'\np0.yaxis.axis_label = 'Number of Respondents'\np0.yaxis.axis_label_text_font = 'times'\np0.yaxis.axis_label_text_font_size = '12pt'\np0.xaxis.axis_label_text_font = 'times'\np0.xaxis.axis_label_text_font_size = '12pt'\np0.ygrid.grid_line_color = None\np0.xgrid.grid_line_color = None\np0.add_layout(labels0)\n\nyears = ['2017', '2018', '2019']\ncol_name = ['No of Questions', 'Median Response Time']\n\nplot_data = [(year, analysis_type) for year in years for analysis_type in col_name]\nsurvey_data['Median Response Time'] = survey_data['Median Response Time'].apply(lambda x:x.replace(' min', ''))\ncounts = list(survey_data.iloc[0,1:].values) + list(survey_data.iloc[1,1:].values) + list(survey_data.iloc[2,1:].values)\n\nsource = ColumnDataSource(data=dict(x=plot_data, counts=counts))\nsource1 = ColumnDataSource(dict(x=survey_data.index.values, y=survey_data.iloc[:,1].values.reshape(len(survey_data))))\nsource2 = ColumnDataSource(dict(x=survey_data.index.values, y=survey_data.iloc[:,2].values.reshape(len(survey_data))))\n\nlabels1 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-30, y_offset=0, source=source1, render_mode='canvas')\nlabels2 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=5, y_offset=0, source=source2, render_mode='canvas')\n\np1 = figure(x_range = FactorRange(*plot_data), y_range = (0,70), plot_width = 400, plot_height = 500, tools = TOOLS, title = \"Number Of Questions v\/s Mean Response time in Survey\")\np1.vbar(x='x', top='counts', width=0.9, source=source, fill_color=factor_cmap('x', palette=['mediumslateblue', 'burlywood'], factors=col_name, start=1, end=2))\np1.xaxis.axis_label = 'Year'\np1.yaxis.axis_label_text_font = 'times'\np1.yaxis.axis_label_text_font_size = '12pt'\np1.xaxis.axis_label_text_font = 'times'\np1.xaxis.axis_label_text_font_size = '12pt'\np1.ygrid.grid_line_color = None\np1.xgrid.grid_line_color = None\np1.xaxis.major_label_orientation = math.pi\/2\np1.add_layout(labels1)\np1.add_layout(labels2)\n\nshow(row(p0,p1))","236b68ad":"sns.set_style('darkgrid')\nplt.figure(figsize= (16,16))\n\n### Histogram plot\nplt.subplot(421)\nplt.hist(multiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'], bins = 50, color= 'indianred')\nplt.yscale('log')\n# plt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Number of Respondents', fontsize = 'large')\nplt.title('2018', fontsize = 'x-large', fontweight = 'roman')\n\n### Density plot\nplt.subplot(423)\nax = sns.kdeplot(multiplechoice_old['Time from Start to Finish (seconds)_Duration (in seconds)'], color= 'indianred')\nax.legend_.remove()\nplt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Density', fontsize = 'large')\n# plt.title('2018', fontsize = 'x-large', fontweight = 'roman')\n\n### Histogram plot\nplt.subplot(422)\nplt.hist(multiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'], bins = 50, color= 'darkslateblue')\nplt.yscale('log')\n# plt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Number of Respondents', fontsize = 'large')\nplt.title('2019', fontsize = 'x-large', fontweight = 'roman')\n\n### Density plot\nplt.subplot(424)\nax = sns.kdeplot(multiplechoice_new['Time from Start to Finish (seconds)_Duration (in seconds)'], color= 'darkslateblue')\nax.legend_.remove()\nplt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Density', fontsize = 'large')\n# plt.title('2019', fontsize = 'x-large', fontweight = 'roman')","a8edc360":"gender_beg = multiplechoice_beg['GenderSelect'].value_counts().to_frame()\ngender_old = multiplechoice_old['Q1_What is your gender? - Selected Choice'].value_counts().to_frame()\ngender_new = multiplechoice_new['Q2_What is your gender? - Selected Choice'].value_counts().to_frame()\ngender_beg.index = gender_old.index.values\n\ngender_beg = round(gender_beg\/gender_beg.sum(), 2)*100\ngender_old = round(gender_old\/gender_old.sum(), 2)*100\ngender_new = round(gender_new\/gender_new.sum(), 2)*100","8edd704e":"p0 = figure(x_range = gender_beg.index.values, y_range = (0,90), plot_width = 265, plot_height = 400, tools = TOOLS, )\nsource0 = ColumnDataSource(dict(x=gender_beg.index.values, y=gender_beg.values.reshape(len(gender_beg))))\nlabels0 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-10, y_offset=0, source=source0, render_mode='canvas')\n\np0.vbar(gender_beg.index.values, width = 0.5, top = gender_beg.values.reshape(len(gender_beg)), color=d3['Category20b'][len(gender_beg)])\np0.xaxis.axis_label = 'Gender'\np0.yaxis.axis_label = 'Percentage of Respondents -2017'\np0.yaxis.axis_label_text_font = 'times'\np0.yaxis.axis_label_text_font_size = '12pt'\np0.xaxis.axis_label_text_font = 'times'\np0.xaxis.axis_label_text_font_size = '12pt'\np0.ygrid.grid_line_color = None\np0.xgrid.grid_line_color = None\np0.xaxis.major_label_orientation = math.pi\/4\np0.add_layout(labels0)\n\np1 = figure(x_range = gender_old.index.values, y_range = (0,90), plot_width = 265, plot_height = 400, tools = TOOLS, )\nsource1 = ColumnDataSource(dict(x=gender_old.index.values, y=gender_old.values.reshape(len(gender_old))))\nlabels1 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-10, y_offset=0, source=source1, render_mode='canvas')\n\np1.vbar(gender_old.index.values, width = 0.5, top = gender_old.values.reshape(len(gender_old)), color=d3['Category20b'][len(gender_old)])\np1.xaxis.axis_label = 'Gender'\np1.yaxis.axis_label = 'Percentage of Respondents -2018'\np1.yaxis.axis_label_text_font = 'times'\np1.yaxis.axis_label_text_font_size = '12pt'\np1.xaxis.axis_label_text_font = 'times'\np1.xaxis.axis_label_text_font_size = '12pt'\np1.ygrid.grid_line_color = None\np1.xgrid.grid_line_color = None\np1.xaxis.major_label_orientation = math.pi\/4\np1.add_layout(labels1)\n\np2 = figure(x_range = gender_new.index.values, y_range = (0,90), plot_width = 265, plot_height = 400, tools = TOOLS, )\nsource2 = ColumnDataSource(dict(x=gender_new.index.values, y=gender_new.values.reshape(len(gender_new))))\nlabels2 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-10, y_offset=0, source=source2, render_mode='canvas')\n\np2.vbar(gender_new.index.values, width = 0.5, top = gender_new.values.reshape(len(gender_new)), color=d3['Category20b'][len(gender_new)])\np2.xaxis.axis_label = 'Gender'\np2.yaxis.axis_label = 'Percentage of Respondents -2019'\np2.yaxis.axis_label_text_font = 'times'\np2.yaxis.axis_label_text_font_size = '12pt'\np2.xaxis.axis_label_text_font = 'times'\np2.xaxis.axis_label_text_font_size = '12pt'\np2.ygrid.grid_line_color = None\np2.xgrid.grid_line_color = None\np2.xaxis.major_label_orientation = math.pi\/4\np2.add_layout(labels2)\n\nshow(row(p0,p1,p2))","ea9b5704":"country = ['France', 'Canada','UK', 'Germany', 'Brazil', 'Russia', 'China', 'India', 'USA', 'Japan']\n\nmultiplechoice_beg['Country'] = multiplechoice_beg['Country'].replace(\"People 's Republic of China\", 'China').replace('United Kingdom', 'UK').replace('United States', 'USA')\nmultiplechoice_old['Q3_In which country do you currently reside?'] = multiplechoice_old['Q3_In which country do you currently reside?'].replace('United Kingdom of Great Britain and Northern Ireland', 'UK').replace('United States of America', 'USA')\nmultiplechoice_new['Q3_In which country do you currently reside?'] = multiplechoice_new['Q3_In which country do you currently reside?'].replace('United Kingdom of Great Britain and Northern Ireland', 'UK').replace('United States of America', 'USA')","436db90b":"top_countries_beg = multiplechoice_beg['Country'].value_counts().to_frame().loc[country].sort_values('Country')\ntop_countries_old = multiplechoice_old['Q3_In which country do you currently reside?'].value_counts().to_frame().loc[country].sort_values('Q3_In which country do you currently reside?')\ntop_countries_new = multiplechoice_new['Q3_In which country do you currently reside?'].value_counts().to_frame().loc[country].sort_values('Q3_In which country do you currently reside?')\n\ntop_countries_beg = round(top_countries_beg\/top_countries_beg.sum(), 2)*100\ntop_countries_old = round(top_countries_old\/top_countries_old.sum(), 2)*100\ntop_countries_new = round(top_countries_new\/top_countries_new.sum(), 2)*100\n\ntop_countries_old = top_countries_old.reindex(list(top_countries_beg.index))\ntop_countries_new = top_countries_new.reindex(list(top_countries_beg.index))","07bb7db6":"country_list = list(top_countries_beg.index)\nbeg = list(top_countries_beg['Country'])\nold = list(top_countries_old['Q3_In which country do you currently reside?'])\nnew = list(top_countries_new['Q3_In which country do you currently reside?'])\n\ndot = figure(title=\"Participants by Country\", tools=TOOLS, plot_width = 800, plot_height = 400, y_range=country_list, x_range=[0,42])\n\ndot.segment(0, country_list, beg, country_list, line_width=2, line_color=\"sienna\", legend='2017')\ndot.circle(beg, country_list, size=15, fill_color=\"plum\", line_color=\"sienna\", line_width=1, legend='2017')\ndot.segment(0, country_list, old, country_list, line_width=2, line_color=\"sienna\", legend='2018')\ndot.circle(old, country_list, size=15, fill_color=\"skyblue\", line_color=\"sienna\", line_width=1, legend='2018')\ndot.segment(0, country_list, new, country_list, line_width=2, line_color=\"sienna\", legend='2019')\ndot.circle(new, country_list, size=15, fill_color=\"yellowgreen\", line_color=\"sienna\", line_width=1, legend='2019')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Country'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","1b8a0566":"age_beg_dict = {\n'18-21' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 18) & (multiplechoice_beg['Age'] < 21)]['Age']),\n'22-24' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 21) & (multiplechoice_beg['Age'] < 25)]['Age']),\n'25-29' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 24) & (multiplechoice_beg['Age'] < 30)]['Age']),\n'30-34' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 29) & (multiplechoice_beg['Age'] < 35)]['Age']),\n'35-39' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 34) & (multiplechoice_beg['Age'] < 40)]['Age']),\n'40-44' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 39) & (multiplechoice_beg['Age'] < 45)]['Age']),\n'45-49' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 44) & (multiplechoice_beg['Age'] < 50)]['Age']),\n'50-54' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 49) & (multiplechoice_beg['Age'] < 55)]['Age']),\n'55-59' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 54) & (multiplechoice_beg['Age'] < 60)]['Age']),\n'60-69' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 59) & (multiplechoice_beg['Age'] < 70)]['Age']),\n'70+' : len(multiplechoice_beg[(multiplechoice_beg['Age'] > 70)])}","95455291":"ylab_old = multiplechoice_old['Q2_What is your age (# years)?'].sort_values().unique()\nylab_new = multiplechoice_new['Q1_What is your age (# years)?'].sort_values().unique()\n\nage_df_beg = pd.DataFrame(age_beg_dict, index = range(12)).T[0]\nage_df_old = multiplechoice_old['Q2_What is your age (# years)?'].value_counts().to_frame().loc[ylab_old]\nage_df_new = multiplechoice_new['Q1_What is your age (# years)?'].value_counts().to_frame().loc[ylab_new]\n\nage_df_old_last_row = age_df_old.loc['70-79'] + age_df_old.loc['80+']\nage_df_old = age_df_old.drop(['70-79','80+'])\nage_df_old = age_df_old.append(pd.DataFrame([age_df_old_last_row], columns=['Q2_What is your age (# years)?'], index=['70+']))\n\nage_df_beg = round(age_df_beg\/age_df_beg.sum(), 2)*100\nage_df_old = round(age_df_old\/age_df_old.sum(), 2)*100\nage_df_new = round(age_df_new\/age_df_new.sum(), 2)*100","27d4fbc7":"age_list = list(age_df_beg.index)[::-1]\nbeg = list(age_df_beg)[::-1]\nold = list(age_df_old['Q2_What is your age (# years)?'])[::-1]\nnew = list(age_df_new['Q1_What is your age (# years)?'])[::-1]\n\ndot = figure(title=\"Age Group of Respondents\", tools=TOOLS, plot_width = 800, plot_height = 400, y_range=age_list, x_range=[0,30])\n\ndot.segment(0, age_list, beg, age_list, line_width=2, line_color=\"sienna\", legend='2017')\ndot.circle(beg, age_list, size=15, fill_color=\"plum\", line_color=\"sienna\", line_width=1, legend='2017')\ndot.segment(0, age_list, old, age_list, line_width=2, line_color=\"sienna\", legend='2018')\ndot.circle(old, age_list, size=15, fill_color=\"skyblue\", line_color=\"sienna\", line_width=1, legend='2018')\ndot.segment(0, age_list, new, age_list, line_width=2, line_color=\"sienna\", legend='2019')\ndot.circle(new, age_list, size=15, fill_color=\"yellowgreen\", line_color=\"sienna\", line_width=1, legend='2019')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Age Group'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","6512e400":"educ_lvl_beg = multiplechoice_beg['FormalEducation'].value_counts().to_frame()\neduc_lvl_old = multiplechoice_old['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'].value_counts().to_frame()\neduc_lvl_new = multiplechoice_new['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'].value_counts().to_frame()\neduc_lvl_beg.index = ['Master\u2019s degree', 'Bachelor\u2019s degree', 'Doctoral degree','Some college\/university study without earning a bachelor\u2019s degree',\n                      'Professional degree', 'No formal education past high school', 'I prefer not to answer']\n\neduc_lvl_beg = educ_lvl_beg.drop('I prefer not to answer')\neduc_lvl_old = educ_lvl_old.drop('I prefer not to answer')\neduc_lvl_new = educ_lvl_new.drop('I prefer not to answer')\n\neduc_lvl_beg = round(educ_lvl_beg\/educ_lvl_beg.sum(), 2)*100\neduc_lvl_old = round(educ_lvl_old\/educ_lvl_old.sum(), 2)*100\neduc_lvl_new = round(educ_lvl_new\/educ_lvl_new.sum(), 2)*100\n\neduc_lvl_beg = educ_lvl_beg.reindex(list(educ_lvl_old.index))","255165d7":"educ_list = list(educ_lvl_beg.index)[::-1]\nbeg = list(educ_lvl_beg['FormalEducation'])[::-1]\nold = list(educ_lvl_old['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'])[::-1]\nnew = list(educ_lvl_new['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'])[::-1]\n\ndot = figure(title=\"Education Level\", tools=TOOLS, plot_width = 800, plot_height = 400, y_range=educ_list, x_range=[0,65])\n\ndot.segment(0, educ_list, beg, educ_list, line_width=2, line_color=\"sienna\", legend='2017')\ndot.circle(beg, educ_list, size=15, fill_color=\"plum\", line_color=\"sienna\", line_width=1, legend='2017')\ndot.segment(0, educ_list, old, educ_list, line_width=2, line_color=\"sienna\", legend='2018')\ndot.circle(old, educ_list, size=15, fill_color=\"skyblue\", line_color=\"sienna\", line_width=1, legend='2018')\ndot.segment(0, educ_list, new, educ_list, line_width=2, line_color=\"sienna\", legend='2019')\ndot.circle(new, educ_list, size=15, fill_color=\"yellowgreen\", line_color=\"sienna\", line_width=1, legend='2019')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Education Level'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","db9ae39d":"yearly_comp_old = multiplechoice_old['Q9_What is your current yearly compensation (approximate $USD)?'].value_counts().to_frame()[1:]\nyearly_comp_new = multiplechoice_new['Q10_What is your current yearly compensation (approximate $USD)?'].value_counts().to_frame()[1:]\n\nyearly_comp_old = round(yearly_comp_old\/yearly_comp_old.sum(), 2)*100\nyearly_comp_new = round(yearly_comp_new\/yearly_comp_new.sum(), 2)*100","479354f4":"old_idx_sort = ['0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000','50-60,000', '60-70,000', '70-80,000', '80-90,000', '90-100,000',\\\n'100-125,000', '125-150,000',  '150-200,000', '200-250,000', '250-300,000', '300-400,000', '400-500,000', '500,000+']\n\nnew_idx_sort = ['1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999', '5,000-7,499', '7,500-9,999', '10,000-14,999', '15,000-19,999',\n'20,000-24,999', '25,000-29,999', '30,000-39,999', '40,000-49,999','50,000-59,999', '60,000-69,999', '70,000-79,999',\n'80,000-89,999', '90,000-99,999', '100,000-124,999', '125,000-149,999', '150,000-199,999', '200,000-249,999', '250,000-299,999',\n'300,000-500,000', '> $500,000']\n\nyearly_comp_old = yearly_comp_old.reindex(index = old_idx_sort)\nyearly_comp_new = yearly_comp_new.reindex(index = new_idx_sort)\n\ncomb_idx = ['0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000','50-60,000', '60-70,000', '70-80,000', '80-90,000', '90-100,000',\\\n'100-125,000', '125-150,000',  '150-200,000', '200-250,000', '250-300,000', '300-500,000', '500,000+']","7273fc4e":"yearly_comp_old = pd.DataFrame({'salary': comb_idx,\n                                'count': [x[0] for x in yearly_comp_old[:-3].values] + [yearly_comp_old.iloc[-3:-1].values.sum()] +\\\n                                [yearly_comp_old.iloc[-1][0]]}).set_index('salary')\n\nyearly_comp_new = pd.DataFrame({'salary': comb_idx,\n                                'count': [yearly_comp_new.iloc[:6].values.sum(), yearly_comp_new.iloc[6:8].values.sum(),\n                                 yearly_comp_new.iloc[8:10].values.sum()] + [x[0] for x in yearly_comp_new[10:].values]}).set_index('salary')","4ecea915":"yearly_comp_list = list(yearly_comp_old.index)[::-1]\nold = list(yearly_comp_old['count'])[::-1]\nnew = list(yearly_comp_new['count'])[::-1]\n\ndot = figure(title=\"Yearly Compensation\", tools=TOOLS, plot_width = 800, plot_height = 400, y_range=yearly_comp_list, x_range=[0,31])\n\ndot.segment(0, yearly_comp_list, old, yearly_comp_list, line_width=2, line_color=\"sienna\", legend='2018')\ndot.circle(old, yearly_comp_list, size=15, fill_color=\"skyblue\", line_color=\"sienna\", line_width=1, legend='2018')\ndot.segment(0, yearly_comp_list, new, yearly_comp_list, line_width=2, line_color=\"sienna\", legend='2019')\ndot.circle(new, yearly_comp_list, size=15, fill_color=\"yellowgreen\", line_color=\"sienna\", line_width=1, legend='2019')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Yearly Compensation (approx $USD)'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","a1a492c3":"comb_df = multiplechoice_new.groupby(['Q6_What is the size of the company where you are employed?', 'Q7_Approximately how many individuals are responsible for data science workloads at your place of business?']).count().iloc[:,0]\ncomb_df = comb_df.unstack().reindex(['0-49 employees', '50-249 employees', '250-999 employees', '1000-9,999 employees',\n                                     '> 10,000 employees'])\ncomb_df.columns = ['0', '1-2', '3-4', '5-9', '10-14', '15-19', '20+']\ncomb_df['1-10'] = comb_df['1-2'] + comb_df['3-4'] + comb_df['5-9']\ncomb_df['10+'] = comb_df['10-14'] + comb_df['15-19'] + comb_df['20+']\ncomb_df = comb_df[['0', '1-10', '10+']].reset_index()\n\npatch1 = mpatches.Patch(color='sienna', label='0')\npatch2 = mpatches.Patch(color='olive', label='1-10')\npatch3 = mpatches.Patch(color='slategrey', label='10+')\n\nfig, ax = plt.subplots(figsize=(15,7))\nsns.pointplot(x=\"Q6_What is the size of the company where you are employed?\", y=\"0\", data=comb_df, color= 'sienna')\nsns.pointplot(x=\"Q6_What is the size of the company where you are employed?\", y=\"1-10\", data=comb_df, color= 'olive')\nsns.pointplot(x=\"Q6_What is the size of the company where you are employed?\", y=\"10+\", data=comb_df, color= 'slategrey')\nplt.xticks(rotation=45)\nplt.ylabel('Count', fontsize = 'large')\nplt.xlabel('Company Size', fontsize = 'large')\nplt.legend(title = \"Data Science team size\", handles=[patch1, patch2, patch3])\nplt.title('Company size vs Data Science team size', fontsize = 'large')","55e56489":"source = multiplechoice_new.iloc[:,22:32]\nfor col in source.columns:\n    source[col] = source[col].value_counts()[0]\nsrc_name = [col.split('Choice - ')[1].split(' (')[0] for col in source.columns]\nsource.columns = src_name\nsource = source.drop_duplicates().T\nsource = source.sort_values(by=1)\n\np1 = figure(x_range = source.index.values, y_range = (0,11500), plot_width = 400, plot_height = 400, title = 'Sources people follow to learn Data Science', tools = TOOLS)\np1.vbar(source.index.values, width = 0.5, top = source.values.reshape(len(source)), color=['peru']*10 + ['goldenrod'])\np1.yaxis.axis_label = 'Number of Respondents'\np1.xaxis.axis_label = 'Source for learning DS'\np1.yaxis.axis_label_text_font = 'times'\np1.yaxis.axis_label_text_font_size = '12pt'\np1.xaxis.axis_label_text_font = 'times'\np1.xaxis.axis_label_text_font_size = '12pt'\np1.xaxis.major_label_orientation = math.pi\/4\np1.ygrid.grid_line_color = None\n\nplatform =  multiplechoice_new.iloc[:,35:45]\nfor col in platform.columns:\n    platform[col] = platform[col].value_counts()[0]\nplt_name = [col.split('Choice - ')[1].split(' (')[0] for col in platform.columns]\nplatform.columns = plt_name\nplatform = platform.drop_duplicates().T\nplatform = platform.sort_values(by=1)\n\np2 = figure(x_range = platform.index.values, y_range = (0,9500), plot_width = 400, plot_height = 400, title = 'Platforms used for learning Data Science courses', tools = TOOLS)\np2.vbar(platform.index.values, width = 0.5, top = platform.values.reshape(len(platform)), color=['slateblue']*10 + ['goldenrod', 'slategrey'])\np2.yaxis.axis_label = 'Number of Respondents'\np2.xaxis.axis_label = 'Platforms used'\np2.yaxis.axis_label_text_font = 'times'\np2.yaxis.axis_label_text_font_size = '12pt'\np2.xaxis.axis_label_text_font = 'times'\np2.xaxis.axis_label_text_font_size = '12pt'\np2.xaxis.major_label_orientation = math.pi\/4\np2.ygrid.grid_line_color = None\nshow(row(p1,p2))","69b6e5a0":"prog_lang = multiplechoice_new.iloc[:,[55] + list(range(82,92))]\nprog_lang.columns = ['Coding exp'] + [x.split('Choice -')[1].split(' (')[0] for x in prog_lang.columns[1:]]\nprog_lang = prog_lang.reindex(list(prog_lang['Coding exp'].dropna().index))\nprog_lang = prog_lang.groupby('Coding exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years']).reset_index()\nprog_lang = pd.melt(prog_lang, id_vars=['Coding exp'])\nprog_lang['value'] = prog_lang['value']\/90\n\np = figure(plot_width = 800, plot_height = 650, x_range = prog_lang['Coding exp'].unique(), y_range = prog_lang['variable'].unique(), title=\"Programming Language v\/s Coding Experience\", tools = TOOLS)\nsource = ColumnDataSource(prog_lang)\ncolor_mapper = LinearColorMapper(palette = Plasma256[::-1], low = prog_lang['value'].min(), high = prog_lang['value'].max())\ncolor_bar = ColorBar(color_mapper = color_mapper, location = (0, 0), ticker = BasicTicker())\np.add_layout(color_bar, 'right')\np.scatter(x = 'Coding exp', y = 'variable', size = 'value', legend = None, fill_color = transform('value', color_mapper), source = source)\np.xaxis.axis_label = 'Coding Experience'\np.yaxis.axis_label = 'Programming Language used'\np.yaxis.axis_label_text_font = 'times'\np.yaxis.axis_label_text_font_size = '12pt'\np.xaxis.axis_label_text_font = 'times'\np.xaxis.axis_label_text_font_size = '12pt'\np.xaxis.major_label_orientation = math.pi\/4\np.xgrid.grid_line_color = None\nshow(p)","2f8621c5":"lang_recom = multiplechoice_new.iloc[:,95].value_counts().to_frame().drop(index=['Other','None'])\nlang_recom.columns = ['Language Recommended']\n\nlang_recom.plot(kind='bar', figsize=(16,8), color = 'mediumslateblue', legend=False)\nplt.yscale('log')\nplt.xlabel('Programming Languages', fontsize = 'large')\nplt.ylabel('Number of Respondents', fontsize = 'large')\nplt.title('Language Recommended', fontsize = 'x-large', fontweight = 'roman')","e5d75cf4":"ide = multiplechoice_new.iloc[:,55:66]\nide.columns = ['Coding exp'] + [x.split('Choice -')[1].split(' (')[0] for x in ide.columns[1:]]\nide = ide.reindex(list(ide['Coding exp'].dropna().index))\nide = ide.groupby('Coding exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'])\nide.columns = ['Jupyter', 'RStudio', 'PyCharm', 'Atom', 'MATLAB', 'Visual Studio \/ VS Code', 'Spyder', 'Vim \/ Emacs', 'Notepad++', 'Sublime Text']\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(ide, annot= True, fmt=\"d\", linewidths=.5, cmap='YlGnBu')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('IDE used', fontsize = 'large')\nplt.ylabel('Coding Experience', fontsize = 'large')\nplt.title('IDE v\/s Coding Experience', fontsize = 'large')","6f7c9628":"vis_lib = multiplechoice_new.iloc[:,97:107]\nvis_lib.columns = [x.split('Choice -  ')[1].split(' (')[0] for x in vis_lib.columns]\nfor col in vis_lib.columns:\n    vis_lib[col] = vis_lib[col].value_counts()[0]\nvis_lib = vis_lib.drop_duplicates().T\nvis_lib = vis_lib.sort_values(by=1)\n\ncolor_map = ['cadetblue']*4 + ['plum'] + ['rosybrown'] + ['cadetblue'] + ['rosybrown'] + ['cadetblue']*2\nvis_lib[1].plot(kind='bar', color=tuple(color_map), figsize=(15,7))\ncustom_lines = [Line2D([0], [0], color='cadetblue', lw=4, label='Python'), Line2D([0], [0], color='plum', lw=4, label='Javascript'), Line2D([0], [0], color='rosybrown', lw=4, label='R')]\nplt.legend(['Python', 'Javascript', 'R'], handles = custom_lines, title = 'Programming Language', title_fontsize = 'large')\nplt.xticks(rotation=45)\nplt.xlabel('Visualization Libraries', fontsize = 'large')\nplt.ylabel('Count', fontsize = 'large')\nplt.title('Visualization Libraries used', fontsize = 'large')\nplt.show()","b866f14c":"db = multiplechoice_new.iloc[:,233:240]\ndb.columns = [x.split('Choice -')[1].split(' (')[0] for x in db.columns]\ndb = pd.melt(db).dropna().groupby('variable')['value'].count().sort_values()[::-1]\ndb.rename(index={' AWS Relational Database Service':'Amazon RDS'},inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,7))\nsns.barplot(x=list(db.index), y=list(db.values), palette=\"rocket\")\nplt.axhline(0, color=\"k\", clip_on=False)\nplt.xticks(rotation=45)\nplt.xlabel('Database', fontsize = 'large')\nplt.ylabel('Count', fontsize = 'large')\nplt.title('Database Usage', fontsize = 'large')","8f2e490d":"ml_alg = multiplechoice_new.iloc[:,117:128]\nml_alg.columns = ['ML exp'] + [x.split('Choice -')[1].split(' (')[0] for x in ml_alg.columns[1:]]\nml_alg = ml_alg.reindex(list(ml_alg['ML exp'].dropna().index))\nml_alg = ml_alg.groupby('ML exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-10 years', '10-15 years', '20+ years'])\nml_alg = ml_alg.fillna(0).astype('int').iloc[1:]\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(ml_alg, annot= True, fmt=\"d\", linewidths=.5, cmap='YlOrBr')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('ML Algorithms used', fontsize = 'large')\nplt.ylabel('ML Experience', fontsize = 'large')\nplt.title('ML Experience v\/s Algorithms used', fontsize = 'large')","9ed973f8":"ml_fw = multiplechoice_new.iloc[:,[117] + list(range(155,165))]\nml_fw.columns = ['ML exp'] + [x.split('Choice -')[1].split(' (')[0] for x in ml_fw.columns[1:]]\nml_fw = ml_fw.reindex(list(ml_fw['ML exp'].dropna().index))\nml_fw = ml_fw.groupby('ML exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-10 years', '10-15 years', '20+ years'])\nml_fw = ml_fw.fillna(0).astype('int').iloc[1:]\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(ml_fw, annot= True, fmt=\"d\", linewidths=.5, cmap='GnBu')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('ML Frameworks used', fontsize = 'large')\nplt.ylabel('ML Experience', fontsize = 'large')\nplt.title('ML Experience v\/s Frameworks used', fontsize = 'large')","9609d762":"cloud_plat = multiplechoice_new.iloc[:,168:178]\ncloud_plat.columns = [x.split('Choice -')[1].split(' (')[0] for x in cloud_plat.columns]\ncloud_plat = pd.melt(cloud_plat).dropna().groupby('variable')['value'].count().sort_values()[::-1]\n\nfig, ax = plt.subplots(figsize=(15,7))\nsns.barplot(x=list(cloud_plat.index), y=list(cloud_plat.values), palette=\"rocket\")\nplt.axhline(0, color=\"k\", clip_on=False)\nplt.xticks(rotation=45)\nplt.xlabel('Cloud Platforms', fontsize = 'large')\nplt.ylabel('Count', fontsize = 'large')\nplt.title('Cloud Platform Usage', fontsize = 'large')","32317dd3":"Cloud services is a rapidly growing market. Modern technologies like big data analytics, IoT, artificial intelligence and even web and mobile app hosting all need heavy computing power. Cloud computing offers enterprises an alternative to building their in-house infrastructure. **With cloud computing, anybody using the internet can enjoy scalable computing power on a plug and play basis**. Since this saves organizations from the need to invest and maintain costly infrastructure, it has become a very popular solution. What cloud platforms do people in DS use?","5bab6316":"![much_to_learn](https:\/\/media0.giphy.com\/media\/3ohuAxV0DfcLTxVh6w\/giphy.gif)","640ecb1d":"The basic survey details are listed in the table below:","dc309fc9":"**There should be no shortage of inspirational role models for young girls dreaming of a career in science**. Women have been responsible for some of the most important scientific breakthroughs that shaped the modern world, from Marie Curie\u2019s discoveries about radiation, to Grace Hopper\u2019s groundbreaking work on computer programming, and Barbara McClintock\u2019s pioneering approach to genetics.\n\nBut too often their stories aren\u2019t just about the difficulties they faced in cracking some of the toughest problems in science, but also about overcoming social and professional obstacles just because of their gender. And many of those obstacles still face women working and studying in science today. \n\nIs Data Science a male dominated domian or do females have a good share in its progress?","3cc7ee23":"* The analysis of the survey data reveals that Data Science has been and is still a male dominated domian.\n\n\n* The percentage of Male and Female respondents are almost the same from 2017 to 2019. \n\n\n* There are **almost five Male Kagglers for every Female Kaggler**, indicating Male dominance in the field. There haven't yet been a surge of women kagglers so far, atleast for past 3 years. Women, always have played an important role in many important scientific breakthroughs and thus, more females should step forward to be a part of Data Science.","f37c8838":"* Among the top 10 visualization libraries that the survey respondents use, *seven of them are that of Python, two of them belongs to R and the remaining one is that of Javascript*. [**Matplotlib**](https:\/\/matplotlib.org\/) and [**Seaborn**](https:\/\/seaborn.pydata.org) (which is a high level interface for drawing attractive and informative statistical graphics based on matplotlib) are the most common libraries used. If you are interested in R, then [**ggplot**](http:\/\/ggplot.yhathq.com\/) is what you need to learn and, for those having interest in javascript, [**D3.js**](https:\/\/d3js.org\/) is most used javascript library in DS.","99555563":"Now that you have found the languages that you should learn to have a good start, the next objective is to find some of the most used IDEs(Integrated development environment) where you can practice coding.","f425e819":"1. https:\/\/www.edureka.co\/blog\/what-is-data-science\/\n2. https:\/\/dare2compete.com\/bites\/the-rise-of-data-science\/\n3. https:\/\/www.weforum.org\/agenda\/2019\/03\/gender-equality-in-stem-is-possible\/\n4. https:\/\/www.itproportal.com\/features\/a-snapshot-of-data-scientist-jobs-around-the-world\/\n5. https:\/\/www.census.gov\/library\/stories\/2019\/02\/number-of-people-with-masters-and-phd-degrees-double-since-2000.html\n6. https:\/\/www.kdnuggets.com\/2018\/09\/how-many-data-scientists-are-there.html\n7. https:\/\/octoverse.github.com\/\n8. https:\/\/insights.stackoverflow.com\/survey\/2018\/#technology\n9. https:\/\/www.newgenapps.com\/blog\/top-5-cloud-platforms-and-solutions-to-choose-from","d89d59d2":"### **4. Do people start young?**","297eecd0":"### **7. How do companies respond to Data Science?**","1dfaa3c1":"This is perhaps the most important question. Data Science is viewed as the sexiest job of the 21st century. The growing demand of Data Science is what earned it that title. As the demand for it increases, so should the job openings and pay. Good pay is what attracts the people the most.\n\nIs Data Science the sexiest job in terms of pay? As more and more people entered this domain, what impact did that had on the pay scale?","316fda6c":"### **6. What do DS people earn??**","d2ac6497":"Nobody ever talks about motivation in learning. Data science is a broad and fuzzy field, which makes it hard to learn. Really hard. Without proper motivation, you\u2019ll end up stopping halfway through and believing you can\u2019t do it. So, how did those in the field managed to overcome these troubles and what kept them motivated to push further?","f0b23700":"**Note -** *The legends are interactive. Click on them to enable or disable the values associated with the legend*\n\n\n* Data Science is not the same across the globe. **The opportunities you get with DS depends a lot on where you live**.\n\n\n* The number of survey respondents dropped a lot in USA with the count going down every year, from 2017 to 2019. The same can also be observed for UK and France even though the decline is not as high as in the United States.\n\n\n* On the other hand, the survey responses from India went up a lot. The increase in the number of respondents is pretty high between 2018 and 2019 showing the increasing demand for DS in the country in the past year. Japan and Brazil also follow a similar trend with the increase in the number of respondents.\n\n\n* DS has stayed at the same level in Canada during the past 3 years of survey. \n\n\n* The other interesting case is that of China. China had a huge surge of people into DS between 2017 and 2018 but surprisingly that number has seen a big dip between 2018 and 2019.","69a290e4":"![DS_Future](https:\/\/vivente.com.au\/wp-content\/uploads\/2017\/11\/future.jpeg)","4a90bb1a":"It is clear from the study that Data Science do indeed have a bright future ahead and its never too late to be a part of it.\n\n> The Best Way To Predict The Future Is To Create It  \n       **- Peter Drucker**\n       \n\nIf you are still unceratin about the fututre of DS, then the quote is for you. As Peter Drucker said, **the best way to know the future, is to be among those creating it**.\n\nFinally, if you want to be a part of DS, or if you are just new to this domain, there is always an uncertainity on what you should learn. Learning what is important and keeping up with the present trends can help you become successfull in any domain. The last section takes a look into the present trends in DS that gives you a perfect start.","4425822c":"#### **e. What role does Machine Learning have in Data Science?**","cbfd5644":"The 2019 survey had **respondents from 171 countries and territories**. Kaggle, and with it Data Science is spreading across the globe encouraging people to be a part of it. Does Data Science have a big impact in all of those countries or do only a few countries have the expertise in the domain? This section is to understand the impact of DS across the globe. \n\nTo make this analysis a bit simpler, only the top 10 countries, with respect to the number of survey respondents are selected.","901800e1":"* Over 40% of the respondents have a Master's degree. Most of the other respondents are those with Bachelor's degree. They together make up over 75% of the total respondents, which is huge!!!. So it is **important to have atleast a Bachelor's degree** to easily become a part of DS community\n\n\n* As seen in the analysis of age group of respondents, over 50% of the them are below the age of 30. That might be so because, people starts to get into DS after completing either the Bachelor's or Master's.\n\n\n* There is a good percentage (about 20%) of respondents that have a Doctoral degree. According to the [census](https:\/\/www.census.gov\/library\/stories\/2019\/02\/number-of-people-with-masters-and-phd-degrees-double-since-2000.html) by U.S. Census Bureau, there are only 4.5 million people with Doctoal degree in US in 2018 comapred to around 70 million of them with either Bachelors or Masters. T**he 20% percentage of respondents with Doctoral degree is definitely an indicator that it is, a lot more easier to land in a DS job with a Doctoral degree**.\n\n\n* For those people without a degree, the percentage is too low. Students in universities also makes up a fair percentage of the total respondents. So, generally the higher the degree you have, more likely you are to end up in DS job.","d55b3404":"The future of DS is ofcourse dependent on the people in it. **Youths represent the future, and it is only through their engagement that the field can have a good future**. The percentage of young people in Data Science is a great indicator of where the field is headed. More the percentage, better will be the future.\n\nWhat is the impact of DS on youth population and how do non-youths respond to it?","b0cd9e97":"#### **c. How do you generate insights from data?**","45bb1327":"**Note** - *The number of respondents are in log scale*\n\n* **Python is the most used and recommended language**. Python has now become one of the most popular coding languages in the world. The differentiating factor that Python brings to the table is that it enables programmers to flesh out concepts by writing less and readable code. The developers can further take advantage of several Python frameworks to mitigate the time and effort required for building large and complex software applications. According to [GitHub\u2019s 2019 State of the Octoverse](https:\/\/octoverse.github.com\/), for the first time, Python outranked Java as the second most popular language on GitHub by repository contributors.\n\n\n* SQL is second most used language after python and is closely followed by R. The situtation is reversed when you look at the languages recommended. The respondents believe it is *better to learn R than SQL* to have a better career option in Data Science. R is commonly used in Academics for statistical analysis.\n\n\n* As observed in the analysis of the age group of respondents, most of them belong are under 30 years of age. This is observed again, in the bubble chart where the size of bubble decreases with the increase in coding experience. This is just an indication that there are only a few people with over 10 years of experience among the respondents.","0622aed8":"![pass on](https:\/\/i.imgur.com\/33FHrJT.gif)","79f53150":"![education](https:\/\/s3.ap-southeast-1.amazonaws.com\/images.deccanchronicle.com\/dc-Cover-bsnudco08r3igtj44duecnr7m4-20180630063055.Medi.jpeg)","7193ff9b":"#### **b. What should you learn to start coding?**","bd4c38ef":"We started with a quote ***\"Everything that has a beginning, has an end\"*** and, as the saying goes, this study has come to its end. We did look at some of the most common questions including the uncertainity about the future of Data Science and what you should learn to have a successfull career in Data Science, and did find answers to them. Data Science will be around for many years and it would be great to see many more people being a part of it. So, *now pass on what you have learnt so that others do benefit from it as well !*","922511fb":"**Note** - *As you have seen from the analysis of survey data, the number of respondents is different in all the three years conducted. So, for all the analysis that involves comparison of the survey data belonging to different years, the normalized values of the data are used which makes the analysis more insightful.*","55d77f78":"* The good news is that, most of the companies have a Data Science team within them. Every company wants to be a part of this growing field. Larger the company, larger the probability to find a DS team within it. The size of the DS team is also dependent on the company size.\n\n\n* The number of comapanies, that have a DS team of over 10, increases almost linearly with the size of company, for those with over 1000 employees. Even for smaller companies, having less than 50 employees, the chance of finding a DS team high. These companies mostly have a smaller DS team with 1-10 members in the team.","4d8cbe00":"![ds_comp](http:\/\/ehacking.in\/images\/Cybersecurityeng.jpg)","9c71b39e":"### 2. **The effect of Gender on Data Science**","f94f6ee4":"#### **d. Where do you store all the data?**","bab3b4b7":"**Note** -*The y-axis of the histograms are in the log scale.*\n  \n  \nSome insights from the analysis of the survey data are:\n\n* The number of respondents in the ML & DS survey **increased by about 43%** from 16716 in 2017 to 23859 in 2018. This is an indicator of the increasing interest towards Data Science but that number has then went down to 19717 in 2019.\n\n\n* Both in 2018 and in 2019, the survey was conducted in October *(Oct 22-29 in 2018 and Oct 8-28 in 2019)*. Even by keeping the survey live for more time, the *participation dropped by over 17%*. This is an interesting observation and might indicate the decreasing popularity of Data Science. More analysis needs to be done to confirm this.\n\n\n* The median response time in 2017 survey is *16.4 min*, in 2018 is *17 min* and in 2019 is *9 min*. Looking at the number of questions, it is clear that the response time is proportional to the number of questions. More the number of questions, more the response time.\n\n\n* Larger median survey time means that most respondents have spent time reading and understanding the survey questions before answering. **Thus we can consider the survey data to be genuine**.\n\n\n* The histograms and kde plots gives the distribution of the response time for the survey. As expected, the number of respondents decrease exponentially as we move right, along the survey time axes but unexpectedly, there is a small spike around 50hrs in 2018 survey and around 150hrs in 2019 survey!!! The cause is unknown.","7e093500":"The demand for Data Science appears to be increasing year by year, so how do companies respond to it? Has every company started building a DS team within them and should you join large companies to be in a DS team?","604e0ebe":"The most important part of Data Science is the data itself! As the volume of data increases, there is need for better storage methods where the data is properly structured enabling easy access to it. Databases are used for this purpose. In simple terms, a database is an organized collection of data. To work with database, you need a DBMS(Database Management System) which is a software system that enables users to define, create, maintain and control access to the database. Learning a language that helps in managing the data in DBMS is important and is what you need to learn next.","8ba2e108":"### **8. Data Science - Where and What to Learn?**","4db5c9fa":"***\"Everything that has a beginning, has an end\"*** - Everyday people come up with new ideas and innovations that shape our future. Every   discoveries and inventions that takes place, makes our life easier and better in many ways. With smart devices such as Alexa, you don't even need to get up to switch off the lights or to turn on the music. As new inventions are born, the older ones slowly fades away from existence.\n\nThe discovery of new scientific methods, algorithms and the invention of new and powerful hardwares have lead to the rise of Data Science. At present, Data Science(DS) is one among the most used words. So, *what is Data Science and what is the need for such a field of study?* \n\nSimply saying, **Data Science** is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data. As the world entered the era of big data, the need for its storage also grew. So the main focus was on building frameworks and solutions to store data. Once that was done, the data started to pour in, at a tremendous rate, and it wasn't possible to structure all those data due to their immense volume. So, today, most of stored data are either unstructured or semistructured as shown in the following figure.","5e33384f":"* The most followed source for learning DS is **Kaggle** which is closely followed by Blogs and YouTube. Kaggle is a platform for data science and analytics competitions. It claims to be the world\u2019s largest community of active data scientists and it is where you can work with them, learn from them and stay motivated. Kaggle competitions help the participants prepare for real world problems.\n\n\n* Blogs and YouTube are among the top sources used for learning DS. Details on 10 of the top DS blog sites can be found [here](https:\/\/www.tableau.com\/learn\/articles\/data-science-blogs). Few of the most followed YouTube channels for learning DS include Datacamp, Sentdex and many others where you can understand certain topics a lot better than reading a book or a journal.\n\n\n* In the present world, almost all the contents are found online. **The trend of learning has shifted from attending Universities to learning online**. Looking at the most used platforms for learning DS, there are twice as many users, who learn through [Coursera](https:\/\/www.coursera.org\/) than those attending university courses. Coursera has tons of courses related to Data Science supported by universities around the globe with expert faculties that helps you to learn, ofcourse free of cost unless you need the certification.","664403a9":"**Note **- *The analysis ignores those who do not wish to disclose their yearly compensation*.\n\n* The analysis only compares the yearly compensation of the respondents in 2018 and 2019. The graph is a pleasent sight for all those who are interested in the pay.\n\n\n* The *percentage of respondents increased in all compensation brackets except between the range 0-10k and 90-100k*, though the decrease in 90-100k range isn't too big to worry about.\n\n\n* It is delightful, taking a look at the percentage of respondents who earn more than 100k yearly. The percentage of them have increased a lot from around 12% in 2018 to around 20% in 2019, that is, **every 1 out of 5 respondents earn over 100k yearly!!**\n\n\n* The percentage of those who earn less than 10k yearly dropped from 30% in 2018 to less than 25% in 2019. As a summary, percentage of those who earn less went down and, the percentage of those who earn more rocketed. **Data Science is indeed the sexiest job in terms of yearly compensation** and the demand for people in DS is still high!!","04dcbc12":"![beg_and_end](https:\/\/y.yarn.co\/96b749d9-3268-4274-96ae-62c5ff3470b8_text.gif)","1258c298":"* **MySQL is the most used DBMS** which is then followed by PostgresSQL and MS SQL Server. According to the [survey](https:\/\/insights.stackoverflow.com\/survey\/2018\/#technology) conducted by stack overflow, the top three DBMS among developers are MySQL, MS SQL Server and PostgresSQL. PostgreSQL is gaining lots of traction in the last few years. Developers working with Postgres are very pleased with the product, both in terms of capabilities and performance.","a3b11878":"* **Young people (less than 30 year olds) makes up over 50% of the respondents**. 18-21 is the youngest among them and they might probably represent the student population. Currently, they make up around 15% of the respondents. The *youth population in DS have seen a massive surge* from under 5% in 2017 to around 15% in 2018 and 2019. Youth represent the future and this is indeed a good sign.\n\n\n* Now, move a bit down along the y-axis and have a look at the older population of respondents that belong to the age group of 45+. They represent the **experts** in their respective fields with over 15+ years of experience. The percentage of respondents in this age group went up in the past year. This is a great news, if the experienced are to switch to DS, then they might definetly see a future here.\n\n\n* The percentage of respondents dropped a lot between the age of 25 and 45 from 2017 to 2018, but in the past year the percentage has improved in the age group 35-45, while it remianed constant in the age group 30-34. Looking at the trend, it is clear that, in the past year, more people are into DS.\n\n\n* Age group 22-29 represent the young working population and they make up more than 40% of the total respondents. This particular age group is the only one in the survey that had a massive decline in the percentage of respondents, in the past year. Eventhough there was a good increase in the respondents in the age group 22-24 in 2017-2018, the percentage went down a lot in 2019. The percentage of respondents in 25-29 age group has been in decline right from 2017.","df30b6d7":"The large volume of unstructured data required more complex and advanced analytical tools and algorithms for processing, analyzing and drawing meaningful insights out of it. **Data Science was the answer to this problem!** The idea of Data Science came into existence before 2000s, but it is only recently that, with the discovery of new algorithms and analytical tools, Data Science has gained all the popularity.\n\nMarked as the **highest paying job** in the year 2016 by **Glassdoor**, the field of Data science has witnessed an immense growth in recent years. Employers are in the search of data scientists more than ever. A report by **Indeed** indicated a 29% increase in the demand of data scientists in a year.\n\nIs the situation the same today, or has the popularity of Data Science went down as the years have passed and what are trends seen in this field now? ","d4378b92":"From 2017, **Kaggle**, one of the biggest online community of data scientists and machine learners, have been conducting an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. While not all Data Scientists take part in Kaggle competitions or have a Kaggle account, and not all Kagglers do work of data science, it is reasonable to assume a large overlap. The survey is conducted yearly, usually within a time period of about 3 weeks, and later on, the survey data is made publicly available. The survey data from 2017 to 2019 is used for the study to analyze where Data Science is headed. \n\n> On two occasions I have been asked, \"Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?\". I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.       \n-\u2009**Charles Babbage, Passages from the Life of a Philosopher**\n\nWhat **Charles Babbage** said, refers to **GIGO (garbage in, garbage out)**. GIGO is an important concept in computer science and mathematics which implies that *the quality of output is determined by the quality of input*. The same applies to this study, so lets begin by taking a look at the basic details of the survey.\n\n\n### 1. A look at the survey data over the years  ","c1552d1e":"# **Does Data Science have a Future?**","7b4da5d6":"* [**Jupyter**](https:\/\/jupyter.org\/) **is the most used IDE among the respondents**. Other Python IDEs such as PyCharm and Spyder and R IDEs such as RStudio are also used by many of them for coding. Jupyter supports several coding languages including Python and R, that are the most common among the people in DS, which explains its higher usage.\n\n\n* Source code editors such as **VS Code** are also used by many. VS Code features a lightning fast source code editor, perfect for day-to-day use. With support for hundreds of languages, VS Code helps you be instantly productive with syntax highlighting, bracket-matching, auto-indentation, box-selection, snippets, and more which is exactly what every programmer wants.","91ce1fc3":"![salary](http:\/\/laschoolreport.com\/wp-content\/uploads\/2014\/09\/Teacher-salary-LAUSD.jpg)","f9c57bbc":"![country](https:\/\/knowledge.wharton.upenn.edu\/wp-content\/uploads\/2019\/01\/country-flags-rankings.jpg)","b1f84f4c":"![age](https:\/\/cdn.psychologytoday.com\/sites\/default\/files\/styles\/image-article_inline_full\/public\/field_blog_entry_images\/Longevity%20Cartoon_1.jpg?itok=X89Hn_1J)","b5c7905d":"![gender](https:\/\/www.ft.com\/paidpost\/CBS\/gender_differences\/img\/gender.jpg)","b8663b26":"### **9. References**","41dd79c3":"* Simpler algorithms such as **Linear or Logistic regression and Decision trees or random forests** are the most used ML algorithms. These algorithms are easier to learn and gives reasonably good results making them the most used ones in all categories of ML experience. ML experience only takes into account the experience working with ML algorithms or frameworks.\n\n\n* Among those with ML experience of under 3 years, deep learning methods such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are used more often as compared to Gradient Boosting Machines(GBM) and Bayesian Approaches. **It showcases the interest of the younger generation to be a part of the deep learning community**. For those with over 3 years of ML experience, the usage of algorithms such as GBM and Bayesian are a lot higher comapred to other algorithms.","3b61dd9c":"#### **a. Where do people learn Data Science?**","7871cb46":"The demand for Data Science is on the rise and more and more companies are stepping into this domain. The increasing demand attracts people to be a part of it. This section is for those people who are not yet a part of DS but want in or those who are are new to this domain and unaware of present trends in it. ","a223d9dd":"![data](https:\/\/miro.medium.com\/max\/893\/1*JeIC6PreHjgh06w3WqkXMA.jpeg)","a15d89f5":"* [**Amazon Web Services (AWS)**](https:\/\/aws.amazon.com\/) is most used cloud platform followed by Google Cloud Platform (GCP) and Microsoft Azure. Under AWS, Amazon provides on-demand cloud computing platforms like storage, data analysis, etc. Amazon Web Services allow their subscribers to enjoy a full-fledged virtual cluster of computers, at any time, based on their requirements. The entire service is enabled through the internet. AWS's virtual cloud platform comes with most of the attributes of an actual computer including hardware (CPU(s) & GPU(s) for processing, hard-disk\/SSD for storage & local\/RAM for memory), an operating system to choose from and pre-loaded apps like web servers, databases, CRM, etc.\n\n* GCP offers services in all major spheres including compute, networking, storage, machine learning (ML) and the internet of things (IoT). It also includes tools for cloud management, security, and development. The Google Cloud Storage is a highly dynamic storage solution that supports both SQL (Cloud SQL) and NoSQL (Cloud Datastore) database storage.\n\n* Microsoft Azure is used to deploy code on Microsoft's servers. This code holds access to local storage resources (blobs, queues, and tables). While the SQL Azure it is not a full SQL Server instance it can be integrated with SQL Server. The security features like authentication, security, etc. are supported using Azure AppFabric that allows applications within your LAN to communicate with Azure cloud.","97eadfc6":"### **5. What qualification do you need?**","fbdbb4d1":"### **3. What is the impact of Data Science across the globe?**","ac4bc351":"**Machine learning** is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It is one of most important element in Data Science and can be seen as the reason for the rise in the demand of Data Science. What Machine Learning (ML) skills should you develop to have better oppurtunities in DS?\n\nThe analysis takes a look into the most used ML algorithms and frameworks which helps to get a start in ML.","6352b64d":"Data visualization is a great way to generate insights from data. Data visualization is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns. Because of the way the human brain processes information, using charts or graphs to visualize large amounts of complex data is easier than poring over spreadsheets or reports. Data visualization is a quick, easy way to convey concepts in a universal manner. What are the common libraries used for visualization?","02b88675":"* **Machine Learning Framework** refers to an interface, library or tool which allows developers to more easily and quickly build machine learning models, without getting into the details of the underlying algorithms. After learning ML algorithms, it is important to understand and learn the ML frameworks used to build and deploy ML algorithms.\n\n\n* **Scikit-learn** is the most used ML framework. It has within it almost all the most used ML algorithms implemented including Linear or Logistic regression, Decision trees and many more. Boosting algorithms such as Random Forest and Xgboost are also used by many to build ML algoirthms.\n\n\n* Tensorflow and Keras are the most used frameworks after Scikit-learn. Both Tensorflow and Keras are the frameworks used to build deep learning algorithms such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). PyTorch is another popular deep learning framework and its popularity is expected to grow in the coming years.","16fb8523":"#### **f. Can Data Science be moved into the cloud?**","dfa1e6a4":"How do you implement these algorithms and do you have to built them everytime you work on a problem or are they pre-implemented somewhere that you can use with ease? Machine Learning Framework is the solution to these problems.","debe208a":"**Education is a weapon to improve one\u2019s life. It is probably the most important tool to change one\u2019s life**. Education is very important to land in a good job and to have excellent compensation. Education qualification is a measure of what you have learned and, the more qualified you are more will be the oppurtunities.\n\nDo you need higher degree of qualification to do Data Science or can you start early? The survey responses helps to understand that."}}