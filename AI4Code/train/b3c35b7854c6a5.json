{"cell_type":{"714303b1":"code","0540e47f":"code","aac0cd4c":"code","1921b3c6":"code","6cd8686b":"code","3b907d90":"code","4bbacd04":"code","fdd74c67":"code","0229d68e":"code","3363d1c0":"code","14d0eb04":"code","c71f3919":"code","2e8314d6":"code","de5b9b2c":"code","ab346e14":"code","60d310a8":"code","e3e539c2":"code","59e5483a":"code","d95c3cba":"code","074eff4b":"code","dbb7b58f":"code","3b16ce36":"markdown","0b4cf44a":"markdown","45786524":"markdown","2074b481":"markdown","f7034b72":"markdown","a6f78bd6":"markdown","8ef46800":"markdown","5f78bc76":"markdown","47f2f3d2":"markdown","e721ce55":"markdown","28a4a56b":"markdown","cad7f3b7":"markdown","48f1d8a1":"markdown","57f6e2e5":"markdown","8b97d1e7":"markdown","60140488":"markdown","2d7dfe6d":"markdown","5b529bd2":"markdown","1c766930":"markdown","17f620ce":"markdown"},"source":{"714303b1":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nimport gc\nimport os\nimport time\nimport sys\nimport datetime\nprint(os.listdir(\"..\/input\"))\n","0540e47f":"#train_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\n#test_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","aac0cd4c":"nrows = len(train_transaction)\nncols= len(train_transaction.columns)\ntot_null = train_transaction.isnull().sum().sum()\nprint('Number of rows in Train_transaction file : ',nrows)\nprint('Number of columns in Train_transaction file : ',ncols)\nprint('Number of entries in Train_transaction file : ',ncols*nrows)\nprint('Number of Null value entries in Train_transaction file : ',tot_null)\nprint('Percentage  of Null value entries in Train_transaction file : ',round(tot_null\/(ncols*nrows)*100,2))","1921b3c6":"def percent_na(df):\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    missing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing},index=None)\n    #missing_value_df.sort_values('percent_missing', inplace=True)\n    missing_value_df=missing_value_df.reset_index().drop('index',axis=1)\n    return missing_value_df\ntrain_transaction_na = percent_na(train_transaction)","6cd8686b":"sns.set(rc={'figure.figsize':(16,10)})\ncol_na_count=train_transaction_na.percent_missing.value_counts()[train_transaction_na.percent_missing.value_counts()>1]\ncol_na_count.index = col_na_count.index.to_series().apply(lambda x: np.round(x,2))\nplot=sns.barplot(col_na_count.index,col_na_count.values)\nfor p in plot.patches:\n             plot.annotate(\"%d\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n               ha='center', va='top', fontsize=12, color='black', xytext=(0, 20),\n                 textcoords='offset points')\nplot=plot.set(xlabel='% of Missing Values in a column ',ylabel= 'Number of columns')","3b907d90":"pd.options.display.max_colwidth =300\ncol_na_group= train_transaction_na.groupby('percent_missing')['column_name'].unique().reset_index()\nnum_columns=[]\nfor i in range(len(col_na_group)):\n    num_columns.append(len(col_na_group.column_name[i]))\ncol_na_group['num_columns']=num_columns\ncol_na_group = col_na_group.loc[(col_na_group['num_columns']>1) & (col_na_group['percent_missing']>0),].sort_values(by='percent_missing',ascending=False).reset_index()\ncol_na_group","4bbacd04":"col_group = col_na_group.column_name[4]\ncol_group_row_na = train_transaction[col_group].apply(lambda x: x.count(), axis=1)\ncol_group_row_na.value_counts()","fdd74c67":"sns.set(rc={'figure.figsize':(10,4)})\nprop= train_transaction['isFraud'].value_counts(normalize=True).mul(100).round(2)\nplot= sns.barplot(x=prop.index,y=prop.values)\nfor p in plot.patches:\n    plot.annotate(\"%10.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()\/2),\n               ha='center', va='center', fontsize=8, color='black', xytext=(0, 20),\n                 textcoords='offset points')","0229d68e":"\nsns.set(rc={'figure.figsize':(12,80)})\nplrow= len(col_na_group)\nx=1\nfor num,group in enumerate(col_na_group.column_name):\n    col_group_row_na = train_transaction[group].apply(lambda x: x.count(), axis=1)\n    plt.subplot(plrow,2,x)\n    prop= train_transaction.loc[col_group_row_na==0,]['isFraud'].value_counts(normalize=True).mul(100).round(2)\n    plot= sns.barplot(x=prop.index,y=prop.values)\n    for p in plot.patches:\n            plot.annotate(\"%10.3f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()\/2),\n               ha='center', va='center', fontsize=8, color='black', xytext=(0, 20),\n                 textcoords='offset points')\n    per = round(col_na_group['percent_missing'][num],3)\n    plot.set_title(label= ' By rows with no values in Columns with ' +str(per) +\"% missing values\")\n    plot.set(xlabel= 'Number of rows with all missing values : ' +str(len(train_transaction.loc[col_group_row_na==0,])))\n    plt.subplot(plrow,2,x+1)\n    prop= train_transaction.loc[col_group_row_na!=0,]['isFraud'].value_counts(normalize=True).mul(100).round(2)\n    plot= sns.barplot(x=prop.index,y=prop.values)\n    for p in plot.patches:\n            plot.annotate(\"%10.3f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()\/2),\n               ha='center', va='center', fontsize=8, color='black', xytext=(0, 20),\n                 textcoords='offset points')\n    plot.set_title(label= 'By rows with values in Columns with ' +str(per) +\"% missing values\")\n    plot.set(xlabel= 'Number of rows with values : ' +str(len(train_transaction.loc[col_group_row_na!=0,])))\n    x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","3363d1c0":"for num,alpha in enumerate(col_na_group.column_name):\n    train_transaction['group_mean_'+str(num)] = train_transaction[alpha].mean(axis=1)\n    train_transaction['group_std_'+str(num)] = train_transaction[alpha].std(axis=1)\n    test_transaction['group_mean_'+str(num)] = test_transaction[alpha].mean(axis=1)\n    test_transaction['group_std_'+str(num)] = test_transaction[alpha].std(axis=1)","14d0eb04":"train_transaction['Trans_hour']=pd.to_datetime(train_transaction['TransactionDT'],unit='s').dt.hour\ntrain_transaction['Trans_weekday']=pd.to_datetime(train_transaction['TransactionDT'],unit='s').dt.weekday\ntrain_transaction['groupCpseudoCat_mean'] = train_transaction[['C4','C7','C8','C10','C12']].mean(axis=1)\ntest_transaction['Trans_hour']=pd.to_datetime(test_transaction['TransactionDT'],unit='s').dt.hour\ntest_transaction['Trans_weekday']=pd.to_datetime(test_transaction['TransactionDT'],unit='s').dt.weekday\ntest_transaction['groupCpseudoCat_mean'] = test_transaction[['C4','C7','C8','C10','C12']].mean(axis=1)\n","c71f3919":"y =train_transaction['isFraud']\ntrain_transaction.drop(['TransactionID','isFraud'],axis=1,inplace=True)\ntest_id = test_transaction['TransactionID']\ntest_transaction.drop(['TransactionID'],axis=1,inplace=True)","2e8314d6":"objcols=[cname for cname in train_transaction.columns if train_transaction[cname].dtype == \"object\"]\nfor num,alpha in enumerate(objcols):       \n    missing = [x for x in test_transaction[alpha].unique().tolist() if x not in train_transaction[alpha].unique().tolist()]\n    if len(missing) >0:\n        print(alpha) \n        print(missing)","de5b9b2c":"test_transaction.loc[test_transaction.P_emaildomain=='scranton.edu','P_emaildomain'] =np.nan","ab346e14":"from sklearn.preprocessing import LabelEncoder\nencoders = dict()\nfor col_name in train_transaction[objcols].columns:\n    series = train_transaction[col_name]\n    label_encoder = LabelEncoder()\n    train_transaction[col_name] = pd.Series(\n        label_encoder.fit_transform(series[series.notnull()]),\n        index=series[series.notnull()].index\n    )\n    encoders[col_name] = label_encoder","60d310a8":"for num,alpha in enumerate(objcols):\n    series = test_transaction[alpha]\n    test_transaction[alpha] = pd.Series(\n        encoders[alpha].transform(series[series.notnull()]),\n        index=series[series.notnull()].index\n    )","e3e539c2":"features = [c for c in train_transaction.columns]","59e5483a":"param = {\n        'num_leaves': 10,\n        'max_bin': 127,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.02,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False\n       \n    }","d95c3cba":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nfrom sklearn.metrics import accuracy_score,classification_report,roc_auc_score\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=1337)\noof1 = np.zeros(len(train_transaction))\npredictions1 = np.zeros(len(test_transaction))\nfeature_importance_df1 = pd.DataFrame()\n\nstart = time.time()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_transaction.values, y.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_transaction.iloc[trn_idx][features], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(train_transaction.iloc[val_idx][features], label=y.iloc[val_idx])\n\n    num_round = 10000\n    clf1 = lgb.train(param, trn_data, num_boost_round=10000,valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 500)\n    oof1[val_idx] = clf1.predict(train_transaction.iloc[val_idx][features], num_iteration=clf1.best_iteration)\n    \n    fold_importance_df1 = pd.DataFrame()\n    fold_importance_df1[\"feature\"] = features\n    fold_importance_df1[\"importance\"] = clf1.feature_importance()\n    fold_importance_df1[\"fold\"] = fold_ + 1\n    feature_importance_df1 = pd.concat([feature_importance_df1, fold_importance_df1], axis=0)\n    predictions1 += clf1.predict(test_transaction[features], num_iteration=clf1.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y,oof1)))","074eff4b":"cols = (feature_importance_df1[[\"feature\", \"importance\"]]\n          .groupby(\"feature\")\n          .mean()\n          .sort_values(by=\"importance\", ascending=False)[:100].index)\nbest_features = feature_importance_df1.loc[feature_importance_df1.feature.isin(cols)]\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n             y=\"feature\",\n             data=best_features.sort_values(by=\"importance\",\n                                             ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","dbb7b58f":"submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\nsubmission['isFraud']= predictions1\nsubmission.to_csv('submission_LGBM.csv',index=False)","3b16ce36":"> Label Encoding","0b4cf44a":"Let us create some new features based on the group of columns with  same percent of null values\n","45786524":"How missing values are handled in the data is a very important aspect of Machine learning problems. Some Data Scientists recoomend that if more than 70-75%  data in a feature\/column are missing it's better to drop those features from the model. However if the entire data set has a high proportion of missing data then deleting individual features with higher proportion may work adversely in developing the model. The IEE-CIS Fraud Detection Competion data is one such case. As shown below 41%  of all the data entries in the Train_transaction file are null values.","2074b481":"> Feature Engineering","f7034b72":"Among the C Columns some of the columns are Pseudo categorical in nature . Refer kernel https:\/\/www.kaggle.com\/rajeshcv\/exploring-c-columns  for details. A new feature is developed based on this. Based on transaction date transaction hour and weekday features are also created.","a6f78bd6":"As expected in every row either none of the columns within a group have a  null value or all the columns within a group have null values.\n\nOverall Fraud(isFraud=1) and non-Fraud(isFraud=0) plot is shown below.\n","8ef46800":"Let's encode the object columns to replace string values with numericals. Here the labe encoding id done without changing null values.\n\nA check is done to find if the values in the object type columns of train and test dataset are the same","5f78bc76":"> Feature importance","47f2f3d2":"* The data set has multiple groups of related features\/columns that possibly can be combined to create new features.\n* Presence or absence of data in these related feature\/column groups may be an important feature in developing models.\n","e721ce55":"Since all columns  within the  a column group have the same missing value percentage ,in a particular row of the dataframe all columns will have null values or all columns will have non null values. Let's do a value_count of data entries in all the rows for the the column group with 46 columns and missing value percentage  77.913435 to confirm this. ","28a4a56b":"Let's look at which columns have highest number of null values. It's interesting to observe ,from the plot of percentage of missing values against the Number of columns ,that many columns have exactly the same number of missing values. Columns with same percent of missing values can be grouped.","cad7f3b7":"It looks like incase of some column groups there appears to be a significant difference in the proportion of fraud cases between non null value rows and the null value rows.\n\n","48f1d8a1":"Shown below are  the column_groups with exactly the same number of missing values. It's interesting to note that these  each group is almost a continous series of column names.","57f6e2e5":"P_emaildomain has a value 'scranton.edu' in test dataset which is not in the train dataset. We will replace this with null value","8b97d1e7":"> Model Building","60140488":"**Conclusion**","2d7dfe6d":"What would be interesting is to check within a particular group of columns whether there is any difference in Proportion of Fraud transactions between the rows with all values missing for the columns in that group and rows with values for columns in that group compared to the overall distribution. The graph below shows that distribution","5b529bd2":"> Understanding Null Values","1c766930":"> Results","17f620ce":"Feature importance by top 100 features is shown below"}}