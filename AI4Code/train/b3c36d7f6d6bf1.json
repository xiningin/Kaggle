{"cell_type":{"5727d3d3":"code","68f7181b":"code","a58c1ec7":"code","39ea5ecf":"code","7d2f6114":"code","ab36ab1b":"code","3f5a871c":"code","08e8fc8e":"code","c69c4a28":"code","14100c50":"code","41cdbab9":"code","0a558451":"code","6303b72c":"code","7a160dcc":"code","bc0483da":"code","ea3bd1cd":"code","4098191d":"code","283768b0":"code","1fd6de45":"code","747b5474":"code","a51d602b":"code","9c1a440f":"code","77b8cb4a":"code","4c1a5785":"code","6a82a872":"code","1b9511b1":"code","f9068649":"markdown","dc5afd4e":"markdown","7e6f4f82":"markdown","da5bd4d0":"markdown","5477b2be":"markdown","2b3c2d1d":"markdown"},"source":{"5727d3d3":"import numpy as np \nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings  \nwarnings.filterwarnings('ignore')","68f7181b":"# Checking files\nimport os\nos.listdir(\"..\/input\/horse-colic-dataset\")","a58c1ec7":"# Loading\ntrain = pd.read_csv(\"..\/input\/horse-colic-dataset\/horse.csv\")\ntest = pd.read_csv(\"..\/input\/horse-colic-dataset\/horseTest.csv\")","39ea5ecf":"# Checking dataset shape\nprint(\"--- Dataset Treino ---\")\nprint(\"Vari\u00e1veis:\\t{}\\nEntradas:\\t{}\\n\".format(train.shape[1], train.shape[0]))\n\nprint(\"--- Dataset Teste ---\")\nprint(\"Vari\u00e1veis:\\t{}\\nEntradas:\\t{}\".format(test.shape[1], test.shape[0]))","7d2f6114":"# Checking dataset types\ndisplay(train.dtypes)\ndisplay(train.dtypes.value_counts())\ndisplay(train.head())","ab36ab1b":"#Overview\ntrain.describe()","3f5a871c":"# Checking missing values\nqtde_nulos = train.isna().sum()\n\nprint(qtde_nulos)\n\nplt.figure(figsize=(18,10))\nplt.bar(range(len(qtde_nulos)), qtde_nulos)\nplt.title('Missing Values x Features')\nplt.xlabel('features')\nplt.ylabel('missing')\nplt.xticks(list(range(len(train.columns))), list(train.columns.values), rotation='vertical')\nplt.show()","08e8fc8e":"# Plot histogram\ntrain.hist(figsize=(18,15));","c69c4a28":"# View target variable\nsns.countplot(data=train, x='outcome');\nprint(train.outcome.value_counts())\n","14100c50":"sns.countplot(data=train, x='outcome', hue='pain');\nplt.show()\n\n#A vast majority of horses that suffer extreme or severe pain\n#While most horses involved in euthanasia experienced severe or depressed pain.","41cdbab9":"g = sns.FacetGrid(data=train, col='outcome', margin_titles=True, height=6)\ng.map(plt.hist, 'pulse')\nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('Outcome por Pulso')\n\n#Most of the horses that died had a pulse of approximately 80-100 bpm.","0a558451":"g = sns.catplot(data=train, x='peripheral_pulse', col='outcome', kind='count');\ng.fig.suptitle('Outcome por Pulso Perif\u00e9rico');\nplt.subplots_adjust(top=0.85)\n\n#More than half of the horses that died or were euthanized had a reduced peripheral pulse.","6303b72c":"reduced_absent_pulse = train[train.outcome.isin(('died','euthanized')) & train.peripheral_pulse.isin(('reduced','absent'))]\n\ng = sns.catplot(data=reduced_absent_pulse, x='capillary_refill_time', col='outcome', kind='count');\ng.fig.suptitle('Outcome por Tempo de Preenchimento Capilar');\nplt.subplots_adjust(top=0.85)\n\n#From all horses that died \/ were euthanized and had a reduced \/ absent peripheral pulse,\n#the majority had a capillary filling time of more than 3 seconds.\n#This is the sign of a bad circulatory system.","7a160dcc":"# Join datasets train and test\n\n# save id\ntrain_idx = train.shape[0]\ntest_idx = test.shape[0]\n\n# join train and test \ndf_merged = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n\n# before\nprint(\"train.shape: ({} x {})\".format(train.shape[0], train.shape[1]))\nprint(\"test.shape: ({} x {})\\n\".format(test.shape[0], test.shape[1]))\n\n# after join\nprint(\"df_merged.shape: ({} x {})\".format(df_merged.shape[0], df_merged.shape[1]))\n","bc0483da":"for col in df_merged.columns.values:\n    \n    if (pd.isna(df_merged[col]).sum()) > 0: \n    \n        if pd.isna(df_merged[col]).sum() > (50\/100 * len(df_merged)): \n            print(col,\"removido\") \n            df_merged = df_merged.drop([col], axis=1) \n        \n        elif (df_merged[col].dtype == 'object'):\n            df_merged[col] = df_merged[col].fillna(df_merged[col].mode()[0])        \n        \n        else:\n            df_merged[col] = df_merged[col].fillna(df_merged[col].median())\n            \n                \nprint(df_merged.shape)\nprint(df_merged.isna().sum())","ea3bd1cd":"# Label Encoder to \"outcome\"\n\ndf_merged[\"outcome\"] = df_merged[\"outcome\"].astype('category').cat.codes\ndf_merged.head()","4098191d":"df_merged_corr = df_merged.corr()\ncorr_values = df_merged_corr[\"outcome\"].sort_values(ascending=False)\ncorr_values = abs(corr_values).sort_values(ascending=False)\n\nprint(\"Correlated\")\nprint(abs(corr_values).sort_values(ascending=False))","283768b0":"# Removing features where the correlation is practically nonexistent\n\ndf_merged = df_merged.drop(columns=['hospital_number'], axis=1)\ndf_merged = df_merged.drop(columns=['respiratory_rate'], axis=1)\ndf_merged = df_merged.drop(columns=['lesion_3'], axis=1)\ndf_merged = df_merged.drop(columns=['rectal_temp'], axis=1)\n\ndf_merged.head()","1fd6de45":"# Converting categorial data do numeric - One Hot Encoding\ndf_merged = pd.get_dummies(df_merged)\ndf_merged.head(10)","747b5474":"# Recovering datasets train and test\ntrain = df_merged.iloc[:train_idx]\ntest = df_merged.iloc[train_idx:]\n\n# Checking shape  \nprint(\"--- Dataset Train ---\")\nprint(\"Vari\u00e1veis:\\t{}\\nEntradas:\\t{}\\n\".format(train.shape[1], train.shape[0]))\n\nprint(\"--- Dataset Test ---\")\nprint(\"Vari\u00e1veis:\\t{}\\nEntradas:\\t{}\".format(test.shape[1], test.shape[0]))","a51d602b":"# Extract results (outcome) and removing at datasets to training of the models\nX_train = train.drop(\"outcome\", axis=1).values\nY_train = train[\"outcome\"]\nX_test  = test.drop(\"outcome\", axis=1).values\nY_test  = test[\"outcome\"]","9c1a440f":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=150, min_samples_leaf=3, max_features=0.5, n_jobs=-1)\nrandom_forest.fit(X_train, Y_train)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint('Acur\u00e1cia do modelo RandomForestClassifier:',acc_random_forest,\"\\n\")\n\nY_pred1 = random_forest.predict(X_test)\n\n# Confusion Matrix \nprint(pd.crosstab(Y_test,Y_pred1,\n                  rownames=[\"Real\"], \n                  colnames=[\"Predict\"], \n                  margins=True))","77b8cb4a":"# Decision Tree\ndecision_tree = DecisionTreeClassifier(max_depth = 3)\ndecision_tree.fit(X_train, Y_train)\n\nY_pred2 = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint('Acur\u00e1cia do modelo DecisionTreeClassifier:',acc_decision_tree, \"\\n\")\n\n# Confusion Matrix \nprint(pd.crosstab(Y_test,Y_pred2,\n                  rownames=[\"Real\"], \n                  colnames=[\"Predict\"], \n                  margins=True))","4c1a5785":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred3 = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint('Acur\u00e1cia do modelo KNeighborsClassifier:',acc_knn, \"\\n\")\n\n# Confusion Matrix \nprint(pd.crosstab(Y_test,Y_pred3,\n                  rownames=[\"Real\"], \n                  colnames=[\"Predict\"], \n                  margins=True))","6a82a872":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint('Acur\u00e1cia do modelo LogisticRegression:',acc_log, \"\\n\")\n\nY_pred4 = logreg.predict(X_test)\n\n# Confusion Matrix \nprint(pd.crosstab(Y_test,Y_pred4,\n                  rownames=[\"Real\"], \n                  colnames=[\"Predict\"], \n                  margins=True))\n","1b9511b1":"# Final Ranking \nresults = pd.DataFrame({\n    'Model': ['Random Forest','Logistic Regression','KNN','Decision Tree'],\n    'Score': [acc_random_forest, acc_log, acc_knn, acc_decision_tree]})\n    \nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","f9068649":"Step 1 - Loading datasets","dc5afd4e":"Stepe 2 - Exploring Data Analisys","7e6f4f82":"Inspecting the correlation between features and results\n\nThe correlation shows how strong the attributes are related to each other. We will check the correlation of each column with the result.\n\nIf the correlation value is positive, the feature is positively correlated to the result. If the correlation value is negative, the feature is negatively correlated to the result.\n\nIf the correlation value is 0, the two attributes are not correlated.\n\n     | valor | > 0,7: positively correlated\n     0,7 <| valor | > 0.3: normal correlated\n     0,3 <| valor | > 0: not correla\u00e7\u00e3o","da5bd4d0":"RULE >>> Removing and filling in missing values \u200b\u200bin numerical and categorical data\n\nFor columns with more than 50% NAN value: remove columns\n\nFor columns with less than 50% NAN value:\n\nFor numeric data: Replaces the NAN values \u200b\u200bwith the median value for the specific column\nFor categorical data: Replaces the NAN values \u200b\u200bwith the mode value of the specific column","5477b2be":"Etapa 4 - Aplica\u00e7\u00e3o dos Modelos de ML ","2b3c2d1d":"Etapa 3 - Tratamento dos dados"}}