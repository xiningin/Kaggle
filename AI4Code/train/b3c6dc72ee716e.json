{"cell_type":{"c53048fd":"code","075e8c68":"code","0ef6da9f":"code","09f06a4f":"code","3e178ed9":"code","d88838ed":"code","f86357c7":"code","ec11a2f7":"code","3a6bd5f2":"code","3da03158":"code","4cc83735":"code","c8d7cbf1":"code","349c4880":"code","6cba2c6e":"code","b5894096":"code","e5760b98":"code","946f6fce":"code","48fb9e9b":"code","fb8c5fa1":"code","92bee8fb":"code","e8c03b15":"code","98ece79e":"code","e5536223":"code","08833db7":"code","3b18f464":"code","b5502272":"code","4150b0bf":"code","1cc55e69":"code","349b2afb":"code","65c00d65":"code","b426dbfc":"code","507b91f3":"code","6c48d176":"code","06cedf8b":"code","022fe6ad":"code","5c159b95":"markdown","2c6173d5":"markdown","2f9c633b":"markdown","6ef3f462":"markdown","e7895f5b":"markdown","cf89fe98":"markdown","048fbd4b":"markdown","eeae5cea":"markdown","1c4c1052":"markdown","4fe48981":"markdown","d8324820":"markdown","2b551610":"markdown","537cb7f8":"markdown","04b4d51b":"markdown","132aa975":"markdown","42bafc91":"markdown","8de4d000":"markdown"},"source":{"c53048fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","075e8c68":"df=pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\")","0ef6da9f":"df.head()","09f06a4f":"df.info()","3e178ed9":"df.describe()","d88838ed":"x=df.iloc[:,3:13]\ny=df.iloc[:,13]","f86357c7":"x.head()","ec11a2f7":"geo=pd.get_dummies(x['Geography'],drop_first=True)\ngen=pd.get_dummies(x['Gender'],drop_first=True)","3a6bd5f2":"x=x.drop(['Gender','Geography'],axis=1)","3da03158":"x=pd.concat([x,geo,gen],axis=1)","4cc83735":"x.head()","c8d7cbf1":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=0)","349c4880":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nxtrain=sc.fit_transform(xtrain)\nxtest=sc.transform(xtest)","6cba2c6e":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","b5894096":"classifier = Sequential()","e5760b98":"classifier.add(Dense(units=6,activation='relu',kernel_initializer='he_uniform',input_dim=11))","946f6fce":"classifier.add(Dense(units=6,activation='relu',kernel_initializer='he_uniform'))","48fb9e9b":"classifier.add(Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform'))","fb8c5fa1":" classifier.summary()","92bee8fb":"classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","e8c03b15":"model_history=classifier.fit(xtrain,ytrain,batch_size=10,epochs=100,validation_split=0.33)","98ece79e":"print(model_history.history.keys())","e5536223":"plt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","08833db7":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","3b18f464":"ypred=classifier.predict(xtest)\nypred=(ypred>0.5)","b5502272":"from sklearn.metrics import confusion_matrix\nconf=confusion_matrix(ytest,ypred)","4150b0bf":"print(conf)","1cc55e69":"from sklearn.metrics import accuracy_score\nacc=accuracy_score(ypred,ytest)\nprint(acc)","349b2afb":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom keras.layers import Dropout,BatchNormalization,Flatten,Dense,Activation\nfrom keras.activations import relu,sigmoid","65c00d65":"def createmodel(layers,activation):\n    model=Sequential()\n    \n    for i,nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,input_dim=xtrain.shape[1]))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n        else:\n            model.add(Dense(nodes))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n            \n    model.add(Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform'))\n    \n    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n    return model\n            \n        ","b426dbfc":"model=KerasClassifier(build_fn=createmodel,verbose=1)","507b91f3":"layers=[(20),(40,20),(45,30,15)]\nactivations=['sigmoid','relu']\nparam=dict(layers=layers,activation=activations,batch_size=[128,256],epochs=[30])\ngrid=GridSearchCV(estimator=model,param_grid=param,cv=5)","6c48d176":"gridresult=grid.fit(xtrain,ytrain)","06cedf8b":"print(gridresult.best_params_)","022fe6ad":"print(gridresult.best_score_)","5c159b95":"**Here I separate the dependent and independent features.Exited is the dependent feature**","2c6173d5":"**we can also change he_normal,dropouts and more hidden layers**","2f9c633b":"**Units is the no of hidden unit in the first layer ,he_uniform is the weight initialization technique for relu activation func and and input_dim is the no of features in the dataset**","6ef3f462":"**Compile the ANN**","e7895f5b":"**From the above result , we can say that relu activation, batch size of 128, epoch of 30 and layers =(45,30,15) will give higher accuracy score of 85%**","cf89fe98":"**Here the geography and gender are categorical variables. So I have to change that into numerical value**","048fbd4b":"**First hidden layer**","eeae5cea":"# Feature Scaling","1c4c1052":"**The accuracy of train is pretty much similar to the validation data without overfitting to the train data**","4fe48981":"**Fitting the ANN model to train data**","d8324820":"# Train Test split","2b551610":"**The true positive and false negative are good in this model**","537cb7f8":"# Model Development","04b4d51b":"**Second hidden layer**","132aa975":"# Perform Hyper parameter tuning for the model","42bafc91":"**Output layer**","8de4d000":"**Initialize the ANN**"}}