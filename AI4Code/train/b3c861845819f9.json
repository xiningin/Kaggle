{"cell_type":{"f58bb64c":"code","0c163a8a":"code","09c022d7":"code","f531c975":"code","819b45ff":"code","1cc6a026":"code","a5c2fe25":"code","453e4711":"code","3728cc53":"code","74bf3e0a":"code","0cb04fd9":"code","aa9dc752":"code","fd7a49cd":"code","e55ac438":"code","c93ff0f3":"code","cf0fdafb":"code","11bb146b":"code","dc33edf1":"code","3e2ab0f4":"markdown","35b53f88":"markdown","50fa45b6":"markdown","67ef8168":"markdown","10a1e581":"markdown","08828596":"markdown","3aeaab17":"markdown"},"source":{"f58bb64c":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\nfrom tensorflow import keras\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical","0c163a8a":"#importing the dataset\ndir = pd.read_csv('..\/input\/news-aggregator-dataset\/uci-news-aggregator.csv')\npd.set_option('display.max_columns', None)\ndir.head()","09c022d7":"#creating a new dataset with only relevant features.\nds = dir[['TITLE','CATEGORY']]\nds.head()","f531c975":"#shuffling rows with the help of sample, here (frac = 1) means return all rows\nds = ds.sample(frac=1).reset_index(drop=True)\nds.head()","819b45ff":"#checking for null values\nds.isnull().sum()","1cc6a026":"#plotting graph for categories\nsns.countplot(x = 'CATEGORY',data = ds)","a5c2fe25":"#cleaning and preprocessing the text\n\ncleaned = []\nfor i in range(0,len(ds)):\n    \n    #removing any other words than (a-z) and (A-Z)\n    msg = re.sub('[^a-zA-Z]',' ',ds['TITLE'][i])\n    \n    #converting all texts to lower case\n    msg = msg.lower()\n    \n    #tokenizing\n    msg = msg.split()\n    \n    #stemming and removing stopwords\n    ps = PorterStemmer()\n    msg = [ps.stem(words) for words in msg if not words in set(stopwords.words('english'))]\n    msg = ' '.join(msg)\n    cleaned.append(msg)","453e4711":"#cleaned data with no punctuations,stopwords and all texts in lowercase.\ncleaned[:5]","3728cc53":"#taking dictionary size 5000\ndict_size = 5000\n\n#one hot encoding\none_hot_mat = [one_hot(words,dict_size) for words in cleaned]\n\n#now for input as an embedding layer length of all rows should be equal therefore applying padding\n#this will make size of all rows equal by adding 0 at starting of the shorter rows\n#size of each row will be equal to length of longest row.\nembedded_layer = pad_sequences(one_hot_mat,padding = 'pre',maxlen = 150)\nembedded_layer","74bf3e0a":"#now creating independent and dependent features\nx = embedded_layer\ny = np.array(ds['CATEGORY'])","0cb04fd9":"#converting categorical values of y using OneHotEncoding\nle = LabelEncoder()\ny = le.fit_transform(y)\ny = to_categorical(y,4)","aa9dc752":"y[:10]","fd7a49cd":"#splitting the Dataset into Train and Test set\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","e55ac438":"#creating model using LSTM\nmodel = Sequential()\n\n#taking number features as 50\nmodel.add(Embedding(dict_size,50,input_length = len(x[0])))\nmodel.add(Dropout(0.2))\n\n#adding LSTM layers with 100 neurons\nmodel.add(LSTM(100))\n\n#adding output layer \nmodel.add(Dense(4,activation=\"softmax\"))\n\n#compiling the model\nmodel.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n\n#summary of model\nmodel.summary()\n\n#training the model\nrnn = model.fit(x_train, y_train, validation_data = (x_test,y_test), epochs = 10, batch_size = 256)","c93ff0f3":"#evaluating our model\nmodel.evaluate(x_test,y_test)","cf0fdafb":"#making predictions\npred = model.predict(x_test)\n\n#saving index of maximum value of pred in preds (because in pred probabilities will come)\npreds = []\nfor i in range(0,len(pred)):\n    preds.append(pred[i].argmax())\n\n#saving index of maximum value of y_test in actual\nactual = []\nfor i in range(0,len(y_test)):\n    actual.append(y_test[i].argmax())\n","11bb146b":"#classification report\nfrom sklearn import metrics\nreport = metrics.classification_report(actual, preds, target_names = ['b','t','e','m'])\nprint(report)","dc33edf1":"#checking category of a text\ntxt = [\"An apple a day keeps doctor away.\"]\n\n#cleaning and preprocessing the text\ncleaned = []\nfor i in range(0,len(txt)):\n    msg = re.sub('[^a-zA-Z]',' ',txt[i])\n    msg = msg.lower()\n    msg = msg.split()\n    ps = PorterStemmer()\n    msg = [ps.stem(words) for words in msg if not words in set(stopwords.words('english'))]\n    msg = ' '.join(msg)\n    cleaned.append(msg)\n\n#one hot encoding and embedding layer\none_hot_mat = [one_hot(words,dict_size) for words in cleaned]\nembedded_layer = pad_sequences(one_hot_mat,padding = 'pre',maxlen = 150)\nembedded_layer\n\n#prediction\npred = model.predict(embedded_layer)\ncat = ['Business','Science','Entertainment','Health']\nprint(pred, cat[np.argmax(pred)])","3e2ab0f4":"**DATASET IS NOW SHUFFLED**","35b53f88":"**THERE ARE FOUR TYPES OF CATEGORIES-**\n1. **b : business (~115000)**\n2. **t : science and technology (~110000)**\n3. **e : entertainment (~150000)**\n4. **m : health (~40000)**\n","50fa45b6":"**HERE YOU CAN SEE THAT ALL CATEGORIES ARE IN ORDER(ALL B's TOGETHER AND SO ON), THEREFORE SHUFFLING THEM FOR OUR CONVENIENCE**","67ef8168":"**NOW MOVING ONTO CLEANING AND PREPROCESSING OF THE TEXT DATA**","10a1e581":"**WE HAVE ONLY TWO FEATURES OF USE**\n\n1. **TITLE**\n2. **CATEGORY**","08828596":"# **News Category Classification using LSTM**\n**News categories included in this dataset include business; science and technology; entertainment; and health.** \n\n**Different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.**","3aeaab17":"\n**NO NULL VALUES FOUND**"}}