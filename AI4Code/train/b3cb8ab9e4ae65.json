{"cell_type":{"6da565ae":"code","280942f9":"code","474b244a":"code","4dfddf02":"code","c698ef33":"code","61b04aa5":"code","5a37b329":"code","1765dd23":"code","46dd152c":"code","6ec7e85d":"code","09bc7a12":"code","12dab9b3":"code","44c5dea4":"code","638d7769":"code","cea717d5":"code","84a838f8":"code","7c4b8f8a":"code","dd272202":"code","cdb4ac1e":"code","4e691276":"code","1c23c2f5":"code","6d9e5a08":"code","ec71101b":"code","74c070eb":"code","75251b88":"code","bff18f87":"code","dee47c8f":"code","3daa09ef":"code","ac139484":"markdown","dfbc2470":"markdown","ea536760":"markdown","3a1e8a70":"markdown","11cde1ff":"markdown","c7db080c":"markdown","e56212df":"markdown","3b913af9":"markdown","5f0a35ac":"markdown","610fa120":"markdown","71aeffb1":"markdown","f81ac14a":"markdown","53fa9786":"markdown","2ab7ea4f":"markdown","6524b10b":"markdown"},"source":{"6da565ae":"import cv2\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plot\n%matplotlib inline","280942f9":"import tensorflow\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\nfrom keras.preprocessing.image import ImageDataGenerator","474b244a":"import os\nos.listdir(\"..\/input\/\")","4dfddf02":"def get_images(directory):\n    Images = []\n    Labels = []\n    for dir_name in os.listdir(directory): \n        for image_file in os.listdir(directory+dir_name):\n            image = cv2.imread(directory+dir_name+r'\/'+image_file)\n            if image is not None:\n                image = cv2.resize(image,(300,300),)\n                Images.append(image)\n                Labels.append(dir_name)\n    return Images, Labels","c698ef33":"Images, Labels = get_images('..\/input\/train\/')","61b04aa5":"labels = []\nmapping = { 'Sugar beet': 0, 'Fat Hen': 1, 'Scentless Mayweed' : 2, 'Charlock' : 3,\n           'Small-flowered Cranesbill': 4, 'Maize': 5, 'Shepherds Purse' :6, 'Common wheat': 7,\n           'Common Chickweed': 8, 'Cleavers': 9, 'Loose Silky-bent' : 10, 'Black-grass': 11 }\nfor label in Labels:\n    labels.append(mapping[label])\ndel Labels","5a37b329":"Images[0].shape","1765dd23":"Images = np.reshape(Images,(-1,300,300,3))\nLabels = np.array(labels)","46dd152c":"print(\"Shape of training data: \", Images.shape)\nprint(\"Shape of labels data: \", Labels.shape)","6ec7e85d":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(Images, Labels, test_size=.2, random_state=42, stratify = Labels)","09bc7a12":"from keras.utils import np_utils\ny_train = np_utils.to_categorical(y_train,num_classes=12)\ny_val = np_utils.to_categorical(y_val,num_classes=12)","12dab9b3":"train_datagen = ImageDataGenerator(\n                                   rotation_range=20,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                  )\n\nvalidation_datagen = ImageDataGenerator()","44c5dea4":"del Images\ndel Labels","638d7769":"train_generator = train_datagen.flow(x_train, y_train, batch_size=32)\nvalidation_generator = validation_datagen.flow(x_val, y_val, batch_size=16)","cea717d5":"from tensorflow.keras.applications import VGG16\nvgg = VGG16(include_top=\n            False, weights='imagenet', input_shape = (300,300,3))","84a838f8":"import tensorflow.keras.optimizers as Optimizer\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAvgPool2D, GlobalMaxPooling2D, Concatenate\nfrom tensorflow.keras.models import Model","7c4b8f8a":"vgg.trainable=False\nfor layer in vgg.layers:\n    layer.trainable = False\n\nfc1 = Concatenate(axis=-1)([GlobalAvgPool2D()(vgg.output), GlobalMaxPooling2D()(vgg.output)])\nfc1 = Dense(400, activation='relu')(fc1)\nfc1_dropout = Dropout(0.3)(fc1)\nfc2 = Dense(200, activation='relu')(fc1_dropout)\nfc2_dropout = Dropout(0.3)(fc2)\nfc2 = Dense(75, activation='relu')(fc1_dropout)\noutput = Dense(12, activation='softmax')(fc2_dropout)\nmodel = Model(vgg.input, output)\n\nmodel.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()\n","dd272202":"from tensorflow.keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint('saved_model.hdf5', monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\ntrained = model.fit_generator(train_generator,steps_per_epoch = 25, epochs=200, validation_data = validation_generator,\n                              validation_steps=10, \n                              verbose=1, callbacks = callbacks_list)","cdb4ac1e":"plot.plot(trained.history['acc'])\nplot.plot(trained.history['val_acc'])\nplot.title('Model accuracy')\nplot.ylabel('Accuracy')\nplot.xlabel('Epoch')\nplot.legend(['Train', 'Test'], loc='upper left')\nplot.show()\n\nplot.plot(trained.history['loss'])\nplot.plot(trained.history['val_loss'])\nplot.title('Model loss')\nplot.ylabel('Loss')\nplot.xlabel('Epoch')\nplot.legend(['Train', 'Test'], loc='upper left')\nplot.show()","4e691276":"def get_test_images(directory):\n    Images = []\n    Image_names = []\n    for image_file in os.listdir(directory):\n        Image_names.append(image_file)\n        image = cv2.imread(directory+r'\/'+image_file)\n        if image is not None:\n            image = cv2.resize(image,(300,300),)\n            Images.append(image)\n    return Images, Image_names","1c23c2f5":"test_images, image_names = get_test_images('..\/input\/test\/')\ntest_images = np.array(test_images)\nprint(test_images.shape)","6d9e5a08":"vgg = VGG16(include_top=\n            False, weights='imagenet', input_shape = (300,300,3))\n\nvgg.trainable=False\nfor layer in vgg.layers:\n    layer.trainable = False\n\nfc1 = Concatenate(axis=-1)([GlobalAvgPool2D()(vgg.output), GlobalMaxPooling2D()(vgg.output)])\nfc1 = Dense(400, activation='relu')(fc1)\nfc1_dropout = Dropout(0.3)(fc1)\nfc2 = Dense(200, activation='relu')(fc1_dropout)\nfc2_dropout = Dropout(0.3)(fc2)\nfc2 = Dense(75, activation='relu')(fc1_dropout)\noutput = Dense(12, activation='softmax')(fc2_dropout)\nmodel = Model(vgg.input, output)","ec71101b":"model.load_weights('saved_model.hdf5')","74c070eb":"tensorflow.keras.models.save_model(\n    model,\n    'tf_model.hdf5',\n    overwrite=True,\n    include_optimizer=True\n)","75251b88":"from tensorflow.keras.models import load_model\nmodel = load_model('tf_model.hdf5')","bff18f87":"predictions = model.predict(test_images)\npredictions = np.argmax(predictions, axis = 1)","dee47c8f":"labelled_predictions = []\nmapping = {0: 'Sugar beet',1:'Fat Hen' ,2: 'Scentless Mayweed',3:  'Charlock', \n        4:'Small-flowered Cranesbill', 5:'Maize' ,\n        6: 'Shepherds Purse' ,7:'Common wheat' ,8:'Common Chickweed' ,\n        9:'Cleavers' ,10:'Loose Silky-bent'  ,11: 'Black-grass'}\nfor pred in predictions:\n    labelled_predictions.append(mapping[pred])","3daa09ef":"d = []\ni=0\nfor pred in labelled_predictions:\n    d.append({'file': image_names[i], 'species': pred})\n    i=i+1\noutput = pd.DataFrame(d)\noutput.to_csv('submission.csv',index=False)","ac139484":"**Here, I am using VGG16 Pretrained Network**","dfbc2470":"**During training, you can save the model's best weights using ModelCheckpoint. The one with the minimum validation loss is saved.**","ea536760":"**The below function fetches all the images of training set from the directory.**","3a1e8a70":"**If you ever want to save the entire model, you can save using tensorflow.keras.models.save_model()**","11cde1ff":"**Plotting the graph of model's accuracy and loss.**","c7db080c":"**The below function performs one hot encoding on the labels.**","e56212df":"**Encoding the text labels to numericals, since machine learning models only understand data in numbers.**","3b913af9":"**Here, the model predicts the new images using function model.predict()**","5f0a35ac":"**Splitting the data into Training and Validation to check the accuracy of the model on unseen data. **","610fa120":"**Reshaping the images to 4 dimensional tensors. (Model requires the input data to be in 4 dimensional format [no. of images, height, width, channels])**","71aeffb1":"**ImageDataGenerator helps in image augmentation by performing various operations on the existing images.**","f81ac14a":"**Import tensorflow libraries**","53fa9786":"**Then you can load the entire model using from tensorflow.keras.models**","2ab7ea4f":"**Here, To load the saved weights we need to define the same model architecture again. Also, make sure you do not compile the model this time.**","6524b10b":"**Preparing the predictions for submission**"}}