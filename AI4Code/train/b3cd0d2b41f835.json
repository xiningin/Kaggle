{"cell_type":{"f61fd86c":"code","b9e0e4cf":"code","65d54a64":"code","6d77a613":"code","71bdb0bf":"code","987b40b8":"code","e32ec245":"code","8cda890c":"code","73db7b41":"code","b16d0e3f":"code","50fa1be2":"code","dd39fa13":"code","0a27f246":"code","da9ffa09":"code","ed1de348":"markdown","e74ed5cf":"markdown","c6861bdf":"markdown","a50b23f0":"markdown","57dd6122":"markdown","e6accd72":"markdown"},"source":{"f61fd86c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('\/kaggle\/usr\/lib\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","b9e0e4cf":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom shutil import copyfile\ncopyfile(src = \"\/kaggle\/usr\/lib\/testcases_ffc\/testcases_ffc.py\", dst = \"..\/working\/testcases_ffc.py\")\ncopyfile(src = \"\/kaggle\/usr\/lib\/reg_utils_py\/reg_utils_py.py\", dst = \"..\/working\/reg_utils_py.py\")\n\nfrom reg_utils_py import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\nfrom reg_utils_py import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n\nimport sklearn\nimport sklearn.datasets\nimport scipy.io\n\nfrom testcases_ffc import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'","65d54a64":"train_X, train_Y, test_X, test_Y = load_2D_dataset()","6d77a613":"def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    parameters = initialize_parameters(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob < 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        \n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n                                            # but this assignment will only explore one at a time\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob < 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","71bdb0bf":"parameters = model(train_X, train_Y)\nprint (\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","987b40b8":"plt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)","e32ec245":"def compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function\n    \"\"\"\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    \n    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n    \n    L2_regularization_cost = lambd\/(2*m)*np.sum(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n    \n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost","8cda890c":"def backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    \n    dW3 = 1.\/m * np.dot(dZ3, A2.T) + (W3*lambd)\/m\n    db3 = 1.\/m * np.sum(dZ3, axis=1, keepdims = True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1.\/m * np.dot(dZ2, A1.T) + (W2*lambd)\/m\n    db2 = 1.\/m * np.sum(dZ2, axis=1, keepdims = True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1.\/m * np.dot(dZ1, X.T) + (W1*lambd)\/m\n    db1 = 1.\/m * np.sum(dZ1, axis=1, keepdims = True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients","73db7b41":"parameters = model(train_X, train_Y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","b16d0e3f":"plt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)","50fa1be2":"def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    np.random.seed(1)\n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    D1 = np.random.rand(A1.shape[0], A1.shape[1])  # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n    D1 = (D1 < keep_prob)                          # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n    A1 = A1*D1                                     # Step 3: shut down some neurons of A1\n    A1 = A1 \/ keep_prob                            # Step 4: scale the value of neurons that haven't been shut down\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    D2 = np.random.rand(A2.shape[0], A2.shape[1])  # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n    D2 = (D2 < keep_prob)                          # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n    A2 = A2*D2                                     # Step 3: shut down some neurons of A2\n    A2 = A2 \/ keep_prob                            # Step 4: scale the value of neurons that haven't been shut down\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    return A3, cache","dd39fa13":"def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1.\/m * np.dot(dZ3, A2.T)\n    db3 = 1.\/m * np.sum(dZ3, axis=1, keepdims = True)\n    dA2 = np.dot(W3.T, dZ3)\n    dA2 = dA2*D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n    dA2 = dA2\/keep_prob       # Step 2: Scale the value of neurons that haven't been shut down\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1.\/m * np.dot(dZ2, A1.T)\n    db2 = 1.\/m * np.sum(dZ2, axis=1, keepdims = True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dA1 = dA1*D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n    dA1 = dA1\/keep_prob       # Step 2: Scale the value of neurons that haven't been shut down\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1.\/m * np.dot(dZ1, X.T)\n    db1 = 1.\/m * np.sum(dZ1, axis=1, keepdims = True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients","0a27f246":"parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","da9ffa09":"plt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)","ed1de348":"#### Import Modules","e74ed5cf":"#### Problem Statement\nYou have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France's goal keeper should kick the ball so that the French team's players can then hit it with their head.\nThey give you the following 2D dataset from France's past 10 games. Each dot corresponds to a position on the football field where a football player has hit the ball with his\/her head after the French goal keeper has shot the ball from the left side of the football field.\n\n* If the dot is blue, it means the French player managed to hit the ball with his\/her head\n* If the dot is red, it means the other team's player hit the ball with their head\n\nYour goal: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.","c6861bdf":"#### Visualize the Data","a50b23f0":"#### L-2 Regularized Model","57dd6122":"#### Dropout Regularization Model","e6accd72":"#### Non-regularized Model"}}