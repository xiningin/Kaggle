{"cell_type":{"0e624215":"code","2fd50475":"code","6a2d05e7":"code","727c935b":"code","4f694edb":"code","fad9774f":"code","ac9e914d":"code","df2e5910":"code","2d9e5177":"code","e65434df":"code","52202ae4":"code","4b810b14":"code","67cc77a4":"code","433cb926":"code","3f50df92":"code","02085c19":"code","618c72dc":"code","9c4b75bb":"code","900d041e":"code","eefca32a":"markdown","e6c4c1a0":"markdown","e68bdf8c":"markdown","c1a20638":"markdown","3a92af5e":"markdown","3b93981d":"markdown","4fa6dc88":"markdown","98b493ea":"markdown","f125de61":"markdown","1cd0bf45":"markdown"},"source":{"0e624215":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf ","2fd50475":"image_dir = Path('..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df = image_df[image_df['Label'].apply(lambda x: x[-2:] != 'GT')]","6a2d05e7":"# Activate this code to use only 100 pictures for each label\n# lst = []\n# for l in image_df['Label'].unique():\n#     lst.append(image_df[image_df['Label'] == l] .sample(100, random_state = 0))\n# # Concatenate the DataFrames\n# image_df = pd.concat(lst)\n","727c935b":"# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head(3)","4f694edb":"# Display 20 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[i]))\n    ax.set_title(image_df.Label[i])\nplt.tight_layout()\nplt.show()","fad9774f":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, train_size=0.9, shuffle=True, random_state=1)","ac9e914d":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","df2e5910":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","2d9e5177":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","e65434df":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(9, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=50,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=1,\n            restore_best_weights=True\n        )\n    ]\n)","52202ae4":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","4b810b14":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","67cc77a4":"results = model.evaluate(test_images, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","433cb926":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","3f50df92":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","02085c19":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","618c72dc":"# Display 15 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","9c4b75bb":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None","900d041e":"# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","eefca32a":"## Examples of prediction","e6c4c1a0":"# 4. Train the model<a class=\"anchor\" id=\"4\"><\/a>","e68bdf8c":"# 2. Display 15 pictures of the dataset<a class=\"anchor\" id=\"2\"><\/a>","c1a20638":"# 6. Class activation heatmap for image classification<a class=\"anchor\" id=\"6\"><\/a>\n## Grad-CAM class activation visualization\n*Code adapted from keras.io*","3a92af5e":"# Table of contents\n\n[<h3>1. Load and transform the dataset<\/h3>](#1)\n\n[<h3>2. Display 15 pictures of the dataset<\/h3>](#2)\n\n[<h3>3. Load the Images with a generator<\/h3>](#3)\n\n[<h3>4. Train the model<\/h3>](#4)\n\n[<h3>5. Visualize the result<\/h3>](#5)\n\n[<h3>6. Class activation heatmap for image classification<\/h3>](#5)","3b93981d":"# General Description about the dataset and its paper\n\n*** A Large-Scale Dataset for Segmentation and Classification ***\n\nAuthors: O. Ulucan, D. Karakaya, M. Turkan\nDepartment of Electrical and Electronics Engineering, Izmir University of Economics, Izmir, Turkey\nCorresponding author: M. Turkan\n\n\n***General Introduction***\n\nThis dataset contains 9 different seafood types collected from a supermarket in Izmir, Turkey\nfor a university-industry collaboration project at Izmir University of Economics, and this work\nwas published in ASYU 2020.\nDataset includes, gilt head bream, red sea bream, sea bass, red mullet, horse mackerel, \nblack sea sprat, striped red mullet, trout, shrimp image samples. \n\nIf you use this dataset in your work, please consider to cite:\n\n* O.Ulucan , D.Karakaya and M.Turkan.(2020) A large-scale dataset for fish segmentation and classification.\nIn Conf. Innovations Intell. Syst. Appli. (ASYU)\n\n\n***Purpose of the work***\n\nThis dataset was collected in order to carry out segmentation, feature extraction and classification tasks\nand compare the common segmentation, feature extraction and classification algortihms (Semantic Segmentation, Convolutional Neural Networks, Bag of Features).\nAll of the experiment results prove the usability of our dataset for purposes mentioned above.\n\n\n\n***Data Gathering Equipment and Data Augmentation***\n\nImages were collected via 2 different cameras, Kodak Easyshare Z650 and Samsung ST60. \nTherefore, the resolution of the images are 2832 x 2128, 1024 x 768, respectively.\n\nBefore the segmentation, feature extraction and classification process, the dataset was resized to 590 x 445\nby preserving the aspect ratio. After resizing the images, all labels in the dataset were augmented (by flipping and rotating). \n\nAt the end of the augmentation process, the number of total images for each class became 2000; 1000 for the RGB fish images\nand 1000 for their pair-wise ground truth labels. \n\n\n\n***Description of the data in this data set***\n\nThe dataset contains 9 different seafood types. For each class, there are 1000 augmented images and their pair-waise augmented ground truths.\nEach class can be found in the \"Fish_Dataset\" file with their ground truth labels. All images for each class are ordered from \"00000.png\" to \"01000.png\".\n \nFor example, if you want to access the ground truth images of the shrimp in the dataset, the order should be followed is \"Fish->Shrimp->Shrimp GT\".","4fa6dc88":"# Classify fish species using transfer learning\n## \\# Class activation heatmap for image classification\n## \\# Grad-CAM class activation visualization\n\nHaving around 9.000 pictures of 9 different seafood types, the goal is to create a model to classify them.\n\n<img src=\"https:\/\/i.imgur.com\/L9aeCGo.png\" style=\"width: 70%; height: 70%\" align = \"left\">","98b493ea":"# 1. Load and transform the dataset<a class=\"anchor\" id=\"1\"><\/a>","f125de61":"# 3. Load the Images with a generator<a class=\"anchor\" id=\"3\"><\/a>","1cd0bf45":"# 5. Visualize the result<a class=\"anchor\" id=\"5\"><\/a>"}}