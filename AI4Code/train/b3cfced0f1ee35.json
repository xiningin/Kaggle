{"cell_type":{"b9903315":"code","68525d9d":"code","36dded9b":"code","e7f43674":"code","f0269e1c":"code","3025fb27":"code","e41fc4c1":"code","797dd013":"code","c5fe675f":"code","8d1c2c3b":"code","12c8017f":"code","4a305baf":"code","03daffe3":"code","79f99175":"code","7f1fefdb":"code","042b69d4":"code","de8be825":"code","4223f6aa":"code","d18d804d":"code","10bb9ce8":"code","5857fe13":"code","119e17c1":"code","748ca936":"code","a2570cb4":"code","b41c8e35":"code","8313846c":"code","3f4f3882":"code","46c3524d":"code","32bbac74":"code","e7d0a283":"markdown","24d87ae1":"markdown","3c4b6e94":"markdown","3a637bfe":"markdown","c7aa80c0":"markdown","f13a5bec":"markdown","d1c4d622":"markdown","8c90f7f3":"markdown","0e2acce9":"markdown","1be09813":"markdown","e619516d":"markdown","21731c36":"markdown","a766112c":"markdown","630371c5":"markdown","a0337043":"markdown","6dcd243b":"markdown","4ba6c3cf":"markdown","d8a7c679":"markdown","aaa6ad5b":"markdown","20a157cc":"markdown","29d35ce5":"markdown","26993809":"markdown","7a14b488":"markdown","2d1d8b21":"markdown"},"source":{"b9903315":"import numpy as np\nimport pandas as pd\nimport torch\n\nimport PIL\nprint(PIL.PILLOW_VERSION)\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","68525d9d":"import pickle\nimport numpy as np\nfrom skimage import io\nimport random\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\n# \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c warnings, \u0442\u0430\u043a \u043a\u0430\u043a \u0432\u044b \u0431\u043b\u043e\u043a\u043d\u043e\u0442\u0430\u0445 \u0432 sklearn \u043d\u0435 \u0432\u0441\u0435 \u0431\u044b\u0432\u0430\u0435\u0442 \u0433\u043b\u0430\u0434\u043a\u043e\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","36dded9b":"SEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","e7f43674":"DATA_MODES = ['train', 'val', 'test']\nRESCALE_SIZE = 224\nDEVICE = torch.device(\"cuda\")","f0269e1c":"import albumentations as A\nfrom albumentations.pytorch import ToTensor\n\naugmentations_pipeline = A.Compose(\n    [\n        A.HorizontalFlip(p = 0.5), # \u043f\u0440\u043c\u0435\u043d\u044f\u0435\u043c horizontal flip \u043a 50% \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\n        A.OneOf(\n            [\n                # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c 1 \u0438\u0437 \u0443\u043f\u0430\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0432 OneOf \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0439 \u043a 50% \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\n                A.RandomContrast(), # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c random contrast\n                A.RandomBrightness(), # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c random brightness\n            ],\n            p = 0.5\n        ),\n        \n        A.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]),\n        \n        ToTensor() # \u0443\u0440\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0432 PyTorch \u0442\u0435\u043d\u0437\u043e\u0440\n    ],\n    p = 1\n)","3025fb27":"class PlanesDataset(Dataset):\n  def __init__(self, files, mode, augmentations = None):\n    super().__init__()\n    self.files = files\n    self.mode = mode\n    self.augmentations = augmentations\n\n    if self.mode not in DATA_MODES:\n      print(f'wrong mode: {self.mode}')\n      raise NameError\n\n    self.len_ = len(self.files)\n    self.label_encoder = LabelEncoder()\n\n    if self.mode != 'test':\n      self.labels = [path.parent.name for path in self.files]\n      self.label_encoder.fit(self.labels)\n\n      with open('label_encoder.pkl', 'wb') as le_dump:\n        pickle.dump(self.label_encoder, le_dump)\n\n  def __len__(self):\n    return self.len_\n\n  def load_sample(self, file):\n    image = Image.open(file)\n    image.load()\n    return image\n\n  def __getitem__(self, index):\n    transform = transforms.Compose([\n      transforms.ToTensor(),\n      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                \n    ])\n\n    x = self.load_sample(self.files[index])\n    x = self._prepare_sample(x)\n    x = np.array(x \/ 255, dtype='float32')\n\n    x = transform(x)\n  \n    if self.mode == 'test':\n      return x\n    else:        \n      label = self.labels[index]\n      label_id = self.label_encoder.transform([label])\n      y = label_id.item()\n      return x, y\n\n  def _prepare_sample(self, image):\n    image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n    return np.array(image)","e41fc4c1":"TRAIN_DIR = Path('\/kaggle\/input\/summer-school-2020\/train')\nTEST_DIR = Path('\/kaggle\/input\/summer-school-2020\/testset')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","797dd013":"from sklearn.model_selection import train_test_split\n\n# \u0434\u0435\u043b\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0441\u0435\u0442 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 30\/70\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.3, \\\n                                          stratify=train_val_labels)","c5fe675f":"val_dataset = PlanesDataset(val_files, mode='val')\ntrain_dataset = PlanesDataset(train_files, mode='train')","8d1c2c3b":"def imshow(img, title=None, plt_ax=plt, default=False):\n  img = img.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  img = std * img + mean\n  img = np.clip(img, 0, 1)\n  plt_ax.imshow(img)\n  if title is not None:\n    plt_ax.set_title(title)\n  plt_ax.grid(False)","12c8017f":"# \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(10,10), sharex=True, sharey=True)\n\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,200))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","4a305baf":"# \u041e\u0447\u0435\u043d\u044c \u043f\u0440\u043e\u0441\u0442\u0430\u044f \u0441\u0435\u0442\u044c\nclass SimpleCnn(nn.Module):\n  \n    def __init__(self, n_classes):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        self.out = nn.Linear(96 * 5 * 5, n_classes)\n  \n  \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n\n        x = x.view(x.size(0), -1)\n        logits = self.out(x)\n        return logits","03daffe3":"def fit_epoch(model, train_loader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n              \n    train_loss = running_loss \/ processed_data\n    train_acc = running_corrects.cpu().numpy() \/ processed_data\n    return train_loss, train_acc\n  \ndef eval_epoch(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss \/ processed_size\n    val_acc = running_corrects.double() \/ processed_size\n    return val_loss, val_acc\n  \ndef train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n      \n        #\u0417\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0441\u0445\u0435\u043c\u0443 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f learning rate\n        \n        opt = torch.optim.Adam(model.parameters())\n\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","79f99175":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","7f1fefdb":"n_classes = len(np.unique(train_val_labels))\nmodel = SimpleCnn(n_classes).to(DEVICE)\nprint(model)","042b69d4":"history = train(train_dataset, val_dataset, model=model, epochs=2, batch_size=32)","de8be825":"loss, acc, val_loss, val_acc = zip(*history)","4223f6aa":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","d18d804d":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435, \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","10bb9ce8":"random_characters = int(np.random.uniform(0,200))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(model, ex_img.unsqueeze(0))","5857fe13":"idxs = list(map(int, np.random.uniform(0,200, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(model, imgs)","119e17c1":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","748ca936":"y_pred = np.argmax(probs_ims,-1)\n\nactual_labels = [val_dataset[id][1] for id in idxs]\nactual_labels = [label_encoder.classes_[i] for i in actual_labels]\npreds_class = [label_encoder.classes_[i] for i in y_pred]","a2570cb4":"actual_labels","b41c8e35":"preds_class","8313846c":"import matplotlib.patches as patches\nfrom matplotlib.font_manager import FontProperties\n\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,200))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    \n    \n\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)\n    \n    actual_text = \"Actual : {}\".format(img_label)\n            \n    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n    font0 = FontProperties()\n    font = font0.copy()\n    font.set_family(\"fantasy\")\n    prob_pred = predict_one_sample(model, im_val.unsqueeze(0))\n    predicted_proba = np.max(prob_pred)*100\n    y_pred = np.argmax(prob_pred)\n    \n    predicted_label = label_encoder.classes_[y_pred]\n    predicted_label = predicted_label[:len(predicted_label)\/\/2] + '\\n' + predicted_label[len(predicted_label)\/\/2:]\n    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n            \n    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')","3f4f3882":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\ntest_dataset = PlanesDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(model, test_loader)\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name.split('.')[0] for path in test_dataset.files]","46c3524d":"submit = pd.DataFrame({'id_image': test_filenames, 'label': preds})\nsubmit.head()","32bbac74":"submit.to_csv('submission.csv', index=False)","e7d0a283":"# \u0418\u0437\u0443\u0447\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","24d87ae1":"\u0412\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0443 \u043d\u0430 \u0413\u041f\u0423\n![image.png](attachment:image.png)","3c4b6e94":"\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0447\u0442\u043e \u0432 \u043d\u0435\u0439 \u0442\u0435 \u0441\u043b\u043e\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u044b.","3a637bfe":"![image.png](attachment:image.png)","c7aa80c0":"![image.png](attachment:image.png)","f13a5bec":"\u0417\u0430\u043f\u0443\u0441\u0442\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c. \u041e\u0431\u0440\u0430\u0442\u0438 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 (\u0438 score) \u043c\u043e\u0436\u0435\u0442 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c\u0441\u044f \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 batch_size.\n\u0422\u0443\u0442 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043c\u0435\u043d\u044f\u0442\u044c \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445. \u041d\u043e \u043d\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u0441\u044c!","d1c4d622":"# \u0418\u0437\u0443\u0447\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438","8c90f7f3":"\u0421\u043e\u0431\u0435\u0440\u0435\u043c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c, \u0441\u0435\u0439\u0447\u0430\u0441 \u043e\u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u0430\u044f \u2014 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0432\u0435\u0440\u0442\u043e\u043a \u0438 FC (fully conected) \u0441\u043b\u043e\u0435\u0432. \u041d\u0430\u043a\u0438\u043d\u0443\u0442\u044c \u0441\u043a\u043e\u0440 \u043c\u043e\u0436\u043d\u043e \u0437\u0430 \u0441\u0447\u0435\u0442 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438 \u0441\u0430\u043c\u043e\u0439 \u0441\u0435\u0442\u0438","0e2acce9":"\n\n\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0441\u043b\u043e\u0435\u0432:\n\n1.     \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432\u0445\u043e\u0434\u0430: 3x224x224\n2.     \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u0441\u043b\u0435 \u0441\u043b\u043e\u044f: 8x111x111\n3.     16x54x54\n4.     32x26x26\n5.     64x12x12\n6.     \u0432\u044b\u0445\u043e\u0434: 96x5x5\n","1be09813":"\u041f\u043e\u044f\u0432\u0438\u0442\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0432\u0430\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0438. \u0420\u0430\u0434\u0443\u0435\u043c\u0441\u044f \u0441\u0435\u0442\u043e\u0447\u043d\u043e\u0439 \u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u043d\u0430\u0434 \u0435\u0435 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435\u043c)","e619516d":"# \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u043b\u0435\u0442\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0430\u043f\u043f\u0430\u0440\u0442\u0430\u0442\u043e\u0432.\n\n\n\u041f\u0435\u0440\u0435\u0434 \u0432\u0430\u043c\u0438 \u0441\u0442\u043e\u0438\u0442 \u0437\u0430\u0434\u0430\u0447\u0430 \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u0441\u0432\u043e\u044e \u0441\u0435\u0442\u043e\u0447\u043a\u0443 \u0431\u0435\u0437 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445. \u041d\u0430\u0448\u0435\u0439 \u0446\u0435\u043b\u044c\u044e \u0431\u0443\u0434\u0435\u0442 \u043d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043b\u0435\u0442\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0430\u043f\u043f\u0430\u0440\u0430\u0442\u044b \u0432 \u043d\u0435\u0431\u0435, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0441\u0430\u043c\u043e\u043b\u0435\u0442, \u0434\u0436\u0435\u0442, \u0434\u0440\u043e\u043d, \u0432\u0435\u0440\u0442\u043e\u043b\u0435\u0442 \u0438 \u0440\u0430\u043a\u0435\u0442\u0430. \n\n\n\n ![alt text](https:\/\/i.mycdn.me\/i?r=AzF-kPXTZw6IaWs3aSUGrfjPqUvSFXbXZjS9ETU_SPXIDPRCu6D2qclJT0qwJlg-gZA)\n\n\n\n\n","21731c36":"# \u041d\u0430\u0447\u043d\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c","a766112c":"\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438:\n> \n- \u0421\u0445\u0435\u043c\u0430 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 (\u043f\u043e\u043c\u0438\u043c\u043e \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0443\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u043f\u0440\u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u0441\u043b\u043e\u0435\u0432, \u043f\u043e\u0447\u0438\u0442\u0430\u0439 \u0435\u0449\u0435 \u043f\u0440\u043e:)\n- - **\u0431\u0430\u0442\u0447-\u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e** \n- - \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0441\u043a\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0442\u0435\u043c, \u0447\u0442\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043c\u0430\u0442\u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435=0 \u0438 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044e=1 (\u0432 \u043f\u0443\u0442\u043e\u0440\u0447\u0435 BatchNorm2d\/BatchNorm1d) \n- - **\u0434\u0440\u043e\u043f\u0430\u0443\u0442** \n- -  \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u0435\u043c,\u0447\u0442\u043e \u043e\u0442\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f\u0445 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438\u044f (\u0432 \u043f\u0443\u0442\u043e\u0440\u0447\u0435 Dropout)\n* \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439\n* \u041e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u0433\u0440\u0430\u0435\u043c \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438\n* \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c learning rate, batch size \u0438 \u0432\u043e\u0442 \u044d\u0442\u043e \u0432\u0441\u0451\n\n\nAlbumentations\n\nhttps:\/\/github.com\/albumentations-team\/albumentations#how-to-use","630371c5":"# \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430","a0337043":"\u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d \u043b\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 (\u0441\u043f\u0440\u0430\u0432\u0430 \u0432\u043a\u043b\u0430\u0434\u043a\u0430 Data)\n![image.png](attachment:image.png)\n\n\u043f\u0443\u0442\u0438 \u043a \u043f\u0430\u043f\u043a\u0430\u043c:\n#\/kaggle\/input\/summer-school-2020\/train\n#\/kaggle\/input\/summer-school-2020\/testset","6dcd243b":"\u0421\u0435\u0442\u043e\u0447\u043a\u0443 \u043e\u0431\u0443\u0447\u0438\u043b\u0438. \u0412\u044b\u0432\u0435\u043b\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432. \u0425\u043e\u0440\u043e\u0448\u043e \u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c, \u043a\u0430\u043a \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0441\u0430\u0431\u043c\u0438\u0442. \u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0441\u0435\u0442\u044c \u0438 \u043c\u0435\u0442\u043e\u0434\u044b eval \u0443 \u043d\u0435\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u0441\u0435\u0442\u044c \u0432 \u0440\u0435\u0436\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f. \u0421\u0442\u043e\u0438\u0442 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u0447\u0442\u043e \u0443 \u043d\u0430\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c \u0441\u043b\u043e\u0435 \u0441\u0442\u043e\u0438\u0442 softmax, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u0431\u044a\u0435\u043a\u0442 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u0442\u043e\u043c\u0443 \u0438\u043b\u0438 \u0438\u043d\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443. \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u044d\u0442\u0438\u043c.\n","4ba6c3cf":"\u0412 \u0438\u0442\u043e\u0433\u0435 \u043c\u044b \u043e\u0431\u0443\u0447\u0438\u043b\u0438 \u0441\u0435\u0442\u043a\u0443 \u0438 \u0441\u043e\u0437\u0434\u0430\u043b\u0438 \u0444\u0430\u0439\u043b \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u043c\u0438 \u043a\u043b\u0430\u0441\u0441\u0430\u043c\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445. \u0414\u0430\u043b\u0435\u0435 \u0436\u043c\u044f\u043a\u0430\u0435\u043c \u043d\u0430 \"Save version\", \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c, \u0447\u0442\u043e\u0431 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u043e\u0441\u044c \u0441 \u0430\u0443\u0442\u043f\u0443\u0442\u043e\u043c, \u0438 \u0441\u043e\u0445\u0440\u0432\u043d\u044f\u0435\u043c \u043d\u0430\u0448\u0443 \u0432\u0435\u0440\u0441\u0438\u044e \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430 \u0441 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438. ","d8a7c679":"![image.png](attachment:image.png)","aaa6ad5b":"\u041f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0437\u0430\u0432\u0435\u0440\u0448\u0438\u0442\u0441\u044f, \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0438\u043c \u0432\u043e \u0412\u044c\u044e\u0432\u0435\u0440, \u0441\u043a\u0440\u043e\u043b\u0438\u043c \u0441\u043d\u0438\u0437 \u0434\u043e Output. \u041a\u043b\u0438\u043a\u0430\u0435\u043c \u043d\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u0439 \u043d\u0430\u043c\u0438 \u0444\u0430\u0439\u043b submission.csv \u0438 \u0441\u0430\u0431\u043b\u0438\u0442\u0438\u043c \u043e\u0442\u0432\u0435\u0442 (\u043a\u043d\u043e\u043f\u043a\u0430 \u0421\u0430\u0431\u043c\u0438\u0442).","20a157cc":"\u041f\u0440\u0438\u043c\u0435\u0440 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439, \u043f\u043e\u043c\u0435\u043d\u044f\u0439\u0442\u0435 \u043d\u0430 \u0442\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0435 \u043d\u0443\u0436\u043d\u044b\u043c\u0438 \u0438 \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u043a\u043e\u0434 \u0432 \u0441\u0432\u043e\u0439 \u043a\u043b\u0430\u0441\u0441 Dataset'a.\n\u0421\u0430\u043c\u0430\u044f \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 \u0434\u043b\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439 \u043d\u0430 kaggle \u2014 **Albumentations**.\n\nhttps:\/\/github.com\/albumentations-team\/albumentations_examples\/blob\/master\/notebooks\/example.ipynb","29d35ce5":"\u0424\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438. \u0422\u0443\u0442 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0438\u0433\u0440\u0430\u0442\u044c\u0441\u044f \u0441 learning rate, \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0434\u043b\u044f \u043d\u0435\u0433\u043e scheduler, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0435\u0433\u043e \u0432\u0430\u0440\u044c\u0438\u0440\u043e\u0432\u0430\u0442\u044c.","26993809":"# Submission","7a14b488":"\u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u043d\u0443\u044e \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0435\u0442\u044c \u0443\u0432\u0435\u0440\u0435\u043d\u0430 \u0432 \u0441\u0432\u043e\u0438\u0445 \u043e\u0442\u0432\u0435\u0442\u0430\u0445. \u041c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e, \u0447\u0442\u043e\u0431\u044b \u043e\u0442\u043b\u0430\u0436\u0438\u0432\u0430\u0442\u044c \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0432\u043e\u0434\u0430.","2d1d8b21":"### \u041f\u0440\u0438\u043c\u0435\u0440 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439"}}