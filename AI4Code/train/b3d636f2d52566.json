{"cell_type":{"19448552":"code","c0f575af":"code","34f8171f":"code","1168a289":"code","f6faa16d":"code","517e20be":"code","f2c7b448":"code","3ded0d4a":"code","6b6056be":"code","9d539225":"code","97114a57":"code","713139d9":"code","1bd372b0":"code","865658ec":"code","c1b8dd63":"code","4641fb94":"code","99877269":"code","46264a8d":"code","ddb10fd5":"code","b694feb6":"code","cc5caf7a":"code","93cb8c8a":"code","ce1df498":"code","133fd9ac":"code","2c6a27e6":"code","b2471f64":"code","583bd070":"code","9fcca703":"code","0f859955":"code","135d3d55":"code","a1c332b3":"code","54c07cd6":"code","dcdc112a":"markdown","eb9e595f":"markdown","a4c16cd1":"markdown","ff076d8c":"markdown","14ef11f9":"markdown","52702b28":"markdown","ba222fad":"markdown","018dbad0":"markdown","b815cc7d":"markdown","811fbd80":"markdown","75117cb9":"markdown","151199db":"markdown","2b454a3e":"markdown","7455fa87":"markdown","4785a3c6":"markdown","1091cb85":"markdown","8961c557":"markdown"},"source":{"19448552":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0f575af":"# Code you have previously used to load data\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\n\n#\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# model\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n\n# Mute warnings\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","34f8171f":"train = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv')\nprint(train.shape)\ntrain.info()","1168a289":"test = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv')\nprint(test.shape)\ntest.head()","f6faa16d":"#Id column looks useless so we can safely drop it from both. \n# Dropping unnecessary Id column.\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n","517e20be":"# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","f2c7b448":"train.Exterior2nd.unique()","3ded0d4a":"train.GarageYrBlt.unique()","6b6056be":"def clean(df):\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"Threeseasonporch\"}, inplace=True\n    )\n    return df","9d539225":"# The numeric features are already encoded correctly, so we focus on categorical features\n\n# The Nominal(unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\"\n                , \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\"\n                , \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\"\n                , \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n\n# The Ordinal(ordered) categorical features \nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}","97114a57":"def encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,ordered=True)) # make order for the categorical like N<Y\n    return df","713139d9":"# Get names of columns with missing values and numbers\ncols_with_missing = [col for col in train.columns\n                     if train[col].isnull().any()]\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n\n#reduced_train_attributes = train_attributes.drop(cols_with_missing, axis=1)","1bd372b0":" # function for returning missing ratios\ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","865658ec":"# Checking 'NaN' values.\n\nmissing = missing_percentage(train)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","c1b8dd63":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n        df[name] = df[name].cat.codes\n    return df","4641fb94":"def load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/housing-prices-competition-for-kaggle-learn-users\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","99877269":"df_train, df_test = load_data()\ndf_train.info()","46264a8d":"df_test=df_test.drop(\"SalePrice\",axis=1)\ndf_test.shape","ddb10fd5":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX.head()","b694feb6":"# Loading neccesary packages for modelling.\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor # This is for stacking part, works well with sklearn and others...","cc5caf7a":"def score_dataset(X, y,xgb):\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=10, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","93cb8c8a":"xgb_params =dict(\n    learning_rate=0.0139,       # effect of each tree - try 0.0001 to 0.1\n    n_estimators=4500,          # number of trees (that is, boosting rounds) - try 1000 to 8000\n    max_depth=4,                # maximum depth of each tree - try 2 to 10\n    min_child_weight=1,         # minimum number of houses in a leaf - try 1 to 10\n    subsample=0.7968,           # fraction of instances (rows) per tree - try 0.2 to 1.0\n    colsample_bytree=0.4064,    # fraction of features (columns) per tree - try 0.2 to 1.0\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\nxgb = XGBRegressor(**xgb_params)","ce1df498":"score = score_dataset(X, y,xgb)\nprint(f\"score: {score:.5f} \")","133fd9ac":"xgb.fit(X, y)\npredictions = xgb.predict(df_test)\n\noutput = pd.DataFrame({'Id': df_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","2c6a27e6":"# Setting kfold for future use.\n\nkf = KFold(10, random_state=42)","b2471f64":"# Some parameters for ridge, lasso and elasticnet.\n\nalphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv:\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr:\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting:\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\n\n# hist gradient boosting regressor:\n\nhgrd= HistGradientBoostingRegressor(    loss= 'least_squares',\n    max_depth= 2,\n    min_samples_leaf= 40,\n    max_leaf_nodes= 29,\n    learning_rate= 0.15,\n    max_iter= 225,\n                                    random_state=42)\n\n# tweedie regressor:\n \ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\n\n# stacking regressor:\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm,hgrd, tweed),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","583bd070":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","9fcca703":"# Setting list of estimators and labels for them:\n\nestimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n]","0f859955":"# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","135d3d55":"# Fitting the models on train data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y.values)\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","a1c332b3":"# Blending models by assigning weights:\n\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.05 * gbr_model_full_data.predict(X)) +\n            (0.1 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) +\n            (0.05 * hist_full_data.predict(X)) +\n            (0.1 * tweed_full_data.predict(X)) +\n            (0.25 * stack_gen_model.predict(X.values)))","54c07cd6":"\nsubmission = np.floor(np.expm1(blend_models_predict(df_test)))\n\n\noutput = pd.DataFrame({'Id': df_test.index, 'SalePrice': submission})\noutput.to_csv('my_submission_stacking.csv', index=False)\nprint(\"Your submission was successfully saved!\")","dcdc112a":"# Clean\n#### Some of the categorical features in this dataset have what are apparently typos in their categories:","eb9e595f":"# Data Preprocessing\n#### Before we can do any feature engineering, we need to preprocess the data to get it in a form suitable for analysis.\n\n#### we'll need to:\n* **Load** the data from CSV files\n* **Clean** the data to fix any errors or inconsistencies\n* **Encode** the statistical data type (numeric, categorical)\n* **Replace** any missing values","a4c16cd1":"# Missing Value\n### check Missing Value of train data","ff076d8c":"# Replace Missing Values\n#### We can simply impute 0 for missing numeric values and \"None\" for missing categorical values. ","14ef11f9":"# Model","52702b28":"### Visualize Missing value of train test data ","ba222fad":"# EDA","018dbad0":"# Cross_validation","b815cc7d":"\n\n* 1.Begin by clicking on the blue Save Version button in the top right corner of the window. This will generate a pop-up window.\n* 2.Ensure that the Save and Run All option is selected, and then click on the blue Save button.\n* 3.This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the Save Version button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (...) to the right of the most recent version, and select Open in Viewer. This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n* 4.Click on the Output tab on the right of the screen. Then, click on the file you would like to submit, and click on the blue Submit button to submit your results to the leaderboard.\n","811fbd80":"# Submission step","75117cb9":"# result","151199db":"# Encode\n#### Encoding each feature(numeric, categorical, etc.).  with its correct type helps ensure each feature is treated appropriately by whatever functions we use","2b454a3e":"# Load Data","7455fa87":"# Load","4785a3c6":"### In this notebook we are going to try explore the data we have and answer question likes below:\n* 1.Can we predict the price of a house with the given traning data using machine learning techniques.\n* 2.There are some features that can be modified and depends on the building but there are some other features like cannot be changed like location of the house, which group is effecting house prices?\n* 3.Is quality of the house alone more important than having nice garages or basements?\n* 4.What are the main predictors for house pricing?\n* 5.What is more important on pricing, having big area for housing or just being in better neighborhood?\n* 6.What can our predictions achieve with different approaches?\n","1091cb85":"### We're going to start with basic correlation table here. \nWith this table we can understand some linear relations between different features.\n\n### Observations:\n(Look at last row)\n* There's strong relation between overall quality of the houses and their sale prices.\n*  grade living area seems strong indicator for sale price.\n* Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n* Overall condition of the house seems less important on the pricing","8961c557":"# Staking model"}}