{"cell_type":{"af0aea4d":"code","d3cb6eeb":"code","414ad598":"code","a3f7cf59":"code","5584c401":"code","2125275f":"code","c1fe0100":"code","96de629a":"code","ed50f393":"code","05d38584":"code","e7d3c207":"code","63f6c9e9":"code","cff87c18":"code","becc9c7e":"code","0c709c3b":"code","def26b1a":"code","4761905f":"code","ed0d6692":"code","8bb22daa":"code","15aeb0f7":"code","40f870b9":"code","d117191d":"code","c759afc2":"code","8a2fe983":"code","0a0cc969":"code","fcbcbd81":"code","09a2efc8":"code","edad51c8":"code","b7c023e8":"code","ef3fcd1f":"code","1474af97":"code","24e278a4":"code","7db8331f":"code","174d1dfb":"code","510fbbe3":"code","a2db09cb":"code","dac0c94f":"code","93521386":"code","2753bc22":"markdown","5a2a33a4":"markdown","679d4ff7":"markdown","d1da7535":"markdown","0b6e291b":"markdown","0c5fa110":"markdown","55116187":"markdown","34f3eba8":"markdown","443970d3":"markdown","d65da7b0":"markdown","5aa94768":"markdown","e3cc19b1":"markdown","293cb61b":"markdown","6979873c":"markdown","7576f7f0":"markdown","1c8f677f":"markdown","b9d6d799":"markdown","950d3be0":"markdown","5278a930":"markdown","a0a30145":"markdown","7c668dc8":"markdown","a4f2697f":"markdown","050eced4":"markdown","69167091":"markdown","b151433a":"markdown","f25a1f57":"markdown","822e97be":"markdown","a58a4901":"markdown","4399c580":"markdown","2f95d996":"markdown","c9cd910c":"markdown","3548585f":"markdown","e89c9e58":"markdown","546cdf1a":"markdown","94a63304":"markdown","a3f98235":"markdown","4be54300":"markdown","3781bde4":"markdown","cfaf4db7":"markdown","58b25f47":"markdown","209eb612":"markdown","68bc909f":"markdown","d157b091":"markdown"},"source":{"af0aea4d":"from tensorflow.keras import models\nfrom tensorflow.keras import layers\n\n# Sequential Model is a linear stack of layers\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n\nprint(model.summary())","d3cb6eeb":"# Load some useful libraries for machine learning\nimport numpy as np # Working with tensors and numerical computing\nimport matplotlib.pyplot as plt # Plotting charts and graphs\nfrom tensorflow.keras import models # Creating ML deep learning models\nfrom tensorflow.keras import layers  # Creating layers\nfrom tensorflow.keras import optimizers # Customizing optimizers\nimport os # For working with the operating system e.g: creating dirs","414ad598":"base_dir = '\/kaggle\/input\/intel-image-classification' # original directory of the data (read-only)\nbase_custom_dir = '\/kaggle\/working\/custom' # custom directory where to copy the data too (all privileges) \n\noriginal_train_dir = os.path.join(base_dir, 'seg_train', 'seg_train')\noriginal_test_dir = os.path.join(base_dir, 'seg_test', 'seg_test')\ntrain_dir = os.path.join(base_custom_dir,  'seg_train', 'seg_train')\ntest_dir = os.path.join(base_custom_dir, 'seg_test', 'seg_test')\nvalidation_dir = os.path.join(base_custom_dir, 'seg_val', 'seg_val' )","a3f7cf59":"# A utility function to handle the moving of copying of the images.\nimport shutil\n\ndef copy_tree(src, dst, symlinks=False, ignore=None):\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            shutil.copytree(s, d, symlinks, ignore)\n        else:\n            shutil.copy2(s, d)","5584c401":"# Copy data from the original directory to the custom directory\n# Since the original directory does not allow modification of the data,\n# which is required for separating the validation data from the training data.\n# Also the original training data is required for the final training of the model.\ncopy_tree(base_dir, base_custom_dir)","2125275f":"# Create a directory in the base custom directory for the validation set\nos.makedirs(validation_dir) # Makes directories that may be missing in the path\nprint(os.listdir(base_custom_dir)) # Check if the files were moved successfully and the validation dir was made","c1fe0100":"# Separate training set from validation set\ncategories_dirs = os.listdir(train_dir) # list of folders in the custom trainin directory.\nval_number = 300 # Last index of the image from each category\nfor category in categories_dirs:\n    category_dir = os.path.join(train_dir, category) # Original category directory\n    category_val_dir = os.path.join(validation_dir, category) # Validation direcoty\n    if not os.path.exists(category_val_dir):\n        os.mkdir(category_val_dir)\n    print(f\"Moving {category}\")\n    for i, filename in enumerate(os.listdir(category_dir)):\n        file_path = os.path.join(category_dir, filename) # Original  file path\n        file_val_path = os.path.join(category_val_dir, filename) # Validation file path\n        \n        if i > val_number:\n            break;\n        shutil.move(file_path, file_val_path)\n    print(f\"Finished Moving {category}\")\n        \n\nprint(\"Folders in the training directory.\")\nprint(os.listdir(train_dir))\nprint(\"Folders in the testing directory.\")\nprint(os.listdir(test_dir))\nprint(\"Folders in the validation directory\")\nprint(os.listdir(validation_dir))\n\n# Number of images in each validation category\nfor category in categories_dirs:\n    print(category, len(os.listdir(os.path.join(validation_dir, category))))","96de629a":"# Display a sample of images from each category\nfig = plt.figure(figsize=(10, 10))\nfig.suptitle(\"Samples of images from the training data.\")\nnumber_of_samples = 2\nsub_plot_number = 0\nfor category in categories_dirs:\n    category_dir = os.path.join(train_dir, category)\n    for i, filename in enumerate(os.listdir(category_dir)):\n        file_path = os.path.join(category_dir, filename)\n\n        if i >= number_of_samples:\n            break\n        else:\n            plt.subplot(5, 5, sub_plot_number+1)\n            plt.xticks([])\n            plt.yticks([])\n            image = plt.imread(file_path)\n            plt.imshow(image, cmap=plt.cm.binary)\n            plt.xlabel(category)\n            sub_plot_number+=1\nplt.show()","ed50f393":"from tensorflow.keras.preprocessing.image import ImageDataGenerator # Generate Image data from folders\n# Since we have  a fairly large number of images it would not be wise to read them into memory,\n# luckily keras comes with the ImageDataGenerator class that handles the actual loading of images\n# it also handle normalization `rescale=1.\/2555` of images as well as,\n# resizing images to the required size for the model `target_size=(150, 150)`\ntrain_datagen = ImageDataGenerator(rescale=1.\/255) # Image generator for training data\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255) # Image generator for test data\n\nvalidation_datagen = ImageDataGenerator(rescale=1.\/255) # Image generator for validation data\n\n# Generator for training data\ntrain_datagen = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=True\n)\n\nprint(train_datagen.class_indices)\n\n# Generator for testing data\ntest_datagen = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary')\n\nprint(test_datagen.class_indices)\n\n# Generator for validating data\nvalidation_datagen = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=True\n)\n\nprint(validation_datagen.class_indices)","05d38584":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\nmodel.add(layers.Conv2D(128,(3, 3), activation=\"relu\"))\nmodel.add(layers.Flatten()) # Output from Conv2D is in the shape (height, width, depth) dense layers expect 1D array\nmodel.add(layers.Dense(512, activation=\"relu\"))\nmodel.add(layers.Dense(6, activation=\"softmax\")) # Outputs the probabilty of each of the classes\/labels\n\nprint(model.summary())","e7d3c207":"model.compile(\n    optimizer='Adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])","63f6c9e9":"# steps_per_epoch = ceil(number_of_training_images \/ batch_size)\n# validation_steps = ceil(number_of_validation_images \/ batch_size)\nhistory = model.fit_generator(\n    train_datagen,\n    steps_per_epoch=383,\n    epochs=35,\n    validation_data=validation_datagen,\n    validation_steps=57)","cff87c18":"# Utility function to plot the results of training and validation performance\ndef plot_train_val_performance(history):\n    history_dict = history.history\n    accuracy = history_dict['accuracy']\n    val_accuracy = history_dict['val_accuracy']\n    loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n\n    epochs = range(1, len(accuracy) + 1)\n\n    # Accuracy Performance\n    plt.figure()\n    plt.suptitle(\"Accuracy Performance\")\n    plt.plot(epochs, accuracy, 'b', label='Training Accuracy')\n    plt.plot(epochs, val_accuracy, 'y', label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    # Loss Performance\n    plt.figure()\n    plt.suptitle(\"Loss Performance\")\n    plt.plot(epochs, loss, 'b', label='Training Loss')\n    plt.plot(epochs, val_loss, 'y', label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n","becc9c7e":"plot_train_val_performance(history)","0c709c3b":"# Data augmentation with Keras\ndatagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')","def26b1a":"# Example of data augmentation\nimport random\nfrom tensorflow.keras.preprocessing import image\n\nmountains_dir = os.path.join(train_dir, \"mountain\")\n\n# generate a list of with the path to each image in the mountains directory\nmountain_pic_paths = [os.path.join(mountains_dir, pic_name) for pic_name in os.listdir(mountains_dir)]\n\n# select a random path of image from the sequence\nrandom_mountain_path = random.choice(mountain_pic_paths)\n\n# load the image\nmountain_img = image.load_img(random_mountain_path, target_size=(150, 150))\n\n# convert image to a numpy array\nimg_arr = image.img_to_array(mountain_img)\n\nimg_arr = img_arr.reshape((1,) + img_arr.shape)\n\ni=0\nfor batch in datagen.flow(img_arr, batch_size=1):\n    plt.figure(i)\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","4761905f":"# Modify the train data generator to perform data augmentationdatagen = ImageDataGenerator(\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n# The rest of the generator remain unchanged since its only the training data that needs augmentation\ntrain_datagen = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary')","ed0d6692":"\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\nmodel.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation=\"relu\"))\nmodel.add(layers.Dense(6, activation=\"softmax\"))\n\nprint(model.summary())","8bb22daa":"model.compile(\n    optimizer = 'Adam',\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)","15aeb0f7":"# steps_per_epoch = ceil(number_of_training_images \/ batch_size)\n# validation_steps = ceil(number_of_validation_images \/ batch_size)\nhistory = model.fit_generator(\n    train_datagen,\n    steps_per_epoch=383,\n    epochs=35,\n    validation_data=validation_datagen,\n    validation_steps=57)","40f870b9":"plot_train_val_performance(history)","d117191d":"# Load VGG16 model that comes bundled with keras\nfrom tensorflow.keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                include_top=False,\n                input_shape=(150, 150, 3))\n\nprint(conv_base.summary())","c759afc2":"conv_base.trainable = False","8a2fe983":"model = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.7))\nmodel.add(layers.Dense(256, activation=\"relu\"))\nmodel.add(layers.Dense(6, activation=\"softmax\"))\nprint(model.summary())","0a0cc969":"\nmodel.compile(\n    optimizer = 'Adam',\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)","fcbcbd81":"# steps_per_epoch = ceil(number_of_training_images \/ batch_size)\n# validation_steps = ceil(number_of_validation_images \/ batch_size)\nhistory = model.fit_generator(\n    train_datagen,\n    steps_per_epoch=383,\n    epochs=35,\n    validation_data=validation_datagen,\n    validation_steps=57)","09a2efc8":"plot_train_val_performance(history)","edad51c8":"# unfreeze the layers in block5\nfrom tensorflow.keras.applications import VGG16\n\nconv_base = VGG16(\n    weights='imagenet', \n    include_top=False, \n    input_shape=(150, 150, 3))\n\nconv_base.trainable = True\nprint(conv_base.summary())\n\n# Unfreeze the top layers\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","b7c023e8":"# define our model\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation=\"relu\"))\nmodel.add(layers.Dense(6, activation=\"softmax\"))\n\nprint(model.summary())","ef3fcd1f":"model.compile(\n    optimizer='Adam',\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)","1474af97":"# steps_per_epoch = ceil(number_of_training_images \/ batch_size)\n# validation_steps = ceil(number_of_validation_images \/ batch_size)\nhistory = model.fit_generator(\n    train_datagen,\n    steps_per_epoch=383,\n    epochs=35,\n    validation_data=validation_datagen,\n    validation_steps=57)","24e278a4":"plot_train_val_performance(history)","7db8331f":"from tensorflow.keras.preprocessing.image import ImageDataGenerator # Generate Image data from folders\n# Since we have  a fairly large number of images it would not be wise to read them into memory,\n# luckily keras comes with the ImageDataGenerator class that handles the actual loading of images\n# it also handle normalization `rescale=1.\/2555` of images as well as,\n# resizing images to the required size for the model `target_size=(150, 150)`\ntrain_datagen = ImageDataGenerator(rescale=1.\/255) # Image generator for training data\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255) # Image generator for test data\n\n# Generator for training data\ntrain_datagen = train_datagen.flow_from_directory(\n    original_train_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=True\n)\n\nprint(train_datagen.class_indices)\n\n# Generator for testing data\ntest_datagen = test_datagen.flow_from_directory(\n    original_test_dir,\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary')\n\nprint(test_datagen.class_indices)","174d1dfb":"model.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='Adam',\n    metrics=['accuracy']\n)","510fbbe3":"# steps_per_epoch = ceil(number_of_training_images \/ batch_size)\n# validation_steps = ceil(number_of_validation_images \/ batch_size)\nhistory = model.fit_generator(\n    train_datagen,\n    steps_per_epoch=439,\n    epochs=20\n)","a2db09cb":"history_dict = history.history\naccuracy = history_dict['accuracy']\nloss = history_dict['loss']\n\nepochs = range(1, len(accuracy) + 1)\n\n# Accuracy Performance\nplt.figure()\nplt.suptitle(\"Accuracy Performance\")\nplt.plot(epochs, accuracy, 'b', label='Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Loss Performance\nplt.figure()\nplt.suptitle(\"Loss Performance\")\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","dac0c94f":"results = model.evaluate_generator(test_datagen)\naccuracy = results[1]\nprint(f\"The model has an accuracy of {accuracy}\")","93521386":"model.save(\"intel_image_classification_V1.h5\")","2753bc22":"## Fine tuning","5a2a33a4":"## Final Training\n<p>\n    Using the model with fine-tuning we shall train it one final time using all our training data(from the original source)\n<\/p>","679d4ff7":"<p> Since our convolutional base is already trained, to avoid destroying it we freeze the base <code>layer.trainable = False <\/code>.<\/p>","d1da7535":"### Plot the performances","0b6e291b":"## [Intel Image Classification](https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification)\n<p> \nThe image classification task involves training a model to classify images in the following categories: <code>{'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 }<\/code>\n<\/p>\n<p>\nThe data available is as follows:\n<ul>\n<li>\n<b> Training Data<\/b>\n12228 images belonging to 6 classes.\n{'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n<\/li>\n<li>\n<b> Testing Data <\/b>\n3000 images belonging to 6 classes.\n{'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n<\/li>\n<li>\n<b>Data to predict<\/b>\n1806 images belonging to 6 classes.\n{'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n<\/li>\n<\/ul>\n<\/p>\n<p>\nIf you would like to follow along please create a kernel first on <a href=\"kaggle.com\">kaggle<\/a>.\n<\/p>","0c5fa110":"# Convolutional Neural Networks (CNNs)","55116187":"### Compile the model","34f3eba8":"### Creating generators for data","443970d3":"### Pre-trained model","d65da7b0":"<p>At this point there are two ways we could go about using the convolutional base:<\/p>\n1. We could use the base to extract features and save them to disk. Then use this extracted features to train our Dense network classifier.\n2. Extend the convolutional base with our Dense network classifier and train them together.\n<p>For brevity; we shall take the second option: for the first option and in-depth explanations consult [Deep Learning with python](https:\/\/www.manning.com\/books\/deep-learning-with-python).<\/p>","5aa94768":"<p> \n    From the plots above we can see that our validation accuracy raises to 80%. \n    To improve the model further we can unfreeze a few top layers of our model and train them together with our classifier.\n    This helps the model to adapt better to the data that we have.\n<\/p>","e3cc19b1":"### Fit the model","293cb61b":"<p>\nEven with a Dropout Layer and Data Augmentation our module's accuracy is only about 84%.\nSince we have such a small dataset it is impossible to achieve very high accuracy.\n<\/p>\n<p>\nIn deep learning however, the layers that are lower in the network learn more generalized features, such as: edges and textures. In CNNs the convolution base(alternating layers of Conv2D and MaxPooling layers) of a network that was pre-trained on somewhat similar data can be reused in a new model to boost the models accuracy.\n<\/p>\n<p>\nThere are two ways in which the convlotional base can be reused:\n<ol>\n<li> <h5>FeatureExtraction<\/h5> Use the convolutional base to extract features from our data, then train the classifier on these extracted features.<\/li>\n<li> <h5>FineTuning<\/h5> \nFine tuning involves unfreezing a few of the top layers in the convolutional base and training them together with our network. This makes the model more relevant for the dataset.\n<\/li>\n<\/ol>\n<\/p>","6979873c":"### Fit the model","7576f7f0":"### Preparing Data\n<p>Since we have training and testing data only that we can use during the actual process of training the model it is important to have some validation data so we can observe how good our model generalizes to new data. \n<\/p>\n<p>\nThis is important as we have little training data, hence overfitting will be our main concern. Later we shall see some techniques to deal with this problem.\n<\/p>\n<p>\nWe shall split our training data into training and validation set. The validation set will have <b>301<\/b> samples of data from each category.\n<\/p>","1c8f677f":"A CNN is basically composed of Convolutional Layers and Pooling layers.","b9d6d799":"### Prepare Generators","950d3be0":"### Evaluate the model","5278a930":"### Fit the model","a0a30145":"### FeatureExtraction\n\n<p>\nThe model that will  VGG16 Architecture)be used is the VGG16 architecture. It is a stack of Conv2D and MaxPooling2D layers just like our previous models.\n<img src=\"https:\/\/www.researchgate.net\/publication\/321829624\/figure\/fig2\/AS:571845657481217@1513350037610\/VGG16-architecture-16.png\" alt=\"VGG16 Architecture\" \/>\n<\/p>","7c668dc8":"<ol>\n<li><b>rotation_range<\/b>: (0-180) range to randomly rotate images<\/li>\n<li><b>width_shift_range<\/b> and <b>height_shift_range<\/b> ranges within which to randomly translate images expressed as a fraction of the width and fraction of height respectively.<\/li>\n<li><b>shear_range<\/b>: range within which to apply the shear transformation<\/li>\n<li><b>zoom_range<\/b>: range within which to zoom into the image.<\/li>\n<li><b>horizontal_flip<\/b>: randomly flip images horizontally.<\/li>\n<\/ol>","a4f2697f":"### A model that uses Data Augmentation and Dropout Layer","050eced4":"### Plot the performances","69167091":"### Save the trained model","b151433a":"## Overfitting\n<p>\nFrom the above plots we observe that the training accuracy rises to <b>90%<\/b> while that of validation is stuck at around <b>79%<\/b>.\nA common problem in deep learning is overfitting which is the culprit in our case. Since we do not have a lot of data, it is not possible  to show the model all the variations that are possible in our sample space, hence the model performs well for the  data that it has already seen(training data).When it comes to the validation set the model encouters new samples from our sample space that it has not seen hence the poor prediction.\nWe can say that our model does not generalize well to new data.\n<\/p>\n<p>\nTo fix the problem of overfitting there are some things that we can do:\n<ol>\n<li> Use data augmentation <\/li>\n<li> Add a Dropout layer <\/li>\n<li> Reduce size of our network <\/li>\n<\/ol>\n<\/p>","f25a1f57":"## Pooling Layers\n<p>These layers are used to downsample feature maps. They do this by perfoming a statistical summary of the output of the network at a given layer; using a <i>pooling function<\/i>; then use this statistical summary to replace the output of the network at that given layer.<\/p>\n<p> Pooling enables CNNs to ignore minor details such as the position of a pattern, hence preventing feature maps that actually encode the same pattern but only the position differs.\n<\/p>\n<p> There are various pooling functions:\n<ol>\n<li> Max pooling. Takes the maximum output from each patch of the feature map. \n    In the example above, the <b>MaxPooling2D<\/b> has a window size of <code>(2, 2)<\/code>,\n    and a <a href=\"https:\/\/deepai.org\/machine-learning-glossary-and-terms\/stride\">stride<\/a> of 2, this means that each dimension of the input feature map is halved.\n<\/li>\n<li> Average pooling. Takes the average output from each patch of the feature map.\n<\/li>\n<li>\n    L<sup>2<\/sup> norm of the patch.\n<\/li>\n<li>\n    Weighted average based on the distance from the central pixel.    \n<\/li>\n<\/ol>\n<\/p>","822e97be":"<p>CNN is a deep learning model that applys a mathematical operation called convolution in its layers. <\/p>\n<p>CNNs are mainly used for:\n<ol>\n    <li>Computer Vision<\/li>\n    <li>Time series data processing<\/li>\n    <li>Image Recognition<\/li>\n<\/ol>\n<\/p>","a58a4901":"### A Simple Example","4399c580":"## References\n<p>\n    <ol>\n        <li> Deep Learning with Python - FRAN\u00c7OIS CHOLLET <\/li>\n        <li>Deep Learning (2017, MIT) - Ian Goodfellow, Yoshua Bengio, Aaron Courville <\/li>\n    <\/ol>\n<\/p>","2f95d996":"### Plot the performances","c9cd910c":"### Fit the model","3548585f":"## Using a Pretrained ConvNet","e89c9e58":"### A Naive Model\n<p> \nThe model below is just a good enough model then we shall tweak it based on its performance to come up with a better model.\n<\/p>","546cdf1a":"### Compile the model","94a63304":"### Compile The Model","a3f98235":"### Compile the model","4be54300":"### Plot the performance","3781bde4":"### Data Augmentation\n<p>\nAs we said, if the model was exposed to every possible sample in our sample space the model would generalize well to new data.\n<\/p>\n<p>\nData Augmentation generates new images from our data using a series of transformations hence the model is not exposed to the same image over and over thus generalizing better.\n<\/p>\n<p>\nData Augmentation should only be performed on the training data.\n<\/p>","cfaf4db7":"### Compile the model","58b25f47":"### Fit The Model","209eb612":"## Convolutional Layers\n<p>These are the layers that do the actual learning. The convolutional layers process 3D tensors with a shape of <code>(height, width, depth)<\/code> called <i> feature maps <\/i> and produces <i> output feature maps <\/i> of the shape <code>(height, width, depth)<\/code>.<\/p>\n<p> The way that the convolutional layers work is by extracting patches from the input tensors determined by the <i> window size <\/i>; e.g in the first convolutional layer the window size is <code>(3, 3)<\/code>. The layers then apply transformations determined by their <i> activation function <\/i> to these patches. The depth of the output feature map is determined by the first argument to the <b> Conv2D <\/b> layer i.e the number of <i>filters <\/i> calculated by the layer.<\/p>\n<p> Due to the fact that convolutional layers work on patches they have two interesting properties. \n<ol>\n<li>Once they learn a pattern they can recognize it regardless of where the pattern appears, without relearning the pattern again.<\/li>\n<li> Lower layers in the model can learn patterns such as edges and textures while higher layers can learn more abstract patterns such as ears and noses. This enables them to learn of hierarchies in data. <\/li>\n<\/ol> ","68bc909f":"### Plot performance","d157b091":"### [Dropout Layers](https:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/)\n<p>\nDuring training a number of units are ignored. The units are temporarily removed along with their incoming as well as outgoing nodes.\n<\/p>\n<p>\nDropout prevents situations where the network layers co-adapt to fix the mistakes of the prior layers making the network more generalized.\n<blockquote>\nunits may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data.\n<\/blockquote>\n<\/p>\n<p>\nThe Dropout layer takes a hyperparameter which is a fraction of the units to drop. <code> 0.5 <\/code> is usually a good estimate.\n<\/p>"}}