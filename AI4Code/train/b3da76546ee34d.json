{"cell_type":{"f35f248b":"code","e6bda97e":"code","0ccce414":"code","9d0a29c8":"code","65093957":"code","ea5ea104":"code","fcb2214c":"code","70609fa8":"code","683fcf21":"code","9335111d":"code","79c23744":"code","3fe7cc8f":"code","c384248f":"code","80e0d857":"code","535c5d90":"markdown","e335c8b9":"markdown","17e593b6":"markdown","4fcad138":"markdown","8d80c01f":"markdown","46551a41":"markdown","211b43fb":"markdown","36437a6f":"markdown","259b9b99":"markdown","424d3a9a":"markdown","94677764":"markdown","a9cca858":"markdown","b9882f7c":"markdown"},"source":{"f35f248b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6bda97e":"# import input_data # standard python class for downloading datasets\n# read MNIST data\n# https:\/\/stackoverflow.com\/a\/37540230\/5411712\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_Data\", one_hot=True)\nprint(mnist)","0ccce414":"import tensorflow as tf","9d0a29c8":"learning_rate = 0.01 # how fast to update weights; 0.01 is standard and pretty good\n        # too big >> miss optimal soln; too small >> takes too long to find optimal soln\ntraining_iteration = 30 # number of times to run the gradient descent (optimizer) step\nbatch_size = 100\ndisplay_step = 2","65093957":"# TF graph input\nx = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape; 28*28=784\n     # notice images are 28px by 28px arrays & get \"flattened\" into 1D array of 784 pixels\ny = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition >> 10 classes to be \"classified\"\n\n# create a model\n\n# set model parameters\nW = tf.Variable(tf.zeros([784, 10])) # weights (probabilities that affect how data flows in graph)\nb = tf.Variable(tf.zeros([10]))      # biases (lets us shift the regression line to fit data)","ea5ea104":"# \"scopes help us organize nodes in the graph visualizer called, Tensorboard\"\nwith tf.name_scope(\"Wx_b\") as scope:\n    # First scope constructs a linear model (Logistic Regression)\n    # `tf.nn` --- https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\n    model = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax???? what about ReLU? Sigmoid? \n                                               # tf.nn.relu(biases=,features=,name=,weights=,x=)\n                                               # tf.nn.softmax(_sentinel=,axis=,dim=,labels=,logits=,name=)","fcb2214c":"# Add summary operations to collect data\n# helps us later visualize the distribution of the Weights and biases\n# https:\/\/github.com\/tensorflow\/serving\/issues\/270\nw_h = tf.summary.histogram(\"weights\", W)\nb_h = tf.summary.histogram(\"biases\", b)","70609fa8":"# More name scopes will clean up graph representation\nwith tf.name_scope(\"cost_function\") as scope:\n    # Second scope minimizes error using \"cross entropy function\" as the \"cost function\"\n    # cross entropy function\n    cost_function = -tf.reduce_sum(y*tf.log(model))\n    # create a summary to monitor the cost function; for later visualization\n    tf.summary.scalar(\"cost_function\", cost_function)","683fcf21":"with tf.name_scope(\"train\") as scope:\n    # Last scope Gradient Descent; the training algorithm\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)","9335111d":"# initialize the variables\ninit = tf.initialize_all_variables()","79c23744":"# merge summaries into 1 operation\n# https:\/\/github.com\/tensorflow\/tensorflow\/issues\/7737\nmerged_summary_op = tf.summary.merge_all()","3fe7cc8f":"print(\"learning_rate\\t\\t=\\t\" + str(learning_rate))\nprint(\"training_iteration\\t=\\t\" + str(training_iteration))\nprint(\"batch_size\\t\\t=\\t\" + str(batch_size))\nprint(\"display_step\\t\\t=\\t\" + str(display_step))","c384248f":"# Start training by launching a session that executes the data flow graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Set the logs writer to the folder \/tmp\/tensorflow_logs\n    # This is for all the visualizations later\n    # https:\/\/stackoverflow.com\/a\/41483033\/5411712\n    summary_writer = tf.summary.FileWriter('.\/logs', graph_def=sess.graph_def)\n    \n    # Training cycle\n    for i in range(training_iteration):\n        avg_cost = 0.0 # prints out periodically to make sure model is \"improving\" ... goal is to minimize cost\n        total_batch = int(mnist.train.num_examples \/ batch_size)\n        # loop over all batches\n        for b in range(total_batch): # for each example in training data\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # fit training using batch data\n            # `optimizer` is Gradient Descent; used for 'backpropagation'\n            sess.run(optimizer, feed_dict={x:batch_xs, y:batch_ys})\n            # compute the average loss\n            avg_cost += sess.run(cost_function, feed_dict={x:batch_xs, y:batch_ys})\/total_batch\n            # write logs for each iteration\n            summary_str = sess.run(merged_summary_op, feed_dict={x:batch_xs, y:batch_ys})\n            summary_writer.add_summary(summary_str, i * total_batch + b)\n                                            # why `i * total_batch + b` ??? idk.\n        # Display logs per iteration step\n        if (i % display_step == 0):\n            print(\"iteration:\", '%04d' % (i+1), \"avg_cost=\", \"{:9f}\".format(avg_cost))\n\n    print(\"\\nTraining completed!\\n\")\n\n    # Test the model\n    # remember 'y' is the prediction variable\n    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))","80e0d857":"# optionally run the command in the notebook itself by uncommenting the line below\n#!tensorboard --logdir=.\/logs","535c5d90":"# Gradient Descent:\n![Imgur](https:\/\/i.imgur.com\/i6WW4gH.png)","e335c8b9":"# Future Learning\n* What is PyTorch and how does it compare to Tensorflow?\n   * https:\/\/www.youtube.com\/watch?v=nbJ-2G2GXL0\n     * Would PyTorch reduce the need to define \"placeholders\" because that was, frankly, weird to see in a language like python?\n* [But what *is* a Neural Network?](https:\/\/youtu.be\/aircAruvnKk)\n  * [and how do they learn?](https:\/\/youtu.be\/IHZwWFHWa-w) \n  * [and what is backprop really doing?](https:\/\/youtu.be\/Ilg3gGewQ5U)\n  * [and how does backprop use calculus?](https:\/\/youtu.be\/tIeHLnjs5U8)\n* How can the accuracy found above ```0.9254``` be improved to closer to ```0.95``` or ```0.99```?\n   * To what extend does changing the ```learning_rate``` or ```training_iteration``` or ```batch_size``` affect the accuracy? \n     * I dont think batch_size should have any affect. \n     * with ```training_iteration=30``` and ```learning_rate=0.01``` the algorithm ran in less than a few minutes and achieved ```0.9254```. Perhaps allowing it to train for several hours would boost the accuracy?\n       * [relevant quora question](http:\/\/qr.ae\/TUGJid)\n* How long would it take a human toddler to \"classify\" digits (0-9)? An hour or two? maybe less? Of course you would need to hold their attention to the task, haha! \n* What would happen to the accuracy if I modified the test data or the training data to include **random noise** or even attempt the [**one pixel attack**](https:\/\/arxiv.org\/abs\/1710.08864)\n  * [video about one pixel attack](https:\/\/youtu.be\/SA4YEAWVpbk)\n>   \"Now, note that this also means that we have to be able to look into the neural network and have access to the confidence values.\" - [K\u00e1roly Zsolnai-Feh\u00e9r](https:\/\/youtu.be\/SA4YEAWVpbk?t=155)\n  *  Would a method for reducing NerualNet accuracy that only sees output classes, *without accuracy values*, be analogous to humans discovering optical illusions? haha! \ud83d\ude02\n  * [it seems some researchers have tried to trick AI that learned on MNIST data](https:\/\/arxiv.org\/pdf\/1801.02612.pdf)\n  * [another one](https:\/\/arxiv.org\/pdf\/1608.04644.pdf)\n  * [and another one](https:\/\/arxiv.org\/pdf\/1807.10335.pdf)\n  * [and more](https:\/\/www.google.com\/search?safe=off&q=one+pixel+attack+\"mnist\")\n* What would happen to the accuracy if I changed **```tf.nn.softmax```** to **```relu```** or even **```sigmoid```** or **```tanh```**?\n  * [learn more about activation functions](https:\/\/youtu.be\/-7scQpJT7uo)","17e593b6":"# set \"hyperparameters\" (knobs & dials)","4fcad138":"# [MNIST Dataset Source](http:\/\/yann.lecun.com\/exdb\/mnist\/)\n# [I am following this YouTube tutorial](https:\/\/www.youtube.com\/watch?v=2FmcHiLCwTU&vl=en)\n\n### Goal \n* Build a classifier that can look at a 28x28 image of a handwritten digit and classify the digit (0-9).\n  * the \"Hello World\" of Deep Learning\n* Personal goals: \n  * understand Tensorflow's python wrapper & Tensorflow a little bit better\n  * understand neural networks a little bit better\n  * understand some basics of machine learning a little bit better\n\n### Tools\n* Tensorflow\n* Python","8d80c01f":"# Viewing all the summaries in ***Tensorboard***\n##### this should be done locally so make sure to **download** the ```kernal.ipynb``` file and run ```tensorboard --logdir=.\/logs``` in the command line \n* ***Note:*** ```pip install tensorflow``` may be required to import tensorflow","46551a41":"# Logistic Regression:\n![Imgur](https:\/\/i.imgur.com\/rrkOONc.png)","211b43fb":"#### main graph\n<img src=\"https:\/\/i.imgur.com\/f8LgApJ.png\" width=\"400\">\n#### tensorboard_auxilary_nodes\n![tensorboard_auxilary_nodes](https:\/\/i.imgur.com\/ZABjzeR.png)\n#### tensorboard_cost_function\n![tensorboard_cost_function](https:\/\/i.imgur.com\/yTCklib.png)\n#### tensorboard_biases_distribution\n![tensorboard_biases_distribution](https:\/\/i.imgur.com\/iyZupnI.png)\n#### tensorboard_weights_distribution\n![tensorboard_weights_distribution](https:\/\/i.imgur.com\/DxgMGZt.png)\n#### tensorboard_biases_histogram\n![tensorboard_biases_histogram](https:\/\/i.imgur.com\/Af06kgc.png)\n#### tensorboard_weights_histogram\n![tensorboard_weights_histogram](https:\/\/i.imgur.com\/wcbcIdy.png)","36437a6f":"## Notice that the ```avg_cost``` values decrease with each logged iteration. This means that the gradient descent algorithm is minimizing the cost function. \n### I suppose if we ran the code with ```training_iteration``` set to a larger number then we would expect to see little to no improvement on the accuracy since the ```avg_cost``` seems to level off at around 18.","259b9b99":"## notes\n#### tensorflow\n* Tensorflow \"model\" = \"data flow graph\"\n* Graph has nodes called \"operations\"\n  * basic units of math (e.g: addition, multiplication, fancy-schmancy-multivar-calculus, etc)\n  * input: tensor\n  * output: tensor\n* tensor = multidimensional arrays (matrices)\n\n#### conventions\n* x = feature vector \/ the thing(s) that help us do the prediction\n* y = \"output classes\" \/ the thing we want to predict\n* \"**placeholder**\" = a variable that will have data assigned to it later","424d3a9a":"# An image is represented as a matrix of pixel values:\n![Imgur](https:\/\/i.imgur.com\/XYyI1ha.png)\n\n# It gets flattened into a 1D array to be used as the feature vector:\n![Imgur](https:\/\/i.imgur.com\/d9ZvYPV.png)","94677764":"## Learning Rate:\n![learning_rate](https:\/\/i.imgur.com\/3L1qbdT.png)","a9cca858":"# Training","b9882f7c":"# This is what the data looks like:\n![mnist_data](https:\/\/i.imgur.com\/mKstG9R.png)"}}