{"cell_type":{"bf6e4ba6":"code","8801190b":"code","1a36d78e":"code","73a455d1":"code","9be6c04c":"code","d3c7801a":"code","56ae2ae5":"code","a0379be1":"code","ffd65f7e":"code","b2b62351":"code","3d6de70f":"code","021ee56c":"code","d585cb23":"code","7bcabcde":"code","1dbd9d8f":"code","982bce4c":"code","351b4f79":"code","6964db3c":"code","eca9b06c":"code","f63b4209":"markdown","36a60f2e":"markdown","427ebaa9":"markdown","5c761267":"markdown","db15700e":"markdown"},"source":{"bf6e4ba6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wordcloud\n%matplotlib inline\nsns.set()\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8801190b":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n# sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\ntrain[train['target']==0]","1a36d78e":"train['keyword'] = train['keyword'].fillna('')\ntrain['location'] = train['location'].fillna('no_location')\n\ntest['keyword'] = test['keyword'].fillna('')\ntest['location'] = test['location'].fillna('no_location')","73a455d1":"STOPWORDS = set(stopwords.words('english'))","9be6c04c":"fig, axes = plt.subplots( figsize=(8, 4), dpi=100)\nplt.tight_layout()\nsns.countplot(x=train['target'], hue=train['target'], ax=axes)","d3c7801a":"words = \" \".join(text for text in train['text'] if text not in STOPWORDS)\n\nwc = wordcloud.WordCloud().generate(words)\n\nplt.figure(figsize=[12,6])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","56ae2ae5":"train.keyword.unique()","a0379be1":"train['keyword'] = train['keyword'].str.replace('buildings%20burning','burning%20buildings')\ntrain['keyword'] = train['keyword'].str.replace('\\%20',' ')\n\nfrom nltk.stem.porter import *\n\nstemmer = PorterStemmer()\n\ntrain['keyword'] = train['keyword'].apply(lambda x: stemmer.stem(x))\n\ntrain.keyword.unique()","ffd65f7e":"keywords = \" \".join(text for text in train['keyword'] if text not in STOPWORDS)\n\nwc = wordcloud.WordCloud().generate(keywords)\n\nplt.figure(figsize=[12,6])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","b2b62351":"train['text'] = train['keyword'] +\" \"+ train['text']\ntest['text'] = test['keyword'] +\" \"+ test['text']\n\nY = train['target']\nX = train['text']\ntest_id = test['id']\nX_test = test['text']\n\ndel train, test\ndel keywords, words\ngc.collect()","3d6de70f":"# MODEL = 'roberta-base'","021ee56c":"# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# # Tokenize input\n# text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n# tokenized_text = tokenizer.tokenize(text)\n# tokenized_text","d585cb23":"# config = AutoConfig.from_pretrained(MODEL)\n# model = AutoModel.from_pretrained(MODEL, config=config)\n\n# # model.eval()","7bcabcde":"from transformers import pipeline\n\nnlp = pipeline(\"sentiment-analysis\")","1dbd9d8f":"pred = []\nfor t in X_test:\n    pred.append(nlp(t))\n    \n","982bce4c":"pred[0]","351b4f79":"X_test.loc[0]","6964db3c":"predict = []\nfor p in pred:\n    predict.append(1 if p[0]['label'] == 'NEGATIVE' else 0)","eca9b06c":"df = pd.DataFrame({'id':test_id, 'target':predict})\ndf.to_csv('nlp.csv', index=False)\ndf.head()","f63b4209":"# 3. EDA","36a60f2e":"# 4 Hugging Face","427ebaa9":"# 1. Import libraries and data","5c761267":"# 2. Preprocessing","db15700e":"# 5. Predict and send submission"}}