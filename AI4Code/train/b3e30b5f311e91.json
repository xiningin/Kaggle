{"cell_type":{"dd68a6ca":"code","6c15dc35":"code","74984221":"code","18e9dbf8":"code","c515fe6d":"code","f8b9d5eb":"code","75022915":"code","bf10146a":"code","a7b49db8":"code","26cde0a3":"code","ce5a93f6":"code","8a3da844":"code","851953d0":"code","a5186464":"code","ffecbc70":"code","f852df50":"code","fe38b83f":"code","d55a871f":"code","e0eac049":"code","0546bb7c":"code","47ec310a":"code","fdc419e2":"code","ccea4ffd":"code","74630de3":"code","52610579":"code","09317010":"code","e8f7ac41":"code","5ed2afd1":"code","094ac2bd":"code","de7a5c65":"code","69ac1b3c":"code","e25a97b2":"code","2c4d4bea":"code","c24a4619":"code","3cdf234f":"code","92388305":"markdown","56b914dc":"markdown","076c3cd1":"markdown","95218163":"markdown","4d7a4385":"markdown","c5c5f4b5":"markdown","82e1823e":"markdown","f67c465b":"markdown","2b7ebb42":"markdown","209ed228":"markdown","12b2b50d":"markdown","73b20587":"markdown"},"source":{"dd68a6ca":"import matplotlib.pyplot as plt\nimport numpy as np\nimport glob\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB3, MobileNetV2, InceptionV3\nfrom tensorflow.keras.optimizers import Adam","6c15dc35":"img_data = '..\/input\/gtzan-dataset-music-genre-classification\/Data\/images_original\/'","74984221":"BATCH_SIZE=8\nTARGET_SIZE=224 # Based on EfficientNetB0\nNUM_CLASSES=10","18e9dbf8":"train_ds = image_dataset_from_directory(\n  img_data,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(TARGET_SIZE, TARGET_SIZE),\n  batch_size=BATCH_SIZE)","c515fe6d":"val_ds = image_dataset_from_directory(\n  img_data,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(TARGET_SIZE, TARGET_SIZE),\n  batch_size=BATCH_SIZE)","f8b9d5eb":"class_names = train_ds.class_names\nprint(class_names)","75022915":"plt.figure(figsize=(20, 20))\nfor images, labels in train_ds.take(1):\n    for i in range(8):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","bf10146a":"model_save = tf.keras.callbacks.ModelCheckpoint('.\/best_weights.h5', \n                             save_best_only = True, \n                             save_weights_only = True,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 10, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'min', verbose = 1)","a7b49db8":"def plot_hist(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n    plt.grid()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.grid()\n    plt.show()","26cde0a3":"model = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(TARGET_SIZE, TARGET_SIZE, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(NUM_CLASSES)\n])","ce5a93f6":"model.compile(optimizer=Adam(lr = 0.001),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","8a3da844":"model.summary()\n","851953d0":"epochs=30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","a5186464":"plot_hist(history)","ffecbc70":"model = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(TARGET_SIZE, TARGET_SIZE, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(NUM_CLASSES)\n])","f852df50":"model.compile(optimizer=Adam(lr = 0.001),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","fe38b83f":"model.summary()","d55a871f":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","e0eac049":"plot_hist(history)","0546bb7c":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = None,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 3))\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(NUM_CLASSES, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n    return model\nmodel = create_model()\nmodel.summary()","47ec310a":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","fdc419e2":"plot_hist(history)","ccea4ffd":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = \"imagenet\", drop_connect_rate=0.6,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 3))\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(NUM_CLASSES, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n    return model\nmodel = create_model()\nmodel.summary()","74630de3":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","52610579":"plot_hist(history)","09317010":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = \"imagenet\", drop_connect_rate=0.6,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 3))\n    # Freeze pre-trained layers\n    conv_base.trainable = False\n    \n    # Re-build top layers\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.BatchNormalization()(model)\n    \n    dropout_rate=0.2\n    model = layers.Dropout(dropout_rate, name=\"top_dropout\")(model)\n    model = layers.Dense(NUM_CLASSES, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.01),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n    return model\n\nmodel = create_model()\nmodel.summary()","e8f7ac41":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","5ed2afd1":"plot_hist(history)","094ac2bd":"weights_path = '.\/first_finetune_weights.h5'\nmodel.save_weights(weights_path)","de7a5c65":"NUM_UNFREEZE_LAYERS = 100\n\ncont_model = tf.keras.models.clone_model(model)\ncont_model.load_weights(weights_path)\n\ndef unfreeze_model(model):\n    # We unfreeze the top NUM_UNFREEZE_LAYERS layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-NUM_UNFREEZE_LAYERS:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", \n        metrics=[\"accuracy\"]\n    )\n\n\nunfreeze_model(cont_model)\ncont_model.summary()","69ac1b3c":"epochs = 30  # @param {type: \"slider\", min:8, max:50}\nhistory = cont_model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","e25a97b2":"plot_hist(history)","2c4d4bea":"cont_model = tf.keras.models.clone_model(model)\ncont_model.load_weights(weights_path)\n\ndef unfreeze_whole_model(model):\n    # We unfreeze the whole layers while leaving BatchNorm layers frozen\n    for layer in model.layers:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", \n        metrics=[\"accuracy\"]\n    )\n\n\nunfreeze_whole_model(cont_model)\ncont_model.summary()","c24a4619":"epochs = 30  # @param {type: \"slider\", min:8, max:50}\nhistory = cont_model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","3cdf234f":"plot_hist(history)","92388305":"## Transfer learning - EfficientNet (the usual way of transfer learning)","56b914dc":"# GTZAN - Baseline CNN and Transfer learning","076c3cd1":"## Modeling","95218163":"## CNN with Dropout","4d7a4385":"## Data Loader","c5c5f4b5":"## Transfer learning - EfficientNet (mostly retrain)","82e1823e":"## Observations\n- Results:\n    - Baseline CNN - 0.5 val\n    - CNN with Dropout - 0.6 val\n    - EfficientNetB0 transfer learning - 0.78 val\n- All models show overfitting, this may be due to the lack of training data for each class.\n- Ordinary data augmentation may not be feasible for song data like GTZAN, because:\n    - Cannot use typical transformations like rotation, zoom, flipping because spectrogram would be non-sense\n    - Cannot use audio transformation because this will distort the original song.\n    - Solution: Research specific methods of data augmentation for song data.\n- Why usual way of transfer learning does not perform better than fine-tuning the whole EfficientNet model?","f67c465b":"### Reference\n- https:\/\/www.tensorflow.org\/tutorials\/images\/classification\n- https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/","2b7ebb42":"### Unfreeze all the layers","209ed228":"## EfficientNet train from scratch","12b2b50d":"## Callbacks and Helper Functions","73b20587":"### Unfreeze some of the layers"}}