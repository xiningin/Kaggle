{"cell_type":{"3eea9ce5":"code","fe4bac72":"code","4431c56e":"code","1ba47fe2":"code","b328dfa2":"code","65797bc7":"code","b7432e77":"code","3609e0e5":"code","a9249420":"code","62af3bae":"code","4ac3190a":"code","c88a8d57":"code","cc52f402":"code","7f5cdf3c":"code","93d81183":"code","7ea9fabd":"code","6c176081":"code","cbfcdcdb":"code","76df3aee":"code","8f7bf225":"code","e7d53ea6":"code","5610b4d9":"code","aa2650d5":"code","8f609ea8":"code","9b365eb8":"code","83b69951":"code","068b46a0":"code","e504bddb":"code","31970303":"code","69a7f2b5":"code","2a8d6081":"code","efdb41cc":"code","100b30b6":"code","4b929758":"code","c101cbce":"code","3fc3ed3f":"code","22c7365f":"code","c7f5d9bd":"code","71f945be":"code","3d4effff":"code","cd86b484":"code","94f0bcd5":"code","926ed41d":"markdown","b23ca13b":"markdown","b47e6e09":"markdown","dc5c5aac":"markdown","43f790d9":"markdown","0fd6d933":"markdown","fd5989d1":"markdown","c0b51056":"markdown","ea50e020":"markdown","f80f7197":"markdown","64e109dd":"markdown","26a54c6f":"markdown","056c0b5a":"markdown","0f09337e":"markdown","ebf58960":"markdown","2bb51f3e":"markdown","a06b667c":"markdown","0e1e10d9":"markdown","490c3c8b":"markdown","69690956":"markdown","3969a54c":"markdown","1db46bf0":"markdown","52a9352f":"markdown","270aa925":"markdown","63e59c95":"markdown","7f4de70d":"markdown","549bf84b":"markdown","52ddce48":"markdown","ec84b60b":"markdown","c0235ea7":"markdown","8d9170ad":"markdown","e1ecd87b":"markdown","01126317":"markdown","97923b58":"markdown","4163b177":"markdown","840d49d5":"markdown","859b68e2":"markdown","102b2537":"markdown","db94c2aa":"markdown","b7dcef9f":"markdown","37ad2e36":"markdown","3ad44ccb":"markdown","286d4370":"markdown","c31d191e":"markdown","f5805aaa":"markdown","2bc630a6":"markdown","de8034a5":"markdown","71cacfde":"markdown","da51d934":"markdown","70daf0ed":"markdown","2d8e2823":"markdown","c5e6a5a2":"markdown","900d4fd3":"markdown","d76cdef8":"markdown","3b2eba49":"markdown","854625ca":"markdown","ed7103a7":"markdown","1f0e5461":"markdown","d2a84d09":"markdown","8ef9fba1":"markdown","be6713b0":"markdown","9cbcbbbe":"markdown","5cfb5575":"markdown","43ec9c93":"markdown","dd40121a":"markdown","8f64f0fc":"markdown","031c6b03":"markdown","22044db8":"markdown","9eef2779":"markdown","c7ec3dfa":"markdown","e55799e2":"markdown","f6677382":"markdown","e5f07a98":"markdown","596c4250":"markdown"},"source":{"3eea9ce5":"# Pandas is a software library written for the Python programming language for data manipulation and analysis.\nimport pandas as pd\n# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\nimport numpy as np\n# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\nimport matplotlib.pyplot as plt\n#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\nimport seaborn as sns\n# Preprocessing allows us to standarsize our data\nfrom sklearn import preprocessing\n# Allows us to split our data into training and testing data\nfrom sklearn.model_selection import train_test_split\n# Allows us to test parameters of classification algorithms and find the best one\n#from sklearn.model_selection import GridSearchCV  ## We did not use\n# Allows us to test accuracy by jaccard_score\nfrom sklearn.metrics import jaccard_score\n# Allows us to see the confusion matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n# Allows us to see the classification_report or F1 Score\nfrom sklearn.metrics import classification_report\n# Allows us to see the log loss score\nfrom sklearn.metrics import log_loss\n# Logistic Regression classification algorithm\nfrom sklearn.linear_model import LogisticRegression\n# Support Vector Machine classification algorithm\nfrom sklearn.svm import SVC\nfrom sklearn import svm\n# Decision Tree classification algorithm\nfrom sklearn.tree import DecisionTreeClassifier\n# K Nearest Neighbors classification algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import Library for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier","fe4bac72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4431c56e":"data=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","1ba47fe2":"print('1. DATA SHAPE: ', data.shape)\nprint('2. DATA HEAD: '\"\\n\",  data.head(5))\nprint('3. DATA TAIL: '\"\\n\",  data.tail(5))","b328dfa2":"data.info()","65797bc7":"data.isnull().sum()","b7432e77":"pd.set_option('precision',2)\ndata.describe()","3609e0e5":"Total_transactions = len(data)\nsecured = len(data[data.Class == 0])\nfraud = len(data[data.Class == 1])\nfraud_percentage = round(fraud\/secured*100, 2)\nprint('Total number of Transaction are:', Total_transactions)\nprint('Total number of Secured Transaction are:', secured)\nprint('Total number of Fraud Transaction are:', fraud)\nprint('Percentage of Fraud Transaction are:', fraud_percentage)","a9249420":"X = data.loc[:, data.columns != 'Class']\nX[0:5]","62af3bae":"x_scaled = preprocessing.scale(X)\nX = pd.DataFrame(x_scaled)\nX","4ac3190a":"y = data[\"Class\"].to_numpy()\ny[0:5]","c88a8d57":"X = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","cc52f402":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=2)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","7f5cdf3c":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","93d81183":"yhat = LR.predict(X_test)\nyhat","7ea9fabd":"yhat_prob = LR.predict_proba(X_test)\nyhat_prob","6c176081":"LR_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\n\nLR_AS","cbfcdcdb":"#yhat=logreg_cv.predict(X_test)\ncm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","76df3aee":"print (classification_report(y_test, yhat))","8f7bf225":"fraudTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nfraudTree # it shows the default parameters","e7d53ea6":"fraudTree.fit(X_train,y_train)","5610b4d9":"predTree = fraudTree.predict(X_test)","aa2650d5":"print (predTree [0:5])\nprint (y_test [0:5])","8f609ea8":"DT_AS = round(jaccard_score(y_test, predTree,pos_label=0)*100,2)\n\nDT_AS","9b365eb8":"cm = confusion_matrix(y_test, predTree, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","83b69951":"print (classification_report(y_test, predTree))","068b46a0":"k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","e504bddb":"yhat = neigh.predict(X_test)\nyhat[0:5]","31970303":"KNN_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\n\nKNN_AS","69a7f2b5":"cm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","2a8d6081":"print (classification_report(y_test, yhat))","efdb41cc":"clf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) ","100b30b6":"yhat = clf.predict(X_test)\nyhat [0:5]","4b929758":"SVM_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\n\nSVM_AS","c101cbce":"cm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","3fc3ed3f":"print (classification_report(y_test, yhat))","22c7365f":"RF = RandomForestClassifier()\n\n#Train the model using Training Dataset\nRF.fit(X_train, y_train)","c7f5d9bd":"yhat = RF.predict(X_test)\nyhat [0:5]","71f945be":"RF_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\nRF_AS\n","3d4effff":"cm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","cd86b484":"print (classification_report(y_test, yhat))","94f0bcd5":"BPM = pd.DataFrame({\n    'Models': ['Logistic Regression', 'Decision Trees','K-Nearest-neighbours (KNN)', \n               'Support Vector Machine (SVM)', 'Random Forest'],\n    'Accuracy_Score': [LR_AS, DT_AS, KNN_AS, SVM_AS, RF_AS]})\n\nBPM.sort_values(by='Accuracy_Score', ascending=False)","926ed41d":"#### Accuracy Evaluation by Confusion Matrix","b23ca13b":"#### Checking F1 Score","b47e6e09":"#### Checking F1 Score","dc5c5aac":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","43f790d9":"You can print out <b>predTree<\/b> and <b>y_test<\/b> if you want to visually compare the predictions to the actual values.","0fd6d933":"#### Accuracy Evaluation by Jaccard Index","fd5989d1":"## Conclusion","c0b51056":"**C** parameter indicates **inverse of regularization strength** which must be a positive float.","ea50e020":"*   We found **0.17%** Fraud Transaction were occured.\n*   We found **99.96%** accuracy score for **Random Forest**. \n ","f80f7197":"Now we can predict using our test set:","64e109dd":"#### Checking transaction distribution","26a54c6f":"#### Accuracy Evaluation by Confusion Matrix","056c0b5a":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","0f09337e":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56873 true positive and 5 false positive result. That explains, out of 56873+5= 56878, we have 56873 successfully classified normal transaction and 5 were falsely classified as normal but they were fraudlent.","ebf58960":"#### Accuracy Evaluation by Confusion Matrix","2bb51f3e":"Let's start with Radial Basis Function (RBF) kernel.","a06b667c":"**predict_proba**  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X):","0e1e10d9":"we can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","490c3c8b":"Next, we will fit the data with the training feature matrix <b> X_trainset <\/b> and training  response vector <b> y_trainset <\/b>","69690956":"## Import Libraries and Define Auxiliary Functions","3969a54c":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","1db46bf0":"## Best Performed Method","52a9352f":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","270aa925":"### Logistic Regression with Scikit-learn","63e59c95":"## Spliting Data into Train and Test Set","7f4de70d":"# Thank you very much for reading this NoteBook. Please share your comments and suggestions for improving more.","549bf84b":"#### We will import the following libraries for this project","52ddce48":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56867 true positive and 11 false positive result. That explains, out of 56867+11= 56878, we have 56867 successfully classified normal transaction and 11 were falsely classified as normal but they were fraudlent.","ec84b60b":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","c0235ea7":"### Support Vector Machine with Scikit-learn","8d9170ad":"#### Accuracy Evaluation by Jaccard Index","e1ecd87b":"We can use the model to make predictions on the test set:","01126317":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","97923b58":"#### Accuracy Evaluation by Jaccard Index","4163b177":"## Explore the dataset","840d49d5":"#### Shape of the data, head and tail","859b68e2":"#### Check the types of the data","102b2537":"#### Independent Variables","db94c2aa":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","b7dcef9f":"Initialize the Random Forest","37ad2e36":"Let's make some <b>predictions<\/b> on the testing dataset and store it into a variable called <b>predTree<\/b>.","3ad44ccb":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","286d4370":"We will first create an instance of the **DecisionTreeClassifier** called **fraudTree**.\n    Inside of the classifier, specify  *criterion=\"entropy\"* so we can see the information gain of each node.","c31d191e":"#### Dependent Variable","f5805aaa":"#### Checking missing values","2bc630a6":"Perform exploratory  Data Analysis and determine Training Labels\n\n*   Create a column for the class\n*   Standardize the data\n*   Split into training data and test data\n\n\\-Find best Hyperparameter for **Logistic Regression**, **Decision Trees**, **K-Nearest-neighbours (KNN)**, **Support Vector Machine (SVM)**, **Random Forest**, and \n\n*   Find the method performs best using test data\n","de8034a5":"#### Checking F1 Score","71cacfde":"### Decision Trees with Scikit-learn","da51d934":"After being fitted, the model can then be used to predict new values:","70daf0ed":"#### Standardize the data in X ","2d8e2823":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","c5e6a5a2":"Let's start the training algorithm with k=4 for now:","900d4fd3":"#### Load the data","d76cdef8":"# Objectives","3b2eba49":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56875 true positive and 3 false positive result. That explains, out of 56875+3= 56878, we have 56875 successfully classified normal transaction and 3 were falsely classified as normal but they were fraudlent.","854625ca":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56874 true positive and 4 false positive result. That explains, out of 56874+4= 56878, we have 56874 successfully classified normal transaction and 4 were falsely classified as normal but they were fraudlent.","ed7103a7":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","1f0e5461":"#### Accuracy Evaluation by Jaccard Index","d2a84d09":"#### Now, we normalize the dataset","8ef9fba1":"## Modeling","be6713b0":"#### Describe the summary statistics","9cbcbbbe":"### Random Forest with Scikit-learn","5cfb5575":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","43ec9c93":"#### Accuracy Evaluation by Confusion Matrix","dd40121a":"#### Checking F1 Score","8f64f0fc":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56870 true positive and 8 false positive result. That explains, out of 56870+8= 56878, we have 56870 successfully classified normal transaction and 8 were falsely classified as normal but they were fraudlent.","031c6b03":"#### Accuracy Evaluation by Confusion Matrix","22044db8":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","9eef2779":"### K-Nearest-neighbours with Scikit-learn","c7ec3dfa":"## Load the dataframe","e55799e2":"#### Checking F1 Score","f6677382":"#### Accuracy Evaluation by Jaccard Index","e5f07a98":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","596c4250":"After being fitted, the model can then be used to predict new values:"}}