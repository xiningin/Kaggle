{"cell_type":{"8bd6bf4d":"code","7cb40b84":"code","4ccce027":"code","d586b236":"code","2c4e4fc3":"code","5e15e820":"code","37a913ca":"code","46a250fb":"code","c5822f15":"code","7241a7ce":"code","e7af7dff":"code","deede97a":"code","612dcbf4":"markdown","8335815e":"markdown","5c7fc031":"markdown","137b3cc3":"markdown","334bca1a":"markdown","0e3b3b78":"markdown","01951b42":"markdown","ce735fd8":"markdown","53da28d5":"markdown","6cd303ef":"markdown"},"source":{"8bd6bf4d":"# Install libraries\n!pip install '..\/input\/pawpularset\/Keras_Applications-1.0.8-py3-none-any.whl'\n!pip install '..\/input\/pawpularset\/efficientnet-1.1.1-py3-none-any.whl'\n\n# Import libraries\nimport gc\nimport numpy as np\nimport pandas as pd\nimport random\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.layers import Input","7cb40b84":"# Constants\nIMG_SIZE = 384\nCHANNELS = 3\nBATCH_SIZE = 16\nQ = 30\nEPOCHS = 10\nFOLDS = 6\nFEATURE_FOLDS = 10\nSEED = 4261\nVERBOSE = 1\nLR = 0.000005\n\n# Logic...\nTRAIN_FEATURE_MODEL = True\n\n# Folders\nDATA_DIR = '..\/input\/petfinder-pawpularity-score\/'\nTRAIN_DIR = DATA_DIR + 'train\/'\nTEST_DIR = DATA_DIR + 'test\/'","4ccce027":"# Configure Strategy. Assume TPU...if not set default for GPU\/CPU\ntpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    # Enable XLA\n    tf.config.optimizer.set_jit(enabled = \"autoclustering\")\n    strategy = tf.distribute.get_strategy()\n    \n# Set Auto Tune\nAUTOTUNE = tf.data.experimental.AUTOTUNE   ","d586b236":"# Load Train Data\ntrain_df = pd.read_csv(f'{DATA_DIR}train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n\n# Set a specific label to be able to perform stratification\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = Q, labels = range(Q))\n\n# Label value to be used for feature model 'classification' training.\ntrain_df['target_value'] = train_df['Pawpularity'] \/ 100.\n\n# Summary\nprint(f'train_df: {train_df.shape}')\ntrain_df.head()","2c4e4fc3":"# Load Test Data\ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\n\n# Summary\nprint(f'test_df: {test_df.shape}')\ntest_df.head()","5e15e820":"def build_augmenter(is_labelled):\n    def augment(img):\n        # Only use basic augmentations...too much augmentation hurts performance\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        \n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        # Read Image\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = CHANNELS)\n        \n        # Normalize and Resize\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        \n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, batch_size = 32, is_labelled = False, augment = False, repeat = False, shuffle = False):\n    decode_fn = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    \n    return dataset","37a913ca":"# Set Callbacks\ndef model_checkpoint(fold):\n    return tf.keras.callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                              verbose = 1, \n                                              monitor = 'val_rmse', \n                                              mode = 'min', \n                                              save_weights_only = True,\n                                              save_best_only = True)\n\ndef unfreeze_model(model):\n    # Unfreeze layers while leaving BatchNorm layers frozen\n    for layer in model.layers:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = True\n        else:\n            layer.trainable = False\n\ndef create_model(): \n    # Create and Compile Model and show Summary\n    effnet_model = efn.EfficientNetB2(include_top = False, \n                                      classes = None, \n                                      input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS), \n                                      weights = '..\/input\/pawpularset\/efficientnet-b2_noisy-student_notop.h5', \n                                      pooling = 'avg')\n\n    # Set all layers to Trainable except BN layers\n    unfreeze_model(effnet_model)\n    \n    X = tf.keras.layers.Dropout(0.25)(effnet_model.output)\n    output = tf.keras.layers.Dense(1, activation = 'sigmoid')(X)\n    \n    # Create Final Model\n    model = tf.keras.Model(inputs = effnet_model.input, outputs = output)\n\n    # Compile\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR), \n                  loss = tf.keras.losses.BinaryCrossentropy(), \n                  metrics = [tf.keras.metrics.RootMeanSquaredError('rmse')])        \n    \n    return model\n\n## EfficientNet Feature Model Training","46a250fb":"if TRAIN_FEATURE_MODEL:\n    # OOF RMSE Placeholder\n    all_val_rmse = []\n\n    # Stratified Training\n    kfold = StratifiedKFold(n_splits = FEATURE_FOLDS, shuffle = True, random_state = SEED)\n    for fold, (train_index, val_index) in enumerate(kfold.split(train_df.index, train_df['stratify_label'])):\n        print(f'\\n===== Fold {fold}\\n')\n\n        # Pre model.fit cleanup\n        tf.keras.backend.clear_session()\n        gc.collect()\n\n        # Create Model\n        model = create_model()\n\n        # Create TF Datasets\n        trn = train_df.iloc[train_index]\n        val = train_df.iloc[val_index]\n        training_dataset = create_dataset(trn, batch_size = BATCH_SIZE, is_labelled = True, augment = True, repeat = True, shuffle = True)\n        validation_dataset = create_dataset(val, batch_size = BATCH_SIZE, is_labelled = True, augment = False, repeat = True, shuffle = False)\n\n        # Fit Model\n        history = model.fit(training_dataset,\n                            epochs = EPOCHS,\n                            steps_per_epoch = trn.shape[0] \/\/ BATCH_SIZE,\n                            validation_steps = val.shape[0] \/\/ BATCH_SIZE,\n                            callbacks = [model_checkpoint(fold)],\n                            validation_data = validation_dataset,\n                            verbose = 1)   \n\n        # Validation Information\n        best_val_rmse = min(history.history['val_rmse'])\n        all_val_rmse.append(best_val_rmse)\n        print(f'\\nValidation RMSE: {best_val_rmse}\\n')\n\n    # Summary\n    print(f'Final Mean RMSE for {FEATURE_FOLDS} Fold CV Training: {np.mean(all_val_rmse)}')","c5822f15":"# Placeholders\npreds_final = np.zeros((test_df.shape[0], 1))\nall_oof_score = []\n\n# Stratification and Label values\nY_strat = train_df['stratify_label'].values\nY_pawpularity = train_df['Pawpularity'].values","7241a7ce":"# Loop through all Feature Extraction Models\nfor fold_index in range(FEATURE_FOLDS):\n    print('\\n\\n====================================================================================================')\n    print(f'===== Run for Feature Model {fold_index} ======================================================================\\n')\n\n    # Pre model.fit cleanup\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Create Model\n    model = create_model()\n    \n    # Load Weights...Use the provided weight files...or modify for your own set.\n    #model.load_weights(f'..\/input\/pawpularset\/feature_model_{fold_index}.h5')\n    # Use as below when TRAIN_FEATURE_MODEL = True\n    model.load_weights(f'feature_model_{fold_index}.h5')\n    \n    # Strip Last layers to be able to extract features\n    model = tf.keras.Model(inputs = model.input, outputs = model.layers[-3].output)\n    \n    # Summary...only on first load\n    if fold_index == 0: print(model.summary())        \n        \n    # Feature Extraction\n    print('\\n===== Extracting Features')\n    cb_train_set = create_dataset(train_df, batch_size = BATCH_SIZE, is_labelled = True, augment = False, repeat = False, shuffle = False)\n    cb_test_set = create_dataset(test_df, batch_size = BATCH_SIZE, is_labelled = False, augment = False, repeat = False, shuffle = False)\n    cb_train_features = model.predict(cb_train_set, verbose = VERBOSE)\n    cb_test_features = model.predict(cb_test_set, verbose = VERBOSE)\n    \n    print('\\n===== Feature Set Shapes')\n    print(f'Train Feature Set Shape: {cb_train_features.shape}')\n    print(f'Test Feature Set Shape: {cb_test_features.shape}')\n    \n    # Stratified Training for CatBoost\n    print(f'\\n===== Running CatBoost - SEED {SEED}')\n    \n    # Placeholders\n    oof_score = 0\n\n    kfold = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n    for idx, (train, val) in enumerate(kfold.split(cb_train_features, Y_strat)):\n        print(f'\\n===== CatBoost Fold {idx} ===============================================================================================')\n\n        train_x, train_y = cb_train_features[train], Y_pawpularity[train]\n        val_x, val_y = cb_train_features[val], Y_pawpularity[val]\n        \n        # Set CatBoost Parameters\n        cb_params = {'loss_function' : 'RMSE',\n                     'eval_metric' : 'RMSE',\n                     'iterations' : 1000,\n                     'grow_policy' : 'SymmetricTree',\n                     'depth' : 6,\n                     'l2_leaf_reg' : 2.0,\n                     'random_strength' : 1.0,\n                     'learning_rate' : 0.05,\n                     'task_type' : 'CPU',\n                     'devices' : '0',\n                     'verbose' : 0,\n                     'random_state': SEED}\n        \n        # Create and Fit CatBoost Model\n        cb_model = CatBoostRegressor(**cb_params)\n        cb_model.fit(train_x, train_y, eval_set = [(val_x, val_y)], early_stopping_rounds = 100, verbose = 250)\n\n        y_pred = cb_model.predict(val_x)\n        preds_final += np.array([cb_model.predict(cb_test_features)]).T\n\n        # Update OOF Score\n        oof_score += np.sqrt(mean_squared_error(val_y, y_pred))        \n\n        # Cleanup\n        del cb_model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()   \n    \n    # OOF Score for CatBoost run\n    oof_score \/= FOLDS\n    all_oof_score.append(oof_score)\n    print(f'CatBoost OOF Score: {oof_score}')\n    print('Test Predictions Cumulative...')\n    print(preds_final[:5])\n    \n    # Increase to improve randomness on the next feature model run\n    SEED += 1","e7af7dff":"# Final OOF score for All Feature Models\nprint(f'Final OOF RMSE Score for all feature models: {np.mean(all_oof_score)}')","deede97a":"preds_final \/= (FOLDS*FEATURE_FOLDS)\nsubmission_df = pd.read_csv(f'{DATA_DIR}sample_submission.csv')\nsubmission_df['Pawpularity'] = preds_final.ravel()\nsubmission_df.to_csv('submission.csv', index = False)\n\n# Summary\nsubmission_df.head(10)","612dcbf4":"In the last step I'am looping through and loading each of the 10 trained feature extraction models. For each of the individual feature extraction models we will extract the features for the training and test data. Next a complete 6 fold Cross Validation training run is performed with CatBoost.\n\nThis means that the final predictions are based on the average of 10 * 6 predictions for the PawPularity score. Not bad ;-)\n\nIf you do training of the feature models and running CatBoost in one go .. then modify the path from which the feature models are loaded.","8335815e":"Next we train a number of EfficientNet B2 models to be used as feature extractors. Training is only done for a few epochs with a low learning rate. Using the 'noisy-student' weights as baseline to finetune turned out to work a lot better than the 'imagenet' weights.\n\nA baseline set of fine-tuned models is available in the added dataset.","5c7fc031":"## Training data","137b3cc3":"## Test data","334bca1a":"## TF Dataset support code","0e3b3b78":"## SET TPU \/ GPU","01951b42":"## CatBoost 6 Fold CV Training","ce735fd8":"## EfficientNet Feature Model Support Code","53da28d5":"In the current competition there are already a lot of very strong performing notebooks such as those from [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte) and [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek). These notebooks are however based on the current State of the Art models suchs as SWIN Transformers.\n\nI was pretty surprised when I noticed that the score of my notebook with only an EfficientNet B2 (and not even the V2 version...) came pretty close to their scores.\n\nSo ... why not share it with the Kaggle Community ;-)\n\nThe main process this notebook uses is the following:\n1. An EfficientNetB2-NS Classification model is trained as classifier with Stratified 10 Fold Cross Validation. Key is only limited epochs and very low learning rate.\n2. Next each of the 10 feature models is used to extract features for the train and test.\n3. As a last step with the extracted features a Stratified 6 Fold CV training with CatBoost is performed and the predictions on the test set are made.\n\nAnd if you do like the notebook ... then please give an upvote for it. And definitely let me know your questions and\/or remarks in the comments for this notebook.\n\nEnjoy!","6cd303ef":"## Create submission file"}}