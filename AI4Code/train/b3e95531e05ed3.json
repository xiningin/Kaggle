{"cell_type":{"49706ef3":"code","1e9fa203":"code","6c5fb1d9":"code","b2ce0d55":"code","17d248de":"code","2974a0f3":"code","71525a91":"code","86803c68":"code","f1af7cbc":"code","31c5bb17":"code","c213413b":"code","f0b8b6e5":"code","13f7feaf":"code","500fe39f":"code","37bd80f7":"code","8332981b":"code","bafeac73":"code","12181d0a":"markdown","cc26f11e":"markdown","99a4ccc8":"markdown","aa304168":"markdown","870f1c7d":"markdown","246a2a8f":"markdown","852bc56a":"markdown","11ef2f4c":"markdown","2677e554":"markdown","31d1b352":"markdown","718761f9":"markdown","7739e5cc":"markdown","ea707c3f":"markdown","ae714f1a":"markdown"},"source":{"49706ef3":"#Importando as bibliotecas\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold","1e9fa203":"dfTitanic = pd.read_csv('..\/input\/lab5_train_no_nulls_no_outliers_ohe.csv')\ndfTitanic.head(3)","6c5fb1d9":"dfTitanic = dfTitanic.sort_values('Survived')","b2ce0d55":"# aqui montamos a matriz de atributos X e o vetor coluna de respostas Y.\n# Note que n\u00e3o selecionamos algumas colnas, como Nome e Ticket\ny = dfTitanic['Survived'].values\nX = dfTitanic[['Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'S', '1', '2', '3', 'female', 'male']].values","17d248de":"from sklearn.manifold import TSNE","2974a0f3":"%%time\nX_tsne = TSNE(n_components=2).fit_transform(X)\nX_tsne.shape","71525a91":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\n# We import seaborn to make nice plots.\nimport seaborn as sns","86803c68":"def scatter(x, colors):\n    \"\"\"this function plots the result\n    - x is a two dimensional vector\n    - colors is a code that tells how to color them: it corresponds to the target\n    \"\"\"\n    \n    # We choose a color palette with seaborn.\n    palette = np.array(sns.color_palette(\"hls\", 2))\n\n    # We create a scatter plot.\n    f = plt.figure(figsize=(10, 8))\n    ax = plt.subplot(aspect='equal')\n    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n                    c=palette[colors.astype(np.int)])\n    \n    ax.axis('off') # the axis will not be shown\n    ax.axis('tight') # makes sure all data is shown\n    \n    # set title\n    plt.title(\"Featurespace Visualization Titanic\", fontsize=25)\n    \n    # legend with color patches\n    survived_patch = mpatches.Patch(color=palette[1], label='Survived')\n    died_patch = mpatches.Patch(color=palette[0], label='Died')\n    plt.legend(handles=[survived_patch, died_patch], fontsize=20, loc=1)\n\n    return f, ax, sc\n\n# Use the data to draw ths scatter plot\nscatter(X_tsne, y)","f1af7cbc":"# Dividindo os dados em 5 folds.\nkf = KFold(n_splits=5, shuffle=True, random_state=5)","31c5bb17":"#Fun\u00e7\u00e3o id\u00eantica \u00e0 usada nos modelos de regress\u00e3o.\ndef avalia_classificador(clf, kf, X, y, f_metrica):\n    metrica_val = []\n    metrica_train = []\n    for train, valid in kf.split(X,y):\n        x_train = X[train]\n        y_train = y[train]\n        x_valid = X[valid]\n        y_valid = y[valid]\n        clf.fit(x_train, y_train)\n        y_pred_val = clf.predict(x_valid)\n        y_pred_train = clf.predict(x_train)\n        metrica_val.append(f_metrica(y_valid, y_pred_val))\n        metrica_train.append(f_metrica(y_train, y_pred_train))\n    return np.array(metrica_val).mean(), np.array(metrica_train).mean()","c213413b":"from sklearn.metrics import accuracy_score, roc_auc_score\n\ndef apresenta_metrica(nome_metrica, metrica_val, metrica_train, percentual = False):\n    c = 100.0 if percentual else 1.0\n    print('{} (valida\u00e7\u00e3o): {}{}'.format(nome_metrica, metrica_val * c, '%' if percentual else ''))\n    print('{} (treino): {}{}'.format(nome_metrica, metrica_train * c, '%' if percentual else ''))","f0b8b6e5":"from sklearn.ensemble import AdaBoostClassifier","13f7feaf":"%%time\nada = AdaBoostClassifier()\nmedia_acuracia_val, media_acuracia_train = avalia_classificador(ada, kf, X, y, accuracy_score) \napresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\nmedia_auc_val, media_auc_train = avalia_classificador(ada, kf, X, y, roc_auc_score) \napresenta_metrica('AUC', media_auc_val, media_auc_train, percentual=True)","500fe39f":"%%time\nbest_acc = 0\nfor n in [50, 200, 500]:\n    for l in [1, 0.5, 0.3, 0.2]:\n        print('n_estimators = {}, learning_rate = {}'.format(n, l))\n        ada = AdaBoostClassifier(n_estimators=n, learning_rate=l)\n        media_acuracia_val, media_acuracia_train = avalia_classificador(ada, kf, X, y, accuracy_score) \n        apresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\n        if media_acuracia_val > best_acc:\n            best_acc = media_acuracia_val\n            best_train = media_acuracia_train\n            best_n = n\n            best_l = l\nprint('\\nMelhores hiperpar\u00e2metros: n_estimators = {}, learning_rate = {}'.format(best_n, best_l))\napresenta_metrica('Acur\u00e1cia', best_acc, best_train, percentual=True)","37bd80f7":"from sklearn.tree import DecisionTreeClassifier","8332981b":"%%time\nada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2))\nmedia_acuracia_val, media_acuracia_train = avalia_classificador(ada, kf, X, y, accuracy_score) \napresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\nmedia_auc_val, media_auc_train = avalia_classificador(ada, kf, X, y, roc_auc_score) \napresenta_metrica('AUC', media_auc_val, media_auc_train, percentual=True)","bafeac73":"%%time\nbest_acc = 0\nfor n in [50, 200, 500]:\n    for l in [1, 0.5, 0.3, 0.2]:\n        print('n_estimators = {}, learning_rate = {}'.format(n, l))\n        ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=n, learning_rate=l)\n        media_acuracia_val, media_acuracia_train = avalia_classificador(ada, kf, X, y, accuracy_score) \n        apresenta_metrica('Acur\u00e1cia', media_acuracia_val, media_acuracia_train, percentual=True)\n        if media_acuracia_val > best_acc:\n            best_acc = media_acuracia_val\n            best_train = media_acuracia_train\n            best_n = n\n            best_l = l\nprint('\\nMelhores hiperpar\u00e2metros: n_estimators = {}, learning_rate = {}'.format(best_n, best_l))\napresenta_metrica('Acur\u00e1cia', best_acc, best_train, percentual=True)","12181d0a":"Aqui vamos usar o arquivo de features onde usamos OHE (one hot encoding). Fique livre para escolher outro conjunto de *features* que geramos anteriormente ou mesmo gerar suas pr\u00f3prias *features*.","cc26f11e":"Vamos visualizar os dados do Titanic ap\u00f3s serem transformados via t-SNE.","99a4ccc8":"Assim como no laborat\u00f3rio anterior, aqui temos uma fun\u00e7\u00e3o para avaliar o desempenho dos algoritmos sem termos de ficar repetindo c\u00f3digo. Ele retorna a m\u00e9dia das m\u00e9tricas, acur\u00e1cia no caso.","aa304168":"Vamos importar umas bibliotecas gr\u00e1ficas","870f1c7d":"## Prepara\u00e7\u00e3o para usar modelos de machine learning","246a2a8f":"Para simplificar a apresenta\u00e7\u00e3o dos resultados e evitar repeti\u00e7\u00e3o de c\u00f3digo criamos a seguinte fun\u00e7\u00e3o auxuliar para imprimir os resultados.","852bc56a":"### Carregando os dados que trabalhamos anteriormente","11ef2f4c":"Agora, vamos fazer o mesmo processo anterior, por\u00e9m trocando o weak learner para \u00e1rvore de decis\u00e3o com profundidade = 2 (j\u00e1 n\u00e3o \u00e9 t\u00e3o _weak_ assim, convenhamos). Notem que a acur\u00e1cia de treino aumenta bastante.","2677e554":"## AdaBoost\n\nVamos rodar o [AdaBoost](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html) para classificar os sobreviventes do Titanic em cima dos features calculados pelo t-SNE. Usaremos como weak learner o algoritmo padr\u00e3o: \u00e1rvore de decis\u00e3o de profundidade igual a 1.","31d1b352":"## t-SNE\n\nO t-SNE \u00e9 um algoritmo de redu\u00e7\u00e3o de dimensionalidade estoc\u00e1stico (ie, n\u00e3o-determin\u00edstico). Usaremos [sua implementa\u00e7\u00e3o contida na biblioteca scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html) em cima dos dados do Titanic. A inspira\u00e7\u00e3o, e um pouco do c\u00f3digo, [veio desta postagem](https:\/\/www.kaggle.com\/sabinem\/titanic-survivor-geometry).","718761f9":"Aqui geramos os K-folds para nossa valida\u00e7\u00e3o cruzada dos algoritmos.","7739e5cc":"# Laborat\u00f3rio Extra - AdaBoost, TSNE e GridSearch","ea707c3f":"### Nesse laborat\u00f3rio, vamos partir do dataset utilizado como entrada no Laborat\u00f3rio 5 (Modelos de Classifica\u00e7\u00e3o com o Titanic) e fazer o seguinte:\n- Aplicar o algoritmo t-SNE de redu\u00e7\u00e3o de dimensionalidade nos dados do Titanic para gerar um novo conjunto de features;\n- Rodar o AdaBoost nesse conjunto de features transformado pelo t-SNE\n- Melhorar o AdaBoost fazendo um Grid Search manual.","ae714f1a":"Agora, vamos fazer um Grid Search: vamos variar o n\u00famero de estimadores utilizados pelo ensemble (default = 50) e tamb\u00e9m a taxa de aprendizado (default = 1). Vamos testar diversas combina\u00e7\u00f5es, com o objetivo de escolher o modelo que possua a melhor acur\u00e1cia calculada no conjunto de valida\u00e7\u00e3o. Nota: o scikit-learn possui o [Grid Search j\u00e1 implementado](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html), com a vantagem de utilizar paralelismo. ;-)"}}