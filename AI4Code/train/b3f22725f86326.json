{"cell_type":{"9a1af433":"code","ddec4148":"code","4df1177c":"code","7eed1f48":"code","66d52391":"code","d6f395e8":"code","c8ca9a91":"code","66707492":"code","6c42f5b9":"code","9c185f8a":"code","53190e76":"code","f58b4023":"code","88b47171":"code","4e42a0ac":"code","9774ef3b":"code","6d38f5c1":"code","1300b168":"code","fb6f5bee":"code","e2f20f34":"code","aaeff2ad":"code","b42b497c":"code","ffdcd12b":"markdown","74bd077d":"markdown","749fec57":"markdown","055cfa46":"markdown","b46bcd22":"markdown","02e3d915":"markdown","ade69182":"markdown"},"source":{"9a1af433":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt","ddec4148":"%pwd\n%cd ..\/..\/kaggle","4df1177c":"data = pd.read_csv(r'.\/input\/all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')","7eed1f48":"#print 5 first values of the dataset\ndata.head()","66d52391":"#checking data shape\ndata.shape","d6f395e8":"#checking data for null values\ndata.isnull().sum()","c8ca9a91":"data['date'][0]","66707492":"#normalizing date format\ndata['date'] = pd.to_datetime(data['date']).dt.date","6c42f5b9":"data['date'][0]","9c185f8a":"plt.figure(figsize=(15,10))\nlocation_serie = data['user_location']\nlocation_serie = location_serie.value_counts(dropna=True).nlargest(20)\nlocation_serie.plot(kind ='bar', color='green')\nplt.show()","53190e76":"#deleting repeated tweets\ndata = data.drop_duplicates(subset='text')","f58b4023":"#number of unique dates since we began scraping data\nlen(data['date'].unique())","88b47171":"#we have almost a year of data concerning vaccine tweets\n#then, we sort data by date\ndata.sort_values(by=['date'], ascending=True)","4e42a0ac":"#then we delete all unnecessary columns before starting the cleaning process\ndata.drop(data.columns.difference(['id', 'user_location', 'date','text']), axis=1, inplace=True)\ndata.head()","9774ef3b":"#checking if emot is installed, if not install it\ntry:\n    import emot\nexcept:\n    !pip install emot\n    import emot","6d38f5c1":"from emot.emo_unicode import UNICODE_EMOJI\nemojis = list(UNICODE_EMOJI.keys())\n\nfrom nltk.corpus import stopwords\nstopwords_en = stopwords.words('english')\n\nimport re","1300b168":"def clean_tweets(text):\n    clean_text = text.lower()\n    clean_text = clean_text.strip()\n    \n    clean_text = re.sub('http\\S+|www\\S+|https\\S+', '', clean_text) #removing urls \n    clean_text = re.sub('[\\#\\@]\\w+','',clean_text) #removing # and @\n    clean_text = re.sub('[\\d+]','',clean_text) #removing numbers\n    clean_text = re.sub('[\\.\\,\\;\\!\\?\\*\\$]','',clean_text) #removing some non-sense characters\n    clean_text = ' '.join([w for w in clean_text.split() if(len(w)>1)]) #deleting single characters\n    #text = re.sub(' +',' ', text)#we want to remove double spaces left after deletion\n    clean_text = ' '.join([word for word in clean_text.split() if word not in stopwords_en]) #removing stop words\n    clean_text = ' '.join([word for word in clean_text.split() if word not in emojis]) #removing emojis\n    return clean_text","fb6f5bee":"#example of cleaning data\nprint(clean_tweets(\"I have tested positive for #Covid during 2020 \ud83d\ude22 . It was a difficult period, but I am happy \ud83d\ude0a with how things worked out for me\"))","e2f20f34":"#now as for cleaning our data, we proceed as follows :\n\ndata['clean_text'] = data['text'].apply(lambda text : clean_tweets(text))","aaeff2ad":"data[['clean_text','text']].head()","b42b497c":"#we count 50 most common words\nfrom collections import Counter\nCounter_ = Counter()\n\nfor i in data['clean_text'].values:\n    for j in i.split():\n        Counter_[j]+=1\nCounter_.most_common(20)        ","ffdcd12b":"# First Step : ","74bd077d":"### Visualizing Data distribution","749fec57":"### Affecting Sentiment to Dataset","055cfa46":"### Data Cleaning","b46bcd22":"## Exploratory Data Analysis","02e3d915":"#### Cleaning the Dataset from Hashtags, Links, Redundant Letters, ...","ade69182":"#### By Country"}}