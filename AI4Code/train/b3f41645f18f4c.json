{"cell_type":{"fb47a6e4":"code","02e1c3de":"code","524874cf":"code","1a384e2f":"code","a862f16e":"code","a2a0729f":"code","27bc97af":"code","94003e8f":"code","43df5eeb":"code","f20e4b9f":"code","fd1d1135":"code","42b08157":"code","ff59e9a9":"code","03b0022b":"markdown","42bf76b7":"markdown","ee381cef":"markdown","cafd96da":"markdown","9a92c1c3":"markdown","0b6bfe70":"markdown","dd1dd6f1":"markdown","0f2ce4ac":"markdown","72a26cf8":"markdown","f0bf492d":"markdown","5b6808f6":"markdown","f5f85e33":"markdown","050c5329":"markdown"},"source":{"fb47a6e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport sys\nimport pickle\nfrom sklearn import preprocessing\nfrom time import time\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n# from sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport sys\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsys.path.append('\/kaggle\/input\/enron-project\/')\nfrom feature_format import featureFormat\nfrom feature_format import targetFeatureSplit\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02e1c3de":"original = \"\/kaggle\/input\/enron-project\/final_project_dataset.pkl\"\ndestination = \"final_project_dataset_unix.pkl\"\n\ncontent = ''\noutsize = 0\nwith open(original, 'rb') as infile:\n    content = infile.read()\nwith open(destination, 'wb') as output:\n    for line in content.splitlines():\n        outsize += len(line) + 1\n        output.write(line + str.encode('\\n'))\n# data_dict","524874cf":"# with open(\"\/kaggle\/input\/enron-project\/final_project_dataset.pkl\", 'rb') as f:\n#     data_dict = pickle.load(f)\n\ndata_dict = pickle.load(open(\"\/kaggle\/input\/enron-project\/final_project_dataset_unix.pkl\", 'rb') )\n\ndata_dict.pop('TOTAL')\nabc = featureFormat(data_dict, ['poi','salary','total_payments'])\n# featureFormat()\nprint(\"The number of people in the dataset:\",len(data_dict))\nprint(list(data_dict.keys())[0],\"\\n\",data_dict[list(data_dict.keys())[0]])\n\nfor key in range(len(abc)):\n    if abc[key][1]<1000000:\n        if abc[key][0]==True:\n            plt.scatter(abc[key][1],abc[key][2],color = 'b')\n        else:\n            plt.scatter(abc[key][1],abc[key][2],color = 'r')\n# plt.scatter(abc[:][0],abc[:][1])\n\nplt.ylabel('tot_payments')\nplt.xlabel('salary')   \nplt.show()\nplt.figure(figsize=(20,10))\nsns.heatmap(pd.DataFrame(data_dict).T.dropna(how = 'all').replace({'NaN':0}).drop(['other'],axis = 1).corr(),vmin=-1,cmap='coolwarm')","1a384e2f":"# plt.figure(figsize=(20,6))\nabc2 = featureFormat(data_dict,['poi','salary','bonus'])\nabc3 = featureFormat(data_dict,['poi','salary','total_payments','deferral_payments'])\nabc4 = featureFormat(data_dict,['poi','salary','total_payments','deferred_income'])\nabc5 =  featureFormat(data_dict,['poi','from_this_person_to_poi','from_poi_to_this_person'])\nfig,a =  plt.subplots(2,2,squeeze=False,figsize=(17,10))\na[0][1].set(ylim=(0, 20000000))\na[1][0].set(ylim=(0, 20000000))\na[1][1].set(xlim=(0,210), ylim=(0,320))\nfor key in range(len(abc2)):\n    if abc2[key][0] == True:\n         a[0][0].scatter(abc2[key][1],abc2[key][2],color = 'b')\n    else:\n        a[0][0].scatter(abc2[key][1],abc2[key][2],color = 'r')\nx = np.arange(0,1000000,0.1)\n# a[0][0].ylabel('bonus')\n# a[0][0].xlabel('salary')  \na[0][0].plot(x,x**1.07,'b')\n\nn = 0\nm = 0\nfor key in range(len(abc3)):\n    if abc3[key][0] == True:\n        m+=1\n        if abc3[key][3]==0:\n             a[0][1].plot(abc3[key][1],abc3[key][2],'b^')\n             n+=1\n        else:\n            a[0][1].plot(abc3[key][1],abc3[key][2],'bo')\n    else:\n        if abc3[key][3]==0:\n            a[0][1].plot(abc3[key][1],abc3[key][2],'r^')\n        else:\n            a[0][1].plot(abc3[key][1],abc3[key][2],'ro')\n\nmm  = 0\nnn = 0\nfor key in range(len(abc4)):\n    if abc4[key][0] == True:\n        mm+=1\n        if abc4[key][3]==0:\n             a[1][0].plot(abc4[key][1],abc4[key][2],'b^')\n             nn+=1\n        else:\n            a[1][0].plot(abc4[key][1],abc4[key][2],'bo')\n    else:\n        if abc4[key][3]==0:\n            a[1][0].plot(abc4[key][1],abc4[key][2],'r^')\n        else:\n            a[1][0].plot(abc4[key][1],abc4[key][2],'ro')\n\nfor key in range(len(abc5)):\n    if abc5[key][0] == True:\n         a[1][1].scatter(abc5[key][1],abc5[key][2],color = 'b')\n    else:\n        a[1][1].scatter(abc5[key][1],abc5[key][2],color = 'r')\n\nprint(n,\" out of \",m,\" poi have deffereal payments equal to 0\",\" & \\n\",nn,\" out of \",mm,\" poi have deffered income equal to 0\")\nplt.show()","a862f16e":"def dict_to_list(key,normalizer):\n    new_list=[]\n\n    for i in data_dict:\n        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n            new_list.append(0.)\n        elif data_dict[i][key]>=0:\n            new_list.append(float(data_dict[i][key])\/float(data_dict[i][normalizer]))\n    return new_list\n\nfraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\nfraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\nj = 0\nfor i in data_dict:\n    data_dict[i][\"fraction_from_poi_email\"]=fraction_from_poi_email[j]\n    data_dict[i][\"fraction_to_poi_email\"]=fraction_to_poi_email[j]\n    j+=1","a2a0729f":"features_list = ['poi','shared_receipt_with_poi','fraction_from_poi_email','fraction_to_poi_email']\nfeatures_list2 = ['poi','shared_receipt_with_poi','fraction_from_poi_email','fraction_to_poi_email','deferral_payments']\ndata = featureFormat(data_dict, features_list2)\nvalue , features = targetFeatureSplit(data)\n\nfor key in range(len(data)):\n    if abc[key][0]==True:\n        plt.scatter(data[key][2],data[key][3],color = 'b')\n    else:\n        plt.scatter(data[key][2],data[key][3],color = 'r')\n# plt.scatter(abc[:][0],abc[:][1])\n\nplt.ylabel('fraction_to_poi_email')\nplt.xlabel('fraction_from_poi_email')   \nplt.show()\nX_train, X_test, Y_train, Y_test = train_test_split(features, value, test_size = 0.1)","27bc97af":"from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV\n\nfrom sklearn.metrics import f1_score\n\nclf = xgboost.XGBClassifier()\nreg = LogisticRegression()\nreg2 = svm.SVC()\nreg3 = RandomForestClassifier(max_depth=10, criterion = 'entropy')\nmodel = AdaBoostClassifier(n_estimators=100,base_estimator = reg3)\n# reg3.fit(X_train,Y_train)\n# print(accuracy_score(Y_test, model.predict(X_test)))\n# model.predict(X_test)","94003e8f":"import warnings\nwarnings.filterwarnings('ignore')\n\nmodel.fit(X_train,Y_train)\nskf = StratifiedKFold(n_splits = 8)\nf1 = np.array(features)\nv1 = np.array(value)\nacc = []\nfor train_index, test_index in skf.split(f1, v1):\n    X1_train, X1_test = f1[train_index], f1[test_index]\n    Y1_train, Y1_test = v1[train_index], v1[test_index]\n    model.fit(X1_train,Y1_train)\n    acc.append(accuracy_score(Y1_test,model.predict(X1_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y1_test),np.array(model.predict(X1_test)))[311:315]))\nprint(acc)","43df5eeb":"parameters = {\n    'learning_rate' : [0.2 , 0.3 , 0.5 , 0.6],\n    'gamma' : [0, 0.2, 0.5, 0.8],\n    'max_depth' : [3, 4, 6, 8, 10],\n    'min_child_weight' : [0.5, 1, 2, 4, 6],\n}\n\nrnd_search = RandomizedSearchCV(clf,param_distributions=parameters,cv = 8,n_jobs = -1,scoring = 'accuracy')\nrnd_search.fit(f1, v1)\nclf2 = rnd_search.best_estimator_","f20e4b9f":"acc2 = []\ni = 0\nfor train_index, test_index in skf.split(f1, v1):\n    X1_train, X1_test = f1[train_index], f1[test_index]\n    Y1_train, Y1_test = v1[train_index], v1[test_index]\n    clf2.fit(X1_train,Y1_train)\n    acc2.append(accuracy_score(Y1_test,clf2.predict(X1_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y1_test),np.array(clf2.predict(X1_test)))[311:315]))\n    \nprint(acc2)","fd1d1135":"param = {\n    'n_estimators' : [80 , 100 , 120 , 150],\n    'max_depth' : [3, 4, 6, 8, 10],\n    'min_samples_split' : [2, 3]\n}\nrnd = RandomizedSearchCV(reg3,param_distributions=param,cv = 8,n_jobs = -1,scoring = 'f1')\nrnd.fit(f1,v1)\nreg3_ = rnd.best_estimator_\n\nacc3 = []\nfor train_index, test_index in skf.split(f1, v1):\n    X2_train, X2_test = f1[train_index], f1[test_index]\n    Y2_train, Y2_test = v1[train_index], v1[test_index]\n    reg3_.fit(X2_train,Y2_train)\n    acc3.append(accuracy_score(Y2_test,reg3_.predict(X2_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y2_test),np.array(reg3_.predict(X2_test)))[311:315]))\n    \nprint(acc3)","42b08157":"acc4 = []\nfor train_index, test_index in skf.split(f1, v1):\n    X3_train, X3_test = f1[train_index], f1[test_index]\n    Y3_train, Y3_test = v1[train_index], v1[test_index]\n    reg2.fit(X3_train,Y3_train)\n    acc2.append(accuracy_score(Y3_test,reg2.predict(X3_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y3_test),np.array(reg2.predict(X3_test)))[311:315]))\n    \nprint(acc3)","ff59e9a9":"pickle.dump(clf2, open(\"my_classifier.pkl\", \"wb\") )\npickle.dump(data_dict, open(\"my_dataset.pkl\", \"wb\") )\npickle.dump(features_list2, open(\"my_feature_list.pkl\", \"wb\") )","03b0022b":"# Outputting the data as a pickle file\n\n### As can be noticed after parameter optimization, XGBoost performs slightly better than ADAboost with Random Forest Classifier","42bf76b7":"# Code to convert new line characters in unix","ee381cef":"### The parameters like number of estimators and max_depth has been optimized according to data","cafd96da":"# **Using Stratified K Fold to check accuracy for ADAboost**","9a92c1c3":"# Importing Required Libraries","0b6bfe70":"# Loading the file and Removing the \"TOTAL\" key###","dd1dd6f1":"### Using different algorithms to fit the data","0f2ce4ac":"### It can be cleary seen that the model does well in terms of f1-score","72a26cf8":"# Using SVM to predict poi","f0bf492d":"# Parameter Optimization For RandomForestClassifier","5b6808f6":"# Plotting graphs to see dependence on various factors","f5f85e33":"# Parameter Optimization for XGBoost \n### And then using Stratified KFold to see accuracy","050c5329":"# Introduing new features "}}