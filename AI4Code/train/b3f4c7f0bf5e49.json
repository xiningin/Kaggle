{"cell_type":{"a353d8bd":"code","0858f011":"code","8c0319ec":"code","a70f3784":"code","56a8c3ce":"code","600b1251":"code","891f237a":"code","671da015":"markdown","a78d7d4b":"markdown","32961802":"markdown","54413f7c":"markdown","594857b8":"markdown","94a366c0":"markdown","7c40695a":"markdown"},"source":{"a353d8bd":"import pandas as pd\ndef sample_df(data, n):\n    \n    \"\"\" \n    # Create a DataFrame that has the same column names as data and is empty\n    # besides the Energy column.\"\"\" \n    sample_table = pd.DataFrame(columns = data.columns)             \n    sample_table[('0', 'channel')] = data[('0', 'channel')]\n\n    # Iterating over every column and using pandas sample function with the \n    # column values of the data DataFrame as weights to generate a sample \n    # with replacement. The sampled values are then counted, reindexed and \n    # saved in the corresponding column in the sample_table that is returned \n    # as the functions return value.\n    for k in sample_table.columns[1:]:\n        sample_table[k] = range(len(sample_table))\n        sample_table[k] = sample_table[k].sample(n = n, \n            replace = True, \n            weights = data[k], \n            random_state = None).value_counts().reindex(range(len(sample_table)), \n                                                        fill_value = 0)\n    \n    return sample_table","0858f011":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n#Import high resolution Data \ndf = pd.read_csv('..\/input\/metalclass\/materials.csv', header = [0,1] )\n\ndef DataGenerator(n, df):\n    #Create a list that contains all n-values to be used in the sample_df function\n    count_list= []\n    count_per_area = int(n\/6)\n    halfrange = int(count_per_area\/2)\n    count_list += list(range(300-halfrange,300+halfrange))\n    count_list += list(range(2500-halfrange,2500+halfrange))\n    count_list += list(range(25000-halfrange*2,25000+halfrange*2,2))\n    count_list += list(range(50000-halfrange*3,50000+halfrange*3,3))\n    count_list += list(range(500000-halfrange*4,500000+halfrange*4,4))\n    count_list += list(range(1000000-halfrange*5,1000000+halfrange*5,5))\n    \n    #Loop through the n-values, run sample_df function with eatch n-value and reformat the Dataframe \n    data_df = pd.DataFrame(columns=pd.RangeIndex(start=0, stop=16244, step=1))\n    for count in tqdm(count_list):\n        temp_data = sample_df(df,count)\n        temp_data = temp_data.drop(df.columns[0],1)\n        temp_data = temp_data.astype(int)\n        temp_flipped = temp_data.transpose()\n        data_df = data_df.append(temp_flipped, ignore_index=True)\n    return data_df\n        \n#n of 2016 was used during the challenge     \ndata = DataGenerator(108,df)\nprint(data.info())","8c0319ec":"#Normalize data and round it to seven digits\ndata_normalized = data.div(data.sum(axis=1), axis=0).round(7)\nprint(data_normalized.info())","a70f3784":"from sklearn.model_selection import train_test_split\n\n#convert dataFrame to numpy array \ndata_normalized = data_normalized.to_numpy(dtype=np.float)\n\n#create labels\nn= 108\nlabels = np.tile(np.arange(0,21,1), n)\n\n#split data\nX_train, X_test, y_train, y_test = train_test_split(data_normalized, labels, test_size = 0.2, random_state = 0)","56a8c3ce":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n#Create dict of possible hyperparameters\nleaf_size=list(range(20,22))\nn_neighbors = list(range(9,12))\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors)\n\n\nknn_model = KNeighborsClassifier()\nknn_gscv = GridSearchCV(knn_model, hyperparameters, cv=3,  verbose=10)\nknn_gscv.fit(X_train, y_train)\nprint(knn_gscv.best_params_)\nprint(knn_gscv.best_score_)","600b1251":"# instantiate the model\nknn = KNeighborsClassifier(n_neighbors=10, leaf_size=20)\n\n# fit the model to the training set\nknn.fit(X_train, y_train)\n\n#test accuracy\ny_pred_test = knn.predict(X_test)\nprint('Test-set accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))\n\n#train accuray\ny_pred_train = knn.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n","891f237a":"from sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15, 15))\nplot_confusion_matrix(knn, X_test, y_test, normalize=\"true\", ax=ax)  \nplt.show() ","671da015":"#### Train the knn model with the determined parameters and calculate the test and training accuracy.","a78d7d4b":"# 2nd AI Challenge Days\n### Team humanoid hyperparameter tuning \n\n#### Definition of the given function to create training data with the parameter n.\n#### n is the sum of all photons hitting the sensor.\n","32961802":"#### Displaying the results in a confusion matrix.","54413f7c":"#### Creating the labels, splitting the data into training and test data sets, and mixing the data.","594857b8":"#### Each vector is normalized to a probability vector.","94a366c0":"#### In the picture you can see the distribution of the n values in the materials.csv dataset.\n\n![png.png](attachment:df2b467c-ed09-4b21-aa0d-7826d8e49641.png)\n\n\n#### In the following code a list of n-values is created which resembles the distribution in the materials.csv dataset.\n#### Using the function defined above, a training dataset is created, containing 42336 (21*2016) vectors with 16244 values each.","7c40695a":"#### Using sklearn's GridSearchCV function, we found the best hyperparameters for this dataset.\n#### Since we are working with only 2268 vectors here instead of 42336, the optimal parameters differ.\n\n#### Result with 42336 vectors:\n#### n_neighbors = 10\n#### leaf_size = 20"}}