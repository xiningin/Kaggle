{"cell_type":{"e89cd78f":"code","37fc118d":"code","93c87988":"code","580f7e77":"code","a24cf360":"code","cf66ef79":"code","c8c4c389":"code","e8cc024d":"code","0b2b1854":"code","1d981f35":"code","75f6cc3b":"code","310e8f64":"code","ddfef833":"code","b3921d18":"code","8b7ec230":"code","5604b0b7":"code","b88b7569":"code","091cb10a":"code","7ed77443":"code","ca197a56":"code","205e7c9b":"code","b34e9d85":"code","07b1ed5a":"code","a19c13ad":"code","f2dc52a6":"code","16e39a75":"code","b90a637a":"code","ea01df07":"code","f6115bdf":"code","805f1aff":"markdown","785dd8d2":"markdown","05fd065f":"markdown","23315cec":"markdown","21efab1b":"markdown","1d0284ce":"markdown","0e20c2fe":"markdown","3203ab55":"markdown","a030ea5f":"markdown","ececdb6a":"markdown","4beb28a9":"markdown","422df627":"markdown","4974ef97":"markdown","5b0ec2ec":"markdown","5e139aac":"markdown"},"source":{"e89cd78f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import class_weight\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37fc118d":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()","93c87988":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","580f7e77":"print(train.shape, test.shape)","a24cf360":"train['Survived'].value_counts().plot(kind='bar')\nplt.title('Labels counts')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()","cf66ef79":"class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(train.Survived),\n                                                 train.Survived)\nclass_weights = {i : class_weights[i] for i in range(len(class_weights))}\nprint(class_weights)","c8c4c389":"# Save the 'PassengerId' column\ntrain_ID = train['PassengerId']\ntest_ID = test['PassengerId']\n\n#Now drop the  'PassengerId' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"PassengerId\", axis = 1, inplace = True)\ntest.drop(\"PassengerId\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'PassengerId' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","e8cc024d":"# Merge train set and test set\n\nntrain = train.shape[0]\nntest = test.shape[0]\n\ntrain_labels = train['Survived'].copy()\ntrain = train.drop('Survived', axis=1)\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\nall_data\n","0b2b1854":"all_data.info()","1d981f35":"all_data_num = all_data.select_dtypes(exclude=['object'])\nall_data_num","75f6cc3b":"all_data_cat = all_data.select_dtypes(include=['object'])\nall_data_cat","310e8f64":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])\n\n\ncat_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant')),\n    ('onehotcode',  OneHotEncoder())\n])","ddfef833":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(all_data_num.columns)\ncat_attribs = list(all_data_cat.columns)\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipe, num_attribs),\n    ('cat', cat_pipe, cat_attribs)\n])\n\nall_data_prepared = full_pipeline.fit_transform(all_data)","b3921d18":"train_data = all_data_prepared[:ntrain]\ntest_data = all_data_prepared[ntrain:]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, random_state=42)","8b7ec230":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nlogreg = LogisticRegression(class_weight = class_weights)\n\nparam_grid = {'max_iter': [2000],\n             'penalty': ['l1', 'l2'],\n             'C': np.logspace(-4,4,20),\n             'solver': ['liblinear']}\nclf_lr = GridSearchCV(logreg, param_grid, cv = 5, verbose = 1, n_jobs = -1)\nbestF = clf_lr.fit(train_data, train_labels)","5604b0b7":"bestF.best_score_","b88b7569":"bestF.best_params_","091cb10a":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(class_weight = class_weights)\n\nscores = cross_val_score(decision_tree, train_data, train_labels, scoring ='accuracy', cv=10)\nscores.mean()","7ed77443":"from sklearn.ensemble import RandomForestClassifier\n\n\nforest = RandomForestClassifier(random_state = 42)\nn_estimators = [5, 10, 50, 100]\nmax_depth = [3,5, 8, 10, 20]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 5] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(train_data, train_labels)","ca197a56":"bestF.best_params_","205e7c9b":"bestF.best_score_","b34e9d85":"forest = RandomForestClassifier(max_depth=20, n_estimators=100, random_state=42)","07b1ed5a":"scores = cross_val_score(forest, train_data, train_labels, scoring ='accuracy', cv=5)\nscores.mean()","a19c13ad":"from sklearn.svm import SVC\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_clas= SVC(class_weight = class_weights)\ngrid_search = GridSearchCV(svm_clas, param_grid, cv=5,verbose=2)\ngrid_search.fit(train_data, train_labels)","f2dc52a6":"grid_search.best_params_","16e39a75":"grid_search.best_score_","b90a637a":"svc = SVC(C=30.0, gamma=0.03, class_weight = class_weights)\nscores = cross_val_score(svc, train_data, train_labels, scoring ='accuracy', cv=10)\nscores.mean()","ea01df07":"svc.fit(train_data, train_labels)\n\nresults = svc.predict(test_data)\nresults","f6115bdf":"sub = pd.DataFrame()\nsub['PassengerId'] = test_ID\nsub['Survived'] = results\nsub.to_csv('submission.csv',index=False)","805f1aff":"# Data Preprocessing","785dd8d2":"### Split Data","05fd065f":"### Explore Target Variable\n\n","23315cec":"### Logistic Regression","21efab1b":"### Numerical variables","1d0284ce":"### Prediction","0e20c2fe":"**Class Imbalance**","3203ab55":"### Random Forest Classifier","a030ea5f":"## Training\n","ececdb6a":"### Categorical variables\n","4beb28a9":"### Support Vector Machine","422df627":"## Best results: 0.79904 - Top 5%","4974ef97":"### Transformation Pipelines","5b0ec2ec":"### Import Data","5e139aac":"### Decision Tree Classifier"}}