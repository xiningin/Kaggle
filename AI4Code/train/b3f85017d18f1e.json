{"cell_type":{"63da1afa":"code","6c7bdd82":"code","c09bbbe1":"code","01e0f7fc":"code","b09feba4":"code","5490adb5":"code","7599c6be":"code","d4fc1c67":"code","ad15d7db":"code","9b170ac3":"code","5e4862f2":"code","e1576e5f":"code","d8596469":"code","07841b35":"code","6a31e095":"markdown","9639054d":"markdown","0681ff5c":"markdown","1b2e1673":"markdown","ff105a59":"markdown","1d56dac3":"markdown","6423f49f":"markdown","42b1afa6":"markdown","0c496800":"markdown","ec68474e":"markdown","48c363e8":"markdown","89112cf7":"markdown"},"source":{"63da1afa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6c7bdd82":"MELBOURNE = pd.read_csv(\"..\/input\/melb_data.csv\")\nMELBOURNE.head()","c09bbbe1":"MISSING = MELBOURNE.isnull().sum()\nprint(MISSING[MISSING > 0])","01e0f7fc":"#Create a new dataset without the columns that have at least one missing\nMELBOURNE_MISSING_FREE = MELBOURNE.dropna(axis=1)\nprint(MELBOURNE_MISSING_FREE.head())\nDIMENSIONS = [[MELBOURNE.shape], [MELBOURNE_MISSING_FREE.shape]]\nprint(DIMENSIONS)\n\n","b09feba4":"#Sklearn algorithm to fill nan with the mean of the column\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nMEBOURNE_MEAN_IMPUTER = my_imputer.fit_transform(MELBOURNE)\n#DON'T PANIC!!!:\n#L'errore che viene fuori \u00e8 dovuto alla presenza di categoriche (stringhe)\n#per il momento imputiamo i valori per le sole variabili quantitative","5490adb5":"#print(MELBOURNE.dtypes)\n#print(MELBOURNE.iloc[0])\n#prendiamo solo le variabili numeriche (int64, float64)\nMELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE = MELBOURNE.select_dtypes(exclude = [\"object\"])\nCOLUMNS = MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.columns\nMELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE = my_imputer.fit_transform(MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE)\nMELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE = pd.DataFrame(MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE, columns=COLUMNS)\nMELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.head()\n\n\n\n#print(MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.head())\n#DIMENSIONS = [[MELBOURNE.shape], [MELBOURNE_MISSING_FREE.shape], [MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.shape]]\n#print(DIMENSIONS)\n#print(MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.columns)\n","7599c6be":"y = MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.Price\nX = MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE.drop(\"Price\", axis=1)\nPOTENTIAL_REGRESSOR = [\"Rooms\", \"Distance\", \"YearBuilt\"]\n#X = MELBOURNE_MEAN_IMPUTER_CATEGORIES_FREE[POTENTIAL_REGRESSOR]\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(X,y)\n\nimport statsmodels.api as sm\n\nX2 = sm.add_constant(X)\nest = sm.OLS(y, X2)\nest2 = est.fit()\nprint(est2.summary())","d4fc1c67":"# make copy to avoid changing original data (when Imputing)\nNEW_MELBOURNE = MELBOURNE.copy()\nNEW_MELBOURNE.columns = MELBOURNE.columns\n\n# make new columns indicating what will be imputed\ncols_with_missing = (col for col in NEW_MELBOURNE.columns \n                                 if NEW_MELBOURNE[col].isnull().any())\nfor col in cols_with_missing:\n    NEW_MELBOURNE[col + '_was_missing'] = NEW_MELBOURNE[col].isnull()\n\n\"\"\"\n# Imputation\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\nnew_data.columns = original_d\n\"\"\"\n# DON'T PANIC: l'ultimo blocco non va perch\u00e9 sono presenti variabili categoriche e non posso fare l'imputazione numerica della media","ad15d7db":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n#riusero la stessa y di sopra. Come X utilizzo tutte le numeriche(elimino le categoriche) che presentano i missing \nmelb_X = MELBOURNE.drop(['Price'], axis=1)\nmelb_numeric_X = melb_X.select_dtypes(exclude=['object'])\nmelb_numeric_X = melb_X.select_dtypes(exclude=['object'])\n","9b170ac3":"X_train, X_test, y_train, y_test = train_test_split(melb_numeric_X, \n                                                    y,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0)\n# Function to compute the quality of prediction (MAE)\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)\n\n","5e4862f2":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nX_train_reduced = X_train.drop(cols_with_missing, axis=1)\nX_test_reduced = X_test.drop(cols_with_missing, axis=1)\n\nprint(\"Dropped missing columns MAE\")\nMAE_DROP = (score_dataset(X_train_reduced, X_test_reduced, y_train, y_test))\nprint(SCORE_FROM_DROPPING)","e1576e5f":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nX_train_imputed = my_imputer.fit_transform(X_train)\nX_test_imputed = my_imputer.fit_transform(X_test)\n\nprint(\"Mean imputed MAE\")\nMAE_IMPUTED = score_dataset(X_train_imputed, X_test_imputed, y_train, y_test)\nprint(MAE_IMPUTED)","d8596469":"X_train_imputed_extra = X_train.copy()\nX_test_imputed_extra = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    X_train_imputed_extra[col + '_was_missing'] = X_train_imputed_extra[col].isnull()\n    X_test_imputed_extra[col + '_was_missing'] = X_test_imputed_extra[col].isnull()\n\n#imputation\nmy_imputer = SimpleImputer()\nX_train_imputed_extra = my_imputer.fit_transform(X_train_imputed_extra)\nX_test_imputed_extra = my_imputer.fit_transform(X_test_imputed_extra)\n\n#getting score\nprint(\"Mean imputed+extra col MAE\")\nMAE_IMPUTED_EXTRA = score_dataset(X_train_imputed_extra, X_test_imputed_extra, y_train, y_test)\nprint(MAE_IMPUTED_EXTRA)","07841b35":"MELBOURNE.dtypes.sample(10)\n\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","6a31e095":"**1.2.1 SOLUTION_2.2**\n\n**An extention of imputation:**\n1. Creo un nuovo dataset con delle colonne che indicano la presenza (logica) di missing","9639054d":"**1.3: COMPARING ALL SOLUTIONS**","0681ff5c":"**1.3.3: MEAN IMPUTED + EXTRA INDICATOR COLUMNS FOR MISSING MAE**","1b2e1673":"**piccola parabola: PROVO A FARE UN MODELLO OLS**","ff105a59":"**1.1 SOLUTION_1: **\n\n**Drop Columns with missings**\n\nPro:\n* Fast and easy solution\n* Usefull for linear models if a column is too correlated with another column\n* Usefull if there are too much missing in a column\n\nCons:\n* It can lead to a huge loss of information that models cannot take in account\n* We can usually find better solutions\n\n","1d56dac3":"**2: ONE HOT ENCODING FOR USING CATEGORIES**\n\nN.B.\n\nObject indicates a column has text (there are other things it could be theoretically be, but that's unimportant for our purposes). It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. Pandas offers a convenient function called get_dummies to get one-hot encodings. Call it like this:\n\n","6423f49f":"**0. Import dataset and misc**","42b1afa6":"**1.3.2: Get score from mean imputation of missing**","0c496800":"**1.2.1 SOLUTION_2.1**\n\n**A Better solution: mean missing imputation**","ec68474e":"**1.3.1: \nGet Model Score from Dropping Columns with Missing Values\n**","48c363e8":"**1 Handling Missing Values** \n\nHow Missing (nan) shows up, a few example:\n* A 2 bedroom house doen't include an answer for how large the 3' room is\n* Someone being surveied may choose not to share their income\n* Random errors \n\nN.B.\n\nMost libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below.\n\n","89112cf7":"**1.3.0: Test and Train**"}}