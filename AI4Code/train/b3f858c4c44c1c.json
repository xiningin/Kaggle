{"cell_type":{"34ab88f8":"code","b1383ad9":"code","effb1d5e":"code","9b42cafb":"code","aa85ec02":"code","fe814958":"code","8b295c14":"code","31b2a297":"code","665ca152":"code","832d3895":"code","4fd5ae0d":"code","0db695b7":"code","1992d97a":"code","e6636a43":"code","dce8056c":"code","dcaea634":"code","01f24bb7":"code","e6420194":"code","01e9bb4f":"code","04f97f42":"code","301fb6e7":"code","a7d3d04e":"code","d4f34333":"code","00189050":"code","8d19fe2a":"code","8d9eb85d":"code","1e562239":"code","b7c250e7":"code","f126a4c6":"code","5b43c720":"markdown","5fa92468":"markdown","92e76ead":"markdown","533d834c":"markdown","1fd30bd8":"markdown","3a0a57bb":"markdown","09ccf6ae":"markdown","c0c7e531":"markdown","9f7b27da":"markdown","e2785f3c":"markdown","131b9a47":"markdown","e255a7da":"markdown","52c49187":"markdown","302c21d1":"markdown","5be7f2dd":"markdown","385dc339":"markdown","0c1defee":"markdown","c0f4c598":"markdown","77f16595":"markdown","f3f965ca":"markdown","3a821f2e":"markdown","0699bf9a":"markdown","6b977fca":"markdown","42c4a5e0":"markdown","1c967e76":"markdown","70260222":"markdown","d1d81f93":"markdown","741969b4":"markdown","1abf4392":"markdown","871a0962":"markdown","0d40f86f":"markdown","ffb8d270":"markdown","d005c642":"markdown","ee19bc66":"markdown","a5b1a597":"markdown","a95224aa":"markdown","f62931f6":"markdown","212ecb18":"markdown","b1527bb5":"markdown","77e910e8":"markdown","cb7532bf":"markdown","23618375":"markdown","8b97fd4d":"markdown","6b93bdd3":"markdown","01747f48":"markdown","9f5ca1bb":"markdown"},"source":{"34ab88f8":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nwarnings.filterwarnings(\"ignore\")","b1383ad9":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndiabetes = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n","effb1d5e":"diabetes.head(10)","9b42cafb":"print(f\"Number of rows in dataframe are : {diabetes.shape[0]} \\nNumber of columns in dataframe are : {diabetes.shape[1]} \\n\")","aa85ec02":"diabetes.info()","fe814958":"diabetes.describe()","8b295c14":"diabetes[diabetes.duplicated()]","31b2a297":"diabetes.nunique()","665ca152":"diabetes.isnull().sum()","832d3895":"print(\"Number of samples for Outcome 0 are : \",len(diabetes[diabetes['Outcome']==0]))\nprint(\"Number of samples for Outcome 1 are : \",len(diabetes[diabetes['Outcome']==1]))\n","4fd5ae0d":"# Add all column names to a list except for the target variable\ncolumns=diabetes.columns\ncolumns=list(columns)\ncolumns.pop()\nprint(\"Column names except for the target column are :\",columns)\n\n#Graphs to be plotted with these colors\ncolours=['b','c','g','k','m','r','y','b']\nprint()\nprint('Colors for the graphs are :',colours)","0db695b7":"sns.set(rc={'figure.figsize':(15,17)})\nsns.set_style(style='white')\nfor i in range(len(columns)):\n    \n    plt.subplot(4,2,i+1)\n    sns.distplot(diabetes[columns[i]], hist=True, rug=True, color=colours[i])","1992d97a":"sns.set(rc={'figure.figsize':(15,17)})\ncolors_list = ['#78C850', '#F08030']\nj=1\nsns.set_style(style='white')\n\nfor i in (columns):\n    plt.subplot(4,2,j)\n\n    sns.violinplot(x=\"Outcome\", y=i,data=diabetes, kind=\"violin\", split=True, height=4, aspect=.7,palette=colors_list)\n       \n    sns.swarmplot(x='Outcome', y=i,data=diabetes, color=\"k\", alpha=0.8)\n    \n\n    j=j+1\n","e6636a43":"sns.set(rc={'figure.figsize':(20,100)})\nj=1\n\nsns.set_style(style='white')\nfor i in range(len(columns)):\n    for k in range(i,len(columns)):\n        try:\n            if i==k:\n                continue\n            plt.subplot(18,2,j)\n            sns.scatterplot(x=diabetes[columns[i]],y=diabetes[columns[k]],hue=\"Outcome\",data=diabetes)\n            j=j+1\n        except:\n            break","dce8056c":"sns.set(rc={'figure.figsize':(15,15)})\nj=1\nsns.set_style(style='white')\n\nfor i in range(len(columns)):\n    plt.subplot(4,2,j)\n    sns.stripplot(x='Outcome', y=columns[i] , data=diabetes)\n    j=j+1\n\n    \n","dcaea634":"sns.set(rc={'figure.figsize':(15,100)})\nsns.set_style(style='white')\n\nsns.pairplot(diabetes, hue='Outcome')\n","01f24bb7":"plt.figure(figsize=(20,20))\nsns.light_palette(\"seagreen\", as_cmap=True)\nsns.heatmap(diabetes.corr(), annot=True)\n","e6420194":"plt.figure(figsize=(6,8))\nsns.set_style(style='white')\nsns.countplot(diabetes['Outcome'])\n","01e9bb4f":"with sns.axes_style(\"white\"):\n    sns.set_palette(\"BuGn_r\")\n    g2 = sns.jointplot(\"Insulin\", \"BMI\", data=diabetes,\n                kind=\"kde\", space=0)\n","04f97f42":"# Scaling those columns which have values greater than 1\n\nscaleIt = MinMaxScaler()\ncolumns_to_be_scaled = [c for c in diabetes.columns if diabetes[c].max() > 1]\nprint(\"The columns which are to be scaled are :\",columns_to_be_scaled)\n\nscaled_columns = scaleIt.fit_transform(diabetes[columns_to_be_scaled])\nscaled_columns = pd.DataFrame(scaled_columns, columns=columns_to_be_scaled)\nscaled_columns['Outcome'] = diabetes['Outcome'] \n\n\n\n#copying the scaled DataFrame to original DataFrame\n\ndiabetes=scaled_columns\ndiabetes","301fb6e7":"x=diabetes.iloc[:,:-1]\ny=diabetes.iloc[:,-1:]\nx.head(5),y.head(5)","a7d3d04e":"x_train, x_test, y_train, y_test = train_test_split(x,y , test_size = 0.2, random_state = 42)","d4f34333":"print(\"Percentage of Positive Values in training data before Smote :\",y_train.value_counts(normalize=True)[1]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\nprint(\"Percentage of Negative Values in training data before Smote :\",y_train.value_counts(normalize=True)[0]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\n\nprint()\nprint('Shape of x before applying SMOTE :', x_train.shape)\n\n\nsmote = SMOTE()\nx_train,y_train = smote.fit_resample(x_train,y_train)\n\nprint('Shape of x after applying SMOTE : ', x_train.shape)\nprint()\n\nprint(\"Percentage of Positive Values in training data after Smote :\",y_train.value_counts(normalize=True)[1]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\nprint(\"Percentage of Negative Values in training data after Smote :\",y_train.value_counts(normalize=True)[0]\/(y_train.value_counts(normalize=True)[0]+y_train.value_counts(normalize=True)[1])*100,\"%\")\n","00189050":"model = LogisticRegression()\nmodel.fit(x_train, y_train)\npredicted=model.predict(x_test)\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for Logistic Regression is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for Logistic Regression is :\",recall_score(y_test, predicted,)*100, \"%\")\n","8d19fe2a":"model = GaussianNB()\nmodel.fit(x_train, y_train)\n  \npredicted = model.predict(x_test)\n  \nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of Gaussian Naive Bayes is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for Gaussian Naive Bayes is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for Gaussian Naive Bayes is :\",recall_score(y_test, predicted,)*100, \"%\")\n","8d9eb85d":"model = BernoulliNB()\nmodel.fit(x_train, y_train)\n  \npredicted = model.predict(x_test)\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of Bernoulli Naive Bayes is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for Bernoulli Naive Bayes is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for Bernoulli Naive Bayes is :\",recall_score(y_test, predicted,)*100, \"%\")\n","1e562239":"model = SVC()\nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\n\nconf = confusion_matrix(y_test, predicted)\nprint (\"Confusion Matrix : \\n\", conf)\nprint()\nprint (\"The accuracy of SVM is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for SVM is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for SVM is :\",recall_score(y_test, predicted,)*100, \"%\")\n","b7c250e7":"model = KNeighborsClassifier(n_neighbors = 1)  \nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\n\nprint()\nprint (\"The accuracy of KNN is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for KNN is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for KNN is :\",recall_score(y_test, predicted,)*100, \"%\")\n\n\n","f126a4c6":"model = xgb.XGBClassifier(use_label_encoder=False)\nmodel.fit(x_train, y_train)\npredicted = model.predict(x_test)\n\n\nprint()\nprint (\"The accuracy of XGBoost is : \", accuracy_score(y_test, predicted)*100, \"%\")\nprint()\nprint(\"Precision score for XGBoost is :\",precision_score(y_test, predicted,)*100, \"%\")\nprint()\nprint(\"Recall score for XGBoost is :\",recall_score(y_test, predicted,)*100, \"%\")\n","5b43c720":"# **<p style=\"color:red;\">About The Dataset :<\/p>**\n\n<p>This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.<\/p>\n","5fa92468":"**5.1 Feature Engineering**","92e76ead":"#  <p style=\"color:Blue;\">5. Data PreProcessing<\/p>","533d834c":"**3.5 Checking The Duplicate Rows**","1fd30bd8":"# **<p style=\"color:red;\">About The Data :<\/p>**\n","3a0a57bb":"#  <p style=\"color:Blue;\">6. Building The Models<\/p>","09ccf6ae":"**4.6 HeatMap**","c0c7e531":"**5.3 Train Test Split**","9f7b27da":"**3.6 Checking Unique Values In DataFrame**","e2785f3c":"### 4.8 KDEplot","131b9a47":"**5.4 Using SMOTE To Handle Class Imbalance**\n","e255a7da":"#  <p style=\"color:Blue;\">2. Reading The Dataset and Creating DataFrame. <\/p>","52c49187":"**5.2 Dividing The Data Into X And Y**","302c21d1":"> **0** null values found","5be7f2dd":"**6.6 X Gradient Boosting**","385dc339":"**3.3 Checking The Information Of DataFrame**","0c1defee":"**6.5 K Nearest Neighbours**","c0f4c598":"**6.4 Support Vector Machine**\n","77f16595":"**3.2 Checking The Number Of Rows and Columns In DataFrame**","f3f965ca":"#  <p style=\"color:Blue;\">1. Importing The Modules :<\/p>\n","3a821f2e":"#  <p style=\"color:Blue;\">3. Analyzing the Data <\/p>","0699bf9a":"[](https:\/\/www.google.com\/imgres?imgurl=https%3A%2F%2Fcdn-prod.medicalnewstoday.com%2Fcontent%2Fimages%2Farticles%2F323%2F323627%2Fdiabetes.jpg&imgrefurl=https%3A%2F%2Fwww.medicalnewstoday.com%2Farticles%2F323627&tbnid=8Uh9XWHpI-PPHM&vet=12ahUKEwi3k7u7ppHwAhXnDLcAHYrgCNIQMygAegUIARDQAQ..i&docid=F90ufqoDOf6rXM&w=1100&h=734&q=diabetes&ved=2ahUKEwi3k7u7ppHwAhXnDLcAHYrgCNIQMygAegUIARDQAQ)","6b977fca":"> * Highest number of unique values are in column DiabetesPedigreeFunction.","42c4a5e0":"None of the graphs here are following a normal distribution.","1c967e76":"**6.3 Bernoulli Naive Bayes**\n","70260222":"**3.8 Checking Class Distribution**","d1d81f93":"**4.1 Distplot For Various Features**","741969b4":"#  <p style=\"color:Blue;\">7. Conclusion<\/p>\n1. Class Imbalance was found.\n2. Gaussian Naive bayes is performing the best in context of precision and recall.","1abf4392":"**3.4 Checking Statistical Data Of DataFrame**","871a0962":"**4.4 Strip Plot Distribution Of Attributes Vs Outcome**","0d40f86f":"Building a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\n","ffb8d270":"**3.1 Checking Initial 10 Records of the DataFrame**","d005c642":"1. Pregnancies - Number of times pregnant\n\n2. Glucose - Plasma glucose concentration \n\n3. BloodPressure - Diastolic blood pressure (mm Hg)\n\n4. SkinThickness - Triceps skin fold thickness (mm)\n\n5. Insulin - 2-Hour serum insulin (mu U\/ml)\n\n6. BMI - Body mass index\n\n7. DiabetesPedigreeFunction - Diabetes pedigree function\n\n8. Age - Age (years)\n\n9. Outcome - Class variable (0 or 1) - (Target variable)\n","ee19bc66":"**6.1 Logistic Regression**","a5b1a597":"**4.2 ViolinPlot For Outcome Vs. Other Attributes**","a95224aa":"**6.2 Gaussian Naive Bayes**\n","f62931f6":"**4.5 Plotting The Pair Plot**","212ecb18":"> * 7 columns of type \"int\".\n> * 2 columns of type \"float\"","b1527bb5":"> **0** duplicate rows found in the dataframe.","77e910e8":"> Classes are **imbalanced**.","cb7532bf":"<img src= \"https:\/\/www.pngitem.com\/pimgs\/m\/255-2558137_diabetes-png-pictures-diabetes-png-transparent-png.png\" alt =\"Titanic\" style='width: 600px;'>","23618375":"#  <p style=\"color:Blue;\">4. Data Visualization<\/p>","8b97fd4d":"**4.3 ScatterPlot Of All Attributes Against Each Other**","6b93bdd3":"# **<p style=\"color:red;\">Aim?<\/p>**\n","01747f48":"**4.7 Distribution Of Target Variable**","9f5ca1bb":"**3.7 Checking For The Null Values in DataFrame**"}}