{"cell_type":{"036a78a8":"code","467640e8":"code","1c209256":"code","46b8c919":"code","9025448b":"code","4297f62e":"code","066a2664":"code","82fe3bd3":"code","ce7c9538":"code","b7c91029":"code","2ff2a29a":"code","509b4b36":"code","1ff353c4":"code","c6217540":"code","1cd2411a":"code","bace4784":"code","30832163":"code","75296c7f":"code","fc9a903b":"code","241846d5":"code","dcd6558d":"code","26cb87cb":"code","b777deb9":"code","90aab25e":"code","267ebe68":"code","3c88bd5e":"code","f96f3c7d":"code","2c01f48b":"code","f0a8c14b":"code","78b44607":"code","3acd08e8":"code","5ae84c5a":"code","b883c46f":"code","bc96eeee":"code","b5d7b4dc":"code","f2b88fd7":"code","cfc28c27":"code","f983189a":"code","2a668039":"code","7bc2090f":"code","27d03df3":"code","924377b5":"code","5237bbeb":"markdown","788137a1":"markdown","dc0b2c8a":"markdown","d9649f95":"markdown","6f7a79dd":"markdown","206241f9":"markdown","f625762a":"markdown","5c5711cc":"markdown"},"source":{"036a78a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf \nimport matplotlib.pyplot as plt\nimport sklearn as sk","467640e8":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1c209256":"%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\nsns.set(rc={'figure.figsize' : (10, 5)})\nsns.set_style(\"darkgrid\", {'axes.grid' : True})","46b8c919":"diabetes = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndia = diabetes.copy()","9025448b":"diabetes.describe().T","4297f62e":"# diabetes.Pregnancies = diabetes.Pregnancies.replace(0,333)\n# diabetes.Outcome = diabetes.Outcome.replace(0,333)\n# diabetes\n\n# diabetes = diabetes.replace(0, np.nan)\n# diabetes = diabetes.dropna()\n# # df = df.replace(np.nan, 0.0)\n\n# diabetes.Pregnancies = diabetes.Pregnancies.replace(333,0)\n# diabetes.Outcome = diabetes.Outcome.replace(333,0)\n# diabetes","066a2664":"corrMatrix = diabetes.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","82fe3bd3":"from sklearn.dummy import DummyClassifier\nx = diabetes.drop(columns = 'Outcome')\ny = diabetes['Outcome']","ce7c9538":"x.head()","b7c91029":"y.head()","2ff2a29a":"dummy = DummyClassifier('most_frequent') #returining most frequent class in this case 1\/\nresults = dummy.fit(x,y)\nresults.score(x,y)","509b4b36":"len( # Number of recrds that have at least one zero in it\n    x[(x.Glucose == 0) |\n    (x.BloodPressure ==0) |\n    (x.SkinThickness==0) |\n    (x.Insulin==0) |\n    (x.BMI==0) |\n    (x.DiabetesPedigreeFunction==0) |\n    (x.Age==0)]\n)","1ff353c4":"from tensorflow import keras\nimport tensorflow.keras.backend as K\n\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","c6217540":"def max_metric (history):\n    max_acc = max(history.history['accuracy'])\n    max_f1 = max(history.history['get_f1'])\n    min_loss = min(history.history['loss'])\n    max_val_acc = max(history.history['val_accuracy'])\n    max_val_f1 = max(history.history['val_get_f1'])\n    min_val_loss = min(history.history['val_loss'])\n    print(f\"Maximum Accuracy: {max_acc} \\nMaximum F1 Score: {max_f1} \\nMinimum Binary CrossEntropy Loss: {min_loss} \\nMaximum Validation Accuracy: {max_val_acc} \\nMaximum Validation F1 Score: {max_val_f1} \\nMaximum Validation Binary CrossEntropy Loss: {min_val_loss} \\n\")\n","1cd2411a":"def plot_this(history):\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    \n    # summarize history for f1\n    plt.plot(history.history['get_f1'])\n    plt.plot(history.history['val_get_f1'])\n    plt.title('model f1')\n    plt.ylabel('f1')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","bace4784":"diabetes.columns","30832163":"# normalize the data\n# we do not want to modify our label column Exited\ncols_to_norm = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n       'DiabetesPedigreeFunction', 'Age']\n\n# copy churn dataframe to churn_norm to do not affect the original data\ndia_norm = diabetes.copy()\n\n# normalize churn_norm dataframe \ndia_norm[cols_to_norm] = diabetes[cols_to_norm].apply(lambda x: (x - x.min())\/ (x.max() - x.min()) )\n\nx = dia_norm.drop(columns = 'Outcome')\ny = dia_norm['Outcome']","75296c7f":"dia_norm","fc9a903b":"dia_norm.describe().T","241846d5":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing\nimport warnings","dcd6558d":"min_max_scaler = preprocessing.MinMaxScaler()\nX_train_minmax = min_max_scaler.fit_transform(x)\nX_train, X_test, y_train, y_test = train_test_split(X_train_minmax, y, test_size=0.33, random_state=42)","26cb87cb":"parameters = {'C': np.linspace(0.0001, 100, 40)}\ngrid_search = GridSearchCV(LogisticRegression(max_iter=3000, class_weight={0:0.35, 1:0.65}), parameters, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nprint('best parameters: ', grid_search.best_params_)\nprint('best scrores: ', grid_search.best_score_)","b777deb9":"lr_clf = LogisticRegression(C=2.5642, max_iter=3000, class_weight={0:0.35, 1:0.65})\nlr_clf.fit(X_train, y_train)","90aab25e":"lr_clf.score(X_test, y_test)","267ebe68":"from sklearn.metrics import f1_score\ny_hat = lr_clf.predict(X_test)\nf1 = f1_score(y_test, y_hat)\nprint (f\"f1 socre is: {f1} \")","3c88bd5e":"#lr_clf.predict_proba(x.iloc[[700]]) #FOR PREDICtion","f96f3c7d":"parameters = {'C': np.linspace(0.0001, 100, 40)}\ngrid_search = GridSearchCV(svm.SVC(probability=True, max_iter=300, class_weight={1: 0.65, 0:0.35}), parameters, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nprint('best parameters: ', grid_search.best_params_)\nprint('best scrores: ', grid_search.best_score_)","2c01f48b":"from sklearn import svm\n\nclf = svm.SVC(C=28.205199999999998, gamma='auto', probability=True, verbose=True, max_iter=3000, class_weight={1: 0.65, 0:0.35})\nclf.fit(X_train, y_train)","f0a8c14b":"clf.score(X_test, y_test)","78b44607":"from sklearn.metrics import f1_score\ny_hat = clf.predict(X_test)\nf1 = f1_score(y_test, y_hat)\nprint (f\"f1 socre is: {f1} \")","3acd08e8":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)","5ae84c5a":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_pred)\nprint (f\"f1 socre is: {f1} \")","b883c46f":"model2 = tf.keras.Sequential()\nmodel2.add(tf.keras.layers.Dense(16, input_dim=x.shape[1], kernel_regularizer=tf.keras.regularizers.l2(0.001)))#activation = 'relu' ))\nmodel2.add(tf.keras.layers.ELU(alpha=1))\nmodel2.add(tf.keras.layers.Dropout(0.2))\nmodel2.add(tf.keras.layers.Dense(16,kernel_regularizer=tf.keras.regularizers.l2(0.001)))# activation='relu'))\nmodel2.add(tf.keras.layers.ELU(alpha=1))\nmodel2.add(tf.keras.layers.Dropout(0.2))\nmodel2.add(tf.keras.layers.Dense(16,kernel_regularizer=tf.keras.regularizers.l2(0.001)))# activation='relu'))\nmodel2.add(tf.keras.layers.ELU(alpha=1))\nmodel2.add(tf.keras.layers.Dropout(0.2))\nmodel2.add(tf.keras.layers.Dense(16,kernel_regularizer=tf.keras.regularizers.l2(0.001)))# activation='relu'))\nmodel2.add(tf.keras.layers.ELU(alpha=1))\nmodel2.add(tf.keras.layers.Dense(1, activation='sigmoid'))","bc96eeee":"model2.compile(optimizer='rmsprop', loss='MSE', metrics=['accuracy', get_f1])","b5d7b4dc":"history2 = model2.fit(X_train, y_train, validation_split=0.20, batch_size=64, workers=-1, epochs=100, verbose=2, class_weight={0:0.35, 1:0.65})","f2b88fd7":"max_metric(history2)\nplot_this(history2)","cfc28c27":"# model2.save(\"model2.h5\")","f983189a":"# from  tensorflow.keras.utils import plot_model\n# plot_model(model2, to_file='model.png', show_shapes=True, rankdir=\"LR\", expand_nested=False ,dpi=200)","2a668039":"from sklearn.metrics import f1_score\n\ny_pred = model2.predict(X_test, verbose=1)\ny_pred = y_pred>0.5\nf1 = f1_score(y_test, y_pred)\nprint (f\"f1 socre is: {f1} \")","7bc2090f":"# apply(lambda x: (x - x.min())\/ (x.max() - x.min()) \n# normalize the data\n# we do not want to modify our label column Exited\n# Pregnancies                  0.000\n# Glucose                      0.000\n# BloodPressure                0.000\n# SkinThickness                0.000\n# Insulin                      0.000\n# BMI                          0.000\n# DiabetesPedigreeFunction     0.078\n# Age                         21.000\n\n\n# Pregnancies                  17.00\n# Glucose                     199.00\n# BloodPressure               122.00\n# SkinThickness                99.00\n# Insulin                     846.00\n# BMI                          67.10\n# DiabetesPedigreeFunction      2.42\n# Age                          81.00\n\n\n## Sorry for hardcoding, i wll fix it ASAP!\n\ndef norm_a_data(data):\n    data[0] = (data[0] - 0)    \/ (17 - 0)\n    data[1] = (data[1] - 0)    \/ (199 - 0)\n    data[2] = (data[2] - 0)    \/ (122 - 0)\n    data[3] = (data[3] - 0)    \/ (99 - 0)\n    data[4] = (data[4] - 0)    \/ (846 - 0)\n    data[5] = (data[5] - 0)    \/ (67 - 0)\n    data[6] = (data[6] - 0.078) \/ (2 - 0.078)\n    data[7] = (data[7] - 21)    \/ (81 - 21)\n    return data[:]","27d03df3":"my_data = [6, 148, 72, 35, 0, 33.6, 0.627, 50]\nmy_data = diabetes.iloc[245].values[:8]\nmy_data = norm_a_data(my_data)\nmy_data=np.array(my_data)\nmy_data = my_data.reshape(8,1)\n# pd.DataFrame(my_data, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n#        'DiabetesPedigreeFunction', 'Age'])\nmodel2.predict(my_data.transpose())","924377b5":"diabetes.iloc[245].values[-1]","5237bbeb":"# TODO\n- \u2705 add unblacend class\n- \u2b1c batch norm\n- \u2b1c regularizer\n- \u2705 train\/dev\/test\n- \u2b1c lr tuner\n- \u2705 add normalizing \n\n\n\n","788137a1":"## Some Funcs","dc0b2c8a":"## Naive Bayes","d9649f95":"## Neural net","6f7a79dd":"## Normalization","206241f9":"## SVM","f625762a":"Reading csv files and showing first and last 5 records. ","5c5711cc":"## Logistic Regression"}}