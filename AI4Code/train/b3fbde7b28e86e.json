{"cell_type":{"aabf098a":"code","9b1fd2c8":"code","657f3103":"code","d2df0d26":"code","2bb1c4fb":"code","08d9d16e":"code","2e5a79d3":"code","90710b28":"code","f8832f77":"code","07c2e495":"code","a18bc07e":"code","7a7209d5":"code","b6eda4c6":"code","9e431732":"code","c1586b6e":"code","409cfc22":"code","659dfca7":"code","fba25e7a":"code","574461da":"code","e3c807b3":"code","f8683cd0":"code","b6bbc889":"code","b76e5ea9":"code","b6e2bf58":"code","b0cb1158":"code","30823b76":"code","f6bd7d93":"code","5a19b5f1":"code","06798061":"code","5a50e54f":"code","6714e586":"code","76f9350f":"markdown","3d0801fe":"markdown","78e0640f":"markdown","5f9289be":"markdown","62395d1e":"markdown","e4a285f6":"markdown","d757e396":"markdown","f395ada0":"markdown","84c84c9a":"markdown","6fde3371":"markdown"},"source":{"aabf098a":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGradientBoostingEstimator","9b1fd2c8":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","657f3103":"# dimensions\nprint('Train Set:', df_train.shape)\nprint('Test Set :', df_test.shape)","d2df0d26":"# structure\ndf_train.info()","2bb1c4fb":"# basic stats\nprint(df_train.target.value_counts())\ndf_train.target.value_counts().sort_index().plot(kind='bar')\nplt.grid()\nplt.show()","08d9d16e":"# extract features from column names\nfeatures = df_train.columns.tolist()\nfeatures.remove('id')\nfeatures.remove('target')","2e5a79d3":"# basic summary stats\npd.set_option('display.max_columns', None) # show all columns\ndf_train[features].describe()","90710b28":"# correlation\ncorr_pearson = df_train[features].corr(method='pearson')\nplt.figure(figsize=(12,12))\nsns.heatmap(corr_pearson, annot=False, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","f8832f77":"print('Maximum correlation:', np.round(corr_pearson[corr_pearson!=1].max().max(),5))\nprint('Minimum correlation:', np.round(corr_pearson[corr_pearson!=1].min().min(),5))","07c2e495":"# select a feature\nf = 'feature_2'","a18bc07e":"# show distribution\ndf_train[f].value_counts().plot(kind='bar')\nplt.title('Distribution of ' + f)\nplt.grid()\nplt.show()","7a7209d5":"# violinplots by class\nplt.figure(figsize=(10,5))\nsns.violinplot(x=f, y='target', data=df_train)\nmy_title = 'Distribution by class for ' + f\nplt.title(my_title)\nplt.grid()","b6eda4c6":"# cross table - absolute counts...\nctab = pd.crosstab(df_train.target, df_train[f])\nctab","9e431732":"# ...and normalized by column\nctab_norm = ctab \/ ctab.sum()\nctab_norm","c1586b6e":"# visualize\nplt.figure(figsize=(12,6))\np1 = plt.bar(ctab_norm.columns, ctab_norm.iloc[0])\nbot = ctab_norm.iloc[0]\np2 = plt.bar(ctab_norm.columns, ctab_norm.iloc[1], bottom=bot)\nbot = bot + ctab_norm.iloc[1]\np3 = plt.bar(ctab_norm.columns, ctab_norm.iloc[2], bottom=bot)\nbot = bot + ctab_norm.iloc[2]\np4 = plt.bar(ctab_norm.columns, ctab_norm.iloc[3], bottom=bot)\nplt.xlabel('Feature Value')\nplt.ylabel('Relative Frequency of Target Classes')\nplt.title('Target vs ' + f)\nplt.legend((p1[0],p2[0],p3[0],p4[0]), ('Class_1', 'Class_2', 'Class_3', 'Class_4'))\nplt.grid()\nplt.show()","409cfc22":"# select predictors\npredictors = features\nprint('Number of predictors: ', len(predictors))","659dfca7":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","fba25e7a":"# upload data frames in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n\n# force categorical target\ntrain_hex['target'] = train_hex['target'].asfactor()","574461da":"# factors for class sampling => trying to mitigate unbalanced target distribution\ncsf = [6.77, 1.00, 2.68, 4.57]","e3c807b3":"# fit Gradient Boosting model\nn_cv = 5\n\nfit_GBM = H2OGradientBoostingEstimator(ntrees=100,\n                                       max_depth=6,\n                                       min_rows=50,\n                                       learn_rate=0.1, # default: 0.1\n                                       sample_rate=1,\n                                       col_sample_rate=0.5,\n                                       nfolds=n_cv,\n                                       score_each_iteration=True,\n                                       stopping_metric='logloss',\n                                       stopping_rounds=5,\n                                       stopping_tolerance=0.0001,\n                                       balance_classes=True,\n                                       class_sampling_factors=csf,\n                                       seed=999)\n# train model\nt1 = time.time()\nfit_GBM.train(x=predictors,\n              y='target',\n              training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","f8683cd0":"# show cross validation metrics\nfit_GBM.cross_validation_metrics_summary()","b6bbc889":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_GBM.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_logloss, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_logloss, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('logloss')\n    plt.ylim(0,2)\n    plt.legend()\n    plt.grid()\n    plt.show()","b76e5ea9":"# variable importance\nfit_GBM.varimp_plot(-1)","b6e2bf58":"# predict on train set\npred_train_GBM = fit_GBM.predict(train_hex).as_data_frame()\n# add ground truth\npred_train_GBM['target'] = train_hex['target'].as_data_frame()\npred_train_GBM.head()","b0cb1158":"# predicted frequencies\npred_train_GBM[['Class_1','Class_2','Class_3','Class_4']].sum()","30823b76":"# actual frequencies\ndf_train.target.value_counts().sort_index()","f6bd7d93":"# confusion matrix - training data\nconf_train = pd.crosstab(pred_train_GBM.target, pred_train_GBM.predict)\nsns.heatmap(conf_train, cmap='Blues',\n            annot=True, fmt='d',\n            vmin=0, vmax=60000,\n            linecolor='black',\n            linewidths=0.1)\nplt.title('Confusion Matrix - Training')\nplt.show()","5a19b5f1":"# predict on test set\npred_test_GBM = fit_GBM.predict(test_hex).as_data_frame()\npred_test_GBM","06798061":"# submission\ndf_sub_GBM = df_sub.copy()\ndf_sub_GBM.Class_1 = pred_test_GBM.Class_1\ndf_sub_GBM.Class_2 = pred_test_GBM.Class_2\ndf_sub_GBM.Class_3 = pred_test_GBM.Class_3\ndf_sub_GBM.Class_4 = pred_test_GBM.Class_4\ndf_sub_GBM","5a50e54f":"# export submission\ndf_sub_GBM.to_csv('submission_GBM.csv', index=False)","6714e586":"# multi-dimensional visualization of submission\nsns.pairplot(df_sub_GBM[['Class_1','Class_2','Class_3','Class_4']],\n             diag_kws = {'alpha': 1.0},\n             plot_kws = {'alpha': 0.1})\nplt.show()","76f9350f":"### We observe almost no correlation between the features...","3d0801fe":"# Table of Contents\n* [Target Exploration](#1)\n* [Features](#2)\n* [Target vs Features](#3)\n* [Fit Model](#4)","78e0640f":"### Evaluate Predictions on Training Data","5f9289be":"<a id='2'><\/a>\n# Features","62395d1e":"### => No missings this time :-)","e4a285f6":"<a id='1'><\/a>\n# Target Exploration","d757e396":"### Predict on Test Set","f395ada0":"### We have too many features here to plot all, so let's just show an example:","84c84c9a":"<a id='4'><\/a>\n# Fit Model","6fde3371":"<a id='3'><\/a>\n# Target vs Features"}}