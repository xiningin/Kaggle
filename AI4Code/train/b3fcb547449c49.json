{"cell_type":{"6811b53d":"code","c5d2e57a":"code","155e28cc":"code","47dddd3c":"code","4b1f42c5":"code","2145ea1f":"code","219b8060":"code","7ac9cb32":"code","9ebcbc09":"code","fe3d106c":"code","9ec9be10":"code","8ddd5df3":"code","761b2de3":"code","0a235710":"code","e5037a04":"code","4b772353":"code","40ba4a17":"code","aa5229a8":"code","6160fcc3":"code","34e4eb7a":"code","e957864d":"code","75271c48":"code","b443bd7c":"code","3ee28c86":"code","4a22b6a2":"code","ab14d51e":"code","e74b3713":"code","7cd0bda6":"code","2581ca63":"code","95978b27":"code","a9a986dd":"markdown"},"source":{"6811b53d":"\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport sklearn","c5d2e57a":"# get titanic & test csv files as a DataFrame\ntitanic_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df    = pd.read_csv(\"..\/input\/test.csv\")\n\n# preview the data\ntitanic_df.head()","155e28cc":"titanic_df.info()\nprint(\"----------------------------\")\ntest_df.info()","47dddd3c":"def get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'\n\ndef title_map(title):\n    if title in ['Mr']:\n        return 1\n    elif title in ['Master']:\n        return 3\n    elif title in ['Ms','Mlle','Miss']:\n        return 4\n    elif title in ['Mme','Mrs']:\n        return 5\n    else:\n        return 2\n    \ntitanic_df['title'] = titanic_df['Name'].apply(get_title).apply(title_map)   \ntest_df['title'] = test_df['Name'].apply(get_title).apply(title_map)\ntitle_xt = pd.crosstab(titanic_df['title'], titanic_df['Survived'])\ntitle_xt_pct = title_xt.div(title_xt.sum(1).astype(float), axis=0)\n\ntitle_xt_pct.plot(kind='bar', \n                  stacked=True, \n                  title='Survival Rate by title')\nplt.xlabel('title')\nplt.ylabel('Survival Rate')","4b1f42c5":"# drop unnecessary columns, these columns won't be useful in analysis and prediction\ntitanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\ntest_df    = test_df.drop(['Name','Ticket'], axis=1)","2145ea1f":"# Embarked\n\n# only in titanic_df, fill the two missing values with the most occurred value, which is \"S\".\ntitanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].fillna(\"S\")\n\n# plot\n#sns.factorplot('Embarked','Survived', data=titanic_df,size=4,aspect=3)\n\n#fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n\n# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)\n# sns.factorplot('Survived',hue=\"Embarked\",data=titanic_df,kind='count',order=[1,0],ax=axis2)\n#sns.countplot(x='Embarked', data=titanic_df, ax=axis1)\n#sns.countplot(x='Survived', hue=\"Embarked\", data=titanic_df, order=[1,0], ax=axis2)\n\n# group by embarked, and get the mean for survived passengers for each value in Embarked\n#embark_perc = titanic_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\n#sns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n\n# Either to consider Embarked column in predictions,\n# and remove \"S\" dummy variable, \n# and leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n\n# OR, don't create dummy variables for Embarked column, just drop it, \n# because logically, Embarked doesn't seem to be useful in prediction.\n\nembark_dummies_titanic  = pd.get_dummies(titanic_df['Embarked'])\nembark_dummies_titanic.drop(['S'], axis=1, inplace=True)\n#print(embark_dummies_titanic)\n\nembark_dummies_test  = pd.get_dummies(test_df['Embarked'])\nembark_dummies_test.drop(['S'], axis=1, inplace=True)\n\ntitanic_df = titanic_df.join(embark_dummies_titanic)\n#print(titanic_df)\ntest_df    = test_df.join(embark_dummies_test)\n\ntitanic_df.drop(['Embarked'], axis=1,inplace=True)\ntest_df.drop(['Embarked'], axis=1,inplace=True)","219b8060":"## Fare\n\n# only for test_df, since there is a missing \"Fare\" values\ntest_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n\ntitanic_df.loc[ titanic_df['Fare'] <= 7.91, 'Fare'] = 0\ntitanic_df.loc[(titanic_df['Fare'] > 7.91) & (titanic_df['Fare'] <= 14.454), 'Fare'] = 1\ntitanic_df.loc[(titanic_df['Fare'] > 14.454) & (titanic_df['Fare'] <= 31), 'Fare'] = 2\ntitanic_df.loc[ titanic_df['Fare'] > 31, 'Fare'] = 3\ntest_df.loc[ test_df['Fare'] <= 7.91, 'Fare'] = 0\ntest_df.loc[(test_df['Fare'] > 7.91) & (test_df['Fare'] <= 14.454), 'Fare'] = 1\ntest_df.loc[(test_df['Fare'] > 14.454) & (test_df['Fare'] <= 31), 'Fare'] = 2\ntest_df.loc[test_df['Fare'] > 31, 'Fare'] = 3\n\n# convert from float to int\ntitanic_df['Fare'] = titanic_df['Fare'].astype(int)\ntest_df['Fare']    = test_df['Fare'].astype(int)\n\n\n# get fare for survived & didn't survive passengers \n#fare_not_survived = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 0]\n#fare_survived     = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 1]\n\n# get average and std for fare of survived\/not survived passengers\n#avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n#std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n\n# plot\n#titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n\n#avgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\n#avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)","7ac9cb32":"# Age impute\n\ntitanic_df['Age'] = titanic_df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))\ntest_df['Age'] = test_df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.mean()))","9ebcbc09":"# Age \n\n#fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n#axis1.set_title('Original Age values - Titanic')\n#axis2.set_title('New Age values - Titanic')\n\n# axis3.set_title('Original Age values - Test')\n# axis4.set_title('New Age values - Test')\n\n# get average, std, and number of NaN values in titanic_df\n#average_age_titanic   = titanic_df[\"Age\"].mean()\n#std_age_titanic       = titanic_df[\"Age\"].std()\n#count_nan_age_titanic = titanic_df[\"Age\"].isnull().sum()\n\n# get average, std, and number of NaN values in test_df\n#average_age_test   = test_df[\"Age\"].mean()\n#std_age_test       = test_df[\"Age\"].std()\n#count_nan_age_test = test_df[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\n#rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\n#rand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\n#titanic_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# fill NaN values in Age column with random values generated\n#titanic_df[\"Age\"][np.isnan(titanic_df[\"Age\"])] = rand_1\n#test_df[\"Age\"][np.isnan(test_df[\"Age\"])] = rand_2\n\n# convert from float to int\ntitanic_df['Age'] = titanic_df['Age'].astype(int)\ntest_df['Age']    = test_df['Age'].astype(int)\n\ntitanic_df.loc[ titanic_df['Age'] <= 16, 'Age'] = 0\ntitanic_df.loc[(titanic_df['Age'] > 16) & (titanic_df['Age'] <= 32), 'Age'] = 1\ntitanic_df.loc[(titanic_df['Age'] > 32) & (titanic_df['Age'] <= 48), 'Age'] = 2\ntitanic_df.loc[(titanic_df['Age'] > 48) & (titanic_df['Age'] <= 64), 'Age'] = 3\ntitanic_df.loc[(titanic_df['Age'] > 64), 'Age'] = 4\n\ntest_df.loc[ test_df['Age'] <= 16, 'Age'] = 0\ntest_df.loc[(test_df['Age'] > 16) & (test_df['Age'] <= 32), 'Age'] = 1\ntest_df.loc[(test_df['Age'] > 32) & (test_df['Age'] <= 48), 'Age'] = 2\ntest_df.loc[(test_df['Age'] > 48) & (test_df['Age'] <= 64), 'Age'] = 3\ntest_df.loc[(test_df['Age'] > 64), 'Age'] = 4\n        \n# plot new Age Values\n#titanic_df['Age'].hist(bins=70, ax=axis2)\n# test_df['Age'].hist(bins=70, ax=axis4)","fe3d106c":"# .... continue with plot Age column\n\n# peaks for survived\/not survived passengers by their age\n#facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n#facet.map(sns.kdeplot,'Age',shade= True)\n#facet.set(xlim=(0, titanic_df['Age'].max()))\n#facet.add_legend()\n\n# average survived passengers by age\n#fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n#average_age = titanic_df[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\n#sns.barplot(x='Age', y='Survived', data=average_age)","9ec9be10":"# Cabin\n# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\ntitanic_df.drop(\"Cabin\",axis=1,inplace=True)\ntest_df.drop(\"Cabin\",axis=1,inplace=True)","8ddd5df3":"# Family\n\n# Instead of having two columns Parch & SibSp, \n# we can have only one column represent if the passenger had any family member aboard or not,\n# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\ntitanic_df['Family'] =  titanic_df[\"Parch\"] + titanic_df[\"SibSp\"]\ntitanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\ntitanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n\ntest_df['Family'] =  test_df[\"Parch\"] + test_df[\"SibSp\"]\ntest_df['Family'].loc[test_df['Family'] > 0] = 1\ntest_df['Family'].loc[test_df['Family'] == 0] = 0\n\n# drop Parch & SibSp\ntitanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\ntest_df    = test_df.drop(['SibSp','Parch'], axis=1)\n\n# plot\n#fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n\n# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n#sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n\n# average of survived for those who had\/didn't have any family member\n#family_perc = titanic_df[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\n#sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n\n#axis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)","761b2de3":"# Sex\n\n# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n# So, we can classify passengers as males, females, and child\n#def get_person(passenger):\n    #age,sex = passenger\n    #return 'child' if age < 16 else sex\n    \n#titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)\n#test_df['Person']    = test_df[['Age','Sex']].apply(get_person,axis=1)\n\n# No need to use Sex column since we created Person column\n#titanic_df.drop(['Sex'],axis=1,inplace=True)\n#test_df.drop(['Sex'],axis=1,inplace=True)\n\n# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\n#person_dummies_titanic  = pd.get_dummies(titanic_df['Person'])\n#person_dummies_titanic.columns = ['Child','Female','Male']\n#person_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n\n#person_dummies_test  = pd.get_dummies(test_df['Person'])\n#print(person_dummies_test)\n#person_dummies_test.columns = ['Child','Female','Male']\n#person_dummies_test.drop(['Male'], axis=1, inplace=True)\n\n#titanic_df = titanic_df.join(person_dummies_titanic)\n#test_df    = test_df.join(person_dummies_test)\n\n#fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n\n# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)\n#sns.countplot(x='Person', data=titanic_df, ax=axis1)\n\n# average of survived for each Person(male, female, or child)\n#person_perc = titanic_df[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\n#sns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n\n#titanic_df.drop(['Person'],axis=1,inplace=True)\n#test_df.drop(['Person'],axis=1,inplace=True)\nsexes = sorted(titanic_df['Sex'].unique())\ngenders_mapping = dict(zip(sexes, range(0, len(sexes) + 1)))\ntitanic_df['Sex'] = titanic_df['Sex'].map(genders_mapping).astype(int)\ntest_df['Sex'] = test_df['Sex'].map(genders_mapping).astype(int)","0a235710":"# Pclass\n\n# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n#sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n\n# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n#pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n#pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n#pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n\n#pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n#pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n#pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n\n#titanic_df.drop(['Pclass'],axis=1,inplace=True)\n#test_df.drop(['Pclass'],axis=1,inplace=True)\n\n#titanic_df = titanic_df.join(pclass_dummies_titanic)\n#test_df    = test_df.join(pclass_dummies_test)\ntitanic_df['age_class'] = titanic_df['Age'] * titanic_df['Pclass']\ntest_df['age_class'] = test_df['Age'] * test_df['Pclass']","e5037a04":"titanic_df.head()\ntest_df.head()","4b772353":"# define training and testing sets\n\nX_train = titanic_df.drop(\"Survived\",axis=1)\nY_train = titanic_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\",axis=1).copy()","40ba4a17":"# Logistic Regression\n\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nlogreg.score(X_train, Y_train)","aa5229a8":"##Support Vector Machines\n\nsvc = SVC()\n\nsvc.fit(X_train, Y_train)\n\nY_pred_4 = svc.predict(X_test)\n\nsvc.score(X_train, Y_train)","6160fcc3":"# Random Forests\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n#random_forest = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, max_features='sqrt', min_samples_split=5)\n\nrandom_forest.fit(X_train, Y_train)\n\nY_pred_1 = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)","34e4eb7a":"#grid_2 = { \"loss\"          : [\"deviance\",\"exponential\"],\n #              \"n_estimators\"  : [100],\n #             \"max_features\"      : ['sqrt','log2',0.2,0.5,0.8]}\n#GB=GradientBoostingClassifier()\n#grid_search = sklearn.model_selection.GridSearchCV(GB, grid_2, n_jobs=-1, cv=5)\n#grid_search.fit(X_train, Y_train)\n#grid_search.best_params_","e957864d":"#gradient_boost = GradientBoostingClassifier(n_estimators=100,loss='exponential',max_features='log2')\ngradient_boost = GradientBoostingClassifier(n_estimators=100)\ngradient_boost.fit(X_train, Y_train)\n\nY_pred_2 = gradient_boost.predict(X_test)\n\ngradient_boost.score(X_train, Y_train)","75271c48":"#grid_3 = { \"n_estimators\" : [100],\n              # \"criterion\"         : [\"gini\", \"entropy\"],\n             #  \"max_features\"      : ['sqrt','log2',0.2,0.5,0.8],\n             #  \"max_depth\"         : [4,7,10],\n            #   \"min_samples_split\" : [2, 5, 10] }\n#ET=ExtraTreesClassifier()\n#grid_search = sklearn.model_selection.GridSearchCV(ET, grid_3, n_jobs=-1, cv=5)\n#grid_search.fit(X_train, Y_train)\n#grid_search.best_params_","b443bd7c":"#extra_tree = ExtraTreesClassifier(n_estimators=100,criterion='gini',max_depth=10,max_features='log2',min_samples_split=10)\nextra_tree = ExtraTreesClassifier(n_estimators=100)\nextra_tree.fit(X_train, Y_train)\n\nY_pred_3 = extra_tree.predict(X_test)\n\nextra_tree.score(X_train, Y_train)","3ee28c86":"#grid_4 = { \"n_estimators\"      : [100,150,200],\n               #\"algorithm\"  : ['SAMME','SAMME.R'] }\n#AB=AdaBoostClassifier()\n#grid_search = sklearn.model_selection.GridSearchCV(AB, grid_4, n_jobs=-1, cv=5)\n#grid_search.fit(X_train, Y_train)\n#grid_search.best_params_","4a22b6a2":"ada_boost = AdaBoostClassifier(n_estimators=100,algorithm='SAMME')\n\nada_boost.fit(X_train, Y_train)\n\nY_pred_4 = ada_boost.predict(X_test)\n\nada_boost.score(X_train, Y_train)","ab14d51e":"#grid_5 = { \"n_neighbors\"      : [3,5,7],\n               #\"weights\"  : ['uniform','distance'] }\n#KNN=sklearn.neighbors.KNeighborsClassifier()\n#grid_search = sklearn.model_selection.GridSearchCV(KNN, grid_5, n_jobs=-1, cv=5)\n#grid_search.fit(X_train, Y_train)\n#grid_search.best_params_","e74b3713":"knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 7,weights='distance')\nknn.fit(X_train, Y_train)\nY_pred_5 = knn.predict(X_test)\nknn.score(X_train, Y_train)","7cd0bda6":"knn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nknn.score(X_train, Y_train)","2581ca63":"\n\ngaussian = GaussianNB()\n\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\ngaussian.score(X_train, Y_train)","95978b27":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","a9a986dd":"Exploratory data analysis & model fit & predict"}}