{"cell_type":{"bf446a6b":"code","b2b53209":"code","abb74c03":"code","a996e456":"code","0e193e01":"code","27b8a0a8":"code","5dd7429b":"code","e8379b9f":"code","57505c68":"code","82154644":"code","730deb2e":"code","ba2615f9":"code","2368a687":"code","50710725":"code","6c3e3cd9":"code","151e2d8f":"code","6aa63be8":"code","31a91cef":"code","11cc86f6":"code","fef27131":"code","87752bd0":"code","b6cf5e57":"code","0c6dfe96":"code","a915657e":"code","84cb8c48":"code","d2779a9a":"code","6dc4f801":"code","bdde4daa":"code","922bcd09":"code","95da3f93":"code","8fd9969f":"code","da77878d":"code","d6dae5b9":"code","4cb383d0":"code","12dd3be7":"code","78641e0f":"code","0f02a60e":"code","3d369022":"code","4d1c9b3b":"code","a9ff5e1b":"code","34c810d9":"code","ee0bec8d":"code","7d7b3fc2":"code","9e30f493":"code","036a1372":"code","128e0d90":"code","93e9dfe6":"code","fafc45e9":"code","65cb274f":"code","f4d6dadd":"code","de8403dc":"code","438b2b8f":"code","7e205e05":"code","e2141a26":"code","621764ee":"code","3d0a3328":"code","a7fff479":"code","dd6dae1e":"markdown","9925cbbd":"markdown","d53732ac":"markdown","18f8e42a":"markdown","59924808":"markdown","f6f47dc2":"markdown","f00d46d4":"markdown","b582cf67":"markdown","2489e52a":"markdown","c2aa3bb4":"markdown","26b06566":"markdown","e51a57a4":"markdown","c2a394f5":"markdown","88ebb908":"markdown","70582ba8":"markdown","a3a22453":"markdown","c6f417e7":"markdown","1b35d484":"markdown","a3932cb3":"markdown","3b18dac2":"markdown","9dd3e329":"markdown","80b6a295":"markdown","af58ecdb":"markdown","c146c6c6":"markdown","a01f47a8":"markdown","a1a0bfba":"markdown","d3bcd9f4":"markdown","c4627de1":"markdown","54685491":"markdown","d14a839f":"markdown","6536751e":"markdown","734e577a":"markdown","2df511a6":"markdown","28649186":"markdown","4ca05193":"markdown"},"source":{"bf446a6b":"!pip install nb_black -q","b2b53209":"%load_ext nb_black","abb74c03":"%%bash\n\napt install --assume-yes p7zip-full\n7z x ..\/input\/mercari-price-suggestion-challenge\/train.tsv.7z -y\n7z x ..\/input\/mercari-price-suggestion-challenge\/test.tsv.7z -y\n7z x ..\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip -y\n7z x ..\/input\/mercari-price-suggestion-challenge\/sample_submission.csv.7z -y\n7z x ..\/input\/mercari-price-suggestion-challenge\/sample_submission_stg2.csv.zip -y","a996e456":"import math\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom wordcloud import WordCloud\nfrom string import punctuation\nfrom string import punctuation\nimport nltk\n\nwarnings.filterwarnings(\"ignore\")","0e193e01":"train = pd.read_csv(\n    \"train.tsv\",\n    sep=\"\\t\",\n    usecols=[\n        \"name\",\n        \"item_condition_id\",\n        \"category_name\",\n        \"brand_name\",\n        \"price\",\n        \"shipping\",\n        \"item_description\",\n    ],\n)\ntrain.head()","27b8a0a8":"train.brand_name.fillna(\"No Brand\", inplace=True)\ntrain.dropna(inplace=True)\ntrain.info()","5dd7429b":"punctuation = [p for p in punctuation]\nstopwords = nltk.corpus.stopwords.words(\"english\")\nstopwords = stopwords + punctuation + [\"...\"] + [\"!!\"]\ntoken_punct = nltk.WordPunctTokenizer()\nstemmer = nltk.RSLPStemmer()","e8379b9f":"def plot_value_counts(serie, name_column, number=20):\n    y = serie.value_counts()[:number].values\n    x = serie.value_counts()[:number].index\n\n    fig = go.Figure(data=[go.Bar(x=x, y=y, text=y, textposition=\"auto\",)])\n    fig.update_layout(\n        title_text=\"Counting \" + name_column,\n        xaxis_title=name_column,\n        yaxis_title=\"count\",\n    )\n    fig.show()\n\n\ndef hist_plot(serie, titles=[\"Histogram\", \"Acumulative\"]):\n    fig = make_subplots(rows=1, cols=2, subplot_titles=titles)\n    fig.add_trace(\n        go.Histogram(x=serie), row=1, col=1,\n    )\n    fig.add_trace(\n        go.Histogram(x=serie, cumulative_enabled=True), row=1, col=2,\n    )\n    fig.show()\n\n\ndef remove_punct(my_str):\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuation:\n            no_punct = no_punct + char\n    return no_punct\n\n\ndef tokenizer_column(serie):\n    clear_col = list()\n    for row in serie:\n        new_line = list()\n        line = token_punct.tokenize(remove_punct(row.lower()))\n        for word in line:\n            if word not in stopwords:  # stopwords\n                new_line.append(stemmer.stem(word))\n        clear_col.append(\" \".join(new_line))\n    return clear_col\n\n\ndef wordcloud(text, column_name, title):\n    all_words = \" \".join([text for text in text[column_name]])\n    wordcloud = WordCloud(\n        width=800, height=500, max_font_size=110, collocations=False\n    ).generate(all_words)\n    plt.figure(figsize=(24, 12))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()","57505c68":"column = train.category_name\nprint(\"How much NAN we have here?\", column.isna().sum())\nprint(\"How much categories we have here?\", len(column.unique()))\ntrain.category_name = train.category_name.fillna(\"no category\")","82154644":"plot_value_counts(column, \"category_name\")","730deb2e":"def transform_split_category_name(df):\n    aux = df[\"category_name\"].str.split(\"\/\", n=2, expand=True)\n    for i in [0, 1, 2]:\n        df[\"category_name_\" + str(i)] = aux[i]\n        df[\"category_name_\" + str(i)].fillna(\"No category\", inplace=True)\n\n    return df","ba2615f9":"transform_split_category_name(train)\nplot_value_counts(train.category_name_0, \"category_name_0\")\nplot_value_counts(train.category_name_1, \"category_name_1\")\nplot_value_counts(train.category_name_2, \"category_name_2\")","2368a687":"train = train[train.category_name_0.isin([\"Electronics\"])]","50710725":"plot_value_counts(train.category_name_1, \"category_name_1\")","6c3e3cd9":"plot_value_counts(train.category_name_2, \"category_name_2\")","151e2d8f":"train.info()","6aa63be8":"column = train.name\nprint(\"How much NAN we have here?\", column.isna().sum())\nprint(\"How much categories we have here?\", len(column.unique()))","31a91cef":"train.name = tokenizer_column(train.name)","11cc86f6":"plot_value_counts(column, \"name\")","fef27131":"wordcloud(train, \"name\", \"Name wordcloud\")","87752bd0":"train.item_description = tokenizer_column(train.item_description)","b6cf5e57":"wordcloud(train, \"item_description\", \"Description wordcloud\")","0c6dfe96":"column = train.item_condition_id\nprint(\"How much NAN we have here?\", column.isna().sum())\nprint(\"How much categories we have here?\", len(column.unique()))\nplot_value_counts(column, \"item_condition_id\")","a915657e":"column = train.brand_name\nprint(\"How much NAN we have here?\", column.isna().sum())\nprint(\"How much categories we have here?\", len(column.unique()))","84cb8c48":"train.brand_name = train.brand_name.fillna(\"no brand\")\ntrain.brand_name = train.brand_name.str.lower()","d2779a9a":"plot_value_counts(column.sample(10000), \"brand_name\")","6dc4f801":"list_top_50_brand = train.brand_name.value_counts()[:51].index\nlist_top_50_brand","bdde4daa":"brands_column = []\nfor i, brand in enumerate(train.brand_name):\n    if not brand in list_top_50_brand:\n        brands_column.append(\"other brand\")\n    else:\n        brands_column.append(brand)\nbrands_column[:5]","922bcd09":"train.brand_name = brands_column\nplot_value_counts(column.sample(10000), \"brand_name\", 52)","95da3f93":"column = train.price\nprint(\"How much NAN we have here?\", column.isna().sum())","8fd9969f":"train = train[train.price.isin([0]) == False]  # removing 0 prices\nroof = train.price.quantile(0.98)  # removing outlines\nprint(f\"removing values higher than {roof}\")\ntrain = train.query(f\"price < {roof}\")","da77878d":"hist_plot(column.sample(10000))","d6dae5b9":"train[\"log_price\"] = np.log(train.price)\nhist_plot(train[\"log_price\"].sample(10000))","4cb383d0":"column = train.shipping\nprint(\"How much NAN we have here?\", column.isna().sum())\nprint(\"How much categories we have here?\", len(column.unique()))","12dd3be7":"plot_value_counts(column.sample(10000), \"shipping\")","78641e0f":"train.info()","0f02a60e":"px.box(train.sample(10000), x=\"item_condition_id\", y=\"price\", color='item_condition_id',title=\"Price boxplot by condition id\")\n","3d369022":"table = train.groupby(\"item_condition_id\")[\"price\"].describe().round(3)\nff.create_table(table, height_constant=40, index=True, index_title=\"item_condition_id\")","4d1c9b3b":"# 1 means free ship for the customer\n# 0 means the customer have to pay the ship\n\ntable = train.groupby(\"item_condition_id\")[\"shipping\"].describe()[[\"count\", \"mean\"]]\ntable[\"free ship in %\"] = table[\"mean\"].round(3) * 100\ntable[\"not free ship in %\"] = 100 - (table[\"mean\"].round(3) * 100)\nff.create_table(\n    table.round(3), height_constant=40, index=True, index_title=\"item_condition_id\"\n)","a9ff5e1b":"most_popular_categories = train.category_name.value_counts()[:30].index\n\n\ndef count_cond_cat(id_v):\n    aux = train[\n        train.item_condition_id.isin([id_v])\n        & train.category_name.isin(most_popular_categories)\n    ]\n    aux = aux.category_name_0.value_counts()\n    x = aux.index\n    y = aux.values\n    return x, y","34c810d9":"fig = go.Figure()\n\nx, y = count_cond_cat(1)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 1\"))\n\nx, y = count_cond_cat(2)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 2\"))\n\nx, y = count_cond_cat(3)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 3\"))\n\nx, y = count_cond_cat(4)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 4\"))\n\nx, y = count_cond_cat(5)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 5\"))\n\nfig.update_layout(\n    title=\"Comparative item_condition by most popular categories in our dataset\",\n    xaxis_tickfont_size=14,\n    yaxis=dict(title=\"count\", titlefont_size=16, tickfont_size=14,),\n    legend=dict(\n        bgcolor=\"rgba(255, 255, 255, 0)\", bordercolor=\"rgba(255, 255, 255, 0)\",\n    ),\n    barmode=\"group\",\n    bargap=0.15,  # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1,  # gap between bars of the same location coordinate.\n)\nfig.show()","ee0bec8d":"most_popular_categories = train.brand_name.value_counts()[\n    :30\n].index  # skipping the most popular: 'No brand' hahaha\n\n\ndef count_brand_cat(id_v):\n    aux = train[\n        train.item_condition_id.isin([id_v])\n        & train.brand_name.isin(most_popular_categories)\n    ]\n    aux = aux.brand_name.value_counts()\n    x = aux.index\n    y = aux.values\n    return x, y","7d7b3fc2":"fig = go.Figure()\n\nx, y = count_brand_cat(1)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 1\"))\n\nx, y = count_brand_cat(2)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 2\"))\n\nx, y = count_brand_cat(3)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 3\"))\n\nx, y = count_brand_cat(4)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 4\"))\n\nx, y = count_brand_cat(5)\nfig.add_trace(go.Bar(x=x, y=y, name=\"item_condition_id = 5\"))\n\nfig.update_layout(\n    title=\"Comparative brand_name by most popular categories in our dataset\",\n    xaxis_tickfont_size=14,\n    yaxis=dict(title=\"count\", titlefont_size=16, tickfont_size=14,),\n    legend=dict(\n        bgcolor=\"rgba(255, 255, 255, 0)\", bordercolor=\"rgba(255, 255, 255, 0)\",\n    ),\n    barmode=\"group\",\n    bargap=0.15,  # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1,  # gap between bars of the same location coordinate.\n)\nfig.show()","9e30f493":"roof = train.price.quantile(0.95)\npx.box(\n    train.query(f\"price < {roof}\").sample(10000),\n    x=\"brand_name\",\n    y=\"price\",\n    color=\"brand_name\",\n    title=\"Price boxplot by brand_name\",\n)","036a1372":"px.box(\n    train.sample(10000),\n    x=\"brand_name\",\n    y=\"log_price\",\n    color=\"brand_name\",\n    title=\"Price boxplot by brand_name with log transformation\",\n)","128e0d90":"train = train.sample(10000)\ntrain.reset_index(inplace=True, drop=True)\ntrain.head()","93e9dfe6":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.linear_model import (\n    BayesianRidge,\n    SGDRegressor,\n)\n\n# split the dataset beteween train and test\ndef split(x, y, plot=False):\n    # seed\n    # train_test_split\n    train_x, test_x, train_y, test_y = train_test_split(\n        x, y, test_size=0.1, random_state=42367,\n    )\n    if plot:\n        print(\n            \"sizes: train (x,y) and test (x,y)\",\n            train_x.shape,\n            train_y.shape,\n            test_x.shape,\n            test_y.shape,\n        )\n    return train_x, test_x, train_y, test_y\n\n\n# Just train and valid the model\ndef run_reg_linear(train_x, test_x, train_y, test_y, model, plot=False):\n    model.fit(train_x, train_y)\n    test_pred = model.predict(test_x)\n\n    mse = mean_squared_error(test_y, test_pred)\n    mae = mean_absolute_error(test_y, test_pred)\n    r2 = r2_score(test_y, test_pred)\n\n    if plot:\n        print(\"*\" * 40)\n        print(\"r2 score\", r2)\n        print(\"mse\", mse)\n        print(\"mae\", mae)\n        print(\"*\" * 40)\n\n    return r2, mse\n\n\n# Train with all models then return a table with scores\ndef train_test_show(train_x, test_x, train_y, test_y):\n    valores = []\n    models = [\n        (\"BayesianRidge\", BayesianRidge()),\n        (\"MLPRegressor\", MLPRegressor()),\n        (\"SGDRegressor\", SGDRegressor()),\n        (\"RandomForestRegressor\", RandomForestRegressor(n_jobs=-1)),\n    ]\n    for model in models:\n        print(model[0])\n        valores.append(\n            (model[0], *run_reg_linear(train_x, test_x, train_y, test_y, model[1]))\n        )\n    valores = pd.DataFrame(valores, columns=[\"Model\", \"R2\", \"MSE\"])\n    return valores.style.background_gradient(cmap=\"Reds\", low=0, high=1)","fafc45e9":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer","65cb274f":"enc = OneHotEncoder()\npca = PCA(n_components=200)\nvectorizer = TfidfVectorizer(\n    max_features=50000,\n    min_df=10,\n    ngram_range=(1, 3),\n    analyzer=\"word\",\n    stop_words=\"english\",\n)\n\n\nX_ohe = pca.fit_transform(\n    np.concatenate(\n        (\n            vectorizer.fit_transform(train[\"name\"]).toarray(),\n            vectorizer.fit_transform(train[\"item_description\"]).toarray(),\n            vectorizer.fit_transform(train[\"name\"]).toarray(),\n            enc.fit_transform(\n                train[\n                    [\n                        \"brand_name\",\n                        # \"category_name_0\",\n                        \"category_name_1\",\n                        \"category_name_2\",\n                        \"shipping\",\n                        \"item_condition_id\",\n                    ]\n                ].values\n            ).toarray(),\n        ),\n        axis=1,\n    )\n)\nY_ohe = train.log_price\n\nprint(\"X shape ->\", X_ohe.shape)","f4d6dadd":"%%time\ntrain_x_ohe, test_x_ohe, train_y_ohe, test_y_ohe = split(X_ohe, Y_ohe, True)\n\ntrain_test_show(train_x_ohe, test_x_ohe, train_y_ohe, test_y_ohe)","de8403dc":"enc = LabelEncoder()\ntrain_values = train[\n    [\n        \"name\",\n        \"brand_name\",\n        \"category_name_0\",\n        \"category_name_1\",\n        \"category_name_2\",\n        \"shipping\",\n        \"item_condition_id\",\n        \"item_description\",\n    ]\n]\n\n\nfor col in train_values.columns:\n    train_values[col] = enc.fit_transform(train_values[col])\n\nX_le = train_values.values\nY_le = train.log_price.values\nprint(\"X shape ->\", X_le.shape)","438b2b8f":"%%time\n\ntrain_x_le, test_x_le, train_y_le, test_y_le = split(X_le, Y_le, True)\n\ntrain_test_show(train_x_le, test_x_le, train_y_le, test_y_le)","7e205e05":"enc = LabelEncoder()\nscaler = StandardScaler()\n\ntrain_values = train[\n    [\n        \"name\",\n        \"brand_name\",\n        \"category_name_0\",\n        \"category_name_1\",\n        \"category_name_2\",\n        \"shipping\",\n        \"item_condition_id\",\n        \"item_description\",\n    ]\n]\n\n\nfor col in train_values.columns:\n    train_values[col] = scaler.fit_transform(\n        enc.fit_transform(train_values[col]).reshape(-1, 1)\n    )\n\nX_le_sc = train_values.values\nY_le_sc = train.log_price.values\nprint(\"X shape ->\", X_le_sc.shape)","e2141a26":"%%time\n\ntrain_x_le_sc, test_x_le_sc, train_y_le_sc, test_y_le_sc = split(X_le_sc, Y_le_sc, True)\n\ntrain_test_show(train_x_le_sc, test_x_le_sc, train_y_le_sc, test_y_le_sc)","621764ee":"from sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=10, stop=100, num=10)]\n# Number of features to consider at every split\nmax_features = [\"auto\", \"sqrt\"]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {\n    \"n_estimators\": n_estimators,\n    \"max_features\": max_features,\n    \"max_depth\": max_depth,\n    \"min_samples_split\": min_samples_split,\n    \"min_samples_leaf\": min_samples_leaf,\n    \"bootstrap\": bootstrap,\n}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation,\n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=random_grid,\n    n_iter=50,\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1,\n)\n# Fit the random search model\nrf_random.fit(\n   X_ohe, Y_ohe\n)\n","3d0a3328":"rf_random.best_params_","a7fff479":"# Plot-outputs\nmodel = RandomForestRegressor(\n    n_estimators=80,\n    min_samples_split=2,\n    min_samples_leaf=4,\n    max_features=\"sqrt\",\n    max_depth=10,\n    bootstrap=True,\n)\n\nmodel.fit(train_x_ohe, train_y_ohe)\ny_predict = model.predict(test_x_ohe)\n\nprint(\"MSE: \", mean_squared_error(test_y_ohe, y_predict))\nprint(\"R2: \", r2_score(test_y_ohe, y_predict))","dd6dae1e":"### price x brand","9925cbbd":"### item_condition_id x price","d53732ac":"## category_name\n\n","18f8e42a":"# EAD","59924808":"Building function to help in train and spit.","f6f47dc2":"## Data aproach's\n\nTokenize the name using TfidfVectorizer.","f00d46d4":" ## shipping ","b582cf67":"Removing outlines using quantile as 0.98, 98%.","2489e52a":"Log transformation","c2aa3bb4":"## Helpers","26b06566":"More than 70% items on item condition id equals to 1 are free ship.","e51a57a4":"### LabelEncoder Standard\n\nUsing all columns with LabelEncoder + StandardScaler.","c2a394f5":"Looks like a kind of rank for product condition and 1 must be good and 5 must be bad.","88ebb908":"# Regression","70582ba8":"## brand_name","a3a22453":"### item_condition_id x shipping","c6f417e7":"### item_condition_id x brand_name\n\nFilltering the brands by 30 most popular.","1b35d484":"### LabelEncoder\n\nUsing all columns with Label encoder.","a3932cb3":"## item_description","3b18dac2":"## item_condition_id\nCondition comes from 1 to 5","9dd3e329":"## price","80b6a295":"## item_condition_id","af58ecdb":"### item_condition_id x category_name (category_name_0~1)","c146c6c6":"To plot the real price value, we must remove the outline, that means filter the values using the roof.","a01f47a8":"### Models","a1a0bfba":"The libs","d3bcd9f4":"### name\n\nAnalysis the column name.","c4627de1":"Grouping the less popular brands  as 'other brand'.","54685491":"Setting NaN as 'no brand' and puting all lower.","d14a839f":"# Clean data\nFirst fill in brand_name because there are so many rows without a brand...","6536751e":"To unpack our dataset:","734e577a":"Reading the dataset with 100k sample.","2df511a6":"## RandomForestRegressor\n\nBoosting this model.","28649186":"The data is too large, because this I'll reduce until 50k.","4ca05193":"### OneHotEncoder\nOneHotEncoder to get our dummies for both column: name and item_description. \n\nUsing TfidfVectorizer to process the text data and get dummies.\n\nAt least PCA will reduce our dimensionality util 200 features."}}