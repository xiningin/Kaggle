{"cell_type":{"c82ed5fd":"code","5a305168":"code","b778cc02":"code","b2edeb8c":"code","63044f2a":"code","08ad28c4":"code","88257032":"code","2ca45e57":"code","085d0b94":"code","980a1bf3":"code","ed232661":"code","70665713":"code","6b7894e2":"code","380245dc":"code","8f5b3ea4":"code","5c5e31ff":"code","b1053133":"code","048cba18":"code","9d49ae3c":"code","8712a7c8":"code","5b99a364":"code","9265c13e":"code","9ed61696":"code","fd9bd7e9":"code","534acf7b":"code","0933ec72":"code","0bc85758":"code","1c7af1d6":"code","84741bce":"code","0ae6925f":"code","70fae1a1":"code","dc3cbb5d":"code","84c2c756":"code","15fc93ea":"code","3e517da8":"code","ab4cabd1":"code","4edb9408":"code","2fa2acfd":"code","a33d6777":"code","eec4d5a2":"code","3b3be295":"code","4a02028c":"code","38400326":"markdown","df99aa2a":"markdown","60e06827":"markdown","602e28e5":"markdown","86dfeaf5":"markdown","fddc3895":"markdown","e5390b74":"markdown","fe5d94ba":"markdown","fe30523e":"markdown","3fe8809b":"markdown","2e54e552":"markdown","aa9de6d2":"markdown","a2b81f1f":"markdown","643767d7":"markdown","168aa65f":"markdown","e0e4d465":"markdown","d7b86514":"markdown","b0695cfe":"markdown","3ef17ed5":"markdown","3969094f":"markdown","d1acda93":"markdown","60384c9c":"markdown","3158246c":"markdown"},"source":{"c82ed5fd":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5a305168":"# We need to install a wide variety of libraries. For this we will install pandas, numpy, seaborn and matplotlib libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport string\nimport re\nsns.set()\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import ngrams\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'","b778cc02":"train_data = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\ntrain_data.head()","b2edeb8c":"train_data.describe()","63044f2a":"train_data.isnull().sum()","08ad28c4":"# Dropping missing values\ntrain_data.dropna(inplace= True)","88257032":"train_data['sentiment'].value_counts()","2ca45e57":"train_data['sentiment'].value_counts(normalize= True)","085d0b94":"sns.countplot(data= train_data, x= 'sentiment',\n             order = train_data['sentiment'].value_counts().index);","980a1bf3":"# Positive tweet\nprint('Positive Tweet example:', train_data[train_data['sentiment'] == 'positive']['text'].values[0])\n\n# Negative tweet\nprint('negative Tweet example:', train_data[train_data['sentiment'] == 'negative']['text'].values[0])\n\n# Neutral tweet\nprint('Neutral Tweet example:', train_data[train_data['sentiment'] == 'neutral']['text'].values[0])","ed232661":"stop_words = stopwords.words('english')\nstemmer    = nltk.SnowballStemmer(\"english\")","70665713":"def clean_text(text):\n    '''\n        Make text lowercase, remove text in square brackets,remove links,remove punctuation\n        and remove words containing numbers.\n    '''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove urls\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","6b7894e2":"def preprocess_data(text):\n    text = clean_text(text)                                                     # Clean puntuation, urls, and so on\n    text = ' '.join(word for word in text.split() if word not in stop_words)    # Remove stopwords\n    text = ' '.join(stemmer.stem(word) for word in text.split())                # Stemm all the words in the sentence\n    return text","380245dc":"train_data['clean_text'] = train_data['text'].apply(preprocess_data)\ntrain_data.head()","8f5b3ea4":"# Convert sentiment to numerical variable\ntrain_data['label'] = train_data.sentiment.map({'negative': 0,\n                                                'positive': 1,\n                                                'neutral': 2})\ntrain_data.head()\n","5c5e31ff":"train_data['text_n_chars'] = train_data.text.apply(len) # count all chars in each sentence\ntrain_data['text_n_words'] = train_data.text.apply(lambda sent: len(sent.split())) # count number of words in each sentence\ntrain_data.head()","b1053133":"sns.histplot(data= train_data, x= 'text_n_words', hue= 'sentiment', multiple= 'stack');\n","048cba18":"sns.histplot(data= train_data, x= 'text_n_chars', hue= 'sentiment', multiple= 'stack');\n","9d49ae3c":"from collections import Counter","8712a7c8":"words = [word for sent in train_data['clean_text'] for word in sent.split()]\nwords[:10] # words without sorting","5b99a364":"# sort words descending order\nfreq_words = Counter(words)\nfreq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\nfreq_words_df = pd.DataFrame(freq_words_sorted[:20], columns=['word', 'counts'])","9265c13e":"freq_words_df.head(10)","9ed61696":"plt.figure(figsize=(12, 6))\nsns.barplot(data= freq_words_df , x= 'counts', y= 'word')\nplt.title('Top 20 words in whole text')\nplt.show();","fd9bd7e9":"def freq_sentiment_words(text, sentiment, num):\n    '''\n        take the whole data, and return data which is have # of words in each sentiment has been passed\n    '''\n    words = [word for sent in text[text['sentiment'] == sentiment]['clean_text'] for word in sent.split()]\n    freq_words = Counter(words)\n    freq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(freq_words_sorted[:num], columns=['word', 'counts'])\n    return freq_words_df","534acf7b":"positive_words = freq_sentiment_words(train_data, 'positive', 20)\npositive_words.head()","0933ec72":"def plot_freq(data, st):\n    '''\n        take the data, and st refeere to kind of sentiment\n    '''\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data= data , x= 'counts', y= 'word')\n    plt.title(f'Top 20 words in {st} sentiment')\n    plt.show();","0bc85758":"plot_freq(positive_words, 'positive')","1c7af1d6":"negative_words = freq_sentiment_words(train_data, 'negative', 20)\nnegative_words.head()","84741bce":"plot_freq(negative_words, 'negative')","0ae6925f":"neutral_words = freq_sentiment_words(train_data, 'neutral', 20)\nneutral_words.head()","70fae1a1":"plot_freq(neutral_words, 'neutral')","dc3cbb5d":"def get_top_n_gram(corpus, sentiment,  n_gram, top_n=None):\n    \n    # list of splited senteces, which is just list of words\n    text = [word for sent in corpus[corpus['sentiment'] == sentiment]['clean_text'] for word in sent.split()]\n\n    grams = ngrams(text, n_gram)\n    grams = (' '.join(g) for g in grams)\n    num_of_grams = [words for words in grams]\n    freq_words = Counter(num_of_grams)\n    freq_words_sorted = sorted(freq_words.items(), key=lambda pair: pair[1], reverse=True)\n    freq_words_df = pd.DataFrame(freq_words_sorted[:top_n], columns=['word', 'counts'])\n    return freq_words_df[:top_n]","84c2c756":"positive_gram = get_top_n_gram(train_data, 'positive', 2, 20)\npositive_gram.head()","15fc93ea":"plot_freq(positive_gram, 'positive')","3e517da8":"negative_gram = get_top_n_gram(train_data, 'negative', 2, 20)\nnegative_gram.head()","ab4cabd1":"plot_freq(negative_gram, 'negative')","4edb9408":"netutral_gram = get_top_n_gram(train_data, 'neutral', 2, 20)\nnetutral_gram.head()","2fa2acfd":"plot_freq(netutral_gram, 'neutral')","a33d6777":"# getting list of positive words \npositive_text_clean = train_data[train_data['sentiment' ] == 'positive']['clean_text']\npositive_clean_words = [word for words in positive_text_clean for word in words.split()]\npositive_clean_words[:10]","eec4d5a2":"# getting list of negative words \nnegative_text_clean = train_data[train_data['sentiment' ] == 'negative']['clean_text']\nnegative_clean_words = [word for words in negative_text_clean for word in words.split()]\nnegative_clean_words[:10]","3b3be295":"# getting list of neutral words \nneutral_text_clean = train_data[train_data['sentiment' ] == 'neutral']['clean_text']\nneutral_clean_words = [word for words in neutral_text_clean for word in words.split()]\nneutral_clean_words[:10]","4a02028c":"from wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_clean_words))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_clean_words))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_clean_words))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40);","38400326":"### Frequent words for each sentiment","df99aa2a":"### Word Cloud","60e06827":"# 2- EDA","602e28e5":"We found one row is missing for text and selected_text, so we need to replace it or drop it.","86dfeaf5":"#### `In Negative Sentiment`","fddc3895":"### `Missing Values treatment in the dataset`","e5390b74":"#### `Bi-Gram for negative sentiment`","fe5d94ba":"### `The distribution of number of words for each sentiment.`","fe30523e":"#### `Bi-Gram for neutral sentiment`","3fe8809b":"Thanks for your time ^_^.","2e54e552":"### `Distribution of the Sentiment Column`","aa9de6d2":"### `Examples of each sentiment`","a2b81f1f":"# 3- Text Data Preprocessing\nWe need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\n* Tokenizes\n* Make text lowercase\n* Removes hyperlinks\n* Remove punctuation\n* Removes numbers\n* Removes useless words \"stopwords\"\n* Stemming\/Lemmatization\n\n","643767d7":"### `The distribution of number of letters for each sentiment`","168aa65f":"### Distribution of top n-grams","e0e4d465":"# 4- Analyzing Text Statistics\nWe can now do some statistical analysis to explore the data like:\n* Text length analysis.\n    * length for whole sentence, # of each character in the sentence.\n    *  count # of word in each sentence.\n* word frequency analysis\n\n","d7b86514":"# 1- Reading the datasets","b0695cfe":"#### `In Neutral Sentiment`","3ef17ed5":"#### `In whole Text`","3969094f":"#### `In Positive Sentiment`","d1acda93":"### Most frequent words.","60384c9c":"**We can easily make tri-grams for sentiment using this function `get_top_n_gram` by passing n_gram = 3**","3158246c":"#### `Bi-Gram for positive sentiment`"}}