{"cell_type":{"842f7f5f":"code","7e0edf6f":"code","835c2ea7":"code","2466b4e9":"code","86e78a66":"code","f04e4628":"code","5dbbda0b":"code","f04dd8b1":"code","a60c0208":"code","c74cb002":"code","11149ce2":"code","24b969a9":"code","4a1002fd":"code","c9dac992":"code","f5521def":"markdown","68bb865b":"markdown","aa8e3327":"markdown","9bc59b79":"markdown","e9cdec1f":"markdown","cc4ba6fd":"markdown","bb1e60da":"markdown","5d048b51":"markdown","0b23d4db":"markdown","22667a52":"markdown","5a022dfa":"markdown"},"source":{"842f7f5f":"import torch\nimport spacy\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom torch import nn\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom sklearn.linear_model import Ridge\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","7e0edf6f":"BATCH_SIZE = 64\nRANDOM_STATE = 41\nFEATURES_SIZE = 300\nnlp = spacy.load('en_core_web_lg')","835c2ea7":"COMPETITION_DATA_PATH = Path('..\/input\/commonlitreadabilityprize')\nTRAIN_DATA_PATH = COMPETITION_DATA_PATH \/ 'train.csv'\nTEST_DATA_PATH =  COMPETITION_DATA_PATH \/ 'test.csv'","2466b4e9":"train_data = pd.read_csv(TRAIN_DATA_PATH)\n# Remove this for submission\n# train_data = train_data.sample(frac=0.05)\ntest_data = pd.read_csv(TEST_DATA_PATH)\ntrain_data, valid_data = train_test_split(train_data, test_size=0.1, random_state=RANDOM_STATE)\n\nprint(f'Length of train data: {len(train_data)}')\nprint(f'Length of valid data: {len(valid_data)}')\nprint(f'Length of test data : {len(test_data)} ')","86e78a66":"def create_features(text_excerpts):\n    with nlp.disable_pipes():\n        features = np.vstack([nlp(text).vector for text in text_excerpts])\n    return features\n\ndef create_targets(targets):\n    targets = targets.reshape(-1, 1).astype(np.float32)\n    return targets","f04e4628":"CREATE_AND_SAVE_INPUTS = True\nif CREATE_AND_SAVE_INPUTS:\n    X_train = create_features(train_data['excerpt'].tolist())\n    y_train = create_targets(train_data['target'].to_numpy())\n    X_valid = create_features(valid_data['excerpt'].tolist())\n    y_valid = create_targets(valid_data['target'].to_numpy())\n    # Variables -> File\n    with open('spacy_features_targets.npy', 'wb') as f:\n        np.savez(f, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid)\nelse:\n    with open('spacy_features_targets.npy', 'rb') as f:\n        spacy_features_targets = np.load(f)\n        # File -> Variables\n        X_train = spacy_features_targets['X_train']\n        X_valid = spacy_features_targets['X_valid']\n        y_train = spacy_features_targets['y_train']\n        y_valid = spacy_features_targets['y_valid']","5dbbda0b":"class TrainingDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.targets[idx]\n    \nclass PredictionDataset(Dataset):\n    def __init__(self, text_excerpts):\n        self.text_excerpts = text_excerpts\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        text = self.text_excerpts[idx]\n        with nlp.disable_pipes():\n            features = nlp(text).vector\n        return features","f04dd8b1":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Sequential(\n            nn.Linear(FEATURES_SIZE, FEATURES_SIZE),\n            nn.Dropout(p=0.2),\n            nn.ReLU(),\n            nn.Linear(FEATURES_SIZE, 1)\n        )\n    def forward(self, x):\n        x = self.linear(x)\n        return x","a60c0208":"train_dataset = TrainingDataset(features=X_train, targets=y_train)\nvalid_dataset = TrainingDataset(features=X_valid, targets=y_valid)\ntest_dataset = PredictionDataset(text_excerpts=test_data['excerpt'].tolist())\n\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)","c74cb002":"class EarlyStopping:\n    def __init__(self, patient_epochs=2):\n        self.best_valid_loss = np.inf\n        self.best_epoch = -1\n        self.patient_epochs = patient_epochs\n    \n    def should_stop(self, current_epoch, current_valid_loss):\n        if current_valid_loss < self.best_valid_loss:\n            self.best_valid_loss = current_valid_loss\n            self.best_epoch = current_epoch\n        return True if current_epoch > self.best_epoch + self.patient_epochs else False","11149ce2":"def train_one_epoch(dataloader, model, optimizer, loss_fn):\n    model.train()\n    total_loss = 0\n    for batch_num, (X, y) in enumerate(dataloader):\n        # Forward pass\n        y_pred = model(X)\n        if str(loss_fn)=='MarginRankingLoss()':\n            y_shuffled, y_pred_shuffled = shuffle(y, y_pred)\n            y_comparison = (y - y_shuffled).sign()\n            loss = loss_fn(y_pred, y_pred_shuffled, y_comparison)\n        if str(loss_fn)=='MSELoss()':\n            loss = loss_fn(y_pred, y)\n        total_loss += loss.item()\n        \n        # Backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    average_loss = total_loss \/ (batch_num + 1)\n    return average_loss\n\ndef validate_one_epoch(dataloader, model, loss_fn):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch_num, (X, y) in enumerate(dataloader):\n            # Forward pass\n            y_pred = model(X)\n            if str(loss_fn)=='MarginRankingLoss()':\n                y_shuffled, y_pred_shuffled = shuffle(y, y_pred)\n                y_comparison = (y - y_shuffled).sign()\n                loss = loss_fn(y_pred, y_pred_shuffled, y_comparison)\n            if str(loss_fn)=='MSELoss()':\n                loss = loss_fn(y_pred, y)\n            total_loss += loss.item()\n    average_loss = total_loss \/ (batch_num + 1)\n    return average_loss\n\ndef predict(dataloader, model):\n    model.eval()\n    with torch.no_grad():\n        y_preds = []\n        for batch_num, X in enumerate(dataloader):\n            y_pred = model(X)\n            y_preds.append(y_pred.cpu().detach().numpy())\n    y_preds = np.vstack(y_preds)\n    return y_preds","24b969a9":"model = Model()\noptimizer = torch.optim.SGD(params=model.parameters(), lr=0.005, momentum=0.9, nesterov=True)\nearly_stopping = EarlyStopping(patient_epochs=50)\nloss_fn = nn.MarginRankingLoss(margin=0.5)\n\nfor epoch in range(1000):\n    train_loss = train_one_epoch(train_dataloader, model, optimizer, loss_fn)\n    valid_loss = validate_one_epoch(valid_dataloader, model, loss_fn)\n    if early_stopping.should_stop(current_epoch=epoch, current_valid_loss=valid_loss):\n        print(f'Exiting At epoch: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}')\n        break\n    if epoch % 10 == 0:\n        print(f'Training loss at epoch {epoch} is {train_loss: .6f}')\n        print(f'Validation loss at epoch {epoch} is {valid_loss: .6f}')\n        \nvalid_prediction_dataset = PredictionDataset(text_excerpts=valid_data['excerpt'].tolist())\nvalid_prediction_dataloader = DataLoader(dataset=valid_prediction_dataset, batch_size=BATCH_SIZE, shuffle=False)\ny_valid_pred = predict(valid_prediction_dataloader, model)\nvalid_data_pred_df = pd.DataFrame.from_dict({'pred': y_valid_pred.reshape(-1), 'target': valid_data['target'].to_numpy()})\nfig = px.scatter(valid_data_pred_df, x=\"pred\", y=\"target\", trendline=\"ols\", title=\"Margin Ranking loss\")\nfig.show()","4a1002fd":"regressor = Ridge().fit(valid_data_pred_df['pred'].to_numpy().reshape(-1, 1), valid_data_pred_df['target'].to_numpy().reshape(-1, 1))\ntest_data['target'] = regressor.predict(predict(test_dataloader, model))\ntest_data[['id','target']].to_csv('submission.csv', index=False)","c9dac992":"model = Model()\noptimizer = torch.optim.SGD(params=model.parameters(), lr=0.005, momentum=0.9, nesterov=True)\nearly_stopping = EarlyStopping(patient_epochs=50)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(1000):\n    train_loss = train_one_epoch(train_dataloader, model, optimizer, loss_fn)\n    valid_loss = validate_one_epoch(valid_dataloader, model, loss_fn)\n    if early_stopping.should_stop(current_epoch=epoch, current_valid_loss=valid_loss):\n        print(f'Exiting At epoch: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}')\n        break\n    if epoch % 10 == 0:\n        print(f'Training loss at epoch {epoch} is {train_loss: .6f}')\n        print(f'Validation loss at epoch {epoch} is {valid_loss: .6f}')\n        \nvalid_prediction_dataset = PredictionDataset(text_excerpts=valid_data['excerpt'].tolist())\nvalid_prediction_dataloader = DataLoader(dataset=valid_prediction_dataset, batch_size=BATCH_SIZE, shuffle=False)\ny_valid_pred = predict(valid_prediction_dataloader, model)\nvalid_data_pred_df = pd.DataFrame.from_dict({'pred': y_valid_pred.reshape(-1), 'target': valid_data['target'].to_numpy()})\nfig = px.scatter(valid_data_pred_df, x=\"pred\", y=\"target\", trendline=\"ols\", title=\"RMSE loss\")\nfig.show()","f5521def":"# Training and evaluation loop","68bb865b":"# MSELoss","aa8e3327":"# Earlystopping","9bc59b79":"# Model definition","e9cdec1f":"# MarginRankingLoss","cc4ba6fd":"# Spacy Feature extraction\nAll Credits to Sumit Kumar @anaverageengineer https:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners","bb1e60da":"# Inference\nAlthought the correlation of predictions and targets are similar for both the losses, MarginRankingLoss provides the advantage of adding external data for pretraining the model very easily. For instance we know that text from Simple Wikipedia is easier in comparison to Wikipedia. We know that lessons of grade 8 text are easier comparison to grade 12. So finally we can scrape and add a lot of external data and pretrain the network for achieving better performance.\n\nThank you for reading!","5d048b51":"# Create datasets and dataloaders","0b23d4db":"![image.png](attachment:ac3be737-357d-4dc8-b9a6-d09cc3cdcde7.png)","22667a52":"# Inspiration\n**Scenario 1**: Someone gives us a single piece of text and asks us to rate its readability on a scale of -3 to 3  \n**Scenario 2**: Someone gives us two piece of texts and asks us to which one is easier to read\n\n**Scenario 1** corresponds to training a Neural network based on RMSE loss  \n**Scenario 2** corresponds to training a Neural network based on Ranking Loss\n\nEssentially once can see this task as a Ranking task where in we rank the text excerpts based on ease of reading. I personally feel its more intuitive to for the network to learn the parameters while comparing two pieces of texts. In that way the network can more directly learn weight parameters that are contrastive enough to learn discriminatory features. ","5a022dfa":"# Datasets and Dataloaders definition"}}