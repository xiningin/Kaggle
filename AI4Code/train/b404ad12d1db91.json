{"cell_type":{"917b303e":"code","998b78cb":"code","9fda6ca6":"code","1b3feee4":"code","03476745":"code","d39177a1":"code","137ba8e0":"code","02d1e14e":"code","859fa7d0":"code","16abafc0":"code","cecb6358":"code","b229da0d":"code","559ba4a6":"markdown","de8f5c43":"markdown"},"source":{"917b303e":"import numpy as np \nimport pandas as pd \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nimport lightgbm as lgb\nimport gc\nfrom tqdm import tqdm\n\nSEED = 0","998b78cb":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col='id')","9fda6ca6":"features = [x for x in train.columns.values if x[0]==\"f\"]","1b3feee4":"train['n_missing'] = train[features].isna().sum(axis=1)\ntrain['abs_sum'] = train[features].abs().sum(axis=1)\ntrain['sem'] = train[features].sem(axis=1)\ntrain['std'] = train[features].std(axis=1)\ntrain['avg'] = train[features].mean(axis=1)\ntrain['max'] = train[features].max(axis=1)\ntrain['min'] = train[features].min(axis=1)\n\ntest['n_missing'] = test[features].isna().sum(axis=1)\ntest['abs_sum'] = test[features].abs().sum(axis=1)\ntest['sem'] = test[features].sem(axis=1)\ntest['std'] = test[features].std(axis=1)\ntest['avg'] = test[features].mean(axis=1)\ntest['max'] = test[features].max(axis=1)\ntest['min'] = test[features].min(axis=1)\n","03476745":"# imputer = SimpleImputer(strategy=\"median\")\n# X = imputer.fit_transform(X)\n# X_test = imputer.transform(X_test)","d39177a1":"X = train.drop([\"claim\"], axis=1)\nX_test = test\ny = train[\"claim\"]","137ba8e0":"scaler = RobustScaler()\nX = pd.DataFrame(scaler.fit_transform(X), index=train.index, columns=test.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), index=test.index, columns=test.columns)","02d1e14e":"fill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nfor col in tqdm(features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = X[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = X[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = X[col].mode().iloc[0]\n    \n    X[col].fillna(fill_value, inplace=True)\n    X_test[col].fillna(fill_value, inplace=True)","859fa7d0":"X = X.values\nX_test = X_test.values","16abafc0":"del test, train, scaler\ngc.collect()","cecb6358":"import optuna\nfrom optuna.samplers import TPESampler\nfrom lightgbm import LGBMClassifier\n\ndef train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n       \n    #A set of hyperparameters to optimize by optuna\n    lgbm_params = {\n                    \"objective\": trial.suggest_categorical(\"objective\", ['binary']),\n                    \"boosting_type\": trial.suggest_categorical(\"boosting_type\", ['gbdt']),\n                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n                    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, step=0.001),\n                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 35000, 45000),        \n                    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 50.0, step=0.1),\n                    \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 200.0, step=0.1),\n                    \"random_state\": trial.suggest_categorical(\"random_state\", [0]),\n                    \"bagging_seed\": trial.suggest_categorical(\"bagging_seed\", [0]),\n                    \"feature_fraction_seed\": trial.suggest_categorical(\"feature_fraction_seed\", [0]), \n                    \"n_jobs\": trial.suggest_categorical(\"n_jobs\", [4]), \n                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1, step=0.01),\n                    \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 7),\n                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1, step=0.001),\n                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n                    'metric': trial.suggest_categorical('metric', ['AUC'])\n                    }\n    \n    # Model loading and training\n    model = LGBMClassifier(**lgbm_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric=\"auc\",\n              early_stopping_rounds=100,\n              verbose=False)\n  \n    print(f\"Number of boosting rounds: {model.best_iteration_}\")\n    oof = model.predict_proba(X_valid)[:, 1]\n   \n    return roc_auc_score(y_valid, oof)","b229da0d":"%%time\n# Splitting data into train and valid folds using target bins for stratification\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n# Setting optuna verbosity to show only warning messages\n# If the line is uncommeted each iteration results will be shown\n# optuna.logging.set_verbosity(optuna.logging.WARNING)\n\ntime_limit = 3600 * 7\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(lambda trial: train_model_optuna(trial,\n                                                X_train,\n                                                X_valid,\n                                                y_train, y_valid),\n               timeout=time_limit\n              )\n\n# Showing optimization results\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","559ba4a6":"Idea taken from www.kaggle.com\/dlaststark\/tps-sep-single-xgboost-model\nI have modified the choices using the following rationale:\n1. Mean: normal distribution\n2. Median: unimodal and skewed\n3. Mode: all other cases","de8f5c43":"# Contributions\nPlease try out the following and let me know if it helps at all\n1. Trying new feature engineering combinations: None of the notebooks I've looked at have used abs()+sum(), or sem()\n\n![image.png](attachment:2f8d1cf0-2eb2-46f0-b5cd-0c896fbc9877.png)\n![image.png](attachment:30ec3830-9d45-401c-8686-be109661a809.png)\n\n2. Adjusting learning rate: You can save time as well as fine tune your models"}}