{"cell_type":{"304981fd":"code","c5430981":"code","4f4834d7":"code","319a33cb":"code","4ae60095":"code","ce821c78":"code","7e4f1604":"code","2970e47a":"code","2a638c28":"code","b12ccec8":"code","00110618":"code","fe6b04ff":"code","721c78f2":"code","d66b435f":"code","695bf4dd":"code","4731b94f":"code","e4e7a1f8":"code","55d4dee3":"code","7c7b6c31":"code","6fea69d0":"code","1a043abe":"code","a39f3325":"code","e1abeb91":"code","89d03fad":"code","265ae591":"code","499d02e1":"code","37c2cfe0":"code","f1ca53c2":"code","1477660c":"code","48222e04":"markdown","8bfc5f97":"markdown","8664f036":"markdown","a16bbb01":"markdown","c145051a":"markdown","866a6164":"markdown","5d9de7e2":"markdown","904d4e1c":"markdown","dec907ad":"markdown","e2ff970a":"markdown"},"source":{"304981fd":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport transformers #huggingface transformers library\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c5430981":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","4f4834d7":"df = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines = True)\ndf.head()","319a33cb":"# WORLDPOST and THE WORLDPOST were given as two separate categories in the dataset. Here I change the category THE WORLDPOST to WORLDPOST \ndf.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)","4ae60095":"print(f\"The dataset contains { df.category.nunique() } unique categories\")","ce821c78":"#label encoding the categories. After this each category would be mapped to an integer.\nencoder = LabelEncoder()\ndf['categoryEncoded'] = encoder.fit_transform(df['category'])","7e4f1604":"#since I am using bert-large-uncased as the model, I am converting each of the news headlines and descriptions into lower case.\ndf['headline'] = df['headline'].apply(lambda headline: str(headline).lower())\ndf['short_description'] = df['short_description'].apply(lambda descr: str(descr).lower())","2970e47a":"#calculating the length of headlines and descriptions\ndf['descr_len'] = df['short_description'].apply(lambda x: len(str(x).split()))\ndf['headline_len'] = df['headline'].apply(lambda x: len(str(x).split()))","2a638c28":"df.describe()","b12ccec8":"sns.distplot(df['descr_len'])\nplt.title('Description Number of Words')\nplt.show()","00110618":"sns.distplot(df['headline_len'])\nplt.title('Headline Number of Words')\nplt.show()","fe6b04ff":"df['short_description'] = df['headline'] + df['short_description']","721c78f2":"sns.distplot(df['headline_len'] + df['descr_len'])\nplt.title('Short Description Number of Words')\nplt.show()","d66b435f":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","695bf4dd":"#bert large uncased pretrained tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')","4731b94f":"X_train,X_test ,y_train,y_test = train_test_split(df['short_description'], df['categoryEncoded'], random_state = 2020, test_size = 0.3)","e4e7a1f8":"#tokenizing the news descriptions and converting the categories into one hot vectors using tf.keras.utils.to_categorical\nXtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen=80)\nytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes=40,dtype = 'int32')\nXtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen=80)\nytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes=40,dtype = 'int32')","55d4dee3":"def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(0.3)(cls_token)\n    #using a dense layer of 40 neurons as the number of unique categories is 40. \n    out = tf.keras.layers.Dense(40, activation='softmax')(x)\n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    #using categorical crossentropy as the loss as it is a multi-class classification problem\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=['accuracy'])\n    return model","7c7b6c31":"#building the model on tpu\nwith strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n    model = build_model(transformer_layer, max_len=80)\nmodel.summary()","6fea69d0":"#creating the training and testing dataset.\nBATCH_SIZE = 32*strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((Xtrain_encoded, ytrain_encoded))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(Xtest_encoded)\n    .batch(BATCH_SIZE)\n)","1a043abe":"#training for 10 epochs\nn_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    epochs=10\n)","a39f3325":"#making predictions\npreds = model.predict(test_dataset,verbose = 1)\n#converting the one hot vector output to a linear numpy array.\npred_classes = np.argmax(preds, axis = 1)","e1abeb91":"#extracting the classes from the label encoder\nencoded_classes = encoder.classes_\n#mapping the encoded output to actual categories\npredicted_category = [encoded_classes[x] for x in pred_classes]\ntrue_category = [encoded_classes[x] for x in y_test]","89d03fad":"result_df = pd.DataFrame({'description':X_test,'true_category':true_category, 'predicted_category':predicted_category})\nresult_df.head()","265ae591":"print(f\"Accuracy is {sklearn.metrics.accuracy_score(result_df['true_category'], result_df['predicted_category'])}\")","499d02e1":"result_df.to_csv('testPredictions.csv', index = False)","37c2cfe0":"result_df[result_df['true_category']!=result_df['predicted_category']]","f1ca53c2":"confusion_mat = confusion_matrix(y_true = true_category, y_pred = predicted_category, labels=list(encoded_classes))","1477660c":"df_cm = pd.DataFrame(confusion_mat, index = list(encoded_classes),columns = list(encoded_classes))\nplt.rcParams['figure.figsize'] = (20,20)\nsns.heatmap(df_cm)","48222e04":"## Importing Necessary Libraries","8bfc5f97":"As we can see, that there are a lot of samples that have a description length of 0, however almost all articles have a headline. Going with the intuition that the headline is often more descriptive of the category of the news, as well as to provide more text data to the model, I add news headlines to the short description and modify the description of the news samples.","8664f036":"## Preprocessing","a16bbb01":"![](https:\/\/miro.medium.com\/max\/700\/1*HgXA9v1EsqlrRDaC_iORhQ.png)","c145051a":"## Configuration for TPUs\n\nThe given news classifier is trained on using BERT model. Since, BERT is a very large model, it requires gpu's and tpu's to train quickly. Here, I have used TPU's provided by Kaggle to train this model.","866a6164":"## Confusion Matrix","5d9de7e2":"## Training","904d4e1c":"## Evaluation","dec907ad":"## Tokenizing\n\n* I use the HuggingFace tokenizer for bert to tokenize the given news descriptions. After splitting up the dataset into train and test, we encode both the training data and the testing data. \n* One more thing to notice is that we truncate the last part of the sentence when encoding it, thus adding the headlines to the front of the description means that our model will always have the headline of the news sample.","e2ff970a":"## Building the model"}}