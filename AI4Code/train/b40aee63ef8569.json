{"cell_type":{"cb6c7d3b":"code","d19e41a0":"code","bd16fb71":"code","432ea4b3":"code","e5698c62":"code","0bf5c797":"code","fa3f5708":"code","12e13cc2":"code","e3c40655":"code","4b07bac4":"code","ffa79d3d":"code","1c3cd9f6":"code","e537180e":"code","525d3289":"code","f155e73b":"code","f2690ea6":"code","548ea553":"code","19b1f2a2":"code","e43afa2d":"code","b5f6fda2":"code","19e33f80":"code","319fa60e":"code","1a932f51":"code","18aa2b7b":"code","e66e25e0":"code","ea7a24ff":"markdown"},"source":{"cb6c7d3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","d19e41a0":"with open('..\/input\/chinese-ner\/train_data.txt', 'r', encoding='utf-8') as f:\n    content = f.readlines()\n    \ncontent = [w.strip().split() for w in content]","bd16fb71":"words = list(set([c[0] for c in content if len(c)!=0]))\nwords.append(\"ENDPAD\")\nn_words = len(words); n_words","432ea4b3":"tags = list(set([c[1] for c in content if len(c)!=0]))\nn_tags = len(tags); n_tags","e5698c62":"word2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}","0bf5c797":"sentences = []\ntemp = []\nfor item in content:\n    if len(item) != 0:\n        temp.append((item[0], item[1]))\n    else:\n        sentences.append(temp)\n        temp = []","fa3f5708":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sequence length:', maxlen)","12e13cc2":"# Check how long sentences are so that we can pad them\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","e3c40655":"plt.hist([len(s) for s in sentences], bins=50)\nplt.show()","4b07bac4":"from keras.preprocessing.sequence import pad_sequences\nX = [[word2idx[w[0]] for w in s] for s in sentences]\nX = np.array(pad_sequences(sequences=X, maxlen=maxlen, padding='post', value=n_words-1))","ffa79d3d":"y = [[tag2idx[w[1]] for w in s] for s in sentences]\ny = np.array(pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag2idx[\"O\"]))","1c3cd9f6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","e537180e":"!pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","525d3289":"from keras.models import Model, Input, Sequential\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom keras_contrib.layers.crf import CRF\nfrom keras.optimizers import Adam","f155e73b":"EMBEDDING_OUT_DIM = 200\nmodel = Sequential()\nmodel.add(Embedding(input_dim=n_words, output_dim=EMBEDDING_OUT_DIM, input_length=maxlen))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.5)))\nmodel.add(TimeDistributed(Dense(n_tags)))\ncrf_layer = CRF(n_tags, sparse_target=True)\nmodel.add(crf_layer)","f2690ea6":"model.summary()","548ea553":"model.compile(optimizer='adam', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])","19b1f2a2":"history = model.fit(X_train, y_train.reshape(y_train.shape[0], y_train.shape[1], 1), batch_size=32, epochs=5, validation_split=0.2, verbose=1)","e43afa2d":"model.evaluate(X_test, y_test.reshape(y_test.shape[0], y_test.shape[1], 1))","b5f6fda2":"import re\nclass NERInference:\n    def __init__(self, model, words, word2idx, tags, n_words, maxlen, split_pattern=\"(,|!|\\.| +)\"):\n        self.model = model\n        self.words = words\n        self.word2idx = word2idx\n        self.tags = tags\n        self.n_words = n_words\n        self.pattern = split_pattern\n        self.maxlen = maxlen\n        \n    def predict(self, string):\n        preds = []\n        tokens = [[self.word2idx[word] for word in string if word in self.words]]\n        padded = pad_sequences(tokens, maxlen=self.maxlen, padding='post', value=self.n_words-1)\n        pred_ner = np.argmax(self.model.predict(padded), axis=-1)\n        for w,pred in zip(padded[0], pred_ner[0]):\n            if w == self.n_words - 1:\n                break\n            #print(\"{:15}: {}\".format(self.words[w], self.tags[pred]))\n            preds.append(self.tags[pred])\n        return preds","19e33f80":"myNerInfer = NERInference(model, word2idx=word2idx, words=words, tags=tags, n_words=n_words, maxlen=maxlen)","319fa60e":"new_string4 = '\u738b\u5c0f\u660e\u51fa\u751f\u5728\u5317\u4eac'\nnew_string4_pred = myNerInfer.predict(string=new_string4)\nnew_string4_pred","1a932f51":"class FindNamedEntites(object):\n    new_string_pred = []\n    new_string = ''\n    \n    def __init__(self, new_string):\n        self.new_string = new_string\n        \n    def make_preds(self):\n        self.new_string_pred = myNerInfer.predict(string=self.new_string)\n    def find_person(self, new_string):\n        if len(self.new_string_pred)==0:\n            self.new_string_pred = myNerInfer.predict(string=new_string)\n        persons = []\n        person = ''\n        for w, t in zip(new_string, self.new_string_pred):\n            if t == 'B-PER' or t == 'I-PER':\n                person+=w\n            else:\n                if person!='': persons.append(person)\n                person=''\n        return persons\n    def find_org(self, new_string):\n        if len(self.new_string_pred)==0:\n            self.new_string_pred = myNerInfer.predict(string=new_string)\n        persons = []\n        person = ''\n        for w, t in zip(new_string, self.new_string_pred):\n            if t == 'B-ORG' or t == 'I-ORG':\n                person+=w\n            else:\n                if person!='': persons.append(person)\n                person=''\n        return persons\n    def find_loc(self, new_string):\n        if len(self.new_string_pred)==0:\n            self.new_string_pred = myNerInfer.predict(string=new_string)\n        if new_string:\n            self.new_string_pred = myNerInfer.predict(string=new_string)\n        persons = []\n        person = ''\n        for w, t in zip(new_string, self.new_string_pred):\n            if t == 'B-LOC' or t == 'I-LOC':\n                person+=w\n            else:\n                if person!='': persons.append(person)\n                person=''\n        return persons\n\n    def find_ner(self):\n        persons = self.find_person(self.new_string)\n        orgs = self.find_org(self.new_string)\n        locs = self.find_loc(self.new_string)\n        print(\"{:12} : \".format('PERSON'), persons)\n        print(\"{:12} : \".format('ORGNIZATION'), orgs)\n        print(\"{:12} : \".format('LOCATION'), locs)","18aa2b7b":"ner_recog = FindNamedEntites('\u738b\u7855\u51fa\u751f\u5728\u5e7f\u5dde\uff0c\u4ed6\u73b0\u5728\u5728\u4e2d\u56fd\u5927\u4f7f\u9986\u7ebd\u7ea6\u529e\u4e8b\u5904\u5de5\u4f5c\u3002\u4ed6\u8bf4\u4e0b\u6b21\u56de\u56fd\u7684\u65f6\u5019\u8981\u53bb\u9ec4\u5c71\u65c5\u6e38\uff0c\u4ed6\u4e0d\u559c\u6b22\u8ddf\u7740\u5e7f\u5dde\u5e02\u6559\u80b2\u5c40\u53bb\uff0c\u60f3\u8981\u81ea\u9a7e\u6e38')\nner_recog.make_preds()\nner_recog.find_ner()","e66e25e0":"print(ner_recog.find_loc('\u4ece\u5e7f\u5dde\u5230\u5317\u4eac\u7684\u4ea4\u901a\u8def\u7ebf'))\nprint(ner_recog.find_loc('\u600e\u4e48\u6837\u4ece\u5317\u4eac\u53bb\u5230\u5e7f\u5dde\u5462\uff1f'))","ea7a24ff":"\n**Importing the dataset for named entity recognition model**"}}