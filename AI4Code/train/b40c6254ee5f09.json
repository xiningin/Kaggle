{"cell_type":{"bbd824c0":"code","16f2e163":"code","e74f297f":"code","15b35c18":"code","e611b2d4":"code","c87c0601":"code","f887ba34":"code","90dc6088":"code","5b9665d4":"code","c94812a0":"code","bc42e21b":"code","eaab5777":"code","83c4b4ef":"code","ab2b03d9":"code","ad35cd70":"markdown","1ae4903c":"markdown","140f5060":"markdown","76300126":"markdown","9bbeb8a0":"markdown","3ea72310":"markdown","5e09c6c3":"markdown","23d00079":"markdown","e684c545":"markdown","eaef3f3a":"markdown","c8cc0fec":"markdown","3920e468":"markdown","6ba13f8f":"markdown","be290e17":"markdown"},"source":{"bbd824c0":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nif torch.cuda.is_available():\n    dev = torch.device(\"cuda\")\nelse:\n    dev = torch.device(\"cpu\")\n\nimport pickle\n    \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","16f2e163":"train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nbatch_size = len(train)","e74f297f":"wrtensor = torch.tensor(train.loc[:,['weight','resp']].to_numpy(),dtype=torch.float)\nwrtensor = torch.mul(wrtensor[:,0],wrtensor[:,1]).to(dev)\nitensor = torch.tensor(((train.loc[:,'feature_0']+1)\/\/2).to_numpy(),dtype=torch.long,device=dev)","15b35c18":"feature_names = ['feature_'+str(i) for i in range(1,130)]\ntrain = train[feature_names]","e611b2d4":"maxindex = np.zeros((129,3))\nfor i in range(129):\n    counts = train[feature_names[i]].value_counts()\n    mean = train[feature_names[i]].mean()\n    std = train[feature_names[i]].std()\n    sigmas = np.abs(counts.index[0]-mean)\/std\n    maxindex[i] = [counts.index[0], counts.iloc[0], sigmas]\n    \nfor i in range(129):\n    if maxindex[i,1] > 100 and maxindex[i,2] > 1:\n        train.replace({feature_names[i]: maxindex[i,0]},np.nan)","c87c0601":"fill_val=train.mean()\ntrain = train.fillna(fill_val)","f887ba34":"pca_components = 60\nsc = StandardScaler().fit(train.to_numpy())\ntrain = sc.transform(train.to_numpy())\npca = PCA(n_components = pca_components).fit(train)\ntrain=pca.transform(train)","90dc6088":"train = torch.tensor(train,dtype=torch.float,device=dev)","5b9665d4":"e_size = 64\nfc_input = pca_components\nh_dims = [512,512,256,128]\ndropout_rate = 0.5\nepochs = 200\nminibatch_size = 100000\n\nclass MarketPredictor(nn.Module):\n    def __init__(self):\n        super(MarketPredictor, self).__init__()\n        \n        self.e = nn.Embedding(2,e_size)\n        self.deep = nn.Sequential(\n            nn.Linear(fc_input,h_dims[0]),\n            nn.BatchNorm1d(h_dims[0]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[0],h_dims[1]),\n            nn.BatchNorm1d(h_dims[1]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[1],h_dims[2]),\n            nn.BatchNorm1d(h_dims[2]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[2],h_dims[3]),\n            nn.BatchNorm1d(h_dims[3]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[3],e_size),\n            nn.BatchNorm1d(e_size),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate)\n            )\n        self.reduce = nn.utils.weight_norm(nn.Linear(e_size,1))\n        self.sig = nn.Sigmoid()\n        \n    def forward(self,xi,xf):\n        e_out = self.e(xi)\n        f_out = self.deep(xf)\n        ef_out = self.reduce(e_out+f_out)\n        sig_out = self.sig(ef_out)\n        \n        return sig_out\n        ","c94812a0":"def loss(s,wr):\n    return - torch.dot(s,wr)","bc42e21b":"model = MarketPredictor().to(dev)\nopt = optim.Adam(model.parameters())","eaab5777":"minibatches = batch_size\/\/minibatch_size\n\nfor i in range(epochs):\n    permutation = torch.randperm(batch_size)\n    print('Epoch is',i,'\/',epochs)\n    for j in range(minibatches):\n        opt.zero_grad()\n        s = model(itensor[permutation[j*minibatch_size:(j+1)*minibatch_size]],train[permutation[j*minibatch_size:(j+1)*minibatch_size]])\n        c = loss(s.squeeze(),wrtensor[permutation[j*minibatch_size:(j+1)*minibatch_size]])\n        c.backward()\n        opt.step()\n    print('Loss is',c.item())","83c4b4ef":"path = 'marketpredictor_state_dict_'+str(epochs)+'epochs.pt'\ntorch.save(model.state_dict(),path)","ab2b03d9":"with open('feature_processing.pkl','wb') as f:\n    pickle.dump([sc,pca,maxindex,fill_val],f)","ad35cd70":"We compute the principal components and reduce the feature space using sklearn","1ae4903c":"Finally we have a tensor with the last features we will use","140f5060":"# Simple Predictor for Jane-Street Market competition\n\nHere I show how to build a very simple neural network model in Pytorch and train it on a GPU.\n\nPre-processing of data is done as described in [this notebook](https:\/\/www.kaggle.com\/andreasthomasen\/preprocessing-and-feature-selection). The main difference is that we only do PCA here and retain a lot of features. The reason is that we do not use RNNs, but instead only rely on instantaneous feature values. So this model can be trained with quite a lot of features included.\n\nIf you read this notebook from start to finish, you will learn how to\n* Load data into pandas\n* Do feature reduction using PCA\n* Define a neural network model in pytorch\n* Train the model and save it using pickle\n\nThanks for reading, if you like it, feel free to copy it. Nothing revolutionary in this notebook. It would also be helpful if you upvoted :)\n\nUPDATE: Including the training step, it took too long to run this notebook for submission. So instead it now saves the model at the end. You can run it later in a private submission.\nEnjoy!","76300126":"# Load data and reduce dimensions","9bbeb8a0":"Let's remove outliers first","3ea72310":"Now we need to deal with NaN. We impute those missing values with the mean of each column.","5e09c6c3":"We make a separate tensor that contains all other features","23d00079":"We will make a very simple model at first using pytorch. The idea is to have fully connected layers deal with all of the floating point features, while feature_0 is used in an embedding layer.","e684c545":"# Model","eaef3f3a":"# Saving the model\nIt's pretty easy to save a pytorch model. We will use pickle and save the state dict of the model.","c8cc0fec":"We will also need the standard scaler and pca objects, as well as the maxindex and fill_val for when we run things for submission later","3920e468":"The tensors below will be used later. The wrtensor is used in training. We store feature_0 in a separate tensor since it is the only integer valued feature.","6ba13f8f":"Now we train it. Let's define the loss function first. In the competition we're told that the return on day $i$ is\n\\begin{equation}\np_i = \\sum_j (\\mathit{weight}_{ij}*\\mathit{resp}_{ij}*\\mathit{action}_{ij})\n\\end{equation}\nThe way we've made the network it gives a sigmoidal output $s_{ij} \\in[0;1]$. Let's make the cost-function\n\\begin{equation}\nC = \\sum_i c_i = -\\sum_{i,j} (\\mathit{weight}_{ij}*\\mathit{resp}_{ij}*s_{ij}).\n\\end{equation}\nThis has the same minimum as $p_i$, but the advantage is that it's got finite gradients with respect to the model parameters, and so should work better with SGD.\n\nWe will also use minibatches to prevent overfitting.","be290e17":"Let's make some torch tensors which hold the training data and apply our model to it"}}