{"cell_type":{"c7d12e79":"code","49b46b90":"code","0a9c759d":"code","9380dde6":"code","2a1c34a7":"code","540be7c9":"code","20757fce":"code","31ad956c":"code","1aa49de8":"code","45c23201":"code","9330c8a9":"code","ad88f508":"code","845aec68":"code","f376c1f9":"markdown","8dd53279":"markdown","9636acf0":"markdown","a3ba08b7":"markdown","5e48234d":"markdown","23916c2b":"markdown","17dc2118":"markdown","ecf7e79a":"markdown","8ecd6bde":"markdown","1e2bd0c4":"markdown","cb160d1b":"markdown","69cfbfa7":"markdown","df234e7a":"markdown"},"source":{"c7d12e79":"from IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#8F003C','#eb3446','Tourney','Smokum',45,10\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h4 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h4>\"\"\"%string))\n    \nfrom IPython.display import HTML\nHTML(\"\"\"\n<style>\nh1,h2,h3 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\tbox-shadow: \n\t\tinset 0 0 0 1px rgba(97,0,45, 1), \n\t\tinset 0 0 5px rgba(53,86,129, 1),\n\t\tinset -285px 0 35px #F2D8FF;\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n},\n\nh4 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n}\n<\/style>\n\"\"\")","49b46b90":"import pandas as pd\ntrain =pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\ntrain = train.drop('id',axis=1)\ntest = test.drop('id',axis=1)\n\nprint(\"train data : \",len(train))\nprint(\"test data : \",len(test))","0a9c759d":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfeatures = [feature for feature in train.columns if feature not in ['id', 'claim']]\nunique_value_feature_train = pd.DataFrame(train[features].nunique())\nunique_value_feature_train = unique_value_feature_train.reset_index(drop=False)\nunique_value_feature_train.columns = ['Features', 'Count']\nunique_value_feature_test = pd.DataFrame(test[features].nunique())\nunique_value_feature_test = unique_value_feature_test.reset_index(drop=False)\nunique_value_feature_test.columns = ['Features', 'Count']\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4, 12), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\n#sns.set_palette(['#ffd514']*75)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=unique_value_feature_train['Features'], x=unique_value_feature_train['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n#ax0.text(0, -1.9, 'Unique Values - Train Dataset', fontsize=6, ha='left', va='top', weight='bold')\n#ax0.text(0, -1.2, 'feature_1, feature55 , feature 86 have less unique values in test set', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():.0f}'\n    x = p.get_x() + p.get_width() + 7\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(\"RdBu\", 10)\n\nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.barplot(ax=ax1, y=unique_value_feature_test['Features'], x=unique_value_feature_test['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax1_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax1_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax1_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.text(0, -1.9, 'Unique Values - Test Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax1.text(0, -1.2, 'feature_1, feature55 , feature 86 have less unique values in test set', fontsize=4, ha='left', va='top')\nfor p in ax1.patches:\n    value = f'{p.get_width():.0f}'\n    x = p.get_x() + p.get_width() + 7\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x, y, value, ha='center', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","9380dde6":"fig, axes = plt.subplots(12,9,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    if idx in [0,118,119,120,121]:\n        continue\n    else:\n      sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n      sns.kdeplot(data=test, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n      ax.set_xticks([])\n      ax.set_yticks([])\n      ax.set_xlabel('')\n      ax.set_ylabel('')\n      ax.spines['left'].set_visible(False)\n      ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()\n    ","2a1c34a7":"\n\nbackground_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(24, 15), facecolor=background_color)\n\ncolors = [\"#2f5586\", \"#f6f5f5\",\"#2f5586\"]\ncolormap = mpl.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\n\ntrain_corr = train.corr()\ntrain_mask = np.triu(np.ones_like(train_corr, dtype=bool))\n\nfig = plt.figure(figsize=(16, 16), facecolor='#f6f5f5')\n\ntrain_corr1 = train_corr[train_corr > 0.01]\nsns.heatmap(train_corr1, \n            square=True, \n            mask=train_mask,\n            annot=False,\n            cmap=sns.diverging_palette(240, 10),\n            #ax=ax0\n           )","540be7c9":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.pipeline import Pipeline\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Dense, Activation,  BatchNormalization, Dropout,  Concatenate, Embedding,  Flatten, Conv1D, LSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,  QuantileTransformer,  KBinsDiscretizer, PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom tensorflow import keras\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom colorama import Fore\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\n\nc_ = Fore.CYAN\nm_ = Fore.MAGENTA\nr_ = Fore.RED\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\ng_ = Fore.GREEN","20757fce":"BATCH_SIZE=1024\nSHUFFLE_BUFFER_SIZE =1024\nN_FOLD = 10\nEPOCH = 10\nLR = 5e-4\nDECAY_STEP = 4000\nDECAY_RATE =0.864\nQUANT =64\nBINS = 64\n\n\nN_SPLITS = 5\nN_ESTIMATORS = 20000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 49\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)\n\n\nINPUT = \"..\/input\/tabular-playground-series-sep-2021\/\"\n\ntrain = pd.read_csv(INPUT + \"train.csv\")\ntest = pd.read_csv(INPUT + \"test.csv\")\nsubmission = pd.read_csv(INPUT + \"sample_solution.csv\")\nfeatures = [col for col in test.columns if 'f' in col]\nTARGET = 'claim'\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)\n\nfeatures += ['n_missing']\nn_missing = train['n_missing'].copy()\n\n\ntrain[features] = train[features].interpolate(method='polynomial',order=1)\ntest[features] = test[features].interpolate(method='polynomial',order=1)\n                        \ntrain[features] = train[features].fillna(train[features].median())\ntest[features] = test[features].fillna(test[features].median())","31ad956c":"scaler = RobustScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])\ntrain.shape, test.shape","1aa49de8":"y = train['claim']\ntrain.drop(['id', 'claim'], axis = 1, inplace = True)\ntest.pop('id')\n\nX , x_test = train, test\n\npipe = Pipeline([(\"scaler\", QuantileTransformer(n_quantiles=QUANT, output_distribution='uniform')),\n        (\"binning\", KBinsDiscretizer(n_bins=BINS, encode='ordinal',strategy='uniform'))])\nX = pipe.fit_transform(X)\nx_test = pipe.transform(x_test)","45c23201":"train.head()","9330c8a9":"def Simple_NN(input_shape):\n    X_input = Input(input_shape)\n    X = Embedding(input_dim=BINS, output_dim=4, embeddings_initializer = \"glorot_uniform\")(X_input)\n    X = Flatten()(X)\n    X = Dense(64,  activation='swish')(X)\n    X = Dropout(0.6)(X)\n    X = Dense(32,  activation='swish')(X)\n    X = Dropout(0.4)(X)\n    X = Dense(8,  activation='swish')(X)\n    X = Dropout(0.2)(X)\n    X = Dense(1, activation='sigmoid')(X)\n    model = Model(inputs = X_input, outputs = X, name='simple_keras')\n\n    return model\n\ndef KFold():\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle=True)\n    \n    \n    keras.backend.clear_session()\n    print(b_,\"Train: \" )\n    print(g_, '*' * 60, c_)\n\n    \n\n    checkpoint_filepath = '\/kaggle\/working\/ckpt_cv'\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                                        filepath=checkpoint_filepath,\n                                        save_weights_only=True,\n                                        monitor='val_aucroc',\n                                        mode='max',\n                                        save_best_only=True)\n\n\n    train_dataset = Dataset.from_tensor_slices((np.float32(X_train), np.float32(y_train)))\n    val_dataset = Dataset.from_tensor_slices((np.float32(X_val), np.float32(y_val)))\n    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n    val_dataset = val_dataset.batch(BATCH_SIZE)\n\n    model = Simple_NN(X_train.shape[1:])\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=LR,\n        decay_steps = DECAY_STEP,\n        decay_rate= DECAY_RATE)\n    optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n\n    auc = tf.keras.metrics.AUC(name='aucroc')\n    model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[auc])        \n    model.fit(train_dataset, epochs = EPOCH,\n    validation_data=val_dataset,\n    callbacks=[model_checkpoint_callback])\n    keras.backend.clear_session()\n\n    model.load_weights(checkpoint_filepath)\n        #create predictions\n\n    \n    #create predictions\n    y_pred = model.predict(X_test)\n    \n    return y_pred\n\n","ad88f508":"X_test = Dataset.from_tensor_slices(np.float32(x_test))\nX_test = X_test.batch(BATCH_SIZE)\npred = KFold()","845aec68":"sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub.iloc[:,1]= pred\nsub=sub.set_index('id')\nsub.to_csv('submission.csv')","f376c1f9":"**Activation function**\n-the activation function of a node defines the output of that node given an input or set of inputs.\n\n**Loss Function its also called error function** \n-When you train Deep learning models, you feed data to the network, generate predictions, compare them with the actual values (the targets) and then compute what is known as a loss. This loss essentially tells you something about the performance of the network: the higher it is, the worse your network performs overall.\n\n**Highly important** - activation function is for individual perceptron(basic unit of neural network) while loss function is calculated at final output layer..\n\n**Optimization**\nthe calculated loss, which tells us how poorly the model is performing at that current instant. Now we need to use this loss to train our network such that it performs better. Essentially what we need to do is to take the loss and try to minimize it, because a lower loss means our model is going to perform better. The process of minimizing (or maximizing) any mathematical expression is called optimization.\nOptimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function.\n\n![loss](https:\/\/static.wixstatic.com\/media\/3eee0b_33163162ddd94900b7d9f5b049e9b7e3~mv2.gif)\n\nlist of optimizer on tensorflow \n\n* Gradient Descent\n* Stochastic Gradient Descent (SGD)\n* Mini Batch Stochastic Gradient Descent (MB-SGD)\n* SGD with momentum\n* Nesterov Accelerated Gradient (NAG)\n* Adaptive Gradient (AdaGrad)\n* AdaDelta\n* RMSprop\n* Adam\n\n**Gradient** - in short its rate of change of error(loss) with respect of neural network parameter..\nits like adjusting weights so that loss is minimum. Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function.\n\n**learning rate** -the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function \n\n![lrate](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/02\/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n\n\n","8dd53279":"Base kernals.\n\n[TPS-09 NN](https:\/\/www.kaggle.com\/lukaszborecki\/tps-09-nn) - @Lukasz Borecki\n\n[\ud83e\ude7a Single NN](https:\/\/www.kaggle.com\/firefliesqn\/single-nn) - @Chi\u1ebfn Nguy\u1ec5n\n\n\n\n","9636acf0":"<h1 style=\"background-color:lightpink;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   3:Corelation Analysis<\/h1>\n\n","a3ba08b7":"<h1 style=\"background-color:lightpink;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   1. INTRODUCTION:<\/h1>\n\n![nmnm](https:\/\/th.bing.com\/th\/id\/R.26b3039031a6b7be0aacce7908a74b22?rik=7Ru7SFUZSBd2PA&riu=http%3a%2f%2fgadget.fsetyt.com%2fwp-content%2fuploads%2f2017%2f06%2fsimple-words-about-the-complex-what-are-neural-networks-1.png&ehk=%2bJTf%2bVipYUhzbdcZSOeZUeER5Xr3qcbij3WTTRKlkOo%3d&risl=&pid=ImgRaw&r=0)","5e48234d":"![i](https:\/\/miro.medium.com\/max\/1104\/1*A5-BsAbWw4UosgIei1JEpg.jpeg)\n\n\n* **Sigmoid functions and their combinations generally work better in the case of classification problems.**\n* **Sigmoid and tanh functions are sometimes avoided due to the vanishing gradient problem.**\n* **Tanh is avoided most of the time due to dead neuron problem.**\n* **ReLU activation function is widely used and is default choice as it yields better results.**\n* **If we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice.**\n* **ReLU function should only be used in the hidden layers.**\n* **An output layer can be linear activation function in case of regression problems.**\n* [see list of available Activation functions in tensorflow ](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations)\n* **swish is outperforming Relu.. so a better to start from**\n","23916c2b":"<h1 style=\"background-color:lightpink;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d  Basics of Neural Network<\/h1>","17dc2118":"**Columns that show categorical behaviour**\n\n1. f40\n2. f42\n3. f47\n4. f50\n5. f65\n6. f70\n7. f75","ecf7e79a":"![hh](https:\/\/www.researchgate.net\/profile\/Kamlesh_Golhani2\/publication\/325063770\/figure\/download\/fig2\/AS:715668395679744@1547640050215\/The-multi-layer-NN-consisting-input-layer-hidden-layer-and-output-layer.png)","8ecd6bde":"**The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.**\n\n**As is shown in the result before discretization, linear model is fast to build and relatively straightforward to interpret, but can only model linear relationships, while decision tree can build a much more complex model of the data. One way to make linear model more powerful on continuous data is to use discretization (also known as binning). In the example, we discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide, there would appear to be a substantially increased risk of overfitting, so the discretizer parameters should usually be tuned under cross validation.**\n\n**After discretization, linear regression and decision tree make exactly the same prediction. As features are constant within each bin, any model must predict the same value for all points within a bin. Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible. Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere**\n\n![png](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_discretization_001.png)\n","1e2bd0c4":"1. **N_missing values in a row as a column**\n\n2. **interpolation() -** consider a column as arthmetic series although its not:[1 4 7 10 . 16 19 . . 28] interpolate() function is used to fill NA values in the dataframe. But, this is a very powerful function to fill the missing values. It uses various interpolation technique to fill the missing values rather than hard-coding the value.\n[pandas.interpolate() link](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.interpolate.html)","cb160d1b":"<h1 style=\"background-color:lightpink;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   3. Univeriate Distribution:<\/h1>\n","69cfbfa7":"<h1 style=\"background-color:lightpink;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   3: Data preprocessing <\/h1>\n","df234e7a":"<h1 style=\"background-color:lightpink;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   2. EDA: Missing value itself a challenge to solve<\/h1>"}}