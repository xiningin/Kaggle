{"cell_type":{"571ef262":"code","dfb01f3d":"code","7b1412ae":"code","3282eb17":"code","588410b5":"code","d0c8fd33":"code","7e5008b3":"code","7a17b82c":"code","6d34f434":"code","ac0fc635":"code","4fa4936a":"code","5679b7dc":"code","922b20f8":"code","b7a42387":"code","be9df426":"code","84037aeb":"code","0524b71d":"code","988a859e":"code","c638c2ac":"code","5fa1c85d":"code","74eb4416":"code","4af505fe":"code","f3ccdbc7":"code","e6552b1a":"code","f66e0c89":"code","2123c0c7":"code","68ca4077":"code","76ef49be":"code","7030ae63":"code","c3ebd7e6":"code","14f9c8c4":"code","e7b47829":"markdown","af40a21e":"markdown","6264e855":"markdown","65018967":"markdown","c0fb48be":"markdown","c8d13f26":"markdown"},"source":{"571ef262":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfb01f3d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\n\n#sklearn model\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nwarnings.filterwarnings('ignore')","7b1412ae":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","3282eb17":"train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\n\n\n# train = train.sample(10000)\n# test = test.sample(10000)","588410b5":"train.head()","d0c8fd33":"test.head()","7e5008b3":"print(f'train shape: {train.shape}')\nprint(f'test shape: {test.shape}')","7a17b82c":"train.info()","6d34f434":"train.describe()","ac0fc635":"train.drop([\"Soil_Type7\", \"Id\", \"Soil_Type15\"], axis=1, inplace=True)\ntest.drop([\"Soil_Type7\", \"Id\", \"Soil_Type15\"], axis=1, inplace=True)","4fa4936a":"train = train[train.Cover_Type != 5]","5679b7dc":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain.rename(new_names, axis=1, inplace=True)\ntest.rename(new_names, axis=1, inplace=True)","922b20f8":"train[\"Aspect\"][train[\"Aspect\"] < 0] += 360\ntrain[\"Aspect\"][train[\"Aspect\"] > 359] -= 360\n\ntest[\"Aspect\"][test[\"Aspect\"] < 0] += 360\ntest[\"Aspect\"][test[\"Aspect\"] > 359] -= 360","b7a42387":"train[\"mnhttn_dist_hydrlgy\"] = np.abs(train[\"x_dist_hydrlgy\"]) + np.abs(train[\"y_dist_hydrlgy\"])\ntest[\"mnhttn_dist_hydrlgy\"] = np.abs(test[\"x_dist_hydrlgy\"]) + np.abs(test[\"y_dist_hydrlgy\"])\n\n# Euclidean distance to Hydrology\ntrain[\"ecldn_dist_hydrlgy\"] = (train[\"x_dist_hydrlgy\"]**2 + train[\"y_dist_hydrlgy\"]**2)**0.5\ntest[\"ecldn_dist_hydrlgy\"] = (test[\"x_dist_hydrlgy\"]**2 + test[\"y_dist_hydrlgy\"]**2)**0.5","be9df426":"train.loc[train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain.loc[train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain.loc[train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain.loc[train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain.loc[train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain.loc[train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","84037aeb":"features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ndef addFeature(X):\n    # Thanks @mpwolke : https:\/\/www.kaggle.com\/mpwolke\/tooezy-where-are-you-no-camping-here\n    X[\"Soil_Count\"] = X[soil_features].apply(sum, axis=1)\n\n    # Thanks @yannbarthelemy : https:\/\/www.kaggle.com\/yannbarthelemy\/tps-december-first-simple-feature-engineering\n    X[\"Wilderness_Area_Count\"] = X[wilderness_features].apply(sum, axis=1)\n    X[\"Hillshade_mean\"] = X[features_Hillshade].mean(axis=1)\n    X['amp_Hillshade'] = X[features_Hillshade].max(axis=1) - X[features_Hillshade].min(axis=1)","0524b71d":"addFeature(train)\naddFeature(test)","988a859e":"train = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)\n","c638c2ac":"x_train = train.drop('Cover_Type', axis=1)\ny_train = train.Cover_Type\n\nx_test = test","5fa1c85d":"from sklearn.cluster import KMeans\n\ncols = x_train.columns\nfor i in range(3,10):\n    kmeans = KMeans(n_clusters=i, random_state=0).fit(x_train[cols])\n    \n    y_cluster = kmeans.predict(x_train[cols])\n    x_train['cluster_%d'%i] = y_cluster\n\n    x_test['cluster_%d'%i] = kmeans.predict(x_test[cols])","74eb4416":"x_train.head()","4af505fe":"# target visualization\nplt.pie(y_train.value_counts(), autopct='%1.1f%%')\nplt.axis('equal') ","f3ccdbc7":"# scaler = StandardScaler()\n# x_train = scaler.fit_transform(x_train)\n# x_test = scaler.transform(x_test)\n\nx_train = x_train.values\nx_test = x_test.values \ny_train = y_train.values\ngc.collect()","e6552b1a":"def objective(trial):\n\n    param_grid = {\n              'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n              'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n              'subsample': trial.suggest_categorical ('subsample', [0.2,0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1.0, 0.1),\n              'max_depth': trial.suggest_int('max_depth', 2, 20),\n              'booster': 'gbtree',\n              'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n              'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n              'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n              'random_state': 42,\n                 }\n\n    x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=50)\n    xgb_model = XGBClassifier(**param_grid, tree_method='gpu_hist', predictor='gpu_predictor',\n                            eval_metric=['logloss'])\n\n    xgb_model.fit(x_train_, y_train_, verbose=False)\n    y_pred = xgb_model.predict(x_val)\n    return accuracy_score(y_val, y_pred)","f66e0c89":"train_time = 1 * 30 * 60 # h * m * s\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='XGBClassifier')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","2123c0c7":"xgb_params = trial.params\n# xgb_params = {\n#         'n_estimators': 1738,\n#         'learning_rate': 0.02,\n#         'subsample': 0.8,\n#         'colsample_bytree': 0.9,\n#         'max_depth': 14,\n#         'gamma': 1.6093487810582865,\n#         'reg_alpha': 70,\n#         'reg_lambda': 73 }\n\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'","68ca4077":"from sklearn.model_selection import KFold\n\nn_split = 5\nkfold = KFold(n_split)\n\nval_pred = np.zeros(y_train.shape)\ny_test = np.zeros((n_split, x_test.shape[0]))\n\nfor i, (train_index, val_index) in enumerate(kfold.split(x_train)):\n    # train model\n    print(\"fold {} training\".format(i))\n    model = XGBClassifier(**xgb_params)\n#     print( pd.value_counts(y_train[train_index]))\n    model.fit(x_train[train_index], y_train[train_index])\n    \n    # predict val and test\n    val_pred[val_index] = model.predict(x_train[val_index])\n    vla_score = accuracy_score(y_train[val_index], val_pred[val_index])\n    print(\"fold {} validation acc score {}\".format(i, vla_score))\n    \n    y_test[i] = model.predict(x_test)","76ef49be":"y_test = y_test.astype('int32')","7030ae63":"for i in range(y_test.shape[1]):\n    count = np.bincount(y_test[:,i])\n    y_test[0][i]= np.argmax(count)\n\ny_test = y_test[0: 1].reshape((-1,))\nprint(y_test[:10])\n","c3ebd7e6":"sub_mission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsub_mission.Cover_Type = y_test\nsub_mission.to_csv('submission.csv', index=False)\n","14f9c8c4":"sub_mission.head()","e7b47829":"## EDA","af40a21e":"### data visualization","6264e855":"## Train Model","65018967":"## Read Data","c0fb48be":"### feature engineering","c8d13f26":"## Submission"}}