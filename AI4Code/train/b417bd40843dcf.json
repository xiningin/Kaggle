{"cell_type":{"2189eca8":"code","1234f397":"code","b6b73f17":"code","9fe18110":"code","70c20af1":"code","2b2d3e0b":"code","bcbce34e":"code","df3a10db":"code","377a4269":"code","55e095ad":"code","52b25836":"code","91f0b26a":"markdown","c9b64802":"markdown","ed67164d":"markdown"},"source":{"2189eca8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1234f397":"import pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nrandom.seed(777)\ntorch.manual_seed(777)\nif device == 'cuda':\n  torch.cuda.manual_seed_all(777)","b6b73f17":"train_data=pd.read_csv('\/kaggle\/input\/stay-or-leave\/train_data.csv',header=None,skiprows=1,usecols=range(0,10))\ntest_data=pd.read_csv('\/kaggle\/input\/stay-or-leave\/test_data.csv',header=None,skiprows=1,usecols=range(0,9))","9fe18110":"x_train=train_data.loc[:,0:8]\ny_train=train_data.loc[:,[9]]\n\nx_train=np.array(x_train)\ny_train=np.array(y_train)\n\nscaler=MinMaxScaler()\nx_train=scaler.fit_transform(x_train)\n\nx_train=torch.FloatTensor(x_train)\ny_train=torch.FloatTensor(y_train)","70c20af1":"dataset=TensorDataset(x_train,y_train)\ndataloader=DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True)","2b2d3e0b":"linear1=torch.nn.Linear(9,512,bias=True)\nlinear2=torch.nn.Linear(512,512,bias=True)\nlinear3=torch.nn.Linear(512,256,bias=True)\nlinear4=torch.nn.Linear(256,256,bias=True)\nlinear5=torch.nn.Linear(256,128,bias=True)\nlinear6=torch.nn.Linear(128,128,bias=True)\nlinear7=torch.nn.Linear(128,64,bias=True)\nlinear8=torch.nn.Linear(64,64,bias=True)\nlinear9=torch.nn.Linear(64,32,bias=True)\nlinear10=torch.nn.Linear(32,1,bias=True)\nrelu=torch.nn.ReLU()\n\ntorch.nn.init.kaiming_normal_(linear1.weight)\ntorch.nn.init.kaiming_normal_(linear2.weight)\ntorch.nn.init.kaiming_normal_(linear3.weight)\ntorch.nn.init.kaiming_normal_(linear4.weight)\ntorch.nn.init.kaiming_normal_(linear5.weight)\ntorch.nn.init.kaiming_normal_(linear6.weight)\ntorch.nn.init.kaiming_normal_(linear7.weight)\ntorch.nn.init.kaiming_normal_(linear8.weight)\ntorch.nn.init.kaiming_normal_(linear9.weight)\ntorch.nn.init.kaiming_normal_(linear10.weight)","bcbce34e":"model=torch.nn.Sequential(linear1,relu,linear2,relu,linear3,relu,linear4,relu,linear5,relu,linear6,relu,linear7,relu,linear8,relu,linear9,relu,linear10).to(device)\nloss=torch.nn.MSELoss().to(device)\noptimizer=torch.optim.Adam(model.parameters(),lr=1e-3)","df3a10db":"epochs=50\n\nfor epoch in range(epochs):\n\n  for x,y in dataloader:\n    x=x.to(device)\n    y=y.to(device)\n\n    hypo=model(x)\n    cost=loss(hypo,y)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n  print('epoch {} cost {}'.format(epoch,cost))","377a4269":"x_test=test_data.loc[:,:]\nx_test=np.array(x_test)\nx_test=scaler.transform(x_test)\nx_test=torch.FloatTensor(x_test).to(device)\n\npredict=model(x_test)","55e095ad":"predict[predict>0.5350]=1\npredict[predict<=0.5350]=0","52b25836":"predict=predict.long().cpu().numpy().reshape(-1,1)\nid=np.array([i for i in range(len(predict))]).reshape(-1,1).astype(np.uint32)\n\nresult = np.hstack([id,predict])\n\nsubmit= pd.DataFrame(result, columns=('Id','Expected'))","91f0b26a":"# \ubaa8\ub378\ud559\uc2b5","c9b64802":"# \ub370\uc774\ud130\ub85c\ub4dc","ed67164d":"# \ub370\uc774\ud130\uc608\uce21"}}