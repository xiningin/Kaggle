{"cell_type":{"f7b6b1f4":"code","713d4e62":"code","6c33771a":"code","2e176cfd":"code","9ca8ecea":"code","bed1441f":"code","a377e75f":"code","960d3639":"code","b065bdbb":"code","14cd1d07":"code","21d912eb":"code","cb8c1aec":"code","d5566c07":"code","22ae8636":"code","96f5811f":"code","ba4227bb":"code","3045a1f0":"code","ce89fefb":"code","dc9a59b1":"code","00794311":"code","2da06ae5":"code","05d1a25a":"code","0c76af80":"code","31d74d52":"markdown","3a6d7b78":"markdown","4b77c9cd":"markdown","6bd0ebea":"markdown","cf7cbfaa":"markdown","863f04a0":"markdown","37ec19b5":"markdown","2ac7ed49":"markdown","266a8a4f":"markdown","0a629d7b":"markdown","01d624bc":"markdown","929ffebe":"markdown","b6f0c588":"markdown","60363e20":"markdown","d7f04935":"markdown","8f59c6a1":"markdown","4053834b":"markdown","d547a725":"markdown","6abbaed4":"markdown"},"source":{"f7b6b1f4":"!unzip -o '\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/*.zip' -d \/kaggle\/working > \/dev\/null","713d4e62":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","6c33771a":"train=pd.read_csv(\"train.csv\")\ntest=pd.read_csv(\"test.csv\")\nsamp_sub=pd.read_csv(\"sample_submission.csv\")\n\nprint(f\"Train Data Dimensions are {train.shape}\")\nprint(f\"Test Data Dimensions are {test.shape}\")\nprint(f\"Sample Submission Dimensions are {samp_sub.shape}\")\ntrain.head(3)","2e176cfd":"import plotly.express as px\n\ndf1=pd.DataFrame(train[train.columns[2:]].sum(axis=0)).reset_index()\ndf1[\"Label\"]=df1[\"index\"]\ndf1[\"Count of Comments\"]=df1[0]\ndf1=df1.sort_values(0, ascending=False)\nfig = px.bar(df1, x=\"Label\", y=\"Count of Comments\", title=\"No. of comments per label\",color=\"Count of Comments\",text=\"Count of Comments\")\nfig.update_traces( textposition='outside')\nfig.show()","9ca8ecea":"df2=pd.DataFrame(pd.DataFrame(train[train.columns[2:]].sum(axis=1)).reset_index()[0].value_counts())\ndf2[\"Count of sentences\"]=df2[0]\ndf2[\"No. of labels in a sentence\"]=df2.index\n\nfig = px.bar(df2, x=\"No. of labels in a sentence\", y=\"Count of sentences\", title=\"No of comments based on the count of labels\",color=\"Count of sentences\", text=\"Count of sentences\")\nfig.update_traces( textposition='outside')\nfig.show()","bed1441f":"del df1\ndel df2","a377e75f":"for i in range(3):\n    print(i, \"--\",train[\"comment_text\"][i])\n    print(\"\\n\")","960d3639":"from nltk import word_tokenize\n\ntrain['tokenized_sents'] = train.apply(lambda row: word_tokenize(row['comment_text']), axis=1)\nlengths = [len(line) for line in train[\"tokenized_sents\"]]\npx.histogram(lengths)","b065bdbb":"train[\"comment_text\"][np.argmax(lengths)]","14cd1d07":"del lengths\nimport re\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ntrain['preprocess'] = train.apply(lambda row: row['comment_text'].replace(\"\\n\",\" \"), axis=1) #removes new line character\ntest['preprocess'] = test.apply(lambda row: row['comment_text'].replace(\"\\n\",\" \"), axis=1)\n\n#removes urls\ntrain['preprocess']=train.apply(lambda row: re.sub('http:\/\/\\S+|https:\/\/\\S+', 'urls',row['preprocess']).lower(), axis=1)\ntest['preprocess']=test.apply(lambda row: re.sub('http:\/\/\\S+|https:\/\/\\S+', 'urls',row['preprocess']).lower(), axis=1)\n\n#remove all non-alphanumeric values(Except single quotes)\ntrain['preprocess']=train.apply(lambda row: re.sub('[^A-Za-z\\' ]+', '',row['preprocess']).lower(), axis=1)\ntest['preprocess']=test.apply(lambda row: re.sub('[^A-Za-z\\' ]+', '',row['preprocess']).lower(), axis=1)\n\n#remove stopwords as they occupy major chunk of the vocabulary\ntrain['preprocess'] = train['preprocess'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest['preprocess'] = test['preprocess'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n#removes all additional spaces\ntrain['preprocess']=train.apply(lambda row: re.sub('  +', ' ',row['preprocess']).strip(), axis=1)\ntest['preprocess']=test.apply(lambda row: re.sub('  +', ' ',row['preprocess']).strip(), axis=1)","21d912eb":"train['tokenized_sents'] = train.apply(lambda row: word_tokenize(row['preprocess']), axis=1)\nlengths = [len(line) for line in train[\"tokenized_sents\"]]\npx.histogram(lengths)","cb8c1aec":"del lengths\nprint(train[\"comment_text\"][70],\"\\n\",\"---------------\",\"\\n\",train[\"preprocess\"][70])","d5566c07":"train[\"preprocess1\"] = train.apply(lambda x: x[\"comment_text\"] if len(x[\"preprocess\"])==0 else x['preprocess'], axis=1)\ntest[\"preprocess1\"] = test.apply(lambda x: x[\"comment_text\"] if len(x[\"preprocess\"])==0 else x['preprocess'], axis=1)","22ae8636":"traindf=train[['preprocess1','toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]\ntestdf=test[['id','preprocess1']]","96f5811f":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Input","ba4227bb":"traind=traindf[\"preprocess1\"]\ntrain_label=traindf[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]\ntestd=testdf[\"preprocess1\"]","3045a1f0":"#prepare tokenizer\ntokenizer = Tokenizer(num_words = 30000) #only 30000 words i am considering here\ntokenizer.fit_on_texts(traind)\n\n#convert each text into array of integers with help of tokenizer.\ntrain_final = tokenizer.texts_to_sequences(traind)\ntest_final = tokenizer.texts_to_sequences(testd)","ce89fefb":"traind=pad_sequences(train_final, maxlen=200)\ntestd=pad_sequences(test_final, maxlen=200)","dc9a59b1":"print(traind.shape,train_label.shape)","00794311":"model = Sequential()\nmodel.add(Embedding(30000, 128))\nmodel.add(LSTM(units = 128, dropout = 0.2, recurrent_dropout = 0.2,return_sequences=True))\nmodel.add(LSTM(units = 128, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(units = 6, activation = 'sigmoid'))\nmodel.summary()","2da06ae5":"from sklearn.model_selection import train_test_split\nmodel.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"AUC\"])\nx_train, x_val, y_train, y_val = train_test_split(traind, train_label, shuffle = True, random_state = 123)\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\nmodel.fit(x_train, y_train, batch_size = 128, epochs = 1, validation_data = (x_val, y_val))","05d1a25a":"result = pd.merge(testdf, samp_sub, on = \"id\")\npreds = model.predict(testd)\nresult[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = preds\nresult.drop([\"preprocess1\"], axis = 1, inplace = True)\nresult.to_csv(\"submission.csv\", index = False)","0c76af80":"test[test[\"preprocess\"].str.len()==0]","31d74d52":"### Great!! The maximum length reducing to 1200 because of preprocessing. Looks fine. Now lets compare few raw strings and preprocessed strings to see whether we capture all the relevant information and also lets take care of zero length rows","3a6d7b78":"## Lets create a model and train it. As the problem belongs to mutilabel classification, final layer activation is sigmoid and loss function is binary cross entropy. We  will see auc as the performance metric","4b77c9cd":"# ERROR ANALYSIS - I also observed some sentences in test data belong to other languages. Some of them are shown below","6bd0ebea":"# That's all in this notebook. First task of NLP kaggle series is done. Stat tuned for the next tasks. Do upvote if you find this helpful","cf7cbfaa":"* From the above graph it is evident that most of the comments having toxic label\n* Now let's observe the distribution os sentences based on the count of labels","863f04a0":"* From the above graph we can see that 1,43,346 out of 1,59,571 sentences does not have any labels","37ec19b5":"In the above cell we can see a small segment of train data. We have a text_id, text and the labels. \"0\" and \"1\" are used to present the labels.","2ac7ed49":"### It is good to see most of the sentences are falling between the range of 0-500. We have few outliers where no.of words is beyond 500. Lets look at an example","266a8a4f":"## First step let's import some basic libraries and have a look at the data","0a629d7b":"## I got 95.4% as final accuracy. It can be improved by stacking hyperparameter tuning and running for more epochs. It can also be improved by using transformers pretrained models. ","01d624bc":"## Ahh! This is why we need preprocessing. The reason why we have such huge length sentences is because we have lot of special characters. Lets prepocess the data and then see the distribution of sentence lengths again","929ffebe":"# Kaggle NLP series - 1st Task - Toxic comment classification\n\nHi Everyone, This is the **first notebook in my NLP Kaggle series**. I took up a simple toxic comment classification in the first notebook and as the series progress I will attempt more complex NLP problems. \n\n###  Do upvote if you find this helpful. Let's get started. \n\nI am going to perform EDA and use different models for feature generation (embeddings) and classification","b6f0c588":"## If we use a pretrained multilingual tranformer models, there is a high chance to increase the accuracy. ","60363e20":"### It looks like we have sentences of all different kind of lengths.. lets see their distribution","d7f04935":"## Brief description about the problem:\n\n* A large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are: toxic, severe_toxic, obscene, threat, insult and identity hate.\n\n* I have to build a multi-headed model that\u2019s capable of detecting different types of of toxicity\n\n\n### Multiclass vs Multilabel Classification\n\n* Let me give an example to distinguish between multiclass and multi label.\n\n![image.png](attachment:image.png)\n\n#### The Godfather movie rated as \"R\" which is highlighted in red. If you consider any movie there can be only one type of rating for it. It can be rated as G, or PG or R but only one type. So classifying a movie based on its rating is a **multiclass classification** problem (Each sample is assigned to only one label).\n\n#### Now consider the genre of the same movie which is highlighted in blue. There can be a FINITE set of genres and a movie can fall into one or multiple genres. It can be a (fantasy & thriller & crime) or it can be (drama&crime), you name it. So detecting different type of genres that a movie belongs is a **multilabel classification** problem","8f59c6a1":"### Downloading data from kaggle","4053834b":"### Now lets observe the sentences","d547a725":"### Now lets pad the sentences. From the above histogram it is evident that most of sentences falling in range 1-200 lengths. So I will keep max length=200. We will trim any sentence above that length and we will pad zeros for all the sentence below 200","6abbaed4":"## Data preprocessed. Now lets prepare data for training"}}