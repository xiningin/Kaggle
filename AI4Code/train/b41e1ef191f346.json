{"cell_type":{"48508cf0":"code","ab957ea2":"code","630c333f":"code","cb2d5e1e":"code","6afd4535":"code","5dad6dad":"code","3e00538f":"code","d54d8457":"code","3210b96d":"code","260e91ba":"code","788f8c35":"code","3fdf2707":"code","58dffda1":"code","83aacf05":"code","1345fe95":"code","ff9bc682":"code","abaf78f4":"code","596694c2":"code","b3820d7b":"code","ccc33cf8":"code","e43366cb":"code","03e588dd":"code","b0b0bc10":"code","4b375930":"code","62bf40fc":"code","5184960a":"code","7ad2717b":"code","4e443954":"code","f4266bc9":"code","a4067dbe":"code","3ca529fb":"code","b00edd81":"code","67a8c28d":"code","11dfa8be":"markdown","94dc7dca":"markdown","5c0a8bcb":"markdown","0d70e134":"markdown","eab5024d":"markdown","927981a6":"markdown","45ff341d":"markdown","aad5bf6c":"markdown","a3d45919":"markdown","15943218":"markdown","3de79f4c":"markdown","7b37a406":"markdown","5fa198e4":"markdown","d20c43da":"markdown","6f1e5cbc":"markdown","74e3a4a7":"markdown","931c4a4b":"markdown","b2de88fe":"markdown","15954a48":"markdown","43ecc901":"markdown","6c767de7":"markdown","d00b9749":"markdown","632dfb4e":"markdown","8a1f1566":"markdown","ea2ac2e8":"markdown","c9048cb2":"markdown","d439ac33":"markdown","8bc73878":"markdown","e0e1f255":"markdown","e04cf684":"markdown","df74c466":"markdown","7ba9ee30":"markdown","541e60b9":"markdown","9a0ae634":"markdown","01bb9485":"markdown","f3131a20":"markdown","3d7daef8":"markdown","183e9877":"markdown","8fbc7e5d":"markdown"},"source":{"48508cf0":"!pip install pycaret --quiet","ab957ea2":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n#import regression module\nfrom pycaret.regression import *","630c333f":"BASE_PATH = '..\/input\/trends-assessment-prediction'\n\nfnc_df = pd.read_csv(f\"{BASE_PATH}\/fnc.csv\")\nloading_df = pd.read_csv(f\"{BASE_PATH}\/loading.csv\")\nlabels_df = pd.read_csv(f\"{BASE_PATH}\/train_scores.csv\")","cb2d5e1e":"fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nlabels_df[\"is_train\"] = True\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\nprint(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')","6afd4535":"target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\ndf.drop(['is_train'], axis=1, inplace=True)\ntest_df = test_df.drop(target_cols + ['is_train'], axis=1)\n\n\n# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1\/500\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","5dad6dad":"def get_train_data(target):\n    other_targets = [tar for tar in target_cols if tar != target]\n    train_df = df.drop( other_targets, axis=1)\n    return train_df","3e00538f":"target = 'age'\n\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)","d54d8457":"blacklist_models = ['tr']","3210b96d":"compare_models(\n    blacklist = blacklist_models,\n    fold = 10,\n    sort = 'MAE', ## competition metric\n    turbo = True\n)\n","260e91ba":"br_age = create_model(\n    estimator='br',\n    fold=10\n)","788f8c35":"# here we are tuning the above created model\ntuned_br_age = tune_model(\n    estimator='br',\n    fold=10,\n    optimize = 'mae',\n    n_iter=50\n)","3fdf2707":"# plot_model(estimator = None, plot = \u2018residuals\u2019)\nplot_model(estimator = tuned_br_age, plot = 'learning')","58dffda1":"plot_model(estimator = tuned_br_age, plot = 'residuals')","83aacf05":"plot_model(estimator = tuned_br_age, plot = 'feature')","1345fe95":"evaluate_model(estimator=tuned_br_age)","ff9bc682":"predictions =  predict_model(tuned_br_age, data=test_df)","abaf78f4":"predictions.head()","596694c2":"target = target_cols[0]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 10,\n    sort = 'MAE',\n    turbo = True\n)","b3820d7b":"target = target_cols[1]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 10,\n    sort = 'MAE',\n    turbo = True\n)","ccc33cf8":"target = target_cols[2]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 10,\n    sort = 'MAE',\n    turbo = True\n)","e43366cb":"target = target_cols[3]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 10,\n    sort = 'MAE',\n    turbo = True\n)","03e588dd":"target = target_cols[4]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 10,\n    sort = 'MAE',\n    turbo = True\n)","b0b0bc10":"# mapping targets to their corresponding models\n\nmodels = []\n\ntarget_models_dict = {\n    'age': 'br',\n    'domain1_var1':'catboost',\n    'domain1_var2':'svm',\n    'domain2_var1':'catboost',\n    'domain2_var2':'catboost',\n}\n\ndef tune_and_ensemble(target):\n    train_df = get_train_data(target)    \n    exp_reg = setup(\n        data = train_df,\n        target = target,\n        train_size=0.8,\n        numeric_imputation = 'mean',\n        silent = True\n    )\n    \n    model_name = target_models_dict[target]\n    tuned_model = tune_model(model_name, fold=10)\n    model = ensemble_model(tuned_model, fold=10)\n    return model\n","4b375930":"target = target_cols[0]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","62bf40fc":"target = target_cols[1]\nmodel = tune_and_ensemble(target)\nmodels.append(model)\n%tb","5184960a":"target","7ad2717b":"target = target_cols[2]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","4e443954":"target = target_cols[3]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","f4266bc9":"target = target_cols[4]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","a4067dbe":"### create a pipeline or function to run for all targets\n\ndef finalize_model_pipeline(model, target):\n    # this will train the model on holdout data\n    finalize_model(model)\n    save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)\n    # making predictions on test data\n    predictions = predict_model(model, data=test_df)\n    test_df[target] = predictions['Label'].values","3ca529fb":"for index, target in enumerate(target_cols):\n    model = models[index]\n    finalize_model_pipeline(model,target)","b00edd81":"sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\n\nsub_df.to_csv(\"submission1.csv\", index=False)","67a8c28d":"sub_df.head()","11dfa8be":"### 5.1 Tuning Bayesian Ridge Model for `age`","94dc7dca":"![pycaret](https:\/\/miro.medium.com\/max\/1400\/1*Q34J2tT_yGrVV0NU38iMig.jpeg)","5c0a8bcb":"### 5.3 Tuning Support Vector Machines Model for `domain1_var2`","0d70e134":"### Observations:\n\nOn close observation of ablove model comparisons, we have made following observations:\n* `age`: Bayesian Ridge\thas the minimum MAE\n* `domain1_var1`: CatBoost Regressor has the minimum MAE\n* `domain1_var2`: Support Vector Machines has the minimum MAE\n* `domain2_var1`: CatBoost Regressor has the minimum MAE\n* `domain2_var2`: CatBoost Regressor has the minimum MAE\n\n> **Note: For demo purpose i have taken the model with lowest MAE in each target category, feel free to experiment with lowest 2 or 3 models and also try ensemble of various models in each category for better results.**","eab5024d":"## 1.1 Installation (Let's install PyCaret)","927981a6":"## 3. Let's proceed with PyCaret for regression","45ff341d":"## 2.1 Load Data","aad5bf6c":"## 2 Importing Libraries","a3d45919":"### 5.2 Tuning CatBoost Regressor Model for `domain1_var1`","15943218":"### 3.2 Compare Models\n\n* `compare_models` function uses all models in the model library and scores them using K-fold Cross Validation. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold (default CV = 10 Folds) of all the available models in model library.\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#compare-models","3de79f4c":"# References\n\n* https:\/\/pycaret.org\/regression\/\n* https:\/\/towardsdatascience.com\/machine-learning-in-power-bi-using-pycaret-34307f09394a","7b37a406":"### 5.4 Tuning CatBoost Regressor Model for `domain2_var1`","5fa198e4":"### 3.5.2 plotting residuals","d20c43da":"# END NOTES\n\n* This notebook is work in progress.  I will kepp updating this kernel with more and more info.\n* Feel free to use this kernel as the starting point. Happy kaggling:)\n\n**<span style=\"color:Red\">Please upvote this kernel if you like it . It motivates me to produce more quality content :)**  ","6f1e5cbc":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Introduction-to-PyCaret\" data-toc-modified-id=\"1.-Introduction-to-PyCaret-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>1. Introduction to PyCaret<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Installation-(Let's-install-PyCaret)\" data-toc-modified-id=\"1.1-Installation-(Let's-install-PyCaret)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>1.1 Installation (Let's install PyCaret)<\/a><\/span><\/li><li><span><a href=\"#2-Importing-Libraries\" data-toc-modified-id=\"2-Importing-Libraries-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>2 Importing Libraries<\/a><\/span><\/li><li><span><a href=\"#2.1-Load-Data\" data-toc-modified-id=\"2.1-Load-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>2.1 Load Data<\/a><\/span><\/li><li><span><a href=\"#2.2-Utils\" data-toc-modified-id=\"2.2-Utils-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;<\/span>2.2 Utils<\/a><\/span><\/li><li><span><a href=\"#3.-Let's-proceed-with-PyCaret-for-regression\" data-toc-modified-id=\"3.-Let's-proceed-with-PyCaret-for-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;<\/span>3. Let's proceed with PyCaret for regression<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Setup-our-dataset-(For-demo-just-using-age-target)\" data-toc-modified-id=\"3.1-Setup-our-dataset-(For-demo-just-using-age-target)-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;<\/span>3.1 Setup our dataset (For demo just using <code>age<\/code> target)<\/a><\/span><\/li><li><span><a href=\"#3.2-Compare-Models\" data-toc-modified-id=\"3.2-Compare-Models-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;<\/span>3.2 Compare Models<\/a><\/span><\/li><li><span><a href=\"#3.3-Create-Model\" data-toc-modified-id=\"3.3-Create-Model-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;<\/span>3.3 Create Model<\/a><\/span><\/li><li><span><a href=\"#3.4-Tune-Model-Hyperparameters\" data-toc-modified-id=\"3.4-Tune-Model-Hyperparameters-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;<\/span>3.4 Tune Model Hyperparameters<\/a><\/span><\/li><li><span><a href=\"#3.5-Plot-Model\" data-toc-modified-id=\"3.5-Plot-Model-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;<\/span>3.5 Plot Model<\/a><\/span><\/li><li><span><a href=\"#3.5.1-Plotting-learning-curve\" data-toc-modified-id=\"3.5.1-Plotting-learning-curve-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;<\/span>3.5.1 Plotting learning curve<\/a><\/span><\/li><li><span><a href=\"#3.5.2-plotting-residuals\" data-toc-modified-id=\"3.5.2-plotting-residuals-1.5.7\"><span class=\"toc-item-num\">1.5.7&nbsp;&nbsp;<\/span>3.5.2 plotting residuals<\/a><\/span><\/li><li><span><a href=\"#3.5.3-plotting-feature-importance\" data-toc-modified-id=\"3.5.3-plotting-feature-importance-1.5.8\"><span class=\"toc-item-num\">1.5.8&nbsp;&nbsp;<\/span>3.5.3 plotting feature importance<\/a><\/span><\/li><li><span><a href=\"#3.6-Evaluate-Model\" data-toc-modified-id=\"3.6-Evaluate-Model-1.5.9\"><span class=\"toc-item-num\">1.5.9&nbsp;&nbsp;<\/span>3.6 Evaluate Model<\/a><\/span><\/li><li><span><a href=\"#3.7-Interpret-Model\" data-toc-modified-id=\"3.7-Interpret-Model-1.5.10\"><span class=\"toc-item-num\">1.5.10&nbsp;&nbsp;<\/span>3.7 Interpret Model<\/a><\/span><\/li><li><span><a href=\"#3.8-Predict-Model\" data-toc-modified-id=\"3.8-Predict-Model-1.5.11\"><span class=\"toc-item-num\">1.5.11&nbsp;&nbsp;<\/span>3.8 Predict Model<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#4.-Let's-Proceed-With-Other-Targets\" data-toc-modified-id=\"4.-Let's-Proceed-With-Other-Targets-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;<\/span>4. Let's Proceed With Other Targets<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.1.-comparing-models-for-age\" data-toc-modified-id=\"4.1.-comparing-models-for-age-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;<\/span>4.1. comparing models for <code>age<\/code><\/a><\/span><\/li><li><span><a href=\"#4.2.-comparing-models-for-domain1_var1\" data-toc-modified-id=\"4.2.-comparing-models-for-domain1_var1-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;<\/span>4.2. comparing models for <code>domain1_var1<\/code><\/a><\/span><\/li><li><span><a href=\"#4.3.-comparing-models-for-domain1_var2\" data-toc-modified-id=\"4.3.-comparing-models-for-domain1_var2-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;<\/span>4.3. comparing models for <code>domain1_var2<\/code><\/a><\/span><\/li><li><span><a href=\"#4.4.-comparing-models-for-domain2_var1\" data-toc-modified-id=\"4.4.-comparing-models-for-domain2_var1-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;<\/span>4.4. comparing models for <code>domain2_var1<\/code><\/a><\/span><\/li><li><span><a href=\"#4.5.-comparing-models-for-domain2_var2\" data-toc-modified-id=\"4.5.-comparing-models-for-domain2_var2-1.6.5\"><span class=\"toc-item-num\">1.6.5&nbsp;&nbsp;<\/span>4.5. comparing models for <code>domain2_var2<\/code><\/a><\/span><\/li><li><span><a href=\"#Observations:\" data-toc-modified-id=\"Observations:-1.6.6\"><span class=\"toc-item-num\">1.6.6&nbsp;&nbsp;<\/span>Observations:<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#5.-Tuning-Selected-Models\" data-toc-modified-id=\"5.-Tuning-Selected-Models-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;<\/span>5. Tuning Selected Models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Tuning-Bayesian-Ridge-Model-for-age\" data-toc-modified-id=\"5.1-Tuning-Bayesian-Ridge-Model-for-age-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;<\/span>5.1 Tuning Bayesian Ridge Model for <code>age<\/code><\/a><\/span><\/li><li><span><a href=\"#5.2-Tuning-CatBoost-Regressor-Model-for-domain1_var1\" data-toc-modified-id=\"5.2-Tuning-CatBoost-Regressor-Model-for-domain1_var1-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;<\/span>5.2 Tuning CatBoost Regressor Model for <code>domain1_var1<\/code><\/a><\/span><\/li><li><span><a href=\"#5.3-Tuning-Support-Vector-Machines-Model-for-domain1_var2\" data-toc-modified-id=\"5.3-Tuning-Support-Vector-Machines-Model-for-domain1_var2-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;<\/span>5.3 Tuning Support Vector Machines Model for <code>domain1_var2<\/code><\/a><\/span><\/li><li><span><a href=\"#5.4-Tuning-CatBoost-Regressor-Model-for-domain2_var1\" data-toc-modified-id=\"5.4-Tuning-CatBoost-Regressor-Model-for-domain2_var1-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;<\/span>5.4 Tuning CatBoost Regressor Model for <code>domain2_var1<\/code><\/a><\/span><\/li><li><span><a href=\"#5.5-Tuning-CatBoost-Regressor-Model-for-domain2_var2\" data-toc-modified-id=\"5.5-Tuning-CatBoost-Regressor-Model-for-domain2_var2-1.7.5\"><span class=\"toc-item-num\">1.7.5&nbsp;&nbsp;<\/span>5.5 Tuning CatBoost Regressor Model for <code>domain2_var2<\/code><\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#6.-Finalize,-save-model-and-Inference\" data-toc-modified-id=\"6.-Finalize,-save-model-and-Inference-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;<\/span>6. Finalize, save model and Inference<\/a><\/span><\/li><li><span><a href=\"#Create-Submission\" data-toc-modified-id=\"Create-Submission-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;<\/span>Create Submission<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#References\" data-toc-modified-id=\"References-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>References<\/a><\/span><\/li><li><span><a href=\"#END-NOTES\" data-toc-modified-id=\"END-NOTES-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>END NOTES<\/a><\/span><\/li><\/ul><\/div>","74e3a4a7":"interpret_model(\n    estimator=tuned_br_age,\n    plot = 'summary',\n    feature = None,\n    observation = None\n)","931c4a4b":"### 3.8 Predict Model\n\n* `predict_model` function is used to predict new data using a trained estimator. It accepts an estimator created using one of the function in pycaret that returns a trained  model object or a list of trained model objects created using stack_models() or create_stacknet(). New unseen data can be passed to data param as pandas Dataframe.  If data is not passed, the test \/ hold-out set separated at the time of setup() is used to generate predictions.\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#predict-model\n\n\n","b2de88fe":"### 3.6 Evaluate Model\n\n* `evaluate_model` function displays a user interface for all of the available plots for a given estimator. It internally uses the plot_model() function.\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#evaluate-model\n","15954a48":"Before proceeding let me clear few things:\n* Currently `PyCaret` do not have support for multitarget regression. So, instead of 1 model for our 5 targets, we need to create individual model for each target.","43ecc901":"### 4.1. comparing models for `age`","6c767de7":"### 4.4. comparing models for `domain2_var1`","d00b9749":"## 2.2 Utils","632dfb4e":"# 1. Introduction to PyCaret\n\nPyCaret is an open source, **low-code** machine learning library in Python that aims to reduce the cycle time from hypothesis to insights. It is well suited for **seasoned data scientists** who want to increase the productivity of their ML experiments by using PyCaret in their workflows or for **citizen data scientists** and those **new to data science** with little or no background in coding. PyCaret allows you to go from preparing your data to deploying your model within seconds using your choice of notebook environment. Please click [this](https:\/\/pycaret.org\/guide\/) link to continue learning more about PyCaret. \n\n\n**<span style=\"color:Red\">Please upvote this kernel if you like it . It motivates me to produce more quality content :)**\n    \nNOTE : This notebook is derived from this excellent one : https:\/\/www.kaggle.com\/rohitsingh9990\/trends-pycaret-training-inference I have just done the training with my results.\n","8a1f1566":"### 4.5. comparing models for `domain2_var2`","ea2ac2e8":"## 6. Finalize, save model and Inference","c9048cb2":"## 4. Let's Proceed With Other Targets","d439ac33":"### 3.7 Interpret Model\n\n\n* `interpret_model` function takes a trained model object and returns an interpretation plot based on the test \/ hold-out set. It only supports tree based algorithms. This function is implemented based on the SHAP (SHapley Additive exPlanations), which is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations.\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#interpret-model\n\n","8bc73878":"For list of all available estimators and their abbreviations please visit https:\/\/pycaret.org\/regression\/","e0e1f255":"### 3.4 Tune Model Hyperparameters\n\n* `tune_model` function tunes the hyperparameters of a model and scores it using K-fold Cross Validation. The output prints the score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold (by default = 10 Folds). This function returns a trained model object.  \n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#tune-model\n","e04cf684":"### 3.5 Plot Model\n\n* `plot_model` function takes a trained model object and returns a plot based on the test \/ hold-out set. The process may require the model to be re-trained in certain cases. See list of plots supported below. Model must be created using create_model() or tune_model().\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#plot-model\n\n","df74c466":"### 4.2. comparing models for `domain1_var1`","7ba9ee30":"### 4.3. comparing models for `domain1_var2`","541e60b9":"### 3.5.3 plotting feature importance","9a0ae634":"### 5.5 Tuning CatBoost Regressor Model for `domain2_var2`","01bb9485":"## Create Submission","f3131a20":"## 5. Tuning Selected Models","3d7daef8":"### 3.3 Create Model\nFor demo purpose let's create a Bayesian Ridge\n\n* `create_model` function creates a model and scores it using K-fold Cross Validation. (default = 10 Fold). The output prints a score grid that shows MAE, MSE, RMSE, RMSLE, R2 and MAPE. This function returns a trained model object. setup() function must be called before using create_model()\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#create-model","183e9877":"### 3.1 Setup our dataset (For demo just using `age` target)\n\n* `setup` function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must called before executing any other function in pycaret. It takes two mandatory parameters: dataframe {array-like, sparse matrix} and name of the target column. All other parameters are optional.\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#setup","8fbc7e5d":"### 3.5.1 Plotting learning curve"}}