{"cell_type":{"60fb47c4":"code","84da53a6":"code","8591b60f":"code","c334517c":"code","e1571556":"code","42841f69":"code","3c071c0c":"code","5739ffd9":"code","ad8a7d46":"code","5e808332":"code","f4151245":"code","6386482a":"code","ef7430a2":"code","b9934cbc":"code","afd05fc4":"code","c4487103":"code","19300d14":"code","741e197c":"code","198dc6e4":"code","690b1c26":"code","5ceda192":"code","2dfa2dd5":"code","109dba52":"code","658a21bc":"code","c9543bf4":"code","6a6fb984":"code","5170b952":"code","ccbbf3ef":"code","ccf92ba1":"code","b99b8c19":"code","552a0d82":"code","ccac9f98":"code","d23815c8":"code","b89d02fc":"code","b319db66":"code","8e0dfe99":"code","b544df7c":"code","26fb891a":"code","4feab202":"code","bde3d1e9":"code","5d2ac22b":"code","8d31eded":"code","2d4e752c":"code","8bfdf0b0":"code","01cae6da":"code","ffe3d6c4":"code","776f206f":"code","ff0b0b96":"code","95e0443c":"code","f4fcb499":"code","9a88906b":"code","303d4a8e":"code","8fbe9576":"code","c0576323":"markdown","cd23fda3":"markdown","47e47c7d":"markdown","c888405b":"markdown","54afa2ba":"markdown","a7e32cb4":"markdown","439bf624":"markdown","eed6df36":"markdown","2a66c05e":"markdown","fb72a12d":"markdown","6a6ffe74":"markdown","30d50790":"markdown","305e97ed":"markdown","e920b1b7":"markdown","84e290ff":"markdown","e20ddad9":"markdown","d1b0ffa9":"markdown","177ed257":"markdown","b71b59e9":"markdown","8ba41764":"markdown","77b87640":"markdown","709abd10":"markdown"},"source":{"60fb47c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","84da53a6":"# Veri tan\u0131mlama\nimport numpy as np \nimport pandas as pd \n\n# Veri G\u00f6rselle\u015ftirme\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n#Sistem k\u00fct\u00fcphaneleri\nimport os\nimport warnings\n\n# \u00c7\u0131kt\u0131larda karma\u015f\u0131kl\u0131\u011fa sebep oldu\u011fu i\u00e7in uyar\u0131l\u0131r\u0131 iptal ediyoruz\nwarnings.filterwarnings(\"ignore\")\nprint(\"Warnings Ignore\")\n\nprint(os.listdir(\"..\/\"))\n","8591b60f":"print(os.listdir(\"..\/input\/red-wine-quality-cortez-et-al-2009\"))","c334517c":"data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","e1571556":"data.info()","42841f69":"print(data.columns)","3c071c0c":"data.shape","5739ffd9":"data.head()","ad8a7d46":"data.describe()","5e808332":"data.quality.unique()","f4151245":"data.quality.value_counts()","6386482a":"data.groupby([\"quality\"], as_index = True).mean()","ef7430a2":"data.corr()","b9934cbc":"\ndata[\"status\"] = data.quality.apply(lambda x: \"Good\" if x > 5 else \"Bad\")\ndata.head()","afd05fc4":"plt.figure(figsize = (8,8))\nlabels = data.status.value_counts().index\nplt.pie(data.status.value_counts(), autopct='%1.1f%%', pctdistance=0.8, textprops={'size':\"30\", 'color':\"w\"},\n        shadow=True, startangle=360)\nplt.legend(labels, title=\"Status\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1), fontsize=16)\nplt.axis('equal')\nplt.title('QUAL\u0130TY', fontsize=32)\nplt.show()","c4487103":"x = list(data.columns)\nfor i in x:\n    data.plot(kind='scatter', x=i, y='quality',alpha = 0.5,color =\"red\" )   \n    plt.title('Scatter Plot')          \n    plt.show()","19300d14":"#correlation matrix\nplt.figure(figsize=(10,5))\nheatmap = sns.heatmap(data.corr(), annot=True, fmt=\".1f\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.show()","741e197c":"cols_sns = ['residual sugar', 'chlorides', 'density', 'pH', 'alcohol', 'quality']\nsns.set(style=\"ticks\")\nsns.pairplot(data[cols_sns], hue='quality')\nplt.show()","198dc6e4":"sns.countplot(x='quality', data=data)\nplt.show()","690b1c26":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","5ceda192":"x = data.drop(['quality','status'], axis =1)\ny = data['quality'] ","2dfa2dd5":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()  \n\nx = scaler.fit_transform(x)","109dba52":"x","658a21bc":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)","c9543bf4":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train,y_train)\ntree_pred = decision_tree.predict(x_test)\nprint('Decision Tree:', accuracy_score(y_test, tree_pred)*100,'%')","6a6fb984":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\nrf_pred = rf.predict(x_test)\nprint('Random Forest:', accuracy_score(y_test, rf_pred)*100,'%')","5170b952":"KN = KNeighborsClassifier()\nKN.fit(x_train,y_train)\nKN_pred = KN.predict(x_test)\nprint('KNeighbors:',accuracy_score(y_test, KN_pred)*100,'%')","ccbbf3ef":"Gaussian = GaussianNB()\nGaussian.fit(x_train,y_train)\nGaussian_pred = Gaussian.predict(x_test)\nprint('GaussianNB:',accuracy_score(y_test, Gaussian_pred)*100,'%')","ccf92ba1":"svc = SVC()\nsvc.fit(x_train,y_train)\nsvc_pred = svc.predict(x_test)\nprint('SVC:',accuracy_score(y_test, svc_pred)*100,'%')","b99b8c19":"k = []\nl = []\nfor i in range(1,250):\n    rf_tune = RandomForestClassifier(n_estimators=i)\n    rf_tune.fit(x_train,y_train)\n    y_pred = rf_tune.predict(x_test)\n    k.append(float(accuracy_score(y_test, y_pred)*100))\n    l.append(i)\n    \nk = pd.DataFrame(k , columns=['Accuracy']) \nl = pd.DataFrame(l , columns=['n_estimator'])\ndf = pd.concat([k, l], axis = 1)","552a0d82":"df.sort_values(by='Accuracy', ascending=False)","ccac9f98":"from sklearn.decomposition import PCA\npca = PCA(n_components = 3, whiten= True )  # whitten = normalize\npca.fit(x)\n\nx_pca = pca.transform(x)\n\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\n\nprint(\"sum: \",sum(pca.explained_variance_ratio_))\n\n#%%","d23815c8":"x_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.3)","b89d02fc":"k = []\nl = []\nfor i in range(1,250):\n    rf_tune = RandomForestClassifier(n_estimators=i)\n    rf_tune.fit(x_train,y_train)\n    y_pred = rf_tune.predict(x_test)\n    k.append(float(accuracy_score(y_test, y_pred)*100))\n    l.append(i)\n    \nk = pd.DataFrame(k , columns=['Accuracy']) \nl = pd.DataFrame(l , columns=['n_estimator'])\ndf = pd.concat([k, l], axis = 1)","b319db66":"df.sort_values(by='Accuracy', ascending=False)","8e0dfe99":"data.corr()","b544df7c":"x = data.drop(['fixed acidity','citric acid','free sulfur dioxide','total sulfur dioxide','pH','quality', 'status'], axis =1)\ny = data['quality'] ","26fb891a":"x.corr()","4feab202":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)","bde3d1e9":"k = []\nl = []\nfor i in range(1,250):\n    rf_tune = RandomForestClassifier(n_estimators=i)\n    rf_tune.fit(x_train,y_train)\n    y_pred = rf_tune.predict(x_test)\n    k.append(float(accuracy_score(y_test, y_pred)*100))\n    l.append(i)\n    \nk = pd.DataFrame(k , columns=['Accuracy']) \nl = pd.DataFrame(l , columns=['n_estimator'])\ndf = pd.concat([k, l], axis = 1)","5d2ac22b":"df.sort_values(by='Accuracy', ascending=False)","8d31eded":"data[\"status\"] = data.quality.apply(lambda x: 1 if x > 5 else 0)\ndata.head()","2d4e752c":"x = data.drop(['quality','status' ,'fixed acidity','residual sugar','free sulfur dioxide','pH'], axis =1)\ny = data['status']","8bfdf0b0":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)","01cae6da":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))","ffe3d6c4":"#%%\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\ny_tahmin = lr.predict(x_test)\nresults = confusion_matrix(y_tahmin, y_test)\nprint(\"Confusion Matrix : \")\nprint(results)","776f206f":"data_cls = data.drop(['quality','status'], axis =1)","ff0b0b96":"from sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data_cls)\n    wcss.append(kmeans.inertia_)","95e0443c":"plt.figure(figsize=(10,5))\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\" , fontsize=26)\nplt.ylabel(\"Within Cluster Sum of Squares\" , fontsize=26)\nplt.show()","f4fcb499":"kmeans2 = KMeans(n_clusters=3)\nclusters= kmeans2.fit_predict(data_cls)\ndata_cls[\"label\"] = clusters","9a88906b":"data_cls","303d4a8e":"from sklearn.decomposition import PCA\npca = PCA(n_components =2 , whiten= True )  # whitten = normalize\npca.fit(data_cls)\n\nscatter =pd.DataFrame(pca.transform(data_cls))","8fbe9576":"scatter[\"label\"] = clusters\n\nplt.scatter(scatter[0][scatter.label == 0 ],scatter[1][scatter.label == 0],color = \"red\")\nplt.scatter(scatter[0][scatter.label == 1 ],scatter[1][scatter.label == 1],color = \"green\")\nplt.scatter(scatter[0][scatter.label == 2 ],scatter[1][scatter.label == 2],color = \"blue\")\nplt.show()","c0576323":"<a class=\"anchor\"><\/a>**Random Forest Classification**","cd23fda3":"<font size=12 color=\"black\" face=\"verdana\"><center> **Red Wine Quality Forecast**","47e47c7d":"<font size = 5 color = \"black\" face = \"verdana\"> Prepared for the data set of the study using the Red Wine Quality dataset from kaggle.com.\n\n<font size = 5 color = \"black\" face = \"verdana\"> This data set was arranged as shown below, chemicals affecting the quality of the wine were analyzed and the quality value was estimated. In this estimate, the models with the highest ratio were selected by trying various models.\n\n<font size = 5 color = \"black\" face = \"verdana\"> The data set was first analyzed using the numpy and pandas libraries, data distributions were examined, and the changes in features were observed with the group by method. Later, data visualizations were made using Seaborn and Matplotlib Libraries and the correlations between the variables were visually examined.\n\n<font size = 5 color = \"black\" face = \"verdana\"> In order to use Machine Learning algorithms, the quality variable dependent variable, which is among the data variables, has been accepted and a model has been created to estimate this value. By choosing the model with the highest accuracy among the created models, it was tried to reach higher accuracy on this model.\n\n<font size = 5 color = \"black\" face = \"verdana\"> In order to use the Logistic Regression Classification model, we tried to estimate the status of our data as 1 or 0 by accepting our dependent variable as 1 for values \u200b\u200babove 5 and 0 for others.\n\n<font size = 5 color = \"black\" face = \"verdana\"> We applied Clustering models to observe how our data is distributed in order to observe how our data is distributed after these model applications.","c888405b":"<a class=\"anchor\"><\/a>**SVC Classification**","54afa2ba":"<a class=\"anchor\" id=\"1.1.\"><\/a>**1.1. Inclusion of Required Libraries in Development Environment**","a7e32cb4":"The Highest Accuracy Random Forest model was selection.","439bf624":"[Contents](#0.)\n# <a class=\"anchor\" id=\"2.\"><\/a>**2. Importing to the Data Set**","eed6df36":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/4458\/6836\/30587db9a40233164f65a4a3f148f40d\/dataset-cover.jpg?t=2017-11-12-14-28-34\" alt=\"Girl in a jacket\" style=\"width:1448px;height:200px;\">\n","2a66c05e":"<a class=\"anchor\"><\/a>**Decision Tree Classification**","fb72a12d":"1. Decision Tree\n2. Random Forest\n3. KNeighbors\n4. GaussianNB\n5. SVC","6a6ffe74":"<a class=\"anchor\"><\/a>**Gaussian Classification**","30d50790":"[Contents](#0.)\n\n* # <a class=\"anchor\" id=\"3.\"><\/a>**3. Technical Evaluation About the Data Set** ","305e97ed":"<a class=\"anchor\"><\/a>**K-Nearest Neighbour (KNN) Classification**","e920b1b7":"<a class=\"anchor\" id=\"0.\"><\/a>\n<font size=10  color=\"black\" face=\"verdana\"  ><center> **Contents** <\/center>\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\"  >[1. Summary of Work](#1.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\" >[1.1. Inclusion of Required Libraries in Development Environment](#1.1.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\" >[2. Importing to the Data Set](# 2.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\"  >[3. Technical Evaluation About the Data Set](#3.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\"  >[4. Visualization of the Data Set](#4.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\"  >[5. Applying Machine Learning Algorithms](#5.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\"  >[6. Model Selection](#6.)\n    <h2>\n        <h2>\n<font size=6 color=\"black\" face=\"verdana\" >[7. Implementing Clustering Algorithms](#7.)\n    <h2>\n        <h2>","84e290ff":"[Contents](#0.)\n\n# <a class=\"anchor\" id=\"1.\"><\/a>**1. Summary of Work**","e20ddad9":"<a class=\"anchor\"><\/a>**Logistic Regression Classification**","d1b0ffa9":"[Contents](#0.)\n\n# <a class=\"anchor\" id=\"4.\"><\/a>**4. Visualization of the Data Set**  ","177ed257":"[Contents](#0.)\n\n# <a class=\"anchor\" id=\"6.\"><\/a>**6. Model Selection**","b71b59e9":"<a class=\"anchor\"><\/a>**K-Means Clustering**","8ba41764":"<a class=\"anchor\"><\/a>**Principal Component Analysis (PCA)**","77b87640":"[Contents](#0.)\n\n# <a class=\"anchor\" id=\"5.\" ><\/a>**5. Applying Machine Learning Algorithms**","709abd10":"[Contents](#0.)\n\n# <a class=\"anchor\" id=\"7.\"><\/a>**7.  Implementing Clustering Algorithms**"}}