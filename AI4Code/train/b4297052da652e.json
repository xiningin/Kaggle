{"cell_type":{"fc4c0210":"code","e1926aa0":"code","29ad4251":"code","af3407e4":"code","4871153c":"code","d026368f":"code","952618d5":"code","9ae55a31":"code","378d692f":"code","5e77271e":"code","c8754951":"code","91b1cb33":"code","f484dc2b":"code","45a086b7":"code","99c520f8":"code","b261d690":"code","5db9e8fe":"code","0efda791":"code","061b1141":"code","1c87e9e0":"code","e7149867":"code","56c1b991":"code","5a2de973":"code","65d9c476":"code","e31b3be3":"code","27fa920a":"code","71cc905d":"code","8e6742d4":"code","02a9d77c":"code","765bce9a":"code","de305e5b":"code","0e037407":"code","045618bc":"code","79e57094":"code","1299dfa9":"code","3fb21eee":"code","8154826c":"code","bac16e68":"code","4128c1df":"code","d4d14573":"code","aace4860":"code","c2c9adae":"code","ce85be4d":"code","caef03b4":"code","0ffdf4be":"markdown","563112b5":"markdown","dcd0ba89":"markdown","e290ec41":"markdown","90df5cf2":"markdown","28c59cd1":"markdown","1a8cf249":"markdown","ed56c400":"markdown","6eb008f7":"markdown","06192aa1":"markdown","ef1cb519":"markdown","aebf2f33":"markdown","d17fdae2":"markdown","91d7d5a6":"markdown","f394768d":"markdown","dceee572":"markdown","2660a15e":"markdown","d3750280":"markdown","7ad516fd":"markdown","297aab74":"markdown","274d3da8":"markdown","f1dbc19d":"markdown","0c9dabfa":"markdown"},"source":{"fc4c0210":"import numpy as np\nimport pandas as pd\nimport string","e1926aa0":"question= pd.read_csv('..\/input\/stacksample\/Questions.csv', encoding='latin')\nanswer= pd.read_csv('..\/input\/stacksample\/Answers.csv', encoding='latin')\ntags= pd.read_csv('..\/input\/stacksample\/Tags.csv', encoding='latin')","29ad4251":"question.head()","af3407e4":"answer.head()","4871153c":"tags.head()","d026368f":"print(question.shape, answer.shape, tags.shape)","952618d5":"print(question.Id.nunique(), answer.ParentId.nunique(), tags.Id.nunique())","9ae55a31":"answer.drop(columns=['Id','OwnerUserId', 'CreationDate'],inplace=True)\nanswer.columns=['Id', 'A_Score', 'A_Body']","378d692f":"grouped_answer = answer.groupby(\"Id\")['A_Body'].apply(lambda answer: ' '.join(answer))\ngrouped_answer= grouped_answer.to_frame()\ngrouped_answer= grouped_answer.sort_values(by='Id')\ngrouped_answer.head()","5e77271e":"tags['Tag']= tags['Tag'].astype(str)\ngrouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n#grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n\ngrouped_tags= grouped_tags.to_frame()\ngrouped_tags= grouped_tags.sort_values(by='Id')\ngrouped_tags.head()","c8754951":"print(grouped_answer.shape, grouped_tags.shape)","91b1cb33":"grouped_answer['Ids']= grouped_answer.index\ngrouped_tags['Ids']= grouped_tags.index\nquestion.columns= ['Ids', 'OwnerUserId', 'CreationDate', 'ClosedDate', 'Score', 'Title',\n       'Body']\nquestion= question.sort_values(by='Ids')\ndf= pd.merge(question,grouped_answer,how='left')\ndf1= pd.merge(df,grouped_tags,how='left',on='Ids')","f484dc2b":"df1.head()","45a086b7":"df1.drop(columns=['Ids', 'OwnerUserId', 'CreationDate', 'ClosedDate'],inplace=True)\ndf1.head()","99c520f8":"df1=df1.drop_duplicates()\ndf1.shape","b261d690":"print(df1.Score.min(), df1.Score.max())","5db9e8fe":"z= df1['Tag'].value_counts().sort_values(ascending=False)\nz.index","0efda791":"df2= df1.groupby(by='Tag')['Tag'].count().sort_values(ascending=False).to_frame()\ndf2.columns= ['Tag_count']\ndf2['Tags']=df2.index","061b1141":"df1.columns= ['Score', 'Title', 'Body', 'A_Body', 'Tags']\ndf1= pd.merge(df1,df2,how='left',on='Tags')\ndf1.head()","1c87e9e0":"df1= df1[df1['Tag_count']>=1000]\ndf1= df1[df1['Score']>3]\ndf1.shape","e7149867":"df1.Tags.value_counts().sort_values(ascending=False)","56c1b991":"print(df1.isnull().sum())\n\nprint('Shape of df1:',df1.shape)","5a2de973":"df1.drop(columns=['A_Body'],inplace=True)","65d9c476":"def remove_punctuation(text):\n    for punctuation in string.punctuation:\n        text= text.replace(punctuation,'')\n    return text\n    ","e31b3be3":"df1['Title']= df1['Title'].astype(str)\n\ndf1['Title1']= df1['Title'].apply(remove_punctuation)\ndf1['Title1']=df1['Title1'].str.lower()\ndf1['Title1']= df1['Title1'].str.split()\ndf1['Title1'].head()","27fa920a":"df1['Body']= df1['Body'].astype(str)\nimport re\n\ndf1['Body1']= df1['Body'].apply(lambda x: re.sub('<[^<]+?>','',x))\ndf1['Body1'].head()","71cc905d":"df1['Body1']= df1['Body1'].apply(remove_punctuation)\ndf1['Body1']=df1['Body1'].str.lower()\ndf1['Body1']= df1['Body1'].str.split()\ndf1['Body1'].head()","8e6742d4":"from nltk.stem import WordNetLemmatizer\nlematizer= WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    lem_text=[lematizer.lemmatize(i) for i in text]\n    return lem_text","02a9d77c":"df1['Title1']= df1['Title1'].apply(lambda x: word_lemmatizer(x))","765bce9a":"df1['Body1']= df1['Body1'].apply(lambda x: word_lemmatizer(x))","de305e5b":"import spacy\nsp= spacy.load('en_core_web_sm')\nall_stopwords= sp.Defaults.stop_words\ndf1['Title1']= df1['Title1'].apply(lambda x:[word for word in x if not word in all_stopwords])\ndf1['Title1'].head()                                   \n","0e037407":"import spacy\nsp= spacy.load('en_core_web_sm')\nall_stopwords= sp.Defaults.stop_words\ndf1['Body1']= df1['Body1'].apply(lambda x:[word for word in x if not word in all_stopwords])\ndf1['Body1'].head()                                   ","045618bc":"df1.drop(columns=['Title', 'Body', 'Tag_count','Score'], inplace=True)\ndf1.head()","79e57094":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndf1['Title1']= df1['Title1'].astype(str)\nvectorizer = TfidfVectorizer()\nX1 = vectorizer.fit_transform(df1['Title1'].str.lower())","1299dfa9":"df1['Body1']= df1['Body1'].astype(str)\nvectorizer = TfidfVectorizer()\nX2 = vectorizer.fit_transform(df1['Body1'].str.lower())","3fb21eee":"from sklearn.preprocessing import LabelEncoder\nle= LabelEncoder() \ndf1['Tags']= le.fit_transform(df1['Tags'])","8154826c":"y = df1['Tags'].values","bac16e68":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X2, y, test_size=0.30, random_state=42)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","4128c1df":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","d4d14573":"clf = LogisticRegression(C=10)\n\n# Creating the model on Training Data\nLOG=clf.fit(x_train,y_train)\nprediction=LOG.predict(x_test)\n\n# Printing the Overall Accuracy of the model\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","aace4860":"clf=XGBClassifier(max_depth=2, learning_rate=0.2, n_estimators=400, objective='binary:logistic', booster='gbtree')\n\n# Creating the model on Training Data\nXGB=clf.fit(x_train,y_train)\nprediction=XGB.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","c2c9adae":"model = MultinomialNB().fit(x_train,y_train)\nprediction= model.predict(x_test)\n\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","ce85be4d":"clf = KNeighborsClassifier(n_neighbors=4)\n\n# Creating the model on Training Data\nKNN=clf.fit(x_train,y_train)\nprediction=KNN.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n","caef03b4":"clf = RandomForestClassifier(max_depth=4, n_estimators=600,criterion='entropy')\n\n# Creating the model on Training Data\nRF=clf.fit(x_train,y_train)\nprediction=RF.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","0ffdf4be":"Number of rows are different in the dataframes. \nThe ids have been repeated in the tags & answer dataframe, because of the discrepancy between the number of rows and the unique IDs.","563112b5":"#### PUNCTUATION & HTML TAGS REMOVAL, LOWERCASE, WORD TOKENIZATION","dcd0ba89":"1. Merging Question and grouped_answer dataframes to get df\n2. Merging df and grouped_answer dataframes to get df1","e290ec41":"#### STOPWORD REMOVAL USING SPACY","90df5cf2":"A_Body is neither categorical nor continuous in nature. So it is not possible to impute its missing values. So we will remove it from our analysis","28c59cd1":"1. Removing punctuation\n2. Removing HTML tags (if required)\n3. Changing text into lowercase\n4. Splitting the text into words\n5. Removing stopwords","1a8cf249":"### REMOVING UNNECESSARY VARIABLES","ed56c400":"### CHANGING CATEGORICAL VARIABLES INTO NUMERIC","6eb008f7":"### CLEANING THE TEXT FOR TITLE AND BODY","06192aa1":"### READING THE DATA","ef1cb519":"### FILTERING DATA BASED ON SCORE AND MOST FREQUENTLY USED TAGS","aebf2f33":"### TF-IDF VECTORIZATION","d17fdae2":"__Applied the ML algorithms on the TF-IDF vectorization of body because it is giving a higher accuracy as compared to the TF-IDF vectorization of the title.__\n\n__XGBoost is giving the highest accuracy out of all the algorithms that have been applied.__\n\n__F1 Score: 55%__","91d7d5a6":"### SPLITTING THE DATASET INTO TRAIN AND TEST SET","f394768d":"For better prediction we will be using only those tags which have been repeated for atleast 1000 times and the score is more than 3. Low scores mean that the question is either erroneous or does not have sufficient information.","dceee572":"### CHECKING FOR DUPLICATE ROWS","2660a15e":"### CHECKING FOR MISSING VALUES","d3750280":"There are no duplicate rows because the number of rows after applying drop_duplicates function, remains the same. ","7ad516fd":"### APPLYING DIFFERENT ALGORITHMS","297aab74":"### FINAL DATAFRAME AFTER TEXT CLEANING","274d3da8":"### MERGING THE DATA FRAMES","f1dbc19d":"#### LEMMATIZATION","0c9dabfa":"### IMPORTING LIBRARIES"}}