{"cell_type":{"d8ac3b1f":"code","f9e5caf0":"code","bca11bbf":"code","bd6d8034":"code","b127001d":"code","2c3fe0b8":"code","5d814d76":"code","8b8bb215":"code","3624c465":"code","ec29fdea":"code","bcc242be":"code","b008fc74":"code","de9cbe51":"code","7c0a6e11":"code","0089243f":"code","085f3689":"code","7af336c0":"code","1c51bc23":"code","edee3b7a":"code","4e990a7e":"code","514a8373":"code","727ab92a":"code","c1b3eb56":"code","e57ed852":"code","ee2b8045":"code","d6437947":"code","2baa1294":"code","a03a689c":"code","3ebea5a3":"code","1c58960a":"code","98047438":"code","38a0784d":"code","d6c01f8e":"code","46997490":"code","1dedc72a":"code","0247044e":"code","024707d8":"code","be7de693":"code","207686f2":"code","adf66811":"code","e5202ea8":"code","236c5e2e":"code","38c3c6a9":"code","7f3aa23e":"code","191e2c71":"code","c34a0742":"code","bf79d926":"code","6f62e6e8":"code","ff35ca81":"code","cd4f9019":"code","59a32fe2":"code","822f1a0c":"code","6792ffd6":"code","167acf3e":"code","92dcbfa5":"code","3f70474d":"code","ac2aced0":"code","e4d798f7":"code","26701448":"code","42a3cb7c":"code","519db623":"code","f22a73db":"code","376fcb6f":"code","b4e3dd85":"code","e3b3797c":"code","cbf2f2ba":"code","36870420":"code","9c07eff7":"code","8e446d8e":"code","8ba062d1":"code","d0fd7197":"code","e025f75d":"code","9d0cfc98":"code","cca32e93":"code","e151d8f7":"code","3b3c1954":"code","862e4751":"code","205b6ddc":"code","78ad8a24":"code","708e782e":"code","2d9090f9":"code","5f0a853b":"code","8e4b7d34":"code","9c6b6945":"code","b0008da9":"code","d1206dfd":"code","a29f6c22":"code","70ae1cca":"code","27ae9416":"markdown","c71342b6":"markdown","b485ba83":"markdown","d836eaca":"markdown","2371cf9d":"markdown","11a3a98c":"markdown","51c37e84":"markdown","5ce6e157":"markdown","098965e9":"markdown","a2b6fa35":"markdown","ec6845ec":"markdown","84ab54a6":"markdown","f0baac53":"markdown","c2c0e48f":"markdown","9e0f26a3":"markdown","29f3bc59":"markdown","611a8c46":"markdown","af31a19c":"markdown","9d9f0c86":"markdown","47bf7d1c":"markdown","9fa590f5":"markdown","fd7e8fc0":"markdown","b85f04c1":"markdown","a8f7d1e7":"markdown","43d9b30e":"markdown","136525e1":"markdown","e19c3376":"markdown","509ca8f6":"markdown","bfd520e0":"markdown","fb983cb5":"markdown","fc17a3d6":"markdown","ff678eca":"markdown","148b3221":"markdown","4545381b":"markdown","edfec6c7":"markdown","17ad82ae":"markdown","31ba1b3c":"markdown","c4ae76b0":"markdown","83e060af":"markdown","c9049c2a":"markdown","e470bb6a":"markdown","25fced50":"markdown","90252124":"markdown","6162abe2":"markdown","6220b65e":"markdown","3fe4cd01":"markdown","f721abb2":"markdown","c2e1862f":"markdown","7e2502e1":"markdown","6dd0b7f6":"markdown","e2735bad":"markdown","8e52514c":"markdown","4de59d91":"markdown","9fd9cb1a":"markdown","db239184":"markdown","0db4cffe":"markdown","179e4ee5":"markdown","c851459a":"markdown","4da6eb3b":"markdown","9f5c6030":"markdown","905e8b36":"markdown","458f6f1a":"markdown","cbb9d9c6":"markdown","9af4e84e":"markdown","9cb31f66":"markdown","6aba3db2":"markdown","c19b5ffc":"markdown"},"source":{"d8ac3b1f":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")\n\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/t\/5090b249e4b047ba54dfd258\/1351660113175\/TItanic-Survival-Infographic.jpg?format=1500w\")","f9e5caf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Data visualization\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nsns.set()\n\n\n# Pre-Modeling Tasks\n\n\nfrom sklearn import preprocessing\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler    # to scaling our data \n\nfrom sklearn.model_selection import train_test_split  #to create validation data set\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.model_selection import learning_curve\n\n\n\n\n # Measure of Performance\n    \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bca11bbf":"# lets load the data set\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n","bd6d8034":"train.shape","b127001d":"test.shape","2c3fe0b8":"train.head()","5d814d76":"train.shape","8b8bb215":"test.head()","3624c465":"test.shape","ec29fdea":"train.describe()","bcc242be":"# Correlation Map\n\n# Display the negative and postive correlation between variables\n\ntrain.corr\nf,ax = plt.subplots(figsize=(15,10))\nsns.heatmap(train.corr(), annot =True, linewidth =\".5\", fmt =\".2f\", cmap='GnBu')\nplt.show()\n\n\n\n#figsize - image size\n#data.corr() - Display positive and negative correlation between columns\n#annot=True -shows correlation rates\n#linewidths - determines the thickness of the lines in between\n#cmap - determines the color tones we will use\n#fmt - determines precision(Number of digits after 0)\n#if the correlation between the two columns is close to 1 or 1, the correlation between the two columns has a positive ratio.\n#if the correlation between the two columns is close to -1 or -1, the correlation between the two columns has a negative ratio.\n#If it is close to 0 or 0 there is no relationship between them.","b008fc74":"train[\"Survived\"].value_counts()","de9cbe51":"# BarPlot\n\n# Set the width and the height of the figure\nplt.figure(figsize=(15,8))\n\n# Add the title\nplt.title(\"Survived people based on gender\")\n\n# Draw a barplot of survival people by sex\nsns.barplot(x=\"Sex\",y=\"Survived\", data =train,palette=('RdPu_r'))\n\n# Print percentage of males vs females that are survived \nprint(\"Percentage of females who survived :\",  train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint (\"Percentage of males who survived :\",  train[\"Survived\"][train[\"Sex\"]==  'male'].value_counts(normalize= True)[1]*100)","7c0a6e11":"# swarmplot\n\nplt.figure(figsize=(15,9))\n\nsns.swarmplot(x=train['Age'], y=train['Sex'], hue='Survived', data =train, palette= 'ocean_r')","0089243f":"# Barplot\n\n# Set the width and the height of the figure\nplt.figure(figsize=(15,9))\nsns.barplot(x=\"Parch\", y=\"Survived\", data = train, palette='magma_r')\nplt.show","085f3689":"# Barplot\n\nplt.figure(figsize=(15,9))\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train, palette='spring')\nplt.show","7af336c0":"# Violinplot\n\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x =\"Embarked\", y =\"Fare\", hue =\"Survived\", data=train, split =True , palette = {0: \"#3498db\", 1:\"#2ecc71\"});","1c51bc23":"# Subplot\n\nfig, ax = plt.subplots(figsize = (20,10))\nax = sns.countplot(x = 'Survived', hue = 'Pclass', data = train, palette = 'Set1')\nax.set_xlabel('Survived')\nax.set_title('Survival Rate for Passenger Classes', fontsize = 14, fontweight='bold');","edee3b7a":"# let's take more detaild look of what data is actually missing \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent_1 = train.isnull().sum()\/train.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","4e990a7e":"train.isnull().any()","514a8373":"test.isnull().any()","727ab92a":"# Processing Age\n\n\ntrain[\"Age\"] = train[\"Age\"]. fillna(train[\"Age\"].mean())\n\ntest[\"Age\"]  = test[\"Age\"] . fillna(test[\"Age\"].mean())","c1b3eb56":"# Processing Fare\n\ntest[\"Fare\"] = test [\"Fare\"]. fillna(test[\"Fare\"].mean())","e57ed852":"# Processing Sex","ee2b8045":"train.loc[train[\"Sex\"] == \"male\" , \"Sex\"] = 0\ntrain.loc[train[\"Sex\"] == \"female\",\"Sex\"] = 1\n\n\n\ntest.loc[test[\"Sex\"] == \"male\", \"Sex\"] = 0\ntest.loc[test[\"Sex\"] == \"female\", \"Sex\"] = 1\n","d6437947":"# Processing Embarked","2baa1294":"train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntrain.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntrain.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\n\ntest.loc[test[\"Embarked\"]  == \"S\", \"Embarked\"] = 0\ntest.loc[test[\"Embarked\"]  == \"C\", \"Embarked\"] = 1\ntest.loc[test[\"Embarked\"]  == \"Q\", \"Embarked\"] = 2","a03a689c":"train[\"Embarked\"].fillna(\"S\", inplace = True)","3ebea5a3":"train[\"FamSize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamSize\"]  =  test[\"SibSp\"] + test[\"Parch\"]  + 1","1c58960a":"train[\"IsAlone\"] = train.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntest[\"IsAlone\"]  = test.FamSize.apply( lambda x: 1 if x == 1 else 0)","98047438":"# Extraction the passengers titles","38a0784d":"for name in train[\"Name\"]:\n    train[\"Title\"] = train[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in test[\"Name\"]:\n    test[\"Title\"] = test[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\", \"Rev\": \"Other\", \"Dr\": \"Other\"}\n\ntrain.replace({\"Title\": title_replacements}, inplace=True)\ntest.replace({\"Title\": title_replacements}, inplace=True)\n\ntrain.loc[train[\"Title\"] == \"Miss\", \"Title\"] = 0\ntrain.loc[train[\"Title\"] == \"Mr\", \"Title\"] = 1\ntrain.loc[train[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntrain.loc[train[\"Title\"] == \"Master\", \"Title\"] = 3\ntrain.loc[train[\"Title\"] == \"Other\", \"Title\"] = 4\n\ntest.loc[test[\"Title\"] == \"Miss\", \"Title\"] = 0\ntest.loc[test[\"Title\"] == \"Mr\", \"Title\"] = 1\ntest.loc[test[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntest.loc[test[\"Title\"] == \"Master\", \"Title\"] = 3\ntest.loc[test[\"Title\"] == \"Other\", \"Title\"] = 4","d6c01f8e":"print(set(train[\"Title\"]))","46997490":"train.head()","1dedc72a":"train['Age'] = train['Age'].astype(int)\ntrain.loc[ train['Age'] <= 11, 'Age'] = 0\ntrain.loc[(train['Age'] > 11) & (train['Age'] <= 18), 'Age'] = 1\ntrain.loc[(train['Age'] > 18) & (train['Age'] <= 22), 'Age'] = 2\ntrain.loc[(train['Age'] > 22) & (train['Age'] <= 27), 'Age'] = 3\ntrain.loc[(train['Age'] > 27) & (train['Age'] <= 33), 'Age'] = 4\ntrain.loc[(train['Age'] > 33) & (train['Age'] <= 40), 'Age'] = 5\ntrain.loc[(train['Age'] > 40) & (train['Age'] <= 66), 'Age'] = 6\ntrain.loc[ train['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed train_df['Age'].value_counts()","0247044e":"test['Age'] = test['Age'].astype(int)\ntest.loc[test['Age']<= 11, 'Age'] =0\ntest.loc[(test['Age'] > 11)& (test['Age'] <= 18), 'Age'] = 1\ntest.loc[(test['Age'] > 18) & (test['Age'] <= 22), 'Age'] = 2\ntest.loc[(test['Age'] > 22) & (test['Age'] <= 27), 'Age'] = 3\ntest.loc[(test['Age'] > 27) & (test['Age'] <= 33), 'Age'] = 4\ntest.loc[(test['Age'] > 33) & (test['Age'] <= 40), 'Age'] = 5\ntest.loc[(test['Age'] > 40) & (test['Age'] <= 66), 'Age'] = 6\ntest.loc[ test['Age'] > 66, 'Age'] = 6","024707d8":"features_drop = ['Ticket', 'SibSp', 'Parch', \"Name\", \"Cabin\", \"Fare\"]\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","be7de693":"train = pd.get_dummies(train, columns=['Pclass','Sex','Embarked','Title'], \n                       drop_first=False)\ntest = pd.get_dummies(test, columns=['Pclass','Sex','Embarked','Title'],\n                      drop_first=False)","207686f2":"X = train.drop('Survived', axis=1)\ny = train['Survived']\n\nX.shape,y.shape","adf66811":"\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","e5202ea8":"len(X_train)","236c5e2e":"len(X_test)","38c3c6a9":"std_scale = StandardScaler()\n\n## transforming \"train_x\"\nX_train = std_scale.fit_transform(X_train)\n## transforming \"test_x\"\nX_test = std_scale.transform(X_test)\n","7f3aa23e":"from sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n#Ensemble Modeling \n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport xgboost as xgb\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n\nfrom sklearn.ensemble import VotingClassifier\n","191e2c71":"logreg= LogisticRegression()\n\nlogreg.fit(X_train,y_train)\n\npred_logreg = logreg.predict(X_test)","c34a0742":"pd.DataFrame(confusion_matrix(y_test, pred_logreg),\\\n             columns= [\"Predicted Not Survived\", \"Predicted Survived\"],\\\n             index = [\"Not-Survived\", \"Survived\"])\n","bf79d926":"acc_score_logreg= accuracy_score(y_test, pred_logreg)\nprint(acc_score_logreg)","6f62e6e8":"precision_score_logreg = precision_score(y_test,pred_logreg)\nprint(precision_score_logreg)","ff35ca81":"recall_score_logreg = recall_score(y_test,pred_logreg)\nprint(recall_score_logreg)","cd4f9019":"from sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()","59a32fe2":"gnb = GaussianNB()\n\ncv = cross_val_score(gnb, X_train,y_train, cv=5)\n\nprint(cv)\nprint(cv.mean())","822f1a0c":"log_reg = LogisticRegression(max_iter = 2000)\n\ncv = cross_val_score(log_reg, X_train,y_train, cv=5)\n\n\nprint(cv)\nprint(cv.mean())","6792ffd6":"dt = DecisionTreeClassifier(random_state= 1)\n\ncv= cross_val_score(dt, X_train,y_train,cv=5)\n\nprint(cv)\nprint(cv.mean())","167acf3e":"knn = KNeighborsClassifier(n_neighbors=3)\n\ncv= cross_val_score(knn, X_train,y_train, cv=5)\n\nprint(cv)\nprint(cv.mean())","92dcbfa5":"rf_clf = RandomForestClassifier(random_state= 1)\n\ncv = cross_val_score(rf_clf, X_train,y_train, cv=5)\n\nprint(cv)\nprint(cv.mean())","3f70474d":"svc = SVC(probability = True)\n\ncv = cross_val_score(svc, X_train,y_train, cv=5)\n\nprint(cv)\nprint(cv.mean())","ac2aced0":"xgb= XGBClassifier(random_state=1)\n\ncv = cross_val_score(xgb, X_train,y_train, cv= 5)\n\nprint(cv)\nprint(cv.mean())\n","e4d798f7":"voting_clf = VotingClassifier(estimators=[('gnb',gnb),('log_reg',log_reg),('dt',dt),('knn',knn),('rf_clf',rf_clf),('svc',svc),('xgb',xgb) ], voting='soft', n_jobs=4)\n","26701448":"cv = cross_val_score(voting_clf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","42a3cb7c":"\n# Logistic Regression\n\n# Choose a cross validation strategy. \n\ncv = StratifiedShuffleSplit(n_splits=10, test_size= .25)\n\n# define models and parameters\n\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\nlog_parameters = {'C': c_values}\n\n\n# define the grid search\n\n\nlog_grid = GridSearchCV(LogisticRegression(),scoring= 'accuracy', param_grid= log_parameters, n_jobs=-1, cv = 5, verbose = True,error_score=0)\nlog_grid_model = log_grid.fit(X,y)\n","519db623":"\nprint(log_grid.best_score_)\nprint(log_grid.best_params_)\nprint(log_grid.best_estimator_)","f22a73db":"acc_log = log_grid_model.score(X,y)\nacc_log","376fcb6f":"# K-Nearest Neighbors (KNN)\n\n# Define model and parameters\n\nn_neighbors = range(1,21,2)\nweights = ['uniform','distance']\nmetric = ['euclidean', 'minkowski']\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\n# define the gridsearchCV\n\nknn_parameters = {'n_neighbors': n_neighbors, 'weights':weights, 'metric': metric}\n\nknn_grid = GridSearchCV( KNeighborsClassifier(),param_grid= knn_parameters, cv=cv, n_jobs=-1, verbose = True, scoring='accuracy')\n\nknn_grid_model = knn_grid.fit(X,y)\n","b4e3dd85":"# summarize results\n\nprint(knn_grid.best_score_)\nprint(knn_grid.best_estimator_)\nprint(knn_grid.best_params_)","e3b3797c":"acc_knn = knn_grid_model.score(X,y)\nacc_knn","cbf2f2ba":"# Support Vector Machine Classifier(SVM)\n\n\n#define the models and parameters\n\n\nparam_grid = {'C': [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10], 'gamma': [0.0001,0.001, 0.01, 0.1, 1]}\n\n# Define a cross validation strategy\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\n# Define the GridsEARCHcv\n\nsvc_grid = GridSearchCV(SVC(kernel= 'rbf', probability=True ),param_grid,cv=cv,verbose=2)\nsvc_grid_model = svc_grid.fit(X,y)","36870420":"print(svc_grid.best_score_)\nprint(svc_grid.best_estimator_)\nprint(svc_grid.best_params_)","9c07eff7":"acc_svc = svc_grid_model.score(X_train,y_train)\nacc_svc","8e446d8e":"rf = RandomForestClassifier(random_state = 1)\n\nrf_param_grid =  {'n_estimators': [140,150,160,180],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [1,10],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n                                  \nrf_grid = GridSearchCV(RandomForestClassifier(warm_start=False), param_grid = rf_param_grid, cv = cv, verbose = True, n_jobs = -1)\n\nrf_grid_model = rf_grid.fit(X,y)\n\n","8ba062d1":"# summarize results\n\nprint(rf_grid.best_score_)\nprint(rf_grid.best_estimator_)\nprint(rf_grid.best_params_)","d0fd7197":"### Using the best parameters from the grid-search.\n\nacc_random_forest = rf_grid_model.score(X,y)\nacc_random_forest","e025f75d":"# Decision Tree Classifier\n\n# Define the model and parameters\n\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\n\n# define the GridSearchCV\n\ndt_grid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedShuffleSplit(n_splits=20, random_state=15),\n                                n_jobs = -1)\ndt_grid_model = dt_grid.fit(X,y) ","9d0cfc98":"# summarize results\n\nprint(dt_grid_model.best_score_)\nprint(dt_grid_model.best_estimator_)\nprint(dt_grid_model.best_params_)","cca32e93":"acc_dt_grid = dt_grid_model.score(X,y)\nacc_dt_grid","e151d8f7":"\n# Define the model and parameters\n\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\n\n\n# Define the GridSearchCV\n\n\nbagging_grid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\nbagging_grid_model = bagging_grid.fit(X,y) ","3b3c1954":"# summarize results\n\nprint (bagging_grid_model.best_score_)\nprint (bagging_grid_model.best_params_)\nprint (bagging_grid_model.best_estimator_)","862e4751":"\nacc_bagging_grid = bagging_grid_model.score(X,y)\nacc_bagging_grid\n","205b6ddc":"# AdaBoost\n\n# Define the parameters\n\n\nada_param_grid = {\n              \"n_estimators\" :[100,140,145,150,160, 170,175,180,185],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\n\n# run the gridsearchCV\n\nadaboost_grid = GridSearchCV(AdaBoostClassifier(base_estimator=None), cv= 5, n_jobs= 4,param_grid = ada_param_grid)\n\n\nadaboost_grid_model = adaboost_grid.fit(X,y)\n\n","78ad8a24":"# summarize results\n\n\nprint (adaboost_grid_model.best_score_)\nprint (adaboost_grid_model.best_params_)\nprint (adaboost_grid_model.best_estimator_)","708e782e":"\nacc_adaboost_grid = adaboost_grid_model.score(X,y)\nacc_adaboost_grid","2d9090f9":"\ngradient_boost = GradientBoostingClassifier()\n\n\ngradient_boost.fit(X,y)\n\n\ny_pred = gradient_boost.predict(X_test)\n\n\nacc_gradient_boosting = round(accuracy_score(y_pred, y_test), 3)\nacc_gradient_boosting","5f0a853b":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)","8e4b7d34":"# Define the model \nfrom sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\n\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\n\nprint(gau_pro_accy)","9c6b6945":"\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lg', log_grid_model),\n    ('svc', svc_grid_model),\n    ('random_forest', rf_grid_model),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dt_grid_model),\n    ('knn_classifier', knn_grid_model),\n    ('bagging_classifier', bagging_grid_model),\n    ('adaBoost_classifier',adaboost_grid_model),\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)","b0008da9":"y_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)","d1206dfd":"models = pd.DataFrame({\n    'Model': ['LogisticRegression','Support Vector Machines','Naive Bayes','Random Forest','Decision Tree', 'Bagging Classifier', \n              'AdaBoost Classifier', 'Gradient Boosting', 'voting Classifier'],\n    'Score': [acc_log, acc_svc, acc_knn, acc_random_forest, acc_dt_grid, acc_bagging_grid, acc_adaboost_grid, acc_gradient_boosting,\n             voting_accy]})\n\nmodels.sort_values(by='Score', ascending=False)","a29f6c22":"Y_pred = bagging_grid_model.predict(test)","70ae1cca":"\nsubmission = pd.DataFrame({\n    \"PassengerId\": test.PassengerId,\n    \"Survived\": Y_pred\n})\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission1.csv\", index=False)","27ae9416":"- Naive Bayes :(0.74%)\n- Logistic Regression : (0.83%)\n- Decision Tree Classifier : (0.78%)\n- K Nearest Neighbor : (0.79%)\n- Random Forest Classifier : (0.80%)\n- SVC : (0.81%)\n- XGB : (0.80%)\n- Voting Classifier : (0.80%)","c71342b6":"## - Embarked and fare features","b485ba83":"# AUC & ROC Curve","d836eaca":"Distribution of survivals : 1 is for survival and 0 is for not","2371cf9d":"# Voting Classifier","11a3a98c":"##### Checking for missing values","51c37e84":"### 3.3 Feature Scaling","5ce6e157":"We can see that all variables in both datasets now have values in the desired range of 0 to 1.","098965e9":"## - PARCH feature vs Survived feature","a2b6fa35":"The next step we'll do some descriptive statistics, this one helps us to describe and understand the features of a specific data by giving short \n\nsummaries about the sample and measures of the data \n","ec6845ec":"# 1. EXPLORATORY DATA ANALYSIS","84ab54a6":"# 4- Modeling ","f0baac53":"A critical part of the success of a Machine Learning Project is Feature Engineering. \n\nFeature Engineering is a process of transforming the data into data which is easier to interept and also, to increase the predictive power of learning algorithm\n\nIn Feature Engineering we will create a new features that could improve predictions such as if the passenger is alone or not,\nand combining existing features to produce a more useful one, and dropping the columns doesn't improve predictions.\n\nSo Feature Engineering have two goals:\n\n- Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n- Improving the performance of a machine learning model \n\nIn this section, we'll be doing four things.\n\nCleaning : we'll fill in missing values.\n\nRemoving : we'll removing the less relevant variables that doesn't improve predictions, as \"Ticket\" and \"Cabin\", because they contains many null values both in training and test datatest. Therefore, \"PassengerId\" may be also dropped from training dataset as it does not contribute to survival\n\nData Binning : we'll transform  categorical features into numerical as \"Sex\", \"Embarked\"\n\nCreating New Varibles: we'll create new variable called \"FamilySize\" based on \"Parch\" feature and \"SibSp\" feature.\n                       we'll create \"IsAlone\" feature to check if a person traveling alone is more likely to survived or died\n                       We may want to engineer the Name feature to extract Title as a new feature, to determine if it played a role in survival.","c2c0e48f":"# Confusion Matrix","9e0f26a3":"# 7.Predictions ","29f3bc59":"## - Passenger Class","611a8c46":"# 2. Feature Engineering","af31a19c":"Then, introducing new features as Family size (to join these Parch and SibSp)","9d9f0c86":"## 1.2 Viewing Data Set\n\nIn this step we start by loading our data and the data set has split into two groups:\n\n-Training set: (train_csv) ; includes our variable target(dependent variable),passenger survival along with other features (Sex, Age, Parch,Sibsp, fare, cabin, Ticket)\n\n-Test et( test_csv);  should be used to see how well our model performs on unseen data. When we say unseen data, we mean that the algorithm or machine learning models have no relation to the test data. We do not want to use any part of the test data in any way to modify our algorithms; Which are the reasons why we clean our test data and train data separately","47bf7d1c":"## Bagging Classifier","9fa590f5":"### 3.1 Separating the independant and the dependant variable","fd7e8fc0":"A critical part of the success of a Machine Learning Project is Feature Engineering","b85f04c1":"# Titanic : Machine Learning From Disaster","a8f7d1e7":"##### Checking for the correlation","43d9b30e":"## 2.2 Bining Categorical variables","136525e1":"### - Evaluating the performance of the model","e19c3376":"# Useful resources","509ca8f6":"In this part we'll try differents models of Machine learning: Logistic Regression, Random Forest, Support Vector Machine, Lsvm, Decision tree, GaussianNB Medel and KNeighbors Model\n","bfd520e0":"# Gradient Boosting Classifier","fb983cb5":"## 2.3 Creating New Features","fc17a3d6":"### - Survived feature","ff678eca":"Feature scaling is a method used to standardize the range of independent variables or features of data. Scaling the data is very important to boost the score.","148b3221":"### 3.2 Splitting the training data \n","4545381b":"# ExtraTrees Classifier","edfec6c7":"If we have a quick look in the names of the passengers we will notice that each name has a title in it, so it can be a useful information for our analyze. Therefore we can extract this title from the name of each passenger and then encode it like we did for Sex and Embarked.","17ad82ae":"# Ensemble Modeling","31ba1b3c":"Let's now see how the embarkation site affects the survival.","c4ae76b0":"# Boosting Classifier","83e060af":"## - SibSp Feature ","c9049c2a":"\nBefore we fit the data into a machine learning algorithm, there is a step very crucial is that we make sure to encode categorical variables correctly\n\nWe will change Sex to binary, as either 1 for female or 0 for male. We do the same for Embarked. We do this same process on both the training and testing set to prepare our data for Machine Learning.","e470bb6a":"## - Sex feature vs Survived feature","25fced50":"## Summary of the accuracy of all models","90252124":"# Ada Boost Classifier ","6162abe2":"# Accuracy Score","6220b65e":"## 2.1 Filling missing Values","3fe4cd01":"# I describe below the process that i followed","f721abb2":"# Recall Score","c2e1862f":"#### logistic Regression","7e2502e1":"# Gaussian Process Classifier","6dd0b7f6":"\n\n 1. EXPLORATORY DATA ANALYSIS\n\n   - 1.1 Data Extraction\n   \n   - 1.2 Overview\n  \n   - 1.3 Descriptive statistics\n  \n   - 1.4 Data visualisation\n   \n\n 2. Feature Engineering\n\n   - 2.1 Filling missing Values\n  \n   - 2.2 Binning the categorical features\n  \n   - 2.3 Creating New Features\n   \n   - 2.4 Removing irrelevant variables\n   \n   - 2.4 Creating dummy variables\n   \n  \n\n 3. Pre-Modeling Tasks\n\n\n   -  3.1 Defining Features in Training\/Test Set\n   \n   -  3.2 Splitting the dataset\n   \n   -  3.3 Scaling the dataset\n   \n\n 4. Modeling\n  \n   - Logistic Regression\n   \n   - Evaluate the model performance\n     \n     - Confusion Matrix\n     - Accuracy Score\n     - Precision Score\n     - Recall Score\n     - AUC & ROC Curve\n     \n   - GaussianNB Model\n   - Logistic Regression Model\n   - Support Vector Machine \n   - KNeighbors Model\n   - Decision Tree Model\n   - Random Forest Model\n   - XGB Classifier\n   - Voting Classifier\n   \n    \n 5. Model Tuning using Grid Search\n \n\n 6. Ensemble Modeling\n\n   - Bagging Classifier\n   \n   - Boosting Classifier\n    - Ada Boost Classifier\n    - Gradient Boosting classifier\n    \n   - XGB Classifier\n   \n   - Extra Trees Classifier\n   \n   - Gaussian Process NB\n   \n   - Voting Classifier\n   \n\n7. Predictions\n\n\n- Useful resources\n  ","e2735bad":"## Predict and Submit results","8e52514c":"\n# 1.1 Data Extraction\n\nThis step it's about installing packages, and the best way is to install the conda distribution that contains them all.\n\nThen, we import the relevant libraries to help us manipulate the data.\n\nNumpy for linear algebra and matrices.\n\nPandas for data analysis,manipulation tool and data processing.\n\nMatplotlib and seaborn for the visualization of the data set.\n\nSklearn is a collection of machine learning algorithms and predictive modeling","4de59d91":"## 1.4 Data Visualization","9fd9cb1a":"People with SibSp or spouses were less likely to survive, therefore people with no children were more less likely to survived than those with one children or two.","db239184":"To look at the correlation between passenger class and survival statistics I would plot a countplot.","0db4cffe":"# precision Score","179e4ee5":"\n-PassengerId is the unique id of the row and it doesn't have any effect on target\n\n-Survived is the target variable we are trying to predict (0 or 1):\n1 = Survived\n0 = Not Survived\n\n-Pclass (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (1, 2 or 3):\n1 = Upper Class\n2 = Middle Class\n3 = Lower Class\n\n-Name, Sex and Age are self-explanatory.\n\n-SibSp is the total number of the passengers' siblings and spouse.\n\n-Parch is the total number of the passengers' parents and children.\n\n-Ticket is the ticket number of the passenger.\n\n-Fare is the passenger fare.\n\n-Cabin is the cabin number of the passenger.\n\n-Embarked is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\nC = Cherbourg\nQ = Queenstown\nS = Southampton","c851459a":"## 2.4 Removing irrelevant variables","4da6eb3b":"# Tuning Hypereparameter ","9f5c6030":"## 1.3 Descriptive Statistics","905e8b36":"According to this graph, we can notice that womens are more likely to survive","458f6f1a":"The next option is to cerate IsAlone feature to check wheter a person traveling alolne is more likely to survived or died","cbb9d9c6":"# 3 Pre-Modeling Tasks","9af4e84e":"the next step is dropping the less relevant features beacuse, The problem with less important features is that they create more noice and actually take over the importance of real features like Sex and Pclass.","9cb31f66":"People with less than four parents or childrens aboard more likely to survive","6aba3db2":"- [Ensemble Machine Learning Algorithms in Python with scikit-learn](https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/)\n\n- [ZacharyJWyman\/ML-Techniques](https:\/\/github.com\/ZacharyJWyman\/ML-Techniques)\n\n- [How to Build a Machine Learning Model](https:\/\/towardsdatascience.com\/how-to-build-a-machine-learning-model-439ab8fb3fb1)\n\n- [Ensemble methods: bagging, boosting and stacking](https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n\n- [How to find optimal parameters using GridSearchCV?](https:\/\/www.dezyre.com\/recipes\/find-optimal-parameters-using-gridsearchcv)\n\n- [Gradient Boosting Classifiers in Python with Scikit-Learn](https:\/\/stackabuse.com\/gradient-boosting-classifiers-in-python-with-scikit-learn\/)\n\n","c19b5ffc":"## 2.5 Creating dummy variables"}}