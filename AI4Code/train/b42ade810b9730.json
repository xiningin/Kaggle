{"cell_type":{"842c76a0":"code","0e156d19":"code","2048ccc5":"code","ead02163":"code","11e6afc7":"code","ca7d1d0b":"code","4213f52a":"code","7980bfc8":"code","adb5f4c6":"code","41f3c94a":"code","e4b58cf9":"code","22b14871":"code","2d557a45":"code","197c3f49":"code","10591ffd":"code","d2aee2df":"code","aa50d642":"code","33e91a46":"code","e972abea":"code","0c8bdb05":"code","9014f725":"markdown","634e84f1":"markdown","d6f88fe4":"markdown","58a3c50c":"markdown"},"source":{"842c76a0":"# Importing all necessary libraries.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","0e156d19":"df = pd.read_table('..\/input\/human-dna\/human_dna.txt')","2048ccc5":"df.head()","ead02163":"df.info()","11e6afc7":"df.describe()","ca7d1d0b":"df.shape","4213f52a":"# Creating a function to convert sequence strings into k-mer words, default size = 6 (hexamer words).\ndef getKmers(sequence, size=6):\n    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]","7980bfc8":"df['words'] = df.apply(lambda x: getKmers(x['sequence']), axis=1)\ndf = df.drop('sequence', axis=1)","adb5f4c6":"df.head()","41f3c94a":"human_texts = list(df['words'])\nfor item in range(len(human_texts)):\n    human_texts[item] = ' '.join(human_texts[item])\ny_data = df.iloc[:, 0].values","e4b58cf9":"print(human_texts[2])","22b14871":"y_data","2d557a45":"# Creating the Bag of Words model using CountVectorizer().\n# This is equivalent to k-mer counting.\n# The n-gram size of 4 was previously determined by testing.\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range=(4,4))\nX = cv.fit_transform(human_texts)","197c3f49":"print(X.shape)","10591ffd":"df['class'].value_counts().sort_index().plot.bar()","d2aee2df":"# Splitting the human dataset into the training testing sets.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_data, test_size = 0.20, random_state=42)","aa50d642":"print(X_train.shape)\nprint(X_test.shape)","33e91a46":"# Implementing a Multinomial Naive Bayes Classifier. \n# The alpha parameter was determined by grid search previously.\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB(alpha=0.1)\nclassifier.fit(X_train, y_train)","e972abea":"y_pred = classifier.predict(X_test)","0c8bdb05":"# Getting the accuracy of the model.\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nprint(\"Confusion matrix\\n\")\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\ndef get_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    return accuracy, precision, recall, f1\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))","9014f725":"In this notebook I take a look at classifying DNA using Natural Language Processing.","634e84f1":"# DNA Classification using NLP","d6f88fe4":"# Data","58a3c50c":"# Importing Libraries"}}