{"cell_type":{"5b8dca22":"code","aeb633b4":"code","59b5bdda":"code","fbaa384e":"code","a1d502af":"code","5285bc2f":"code","ad33306c":"code","e568eef1":"code","1b63f117":"code","59cb7bf4":"code","6de6b30d":"code","44a4fe48":"code","f8d8ec12":"code","9a20640c":"code","5f62c3ae":"code","40da9a5e":"code","ef664101":"code","29979b64":"code","008a78f8":"code","69fc7c8b":"code","0adf1476":"markdown","28e569a5":"markdown","3472bf81":"markdown","e17ce617":"markdown","094b90ae":"markdown","d00ffd45":"markdown","f8c9fe12":"markdown","60f14bd6":"markdown","dcbc6ec8":"markdown"},"source":{"5b8dca22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","aeb633b4":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubs = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","59b5bdda":"train.info()","fbaa384e":"train.head()","a1d502af":"train.describe()","5285bc2f":"train=train.drop(columns=\"Id\")\ntest=test.drop(columns=\"Id\")","ad33306c":"nan_count=100*train.isna().sum().sort_values(ascending=False)\/train.shape[0]\nfig=px.bar(x=nan_count.index,y=nan_count.values, labels={\"y\": \"Nan ammount (%)\",\"x\": \"Feature\"})\nfig.show()","e568eef1":"train=train.drop(columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence',\"FireplaceQu\"])\ntest=test.drop(columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence',\"FireplaceQu\"])","1b63f117":"numeric_features=[ feature  for feature in train.columns if  train[feature].dtypes!=\"object\" and feature!=\"SalePrice\"]\ncategorical_features=[ feature  for feature in train.columns if  train[feature].dtypes==\"object\"]","59cb7bf4":"#replacing train NaNs with modes\nnans=train.isna().sum()\nnans=nans[nans>0]\nfor feature in nans.index:\n    train[feature] = train[feature].fillna(train[feature].mode()[0])\n#replacing test NaNs with modes\nnans=test.isna().sum()\nnans=nans[nans>0]\nfor feature in nans.index:\n    test[feature] = test[feature].fillna(test[feature].mode()[0])","6de6b30d":"for feature in categorical_features:    \n    #some string values are present only in one of the dataset, so it is needed an unique list of both dataset to avoid conflicts\n    for num, value in enumerate(np.unique((list(train[feature].unique())+list(test[feature].unique())))):          \n        train[feature+\"_\"+str(num)]=pd.Series(train[feature]==value,dtype=\"int\")        \n        test[feature+\"_\"+str(num)]=pd.Series(test[feature]==value,dtype=\"int\")\n    train=train.drop(columns=feature)\n    test=test.drop(columns=feature)\n    ","44a4fe48":"train","f8d8ec12":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ntrain[numeric_features]=scaler.fit_transform(train[numeric_features])\ntest[numeric_features]=scaler.transform(test[numeric_features])","9a20640c":"x_train=train.drop(columns=\"SalePrice\")\ny_train=train['SalePrice']\npca = PCA(n_components=train.shape[1]-1)\nx_train=pca.fit_transform(x_train)\nfig=go.Figure()\nfig.add_traces(go.Bar(x=np.arange(train.shape[1]-1),y=np.cumsum(pca.explained_variance_ratio_),name=\"Cumulative Variance\"))\n#n_comp will be the number of components that explains the 95% of the data variance\nn_comp=np.where(np.cumsum(pca.explained_variance_ratio_)>0.95)[0][0]\nfig.add_traces(go.Scatter(x=np.arange(train.shape[1]-1),y=[0.95]*(train.shape[1]-1),name=\"Variance at 95%\"))\nfig.update_layout(title=\"How many components we need?\",xaxis_title=\"Components\",yaxis_title=\"Cumulative Variance\", font=dict(\n        family=\"Arial\",\n        size=18,\n    ))\nfig.show()\nprint(\"With n_components=\"+str(n_comp)+\" we have the 95% of the data variance, so we will choose this value.\")","5f62c3ae":"pca = PCA(n_components=n_comp+50)\nx_train=pca.fit_transform(train.drop(columns=[\"SalePrice\"]))","40da9a5e":"model = tf.keras.Sequential([\n      layers.Dense(2048, activation='relu'),\n      layers.Dropout(0.5),\n      layers.Dense(2048, activation='relu'),\n      layers.Dropout(0.5),\n      layers.Dense(1)\n  ])\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adamax(1e-3))","ef664101":"history = model.fit(x_train,y_train,validation_split=0.1,verbose=0, epochs=300)","29979b64":"fig=go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(300), y=history.history['loss'],mode='lines', name='Train Loss'))\nfig.add_trace(go.Scatter(x=np.arange(300), y=history.history['val_loss'],mode='lines', name='Validation Loss',))\nfig.update_layout(title=\"MAE loss on train and validation set\",xaxis_title=\"Epoch\", yaxis_title=\"Loss\", font=dict(\n        family=\"Arial\",\n        size=18,\n    ))\nfig.show()","008a78f8":"print(\"Validation loss:\",history.history['val_loss'][-1])\nprint(\"Training loss:\",history.history['loss'][-1])\nprint(\"Loss on entire train set:\",mean_absolute_error(model.predict(x_train),y_train))\nprint(\"R2 score(Train):\",r2_score(model.predict(x_train),y_train))","69fc7c8b":"sub_preds = model.predict(pca.transform(test))\nsubs[\"SalePrice\"] = sub_preds\nsubs.to_csv(\"submission.csv\", index = False)\nprint(\"Submission done!\")","0adf1476":"# Model definition and training\n","28e569a5":"# Model evaluation and submission\n","3472bf81":"One hot encoding the categorical feature of train and test set.","e17ce617":"Now we are going to remove the last NaN values with the median value of each feature.","094b90ae":"Standard transformation\nof the train and test test (only numeric features).","d00ffd45":"Let's see how many NaNs we have for each feature.","f8c9fe12":"# Dataset Preprocessing\n","60f14bd6":"We can remove the features with NaN>40%, while the others will be handled replacing NaN with the respective median value.","dcbc6ec8":"Before feeding the data to the Neural Network, we peforme an PCA dimensionality reduction to reduce the noise of the data and to ease the calculation of the neural net."}}