{"cell_type":{"6636523f":"code","0fca89b5":"code","8c4588cb":"code","5f716995":"code","a565ac0c":"code","82f891d7":"code","fe68eb74":"code","2276849d":"code","350d761c":"code","827ae7e8":"code","da4dcf56":"code","14040a32":"code","d6663295":"code","4ffbae05":"code","8819068a":"code","9ced99d5":"code","fc2b8b56":"code","31941229":"code","8fff1c2a":"code","8560834b":"code","a3360997":"code","e5770a0c":"code","f3421b5c":"code","688d14ad":"markdown","702a0087":"markdown","7345d709":"markdown","4b06014c":"markdown","78d80a99":"markdown","010f7cd6":"markdown","08746afc":"markdown","8da0b71e":"markdown","bd819992":"markdown","6c3a8f86":"markdown","0b0d0536":"markdown","da03cad5":"markdown","5418f892":"markdown","540101ce":"markdown","f870362c":"markdown","68eb4126":"markdown","330d78d1":"markdown","6c227974":"markdown","bf154908":"markdown"},"source":{"6636523f":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nfrom matplotlib import style\nstyle.use('ggplot')\n\nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0fca89b5":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","8c4588cb":"df.head()","5f716995":"df.info()","a565ac0c":"# Apply a first round of text cleaning techniques\nimport re\nimport string\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)","82f891d7":"# Let's take a look at the updated text\ndf['clean_text'] = df.text.apply(round1)\ndf.head(2)","fe68eb74":"# Apply a second round of cleaning\ndef clean_text_round2(text):\n    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\nround2 = lambda x: clean_text_round2(x)","2276849d":"# Let's take a look at the updated text\ndf['clean_text'] = df.text.apply(round2)\ndf.head(2)","350d761c":"# remove special characters, numbers, punctuations\ndf['clean_text'] = df['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")","827ae7e8":"df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","da4dcf56":"tokenized_tweet = df['clean_text'].apply(lambda x: x.split())\ntokenized_tweet.head()","14040a32":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()","d6663295":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n\ndf['clean_text'] = tokenized_tweet","4ffbae05":"all_words = ' '.join([text for text in df['clean_text']])\n#print(len(all_words))","8819068a":"\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","9ced99d5":"\nnot_disaster =' '.join([text for text in df['clean_text'][df['target'] == 0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(not_disaster)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","fc2b8b56":"disaster =' '.join([text for text in df['clean_text'][df['target'] == 1]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(disaster)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","31941229":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","8fff1c2a":"# extracting hashtags from non Disaster tweets\n\nHT_Not_Disaster = hashtag_extract(df['clean_text'][df['target'] == 0])\n\n# extracting hashtags from Disaster tweets\nHT_Disaster = hashtag_extract(df['clean_text'][df['target'] == 1])\n\n# unnesting list\nHT_Not_Disaster = sum(HT_Not_Disaster,[])\nHT_Disaster = sum(HT_Disaster,[])","8560834b":"# Fake Disaster tweets\n\na = nltk.FreqDist(HT_Not_Disaster)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","a3360997":"# True Disaster tweets\n\nb = nltk.FreqDist(HT_Disaster)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n# selecting top 10 most frequent hashtags\ne = e.nlargest(columns=\"Count\", n = 10)   \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","e5770a0c":"from sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# bag-of-words feature matrix\nbow = bow_vectorizer.fit_transform(df['clean_text'])","f3421b5c":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(df['clean_text'])","688d14ad":"## Visualization from Tweets","702a0087":"## Introduction\n\nIn this notebook we will go through many Tweets about disaster and see which tweets is real and which is fake tweets.\n\nTable of Contents\n\n1. Tweets Preprocessing and Cleaning\n1. Visualization from Tweets\n1. Extracting Features from Cleaned Tweets\n1. Model Building\n","7345d709":"Now let\u2019s stitch these tokens back together.\n","4b06014c":"**Word Cloud of Not Disaster (Fake) Tweets **","78d80a99":"TF-IDF Features\n","010f7cd6":"## Extracting Features from Cleaned Tweets\n\nBag-of-Words Features\n","08746afc":"#### Selecting top 10 most frequent hashtags","8da0b71e":"**Let's see impact of Hashtags on tweets sentiment**","bd819992":"**Removing Punctuations, Numbers, and Special Characters**","6c3a8f86":"Let create Word cloud to see what type of words are used in such tweets.","0b0d0536":"## Tweets Preprocessing and Cleaning","da03cad5":"**Word Cloud of All Tweets **","5418f892":"> Now it looks something meaningful","540101ce":"**Tokenization**\n\n---\nNow we will tokenize all the cleaned tweets in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","f870362c":"**Word Cloud of Real Disaster Tweets **","68eb4126":"**Stemming**\n\n---\n\nStemming is a rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word. For example, For example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d.","330d78d1":"It include all word with Real Tweets and Fake Tweets. It doesn\u2019t give us any idea about the words associated with the disaster tweets. Hence, we will plot separate wordclouds for both the classes(real disaster or not) in our train data.","6c227974":"**Rrmoving short word**","bf154908":"> #### By Looking Bar Plot of Hashtag used in Tweets for both Real and Fake Disaster tweets, we get a overview that Hashtag also giving a sence about Fake and Real Tweet. \n\n"}}