{"cell_type":{"c8b8b0d9":"code","3d3eb1b6":"code","7ba0c5aa":"code","465e96d4":"code","ccaf7889":"code","0566c60d":"code","af4ee148":"code","af0d3998":"code","678334ce":"code","603b5836":"code","1f873d39":"code","62db77e2":"code","6a449459":"code","59776ab6":"code","7f9f1879":"code","882f4d15":"code","6a509473":"code","0bcf37ae":"code","d5cbcecb":"code","519379db":"code","9720eb27":"code","a7524978":"code","43ee2649":"code","aa53967c":"code","16625b52":"code","f6a9102a":"code","4c5335a1":"code","ab82370a":"code","0bc27ae8":"code","78e08014":"code","1869ed96":"code","73d34815":"code","28717e09":"code","56ea6a17":"code","5b0e9a63":"code","eaad171b":"code","7fedba0e":"code","d29fa34d":"code","91d20821":"code","64c30c5d":"code","8931151b":"code","7fb328a9":"code","ae70266f":"code","f14b82de":"markdown","df2ecb61":"markdown","bb77fde3":"markdown","4223cdba":"markdown","41f1c026":"markdown","dff6c10d":"markdown","0675c4cc":"markdown","3fcb4566":"markdown","38842714":"markdown","8ce72107":"markdown","0e3f7d6a":"markdown","888dcfdb":"markdown"},"source":{"c8b8b0d9":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom collections import defaultdict, deque\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport os,sys,matplotlib,re\nfrom PIL import Image\nfrom skimage import exposure\nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom pathlib import Path","3d3eb1b6":"# Clone the dataset resize to 512 * 512, \n!git clone https:\/\/github.com\/CartagenaMinas\/s3","7ba0c5aa":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","465e96d4":"core_df=pd.read_csv('..\/input\/mining-core-sampling-dataset\/train.csv')\ncore_df.head(5)","ccaf7889":"core_df['image_name'].unique().shape,core_df['label_name'].unique()","0566c60d":"core_df['image_height'].hist()","af4ee148":"core_df['image_height'].hist()","af0d3998":"imgs=os.listdir(\"..\/input\/mining-core-sampling-dataset\/train\")\npath=Path(\"..\/input\/mining-core-sampling-dataset\/train\/\")","678334ce":"shapes = []\n\nfor img in tqdm(imgs):\n    img = torchvision.io.read_image(str(path\/img))\n    if img.shape not in shapes:\n        shapes.append(img.shape)","603b5836":"len(shapes)","1f873d39":"df = pd.read_csv('.\/s3\/train.csv')\ndf.head(5)","62db77e2":"path = \".\/s3\/train2\/\" ","6a449459":"# we choose an image to view it\nid = 'M3-BH130-1.jpg'\nVars = [\"x_min\", \"y_min\", \"w\", \"h\",\"source\"]\nbboxes = df[df[\"image_id\"] == id].loc[:, Vars]","59776ab6":"# Create a graph\n# Wood = 1 \/\/ +10cm Rock = 2\nig,ax = plt.subplots(figsize=(20,15)) # Image size\nfor i in range(bboxes.shape[0]): # I go through all the list\n    img = immg.imread(path + id)\n    ax.imshow(img,cmap='binary')\n    row = bboxes.iloc[i].values\n    if (row[4]==2):\n        rectangle = matplotlib.patches.Rectangle((row[0], row[1]), row[2], row[3], linewidth = 4, edgecolor='b', facecolor='none')\n        ax.text(*row[:2], \"Core +10 cm\", verticalalignment='top', color='white', fontsize=15, weight='bold')\n        ax.add_patch(rectangle)\n    else:\n        rectangle = matplotlib.patches.Rectangle((row[0], row[1]), row[2], row[3], linewidth = 4, edgecolor='r', facecolor='none')\n        ax.text(*row[:2], \"Wood\", verticalalignment='top', color='white', fontsize=15, weight='bold')\n        ax.add_patch(rectangle)\nplt.show()","7f9f1879":"image_ids = df['image_id'].unique()\nval_ids = image_ids[-45:]\ntrain_ids = image_ids[:-45]\nval_df = df[df['image_id'].isin(val_ids)]\ntrain_df = df[df['image_id'].isin(train_ids)]","882f4d15":"class CoreDataset(Dataset):\n    def __init__(self, df, IMG_DIR, transform = None):\n        self.df = df # We pass the dataframe\n        self.img_dir = IMG_DIR # We pass the path \n        self.image_ids = self.df[\"image_id\"].unique().tolist() # We create a list with the names of the photos\n        self.transform = transform # We passed the transformations\n    def __len__(self):\n        # Gives us the length of our data set\n        return len(self.image_ids)\n    \n    \n    def __getitem__(self, idx):\n        \n        coords = [\"x_min\", \"y_min\", \"x_max\", \"y_max\"] \n        image_id = self.image_ids[idx]   \n        boxes = self.df[self.df[\"image_id\"] == image_id].loc[:, coords].values\n        area = (boxes[:, 2] - boxes[:, 0])*(boxes[:, 3] - boxes[:, 1]) \n        image = cv2.imread(self.img_dir+image_id,cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\/255.0\n        labels = self.df[self.df[\"image_id\"] == image_id].loc[:, \"source\"].values \n        labels = torch.tensor((labels), dtype=torch.int64)\n        target = {\n                  'boxes': boxes,\n                  'labels': labels,\n                  'image_id': torch.tensor([idx]),\n                  'area': torch.as_tensor(area, dtype = torch.float32),\n                  'iscrowd': torch.zeros((boxes.shape[0],), dtype = torch.int64)\n                 }\n    \n        if self.transform:\n            sample = {\n                      'image': image,\n                      'bboxes': target['boxes'],\n                      'labels': labels\n                     }\n            sample = self.transform(**sample)\n            image = sample['image']\n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return torch.tensor(image), target, image_id","6a509473":"def get_train_transform():\n    return A.Compose([ # Compose to make multiple transform\n        A.Flip(0.5), \n        A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                     val_shift_limit=0.2, p=0.9),\n                A.RandomBrightnessContrast(brightness_limit=0.2, \n                                           contrast_limit=0.2, p=0.9),\n            ],p=0.9),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}) # bbox parameters\n\ndef get_val_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}) # bbox parameters","0bcf37ae":"CoreTrain = CoreDataset(train_df, path, get_train_transform())","d5cbcecb":"# We visualize the image applying the transformations\nimg, tar,_ = CoreTrain[0]\nbbox = tar['boxes'].numpy()\nfig,ax = plt.subplots(figsize=(18,10))\nax.imshow(img.permute(1,2,0).cpu().numpy())\nfor i in range(len(bbox)):\n    box = bbox[i]\n    x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n    rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2,edgecolor='b',facecolor='none',)\n    ax.text(*box[:2], \"box\", verticalalignment='top', color='white', fontsize=13, weight='bold')\n    ax.add_patch(rect)\nplt.show()","519379db":" CoreVal = CoreDataset(val_df, path, get_val_transform())","9720eb27":"def collate_fn(batch):\n    return tuple(zip(*batch))\nindices = torch.randperm(len(CoreTrain)).tolist()\ntrain_data_loader = DataLoader(\n    CoreTrain,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    CoreVal,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","a7524978":"num_classes = 3  # 0 (background) + 1 (wood)+ 2 (rock +10 cm)\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","43ee2649":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","aa53967c":"class Averager:\n    \n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n        \n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n        \n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","16625b52":"def apply_nms(orig_prediction, iou_thresh=0.2):\n\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction","f6a9102a":"num_epochs = 15","4c5335a1":"loss_hist = Averager()\nbest_epoch = 0\nmin_loss = sys.maxsize\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    tk = tqdm(train_data_loader)\n    model.train();\n    for images, targets, image_ids in tk:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        tk.set_postfix(train_loss=loss_value)\n    tk.close()\n    \n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") \n    \n    if loss_hist.value<min_loss:\n        print(\"Better model found at epoch {0} with {1:0.5f} loss value\".format(epoch,loss_hist.value))\n        torch.save(model.state_dict(), f\"model_state_epoch_{epoch}.pth\")\n        min_loss = loss_hist.value\n        best_epoch = epoch\n    #validation \n    model.eval();\n    with torch.no_grad():\n        tk = tqdm(valid_data_loader)\n        for images, targets, image_ids in tk:\n        \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            val_output = model(images)\n            val_output = [{k: v.to('cpu') for k, v in t.items()} for t in val_output]\n            IOU = []\n            for j in range(len(val_output)):\n                val_out = apply_nms(val_output[j])\n                a,b = val_out['boxes'].cpu().detach(), targets[j]['boxes'].cpu().detach()\n                chk = torchvision.ops.box_iou(a,b)\n                res = np.nanmean(chk.sum(axis=1)\/(chk>0).sum(axis=1))\n                IOU.append(res)\n            tk.set_postfix(IoU=np.mean(IOU))\n        tk.close()\n        \nmodel.load_state_dict(torch.load(f\".\/model_state_epoch_{best_epoch}.pth\"));","ab82370a":"model.load_state_dict(torch.load(f\".\/model_state_epoch_{best_epoch}.pth\"));","0bc27ae8":"img,target,_ = CoreVal[5]\n\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['boxes']))\nprint('real #boxes: ', len(target['boxes']))","78e08014":"bbox = target['boxes'].numpy()\nfig,ax = plt.subplots(1,figsize=(18,10))\nax.imshow(img.permute(1,2,0).cpu().numpy())\nfor i in range(len(bbox)):\n    box = bbox[i]\n    x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n    rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2,edgecolor='r',facecolor='none',)\n    ax.text(*box[:2], \"core\", verticalalignment='top', color='red', fontsize=13, weight='bold')\n    ax.add_patch(rect)\nplt.show()","1869ed96":"def plot_valid(img,prediction,nms=True,detect_thresh=0.55):\n    fig,ax = plt.subplots(figsize=(18,10))\n    val_img = img.permute(1,2,0).cpu().numpy()\n    ax.imshow(val_img)\n    nms_prediction = apply_nms(prediction, iou_thresh=0.2) if nms else prediction\n    #print(nms_prediction)\n    val_scores = nms_prediction['scores'].cpu().detach().numpy()\n    bbox = nms_prediction['boxes'].cpu().detach().numpy()\n    labels = nms_prediction['labels'].cpu().detach().numpy() # +10cm rock 2 \/\/ wood 1\n    for i in range(len(bbox)):\n        if val_scores[i]>=detect_thresh:\n            box = bbox[i]\n            label=labels[i]\n            x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n            \n            if (label==1):\n                rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=3 ,edgecolor='r',facecolor='none',)\n                ax.text(*box[:2], \"wood {0:.3f}\".format(val_scores[i]), verticalalignment='top', color='white', fontsize=12, weight='bold')\n                ax.add_patch(rect)\n            else:\n                if (w>47):\n                    rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=3 ,edgecolor='b',facecolor='none',)\n                    ax.text(*box[:2], \"rock {0:.3f}\".format(val_scores[i]), verticalalignment='top', color='white', fontsize=12, weight='bold')\n                    ax.add_patch(rect)\n    plt.show()","73d34815":"plot_valid(img,prediction)","28717e09":"submission = pd.read_csv('.\/s3\/test1.csv')\nsubmission.head()","56ea6a17":"class TestDataset(object):\n    def __init__(self, df, IMG_DIR, transforms=None):\n        \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.transforms = transforms\n        self.image_ids = self.df['image_id'].tolist()\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        \n        image_id = self.image_ids[idx]\n        image = cv2.imread(self.img_dir+image_id,cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\/255.0\n        \n        \n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id","5b0e9a63":"def get_test_transform(IMG_SIZE=(512,512)):\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ])","eaad171b":"test_img_dir = '.\/s3\/test2\/'\nIMG_SIZE = (512,512)","7fedba0e":"test_dataset = TestDataset(submission, test_img_dir ,get_test_transform(IMG_SIZE))","d29fa34d":"for j in range(submission.shape[0]):\n#for j in range(5):\n    img,_ = test_dataset[j]\n    model.eval()\n    with torch.no_grad():\n        prediction = model([img.to(device)])[0]\n    plot_valid(img,prediction)","91d20821":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\ncarpeta_de_imagenes_hd=\"..\/input\/mining-core-sampling-dataset\/test\"","64c30c5d":"def plot_valid2(img,prediction,nms=True,detect_thresh=0.55):\n    fig,ax = plt.subplots(figsize=(40,40))\n    val_img = mpimg.imread(f'{carpeta_de_imagenes_hd}\/{cadena}')\n    ax.imshow(val_img)\n    t_y,t_w=val_img.shape[0],val_img.shape[1]\n    nms_prediction = apply_nms(prediction, iou_thresh=0.2) if nms else prediction\n    val_scores = nms_prediction['scores'].cpu().detach().numpy()\n    bbox = nms_prediction['boxes'].cpu().detach().numpy()\n    labels = nms_prediction['labels'].cpu().detach().numpy() # +10cm rock 2 \/\/ wood 1\n    for i in range(len(bbox)):\n        if val_scores[i]>=detect_thresh:\n            box = bbox[i]\n            label=labels[i]\n            w2= box[2]-box[0]\n            x,y,w,h = box[0]*t_w\/512, box[1]*t_y\/512, box[2]*t_w\/512-box[0]*t_w\/512, box[3]*t_y\/512-box[1]*t_y\/512\n            w3=round((w*107\/t_w),1)\n            if (label==1):\n                rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=4 ,edgecolor='r',facecolor='none',)\n                ax.text(box[0]*t_w\/512,box[1]*t_y\/512, \"wood\", verticalalignment='center', color='white', fontsize=15, weight='bold')\n                ax.add_patch(rect)\n            else:\n                if (w2>47):\n                    rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=4 ,edgecolor='b',facecolor='none',)\n                    ax.text(box[0]*t_w\/512,box[1]*t_y\/512, f\"rock {w3}cm\", verticalalignment='center', color='white', fontsize=20, weight='bold')\n                    ax.add_patch(rect)\n    plt.savefig(\"Core.jpg\", bbox_inches='tight')\n    plt.show()","8931151b":"import random\nix=30\n#ix = random.randint(0, submission.shape[0]-1)\nimagen=submission.iloc[ix:ix+1,0].values.tolist()\ncadena = \" \".join(imagen)\nimg,_ = test_dataset[ix]\nmodel.eval()\nwith torch.no_grad():\n        prediction = model([img.to(device)])[0]\nplot_valid2(img,prediction)","7fb328a9":"torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","ae70266f":"torch.jit.save(torch.jit.script(model.cpu()),\"model.zip\")","f14b82de":"# Preparing the Data and Visualization\n\nchange the resolution of the images to 512 * 512 and adapt the box positions to match the new resolution.","df2ecb61":"# Save Model","bb77fde3":"# Training","4223cdba":"# EDA","41f1c026":"# Predictions in the test data","dff6c10d":"# Preparing dataset","0675c4cc":"# Next step\n\n- Cross validation\n- Test Time Augmentation (TTA)\n- Work with YoloV5 or EfficientDet7, or a better model\n- Pseudo Label Since we have a large amount of data in mining, working with this technique seems like a very good idea.\n- More Data Augmentation\n- INCREASE THE SIZE OF THE DATASET\n\n\nin the following weeks I will upload a new notebook with the next steps mentioned.\n\nI hope it will be useful to you and if you have any data set focused on mining, I would like you to let me know to continue practicing.","3fcb4566":"# About this Notebook\n\nHello Everyone!\n\nEn este cuaderno voy a realizar un ajuste de un modelo de detecci\u00f3n de objetos para fasterrcnn_resnet50_fpn donde se intentar\u00e1 localizar los cores de +10 cm y los separadores \nde madera en taladros de exploraci\u00f3n, desde hace 1 a\u00f1o me intereso el mundo de Deep Learning y me alegre mucho al ver un Dataset dirigido a la miner\u00eda, ya que es mi profesi\u00f3n, \nrealizo este notebook para quien desee iniciar en este mundo tenga un punto de partida de donde guiarse, espero que les sea de utilidad, reci\u00e9n estoy empezando en este mundo \npor lo que este notebook puede tener algunos errores. si desean m\u00e1s informaci\u00f3n al respecto dejo 2 enlaces de donde me inspire para hacer este \nnotebook. [Link 1]( https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html) \n[Link 2](https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html)\n\n\nIn this notebook I am going to make an adjustment of an object detection model for fasterrcnn_resnet50_fpn where it will try to locate the core of +10 cm and the wooden spacers in exploration holes, for 1 year I have been interested in the world of Deep Learning and II am very happy to see a Dataset aimed at mining, since it is my profession, I make this notebook for those who want to start in this world have a starting point from which to guide themselves, I hope it will be useful to them , I am just starting in this world by what this notebook may have some errors. If you want more information about it, I leave 2 links where there is a pytorch tutorial on the subject. [Link 1]( https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html) \n[Link 2](https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html)","38842714":"# Plot in max resolution","8ce72107":"# Augmentations","0e3f7d6a":"# Model","888dcfdb":"# PYTORCH BASELINE OBJECT DETECTION"}}