{"cell_type":{"af63d292":"code","431ccabb":"code","9c96f6ec":"code","40bf283d":"code","d68bf059":"code","e87d0c83":"code","a1c871cb":"code","306b6264":"code","14da571b":"code","eefb9b7f":"code","cd087aa2":"code","e01146b7":"code","bdfae895":"code","c5ae158c":"code","1131b7f7":"code","51e2735f":"code","00e3757e":"code","d0cb5dcf":"markdown","f2efb75a":"markdown","a36cd789":"markdown","0a84cf40":"markdown","820e6915":"markdown","f63a5e4e":"markdown","38784472":"markdown","cc6e8f6d":"markdown","b7adb94c":"markdown","3e354745":"markdown"},"source":{"af63d292":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.ensemble import RandomForestRegressor\n%matplotlib inline\nimport seaborn as sns","431ccabb":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","9c96f6ec":"constant = train.nunique().reset_index()\nconstant.columns = [\"col\", \"count\"]\nconstant = constant.loc[constant[\"count\"]==1]\ntrain = train.drop(columns=constant.col,axis = 1)\ntest = test.drop(columns=constant.col,axis = 1)","40bf283d":"y = train[\"target\"]\ntrain = train.drop([\"ID\",\"target\"],axis=1)\ntest = test.drop(\"ID\",axis=1)\ntrain[\"ID\"] = train.index\ntest[\"ID\"] = test.index","d68bf059":"def maxabs(train,test):\n    scaler = MaxAbsScaler()\n    scaler.fit(train)\n    train = scaler.transform(train)\n    test = scaler.transform(test)\n    return train,test","e87d0c83":"y_train=np.log1p(y)\n##start to test RF and tsvd below","a1c871cb":"trainSVD = train.copy()\ntestSVD = test.copy()","306b6264":"def rowagg(train,test):\n    ##\n    train[\"sum\"] = train.sum(axis=1)\n    test[\"sum\"] = test.sum(axis=1)\n    train[\"var\"] = train.var(axis=1)\n    test[\"var\"] = test.var(axis=1)\n    train[\"median\"] = train.median(axis=1)\n    test[\"median\"] = test.median(axis=1)\n    train[\"mean\"] = train.mean(axis=1)\n    test[\"mean\"] = test.mean(axis=1)\n    train[\"std\"] = train.std(axis=1)\n    test[\"std\"] = test.std(axis=1)\n    train[\"max\"] = train.max(axis=1)\n    test[\"max\"] = test.max(axis=1)\n    train[\"min\"] =train.min(axis=1)\n    test[\"min\"] = test.min(axis=1)\n    train[\"skew\"] = train.skew(axis=1)\n    test[\"skew\"] = test.skew(axis=1)\n    print (\"Null values in train: \"+ str(np.sum(np.sum(pd.isnull(train)))))\n    print (\"NAN values in train: \"+ str(np.sum(np.isnan(train.values))))\n    print (\"Null values in test: \"+ str(np.sum(np.sum(pd.isnull(test)))))\n    print (\"NAN values in test: \"+ str(np.sum(np.isnan(test.values))))\n    return train,test","14da571b":"from sklearn.decomposition import TruncatedSVD\ntrainSVD,testSVD = rowagg(trainSVD,testSVD)\nsvd = TruncatedSVD(n_components=2000)\nres = svd.fit(trainSVD)\nprint (np.sum(res.explained_variance_ratio_))","eefb9b7f":"trainSVD = res.transform(trainSVD)\ntestSVD = res.transform(testSVD)","cd087aa2":"import lightgbm as lgb\ndef run_lgb(X_train, Y_train, X_valid, Y_valid, test):\n    seed = 42\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"task\": \"train\",\n        \"boosting type\":'dart',\n        \"num_leaves\" :500,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.8,\n        \"feature_fraction\" : 0.8,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : seed,\n        \"verbosity\" : -1,\n        \"seed\": seed\n    }\n    lgtrain = lgb.Dataset(X_train,label= Y_train)\n    lgval = lgb.Dataset(X_valid,label =Y_valid)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                  valid_sets=[lgtrain, lgval], \n                  early_stopping_rounds=300, \n                  verbose_eval=100, \n                  evals_result=evals_result)\n    lgb_prediction = np.expm1(model.predict(test, num_iteration=model.best_iteration))\n    return lgb_prediction, model, evals_result","e01146b7":"from sklearn.model_selection import train_test_split\ntrainSVD,testSVD = maxabs(trainSVD,testSVD)\nX_train, X_test, Y_train, Y_test = train_test_split(trainSVD, y_train, test_size=0.1, random_state=0)\nlgb_predSVD, model, evals_resultRF = run_lgb(X_train, Y_train, X_test, Y_test, testSVD)","bdfae895":"##stage2 :test on Random Forest\ntrainRF = train.copy()\ntestRF = test.copy()\ntrainRF,testRF = rowagg(trainRF,testRF)","c5ae158c":"rf_clf=RandomForestRegressor(random_state=42,n_jobs=-1)\nrf_clf.fit(trainRF,y_train)\nrank = pd.DataFrame()\nrank[\"importance\"] = np.array(rf_clf.feature_importances_)\nrank[\"feature\"] = np.array(trainRF.columns).T\nrank = rank.sort_values(by=['importance'], ascending=False)\ncol = rank[:2000]","1131b7f7":"trainRF=trainRF[col.feature]\ntestRF=testRF[col.feature]\ntrainRF,testRF = maxabs(trainRF,testRF)\nX_train, X_test, Y_train, Y_test = train_test_split(trainRF, y_train, test_size=0.1, random_state=0)\nlgb_predRF, model, evals_resultRF = run_lgb(X_train, Y_train, X_test, Y_test, testRF)","51e2735f":"sub=pd.read_csv('..\/input\/sample_submission.csv')\nsub[\"target\"] = lgb_predRF\nsub.to_csv('sub.csv', index=False)","00e3757e":"sub.head()","d0cb5dcf":"**For this notebook, I want show my special thanks to @AmarjeetKumarRandom for his work in preprocessing and lightGBM, give me a good lesson on how it works!**\n","f2efb75a":"We will add some additional columns in the feature selection process.\n\nThose are what being mentioned in the discussion section  and someone names it as \"Row aggregation\". \n\nIn case for missing\/NAN shows after modification, I will place some check at the end of the function.","a36cd789":"If you go check , you will find some zero constant columns.  What we do here is just delete them.\n\nIn the meanwhile, \"ID\" column cannot be used for regression so we delete it. I checked it locally and there is no duplicate values so we just use index to override it.\n\nYou can consider this as a label encoder transformation.","0a84cf40":"**The following block of code are from @AmarjeetKumarRandom 's work, his lgb notebook is concise on how lgb runs. Really a good job.**\n","820e6915":"## Compare two different methods of Feature Extraction: Tsvd VS RF\n\nThis post is aiming to compare two different categories of feature selection technique.\n\nThe first one is pretty traditional, just the normal dimensional reduction technique. The typical one of those examples are PCA\/LDA\/T-svd.\n\nSecond example is tree-based regressor(Random forest as example). Those kind of regressors are normally considered better for the fact that they will consider correlations between different feature and is capable of forward\/backward feature elimination.\n\nHowever, it is hard to say which one is better in this competition without further investgation, For something special showed in this competiition:\n\n1. There are many zeros exist in both trainset and testset. In this case, sparse matrix shows and tsvd is considered better to recongnize the eigenvector and eigenvalue.\n\n2. If you conpute correlations between feature, normally they ranges from 0.03 to 0.1, which suggests a weak corrrelation between each other.(np.corr can achieve this).\n\n3. It is possible to overfit dataset with random forest feature selection.","f63a5e4e":"## Test on Random forest feature selection","38784472":"## Test on tsvd algorithm","cc6e8f6d":"Here we use maxabs scaler to scale the data to help optimization converge in a faster speed.\n\nMinmax is also a preferable choice. Anyone will work here, given that they can preserve the sparse of the matrix.","b7adb94c":"## Summary\nIt is surprising to see RF is still working better than tsvd base on cv score showed during lgb train process. \n\nYou may wonder how it works in k-fold cv. I tested it locally(k=10) and as it turns out, on average, the RF's score is still better than tsvd's. Due to the kernel time limit, I cannot run k-fold test on this kernel because it takes longer than time allowed.\n\nI guess you may find something here are questionable. If yes, please comment this notebook so I can see and update it.","3e354745":"## Test plan\nHere we will copy our datasets and use correspond datasets to check the performance. \n\nYou can obtain result via Lightgbm training process."}}