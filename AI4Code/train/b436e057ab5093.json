{"cell_type":{"384f2f04":"code","c57ddf5a":"code","79096bc4":"code","4d2ed936":"code","3bd70a44":"code","b7014bca":"code","31544546":"code","c4e99b4f":"code","eaddc129":"code","c801ccda":"code","d6f755c9":"code","f2485bf9":"code","ac112e64":"code","973e5f16":"code","6f058798":"code","a03c04de":"code","11471f8a":"code","152a8dd2":"code","e4e3a0c6":"code","08e4242c":"code","a2a7ec6b":"code","cbfcc5b8":"code","f8cf48d5":"code","64691850":"code","8c9935de":"code","8205b393":"code","604190fa":"code","ce2db69b":"code","5a10d315":"code","7141d46e":"code","bf93dac5":"code","ebd749a7":"code","a4c689f9":"code","9347c53a":"code","921a8f6f":"code","e6214716":"code","967373ec":"code","3a4504eb":"code","406a42d8":"code","b7beca0d":"code","64e90132":"code","3447868e":"code","903a90c1":"code","e2cbd77e":"code","3009e56f":"code","c4fc1df9":"code","5f3acb7a":"code","95850257":"code","fb56f13c":"code","8eda6850":"code","ca2cc82f":"code","4494a040":"code","6b1ae93e":"code","a8b49b9f":"code","0dcfe6f2":"code","6f7a1f2c":"code","f9391059":"code","fe4f1afb":"code","7693ba50":"code","7fddd1b0":"code","3da9a0ac":"code","dbe8c2ea":"markdown","6897ff96":"markdown","942d08ea":"markdown","51febbfc":"markdown","cef8fd74":"markdown","053e14b2":"markdown","aa7c3c73":"markdown","74a5904e":"markdown","9e00a3f7":"markdown","d0af6007":"markdown","c9a630c9":"markdown","ed5f5bb6":"markdown","886459c5":"markdown","e4dc73d4":"markdown","91c072db":"markdown","db2871ca":"markdown","6e60e3b2":"markdown","c34dbcaf":"markdown","48a76d8a":"markdown","326ff57a":"markdown","9c73e06f":"markdown","71a97620":"markdown","72a51fd1":"markdown","b16a9970":"markdown","4e1f50e0":"markdown","e0680dc9":"markdown","770eb849":"markdown","59820035":"markdown","e11a5a34":"markdown","65a3eda8":"markdown","8811cd71":"markdown","0df70267":"markdown","c16ff2f9":"markdown","92263d47":"markdown","7cc0eb07":"markdown","a2b4e5cb":"markdown","60c30e67":"markdown","7958810e":"markdown","391f0099":"markdown","9e21ac68":"markdown","3cd563c8":"markdown","c7587531":"markdown","6595c31d":"markdown","3410f0f4":"markdown","7e1099ee":"markdown","1e2064c7":"markdown","0b35e838":"markdown","faa049c8":"markdown","f56b8d7b":"markdown","470bf9d3":"markdown","db9ce1d8":"markdown","584bedf8":"markdown","32e5572e":"markdown","986a1816":"markdown","7237750a":"markdown","cb861431":"markdown","45f08108":"markdown","55f32754":"markdown","5dc4bc45":"markdown"},"source":{"384f2f04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom IPython.display import display\n\n# remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.\n","c57ddf5a":"# Loading Modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n\n# Loading Datasets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Data Prepration\n# save PassengerId for final submission\npassengerId = test.PassengerId\n# merge train and test\ntitanic = train.append(test, ignore_index=True)\n\n\n# create indexes to separate data later on\ntrain_idx = len(train)\ntest_idx = len(titanic) - len(test)","79096bc4":"# Let's take a look\n# describe(include = ['O']) will show the descriptive statistics of object data types\ntitanic.describe(include=['O'])","4d2ed936":"display( titanic.head(5), \ntitanic.info(), \ntitanic.isnull().sum()\n       )","3bd70a44":"# Survival and Precent\nsurvived = titanic[titanic['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]             # Using train dataset since titanic includes test as well\n\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived))\/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint (\"Total: %i\"%len(train))\n","b7014bca":"# Mathmetically \nprint( train.groupby('Pclass').Survived.value_counts(), \n      train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean()\n     )\n\nsns.barplot(x='Pclass', y='Survived', data=train)","31544546":"print( titanic.Sex.value_counts(),\ntrain.groupby('Sex').Survived.value_counts())\n\nsns.barplot(x='Sex', y='Survived', data=train)","c4e99b4f":"tab = pd.crosstab(titanic['Pclass'], titanic['Sex'])\nprint (tab)\n\n# Floating division of dataframe and other, element-wise (binary operator truediv).\n# Equivalent to dataframe \/ other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv.\ntab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n\nplt.xlabel('Pclass')\nplt.ylabel('Percentage')\nplt.xticks(rotation=0)\n\n# sns.factorplot('Sex', 'Survived', hue='Pclass', size=4, aspect=2, data=train)\nsns.catplot('Sex', 'Survived', hue='Pclass', height=4, aspect=2, data=train, kind='point')","eaddc129":"sns.catplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train, kind = \"point\" )","c801ccda":"print( train.Embarked.value_counts(), \n     train.groupby('Embarked').Survived.value_counts(),\n     train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n\nsns.barplot('Embarked', 'Survived', hue='Sex', data = train)","d6f755c9":"print( train.Parch.value_counts(),\ntrain.groupby('Parch').Survived.value_counts(),\ntrain[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean() )\n\nsns.barplot(x='Parch', y='Survived', ci=None, data=train) # ci=None will hide the error bar","f2485bf9":"print( train.SibSp.value_counts(), \n      train.groupby('SibSp').Survived.value_counts(), \n      train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean()\n)\n\nsns.barplot( x = 'SibSp', y = 'Survived', ci = None, data = train)","ac112e64":"fig , (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\nfig.set_size_inches(15,5)\n\nsns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax1)\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax2)\nsns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax3)","973e5f16":"total_survived = train[train['Survived']==1]\ntotal_not_survived = train[train['Survived']==0]\nmale_survived = train[(train['Survived']==1) & (train['Sex']==\"male\")]\nfemale_survived = train[(train['Survived']==1) & (train['Sex']==\"female\")]\nmale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"male\")]\nfemale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"female\")]\n\nimport matplotlib.gridspec as gridspec\ngs = gridspec.GridSpec(2, 2, right=2, top= 2)\nax1 = plt.subplot(gs[0, :])\nax2 = plt.subplot(gs[1,0])\nax3 = plt.subplot(gs[1,1])\n\nsns.distplot(total_survived['Age'].dropna().values, bins=range(0, 81, 2), kde=False, color='blue', ax = ax1)\nsns.distplot(total_not_survived['Age'].dropna().values, bins=range(0, 81, 2), kde=False, color='red', axlabel='Age', ax = ax1)\n\nsns.distplot(female_survived['Age'].dropna().values, bins=range(0, 81, 2), kde=False, color='blue', ax = ax2)\nsns.distplot(female_not_survived['Age'].dropna().values, bins=range(0, 81, 2), kde=False, color='red', axlabel='Female Age', ax = ax2)\n\nsns.distplot(male_survived['Age'].dropna().values, bins=range(0, 81, 2), kde=False, color='blue', ax = ax3)\nsns.distplot(male_not_survived['Age'].dropna().values, bins=range(0, 81, 2), kde=False, color='red', axlabel='Male Age', ax = ax3)\n\nax1.legend(['Total Survived', 'Total Not Survived'],loc=\"upper right\")\nax2.legend(['Females Survived', 'Females Not Survived'],loc=\"upper right\")\nax3.legend(['Males Survived', 'Males Not Survived'],loc=\"upper right\")\n","6f058798":"plt.figure(figsize=(15,6))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.7, square=False, annot=True)","a03c04de":"# Extracting Title \n# create a new feature to extract title names from the Name column\ntitanic['Title'] = titanic.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# normalize the titles\nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}\n# map the normalized titles to the current titles \ntitanic.Title = titanic.Title.map(normalized_titles)\n# view value counts for the normalized titles\npd.crosstab(titanic['Title'], titanic['Sex'])\nprint(titanic.Title.value_counts())","11471f8a":"# A Scikit Label Encoder \nfrom sklearn.preprocessing import LabelEncoder\n\n# Encoding Titles\ntitle_encoder = LabelEncoder()\n\n# Fitting\ntitanic['Title'] = title_encoder.fit_transform(titanic['Title'])\n# test['Title'] = title_encoder.transform( test['Title'] )\n\n# Viewing results\nprint( titanic.head(),\ntitanic.info() )","152a8dd2":"titanic.info()\ntitanic.Cabin[:10]","e4e3a0c6":"# Fill Cabin NaN with U for unknown\ntitanic.Cabin = titanic.Cabin.fillna('U')\n\n# map first letter of cabin to itself\ntitanic['Deck'] = titanic.Cabin.map(lambda x: x[0])\n\n# Create room number\ntitanic['Room'] = titanic['Cabin'].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\n\n\n### Filling missing data\n\n\n#     Now we will fill the missing data\n# titanic[\"Deck\"] = titanic[\"Deck\"].fillna(\"U\")\ntitanic[\"Room\"] = titanic[\"Room\"].fillna(titanic[\"Room\"].mean())\n","08e4242c":"titanic.head()","a2a7ec6b":"# Encoding Gender\n# Should investigate further setting labels for LE, currently alphabatic\n\nsex_encoder = LabelEncoder()\nX = ['male', 'female']\n\n# Fitting\nsex_encoder.fit(X)\n\n# Applying\ntitanic['Sex'] = sex_encoder.fit_transform(titanic['Sex'])\n# test['Sex'] = sex_encoder.transform(test['Sex'])\n\n# display( train.head(),\n#        test.head())\n\n# print( titanic.Sex.unique())","cbfcc5b8":"display( titanic.Embarked.unique(),\n\n# Checking the numbers\ntitanic.Embarked.value_counts(),\ntitanic.Embarked.isna().sum() \n       )","f8cf48d5":"# Adding the value of the most common Embarkation point\n\n# find most frequent Embarked value and store in variable\nmost_embarked = titanic.Embarked.value_counts().index[0]\n# fill NaN with most_embarked value\ntitanic.Embarked = titanic.Embarked.fillna(most_embarked)","64691850":"titanic.info()","8c9935de":"# for dataset in train_test_data:\n#     #print(dataset.Embarked.unique())\n#     dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntitanic['Embarked'] = titanic['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntitanic['Embarked'].head()","8205b393":"# group by Sex, Pclass, and Title \ngrouped = titanic.groupby(['Sex','Pclass', 'Title'])  \n# view the median Age by the grouped features \ngrouped.Age.median()\n","604190fa":"# apply the grouped median value on the Age NaN\ntitanic.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n# for dataset in train_test_data:\n#     age_avg = dataset['Age'].mean()\n#     age_std = dataset['Age'].std()\n#     age_null_count = dataset['Age'].isnull().sum()\n#     age_null_random_list = np.random.randint ( age_avg - age_std, age_avg + age_std, size = age_null_count)\n#     dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n#     dataset['Age'] = dataset['Age'].astype(int)\n    \n\n# Transforming into bands \n# train['AgeBand'] = pd.cut(train['Age'], 5)\n# print (train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean())\n# # train.drop('AgeBand', axis = 1)\n\n# # Applying the change to Age\n# for dataset in train_test_data:\n#     dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n#     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n#     dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n#     dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n#     dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n\n# Viewing \ntitanic.head()\ntitanic.info()","ce2db69b":"titanic.head()","5a10d315":"# group by Sex, Pclass, and Title \nfare_group = titanic.groupby(['Pclass', 'Deck', 'Room'])  \n\n# view the median Age by the grouped features \nfare_group.Fare.median()\n# apply the grouped median value on the Age NaN\ntitanic.Fare = fare_group.Fare.apply(lambda x: x.fillna(x.median()))\n\n# # FareBand\n# titanic['FareBand'] = pd.qcut(titanic['Fare'], 4)\n# print (titanic[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean())\n# print( pd.qcut(titanic['Fare'], 4).unique())\n\n# # Mapping to Band\n# # for dataset in train_test_data:\n# #     dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n# #     dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n# #     dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n# #     dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n# #     dataset['Fare'] = dataset['Fare'].astype(int)\n    \ndisplay(titanic.head(), \n     grouped.Age.apply(lambda x: x.fillna(x.median())),\ntitanic.info())","7141d46e":"# for dataset in train_test_data:\n#     dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1           # Adding 1 to count for the person\n\n# print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())\n\n# size of families (including the passenger)\ntitanic['FamilySize'] = titanic.Parch + titanic.SibSp + 1\n\nprint(titanic.FamilySize[:5])","bf93dac5":"# for dataset in train_test_data:\n#     dataset['IsAlone'] = 0\n#     dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())\n\ntitanic['IsAlone'] = 0\ntitanic.loc[titanic['FamilySize'] == 1, 'IsAlone'] = 1\nprint (titanic[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())\n","ebd749a7":"# create train and test data\ntrain = titanic[ :train_idx]\ntest = titanic[test_idx: ]\n# convert Survived back to int\ntrain.Survived = train.Survived.astype(int)\n\n# create X and y for data and target values \nX_train = train.drop('Survived', axis=1)\ny_train = train.Survived\n# create array for test set\nX_test = test.drop('Survived', axis=1)\n\n\ndisplay( X_train[:5], y_train[:5], X_test[:5] )\nX_train.shape, y_train.shape, X_test.shape","a4c689f9":"features = [ 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title','SibSp',\n       'Parch', 'FamilySize', 'IsAlone']\n\n# features = [ 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title',\n#         'FamilySize', 'IsAlone']\n\n\nX_train = X_train[features]\n# y_train = train.Survived\n# create array for test set\nX_test = X_test[features]","9347c53a":"display(\ntitanic.isna().sum())","921a8f6f":"# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection","e6214716":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')*100\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\n# Create MLA Table\nAlgorithms = pd.DataFrame( columns = ['ML Algorithm', 'Parameters','Train Accuracy Mean', 'Test Accuracy Mean', 'Mean Absolute Error', 'Test Accuracy 3*STD' ,'Time', 'Output'])\n\n## split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                      'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                      'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_log_reg'}, ignore_index=1)\n","967373ec":"display(Algorithms)","3a4504eb":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_svc)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_svc = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_svc'}, ignore_index=1)","406a42d8":"clf = LinearSVC()\nclf.fit(X_train, y_train)\ny_pred_linear_svc = clf.predict(X_test)\nacc_linear_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_linear_svc)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, train[features], train['Survived'], scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_lsvc = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_linear_svc'}, ignore_index=1)","b7beca0d":"clf = KNeighborsClassifier(n_neighbors = 10)\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_knn)\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_knn = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_knn'}, ignore_index=1)","64e90132":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\n\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_decision_tree)\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_decision_tree = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_decision_tree'}, ignore_index=1)","3447868e":"display(cv_results)","903a90c1":"# #Graph MLA version of Decision Tree: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html\n# import graphviz \n# dot_data = clf.export_graphviz(dtree, out_file=None, \n#                                 feature_names = data1_x_bin, class_names = True,\n#                                 filled = True, rounded = True)\n# graph = graphviz.Source(dot_data) \n# graph","e2cbd77e":"clf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\n\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_random_forest)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_rforest = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_random_forest'}, ignore_index=1)","3009e56f":"# Loading GridSearchCV \nfrom sklearn.model_selection import GridSearchCV\n\n# create param grid object \nforrest_params = dict(     \n    max_depth = [n for n in range(9, 14)],     \n    min_samples_split = [n for n in range(4, 11)], \n    min_samples_leaf = [n for n in range(2, 5)],     \n    n_estimators = [n for n in range(10, 60, 10)],\n#      n_jobs= [n for n in range(3, 3, 1)]\n    n_jobs= [3]\n)","c4fc1df9":"# # Instiatie Random Forest \n# clf = RandomForestClassifier()\n\n# # Build a GridSearchCV\n# forest_cv = GridSearchCV(estimator=clf, param_grid=forrest_params, cv=5) \n\n# # Fitting to find the best_score & best_estimator_\n# forest_cv.fit(X_train, y_train)","5f3acb7a":"# print(\"Best score: {}\".format(forest_cv.best_score_))\n# print(\"Optimal params: {}\".format(forest_cv.best_estimator_))\n\n# Best score: 0.8439955106621774\n# Optimal params: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#             max_depth=12, max_features='auto', max_leaf_nodes=None,\n#             min_impurity_decrease=0.0, min_impurity_split=None,\n#             min_samples_leaf=3, min_samples_split=6,\n#             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=3,\n#             oob_score=False, random_state=None, verbose=0,\n#             warm_start=False)","95850257":"## Using the best parameters to avoid the time penality \nforest_cv = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=12, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=3,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\n# Fitting to find the best_score & best_estimator_\nforest_cv.fit(X_train, y_train)","fb56f13c":"y_pred_grid_rforest = forest_cv.predict(X_test)\nacc_grid_rforest = round(forest_cv.score(X_train, y_train) * 100, 2)\nprint (acc_grid_rforest)\n\n# cv_scores = cross_val_score(forest_cv, X_train, y_train, scoring='accuracy')\n# print('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_grid_rforest = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : 'RandomForest_GridCV' , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_grid_rforest'}, ignore_index=1)","8eda6850":"clf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred_gnb = clf.predict(X_test)\nacc_gnb = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_gnb)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_nbayes = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_gnb'}, ignore_index=1)","ca2cc82f":"clf = Perceptron(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_perceptron = clf.predict(X_test)\nacc_perceptron = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_perceptron)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_perc = -1 * scores.mean()\n\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_perceptron'}, ignore_index=1)","4494a040":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_sgd = clf.predict(X_test)\n\nacc_sgd = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_sgd)\n\ncv_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy')\nprint('Model Accuracy %.2f' %(cv_scores.mean()*100))\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_sgd = -1 * scores.mean()\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_sgd'}, ignore_index=1)","6b1ae93e":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Preping Data\n# Here we are using train_X and train_y not to mess with other models\ngb_X_train, gb_X_test, gb_y_train, gb_y_test = train_test_split(X_train, y_train, test_size=0.25)\n\nclf = XGBClassifier()\n# Add silent=True to avoid printing out updates with each cycle\nclf.fit(gb_X_train, gb_y_train, verbose=True)\n\n\n#We similarly evaluate a model and make predictions as we would do in scikit-learn.\n# make predictions\n# predictions = clf.predict(test_X)","a8b49b9f":"clf = XGBClassifier(n_estimators=1000)\nclf.fit(gb_X_train, gb_y_train, early_stopping_rounds=10, \n             eval_set=[(gb_X_train, gb_y_train)], verbose=False)","0dcfe6f2":"my_model = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(gb_X_train, gb_y_train, early_stopping_rounds=10, \n             eval_set=[(gb_X_test, gb_y_test)], verbose=False)","6f7a1f2c":"# Model\nclf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, importance_type='gain',\n       learning_rate=0.05, max_delta_step=0, max_depth=3,\n       min_child_weight=1, missing=None, n_estimators=2000, n_jobs=3,\n       nthread=-1, objective='binary:logistic', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=1)\n        \n# param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' , 'early_stopping_rounds':10 }\n# param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'reg:linear' , 'early_stopping_rounds':10 }\n\n# num_round = 2\n\n# Fitting\nclf.fit(X_train, y_train, early_stopping_rounds=10, eval_set= [(X_train, y_train)], verbose = False )\n\nprint( clf.get_xgb_params )\n\n# Model Predications\ny_pred_xgb = clf.predict(X_test)\n\n# Results\nacc_xgb = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_xgb)\n\nscores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\nmae_xgb = -1 * scores.mean()\n\n# CV Results\ncv_results = model_selection.cross_validate(clf, X_train, y_train, cv  = cv_split)\n# Appending to Algorithms evaluation DF\nAlgorithms =  Algorithms.append( {'ML Algorithm' : clf.__class__.__name__ , \n                                     'Parameters': clf.get_params(),\n                                     'Train Accuracy Mean' : cv_results['train_score'].mean(), \n                                     'Test Accuracy Mean' : cv_results['test_score'].mean(), \n                                  'Mean Absolute Error' : cross_val_score(clf, X_train, y_train, scoring='neg_mean_absolute_error').mean() * -1 ,\n                                     'Test Accuracy 3*STD' : cv_results['test_score'].std()*3,\n                                      'Time' : cv_results['fit_time'].mean(),\n                                     'Output' : 'y_pred_xgb'}, ignore_index=1)","f9391059":"# features = [ \"your list of features ...\" ]\n# mapFeat = dict(zip([\"f\"+str(i) for i in range(len(features))],features))\n# print(mapFeat, clf.booster().get_fscore() )\n# ts = pd.Series(clf.booster().get_fscore())\n# ts.index = ts.reset_index()['index'].map(mapFeat)\n# ts.order()[-15:].plot(kind=\"barh\", title=(\"features importance\"))","fe4f1afb":"from sklearn.metrics import confusion_matrix\nimport itertools\n\n# clf = RandomForestClassifier(n_estimators=100)\n# clf.fit(X_train, y_train)\ny_pred_random_forest_training_set = forest_cv.predict(X_train)\nacc_random_forest = round(forest_cv.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy: %i %% \\n\"%acc_random_forest)\n\nclass_names = ['Survived', 'Not Survived']\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_pred_random_forest_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\n\ncnf_matrix_percent = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]\n\nprint ('Confusion Matrix in Percentage')\nprint (cnf_matrix_percent)\nprint ('')\n\ntrue_class_names = ['True Survived', 'True Not Survived']\npredicted_class_names = ['Predicted Survived', 'Predicted Not Survived']\n\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\n\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, \n                                     index = true_class_names,\n                                     columns = predicted_class_names)\n\nplt.figure(figsize = (15,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)","7693ba50":"# models = pd.DataFrame({\n#     'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', \n#               'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', \n#               'Perceptron', 'Stochastic Gradient Decent', 'XGBoost Classifier', 'Grid Random Forest'],\n    \n#     'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n#               acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n#               acc_perceptron, acc_sgd, acc_xgb, acc_grid_rforest],\n    \n#     'MAE' : [mae_log_reg, mae_svc, mae_lsvc, \n#              mae_knn, mae_decision_tree, mae_rforest, mae_nbayes,\n#              mae_perc, mae_sgd, mae_xgb, mae_grid_rforest],\n    \n#     'Output': ['y_pred_log_reg' , 'y_pred_svc' , 'y_pred_linear_svc' ,\n#                'y_pred_knn' , 'y_pred_decision_tree' , 'y_pred_random_forest' ,'y_pred_gnb' ,\n#                'y_pred_perceptron' , 'y_pred_sgd' , 'y_pred_xgb' , 'y_pred_grid_rforest']\n#     })\n\n# models.sort_values(by='Score', ascending=False)\n","7fddd1b0":"with pd.option_context('display.max_rows', 20, 'display.max_columns', 25, 'display.max_colwidth', 360):    \n    display(Algorithms.sort_values(by =['Test Accuracy Mean'], ascending = 0) )\n    display(Algorithms.sort_values(by =['Test Accuracy 3*STD'], ascending = 0) )","3da9a0ac":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred_grid_rforest\n    })\n\nsubmission.to_csv('submission.csv', index=False)\n\n# Random Forest = 0.77\n# XGBoost  = 0.79904\n# Grid Random Forest = 0.79904","dbe8c2ea":"## Feature Importance","6897ff96":"### There are many classifying algorithms present. Among them, we choose the following Classification algorithms for our problem:\n\n* Logistic Regression\n* Support Vector Machines (SVC)\n* Linear SVC\n* k-Nearest Neighbor (KNN)\n* Decision Tree\n* Random Forest\n* Naive Bayes (GaussianNB)\n* Perceptron\n* Stochastic Gradient Descent (SGD)\n\nHere's the training and testing procedure:\n1. First, we train these classifiers with our training data.\n2. After that, using the trained classifier, we predict the Survival outcome of test data.\n3. Finally, we calculate the accuracy score (in percentange) of the trained classifier.\n\n**Please note: that the accuracy score is generated based on our training dataset.**\n\n","942d08ea":"## SibSp & Parch Feature\nCombining SibSp & Parch feature, we create a new feature named FamilySize.","51febbfc":"Age, Cabin values are missing ( only present in 714, 204 respectively )\n\nThere is also 2 rows with missing Embarked Info","cef8fd74":"# Feature Extraction\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.\n\n## Title Feature\nLet's first extract titles from Name column.","053e14b2":"### Conclusion\n**XGBoost** is currently the dominant algorithm for building accurate models on conventional data (also called tabular or strutured data). Go apply it to improve your models!\n\n## Modeling with XGB","aa7c3c73":"\n## Emabrked Feature\nWe now convert the categorical value of Embarked into numeric. We represent 0 as S, 1 as C and 2 as Q.\n\nThere are empty values for some rows for Embarked column. The empty values are represented as \"nan\" in below list.","74a5904e":"## Cabin Feature\nFrom the previous cell it appears as if most of the cabins consist of a single letter at the beginning followed by a 2 or three digit number. It seems logical that the letter would represent the deck or section of boat where the cabin was located followed by the room number. It would seem that if you knew the section of the boat where someone was staying it would give you a lot of insight into their chances of survival. With that in mind let's work on cleaning up that column and seeing what we can get out of it.\n\nDeduce Deck and Room information\n","9e00a3f7":"When using **early_stopping_rounds**, you need to set aside some of your data for checking the number of rounds to use. If you later want to fit a model with all of your data, set **n_estimators** to whatever value you found to be optimal when run with early stopping.\n\n### learning_rate\nHere's a subtle but important trick for better XGBoost models:\n\nInstead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n\nSo, you can use a higher value of **n_estimators** without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n\nIn general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n\nModifying the example above to include a learing rate would yield the following code:","d0af6007":"## Sex vs. Survival\nassumption: Females have better chance of survival","c9a630c9":"### Decision Tree\nA [decision tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree) is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","ed5f5bb6":"   **Printing the GridSearch results**","886459c5":"## Age vs Survival","e4dc73d4":"## Sex Feature\nWe convert the categorical value of Sex into numeric. We represent 0 as female and 1 as male.","91c072db":"## Pclass vs. Survival\nassumption: Hi Class have better chance of survival","db2871ca":"## Comparing Models\nLet's compare the accuracy score of all the classifier models used above.","6e60e3b2":"**We use info() method to see more information of our train dataset.**","c34dbcaf":"### About data shows that:\n\n* Having FamilySize upto 4 (from 2 to 4) has better survival chance.\n* FamilySize = 1, i.e. travelling alone has less survival chance.\n* Large FamilySize (size of 5 and above) also have less survival chance.\n\nLet's create a new feature named IsAlone. This feature is used to check how is the survival chance while travelling alone as compared to travelling with family.","48a76d8a":"## Embarked vs. Survived","326ff57a":"### Perceptron\n[Perceptron](https:\/\/en.wikipedia.org\/wiki\/Perceptron) is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.","9c73e06f":"### Logistic Regression\n[Logistic regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression), or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. \n\nThis article covers the case of a binary dependent variable\u2014that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. \nCases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression.\n\n","71a97620":"**From the above figures, we can see that:**\n\nCombining both male and female, we can see that children with age between 0 to 5 have better chance of survival.\nFemales with age between \"18 to 40\" and \"50 and above\" have higher chance of survival.\nMales with age between 0 to 14 have better chance of survival.","72a51fd1":"## Model Tuning\nXGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. The first parameters you should understand are:\n\n**n_estimators and early_stopping_rounds**\nn_estimators specifies how many times to go through the modeling cycle described above.\n\nIn the *underfitting vs overfitting graph*, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the **learning rate** discussed below.\n\nThe argument **early_stopping_rounds** offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for **n_estimators** and then use **early_stopping_rounds** to find the optimal time to stop iterating.\n\nSince random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. **early_stopping_rounds = 5 ** is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores.\n\nHere is the code to fit with early_stopping:\n\n","b16a9970":"### Linear SVM\nLinear SVM is a SVM model with linear kernel.\n\nIn the below code, [LinearSVC](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html) stands for Linear Support Vector Classification.","4e1f50e0":"### Gaussian Naive Bayes\n[Naive Bayes classifiers](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier) are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\n[Bayes' theorem](https:\/\/en.wikipedia.org\/wiki\/Bayes%27_theorem) (alternatively **Bayes' law** or **Bayes' rule**) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.\n\nNaive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.","e0680dc9":"## Age Feature\nWe first fill the NULL values of Age with a random number between (mean_age - std_age) and (mean_age + std_age).\n\nWe then create a new column named AgeBand. This categorizes age into 5 different age range.\nI could use it to estimate the missing ages just a little bit better. The next step is to estimate the missing Age values\n\n** After experminating with turn this feature into catgeories it proved that its better off as is** ","770eb849":"## Fare Feature\n\nFirst, we replace missing features with the median value\n\nNext we create 4 cat ranges \n\nLastly, we map Fare to FareBand\n** After experminating, it is better to leave as is.**","59820035":"## Pclass & Sex vs. Survival\nBelow, we just find out how many males and females are there in each Pclass. \n\nWe then plot a stacked bar diagram with that information. We found that there are more males among the 3rd Pclass passengers.","e11a5a34":"**This shows that travelling alone has only 30% survival chance.**","65a3eda8":"# Classification & Accuracy\n","8811cd71":"## Correlating Features\nHeatmap of Correlation between different features:\n\n> Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n> Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the Survived feature.\n\n","0df70267":"### After that, we convert the categorical Title values into numeric form.","c16ff2f9":"This shows that there are duplicate Ticket number and Cabins shared. The highest number of duplicate ticket number is \"CA. 2343\". It has been repeated 7 times. Similarly, the highest number of people using the same cabin is 4. They are using cabin number \"C23 C25 C27\".\n\nWe also see that 644 people were embarked from port \"S\".\n\nAmong 891 rows, 577 were Male and the rest were Female.","92263d47":"### Support Vector Machine (SVM)\n[Support Vector Machine](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine) (SVM) model is a Supervised Learning model used for classification and regression analysis. It is a representation of the examples as points in \nspace, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a  p -dimensional vector (a list of  p  numbers), and we want to know whether we can separate such points with a  (p\u22121) -dimensional hyperplane.\n\nWhen data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass.\n\nIn the below code, SVC stands for Support Vector Classification.","7cc0eb07":"## Deducing Deck & Room numbers from Cabin\n\nWe will leverage the Cabin feature and simply extract the first letter of the cabin which determines the section where the room would have been. This is potentially relevant since it is possible that some cabins were closer to the life boats and thus those that were closer to them may have had a greater chance at securing a spot.","a2b4e5cb":"## Feature Selection\nWe drop unnecessary columns\/features and keep only the useful ones for our experiment. Column PassengerId is only dropped from Train set because we need PassengerId in Test set while creating Submission file to Kaggle.\n\nDefine training and testing set\n\n** Adding Siblings and Parents improved the results **\n\n","60c30e67":"### From Pclass Violinplot, we can see\n* 1st Class has very few childern population\n* 1st Class has more old population\n* Almost all childern are in 2nd class survived\n* majority of 3rd class childern survived\n* Younger pop survived in Class 1 vs Older pop\n\n### From Sex violinplot, we see\n* Most male children (between age 0 to 14) survived.\n* Females with age between 18 to 40 have better survival chance.\n* Older Males has a lower chance of survival than females","7958810e":"## Parch vs. Survival","391f0099":"We find that category \"S\" has maximum passengers. Hence, we replace \"nan\" values with \"S\".\n**Could be refined in the future**","9e21ac68":"## XGBoost Classifier\n**XGBoost** is the leading model for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). XGBoost models dominate many Kaggle competitions.\n\nTo reach peak accuracy, XGBoost models require more knowledge and model tuning than techniques like Random Forest. After this tutorial, you'ill be able to\n\n* Follow the full modeling workflow with XGBoost\n* Fine-tune XGBoost models for optimal performance\n**XGBoost** is an implementation of the **Gradient Boosted Decision Trees algorithm** (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.) What is Gradient Boosted Decision Trees? We'll walk through a diagram.\n\n","3cd563c8":"### n_jobs\nOn larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter **n_jobs** equal to the number of cores on your machine. On smaller datasets, this won't help.\n\nThe resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the *fit* command.\n\nXGBoost has a multitude of other parameters, but these will go a very long way in helping you fine-tune your XGBoost model for optimal performance.","c7587531":"### Stochastic Gradient Descent (SGD)\n[Stochastic gradient descent](https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent) (often shortened in **SGD**), also known as incremental gradient descent, is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions. In other words, SGD tries to find minima or maxima by iteration.","6595c31d":"Instead of simply filling in the missing Age values with the mean or median age of the dataset, by grouping the data by a passenger\u2019s sex, class, and title, we can drill down a bit deeper and get a closer approximation of what a passenger\u2019s age might have been. \n\nUsing the grouped.Age variable, we can fill in the missing values for Age.","3410f0f4":"**From the above plot, it can be seen that:**\n\n* Women from 1st and 2nd Pclass have almost 100% survival chance.\n* Men from 2nd and 3rd Pclass have only around 10% survival chance.\n\nGenerally, Women have a higher chance of survival","7e1099ee":"## SibSp vs. Survival","1e2064c7":"# Next we Look into Features and Survival \nWe will analyze the relationship between different features and *Survival*.\n\nWe see how different features are correlated with the different survival propability\n\nWe will plot different diagrams to **visualize** the findings.","0b35e838":"**From the above plot, it can be seen that:**\n\n* Almost all females from Pclass 1 and 2 survived.\n* Females dying were mostly from 3rd Pclass.\n* Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3.\n\nEmbarked shows interesting graph","faa049c8":"**References:**\nThis notebook is created by learning from the following notebooks:\n\n* [A Journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic)\n* [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [Pytanic](https:\/\/www.kaggle.com\/headsortails\/pytanic)\n* [Titanic best working Classifier](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier)\n* [My approach to Titanic competition](https:\/\/www.kaggle.com\/rafalplis\/my-approach-to-titanic-competition)","f56b8d7b":"## Optimizing Random Forest using GridSearchCV\n### Defining A dictionary of parameters with different ranges","470bf9d3":"### *k*-Nearest Neighbors\n[k-nearest neighbors algorithm](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm) (k-NN) is one of the simplest machine learning algorithms and is used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k=1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.","db9ce1d8":"### Random Forest\n[Random forests](https:\/\/en.wikipedia.org\/wiki\/Random_forest) or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\n[Ensemble methods](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning) use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.","584bedf8":"# Create Submission File to Kaggle","32e5572e":"We now convert the categorical value of Embarked into numeric. We represent 0 as S, 1 as C and 2 as Q.","986a1816":"## Data Definition\n### VariableDefinitionKey \n* **survival** Survival 0 = No, 1 = Yes \n* **pclass** Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd \n* **sex** Sex \n* **Age** Age in years \n* **sibsp** # of siblings \/ spouses aboard the Titanic \n* **parch** # of parents \/ children aboard the Titanic \n* **ticket** Ticket number \n* **fare** Passenger fare \n* **cabin** Cabin number \n* **embarked** Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n**Variable Notes**\n* pclass: A proxy for socio-economic status (SES)\n    *  1st = Upper\n    *  2nd = Middle\n    *  3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n    * Sibling = brother, sister, stepbrother, stepsister\n    * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n    * Parent = mother, father\n    * Child = daughter, son, stepdaughter, stepson\n    * Some children travelled only with a nanny, therefore parch=0 for them.","7237750a":"## Confusion Matrix\nA [confusion matrix](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix), also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).\n\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.\n\nHere's another guide explaining [Confusion Matrix with examples](http:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/).\n\n\\begin{matrix} & Predicted Positive & Predicted Negative \\\\ Actual Positive & TP & FN \\\\ Actual Negative & FP & TN \\end{matrix}\n\nIn our (Titanic problem) case:\n* **True Positive**: The classifier predicted Survived and the passenger actually Survived.\n* **True Negative**: The classifier predicted Not Survived and the passenger actually Not Survived.\n* **False Postiive**: The classifier predicted Survived but the passenger actually Not Survived.\n* **False Negative**: The classifier predicted Not Survived but the passenger actually Survived.\n\nIn the example code below, we plot a confusion matrix for the prediction of Random Forest Classifier on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer.","cb861431":"## Pclass, Sex & Embarked vs. Survival","45f08108":"From the above table, we can see that Decision Tree and Random Forest classfiers have the highest accuracy score.\n\nAmong these two, we choose Random Forest classifier as it has the ability to limit overfitting as compared to Decision Tree classifier.","55f32754":"# Splitting Data\n\nOur data is now in the format we need to perform some modeling. \n\nLet\u2019s separate it back into *train* and *test* data frames using the *train_idx* and *test_idx* we created in the beginning of the exercise. \n\nWe will also separate our training data into X for the predictor variables and y for our response variable which in this case is the Survived labels.","5dc4bc45":"The number of passengers with each Title is shown above.\n\nWe now replace some less common titles with the name \"Other\"."}}