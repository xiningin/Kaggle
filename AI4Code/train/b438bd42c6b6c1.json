{"cell_type":{"8900732d":"code","f11c52ea":"code","86638e49":"code","089ed407":"code","26f26978":"code","410a1470":"code","826e19a6":"code","f32cd401":"code","074625e7":"code","8a671242":"code","47cf4e57":"code","f5f4e58f":"code","8d441873":"code","a62282fb":"code","f7fb00e4":"code","5030096f":"code","71e4b832":"code","a5f8c0ab":"code","9ebe3f4f":"code","684aef5b":"code","a9671bf3":"code","336b0b0f":"code","40059b1d":"code","ca93d1cb":"code","17552f69":"code","a708281f":"code","8727d04e":"code","f23168b4":"code","035d8006":"code","d5af7976":"code","bbd3b5f8":"code","7031e626":"code","2dc9193e":"code","82132e03":"code","3a91e093":"code","421ccfb5":"code","c50cde63":"markdown","3901fbf1":"markdown","df94fdd2":"markdown","38ebd4a2":"markdown","c36b4849":"markdown","3b9b595c":"markdown","90d304ff":"markdown","56589a36":"markdown","cd399b48":"markdown","05222fd7":"markdown","05037496":"markdown","6fad763f":"markdown","209c2903":"markdown"},"source":{"8900732d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f11c52ea":"import re\nimport time\nimport csv\nimport math\nimport sqlite3\nimport warnings\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom joblib import dump, load\nfrom scipy.sparse import hstack\nfrom collections import Counter, defaultdict\nfrom sqlalchemy import create_engine # Database connection\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, normalized_mutual_info_score\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nwarnings.filterwarnings(\"ignore\")","86638e49":"data = pd.read_csv(\"..\/input\/d\/elemento\/quora-question-pairs\/final_features_sm.csv\", nrows = 100000)\nprint(data.shape)\ndata.head()","089ed407":"y_true = data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id','is_duplicate'], axis=1, inplace=True)\nprint(data.shape)","26f26978":"data.head()","410a1470":"X_train, X_test, y_train, y_test = train_test_split(data, y_true, stratify = y_true, test_size = 0.3)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","826e19a6":"print(\"Number of data points in train data:\",X_train.shape)\nprint(\"Number of data points in test data:\",X_test.shape)","f32cd401":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])\/train_len,\"Class 1: \", int(train_distr[1])\/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])\/test_len, \"Class 1: \",int(test_distr[1])\/test_len)","074625e7":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    \"\"\"\n    This function uses Seaborn Heatmap(s) to plot 3 different matrices. It plots the \n    Confusion Matrix, Precision Matrix and the Recall Matrix. The confusion matrix is as usual.\n    In the Precision Matrix, the sum of the values in each column is 1. Similarly, in the\n    Recall Matrix, the sum of the values in each row is 1.\n    \"\"\"\n    \n    C = confusion_matrix(test_y, predict_y)\n    # Divide each element of the confusion matrix with the sum of elements in that column\n    A = (((C.T)\/(C.sum(axis=1))).T)\n    # Divide each element of the confusion matrix with the sum of elements in that row\n    B = (C\/C.sum(axis=0))\n\n    plt.figure(figsize=(12,10))\n    \n    labels = [0,1]\n    cmap=sns.light_palette(\"blue\")\n    \n    # Representing C in heatmap format\n    plt.subplot(2, 2, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    # Representing B in heatmap format\n    plt.subplot(2, 2, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    # Representing A in heatmap format\n    plt.subplot(2, 2, 3)\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","8a671242":"# We need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https:\/\/stackoverflow.com\/a\/18662466\/4084039\n# We create an output array that has exactly same size as the CV data\npredicted_y = np.zeros((test_len, 2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs \/ sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\", log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y = np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","47cf4e57":"# # Hyper-parameters for SGD classifier\n# alpha = [10 ** x for x in range(-5, 2)] \n\n# # Read more about SGDClassifier() at\n# # http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n\n# log_error_array=[]\n# for i in alpha:\n#     clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n#     clf.fit(X_train, y_train)\n#     sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n#     sig_clf.fit(X_train, y_train)\n#     predict_y = sig_clf.predict_proba(X_test)\n#     log_error_array.append(log_loss(y_test, predict_y, labels = clf.classes_))\n#     print('Alpha:', i, \"Log loss:\", log_loss(y_test, predict_y, labels = clf.classes_))\n\n# fig, ax = plt.subplots()\n# ax.plot(alpha, log_error_array,c='g')\n# for i, txt in enumerate(np.round(log_error_array, 3)):\n#     ax.annotate((alpha[i], np.round(txt,3)), (alpha[i], log_error_array[i]))\n# plt.grid()\n# plt.title(\"Cross Validation Error for each alpha\")\n# plt.xlabel(\"Alpha(s)\")\n# plt.ylabel(\"Error measure\")\n# plt.show()\n\n# best_alpha = np.argmin(log_error_array)\n# clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","f5f4e58f":"# Saving the Model\n# dump(sig_clf, 'logistic_regression.joblib')\n\n# Loading the Model\nsig_clf = load('..\/input\/d\/elemento\/quora-question-pairs\/logistic_regression.joblib')","8d441873":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\",log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\",log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","a62282fb":"# # Hyper-parameters for SGD classifier\n# alpha = [10 ** x for x in range(-5, 2)]\n\n# # Read more about SGDClassifier() at\n# # http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n\n# log_error_array=[]\n# for i in alpha:\n#     clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n#     clf.fit(X_train, y_train)\n#     sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n#     sig_clf.fit(X_train, y_train)\n#     predict_y = sig_clf.predict_proba(X_test)\n#     log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_))\n#     print('Alpha:', i, \"Log loss:\", log_loss(y_test, predict_y, labels = clf.classes_, eps=1e-15))\n\n# fig, ax = plt.subplots()\n# ax.plot(alpha, log_error_array,c='g')\n# for i, txt in enumerate(np.round(log_error_array,3)):\n#     ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n# plt.grid()\n# plt.title(\"Cross Validation Error for each alpha\")\n# plt.xlabel(\"Alpha(s)\")\n# plt.ylabel(\"Error measure\")\n# plt.show()\n\n# best_alpha = np.argmin(log_error_array)\n# clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","f7fb00e4":"# Saving the Model\n# dump(sig_clf, 'linear_svm.joblib')\n\n# Loading the Model\nsig_clf = load('..\/input\/d\/elemento\/quora-question-pairs\/linear_svm.joblib')","5030096f":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\", log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\", log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","71e4b832":"# params = {}\n# params['objective'] = 'binary:logistic'\n# params['eval_metric'] = 'logloss'\n# params['eta'] = 0.02\n# params['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label = y_train)\nd_test = xgb.DMatrix(X_test, label = y_test)\n\n# watchlist = [(d_train, 'train'), (d_test, 'valid')]\n# bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)","a5f8c0ab":"# Saving the Model\n# dump(bst, 'xg_boost.joblib')\n\n# Loading the Model\nbst = load('..\/input\/d\/elemento\/quora-question-pairs\/xg_boost.joblib')","9ebe3f4f":"xgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"Test log loss:\", log_loss(y_test, predict_y, labels = [0, 1]))\npredicted_y = np.array(predict_y > 0.5, dtype=int)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","684aef5b":"# params = {\n#     'n_estimators': [50, 100, 150],\n#     'max_depth': [15, 20, None], \n#     'objective': ['binary:logistic'],\n#     'eval_metric': ['logloss'],\n# }\n\n# xgbr = xgb.XGBClassifier(verbosity = 1)\n# clf = RandomizedSearchCV(xgbr, params)\n# clf.fit(X_train, y_train)\n# print(clf.best_params_)","a9671bf3":"# The best hyper-parameters are as follows:\n# n_estimators = 150, max_depth = 15, eval_metric = logloss\n\n# Training XGBClassifier with the best hyper-parameters\n# xgbr = xgb.XGBClassifier(verbosity = 1, n_estimators = 150, max_depth = 15, \n#     objective = 'binary:logistic', eval_metric = 'logloss')\n# xgbr.fit(X_train, y_train)","336b0b0f":"# Saving the Model\n# dump(xgbr, 'xgb_classifier_hpt.joblib')\n\n# Loading the Model\nxgbr = load('..\/input\/d\/elemento\/quora-question-pairs\/xgb_classifier_hpt.joblib')","40059b1d":"predict_y = xgbr.predict_proba(X_train)\nprint(\"n_estimators: , max_depth: , Train log loss:\", log_loss(y_train, predict_y, labels=[0,1]))\npredict_y = xgbr.predict_proba(X_test)\nprint(\"n_estimators: , max_depth: , Test log loss:\", log_loss(y_test, predict_y, labels=[0,1]))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","ca93d1cb":"# # Using TF-IDF\n# df = pd.read_csv(\"..\/input\/quora-question-pairs\/train.csv.zip\")\n# df.dropna(axis = 0, subset = ['question1', 'question2'], inplace = True)\n# print(df.shape)\n# df.head()","17552f69":"# # Merge the Questions\n# questions = list(df['question1']) + list(df['question2'])\n\n# tfidf = TfidfVectorizer(min_df = 5, max_features = 500)\n# tfidf.fit(questions)\n\n# q1 = tfidf.transform(df['question1'])\n# q2 = tfidf.transform(df['question2'])\n# print(q1.shape, q2.shape)","a708281f":"# dfnlp = pd.read_csv(\"..\/input\/d\/elemento\/quora-question-pairs\/nlp_features_train.csv\",encoding='latin-1')\n# dfppro = pd.read_csv(\"..\/input\/d\/elemento\/quora-question-pairs\/df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n# df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n# df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n# df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n\n# # Creating a DataFrame for the TF-IDF feature vectors\n# df3_q1 = pd.DataFrame.sparse.from_spmatrix(q1, index = df3.index)\n# df3_q2 = pd.DataFrame.sparse.from_spmatrix(q2, index = df3.index)\n# print(df3_q1.shape, df3_q2.shape)","8727d04e":"# # Making the Dataset with TF-IDF feature vectors\n# df3_q1['id'] = df1['id']\n# df3_q2['id'] = df1['id']\n# df1  = df1.merge(df2, on='id', how='left')\n# df2  = df3_q1.merge(df3_q2, on='id', how='left')\n# result  = df1.merge(df2, on='id', how='left')\n# result.to_csv('final_features_tfidf.csv')","f23168b4":"# Loading the TF-IDF based Dataset\ndata = pd.read_csv(\"..\/input\/d\/elemento\/quora-question-pairs\/final_features_tfidf.csv\", nrows = 100000)\nprint(data.shape)\ndata.head()","035d8006":"y_true = data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id','is_duplicate'], axis=1, inplace=True)\nprint(data.shape)","d5af7976":"X_train, X_test, y_train, y_test = train_test_split(data, y_true, stratify = y_true, test_size = 0.3)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","bbd3b5f8":"# # Using the hyper-parameters obtained from the previous hyper-parameter tuning\n# clf = SGDClassifier(alpha=0.01, penalty='l2', loss='log', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","7031e626":"# Saving the Model\n# dump(sig_clf, 'logistic_regression_tfidf.joblib')\n\n# Loading the Model\nsig_clf = load('..\/input\/d\/elemento\/quora-question-pairs\/logistic_regression_tfidf.joblib')","2dc9193e":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\", log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\", log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","82132e03":"# # Using the hyper-parameters obtained from the previous hyper-parameter tuning\n# clf = SGDClassifier(alpha=0.01, penalty='l2', loss='hinge', random_state=42)\n# clf.fit(X_train, y_train)\n# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n# sig_clf.fit(X_train, y_train)","3a91e093":"# Saving the Model\n# dump(sig_clf, 'linear_svm_tfidf.joblib')\n\n# Loading the Model\nsig_clf = load('..\/input\/d\/elemento\/quora-question-pairs\/linear_svm_tfidf.joblib')","421ccfb5":"predict_y = sig_clf.predict_proba(X_train)\nprint(\"Best Alpha: 0.01, Train log loss:\", log_loss(y_train, predict_y, labels=sig_clf.classes_))\npredict_y = sig_clf.predict_proba(X_test)\nprint(\"Best Alpha: 0.01, Test log loss:\", log_loss(y_test, predict_y, labels=sig_clf.classes_))\npredicted_y = np.argmax(predict_y, axis=1)\nprint(\"Total number of data points:\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","c50cde63":"## 4.7 XGBoost","3901fbf1":"# Quora Question Pairs: Modeling\n- Hola amigos, this notebook covers my code for the Modeling of **Quora Question Pairs** challenge, which can be found [here](https:\/\/www.kaggle.com\/c\/quora-question-pairs).\n- This is the **second notebook**. Check out my [first notebook](https:\/\/www.kaggle.com\/elemento\/quora-questionpairs-eda-visualization), that covers the **EDA & Visualization** aspect of this competition.","df94fdd2":"# 5. Modeling Continues\n1. Perform hyperparameter tuning  of XgBoost models using RandomsearchCV with vectorizer as TF-IDF W2V  to reduce the log-loss.\n2. Try out models (Logistic regression, Linear-SVM) with simple TF-IDF vectors instead of TD_IDF weighted word2Vec.\n\n## 5.1 Hyper-parameter tuning of XGBoost with TF-IDF W2V","38ebd4a2":"# 4. Machine Learning Models\n## 4.1 Reading data from file and storing into sql table","c36b4849":"## 5.2 Trying out LR, Linear-SVM with TF-IDF","3b9b595c":"### 5.2.1 Logistic Regression","90d304ff":"### 5.2.2 Linear SVM","56589a36":"## 4.4 Building a random model (Finding worst-case log-loss)","cd399b48":"## 4.2 Random train test split( 70:30)","05222fd7":"## 4.6 Linear SVM with hyperparameter tuning","05037496":"## 4.5 Logistic Regression with hyperparameter tuning","6fad763f":"- We have a 218-dimensional dataset, which can be considered as neither very high, nor very low. In such cases, we can try both Linear Models as well as more complex models, like GBDT, to make sure that our linear models are not under-fitting.","209c2903":"# Installing & Importing Packages"}}