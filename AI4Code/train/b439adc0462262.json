{"cell_type":{"fa643506":"code","c29aeeee":"code","26d8c2b0":"code","dcd5e294":"code","e0d17d75":"code","2e0e9874":"code","017a77e9":"code","384e8d8e":"code","f36cf8dc":"code","46fb3173":"code","d22913e0":"code","f2458db3":"code","cd84ec5c":"code","2da0f2e6":"code","30250ba4":"code","186be87f":"code","849655a7":"code","2e6f779c":"code","20ba2ee8":"code","81e7327d":"code","25eea1f8":"code","ed8f3072":"code","52585abb":"code","fee5d525":"code","8af7dde4":"code","3bfb44e8":"code","e78f7223":"code","99ee1dec":"code","ccf41c58":"code","0fea0b06":"code","6993848c":"code","d8a51015":"code","b48519f2":"code","59f05155":"code","8050a2e3":"code","89259b28":"code","ff5e5009":"code","947d73f2":"code","319f28ca":"code","851770ab":"code","b90eb297":"code","c4919090":"code","2e20348f":"code","edb047e1":"code","0f599f39":"code","3b1b1d5c":"code","4e741024":"code","7e20b66d":"code","40132ef9":"code","ba7e1da7":"code","28884f50":"code","8655b32b":"code","5285d6ca":"code","2c6c4434":"code","95b7c19c":"code","c071a0e6":"code","17170bf2":"code","b917ea13":"code","11a73d9d":"code","0f7217c7":"code","c06d31a8":"code","23fc57fb":"code","8ebb4e02":"markdown","2413b122":"markdown","b5028778":"markdown","f57c1c76":"markdown","6d1781d3":"markdown","dcb2f70d":"markdown","a3749ae5":"markdown","2c66a27c":"markdown","751f9e09":"markdown","cfe5afc0":"markdown"},"source":{"fa643506":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n%matplotlib inline \nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nimport seaborn as sns\nsns.set(style=\"white\", color_codes=True)\nsns.set(font_scale=1.5)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nimport xgboost as xgb\n","c29aeeee":"df_train=pd.read_csv(r'..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ndf_test= pd.read_csv(r'..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')\n                     ","26d8c2b0":"df_train.head()","dcd5e294":"df_train.describe()","e0d17d75":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndf_train.hist(bins=10, figsize=(20,15))\nplt.show()\n","2e0e9874":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df_train, df_train['Loan_Status']):\n    strat_train_set = df_train.loc[train_index]\n    strat_test_set = df_train.loc[test_index]","017a77e9":"strat_test_set['Loan_Status'].value_counts()\/ len(strat_test_set)","384e8d8e":"df_train['Loan_Status'].value_counts()\/len(df_train)","f36cf8dc":"sns.FacetGrid(df_train,hue=\"Loan_Status\",height=5).map(plt.scatter,\"ApplicantIncome\",\"LoanAmount\").add_legend();\nplt.show()","46fb3173":"corr=df_train.corr\n\nsns.set(style=\"white\")\n\ncorr = df_train.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(15, 10))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","d22913e0":"df_train.columns","f2458db3":"# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',]\nscatter_matrix(df_train[attributes], figsize=(10, 6))\n","cd84ec5c":"strat_train_set.isnull().sum()","2da0f2e6":"strat_train_set.dtypes","30250ba4":"X_train=strat_train_set.drop(\"Loan_Status\",axis=1)\ny_train=strat_train_set['Loan_Status'].copy()","186be87f":"df_train['Loan_Amount_Term'].value_counts()","849655a7":"X_train['Loan_Amount_Term_Cat'] = pd.cut(x=X_train['Loan_Amount_Term'], bins=[6,119,239,359,480])","2e6f779c":"X_train['Loan_Amount_Term_Cat'].value_counts()","20ba2ee8":"X_train.head()","81e7327d":"X_train.columns","25eea1f8":"X_train_Num = X_train[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\nX_train_Cat = X_train[['Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed','Credit_History', 'Property_Area',\n       'Loan_Amount_Term_Cat']]","ed8f3072":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\nX_train_Num_tr = num_pipeline.fit_transform(X_train_Num)","52585abb":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder(sparse=False)\n\nCat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n     ])\nX_train_cat_tr = Cat_pipeline.fit_transform(X_train_Cat)","fee5d525":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(X_train_Num)\ncat_attribs = list(X_train_Cat)\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", Cat_pipeline, cat_attribs),\n    ])\n\nX_train_prepared = full_pipeline.fit_transform(X_train)","8af7dde4":"X_train_prepared.shape","3bfb44e8":"y_train.replace([\"Y\",\"N\"],[1,0],inplace=True)","e78f7223":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train_prepared, y_train)\n","99ee1dec":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\np1=forest_clf.predict(X_train_prepared)\nprint(confusion_matrix(y_train,p1))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_train,p1)))\nprint(\"Classification report: {}\".format(classification_report(y_train,p1)))","ccf41c58":"X_test=strat_test_set.drop(\"Loan_Status\",axis=1)\ny_test=strat_test_set['Loan_Status'].copy()","0fea0b06":"X_test['Loan_Amount_Term_Cat'] = pd.cut(x=X_test['Loan_Amount_Term'], bins=[6,119,239,359,480])","6993848c":"X_test_Num = X_test[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\nX_test_Cat = X_test[['Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed','Credit_History', 'Property_Area',\n       'Loan_Amount_Term_Cat']]","d8a51015":"num_pipeline2 = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\nX_test_Num_tr = num_pipeline2.fit_transform(X_test_Num)","b48519f2":"Cat_pipeline2 = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n     ])\nX_test_cat_tr = Cat_pipeline2.fit_transform(X_test_Cat)","59f05155":"num_attribs2 = list(X_test_Num)\ncat_attribs2 = list(X_test_Cat)\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline2, num_attribs2),\n        (\"cat\", Cat_pipeline2, cat_attribs2),\n    ])\n\nX_test_prepared = full_pipeline.fit_transform(X_test)","8050a2e3":"X_test_prepared.shape","89259b28":"X_test.shape","ff5e5009":"y_test.replace([\"Y\",\"N\"],[1,0],inplace=True)","947d73f2":"p2=forest_clf.predict(X_test_prepared)\nprint(confusion_matrix(y_test,p2))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,p2)))\nprint(\"Classification report: {}\".format(classification_report(y_test,p2)))","319f28ca":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(random_grid)","851770ab":"rf=RandomForestClassifier()\nrf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)\n### fit the randomized model\nrf_randomcv.fit(X_train_prepared,y_train)","b90eb297":"rf_randomcv.best_params_","c4919090":"rf_randomcv.best_estimator_","2e20348f":"best_random_grid=rf_randomcv.best_estimator_","edb047e1":"p3=best_random_grid.predict(X_test_prepared)\nprint(confusion_matrix(y_test,p3))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,p3)))\nprint(\"Classification report: {}\".format(classification_report(y_test,p3)))","0f599f39":"rf_randomcv.best_params_","3b1b1d5c":"## Defining  a range around the best parameters from the Random Search CV\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid1 = {\n    'criterion': [rf_randomcv.best_params_['criterion']],\n    'max_depth': [rf_randomcv.best_params_['max_depth']],\n    'max_features': [rf_randomcv.best_params_['max_features']],\n    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], \n                         rf_randomcv.best_params_['min_samples_leaf']+2, \n                         rf_randomcv.best_params_['min_samples_leaf'] + 4],\n    'min_samples_split': [rf_randomcv.best_params_['min_samples_split'] - 2,\n                          rf_randomcv.best_params_['min_samples_split'] - 1,\n                          rf_randomcv.best_params_['min_samples_split'], \n                          rf_randomcv.best_params_['min_samples_split'] +1,\n                          rf_randomcv.best_params_['min_samples_split'] + 2],\n    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100, \n                     rf_randomcv.best_params_['n_estimators'], \n                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]\n}\n\nprint(param_grid1)","4e741024":"rfc=RandomForestClassifier()\ngrid_search=GridSearchCV(estimator=rfc,param_grid=param_grid1,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train_prepared,y_train)","7e20b66d":"best_grid=grid_search.best_estimator_","40132ef9":"p4=best_grid.predict(X_test_prepared)\nprint(confusion_matrix(y_test,p4))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,p4)))\nprint(\"Classification report: {}\".format(classification_report(y_test,p4)))","ba7e1da7":"import pickle\n# open a file, where you ant to store the data\nfile = open('RFbestGridLat.pkl', 'wb')\n\n# dump information to that file\npickle.dump(best_grid, file)","28884f50":"\nfrom hyperopt import hp,fmin,tpe,STATUS_OK,Trials","8655b32b":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n    }","5285d6ca":"space","2c6c4434":"def objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_train_prepared, y_train, cv = 5).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }","95b7c19c":"from sklearn.model_selection import cross_val_score\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","c071a0e6":"crit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\n\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","17170bf2":"best['min_samples_leaf']","b917ea13":"trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]).fit(X_train_prepared,y_train)\npredictionforest = trainedforest.predict(X_test_prepared)\nprint(confusion_matrix(y_test,predictionforest))\nprint(accuracy_score(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc5 = accuracy_score(y_test,predictionforest)","11a73d9d":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nparam = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(param)","0f7217c7":"from tpot import TPOTClassifier","c06d31a8":"tpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict={'sklearn.ensemble.RandomForestClassifier': param}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(X_train_prepared,y_train)","23fc57fb":"accuracy = tpot_classifier.score(X_test_prepared, y_test)\nprint(accuracy)","8ebb4e02":"### Genetic Algorithms\nGenetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts.\n\nLet's immagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again calculate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.","2413b122":" ## Bayesian Optimization HyperOpt","b5028778":"A bunch of outliers, that needs to be taken care of","f57c1c76":"#### we see an increase of 5% in the accuracy! lets take a range of best params and fetch it in GridSearch CV and lets see if it helps","6d1781d3":"## Randomized SearchCV","dcb2f70d":"_____________________________________________________________________________________________","a3749ae5":"## GridSearch CV","2c66a27c":"### Best Accuracy ~ 86%","751f9e09":"{'n_estimators': 1400,\n 'min_samples_split': 5,\n 'min_samples_leaf': 6,\n 'max_features': 'sqrt',\n 'max_depth': 120,\n 'criterion': 'gini'}","cfe5afc0":"## Preparing Test Data"}}