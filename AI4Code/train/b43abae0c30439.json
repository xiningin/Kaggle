{"cell_type":{"1bdd79d9":"code","15b87824":"code","5554e26b":"code","432cd88c":"code","db7a94ad":"code","f104f2da":"code","235b736b":"code","ccb173c2":"code","c945e01c":"code","af2fea27":"code","eee3dbab":"code","b65f7559":"code","117f9283":"code","b2d20e17":"code","be48fcb3":"code","b3583a9f":"code","3d845004":"code","7854f922":"code","274996ac":"code","5e056d0c":"code","f34dcdab":"code","b3b6e4e3":"code","1835fcfe":"code","482ade65":"code","509f9323":"code","73071ab9":"code","4a4ebba5":"code","a4bcb616":"code","7153bf82":"code","9732e9ab":"code","f512c496":"code","5778677b":"code","b7dbd151":"code","20752966":"code","0bb1b05b":"code","237f0d9c":"code","555d372d":"code","502aaf7b":"code","471b6d96":"code","77a977bc":"code","71c2dea2":"code","07aa75df":"code","e3ace80a":"code","d53935c1":"code","b00f4d58":"code","d9b1382f":"code","9ca79e01":"code","a6f8a731":"code","55f340f7":"code","e91daf72":"code","09dce501":"code","74e6b864":"code","5bdbf597":"code","bc6aaccd":"code","9e4deb11":"code","d12492ca":"code","4c4a0f7e":"code","f0fed82d":"code","3ef96b8d":"code","7ea80651":"code","e3e7afa3":"code","7f950d46":"code","8f0c386b":"code","7c23149f":"code","d39e4cb3":"code","58fdfb39":"code","4b1d3f5c":"code","3f554b43":"code","0d690d91":"code","6b54a68f":"code","b3699bc1":"code","afd82652":"code","f4f762d7":"code","32dd77eb":"code","64897b80":"code","11e6019c":"code","d3ff8503":"code","c80f4218":"code","32b4233c":"code","81e64cb3":"code","3c706274":"markdown","d91859b2":"markdown","fb7c5a9b":"markdown","7b5e80c0":"markdown","d2ae38d4":"markdown","6bd3efab":"markdown","a21353f9":"markdown","c5cae518":"markdown","56da7edc":"markdown","ef677186":"markdown","18eb8a98":"markdown","c28f130c":"markdown","05468eab":"markdown","6288d671":"markdown","66ed6ca4":"markdown"},"source":{"1bdd79d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15b87824":"import matplotlib.pyplot as plt\nimport seaborn as sns","5554e26b":"df = pd.read_json('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines=True)","432cd88c":"df.head()","db7a94ad":"df.info()","f104f2da":"df.dtypes","235b736b":"df.shape #28619 \u0441\u0442\u0440\u043e\u043a \u0438 3 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432","ccb173c2":"df.is_sarcastic.unique() #array([1,  0])","c945e01c":"df.describe() #\u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u043f\u043e \u0432\u0441\u0435\u043c \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c","af2fea27":"df.is_sarcastic.value_counts()","eee3dbab":"df['source'] = df.article_link.apply(lambda x: x.split('.')[1])\ndf = df.drop('article_link', axis=1)\ndf.shape","b65f7559":"df.head()","117f9283":"df.source.unique()","b2d20e17":"df.source.value_counts()","be48fcb3":"all_words = df['headline'].str.split(expand=True).unstack().value_counts()","b3583a9f":"all=pd.DataFrame(all_words).reset_index()","3d845004":"sns.set(rc={'figure.figsize':(17,11)})\nsns.barplot(x=all['index'][0:50], y=all[0][0:50], data=all,palette=\"ch:.25\")\nplt.xlabel('Words')  # Add an x-label to the axes.\nplt.ylabel('Count')  # Add a y-label to the axes.\nplt.title(\"Frequent Occuring words in Headlines\") \nplt.xticks(rotation=80);","7854f922":"all_words = df['headline'].str.split(expand=True).unstack().value_counts()","274996ac":"non_sarc=pd.DataFrame(df[df['is_sarcastic']==0]['headline'].str.split(expand=True).unstack().value_counts()).reset_index()","5e056d0c":"sarc=pd.DataFrame(df[df['is_sarcastic']==1]['headline'].str.split(expand=True).unstack().value_counts()).reset_index()","f34dcdab":"sns.set(rc={'figure.figsize':(17,11)})\nsns.barplot(x=non_sarc['index'][0:50], y=non_sarc[0][0:50], data=non_sarc,palette=\"ch:.25\")\nplt.xlabel('Words')  # Add an x-label to the axes.\nplt.ylabel('Count')  # Add a y-label to the axes.\nplt.title(\"Frequent Occuring non-sarcastic words in Headlines\") \nplt.xticks(rotation=80);","b3b6e4e3":"sns.set(rc={'figure.figsize':(17,11)})\nsns.barplot(x=sarc['index'][0:50], y=sarc[0][0:50], data=sarc,palette=\"ch:.25\")\nplt.xlabel('Words')  # Add an x-label to the axes.\nplt.ylabel('Count')  # Add a y-label to the axes.\nplt.title(\"Frequent Occuring sarcastic words in Headlines\") \nplt.xticks(rotation=80);","1835fcfe":"import nltk\nnltk.download('stopwords')\nnltk.download(\"wordnet\")","482ade65":"nltk.download('punkt')","509f9323":"from nltk.corpus import wordnet","73071ab9":"sar_tokens = df[df['is_sarcastic'] == 1]['headline'].apply(lambda x: nltk.word_tokenize(x))  #don't like it because of \"'s'\" and so on","4a4ebba5":"nonsar_tokens = df[df['is_sarcastic'] == 0]['headline'].apply(lambda x: nltk.word_tokenize(x))","a4bcb616":"stopwords = nltk.corpus.stopwords.words('english')","7153bf82":"sarc2=[each_word.lower() for each_word in sarc['index']] #lowcase","9732e9ab":"sarc_nostop = [word for word in sarc2 if word not in stopwords]","f512c496":"non_sarc2=[each_word.lower() for each_word in non_sarc['index']]","5778677b":"non_sarc_nostop = [word for word in non_sarc2 if word not in stopwords]","b7dbd151":"from wordcloud import WordCloud, ImageColorGenerator","20752966":"plt.figure(figsize = (10,10))\nwc = WordCloud(width = 2000 , height = 1000 , max_words = 500).generate(' '.join(word for word in sarc_nostop))\nplt.axis(\"off\")\nplt.title('Worcloud of sarcastic words')\nplt.imshow(wc , interpolation = 'bilinear')","0bb1b05b":"plt.figure(figsize = (10,10))\nwc = WordCloud(width = 2000 , height = 1000 , max_words = 500).generate(' '.join(word for word in non_sarc_nostop))\nplt.axis(\"off\")\nplt.title('Worcloud of non-sarcastic words')\nplt.imshow(wc , interpolation = 'bilinear')","237f0d9c":"df['text_len'] = df['headline'].apply(lambda x: len(x.split(' ')))","555d372d":"df[df['text_len']>140]","502aaf7b":"df['headline'][7302]","471b6d96":"plt.figure(figsize = (7 ,10))\nsns.boxplot(y='text_len', x=\"is_sarcastic\", data=df)\ny=df['text_len']\nplt.yticks(np.arange(0, max(y), 10));","77a977bc":"import re\ndef parser(x):\n    x = re.sub('[^a-z\\s]', '', x.lower())  #drop any symbol except a-z\n    x = [w for w in x.split() if w not in set(stopwords)]\n    x = [w for w in x if wordnet.synsets(w)]\n    return ' '.join(x)\n\ndf['headline'] = df.headline.apply(lambda x: parser(x))","71c2dea2":"X = df['headline']\ny = df.is_sarcastic","07aa75df":"# 2. Split into Training and Test data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)","e3ace80a":"from sklearn.feature_extraction.text import CountVectorizer","d53935c1":"cv = CountVectorizer(ngram_range=(1,3))\nX_cv_train = cv.fit_transform(X_train)\nX_cv_test = cv.transform(X_test)","b00f4d58":"from sklearn.tree import DecisionTreeClassifier","d9b1382f":"tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)\ntree_model.fit(X_cv_train, y_train)","9ca79e01":"from sklearn import tree\ntree.plot_tree(tree_model)\nplt.savefig('hey.pdf')\nplt.show()","a6f8a731":"from sklearn.metrics import confusion_matrix, classification_report","55f340f7":"y_pred = tree_model.predict(X_cv_test)\ntest = np.array(y_test)\npredictions = np.array(y_pred)\nconfusion_matrix(test, predictions)","e91daf72":"from sklearn.metrics import accuracy_score, f1_score","09dce501":"print('accuracy:', accuracy_score(y_test,y_pred))\nprint('f1-score:', f1_score(y_test,y_pred))","74e6b864":"acc_tree = accuracy_score(y_test,y_pred)","5bdbf597":"from sklearn.linear_model import LogisticRegression","bc6aaccd":"logreg = LogisticRegression()\nlogreg.fit(X_cv_train, y_train)","9e4deb11":"logreg.score(X_cv_train, y_train)","d12492ca":"y_pred_logr = logreg.predict(X_cv_test)\nprint('accuracy:', accuracy_score(y_test,y_pred_logr))\nprint('f1-score:', f1_score(y_test,y_pred_logr))","4c4a0f7e":"acc_logr = accuracy_score(y_test,y_pred_logr)","f0fed82d":"from sklearn import svm","3ef96b8d":"model = svm.SVC(kernel='linear', gamma=1) \nmodel.fit(X_cv_train, y_train)","7ea80651":"model.score(X_cv_train, y_train)","e3e7afa3":"y_pred_svm= model.predict(X_cv_test)\nprint('accuracy:', accuracy_score(y_test,y_pred_svm))\nprint('f1-score:', f1_score(y_test,y_pred_svm))","7f950d46":"acc_svm = accuracy_score(y_test,y_pred_svm)","8f0c386b":"from sklearn.ensemble import RandomForestClassifier","7c23149f":"forest = RandomForestClassifier (criterion='gini',\n                                n_estimators=12, \n                                random_state=1)\nforest.fit(X_cv_train, y_train)","d39e4cb3":"forest.score(X_cv_train, y_train)","58fdfb39":"y_pred_forest= forest.predict(X_cv_test)\nprint('accuracy:', accuracy_score(y_test,y_pred_forest))\nprint('f1-score:', f1_score(y_test,y_pred_forest))","4b1d3f5c":"acc_rf = accuracy_score(y_test,y_pred_forest)","3f554b43":"from sklearn.naive_bayes import MultinomialNB","0d690d91":"nb=MultinomialNB()\nnb.fit(X_cv_train, y_train)","6b54a68f":"nb.score(X_cv_train, y_train)","b3699bc1":"y_pred_nb= nb.predict(X_cv_test)\nprint('accuracy:', accuracy_score(y_test,y_pred_nb))\nprint('f1-score:', f1_score(y_test,y_pred_nb))","afd82652":"print(classification_report(y_test,y_pred_nb))","f4f762d7":"acc_nb = accuracy_score(y_test,y_pred_nb)","32dd77eb":"from sklearn.neighbors import KNeighborsClassifier","64897b80":"knn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_cv_train, y_train)","11e6019c":"y_pred_knn= knn.predict(X_cv_test)\nprint('accuracy:', accuracy_score(y_test,y_pred_knn))\nprint('f1-score:', f1_score(y_test,y_pred_knn))","d3ff8503":"acc_knn = accuracy_score(y_test,y_pred_knn)","c80f4218":"models = ['DesicionTree','LogReg','SVM', 'RandomForest', 'NaiveBayes', 'kNN']\ncol = [acc_tree, acc_logr, acc_svm, acc_rf, acc_nb, acc_knn]\ndata = {'Models':models,'Accuracy':col}\ngraph_df = pd.DataFrame(data)\ngraph_df","32b4233c":"graph_df = graph_df.sort_values(by=['Accuracy'], axis = 0, ascending=False)","81e64cb3":"fig, ax = plt.subplots()\nsns.barplot(x=graph_df['Models'], y=graph_df['Accuracy'], data=graph_df);","3c706274":"### Comparison of the length of sarcastic and non-sarcastic headlines","d91859b2":"### SVM","fb7c5a9b":"### kNN","7b5e80c0":"#### Stop words removal","d2ae38d4":"### Visualize with WordCloud","6bd3efab":"### Logistic regression","a21353f9":"### Naive Bayes","c5cae518":"There are some diffencies between sarcastic and non-sarcastic classes in the length of headlines. Non-sarcastic headlines are more similar in the length, whereas the longest headings tend to be satirical. ","56da7edc":"#### One more way to get words","ef677186":"### Comparison of models' accuracy","18eb8a98":"### DecisionTreeClassifier","c28f130c":"Classes of is_sarcastic column are well-balanced and there are no nulls","05468eab":"### Random Forest","6288d671":"### X and Y preparing","66ed6ca4":"Parse the data removing stopwords and non-alpha-numeric characters and words containing them from the headlines"}}