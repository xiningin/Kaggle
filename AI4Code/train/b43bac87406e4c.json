{"cell_type":{"f85a93a4":"code","b36eb17c":"code","22b05c0f":"code","76c43cbd":"code","f63297ea":"code","d801b7cd":"code","98c6e688":"code","e9388252":"code","aece9e50":"code","e022773e":"code","85fb0f7d":"code","399caa23":"code","55b53350":"code","7df3ba82":"code","4303e5bd":"code","0a967d0f":"code","efb28a74":"code","6ba87ea0":"code","34c659c0":"code","8510c4f6":"code","e08dc46d":"code","b320bd3f":"code","f1dcaa57":"code","f9c51ff8":"markdown","ef02b5bb":"markdown","e10d0bbb":"markdown","08f72eb3":"markdown","ea727b09":"markdown","4d98907e":"markdown","d0080717":"markdown","35c56623":"markdown","ca03c996":"markdown","9cc145ce":"markdown","f28fbca9":"markdown","d5dd8ff4":"markdown","0fdbc4ba":"markdown","e0da2032":"markdown","2776bb59":"markdown","de8e2662":"markdown","93eddd3d":"markdown","9fa8afa8":"markdown","f4e4e58a":"markdown","d658fd61":"markdown","bf74502b":"markdown","15d7c498":"markdown","e677e520":"markdown","a7d19c4e":"markdown","50492d58":"markdown","9edf9982":"markdown","e9f2ca6f":"markdown","0136af38":"markdown"},"source":{"f85a93a4":"#!pip install scikit-surprise\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise import  SVD, NormalPredictor, KNNBasic\nfrom surprise.model_selection import cross_validate\nfrom surprise import accuracy\nfrom IPython.display import display\nfrom collections import defaultdict\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import train_test_split","b36eb17c":"print(os.path.abspath(os.curdir))","22b05c0f":"col_names = ['user_id', 'item_id', 'rating', 'timestamp']\ndata = pd.read_table('..\/input\/u-data\/u.data', names=col_names)\ndata = data.drop('timestamp', axis=1)\ndata.info()","76c43cbd":"data.head()","f63297ea":"data.rating.hist(bins=5)\nplt.show()","d801b7cd":"data.rating.hist()\nplt.show()","98c6e688":"num_ratings = float(len(data))\nnum_movies = float(len(np.unique(data[\"item_id\"])))\nnum_users = float(len(np.unique(data[\"user_id\"])))\nsparsity = (num_ratings \/ (num_movies * num_users)) * 100.0\nprint(\"Sparsity of Dataset is\", sparsity, \"Percent\")","e9388252":"#coverting data in to surprise dataset\nreader = Reader(rating_scale=(1,5))\ndata = Dataset.load_from_df(data[['user_id', 'item_id', 'rating']], reader)","aece9e50":"#Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\nmodel_random = NormalPredictor()","e022773e":"model_using_normal_predictor = cross_validate(model_random, data,measures=['RMSE'],cv=5, verbose=False)","85fb0f7d":"print('Average RMSE for Test Set using {} is '.format(model_random.__class__.__name__),model_using_normal_predictor['test_rmse'].mean())","399caa23":"model_user_based = KNNBasic(sim_options ={'name':'cosine','user_base':True},verbose=False)\nmodel_using_KNNbasic_cos_user = cross_validate(model_user_based,data,measures=['RMSE'],cv=5, verbose=False)","55b53350":"print('Average RMSE for Test Set using {} is '.format(model_user_based.__class__.__name__),model_using_KNNbasic_cos_user['test_rmse'].mean())","7df3ba82":"model_item_based = KNNBasic(sim_options ={'name':'cosine','user_base':False},verbose=False)\nmodel_using_KNNbasic_cos_item = cross_validate(model_item_based,data,measures=['RMSE'],cv=5, verbose=False)","4303e5bd":"print('Average RMSE for Test Set using {} is '.format(model_item_based.__class__.__name__),model_using_KNNbasic_cos_item['test_rmse'].mean())","0a967d0f":"#matrix factorisation using svd\nmodel_svd = SVD()\nmodel_using_svd = cross_validate(model_svd,data,measures=['RMSE'],cv=5, verbose=False)","efb28a74":"print('Average RMSE for Test Set using {} is '.format(model_svd.__class__.__name__),model_using_svd['test_rmse'].mean())","6ba87ea0":"#function can be found on surprise documentation FAQs\ndef precision_recall_at_k(predictions, k=10, threshold=3.5):\n    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n\n        precisions[uid] = n_rel_and_rec_k \/ n_rec_k if n_rec_k != 0 else 0\n\n        # Recall@K: Proportion of relevant items that are recommended\n        # When n_rel is 0, Recall is undefined. We here set it to 0.\n\n        recalls[uid] = n_rel_and_rec_k \/ n_rel if n_rel != 0 else 0\n\n    return precisions, recalls","34c659c0":"#A basic cross-validation iterator.\nkf = KFold(n_splits=5)","8510c4f6":"# Make list of k values\nK = [5, 10]\n\n# Make list of models\nmodels = [model_random, model_user_based, model_item_based, model_svd]\n\nfor k in K:\n    for model in models:\n        print('> k={}, model={}'.format(k,model.__class__.__name__))\n        p = []\n        r = []\n        for trainset, testset in kf.split(data):\n            model.fit(trainset)\n            predictions = model.test(testset, verbose=False)\n            precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5)\n\n            # Precision and recall can then be averaged over all users\n            p.append(sum(prec for prec in precisions.values()) \/ len(precisions))\n            r.append(sum(rec for rec in recalls.values()) \/ len(recalls))\n        \n        print('-----> Precision: ', round(sum(p) \/ len(p), 3))\n        print('-----> Recall: ', round(sum(r) \/ len(r), 3))","e08dc46d":"#function can be found on surprise documentation FAQs\ndef get_top_n(predictions, n=10):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","b320bd3f":"trainset = data.build_full_trainset()\ntestset = trainset.build_anti_testset()","f1dcaa57":"models = [model_random, model_user_based, model_item_based, model_svd]\nfor model in models:\n    model.fit(trainset)\n    predictions = model.test(testset)\n    top_n = get_top_n(predictions, n=5)\n    # Print the first one\n    user = list(top_n.keys())[0]\n    print(f'model: {model.__class__.__name__}, {user}: {np.round(top_n[user],2)}')","f9c51ff8":"## 3.\tThe matrix factorization model is different from the collaborative filtering models. Briefly describe this difference. Also, compare the RMSE again. Does it improve? Can you offer any reasoning as to why that might be?","ef02b5bb":"## Top-n Predictions\n\nFinally, we want to actually see what ratings the model predicts for our users. We can vary the amount of top movies we see per user by varying the value of n.\n\n <a href=https:\/\/surprise.readthedocs.io\/en\/stable\/FAQ.html> Documentation for Surprise<\/a>","e10d0bbb":"## Cross Validation\n\n\nWe will be using cross validation a lot in this code in the training and evaluation of our models. This strategy builds upon the idea of a train-test split, which you should already be familiar with.\n\nInstead of doing 1 data split, though, we will do several of them. Each split of the data is called a fold. We let k denote the number of folds we use. k=5 is a common number to use.\n\nThis image provides a visual explanation of how cross validation works.\n\n<center><img src =\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1c\/K-fold_cross_validation_EN.jpg\"><\/img><\/center>","08f72eb3":"## Precision\nPrecision attempts to answer the following question:\n\n> What proportion of positive identifications was actually correct?\n\nPrecision is defined as follows:\n\n<center><img src='https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/prec.png'><\/center>","ea727b09":"**Ans:**\n\n> Precision quantifies the number of positive class predictions that actually belong to the positive class. \n\n> Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. \n\n*These values are lowest for Random. Collaborative Filtering performed well in both the k values with Precision value ~70% and with k=5, ~73%.*\n*(The values while creating the notebook were different and hence might be different from those above)*\n\n*SVD has better RMSE but Collaborative Filtering using Item-Item or User-User have better Precision & Recall.*\n\n*RMSE values are used for Continuous d-type while Precision-Recall are calculated for categorical d-type using Confusion matrix. Thus cannot be compared directly.*","4d98907e":"## Recall\nRecall attempts to answer the following question:\n\n> What proportion of actual positives was identified correctly?\n\nMathematically, recall is defined as follows:\n\n<center><img src='https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/recall.png'><\/center>","d0080717":"*We will use KNNBasic and add parameter 'cosine'*","35c56623":"## 4.\tCompute the precision and recall, for each of the 4 models, at k = 5 and 10. This is 2 x 2 x 4 = 16 numerical values. Do you note anything interesting about these values? Anything different from the RMSE values you computed above?","ca03c996":"## 5.\tDo the top n predictions that you received make sense? What is the rating value (1-5) of these predictions? How could you use these predictions in the real-world if you were trying to build a generic content recommender system for a company?","9cc145ce":"**Ans:**\n\n*The dataset has 100000 rows and 3 columns. There are no Nulls in the dataset. The ratings are ranging from 1 to 5*.\n\n*Data seems to be highly skewed towards rating '4'. Rating '3' is second highest in count folowed by Rating '5'*.","f28fbca9":"## Model 2: User-Based Collaborative Filtering\n\nSurely, we can do much better than guessing the movie ratings randomly! Our next model will use the user-user defined notion of similarity to implement collaborative filtering.\n\nUser-Based Collaborative Filtering is a technique used to predict the items that a user might like on the basis of ratings given to that item by the other users who have similar taste with that of the target user.\nMany websites use collaborative filtering for building their recommendation system.","d5dd8ff4":"## Model 3: Item-Based Collaborative Filtering\n\nOur next model will use the item-item defined notion of similarity to once again implement collaborative filtering.\n\nHere, we explore the relationship between the pair of items (the user who bought Y, also bought Z). We find the missing rating with the help of the ratings given to the other items by the user.\n\nIt was first invented and used by Amazon in 1998. Rather than matching the user to similar customers, item-to-item collaborative filtering matches each of the user\u2019s purchased and rated items to similar items, then combines those similar items into a recommendation list. Now, let us discuss how it works.","0fdbc4ba":"## Reading the Data","e0da2032":"**Ans:**\n\n*As expected, we have got the predictions (n=5) for each model. We printed the predictions for One user for each model. Collaborative models  (User-based & Item-based) have given exactly same recommendations.*\n\n*The rating values for Random, User-based,and Item-based is fixed at 5.*\n*The rating values for Matrix Factorization using SVD have a mean of 4.474 and a standard deviation of 0.1.* \n\n*(The value can differ everytime the code is executed. The present value might be different owing to different results while training and testing)*\n\n*Recommender systems are used by E-commerce portals to recommend products to their customers. The products can be recommended based on the top overall sellers on a site, based on the demographics of the customer, or based on an analysis of the past buying behavior of the customer as a prediction for future buying behavior.*","2776bb59":"**Ans:**\n\n*Collaborative Filtering searches for neighbors based on similarity of product (example) preferences and recommend product that those neighbors bought\/reviewed while Matrix factorization works by decomposing the user-item matrix into the product of two lower dimensionality rectangular matrices.*\n\n*RMSE for Matrix Factorization is slightly better than the Collaborative Filtering Models.*\n\n*Matrix Factorization has lower RMSE due to the reason that it assumes \u00a0that \u00a0both product and users are present in some low dimensional space describing their properties and recommend a product based on its proximity to the user in the latent space. Implying it accounts for latent factors as well*","de8e2662":"## 2.\tCompare the results from the user-user and item-item models. How do they compare to each other? How do they compare to our original \"random\" model? Can you provide any intuition as to why the results came out the way they did?","93eddd3d":"## Model 4: Matrix Factorization\n\nOur final model for this case study will use the matrix factorization approach with the SVD algorithm to try to predict user\u2019s movie ratings. Here, we try to determine some underlying mathematical structure in the user rating matrix, which can help us predict missing ratings in the future.\n\nMatrix factorization is a way to generate latent features when multiplying two different kinds of entities. Collaborative filtering is the application of matrix factorization to identify the relationship between items\u2019 and users\u2019 entities. With the input of users\u2019 ratings on the shop items, we would like to predict how the users would rate the items so the users can get the recommendation based on the prediction.","9fa8afa8":"### We will use \"cross_validate\" from surprise package to run the models as listed and check their respective RMSEs","f4e4e58a":"# <center>  Multi-Model Movie Recommendation System Analysis <\/center>","d658fd61":"## Model 1: Random\n\nWe want to first get a baseline value for our model. What better way to do that than with a random algorithm! Essentially, this first algorithm is not personalized to the desires of any users - we just assign them movie ratings based on the initial distribution of the data.","bf74502b":"<H3 style =\"text-align:center\"><strong> Happy Learning<\/strong><\/H3>","15d7c498":"## Import\n\n### One of the first steps in any data science task is importing the necessary tools you will use.","e677e520":"**Ans:**\n*User-based and Item-based Collaborative Models have nearly same RMSE values while the \"random\" model's RMSE is highest. Clearly, Collaborative Filtering Models have performed better than random model.* \n\n*The Collaborative Models use the user-item-ratings data to find similarities and make predictions rather than just predicting a random rating based on the distribution of the data. This could a reason why the Collaborative Models performed well.*","a7d19c4e":"## 1. Describe the dataset. How many ratings are in the dataset? How would you describe the distribution of ratings? Is there anything else we should observe? Make sure the histogram is visible in the notebook. ","50492d58":"**In AI inference and machine learning, sparsity refers to a matrix of numbers that includes many zeros or values that will not significantly impact a calculation.**","9edf9982":"## Precision and Recall @ k\n\nRMSE is not the only metric we can use here. We can also examine two fundamental measures, precision and recall. We also add a parameter k which is helpful in understanding problems with multiple rating outputs.","e9f2ca6f":"**Greetings to everyone!**\n\n*The aim of this notebook is to create a recommendation system by first beginning with data exploration and then using 4 different models to create a recommender system.*\n\n*It also answers some key questions to be considered when comparing different models of recommendation systems!*\n\n\n\n> Model 1: Random\n\n> Model 2: User-Based Collaborative Filtering\n\n> Model 3: Item-Based Collaborative Filtering\n\n> Model 4: Matrix Factorization","0136af38":"## Computing the sparsity of the dataset"}}