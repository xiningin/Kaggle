{"cell_type":{"6c29b91e":"code","6c178826":"code","9b00a108":"code","23d8dfca":"code","787376bc":"code","3f525546":"code","b4813233":"code","36d36812":"code","a88babd2":"code","5b917145":"code","6cdd7bcb":"code","dc1ed6c7":"code","75fddbd7":"code","ac45169a":"code","81095dc1":"code","21c503ce":"code","41e5bca9":"code","bfe92f83":"code","97a4b2a9":"markdown","28921a6b":"markdown","b12bfb57":"markdown","27b27a47":"markdown","255bfa16":"markdown","ecb2dc73":"markdown","ef00e54b":"markdown","b1bfc92e":"markdown"},"source":{"6c29b91e":"# run in Console\n# !apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools","6c178826":"!pip install tiatoolbox","9b00a108":"import numpy as np\nimport pathlib\nimport os\nimport shutil\n\n\ndef remap_label(pred, by_size=False):\n    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3] \n    not [0, 2, 4, 6]. The ordering of instances (which one comes first) \n    is preserved unless by_size=True, then the instances will be reordered\n    so that bigger nucler has smaller ID.\n\n    Args:\n        pred (ndarray): the 2d array contain instances where each instances is marked\n            by non-zero integer.\n        by_size (bool): renaming such that larger nuclei have a smaller id (on-top).\n\n    Returns:\n        new_pred (ndarray): Array with continguous ordering of instances.\n\n    \"\"\"\n    pred_id = list(np.unique(pred))\n    pred_id.remove(0)\n    if len(pred_id) == 0:\n        return pred  # no label\n    if by_size:\n        pred_size = []\n        for inst_id in pred_id:\n            size = (pred == inst_id).sum()\n            pred_size.append(size)\n        # sort the id by size in descending order\n        pair_list = zip(pred_id, pred_size)\n        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n        pred_id, pred_size = zip(*pair_list)\n\n    new_pred = np.zeros(pred.shape, np.int32)\n    for idx, inst_id in enumerate(pred_id):\n        new_pred[pred == inst_id] = idx + 1\n    return new_pred\n\n\ndef cropping_center(x, crop_shape, batch=False):\n    \"\"\"Crop an array at the centre with specified dimensions.\"\"\"\n    orig_shape = x.shape\n    if not batch:\n        h0 = int((orig_shape[0] - crop_shape[0]) * 0.5)\n        w0 = int((orig_shape[1] - crop_shape[1]) * 0.5)\n        x = x[h0 : h0 + crop_shape[0], w0 : w0 + crop_shape[1]]\n    else:\n        h0 = int((orig_shape[1] - crop_shape[0]) * 0.5)\n        w0 = int((orig_shape[2] - crop_shape[1]) * 0.5)\n        x = x[:, h0 : h0 + crop_shape[0], w0 : w0 + crop_shape[1]]\n    return x\n\n\ndef rm_n_mkdir(dir_path):\n    \"\"\"Remove and make directory.\"\"\"\n    if os.path.isdir(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n\ndef rmdir(dir_path):\n    if os.path.isdir(dir_path):\n        shutil.rmtree(dir_path)\n    return\n\n\ndef recur_find_ext(root_dir, ext_list):\n    \"\"\"Recursively find all files in directories end with the `ext` such as `ext='.png'`.\n\n    Args:\n        root_dir (str): Root directory to grab filepaths from.\n        ext_list (list): File extensions to consider.\n\n    Returns:\n        file_path_list (list): sorted list of filepaths.\n    \"\"\"\n    file_path_list = []\n    for cur_path, dir_list, file_list in os.walk(root_dir):\n        for file_name in file_list:\n            file_ext = pathlib.Path(file_name).suffix\n            if file_ext in ext_list:\n                full_path = os.path.join(cur_path, file_name)\n                file_path_list.append(full_path)\n    file_path_list.sort()\n    return file_path_list\n\n\ndef rm_n_mkdir(dir_path):\n    \"\"\"Remove and then make a new directory.\"\"\"\n    if os.path.isdir(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n\ndef get_bounding_box(img):\n    \"\"\"Get the bounding box coordinates of a binary input- assumes a single object.\n\n    Args:\n        img: input binary image.\n\n    Returns:\n        bounding box coordinates\n\n    \"\"\"\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    # due to python indexing, need to add 1 to max\n    # else accessing will be 1px in the box, not out\n    rmax += 1\n    cmax += 1\n    return [rmin, rmax, cmin, cmax]\n","23d8dfca":"import math\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# from .utils import cropping_center\n\n\nclass PatchExtractor(object):\n    \"\"\"Extractor to generate patches with or without padding.\n    Turn on debug mode to see how it is done.\n\n    Args:\n        x: input image, should be of shape HWC\n        win_size: a tuple of (h, w).\n        step_size: a tuple of (h, w).\n        debug: flag to see how it is done.\n        \n    Returns:\n        a list of sub patches, each patch has dtype same as x.\n\n    Examples:\n        >>> xtractor = PatchExtractor(450, 120)\n        >>> img = np.full([1200, 1200, 3], 255, np.uint8)\n        >>> patches = xtractor.extract(img, 'mirror')\n\n    \"\"\"\n\n    def __init__(self, win_size, step_size, debug=False):\n        assert isinstance(win_size, int)\n        assert isinstance(step_size, int)\n\n        self.patch_type = \"mirror\"\n        self.win_size = [win_size, win_size]\n        self.step_size = [step_size, step_size]\n        self.debug = debug\n        self.counter = 0\n\n    def __get_patch(self, x, ptx):\n        pty = (ptx[0] + self.win_size[0], ptx[1] + self.win_size[1])\n        win = x[ptx[0] : pty[0], ptx[1] : pty[1]]\n        assert (\n            win.shape[0] == self.win_size[0] and win.shape[1] == self.win_size[1]\n        ), \"[BUG] Incorrect Patch Size {0}\".format(win.shape)\n        if self.debug:\n            if self.patch_type == \"mirror\":\n                cen = cropping_center(win, self.step_size)\n                cen = cen[..., self.counter % 3]\n                cen.fill(150)\n            cv2.rectangle(x, ptx, pty, (255, 0, 0), 2)\n            plt.imshow(x)\n            plt.show(block=False)\n            plt.pause(1)\n            plt.close()\n            self.counter += 1\n        return win\n\n    def __extract_valid(self, x):\n        \"\"\"Extracted patches without padding, only work in case win_size > step_size.\n        \n        Note: to deal with the remaining portions which are at the boundary a.k.a\n        those which do not fit when slide left->right, top->bottom), we flip \n        the sliding direction then extract 1 patch starting from right \/ bottom edge. \n        There will be 1 additional patch extracted at the bottom-right corner.\n\n        Args:\n            x: input image, should be of shape HWC.\n            win_size: a tuple of (h, w).\n            step_size: a tuple of (h, w).\n            \n        Returns:\n            a list of sub patches, each patch is same dtype as x.\n\n        \"\"\"\n        im_h = x.shape[0]\n        im_w = x.shape[1]\n\n        def extract_infos(length, win_size, step_size):\n            flag = (length - win_size) % step_size != 0\n            last_step = math.floor((length - win_size) \/ step_size)\n            last_step = (last_step + 1) * step_size\n            return flag, last_step\n\n        h_flag, h_last = extract_infos(im_h, self.win_size[0], self.step_size[0])\n        w_flag, w_last = extract_infos(im_w, self.win_size[1], self.step_size[1])\n\n        sub_patches = []\n        #### Deal with valid block\n        for row in range(0, h_last, self.step_size[0]):\n            for col in range(0, w_last, self.step_size[1]):\n                win = self.__get_patch(x, (row, col))\n                sub_patches.append(win)\n        #### Deal with edge case\n        if h_flag:\n            row = im_h - self.win_size[0]\n            for col in range(0, w_last, self.step_size[1]):\n                win = self.__get_patch(x, (row, col))\n                sub_patches.append(win)\n        if w_flag:\n            col = im_w - self.win_size[1]\n            for row in range(0, h_last, self.step_size[0]):\n                win = self.__get_patch(x, (row, col))\n                sub_patches.append(win)\n        if h_flag and w_flag:\n            ptx = (im_h - self.win_size[0], im_w - self.win_size[1])\n            win = self.__get_patch(x, ptx)\n            sub_patches.append(win)\n        return sub_patches\n\n    def __extract_mirror(self, x):\n        \"\"\"Extracted patches with mirror padding the boundary such that the \n        central region of each patch is always within the orginal (non-padded)\n        image while all patches' central region cover the whole orginal image.\n\n        Args:\n            x: input image, should be of shape HWC.\n            win_size: a tuple of (h, w).\n            step_size: a tuple of (h, w).\n            \n        Returns:\n            a list of sub patches, each patch is same dtype as x.\n            \n        \"\"\"\n        diff_h = self.win_size[0] - self.step_size[0]\n        padt = diff_h \/\/ 2\n        padb = diff_h - padt\n\n        diff_w = self.win_size[1] - self.step_size[1]\n        padl = diff_w \/\/ 2\n        padr = diff_w - padl\n\n        pad_type = \"constant\" if self.debug else \"reflect\"\n        x = np.lib.pad(x, ((padt, padb), (padl, padr), (0, 0)), pad_type)\n        sub_patches = self.__extract_valid(x)\n        return sub_patches\n\n    def extract(self, x, patch_type):\n        \"\"\"Extract the patches.\n        \n        Args:\n            x: input array to extract patches from.\n            patch_type:\n                'valid' extract patches without reflection at boundary. However, \n                    in case the img size < patch size, the img will be mirror-padded \n                    to fit a single patch.\n                'mirror' extract patches with mirror padding at boundary.\n\n        \"\"\"\n        patch_type = patch_type.lower()\n        self.patch_type = patch_type\n        if patch_type == \"valid\":\n            # padding in case request size larger img size\n            img_y, img_x = x.shape[:2]\n            if img_x < self.win_size[1]:\n                diff_x = self.win_size[1] - img_x\n                pad_x = int(math.ceil(diff_x \/ 2))\n                x = np.lib.pad(x, ((0, 0), (pad_x, pad_x), (0, 0)), \"reflect\")\n            if img_y < self.win_size[0]:\n                diff_y = self.win_size[0] - img_y\n                pad_y = int(math.ceil(diff_y \/ 2))\n                x = np.lib.pad(x, ((pad_y, pad_y), (0, 0), (0, 0)), \"reflect\")\n            return self.__extract_valid(x)\n        elif patch_type == \"mirror\":\n            return self.__extract_mirror(x)\n        else:\n            assert False, \"Unknown Patch Type [%s]\" % patch_type\n        return\n\n\n# ----------------------------------------------------------------------------\n\n# if __name__ == \"__main__\":\n#     # example for debug\n#     xtractor = PatchExtractor((256, 256), (128, 128), debug=True)\n#     a = np.full([1200, 1200, 3], 255, np.uint8)\n#     xtractor.extract(a, \"mirror\")\n#     xtractor.extract(a, \"valid\")\n","787376bc":"import sys\nfrom collections import OrderedDict\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision.models.resnet import Bottleneck as ResNetBottleneck\nfrom torchvision.models.resnet import ResNet\n\nsys.path.append(\"..\/..\/tiatoolbox\")\n\nfrom tiatoolbox.models.abc import ModelABC\nfrom tiatoolbox.models.architecture.hovernet import HoVerNet as TIAHoVerNet\nfrom tiatoolbox.models.architecture.utils import UpSample2x\n\n\nclass ResNetExt(ResNet):\n    def _forward_impl(self, x, freeze):\n        # See note [TorchScript super()]\n        if self.training:\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n            with torch.set_grad_enabled(not freeze):\n                x1 = x = self.layer1(x)\n                x2 = x = self.layer2(x)\n                x3 = x = self.layer3(x)\n                x4 = x = self.layer4(x)\n        else:\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n            x1 = x = self.layer1(x)\n            x2 = x = self.layer2(x)\n            x3 = x = self.layer3(x)\n            x4 = x = self.layer4(x)\n        return x1, x2, x3, x4\n\n    def forward(self, x: torch.Tensor, freeze: bool = False) -> torch.Tensor:\n        return self._forward_impl(x, freeze)\n\n    @staticmethod\n    def resnet50(num_input_channels, pretrained=None):\n        model = ResNetExt(ResNetBottleneck, [3, 4, 6, 3])\n        model.conv1 = nn.Conv2d(\n            num_input_channels, 64, 7, stride=1, padding=3)\n        if pretrained is not None:\n            pretrained = torch.load(pretrained)\n            (\n                missing_keys, unexpected_keys\n            ) = model.load_state_dict(pretrained, strict=False)\n        return model\n\n\nclass DenseBlock(nn.Module):\n    \"\"\"Dense Block as defined in:\n\n    Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. \n    \"Densely connected convolutional networks.\" In Proceedings of the IEEE conference \n    on computer vision and pattern recognition, pp. 4700-4708. 2017.\n\n    Only performs `valid` convolution.\n\n    \"\"\"\n\n    def __init__(self, in_ch, unit_ksize, unit_ch, unit_count, split=1):\n        super().__init__()\n        assert len(unit_ksize) == len(unit_ch), \"Unbalance Unit Info\"\n\n        self.nr_unit = unit_count\n        self.in_ch = in_ch\n        self.unit_ch = unit_ch\n\n        # ! For inference only so init values for batchnorm may not match tensorflow\n        unit_in_ch = in_ch\n        pad_vals = [v \/\/ 2 for v in unit_ksize]\n        self.units = nn.ModuleList()\n        for idx in range(unit_count):\n            self.units.append(\n                nn.Sequential(\n                    nn.BatchNorm2d(unit_in_ch, eps=1e-5),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(\n                        unit_in_ch, unit_ch[0], unit_ksize[0],\n                        stride=1, padding=pad_vals[0], bias=False,\n                    ),\n                    nn.BatchNorm2d(unit_ch[0], eps=1e-5),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(\n                        unit_ch[0], unit_ch[1], unit_ksize[1],\n                        stride=1, padding=pad_vals[1], bias=False,\n                        groups=split,\n                    ),\n                )\n            )\n            unit_in_ch += unit_ch[1]\n\n        self.blk_bna = nn.Sequential(\n            nn.BatchNorm2d(unit_in_ch, eps=1e-5),\n            nn.ReLU(inplace=True)\n        )\n\n    def out_ch(self):\n        return self.in_ch + self.nr_unit * self.unit_ch[-1]\n\n    def forward(self, prev_feat):\n        for idx in range(self.nr_unit):\n            new_feat = self.units[idx](prev_feat)\n            prev_feat = torch.cat([prev_feat, new_feat], dim=1)\n        prev_feat = self.blk_bna(prev_feat)\n\n        return prev_feat\n\n\nclass HoVerNetConic(ModelABC):\n    \"\"\"Initialise HoVer-Net.\"\"\"\n\n    def __init__(\n            self,\n            num_types=None,\n            freeze=False,\n            pretrained_backbone=None,\n            ):\n        super().__init__()\n        self.freeze = freeze\n        self.num_types = num_types\n        self.output_ch = 3 if num_types is None else 4\n\n        self.backbone = ResNetExt.resnet50(\n            3, pretrained=pretrained_backbone)\n        self.conv_bot = nn.Conv2d(\n            2048, 1024, 1, stride=1, padding=0, bias=False)\n\n        def create_decoder_branch(out_ch=2, ksize=5):\n            pad = ksize \/\/ 2\n            module_list = [\n                nn.Conv2d(1024, 256, ksize, stride=1, padding=pad, bias=False),\n                DenseBlock(256, [1, ksize], [128, 32], 8, split=4),\n                nn.Conv2d(512, 512, 1, stride=1, padding=0, bias=False),\n            ]\n            u3 = nn.Sequential(*module_list)\n\n            module_list = [\n                nn.Conv2d(512, 128, ksize, stride=1, padding=pad, bias=False),\n                DenseBlock(128, [1, ksize], [128, 32], 4, split=4),\n                nn.Conv2d(256, 256, 1, stride=1, padding=0, bias=False),\n            ]\n            u2 = nn.Sequential(*module_list)\n\n            module_list = [\n                nn.Conv2d(256, 64, ksize, stride=1, padding=pad, bias=False),\n            ]\n            u1 = nn.Sequential(*module_list)\n\n            module_list = [\n                nn.BatchNorm2d(64, eps=1e-5),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(64, out_ch, 1, stride=1, padding=0, bias=True),\n            ]\n            u0 = nn.Sequential(*module_list)\n\n            decoder = nn.Sequential(\n                OrderedDict([(\"u3\", u3), (\"u2\", u2), (\"u1\", u1), (\"u0\", u0)])\n            )\n            return decoder\n\n        ksize = 3\n        if num_types is None:\n            self.decoder = nn.ModuleDict(\n                OrderedDict(\n                    [\n                        (\"np\", create_decoder_branch(ksize=ksize, out_ch=2)),\n                        (\"hv\", create_decoder_branch(ksize=ksize, out_ch=2)),\n                    ]\n                )\n            )\n        else:\n            self.decoder = nn.ModuleDict(\n                OrderedDict(\n                    [\n                        (\"tp\", create_decoder_branch(ksize=ksize, out_ch=num_types)),\n                        (\"np\", create_decoder_branch(ksize=ksize, out_ch=2)),\n                        (\"hv\", create_decoder_branch(ksize=ksize, out_ch=2)),\n                    ]\n                )\n            )\n\n        self.upsample2x = UpSample2x()\n\n    def forward(self, imgs):\n        imgs = imgs \/ 255.0  # to 0-1 range to match XY\n\n        d0, d1, d2, d3 = self.backbone(imgs, self.freeze)\n        d3 = self.conv_bot(d3)\n        d = [d0, d1, d2, d3]\n\n        out_dict = OrderedDict()\n        for branch_name, branch_desc in self.decoder.items():\n            u3 = self.upsample2x(d[-1]) + d[-2]\n            u3 = branch_desc[0](u3)\n\n            u2 = self.upsample2x(u3) + d[-3]\n            u2 = branch_desc[1](u2)\n\n            u1 = self.upsample2x(u2) + d[-4]\n            u1 = branch_desc[2](u1)\n\n            u0 = branch_desc[3](u1)\n            out_dict[branch_name] = u0\n\n        return out_dict\n\n    @staticmethod\n    def _proc_np_hv(np_map: np.ndarray, hv_map: np.ndarray, fx: float = 1):\n        return TIAHoVerNet._proc_np_hv(np_map, hv_map, fx)\n\n    @staticmethod\n    def _get_instance_info(pred_inst, pred_type=None):\n        return TIAHoVerNet._get_instance_info(pred_inst, pred_type)\n\n    @staticmethod\n    # skipcq: PYL-W0221\n    def postproc(raw_maps: List[np.ndarray]):\n        return TIAHoVerNet.postproc(raw_maps)\n\n    @staticmethod\n    def infer_batch(model, batch_data, on_gpu):\n        return TIAHoVerNet.infer_batch(model, batch_data, on_gpu)\n","3f525546":"!mkdir .\/exp_output","b4813233":"import sys\nimport logging\nimport os\n\nimport cv2\nimport joblib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom IPython.utils import io as IPyIO\nfrom tqdm import tqdm\n\nmpl.rcParams['figure.dpi'] = 300\n\n# adding the project root folder\nsys.path.append('..\/')\nfrom tiatoolbox.models import IOSegmentorConfig, SemanticSegmentor\nfrom tiatoolbox.utils.visualization import overlay_prediction_contours\n\n# from ..\/input\/hovernet-conic-weights\/misc.utils import cropping_center, recur_find_ext, rm_n_mkdir, rmdir\n\n# Random seed for deterministic\nSEED = 5\n# The number of nuclei within the dataset\/predictions.\n# For CoNIC, we have 6 (+1 for background) types in total.\nNUM_TYPES = 7\n# The path to the directory containg images.npy etc.\nDATA_DIR = '..\/input\/conic-challenge-dataset\/CoNIC_Challenge\/'\n# The path to the pretrained weights\nPRETRAINED = '..\/input\/hovernet-conic-weights\/hovernet-conic.pth'\n# The path to contain output and intermediate processing results\nOUT_DIR = '.\/exp_output'","36d36812":"NUM_TRIALS = 10  # number of splits to be generated\nTRAIN_SIZE = 0.8\nVALID_SIZE = 0.2","a88babd2":"from sklearn.model_selection import StratifiedShuffleSplit\n\ninfo = pd.read_csv(f'{DATA_DIR}\/patch_info.csv')\nfile_names = np.squeeze(info.to_numpy()).tolist()\n\nimg_sources = [v.split('-')[0] for v in file_names]\nimg_sources = np.unique(img_sources)\n\ncohort_sources = [v.split('_')[0] for v in img_sources]\n_, cohort_sources = np.unique(cohort_sources, return_inverse=True)\n\nsplitter = StratifiedShuffleSplit(\n    n_splits=NUM_TRIALS,\n    train_size=TRAIN_SIZE,\n    test_size=VALID_SIZE,\n    random_state=SEED\n)\n\nsplits = []\nsplit_generator = splitter.split(img_sources, cohort_sources)\nfor train_indices, valid_indices in split_generator:\n    train_cohorts = img_sources[train_indices]\n    valid_cohorts = img_sources[valid_indices]\n    assert np.intersect1d(train_cohorts, valid_cohorts).size == 0\n    train_names = [\n        file_name\n        for file_name in file_names\n        for source in train_cohorts\n        if source == file_name.split('-')[0]\n    ]\n    valid_names = [\n        file_name\n        for file_name in file_names\n        for source in valid_cohorts\n        if source == file_name.split('-')[0]\n    ]\n    train_names = np.unique(train_names)\n    valid_names = np.unique(valid_names)\n    print(f'Train: {len(train_names):04d} - Valid: {len(valid_names):04d}')\n    assert np.intersect1d(train_names, valid_names).size == 0\n    train_indices = [file_names.index(v) for v in train_names]\n    valid_indices = [file_names.index(v) for v in valid_names]\n    splits.append({\n        'train': train_indices,\n        'valid': valid_indices\n    })\njoblib.dump(splits, f\"{OUT_DIR}\/splits.dat\")","5b917145":"# The fold to use\nFOLD_IDX = 0","6cdd7bcb":"imgs = np.load(f'{DATA_DIR}\/images.npy')\nlabels = np.load(f'{DATA_DIR}\/labels.npy')\n\nsplits = joblib.load(f'{OUT_DIR}\/splits.dat')\nvalid_indices = splits[FOLD_IDX]['valid']\n\nrm_n_mkdir(f'{OUT_DIR}\/imgs\/')\nfor idx in valid_indices:\n    img = imgs[idx]\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    cv2.imwrite(f'{OUT_DIR}\/imgs\/{idx:04d}.png', img)\n\nvalid_labels = labels[valid_indices]\nnp.save(f'{OUT_DIR}\/valid_true.npy', valid_labels)","dc1ed6c7":"# from net_desc import HoVerNetConic\n\npretrained = torch.load(PRETRAINED)\nmodel = HoVerNetConic(num_types=NUM_TYPES)\nmodel.load_state_dict(pretrained)\n\n# Tile prediction\npredictor = SemanticSegmentor(\n    model=model,\n    num_loader_workers=2,\n    batch_size=6,\n)","75fddbd7":"# Define the input\/output configurations\nioconfig = IOSegmentorConfig(\n    input_resolutions=[\n        {'units': 'baseline', 'resolution': 1.0},\n    ],\n    output_resolutions=[\n        {'units': 'baseline', 'resolution': 1.0},\n        {'units': 'baseline', 'resolution': 1.0},\n        {'units': 'baseline', 'resolution': 1.0},\n    ],\n    save_resolution={'units': 'baseline', 'resolution': 1.0},\n    patch_input_shape=[256, 256],\n    patch_output_shape=[256, 256],\n    stride_shape=[256, 256],\n)\n\nlogger = logging.getLogger()\nlogger.disabled = True\n\ninfer_img_paths = recur_find_ext(f'{OUT_DIR}\/imgs\/', ['.png'])\nrmdir(f'{OUT_DIR}\/raw\/')\n\n# capture all the printing to avoid cluttering the console\nwith IPyIO.capture_output() as captured:\n    output_file = predictor.predict(\n        infer_img_paths,\n        masks=None,\n        mode='tile',\n        on_gpu=True,\n        ioconfig=ioconfig,\n        crash_on_exception=True,\n        save_dir=f'{OUT_DIR}\/raw\/'\n    )","ac45169a":"def process_segmentation(np_map, hv_map, tp_map):\n    # HoVerNet post-proc is coded at 0.25mpp so we resize\n    np_map = cv2.resize(np_map, (0, 0), fx=2.0, fy=2.0)\n    hv_map = cv2.resize(hv_map, (0, 0), fx=2.0, fy=2.0)\n    tp_map = cv2.resize(\n                    tp_map, (0, 0), fx=2.0, fy=2.0,\n                    interpolation=cv2.INTER_NEAREST)\n\n    inst_map = model._proc_np_hv(np_map[..., None], hv_map)\n    inst_dict = model._get_instance_info(inst_map, tp_map)\n\n    # Generating results match with the evaluation protocol\n    type_map = np.zeros_like(inst_map)\n    inst_type_colours = np.array([\n        [v['type']] * 3 for v in inst_dict.values()\n    ])\n    type_map = overlay_prediction_contours(\n        type_map, inst_dict,\n        line_thickness=-1,\n        inst_colours=inst_type_colours)\n\n    pred_map = np.dstack([inst_map, type_map])\n    # The result for evaluation is at 0.5mpp so we scale back\n    pred_map = cv2.resize(\n                    pred_map, (0, 0), fx=0.5, fy=0.5,\n                    interpolation=cv2.INTER_NEAREST)\n    return pred_map","81095dc1":"def process_composition(pred_map):\n    # Only consider the central 224x224 region,\n    # as noted in the challenge description paper\n    pred_map = cropping_center(pred_map, [224, 224])\n    inst_map = pred_map[..., 0]\n    type_map = pred_map[..., 1]\n    # ignore 0-th index as it is 0 i.e background\n    uid_list = np.unique(inst_map)[1:]\n\n    if len(uid_list) < 1:\n        type_freqs = np.zeros(NUM_TYPES)\n        return type_freqs\n    uid_types = [\n        np.unique(type_map[inst_map == uid])\n        for uid in uid_list\n    ]\n    type_freqs_ = np.unique(uid_types, return_counts=True)\n    # ! not all types exist within the same spatial location\n    # ! so we have to create a placeholder and put them there\n    type_freqs = np.zeros(NUM_TYPES)\n    type_freqs[type_freqs_[0]] = type_freqs_[1]\n    return type_freqs","21c503ce":"output_file = f'{OUT_DIR}\/raw\/file_map.dat'\noutput_info = joblib.load(output_file)\n\nsemantic_predictions = []\ncomposition_predictions = []\nfor input_file, output_root in tqdm(output_info):\n    img = cv2.imread(input_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    np_map = np.load(f'{output_root}.raw.0.npy')\n    hv_map = np.load(f'{output_root}.raw.1.npy')\n    tp_map = np.load(f'{output_root}.raw.2.npy')\n\n    pred_map = process_segmentation(np_map, hv_map, tp_map)\n    type_freqs = process_composition(pred_map)\n    semantic_predictions.append(pred_map)\n    composition_predictions.append(type_freqs)\nsemantic_predictions = np.array(semantic_predictions)\ncomposition_predictions = np.array(composition_predictions)","41e5bca9":"# Saving the results for segmentation\nnp.save(f'{OUT_DIR}\/valid_pred.npy', semantic_predictions)\n\n# Saving the results for composition prediction\nTYPE_NAMES = [\n    \"neutrophil\", \"epithelial\", \"lymphocyte\",\n    \"plasma\", \"eosinophil\", \"connective\"\n]\ndf = pd.DataFrame(\n    composition_predictions[:, 1:].astype(np.int32),\n)\ndf.columns = TYPE_NAMES\ndf.to_csv(f'{OUT_DIR}\/valid_pred_cell.csv', index=False)\n\n# Load up the composition ground truth and\n# save the validation portion\ndf = pd.read_csv(f'{DATA_DIR}\/counts.csv')\ntrue_compositions = df.to_numpy()[valid_indices]\ndf = pd.DataFrame(\n    true_compositions.astype(np.int32),\n)\ndf.columns = TYPE_NAMES\ndf.to_csv(f'{OUT_DIR}\/valid_true_cell.csv', index=False)","bfe92f83":"semantic_true = np.load(f'{OUT_DIR}\/valid_true.npy')\nsemantic_pred = np.load(f'{OUT_DIR}\/valid_pred.npy')\n\noutput_file = f'{OUT_DIR}\/raw\/file_map.dat'\noutput_info = joblib.load(output_file)\n\nnp.random.seed(SEED)\nselected_indices = np.random.choice(len(valid_indices), 4)\n\nPERCEPTIVE_COLORS = [\n    (  0,   0,   0),\n    (255, 165,   0),\n    (  0, 255,   0),\n    (255,   0,   0),\n    (  0, 255, 255),\n    (  0,   0, 255),\n    (255, 255,   0),\n]\n\nfor idx in selected_indices:\n    img = cv2.imread(output_info[idx][0])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    inst_map = semantic_pred[idx][..., 0]\n    type_map = semantic_pred[idx][..., 1]\n    pred_inst_dict = model._get_instance_info(inst_map, type_map)\n\n    inst_map = semantic_true[idx][..., 0]\n    type_map = semantic_true[idx][..., 1]\n    true_inst_dict = model._get_instance_info(inst_map, type_map)\n\n    inst_type_colours = np.array([\n        PERCEPTIVE_COLORS[v['type']]\n        for v in true_inst_dict.values()\n    ])\n    overlaid_true = overlay_prediction_contours(\n        img, true_inst_dict,\n        inst_colours=inst_type_colours,\n        line_thickness=1\n    )\n\n    inst_type_colours = np.array([\n        PERCEPTIVE_COLORS[v['type']]\n        for v in pred_inst_dict.values()\n    ])\n    overlaid_pred = overlay_prediction_contours(\n        img, pred_inst_dict,\n        inst_colours=inst_type_colours,\n        line_thickness=1\n    )\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.subplot(1, 3, 2)\n    plt.imshow(overlaid_true)\n    plt.title('Ground Truth')\n    plt.axis('off')\n    plt.subplot(1, 3, 3)\n    plt.imshow(overlaid_pred)\n    plt.title('Prediction')\n    plt.axis('off')\n    plt.show()","97a4b2a9":" # Generating Data Splits\n Here, we show how the training data is split into the training\n and validation subsets. The CoNIC training data comes from multiple\n sources. To ensure that we have a balanced dataset, we use stratified sampling according to the data source.\n\n For the baseline model, we utilize 80% the number of patches\n for training and the remaining for validation. However, because\n we apply stratified sampling according to the origin, the final number of patches may not be 80\/20 (each image may contain a different number of patches). Therefore, we generate a number of splits (indicated\n via the `NUM_TRIALS` variable) and select the one that has the\n number of patches that most closely matches with our expected ratio.\n\n Lastly, to ensure the reproducibility of the sampling, we also provide a random seed.","28921a6b":" Once we have the HoVerNet raw inference output, we apply post processing\n to obtain the final results. For CoNIC, there are two tasks that\n are linked with each other:\n - The instance segmentation and classification of nuclei.\n - The cellular compositions within the provided patches.\n Rather than directly predicting the 2nd set of results from images, we simply use the final output of HoVerNet. Thus, to make the code more organised, we separate these tasks into their own funtion.","b12bfb57":" # HoVerNet - The CoNIC Baseline","27b27a47":" # Inference\n To further simplify the inference process, we utilise `tiatoolbox`\n which already contains the inference mechanism. This functionality\n accepts a list of image paths as input. Therefore, we will turn `images.npy` into individual `*.png` for the inference process.\n\n > **Note**: We use the first fold (trial) from the splits generated above\n > for training. Therefore, we need to extract the corresponding ground truth\n > for the validation evaluation process.","255bfa16":" ## Importing Libraries and Workspace Settings\n We import some Python modules that are utilised throughout the notebook.\n\n > **Note**: We use `tiatoolbox` extensively in this repository.\n > You can refer to [here](https:\/\/github.com\/TissueImageAnalytics\/tiatoolbox) for installation instructions.\n > For this notebook, we use the `develop` branch rather than `master`.\n\n We also declare the folders which contain the input\n data and the output.","ecb2dc73":"## About this notebook\nIn this notebook, we provide a pretrained\n[HoVerNet](https:\/\/arxiv.org\/abs\/1812.06499)\nas the baseline model for the CoNIC training data.\nThe HoVerNet utilized here was trained using\nonly data from the CoNIC training set. You can download the pretrained\nweights [here](https:\/\/drive.google.com\/file\/d\/1oVCD4_kOS-8Wu-eS5ZqzE30F9V3Id78d\/view?usp=sharing).\n\nIn the making of the baseline model, we perform following steps:\n- Generating training and validation split.\n- Perform the inference to get raw output.\n- Perform the post-processing to convert the output into an approriate form\nfor the evaluation process in `compute_stats.py`.\n\nAll in all, the HoVerNet trained on the data split obtained\nfrom this notebook achieved the validation results as follows:\n\n**Nucleus Instance Segmentation and Classification**\n---------------------------\n| PQ     | mPQ<sup>+<\/sup>|\n|--------|----------------|\n| 0.6149 | 0.4998         |\n---------------------------\n\n\n**Cell Composition**\n-----------------\n| R<sup>2<\/sup> |\n|---------------|\n| 0.8585        |\n-----------------","ef00e54b":" # Visualization\n To wrap everything up, we randomly select some samples within the validation\n set and plot their associated ground truth and predictions","b1bfc92e":" Now we actually perform the post-processing using the input\n output file mapping obtained previously from the inference process."}}