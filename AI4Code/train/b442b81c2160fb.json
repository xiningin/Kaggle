{"cell_type":{"1418b3ab":"code","019c9d3e":"code","280ec11a":"code","70685e3c":"code","f9a5cc2f":"code","8bccef64":"code","4dcaf981":"code","acc8f631":"code","312a2909":"code","e7175986":"code","3b747882":"code","5be8c7e8":"code","bdc2bbb2":"code","217d23bb":"code","fbb099f9":"code","38a5967a":"markdown","91192fd3":"markdown","729180a6":"markdown","b59db8d3":"markdown","7b76a9f9":"markdown"},"source":{"1418b3ab":"!pip install -U scikit-learn\n!pip install -U bayesian-optimization\n!pip install -U lightgbm\n\nimport os\nimport math\nimport datetime\nimport numpy as np \nimport pandas as pd\nimport scipy as sc\nimport matplotlib.pyplot as plt\n\nRANDOM_SEED = 111\n\nnp.random.seed(RANDOM_SEED)\n\nfrom numpy.random import default_rng\nrng = default_rng(RANDOM_SEED)\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, mean_squared_log_error\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler, OneHotEncoder, Binarizer, KBinsDiscretizer, QuantileTransformer\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import set_config\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.multioutput import MultiOutputRegressor, RegressorChain\n\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\n\nINPUT_DIR = '\/kaggle\/input\/tabular-playground-series-jul-2021'\nOUTPUT_DIR = '.\/'\nBATCH_SIZE = 1024","019c9d3e":"def season(month):\n  if (month == 12 or month == 1 or month == 2):   #winter\n        return 0        \n  elif(month == 3 or month == 4 or month == 5):   #spring\n        return 1       \n  elif(month == 6 or month == 7 or month == 8):   #summer\n        return 2       \n  else:                                           #outemn\n        return 3 \n\ndef daytime(hour):\n  if (hour > 5 and hour < 17):      #light\n    return 0\n  else:                             #darkness\n    return 1\n\ntrain_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'), index_col='date_time')\ntest_df = pd.read_csv(os.path.join(INPUT_DIR,'test.csv'), index_col='date_time')\n\ntrain_df.index = pd.to_datetime(train_df.index)\ntest_df.index = pd.to_datetime(test_df.index)\n\nlabels = train_df[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']]\n\ntrain_df.drop(labels.columns, axis=1, inplace=True)\ntotal_df = train_df.append(test_df)     #pd.concat()\n\ntotal_df['dew_point'] = total_df['deg_C'].apply(lambda x: (17.27 * x) \/ (237.7 + x)) + total_df['absolute_humidity'].apply(lambda x: math.log (x))\ntotal_df['partial_pressure'] = (total_df['deg_C'].apply(lambda x: (237.7 + x) * 286.8) * total_df['absolute_humidity']) \/ 100000\ntotal_df['saturated_wvd'] = (total_df['absolute_humidity'] * 100) \/ total_df['relative_humidity']\n\ntotal_df['dt_low_absolute_humidity'] = (total_df[\"absolute_humidity\"] < 0.25) & (23 < total_df[\"deg_C\"])\n\ntotal_df['dt_hour'] = [x.hour for x in total_df.index]\ntotal_df['dt_weekday'] = [x.weekday() for x in total_df.index]\ntotal_df['dt_month'] = [x.month for x in total_df.index]\ntotal_df['dt_season'] = [season(x.month) for x in total_df.index]\ntotal_df['dt_lights'] = [daytime(x.hour) for x in total_df.index]\ntotal_df['dt_month_s'] = np.sin(np.pi * (total_df['dt_month']-1)\/6)\ntotal_df['dt_month_c'] = np.cos(np.pi * (total_df['dt_month']-1)\/6)\n\ntotal_df['dt_month_s'] = total_df['dt_month_s'].astype('category').cat.codes\ntotal_df['dt_month_c'] = total_df['dt_month_c'].astype('category').cat.codes\n\ntotal_df[\"dt_working_hours\"] = total_df[\"dt_hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\ntotal_df[\"dt_weekend\"] = (total_df[\"dt_weekday\"] >= 5).astype(\"int\")\n\ncat_cols = np.array([col for col in total_df.columns if 'dt_' in col])\nnum_cols = np.array([col for col in total_df.columns if not 'dt_' in col])\ntotal_cols = np.concatenate([num_cols,cat_cols])\ncat_cols_idx = [np.where(total_df.columns == x)[0][0] for x in cat_cols]","280ec11a":"fig, axes = plt.subplots(nrows=3, ncols=3)\nfig.set_size_inches(16, 8)\n\ntotal_df['deg_C'].plot(ax=axes[0, 0], title='deg_C');\ntotal_df['absolute_humidity'].plot(ax=axes[0, 1], title='absolute_humidity');\ntotal_df['relative_humidity'].plot(ax=axes[0, 2], title='relative_humidity');\ntotal_df['dt_season'].plot(ax=axes[1, 0], title='dt_season');\ntotal_df['sensor_1'].plot(ax=axes[1, 1], title='sensor_1');\ntotal_df['sensor_2'].plot(ax=axes[1, 2], title='sensor_2');\ntotal_df['sensor_3'].plot(ax=axes[2, 0], title='sensor_3');\ntotal_df['dt_month_c'].plot(ax=axes[2, 1], title='dt_month_c');\ntotal_df['dt_month_s'].plot(ax=axes[2, 2], title='dt_month_s');","70685e3c":"pd.concat((total_df.min(), total_df.max(), total_df.mean(), total_df.std(), total_df.nunique()), axis=1)","f9a5cc2f":"pd.concat((labels.min(), labels.max(), labels.mean(), labels.nunique()), axis=1)","8bccef64":"labels.corr()","4dcaf981":"class LGBMWrapper(BaseEstimator):\n  def __init__(self, verbose, nfold):\n    self.verbose = verbose\n    self.nfold = nfold\n\n  def fit(self, X, y, **kwargs):\n    y = np.log1p(y)\n    d_train = lgb.Dataset(X, y)\n\n    kwargs['objective'] = 'regression'\n\n    if self.verbose < 0:\n      kwargs['verbose'] = self.verbose \n\n    model = lgb.cv(kwargs, d_train, num_boost_round=10000, nfold=self.nfold, metrics='rmse', \n                   early_stopping_rounds=100, stratified=False, verbose_eval=self.verbose, \n                   return_cvbooster=True, seed=RANDOM_SEED)\n\n    self.booster = model['cvbooster']\n    self.score = model['rmse-mean'][-1]\n    \n  def predict(self, X):\n    y_pred = self.booster.predict(X)\n    y_pred = np.expm1(y_pred).mean(0)\n    return y_pred","acc8f631":"pipe_pre = Pipeline([\n    ('preproc', ColumnTransformer([\n      ('num', Pipeline([\n          #('scale', StandardScaler()),\n          ('gauss', QuantileTransformer(output_distribution=\"normal\")),\n          #('minmax', MinMaxScaler()),\n          ('kbins', KBinsDiscretizer(n_bins=16, encode='ordinal'))  #strategy='uniform'\n      ]), num_cols),\n      #('cat', OrdinalEncoder(), cat_cols)\n      ('cat', OneHotEncoder(sparse=False), cat_cols)\n    ], remainder='passthrough')),\n    #('scale', StandardScaler())\n])\n\npipe_pre.fit(total_df)\ntotal_data = pipe_pre.transform(total_df).astype('float')\n\ntrain_data, test_data = total_data[:train_df.index.shape[0]], total_data[train_df.index.shape[0]:]","312a2909":"pd.DataFrame(index=total_df.index, columns=total_cols, data=total_data).nunique().T","e7175986":"def feval(learning_rate, num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, lambda_l2, lambda_l1, min_data_in_leaf, min_sum_hessian_in_leaf, subsample):\n    lgb_space = {\n        'learning_rate': float(max(min(learning_rate, 1), 0)),\n        'num_leaves': int(round(num_leaves)),\n        'feature_fraction': float(max(min(feature_fraction, 1), 0)),\n        'bagging_fraction': float(max(min(bagging_fraction, 1), 0)),\n        'max_depth': int(round(max_depth)),\n        'max_bin': int(round(max_depth)),\n        \"lambda_l2\" : float(lambda_l2),\n        \"lambda_l1\" : float(lambda_l1),\n        'min_data_in_leaf': int(round(min_data_in_leaf)),\n        'min_sum_hessian_in_leaf': float(min_sum_hessian_in_leaf),\n        'subsample': float(max(min(subsample, 1), 0))\n    }\n         \n    lgbpwrap = RegressorChain(LGBMWrapper(verbose=-1, nfold=3))\n    lgbpwrap.fit(train_data, labels, **lgb_space)\n    scores = [x.score for x in lgbpwrap.estimators_]\n    print('scores: ', scores)\n    return -np.mean(scores)\n\n\nfspace = {\n    'learning_rate': (0.01, 0.2),\n    'num_leaves': (24, 80),\n    'feature_fraction': (0.4, 1),\n    'bagging_fraction': (0.8, 1),\n    'max_depth': (5, 30),\n    'max_bin':(20,90),\n    'lambda_l2': (0.0, 0.05),\n    'lambda_l1': (0.0, 0.05),\n    'min_data_in_leaf': (20, 80),\n    'min_sum_hessian_in_leaf':(0,100),\n    'subsample': (0.01, 1.0)\n}\n\noptimizer = BayesianOptimization(feval, fspace, random_state=RANDOM_SEED)\noptimizer.maximize(init_points=20, n_iter=20)\nbest_params = optimizer.max\nbest_params","3b747882":"print(best_params)\nparams = best_params['params']\nparams['max_bin'] = int(round(params['max_bin']))\nparams['max_depth'] = int(round(params['max_depth']))\nparams['min_data_in_leaf'] = int(round(params['min_data_in_leaf']))\nparams['num_leaves'] = int(round(params['num_leaves']))","5be8c7e8":"params = {\n    'learning_rate': 0.01, \n    'num_leaves': 70,\n    'feature_fraction': 0.4049,\n    'bagging_fraction': 0.8922,\n    'max_depth': 30,\n    'max_bin': 28,\n    'min_data_in_leaf': 25,\n    'min_sum_hessian_in_leaf': 6.056\n}\n\nlgbpwrap = RegressorChain(LGBMWrapper(verbose=1000, nfold=10))\nlgbpwrap.fit(train_data, labels, **params)\noutput_test = lgbpwrap.predict(test_data)","bdc2bbb2":"scores = [x.score for x in lgbpwrap.estimators_]\nfeatures = np.argsort(lgbpwrap.estimators_[0].booster.feature_importance(importance_type='gain')).mean(0)\nfeatures = list(zip(total_cols, features))\n\nprint('scores: ', scores) \nfeatures","217d23bb":"output_res = pd.DataFrame(index=test_df.index, data={'date_time':test_df.index.values})\noutput_res[labels.columns] = output_test\noutput_res.to_csv('.\/submission.csv', index=False)","fbb099f9":"output_res.corr()","38a5967a":"Results without `OneHotEncoder`:","91192fd3":"Here we enveloppe the original `lightgbm.cv` to make it accessible by the sklearn Pipeline.<br\/>\nThe competition required validation shoulf be RMSLE but here we used a built-in RMSE with updating label before (log1p) and after (expm1) the execution flow.<br\/>\nFinally, in the prediciotn step we receive predictions array with the `nfold` dimentions. The simpliest option is just to do their average but there can be applied also other options, like:\n- median (np.median)\n- geometric mean (sc.stats.mstats.gmean)\n- meta-model stacking:\n   - execute `Booster.predict` for X_test\n   - combine nfold test predictions to matrix\n   - execute `Booster.predict` for X_train\n   - combine nfold train predictions in a new LGBM model with nfold features and original labels\n   - train meta-model and predict it with the prepared X_test preditions from the second section\n   \n`stratified=True` doesn't work for regressions.<br\/>\n`kwargs['verbose']=-1` removes all logs and warnings, it is useful for hyperparameter optimization.","729180a6":"The 3 target labels are highly correlated 66%\/80%\/88%. That's why we used `RegressorChain` and not `MultiOutputRegressor`.<br\/>\nThe predicted labels become even more correlated - 93%","b59db8d3":"In this notebook I will train the \"Tabular Playground Series - Jul 2021\" model by the LightGBM.<br\/>\nThis model is a multi-output regression so, because the LGBM doesn't support it we will have to use one of the two available techniques:\n1. Train each label feature separately and combine them back afterwards. This method is implemented by the `sklearn.multioutput.MultiOutputRegressor`.\n2. Train each label feature separately and append to every next model the predicted label from the previous model as a new features. This method is implemented by the `sklearn.multioutput.RegressorChain`.\n\nWe will also optimize the hyperparamenters by the `bayes_opt.BayesianOptimization`.<br\/>\nThe problem with the `RegressorChain` is that it doesn't update the evaluation dataset so, we will have to write an envelope class over the `lightgbm.cv` to make it part of the pipeline. <br\/>\nFinally, the training pipeline chain will look like that:\n```\nBayesianOptimization --> RegressorChain --> LGBMWrapper --> lightgbm.cv\n```","7b76a9f9":"After training the model we could see that the `deg_C` is actually the main feature with highest impact. It also has a strict seasonality.<br\/>\nThe `dew_point`,`partial_pressure` are pretty useless. They are also highly correlated with the `absolute_humidity`.<br\/>\nThe `dt_*` features behaved surprisingly well, despite that they all were extracted from the DateTime index (except `dt_low_absolute_humidity`).\n\n```\n ('deg_C', 20.0),\n ('relative_humidity', 11.9),\n ('absolute_humidity', 14.4),\n ('sensor_1', 15.7),\n ('sensor_2', 17.0),\n ('sensor_3', 19.0),\n ('sensor_4', 9.0),\n ('sensor_5', 1.0),\n ('dew_point', 4.6),\n ('partial_pressure', 4.0),\n ('saturated_wvd', 4.7),\n ('dt_low_absolute_humidity', 7.6),\n ('dt_hour', 8.1),\n ('dt_weekday', 3.8),\n ('dt_month', 11.2),\n ('dt_season', 18.0),\n ('dt_lights', 14.0),\n ('dt_month_s', 12.0),\n ('dt_month_c', 7.0),\n ('dt_working_hours', 3.0),\n ('dt_weekend', 4.0)\n ```"}}