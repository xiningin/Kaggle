{"cell_type":{"58a9a351":"code","35bf2400":"code","c8bfca88":"code","fcbbf254":"code","c2e4cf2c":"code","c2c4e5af":"code","eec14bdb":"code","8133090d":"code","7367395e":"code","48e6ad26":"code","e05bcc02":"code","7407cdf4":"code","7daaf81d":"code","0fd6b2d0":"code","03f72096":"code","164868e9":"code","26144f9a":"code","879de480":"code","8cc82fd9":"code","080d426e":"code","58a99f3b":"code","557da1c8":"code","14a82df6":"code","41014aee":"code","5068a2ec":"code","d0348904":"code","ced49190":"code","1b89a15c":"code","605a0922":"code","ae657003":"code","9499c52e":"code","79e41472":"code","5f0818c5":"code","7d28cc0e":"code","df5f40ee":"code","75019146":"code","b79e8c78":"code","4a35ed85":"code","5298b790":"code","9cb3edbb":"code","8c57582c":"code","1950b777":"code","90e64779":"code","765dae79":"code","b1920efe":"code","4cd710dc":"code","c4c0a48e":"code","c81737bc":"code","30a69200":"code","7d005535":"code","e302d356":"code","200b9603":"code","716d8cde":"code","38cc902a":"code","dab234b9":"code","68f65444":"code","adb637db":"code","2ec87a80":"code","a92b62db":"code","1a131c19":"code","b6024490":"code","7101b5b0":"code","1ae28a7c":"code","8034feb8":"code","3f46ae7a":"code","757b54ba":"code","fdbf259b":"code","37344ad8":"code","93fe6090":"code","0d500e71":"code","3f4379bd":"code","f77e1696":"code","eb52411d":"code","f1ff53f4":"code","27978fef":"code","0029a61b":"code","fa3aadc6":"code","1aec2f36":"code","b8519be2":"code","6252acd9":"code","a15dd62a":"code","048c615c":"code","a15a83f2":"code","3e8bf289":"code","d6cf757d":"code","0f3c8a5a":"code","c6e2ea3e":"code","8a437331":"code","1f061c6e":"code","ac9851a8":"code","c6e1ed5d":"code","f48a28ab":"code","357cad19":"code","6b5b1f9a":"code","7fe43684":"code","15ff0202":"code","e86edede":"code","c65f601e":"code","344a2025":"code","af3a1dec":"code","c16c8a27":"code","d47dd8a3":"code","637ae510":"code","2ec18cdc":"code","f73d7528":"code","7d391ff6":"code","8a89b09e":"code","019313c3":"code","b1f1c98b":"code","5d992270":"code","83edb65b":"code","f7e6c967":"code","ce3fb7ba":"code","5e2bc0aa":"code","6b68ccdc":"code","4ee47b35":"markdown","4a6cb7a9":"markdown","38fc59f6":"markdown","6b71a7e7":"markdown","f78da19d":"markdown","21904126":"markdown","c6e5e4dd":"markdown","d7e305fa":"markdown","0a93b042":"markdown","4ea75e31":"markdown","d2969a01":"markdown","fa3a257a":"markdown","539e1ac4":"markdown","fb595359":"markdown","674d0057":"markdown","62390e74":"markdown","539bd97e":"markdown","d05107ac":"markdown","ef21585a":"markdown","80eadc8b":"markdown","0a554c47":"markdown","9985993b":"markdown","8d8b34e6":"markdown","9eb2f3be":"markdown","5a927b05":"markdown","3be23abf":"markdown","02f12245":"markdown","020fb99f":"markdown","5200e9c5":"markdown","0bee056e":"markdown","96482bba":"markdown","a83e16e7":"markdown","643b883f":"markdown","655bc3c4":"markdown","87835654":"markdown","9fbf0ebd":"markdown","0cb93de4":"markdown","840db60e":"markdown","17f1f16c":"markdown","66f0900d":"markdown","242c1774":"markdown","977fca40":"markdown","7a1a29cb":"markdown","ef4b3b29":"markdown","84dbbb79":"markdown","7811647b":"markdown","aec19573":"markdown","79b9fd98":"markdown","2dd05249":"markdown","ab1e993f":"markdown","9ccfc515":"markdown","c9f26fb8":"markdown","50f2e3ea":"markdown","ac75acee":"markdown","eb33d699":"markdown","f32bfb55":"markdown","1bace00e":"markdown","5d317954":"markdown","fc994203":"markdown","cb55dd5c":"markdown","2b047a44":"markdown","1a6cd55d":"markdown","21868f52":"markdown","872fec02":"markdown","5c9542d7":"markdown","de11933b":"markdown","85683b5c":"markdown","702466b7":"markdown","d0aa74fe":"markdown","3c85b18e":"markdown","a21b2c6a":"markdown"},"source":{"58a9a351":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35bf2400":"## Data Analysis Phase\n## MAin aim is to understand more about the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n## Display all the columns of the dataframe\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy.special import boxcox1p\nfrom subprocess import check_output\n\npd.pandas.set_option('display.max_columns',None)\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n","c8bfca88":"print(check_output([\"ls\", \"\/kaggle\/input\/house-prices-advanced-regression-techniques\"]).decode(\"utf8\")) #check the files available in the directory","fcbbf254":"train=pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","c2e4cf2c":"## print the top5 records\ntrain.head()","c2c4e5af":"## print the top5 records\ntest.head()","eec14bdb":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))","8133090d":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","7367395e":"train['LotFrontage'].mean()","48e6ad26":"## Here we will check the percentage of nan values present in each feature\n## 1 -step make the list of features which has missing values, it includes both numeric and Categorical features\nfeatures_with_na=[features for features in train.columns if train[features].isnull().sum()>1]\n\n\n## 2- step print the feature name and the percentage of missing values\n\n# Here mean will give mean value based on [No of isnull() record]\/ [Total no of record]\nfor feature in features_with_na:\n    print(feature, np.round(train[feature].isnull().mean(), 4),  ' % missing values')","e05bcc02":"for feature in features_with_na:\n    data = train.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    #Here it replaces specific feature value by 0 or 1, if null then 1,else 0\n    \n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # let's calculate the mean SalePrice where the information is missing(1) or present(0)\n    \n    print(data.groupby(feature)['SalePrice'].median())\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.xticks([0,1],['Not Null', 'Null'])\n    plt.show()","7407cdf4":"# list of numerical variables\nnumerical_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(numerical_features))\n\n# visualise the numerical variables\ntrain[numerical_features].head()","7daaf81d":"# list of variables that contain year information\n# Here we are identifying year related field based on field name 'yr' or 'year'\n\nyear_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\n\nyear_feature","0fd6b2d0":"# let's explore the content of these year variables\nfor feature in year_feature:\n    print(feature, train[feature].unique())","03f72096":"# Group based on 'YrSold' and take median for that yearsold\nprint(train.groupby('YrSold')['SalePrice'].median())\n\ntrain.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")\n\n","164868e9":"year_feature","26144f9a":"## Here we will compare the difference between All years feature with SalePrice\n# Here we are taking difference of \"year field\" and \"year_sold\" --> This will be x axis and SalePrice price is the y axis\n\n\nfor feature in year_feature:\n    if feature!='YrSold':\n        data=train.copy()\n        ## We will capture the difference between year variable and year the house was sold for\n        # Here data[actualfeature] = difference of YrSold and yearfeature  \n        data[feature]=data['YrSold']-data[feature]\n\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()\n\n    \n    ","879de480":"\ndiscrete_feature=[feature for feature in numerical_features if len(train[feature].unique())<25 and feature not in year_feature+['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","8cc82fd9":"discrete_feature","080d426e":"train[discrete_feature].head()","58a99f3b":"## Lets Find the realtionship between them and Sale PRice\n\nfor feature in discrete_feature:\n    data=train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","557da1c8":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","14a82df6":"## Lets analyse the continuous values by creating histograms to understand the distribution\n\nfor feature in continuous_feature:\n    data=train.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()\n","41014aee":" ## We will be using logarithmic transformation  - HISTOGRAM PLOT\n# In log func Log10 0 = Not Defined and lne (0) = Not defined , so we are just \"pass\" and not plotting anything\n\nfor feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data[feature].hist(bins=25)\n        plt.xlabel(feature + \" in Logscale\")\n        plt.ylabel('Count')\n        plt.title(feature)\n        plt.show()\n","5068a2ec":" ## We will be using logarithmic transformation log1p - HISTOGRAM PLOT\n# In log1p = LOG(P+1)  --> Here we can use for data which has 0 also\n\nfor feature in continuous_feature:\n    data=train.copy()\n    data[feature]=np.log1p(data[feature])\n    data[feature].hist(bins=25)\n    plt.xlabel(feature + \" in Logscale\")\n    plt.ylabel('Count')\n    plt.title(feature)\n    plt.show()\n","d0348904":"## We will be using logarithmic transformation  - SCATTER PLOT\n# In log func Log10 0 = Not Defined and lne (0) = Not defined , so we are just \"pass\" and not plotting anything\n\nfor feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature + \" in Logscale\")\n        plt.ylabel('SalesPrice in Logscale')\n        plt.title(feature)\n        plt.show()\n        \n    ","ced49190":"categorical_features=[feature for feature in train.columns if train[feature].dtypes=='O']\ncategorical_features","1b89a15c":"train[categorical_features].head()","605a0922":"# No of unique features in categorial variable\n\nfor feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(train[feature].unique())))","ae657003":"for feature in categorical_features:\n    data=train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","9499c52e":"for feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()\n        \n    ","79e41472":"\noutliers=[]\nz_value=[]\n\n# Create Function for Outlier detection by Z-test\n\ndef detect_outliers(data):\n    \n# This is  3rd STD threshold  \n    threshold=3\n    mean = np.mean(data)\n    std =np.std(data)\n    \n    \n    for i in data:\n        z_score= (i - mean)\/std \n        z_value.append(z_score)\n        #print(z_score)\n        # This Z value <-3 or >3 are outlier, so taken abs(z_score)\n        if np.abs(z_score) > threshold:\n            outliers.append(i)\n    return outliers\n    \n","5f0818c5":"# Use z-test  inside for loop for all continous value\n\nfor feature in continuous_feature:\n    data=train.copy()\n    # Call Function\n    outlier = detect_outliers(data[feature])\n    print(\"\\n\",outlier)\n         \n\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","7d28cc0e":"# This combined step of above one\n\"\"\"\nfor feature in continuous_feature:\n    data=train.copy()\n    outliers=[]\n\n    \n    # This is  3rd STD threshold  \n    threshold=3\n    \n    mean = np.mean(data[feature])\n    std =np.std(data[feature])\n    \n    \n    for i in data[feature]:\n        z_score = (i - mean)\/std \n        # This Z value <-3 or >3 are outlier, so taken abs(z_score)\n        if np.abs(z_score) > threshold:\n            outliers.append(i)\n    \n    print(\"\\n\",feature , \" outlier values :\" , outliers)\n            \n\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()\n        \n\"\"\"","df5f40ee":"\nfor feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        \n        #data[feature]=np.log(data[feature])\n        fig, ax = plt.subplots()\n       # data.boxplot(column=feature)\n        ax.scatter(x = data[feature], y = data['SalePrice'])\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()\n        ","75019146":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","b79e8c78":"corrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","4a35ed85":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","5298b790":"for feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        \n        #data[feature]=np.log(data[feature])\n        fig, ax = plt.subplots()\n       # data.boxplot(column=feature)\n        ax.scatter(x = data[feature], y = data['SalePrice'])\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()","9cb3edbb":"# All 3 had same condition , so we can use below one code also, but it removes for \"Lotarea\" also\n# So did seperately below\n\n\"\"\"\nfor feature in continuous_feature:\n    #data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        \n        train = train.drop(train[(train[feature]>4000) & (train['SalePrice']<300000)].index)\n        \n        #data[feature]=np.log(data[feature])\n        fig, ax = plt.subplots()\n       # data.boxplot(column=feature)\n        ax.scatter(x = train[feature], y = train['SalePrice'])\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()\n  \"\"\"      ","8c57582c":"#Deleting outliers in LotFrontage\ntrain = train.drop(train[(train['LotFrontage']>300) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['LotFrontage'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('LotFrontage', fontsize=13)\nplt.show()","1950b777":"#Deleting outliers in 1stFlrSf\n\n#Deleting outliers in GrLivArea\ntrain = train.drop(train[(train['1stFlrSF']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['1stFlrSF'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()","90e64779":"#Deleting outliers in GrLivArea\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","765dae79":"## Before Log ND\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","b1920efe":"## After Log ND  --> Apply to train data\n## Log-transformation of the target variable\n\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","4cd710dc":"ntrain = train.shape[0]\nntest = test.shape[0]\nprint(\"ntrain :\", ntrain,\"ntest :\",ntest )\n\n#Output column\ny_train = train.SalePrice.values\n\n# Concatenate both, here train has slaesprice also\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\n# Drop slaesprice \nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","c4c0a48e":"print(train.shape[1])\nprint(test.shape[1])","c81737bc":"#Same one sorted way\n\n# It displays col name as index and calculated % as column\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\n#print(all_data_na.index) \nprint(all_data_na) \n\n\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)\n","30a69200":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","7d005535":"## Let us capture all the nan values\n## First lets handle Categorical features which are missing\ncat_features_nan=[feature for feature in all_data.columns if all_data[feature].isnull().sum()>1 and all_data[feature].dtypes=='O']\n\n\nfor feature in cat_features_nan:\n    all_data_na  = np.round((all_data[feature].isnull().sum()\/  len(all_data)*100),4)\n    print(\"{}: {}% missing values\".format(feature, all_data_na))\n    \n","e302d356":"# This is generic code, can replace to Missing for all categorical variable, \n# but we commented this and used individual replace fr both categorical and numeric\n## Replace missing value with a new label\n\"\"\"\ndef replace_cat_feature(dataset,cat_features_nan):\n    data=dataset.copy()\n    data[cat_features_nan]=data[cat_features_nan].fillna('Missing')\n    return data\n\n# Here it replaces all NULL categorial variable with value = 'Missing'\nall_data=replace_cat_feature(all_data,cat_features_nan)\n\nall_data[cat_features_nan].isnull().sum()\n\"\"\"","200b9603":"## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in all_data.columns if all_data[feature].isnull().sum()>1 and all_data[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(all_data[feature].isnull().mean(),4)))","716d8cde":"# This is generic code, can replace to median_value for all Numerical variable, \n# but we commented this and used individual replace for both categorical and numeric\n## Replace missing value with a new label\n\"\"\"\n## Replacing the numerical Missing Values\n\nfor feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=all_data[feature].median()\n    \n    ## Here created a new feature to capture nan values like indicator, if 1 then null value, if 0 then non null\n   \n    all_data[feature+'nan']=np.where(all_data[feature].isnull(),1,0)\n    all_data[feature].fillna(median_value,inplace=True)\n    \nall_data[numerical_with_nan].isnull().sum()\n    \"\"\"","38cc902a":"#all_data_na[:,3]","dab234b9":"# PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","68f65444":"#MiscFeature : data description says NA means \"no misc feature\"\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","adb637db":"#Alley : data description says NA means \"no alley access\"\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","2ec87a80":"#Fence : data description says NA means \"no fence\"\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","a92b62db":"#FireplaceQu : data description says NA means \"no fireplace\"\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","1a131c19":"#GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","b6024490":"#BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","7101b5b0":"# MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","1ae28a7c":"#MSZoning (The general zoning classification) : 'RL' is by far the most common value. \n#So we can fill in missing values with 'RL'\nall_data['MSZoning'].value_counts()\n","8034feb8":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","3f46ae7a":"#Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\nall_data = all_data.drop(['Utilities'], axis=1)","757b54ba":"# Functional : data description says NA means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","fdbf259b":"#Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n# Here replaced by most frequent value in field 'Electrical'\nprint(all_data['Electrical'].mode())\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","37344ad8":"#KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","93fe6090":"# Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","0d500e71":"# SaleType : Fill in again with most frequent which is \"WD\"\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['SaleType'].mode()","3f4379bd":"# MSSubClass : Na most likely means No building class. We can replace missing values with None\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","f77e1696":"# LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","eb52411d":"#GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","f1ff53f4":"# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","27978fef":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","0029a61b":"all_data.head(50)","fa3aadc6":"# Check is there any missing value present\n\n[features for features in all_data.columns if all_data[features].isnull().sum()>1]","1aec2f36":"year_feature","b8519be2":"# Before applying Temporal handling we will replace NULL for this value with median, else it creates problem later\nfor feature in year_feature:\n    ## We will replace by using median since there are outliers\n    median_value=all_data[feature].median()\n    print(median_value)\n    ## Here created a new feature to capture nan values like indicator, if 1 then null value, if 0 then non null\n   # dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)\n    all_data[feature].fillna(median_value,inplace=True)\n    \nall_data[year_feature].isnull().sum()","6252acd9":"all_data[year_feature].isnull().sum()","a15dd62a":"# Check is there any missing value present\n[features for features in all_data.columns if all_data[features].isnull().sum()>1]","048c615c":"year_feature","a15a83f2":"## Temporal Variables (Date Time Variables)\n\nfor feature in year_feature:\n    \n    if feature != 'YrSold' :\n       \n        all_data[feature]=all_data['YrSold']-all_data[feature]","3e8bf289":" all_data.loc[:,['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']].head()\n# dataset[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","d6cf757d":"all_data.GarageYrBlt.isnull().sum()","0f3c8a5a":"all_data[all_data.GarageYrBlt.isnull()]","c6e2ea3e":"all_data.head()","8a437331":"#Here we apply  log transformation only to selected numerical fields, which dont have 0\n\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n\nfor feature in num_features:\n    all_data[feature]=np.log1p(all_data[feature])","1f061c6e":"all_data.head()","ac9851a8":"categorical_features=[feature for feature in all_data.columns if all_data[feature].dtype=='O']","c6e1ed5d":"categorical_features","f48a28ab":"all_data.head(20)","357cad19":"# Here in each feature, its category value wise count divided by total row count--> \n# This we do to get % value of that category value\n#Here index will be each category \n\nfor feature in categorical_features:\n    # This gives each categorywise %\n    \n   #temp=dataset.groupby(feature)['SalePrice'].count()\/len(dataset)\n    temp=all_data.groupby(feature)[feature].count()\/len(all_data)\n    print(temp)\n    \n    # if temp greater than 0.01 then, its not a rare category\n    temp_df=temp[temp>0.01].index\n    print(\"This are non rare categories in feature:\", temp_df,\"\\n\")\n    \n    # Here temp_df has non rare feature, if dataset value is in  temp_df--> Then keeps existing category value else replaces with 'Rare_var' \n    all_data[feature]=np.where(all_data[feature].isin(temp_df),all_data[feature],'Rare_var')\n    ","6b5b1f9a":"all_data.head(20)","7fe43684":"#This is krish naik used code, it also works  fine, but now we dont have SalePrice in all_data, so commented this and did minor cahnge in that\n\nfor feature in categorical_features:\n    # Here grouped based on category, then count of the feature categoy (mean of the SalePrice) is calculated are sorted and index(Category value) value only taken into labels_ordered\n    #After grouping index is the category value\n    # Here enumerate(labels_ordered,0) passes i as 0,1,2,3 value and k as category value which is stored in labels_ordered\n    #  k:i  -->> Creates Dict starting from 1--> {1: 'RM',2: 'RH', 3:'RL' } etc\n    # Then this dict used inside map function and replaced in the dataset\n    \n    # labels_ordered=all_data.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered=all_data.groupby([feature])[feature].count().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    all_data[feature]=all_data[feature].map(labels_ordered)\n","15ff0202":"all_data.head(20)","e86edede":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# This statement gives feature name as index and col as skew value in Descending order\n# all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","c65f601e":"# Before apply Boxcox transformation\nall_data.head()","344a2025":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n","af3a1dec":"#boxcox\n\nskewed_features = skewness.index\n\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","c16c8a27":"all_data.head()","d47dd8a3":"\"\"\"scaling_feature=[feature for feature in all_data.columns if feature not in ['Id','SalePrice'] ]\nlen(scaling_feature)\nall_data.head(20)\n\"\"\"","637ae510":"\"\"\"\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nscaler=MinMaxScaler()\nscaler.fit(all_data[scaling_feature])\n\n#Here we are not replaced the scaled feature data in the final data\n#This is in array, we will convert to DF in next step\n\nscaler.transform(all_data[scaling_feature])\n\n# transform the train and test set, and add on the Id and SalePrice variables\n\n#Here we are replaced the scaled feature data and concatenated 'Id', 'SalePrice'\n\ndata = pd.concat(\n                    [all_data[['Id', 'SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(all_data[scaling_feature]), columns=scaling_feature)\n                    ],\n                axis=1)\n\ndata.head(20)\n\n\"\"\"","2ec18cdc":"all_data[ntrain:]","f73d7528":"#Train set\ntrain = all_data[:ntrain]\n# Test set\ntest = all_data[ntrain:]\ntrain.head()","7d391ff6":"test.head()","8a89b09e":"test_ID.shape","019313c3":"test.shape","b1f1c98b":"train_ID.shape","5d992270":"train.shape\n# In train few outlier records are dropped, so we wont append trai_id","83edb65b":"# SalePrice\ny_train.shape ","f7e6c967":"# Dataframe test and test_ID are having different index, so reseting it, else we cant concatenate properly\ntest=test.reset_index(drop=True)","ce3fb7ba":"#Convert array to DF\nsalesprice = pd.DataFrame(y_train, columns=[ 'SalePrice'])\n\n\n# Concatenate both,train_ID,train,y_train\/salesprice\n#train_Processed = pd.concat([train_ID,train,salesprice], sort=True, axis = 1)\ntrain_Processed = pd.concat([train,salesprice], sort=False, axis = 1)\n\n# Concatenate both,test_ID,test\ntest_Processed = pd.concat([test_ID,test], axis = 1)\n\nprint(train_Processed.shape)\ntrain_Processed.head()","5e2bc0aa":"print(test_Processed.shape)\ntest_Processed.head()\n","6b68ccdc":"#Load this data to new csv file \n\n# Kaggle path\ntrain_Processed.to_csv(r'\/kaggle\/working\/train_Processed.csv',float_format='%.3f', index=False)\ntest_Processed.to_csv(r'\/kaggle\/working\/test_Processed.csv',float_format='%.3f',index=False)\n\n#Local path\n#train_Processed.to_csv(r'.\/Dataset\/train_Processed.csv',float_format='%.3f', index=False)\n#test_Processed.to_csv(r'.\/Dataset\/test_Processed.csv',float_format='%.3f',index=False)\n","4ee47b35":"> Looks like median house price was high on 2007  and then reduced, ITS NOT THE REAL Case, so we will compare with other years as well","4a6cb7a9":"## 2.7 Outliers\n\n#### 2.7.1 Using Boxplot\n\n- Apply Boxplot for all continuous_feature are check is there any outlier , This is not used for discrete variable","38fc59f6":"> In Histogram X axis, we can see many range of values\n\n> Here Most of the continous values are not follwed Normal distribution, only last and few feature looks normally distributed , \n\n> Normally in Linear regression problem, if data is not ND and skewed more, then we  apply Log transformation to convert to Normal distribution","6b71a7e7":"## 2.2 Numerical Variables","f78da19d":"#### 2.4.2.2  LOG NORMAL DISTRIBUTION : logarithmic transformation - ON Continuous Variables and plotting  below 2 plot\n\n- 1. Histogram \"Continous feature\" \n- 2. Scatterplot \"Continous feature\" VS \"Salesprice\"\n\n\n- Continous feature has huge range of values, so applyng Log function to reduce the range and shrink it and makes normal distributed\n\n- Here we are not changing original data with log, just applying in copied data and visualizing it","21904126":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 2. Exploratory Data Analysis (EDA) with Graphs and Visualization  <\/h2>\n<\/div>\n\n### Here in EDA we are not handling any missing value etc, just visualizing it,  in Feature Engineering We will do transformation\n","c6e5e4dd":"## KAGGLE INTERMEDIATE FILE HANDLING\n\n\n- In kaggle, we uploaded this notepad and ran completely.\n- It created new processed files  in \"\/kaggle\/working\/*_Processed.csv\" path\n- Then we downloaded this files in my local\n- Uploaded another kernel  \"House_Price_Prediction_Model_Training_2\"\n- There in data section, we uploaded\/added  (Add data) these 2 files \"*_Processed.csv\" from my local download path and used in this kernel\/notepad\n- It craetes new folder in \"input path\" same as file name(converts uppercase to lower and _ to - in new folder name)\n- \"\/kaggle\/input\/train-processedcsv\/train_Processed.csv\"\n\n- [We can't use directly intermediate files in another kernels, so followed this]","d7e305fa":"### 3.2.6.1 Box Cox Transformation of (highly) skewed features\n\n- We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\n- Note that setting  \u03bb=0  is equivalent to log1p used above for the target variable.\n\n- See this page for more details on Box Cox Transformation as well as the scipy function's page","0a93b042":"## 3.2.4 Handling Rare Categorical Feature\nWe will Replace categorical variables that are present less than 1% of the observations\n - Here if any categorical value occurs less than 1%, only those categorical values are replaced as **'Rare_var' **","4ea75e31":"#### 2.4.2.1 Histogram\/Count of Continuous Variables ","d2969a01":"> Few features has many no of categoeies like Neighborhood =25, Exterior1st  =15 , Exterior2st  =16 etc\n\n> If less category we can directly use one-hot encoding, but for many category we will handle different way in FE section,(Else Onehot encoding simply increases no of col)","fa3a257a":"### Now our data is ready with all needed Feature Engineering\n\n- Now we will split to original Train and test data ","539e1ac4":"#####  Here it replaces all NULL Numerical variable with value = median()","fb595359":"#### 2.7.2 Using Z-score\n\n- Here Z-score applied to see the outlier. If z-score>3 then its a outlier","674d0057":"## 2.9 Target variable analysis\n\nSalePrice is the variable we need to predict. So let's do some analysis on this variable first.\n\n- Plot the Distribution of SalePrice by **distplot** and it provides fitted parameter after fit. print mean(mu) and STD (sigma) --> These 2 are fitted parameter","62390e74":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> House Price Prediction: Advanced Regression Techniques  <\/h2>\n<\/div>\n\n\n- The main aim of this project is to predict the house price based on various huge no of features \n- Predict sales prices and practice feature engineering, RFs, and gradient boosting\n\n- Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\n- With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n### Practice Skills\n- Creative feature engineering \n- Advanced regression techniques like random forest and gradient boosting\n\n\n### Acknowledgments\n- The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n    \n    \n### Goal\n- It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n### Metric\n- Submissions are evaluated on **Root-Mean-Squared-Error (RMSE)** between the logarithm of the predicted value and the logarithm of the observed sales price. \n- **Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.**\n\n### Submission File Format\n- The file should contain a header and have the following format:\n\n            Id,SalePrice\n            1461,169000.1\n            1462,187724.1233\n            1463,175221\n            etc.","539bd97e":"###  2.3.1 Lets analyze the Temporal Datetime Variables\n#### We will check whether there is a relation between year the house is sold and the sales price\n","d05107ac":"#### 2.7.3 Using Scatter Plot - Feature vs SalesPrice\n\n- Here Z-score applied to see the outlier. If z-score>3 then its a outlier","ef21585a":"## 2.10 Check correlation \n- Correlation map to see how features are correlated with SalePrice","80eadc8b":"<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3. Feature Engineering  <\/h2>\n<\/div>\n\n## Here we are doing all handling, in EDA We just visualized it, not done any transformation. Here we are doing transformation in final dataset\n\n### 1. Before Train and test  merge:\n\n1. Only in Train Outlier handling: Remove outlier (Its not good practice, here only 2 extreme values, so deleted)\n\n2. Target variable analysis -  Make it to Normal Distribution by log transformation\n\n[ **This train test merge and apply all FE only done in kaggle kind competation. In real world scenario, we wont be knowing test  and leads to data leakage also**]\n\n\n## 2. After Train and test  merge:\n- We will be performing all the below steps in Feature Engineering\n\n1. Missing values Handling\n2. Temporal variables Conversion  -- Like Year,Yr data to Numeric differenced data based on Year_sold\n3. Numerical Variables - Since the numerical variables are skewed we will perform log normal distribution\n4. Categorical variables: remove rare labels and replaced as 'Rare_var'\n5. Convert Categorial varible to Numeric (Not using one hot encoding)\n6. Skewed Numerical features  - Box Cox Transformation of (highly) skewed features\n\t- For linear regression, data should be ND, but here features are skewed little. By this Box Cox Transformation we converted to ND\n\n7. Standarise the values of the variables to the same range - MinMax Scalar (0-1) \n\t- But here we didnt do scalarization, Reason: after Box Cox Transformation All feature looked like scarized\n","0a554c47":"## 3.2.7 Apply Standard Scalarization to all numerical data\n\n- But here we didnt do scalarization, **Reason: after Box Cox Transformation All feature looked like scalarized**\n\n- Here 'Id','SalePerice' (Output col) are excluded, Scalarization appled to total 83 column\n\n- Standardization cannot be done before the partitioning(INCLUDING Y lable), as we don't want to fit the StandardScaler on some observations(Output field) that will later be used in the test set.","9985993b":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> To be continued in \"House_Price_Prediction_Model_Training_2\" Notepad <\/h2>\n<\/div>","8d8b34e6":"## 2.6. Cardinality of Categorical Variables","9eb2f3be":"<div class=\"alert alert-info\" style=\"background-color:#005b02; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> TYPE OF PROBLEM:  <\/h2>\n<\/div>\n\n\n## Building Machine Learning Pipelines: Data Analysis Phase\n\nHere we are creating Machine Learning Pipelines considering all the life cycle of a Data Science Projects. \n\n> This is Regression Problem, so we will use Regression models like below and use related evaluation matrics\n\n          1. Linear Regression\n          2. Robust Regression\n          3. Ridge Regression\n          4. KERNEL Ridge Regression\n          5. LASSO Regression\n          6. Elastic Net\n          7. Polynomial Regression\n          8. Gradient Boosting Regressor\n          9. Stochastic Gradient Descent (SGD)\n          10. Random Forest Regressor\n          11. Support Vector Machine Regression\n          12. XGBOOST\n          13. lightgbm\n          14. Artificial Neaural Networks","5a927b05":"> Check is there any missing value left","3be23abf":"#####  Here it replaces all NULL categorial variable with value = 'Missing'\n\n- Imputing missing values\n","02f12245":"> Here With  the relation between the missing values and the dependent variable is clearly visible.So We need to replace these nan values with something meaningful which we will do in the Feature Engineering section\n\n> Almost all missing value feature also have sales price related to it, so we should handle null value(If we had 0 sales price for all null value feature, we could have ignored\/deleted whole record instead of null handle","020fb99f":"## 3.2.2 Temporal variables\n\n- Here we are converting Temporal variables like year field. Here we are converting to number by substracting each year by YrSold\n\n- So this represents values in no of years","5200e9c5":"## 3.1.1 Outlier handling\n\n>  In SalesPrice vs LotFrontage plot: \n\n- We can see at the bottom right two with extremely **large LotFrontage** that are of a **low price**. These values are huge oultliers. Therefore, we can safely delete them.\n\n>  In SalesPrice vs 1stFlrSf plot: \n\n- We can see at the bottom right one with extremely **large 1stFlrSf** that are of a **low price**. These values are huge oultliers. Therefore, we can safely delete them.\n\n\n>  In SalesPrice vs GrLivArea plot: \n\n- We can see at the bottom right two with extremely **large GrLivArea** that are of a **low price**. These values are huge oultliers. Therefore, we can safely delete them.\n\n\n> These 3 we will remove \n","0bee056e":"> Now all the categorical  values replaced to numeric. Now dataset has only numerical data","96482bba":"\n\n#### Dataset to downloaded from the below link\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\n\n## File descriptions\n\n    train.csv - the training set\n    test.csv - the test set\n    data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n    sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n\n## Data fields\n        Here's a brief version of what you'll find in the data description file.\n\n        SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n        MSSubClass: The building class\n        MSZoning: The general zoning classification\n        LotFrontage: Linear feet of street connected to property\n        LotArea: Lot size in square feet\n        Street: Type of road access\n        Alley: Type of alley access\n        LotShape: General shape of property\n        LandContour: Flatness of the property\n        Utilities: Type of utilities available\n        LotConfig: Lot configuration\n        LandSlope: Slope of property\n        Neighborhood: Physical locations within Ames city limits\n        Condition1: Proximity to main road or railroad\n        Condition2: Proximity to main road or railroad (if a second is present)\n        BldgType: Type of dwelling\n        HouseStyle: Style of dwelling\n        OverallQual: Overall material and finish quality\n        OverallCond: Overall condition rating\n        YearBuilt: Original construction date\n        YearRemodAdd: Remodel date\n        RoofStyle: Type of roof\n        RoofMatl: Roof material\n        Exterior1st: Exterior covering on house\n        Exterior2nd: Exterior covering on house (if more than one material)\n        MasVnrType: Masonry veneer type\n        MasVnrArea: Masonry veneer area in square feet\n        ExterQual: Exterior material quality\n        ExterCond: Present condition of the material on the exterior\n        Foundation: Type of foundation\n        BsmtQual: Height of the basement\n        BsmtCond: General condition of the basement\n        BsmtExposure: Walkout or garden level basement walls\n        BsmtFinType1: Quality of basement finished area\n        BsmtFinSF1: Type 1 finished square feet\n        BsmtFinType2: Quality of second finished area (if present)\n        BsmtFinSF2: Type 2 finished square feet\n        BsmtUnfSF: Unfinished square feet of basement area\n        TotalBsmtSF: Total square feet of basement area\n        Heating: Type of heating\n        HeatingQC: Heating quality and condition\n        CentralAir: Central air conditioning\n        Electrical: Electrical system\n        1stFlrSF: First Floor square feet\n        2ndFlrSF: Second floor square feet\n        LowQualFinSF: Low quality finished square feet (all floors)\n        GrLivArea: Above grade (ground) living area square feet\n        BsmtFullBath: Basement full bathrooms\n        BsmtHalfBath: Basement half bathrooms\n        FullBath: Full bathrooms above grade\n        HalfBath: Half baths above grade\n        Bedroom: Number of bedrooms above basement level\n        Kitchen: Number of kitchens\n        KitchenQual: Kitchen quality\n        TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n        Functional: Home functionality rating\n        Fireplaces: Number of fireplaces\n        FireplaceQu: Fireplace quality\n        GarageType: Garage location\n        GarageYrBlt: Year garage was built\n        GarageFinish: Interior finish of the garage\n        GarageCars: Size of garage in car capacity\n        GarageArea: Size of garage in square feet\n        GarageQual: Garage quality\n        GarageCond: Garage condition\n        PavedDrive: Paved driveway\n        WoodDeckSF: Wood deck area in square feet\n        OpenPorchSF: Open porch area in square feet\n        EnclosedPorch: Enclosed porch area in square feet\n        3SsnPorch: Three season porch area in square feet\n        ScreenPorch: Screen porch area in square feet\n        PoolArea: Pool area in square feet\n        PoolQC: Pool quality\n        Fence: Fence quality\n        MiscFeature: Miscellaneous feature not covered in other categories\n        MiscVal: $Value of miscellaneous feature\n        MoSold: Month Sold\n        YrSold: Year Sold\n        SaleType: Type of sale\n        SaleCondition: Condition of sale","a83e16e7":"\"\"\"\nord_fields=['MSSubClass','ExterQual','LotShape','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1', \n            'BsmtFinType2','HeatingQC','Functional','FireplaceQu','KitchenQual', 'GarageFinish',\n            'GarageQual','GarageCond','PoolQC','Fence']\n\norders=[ #msclass ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180','190'], \n#ExterQual ['Fa','TA','Gd','Ex'], #LotShape ['Reg','IR1' ,'IR2','IR3'], \n#BsmtQual ['None','Fa','TA','Gd','Ex'], #BsmtCond ['None','Po','Fa','TA','Gd','Ex'], \n#BsmtExposure ['None','No','Mn','Av','Gd'], #BsmtFinType1 ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], \n#BsmtFinType2 ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], #HeatingQC ['Po','Fa','TA','Gd','Ex'],\n#Functional ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], \n#FireplaceQu ['None','Po','Fa','TA','Gd','Ex'], #KitchenQual ['Fa','TA','Gd','Ex'],\n#GarageFinish ['None','Unf','RFn','Fin'], #GarageQual ['None','Po','Fa','TA','Gd','Ex'],\n#GarageCond ['None','Po','Fa','TA','Gd','Ex'], #PoolQC ['None','Fa','Gd','Ex'], #Fence ['None','MnWw','GdWo','MnPrv','GdPrv'] ]\n\nfor i in range(len(orders)): \n    ord_en=OrdinalEncoder(categories = {0:orders[i]}) \n    all_data.loc[:,ord_fields[i]]=ord_en.fit_transform(all_data.loc[:,ord_fields[i]].values.reshape(-1,1))\n    \n    \n\"\"\"","643b883f":"## 3.2.6   Skewed Numerical features \n\n- For Numerical feature, we will check the Skewness\n\n- Not clear how it calculated skewness\n\n\n- Skewness Skewness is a measure of the symmetry in a distribution. A symmetrical dataset will have a skewness equal to 0. So, a normal distribution will have a skewness of 0. Skewness essentially measures the relative size of the two tails\n- As a rule of thumb, skewness should be between -1 and 1. In this range, data are considered fairly symmetrical.\n- We can handle skewness by applying log to all numeric variable or by Box-Cox Transformation etc","655bc3c4":"#### 2.4.1.1 Relationship between Discrete Variables and Sales Price","87835654":"## Find out the relationship between  each categoeies of categorical variable and dependent feature SalesPrice","9fbf0ebd":"##### Note :\n- Outliers removal is not always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\n\n- There are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them.","0cb93de4":"<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 1. Import all Librariy  <\/h2>\n<\/div>\n","840db60e":"\n>  In SalesPrice vs LotFrontage plot: \n\n- We can see at the bottom right two with extremely **large LotFrontage** that are of a **low price**. These values are huge oultliers. Therefore, we can safely delete them.\n\n>  In SalesPrice vs 1stFlrSf plot: \n\n- We can see at the bottom right one with extremely **large 1stFlrSf** that are of a **low price**. These values are huge oultliers. Therefore, we can safely delete them.\n\n\n>  In SalesPrice vs GrLivArea plot: \n\n- We can see at the bottom right two with extremely **large GrLivArea** that are of a **low price**. These values are huge oultliers. Therefore, we can safely delete them.\n\n\n> These 2 we will remove \n\n\n> These 2 we can handle in below FE part","17f1f16c":"> No missing values now","66f0900d":"### 3.2.1 Categorical variable Missing Values","242c1774":"> The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n\n> In below Feature engineering part, we will apply Log distribution to Output Salesprice to make it ND","977fca40":"> After Logscale, all continous value looks like follows Normal Distribution","7a1a29cb":"## Lifecycle In A Data Science Projects\n1. Data Analysis(EDA)\/ Data Cleaning\n2. Feature Engineering \n    - Null Handling,Outlier Handling, Numeric and Categorical feature handling, Sacling\n3. Feature Selection\n4. Model Building\n5. Model Evaluation\n6. Finalize Best model\n7. Apply same model to Test data, Rescale O\/P back and Create Final Submission file\n5. Upload Final Submission file in Kaggle or Model Deployment \n\n## Splitted these activities into 2 sets\n\n1. This notepad will do all activities till preprocessing and Feature selctionIn \n2. \"House_Price_Prediction_Model_Training_2\" Notepad will do all Model build, evaluation and Final submission file cration","ef4b3b29":"## 2.4 Distribution of the Numerical Variables\n\n#### Numerical variables are usually of 2 type\n\n-  1. Discrete Variables\n        \n        If distinct values in numeric variable is <25, those are considered as discrete,else its continous \n       \n        Bar Plot used for dicrete variable\n        \n        \n-  2. Continous variable \n\n     Histogram Plot used for Continous variable","84dbbb79":"> Below log1p function apply log by adding 1 to it  --> log1p, this is basically equal to log(1+x). \n\n- A non normal positively skewed random variable can be converted into a normally distributed random variable by applying log1p transformation.\n\n- Whenever you have positively skewed data, we apply log transformation to bring them to same scale. Eg: if data was 1, 10,1000,10000 etc then applying log would change them to 0,1,3,4.\n\n- However, remember that log only works when all elements are greater than zero.  In case your data is positively skewed and also contains 0 then we can not apply log transformation directly \n\n- and hence we add 1 to every element and then apply log i.e. log(1+x) which in numpy can be done by log1p (meaning log 1 plus x)\n\n- To check whether the log1p transformation worked i.e. our variable actually became normal after applying log1p, we run any statistical tests (Anderson Darling Test, KS Test etc) or simply check the probability plot. \n\n- If the data is distributed along the diagonal line of the plot then it means that the data is normal and the transformation worked as expected.","7811647b":"### 2.4.1 Discrete Variables","aec19573":"### 3.5.1 Feature Scaling - MinMax Scalarization\n\n","79b9fd98":"### Drop the Id column","2dd05249":"#### In Data Analysis We will Analyze To Find out the below stuff\n1. Check any Missing Values\n2. Check All The Numerical Variables\n3. Temporal Variables(Eg: Datetime Variables)\u00b6\n4. Distribution of the Numerical Variables (Discret and Continous)\n5. Check Categorical Variables \n6. Cardinality of Categorical Variables (How many distinct categories in categorical feature)\n7. Outliers Check (This is not used for discrete variable, only for Continous variable)\n8. Relationship between independent and dependent feature(SalePrice)\n9. Target variable analysis\n10. Check correlation \n","ab1e993f":"##### We deleted some outlier data in train, so we are not concatenating id here for train","9ccfc515":"### Since they are many missing values, we need to find the relationship between missing values and Sales Price\n\nLet's plot some diagram for this relationship\n","c9f26fb8":"### 2.4.2 Continuous Variables  ","50f2e3ea":"### ALL Numercal - Missing data Imputation","ac75acee":"### Good different way EDA done here\nhttps:\/\/www.kaggle.com\/erikbruin\/house-prices-lasso-xgboost-and-a-detailed-eda","eb33d699":"> After Log normal distribution data follows proper ND","f32bfb55":"## 2.3 Temporal Variables(Eg: Datetime Variables)\n\nFrom the Dataset we have 4 year variables. We have extract information from the datetime variables like no of years or no of days. One example in this specific scenario can be difference in years between the year the house was built and the year the house was sold. We will be performing this analysis in the Feature Engineering.","1bace00e":"## 3.2.5 Convert Categorical variable to numerical data\n\n- Here based on MEAN \"salesprice\" of each categorical values in each feature we give number. \n- if that category value Mean is more no in that feature then that addresses as 1 then 2 like that\n\n- This also another way to make categorical, but not used here\n\nall_data['YrSold'] = all_data['YrSold'].astype(str)","5d317954":"### Done same Feature engineering above for Both train and test data ","fc994203":">  There is a relationship between discrete variable number and SalePrice, if no increases sales price increases in many cases","cb55dd5c":"\n**Few Missing values are valid values**\n- **PoolQC** : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n- **MiscFeature** : data description says NA means \"no misc feature\"\n- **Alley** : data description says NA means \"no alley access\"\n- **Fence** : data description says NA means \"no fence\"\n- **FireplaceQu** : data description says NA means \"no fireplace\"\n- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\/Missing value\n- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath **: missing values are likely zero for having no basement\n- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 **: For all these categorical basement-related features, NaN means that there is no basement.\n- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n- **MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n\n","2b047a44":"## 2.1 Missing Values","1a6cd55d":"### ALL Categorical  - Missing data Imputation","21868f52":"## Concatenate the Train and test --> THEN DO FE\n\n- Let's first concatenate the train and test data in the same dataframe","872fec02":"## 3.2.1 Missing Values\n\n### Below includes all, categorical and Numeric","5c9542d7":"### 3.1.2 Numerical variable Missing Values","de11933b":"> \"Other year field\" value near to \"year sold\" then price is high(Means its a new house), if difference is large(If its 140 yr old house) then price is less\nand values distributed with all different prices for same year difference in starting","85683b5c":"> Ex: Here \" Condition2 \" field has Artiery category as rare caegory, so it replaced as \"Rare_var\"","702466b7":"## 2.5 Categorical Variables","d0aa74fe":"> Most of the features are correlated","3c85b18e":"## 3.2.3 Numerical Variables - LOG NORMAL Distribution\n\nSince the numerical variables are skewed we will perform log normal distribution, so convert skewed data to log transfermed data\n\n- Here we are taken numerical variable, which dont have 0","a21b2c6a":"## 3.1.2  Target variable analysis\n\n- SalePrice is the variable we need to predict. So let's do some analysis on this variable first.\n\n- Plot the Distribution of SalePrice by **distplot** and it provides fitted parameter after fit. print mean(mu) and STD (sigma) --> These 2 are fitted parameter\n\n- We found skewed distribution, so applied Lognormal distribution to OP and made ND"}}