{"cell_type":{"18f6c29a":"code","463052c0":"code","fc863cfa":"code","b2532c10":"code","24b48f7b":"code","9142ac4b":"code","24915d31":"code","67d9fbed":"code","f625db12":"code","9cbc0f90":"code","d459ffc7":"code","2507e2bd":"code","80c3684d":"code","04648cf7":"code","8419ef1d":"code","7c9d84b2":"code","5320b80c":"code","5a65bfb4":"code","0c81f8cd":"code","1f367fe0":"code","d0be1699":"code","26e2a241":"code","b58d4b11":"code","180c4cf8":"code","eb450f0a":"code","35057c14":"code","d11c7527":"code","49a4d802":"code","b10cdbb9":"markdown","575f4ff6":"markdown","37c06836":"markdown","0ac5b94f":"markdown","59383330":"markdown","9a8eb67a":"markdown","0d3f8feb":"markdown","6ab8fc2c":"markdown","f2376e58":"markdown","d8209c7a":"markdown","ada52a0a":"markdown","09f3a7dc":"markdown","24ae9f71":"markdown","f8e44a6a":"markdown","333f5e6e":"markdown","f13f4198":"markdown","0ad139ca":"markdown","ae787111":"markdown"},"source":{"18f6c29a":"from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()","463052c0":"digits.images.shape","fc863cfa":"print(digits.images[0])","b2532c10":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.matshow(digits.images[0], cmap=plt.cm.Greys)","24b48f7b":"digits.data.shape","9142ac4b":"digits.target.shape","24915d31":"digits.target","67d9fbed":"from sklearn.model_selection import train_test_split\n#from sklearn.cross_validation import train_test_split : do not use this line of code because you will get an error like this ModuleNotFoundError: No module named 'sklearn.cross_validation', so try the line above this one \nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)","f625db12":"X_train.shape","9cbc0f90":"X_test.shape","d459ffc7":"from sklearn.svm import LinearSVC","2507e2bd":"svm = LinearSVC(C=0.1)","80c3684d":"svm.fit(X_train, y_train)","04648cf7":"print(svm.predict(X_train))\nprint(y_train)","8419ef1d":"svm.score(X_train, y_train)","7c9d84b2":"svm.score(X_test, y_test)","5320b80c":"from sklearn.ensemble import RandomForestClassifier","5a65bfb4":"rf = RandomForestClassifier(n_estimators=50)","0c81f8cd":"rf.fit(X_train, y_train)","1f367fe0":"rf.score(X_test, y_test)","d0be1699":"import numpy as np\nimport pylab as pl\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split \n#from sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Lin Discr Analysis\", \"Quad Discr Analysis\"]\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\nfigure = pl.figure(figsize=(21, 7))\ni = 1\n# iterate over datasets\nfor ds in datasets:\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = pl.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = pl.subplot(len(datasets), len(classifiers) + 1, i)\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = pl.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, m_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot also the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nfigure.subplots_adjust(left=.02, right=.98)\nfigure.savefig('ClassifierComparison.png')","26e2a241":"import numpy as np\nX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\ny = np.array([1, 1, 2, 2])\nfrom sklearn.svm import SVC\nSVCClf = SVC(kernel = 'linear',gamma = 'scale', shrinking = False,)\nSVCClf.fit(X, y)","b58d4b11":"\nSVCClf.coef_","180c4cf8":"SVCClf.predict([[-0.5,-0.8]])","eb450f0a":"SVCClf.n_support_","35057c14":"SVCClf.support_","d11c7527":"SVCClf.intercept_","49a4d802":"SVCClf.fit_status_","b10cdbb9":"Example\n\nSimilarly, we can get the value of other attributes as follows \u2212","575f4ff6":"## And again","37c06836":"**Data is always a numpy array (or sparse matrix) of shape (n_samples, n_features)**","0ac5b94f":"1) Instantiate an object and set the parameters","59383330":"credits to : https:\/\/www.tutorialspoint.com\/scikit_learn\/scikit_learn_support_vector_machines.htm ","9a8eb67a":"## Really Simple API\n\n0) Import your model class. For this example, we'll use [Linear Support Vector Classification](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html). [More on SVM at Jose's class on Jan 25!]","0d3f8feb":"2) Fit the model","6ab8fc2c":"This chapter deals with a machine learning method termed as Support Vector Machines (SVMs).\n\nIntroduction\nSupport vector machines (SVMs) are powerful yet flexible supervised machine learning methods used for classification, regression, and, outliers\u2019 detection. SVMs are very efficient in high dimensional spaces and generally are used in classification problems. SVMs are popular and memory efficient because they use a subset of training points in the decision function.\n\nThe main goal of SVMs is to divide the datasets into number of classes in order to find a maximum marginal hyperplane (MMH) which can be done in the following two steps \u2212\n\nSupport Vector Machines will first generate hyperplanes iteratively that separates the classes in the best way.\n\nAfter that it will choose the hyperplane that segregate the classes correctly.\n\nSome important concepts in SVM are as follows \u2212\n\nSupport Vectors \u2212 They may be defined as the datapoints which are closest to the hyperplane. Support vectors help in deciding the separating line.\n\nHyperplane \u2212 The decision plane or space that divides set of objects having different classes.\n\nMargin \u2212 The gap between two lines on the closet data points of different classes is called margin.\n\nFollowing diagrams will give you an insight about these SVM concepts \u2212","f2376e58":"3) Apply \/ evaluate","d8209c7a":"Attributes\nFollowings table consist the attributes used by sklearn.svm.SVC class \u2212\n\nSr.No\tAttributes & Description\n1\t\nsupport_ \u2212 array-like, shape = [n_SV]\n\nIt returns the indices of support vectors.\n\n2\t\nsupport_vectors_ \u2212 array-like, shape = [n_SV, n_features]\n\nIt returns the support vectors.\n\n3\t\nn_support_ \u2212 array-like, dtype=int32, shape = [n_class]\n\nIt represents the number of support vectors for each class.\n\n4\t\ndual_coef_ \u2212 array, shape = [n_class-1,n_SV]\n\nThese are the coefficient of the support vectors in the decision function.\n\n5\t\ncoef_ \u2212 array, shape = [n_class * (n_class-1)\/2, n_features]\n\nThis attribute, only available in case of linear kernel, provides the weight assigned to the features.\n\n6\t\nintercept_ \u2212 array, shape = [n_class * (n_class-1)\/2]\n\nIt represents the independent term (constant) in decision function.\n\n7\t\nfit_status_ \u2212 int\n\nThe output would be 0 if it is correctly fitted. The output would be 1 if it is incorrectly fitted.\n\n8\t\nclasses_ \u2212 array of shape = [n_classes]\n\nIt gives the labels of the classes.","ada52a0a":"![](https:\/\/www.tutorialspoint.com\/scikit_learn\/images\/marginal_hyperplane.jpg)","09f3a7dc":"## Visual comparison of classifiers","24ae9f71":"# Scikit Learn - Support Vector Machines","f8e44a6a":"SVM in Scikit-learn supports both sparse and dense sample vectors as input.\n\nClassification of SVM\nScikit-learn provides three classes namely SVC, NuSVC and LinearSVC which can perform multiclass-class classification.\n\nSVC\nIt is C-support vector classification whose implementation is based on libsvm. The module used by scikit-learn is sklearn.svm.SVC. This class handles the multiclass support according to one-vs-one scheme.\n\nParameters\nFollowings table consist the parameters used by sklearn.svm.SVC class \u2212","333f5e6e":"**Implementation Example**\n\nLike other classifiers, SVC also has to be fitted with following two arrays \u2212\n\nAn array X holding the training samples. It is of size [n_samples, n_features].\n\nAn array Y holding the target values i.e. class labels for the training samples. It is of size [n_samples].\n\nFollowing Python script uses sklearn.svm.SVC class \u2212**","f13f4198":"Example\n\nNow, once fitted, we can get the weight vector with the help of following python script \u2212\n","0ad139ca":"Parameters\nFollowings table consist the parameters used by sklearn.svm.SVC class \u2212\n\nSr.No\tParameter & Description\n1\t\nC \u2212 float, optional, default = 1.0\n\nIt is the penalty parameter of the error term.\n\n2\t\nkernel \u2212 string, optional, default = \u2018rbf\u2019\n\nThis parameter specifies the type of kernel to be used in the algorithm. we can choose any one among, \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019. The default value of kernel would be \u2018rbf\u2019.\n\n3\t\ndegree \u2212 int, optional, default = 3\n\nIt represents the degree of the \u2018poly\u2019 kernel function and will be ignored by all other kernels.\n\n4\t\ngamma \u2212 {\u2018scale\u2019, \u2018auto\u2019} or float,\n\nIt is the kernel coefficient for kernels \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\n\n5\t\noptinal default \u2212 = \u2018scale\u2019\n\nIf you choose default i.e. gamma = \u2018scale\u2019 then the value of gamma to be used by SVC is 1\/(\ud835\udc5b_\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\u2217\ud835\udc4b.\ud835\udc63\ud835\udc4e\ud835\udc5f()).\n\nOn the other hand, if gamma= \u2018auto\u2019, it uses 1\/\ud835\udc5b_\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60.\n\n6\t\ncoef0 \u2212 float, optional, Default=0.0\n\nAn independent term in kernel function which is only significant in \u2018poly\u2019 and \u2018sigmoid\u2019.\n\n7\t\ntol \u2212 float, optional, default = 1.e-3\n\nThis parameter represents the stopping criterion for iterations.\n\n8\t\nshrinking \u2212 Boolean, optional, default = True\n\nThis parameter represents that whether we want to use shrinking heuristic or not.\n\n9\t\nverbose \u2212 Boolean, default: false\n\nIt enables or disable verbose output. Its default value is false.\n\n10\t\nprobability \u2212 boolean, optional, default = true\n\nThis parameter enables or disables probability estimates. The default value is false, but it must be enabled before we call fit.\n\n11\t\nmax_iter \u2212 int, optional, default = -1\n\nAs name suggest, it represents the maximum number of iterations within the solver. Value -1 means there is no limit on the number of iterations.\n\n12\t\ncache_size \u2212 float, optional\n\nThis parameter will specify the size of the kernel cache. The value will be in MB(MegaBytes).\n\n13\t\nrandom_state \u2212 int, RandomState instance or None, optional, default = none\n\nThis parameter represents the seed of the pseudo random number generated which is used while shuffling the data. Followings are the options \u2212\n\nint \u2212 In this case, random_state is the seed used by random number generator.\n\nRandomState instance \u2212 In this case, random_state is the random number generator.\n\nNone \u2212 In this case, the random number generator is the RandonState instance used by np.random.\n\n14\t\nclass_weight \u2212 {dict, \u2018balanced\u2019}, optional\n\nThis parameter will set the parameter C of class j to \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60_\ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61[\ud835\udc57]\u2217\ud835\udc36 for SVC. If we use the default option, it means all the classes are supposed to have weight one. On the other hand, if you choose class_weight:balanced, it will use the values of y to automatically adjust weights.\n\n15\t\ndecision_function_shape \u2212 ovo\u2019, \u2018ovr\u2019, default = \u2018ovr\u2019\n\nThis parameter will decide whether the algorithm will return \u2018ovr\u2019 (one-vs-rest) decision function of shape as all other classifiers, or the original ovo(one-vs-one) decision function of libsvm.\n\n16\t\nbreak_ties \u2212 boolean, optional, default = false\n\nTrue \u2212 The predict will break ties according to the confidence values of decision_function\n\nFalse \u2212 The predict will return the first class among the tied classes.","ae787111":"Split the data to get going"}}