{"cell_type":{"c06934cc":"code","aa5ed510":"code","857b05c0":"code","415294c4":"code","be1be829":"code","0178c100":"code","420d623a":"code","26895490":"code","28ddefa5":"code","ffcc93dc":"code","b04dea55":"code","a2d27fab":"code","2c569b60":"code","2e862919":"code","30627d82":"code","eabd2f7c":"code","503e4771":"code","12a1327c":"code","99d936f8":"code","29858104":"code","a01fb2da":"code","78bf41c0":"code","ff29d174":"code","32afc4c0":"code","0619d40f":"code","7214eefb":"markdown","4ea2a6c5":"markdown","87102db2":"markdown","34243f7e":"markdown","cfce5d00":"markdown","7fb90a87":"markdown","51cd37fd":"markdown","cd0237eb":"markdown","82ca5d2d":"markdown","8af1ad08":"markdown"},"source":{"c06934cc":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image, ImageFile\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nImageFile.LOAD_TRUNCATED_IMAGES=True","aa5ed510":"t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\nt_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\nt_c = torch.tensor(t_c)\nt_u = torch.tensor(t_u)","857b05c0":"def model(t_u, w, b):\n    return w * t_u + b","415294c4":"def loss_fn(t_p, t_c):\n    squared_diffs = (t_p - t_c)**2\n    return squared_diffs.mean()","be1be829":"w = torch.randn(t_c.shape)\nb = torch.zeros(t_c.shape)\n\nt_p = model(t_u, w, b)\nloss_fn(t_p,t_c)","0178c100":"delta = 0.1\nlearning_rate = 1e-2\n\nloss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) \/ (2.0 * delta)\nloss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) \/ (2.0 * delta)\nw = w - learning_rate * loss_rate_of_change_w\nb = b - learning_rate * loss_rate_of_change_b","420d623a":"def dloss_fn(t_p, t_c):\n    dsq_diffs = 2 * (t_p - t_c) \/ t_p.size(0)  # <1>\n    return dsq_diffs\n\ndef dmodel_dw(t_u, w, b):\n    return t_u\n\ndef dmodel_db(t_u, w, b):\n    return 1.0","26895490":"def grad_fn(t_u, t_c, t_p, w, b):\n    dloss_dtp = dloss_fn(t_p, t_c)\n    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n    return torch.stack([dloss_dw.sum(), dloss_db.sum()])","28ddefa5":"def training_loop(n_epochs, learning_rate, params, t_u, t_c,print_params=True):\n    for epoch in range(1, n_epochs + 1):\n        w, b = params\n\n        t_p = model(t_u, w, b)  \n        loss = loss_fn(t_p, t_c)\n        grad = grad_fn(t_u, t_c, t_p, w, b)  \n\n        params = params - learning_rate * grad\n\n        if epoch in {1, 2, 3, 10, 100, 1000, 5000}:\n            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n            if print_params:\n                print('    Params:', params)\n                print('    Grad:  ', grad)\n\n        if not torch.isfinite(loss).all():\n            break\n            \n    return params","ffcc93dc":"t_un = (t_u - t_u.mean()) \/ t_u.std()","b04dea55":"training_loop(\n    n_epochs = 500, \n    learning_rate = 1e-2, \n    params = torch.tensor([1.0, 0.0]), \n    t_u = t_un, \n    t_c = t_c)","a2d27fab":"params = torch.tensor([1.0, 0.0], requires_grad=True)\nlearning_rate = 1e-2\noptimizer = optim.SGD([params], lr=learning_rate)","2c569b60":"def training_loop(n_epochs, optimizer, params, t_u, t_c):\n    for epoch in range(1, n_epochs + 1):\n        \n        t_p = model(t_u, *params) \n        loss = loss_fn(t_p, t_c)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n            \n    return params","2e862919":"training_loop(\n    n_epochs = 500, \n    optimizer = optimizer,\n    params = params,\n    t_u = t_un,\n    t_c = t_c)","30627d82":"train_indices, val_indices = train_test_split(list(range(t_c.shape[0])))\ntrain_t_u = t_u[train_indices]\ntrain_t_c = t_c[train_indices]\nval_t_u = t_u[val_indices]\nval_t_c = t_c[val_indices]","eabd2f7c":"train_t_un = (train_t_u - train_t_u.mean()) \/ train_t_u.std()\nval_t_un = (val_t_u - train_t_u.mean()) \/ train_t_u.std()","503e4771":"def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,train_t_c, val_t_c):\n    \n    for epoch in range(1, n_epochs + 1):\n        \n        train_t_p = model(train_t_u, *params)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n        with torch.no_grad():\n            val_t_p = model(val_t_u, *params)\n            val_loss = loss_fn(val_t_p, val_t_c)\n            assert val_loss.requires_grad == False\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n                  f\" Validation loss {val_loss.item():.4f}\")\n            \n    return params","12a1327c":"training_loop(\n    n_epochs = 500, \n    optimizer = optimizer,\n    params = params,\n    train_t_u = train_t_un,\n    val_t_u = val_t_un,\n    train_t_c = train_t_c,\n    val_t_c = val_t_c)","99d936f8":"t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\nt_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\nt_c_batch = torch.tensor(t_c).unsqueeze(1)\nt_u_batch = torch.tensor(t_u).unsqueeze(1)\n\ntrain_t_u = t_u_batch[train_indices]\ntrain_t_c = t_c_batch[train_indices]\nval_t_u = t_u_batch[val_indices]\nval_t_c = t_c_batch[val_indices]\n","29858104":"train_t_un = (train_t_u - train_t_u.mean()) \/ train_t_u.std()\nval_t_un = (val_t_u - train_t_u.mean()) \/ train_t_u.std()","a01fb2da":"NUM_NEURONS = 20\nseq_model = nn.Sequential(\n            nn.Linear(1, NUM_NEURONS),\n            nn.Tanh(),\n            nn.Linear(NUM_NEURONS, 1))\nseq_model","78bf41c0":"for name, param in seq_model.named_parameters():\n    print(name, param.shape)","ff29d174":"optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)","32afc4c0":"def training_loop(n_epochs, optimizer, model, loss_fn, train_t_u, val_t_u,train_t_c, val_t_c):\n\n    for epoch in range(1, n_epochs + 1):\n    \n        train_t_p = model(train_t_u)\n        train_loss = loss_fn(train_t_p, train_t_c)\n\n        with torch.no_grad():\n            val_t_p = model(val_t_u)\n            val_loss = loss_fn(val_t_p, val_t_c)\n            assert val_loss.requires_grad == False\n            \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n                  f\" Validation loss {val_loss.item():.4f}\")\n    ","0619d40f":"training_loop(\n    n_epochs = 500, \n    optimizer = optimizer,\n    model = seq_model,\n    loss_fn = nn.MSELoss(),\n    train_t_u = train_t_un,\n    val_t_u = val_t_un,\n    train_t_c = train_t_c,\n    val_t_c = val_t_c)","7214eefb":"**Optimizers**\n- Adam\n- RMSprop\n- SGD","4ea2a6c5":"**Train - Test Split**\n- Be aware of memory usage","87102db2":"- backpropogation to estimate gradients\n- autograd\n- optimization of weights","34243f7e":"**Deep Neural Networks**\n- Hidden layers and unknown features\n- Activation functions (sigmoid, tanh, relu, leaky relu)","cfce5d00":"![ANN](https:\/\/i.morioh.com\/2019\/11\/05\/17923e19eeaf.jpg)","7fb90a87":"![A layer](https:\/\/hackernoon.com\/hn-images\/1*zjzWdMucBfRbkqMzgm2xKg.png)","51cd37fd":"- Find celcius from fahrenheit temperature via deep learning\n    - Linear model : w (weights) * t_u (fahrenheit) + b (error) = t_c (celcius)\n    - loss function : mean squared error\n    - gradient : get derivative with respect to w, t_u and b","cd0237eb":"**Top Deep Learning Tools**\n- PyTorch\n    - fastai\n    - [MMDetection](https:\/\/github.com\/open-mmlab\/mmdetection)\n    - [Detectron2](https:\/\/github.com\/facebookresearch\/detectron2)\n- Tensorflow\n    - Keras\n    - [Tensorpack](hhttps:\/\/github.com\/tensorpack\/tensorpack)\n- Caffe\n- Theano\n- ....","82ca5d2d":"**Tensors vs Numpy arrays**\n- perform fast operations on GPU\n- distribute operations on multiple GPU \/ CPU's\n- keep track of operations on tensors\n- efficient memory usage\n- faster since written with C++","8af1ad08":"[Deep Learning with PyTorch](https:\/\/pytorch.org\/assets\/deep-learning\/Deep-Learning-with-PyTorch.pdf)\n<br>[Libgen.xx](http:\/\/libgen.rs\/)"}}