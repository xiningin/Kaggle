{"cell_type":{"d248b800":"code","94246ed6":"code","f53930bd":"code","c3ee18f5":"code","52661c0b":"code","fca42b00":"code","2051b43f":"code","e0a3f6a9":"code","a0c3c189":"code","47cb0d7f":"code","3df95d63":"code","c6527a1d":"code","849e8bc0":"code","c4c196b2":"code","4ffc8822":"code","fd6e2c58":"code","7757e363":"code","131ceafc":"code","72e7fed4":"code","e110b953":"code","619137e6":"code","88a89187":"code","5964c8ce":"code","f8d1d1d5":"code","c380025e":"code","8efd1305":"code","4c66319c":"code","9e221469":"code","faa1f336":"code","18fdce1b":"code","6e8a7e91":"code","8157d804":"code","7fe1964e":"code","91072d1b":"code","9d90465a":"code","30b6b508":"code","df370c0c":"code","db090e70":"code","db411a8a":"code","ca562c06":"code","2c6308b6":"code","ce06be4d":"code","7b65e545":"code","ce011b8a":"code","196c669e":"code","eecae686":"code","0997d230":"code","21909315":"code","e2a78f6a":"code","95d180e9":"code","3ac55654":"code","43664024":"code","0edf45ec":"code","f9c1f415":"code","aca31f42":"code","850b7b18":"code","1d83832e":"code","3946b0cb":"code","34d9296f":"code","1c9ac556":"code","085c97b8":"code","fc58fdf1":"code","588da807":"code","ba7e6656":"code","1342075c":"code","a2cf8091":"code","7e51516f":"code","b561d094":"code","6562c57b":"code","aaada9a6":"code","10fdc6a6":"code","7c48cbd3":"code","24143bec":"code","4f8c18b2":"markdown","5d678563":"markdown","336c1707":"markdown","b15e2489":"markdown"},"source":{"d248b800":"import sys\n\n# for kaggle kernel\n# add datasets iterative-stratification and umaplearn\n\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nsys.path.append('..\/input\/umaplearn\/umap')\n%mkdir model\n%mkdir interim\n\nfrom scipy.sparse.csgraph import connected_components\nfrom umap import UMAP\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, RepeatedMultilabelStratifiedKFold\n\nimport numpy as np\nimport scipy as sp\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\nimport time\n# import joblib\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nprint(f\"is cuda available: {torch.cuda.is_available()}\")\n\nimport warnings\n# warnings.filterwarnings('ignore')\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nDEFAULT_SEED = 512\nseed_everything(seed_value=DEFAULT_SEED)","94246ed6":"# file name prefix\nNB = '101'\n\nIS_TRAIN = False ################################################################\n\nMODEL_DIR = \"..\/input\/503-203-tabnet-with-nonscored-features-train\/model\" # \"..\/model\"\nINT_DIR = \"interim\" # \"..\/interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0","f53930bd":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","c3ee18f5":"# test_features_dummy = pd.read_csv('..\/input\/dummytestfeatures\/test_features_dummy.csv')\n# test_features = pd.concat([test_features, test_features_dummy]).reset_index(drop=True)","52661c0b":"from sklearn.preprocessing import QuantileTransformer\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfor col in (GENES + CELLS):\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')        \n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","fca42b00":"# GENES\nn_comp = 50\nn_dim = 15\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=DEFAULT_SEED).fit(train_features[GENES])\n    umap = UMAP(n_components=n_dim, random_state=DEFAULT_SEED).fit(train_features[GENES])\n    pd.to_pickle(pca, f\"{MODEL_DIR}\/{NB}_pca_g.pkl\")\n    pd.to_pickle(umap, f\"{MODEL_DIR}\/{NB}_umap_g.pkl\")\nelse:\n    pca = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_pca_g.pkl\")\n    umap = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_umap_g.pkl\")\n    \ndata2 = pca.transform(data[GENES])\ndata3 = umap.transform(data[GENES])\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_G-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_G-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n#CELLS\nn_comp = 15\nn_dim = 5\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\n\nif IS_TRAIN:\n    pca = PCA(n_components=n_comp, random_state=DEFAULT_SEED).fit(train_features[CELLS])\n    umap = UMAP(n_components=n_dim, random_state=DEFAULT_SEED).fit(train_features[CELLS])\n    pd.to_pickle(pca, f\"{MODEL_DIR}\/{NB}_pca_c.pkl\")\n    pd.to_pickle(umap, f\"{MODEL_DIR}\/{NB}_umap_c.pkl\")\nelse:\n    pca = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_pca_c.pkl\")\n    umap = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_umap_c.pkl\")   \n\ndata2 = pca.transform(data[CELLS])\ndata3 = umap.transform(data[CELLS])\n\ntrain2 = data2[:train_features.shape[0]]\ntest2 = data2[-test_features.shape[0]:]\ntrain3 = data3[:train_features.shape[0]]\ntest3 = data3[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntrain3 = pd.DataFrame(train3, columns=[f'umap_C-{i}' for i in range(n_dim)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest3 = pd.DataFrame(test3, columns=[f'umap_C-{i}' for i in range(n_dim)])\n\ntrain_features = pd.concat((train_features, train2, train3), axis=1)\ntest_features = pd.concat((test_features, test2, test3), axis=1)\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]","2051b43f":"from sklearn.feature_selection import VarianceThreshold\n\nif IS_TRAIN:\n    var_thresh = VarianceThreshold(threshold=0.5).fit(train_features.iloc[:, 4:])\n    pd.to_pickle(var_thresh, f\"{MODEL_DIR}\/{NB}_variance_thresh0_5.pkl\")\nelse:\n    var_thresh = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_variance_thresh0_5.pkl\")\n                                \ndata = train_features.append(test_features)\ndata_transformed = var_thresh.transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint(train_features.shape)\nprint(test_features.shape)","e0a3f6a9":"train = train_features[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","a0c3c189":"train.to_pickle(f\"{INT_DIR}\/{NB}_train_preprocessed.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_test_preprocessed.pkl\")","47cb0d7f":"# file name prefix\nNB = '203'\n\n# IS_TRAIN = True\n\n# MODEL_DIR = \"model\" # \"..\/model\"\n# INT_DIR = \"interim\" # \"..\/interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0\n\n# model hyper params\nHIDDEN_SIZE = 2048\n\n# training hyper params\nEPOCHS = 15\nBATCH_SIZE = 2048\nNFOLDS = 10 # 10\nNREPEATS = 1\nNSEEDS = 5 # 5\n\n# Adam hyper params\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-5\n\n# scheduler hyper params\nPCT_START = 0.2\nDIV_FACS = 1e3\nMAX_LR = 1e-2","3df95d63":"def process_data(data):    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\ndef calc_valid_log_loss(train, target, target_cols):\n    y_pred = train[target_cols].values\n    y_true = target[target_cols].values\n    \n    y_true_t = torch.from_numpy(y_true.astype(np.float64)).clone()\n    y_pred_t = torch.from_numpy(y_pred.astype(np.float64)).clone()\n    \n    return torch.nn.BCELoss()(y_pred_t, y_true_t).to('cpu').detach().numpy().copy()","c6527a1d":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=HIDDEN_SIZE):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n               \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n                \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","849e8bc0":"def run_training(train, test, trn_idx, val_idx, feature_cols, target_cols, fold, seed):\n    \n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    train_df = train_.loc[trn_idx,:].reset_index(drop=True)\n    valid_df = train_.loc[val_idx,:].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=len(feature_cols),\n        num_targets=len(target_cols),\n    )\n    \n    model.to(DEVICE)\n       \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=PCT_START, div_factor=DIV_FACS, \n                                              max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    best_loss_epoch = -1\n    \n    if IS_TRAIN:\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n\n            if valid_loss < best_loss:            \n                best_loss = valid_loss\n                best_loss_epoch = epoch\n                oof[val_idx] = valid_preds\n                model.to('cpu')\n                torch.save(model.state_dict(), f\"{MODEL_DIR}\/{NB}_nonscored_SEED{seed}_FOLD{fold}_.pth\")\n                model.to(DEVICE)\n\n            if epoch % 10 == 0 or epoch == EPOCHS-1:\n                print(f\"seed: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}, best_loss: {best_loss:.6f}, best_loss_epoch: {best_loss_epoch}\")                           \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=len(feature_cols),\n        num_targets=len(target_cols),\n    )\n    \n    model.load_state_dict(torch.load(f\"{MODEL_DIR}\/{NB}_nonscored_SEED{seed}_FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    if not IS_TRAIN:\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        oof[val_idx] = valid_preds\n\n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","c4c196b2":"def run_k_fold(train, test, feature_cols, target_cols, NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    mskf = RepeatedMultilabelStratifiedKFold(n_splits=NFOLDS, n_repeats=NREPEATS, random_state=None)\n    \n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        oof_, pred_ = run_training(train, test, t_idx, v_idx, feature_cols, target_cols, f, seed)\n        \n        predictions += pred_ \/ NFOLDS \/ NREPEATS\n        oof += oof_ \/ NREPEATS\n        \n    return oof, predictions","4ffc8822":"def run_seeds(train, test, feature_cols, target_cols, nfolds=NFOLDS, nseed=NSEEDS):\n    seed_list = range(nseed)\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_start = time.time()\n\n    for seed in seed_list:\n\n        oof_, predictions_ = run_k_fold(train, test, feature_cols, target_cols, nfolds, seed)\n        oof += oof_ \/ nseed\n        predictions += predictions_ \/ nseed\n        print(f\"seed {seed}, elapsed time: {time.time() - time_start}\")\n\n    train[target_cols] = oof\n    test[target_cols] = predictions","fd6e2c58":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","7757e363":"train = pd.read_pickle(f\"{INT_DIR}\/101_train_preprocessed.pkl\")\ntest = pd.read_pickle(f\"{INT_DIR}\/101_test_preprocessed.pkl\")","131ceafc":"train_trainbook = pd.read_pickle(\"..\/input\/503-203-tabnet-with-nonscored-features-train\/interim\/101_train_preprocessed.pkl\")\ntest_trainbook = pd.read_pickle(\"..\/input\/503-203-tabnet-with-nonscored-features-train\/interim\/101_test_preprocessed.pkl\")","72e7fed4":"train_trainbook.head()","e110b953":"train.head()","619137e6":"test_trainbook.head()","88a89187":"test.head()","5964c8ce":"# remove nonscored labels if all values == 0\ntrain_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\nprint(train_targets_nonscored.shape)\n\ntrain = train.merge(train_targets_nonscored, on='sig_id')","f8d1d1d5":"target = train[train_targets_nonscored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfeature_cols = [c for c in process_data(train).columns if c not in target_cols and c not in ['kfold','sig_id']]","c380025e":"run_seeds(train, test, feature_cols, target_cols)","8efd1305":"print(f\"train shape: {train.shape}\")\nprint(f\"test  shape: {test.shape}\")\nprint(f\"features : {len(feature_cols)}\")\nprint(f\"targets  : {len(target_cols)}\")","4c66319c":"valid_loss_total = calc_valid_log_loss(train, target, target_cols)\nprint(f\"CV loss: {valid_loss_total}\")","9e221469":"train.to_pickle(f\"{INT_DIR}\/{NB}_train_nonscored_pred.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_test_nonscored_pred.pkl\")","faa1f336":"valid_results = train_targets_nonscored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_nonscored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","18fdce1b":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","6e8a7e91":"from pytorch_tabnet.tab_model import TabNetRegressor","8157d804":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","7fe1964e":"# file name prefix\nNB = '503'\nNB_PREV = '203'\n\n# IS_TRAIN = False\n\n# MODEL_DIR = \"..\/input\/moa503\/503-tabnet\" # \"..\/model\"\n# INT_DIR = \"..\/input\/moa503\/203-nonscored-pred\" # \"..\/interim\"\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# label smoothing\nPMIN = 0.0\nPMAX = 1.0\n\n# submission smoothing\nSMIN = 0.0\nSMAX = 1.0\n\n# model hyper params\n\n# training hyper params\n# EPOCHS = 25\n# BATCH_SIZE = 256\nNFOLDS = 10 # 10\nNREPEATS = 1\nNSEEDS = 3 # 5\n\n# Adam hyper params\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-5\n\n# scheduler hyper params\nPCT_START = 0.2\nDIV_FACS = 1e3\nMAX_LR = 1e-2","91072d1b":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","9d90465a":"# test_features_dummy = pd.read_csv('..\/input\/dummytestfeatures\/test_features_dummy.csv')\n# test_features = pd.concat([test_features, test_features_dummy]).reset_index(drop=True)","30b6b508":"print(\"(nsamples, nfeatures)\")\nprint(train_features.shape)\nprint(train_targets_scored.shape)\nprint(train_targets_nonscored.shape)\nprint(test_features.shape)\nprint(sample_submission.shape)","df370c0c":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","db090e70":"from sklearn.preprocessing import QuantileTransformer\n\nuse_test_for_preprocessing = False\n\nfor col in (GENES + CELLS):\n\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        if use_test_for_preprocessing:\n            raw_vec = pd.concat([train_features, test_features])[col].values.reshape(vec_len+vec_len_test, 1)\n            transformer.fit(raw_vec)\n        else:\n            raw_vec = train_features[col].values.reshape(vec_len, 1)\n            transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl') \n\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n\n\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","db411a8a":"# GENES\n\nn_comp = 90\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42).fit(data[GENES])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_g.pkl')\n    \ndata2 = (fa.transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\n\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\nif IS_TRAIN:\n    fa = FactorAnalysis(n_components=n_comp, random_state=42).fit(data[CELLS])\n    pd.to_pickle(fa, f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\nelse:\n    fa = pd.read_pickle(f'{MODEL_DIR}\/{NB}_factor_analysis_c.pkl')\n\ndata2 = (fa.transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","ca562c06":"# features_g = list(train_features.columns[4:776])\n# train_ = train_features[features_g].copy()\n# test_ = test_features[features_g].copy()\n# data = pd.concat([train_, test_], axis = 0)\n# km = KMeans(n_clusters=35, random_state=123).fit(data)","2c6308b6":"# km.predict(data)","ce06be4d":"# km.labels_","7b65e545":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        \n        if IS_TRAIN:\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n            pd.to_pickle(kmeans, f\"{MODEL_DIR}\/{NB}_kmeans_{kind}.pkl\")\n        else:\n            kmeans = pd.read_pickle(f\"{MODEL_DIR}\/{NB}_kmeans_{kind}.pkl\")\n            \n        train[f'clusters_{kind}'] = kmeans.predict(train_)\n        test[f'clusters_{kind}'] = kmeans.predict(test_)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)","ce011b8a":"print(train_features.shape)\nprint(test_features.shape)","196c669e":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","eecae686":"print(train_features.shape)\nprint(test_features.shape)","0997d230":"remove_vehicle = True\n\nif remove_vehicle:\n    trt_idx = train_features['cp_type']=='trt_cp'\n    train_features = train_features.loc[trt_idx].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[trt_idx].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[trt_idx].reset_index(drop=True)\nelse:\n    pass","21909315":"# train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# target = train[train_targets_scored.columns]\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","e2a78f6a":"print(target.shape)\nprint(train_features.shape)\nprint(test_features.shape)\nprint(train.shape)\nprint(test.shape)","95d180e9":"train_nonscored_pred = pd.read_pickle(f'{INT_DIR}\/{NB_PREV}_train_nonscored_pred.pkl')\ntest_nonscored_pred = pd.read_pickle(f'{INT_DIR}\/{NB_PREV}_test_nonscored_pred.pkl')","3ac55654":"# remove nonscored labels if all values == 0\ntrain_targets_nonscored = train_targets_nonscored.loc[:, train_targets_nonscored.sum() != 0]\n\n# nonscored_targets = [c for c in train_targets_nonscored.columns if c != \"sig_id\"]","43664024":"train = train.merge(train_nonscored_pred[train_targets_nonscored.columns], on='sig_id')\ntest = test.merge(test_nonscored_pred[train_targets_nonscored.columns], on='sig_id')","0edf45ec":"from sklearn.preprocessing import QuantileTransformer\n\nnonscored_target = [c for c in train_targets_nonscored.columns if c != \"sig_id\"]\n\nfor col in (nonscored_target):\n\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n#     raw_vec = pd.concat([train, test])[col].values.reshape(vec_len+vec_len_test, 1)\n    raw_vec = train[col].values.reshape(vec_len, 1)\n    if IS_TRAIN:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        transformer.fit(raw_vec)\n        pd.to_pickle(transformer, f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')\n    else:\n        transformer = pd.read_pickle(f'{MODEL_DIR}\/{NB}_{col}_quantile_transformer.pkl')        \n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","f9c1f415":"feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]\nlen(feature_cols)","aca31f42":"num_features=len(feature_cols)\nnum_targets=len(target_cols)","850b7b18":"import torch\nimport torch.nn as nn\nfrom pytorch_tabnet.metrics import Metric\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"\n    NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0, n_cls=2):\n        \"\"\"\n        Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing + smoothing \/ n_cls\n        self.smoothing = smoothing \/ n_cls\n\n    def forward(self, x, target):\n        probs = torch.nn.functional.sigmoid(x,)\n        # ylogy + (1-y)log(1-y)\n        #with torch.no_grad():\n        target1 = self.confidence * target + (1-target) * self.smoothing\n        #print(target1.cpu())\n        loss = -(torch.log(probs+1e-15) * target1 + (1-target1) * torch.log(1-probs+1e-15))\n        #print(loss.cpu())\n        #nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        #nll_loss = nll_loss.squeeze(1)\n        #smooth_loss = -logprobs.mean(dim=-1)\n        #loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n    \nclass SmoothedLogLossMetric(Metric):\n    \"\"\"\n    BCE with logit loss\n    \"\"\"\n    def __init__(self, smoothing=0.001):\n        self._name = f\"{smoothing:.3f}\" # write an understandable name here\n        self._maximize = False\n        self._lossfn = LabelSmoothing(smoothing)\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        \"\"\"\n        y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n        y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n#         print(\"smoothed log loss metric: \", self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy())\n        return self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy().take(0)\n    \nclass LogLossMetric(Metric):\n    \"\"\"\n    BCE with logit loss\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        self._name = f\"{smoothing:.3f}\" # write an understandable name here\n        self._maximize = False\n        self._lossfn = LabelSmoothing(smoothing)\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        \"\"\"\n        y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n        y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n#         print(\"log loss metric: \", self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy())\n        return self._lossfn(y_score, y_true).to('cpu').detach().numpy().copy().take(0)","1d83832e":"def process_data(data):\n#     data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2, 0: 0, 1: 1, 2: 2})\n    data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1, 0: 0, 1: 1})   \n    return data\n\ndef run_training_tabnet(train, test, trn_idx, val_idx, feature_cols, target_cols, fold, seed, filename=\"tabnet\"):\n    \n    seed_everything(seed)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    train_df = train_.loc[trn_idx,:].reset_index(drop=True)\n    valid_df = train_.loc[val_idx,:].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n        \n    model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0,\n                            cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                            mask_type='entmax',  # device_name=DEVICE,\n                            scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n                            verbose=10,\n                            seed = seed)\n    \n    loss_fn = LabelSmoothing(0.001)\n#     eval_metric = SmoothedLogLossMetric(0.001)\n#     eval_metric_nosmoothing = SmoothedLogLossMetric(0.)\n       \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    \n    if IS_TRAIN:\n#         print(\"isnan\", np.any(np.isnan(x_train)))\n        model.fit(X_train=x_train, y_train=y_train,\n                  eval_set=[(x_valid, y_valid)], eval_metric=[LogLossMetric, SmoothedLogLossMetric],\n                  max_epochs=200, patience=50, batch_size=1024, virtual_batch_size=128,\n                    num_workers=0, drop_last=False, loss_fn=loss_fn\n                  )\n        model.save_model(f\"{MODEL_DIR}\/{NB}_{filename}_SEED{seed}_FOLD{fold}\")\n            \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    \n    model = TabNetRegressor(n_d=32, n_a=32, n_steps=1, lambda_sparse=0,\n                            cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                            mask_type='entmax',  # device_name=DEVICE,\n                            scheduler_params=dict(milestones=[100, 150], gamma=0.9),#)\n                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n                            verbose=10,\n                            seed = seed)\n    \n    model.load_model(f\"{MODEL_DIR}\/{NB}_{filename}_SEED{seed}_FOLD{fold}.model\")\n\n    valid_preds = model.predict(x_valid)\n\n    valid_preds = torch.sigmoid(torch.as_tensor(valid_preds)).detach().cpu().numpy()\n    oof[val_idx] = valid_preds\n        \n    predictions = model.predict(x_test)\n    predictions = torch.sigmoid(torch.as_tensor(predictions)).detach().cpu().numpy()\n    \n    return oof, predictions","3946b0cb":"def run_k_fold(train, test, feature_cols, target_cols, NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state = seed)\n    \n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        oof_, pred_ = run_training_tabnet(train, test, t_idx, v_idx, feature_cols, target_cols, f, seed)\n        \n        predictions += pred_ \/ NFOLDS \/ NREPEATS\n        oof += oof_ \/ NREPEATS\n        \n    return oof, predictions\n\ndef run_seeds(train, test, feature_cols, target_cols, nfolds=NFOLDS, nseed=NSEEDS):\n    seed_list = range(nseed)\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_start = time.time()\n\n    for seed in seed_list:\n\n        oof_, predictions_ = run_k_fold(train, test, feature_cols, target_cols, nfolds, seed)\n        oof += oof_ \/ nseed\n        predictions += predictions_ \/ nseed\n        print(f\"seed {seed}, elapsed time: {time.time() - time_start}\")\n\n    train[target_cols] = oof\n    test[target_cols] = predictions","34d9296f":"train.to_pickle(f\"{INT_DIR}\/{NB}_pre_train.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_pre_test.pkl\")","1c9ac556":"run_seeds(train, test, feature_cols, target_cols, NFOLDS, NSEEDS)","085c97b8":"train.to_pickle(f\"{INT_DIR}\/{NB}_train.pkl\")\ntest.to_pickle(f\"{INT_DIR}\/{NB}_test.pkl\")","fc58fdf1":"# train[target_cols] = np.maximum(PMIN, np.minimum(PMAX, train[target_cols]))\nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_true = y_true > 0.5\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","588da807":"sub6 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub6.to_csv('submission.csv', index=False)","ba7e6656":"sub6","1342075c":"import glob\n","a2cf8091":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/gen-efficientnet-pretrained\/tf_efficientnet_*.pth \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/deepinsight-resnest-v2-resnest50-output\/resnest50_fast_2s2x40d-9d126481.pth \/root\/.cache\/torch\/hub\/checkpoints\/\n!ls -la \/root\/.cache\/torch\/hub\/checkpoints\/\n","7e51516f":"!python ..\/input\/markscripts\/deepinsight_resnest_lightning_v2_infer.py\nsub5 = pd.read_csv('submission_resnest_v2.csv')","b561d094":"!python ..\/input\/markscripts\/deepinsight_efficientnet_lightning_v7_b3_infer.py\nsub4 = pd.read_csv('.\/submission_effnet_v7_b3.csv')","6562c57b":"! python ..\/input\/updatedsimplenn\/simpleNN_without_ns_newcv.py\nsub3 = pd.read_csv('.\/submission.csv')","aaada9a6":"test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n!python ..\/input\/python-scripts-moa\/2heads_1836_oldcv.py\nsub2 = pd.read_csv('.\/submission.csv')","10fdc6a6":"!python ..\/input\/python2stagenn\/2stageNN_with_ns_oldcv.py\nsub1 = pd.read_csv('.\/submission_2stageNN_with_ns_oldcv_0.01822.csv')","7c48cbd3":"!python ..\/input\/simplennoldcvfinal\/script_NN_836_final.py\nsub7 = pd.read_csv('submission_script_simpleNN_oldcv_0.01836.csv')","24143bec":"submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nsubmission.iloc[:, 1:] = 0\nsubmission.iloc[:, 1:] = (sub1.iloc[:,1:]*0.44 + sub7.iloc[:,1:]*0.15 + sub4.iloc[:,1:]*0.35 +sub2.iloc[:,1:]*0.06)*0.78 + sub6.iloc[:,1:]*0.1 + sub7.iloc[:,1:]*0.05 + sub5.iloc[:,1:]*0.07\nsubmission.to_csv('submission.csv', index=False)","4f8c18b2":"## 203-101-nonscored-pred-2layers.ipynb","5d678563":"## 503-203-tabnet-with-nonscored-features-10fold3seed","336c1707":"## 101-preprocess.ipynb","b15e2489":"### non-scored labels prediction"}}