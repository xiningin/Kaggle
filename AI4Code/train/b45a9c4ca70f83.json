{"cell_type":{"ad9743d3":"code","1c291c36":"code","bf865f6c":"code","86d6f73d":"code","4845dc71":"code","eba33834":"code","707dbb12":"code","d529f1b9":"code","029a9b5e":"code","360647d4":"code","3acc799a":"code","f0f731f4":"code","4388ad2e":"code","963df5b8":"code","9eb3039d":"code","4eb920c9":"code","44a5dbe6":"code","e94f0df0":"code","5ad79681":"code","e094877b":"code","988f0dc0":"code","0660a329":"markdown","616ae874":"markdown","efc66f18":"markdown","57c42cb0":"markdown","9b2c0871":"markdown","f4286cc8":"markdown","55112ed1":"markdown","7b9e5678":"markdown","ff9719ff":"markdown","fff1ea32":"markdown","8cff31ad":"markdown","077788d7":"markdown","b37522ad":"markdown","f253bba5":"markdown","cfdf9762":"markdown","41270449":"markdown","741b575b":"markdown"},"source":{"ad9743d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.offline as py\nfrom numpy import linalg as LA\npy.init_notebook_mode(connected=True)\n\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","1c291c36":"#var_x1=0, var_x2=3\nmu1,mu2,N=0,3,10\ndef plot_gaussian(sigma1,sigma2):     #var = sigma*sigma\n    x1=np.random.normal(mu1,sigma1,N)    # gaussian distributed feature x1\n    x2=np.random.normal(mu2,sigma2,N)    # gaussian distributed feature x2\n    trace=go.Scatter(\n    x=x1,\n    y=x2,\n    mode='markers',\n    )\n    data=[trace]\n    layout=go.Layout(\n        xaxis=dict(\n                range=[-100,100]\n        ),\n        yaxis=dict(\n                range=[-100,100]\n        ),\n    )\n    fig=dict(data=data,layout=layout)\n    py.iplot(fig)","bf865f6c":"plot_gaussian(sigma1=0,sigma2=0)","86d6f73d":"plot_gaussian(sigma1=10,sigma2=0)","4845dc71":"plot_gaussian(sigma1=10,sigma2=2)","eba33834":"\n\nfrom  numpy.random import multivariate_normal as mvr_gauss\nfrom sklearn.preprocessing import StandardScaler\n\nnumber_samples=5000\nmu=np.array([6.0,4.0])   # define the means of the data distributed along both the axes, I chose mu-x=6 and mu-y=4\n#now the desired covariance matrix\n#the diagonal elements of the covariance matrix indicates the variance along the feature \n# while the off-diagonal elements shows the variance between the two features. i.e the covariance\ncvr=np.array([\n    [15,-4],                           \n    [-4,4]\n])\n\n#Generate the random samples\nxf=mvr_gauss(mu,cvr,number_samples)    # xf is now 5000 samples each with 2 features i.e a 5000 X 2 matrix\n\nstdsc=StandardScaler()                #standardization of data\nxf_std=stdsc.fit_transform(xf)\n\n#Create a plotly trace\ntrace=go.Scatter(\n    x=xf_std[:,0],              #xf[:,0] is the feature1\n    y=xf_std[:,1],              #xf[:,1] is the feature2\n    mode='markers',\n        \n)\ndata=[trace]\n\n#layout for naming the axes\nlayout=go.Layout(\n            title='example of 2D correlated data', autosize=False,\n        \n            xaxis=dict(title= 'feature 1',range=[-10,20]),\n            yaxis=dict(title= 'feature 2')#,range=[-10,20]),\n         )\n\n\nfig=dict(data=data,layout=layout)\npy.iplot(fig)\n\n","707dbb12":"\npxd=pd.DataFrame(xf_std)\n\n#calculate the covariance matrix of the dataframe\nsig=pxd.cov()\n\n#calculate the eigenvectors and eigenvalues\neigvals,eigvecs=LA.eig(sig)\n","d529f1b9":"#Create a plotly trace\nimport plotly.figure_factory as ff\nfrom plotly import tools\n\ntrace1=go.Scatter(\n    x=xf_std[:,0],              #xf_std[:,0] is the feature1\n    y=xf_std[:,1],              #xf_std[:,1] is the feature2\n    mode='markers',\n    opacity=0.5,\n    name='data'\n    \n)\n\n#fig=tools.make_subplots(1,1)\nmu_x=0\nmu_y=0\nx0,y0=[mu_x,mu_x],[mu_y,mu_y]\nu,v=eigvecs[:,0]*5,eigvecs[:,1]*5\n\nscale=1\nfig=ff.create_quiver(x0,y0,u,v,scale,arrow_scale=0.03)\n'''fig['layout']['autosize']=False\nfig['layout']['xaxis']['range']=[-8,8]\nfig['layout']['yaxis']['range']=[-8,8]\n'''\nfig.data[0].name='eigenvectors'\n#layout for naming the axes\n'''layout=go.Layout(\n            title='example of 2D correlated data',\n            autosize= False,\n            xaxis=dict(title= 'feature 1'),\n            yaxis=dict(title= 'feature 2'),\n            \n         )'''\nfigg=tools.make_subplots(rows=1,cols=1)\n#fig.append_trace(trace1,1,1)\nfigg.add_trace(fig.data[0],1,1)\nfigg.add_trace(trace1,1,1)\nfigg.layout.autosize=False\nfigg.layout.xaxis.range=[-6,6]\nfigg.layout.yaxis.range=[-6,6]\n\n#fig.layout=layout\n\n#fig=dict(data=data,layout=layout)\npy.iplot(figg)","029a9b5e":"t=np.array(eigvecs).T\nxf_p=np.dot(xf_std,t)\n","360647d4":"#Create a plotly trace\nimport plotly.figure_factory as ff\nfrom plotly import tools\n\ntrace1=go.Scatter(\n    x=xf_std[:,0],              #xf_std[:,0] is the feature1\n    y=xf_std[:,1],              #xf_std[:,1] is the feature2\n    mode='markers',\n    opacity=0.5,\n    name='original standardized data'\n        \n)\n#Transformed data\ntrace_transformed=go.Scatter(\n    x=xf_p[:,0],\n    y=xf_p[:,1],\n    mode='markers',\n    opacity=0.8,\n    name='transformed data' \n    \n)\n\n#fig=tools.make_subplots(1,1)\nmu_x=0\nmu_y=0\nx0,y0=[mu_x,mu_x],[mu_y,mu_y]\nu,v=eigvecs[:,0]*5,eigvecs[:,1]*5\nfigg=tools.make_subplots(rows=1,cols=2)\nscale=1\nfig=ff.create_quiver(x0,y0,u,v,scale,arrow_scale=0.03)\nfig['layout']['autosize']=False\nfig.data[0]['name']='eigenvectors'\n\n#layout for naming the axes\n'''layout=go.Layout(\n            title='example of 2D correlated data',\n            autosize= True,\n            xaxis=dict(title= 'feature 1'),\n            yaxis=dict(title= 'feature 2'),\n            \n         )\n'''\n\nfigg.add_trace(fig.data[0],1,1)\nfigg.add_trace(trace1,1,1)\nfigg.add_trace(trace_transformed,1,2)\nfigg.layout['xaxis2']['range']=[-6,6]\nfigg.layout['yaxis2']['range']=[-6,6]\nfigg.layout['xaxis2']['title']='eigenvector 1'\nfigg.layout['yaxis2']['title']='eigenvector 2'\nfigg.layout['xaxis']['title']='feature 1'\nfigg.layout['yaxis']['title']='feature 2'\n\npy.iplot(figg)","3acc799a":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n#constants\nNUMBER_OF_TRAINING_IMGS=5000\n\n#load the MNIST dataset\nlabeled_images=pd.read_csv('..\/input\/train.csv') \nimages=labeled_images.iloc[0:NUMBER_OF_TRAINING_IMGS,1:] # first NUMBER_OF_TRAINING_IMGS rows,column 2 onwards.\nlabels=labeled_images.iloc[0:NUMBER_OF_TRAINING_IMGS,:1] #first NUMBER_OF_TRAINING_IMGS rows, first column. \n\n#split into train-test\ntrain_images,test_images,train_labels,test_labels=train_test_split(images,labels,test_size=0.2,random_state=13)\n\n#standardize the data\n#stdsc=StandardScaler()\nstdsc=MinMaxScaler()\ntrain_images_std=stdsc.fit_transform(train_images)\ntest_images_std=stdsc.transform(test_images)\n#perform PCA on training data,getting all possible eigenvectors\npca=PCA(svd_solver='randomized',whiten=True)\npca.fit(train_images_std)\n","f0f731f4":"#print(pca.n_components_)\n#print(pca.explained_variance_ratio_)\ndef cummulative(ll):\n    nl=np.empty(len(ll))\n    for i in range(len(ll)):\n        if i==0:\n            nl[i]=ll[i]\n        else:\n            nl[i]=nl[i-1]+ll[i]\n    #print(nl)\n    return nl\/10\n\n\nfig=tools.make_subplots(rows=1,cols=1)\nbardata=go.Bar(\n    x=[xx for xx in range(pca.n_components_)],\n    y=[xx for xx in pca.explained_variance_ratio_],\n    opacity=1.0,\n    name='explained variance ratio'\n)\n\ncummulativeData=go.Bar(\n    x=[xx for xx in range(pca.n_components_)],\n    y=cummulative(pca.explained_variance_ratio_),\n    opacity=0.4,\n    name='cumulative sum (divided by 10)'\n    )\n\n\n#data=[bardata,cummulativeData]\nfig.add_trace(bardata,1,1)\nfig.add_trace(cummulativeData,1,1)\npy.iplot(fig)\n\n","4388ad2e":"pca=PCA(n_components=144,svd_solver='randomized',whiten=True)\ntrain_images_pca=pca.fit_transform(train_images_std)\ntest_images_pca=pca.transform(test_images_std)","963df5b8":"train_images_pca=pd.DataFrame(train_images_pca)\ntrain_images_pca.head()","9eb3039d":"import matplotlib.pyplot as plt\n\n\npd_train_images_std=pd.DataFrame(train_images_std)\n\nfig,axes=plt.subplots(figsize=(10,10),ncols=2,nrows=2)\naxes=axes.flatten()\nfor i in range(0,4):\n    jj=np.random.randint(0,train_images_std.shape[0])          #pick a random image\n    if i%2==0 :\n        IMG_HEIGHT=12\n        IMG_WIDTH=12\n        axes[i].imshow(train_images_pca.iloc[[jj]].values.reshape(IMG_HEIGHT,IMG_WIDTH))\n    else:\n        IMG_HEIGHT=28\n        IMG_WIDTH=28\n        axes[i].imshow(pd_train_images_std.iloc[[jj]].values.reshape(IMG_HEIGHT,IMG_WIDTH))\n    ","4eb920c9":"from sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(criterion='gini',random_state=1)\ntrain_labels.shape\nforest.fit(train_images_pca,train_labels.values.ravel())\n\n","44a5dbe6":"forest.score(train_images_pca,train_labels.values.ravel())","e94f0df0":"forest.score(test_images_pca,test_labels.values.ravel())","5ad79681":"subTest=pd.read_csv('..\/input\/test.csv')\nsubTest_sc=stdsc.transform(subTest)\npred=forest.predict(pca.transform(subTest_sc))\nsubmissions=pd.DataFrame({'ImageId':list(range(1,len(pred)+1)), 'Label':pred})\nsubmissions.head()","e094877b":"submissions.to_csv(\"mnist_pca_randForests_submit.csv\",index=False,header=True)","988f0dc0":"!ls","0660a329":"The data is more variant on the RHS diagram along the vertical axis (direction of eigenvector with the largest ${\\lambda}$)  \n\n**Why we standardize and the correlation matrix**  \nPCA can be defined using the covariance matrix and also using the correlation matrix of ${X}$. Since the two are related as follows:  \n${corr_{ij}=\\dfrac{cov_{ij}}{(\\sigma_{i}\\sigma_{j})}}$  where ${\\sigma_{i}}$ and ${\\sigma_{j}}$ are the standard deviations for the ${i}$-th and ${j}$-th features respectively.\n\nIf we are working with the covariance matrix of X, then the entries of the covariance matrix of X changes as the units used to express X changes. i.e. imagine 2 simple vector samples consisting of height and weight of a person ${{A}=\\matrix{174      & 85  \\\\\n    175       & 88  \\\\}}$ where the height is in cms and the weight in kgs . Now the same samples when  expressed in units of meters and kgs becomes ${{A}=\\matrix{1.74      & 85  \\\\\n    1.75       & 88  \\\\}}$    \n\n\nThe covariance matrix in the first case is  ${{s_{1}}=\\matrix{0.5      & 0.5  \\\\ \n    0.5       & 0.5  \\\\}}$ and for the second case its ${{s_{2}}=\\matrix{0.00005      & 0.005  \\\\\n    0.005       & 0.500  \\\\}}$ which leads to two different sets of eigenvectors.   \n   \n   Next lets consider the case of the correlation matrix . You will find that the correlation matrix in either of the two above cases comes down to   ${{corr}=\\matrix{1.0      & 1.0  \\\\\n    1.0  & 1.0  \\\\}}$      \n    \n   When we standardize the data, we are bringing it to zero mean and unit variance .  \n   The covariance matrix of such data is the same as it's correlation matrix based on the above relation between the two.  \n   Therefore upon standardizing the data , we end up with the same set of eigenvectors from correlation matrix as we do from the covariance matrix.","616ae874":"We note that **the direction along which there is  a large variation** helps distinguish the data better as compared to a direction where the variation isn't as large.  (easier to pick a red shirt out of 4 M sized tshirts). \n\nOk, so why was this exercise important?  \nBecause this ends up as one of the constraints that we will use while deriving the PCA formulation .  \n\nPCA is essentially a  transformation of the existing feature coordinate system to a new coordinate system, the feature vectors of which satisfy the following constraints:  \n\n1. The directions of the new vectors will  be ones where the variance of the data is maximized. (the variance argument we noted above)\n\n2. The new transformed axes should be orthogonal.\n\n3. Yet another constraint we might impose is that these basis vectors be of magnitude 1 i.e normalized. (This is to keep a constant scaling)\n\nSo out of all the possible vectors in this n-dimensional space we need to find a set of ${n}$-dimensional vectors that satisfy the above 3 constraints.  \n\n\nIf our original basis vector set is ${\\vec{B}=\\{\\vec{b_{1}},\\vec{b_{2}},\\cdots,\\vec{b_{n}}\\}}$ where each ${\\vec{b_{i}}}$ is a unit vector lying along the ${i}$-th feature axis then a single data sample ${\\vec{x}=x_{1}{\\vec{b_{1}}} + x_{2}{\\vec{b_{2}}} + \\cdots +x_{n}{\\vec{b_{n}}}   }$ where ${x_{i}}$ is it's ${i}$-th feature  \nSimilarly any vector ${\\vec{\\alpha_{i}}}$ in this space can then be expressed as  ${\\vec{\\alpha_{i}}=\\alpha_{1}\\vec{b_{1}}+\\alpha_{2}\\vec{b_{2}}+\\cdots+\\alpha_{n}\\vec{b_{n}}  }$    \n*Note that ${\\vec\\alpha_{i}}$ is not amongst the samples of our data. It is a direction in the space spanned by our basis vectors. i.e it is a vector that can be \"reached\" using a linear combination of our basis vectors*  \n\nNow we need this ${\\vec{\\alpha_{i}}}$ to fulfill all 3 of the above constraints. i.e.  \n1. Maximize variance of ${\\vec x}$ when ${\\vec x}$ is projected along ${\\vec{\\alpha_{i}}}$ i.e maximize the variance which can be expressed as  \n${var(\\vec\\alpha_{i}\n\\vec{x})=\\vec {\\alpha_{i}}  {\\sum} \\vec {\\alpha_{i}}^{T} }$ where ${\\sum}$ is the covariance matrix of the sample ${\\vec{x}}$\n2.  The new feature axes are orthogonal to each other. i.e . No one feature cannot be expressed as a linear combination of the remaining features.  This is expressed as ${<\\vec{\\alpha_{i}}^{T}.\\vec{\\alpha_{j}}>=0}$ i.e the inner product is zero . In a 2D sense, this implies that the dot product of the two new feature vectors are zero. No component of one feature is projected on another (think of the spatial ${\\hat{i},\\hat{j}}$ vectors that lie along the perpendicular x-y axes) .   \nThis also implies that the i-th Principal Component  ${\\vec{\\alpha_{i}}^{T}\\vec{x}}$ is uncorrelated to the j-th Principal Component  ${\\vec{\\alpha_{j}}^{T}\\vec{x}\\space\\implies\\space cov[{\\vec{\\alpha_{i}}^{T}\\vec{x}},{\\vec{\\alpha_{j}}^{T}\\vec{x}}]=0}$\n3.  ${||\\vec{\\alpha_{i}}||=\\vec {\\alpha}^{T} \\vec {\\alpha}=1}$\n\nLet's find the first such vector ${\\vec\\alpha_{1}}$  \nWe use the method of [Lagrange multipliers](https:\/\/en.wikipedia.org\/wiki\/Lagrange_multiplier) to find the correct ${\\vec \\alpha_{1}}$  that fulfills all the above contraints.\n\n So the equation we need to solve is  ${\\space\\space }$ ${\\vec{\\alpha_{1}}^{T} \\sum \\vec {\\alpha_{1}}   -\\lambda_{1}(\\vec {\\alpha_{1}}^{T} \\vec {\\alpha_{1}} -1)=0}$    ${\\space\\space\\space}$where ${\\lambda_{1} }$ is the Lagrange multiplier  \n\n${\\vec{\\alpha_{1}}^{T} \\sum \\vec {\\alpha_{1}}   -\\lambda_{1}(\\vec {\\alpha_{1}}^{T} \\vec {\\alpha_{1}} -1)=0}$  \n${ Differentiating \\space w.r.t. \\space {\\vec{\\alpha_{1}}^{T}}}$  \n${\\sum\\vec{\\alpha_{1}} -\\lambda_{1}\\vec{\\alpha_{1}}=0}$  \nwhich translates to ${\\sum\\vec{\\alpha_{1}}=\\lambda_{1}\\vec\\alpha_{1}}$ ${\\space}$ i.e. ${\\vec\\alpha_{1}}$ is an eigenvector of the covariance matrix of ${x}$ i.e  ${\\sum}$  with the eigenvalue ${\\lambda_{1}}$  \nAlso since ${\\sum\\vec{\\alpha_{1}}=\\lambda_{1}\\vec\\alpha_{1}}$ ${\\space\\implies\\space}$  ${\\vec{\\alpha_{1}}^{T}\\sum\\vec{\\alpha_{1}}=\\lambda_{1}\\vec{\\alpha}^{T}\\vec\\alpha_{1}}$ ${\\space\\implies\\space}$ ${\\vec{\\alpha_{1}}^{T}\\sum\\vec{\\alpha_{1}}=\\lambda_{1} \\space(since \\space \\vec {\\alpha_{1}}^{T} \\vec {\\alpha_{1}}=1 )}$ ${\\space\\implies\\space}$ ${var(\\vec\\alpha_{1}\n\\vec{x})=\\lambda_{1}}$   \n\ni.e **${\\lambda_{1}}$ is the variance of the data ${\\vec{x}}$ when measured along ${\\vec\\alpha_{1}}$.** \nIf there are p-eigenvectors for ${\\sum}$, we need the one that gives maximum variance i.e ${\\lambda_{1}}$ needs to be as large as possible. **Therefore ${\\vec\\alpha_{1}}$ corresponds to the eigenvector of ${\\sum}$ with the largest eigenvalue**\n\n\nInorder to find the next vector ${\\alpha_{2}}$ we again use the same technique. However note that while the previous find of ${\\alpha_{1}}$ took into account only two of the three constraints (max variance and normalization), for finding the next vector, we include the third constraint i.e orthogonality ${\\alpha_{2}^T \\alpha_{1}=0}$  \nAlso from #2 ${cov[{\\vec{\\alpha_{1}}^{T}\\vec{x}},{\\vec{\\alpha_{2}}^{T}\\vec{x}}]=0}$  \nNow ${cov[{\\vec{\\alpha_{1}}^{T}\\vec{x}},{\\vec{\\alpha_{2}}^{T}\\vec{x}}]=\\vec\\alpha_{1}^{T}\\sum\\vec\\alpha_{2}=\\vec\\alpha_{2}^{T}\\sum\\vec\\alpha_{1}=\\vec\\alpha_{2}^{T}\\lambda_{1}\\vec\\alpha_{1}=\\lambda_{1}\\vec\\alpha_{2}^{T}\\alpha_{1}=\\lambda_{1}\\vec\\alpha_{1}^{T}\\alpha_{2}=0}$  \nThus any of the equations   \n\n${\\vec\\alpha_{1}^{T}\\sum\\vec\\alpha_{2}=0\\space,\\space \\vec\\alpha_{2}^{T}\\sum\\vec\\alpha_{1}=0}$  \n${\\space\\space\\vec\\alpha_{2}^{T}\\alpha_{1}=0\\space,\\space\\space\\vec\\alpha_{1}^{T}\\alpha_{2}=0}$  \nsatisfies ${cov[{\\vec{\\alpha_{1}}^{T}\\vec{x}},{\\vec{\\alpha_{2}}^{T}\\vec{x}}]=0}$   \n\nOk now to find ${\\vec\\alpha_{2}}$ using Lagrange multipliers.\n${\\vec{\\alpha_{2}}^{T} \\sum \\vec {\\alpha_{2}}   -\\lambda_{2}(\\vec {\\alpha_{2}}^{T} \\vec {\\alpha_{2}} -1)-\\phi(\\vec\\alpha_{2}^{T}\\vec\\alpha_{1}-0)=0}$  where ${\\lambda_{2}}$ and ${\\phi}$ are Lagrange multipliers  \nand since  ${\\vec\\alpha_{2}^{T}\\alpha_{1}=0}$ the equation reduces to ${\\vec{\\alpha_{2}}^{T} \\sum \\vec {\\alpha_{2}}   -\\lambda_{2}(\\vec {\\alpha_{2}}^{T} \\vec {\\alpha_{2}} -1)=0 \\implies \\sum\\alpha_{2}=\\lambda_{2}\\alpha_{2}}$  \ni.e ${\\lambda_{2}}$ is the eigenvalue of the eigenvector ${\\vec\\alpha_{2}}$ of the covariance matrix ${\\sum}$ of ${\\vec{x}}$  and since ${\\lambda_{1}}$ is the already the largest eigenvalue , which makes ${\\lambda_{2}}$ the next largest.   \nSimilarly we can find ${\\alpha_{3},\\alpha_{4}}$..etc\n\nWe can therefore find a set of ${p}$ eigenvectors  ${\\{\\alpha_{1},\\alpha_{2},\\cdots,\\alpha_{p}\\}}$ such that their corresponding eigenvalues are in the order ${\\lambda_{1} > \\lambda_{2} > \\cdots > \\lambda_{p}}$\n\n\n\n\n\n\n\n\n\n","efc66f18":"ok. now a side by side comparison between the original 784-Dimensional data and the reduced set of 144 dimensions.  **TO DO display MINST in reduced dim**\n","57c42cb0":"Let's assume we have a ${n}$-dimensional dataset where ${n}$ is a very large number. Training an algorithm in a such high dimensional feature space can be computationally very intensive.\nSome of these features might be correlated. How do we capture the essence of the data without having to go through every single dimension? Is it possible to represent the same data in lower dimensions ? \nThat's where dimensionality reduction methods like PCA help.  Once the mathematical definition of PCA is stated, we will work on trying to derive why the  PCA is defined the way it is using basic principles.\n\nThe definition of PCA is as follows:  \nThe principal components of ${X}$ ${\\in}$ ${R^{n}}$ are defined as  the components of \n${Z}$=${A'}$${X^{*}}$         where ${X^{*}}$ is the standardized  version of the original ${X}$   and  ${A}$ is the matrix that consists of the eigenvectors of the correlation matrix of  ${X^{*}}$\n\n\nOk now to work up from some of the basics \n\nLet's assume a single  sample in our data  is ${X}$ = ${[  x_{1 },  x_{2},...,x_{n} ]^{T}}$  , an ${n}$ dimensional vector where each $x_{i}$ is the component along the ${i}$-th feature axis (basis) .    \nThe features ${x_{i}}$ help distinguish one data point from another in this ${n}$-dimensional space.  Ok but how?  \n\n \n\n**Variance**  \n\nThis is a measure of how spread out the individual values  in a distribution are from their mean value.  \nThe more variance a particular feature exhibits i.e along a certain feature axis, the better the data can be separated along this axis.    \n\nI demonstrate this claim with the simple case of ten data points where each point is described using 2 features. (2D data).  \n\nThe distribution of this data is such that the mean along feature ${x_{1}}$  i.e. ${\\mu _{x_{1}}}=0$ and the mean along feature ${x_{2}}$ i.e. ${\\mu _{x_{2}}}=3$.  \nI will change the variance along each feature axes  from 0 (no deviation from the mean) to a high value and we will see why variance helps identify the ten points nicely in the latter case.  \n\nFirst let's import all relevant libraries.","9b2c0871":"Here I have created some dummy 2-D data where each point that shows a correlation between its two feature axes.","f4286cc8":"Next create 10 2-D points with ${\\mu_{x_{1}}=0}$ and ${\\mu_{x_{2}}=3}$.  \nHere I am defining a function for creating the distribution.  \n\n","55112ed1":"**PCA using scikit-learn on high - dim data ** \n\nNow that some of the basics are covered, we move to using the [PCA functionality offered by scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)","7b9e5678":"In this case  we see that all 10 points are separated because of the variance along feature ${x_{1}}$ but  there is no way to distinguish the data along the ${x_{2}}$ axis since there is no variance along ${x_{2}}$.  (10 t shirts, now showing the whole spectrum of colors (${x_{1}}$) but all sized \"L\" (${x_{2}}$) )  \n\n\nNext we consider the case where data is varying along both the axes.  However to stress my point of larger variance along a feature implying better distinction along the feature , I am distributing the data such that there is more variance along ${x_{1}}$ as compared to ${x_{2}}$ (10 shirts with a higher color range and a small size range S,M,L )\n","ff9719ff":"Now to train a classifier on the reduced dimensional dataset. In one of my [earlier kernels](https:\/\/www.kaggle.com\/sharathnair\/digit-recognizer-using-random-forest) I had used the RandomForestClassifier with a train and test scores of 99.9 : 93.65 resp . The input to the training was the 784-dim dataset . I will be using a similar classifier but in this case offer the reduced 441-dim dataset as an input.","fff1ea32":"no variance along ${x_{1}}$ and ${x_{2}}$ implies all 10 points lie at the point (0,3) i.e. all data lies on the mean $({\\mu_{x_{1}}},{\\mu_{x_{2}}})$. **There is no way to separate out the data points** in this case of zero variance along both the axes . Essentially implying all 10 data points are the same. (e.g.  10 t-shirts all green all marked \"L\". Although in essence each one is different but when plotted just using these 2 features every t shirt is a green L )   \n\nNext we change the variance along ${x_{1}}$ ","8cff31ad":"From the above graph it looks like 94% of the variance in the data is retained by the first 144 eigenvectors. I am choosing 144 here as i intend to later resize the images to 12x12 for display","077788d7":"\nOur first task it to find the eigenvectors of the covariance matrix of our data.  \n","b37522ad":"Case 1: ${var_{x_{1}}=0}$, ${var_{x_{2}}=0}$","f253bba5":" We would like  that the new features (basis vectors) in this lower dimensional space to be orthogonal. i.e. non-redundant. No one feature can be expressed as a linear combination of the remaining features in this new vector space. (Consider the opposite case, where all features are redundant. i.e imagine ending up with a high dimensional vector e.g. the house price described with features of floor size in cms,inches and meters. No new information is added by the inclusion of the second and third feature in this case that the first one doesn't already provide. Orthogonality of features brings more information that leads to a better separation of data )   \nNext step would be to transform the data into this new coordinate system where the basis vectors are the eigenvectors we just found.    \n\n\nWe can put all the ${p}$ eigenvectors into a transformation matrix ${T}$ and then transform the original data ${X}$ to ${X'}$ using    ** ${X'=XT}$**\n\nLet's see the whole process using some dummy data.","cfdf9762":"Now to transform the data to the new coordinate system where the eigenvectors are now the axes. \nWe will create a transformation matrix ${T=[\\alpha_{1},\\alpha_{2}]}$ where ${\\alpha_{1},\\alpha_{2}}$ are the two eigenvectors in our example.  \nThen transform the data using ${X'=XT}$","41270449":"Now to plot the eigenvectors on the data","741b575b":"The PCA extracted all possible eigenvectors (since the parameter 'n_components' was unset) .  \nWe now need to come up with an ideal number of dimensions wherein maximum variance of the data is retained.   For that we use the *'explained_variance_ratio_' *parameter. which for the ${i}$-th eigenvector is the quantity ${\\dfrac{\\lambda_{i}}{\\sum_{k} \\lambda_{k}}}$"}}