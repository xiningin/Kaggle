{"cell_type":{"2193d64a":"code","0d09a87d":"code","1f5297a9":"code","bde76fa9":"code","16a60850":"code","5c3850a6":"code","46e22800":"code","e9d7987e":"code","39c89d4a":"code","a5039f9d":"code","4b94a660":"code","98b525dd":"code","19dbaec6":"code","bda25309":"markdown","05736d71":"markdown","68b475aa":"markdown","0fdfa6a2":"markdown","ef30db7a":"markdown","7344f9f1":"markdown","f55b5da8":"markdown","717444da":"markdown","42ea7b22":"markdown","21c488a6":"markdown","ecbecd28":"markdown"},"source":{"2193d64a":"import glob\nimport numpy as np\nimport pandas as pd\nimport sklearn \nimport seaborn as sns\nimport cv2\nimport random\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom skimage.io import imread\nfrom copy import deepcopy \n\nimport tensorflow.compat.v2 as tf \nimport tensorflow.keras as keras \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, UpSampling2D\nfrom tensorflow.keras.optimizers import Adam","0d09a87d":"PATH_TO_IMG = \"..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/\"\n\npath_to_img_data = glob.glob(PATH_TO_IMG + \"*jpg\")\nlen(path_to_img_data)","1f5297a9":"TRAIN_SIZE = 25600\nTEST_SIZE = 640\n\nBATCH_SIZE = 128\n\nIMG_HEIGHT = 224\nIMG_WIDTH = 184","bde76fa9":"plt.figure(figsize=(10,5))\nfor i,img_path in enumerate(path_to_img_data[:4]):\n    plt.subplot(1,4,i+1)\n    plt.axis('off')\n    img = plt.imread(img_path)\n    plt.imshow(img)   ","16a60850":"def get_input(image_path):\n    img = imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)      \n    img = img\/255\n    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    img = np.reshape(img, (IMG_HEIGHT, IMG_WIDTH, 1))\n\n    return img \n\n\ndef get_output(image_path):\n    img = imread(image_path)\n    img = img\/255.0\n    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    return img\n    \n    \ndef image_generator(files, batch_size = 64): \n    \n    tmp_x = deepcopy(files)\n    \n    while True:\n        len_array = len(tmp_x)\n        if len_array < batch_size:\n          tmp_x = deepcopy(files)\n          len_array = len(tmp_x)\n\n        batch_index = np.random.choice(len_array, size=batch_size, replace=False)\n        batch_paths = [tmp_x[index] for index in batch_index]\n        tmp_x = np.delete(tmp_x, batch_index)\n        batch_input  = []\n        batch_output = [] \n        for input_path in batch_paths:\n            input_img = get_input(input_path)\n            output = get_output(input_path)\n            batch_input.append(input_img)\n            batch_output.append(output)\n        batch_x = np.array( batch_input )\n        batch_y = np.array( batch_output )\n\n        yield( batch_x, batch_y )","5c3850a6":"train_gen = image_generator(path_to_img_data[:TRAIN_SIZE],BATCH_SIZE)\ntest_gen = image_generator(path_to_img_data[TRAIN_SIZE:TRAIN_SIZE+TEST_SIZE],BATCH_SIZE)","46e22800":"plot_example_data = next(test_gen)","e9d7987e":"plt.imshow(plot_example_data[0][1].reshape(IMG_HEIGHT,IMG_WIDTH),cmap='gray')\nplt.show()","39c89d4a":"plt.imshow(plot_example_data[1][1])\nplt.show()","a5039f9d":"model = Sequential()\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)))\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(32, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(16, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])\nmodel.summary()","4b94a660":"STEP = TRAIN_SIZE\/\/BATCH_SIZE\nEPOCHS = 25\nmodel.fit_generator(generator=train_gen,steps_per_epoch=STEP,epochs=EPOCHS)","98b525dd":"test_img = next(test_gen)","19dbaec6":"predicted = model.predict(test_img[0])\n\nn = 5\nplt.figure(figsize=(15, 10))\nfor i in range(n):\n    # display original\n    plt.subplot(3, n, i + 1 )\n    plt.imshow(test_img[1][i].reshape(IMG_HEIGHT, IMG_WIDTH, 3))\n    plt.axis('off')\n    \n    \n    # display grayscale\n    plt.subplot(3, n, i + 1 + n )\n    plt.imshow(test_img[0][i].reshape(IMG_HEIGHT, IMG_WIDTH), cmap='gray')\n    plt.axis('off')\n    \n    # display predict\n    plt.subplot(3, n, i + 1 + n * 2 )\n    plt.imshow(predicted[i].reshape(IMG_HEIGHT, IMG_WIDTH, 3))\n    plt.axis('off')\n        \nplt.show()","bda25309":"My generator, input image in grayscale, output colorated images.","05736d71":"Plot original image","68b475aa":"Set path to images","0fdfa6a2":"Plotting input and output data","ef30db7a":"Model of convolutional autoencoder","7344f9f1":"Train and test generator with TRAIN\/TEST size + BATCH size ","f55b5da8":"First test generation of data","717444da":"Plot - original data, input data, output data","42ea7b22":"Train model","21c488a6":"### Import libraries","ecbecd28":"Set size of dataset, batch size and shape of image"}}