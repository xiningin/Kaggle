{"cell_type":{"31dd6fc4":"code","1f7c5200":"code","8326ccdd":"code","28a9a430":"code","68a2c168":"code","b49ad9e9":"code","70819926":"code","4c132553":"code","7e33ed4f":"code","caf7ef03":"code","d345bb50":"code","84ab5355":"code","68744a13":"code","185166fb":"code","6ee11550":"code","4dcdc1a4":"code","a475ffb3":"code","ee1d0f31":"code","666fbba3":"code","a3eb3429":"code","4d48bd3c":"code","9bd9abc7":"code","a6a5b44a":"code","41b3563c":"code","d3ae8209":"code","f2b691e1":"code","50cab2c8":"code","2372d28f":"code","1c2c2a9d":"code","5fcb3624":"code","e2277fe4":"code","f9db230d":"code","b26e7625":"code","809b4ebb":"code","e34884d9":"code","37f69e1c":"code","e9b9a6d5":"code","5c5bb16c":"code","36050a23":"code","8af6953c":"markdown","68c51d09":"markdown","e3bca4dc":"markdown","db2339cf":"markdown","ffacfd6f":"markdown","26e9f828":"markdown","186b7774":"markdown","e88697e1":"markdown","de5b0389":"markdown","febf74bb":"markdown","1713797f":"markdown","b9c22da8":"markdown","3730c3a4":"markdown","13424ed1":"markdown","000a3cad":"markdown","70ef1c55":"markdown","766d00ee":"markdown","49d796e6":"markdown","e0f2e066":"markdown","a60f0b50":"markdown","2b3a6b19":"markdown","d25c1295":"markdown","550179a5":"markdown","7fe70eab":"markdown","546a536a":"markdown","3c33ca0f":"markdown","c579ec59":"markdown","3fe5e937":"markdown","773c421a":"markdown","2b900aa7":"markdown","6bc3b858":"markdown","12693f17":"markdown","aa3930e5":"markdown"},"source":{"31dd6fc4":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,PowerTransformer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nwarnings.filterwarnings('ignore')","1f7c5200":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n# Preview the data\ntrain.head()","8326ccdd":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","28a9a430":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","68a2c168":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","b49ad9e9":"# Create arrays for the features and the response variable\ny = train['target'].to_numpy()\nX = train.drop(['id','target'], axis=1)","70819926":"X.shape","4c132553":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","7e33ed4f":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","caf7ef03":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","d345bb50":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","84ab5355":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","68744a13":"#train_std = train['cont1'].mean()\n#train_mean = train['cont1'].std()\n\n#cut_off = train_std * 3\n#train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n\n# Trim the test DataFrame\n#trimmed_df = so_test_numeric[(train['cont1'] < train_upper) \\\n                            # & (train['cont1'] > train_lower)]","185166fb":"# Outlier Handle \nclass OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal) ","6ee11550":"class SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n            \n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n            \n        spi = self._create_sparse_interactions(X)\n        return spi\n    \n    \n    def get_feature_names(self):\n        return self.feature_names\n    \n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n        \n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n                \n                # get column multiplications value\n                out = X[:, col_ixs[0]]    \n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)","4dcdc1a4":"# add columns with kmeans, assuming input already scaled\nclass KmeansTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, num_clusters = 4):\n        self.num_clusters = num_clusters\n        if self.num_clusters > 0:\n            self.kmeans = KMeans(n_clusters=self.num_clusters, n_init=3, random_state=0)\n    \n    def fit(self, X, y=None):\n        if self.num_clusters > 0:\n            self.kmeans.fit(X)\n        return self\n    \n    def transform(self, X, y=None):\n        if self.num_clusters > 0:\n            Xkf = self.kmeans.transform(X)\n            df = pd.DataFrame()\n            for i in range(self.num_clusters):\n                df[f\"Centroid_{i}\"] = Xkf[:,i]\n            return df\n        else: # if no kmeans requested, return first column unmodified as answer\n            # need to return a one column dataframe rather than series\n            # in order for FeatureUnion to work correctly\n            tf = np.zeros(X.columns.size,dtype=bool)\n            tf[0] = True\n            return X.loc[:,tf]","a475ffb3":"def condense_category(col, min_freq=0.1, new_name='other'):\n    series = pd.value_counts(col)\n    mask = (series\/series.sum()).lt(min_freq)\n    return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\ntrain_condense=train.copy()\ntrain_condense[cat_columns]=train_condense[cat_columns].apply(condense_category, axis=0)\ntrain_condense[train_condense.select_dtypes(['float64']).columns] = train_condense[train_condense.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain_condense[train_condense.select_dtypes(['object']).columns] = train_condense.select_dtypes(['object']).apply(lambda x: x.astype('category'))","ee1d0f31":"# Create arrays for the features and the response variable\ny_condense = train_condense['target'].to_numpy()\nX_condense = train_condense.drop(['id','target'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train_condense , X_test_condense , y_train_condense , y_test_condense  = train_test_split(X_condense , y_condense , test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test_condense.shape[0], X_train_condense.shape[0], X_test_condense.shape[1]))","666fbba3":"Encoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            #OneHotEncoder(handle_unknown='ignore'),\n            #LabelEncoder(),\n            OrdinalEncoder() ,\n            #SparseInteractions(degree=2)\n              )","a3eb3429":"Scaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        RobustScaler(),\n                        # PowerTransformer(),\n                        # StandardScaler(),\n                        # MinMaxScaler(),\n                        # QuantileTransformer\n)","4d48bd3c":"cross_validation_design = KFold(n_splits=3,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","9bd9abc7":"from xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\n# Random HyperParameters\nxgb_params = {'n_estimators': 7000,\n            'learning_rate': 0.16,\n            'subsample': 0.96,\n            'colsample_bytree': 0.12,\n            'max_depth': 2,\n            'booster': 'gbtree', \n            'reg_lambda': 100.1,\n            'reg_alpha': 15.9,\n            'random_state':40}\nXGBR = XGBRegressor(**xgb_params,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    #tree_method='gpu_hist',\n                    #gpu_id=0, \n                    #predictor=\"gpu_predictor\"\n                   )","a6a5b44a":"# Cat Features  \nOrdinalEncoder111 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nQuantileTransformer111  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        QuantileTransformer()\n)\nOrdinalEncoder_QuantileTransformer111 = make_column_transformer(\n    ( OrdinalEncoder111 , cat_columns),\n    ( QuantileTransformer111, num_columns))\n    \n\nXGBROrdinalEncoderQuantileTransformerwithoutfeatures111= Pipeline([\n        ('preprocess', OrdinalEncoder_QuantileTransformer111),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])  \n# val  :0.719208108093819\/test : 0.7043908348674459 \/Public  : 0.71810","41b3563c":"# Cat Features  \nOrdinalEncoder122 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nMaxAbsScaler122  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        MaxAbsScaler()\n)\nOrdinalEncoder_MaxAbsScaler122 = make_column_transformer(\n    ( OrdinalEncoder122 , cat_columns),\n    ( MaxAbsScaler122, num_columns))\n    \nXGBROrdinalEncoderMaxAbsScalerwithoutfeatures122= Pipeline([\n        ('preprocess', OrdinalEncoder_MaxAbsScaler122),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])  \n# val 0.7193935161595488 \/test: 0.7044497576344873\/ Public : 0.71816","d3ae8209":"# Cat Features  \nOrdinalEncoder133= make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nPowerTransformer133  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        PowerTransformer()\n)\nOrdinalEncoder_PowerTransformer133 = make_column_transformer(\n    ( OrdinalEncoder133 , cat_columns),\n    ( PowerTransformer133, num_columns))\n    \n\nXGBROrdinalEncoderPowerTransformerwithoutreduction_features133 = Pipeline([\n        ('preprocess', OrdinalEncoder_PowerTransformer133),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n# test: 0.7044315280956893 \/  Public  :  18\/18","f2b691e1":"# Cat Features  \nOrdinalEncoder144 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n           # SparseInteractions(degree=2)\n              )\n# Num Features \nRobustScaler144  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        RobustScaler()\n)\nOrdinalEncoder_RobustScaler144 = make_column_transformer(\n    ( OrdinalEncoder144 , cat_columns),\n    ( RobustScaler144, num_columns))\n    \n\nXGBROrdinalEncoderRobustScalerwithoutreduction_features144 = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler144),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)]) \n# test: 0.7044010096971889 \/  Public : 17\/17","50cab2c8":"from sklearn.decomposition import PCA\n# Cat Features  \nOrdinalEncoder155 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nMaxAbsScaler155  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        MaxAbsScaler()\n)\nOrdinalEncoder_MaxAbsScaler155 = make_column_transformer(\n    ( OrdinalEncoder155 , cat_columns),\n    ( MaxAbsScaler155, num_columns))\n    \nXGBROrdinalEncoderMaxAbsScalerwithoutfeaturesf_classif155= Pipeline([\n        ('preprocess', OrdinalEncoder_MaxAbsScaler155),\n        #('reducer', PCA(n_components=0.9))\n         ('dim_red', SelectKBest(f_classif, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)]) \n# val :0.7201929857152718\/test : 0.7047341349984148\/Public : 0.71859","2372d28f":"from sklearn.decomposition import PCA\n# Cat Features  \nOrdinalEncoder166 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nMaxAbsScaler166  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        MaxAbsScaler()\n)\nOrdinalEncoder_MaxAbsScaler166 = make_column_transformer(\n    ( OrdinalEncoder166 , cat_columns),\n    ( MaxAbsScaler166, num_columns))\n    \n\nXGBROrdinalEncoderMaxAbsScalerwithoutfeaturesf_regression166= Pipeline([\n        ('preprocess', OrdinalEncoder_MaxAbsScaler166),\n        #('reducer', PCA(n_components=0.9))\n         ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)]) \n# Val : 0.7206232204049583 \/test:0.7046525124143256 \/public : 0.71891","1c2c2a9d":"# Cat Features  \nOrdinalEncoderSparse21 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            SparseInteractions(degree=2)\n              )\n# Num Features \nPolynomialFeaturesPowerTransformer21  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        PolynomialFeatures(degree=2),\n                        PowerTransformer()\n)\nOrdinalEncoderSparse_PolynomialFeaturesPowerTransformer21 = make_column_transformer(\n    ( OrdinalEncoderSparse21 , cat_columns),\n    ( PolynomialFeaturesPowerTransformer21, num_columns))\n    \n\nXGBROrdinalEncoderPowerTransformerwithoutreduction21 = Pipeline([\n        ('preprocess', OrdinalEncoderSparse_PolynomialFeaturesPowerTransformer21),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])  \n#val: 0.7225059484246128\/ test :0.6928511033984426\/Public :0.72135","5fcb3624":"# encode + featres engineers \nOrdinalEncoderSparse22 = make_pipeline(\n    SimpleImputer(strategy='most_frequent', fill_value='manquante',add_indicator=True),\n    OrdinalEncoder(),\n    SparseInteractions(degree=2)\n)\n# Num Features \nPolynomialFeaturesStandardScaler22  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        PolynomialFeatures(degree=2),\n                        StandardScaler()\n)\nOrdinalEncoderSparse_PolynomialFeaturesStandardScaler22 = make_column_transformer(\n    ( OrdinalEncoderSparse22 , cat_columns),\n    ( PolynomialFeaturesStandardScaler22, num_columns))\n    \n# Pipe 6:OrdinalEncoderSparse \/StandardScalerPolynomial without reduction from previous notebook \n# 0.7048661500017998\/Public Score : 0.71829\nOrdinalEncoderSparse_PolynomialFeaturesStandardScaler22 = Pipeline([\n        ('preprocess', OrdinalEncoderSparse_PolynomialFeaturesStandardScaler22),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n# Test :0.6928556432865892\/Public:0.72119","e2277fe4":"# Cat Features  \nOrdinalEncoderSparse23 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n            SparseInteractions(degree=2)\n              )\n# Num Features \nPolynomialFeaturesStandardScaler23  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        PolynomialFeatures(degree=2),\n                        StandardScaler()\n)\nOrdinalEncoderSparse_PolynomialFeaturesStandardScaler23 = make_column_transformer(\n    ( OrdinalEncoderSparse23 , cat_columns),\n    ( PolynomialFeaturesStandardScaler23, num_columns))\n    \n# Pipe 6:OrdinalEncoderSparse \/StandardScalerPolynomial without reduction\n# 0.7048661500017998\/Public Score : 0.71829\nXGBROrdinalEncoderStandardScalerwithoutreduction23 = Pipeline([\n        ('preprocess', OrdinalEncoderSparse_PolynomialFeaturesStandardScaler23),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n# val 0.7224449457436771\/ test : 0.6930869233652421 \/Public :0.72147","f9db230d":"# Cat Features  \nCatBoostEncoder31 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nPowerTransformer31  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        PowerTransformer()\n)\nCatBoostEncoder_PowerTransformer31 = make_column_transformer(\n    ( CatBoostEncoder31 , cat_columns),\n    ( PowerTransformer31, num_columns))\n    \nXGBRCatBoostEncoderPowerTransformerwithoutfeatures31  = Pipeline([\n        ('preprocess', CatBoostEncoder_PowerTransformer31),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n#validation : 0.719404922322341\/test :0.7071080249822401\/Public:0.71828","b26e7625":"# Cat Features  \nCatBoostEncoder32 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nRobustScaler32  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                       # PolynomialFeatures(degree=2),\n                        RobustScaler()\n)\nCatBoostEncoder_RobustScaler32 = make_column_transformer(\n    ( CatBoostEncoder32 , cat_columns),\n    ( RobustScaler32, num_columns))\n    \nXGBRCatBoostEncoderRobustScalerwithoutfeatures32 = Pipeline([\n        ('preprocess', CatBoostEncoder_RobustScaler32),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])  \n# validation :0.7196597118650423\/ test:0.7069794090551014 \/ Public: 0.71801","809b4ebb":"# Cat Features  \nCatBoostEncoder33 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nQuantileTransformer33  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        QuantileTransformer()\n)\nCatBoostEncoder_QuantileTransformer33 = make_column_transformer(\n    ( CatBoostEncoder33 , cat_columns),\n    ( QuantileTransformer33, num_columns))\n    \n\nXGBRCatBoostEncoderQuantileTransformerwithout_features_reduction33 = Pipeline([\n        ('preprocess', CatBoostEncoder_QuantileTransformer33),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])  \n# test :0.7070272032074714 \/Public :16\/16","e34884d9":"# Cat Features  \nCatBoostEncoder34 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            #SparseInteractions(degree=2)\n              )\n# Num Features \nMaxAbsScaler34  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        MaxAbsScaler()\n)\nCatBoostEncoder_MaxAbsScaler34 = make_column_transformer(\n    ( CatBoostEncoder34 , cat_columns),\n    ( MaxAbsScaler34, num_columns))\n    \n\nXGBRCatBoostEncoderMaxAbsScalerwithoutreduction_features34 = Pipeline([\n        ('preprocess', CatBoostEncoder_MaxAbsScaler34),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)]) \n# test :0.7069290268303777\/Public :15\/15\n","37f69e1c":"# Cat Features  \nCatBoostEncoderSparse41 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            SparseInteractions(degree=2)\n              )\n# Num Features \nPolynomialFeaturesRobustScaler41  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        PolynomialFeatures(degree=2),\n                        RobustScaler()\n)\nCatBoostEncoderSparse_PolynomialFeaturesRobustScaler41 = make_column_transformer(\n    ( CatBoostEncoderSparse41 , cat_columns),\n    ( PolynomialFeaturesRobustScaler41, num_columns))\n\nXGBRCatBoostEncoderRobustScalerwithoutreduction41 = Pipeline([\n        ('preprocess', CatBoostEncoderSparse_PolynomialFeaturesRobustScaler41),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])  \n#val:0.721260544961198\/test: 0.6981736951618922\/Public:0.72130","e9b9a6d5":"# Cat Features  \nCatBoostEncoderSparse42 = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            SparseInteractions(degree=2))\n              \nPolynomialFeaturesPowerTransformer42  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        PolynomialFeatures(degree=2),\n                        PowerTransformer()\n)\nCatBoostEncoderSparse_PolynomialFeaturesPowerTransformer42 = make_column_transformer(\n    ( CatBoostEncoderSparse42 , cat_columns),\n    ( PolynomialFeaturesPowerTransformer42, num_columns))\n    \nXGBRCatBoostEncoderPowerTransformerwithoutreduction42 = Pipeline([\n        ('preprocess', CatBoostEncoderSparse_PolynomialFeaturesPowerTransformer42),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR)])\n# val :0.7225339618603557\/ test : 0.698695194265506\/Public :14\/15","5c5bb16c":"# i sumbit 12 version  version 13 still  not submitted \n# Version 14 and 15 are the same ","36050a23":"XGBROrdinalEncoderPowerTransformerwithoutreduction_features133.fit(X, y)\npreds_valid = XGBROrdinalEncoderPowerTransformerwithoutreduction_features133.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))\ntest_final= test.drop(['id'], axis=1)\n# Use the model to generate predictions\npredictions = XGBROrdinalEncoderPowerTransformerwithoutreduction_features133.predict(test_final)\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,'target': predictions})\noutput.to_csv('XGBROrdinalEncoderPowerTransformerwithoutreduction_features133.csv', index=False)","8af6953c":"## Define the model features and target\n### Extract X and y ","68c51d09":"## check that we have all column","e3bca4dc":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","db2339cf":"## 4-1 Pipe CatBoostEncoder RobustScaler with features engineer without reduction ","ffacfd6f":"## Num Features :","26e9f828":"## 1-2Pipe  OrdinalEncoder MaxAbsScaler without features engineer","186b7774":"## 1-1Pipe  OrdinalEncoder QuantileTransformer without features engineer","e88697e1":"## 3-1Pipe CatBoostEncoder PowerTransformer without features engineer","de5b0389":"# Convert Dtypes : ","febf74bb":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","1713797f":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","b9c22da8":"### Kmeans  Features ","3730c3a4":"## 3-4 Pipe CatBoostEncoder MaxAbsScaler without features without reduction \n**not done**","13424ed1":"# 2-2Pipe OrdinalEncoder StandardScaler features enginner without reduction \nusing encoding from sklearn ","000a3cad":"## Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","70ef1c55":"## 1-4Pipe OrdinalEncoder RobustScaler wihtout features without reduction \n**not done** ","766d00ee":"## 4-2 Pipe CatBoostEncoder PowerTransformer features engineer + without reduction ","49d796e6":"### Num Features ","e0f2e066":"##  2-1 Pipe OrdinalEncoder PowerTransformer features enginner without reduction ","a60f0b50":"##  3-2 pipe  CatBoostEncoder RobustScaler without features engineer","2b3a6b19":"# Train Catboost \/ Xgboost \/ Lgbm\n## Define Baseline XGBR ","d25c1295":"## 2-3Pipe OrdinalEncoder StandardScaler features enginner without reduction ","550179a5":"## 1-5Pipe  OrdinalEncoder MaxAbsScaler without features reduction f_classif","7fe70eab":"### Num\/Cat Features ","546a536a":"## 3-3 Pipe CatBoostEncoder QuantileTransformer without features  without reduction \n**not done**","3c33ca0f":"## Compelete prerocess pipe for  Cat dara ","c579ec59":"## Convert Dtypes :","3fe5e937":"\n## Feature Engineering\nFeature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning. Most machine learning algorithms work with tabular data. When we talk about features, we are referring to the information stored in the columns of these tables\n### Sparse Interactions :\n\n\n","773c421a":"### Qauntile replace outlier ","2b900aa7":"# Data Modeling\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.","6bc3b858":"##  1-6 pipe OrdinalEncoder MaxAbsScaler withoutfeatures reduction: f_regression","12693f17":"###  Outlier Handling \n### Statistical outlier removal\n\nWhile removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together.\nwe can trim data like this :\n","aa3930e5":"##  1-3 Pipe OrdinalEncoder PowerTransformer without reduction without features \n**not done**"}}