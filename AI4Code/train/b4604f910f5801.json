{"cell_type":{"d14c6e10":"code","643052ba":"code","12f9178c":"code","f3f3b73f":"code","6c233402":"code","2d9ce831":"code","9f8d562b":"code","dc264b52":"code","7cb1d891":"code","9d451c05":"code","8f5300b1":"code","bca9b84d":"code","d3aeee9f":"code","4702348b":"code","473e37b0":"code","b1ce1450":"code","6760aec0":"markdown","b1d35fad":"markdown","9dce3875":"markdown","2476fcc7":"markdown","1e8c5b6c":"markdown","16d19646":"markdown","cc8cd691":"markdown","e2964982":"markdown","bbbb9bda":"markdown","32555247":"markdown"},"source":{"d14c6e10":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns \nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import RFE\nsns.set(style=\"ticks\", color_codes=True)","643052ba":"# Loading the training dataset\ndf_train = pd.read_csv(\"..\/input\/train.csv\")","12f9178c":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","f3f3b73f":"scaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","6c233402":"normal_vars = [] # This list will contain the explanatory variables \n                 # that follow a normal distribution\n\ncounter = 0 \nfor c in df_train.columns:\n    \n    df = df_train[c]\n    index_0 = y == 0.0\n    index_1 = y == 1.0 \n    df_0 = df[index_0]\n    df_1 = df[index_1]\n    \n    if stats.kstest(df_0, 'norm')[1] >= 0.5 and stats.kstest(df_1, 'norm')[1] >= 0.5:        \n        print(\"Variable: {} follows a normal distribution on condition of the classes\".format(c))                \n        counter += 1\n        normal_vars.append(c)\n        \n        \nprint(\"Number of variables that follow a normal distribution {}\".format(counter))\n        \n        ","2d9ce831":"def plot_hist(list_var):\n    \n    for l in list_var:\n        df = df_train[l]\n        index_0 = y == 0.0\n        index_1 = y == 1.0 \n        df_0 = df[index_0]\n        df_1 = df[index_1]\n\n        \n        n, bins, patches = plt.hist(x=df_0, bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85); \n        plt.grid(axis='y', alpha=0.75)\n        plt.xlabel('Value')\n        plt.title('Histogram of the variable ' + l + ' on condition of the class 0 ')\n        plt.show()\n        \n        \n        n, bins, patches = plt.hist(x=df_1, bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85); \n        plt.grid(axis='y', alpha=0.75)\n        plt.xlabel('Value')\n        plt.title('Histogram of the variable ' + l + ' on condition of the class 1 ')\n        plt.show()\n        \n        \nlist_var = [\"3\", \"5\", \"6\"]     \n\nplot_hist(list_var)","9f8d562b":"df_train = df_train[normal_vars]\nX = df_train.values\ncolnames2 = df_train.columns","dc264b52":"predictors = colnames2\n\ndef calculate_cor_mat(predictors, X):\n    X = X[predictors]\n    X = X.values\n    correlation_matrix = np.corrcoef(X.T)\n    print(correlation_matrix)\n        \ncalculate_cor_mat(predictors, df_train)","7cb1d891":"def plot_pair_plot(predictors, X):\n    X = X.loc[:, predictors]\n    g = sns.pairplot(X) \n\nplot_pair_plot(predictors[0:10], df_train) # we select only a \n                                           # subset of the predictors\n\n","9d451c05":"model = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\nselector = RFE(model, 2, step=1)\nselector = selector.fit(X, y)\n\n#Good references \n#https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html","8f5300b1":"selected_variables = colnames2[selector.support_ ] # We select the predictors\nselected_variables = selected_variables.values.tolist()\nprint(\"The selected variables are: \")\nprint(selected_variables)","bca9b84d":"predictors = selected_variables\n\n\ndef five_num(X):\n    \n    quartiles = np.percentile(X, [25, 50, 75])\n    data_min, data_max = X.min(), X.max()\n    print(\"Minimum: {}\".format(data_min))\n    print(\"Q1: {}\".format(quartiles[0]))\n    print(\"Median: {}\".format(quartiles[1]))\n    print(\"Q3: {}\".format(quartiles[2]))\n    print(\"Maximum: {}\".format(data_max))    \n\n\ndef fit_discriminant(predictors, X):\n    \n    X = X[predictors]\n    X = X.values \n    \n    skf = StratifiedKFold(n_splits=10)\n    skf.get_n_splits(X, y)\n    \n    \n    train_auc = []\n    valid_auc = []\n    \n    for train_index, test_index in skf.split(X, y):\n        \n        model = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n        model.fit(X[train_index], y[train_index])    \n        \n        y_train = y[train_index]\n        y_test = y[test_index]\n    \n        y_train_predict = model.predict_proba(X[train_index])\n        y_train_predict = y_train_predict[:,1]\n        y_test_predict = model.predict_proba(X[test_index], )\n        y_test_predict = y_test_predict[:,1]           \n        \n        train_auc.append(roc_auc_score(y_train, y_train_predict))\n        valid_auc.append(roc_auc_score(y_test, y_test_predict))\n        \n    n_bins = 5\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, tight_layout=True);\n    ax1.hist(train_auc, bins=n_bins);\n    ax1.set_title(\"Histogram of AUC training\")\n    ax2.hist(valid_auc, bins=n_bins);\n    ax2.set_title(\"Histogram of AUC validation\")  \n    \n    print(\"Five numbers Training AUC\\n\")\n    five_num(np.array(train_auc))\n    print(\"\\nFive numbers Valid AUC\\n\")\n    five_num(np.array(valid_auc))\n\n    ","d3aeee9f":"fit_discriminant(predictors, df_train)","4702348b":"# We fit the model with the whole training dataset\nmodel = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\nmodel.fit(df_train[predictors], y)","473e37b0":"df_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \nX = df_test[predictors]\ndel df_test\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]\n","b1ce1450":"# submit prediction\nsmpsb_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"discrimant_analysisi.csv\", index=None)\n","6760aec0":"## References \n\n[1] https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test","b1d35fad":"Another hyphothesis of linear discriminant analysis is that the explanatory variables must follow a normal distribution on condition of the classes, hence we will find the variables that meet this criterion with the following code: ","9dce3875":"In this kernel we will use the feature selection method called Feature ranking with recursive feature elimination in order to choose a suitable number of predictors. We tried the following values of the number of predictors: 1, 2, 3, and 4 and obtained the following values of the auc in the validation dataset.\n\n| Q1   | Median | Q3   | Number of predictors |\n|------|--------|------|----------------------|\n| 0.49 | 0.52   | 0.59 | 1                    |\n| 0.47 | 0.56   | 0.62 | 2                    |\n| 0.49 | 0.54   | 0.60 | 3                    |\n| 0.49 | 0.51   | 0.60 | 4                    |\n\nWe are going to choose 2 as the number of predictors.\n\n","2476fcc7":"From the results above, we have 129 possible predictors and 250 observations, with this amount of data it is highly likely to include irrelevant variables in the model, moreover linear discriminant analysis will be affected by the curse of dimensionality. We suggest you read the following article in order to understand the curse of dimensionality. \n\n[The curse of dimensionality](https:\/\/medium.com\/@paritosh_30025\/curse-of-dimensionality-f4edb3efa6ec)\n\nThere are two ways to prevent the curse of dimensionality which are shown as follows: \n\n* Feature selection:\u200aSelecting important features which are relevant to model (it avoids the curse of dimensionality)\n* Feature extraction:\u200aTransformation of high dimensional space into lower dimensional space by using various methods such as PCA, TSVD, T-SNE etc\n\nIn this kernel we will use feature selection but before to do that we are going to explore the explanatory variables.","1e8c5b6c":"First of all, we are going to use the statistical model called Fisher's linear discriminant analysis  which finds linear combination of features that separates two or more classes. One advantage of using statistical models is that are less prone to overfit small dataset. The following web page is a good reference in order to understand how linear discriminant analysis works internally. \n\nhttps:\/\/web.stanford.edu\/class\/stats202\/content\/lec9.pdf","16d19646":"Finally, we will send the submission.","cc8cd691":"\"In the cell above we have used the kolmogorov-smirnov test which is a nonparametric test that can be used to compare a sample with a reference probability distribution\" [1].\n\nIn our case we have used the kolmogorov-smirnov test to prove the following  hypothesis: \n\n$H_o$ : The distribution of the explanatory variable follows a normal distribution on condition of the classes\n\n$H_1$ : The distribution of the explanatory variable does not follow a normal distribution on condition of the classes \n\nIf you do not know about hypothesis testing we suggest you to read the following resources from Khan Academy:\n\n[Sampling distribution](https:\/\/www.khanacademy.org\/math\/statistics-probability\/sampling-distributions-library)\n\n[Confidence interval](https:\/\/www.khanacademy.org\/math\/statistics-probability\/confidence-intervals-one-sample)\n\n[Hypothesis testing](https:\/\/www.khanacademy.org\/math\/statistics-probability\/significance-tests-one-sample)\n\nIf we get a p-value greater than or equal to 0.5 in both distributions (1's and 0's) we do not reject the $H_o$ hypothesis, from the cell above there are 129 explanatory variables that meet this criterion and therefore follow a normal distribution. In the next cell we are going to plot some of these variable.","e2964982":"This approach does not work well, the reason for that can be the fact that linear discriminant analysis assumes a covariance matrix common to all categories. ","bbbb9bda":"Linear discriminat analysis is sensitive to variables that are not standardized, that is why we are going to standardize the explanatory variables by removing the mean and scaling to unit variance. The standard score for the variable X is calculated as follows: \n\n$$ z = (X-\\mu) \/ s $$\n\nWhere $\\mu$ is the mean and s is the standard deviation. Moreover, one of the hypothesis of linear discriminant analysis is that the predictors must have the same variance.\n","32555247":"From the above it seems that the predictors are uncorrelated each other."}}