{"cell_type":{"0588ace6":"code","4b55e0ad":"code","e9e6cdc9":"code","86ec93f1":"code","d0b931d0":"code","593bacb3":"code","ce5ee5cb":"code","f065fa98":"code","2ab39d0a":"code","e2d62291":"code","ea45bea7":"code","5d6d2b26":"code","a19d8de1":"code","6f6e85f4":"code","ea1b8958":"markdown","02586125":"markdown","3d22ceff":"markdown","92f3f9af":"markdown","94b3b559":"markdown","38fba6a9":"markdown","1b0ce9ae":"markdown"},"source":{"0588ace6":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nseed=1234\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n%config IPCompleter.use_jedi = False","4b55e0ad":"# Variables with an integer value of 2 as initial value\nx = tf.Variable(2)\nx","e9e6cdc9":"# Nested list as initial value\ny = tf.Variable([[2, 3]], dtype=tf.int32)\ny","86ec93f1":"# Tuples also work but beware it isn't the same as a nested list.\n# Check the difference between the current output and the previous cell output\ny = tf.Variable(((2, 3)), dtype=tf.int32)\ny","d0b931d0":"# You can even pass a tensor object as an initial value\nt = tf.constant([1, 2,], dtype=tf.int32)\nz = tf.Variable(t)\nz","593bacb3":"# An interesting thing to note. \n# You can't change the values of the tensor `t` in the above example\n# but you can change the values of the variable created using it\n\n# This won't work\ntry:\n    t[0] = 1\nexcept Exception as ex:\n    print(type(ex).__name__, ex)\n    \n# This also won't work\ntry:\n    z[0] = 10\nexcept Exception as ex:\n    print(type(ex).__name__, ex)\n    \n# This works though\nprint(\"\\nOriginal variable: \", z)\nz[0].assign(5)\nprint(\"Updated variable: \", z)","ce5ee5cb":"# Most of the properties that we saw for tensors in part1 are the same for variables\nprint(f\"Shape of variable : {z.shape}\")\nprint(f\"Another method to obtain the shape using `tf.shape(..)`: {tf.shape(z)}\")\n\nprint(f\"dtype of the variable: {z.dtype}\")\nprint(f\"Total size of the variable: {tf.size(z)}\")\nprint(f\"Values of the variable: {z.numpy()}\")","f065fa98":"#  This doesn't work though\ntry:\n    print(f\"Rank: {z.ndim}\")\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","2ab39d0a":"# Crap! How to find out the no of dimensions then?\nprint(f\"Rank: {tf.shape(z)} or like this {z.shape}\")","e2d62291":"# Whatever operator overloading is available for a Tensor, is also available for a Variable\n# We have a tensor `t` and a varibale `z`. \n\nt = tf.constant([1, 2,], dtype=tf.int32)\nz = tf.Variable(t)\nprint(\"Tensor t: \", t)\nprint(\"Variable z: \", z)\n\nprint(\"\\nThis works: \", (t+5))\nprint(\"So does this: \", (z +5))\n\nprint(f\"Another example just for demonstration: {(t*5).numpy()}, {(z*5).numpy()}\")","ea45bea7":"# Gather works as well\ntf.gather(z, indices=[1])","5d6d2b26":"# Here is another interesting difference between the properties of \n# a tensor and a variable\n\ntry:\n    print(\"Is variable z trainable? \", z.trainable)\n    print(\"Is tensor t trainable? \", t.trainable)\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","a19d8de1":"x = tf.Variable(2.0, name=\"x\")\ny = tf.Variable(4.0, trainable=False, name=\"y\")\nz = tf.Variable(6.0, name=\"z\")\n\nwith tf.GradientTape() as tape:\n    x = x + 2\n    y = y + 5\n\nprint([variable.name for variable in tape.watched_variables()])","6f6e85f4":"# Create a variable instance\nz = tf.Variable([1, 2], dtype=tf.int32, name=\"z\")\nprint(f\"Variable {z.name}: \", z)\n\n# Can we change the dtype while changing the values?\ntry:\n    z.assign([1.0, 2.0])\nexcept Exception as ex:\n    print(\"\\nOh dear...what have you done!\")\n    print(type(ex).__name__, ex)\n    \n# Can we change the shape while assigning a new value?\ntry:\n    z.assign([1, 2, 3])\nexcept Exception as ex:\n    print(\"\\nAre you thinking clearly?\")\n    print(type(ex).__name__, ex)\n    \n# A way to create variable with an arbitrary shape\nx = tf.Variable(5, dtype=tf.int32, shape=tf.TensorShape(None), name=\"x\")\nprint(\"\\nOriginal Variable x: \", x)\n\n# Assign a proper value with a defined shape\nx.assign([1, 2, 3])\nprint(\"Modified Variable x: \", x)\n\n# Try assigning a value with a diff shape now.\ntry:\n    x.assign([[1, 2, 3], [4, 5, 6]])\n    print(\"\\nThis works!!\")\n    print(\"Variable value modified with a diff shape: \", x)\nexcept Exception as ex:\n    print(\"\\nDid you forget what we just learned?\")\n    print(type(ex).__name__, ex)","ea1b8958":"That's it for part 2! I hope you enjoyed reading it. We will be looking at other things k like `GradientTape` in the next tutorial!<br>\n\n**References**:\n1. https:\/\/www.tensorflow.org\/guide\/variable\n2. https:\/\/keras.io\/getting_started\/intro_to_keras_for_researchers\/\n\n\nUpdate:\n\n[TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)<br>\n[TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray\/)<br>\n[TF_JAX_Tutorials - Part 5 (Pure Functions)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax)","02586125":"The biggest advantage with a variable is that the memory can be reused. You can modify the values without creating a new one, though there are certain things to keep in mind. Check this out","3d22ceff":"In the last notebook, we read about the following:<br>\n1. Tensors\n2. Different ways of creating a tensor in TF\n3. Immutability in TF tensors\n4. Special operations like `tf.gather` and `tf.scatter`\n\nIn this tutorial, we will be looking at another building block. \n\n## Variables\n\nA `Variable` is a \"special\" kind of tensor. It is used to represent or store the mutable state. A `tf.Variable` represents a tensor whose value can be changed by running ops on it. Think of a situation where you would use a `Variable` object? **Weights** of neural networks is one of the best examples of usages of Variables. \n\nWe will first see how Variable objets are created, and then we will look into the properties and some of the gotchas","92f3f9af":"**A few things to note:**\n\n1. You can create a variable by passing an initial value which can be a `Tensor`, or Python object convertible to a `Tensor` \n2. The tensor object that you are passing is immutable but the variable created using it is mutable\n3. Variable is a `special` kind of tensor but the underlying data structure for both `tensors` and `variables` is `tf.Tensor`\n4. Since the data structure is the same, most of the properties for the two are same. We will take an example of this in a moment.\n5. Direct assignment (like z[0]=5) doesn't work with `tf.Variable` as well. For changing the values, you need to call the methods like `assign(...)`, `assign_add(...)` or `assign_sub(...)` \n6. Any Variable has the same lifecycle as any other Python object. When there are no references to a variable it is automatically deallocated.","94b3b559":"### Creating a Variable\n\nGood news! There is only one method that creates a `Variable` object: `tf.Variable(..)`","38fba6a9":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n---\n\n<img src=\"https:\/\/i.ytimg.com\/vi\/yjprpOoH5c8\/maxresdefault.jpg\" width=\"300\" height=\"300\" align=\"center\"\/>\n\nWelcome to another TensorFlow\/JAX tutorial. As I said in [Part1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1), these tutorials are meant for everyone (from novice to advanced). This tutorial is in continuation of the last part. If you haven't gone through [TF-JAX Tutorials - Part1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1), I highly recommend reading it before diving into this one. Let's start!\n\n\n**Reminder** These tutorials are in accordance with this format:\n1. TF Fundamentals (2-3 notebooks)\n2. JAX Fundamentals (2-3 notebooks)\n3. Advanced TF (2-3 notebooks)\n4. Advanced JAX (2-3 notebooks)","1b0ce9ae":"Let's talk about a bit why `trainable` is an interesting property when it comes to a Variable object.\n1. Any variable is tracked by Gradient tape (if it's in the scope) automatically unless it isn't trainable\n2. Any variable that is defined within the scope of a class that inherits `tf.Module` is tracked automatically and can be collected via the `trainable_variables`, `variables`, or `submodule` property (More on this in the future notebooks)\n3. Sometimes we don't want the gradients for a certain Variable. In that case, we can turn off the tracking by setting `trainable=False`. One example for this can be a counter"}}