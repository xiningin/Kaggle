{"cell_type":{"77444b1b":"code","1317b1e5":"code","1d08dd1a":"code","3cba1eed":"code","996a25f7":"markdown","a33e62d6":"markdown","e7049ef1":"markdown","e371a697":"markdown","438c4718":"markdown","668a0158":"markdown","42273baa":"markdown","8f6ee744":"markdown","8dba9cae":"markdown","5f887a60":"markdown","20c1dcbe":"markdown","1af7ffd5":"markdown","80f9f6cf":"markdown","aa7c0221":"markdown","0aa6c2e9":"markdown","afa1d499":"markdown","4c42b3b9":"markdown","84a6b649":"markdown","61477b8b":"markdown","3a881dbe":"markdown","98130f9d":"markdown"},"source":{"77444b1b":"def weighted_sum(a,b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n        \n    return output\n\ndef neural_network(input_val, weights):\n    pred = weighted_sum(input_val, weights)\n    return pred\n\n\ndef ele_mul(number, vector):\n    output = [0,0,0]\n    assert(len(output) == len(vector))\n    for i in range(len(vector)):\n        output[i] = number * vector[i]\n    \n    return output\n\n\n#input variables\ntoes = [8.5, 9.5, 9.9, 9.0]\nwlrec = [0.65, 0.8, 0.8, 0.9]\nnfans = [1.2, 1.3, 0.5, 1.0]\n\nwin_or_lose_binary = [1,1,0,1]\ntrue = win_or_lose_binary[0]\nalpha = 0.01\nweights = [0.1, 0.2, -.1]\ninput_val = [toes[0], wlrec[0], nfans[0]]\n\nfor i in range(3):\n    # do you prediction\n    pred = neural_network(input_val, weights)\n    error = (pred - true) ** 2\n    delta = pred - true\n    weight_deltas = ele_mul(delta, input_val)\n    \n    print(\"Iteration: {}\".format(i+1))\n    print(\"Pred: {}\".format(pred))\n    print(\"Error: {}\".format(error))\n    print(\"Delta: {}\".format(delta))\n    print(\"Weights: {}\".format(weights))\n    print(\"Weight_Deltas: {}\".format(weight_deltas))\n    print(\"Prediction: {}\".format(pred))\n    print()\n    \n    # see page 247 (in the ebook version)\n    # after you calculate the `weight_deltas` for each of the input values\n    # you're saying here\n    # \"I want you to modify each of the original weight values to be the that specific weight_delta\n    # (for that input node) multiplied by the alpha (so we don't overshoot, think about the parabola)\n    # and substract by the original weight\"\n    # then these new weights will be used in making the next prediction\n    for k in range(len(weights)):\n        weights[k] -= alpha * weight_deltas[k]","1317b1e5":"# This code is the same as above except that you are freezing(making the first weight_delta 0) and incresing\n# the alpha to 0.3\n\n#input variables\ntoes = [8.5, 9.5, 9.9, 9.0]\nwlrec = [0.65, 0.8, 0.8, 0.9]\nnfans = [1.2, 1.3, 0.5, 1.0]\n\nwin_or_lose_binary = [1,1,0,1]\ntrue = win_or_lose_binary[0]\nalpha = 0.3\nweights = [0.1, 0.2, -.1]\ninput_val = [toes[0], wlrec[0], nfans[0]]\n\nfor i in range(3):\n    # do you prediction\n    pred = neural_network(input_val, weights)\n    error = (pred - true) ** 2\n    delta = pred - true\n    weight_deltas = ele_mul(delta, input_val)\n    #freezing weight_delta[0]\n    weight_deltas[0] = 0 \n    \n    print(\"Iteration: {}\".format(i+1))\n    print(\"Pred: {}\".format(pred))\n    print(\"Error: {}\".format(error))\n    print(\"Delta: {}\".format(delta))\n    print(\"Weights: {}\".format(weights))\n    print(\"Weight_Deltas: {}\".format(weight_deltas))\n    print(\"Prediction: {}\".format(pred))\n    print()\n    \n    for k in range(len(weights)):\n        weights[k] -= alpha * weight_deltas[k]","1d08dd1a":"def neural_network(input_val, weights):\n    pred = ele_mul(input_val, weights)\n    return pred\n\n\ndef ele_mul(number, vector):\n    output = [0,0,0]\n    assert(len(output) == len(vector))\n    for i in range(len(vector)):\n        output[i] = number * vector[i]\n    \n    return output\n\nweights = [0.3, 0.2, 0.9]\nwlrec = [0.65, 1.0, 1.0, 0.9]\nhurt = [0.1, 0.0, 0.0, 0.1]\nwin = [1, 1, 0, 1]\nsad = [0.1, 0.0, 0.1, 0.2]\ninput_val3 = wlrec[0]\ntrue = [hurt[0], win[0], sad[0]]\n# the original alpha in the book is alpha = 0.1 \n# I increased it to increase the learning rate to the max of 1 with fewer iterations for the predictions to be\n# close to the true values\nalpha = 1\nerror = [0,0,0]\ndelta = [0,0,0]\n\n# I added this for loop to see how many iterations it would take\nfor k in range(10):\n    pred = neural_network(input_val3, weights)\n\n    for i in range(len(true)):\n        error[i] = (pred[i] - true[i]) ** 2\n        delta[i] = pred[i] - true[i]\n\n    weight_deltas = ele_mul(input_val3, delta)\n\n    for i in range(len(weights)):\n        weights[i] -= (weight_deltas[i] * alpha)\n\n    print(\"Step: {}\".format(k))\n    print(\"Weights: {}\".format(weights))\n    print(\"Weight Deltas: {}\".format(weight_deltas))\n    print(\"Prediction: {}\".format(pred))\n    print()","3cba1eed":"import numpy as np\n\ndef weighted_sum(a,b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n        \n    return output\n\ndef neural_network(input_val, weights):\n    pred = vect_mat_mul(input_val, weights)\n    return pred\n\n\ndef ele_mul(number, vector):\n    output = [0,0,0]\n    assert(len(output) == len(vector))\n    for i in range(len(vector)):\n        output[i] = number * vector[i]\n    \n    return output\n\ndef vect_mat_mul(vect, matrix):\n    assert(len(vect) == len(matrix))\n    output = [0,0,0]\n    for i in range(len(vect)):\n        output[i] = weighted_sum(vect, matrix[i])\n    return output\n\n\n#this code for this function was copied from this link\n# https:\/\/integratedmlai.com\/basic-linear-algebra-tools-in-pure-python-without-numpy-or-scipy\/\n# I don't know why Andrew didn't include this in ebook version\ndef zeros_matrix(rows, cols):\n    \"\"\"\n    Creates a matrix filled with zeros.\n        :param rows: the number of rows the matrix should have\n        :param cols: the number of columns the matrix should have\n \n        :return: list of lists that form the matrix\n    \"\"\"\n    M = []\n    while len(M) < rows:\n        M.append([])\n        while len(M[-1]) < cols:\n            M[-1].append(0.0)\n \n    return M\n\ndef outer_prod(vec_a, vec_b):\n    out = zeros_matrix(len(vec_a), len(vec_b))\n    \n    for i in range(len(vec_a)):\n        for j in range(len(vec_b)):\n            out[i][j] = vec_a[i] * vec_b[j]\n    \n    return out\n\n#defining variables\ntoes = [8.5, 9.5, 9.9, 9.0]\nwlrec = [0.65, 0.8, 0.8, 0.9]\nnfans = [1.2, 1.3, 0.5, 1.0]\n\nhurt = [0.1, 0.0, 0.0, 0.1]\nwin = [1, 1, 0, 1]\nsad = [0.1, 0.0, 0.1, 0.2]\n\nalpha = 0.7\n\ninput_val4 = [toes[0], wlrec[0], nfans[0]]\ntrue = [hurt[0], win[0], sad[0]]\nweights = [[0.1, 0.1, -0.3],\n          [0.1, 0.2, 0.0],\n          [0.0, 1.3, 0.1]]\nerror =[0,0,0]\ndelta = [0,0,0]\n\nfor iteration in range(3):\n    pred = neural_network(input_val4, weights)\n    \n    for i in range(len(true)):\n        error[i] = (pred[i] - true[i]) ** 2\n        delta[i] = pred[i] - true[i] \n        \n    weight_deltas = outer_prod(input_val4, delta)\n    \n    for new_i in range(len(weights)):\n        for j in range(len(weights[0])):\n            weights[new_i][j] -= alpha * weight_deltas[new_i][j]\n            \n    print(\"Step: {}\".format(iteration))\n    print(\"Weights: {}\".format(weights))\n    print(\"Weight Deltas: {}\".format(weight_deltas))\n    print(\"Prediction: {}\".format(pred))\n    print()\n    \n","996a25f7":"Since you want your `pred` value to be as close to 1 as possible, after about 3 iterations you are already pretty close with `0.9906`!","a33e62d6":"![display image](https:\/\/thumbs.gfycat.com\/AngryInconsequentialDiplodocus-size_restricted.gif)","e7049ef1":"So in between this notebook and the previous notebook, there is a lot of talking about gradient descent in Deep Learning. However, I want to show a visual representation of what is actually going on with the math. I'm not well suited in matplotlib (as of yet) so I believe the gifs below are good in showing\/plotting what is going on mathematically.\n\nWith both GIF's you see that (whether it's the dots or the black line), both are trying to get to the lowest point in the parabola. Trask says, \"What you're really trying to do with the neural network is find the lowest point on this big error plane (the parabola's below), where the lowest point refers to the lowest `error`\" [2]. This \"lowest error\" means you have reached a point in your iterations where your `pred = input * weights` actually is very close to your values that you want to see or your `true` in this case [2].","e371a697":"[2] \u201cLearning multiple weights at a time: Generalizing Gradient Descent\u201d Grokking Deep Learning, by Andrew W. Trask, Manning Publications, 2019, p. 267.","438c4718":"This example is in the reverse (unlike the above), when you have one input but multiple outputs. Since all of the outputs share one input, they will have different `weight_deltas`.","668a0158":"Fig 1. Gradient Descent GIF (https:\/\/gfycat.com\/angryinconsequentialdiplodocus) taken from Andrew Ng's Machine Learning Course [3]","42273baa":"# References","8f6ee744":"[1] \u201cLearning multiple weights at a time: Generalizing Gradient Descent\u201d Grokking Deep Learning, by Andrew W. Trask, Manning Publications, 2019, p. 246.","8dba9cae":"A few notes: \n\nAs an Amazon Associate I earn from qualifying purchases. I get commissions for purchases made through links in this jupyter notebook. See a more full disclaimer [here](https:\/\/jdridgeway.com\/disclaimer\/)\n\nMaterial that the following code is drawn upon comes from the Grokking Deep Learning book which you can purchase here (ebook: https:\/\/www.manning.com\/books\/grokking-deep-learning) or (physical: https:\/\/amzn.to\/2AOcNCh)\n\nCheck out the blog post accompanying this notebook [here](https:\/\/jdridgeway.com\/deep-learning-from-scratch-generalizing-gradient-descent\/)","5f887a60":"## Gradient Descent with Multiple Inputs and Multiple Outputs","20c1dcbe":"## Couple of Visual Examples of What is Happening W\/ Gradient Descent","1af7ffd5":"[3] Ng, Andrew. \"Linear Regression with One Variable | Gradient Descent - [Andrew Ng]\" *Youtube*, 22 June 2020. https:\/\/www.youtube.com\/watch?v=F6GSRDoB-Cg&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=8","80f9f6cf":"[4] Tejani, Alykhan. \"A Brief Introduction To Gradient Descent\" *alykhantejani.github.io*, 22 June 2020. https:\/\/alykhantejani.github.io\/a-brief-introduction-to-gradient-descent\/","aa7c0221":"So when Trask talks about having multiple input nodes (and their respective weights) with one output node and says, \"In this case, because the three weights share the same output node, they also share that node's `delta`. But the weights have different `weight_deltas` owing to their different `input` values\", this makes sense [1]. That is because though you have done the dot product, because you have multiple yet distinct input values, their \"effect\" individually on the output value is different.","0aa6c2e9":"# Chapter 5: Learning Mulitple Weights at at Time: Generalizing Gradient Descent","afa1d499":"## Let's Get Started \ud83d\udc4c\ud83c\udffe","4c42b3b9":"## Gradient Descent learning with Multiple Outputs","84a6b649":"![visual_image](https:\/\/media.giphy.com\/media\/O9rcZVmRcEGqI\/giphy.gif)","61477b8b":"Something to keep in mind: The difference between `delta = pred-true` and `weight_delta = delta * input` is that `delta` by itself is just telling \"How much are you off by (positive or negative)?\". However, `weight_delta` is telling you \"How much are you off by (positive or negative) in relation to the actual input node?\"","3a881dbe":"Fig. 2 Gradient Descent GIF (https:\/\/giphy.com\/gifs\/gradient-O9rcZVmRcEGqI) taken from Alykhan Tejani's blog [4]","98130f9d":"## Freezing One Weight: What does it do?"}}