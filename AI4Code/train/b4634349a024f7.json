{"cell_type":{"0f382a73":"code","a1758e90":"code","aa112399":"code","93862119":"code","6bf4fc08":"code","902b3526":"code","281c38d7":"code","245d1009":"code","d6a4dd4c":"code","1954ae63":"code","3b4fafaa":"code","be537282":"code","a7df4269":"code","07667629":"code","cd89d72b":"code","4156bf6b":"code","eda73973":"code","edd714ab":"code","ca0634d9":"code","d81c419b":"code","f4c0c043":"code","15587e22":"code","934ffe74":"code","1850e85e":"code","969817b7":"code","a7c7bc5d":"code","fc1df45a":"code","1fe5512a":"code","1f833066":"code","8259be13":"markdown","a55b61f3":"markdown","c9002d72":"markdown","85d03115":"markdown","11dbf765":"markdown"},"source":{"0f382a73":"# For the current version: \n!pip install --upgrade tensorflow\n!pip install ktrain","a1758e90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, shutil\nimport ktrain\nfrom ktrain import text\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aa112399":"!wget http:\/\/mlg.ucd.ie\/files\/datasets\/bbc-fulltext.zip","93862119":"!ls -GFlash --color .\/","6bf4fc08":"!unzip bbc-fulltext.zip\n","902b3526":"!ls .\/bbc","281c38d7":"!ls ..\/base_dir","245d1009":"# if need deleting folder\n!rm -rf ..\/base_dir\n\n# 'business',  'entertainment',  'politics',  'sport',  'tech'\nb_original_dataset_dir = '.\/bbc\/business'\ne_original_dataset_dir = '.\/bbc\/entertainment'\np_original_dataset_dir = '.\/bbc\/politics'\ns_original_dataset_dir = '.\/bbc\/sport'\nt_original_dataset_dir = '.\/bbc\/tech'\n\nbase_dir = '..\/base_dir'\nos.mkdir(base_dir)\n\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\n\ntrain_b_dir = os.path.join(train_dir, 'business')\nos.mkdir(train_b_dir)\ntrain_e_dir = os.path.join(train_dir, 'entertainment')\nos.mkdir(train_e_dir)\ntrain_p_dir = os.path.join(train_dir, 'politics')\nos.mkdir(train_p_dir)\ntrain_s_dir = os.path.join(train_dir, 'sport')\nos.mkdir(train_s_dir)\ntrain_t_dir = os.path.join(train_dir, 'tech')\nos.mkdir(train_t_dir)\n\ntest_b_dir = os.path.join(test_dir, 'business')\nos.mkdir(test_b_dir)\ntest_e_dir = os.path.join(test_dir, 'entertainment')\nos.mkdir(test_e_dir)\ntest_p_dir = os.path.join(test_dir, 'politics')\nos.mkdir(test_p_dir)\ntest_s_dir = os.path.join(test_dir, 'sport')\nos.mkdir(test_s_dir)\ntest_t_dir = os.path.join(test_dir, 'tech')\nos.mkdir(test_t_dir)\n\n!ls ..\/base_dir","d6a4dd4c":"b_100 = ['{}.txt'.format(i) for i in range(100, 300)]\nprint(b_100[:5])\nb_300 = ['{}.txt'.format(i) for i in range(300, 350)]\nprint(b_300[:5])\n\nfor fname in b_100:\n    src = os.path.join(b_original_dataset_dir, fname)\n    dst = os.path.join(train_b_dir, fname)\n    shutil.copyfile(src, dst)\n\nfor fname in b_300:\n    src = os.path.join(b_original_dataset_dir, fname)\n    dst = os.path.join(test_b_dir, fname)\n    shutil.copyfile(src, dst)\n\n#\n!head .\/base_dir\/test\/business\/300.txt","1954ae63":"e_100 = ['{}.txt'.format(i) for i in range(100, 300)]\nprint(e_100[:5])\ne_300 = ['{}.txt'.format(i) for i in range(300, 350)]\nprint(e_300[:5])\n\nfor fname in e_100:\n    src = os.path.join(e_original_dataset_dir, fname)\n    dst = os.path.join(train_e_dir, fname)\n    shutil.copyfile(src, dst)\n\nfor fname in e_300:\n    src = os.path.join(e_original_dataset_dir, fname)\n    dst = os.path.join(test_e_dir, fname)\n    shutil.copyfile(src, dst)\n\n#\n!head .\/base_dir\/test\/entertainment\/300.txt","3b4fafaa":"p_100 = ['{}.txt'.format(i) for i in range(100, 300)]\nprint(p_100[:5])\np_300 = ['{}.txt'.format(i) for i in range(300, 350)]\nprint(p_300[:5])\n\nfor fname in p_100:\n    src = os.path.join(p_original_dataset_dir, fname)\n    dst = os.path.join(train_p_dir, fname)\n    shutil.copyfile(src, dst)\n\nfor fname in p_300:\n    src = os.path.join(p_original_dataset_dir, fname)\n    dst = os.path.join(test_p_dir, fname)\n    shutil.copyfile(src, dst)\n\n#\n!head .\/base_dir\/test\/politics\/300.txt","be537282":"s_100 = ['{}.txt'.format(i) for i in range(100, 300)]\nprint(s_100[:5])\ns_300 = ['{}.txt'.format(i) for i in range(300, 350)]\nprint(s_300[:5])\n\nfor fname in s_100:\n    src = os.path.join(s_original_dataset_dir, fname)\n    dst = os.path.join(train_s_dir, fname)\n    shutil.copyfile(src, dst)\n\nfor fname in s_300:\n    src = os.path.join(s_original_dataset_dir, fname)\n    dst = os.path.join(test_s_dir, fname)\n    shutil.copyfile(src, dst)\n\n#\n!head .\/base_dir\/test\/sport\/300.txt","a7df4269":"t_100 = ['{}.txt'.format(i) for i in range(100, 300)]\nprint(t_100[:5])\nt_300 = ['{}.txt'.format(i) for i in range(300, 350)]\nprint(t_300[:5])\n\nfor fname in t_100:\n    src = os.path.join(t_original_dataset_dir, fname)\n    dst = os.path.join(train_t_dir, fname)\n    shutil.copyfile(src, dst)\n\nfor fname in t_300:\n    src = os.path.join(t_original_dataset_dir, fname)\n    dst = os.path.join(test_t_dir, fname)\n    shutil.copyfile(src, dst)\n\n#\n!head .\/base_dir\/test\/tech\/300.txt","07667629":"import ktrain\nfrom ktrain import text as txt\nktrain.__version__","cd89d72b":"#\n(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder('..\/base_dir', \n                                                                       maxlen=75, \n                                                                       max_features=10000,\n                                                                       preprocess_mode='bert',\n                                                                       #train_test_names=['train', 'validation'],\n                                                                       val_pct=0.1,\n                                                                       classes=['business',  'entertainment',  'politics',  'sport',  'tech'])","4156bf6b":"# STEP 2: Create a Model and Wrap in Learner Object\nmodel = text.text_classifier('bert', (x_train, y_train) , preproc=preproc)\nlearner = ktrain.get_learner(model, \n                             train_data=(x_train, y_train), \n                             val_data=(x_test, y_test), \n                             batch_size=32)","eda73973":"# STEP 3: Train the Model\nlearner.fit_onecycle(2e-5, 3, checkpoint_folder='..\/saved_weights')","edd714ab":"learner.validate(val_data=(x_test, y_test), class_names=['business',  'entertainment',\n                                                         'politics',  'sport ', 'tech'])","ca0634d9":"# Inspecting Misclassifications\nlearner.view_top_losses(n=3, preproc=preproc)","d81c419b":"# Making Predictions on New Data\np = ktrain.get_predictor(learner.model, preproc)","f4c0c043":"p.get_classes()","15587e22":"# Predicting label for the text\np.predict(\"Today crisis is very hight.\")","934ffe74":"# Predicting label for the text\np.predict(\"Today people use a lot of devices.\")","1850e85e":"# Predicting label for the text\np.predict('Todays wordwide the crisis is the biggest in the last forty years.')","969817b7":"p.save('..\/mypred')","a7c7bc5d":"fin_bert_model = ktrain.load_predictor('..\/mypred')","fc1df45a":"# still works\nfin_bert_model.predict('Todays wordwide the crisis is the biggest in the last forty years.')","1fe5512a":"# still works\nfin_bert_model.predict('Todays wordwide crysis is the biggest in the last forty years.')","1f833066":"# still works\nfin_bert_model.predict(\"Bob Dylan has released a song about the Kennedy assassination -- and it's 17 minutes long.\")","8259be13":"THE END","a55b61f3":"### Ktrain library","c9002d72":"## STEP 1:  Load and Preprocess the Data\n\nFirst, we use the `texts_from_folder` function to load and preprocess the data.  We assume that the data is in the following form:\n```\n    \u251c\u2500\u2500 datadir\n    \u2502   \u251c\u2500\u2500 train\n    \u2502   \u2502   \u251c\u2500\u2500 class0       # folder containing documents of class 0\n    \u2502   \u2502   \u251c\u2500\u2500 class1       # folder containing documents of class 1\n    \u2502   \u2502   \u251c\u2500\u2500 class2       # folder containing documents of class 2\n    \u2502   \u2502   \u2514\u2500\u2500 classN       # folder containing documents of class N\n```\nWe set `val_pct` as 0.1, which will automatically sample 10% of the data for validation.  Since we will be using a pretrained BERT model for classification, we specifiy `preprocess_mode='bert'`.  If you are using any other model (e.g., `fasttext`), you should either omit this parameter or use `preprocess_mode='standard'`).\n\n**Notice that there is nothing speical or extra we need to do here for non-English text.**  *ktrain* automatically detects the language and character encoding and prepares the data and configures the model appropriately.","85d03115":"### Building a Sentiment Analyzer","11dbf765":"### Save Predictor for Later Deployment"}}