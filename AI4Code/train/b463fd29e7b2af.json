{"cell_type":{"3350d428":"code","d5482ffe":"code","7cb10d25":"code","cd1d63ac":"code","8f31b19b":"code","0cf44f49":"code","0caf538a":"code","6a440916":"code","9bc9c90d":"code","fba672de":"code","fc5120c9":"markdown"},"source":{"3350d428":"import numpy as np\nimport pandas as pd \n    \ndf_test  = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\n\ndf_train.sample(n=1)","d5482ffe":"# Removing data\n# After fitting a model several times, I noticed that there are some digits that could not be classified even by a human. I want to delete this noisy data to ease learning.\n\ndf_train = df_train.drop(index=[335, 445, 666, 737, 881, 1314, 59, 131, 170, 302, 844, 1725, 1820])","7cb10d25":"# Split data frames into training\/test sets and normalize pixel values\n\nX_train = np.array(df_train.loc[:, df_train.columns != 'label'])\nX_test  = np.array(df_test)\ny_train = np.array(df_train['label'])\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(\"float32\") \/ 255\nX_test  = X_test.reshape(X_test.shape[0], 28, 28, 1).astype(\"float32\") \/ 255\n\nprint(f\"X_train: {X_train.shape}\\nX_test: {X_test.shape}\\ny_train: {y_train.shape}\")","cd1d63ac":"# One-hot encoding for labels\n\nfrom tensorflow.keras.utils import to_categorical\n\ny_train = to_categorical(y_train)\nprint(\"Sample one-hot encoded label:\", y_train[0])","8f31b19b":"# Creating many convolutional neural networks which I'll use for predictions\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n\nnum_models = 8\nnet_models = [0] * num_models\n\nfor i in range(num_models):\n    net_models[i] = Sequential()\n\n    net_models[i].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Conv2D(32, kernel_size = 3, activation='relu'))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Dropout(0.4))\n\n    net_models[i].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Dropout(0.4))\n\n    net_models[i].add(Conv2D(128, kernel_size = 4, activation='relu'))\n    net_models[i].add(BatchNormalization())\n    net_models[i].add(Flatten())\n    net_models[i].add(Dropout(0.4))\n    net_models[i].add(Dense(10, activation='softmax'))\n\n    net_models[i].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","0cf44f49":"# Adding some additional data by applying zoom, rotation and x\/y shifts\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    zoom_range=0.1,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n)\n\ndatagen.fit(X_train)","0caf538a":"# Ensemble models\n# I train each model on a differently split data (80% for training, 20% for validation) and store training histories.\n# After each step I also create a confusion matrix to see what kind of error my networks made after training.\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nhistories = [0] * num_models\ntotal_confusion_matrix = np.zeros((10, 10))\n\nfor i in range(num_models):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)\n    histories[i] = net_models[i].fit_generator(datagen.flow(X_train, y_train, batch_size=16),\n                                               epochs=50, verbose=0, validation_data=(X_valid, y_valid),\n                                               callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, min_delta=0.0001)])\n    \n    print(f\"Trained model no. {i+1}, val_acc: {max(histories[i].history['val_acc'])}, val_loss: {max(histories[i].history['val_loss'])}\")\n    \n    y_valid = [np.argmax(x) for x in y_valid]\n    y_valid_pred = [np.argmax(x) for x in net_models[i].predict(X_valid, batch_size=16)]\n    total_confusion_matrix += confusion_matrix(y_valid, y_valid_pred)","6a440916":"# Plot combined confusion matrix (sum from all models) to see what errors have been made\n\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nlabels = range(0, 10)\nplt.figure(figsize=(20, 6))\ndf_cm = pd.DataFrame(total_confusion_matrix.astype(int), labels, labels)\nsn.set(font_scale=1.4)\nax = sn.heatmap(df_cm, annot=True, fmt='d')\nplt.yticks(rotation=0)\nplt.show()","9bc9c90d":"# Plot validation accuracy and loss for all models\n\nplt.figure(figsize=(15,3))\nplt.subplot(121)\nfor hist in histories:\n    plt.plot(hist.history['val_acc'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'])\n\nplt.subplot(122)\nfor hist in histories:\n    plt.plot(hist.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'])\nplt.show()","fba672de":"# Here I sum all softmax outputs from models and use np.argmax to retrieve final prediction!\n\ny_test = np.zeros((X_test.shape[0], 10))\nfor i in range(num_models):\n    y_test = y_test + net_models[i].predict(X_test)\n    \ny_test = np.argmax(y_test,axis = 1)\n\npd.DataFrame({'ImageId': range(1, 28001), 'Label': y_test}).to_csv(r'submission.csv', index=False)","fc5120c9":"Hi there!\n\nIf you look for a way to beat MNIST challenge or you got stuck at lower score and you want to find a better, simple solution - here it is. This kernel uses very simple CNN model + data augmentation and can be understood and implemented even by a total beginner (like me). Feel free to read my code, comment it and give me feedback about my solution. This solution is inspired by [another](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist) existing Kernel which made me realize that ensembling CNNs makes sense and made me wanna try it."}}