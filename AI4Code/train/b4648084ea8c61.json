{"cell_type":{"2ae54c2f":"code","d1c5143d":"code","b57776c7":"code","eb08aeed":"code","dee5f19f":"code","52413245":"code","257b0236":"code","de023051":"code","4ac6307c":"code","82a5f4f5":"code","4277553b":"code","faff1f26":"code","d0a9bd13":"code","c130979f":"code","556c3697":"code","637e9ae0":"code","c1efcd45":"code","f054c434":"code","a31aeb94":"code","b16bc8c3":"markdown","ecb9396e":"markdown","700766f4":"markdown","2d1302a2":"markdown","cde97bc0":"markdown","689e8d58":"markdown"},"source":{"2ae54c2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1c5143d":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\ntrain","b57776c7":"train.drop(['Id'], axis = 1, inplace = True)\ntest.drop(['Id'], axis = 1, inplace = True)\nTARGET  = 'Cover_Type'\nFEATURES = [col for col in train.columns if col not in ['Id', TARGET]]\ntrain.info()","eb08aeed":"df = pd.concat([train[FEATURES], test[FEATURES]], axis = 0)\n\ncat_features = [col for col in df.columns if df[col].nunique() < 10]\ncont_features = [col for col in df.columns if df[col].nunique() >= 10]\n\ndel df\nprint('Categorical features:', len(cat_features))\nprint('Continuous features:', len(cont_features))","dee5f19f":"import matplotlib.pyplot as plt\n\nplt.pie([len(cat_features), len(cont_features)], \n       labels = ['Categorical', 'Continuous'], \n       autopct = '%.2f%%')\nplt.title('Continuous vs Categorical features')\nplt.show()","52413245":"import seaborn as sns\n\nncols = 5\nnrows = 2\n\nfig, axes = plt.subplots(nrows, ncols, figsize = (20, 10))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r * ncols + c]\n        sns.kdeplot(x = train[col], ax = axes[r, c], label = 'Train data')\n        sns.kdeplot(x = test[col], ax = axes[r, c], label = 'Test data')\nplt.show()","257b0236":"ncols = 5\nnrows = int(len(cat_features) \/ ncols + (len(FEATURES) % ncols > 0)) \n\nfig, axes = plt.subplots(nrows, ncols, figsize = (18, 45))\n\nfor r in range(nrows):\n    for c in range(ncols):\n        if r * ncols + c >= len(cat_features):\n            break\n        col = cat_features[r * ncols + c]\n        sns.countplot(x = train[col], ax = axes[r, c], label = 'Train data')\n        sns.countplot(x = test[col], ax = axes[r, c], label = 'Test data')\nplt.show()","de023051":"# Since Soil_Type7 and Soil_Type15 are all 0 values\ntrain = train.drop(labels = [\"Soil_Type7\" , \"Soil_Type15\"] ,axis = 1)\nFEATURES.remove('Soil_Type7')\nFEATURES.remove('Soil_Type15')","4ac6307c":"sns.countplot(x = train[TARGET])\nplt.show()","82a5f4f5":"train[TARGET].value_counts().sort_index()","4277553b":"train.loc[train['Cover_Type'] == 5]","faff1f26":"# Since Cover_Type = 5 has only one sample, it's prob. safe to remove it. \ntrain.drop(train.loc[train['Cover_Type'] == 5].index, inplace = True)\n\ntrain['Cover_Type'].value_counts()","d0a9bd13":"train[\"mean\"] = train[FEATURES].mean(axis = 1)\ntrain[\"std\"] = train[FEATURES].std(axis = 1)\ntrain[\"min\"] = train[FEATURES].min(axis = 1)\ntrain[\"max\"] = train[FEATURES].max(axis = 1)\n\ntest[\"mean\"] = test[FEATURES].mean(axis = 1)\ntest[\"std\"] = test[FEATURES].std(axis = 1)\ntest[\"min\"] = test[FEATURES].min(axis = 1)\ntest[\"max\"] = test[FEATURES].max(axis = 1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","c130979f":"X = train.drop([TARGET], axis = 1)\ny = train[TARGET]\nX_test = test[FEATURES]","556c3697":"# from xgboost import XGBClassifier\n# from sklearn.model_selection import cross_val_score\n\n# xgb_params = {\n#     'objective': 'multi:softmax',\n#     'eval_metric': 'mlogloss',\n#     'tree_method': 'gpu_hist',\n#     'predictor': 'gpu_predictor',\n#     }\n\n# model = XGBClassifier(**xgb_params)\n# cross_val_score(model, X, y, cv = 5, scoring = 'accuracy')","637e9ae0":"# Since a very large dataset(4000000 rows), one validation set is enough instead of cross-validation\n# But remember to do stratified sampling\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, stratify = train['Cover_Type'])\n# test_size = 0.25 since the data is very large","c1efcd45":"from xgboost import XGBClassifier\n\nxgb_params = {\n    'objective': 'multi:softmax',\n    'eval_metric': 'mlogloss', \n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    }\n\nmodel = XGBClassifier(**xgb_params)","f054c434":"from sklearn.metrics import accuracy_score\n\nmodel.fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\naccuracy_score(y_val, y_val_pred)","a31aeb94":"y_pred = model.predict(X_test)\n\nsubmission[TARGET] = y_pred\nsubmission.to_csv('submission.csv', index = False)","b16bc8c3":"## Target Distribution","ecb9396e":"## XGBoost Classifier","700766f4":"## Feature Engineering","2d1302a2":"## EDA","cde97bc0":"## Categorical Features","689e8d58":"## Continuous Features"}}