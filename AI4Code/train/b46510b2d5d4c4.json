{"cell_type":{"9c62e1dd":"code","e02f9078":"code","032e9631":"code","3ceebf9e":"code","dc9d1539":"code","030fd9f1":"code","d4556e78":"code","99330c45":"code","fa3a0fcc":"code","2f42b45d":"code","e5ed2a6f":"code","6b710451":"code","c3328d76":"code","ce366f36":"code","f08edc5d":"code","7d70014c":"code","43422fad":"code","3b15fb78":"code","00d9b658":"code","b41477fa":"code","f9232fb3":"code","f3fa6740":"code","c3e52309":"code","92db95e9":"code","1ef8402d":"code","0a6824c2":"code","1472eb9c":"code","0e3f08b4":"code","8b49eaec":"code","c5f66e19":"code","7b14dd70":"code","341b9564":"code","d25ab43c":"code","6c2e0f7d":"code","a26a9a7d":"code","078699f6":"code","c0b6cc8b":"code","f1e6ab5b":"code","a7e142c1":"code","699a66c8":"code","e0677c1c":"code","334e9c76":"code","9f5eb7df":"code","7fafaf5b":"code","60bbfca6":"code","5dfc47fa":"code","d43c6971":"code","31378e8e":"code","cd8376ef":"code","c9458181":"code","21bcedda":"code","546ef149":"code","6f6154b6":"code","a2efa352":"code","47b3d898":"code","e4237639":"code","5753b103":"code","5b16bf18":"code","3c51a3b7":"code","6861032d":"code","05ba91c1":"code","d5eda546":"code","9c8f0cea":"code","ba5dac76":"code","a0356d36":"code","577cf7ed":"code","7e8fcc27":"code","165a8bba":"code","3f8eeb0b":"code","8aad4d04":"code","4118b9fd":"code","53a1d3da":"code","96a2f389":"code","ac10eb30":"code","57b42463":"code","ae99ae0e":"code","e52e06fd":"code","310b07e0":"code","45fbf081":"code","b8d86af9":"code","1b8d1f7a":"code","d7ee4342":"code","704afb15":"code","5e2b9a2a":"code","a618fc91":"code","74fab6c8":"code","d507d18b":"code","e6c3559b":"code","2198c61e":"code","81222eb6":"code","0037d039":"markdown","73ae3745":"markdown","c5e6ffca":"markdown","81739d97":"markdown","e1313976":"markdown","986d564b":"markdown","4ee22968":"markdown","7612587a":"markdown","d56be158":"markdown","95ceb670":"markdown"},"source":{"9c62e1dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e02f9078":"df=pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf","032e9631":"df[\"id\"]","3ceebf9e":"df.drop(\"id\",axis=1, inplace=True)\ndf","dc9d1539":"df.drop(\"Unnamed: 32\",axis=1, inplace=True)\ndf\n","030fd9f1":"df.columns","d4556e78":"df.isnull().sum()","99330c45":"from sklearn.preprocessing import LabelEncoder","fa3a0fcc":"le=LabelEncoder()","2f42b45d":"le.fit_transform(df[\"diagnosis\"])","e5ed2a6f":"df[\"diagnosis\"]=le.fit_transform(df[\"diagnosis\"])\ndf","6b710451":"df[\"diagnosis\"].value_counts()","c3328d76":"df.corr()","ce366f36":"import matplotlib.pyplot as plt\nimport seaborn as sns","f08edc5d":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap=\"inferno\")","7d70014c":"df.columns","43422fad":"X=df.drop(\"diagnosis\",axis=1)\nX=X.values\nX","3b15fb78":"y=df[\"diagnosis\"]\ny=y.values\ny","00d9b658":"from sklearn.model_selection import train_test_split","b41477fa":"X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)","f9232fb3":"X_train.shape","f3fa6740":"y_train.shape","c3e52309":"X_test.shape","92db95e9":"y_test.shape","1ef8402d":"from sklearn.preprocessing import StandardScaler","0a6824c2":"sc=StandardScaler()","1472eb9c":"X_train=sc.fit_transform(X_train)\nX_train","0e3f08b4":"X_test=sc.transform(X_test)\nX_test","8b49eaec":"from tensorflow.keras.models import Sequential # creates`Sequential` groups a linear stack of layers into a `tf.keras.Model`.\nfrom tensorflow.keras.layers import Dense #regularly connected neurons)","c5f66e19":"ann=Sequential()","7b14dd70":"ann.add(Dense(units=30, activation=\"relu\")) # here we add a dense layer with 30 neurons","341b9564":"ann.add(Dense(units=15, activation=\"relu\")) # here we add a dense layer with 15 neurons","d25ab43c":"ann.add(Dense(1,activation=\"sigmoid\"))  # here we add output layer with 1 neuron","6c2e0f7d":"ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","a26a9a7d":"ann.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","078699f6":"pd.DataFrame(ann.history.history)","c0b6cc8b":"pd.DataFrame(ann.history.history).plot(figsize=(15,10))","f1e6ab5b":"ann.evaluate(X_train, y_train)","a7e142c1":"ann.evaluate(X_test, y_test)","699a66c8":"X_test","e0677c1c":"X_test[[1]]","334e9c76":"ann.predict(X_test[[1]])","9f5eb7df":"y_test[[1]]","7fafaf5b":"predictions=ann.predict(X_test)\npredictions","60bbfca6":"predictions_df=pd.DataFrame(predictions, columns=[\"Predictions\"])\npredictions_df.head()","5dfc47fa":"y_test_df=pd.DataFrame(y_test, columns=[\"Diagnosis\"])\ny_test_df.head()","d43c6971":"comparison_df=pd.concat([predictions_df, y_test_df], axis=1)\ncomparison_df.head()","31378e8e":"ann2=Sequential()","cd8376ef":"ann2.add(Dense(units=1, activation=\"relu\")) # here we add a dense layer with 1 neuron","c9458181":"ann2.add(Dense(1,activation=\"sigmoid\")) ","21bcedda":"ann2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","546ef149":"ann2.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","6f6154b6":"ann2.evaluate(X_train, y_train)","a2efa352":"ann2.evaluate(X_test, y_test)","47b3d898":"ann3=Sequential()","e4237639":"ann3.add(Dense(units=100, activation=\"relu\")) # here we add a dense layer with 100 neurons","5753b103":"ann3.add(Dense(units=100, activation=\"relu\")) # here we add a dense layer with 100 neurons","5b16bf18":"ann3.add(Dense(units=100, activation=\"relu\")) # here we add a dense layer with 100 neurons","3c51a3b7":"ann3.add(Dense(1,activation=\"sigmoid\")) ","6861032d":"ann3.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","05ba91c1":"ann3.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","d5eda546":"ann3.evaluate(X_train, y_train)","9c8f0cea":"ann3.evaluate(X_test, y_test)","ba5dac76":"history=ann3.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","a0356d36":"ann3.history.history","577cf7ed":"pd.DataFrame(ann3.history.history).plot(figsize=(15,10))","7e8fcc27":"from keras.regularizers import l2","165a8bba":"ann4=Sequential()","3f8eeb0b":"ann4.add(Dense(100,activation=\"relu\", kernel_regularizer=l2(0.001)))","8aad4d04":"ann4.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","4118b9fd":"ann4.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","53a1d3da":"ann4.add(Dense(1,activation=\"sigmoid\")) ","96a2f389":"ann4.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","ac10eb30":"history=ann4.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","57b42463":"pd.DataFrame(ann4.history.history).plot(figsize=(15,10))","ae99ae0e":"from tensorflow.keras.callbacks import EarlyStopping","e52e06fd":"ann5=Sequential()","310b07e0":"ann5.add(Dense(100,activation=\"relu\", kernel_regularizer=l2(0.001)))","45fbf081":"ann5.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","b8d86af9":"ann5.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","1b8d1f7a":"ann5.add(Dense(1,activation=\"sigmoid\")) ","d7ee4342":"ann5.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","704afb15":"history=ann5.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor=\"val_accuracy\",patience=10)])","5e2b9a2a":"pd.DataFrame(ann5.history.history).plot(figsize=(15,10))\n# Now we stopped the training after gettin %100 accuracy in the test set","a618fc91":"from sklearn.tree import DecisionTreeClassifier","74fab6c8":"dtree=DecisionTreeClassifier()","d507d18b":"dtree.fit(X_train,y_train)","e6c3559b":"predictions2=dtree.predict(X_test)","2198c61e":"from sklearn.metrics import classification_report, confusion_matrix","81222eb6":"print(classification_report(y_test,predictions2)) ","0037d039":"In order to see the overfitting:","73ae3745":"Thanks to weight decay we got %100 accuracy in the test set after 65 epoch, after that there happens a overfitting.\n\nWe will use early stopping to prevent this problem:","c5e6ffca":"As we can obviously see, the deep learning model easily outperform normal machine learning algorithms.","81739d97":"An overfit model should show accuracy increasing on both train and test and at some point accuracy drops on the test dataset but continues to rise on the training dataset.","e1313976":"Now we will compare the performance of the deep learning with a normal machine learning model","986d564b":"The activation function is a mathematical \u201cgate\u201d in between the input feeding the current neuron and its output going to the next layer. It can be as simple as a step function that turns the neuron output on and off, depending on a rule or threshold. Or it can be a transformation that maps the input signals into output signals that are needed for the neural network to function.\n\n\nTypes of Activation Function:\n\n1.Sigmoid \/ Logistic\n\n2.TanH \/ Hyperbolic Tangent\n\n3.ReLU (Rectified Linear Unit)\n\n4.Softmax","4ee22968":"Keras provides a weight regularization API that allows you to add a penalty for weight size to the loss function.\n\nThree different regularizer instances are provided; they are:\n\nL1: Sum of the absolute weights.\nL2: Sum of the squared weights.\nL1L2: Sum of the absolute and the squared weights.","7612587a":"We can see an expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again.","d56be158":" optimizer:this determines how we want to perform the gradient descent like adam optimizer loss= represents cost function we want to use\n\nType of Optimizers:\n\n1.Gradient Descent\n\n2.Stochastic Gradient Descent\n\n3.Mini-Batch Gradient Descent\n\n4.Adam:The intuition behind the Adam is that we don\u2019t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\n\n\nChoosing an optimizer: Keep in mind what kind of problem we are trying to solve:\n\nFor a multi-class classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nFor a binary classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nFor a mean squared error regression problem:\n\nmodel.compile(optimizer='rmsprop', loss='mse')","95ceb670":"The most common type of regularization is L2, also called simply \u201cweight decay,\u201d with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc."}}