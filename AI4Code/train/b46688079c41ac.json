{"cell_type":{"62c2844c":"code","38549394":"code","9d01b875":"code","c2eb1655":"code","3fcfb1c9":"code","5b0b9c7a":"code","1b612a62":"code","d983dad2":"code","3eb5bb70":"code","054fd997":"code","2f7ae483":"code","7e63220f":"code","ba5e77cc":"code","5915cfe9":"code","8c9f3e3a":"code","79119cff":"code","9ca5981d":"code","9583f92f":"code","a7120fee":"code","e762ff8b":"code","772c84e9":"code","ffc793ba":"markdown","82d2b81f":"markdown","3a1ce910":"markdown","fed6aaa5":"markdown","44706ee2":"markdown","19d6d3e4":"markdown","a3fc3b22":"markdown","ed373bbc":"markdown","9cb5c0ea":"markdown","b386dbff":"markdown","4377fda1":"markdown","a149693c":"markdown","5dadebdd":"markdown","3082c623":"markdown","4cd98450":"markdown","2fbefd3c":"markdown","338a4f6a":"markdown","451ca387":"markdown","a37123bc":"markdown","411feb6d":"markdown","88482836":"markdown","329c6294":"markdown"},"source":{"62c2844c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport matplotlib.transforms as mtransforms\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport torch\nimport gc\nimport torch.nn as nn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split","38549394":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\nss    = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","9d01b875":"train_df = train.drop(['id'], axis = 1 )\ntest_df = test.drop(['id'], axis = 1)","c2eb1655":"train.isnull().sum()[train.isnull().sum() != 0]","3fcfb1c9":"colormap = plt.cm.PuBu \nplt.figure(figsize=(15, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(train_df.iloc[:, 0:10].astype(float).corr(), linewidths = 0.1, vmax = 1.0,square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 10})","5b0b9c7a":"plt.figure(figsize=(15, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(train_df.iloc[:, 11:20].astype(float).corr(), linewidths = 0.1, vmax = 1.0,square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 10})","1b612a62":"plt.figure(figsize=(15, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(train_df.iloc[:, 21:30].astype(float).corr(), linewidths = 0.1, vmax = 1.0,square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 10})","d983dad2":"train_df_X = train_df.drop('target', axis =1)\ntrain_df_y = train_df['target']","3eb5bb70":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndef show_vif(df):\n    vif = []\n    for idx in range(len(df.columns)):\n        vif.append(variance_inflation_factor(df.values, idx))\n\n    vif_dataframe = pd.DataFrame()\n    vif_dataframe['columns'] = df.columns\n    vif_dataframe['VIF'] = vif\n    return vif_dataframe","054fd997":"show_vif(train_df_X)","2f7ae483":"# Eliminate multicollinearity(Remove vif > 10 higher)\ndef remove_multicollinearity(df):\n    while True:\n        vif_dataframe = show_vif(df)\n        \n        print(len(vif_dataframe[vif_dataframe['VIF'] >= 10]))\n        if len(vif_dataframe[vif_dataframe['VIF'] >= 10]) == 0:\n            break\n        \n        remove_column = vif_dataframe[vif_dataframe['VIF'] >= 10].sort_values(by='VIF', ascending=False)['columns'].reset_index(drop=True)[0]\n        print(f\"remove_column: {remove_column}\")\n        df = df.drop(remove_column, axis=1)\n    return df","7e63220f":"train_removeVIF_df = remove_multicollinearity(train_df_X)","ba5e77cc":"variables = list(train_df_X) ## feature list\n \ny = train['target'] ## label\nselected_variables = variables ## Initially, all variables are selected.\nsl_remove = 0.05\n \nsv_per_step = [] ## Variables selected for each step\nadjusted_r_squared = [] ## Modified r_squared for each step\nsteps = []\nstep = 0\nwhile len(selected_variables) > 0:\n    X = sm.add_constant(train[selected_variables])\n    p_vals = sm.OLS(y,X).fit().pvalues[1:] \n    max_pval = p_vals.max() ## Max p-value\n    if max_pval >= sl_remove: ## Exclude if max p-value is greater than or equal to the reference value\n        remove_variable = p_vals.idxmax()\n        selected_variables.remove(remove_variable)\n \n        step += 1\n        steps.append(step)\n        adj_r_squared = sm.OLS(y,sm.add_constant(train[selected_variables])).fit().rsquared_adj\n        adjusted_r_squared.append(adj_r_squared)\n        sv_per_step.append(selected_variables.copy())\n    else:\n        break","5915cfe9":"fig = plt.figure(figsize=(10,10))\nfig.set_facecolor('white')\n \nfont_size = 15\nplt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\nplt.plot(steps,adjusted_r_squared, marker='o')\n    \nplt.ylabel('Adjusted R Squared',fontsize=font_size)\nplt.grid(True)\nplt.show()","8c9f3e3a":"train_refinded_data = train_df.drop(['f0', 'f52', 'f72', 'f38'], axis = 1)\ntest_refinded_data = test_df.drop(['f0', 'f52', 'f72', 'f38'], axis = 1)","79119cff":"train_refinded_data.head()","9ca5981d":"X = train_refinded_data.drop(['target'], axis=1)\ny = train_refinded_data['target']\nX_test = test_refinded_data.copy()\n\ndel train_refinded_data\ngc.collect()\ndel test_refinded_data\ngc.collect()","9583f92f":"scaler = StandardScaler()\n\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","a7120fee":"%%time\nEPOCHS = 100\nKFold = StratifiedKFold(n_splits=5, random_state=786, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold.split(X, y)):\n    X_train, X_valid = X.iloc[train_idx].values, X.iloc[valid_idx].values\n    y_train, y_valid = y.iloc[train_idx].values, y.iloc[valid_idx].values\n    \n    X_train = torch.from_numpy(X_train.astype(np.float32))\n    X_valid = torch.from_numpy(X_valid.astype(np.float32))\n    y_train = torch.from_numpy(y_train.astype(np.float32).reshape(-1,1))\n    y_valid = torch.from_numpy(y_valid.astype(np.float32).reshape(-1,1))\n    \n    model = nn.Sequential(\n        nn.Linear(96,1),\n        nn.Sigmoid()\n    )\n    \n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n    \n    train_losses = np.zeros(EPOCHS)\n    valid_losses = np.zeros(EPOCHS)\n    \n    scores = np.zeros(EPOCHS)\n    \n    for ep in range(EPOCHS):\n        \n        optimizer.zero_grad()\n        \n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        \n        loss.backward()\n        optimizer.step()\n        \n        outputs_valid = model(X_valid)\n        loss_valid = criterion(outputs_valid, y_valid)\n        \n        scores += roc_auc_score(y_valid.detach().numpy(), outputs_valid.detach().numpy())\n        \n        train_losses[ep] = loss.item()\n        valid_losses[ep] = loss_valid.item()\n        \n    print(f\"Fold: {fold + 1} Loss: {np.mean(valid_losses)} AUC: {np.mean(scores)}\")\n    plt.plot(train_losses, label='train loss')\n    plt.plot(valid_losses, label='test loss')\n    plt.legend()\n    plt.show()","e762ff8b":"X_test = X_test.values\nX_test = torch.from_numpy(X_test.astype(np.float32))\npredictions = model(X_test)\npredictions = predictions.detach().numpy()","772c84e9":"ss['target'] = predictions\nss.to_csv('.\/submission.csv', index=False)\nss.head()","ffc793ba":"Rows 11 to 20","82d2b81f":"Multicollinearity, which is the most problematic when performing LR, is checked.\n\nWhen multicollinearity exists, the explanatory power of the model decreases, and the model breaks when other variables are added.\n\nAfter checking multicollinearity, features with VIF value of 10 or higher are removed.\n\n\nThere are several methods to remove multicollinearity. The main methods are PCA and VIF. I will use VIF\n\nNote that you have to remove them one by one using 'loop'. Remove one and check the VIF value again.\n\nps. It can be used for linear models such as SVM, but do not apply to ensemble models such as RF and Decision Tree.","3a1ce910":"Rows 21 to 30","fed6aaa5":"**Multicollinearity**","44706ee2":"Check for NULL values","19d6d3e4":"**There were no features with multicollinearity greater than 10.**","a3fc3b22":"# Logistic Regression Modeling","ed373bbc":"Data that does not affect the target\nremove = 'f0', 'f52', 'f72', 'f38'","9cb5c0ea":"**Data Scaling**","b386dbff":"# EDA","4377fda1":"Null is nothing","a149693c":"# Load Data","5dadebdd":"# EDA -> Features Engineering","3082c623":"* One of the stepwise regression analysis methods. \n\n* A method of simplifying the model by repeating the process of removing unnecessary independent variables one by one from the model including all variables\n\n* A method of removing explanatory variables with the smallest explanatory power (correlation) one by one from a full model including all explanatory variables","4cd98450":"# EDA -> Select feature","2fbefd3c":"Breaking 10 pieces to figure out the correlation","338a4f6a":"reference LR model : https:\/\/www.kaggle.com\/mohammadkashifunique\/tps-nov-logistic-regression-using-pytorch","451ca387":"Rows 1 to 10","a37123bc":"Visualize and confirm the correlation between features. Although seemingly trivial, statistical analysis is a very important task.\n\nIf you have time, draw a scatter plot between features as well.\n\nYou can get something out of a visualized graph.","411feb6d":"**backward elimination**","88482836":"**HEATMAP**","329c6294":"**The correlation between the explanatory variables is very low. I think it would be suitable for use in the model.**\ud83d\udc4d"}}