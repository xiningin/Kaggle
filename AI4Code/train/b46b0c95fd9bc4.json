{"cell_type":{"4883ebb0":"code","8d183192":"code","32307c82":"code","5efe5fa6":"code","f47030d5":"code","cf87db14":"code","19eae79a":"code","e857a6a3":"code","b06b6357":"code","092a41c1":"code","2718ab3d":"code","e82f4df5":"code","7fa44502":"code","3e57da33":"code","49615196":"code","b87f3651":"code","9ba5b31a":"code","5a8c4500":"code","c18276af":"code","850da67b":"code","e4d470c3":"code","2306c875":"code","00671474":"code","4cae9188":"code","c4c88de1":"code","b4e6b5e9":"code","a233f055":"code","cdc75fe1":"code","ac85de8d":"code","e925e306":"code","d045638f":"code","442412c0":"code","353b55ca":"code","e0758e5d":"code","8d9ba9db":"code","93552af5":"code","356a2018":"code","f80d73e4":"code","ed675720":"code","ad3f7246":"code","c48f8936":"code","fd1286d6":"code","339f3d52":"markdown","a4e87cb5":"markdown","cb48f91e":"markdown","e98e5082":"markdown","88deb0ff":"markdown","351724fb":"markdown","bc122f59":"markdown","9d075654":"markdown","8e59567d":"markdown","d1b6c5fe":"markdown","a309701a":"markdown","1a405c4f":"markdown","5318b73e":"markdown","02137ea5":"markdown","0051731c":"markdown","65df921c":"markdown","c2d4de7f":"markdown"},"source":{"4883ebb0":"import pandas as pd\nimport seaborn as sns\nimport re\nsns.set()\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport matplotlib.ticker as ticker\nplt.rc(\"font\", size=14)\nimport warnings\nwarnings.simplefilter(action='ignore')\nfrom sklearn.metrics import classification_report\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\nfrom sklearn.metrics import roc_curve\nfrom statsmodels.tools import add_constant\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nimport statsmodels.api as sn\nimport statsmodels.discrete.discrete_model as sm","8d183192":"train = pd.read_csv(r'..\/input\/jobathon-may-2021-credit-card-lead-prediction\/train.csv',header=0)\ntest = pd.read_csv(r'..\/input\/jobathon-may-2021-credit-card-lead-prediction\/test.csv',header=0)\nsubmit = pd.read_csv(r'..\/input\/jobathon-may-2021-credit-card-lead-prediction\/sample_submission.csv',header=0)","32307c82":"train.head()","5efe5fa6":"train.info()","f47030d5":"train.describe()","cf87db14":"# As only 'Credit_Product' has missing values, we shall visualize it first.\nsns.set_style('ticks')\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nsns.countplot(data=train, x='Credit_Product', hue='Is_Lead', ax=ax, palette='CMRmap')\nax.set_title('Credit_Product - Is_Lead Plot', size=25, loc='Left', y=1.04)\n\nsns.despine()\nplt.show()","19eae79a":"# Visualizing Categorical features\n# Note we have filled the missng 'Credit_Product' with 'Missing' for sake of visualization\ncat_features = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']\n\nplt.figure(figsize=(16, 14))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(3, 2, i+1)\n    sns.countplot(data=train.fillna('Missing'), x=feature, hue='Is_Lead', palette='summer')  \n    \nsns.despine()","e857a6a3":"# Region Code needs to be visualized more clearly\nseq = list(train['Region_Code'].unique())\nseq_list=[]\nfor i in range(len(seq)):\n    val = seq[i][2:]\n    seq_list.append(int(val))\nseq_list.sort()\nfor i in range(len(seq)):\n    seq_list[i] = 'RG'+str(seq_list[i])\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 15))\nsns.countplot(data=train, y='Region_Code', hue='Is_Lead', ax=ax, palette='CMRmap',orient='v',order=seq_list)\nax.set_title('Region_Code - Is_Lead Plot', size=25, loc='Left', y=1.04)\n\nsns.despine()\nplt.show()","b06b6357":"# It was found that the age could be dividen into age groups\nplt.figure(figsize=(16, 7))\ntemp = train.copy()\ntemp['Age'] = pd.cut(temp.Age, bins=[20, 35, 50, 65, 80, 95])\n\nsns.countplot(data=temp, x='Age', hue='Is_Lead', palette='autumn')\n\nplt.show()","092a41c1":"#We shall now plot the numberical variables to look at the distribution\nnumerical = ['Age','Vintage','Avg_Account_Balance']\nsns.pairplot(data=train,x_vars=numerical, hue = 'Is_Lead', palette='Set1')","2718ab3d":"# We shall log trasform the variables and plot again\ntemp = train.copy()\ntemp[numerical] = np.log(train[numerical])\nsns.pairplot(data=temp,x_vars=numerical, hue = 'Is_Lead', palette='Set1')","e82f4df5":"# processing train and test data together is more convinent.\ndf = pd.concat([train,test],axis=0)","7fa44502":"df.info()","3e57da33":"# There is only missing value in Credit_Product which is replaced by 'Missing'\ndf['Credit_Product'] = df['Credit_Product'].fillna('Missing')\ndf['Is_Lead'] = df['Is_Lead'].fillna(0)\n#We shall now take only log trasnform of 'Avg_Account_Balance' \ndf['Avg_Account_Balance'] = np.log(df['Avg_Account_Balance'])\n# The age variable is cut into categories of 20-35,35-50,50-65,65-80,80-95\ndf['Age'] = pd.cut(df.Age, bins=[20, 35, 50, 65, 80, 95]).astype('O')","49615196":"# The best processing of Region code was to divide in 4 categores: 25,26,27 and 28\ndf[\"Region_Code\"]= df[\"Region_Code\"].str.extract('(\\d+)').astype(int)\/\/10\ndf[\"Region_Code\"]= df[\"Region_Code\"].astype('O')","b87f3651":"#  One hot encoding (Creating Dummy features)\ndf =pd.get_dummies(df, columns=['Age','Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active'], drop_first=True)","9ba5b31a":"# ID is not important, thus we remove this feature\ndel df['ID']","5a8c4500":"new_train = df.iloc[:len(train)]\nnew_test = df.iloc[len(train):]","c18276af":"new_train.shape,new_test.shape","850da67b":"X = new_train.loc[:, new_train.columns != 'Is_Lead']\ny = new_train['Is_Lead']\n# Train test split in 75:25 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2519)","e4d470c3":"# We shall fit Logistic regression to look at the p-values of features before Standard scaling\nx_cons = sn.add_constant(X_train)\nlogit = sm.Logit(y_train,x_cons).fit()\nlogit.summary()","2306c875":"# Now we can proceed with standard scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nnew_test = new_test.loc[:, new_test.columns != 'Is_Lead']\nnew_test = scaler.transform(new_test)","00671474":"clf_lr = LogisticRegression()\nclf_lr.fit(X_train,y_train)\ny_prob = clf_lr.predict_proba(X_test)\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob[:, \n1]) \n   #retrieve probability of being 1(in second column of probs_y)\n#pr_auc = roc_auc_score(recall, precision)\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])","4cae9188":"from numpy import argmax","c4c88de1":"# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, y_prob[:,1])\nprint('Roc_Auc=',auc_score1)\n\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1])\n# get the best threshold\nJ = tpr - fpr\nix = argmax(J)\nbest_thresh = thresholds[ix]\n\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=400)\nplt.show();\n\nprint('The Best Threshold for Logistic model is :%f ' % (best_thresh))","b4e6b5e9":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","a233f055":"rf_clf = RandomForestClassifier(max_depth= 9, max_features= 8,n_estimators= 1000,random_state=2519)","cdc75fe1":"cvrf_clf = rf_clf.fit(X_train, y_train)\ny_rf = cvrf_clf.predict_proba(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, y_rf[:, \n1]) \n   #retrieve probability of being 1(in second column of probs_y)\n#pr_auc = roc_auc_score(recall, precision)\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])","ac85de8d":"# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_rf[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, y_rf[:,1])\nprint('Roc_Auc=',auc_score1)\n\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(y_test, y_rf[:,1])\n# get the best threshold\nJ = tpr - fpr\nix = argmax(J)\nbest_thresh = thresholds[ix]\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Random Forest')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=400)\nplt.show();\n\nprint('The Best Threshold for Random Forest is :%f ' % (best_thresh))","e925e306":"import xgboost as xgb","d045638f":"params = {\n    'max_depth': 8,\n    'subsample': 0.8,\n    'colsample_bytree': 0.9, \n    'objective':'binary:logistic',\n    'eval_metric':'auc',\n    'learning_rate': 0.001,\n    'random_state' : 2519\n}\n\ndef XGBmodel(X_train,X_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(X_train,label=y_train)\n    matrix_test = xgb.DMatrix(X_test,label=y_test)\n    model=xgb.train(params=params,\n                    dtrain=matrix_train,num_boost_round=5000, \n                    early_stopping_rounds=13,evals=[(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(X_train,X_test,y_train,y_test,params)","442412c0":"y_xg = (model.predict(xgb.DMatrix(X_test), ntree_limit = model.best_ntree_limit))\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_xg) \n   #retrieve probability of being 1(in second column of probs_y)\n#pr_auc = roc_auc_score(recall, precision)\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])","353b55ca":"# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_xg, pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, y_xg)\nprint('Roc_Auc=',auc_score1)\n\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(y_test, y_xg)\n# get the best threshold\nJ = tpr - fpr\nix = argmax(J)\nbest_thresh = thresholds[ix]\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='XGBoost')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=400)\nplt.show();\n\nprint('The Best Threshold for XGBoost is :%f ' % (best_thresh))","e0758e5d":"#Confusion Matrix\n#Threshold of XGB model is 0.489467\nxg_pr = (model.predict(xgb.DMatrix(X_test), ntree_limit = model.best_ntree_limit) >= 0.489467)\ncm=confusion_matrix(y_test,xg_pr)\nconf_matrix=pd.DataFrame(data=cm,index=['Actual:0','Actual:1'],columns=['Predicted:0','Predicted:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","8d9ba9db":"# We can observe a decent classification by our XGB model\n# Recall is more important in these problems as the company can afford False Positives but cannot bear skipping leads\nprint('Recall =',11373\/(11373+3279)) ","93552af5":"# Submiting Random Forest","356a2018":"# Threshold of Random Forest is 0.240617\nprediction1 = np.where((cvrf_clf.predict_proba(new_test)[:,1] >= 0.240617),1,0)\nprediction1.shape","f80d73e4":"submit['Is_Lead']=prediction1\nsubmit.head()","ed675720":"submit.to_csv('submit_trail.csv', index=False)","ad3f7246":"# Threshold of XGB is 0.489467\nprediction2 = np.where((model.predict(xgb.DMatrix(new_test), ntree_limit = model.best_ntree_limit) >= 0.489467),1,0)\nprediction2.shape","c48f8936":"submit['Is_Lead']=prediction2\nsubmit.head()","fd1286d6":"submit.to_csv('submit_final.csv', index=False)","339f3d52":"# Model Fitting","a4e87cb5":"# <font color=\"Green\">Credit Card Leads Prediction \ud83d\udcb3\ud83d\udcb3\ud83d\udcb3\ud83d\udcb3 <\/font> \n### A JOB-A-THON conducted by Analytics Vidhya\n##### The problem statement is to predict the credit card leads using the variables such as Age, Occupation, Avg_Account Balance,etc..\n##### It is a binary class classification problem\n##### We are going to look at <font color =\"Blue\">EDA<\/font>,<font color =\"Blue\"> Data processing<\/font> and<font color =\"Blue\"> Data Modeling<\/font> and selecting <font color =\"Red\"> <b>Optimal Threshold<\/b><\/font> for optimal split.\n![](https:\/\/news.mit.edu\/sites\/default\/files\/styles\/news_article__image_gallery\/public\/images\/201809\/MIT-Fraud-Detection-PRESS_0.jpg?itok=n9A9HHwh)","cb48f91e":"* We shall thus proceed converting the age to categorical variable later","e98e5082":"* it is very strange to observe that the missing values do actually have more leads. \n* Thus we shall impute the missing values as 'Missing' instead of 'Yes' or 'No'","88deb0ff":"### Finding threshold using roc curve","351724fb":"### XGBoost","bc122f59":"# Visualizations","9d075654":"* It can be observed that only 'Credit_Product' has missing values.","8e59567d":"### Logistic Regression","d1b6c5fe":"#### We can observe that the highest roc is obtained for XGBoost, but Random Forest almost also have same roc.\n#### Anyways, we shall use the XGB prediction as it has comparitivly higher roc score","a309701a":"### Random Forest","1a405c4f":"# Reading data and Descriptive statistics","5318b73e":"# Submission","02137ea5":"# Conclusion\n#### We have obtained a good roc_auc_score score for test data.\n#### The thresholds for each of the model has helped in decent split and we have successfully achieved the objective\n## Future Improvements:\n#### The models can be tuned for hyperparameter optimization, but because the training data is large, it takes time for parametrs to get tuned.\n<img src='https:\/\/cdn0.iconfinder.com\/data\/icons\/security-system-3\/50\/6-512.png'>\n\n## Thanks for being a good reader!!","0051731c":"# Data Preprocessing","65df921c":"* We shall proceed with taking log trasform of 'Avg_Account_Balance' to obtaion normal distribution","c2d4de7f":"* Not much of information is gained from this variable."}}