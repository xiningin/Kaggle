{"cell_type":{"c23aa3a0":"code","738a71e6":"code","260ebb68":"code","569bbf1c":"code","e01eb159":"code","39454a1e":"code","569920bf":"code","4c736569":"code","65e44653":"code","7398c42a":"code","e7bed29f":"code","cf4a81a4":"code","9d69ef7c":"code","17574a65":"code","2015272a":"code","76cc561f":"code","2ea1fe0b":"code","4ad6fa01":"code","2a1abaec":"code","a91671ce":"code","f48aec3b":"code","5693ec2c":"code","d1f6b553":"code","71e05165":"code","d256fae5":"code","813cfa48":"code","baa79bb8":"markdown","5bbff332":"markdown","02af2f7b":"markdown","3b4be622":"markdown","bf41a66e":"markdown","ac637da8":"markdown","d560e80b":"markdown","661b5368":"markdown","c381978a":"markdown","f16c0f67":"markdown","fd33dbef":"markdown","44ba6a79":"markdown","3bdaee28":"markdown","9cf1ad81":"markdown","bb4b1d2e":"markdown","db4ca433":"markdown","b2dbc543":"markdown","03daea1c":"markdown","5023a0bc":"markdown","6f028121":"markdown","c5c464e6":"markdown","544427b2":"markdown","6e574eab":"markdown","5b67dcc0":"markdown","778343f2":"markdown","b05a3126":"markdown","1fc6a3ad":"markdown"},"source":{"c23aa3a0":"import pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Avoid warning messages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#plotly libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nfrom sklearn.feature_extraction.text import CountVectorizer","738a71e6":"train = pd.read_csv(\"\/kaggle\/input\/shopee-sentiment-analysis\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/shopee-sentiment-analysis\/test.csv\")\ntraintest = train.append(test).reset_index(drop = True)","260ebb68":"all_text = '||'.join(traintest['review'])\n# print(\"TOTAL CHARS include out separator '||' : \", len(all_text))","569bbf1c":"#create list of unique chars\nchars = list(set(all_text))\nprint(\"TOTAL CHARS : \", len(chars))","e01eb159":"#sneak peak into chars\nchars[:10] ","39454a1e":"chars_count = {}\n\ndef append_chars_count(text):\n    for char in list(set(text)): # I count repeated character in one review as one\n        chars_count[char] = chars_count.get(char, 0) + 1\n        \ntraintest['review'].map(append_chars_count);","569920bf":"# #ensure the length is same\n# len(chars_count), len(chars)","4c736569":"# make it into datarame so we can sort it easily\ndf_chars = pd.DataFrame()\ndf_chars['char'] = list(chars_count.keys())\ndf_chars['count'] = list(chars_count.values())\ndf_chars = df_chars.sort_values(\"count\", ascending = False).reset_index(drop = True)","65e44653":"\ndf_chars['count'].value_counts().head(10)","7398c42a":"df_chars.loc[(df_chars['count'] >= 2) & (df_chars['count'] < 4)].sample(10, random_state = 100)","e7bed29f":"df_chars.loc[(df_chars['count'] >= 5) & (df_chars['count'] < 10)].sample(10, random_state = 100)","cf4a81a4":"df_chars.loc[(df_chars['count'] >= 10) & (df_chars['count'] < 20)].sample(20, random_state = 100)","9d69ef7c":"df_chars = df_chars.loc[df_chars['count'] >= 10]","17574a65":"# exclude chracters\nexclude_chars = ['\\n', ' ', '\\\\', '\u00b0', '.', \"'\", '\"', '-', '!', '-', ')', '(', ':', ';',\n                ',', '\\u200b', '\ufe0f', '\/', '\\xa0', \"?\", \"&\", '\u2019', '~', '+', '*', '%', '_', \n                 '#', '=', '@', ']', '['] # from manual observation \n\nmask = df_chars['char'].map(lambda x: not (x.isalnum() or str(x) in exclude_chars))\ndf_chars_special = df_chars.loc[mask]","2015272a":"def counting_char_with_sentiment(char, sentiment = 1):\n    return sum([char in review_ for review_ in list(train.loc[train['rating'] == sentiment]['review'])])\n\nfor i in range(1,6):\n    df_chars_special[f'rating_{i}'] = np.vectorize(counting_char_with_sentiment)(df_chars_special['char'], i)","76cc561f":"df_chars_special.sample(20, random_state = 100).style.background_gradient( cmap='Greens')","2ea1fe0b":"rating_cols = [f'rating_{i}' for i in range(1,6)]\nweighted_score = list(np.arange(-1,1.5, 0.5))\n\ndf_chars_special['score'] = np.array([list(df_chars_special[a] * b) for a,b in zip(rating_cols, weighted_score)]).sum(axis = 0) \/  df_chars_special[rating_cols].sum(axis = 1)","4ad6fa01":"df_chars_special.sort_values('score', ascending= False)[['char', 'score'] + rating_cols].head(10).style.background_gradient( cmap='Greens')","2a1abaec":"df_chars_special.sort_values('score', ascending= True)[['char', 'score'] + rating_cols].head(10).style.background_gradient(cmap='Reds_r')","a91671ce":"def ngrams_top(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range, min_df= 1).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\n\ndef merge_sentiment_ngrams(df, ngram_range,n=None):\n    df_full = ngrams_top(df['review'],ngram_range,n)\n    \n    for i in range(1,6):\n        df_temp = ngrams_top(df.loc[df['rating'] == i,'review'],ngram_range,n)\n        df_temp.columns = ['text', f'rating_{i}']\n        \n        df_full = df_full.merge(df_temp, on = 'text', how = 'outer')\n        df_full = df_full.fillna(0)\n        df_full[f'rating_{i}'] = df_full[f'rating_{i}'].astype(int)\n        \n    \n    df_full = df_full.loc[df_full['count'] >= 3 ]\n    \n    rating_cols = [f'rating_{i}' for i in range(1,6)]\n    weighted_score = list(np.arange(-1,1.5, 0.5))\n\n    df_full['score'] = np.array([list(df_full[a] * b) for a,b in zip(rating_cols, weighted_score)]).sum(axis = 0) \/  df_full[rating_cols].sum(axis = 1)\n    return df_full","f48aec3b":"df_1grams = merge_sentiment_ngrams(train, ngram_range = (1,1))\ndf_2grams = merge_sentiment_ngrams(train, ngram_range = (1,2))\n\ndf_1grams = df_1grams.loc[df_1grams['count'] >= 100 ].sort_values('score', ascending= False)\ndf_2grams = df_2grams.loc[df_2grams['count'] >= 100 ].sort_values('score', ascending= False)\n","5693ec2c":"df_1grams.sort_values('score', ascending= False)[['text', 'score'] + rating_cols].head(10).style.background_gradient( cmap='Greens')","d1f6b553":"df_1grams.sort_values('score', ascending= True)[['text', 'score'] + rating_cols].head(10).style.background_gradient(cmap='Reds_r')","71e05165":"df_2grams.sort_values('score', ascending= False)[['text', 'score'] + rating_cols].head(10).style.background_gradient( cmap='Greens')","d256fae5":"df_2grams.sort_values('score', ascending= True)[['text', 'score'] + rating_cols].head(10).style.background_gradient(cmap='Reds_r')","813cfa48":"df_2grams.loc[((df_2grams['score'] < 0.1) & (df_2grams['score'] > 0))].sort_values('score', ascending= True)[['text', 'score'] + rating_cols].head(10)","baa79bb8":"<font color='red'>Bad rating characters words <\/font>","5bbff332":"2 grams <font color='red'>bad<\/font> rating words","02af2f7b":"The goals is to check whether there are emoji \/ special characters and how useful they are","3b4be622":"1 gram <font color='red'>bad<\/font> rating words","bf41a66e":"See composition of character by their occurance (from column : 'count')","ac637da8":"I would like to focus on non aplha num chracter","d560e80b":"Observation:\n\n* <font color='green'>Positive<\/font> ratings produces adjective words like awesome, *mantap* (awesome in bahasa), and talking about seller good performance\n* <font color='red'>Negative<\/font> ratings produces words like poor, terible, refund. it is divided by 2 bad sides, product or seller quality\n","661b5368":"1 gram <font color='green'>good<\/font> rating words","c381978a":"5 until 10 still do not tell anything \ud83e\udd26","f16c0f67":"2 grams <font color='green'>good<\/font> rating words","fd33dbef":"## 1.2. Chracters with sentiments\ud83d\ude32","44ba6a79":"Do as far as sample view (just change the `random_state` if you want to see other samples) the character is not really depict sentiment\n\ntry larger !!!","3bdaee28":"Lets See if a character belong to unique sentiment or not","9cf1ad81":"# 2. Words Insights? \ud83e\udd10\t","bb4b1d2e":"There are 1468, there should be a lot of emojis","db4ca433":"SEE this worring face \ud83d\ude27 in count 10!!! and this adorable \ud83d\ude38 and heartbreak \ud83d\udc94 !!! should be something. \n\nWe curate our chracter exploration until 10 count ","b2dbc543":"Lets get the character a score with this formula: \n> score = ((#rating1 * -1) + (#rating2 * -0.5) + (#rating3 * 0) + (#rating4 * 0.5) + (#rating5 *1)) \/ (occurance in train)\n\nSo we get -1 if the sentiment is full of bad rating, and 1 when its full of good rating","03daea1c":"# Import Packages and Load Data","5023a0bc":"See... told yaa... EMOJI !!!","6f028121":"2 gram neutral words","c5c464e6":"We can ignore char that happen once, and still considering char which happen 2x or 3x","544427b2":"## 1.1. See Character Composition \ud83e\udd26","6e574eab":"\n\n> I count repeated character in one review as one to avoid emoji spamming in one review","5b67dcc0":"### **LETS VISIT CHARS WHO PICK SIDE!!!** \ud83d\ude20","778343f2":"<font color='green'>Good rating characters words <\/font>","b05a3126":"# 1. Character exploration \ud83d\ude21\ud83d\ude20","1fc6a3ad":"1 table tells more than thousand words"}}