{"cell_type":{"1d736333":"code","130cfced":"code","b4aac074":"code","3b8ea416":"code","06f3adfa":"code","23997cf1":"code","b54028d1":"code","f2c0c396":"code","63b1e270":"code","c64a6ae9":"code","5bb514c3":"code","9e819245":"code","a593aca4":"code","fd530e90":"code","b21d11c0":"code","2c750b42":"code","4b264707":"code","68370390":"code","687d9d69":"code","46e38e9b":"code","d8f8a4c2":"code","2b2e9e9f":"code","f2947b5b":"code","ad1e7f62":"code","376ff27d":"code","4ba7c260":"code","034bb13e":"code","bb5fd64e":"code","f060b327":"code","a2d2737d":"code","2a8dafa9":"code","80033cbf":"code","a229460c":"code","3bfc092d":"code","33a71d89":"code","64bbe1f5":"code","2c8b2355":"code","64ab87e9":"code","9f4e603a":"code","f07196c2":"code","9801ec55":"code","5b35f5fd":"code","9287af81":"code","dd2b803c":"code","261ed4aa":"code","017849ad":"code","0a848677":"code","575ff2d9":"code","8c42feca":"code","b690018c":"code","f2af84f1":"code","09fe7ac7":"code","832dac8b":"code","7242addb":"code","8789967b":"code","c98b90d2":"code","430fda79":"code","523b7f73":"code","0634d727":"code","42f097e8":"code","2ec01898":"code","b024646d":"code","4a21b181":"code","1a294645":"code","152ba90d":"code","ad33eea0":"code","09816d43":"code","d193580c":"code","79000095":"code","fd2f94a5":"code","6525365d":"code","5e2acfa2":"code","401bc60e":"code","13aa644a":"code","018926f7":"code","ba4d659c":"code","81afe274":"code","7461c3d5":"code","ec8ad591":"code","dd5fb9cd":"code","1c5ea006":"code","284a98af":"code","de04c82d":"code","7ea750c6":"code","26c9d03f":"code","07371ab4":"code","021fea88":"code","b38a8057":"code","e7430e58":"markdown","ecb2082a":"markdown","a7ca5744":"markdown","84438dc0":"markdown","c8e548ff":"markdown","709ca542":"markdown","2b50eb03":"markdown","86f506db":"markdown","34401769":"markdown","cbd5887a":"markdown","3e2b322d":"markdown","f6078417":"markdown","306c87b1":"markdown","4e91c24d":"markdown","142f2377":"markdown","8338e3a9":"markdown","7b069fca":"markdown","918ca227":"markdown","a20ce66f":"markdown","539367e4":"markdown","e04e8f1c":"markdown","28bdc4a4":"markdown","c0b03af9":"markdown","be3fbc29":"markdown","176d748e":"markdown","f96b8fe1":"markdown","794434c3":"markdown","49ab392e":"markdown","ca10580e":"markdown","d7fd6caf":"markdown","75cbba4d":"markdown","c9ce178f":"markdown"},"source":{"1d736333":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","130cfced":"import pandas as pd\nimport numpy as np\nfrom numpy import percentile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nimport re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200) \nimport string\nimport nltk # for text manipulation\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom tqdm import tqdm\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import stats \nfrom sklearn import metrics \nfrom sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","b4aac074":"df = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv')","3b8ea416":"df.head()","06f3adfa":"df.info()","23997cf1":"df.shape","b54028d1":"df.columns","f2c0c396":"df.TweetAt.value_counts()","63b1e270":"df['Location'].value_counts()","c64a6ae9":"df['Sentiment'].value_counts()","5bb514c3":"missing_values = pd.DataFrame()\nmissing_values['column'] = df.columns\n\nmissing_values['percent'] = [round(100* df[col].isnull().sum() \/ len(df), 2) for col in df.columns]\nmissing_values = missing_values.sort_values('percent')\nmissing_values = missing_values[missing_values['percent']>0]","9e819245":"plt.figure(figsize=(15, 5))\nsns.set(style='whitegrid', color_codes=True)\nsplot=sns.barplot(x='column', y='percent', data=missing_values)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.ylabel(\"Percentage\", size=14, weight=\"bold\")\nplt.title(\"Percentage of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","a593aca4":"plt.figure(figsize=(17, 5))\nsns.heatmap(df.isnull(), cbar=True, yticklabels=False)\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","fd530e90":"unique_df = pd.DataFrame()\nunique_df['Features'] = df.columns\nunique=[]\nfor i in df.columns:\n    unique.append(df[i].nunique())\nunique_df['Uniques'] = unique\n\nf, ax = plt.subplots(1,1, figsize=(15,7))\n\nsplot = sns.barplot(x=unique_df['Features'], y=unique_df['Uniques'], alpha=0.8)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.title('Bar plot for number of unique values in each column',weight='bold', size=15)\nplt.ylabel('#Unique values', size=12, weight='bold')\nplt.xlabel('Features', size=12, weight='bold')\nplt.xticks(rotation=90)\nplt.show()","b21d11c0":"loc_analysis = pd.DataFrame(df['Location'].value_counts().sort_values(ascending=False))\nloc_analysis = loc_analysis.rename(columns={'Location':'count'})","2c750b42":"import plotly.graph_objects as go","4b264707":"data = {\n   \"values\": loc_analysis['count'][:15],\n   \"labels\": loc_analysis.index[:15],\n   \"domain\": {\"column\": 0},\n   \"name\": \"Location Name\",\n   \"hoverinfo\":\"label+percent+name\",\n   \"hole\": .4,\n   \"type\": \"pie\"\n}\nlayout = go.Layout(title=\"<b>Ratio on Location<\/b>\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\n\ndata = [data]\nfig = go.Figure(data = data, layout = layout)\nfig.update_layout(title_x=0.5)\nfig.show()","68370390":"g=sns.catplot(\"TweetAt\", data=df, kind=\"count\", height=8)\ng.set_xticklabels(rotation=50)","687d9d69":"Top_Location_Of_tweet= df['Location'].value_counts().head(10)\n\nsns.set(rc={'figure.figsize':(12,8)})\nsns.set_style('white')","46e38e9b":"Top_Location_Of_tweet.head(10)","d8f8a4c2":"\nTop_Location_Of_tweet_df=pd.DataFrame(Top_Location_Of_tweet)\nTop_Location_Of_tweet_df.reset_index(inplace=True)\nTop_Location_Of_tweet_df.rename(columns={'index':'Location', 'Location':'Location_Count'}, inplace=True)\nTop_Location_Of_tweet_df","2b2e9e9f":"viz_1=sns.barplot(x=\"Location\", y=\"Location_Count\", data=Top_Location_Of_tweet_df,\n                 palette='Blues_d')\nviz_1.set_title('Locations with most of the tweets')\nviz_1.set_ylabel('Count of listings')\nviz_1.set_xlabel('Location Names')\nviz_1.set_xticklabels(viz_1.get_xticklabels(), rotation=45)","f2947b5b":"sns.set(font_scale=1.1)\nsns.catplot(\"Sentiment\", data=df, kind=\"count\", height=8)","ad1e7f62":"# write function for removing @user\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i,'',input_txt)\n    return input_txt","376ff27d":"# create new column with removed @user\ndf['Tweet'] = np.vectorize(remove_pattern)(df['OriginalTweet'], '@[\\w]*')","4ba7c260":"df.head(4)","034bb13e":"import re\ndf['Tweet'] = df['Tweet'].apply(lambda x: re.split('https:\\\/\\\/.*', str(x))[0])","bb5fd64e":"df.head(3)","f060b327":"# remove special characters, numbers, punctuations\ndf['Tweet'] = df['Tweet'].str.replace('[^a-zA-Z#]+',' ')","a2d2737d":"df.head(5)","2a8dafa9":"# remove short words\ndf['Tweet'] = df['Tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))","80033cbf":"df.head(4)","a229460c":"# create new variable tokenized tweet \ntokenized_tweet = df['Tweet'].apply(lambda x: x.split())","3bfc092d":"df.head(4)","33a71d89":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\n\n# apply stemmer for tokenized_tweet\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])","64bbe1f5":"df.head(4)","2c8b2355":"# join tokens into one sentence\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n# change df['Tweet'] to tokenized_tweet","64ab87e9":"df['Tweet']  = tokenized_tweet\ndf.head(4)","9f4e603a":"# create text from all tweets\nall_words = ' '.join([text for text in df['Tweet']])\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","f07196c2":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Extremely Positive']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","9801ec55":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Positive']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","5b35f5fd":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Extremely Negative']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","9287af81":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Negative']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","dd2b803c":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Neutral']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","261ed4aa":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    for i in x:\n        ht = re.findall(r'#(\\w+)', i)\n        hashtags.append(ht)\n    return hashtags","017849ad":"# extracting hashtags from non racist\/sexist tweets\nHT_Extremely_Positive = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Extremely Positive'])\nHT_Positive = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Positive'])\nHT_Neutral = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Neutral'])\nHT_Negative = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Negative'])\nHT_Extremely_Negative = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Extremely Negative'])\n\n\n\n# unnesting list\nHT_Extremely_Positive = sum(HT_Extremely_Positive, [])\nHT_Positive = sum(HT_Positive, [])\nHT_Neutral = sum(HT_Neutral, [])\nHT_Negative = sum(HT_Negative,[])\nHT_Extremely_Negative = sum(HT_Extremely_Negative,[])","0a848677":"# making frequency distribution top 10 Extremely Positive hashtags\na = nltk.FreqDist(HT_Extremely_Positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","575ff2d9":"# making frequency distribution top 10 Positive hashtags\na = nltk.FreqDist(HT_Positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","8c42feca":"# making frequency distribution top 10 Neutral hashtags\na = nltk.FreqDist(HT_Neutral)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","b690018c":"# making frequency distribution top 10 Extremely Negative hashtags\na = nltk.FreqDist(HT_Extremely_Negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","f2af84f1":"# making frequency distribution top 10 Negative hashtags\na = nltk.FreqDist(HT_Negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","09fe7ac7":"new_df = df[['Tweet','Sentiment']]\nnew_df.head()","832dac8b":"df['Sentiment'].value_counts()","7242addb":"new_df[\"Tweet\"] = new_df[\"Tweet\"].str.lower()#.str.split()","8789967b":"new_df.head()","c98b90d2":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","430fda79":"new_df['Tweet'].apply(lambda x: [item for item in x if item not in stop])","523b7f73":"new_df.isnull().sum()","0634d727":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(new_df,test_size = 0.2,random_state=0,stratify = new_df.Sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","42f097e8":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.Tweet.values)\nX_valid = vectorizer.transform(valid.Tweet.values)\n\ny_train = train.Sentiment.values\ny_valid = valid.Sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","2ec01898":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","b024646d":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","4a21b181":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","1a294645":"#takes huge amount of time to execute\nimport xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","152ba90d":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","ad33eea0":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","09816d43":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost',],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","d193580c":"CATBOOST = new_df[['Tweet','Sentiment']]","79000095":"CATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Positive',1) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Extremely Positive',1) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Neutral',1) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Negative',0) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Extremely Negative',0)","fd2f94a5":"CATBOOST.head(5)","6525365d":"CATBOOST['Sentiment'].value_counts()","5e2acfa2":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(CATBOOST,test_size = 0.2,random_state=0,stratify = CATBOOST.Sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","401bc60e":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.Tweet.values)\nX_valid = vectorizer.transform(valid.Tweet.values)\n\ny_train = train.Sentiment.values\ny_valid = valid.Sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","13aa644a":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","018926f7":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","ba4d659c":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","81afe274":"import xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","7461c3d5":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","ec8ad591":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","dd5fb9cd":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost'],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","1c5ea006":"# Get the predicted classes\ntrain_class_preds = sgd_clf.predict(X_train)\ntest_class_preds = sgd_clf.predict(X_valid)","284a98af":"# Get the confusion matrix for both train and test. We are getting very low type and type 2 errors.\n\nlabels = ['Negative', 'Positive']\ncm = confusion_matrix(y_train, train_class_preds)\nprint(cm)\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)","de04c82d":"# Let's check the overall accuracy. Overall accuracy is very good.\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\ny_pred = sgd_clf.predict(X_valid)\n\nscore =accuracy_score(y_valid,y_pred)\nprint('accuracy is', score)","7ea750c6":"# F1 score for our classifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n\ny_pred =  sgd_clf.predict(X_valid)\nprint(f1_score(y_valid,y_pred, average=\"macro\"))","26c9d03f":"#score is mean accuracy\nscikit_score = sgd_clf.score(X_valid,y_valid)\nprint('scikit score:', scikit_score)","07371ab4":"# Recall score for our winner model\nrecall_score(y_valid, y_pred, average='macro')","021fea88":"# Classification Report for our stochastic gradient descent algorithm\nclassification_report(y_valid,y_pred)","b38a8057":"# Very low type 1 and type 2 error\nconfusion_matrix(y_valid,y_pred)","e7430e58":"### C) Removing Punctuations, Numbers, and Special Characters","ecb2082a":"# EVALUATION OF ALL BINARY CLASSIFICATION MODELS","a7ca5744":"# Use Of Counter Vectorizer For Multi Class Classification","84438dc0":"# LOGISTIC REGRESSION FOR BINARY CLASSIFICATION","c8e548ff":"# All the multiclass models test accuracy in descending order","709ca542":"# Logistic Regression","2b50eb03":"# Data Preprocessing\n### A) Removing @user","86f506db":"### D) Removing Short Words","34401769":"# Extreme Gradient Boosting","cbd5887a":"# Winner Model: Stochastic Gradient Descent-SGD Classifier","3e2b322d":"# USING COUNT VECTORIZER","f6078417":"# Spitting Our Dataset into Training And Testing Dataset ( For Multiclass Classification)","306c87b1":"# NAIVE BAYES CLASSIFIER FOR BINARY CLASSIFICATION.","4e91c24d":"### E) Tokenization\n","142f2377":"# RANDOM FOREST CLASSIFIER FOR BINARY CLASSIFICATION","8338e3a9":"# Understanding the common words used in the tweets: WordCloud","7b069fca":"# XG BOOST( BINARY CLASSIFICATION)","918ca227":"# SUPPORT VECTOR MACHINE(BINARY CLASSIFICATION)","a20ce66f":"# Naive Bayes Classifier for MULTICLASS Classification","539367e4":"### F) Stemming","e04e8f1c":"# DIVIDING OUR DATASET INTO TRAINING AND TESTING","28bdc4a4":"### Removing StopWords","c0b03af9":"# CONVRTING OUR MULTICLASS CLASSIFICATION INTO BINARY CLASSIFICATION","be3fbc29":"# Support vector machine","176d748e":"### B) REMOVED HTTP AND URLS FROM TWEET","f96b8fe1":"# Stochastic Gradient Descent-SGD Classifier( BINARY CLASSIFICATION)","794434c3":"#  Extracting Features from Cleaned Tweets","49ab392e":"# RANDOM FOREST CLASSIFIER ","ca10580e":"# Understanding the impact of Hashtags on tweets sentiment","d7fd6caf":"### G) tokenized_tweet","75cbba4d":"# Stochastic Gradient Descent-SGD Classifier( MULTICLASS CLASSIFICATION)","c9ce178f":"### Converting into lower case"}}