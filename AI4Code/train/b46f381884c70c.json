{"cell_type":{"d53c4dc4":"code","7cd87a48":"code","3aa5ef8f":"code","18db7850":"code","b5773d90":"code","873e6503":"code","c0b55829":"code","28886c84":"code","ab072bd6":"code","ec1fb735":"code","a29220e6":"code","e924741a":"code","02ca992d":"code","bd8212dd":"code","e9914d96":"code","1231e419":"code","9e78945c":"code","9f8df44e":"code","13699dc8":"code","b76d2e7e":"code","053225d9":"code","3814048b":"code","a08474b9":"code","606afaa6":"code","2f0429c6":"code","a670826a":"code","1013718a":"code","cdbb78e4":"code","a4d35d82":"code","07633ada":"code","456ac006":"code","d56097dd":"code","ad3799b7":"code","a1ffa920":"code","77629620":"code","3183e2be":"code","21ba0733":"code","4ca3e730":"code","f399a8d6":"code","6de7f1c6":"code","303ba361":"code","92b6de08":"code","b891f7f4":"code","e415e581":"code","7253457c":"code","42615eb5":"code","8898a7a8":"code","0bb11bbd":"code","c3e28500":"code","54f40332":"code","3dbefbc7":"code","5982fad6":"code","40100164":"code","413deac1":"code","d863cb2c":"code","dc37b4e5":"code","f02fd2cb":"code","915358f5":"markdown","8c35d51f":"markdown","c46617ab":"markdown","6f255b9a":"markdown","7b2a81b8":"markdown","ec7ec5cb":"markdown","eeec76c1":"markdown","dbfafb2e":"markdown","cb295ad3":"markdown","c9591519":"markdown","ba6c5e3a":"markdown","a7836f3f":"markdown","7216d2c3":"markdown","01985093":"markdown","bbacba4c":"markdown","f572903d":"markdown","a5366fc6":"markdown","54b5a391":"markdown","8a7f7858":"markdown","9ad02031":"markdown","95aa359f":"markdown"},"source":{"d53c4dc4":"import IPython.display as ipd  # To play sound in the notebook\nfrom scipy.io import wavfile # for reading wave files as numpy arrays\nimport wave # opening .wav files\nimport struct # for padding\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualizations\n%matplotlib inline\nimport os # operating system\nfrom os.path import join\nimport time\nprint(os.listdir(\"..\/input\"))","7cd87a48":"RATE = 16000 # KHz\nDATA_DIR = \"..\/input\/data\"\nTRAIN_CSV_FILE = \"..\/input\/train_data.csv\"\nTEST_CSV_FILE = \"..\/input\/test_data.csv\"\nFIRST_TRAIN_SAMPLE_DIR = join(DATA_DIR, \"TRAIN\", \"DR1\", \"FCJF0\")","3aa5ef8f":"# Given the test_data.csv filename or train_data.csv filename or both. \n# Output a Pandas DataFrame of the files that have `is_converted_audio == True`\ndef get_good_audio_files(filename):\n    df = pd.read_csv(filename)\n    return df[df['is_converted_audio'] == True]","18db7850":"# Given the test_data.csv filename or train_data.csv filename or both. \n# Output a Pandas DataFrame of the files that have `is_word_file == True`\ndef get_word_files(filename):\n    df = pd.read_csv(filename)\n    return df[df['is_word_file'] == True]","b5773d90":"# Given a file path to the .WAV.wav file\n# Output the result of SciPy\u2019s wavefile.read()\ndef read_audio(wave_path, verbose=False):\n    rate, data = wavfile.read(wave_path)\n    # make sure the rate of the file is the RATE that we want\n    assert rate == RATE\n    print(\"Sampling (frame) rate = \", rate) if verbose else None\n    print(\"Total samples (frames) = \", data.shape) if verbose else None\n    return data","873e6503":"# Given a row (...,'TRAIN','DR4','MMDM0', 'SI681.WAV.wav',...)\n# return the os.path.join of the relevant dirs\n# '..\/input\/data\/TRAIN\/DR4\/MMDM0\/SI681.WAV.wav'\ndef join_dirs(row):\n    return os.path.join(DATA_DIR,\n                       row['test_or_train'],\n                       row['dialect_region'],\n                       row['speaker_id'],\n                       row['filename'])","c0b55829":"# Given a file path to the .WRD file\n# Output a list of tuples containing (start, end, word, speaker_id, sentence_id)\ndef parse_wrd_timestamps(wrd_path, verbose=False):\n    print('wrd_path', wrd_path) if verbose else None\n    speaker_id = wrd_path.split('\/')[-2]\n    sentence_id = wrd_path.split('\/')[-1].replace('.WRD', '')\n    wrd_file = open(wrd_path)\n    content = wrd_file.read()\n    content = content.split('\\n')\n    # print('content b4 tuple', content) if verbose else None\n    content = [tuple(foo.split(' ') + [speaker_id, sentence_id]) for foo in content if foo != '']\n    wrd_file.close()\n    return content","28886c84":"# Given both a time_aligned_words file && the output of read_audio() \n# Output the another list of tuples containing (audio_data, label)\n# e.g.\n# [(array([ 2, 2, -3, ... , 3, 6, 1], dtype=int16), critical), \n#   ... ((array([ 5, -6, 4, ... , 1, 3, 3], dtype=int16),maintenance)]\ndef parse_word_waves(time_aligned_words, audio_data, verbose=False):\n    return [align_data(data, words, verbose) for data, words in zip(audio_data, time_aligned_words)]\n    \n# given numpy wave array and time alignment details\n# output a list of each data with its word\ndef align_data(data, words, verbose=False):\n    aligned = []\n    print('len(data)', len(data)) if verbose else None\n    print('len(words)', len(words)) if verbose else None\n    print('data', data) if verbose else None\n    print('words', words) if verbose else None\n    for tup in words:\n        print('tup',tup) if verbose else None\n        start = int(tup[0])\n        end = int(tup[1])\n        word = tup[2]\n        speaker_id = tup[3]\n        sentence_id = tup[4]\n        assert start >= 0\n        assert end <= len(data)\n        aligned.append( (data[start:end], word, speaker_id, sentence_id) )\n    assert len(aligned) == len(words)\n    return aligned","ab072bd6":"df = get_good_audio_files(TRAIN_CSV_FILE)\ndf.head()","ec1fb735":"df['filepath'] = df.apply(lambda row: join_dirs(row), axis=1)","a29220e6":"df['filepath'][0]","e924741a":"waves = df['filepath']","02ca992d":"audio_data = [read_audio(wave) for wave in waves]","bd8212dd":"audio_data[0]","e9914d96":"assert len(waves) == len(audio_data)","1231e419":"wrd_path = waves[0].replace('.WAV.wav', '') + '.WRD'\nwrd_path","9e78945c":"print(parse_wrd_timestamps('..\/input\/data\/TRAIN\/DR4\/MMDM0\/SI681.WRD', verbose=True))","9f8df44e":"foo = parse_wrd_timestamps('..\/input\/data\/TRAIN\/DR4\/MMDM0\/SI681.WRD')\nprint(align_data(audio_data[0], foo))","13699dc8":"time_aligned_words = [parse_wrd_timestamps(w.replace('.WAV.wav', '') + '.WRD') for w in waves]","b76d2e7e":"word_aligned_audio = parse_word_waves(time_aligned_words, audio_data)\nword_aligned_audio[0]","053225d9":"timestamp = time.strftime(\"%m%d%Y%H%M%S\", time.localtime())\ntimestamp","3814048b":"\"catch_m_monotone_20-a-classroom_l_ewenike_chidi_05312019150206_ewenike_pitch_down_50\".split('_')","a08474b9":"!mkdir waves\nos.chdir(path='waves')\nos.getcwd()","606afaa6":"i = 1\nfor sentence in word_aligned_audio:\n    for word_tup in sentence:\n        timestamp = time.strftime(\"%m%d%Y%H%M%S\", time.localtime())\n        data, word, speaker, sentence = word_tup\n        gender = 'gender-speaker-id'\n        location = 'unknown-location'\n        loudness = 'unknown-loudness'\n        lastname = 'lastname-speaker-id'\n        firstname = 'firstname-speaker-id'\n        nametag = 'timit'\n        description = speaker + '-' + sentence + '-' + str(i)\n        filename = word + '_' + gender + '_' +  description + '_' + location\n        filename += '_' + loudness + '_' + lastname + '_' + firstname\n        filename += '_' + timestamp + '_' + nametag\n        filename += '.wav'\n        \n        # filenames cannot have single quotes\n        filename = filename.replace(\"'\", '')\n        \n        wavfile.write(data=data,filename=filename,rate=RATE)\n        # print(data, filename)\n        i += 1\n\nprint(\"done\")\nstuff = os.listdir('.')\nstuff.remove('.ipynb_checkpoints') if '.ipynb_checkpoints' in stuff else None\nstuff.remove('__notebook_source__.ipynb') if '__notebook_source__.ipynb' in stuff else None\nprint('Saved',len(stuff),'wave files')","2f0429c6":"# archive & compress\n!tar -zcvf ..\/waves.tar.gz .","a670826a":"os.chdir(path='..')\nos.getcwd()","1013718a":"word_audio_files = np.array(stuff)","cdbb78e4":"waf = pd.DataFrame(word_audio_files)\nwaf.columns = ['filenames']\nwaf.dataframeName = 'word_audio.csv'","a4d35d82":"waf.head()","07633ada":"def parse_word(row):\n    feature_list = row.split(\"_\")\n    return feature_list[0]\n\ndef parse_gender(row):\n    feature_list = row.split(\"_\")\n    return feature_list[1]\n\ndef parse_description(row):\n    feature_list = row.split(\"_\")\n    return feature_list[2]\n\ndef parse_location(row):\n    feature_list = row.split(\"_\")\n    return feature_list[3]\n\ndef parse_loudness(row):\n    feature_list = row.split(\"_\")\n    return feature_list[4]\n\ndef parse_full_name(row):\n    feature_list = row.split(\"_\")\n    return feature_list[5] + \"-\" + feature_list[6] \n\ndef parse_timestamp(row):\n    feature_list = row.split(\"_\")\n    return feature_list[7]\n\ndef parse_nametag(row):\n    feature_list = row.split(\"_\")\n    return feature_list[8]","456ac006":"waf['word'] = waf['filenames'].apply(lambda row: parse_word(row))\nwaf['gender'] = waf['filenames'].apply(lambda row: parse_gender(row))\nwaf['description'] = waf['filenames'].apply(lambda row: parse_description(row))\nwaf['location'] = waf['filenames'].apply(lambda row: parse_location(row))\nwaf['loudness'] = waf['filenames'].apply(lambda row: parse_loudness(row))\nwaf['full_name'] = waf['filenames'].apply(lambda row: parse_full_name(row))\nwaf['timestamp'] = waf['filenames'].apply(lambda row: parse_timestamp(row))\nwaf['nametag'] = waf['filenames'].apply(lambda row: parse_nametag(row))","d56097dd":"waf.head()","ad3799b7":"# return a tuple of the description column details\ndef parse_extra_description(row):\n    speaker_id, sentence_id, iteration = row.split('-')\n    return (speaker_id, sentence_id, iteration)","a1ffa920":"waf['speaker_id'] = waf['description'].apply(lambda row: parse_extra_description(row)[0])\nwaf['sentence_id'] = waf['description'].apply(lambda row: parse_extra_description(row)[1])\n# iteration is just the `i` number in the for loop when the file was written to disk\nwaf['iteration'] = waf['description'].apply(lambda row: parse_extra_description(row)[2])","77629620":"waf.head()","3183e2be":"waf.to_csv(\"word_audio.csv\")","21ba0733":"NUM_FRAMES = 22528\n\ndef normalize_bytes(fname):\n    with wave.open(fname, 'rb') as f:\n        data = f.readframes(NUM_FRAMES)\n        \n\ndef normalize(file1):\n    input = wave.open(file1, 'r')\n    norm_value = min(22528, input.getnframes())\n    data = input.readframes(norm_value)\n    params = list(input.getparams())\n    input.close()\n    filename = file1[:-4] + \"_norm.wav\"\n    output = wave.open(filename, 'w')\n    output.setparams(params)\n    output.writeframes(data)\n    while(output.getnframes() < 22528):\n        padding = struct.pack('<h', 0)\n        output.writeframesraw(padding)\n    output.close()\n    return filename","4ca3e730":"def white_noise(file1):\n    input = wave.open(file1, 'r')\n    norm_value = min(22528, input.getnframes())\n    data = np.fromstring(input.readframes(input.getnframes()), dtype=np.int16)\n    params = list(input.getparams())\n    input.close()\n    amplitude = np.random.randint(250, 1000)\n    wn_data = np.random.randint(0, amplitude, 22528, dtype=np.int16)\n    white_noise_data = wn_data + data\n    white_noise_data = white_noise_data.tostring()\n\n    filename = file1[:-4] + \"_wn.wav\"\n    output = wave.open(filename, 'w')\n    output.setparams(params)\n    output.writeframes(white_noise_data)\n    output.close()","f399a8d6":"# FFT on audio\n# Pitch shift\n# INFFT on audio\n\ndef pitch_change(name, write_loc, amount):\n    dir = -1 if amount < 0 else 1\n    filepath = write_loc + name\n    wr = wave.open(filepath, 'r')\n    par = list(wr.getparams())\n    par[3] = 0  # The number of samples will be set by writeframes.\n    par = tuple(par)\n    dir_str = 'down_' if dir == -1 else 'up_'\n    fname = write_loc + name[:-4] + '_pitch_' + dir_str + str(abs(amount)) + '.wav'\n    ww = wave.open(fname, 'w')\n    ww.setparams(par)\n    fr = 10\n    sz = wr.getframerate()\/\/fr  # Read and process 1\/fr second at a time.\n    c = int(wr.getnframes()\/sz)  # count of the whole file\n    shift = amount\/\/fr  # shifting 100 Hz\n    for num in range(c):\n        da = np.fromstring(wr.readframes(sz), dtype=np.int16)\n        # split channels\n        left, right = da[0::2], da[1::2]\n        lf, rf = np.fft.rfft(left), np.fft.rfft(right) # run fft\n        # shift frequency values by desired amount\n        lf, rf = np.roll(lf, shift), np.roll(rf, shift)\n        # clear incorrectly shifted values\n        if(dir == -1):\n            lf[shift:], rf[shift:] = 0, 0\n        else:\n            lf[0:shift], rf[0:shift] = 0, 0\n\n        # run inverse fft\n        nl, nr = np.fft.irfft(lf), np.fft.irfft(rf)\n        # integrate channels\n        ns = np.column_stack((nl, nr)).ravel().astype(np.int16)\n        ww.writeframes(ns.tostring())\n    ww.close()\n    wr.close()\n    return fname\n\ndef run_pitch_change(name, write_loc):\n    fname_list = []\n    for i in range(10):\n        val = -200 + (50 * i)\n        fname = pitch_change(name, write_loc, val)\n        fname_list.append(fname)\n    return fname_list","6de7f1c6":"!mkdir normalized\n!mkdir augmented\n!ls -d *\/","303ba361":"!ls | head","92b6de08":"aug_files = []\nnorm_files = []\n\n# first augment\nfor f in waf['filenames']:\n    res1 = run_pitch_change(f, 'waves\/')\n    res1_renamed = []\n\n    for r in res1:\n        new_aug_name = r.replace('waves\/','augmented\/')\n        aug_files.append(new_aug_name)\n        res1_renamed.append(new_aug_name)\n        os.rename(r, new_aug_name)\n\n    for x in aug_files:\n        res = normalize(x)\n        old_name = res\n        new_name = res.replace('augmented\/', 'normalized\/')\n        norm_files.append(new_name)\n        os.rename(old_name, new_name)\n\n    for r in res1_renamed:\n        os.remove(r)\n\n    aug_files = []","b891f7f4":"# aug_files = []\n\n# # first augment\n# for f in waf['filenames']:\n#     res = run_pitch_change(f, 'waves\/')\n#     for r in res:\n#         new_name = r.replace('waves\/','augmented\/')\n#         aug_files.append(new_name)\n#         os.rename(r, new_name)","e415e581":"!ls augmented | head","7253457c":"aug_files[:5]","42615eb5":"# norm_files = []\n\n# # now normalize\n# for f in aug_files:\n#     res = normalize(f)\n#     old_name = res\n#     new_name = res.replace('augmented\/', 'normalized\/')\n#     norm_files.append(new_name)\n#     os.rename(old_name, new_name)","8898a7a8":"!ls normalized | head","0bb11bbd":"!ls -l","c3e28500":"# archive & compress\n!tar -zcvf waves_aug.tar.gz augmented","54f40332":"# archive & compress\n!tar -zcvf waves_norm.tar.gz normalized","3dbefbc7":"!ls -l","5982fad6":"# lastly, let's clean up the working directory so that Kaggle does not yell at us\n!rm -rf waves\n!rm -rf augmented\n!rm -rf normalized","40100164":"nRow, nCol = waf.shape\nprint(f'There are {nRow} wave files and {nCol} columns')","413deac1":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n\n# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n\n\n# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","d863cb2c":"plotPerColumnDistribution(waf, 12, 5)","dc37b4e5":"valueCounts = waf['speaker_id'].value_counts()\nvalueCounts.plot.bar()","f02fd2cb":"valueCounts = waf['word'].value_counts()\nvalueCounts.plot.bar()","915358f5":"## We want this filename format:\n```\ncatch_m_monotone_20-a-classroom_l_ewenike_chidi_05312019150206_ewenike_pitch_down_50\n```","8c35d51f":"## Okay lets do the data augmentation","c46617ab":"## Archive and compress the files for easy download\nYou can find the `waves.tar.gz` file in the \"Outputs\" section of this notebook","6f255b9a":"## This is a an example of the `wrd_path` input\n```\nwrd_path ..\/input\/data\/TRAIN\/DR4\/MMDM0\/SI681.WRD\n```","7b2a81b8":"## Ok now lets do the Data Augmentation\n","ec7ec5cb":"## Now we can look at some relevant statistics of the word-audio-files","eeec76c1":"### augment_sound.py","dbfafb2e":"## Save a CSV for the word-audio-files","cb295ad3":"### normalize.py","c9591519":"## Clean up the working directory","ba6c5e3a":"## A plot for the count of each word\n\nHard to interpret because there are so many words... but if you zoom in you will see an exponential decay curve which aligns with the word frequency of the english language, such that words like 'a' 'the' 'of' appear more often than 'catch' 'business' 'theoretic' 'informatics' etc. ","a7836f3f":"the following should be empty","7216d2c3":"## Let's define some useful constants\n","01985093":"## Now let's save the files","bbacba4c":"## Now I see a familiar situation \/ problem\n### Solution: https:\/\/www.kaggle.com\/mfekadu\/make-csv-from-files\n","f572903d":"# Extract All Words From TIMIT\nThe TIMIT dataset has time-aligned orthographic annotations. This notebook will parse through each file and write out wave files that have just one word rather than a whole sentence. ","a5366fc6":"## Here is a plot showing the count of each speaker\nIt's a bit messy because there are a lot of speakers\n\nBut the general idea is that some speakers have a bit more words and some speakers have a bit less words. However, most speakers have more than 80 words spoken.","54b5a391":"### noise.py","8a7f7858":"## Let's define some useful functions","9ad02031":"## Now for some fancy plots","95aa359f":"## test case for `align_data`"}}