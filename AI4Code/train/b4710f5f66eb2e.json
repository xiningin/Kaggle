{"cell_type":{"41260255":"code","45547fd8":"code","a99331d2":"code","f5ac9807":"code","d21a4b97":"code","ce22748d":"code","39806bd3":"code","b6d0e931":"code","4969ab18":"code","c75fff21":"code","00805940":"code","625bf122":"code","c445489b":"code","98f87e39":"code","2cc23d24":"code","89dd1523":"code","f5eb7cba":"code","016677f8":"code","5305c261":"code","f3a5022c":"code","8eb1a8f1":"code","8cc5395d":"code","57e12614":"code","23801619":"code","0d0b29e1":"code","37c9cc7d":"code","4fe9347d":"code","90c6d6a0":"code","1e9d888e":"code","bcb608e4":"code","cbe6784d":"code","69449f0b":"code","65b28b59":"code","22fe4f71":"code","f5669ecb":"code","fdae67a3":"code","bc8ca237":"code","f22054d1":"code","8c052a88":"code","6ce912cb":"code","fd08f285":"code","4f8a7476":"code","742cc5c2":"code","53faf9df":"code","2031ad5b":"code","18b738c0":"code","fc1ea53d":"code","6d802d5b":"code","807c5d9d":"code","15dd3963":"code","3c9a6b9e":"code","fbcb3f1a":"code","3ae9b3e8":"code","f3766f37":"code","de9960d0":"code","5ca56e6e":"code","90d334f0":"code","3ad5b53c":"code","15f7c07c":"code","b19851a8":"code","78b46e42":"code","527b72cd":"code","eefe3119":"code","27aa69b8":"code","78ee954b":"code","a889cd9d":"code","d31dadf3":"code","bc6f02f2":"code","54f57c9b":"code","e9cc8f88":"code","74f59212":"code","64bad719":"code","408b5434":"code","d1e16c23":"code","dab06f91":"code","a172f522":"code","e0e6e052":"code","25e91e51":"code","1eb123a6":"code","ed8ce9b4":"code","3d5a9dc6":"code","9be1d157":"code","7385f683":"code","649defb7":"code","1be47055":"code","21b0c585":"code","e605c2ca":"code","9c8a6a64":"code","0a6b5073":"code","54b928fc":"code","49151154":"code","4d4184eb":"code","18914857":"code","757b4e33":"code","9afec40b":"code","6346ecc8":"code","a4b9210b":"code","2101b39f":"code","03079450":"code","4b037314":"code","37115e8f":"code","0c424577":"code","9d9e7a85":"code","66aecfce":"code","ac24461d":"code","83fcf7a8":"code","d47ead2a":"code","f9bc05ff":"code","99329e3d":"code","0a0ba8ec":"code","75d7de92":"code","fc4bb4d3":"code","99cc7aef":"code","372237ff":"code","c2eb7ff1":"code","1d1602df":"code","b38f6240":"code","9822fb87":"code","587ddeab":"code","32f0bc5e":"code","cb8948cc":"markdown","777d53a8":"markdown","539ef290":"markdown","509695bb":"markdown","8a969752":"markdown","b281f80b":"markdown","eef4fb95":"markdown","b29882ff":"markdown","f81252a8":"markdown","95ae152c":"markdown","a3fc1e47":"markdown","d9b0429b":"markdown","de0116e6":"markdown"},"source":{"41260255":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","45547fd8":"# importing data analysis, graph libraries.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# to shuffle dataset.\nfrom sklearn.utils import shuffle\n\n# to ignore warning from sklearn.\nimport warnings\nwarnings.filterwarnings('ignore')","a99331d2":"# Reading train and test data and concat them.\n# We are adding train and test data because a model can predict with same featues which we use train the model.\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# After I got 0.78 score from kaggle I thought maybe\n# I can use test data which I predicted to train algorithms again.\n# In the second run I used test data and predictions to create a train data.\n\nsubmission_df = pd.read_csv('..\/input\/submission080\/submission.csv')\ntest_df['Survived'] = submission_df['Survived']\n\ntitanic_df = train_df.append(test_df, ignore_index=True, sort=False)","f5ac9807":"titanic_df.head()","d21a4b97":"titanic_df.head()","ce22748d":"# PassengerId is a irrelevant column with our dataset so will remove this column.\ntitanic_df.drop('PassengerId', axis=1, inplace=True)","39806bd3":"titanic_df.head(3)","b6d0e931":"# checking whether it has missing values or not.\ntitanic_df['Pclass'].isnull().sum()","4969ab18":"plt.figure(figsize=(12, 6)) # setting figure size.\n\nsns.countplot(x='Pclass', data=titanic_df, hue='Survived')","c75fff21":"# For Pclass column we will leave same for now maybe with other columns we can create some new features.","00805940":"titanic_df['Name'].isnull().sum()","625bf122":"titanic_df['Name'].head()","c445489b":"# For name column we will split the names and look the name titles.","98f87e39":"def find_title(name):\n    \"\"\"\n    This method takes a full name \n    and return the title of name\n    \n    \"\"\"\n    \n    nameList = name.split()\n    \n    for i in nameList:\n        if '.' in i:\n            name = i[:-1]\n    \n    return name","2cc23d24":"# creating a new title column in titanic_df\ntitanic_df['Title'] = titanic_df['Name'].apply(find_title) ","89dd1523":"titanic_df.head()","f5eb7cba":"plt.figure(figsize=(16, 6)) # setting figure size.\n\nsns.countplot(x='Title', data=titanic_df, hue='Survived')\n\nplt.ylim(0, 500)","016677f8":"# In the graph we see that ['Mme', 'Ms', 'L', 'Lady', 'Sir', 'Mlle', 'Countess'] all survived.\n# Also ['Don', 'Rev', 'Capt', 'Jonkheer'] all died.\n# ['Mrs', 'Miss'] have similar ratio.\n# So we will create 6 class for title:\n\n# 1 - ['Mme', 'Ms', 'L', 'Lady', 'Sir', 'Mlle', 'Countess']\n# 2 - ['Don', 'Rev', 'Capt', 'Jonkheer']\n# 3 - ['Mrs', 'Miss']\n# 4 - Master\n# 5 - Mr\n# 6 - Others","5305c261":"def title_class(title):\n    \n    if title in ['L', 'Lady', 'Sir', 'Countess', 'Mme', 'Mlle', 'Ms']:\n        return 0\n    elif title in ['Don', 'Rev', 'Capt', 'Jonkheer']:\n        return 1\n    elif title in ['Mrs', 'Miss']:\n        return 2\n    elif title in ['Master']:\n        return 3\n    elif title in ['Mr']:\n        return 5\n    else:\n        return 6","f3a5022c":"titanic_df['Title'] = titanic_df['Title'].apply(title_class)","8eb1a8f1":"titanic_df.head()","8cc5395d":"# Now we don't need the name column anymore.\ntitanic_df.drop('Name', axis=1, inplace=True)","57e12614":"titanic_df['Sex'].isnull().sum()","23801619":"def sex_column(sex):\n    \n    if sex == 'male':\n        return 0\n    else:\n        return 1","0d0b29e1":"titanic_df['Sex'] = titanic_df['Sex'].apply(sex_column)","37c9cc7d":"titanic_df.head()","4fe9347d":"titanic_df['Age'].isnull().sum()","90c6d6a0":"# We have a lot of missing values. So first we have to fill these missing values.\n# We can find relevant feature columns in age column and give some random age values for missing ages.","1e9d888e":"plt.figure(figsize=(16, 6)) # setting figure size.\n\nsns.barplot(x='Pclass', y='Age', data=titanic_df)","bcb608e4":"titanic_df['Age'].describe()","cbe6784d":"titanic_df[titanic_df['Pclass'] == 1]['Age'].describe()","69449f0b":"titanic_df[titanic_df['Pclass'] == 2]['Age'].describe()","65b28b59":"titanic_df[titanic_df['Pclass'] == 3]['Age'].describe()","22fe4f71":"# Pclass is a option for predicting Age Column.","f5669ecb":"# We can scale the fare column and analyze relationship with age.","fdae67a3":"def fare_class(fare):\n    return fare \/\/ 200","bc8ca237":"titanic_df['FareClass'] = titanic_df['Fare'].apply(fare_class)","f22054d1":"titanic_df.head()","8c052a88":"plt.figure(figsize=(16, 6)) # setting figure size.\n\nsns.barplot(x='FareClass', y='Age', data=titanic_df)","6ce912cb":"# We can use both FareClass and Pclass to fill missing age values.\n# FareClass 2 means Pclass 1 \n# FareClass 1 means Pclass 2 \n# FareClass 0 means Pclass 3 ","fd08f285":"import random\n\ndef fill_age(columns):\n    \n    age = columns[0]\n    pclass = columns[1]\n    fareclass = columns[2]\n        \n    if pd.isnull(age):\n        pclass_mean = int(round(titanic_df[titanic_df['Pclass'] == pclass]['Age'].mean()))\n        fareclass_mean = int(round(titanic_df[titanic_df['FareClass'] == fareclass]['Age'].mean()))\n\n        pclass_std = int(round(titanic_df[titanic_df['Pclass'] == pclass]['Age'].std()))\n        fareclass_std = int(round(titanic_df[titanic_df['FareClass'] == fareclass]['Age'].std()))\n\n        age_max = int(round(((pclass_mean + fareclass_mean) + (pclass_std + fareclass_std))\/2))\n        age_min = int(round(((pclass_mean + fareclass_mean) - (pclass_std + fareclass_std))\/2))\n\n        random_age = random.randint(age_min, age_max)\n        return random_age\n    else:\n        return age","4f8a7476":"titanic_df['Age'] = titanic_df[['Age', 'Pclass', 'FareClass']].apply(fill_age, axis=1)","742cc5c2":"titanic_df['Age'].isnull().sum()","53faf9df":"titanic_df['Age'] = titanic_df['Age'].apply(int)","2031ad5b":"# We may create an age_class like 0-20, 21-40, 40-others","18b738c0":"def age_class(age):\n    \n    if 0 <= age <= 20:\n        return 0\n    elif 20 < age <= 40:\n        return 1\n    elif 40 < age <= 60:\n        return 2\n    else:\n        return 3","fc1ea53d":"titanic_df['AgeClass'] = titanic_df['Age'].apply(age_class)","6d802d5b":"plt.figure(figsize=(16, 6)) # setting figure size.\n\nsns.countplot(x='AgeClass', data=titanic_df, hue='Survived')","807c5d9d":"titanic_df.head()","15dd3963":"titanic_df['SibSp'].isnull().sum()","3c9a6b9e":"titanic_df['Parch'].isnull().sum()","fbcb3f1a":"# We can use these columns to create family size column.\n# We can create a column which is passenger alone or not.","3ae9b3e8":"# We have to add 1 because we have to include passenger too.\ntitanic_df['FamilySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1","f3766f37":"def alone(familysize):\n    \n    if familysize == 1:\n        return 1\n    else:\n        return 0","de9960d0":"titanic_df['Alone'] = titanic_df['FamilySize'].apply(alone)","5ca56e6e":"titanic_df.head()","90d334f0":"sizes = titanic_df['FamilySize'].unique() # unique familysize values.\nsizes","3ad5b53c":"# We can classify the familysize as small medium and big.","15f7c07c":"(max(sizes) - min(sizes)) \/ 3","b19851a8":"def family_class(familysize):\n    \n    if familysize <= 3:\n        return 0\n    elif 3 < familysize <= 7:\n        return 1\n    else:\n        return 2","78b46e42":"titanic_df['FamilyClass'] = titanic_df['FamilySize'].apply(family_class)","527b72cd":"titanic_df.head()","eefe3119":"titanic_df['Ticket'].isnull().sum()","27aa69b8":"tickets = titanic_df['Ticket'].unique()","78ee954b":"# We can split ticket as digit and non digit tickets.\n# We may use the first letter inside tickets.","a889cd9d":"tickets_int = list()\ntickets_str = list()\n\nfor i in range(len(tickets)):\n    try:\n        tickets_int.append(int(tickets[i]))\n    except:\n        tickets_str.append(tickets[i])","d31dadf3":"def ticket_class(ticket):\n    \n    try:\n        int(ticket)\n        return 0\n    except:\n        return 1","bc6f02f2":"titanic_df['TicketClass'] = titanic_df['Ticket'].apply(ticket_class)","54f57c9b":"plt.figure(figsize=(16, 6)) # setting figure size.\n\nsns.countplot(x='TicketClass', data=titanic_df, hue='Survived')","e9cc8f88":"titanic_df.drop('Ticket', axis=1, inplace=True)","74f59212":"titanic_df.head()","64bad719":"# We have already created fareclass just we can make them integer values\ntitanic_df['Fare'].isnull().sum()","408b5434":"titanic_df[titanic_df['Fare'].isnull() == True]","d1e16c23":"pclass_mean = int(round(titanic_df[titanic_df['Pclass'] == 3]['Fare'].mean()))\npclass_std = int(round(titanic_df[titanic_df['Pclass'] == 3]['Fare'].std()))\n\nfare_min = pclass_mean - pclass_std\nfare_max = pclass_mean + pclass_std\n\nrandom_fare = random.randint(fare_min, fare_max)\n\ntitanic_df.loc[titanic_df['Fare'].isnull() == True, 'Fare'] = random_fare","dab06f91":"titanic_df['Fare'].isnull().sum()","a172f522":"titanic_df['FareClass'].isnull().sum()","e0e6e052":"titanic_df.loc[titanic_df['FareClass'].isnull() == True, 'FareClass'] = random_fare \/\/ 200","25e91e51":"titanic_df['FareClass'].isnull().sum()","1eb123a6":"titanic_df['FareClass'] = titanic_df['FareClass'].apply(int)","ed8ce9b4":"titanic_df.head()","3d5a9dc6":"titanic_df['Cabin'].isnull().sum()","9be1d157":"cabins = list(titanic_df['Cabin'].unique())","7385f683":"cabins.remove(np.nan)\ncabins = sorted(cabins)","649defb7":"def cabins(cabin):\n    \n    cabin_allocations = {'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E' : 4, 'F' : 5, 'G' : 6, 'T' : 7}\n    \n    if pd.isnull(cabin):\n        return 8\n    else:\n        return cabin_allocations[cabin[0]]","1be47055":"titanic_df['Cabin'] = titanic_df['Cabin'].apply(cabins)","21b0c585":"titanic_df.head()","e605c2ca":"titanic_df['Embarked'].isnull().sum()","9c8a6a64":"titanic_df[titanic_df['Embarked'].isnull() == True]","0a6b5073":"# I will give S for missing embarked values because the highest probility is 'S'.","54b928fc":"titanic_df.loc[titanic_df['Embarked'].isnull() == True, 'Embarked'] = 'S'","49151154":"def embarked(embarked):\n    \n    embarked_dict = {'S' : 0, 'C' : 1, 'Q' : 2}\n    return embarked_dict[embarked]","4d4184eb":"titanic_df['Embarked'] = titanic_df['Embarked'].apply(embarked)","18914857":"titanic_df['Embarked'].isnull().sum()","757b4e33":"titanic_df.head()","9afec40b":"# Now, We have a featured data set which occured from train and test dataset.\n# We will separete train and test dataset and then we will train our algorithms.","6346ecc8":"train_featured = titanic_df.iloc[:891]\ntest_featued = titanic_df.iloc[891:]\n\ntrain_featured_copy = titanic_df\ntrain_featured_copy = shuffle(train_featured_copy)\ntest_featued_copy = test_featued","a4b9210b":"# First split our train data as train and test data to see accuract values.\n# Sklearn has train_split for dividing dataset and shuffle it.\nfrom sklearn.model_selection import train_test_split\n\ntrain_df = train_featured_copy\n\nX = train_df.drop(['Survived'], axis=1)\ny = train_df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","2101b39f":"# Some algorithms from sklearn to classification. \n# Actually, I didn't use many algorithms because they are almost all will give similar result.\n# Because, the important thing is create a good featured dataset. If we have a good classifiable dataset\n# mostly all algorithm will give similar result.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","03079450":"logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\nlog_predictions = logmodel.predict(X_test)\nprint(classification_report(y_test, log_predictions))","4b037314":"svm_model = SVC()\nsvm_model.fit(X_train, y_train)\nsvm_predictions = svm_model.predict(X_test)\nprint(classification_report(y_test, svm_predictions))","37115e8f":"rdm = RandomForestClassifier()\nrdm.fit(X_train, y_train)\nrdm_predictions = rdm.predict(X_test)\nprint(classification_report(y_test, rdm_predictions))","0c424577":"param_grid = {'C' : [1, 10, 100, 1000, 10000], 'gamma' : [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, refit=True)\ngrid.fit(X_train, y_train)\ngrid_predictions = grid.predict(X_test)\nprint(classification_report(y_test, grid_predictions))","9d9e7a85":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ngbc_pred = gbc.predict(X_test)\nprint(classification_report(y_test, gbc_pred))","66aecfce":"import itertools\nimport time\n\nstart = time.time()\n\ny = train_df['Survived']\ncolumns = list(train_df.columns)\ncolumns.remove('Survived')\n\ntotal_score = [0, 0, 0]\n\nfor k in range(0, len(columns)-8):\n    \n    start_1 = time.time()\n    \n    features = list(itertools.combinations(columns, k))\n\n    score_index = [0, 0]\n    score_max = score_index[0]\n\n    for i in range(len(features)):\n\n        features_extra = list(features[i])\n        features_extra.append('Survived')\n        \n        X = train_df.drop(features_extra, axis=1)\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n        #logmodel = LogisticRegression()\n        #logmodel.fit(X_train, y_train)\n        #log_predictions = logmodel.predict(X_test)\n\n        #svm_model = SVC()\n        #svm_model.fit(X_train, y_train)\n        #svm_predictions = svm_model.predict(X_test)\n\n        #rdm = RandomForestClassifier()\n        #rdm.fit(X_train, y_train)\n        #rdm_predictions = rdm.predict(X_test)\n\n        gbc = GradientBoostingClassifier()\n        gbc.fit(X_train, y_train)\n        gbc_predictions = gbc.predict(X_test)\n\n        scores = dict()\n\n        #scores[accuracy_score(y_test, log_predictions)] = \"Logistic Regression: \"\n        #scores[accuracy_score(y_test, svm_predictions)] = \"SVM: \"\n        #scores[accuracy_score(y_test, rdm_predictions)] = \"Random Forest Classifier: \"\n        scores[accuracy_score(y_test, gbc_predictions)] = \"GradientBoosting Classifier: \"\n\n        sorted_scores = sorted(scores, reverse=True)\n\n        if score_max < max(sorted_scores):\n            score_index[0] = max(sorted_scores)\n            score_index[1] = i\n            score_max = max(sorted_scores)\n            \n        if total_score[1] < max(sorted_scores):\n            total_score[0] = k\n            total_score[1] = max(sorted_scores)\n            total_score[2] = i\n            \n\n        #print(\"------------------------------------Test\", i, '---------------------------------')    \n        #print()\n\n        #for j in sorted_scores:\n        #    print(scores[j], j)\n\n        #print()\n    end_1 = time.time()\n    print(\"------------------------------------ Extra Feature\", k, '---------------------------------')\n    print(\"Extra Feature Count:\",  k, \"\\nMax Score:\", score_index[0], \"\\nFeatue Index:\", score_index[1])\n    print(\"Extra Feature\", k, \": ...done\", end_1 - start_1)\n    \nend = time.time()\ntime_comb = end - start\nprint(\"-----------------------------------------------------------------------------------------------\")\nprint(\"Time for Combinations of Features: \", time_comb)","ac24461d":"# After I got some scores around 79-80 from algorithms. I thought, maybe I have some extra features \n# and I used bruce force to try exery combinations of extra features. \n# Because it takes long time I didn't use some algorithms above.\n# The best score 0.8507462686567164 with 3 extra feature. Lets find these extra features.","83fcf7a8":"print(\"Max Score:\", total_score[1], \"Extra Feature Count:\", total_score[0], \"Featue Index:\", total_score[2])","d47ead2a":"features = list(itertools.combinations(columns, total_score[0]))\nextra_features = list(features[total_score[2]])\nextra_features.append('Survived')","f9bc05ff":"# Now, to be sure I will remove these features and get some scores again from algorithms.","99329e3d":"train_df = train_df\n\nX = train_df.drop(extra_features, axis=1)\ny = train_df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","0a0ba8ec":"logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\nlog_predictions = logmodel.predict(X_test)\nprint(classification_report(y_test, log_predictions))","75d7de92":"svm_model = SVC()\nsvm_model.fit(X_train, y_train)\nsvm_predictions = svm_model.predict(X_test)\nprint(classification_report(y_test, svm_predictions))","fc4bb4d3":"rdm = RandomForestClassifier()\nrdm.fit(X_train, y_train)\nrdm_predictions = rdm.predict(X_test)\nprint(classification_report(y_test, rdm_predictions))","99cc7aef":"param_grid = {'C' : [1, 10, 100, 1000, 10000], 'gamma' : [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, refit=True)\ngrid.fit(X_train, y_train)\ngrid_predictions = grid.predict(X_test)\nprint(classification_report(y_test, grid_predictions))","372237ff":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ngbc_pred = gbc.predict(X_test)\nprint(classification_report(y_test, gbc_pred))","c2eb7ff1":"train_df = train_df\n\nX = train_df.drop(extra_features, axis=1)\ny = train_df['Survived']","1d1602df":"gbc.fit(X, y)","b38f6240":"X_test = test_featued.drop(extra_features, axis=1)\n\ngbc_predictions = gbc.predict(X_test)\ngbc_predictions = pd.DataFrame(gbc_predictions, columns=['Survived'])","9822fb87":"gbc_predictions['Survived'] = gbc_predictions['Survived'].apply(int)","587ddeab":"gbc_predictions.set_index(test_df['PassengerId'], inplace=True)\ngbc_predictions.to_csv('submission.csv')","32f0bc5e":"data_df = pd.read_csv('submission.csv')","cb8948cc":"## 7 - Ticket:","777d53a8":"## 1 - PassengerId:","539ef290":"## 4 - Sex:","509695bb":"## 5 - Age:","8a969752":"# 1 - Step: Feature Engineering and Data Analysis\n### We will analyze the data and try to generate some new features and clean some feature columns.","b281f80b":"## 2 - Pclass:","eef4fb95":"# 2 - Step: Train Test Algorithm and Predict:","b29882ff":"## 10 - Embarked:","f81252a8":"# Final Step:","95ae152c":"## 3 - Name:","a3fc1e47":"## 6 - SibSp and Parch:","d9b0429b":"## 9 - Cabin:","de0116e6":"## 8 - Fare:"}}