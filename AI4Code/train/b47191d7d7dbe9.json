{"cell_type":{"846eca32":"code","eccda496":"code","e6ba5fc5":"code","0018ddc8":"code","53bdbb3f":"code","d7df89fd":"code","61db4e75":"code","90842243":"code","68263bc0":"code","5626dbd1":"code","75f6cc63":"code","367b3a55":"code","5e851897":"code","45063767":"markdown","e420987e":"markdown","e31bb342":"markdown","40ce0870":"markdown","065fbb35":"markdown","a0ba6ab6":"markdown","aa6eee11":"markdown","50c62205":"markdown","4ff9e4f9":"markdown","af01f75c":"markdown","79eaa808":"markdown","c2e77642":"markdown","b2230217":"markdown","2b7e2624":"markdown"},"source":{"846eca32":"import tensorflow as tf\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","eccda496":"\n# example of training an conditional gan on the fashion mnist dataset\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.datasets.fashion_mnist import load_data\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Concatenate\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n","e6ba5fc5":"\n\nDATA_DIR = Path(\"..\/input\/stanford-covid-vaccine\/\")\nBPPS_DIR = DATA_DIR \/ \"bpps\"\n\ntrain = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json',lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsubmission = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')\n\npublic_test_df = test[test['seq_length'] == 107]\nprivate_test_df = test[test['seq_length'] == 130]\nseq_feats = ['sequence', 'structure', 'predicted_loop_type', ]\n# add bppm\ndef get_bppm(id_):\n    return np.load(BPPS_DIR \/ f\"{id_}.npy\")\n\ndef get_structure(structure: str):\n    pm = np.zeros((len(structure), len(structure)))\n    start_token_indices = []\n    for i, token in enumerate(structure):\n        if token == \"(\":\n            start_token_indices.append(i)\n        elif token == \")\":\n            j = start_token_indices.pop()\n            pm[i, j] = 1.0\n            pm[j, i] = 1.0\n    return pm\n\ndef plot_structures(bppm: np.ndarray, pm: np.ndarray):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n    axes[0].imshow(bppm)\n    axes[0].set_title(\"BPPM\")\n    axes[1].imshow(pm)\n    axes[1].set_title(\"structure\")\n    plt.show()\ndef get_bppm_feats(df, base_len=68):\n    bppm = np.zeros((df['id'].nunique(), base_len))\n    for i, id_ in tqdm(enumerate(df['id'].unique())):\n        img = get_bppm(id_)\n        maxv = np.max(img, axis=0)\n        if len(maxv) >= base_len:\n            bppm[i, :base_len] = maxv[:base_len]\n        else:\n            bppm[i, :len(maxv)] = maxv\n    return bppm\n\nbppm_train = get_bppm_feats(train)\npublic_bppm_test = get_bppm_feats(public_test_df, public_test_df['seq_length'].max())\nprivate_bppm_test = get_bppm_feats(private_test_df, private_test_df['seq_length'].max())\n","0018ddc8":"bppm_train.shape\npublic_bppm_test.shape\nprivate_bppm_test.shape","53bdbb3f":"\n# # plot images from the training dataset\nfor i in range(10):\n    # define subplot\n    fig, axes = plt.subplots(1, 2)\n    # turn off axis\n    axes[0].imshow(bppm)\n    axes[0].set_title(\"BPPM\")    # plot raw pixel data\n    sample = train.loc[i]\n    bppm = get_bppm(sample.id)\n    plt.imshow(bppm) \nplt.show()","d7df89fd":"# define the standalone discriminator model\n# def define_discriminator(in_shape=(28,28,1), n_classes=10):\ndef define_discriminator(in_shape=(107,107,1), n_classes=100):#bumber of class is determined heuristically\n\n\t# label input\n\tin_label = Input(shape=(1,))\n\t# embedding for categorical input\n# \tli = Embedding(n_classes, 50)(in_label)\n\tli = Embedding(n_classes, 50)(in_label)\n\n\t# scale up to image dimensions with linear activation\n\tn_nodes = in_shape[0] * in_shape[1]\n\tli = Dense(n_nodes)(li)\n\t# reshape to additional channel\n\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n\t# image input\n\tin_image = Input(shape=in_shape)\n\t# concat label as a channel\n\tmerge = Concatenate()([in_image, li])\n\t# downsample\n\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n\tfe = LeakyReLU(alpha=0.2)(fe)\n\t# downsample\n\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n\tfe = LeakyReLU(alpha=0.2)(fe)\n\t# flatten feature maps\n\tfe = Flatten()(fe)\n\t# dropout\n\tfe = Dropout(0.4)(fe)\n\t# output\n\tout_layer = Dense(1, activation='sigmoid')(fe)\n\t# define model\n\tmodel = Model([in_image, in_label], out_layer)\n\t# compile model\n\topt = Adam(lr=0.0002, beta_1=0.5)\n\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\treturn model","61db4e75":"# define the standalone generator model\ndef define_generator(latent_dim, n_classes=100):\n\t# label input\n\tin_label = Input(shape=(1,))\n\t# embedding for categorical input\n\tli = Embedding(n_classes, 50)(in_label)\n\t# linear multiplication\n# \tn_nodes = 7 * 7\n\tn_nodes = 35 * 35\n\tli = Dense(n_nodes)(li)\n\t# reshape to additional channel\n# \tli = Reshape((7, 7, 1))(li)\n\tli = Reshape((35, 35, 1))(li)\n\t# image generator input\n\tin_lat = Input(shape=(latent_dim,))\n\t# foundation for 7x7 image\n# \tn_nodes = 128 * 7 * 7\n\tn_nodes = 128 * 35 * 35\n\n\tgen = Dense(n_nodes)(in_lat)\n\tgen = LeakyReLU(alpha=0.2)(gen)\n# \tgen = Reshape((7, 7, 128))(gen)\n\tgen = Reshape((35, 35, 128))(gen)\n\n\t# merge image gen and label input\n\tmerge = Concatenate()([gen, li])\n\t# upsample to 14x14\n\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n\tgen = LeakyReLU(alpha=0.2)(gen)\n\t# upsample to 28x28\n\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n\tgen = LeakyReLU(alpha=0.2)(gen)\n\t# output\n# \tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n\tout_layer = Conv2D(1, (35, 35), activation='tanh', padding='same')(gen)\n\n\t# define model\n\tmodel = Model([in_lat, in_label], out_layer)\n\treturn model","90842243":"# define the combined generator and discriminator model, for updating the generator\ndef define_gan(g_model, d_model):\n\t# make weights in the discriminator not trainable\n\td_model.trainable = False\n\t# get noise and label inputs from generator model\n\tgen_noise, gen_label = g_model.input\n\t# get image output from the generator model\n\tgen_output = g_model.output\n\t# connect image output and label input from generator as inputs to discriminator\n\tgan_output = d_model([gen_output, gen_label])\n\t# define gan model as taking noise and label and outputting a classification\n\tmodel = Model([gen_noise, gen_label], gan_output)\n\t# compile model\n\topt = Adam(lr=0.0002, beta_1=0.5)\n\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n\treturn model","68263bc0":"# load fashion mnist images\ndef load_real_samples():\n\t# load dataset\n\t(trainX, trainy), (_, _) = load_data()\n\t# expand to 3d, e.g. add channels\n\tX = expand_dims(trainX, axis=-1)\n\t# convert from ints to floats\n\tX = X.astype('float32')\n\t# scale from [0,255] to [-1,1]\n\tX = (X - 127.5) \/ 127.5\n\treturn [X, trainy]\n\n# select real samples\ndef generate_real_samples(dataset, n_samples):\n\t# split into images and labels\n\timages, labels = dataset\n\t# choose random instances\n\tix = randint(0, images.shape[0], n_samples)\n\t# select images and labels\n\tX, labels = images[ix], labels[ix]\n\t# generate class labels\n\ty = ones((n_samples, 1))\n\treturn [X, labels], y","5626dbd1":"# generate points in latent space as input for the generator\ndef generate_latent_points(latent_dim, n_samples, n_classes=10):\n\t# generate points in the latent space\n\tx_input = randn(latent_dim * n_samples)\n\t# reshape into a batch of inputs for the network\n\tz_input = x_input.reshape(n_samples, latent_dim)\n\t# generate labels\n\tlabels = randint(0, n_classes, n_samples)\n\treturn [z_input, labels]\n\n# use the generator to generate n fake examples, with class labels\ndef generate_fake_samples(generator, latent_dim, n_samples):\n\t# generate points in latent space\n\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n\t# predict outputs\n\timages = generator.predict([z_input, labels_input])\n\t# create class labels\n\ty = zeros((n_samples, 1))\n\treturn [images, labels_input], y","75f6cc63":"# train the generator and discriminator\ndef train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n\tbat_per_epo = int(dataset[0].shape[0] \/ n_batch)\n\thalf_batch = int(n_batch \/ 2)\n\t# manually enumerate epochs\n\tfor i in range(n_epochs):\n\t\t# enumerate batches over the training set\n\t\tfor j in range(bat_per_epo):\n\t\t\t# get randomly selected 'real' samples\n\t\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n\t\t\t# update discriminator model weights\n\t\t\td_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n\t\t\t# generate 'fake' examples\n\t\t\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n\t\t\t# update discriminator model weights\n\t\t\td_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n\t\t\t# prepare points in latent space as input for the generator\n\t\t\t[z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n\t\t\t# create inverted labels for the fake samples\n\t\t\ty_gan = ones((n_batch, 1))\n\t\t\t# update the generator via the discriminator's error\n\t\t\tg_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n\t\t\t# summarize loss on this batch\n\t\t\tprint('>%d, %d\/%d, d1=%.3f, d2=%.3f g=%.3f' %\n\t\t\t\t(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n\t# save the generator model\n\tg_model.save('cgan_generator.h5')","367b3a55":"with tf.device('\/gpu'):\n# with strategy.scope():\n    # size of the latent space\n    latent_dim = 100\n    # create the discriminator\n    d_model = define_discriminator()\n    # create the generator\n    g_model = define_generator(latent_dim)\n    # create the gan\n    gan_model = define_gan(g_model, d_model)\n    # load image data\n    dataset = load_real_samples()\n    # train model\n    train(g_model, d_model, gan_model, dataset, latent_dim)","5e851897":"# example of loading the generator model and generating images\nfrom numpy import asarray\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.models import load_model\nfrom matplotlib import pyplot\n\n# generate points in latent space as input for the generator\ndef generate_latent_points(latent_dim, n_samples, n_classes=10):\n\t# generate points in the latent space\n\tx_input = randn(latent_dim * n_samples)\n\t# reshape into a batch of inputs for the network\n\tz_input = x_input.reshape(n_samples, latent_dim)\n\t# generate labels\n\tlabels = randint(0, n_classes, n_samples)\n\treturn [z_input, labels]\n\n# create and save a plot of generated images\ndef save_plot(examples, n):\n\t# plot images\n\tfor i in range(n * n):\n\t\t# define subplot\n\t\tpyplot.subplot(n, n, 1 + i)\n\t\t# turn off axis\n\t\tpyplot.axis('off')\n\t\t# plot raw pixel data\n\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n\tpyplot.show()\n\n# load model\nmodel = load_model('cgan_generator.h5')\n# generate images\nlatent_points, labels = generate_latent_points(100, 100)\n# specify labels\nlabels = asarray([x for _ in range(10) for x in range(10)])\n# generate images\nX  = model.predict([latent_points, labels])\n# scale from [-1,1] to [0,1]\nX = (X + 1) \/ 2.0\n# plot the result\nsave_plot(X, 10)","45063767":"Conditional GAN for Fashion-MNIST\nIn this section, we will develop a conditional GAN for the Fashion-MNIST dataset by updating the unconditional GAN developed in the previous section.\n\nThe best way to design models in Keras to have multiple inputs is by using the Functional API, as opposed to the Sequential API used in the previous section. We will use the functional API to re-implement the discriminator, generator, and the composite model.\n\nStarting with the discriminator model, a new second input is defined that takes an integer for the class label of the image. This has the effect of making the input image conditional on the provided class label.\n\nThe class label is then passed through an Embedding layer with the size of 50. This means that each of the 10 classes for the Fashion MNIST dataset (0 through 9) will map to a different 50-element vector representation that will be learned by the discriminator model.\n\nThe output of the embedding is then passed to a fully connected layer with a linear activation. Importantly, the fully connected layer has enough activations that can be reshaped into one channel of a 28\u00d728 image. The activations are reshaped into single 28\u00d728 activation map and concatenated with the input image. This has the effect of looking like a two-channel input image to the next convolutional layer.\n\nThe define_discriminator() below implements this update to the discriminator model. The parameterized shape of the input image is also used after the embedding layer to define the number of activations for the fully connected layer to reshape its output. The number of classes in the problem is also parameterized in the function and set.\n\n\n","e420987e":"The plot below summarizes the composite GAN model.\n\nImportantly, it shows the generator model in full with the point in latent space and class label as input, and the connection of the output of the generator and the same class label as input to the discriminator model (last box at the bottom of the plot) and the output of a single class label classification of real or fake.","e31bb342":"Finally, the train() function must be updated to retrieve and use the class labels in the calls to updating the discriminator and generator models.","40ce0870":"there are 60K examples in the training set and 10K in the test set and that each image is a square of 28 by 28 pixels.","065fbb35":"In this case, you can see the 100-element point in latent space as input and subsequent resizing (left) and the new class label input and embedding layer (right), then the concatenation of the two sets of feature maps (center). The remainder of the model is the same as the unconditional case\n![image.png](attachment:image.png)","a0ba6ab6":"The hard part of the conversion from unconditional to conditional GAN is done, namely the definition and configuration of the model architecture.\n\nNext, all that remains is to update the training process to also use class labels.\n\nFirst, the load_real_samples() and generate_real_samples() functions for loading the dataset and selecting a batch of samples respectively must be updated to make use of the real class labels from the training dataset. Importantly, the generate_real_samples() function now returns images, clothing labels, and the class label for the discriminator (class=1).","aa6eee11":"Next, the generate_latent_points() function must be updated to also generate an array of randomly selected integer class labels to go along with the randomly selected points in the latent space.\n\nThen the generate_fake_samples() function must be updated to use these randomly generated class labels as input to the generator model when generating new fake images.","50c62205":"In order to make the architecture clear, below is a plot of the discriminator model.\n\nThe plot shows the two inputs: first the class label that passes through the embedding (left) and the image (right), and their concatenation into a two-channel 28\u00d728 image or feature map (middle). The rest of the model is the same as the discriminator designed in the previous section.","4ff9e4f9":"Finally, the composite GAN model requires updating.\n\nThe new GAN model will take a point in latent space as input and a class label and generate a prediction of whether input was real or fake, as before.\n\nUsing the functional API to design the model, it is important that we explicitly connect the image generated output from the generator as well as the class label input, both as input to the discriminator model. This allows the same class label input to flow down into the generator and down into the discriminator.\n\nThe define_gan() function below implements the conditional version of the GAN.","af01f75c":"![image.png](attachment:image.png)","79eaa808":"Extensions\nThis section lists some ideas for extending the tutorial that you may wish to explore.\n\nLatent Space Size. Experiment by varying the size of the latent space and review the impact on the quality of generated images.\nEmbedding Size. Experiment by varying the size of the class label embedding, making it smaller or larger, and review the impact on the quality of generated images.\nAlternate Architecture. Update the model architecture to concatenate the class label elsewhere in the generator and\/or discriminator model, perhaps with different dimensionality, and review the impact on the quality of generated images.","c2e77642":"![image.png](attachment:image.png)","b2230217":"Running the example may take some time, and GPU hardware is recommended, but not required.\n\nAt the end of the run, the model is saved to the file with name \u2018cgan_generator.h5\u2018.\n\nConditional Clothing Generation\nIn this section, we will use the trained generator model to conditionally generate new photos of items of clothing.\n\nWe can update our code example for generating new images with the model to now generate images conditional on the class label. We can generate 10 examples for each class label in columns.\n\nThe complete example is listed below.","2b7e2624":"Next, the generator model must be updated to take the class label. This has the effect of making the point in the latent space conditional on the provided class label.\n\nAs in the discriminator, the class label is passed through an embedding layer to map it to a unique 50-element vector and is then passed through a fully connected layer with a linear activation before being resized. In this case, the activations of the fully connected layer are resized into a single 7\u00d77 feature map. This is to match the 7\u00d77 feature map activations of the unconditional generator model. The new 7\u00d77 feature map is added as one more channel to the existing 128, resulting in 129 feature maps that are then upsampled as in the prior model.\n\nThe define_generator() function below implements this, again parameterizing the number of classes as we did with the discriminator model."}}