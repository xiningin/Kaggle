{"cell_type":{"8ebc462f":"code","63b26918":"code","5c0abd42":"code","b4f0a7e2":"code","284fb26b":"code","cac0317b":"code","639ecfca":"code","c4afec5b":"code","1407b88b":"code","1259358e":"code","95a5969f":"code","45477427":"code","54c1a9c7":"code","79f0c62a":"code","0016ea86":"code","77de1cff":"code","79b218ae":"code","fe236766":"code","8da424ca":"code","b6b3fea2":"code","2c234ad6":"code","a862e726":"code","77d70c11":"code","9463b757":"code","498fb7f6":"code","dcdbe76a":"code","aca67d6d":"code","db1dad45":"code","7e252fcd":"code","9e053e67":"code","47d6bc7c":"code","fb1b1e85":"code","4237bad2":"code","2d3a1446":"code","16dd8dce":"code","0d712c2f":"code","63f60365":"code","b383b9e3":"code","ca18827f":"code","fb3241ad":"code","5f29f958":"code","d98137fd":"markdown","03addf93":"markdown","ffce7fd8":"markdown","55a05e89":"markdown","c77d8835":"markdown","53c86db1":"markdown","1ffcb2ea":"markdown","653afd20":"markdown","925783c2":"markdown","4928aa9d":"markdown","6440ab30":"markdown","f6caea6d":"markdown","6af401d3":"markdown","63921fd0":"markdown","4caf639a":"markdown","12f8d960":"markdown","e04f8cbb":"markdown","d2c3d59d":"markdown","c6b6a102":"markdown","c88a5f63":"markdown","7425a7ab":"markdown","99880142":"markdown","2bddc6b2":"markdown","b1fb40fd":"markdown","41b1556f":"markdown","fcf509d8":"markdown","75f65c72":"markdown","d00b7b9f":"markdown","ae5a250c":"markdown","a15c787b":"markdown","675ebc4b":"markdown","e27d875d":"markdown","ecba174f":"markdown","4f42cde6":"markdown","92a85ba2":"markdown","3a8b49f4":"markdown","eca461b0":"markdown","11a2a0c5":"markdown"},"source":{"8ebc462f":"#we must update Scikit-learn in order to usce SGD-OCSVM\n!pip3 install -U scikit-learn ","63b26918":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import  classification_report, roc_curve, roc_auc_score,  precision_recall_curve, recall_score, precision_score,f1_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import linear_model\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\n\n\nimport matplotlib.pyplot as plt","5c0abd42":"def plot_rc_curve(axes,preds,model):\n    with plt.style.context(plt.style.available[10]):\n        #axes[0,0].figure(figsize=(8,5))\n        fpr, tpr, _ = roc_curve(y_test,  preds)\n        auc = roc_auc_score(y_test, preds)\n        axes.plot(fpr,tpr,label=\"auc=\"+str(round(auc,3)))\n        axes.plot(np.arange(0,1,0.33),np.arange(0,1,0.33),\"r-\",linestyle='--')\n        axes.set_title(model,fontsize=\"20\")\n        axes.legend(loc=4,fontsize=\"20\")\n        axes.set_xlabel(\"False Positive Rate\", fontsize=\"20\")\n        axes.set_ylabel(\"True Positive Rate\",fontsize=\"20\")","b4f0a7e2":"def plot_recall_precision_curve(axes,preds,model):\n    with plt.style.context(plt.style.available[10]):\n        #axes[0,0].figure(figsize=(8,5))        \n        prec, rec, _ = precision_recall_curve(y_test, preds)\n        precision=precision_score(y_test,preds,average=\"macro\")\n        recall=recall_score(y_test,preds,average=\"macro\")\n        f1=f1_score(y_test,preds,average=\"macro\")\n        axes.plot(prec,rec,label=\"Macro Recall: \"+str(np.around(recall,3))+\"\\nMacro Precision: \"+str(np.around(precision,3))+\n                          \"\\nMacro F1-Score: \"+str(np.around(f1,3)))\n        axes.set_title(model,fontsize=\"20\")\n        axes.legend(loc = 'lower left', fontsize=\"18\",frameon=True,handlelength=0)\n        axes.set_xlabel(\"Recall\", fontsize=\"20\")\n        axes.set_ylabel(\"Precision\",fontsize=\"20\")","284fb26b":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv', skipinitialspace=True)\n\ndf.describe()","cac0317b":"df.info()","639ecfca":"df.isna().sum()","c4afec5b":"df[df.duplicated()].shape[0]","1407b88b":"df=df[~df.duplicated()]","1259358e":"df.drop('Time', axis=1, inplace=True)","95a5969f":"df[df.duplicated()].shape[0]","45477427":"df=df[~df.duplicated()]","54c1a9c7":"good=df[df[\"Class\"]==0].shape[0]\nbad=df[df[\"Class\"]==1].shape[0]\nprint(\"Non-fraud elements:   \"+str(good)+\" (\"+str(round(good\/df.shape[0]*100,3))+\"%)\")\nprint(\"Fraud elements:   \"+str(bad)+\" (\"+str(round(bad\/df.shape[0]*100,3))+\"%)\")","79f0c62a":"df.min()>=0","0016ea86":"df['Amount_log'] = np.log1p(df['Amount'])\ndf.drop(columns=\"Amount\",inplace=True)","77de1cff":"df.loc[df[\"Class\"]==1,\"Class\"]=-1\ndf.loc[df[\"Class\"]==0,\"Class\"]=1","79b218ae":"X_train, X_test, y_train, y_test= train_test_split(df.drop(columns=\"Class\"),df[\"Class\"], test_size=0.2,random_state=1234)","fe236766":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","8da424ca":"from sklearn.metrics import make_scorer\n\ndef scorer(y_true, y_predicted):\n    report=classification_report(y_true,y_predicted,output_dict=True)\n    score=(report[\"-1\"][\"recall\"]*0.6+report[\"1\"][\"recall\"]*0.4)\n    return score\n\nweighted_recall = make_scorer(scorer, greater_is_better=True)","b6b3fea2":"# parameters_linear= {\"nu\":np.arange(0.05,0.6,0.05)}\n\n# #simple SGD-OCSVM\n# clf_linear = linear_model.SGDOneClassSVM(random_state=1234, tol=1e-6)\n# search_linear = GridSearchCV(clf_linear, parameters_linear,verbose=1,scoring=weighted_recall)\n# search_linear.fit(X_train,y_train)\n# print(search_linear.best_params_)","2c234ad6":"clf_linear = linear_model.SGDOneClassSVM(nu=0.55, random_state=1234, tol=1e-6)\nclf_linear.fit(X_train)\npreds_linear=clf_linear.predict(X_train)\nprint(classification_report(y_train,preds_linear))","a862e726":"preds_linear_test=clf_linear.predict(X_test)\nprint(classification_report(y_test,preds_linear_test))","77d70c11":"# parameters_rbf = {\n#                     \"transform_rbf__gamma\":np.arange(0.01,0.3,0.1),\n#                     \"clf_rbf__nu\":np.arange(0.05,0.3,0.05)\n#                 }\n\n\n# #Nystroem allows us to approximate a kernel map using a subset of the training data.\n# #Then we can feed the transformed data to the SGD-OCSVM.\n# transform_rbf = Nystroem(kernel=\"rbf\",random_state=1234,n_components=100,n_jobs=-1)\n# clf_rbf = linear_model.SGDOneClassSVM(random_state=1234, tol=1e-6)\n\n# #First  we transform the data (with transform_rbf) and then train the model (clf_rbf).\n# pipe_rbf = Pipeline(steps=[(\"transform_rbf\", transform_rbf), (\"clf_rbf\", clf_rbf)])\n# search_rbf = GridSearchCV(pipe_rbf, parameters_rbf,verbose=1,scoring=weighted_recall)\n# search_rbf.fit(X_train,y_train)\n# print(search_rbf.best_params_)","9463b757":"transform_rbf = Nystroem(gamma= 0.05, kernel=\"rbf\",random_state=1234,n_components=100,n_jobs=-1)\nclf_rbf = linear_model.SGDOneClassSVM(nu=0.01, random_state=1234, tol=1e-6)\npipe_rbf = Pipeline(steps=[(\"transform\", transform_rbf), (\"clf\", clf_rbf)])\npipe_rbf.fit(X_train)\npreds_rbf=pipe_rbf.predict(X_train)\nprint(classification_report(y_train,preds_rbf))","498fb7f6":"preds_rbf_test=pipe_rbf.predict(X_test)\nprint(classification_report(y_test,preds_rbf_test))","dcdbe76a":"# parameters_poly = {\n#                     \"transform_poly__gamma\":[2,5],\n#                     \"clf_poly__nu\":np.arange(0.1,0.6,0.05)\n#                 }\n\n\n# #Same here as the RBF kernel\n# transform_poly = Nystroem(kernel=\"poly\",random_state=1234,n_components=100,n_jobs=-1)\n# clf_poly = linear_model.SGDOneClassSVM(random_state=1234, tol=1e-6)\n# pipe_poly = Pipeline(steps=[(\"transform_poly\", transform_poly), (\"clf_poly\", clf_poly)])\n# search_poly = GridSearchCV(pipe_poly, parameters_poly,verbose=1,scoring=weighted_recall)\n# search_poly.fit(X_train,y_train)\n# print(search_poly.best_params_)","aca67d6d":"transform_poly = Nystroem(gamma=5, kernel=\"poly\",random_state=1234,n_components=100,n_jobs=-1)\nclf_poly = linear_model.SGDOneClassSVM(nu=0.55, random_state=1234, tol=1e-6)\npipe_poly = Pipeline(steps=[(\"transform\", transform_poly), (\"clf\", clf_poly)])\npipe_poly.fit(X_train)\npreds_poly=pipe_poly.predict(X_train)\nprint(classification_report(y_train,preds_poly))","db1dad45":"preds_poly_test=pipe_poly.predict(X_test)\nprint(classification_report(y_test,preds_poly_test))","7e252fcd":"reports=pd.DataFrame(columns=['Recall_bad', 'Recall_good','Acc','Model'])\n\nreport_poly=classification_report(y_test,preds_poly_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_poly[\"-1\"][\"recall\"],2),'Recall_good':round(report_poly[\"1\"][\"recall\"],2),\n                        'Acc':round(report_poly[\"accuracy\"],2),'Model':\"Poly\"},ignore_index=True)\n\n\nreport_rbf=classification_report(y_test,preds_rbf_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_rbf[\"-1\"][\"recall\"],2),'Recall_good':round(report_rbf[\"1\"][\"recall\"],2),\n                        'Acc':round(report_rbf[\"accuracy\"],2),'Model':\"Rbf\"},ignore_index=True)\n\nreport_linear=classification_report(y_test,preds_linear_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_linear[\"-1\"][\"recall\"],2),'Recall_good':round(report_linear[\"1\"][\"recall\"],2),\n                        'Acc':round(report_linear[\"accuracy\"],2),'Model':\"Linear\"},ignore_index=True)\n\nfig, ax = plt.subplots(figsize=(15,5))\ny = np.arange(3)\nn=2\nrects1=ax.barh(data=reports,y=y- 0.35\/n,width =\"Recall_bad\", height=0.15, label=\"Fraud Recall\")\nrects2=ax.barh(data=reports,y=y,width =\"Recall_good\",height=0.15, label=\"Non Fraud Recall\")\nrects3=ax.barh(data=reports,y=y+ 0.35\/n,width =\"Acc\",height=0.15, label=\"Accuracy\")\n\nplt.grid(linestyle='-', linewidth=1,which='both')\n\nax.bar_label(rects1, padding=3,fontsize=15)\nax.bar_label(rects2, padding=3,fontsize=15)\nax.bar_label(rects3, padding=3,fontsize=15)\nplt.yticks(y,reports[\"Model\"].values,fontsize=20)\nplt.xticks(np.arange(0,1,0.1))\nplt.legend(bbox_to_anchor=(1.1,0.5), loc=\"center right\", fontsize=20, bbox_transform=plt.gcf().transFigure)\nfig.tight_layout()\nplt.title(\"Recalls and accuracy for each model\",fontsize=25)\nplt.show()\n","9e053e67":"fig, axes = plt.subplots(1, 3, figsize=(20, 5))\nfig.suptitle('    ROC curve and AUC scores', fontsize=25)\nfig.subplots_adjust(top=0.85)\nplot_rc_curve(axes[0],preds_linear_test,model=\"Linear\")\nplot_rc_curve(axes[1],preds_rbf_test,model=\"RBF kernel\")\nplot_rc_curve(axes[2],preds=preds_poly_test,model=\"Poly kernel\")","47d6bc7c":"fig, axes = plt.subplots(1, 3, figsize=(25, 5))\nfig.suptitle('    Precision and Recall curves ', fontsize=25)\nfig.subplots_adjust(top=0.85)\nplot_recall_precision_curve(axes[0],preds_linear_test,model=\"Linear\")\nplot_recall_precision_curve(axes[1],preds_rbf_test,model=\"RBF kernel\")\nplot_recall_precision_curve(axes[2],preds=preds_poly_test,model=\"Poly kernel\")","fb1b1e85":"X_train=pd.DataFrame(X_train)\nX_test=pd.DataFrame(X_test)\n#print(X_train.head())\npreds_std_train=np.ones(X_train.shape[0])\npreds_std_test=np.ones(X_test.shape[0])\nfor col,feature in enumerate(df.columns.drop(\"Class\")):\n    mean=X_train.loc[:,col].mean()\n    std=X_train.loc[:,col].std()\n    #I have chosen to consider outliers all elements outside 6*std\n    n=3\n    outlier_train=(X_train.loc[:,col]<mean-n*std) | (X_train.loc[:,col]>mean+n*std)\n    outlier_test=(X_test.loc[:,col]<mean-n*std) | (X_test.loc[:,col]>mean+n*std)\n    preds_std_train[outlier_train]=-1\n    preds_std_test[outlier_test]=-1\nX_train=np.array(X_train)\nX_test=np.array(X_test)\nprint(classification_report(y_train,preds_std_train))","4237bad2":"print(classification_report(y_test,preds_std_test))","2d3a1446":"# parameters_IF = {\n#                     \"n_estimators\":[100,200,500],\n#                     \"max_samples\":[128,256,1000],\n#                 }\n\n# clf_IF = IsolationForest(random_state=1234, n_jobs=-1)\n# search_IF = GridSearchCV(clf_IF, parameters_IF,verbose=1,scoring=weighted_recall)\n# search_IF.fit(X_train,y_train)\n# print(search_IF.best_params_)","16dd8dce":"clf_IF = IsolationForest(n_estimators=100,max_samples=128,random_state=1234, n_jobs=-1)\nclf_IF.fit(X_train)\npreds_IF=clf_IF.predict(X_train)\nprint(classification_report(y_train,preds_IF))","0d712c2f":"preds_IF_test=clf_IF.predict(X_test)\nprint(classification_report(y_test,preds_IF_test))","63f60365":"#the contamination is equal to the portion of outliers (frauds, in this case) in the data.\ncov = EllipticEnvelope(random_state=1234,assume_centered=True,contamination=bad\/df.shape[0]).fit(X_train)\npreds_cov_train=cov.predict(X_train)\nprint(classification_report(preds_cov_train,y_train))","b383b9e3":"preds_cov_test=cov.predict(X_test)\nprint(classification_report(preds_cov_test,y_test))","ca18827f":"reports=pd.DataFrame(columns=['Recall_bad', 'Recall_good','Acc','Model'])\n\n\nreport_rbf=classification_report(y_test,preds_rbf_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_rbf[\"-1\"][\"recall\"],2),'Recall_good':round(report_rbf[\"1\"][\"recall\"],2),\n                        'Acc':round(report_rbf[\"accuracy\"],2),'Model':\"SGD-OCSVM\"},ignore_index=True)\n\nreport_std=classification_report(y_test,preds_std_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_std[\"-1\"][\"recall\"],2),'Recall_good':round(report_std[\"1\"][\"recall\"],2),\n                        'Acc':round(report_std[\"accuracy\"],2),'Model':\"Std-Mean\"},ignore_index=True)\n\nreport_IF=classification_report(y_test,preds_IF_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_IF[\"-1\"][\"recall\"],2),'Recall_good':round(report_IF[\"1\"][\"recall\"],2),\n                        'Acc':round(report_IF[\"accuracy\"],2),'Model':\"IF\"},ignore_index=True)\n\nreport_cov=classification_report(y_test,preds_cov_test,output_dict=True)\nreports=reports.append({'Recall_bad':round(report_cov[\"-1\"][\"recall\"],2),'Recall_good':round(report_cov[\"1\"][\"recall\"],2),\n                        'Acc':round(report_cov[\"accuracy\"],2),'Model':\"Elliptical Envelope\"},ignore_index=True)\n\nfig, ax = plt.subplots(figsize=(15,5))\ny = np.arange(4)\nn=2\nrects1=ax.barh(data=reports,y=y- 0.35\/n,width =\"Recall_bad\", height=0.15, label=\"Recall bad\")\nrects2=ax.barh(data=reports,y=y,width =\"Recall_good\",height=0.15, label=\"Recall good\")\nrects3=ax.barh(data=reports,y=y+ 0.35\/n,width =\"Acc\",height=0.15, label=\"Accuracy\")\n\nplt.grid(linestyle='-', linewidth=1,which='both')\n\nax.bar_label(rects1, padding=3,fontsize=15)\nax.bar_label(rects2, padding=3,fontsize=15)\nax.bar_label(rects3, padding=3,fontsize=15)\nplt.yticks(y,reports[\"Model\"].values,fontsize=20)\nplt.xticks(np.arange(0,1,0.1))\nplt.legend(bbox_to_anchor=(1.1,0.5), loc=\"center right\", fontsize=20, bbox_transform=plt.gcf().transFigure)\nfig.tight_layout()\nplt.title(\"Recalls and accuracy for each model\",fontsize=25)\nplt.show()\n","fb3241ad":"fig, axes = plt.subplots(1, 4, figsize=(25, 5))\nfig.suptitle('    ROC curve and AUC scores', fontsize=15)\nfig.subplots_adjust(top=0.85)\nplot_rc_curve(axes[0],preds_rbf_test,model=\"SGD-OCSVM\")\nplot_rc_curve(axes[1],preds_IF_test,model=\"Isolation Forest\")\nplot_rc_curve(axes[2],preds_std_test,model=\"STD-Mean\")\nplot_rc_curve(axes[3],preds_cov_test,model=\"Elliptical Envelope\")","5f29f958":"fig, axes = plt.subplots(1, 4, figsize=(30, 5))\nfig.suptitle('    Precision and Recall curves ', fontsize=25)\nfig.subplots_adjust(top=0.85)\nplot_recall_precision_curve(axes[0],preds_rbf_test,model=\"SGD-OCSVM\")\nplot_recall_precision_curve(axes[1],preds_IF_test,model=\"Isolation Forest\")\nplot_recall_precision_curve(axes[2],preds=preds_std_test,model=\"STD-Mean\")\nplot_recall_precision_curve(axes[3],preds=preds_cov_test,model=\"Elliptical Envelope\")","d98137fd":"The RBF model is the best choice because of the high accuracy and high good recall on the fraud class.","03addf93":"In this part we define the basic n times std outlier detection","ffce7fd8":"We can also drop the column Time because it only gives us an offset from the first record from the dataset.","55a05e89":"#### Do we have Nan values in the data?","c77d8835":"Now we can check which features are >=0 and if they are, we can log-transform them.","53c86db1":"We have to remove others duplicates.","1ffcb2ea":"#### Are there any duplicates in the data?","653afd20":"#### Poly kernel","925783c2":"# Conclusion","4928aa9d":"The AUC score of SGD-OCSVM is slightly better than the one of Isolation Forest.","6440ab30":"Isolation forest (IF) is one of the fastest and most reliable scikit-outlier detection","f6caea6d":"#### Linear SVM (no kernel)","6af401d3":"With the release of the new version 1.0.0 of Scikit-learn, a lot of useful tools has been made avaible to the users. One of the addictions made is the **Stochastic Gradient Descent** version of the **One-Class SVM(OCSVM) method**: this new version scales linearly with the number of samples, which is a huge improvement considering that the non-SGD OCSVM have a quadratic complexity with respect to the number of samples.\n\nIn this notebook we will test the performances of the SGD-OCSVM and we will compare them to other outlier-detection methods ****without balancing the classes.****","63921fd0":"# SGD-OCSVM vs other models","4caf639a":"# Data reading","12f8d960":"#### RBF Kernel","e04f8cbb":"# Data preparation","d2c3d59d":"#### Confronting recall and accuracy scores of the methods","c6b6a102":"These plots confirm the RBF as the best choice.","c88a5f63":"#### Converting class Values","7425a7ab":"# Introduction # ","99880142":"# SGD-OCSVM Benchmarks","2bddc6b2":"I defined a new scorer that slightly weights the recall of the fraudolent class more than the non-fraud class.\n\nThis scorer will be used for the GridSearch.","b1fb40fd":"#### Isolation forest","41b1556f":"#### Models comparison","fcf509d8":"The classes are heavily unbalanced. We will see if the methods are able to detect frauds.","75f65c72":"Apart of Class, only Amount is >=0.","d00b7b9f":"#### STD Outlier detection","ae5a250c":"#### Log-transformation","a15c787b":"OCSVM is one of the most popular outlier detection method but due to its quadratic complexity it can't be used on big sized dataset: with the release version 1.0 of Scikit-Learn it's avaible to the public the Stochastic Gradient Descent (SGD) OCSVM.\n\nThe SGD version, with the use of a kernel approximation, can reach the non-SGD performance with a linear complexity, bringing the OCSVM \"power\" to dataset that were not suitable before.\n\nFor this dataset, the SGD-OCSVM showed slightly better performances than the ones of the famous Isolation Forest method, confirming his place among the best outlier detection methods.","675ebc4b":"SGD-OCSVM yielded the highest macro recall while the Elliptical Envelope yielded,by far, the hightest macro precision paired with a decent MacroRecall:thus, the last method reached the highest F1-Score.","e27d875d":"After dropping the Time columns it's possible to have others duplicates. Let's check!","ecba174f":"#### Elliptical Envelope\n\nOutlier detection method that defines an elliptical area that divides the outliers from the inliers.","4f42cde6":"#### Elements per class","92a85ba2":"We must remove the duplicated data.","3a8b49f4":"**Thank you for reading this notebook and please upvote if you appreciated the work!**\n\n**Leave a comment if you have any question or if you want to advice me some of your works!**","eca461b0":"All Scikit-kit outlier detection methods return \"-1\" for outliers elements and \"1\" for inliers: we can change the Class values according to this.","11a2a0c5":"In this part of the notebook we will see which kernel has the best performance for this dataset."}}