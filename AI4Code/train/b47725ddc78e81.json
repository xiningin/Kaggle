{"cell_type":{"53ab876d":"code","3dad5953":"code","eaa28ae2":"code","4c8e0617":"code","88e0422f":"code","38c08793":"code","e920afc7":"code","8da07131":"code","615b5145":"code","a9469749":"code","0c006e61":"code","8dad7fbd":"code","fca7084b":"code","b6e9fae1":"code","c84de792":"code","a3faf2e7":"code","15cd7a31":"code","b6b71c20":"code","8010a6a3":"code","608b3824":"code","e7ed921b":"markdown","ea09b026":"markdown","40dc83d8":"markdown","1531163a":"markdown","0bab06d4":"markdown","6e5bd1be":"markdown","45a41dc9":"markdown","9414241f":"markdown","6313fd4b":"markdown","aea2147b":"markdown","24d1aaa1":"markdown","017f60ad":"markdown","e217a5af":"markdown","b23e1c01":"markdown","712a2120":"markdown","cc5f561d":"markdown","f119ebbc":"markdown"},"source":{"53ab876d":"# Import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier","3dad5953":"# Load the data\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n\n# Show the first 10 observations from the dataset to have a basic sense of the data\ndf.head(11)","eaa28ae2":"# Print summary statistics of each feature from the dataframe\ndf.describe()","4c8e0617":"# Explore the features available in the dataframe\nprint(df.info())","88e0422f":"# Count the occurrences of each category from the `Class` variable and print them\nocc = df['Class'].value_counts()\nprint(occ)","38c08793":"# Print the ratio of fraud cases, being `0 = non-fraud` and `1 = fraud`\nprint(occ \/ len(df.index))","e920afc7":"# Define a function to create a scatterplot of the data and labels\ndef plot_data(X, y):\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label = \"Non-Fraud\", alpha = 0.5, linewidth = 0.15)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label = \"Fraud\", alpha = 0.5, linewidth = 0.15, c = 'r')\n    plt.legend()\n    return plt.show()","8da07131":"# Create X and y from df\nX = df.loc[:, df.columns != 'Class'].values\ny = df.Class.values","615b5145":"# Plot the data by running the plot_data function on X and y\nplot_data(X, y)","a9469749":"# Define the resampling method\nmethod = SMOTE(kind = 'regular')\n\n# Create the resampled feature set\nX_resampled, y_resampled = method.fit_sample(X, y)\n\n# Plot the resampled data\nplot_data(X_resampled, y_resampled)","0c006e61":"# Print the value_counts on the original labels y\nprint(pd.value_counts(pd.Series(y)))\n\n# Print the resampled value_counts using SMOTE\nprint(pd.value_counts(pd.Series(y_resampled)))","8dad7fbd":"# Define which resampling method and which ML model to use in the pipeline\nresampling = SMOTE(kind = \"borderline2\")\nmodel_lr = LogisticRegression()\n\n# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\npipeline_lr = Pipeline([('SMOTE', resampling), ('Logistic Regression', model_lr)])\n\n# Using a pipeline\n# Split your data X and y, into a training and a test set and fit the pipeline onto the training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data\npipeline_lr.fit(X_train, y_train)\npredicted_lr = pipeline_lr.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_lr))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_lr))","fca7084b":"# Apply Logistic Regression without SMOTE to compare the scores\nmodel_lr.fit(X_train, y_train)\npredicted_lr_no_smote = model_lr.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_lr_no_smote))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_lr_no_smote))","b6e9fae1":"# Define the model as the random forest\nmodel_rf = RandomForestClassifier(random_state = 5)\n\n# Define the pipeline, tell it to combine SMOTE with the Random Forest model\npipeline_rf = Pipeline([('SMOTE', resampling), ('Random Forest', model_rf)])\n\n# Fit the model to the training set\npipeline_rf.fit(X_train, y_train)\n\n# Obtain predictions from the test data\npredicted_rf = pipeline_rf.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_rf))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_rf))","c84de792":"# Apply Random Forest without SMOTE to compare the scores\nmodel_rf.fit(X_train, y_train)\npredicted_rf_no_smote = model_rf.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_rf_no_smote))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_rf_no_smote))","a3faf2e7":"# GridSearchCV to find Random Forest optimal parameters\n\n# Define the model to use\nmodel_rf = RandomForestClassifier(random_state = 5)\n\n# Define the pipeline, tell it to combine SMOTE with the Random Forest model\npipeline_rf = Pipeline([('SMOTE', resampling), ('RandomForest', model_rf)])\n\n# Define the parameter sets to test\nparam_rf = {\n    'RandomForest__n_estimators': [50, 100], \n    'RandomForest__max_features': ['auto', 'log2'], \n    'RandomForest__max_depth': [8, 12], \n    #'RandomForest__criterion': ['gini', 'entropy']\n}\n\n# Combine the parameter sets with the defined model\ngrid_search_rf = GridSearchCV(estimator = pipeline_rf, param_grid = param_rf, cv = 3, scoring = 'recall', n_jobs = -1)\n\n# Fit the model to the training data and obtain best parameters\ngrid_search_rf.fit(X_train, y_train)\ngrid_search_rf.best_params_","15cd7a31":"# Define the model as the random forest with the optimal parameters found by GridSearchCV\nbest_model_rf = RandomForestClassifier(n_estimators = 50, \n                                       max_features = 'auto', \n                                       max_depth = 8, \n                                       #criterion = 'entropy',\n                                       random_state = 5)\n\n# Define the pipeline, tell it to combine SMOTE with the optimized Random Forest model\npipeline_rf = Pipeline([('SMOTE', resampling), ('RandomForest', best_model_rf)])\n\n# Fit the model to the training set\npipeline_rf.fit(X_train, y_train)\n\n# Obtain predictions from the test data\npredicted_rf = pipeline_rf.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_rf))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_rf))","b6b71c20":"# Define the model as the XGBoost\nmodel_xg_boost = XGBClassifier(random_state = 10)\n\n# Define the pipeline, tell it to combine SMOTE with the XGBoost model\npipeline_xg_boost = Pipeline([('SMOTE', resampling), ('XGBoost', model_xg_boost)])\n\n# Fit the model to the training set\npipeline_xg_boost.fit(X_train, y_train)\n\n# Obtain predictions from the test data\npredicted_xg_boost = pipeline_xg_boost.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_xg_boost))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_xg_boost))","8010a6a3":"# GridSearchCV to find XGBoost optimal parameters\n\n# Define the model to use\nmodel_xg_boost = XGBClassifier(random_state = 10)\n\n# Define the pipeline, tell it to combine SMOTE with the XGBoost model\npipeline_xg_boost = Pipeline([('SMOTE', resampling), ('XGBoost', model_xg_boost)])\n\n# Define the parameter sets to test\nparam_xg = { \n    'XGBoost__n_estimators': [50, 75, 100],\n    'XGBoost__max_depth': [3, 6, 12]\n}\n\n# Find best parameters\ngrid_search_xg = GridSearchCV(estimator = pipeline_xg_boost, param_grid = param_xg, cv = 3, scoring = 'recall', n_jobs = -1)\n\n# Fit the model to the training data and obtain best parameters\ngrid_search_xg.fit(X, y)\ngrid_search_xg.best_params_","608b3824":"# Initialize a XGBoost Classifier with the best parameters\nbest_model_xg_boost = XGBClassifier(n_estimators = 50, max_depth = 3, random_state = 10)\n\n# Define the pipeline, tell it to combine SMOTE with the best XGBoost model\npipeline_best_xg_boost = Pipeline([('SMOTE', resampling), ('XGBoost', best_model_xg_boost)])\n\n# Fit the model to the training set\npipeline_best_xg_boost.fit(X_train, y_train)\n\n# Obtain predictions from the test data\npredicted_best_xg_boost = pipeline_best_xg_boost.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix\nprint('Classifcation report:\\n', classification_report(y_test, predicted_best_xg_boost))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = predicted_best_xg_boost))","e7ed921b":"Analyzing the previous results, it's easy to see that `fraud (Class = 1)` is undersampled, representing less than 0.2% of the total observations, showing that **this dataset is imbalanced.**","ea09b026":"#### Adjusting the XGBoost Model\n\nAs performed for Random Forest, grid search was also applied to optimize XGBoost hyperparameters.","40dc83d8":"It's easy to see from the scatterplot that, as indicated before, there are much less Fraud observations than Non-Fraud.","1531163a":"## Using ML Classification to Catch Fraud","0bab06d4":"## Load and Exploring the Data","6e5bd1be":"## (Work in Progress)","45a41dc9":"## Applying SMOTE\n\nOne way to handle the unbalanced dataset is to apply SMOTE (Synthetic Minority Oversampling Technique), which is an oversampling technique consisting of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.","9414241f":"## Comparing the Results","6313fd4b":"Comparing the scatterplot above with the previous one, it's clear that it's much more balanced, without however changing the distribution of `Fraud` cases. In fact, as can be seen below, there are the same number of `Fraud` and `Non-Fraud` observations.","aea2147b":"### - Random Forest","24d1aaa1":"### - XGBoost","017f60ad":"As described in the dataset, features `V1`, `V2`, ..., `V28` are the principal components obtained with PCA (dimensionality reduction technique), and the only variables which have not been transformed are `Time`, `Amount`, and `Class`.\n\nNone of the features have `null` values, so there is no need to handle missing values.","e217a5af":"#### Adjusting the Random Forest Model\n\nOne way to find better hyperparameters for the Random Forest model is to run a grid search. For the fraud detection problem, it's important to optimize on the recall metric, as false negatives have a higher impact than false positives, and the accuracy alone is not a sufficient measure of performance in such a heavily imbalanced dataset.","b23e1c01":"## Plotting the Data","712a2120":"Since fraud detection consists of a classification problem, where each transaction can be classified as `Fraud` or `Non-Fraud`, this analysis evaluates three different machine learning classifier models: Logistic Regression, Random Forest, and XGBoost. All of the models are implemented as part of Pipeline which applies SMOTE in order to balance the data, as explained in the previous section.","cc5f561d":"### - Logistic Regression","f119ebbc":"Using GridSearchCV to find Random Forest's optimal parameters, we got:\n- `max_depth: 8`\n- `max_features: auto`\n- `n_estimators: 50`"}}