{"cell_type":{"adc90171":"code","074ce7da":"code","4345821e":"code","2462a9ee":"code","1b7eed25":"code","1651f5a3":"code","7999f432":"code","877afbed":"code","f716e576":"code","70b3e2a4":"code","cadfbf8f":"code","bf1238f9":"code","d3cff837":"code","6e689d71":"code","4a4e56f3":"code","e283990b":"code","8f3b6089":"code","c86036c6":"code","68825002":"code","b58db475":"code","01123608":"code","7f0d9eb4":"code","19980394":"code","6e94612e":"code","d566f26b":"code","a7885d6f":"code","180dc7f8":"code","ce7d7702":"code","c3b1566e":"code","cb4b9c95":"code","07f2347e":"code","72df74f0":"code","fc4e69c2":"code","f52e1026":"code","4af0778b":"code","d797bfe1":"code","4a3ff082":"code","cb8e3521":"code","ace3b930":"code","76a3008f":"code","2bb8b300":"code","b7962c43":"code","691ed5ec":"code","724b2534":"code","7f60b81d":"code","385e0c9d":"code","1e51af8f":"code","45a1af00":"code","6210030a":"code","53e210f4":"code","b3dba752":"code","364052eb":"code","66080e23":"code","836010f5":"code","2c8aaf6a":"code","4a177374":"code","04ab6713":"code","0f0327b8":"code","92b4027a":"code","d1ec64a1":"code","bd13093d":"code","61b8bc95":"code","bee14b33":"code","2ad84cd4":"code","2662cf23":"code","e8e13e19":"code","3369d70d":"code","0dba3ccc":"code","d7816ae9":"code","b42ad4d7":"code","9738edf5":"code","06137c76":"code","91c27d39":"code","ef4f5ea6":"code","29ca71b3":"code","414603c1":"code","14020ac8":"code","0fc0372f":"code","20f06a2c":"code","17a141af":"code","eee786da":"code","cc3cc3bc":"code","bfd382a9":"code","b2999c0d":"code","d2d384cd":"code","80ff18e8":"code","d2512931":"code","d0fd99ba":"code","56733109":"code","b620fc6f":"markdown","2a95e570":"markdown","7ad61020":"markdown","52e0a390":"markdown","7be242c0":"markdown","abb8291f":"markdown","bf675bdb":"markdown","8873884d":"markdown","41bb1c27":"markdown","c7589b3b":"markdown","c4e38a99":"markdown","631d8cff":"markdown","87db2b1c":"markdown","4449281d":"markdown","ff7f91ec":"markdown","fb234daa":"markdown","d20399ab":"markdown","ceab4d66":"markdown","89efe3f5":"markdown","53803d06":"markdown","4af7d938":"markdown","2449a45c":"markdown","b35e1485":"markdown","0586e2c0":"markdown","fb93c929":"markdown","2d170ff9":"markdown","68fffaec":"markdown","4da42301":"markdown","6a814f1f":"markdown","e5320044":"markdown","1620246c":"markdown","3c81c17d":"markdown","67adbc32":"markdown","856f610f":"markdown","4a53b324":"markdown","e2060c3b":"markdown","183eeff7":"markdown","61ec6c75":"markdown"},"source":{"adc90171":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nwarnings.filterwarnings(\"ignore\")  # To ignore warnings\nsns.set(rc={\"figure.figsize\":(12,8)})  # Set figure size to 12,8\n\n","074ce7da":"pd.options.display.max_columns=150 # to display all columns ","4345821e":"# to run the code line by line\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"#run single line code","2462a9ee":"from sklearn.preprocessing import StandardScaler as std\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nimport pickle","1b7eed25":"\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.pipeline import Pipeline  # To build pipeline ","1651f5a3":"!kaggle datasets download -d mlg-ulb\/creditcardfraud","7999f432":"#!unzip \"creditcardfraud.csv.zip\"","877afbed":"data=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","f716e576":"df=data.copy()","70b3e2a4":"df.shape,data.shape","cadfbf8f":"df.head()","bf1238f9":"df.dtypes","d3cff837":"# no null values\ndf.isnull().sum()","6e689d71":"df.describe()","4a4e56f3":"round(df.corr(),2)","e283990b":"#sns.pairplot(df)","8f3b6089":"# highly imbalance class\n\ndf.Class.value_counts()\n((df.Class.value_counts())\/df.shape[0])*100","c86036c6":"sns.countplot('Class', data=df)\n","68825002":"fraud = df[df[\"Class\"]==1]\nNon_fraud= df[df[\"Class\"]==0]","b58db475":"# try for other columns\nplt.figure(figsize=(10,5))\nplt.subplot(121)\nfraud.Amount.plot.hist(title=\"Fraud Transacation\")\nplt.subplot(122)\nNon_fraud.Amount.plot.hist(title=\"Non Fraud Transaction\")","01123608":"df.Time.value_counts()","7f0d9eb4":"sns.distplot(df.Time)","19980394":"df['time']=std().fit_transform(df['Time'].values.reshape(-1, 1))","6e94612e":"sns.distplot(df.time)","d566f26b":"df.Amount.value_counts()","a7885d6f":"sns.distplot(df.Amount)","180dc7f8":"df['amount']=std().fit_transform(df['Amount'].values.reshape(-1, 1))","ce7d7702":"#sns.distplot(df.amount)","c3b1566e":"#sns.distplot(df.V1)","cb4b9c95":"df.columns","07f2347e":"final=df[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class', 'amount','time']]","72df74f0":"final.columns","fc4e69c2":"# convert target variable into x,y\ny=final['Class']\nX=final.drop(['Class'],axis=1)","f52e1026":"from sklearn.model_selection import train_test_split","4af0778b":"X_train,  X_val, y_train,y_val = train_test_split(X, y, stratify=y,test_size = 0.30, random_state = 222)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape\n","d797bfe1":"from sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression","4a3ff082":"LRC = LogisticRegression(class_weight=\"balanced\")#solver='newton-cg',max_iter=500\n\nLRC.fit(X_train, y_train)\ny_pred_LRC = LRC.predict(X_val)\n\nprint(classification_report(y_val, y_pred_LRC))","cb8e3521":"from sklearn.tree import DecisionTreeClassifier","ace3b930":"DTC = DecisionTreeClassifier()#criterion = 'entropy', max_features = 'sqrt', max_depth = 15, random_state = 0\n\nDTC.fit(X_train, y_train)\ny_pred_DT = DTC.predict(X_val)\n\nprint(classification_report(y_val, y_pred_DT))","76a3008f":"from sklearn.ensemble import RandomForestClassifier#rfc_65","2bb8b300":"rfc11 = RandomForestClassifier(class_weight=\"balanced\")#n_estimators = 1500, class_weight=\"balanced\"\n\nrfc11.fit(X_train, y_train)\ny_pred_test_RF1 = rfc11.predict(X_val)\n\nprint(classification_report(y_val, y_pred_test_RF1))","b7962c43":"# rfc111 = RandomForestClassifier(n_estimators = 2500, class_weight=\"balanced\")#\n\n# rfc111.fit(X_train, y_train)\n# y_pred_test_RF11 = rfc111.predict(X_val)\n\n# print(classification_report(y_val, y_pred_test_RF11))","691ed5ec":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom sklearn.model_selection import GridSearchCV","724b2534":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)","7f60b81d":"y_pred_xg = xgb.predict(X_val)\n\nprint(classification_report(y_val, y_pred_xg))","385e0c9d":"## explained single line way\nfrom collections import Counter\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import RandomUnderSampler","1e51af8f":"## Find Number of samples which are Fraud\nfrauds = len(df[df['Class'] == 1])## 492","45a1af00":"## Get indices of non fraud samples\nnon_frauds_index = df[df.Class == 0].index ##250000+","6210030a":"## Random sample non fraud indices and it changes \nrandom_indices = np.random.choice(non_frauds_index,frauds, replace=False)","53e210f4":"## Find the indices of fraud samples\nfraud_index = df[df.Class == 1].index","b3dba752":"## Concat fraud indices with sample non-fraud ones\nunder_sample_indices = np.concatenate([fraud_index,random_indices])\n","364052eb":"## Get Balance Dataframe\nunder_sample = final.loc[under_sample_indices]##492+492","66080e23":"under_sample.columns","836010f5":"# you can implement directly by using this code\n# undersamp = RandomUnderSampler(return_indices=True)\n# X_us, y_us, id_us = rus.fit_sample(X, y)\n\n# print('Removed indexes:', id_us)\n","2c8aaf6a":"sns.countplot('Class', data=under_sample)","4a177374":"# convert target variable into x,y\ny=under_sample['Class']\nX=under_sample.drop(['Class'],axis=1)","04ab6713":"from sklearn.model_selection import train_test_split","0f0327b8":"X_train,  X_val, y_train,y_val = train_test_split(X, y, stratify=y,test_size = 0.30, random_state = 22122)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape\n","92b4027a":"from sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression","d1ec64a1":"LRC = LogisticRegression(solver='newton-cg',max_iter=500)\n\nLRC.fit(X_train, y_train)\ny_pred_LRC = LRC.predict(X_val)\n\nprint(classification_report(y_val, y_pred_LRC))","bd13093d":"LRC1 = LogisticRegression(class_weight = 'balanced')\n\nLRC1.fit(X_train, y_train)\ny_pred_LRC1 = LRC1.predict(X_val)\n\nprint(classification_report(y_val, y_pred_LRC1))","61b8bc95":"from sklearn.tree import DecisionTreeClassifier","bee14b33":"DTC = DecisionTreeClassifier(criterion = 'entropy', max_features = 'sqrt', max_depth = 15, random_state = 0)\n\nDTC.fit(X_train, y_train)\ny_pred_DT = DTC.predict(X_val)\n\nprint(classification_report(y_val, y_pred_DT))","2ad84cd4":"from sklearn.ensemble import RandomForestClassifier#rfc_65","2662cf23":"rfc11 = RandomForestClassifier()#n_estimators = 1500, class_weight=\"balanced\"\n\nrfc11.fit(X_train, y_train)\ny_pred_test_RF1 = rfc11.predict(X_val)\n\nprint(classification_report(y_val, y_pred_test_RF1))","e8e13e19":"rfc111 = RandomForestClassifier(n_estimators = 2500, class_weight=\"balanced\")#\n\nrfc111.fit(X_train, y_train)\ny_pred_test_RF11 = rfc111.predict(X_val)\n\nprint(classification_report(y_val, y_pred_test_RF11))","3369d70d":"from sklearn.ensemble import GradientBoostingClassifier","0dba3ccc":"GBC = GradientBoostingClassifier(learning_rate=0.1, n_estimators = 500, max_features=\"sqrt\")\nGBC.fit(X_train, y_train)\n\ny_pred_GBC = GBC.predict(X_val)\nprint(classification_report(y_val, y_pred_GBC))","d7816ae9":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom sklearn.model_selection import GridSearchCV","b42ad4d7":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)","9738edf5":"y_pred_xg = xgb.predict(X_val)\n\nprint(classification_report(y_val, y_pred_xg))","06137c76":"from sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_true=y_val, y_pred=y_pred_xg)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nsns.heatmap(conf_mat,cmap=\"coolwarm_r\",annot=True)\n#ax.set_xticklabels([''] + labels)\n#ax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","91c27d39":"from imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import cross_val_score","ef4f5ea6":"# convert target variable into x,y\ny=final['Class']\nX=final.drop(['Class'],axis=1)","29ca71b3":"from sklearn.model_selection import train_test_split","414603c1":"X_train,  X_val, y_train,y_val = train_test_split(X, y, stratify=y,test_size = 0.30, random_state = 222)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape\n","14020ac8":"oversample = RandomOverSampler(sampling_strategy='minority')#not majority, all,not minority\n\nX_over, y_over = oversample.fit_resample(X_train, y_train)\n\n# summarize class distribution\nprint(\"Distribution of y_over variable:\",Counter(y_over))","0fc0372f":"steps_log= [#('over', RandomOverSampler(sampling_strategy=\"auto\")),\n           ('model', LogisticRegression(random_state=12))]\n\npipeline_log = Pipeline(steps=steps_log)\n\nprint(pipeline_log)","20f06a2c":"pipeline_log.fit(X_over, y_over)","17a141af":"y_pred_pipe_log = pipeline_log.predict(X_val)\n\nprint(classification_report(y_val, y_pred_pipe_log))","eee786da":"# evaluate pipeline\nscores_log = cross_val_score(pipeline_log, X_over, y_over,  cv=3, n_jobs=-1)  ## K-fold cross validation\nprint(\"f1 scores for each fold: \",np.round(scores_log,decimals=3))\n\n## Get the mean score \nscore = mean(scores_log)\nprint('Mean F1 Score: %.3f' % score)","cc3cc3bc":"#print(classification_report(y_train, y_pred))","bfd382a9":"steps_dt= [('over', RandomOverSampler(sampling_strategy=\"minority\",random_state=121)),\n           ('model', DecisionTreeClassifier(random_state=121))]\n\npipeline_dt = Pipeline(steps=steps_dt)\n\nprint(pipeline_dt)","b2999c0d":"pipeline_dt.fit(X_train, y_train)\n\ny_pred_pipe_dt = pipeline_dt.predict(X_val)\n\nprint(classification_report(y_val, y_pred_pipe_dt))","d2d384cd":"cross_val_score(pipeline_dt, X_over, y_over, cv=3, n_jobs=-1)","80ff18e8":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy=0.5,k_neighbors=10,random_state=12)\n\nX_smote, y_smote = smote.fit_resample(X_train, y_train)\n\nprint(\"Distribution of y_smote variable:\",Counter(y_smote))","d2512931":"steps_sm= [('smote', SMOTE(sampling_strategy=\"all\",k_neighbors=10,random_state=12)),\n           ('model', DecisionTreeClassifier(random_state=12))]\n\npipeline_sm = Pipeline(steps=steps_sm)\n\nprint(pipeline_sm)","d0fd99ba":"pipeline_sm.fit(X_train, y_train)\n\ny_pred_pipe_sm = pipeline_sm.predict(X_val)\n\nprint(classification_report(y_val, y_pred_pipe_sm))","56733109":"scores = cross_val_score(pipeline_sm, X_smote, y_smote,  cv=3, n_jobs=-1)  ## K-fold cross validation\nprint(\"f1 scores for each fold: \",np.round(scores,decimals=3))\n\n## Get the mean score \nscore = mean(scores)\nprint('Mean F1 Score: %.3f' % score)","b620fc6f":"## Check class distribution","2a95e570":"## Read data","7ad61020":"## oversample","52e0a390":"# ***Methods to handle imbalance***\n\n**Re-sampling techniques are divided in two categories:**\n1. Under-sampling the majority class(es).\n1. Over-sampling the minority class.\n1. Combining over- and under-sampling.\n1. Create ensemble balanced sets.\n1. Below is a list of the methods currently implemented in this module.\n\n**Under-sampling**\n1. Random majority under-sampling with replacement\n1. Extraction of majority-minority Tomek links \n1. Under-sampling with Cluster Centroids\n1. NearMiss-(1 & 2 & 3) \n1. Condensed Nearest Neighbour \n1. One-Sided Selection \n1. Neighboorhood Cleaning Rule \n1. Edited Nearest Neighbours \n1. Instance Hardness Threshold \n1. Repeated Edited Nearest Neighbours \n1. AllKNN \n\n**Over-sampling**\n1. Random minority over-sampling with replacement\n1. SMOTE - Synthetic Minority Over-sampling Technique \n1. SMOTENC - SMOTE for Nominal Continuous \n1. bSMOTE(1 & 2) - Borderline SMOTE of types 1 and 2\n1. SVM SMOTE - Support Vectors SMOTE \n1. ADASYN - Adaptive synthetic sampling approach for imbalanced learning \n1. KMeans-SMOTE \n1. ROSE - Random OverSampling Examples \n\n**Over-sampling followed by under-sampling**\n1. SMOTE + Tomek links \n1. SMOTE + ENN \n\n**Ensemble classifier using samplers internally**\n1. Easy Ensemble classifier \n1. Balanced Random Forest \n1. Balanced Bagging\n1. RUSBoost ","7be242c0":"## Decision Trees","abb8291f":"### Data reference\n\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n#### Note-\n**For big .csv files you can consider using kaggle kernels.**\n","bf675bdb":"## Models","8873884d":"# Under Sampling\nUndersampling is one of the techniques used for handling class imbalance. In this technique, we under sample majority class to match the minority class. So in our example, we take random sample of majority class to match number of minority samples. This makes sure that the training data has equal amount of majority and minority samples.","41bb1c27":"# Models\n\njust using as stratify from train test split ","c7589b3b":"# Train test Split","c4e38a99":"## Train test Split","631d8cff":"## XGB","87db2b1c":"## Decision Tree","4449281d":"## Logistic Regression","ff7f91ec":"# Statistical analysis","fb234daa":"### Data\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n### Content\n\nThis data contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n### Objective\nIdentify fraudulent credit card transactions.\n","d20399ab":"# Final Data","ceab4d66":"## Logistic Regression","89efe3f5":"## Class\nThe response variable and it takes value 1 in case of fraud and 0 Non fraud.","53803d06":"# Smote models\n **Synthetic Minority Oversampling Technique**\n \n Sampling_strategy = Desired ratio of the number of samples in the minority class over the number of samples in the majority class after resampling.","4af7d938":"## Approach\n\n\n1. Check classes and its distibution.\n2. Decide if there is any class imbalalnce: which technique of resampling will work in this case?\n3. Decide best resampling technique. \n4. Evaluate model by tuning mmultiple classifier and and create pipeline for data preprocessing and model building.\n5. What can be possible evaluation metrics in such cases?\n\n","2449a45c":"## Gradient Boosted Trees","b35e1485":"## XGB","0586e2c0":"**note** purpose creating notebook for learning. No optimisation done. ","fb93c929":"## Logistic","2d170ff9":"## Random Forest","68fffaec":"## V columns\n\nAs we known all values are masked and they are transformed from the PCA values.They are already scaled.","4da42301":"# Column wise","6a814f1f":"## Amount\nthe transaction Amount, this feature can be used for example-dependant cost-senstive learning","e5320044":"## train test split","1620246c":"# Credit Card Fraud Detection\n\n","3c81c17d":"## Time\n The seconds elapsed between each transaction and the first transaction in the dataset\n \n The distribution of values varies from 0 to 172792.0 sec(almost 2 days), so we are converting the data in standardization without distrubing distribution of values ","67adbc32":"## Decision Trees","856f610f":"By seeing the graph and out of class then dataset have highly imbalanced class of non fraud of 99.8% and 0.17% of fraud","4a53b324":"# Over Sampling\nRandom oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. ","e2060c3b":"## Random Forest","183eeff7":"# Data Preprocessing","61ec6c75":"## kaggle  data"}}