{"cell_type":{"6b85140f":"code","32c41ef1":"code","2c459e5f":"code","62bdc628":"code","80481fd2":"code","c4f439cb":"code","94918c27":"code","7e00075f":"code","af8e90aa":"code","d29918c9":"code","103b0877":"code","08c89e65":"code","e47a5da0":"code","f03cb5c5":"code","3ec1eea3":"code","f697446a":"code","8e48eeed":"code","66781459":"code","ed7d3d64":"code","2b8c084c":"code","944412f8":"code","9abc39fe":"code","881c4e54":"code","be89137f":"code","be471d78":"code","af9bc4e0":"code","7e811605":"code","a587fd02":"code","36a011bc":"code","a252b297":"code","18deca35":"code","eb5d1b9d":"markdown","1e12fefc":"markdown","633ef99c":"markdown","338cb238":"markdown","391833f4":"markdown","661b5055":"markdown","1b911fbf":"markdown","cc3f5203":"markdown","b21c32b3":"markdown","678d4faa":"markdown","050d97c9":"markdown","6e5d1765":"markdown","da38852d":"markdown","90084691":"markdown"},"source":{"6b85140f":"import os\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, BatchNormalization, Dropout\nfrom keras.optimizers import Adam","32c41ef1":"train = pd.read_csv('\/kaggle\/input\/handwriting-recognition\/written_name_train_v2.csv')\nvalid = pd.read_csv('\/kaggle\/input\/handwriting-recognition\/written_name_validation_v2.csv')\ntrain","2c459e5f":"plt.figure(figsize=(15, 10))\n\nfor i in range(9):\n    ax = plt.subplot(3,3,i+1)\n    img_dir = '\/kaggle\/input\/handwriting-recognition\/train_v2\/train\/'+train.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap = 'gray')\n    plt.title(train.loc[i, 'IDENTITY'], fontsize=12)\n    plt.axis('off')\n\nplt.subplots_adjust(wspace=0.2, hspace=-0.8)","62bdc628":"print(\"Number of NaNs in train set      : \", train['IDENTITY'].isnull().sum())\nprint(\"Number of NaNs in validation set : \", valid['IDENTITY'].isnull().sum())","80481fd2":"train.dropna(axis=0, inplace=True)#axis =0, removing rows otherwisw axis =1. removing columns\nvalid.dropna(axis=0, inplace=True) #true means dropping","c4f439cb":"unreadable = train[train['IDENTITY'] == 'UNREADABLE']\nunreadable.reset_index(inplace = True, drop=True)\n\nplt.figure(figsize=(15, 10))\n\nfor i in range(9):\n    ax = plt.subplot(3, 3, i+1)\n    img_dir = '\/kaggle\/input\/handwriting-recognition\/train_v2\/train\/'+unreadable.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap = 'gray')\n    plt.title(unreadable.loc[i, 'IDENTITY'], fontsize=12)\n    plt.axis('off')\n\nplt.subplots_adjust(wspace=0.2, hspace=-0.8)","94918c27":"train = train[train['IDENTITY'] != 'UNREADABLE']\nvalid = valid[valid['IDENTITY'] != 'UNREADABLE']\nvalid","7e00075f":"train['IDENTITY'] = train['IDENTITY'].str.upper()\nvalid['IDENTITY'] = valid['IDENTITY'].str.upper()","af8e90aa":"train.reset_index(inplace = True, drop=True) \nvalid.reset_index(inplace = True, drop=True)","d29918c9":"def preprocess(img):\n    (h, w) = img.shape\n    \n    final_img = np.ones([64, 256])*255 # black white image\n    \n    # crop\n    if w > 256:\n        img = img[:, :256]\n        \n    if h > 64:\n        img = img[:64, :]\n    \n    \n    final_img[:h, :w] = img\n    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)","103b0877":"train_size = 10000\nvalid_size= 1000","08c89e65":"train_x = []\n\nfor i in range(train_size):\n    img_dir = '\/kaggle\/input\/handwriting-recognition\/train_v2\/train\/'+train.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    image = preprocess(image)\n    image = image\/255\n    train_x.append(image)","e47a5da0":"valid_x = []\n\nfor i in range(valid_size):\n    img_dir = '\/kaggle\/input\/handwriting-recognition\/validation_v2\/validation\/'+valid.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    image = preprocess(image)\n    image = image\/255\n    valid_x.append(image)","f03cb5c5":"train_x = np.array(train_x).reshape(-1, 256, 64, 1)#array will get reshaped in such a way that the resulting array has only 1 column\nvalid_x = np.array(valid_x).reshape(-1, 256, 64, 1) #(16384,1)","3ec1eea3":"alphabets = u\"!\\\"#&'()*+,-.\/0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \" \nmax_str_len = 24 # max length of input labels\nnum_of_characters = len(alphabets) + 1 # +1 for ctc pseudo blank(epsilon)\nnum_of_timestamps = 64 # max length of predicted labels\n\n\ndef label_to_num(label):\n    label_num = []\n    for ch in label:\n        label_num.append(alphabets.find(ch)) \n        #find() method returns the lowest index of the substring if it is found in given string otherwise -1\n        \n    return np.array(label_num)\n\ndef num_to_label(num):\n    ret = \"\"\n    for ch in num:\n        if ch == -1:  # CTC Blank\n            break\n        else:\n            ret+=alphabets[ch]\n    return ret","f697446a":"name = 'JEBASTIN'\nprint(name, '\\n',label_to_num(name))","8e48eeed":"train_y = np.ones([train_size, max_str_len]) * -1\ntrain_label_len = np.zeros([train_size, 1])\ntrain_input_len = np.ones([train_size, 1]) * (num_of_timestamps-2)\ntrain_output = np.zeros([train_size])\n\nfor i in range(train_size):\n    train_label_len[i] = len(train.loc[i, 'IDENTITY'])\n    train_y[i, 0:len(train.loc[i, 'IDENTITY'])]= label_to_num(train.loc[i, 'IDENTITY'])    ","66781459":"valid_y = np.ones([valid_size, max_str_len]) * -1\nvalid_label_len = np.zeros([valid_size, 1])\nvalid_input_len = np.ones([valid_size, 1]) * (num_of_timestamps-2)\nvalid_output = np.zeros([valid_size])\n\nfor i in range(valid_size):\n    valid_label_len[i] = len(valid.loc[i, 'IDENTITY'])\n    valid_y[i, 0:len(valid.loc[i, 'IDENTITY'])]= label_to_num(valid.loc[i, 'IDENTITY'])    ","ed7d3d64":"print('True label : ',train.loc[100, 'IDENTITY'] , '\\ntrain_y : ',train_y[100],'\\ntrain_label_len : ',train_label_len[100], \n      '\\ntrain_input_len : ', train_input_len[100])","2b8c084c":"input_data = Input(shape=(256, 64, 1), name='input')\n\ninner = Conv2D(32, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)  \ninner = BatchNormalization()(inner)\ninner = Activation('relu')(inner)\ninner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)\n\ninner = Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)\ninner = BatchNormalization()(inner)\ninner = Activation('relu')(inner)\ninner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)\ninner = Dropout(0.3)(inner)\n\ninner = Conv2D(128, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)\ninner = BatchNormalization()(inner)\ninner = Activation('relu')(inner)\ninner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)\ninner = Dropout(0.3)(inner)\n\n# CNN to RNN\ninner = Reshape(target_shape=((64, 1024)), name='reshape')(inner)\ninner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n\n## RNN\ninner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm1')(inner)\ninner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm2')(inner)\n\n## OUTPUT\ninner = Dense(num_of_characters, kernel_initializer='he_normal',name='dense2')(inner)\ny_pred = Activation('softmax', name='softmax')(inner)\n\nmodel = Model(inputs=input_data, outputs=y_pred)\nmodel.summary()","944412f8":"# the ctc loss function\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    # the 2 is critical here since the first couple outputs of the RNN\n    # tend to be garbage\n    y_pred = y_pred[:, 2:, :]\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)","9abc39fe":"labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\ninput_length = Input(name='input_length', shape=[1], dtype='int64')\nlabel_length = Input(name='label_length', shape=[1], dtype='int64')\n\nctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\nmodel_final = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)","881c4e54":"# the loss calculation occurs elsewhere, so we use a dummy lambda function for the loss\nfile_path_best = \"C_LSTM_best.hdf5\"\n\nmodel_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=Adam(lr = 0.0001))\n\ncheckpoint = ModelCheckpoint(filepath=file_path_best, \n                             monitor='val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='min')\n\ncallbacks_list = [checkpoint]","be89137f":"history = model_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output,validation_data=([valid_x, valid_y, valid_input_len, valid_label_len], valid_output),callbacks=callbacks_list,verbose=1,epochs=60, batch_size=128,shuffle=True)","be471d78":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","af9bc4e0":"model.load_weights('..\/input\/trainedmodel\/C_LSTM_best.hdf5')","7e811605":"preds = model.predict(valid_x)\ndecoded = K.get_value(K.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1], \n                                   greedy=True)[0][0])\n\nprediction = []\nfor i in range(valid_size):\n    prediction.append(num_to_label(decoded[i]))","a587fd02":"y_true = valid.loc[0:valid_size, 'IDENTITY']\ncorrect_char = 0\ntotal_char = 0\ncorrect = 0\n\nfor i in range(valid_size):\n    pr = prediction[i]\n    tr = y_true[i]\n    total_char += len(tr)\n    \n    for j in range(min(len(tr), len(pr))):\n        if tr[j] == pr[j]:\n            correct_char += 1\n            \n    if pr == tr :\n        correct += 1 \n    \nprint('Correct characters predicted : %.2f%%' %(correct_char*100\/total_char))\nprint('Correct words predicted      : %.2f%%' %(correct*100\/valid_size))","36a011bc":"test = pd.read_csv('\/kaggle\/input\/handwriting-recognition\/written_name_validation_v2.csv')\n\nplt.figure(figsize=(15, 10))\nfor i in range(16):\n    ax = plt.subplot(4, 4, i+1)\n    img_dir = '\/kaggle\/input\/handwriting-recognition\/validation_v2\/validation\/'+test.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap='gray')\n    \n    image = preprocess(image)\n    image = image\/255.\n    pred = model.predict(image.reshape(1, 256, 64, 1))\n    decoded = K.get_value(K.ctc_decode(pred, input_length=np.ones(pred.shape[0])*pred.shape[1], \n                                       greedy=True)[0][0])\n    plt.title(num_to_label(decoded[0]), fontsize=12)\n    plt.axis('off')\n    \nplt.subplots_adjust(wspace=0.2, hspace=-0.8)","a252b297":"plt.figure(figsize=(5, 5))\n\nimg_dir = \"..\/input\/testimages\/TEST_0004.jpg\"\nimage = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\nplt.imshow(image, cmap='gray')\n\nimage = preprocess(image)\nimage = image\/255\npred = model.predict(image.reshape(1, 256, 64, 1))\ndecoded = K.get_value(K.ctc_decode(pred, input_length=np.ones(pred.shape[0])*pred.shape[1], \n                                   greedy=True)[0][0])\nprint(num_to_label(decoded[0]))","18deca35":"plt.figure(figsize=(5, 5))\n\nimg_dir = \"\/kaggle\/input\/test234567575\/test2.PNG\"\nimage = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\nplt.imshow(image, cmap='gray')\n\nimage = preprocess(image)\nimage = image\/255\npred = model.predict(image.reshape(1, 256, 64, 1))\ndecoded = K.get_value(K.ctc_decode(pred, input_length=np.ones(pred.shape[0])*pred.shape[1], \n                                   greedy=True)[0][0])\nprint(num_to_label(decoded[0]))","eb5d1b9d":"## Some predictions on test set","1e12fefc":"## Train our model","633ef99c":"## Preparing the labels for CTC Loss\n\nLearn more about CTC loss and why its amazing for text recognition from [here](https:\/\/theailearner.com\/2019\/05\/29\/connectionist-temporal-classificationctc\/).\n\nThe labels have to be converted to numbers which represent each character in the training set. The 'alphabets' consist of A-Z and three special characters (-  '  and space). ","338cb238":"## Preprocessing and preparing the images for training","391833f4":"## Cleaning Data","661b5055":"There are some labels which are in lowercase. To maintain uniformity in the labels, I convert all the labels to uppercase.","1b911fbf":"Also, there are some images in our data with the label 'UNREADABLE'. Lets check those images and remove them.","cc3f5203":"The output shape of the predictions is (64, 30). The model predicts words of 64 characters and each character contains the probability of the 30 alphabets which we defined earlier.  ","b21c32b3":"* **train_y** contains the true labels converted to numbers and padded with -1. The length of each label is equal to max_str_len. \n* **train_label_len** contains the length of each true label (without padding) \n* **train_input_len** contains the length of each predicted label. The length of all the predicted labels is constant i.e number of timestamps - 2.  \n* **train_output** is a dummy output for ctc loss. \n","678d4faa":"Reset the index and we are done with cleaning. ","050d97c9":"The model will be trained on 30000 images and validate on 3000 images","6e5d1765":"* The images are loaded as grayscale and reshaped to width 256 and height 64.  \n* The width and height are cropped if they are greater than 256 and 64 respectively. If they are smaller, then the image is padded with white pixels. Finally the image is rotated clockwise to bring the image shape to (x, y). \n* The image is then normalized to range [0, 1]","da38852d":"## Check model performance on validation set","90084691":"## Building our model\n"}}