{"cell_type":{"47ff9782":"code","a42a783f":"code","a703cd7a":"code","b1194665":"code","f1a26b6d":"code","91f57d10":"code","636c0773":"code","13a73d16":"code","fda6545e":"code","c04312ff":"code","ce0be584":"code","6503404f":"code","82839637":"code","e21abca5":"code","68416768":"code","4fde4db7":"code","69ce389c":"code","a556c043":"code","4400a455":"code","263da45b":"code","a12b2497":"code","eabd422f":"code","312c04d7":"code","e6d086a9":"code","ee714b03":"code","7869a327":"code","92981c46":"code","7c12b4bb":"code","a8e45c51":"code","d0a0ec91":"code","bf5ee556":"code","c70bd1ef":"code","0725f7bf":"code","a26b8e8f":"code","78940968":"code","cdc7b891":"code","41f60723":"code","77b8a920":"code","67270871":"code","a3c4d7f6":"code","cbbd357d":"code","c35f24bc":"code","024ef067":"code","488be23f":"markdown","d525effa":"markdown","8d23f7ba":"markdown","f290389d":"markdown","50957b15":"markdown","7057ba40":"markdown","3437ff46":"markdown","4e850949":"markdown","737ad456":"markdown","c4bfd51c":"markdown","32c67651":"markdown","0ee8352b":"markdown","79c43240":"markdown","83c022a6":"markdown"},"source":{"47ff9782":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier\nimport category_encoders as ce\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom scipy.stats import randint as sp_randint,uniform as sp_ranprop\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV,RFE,SelectFromModel\nimport mlxtend as mx\nfrom mlxtend.classifier import  StackingCVClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis,RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB , BernoulliNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom tempfile import mkdtemp\nfrom shutil import rmtree\nfrom lightgbm import LGBMClassifier\n\nimport pandas_profiling\n\ndef misclass(labels,results,wilderness):\n    analysis = pd.DataFrame({'labels':labels,'results':results,'wilderness':wilderness})\n    analysis['correct'] = (analysis.labels == analysis.results)+0\n    print(pd.crosstab(margins=True,index=analysis.labels,columns=analysis_db.wilderness,values=analysis.correct,aggfunc=np.mean,dropna=False))\n    print(pd.crosstab(margins=True,index=analysis.labels,columns=analysis_db.wilderness,values=analysis.correct,aggfunc=np.sum,dropna=False))\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a42a783f":"#Input training data\ntraining_db = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\n","a703cd7a":"#training_db.profile_report()","b1194665":"def transform_db(db):\n    new_db = db.drop(columns=['Soil_Type15','Soil_Type7']).eval(\n        \"calc_slope=Elevation\/(Horizontal_Distance_To_Hydrology+0.01)\").eval(\n        \"calc_slope2=Vertical_Distance_To_Hydrology\/(Horizontal_Distance_To_Hydrology+0.01)\").eval(\n        \"calc_slope3=Vertical_Distance_To_Hydrology\/(Horizontal_Distance_To_Roadways+0.01)\").eval(\n         \"sin_aspect=sin(Aspect\/180*3.14156926)\"\n         ).eval(\"sin_slope=sin(Slope\/180*3.14156926)\").eval(\"h_ratio1=Hillshade_9am\/(Hillshade_3pm+0.01)\").eval(\n            \"hratio2=Hillshade_9am\/(Hillshade_Noon+0.01)\").eval(\n            \"hratio3=Hillshade_3pm\/(Hillshade_Noon+0.01)\").eval(\n    \"diff1 = Elevation - Vertical_Distance_To_Hydrology\"\n    ).eval(\n    \"diff2 = Horizontal_Distance_To_Roadways  - Horizontal_Distance_To_Hydrology\"\n    ).eval(\n    \"diff3 = Horizontal_Distance_To_Roadways  - Horizontal_Distance_To_Fire_Points\"\n    ).eval(\n    \"diff4 = Horizontal_Distance_To_Hydrology- Horizontal_Distance_To_Fire_Points\"\n    ).eval(\"sum1 = Elevation + Vertical_Distance_To_Hydrology\"\n    ).eval(\n    \"sum2 = Horizontal_Distance_To_Roadways  + Horizontal_Distance_To_Hydrology\"\n    ).eval(\n    \"sum3 = Horizontal_Distance_To_Roadways  + Horizontal_Distance_To_Fire_Points\"\n    ).eval(\n    \"sum = Horizontal_Distance_To_Hydrology+ Horizontal_Distance_To_Fire_Points\"\n    ).eval(\"rockoutcropcomplex= Soil_Type1 + Soil_Type3 + Soil_Type4 + Soil_Type5 + Soil_Type6 \\\n                  + Soil_Type11  + Soil_Type28 + Soil_Type33\").eval(\n        \"rubbly = Soil_Type3 + Soil_Type4 + Soil_Type5 + Soil_Type10 + Soil_Type11 +\\\n         Soil_Type13\").eval(\n        \"Vanet = Soil_Type2 +Soil_Type5 + Soil_Type6\").eval(\"Bulwark = Soil_Type10 +Soil_Type11\").eval(\n        \"Leighcan = Soil_Type21 + Soil_Type22 + Soil_Type23 + Soil_Type24 + Soil_Type25+\\\n           Soil_Type27 + Soil_Type28 + Soil_Type31 + Soil_Type32 + Soil_Type33+\\\n           Soil_Type38\").eval(\n        \"ext_stony = Soil_Type1 + Soil_Type24 + Soil_Type25 + Soil_Type27 + Soil_Type28 +\\\n            Soil_Type29 + Soil_Type30 + Soil_Type31 + Soil_Type32 + Soil_Type33 +\\\n            Soil_Type34 + Soil_Type36 + Soil_Type37 + Soil_Type38 + Soil_Type39 +\\\n            Soil_Type40\").eval(\n           \"very_stony = Soil_Type2 + Soil_Type9 + Soil_Type18\").eval(\n           \"stony = Soil_Type6 + Soil_Type12\").eval(\n            \"interact1 = Elevation*Horizontal_Distance_To_Hydrology\"\n    ).eval(\n            \"interact2 = Elevation*Vertical_Distance_To_Hydrology\"\n    ).eval(\n            \"interact3 = Vertical_Distance_To_Hydrology*Horizontal_Distance_To_Hydrology\"\n    ).eval(\n     \"Elevation2 = Elevation*Elevation\"\n    ).eval(\n     \"Vertical_Distance_To_Hydrology2 = Vertical_Distance_To_Hydrology*Vertical_Distance_To_Hydrology\"\n    ).eval(\n     \"Horizontal_Distance_To_Hydrology2 = Horizontal_Distance_To_Hydrology*Horizontal_Distance_To_Hydrology\"\n    ).eval(\"total_light = Hillshade_9am+Hillshade_3pm+Hillshade_Noon\"\n                                                               ).eval(\n    \"morning=Hillshade_9am\/(total_light+0.01)\"\n    ).eval(\n    \"noon=Hillshade_Noon\/(total_light+0.01)\"\n    ).eval(\n    \"afternoon=Hillshade_3pm\/(total_light+0.01)\"\n    ).eval(\n    \"lightdiff1 = Hillshade_9am -Hillshade_3pm\"\n    ).eval(\n    \"lightdiff2 = Hillshade_9am -Hillshade_Noon\"\n    ).eval(\n    \"lightdiff3 = Hillshade_Noon -Hillshade_3pm\"\n    ).eval(\n    \"lightsum1 = Hillshade_9am +Hillshade_3pm\"\n    ).eval(\n    \"lightsum2 = Hillshade_9am +Hillshade_Noon\"\n    ).eval(\n    \"lightsum3 = Hillshade_Noon +Hillshade_3pm\"\n    ).eval(\n    \"North = ((Aspect >= 0) & (Aspect <=45) ) | ((Aspect>=315)&(Aspect<=360) )\"\n    ).eval(\n    \"East = ((Aspect > 45) & (Aspect <=135) )\"\n    ).eval(\n    \"South = ((Aspect > 135) & (Aspect <=225) )\"\n    ).eval(\n    \"West = ((Aspect > 225) & (Aspect <=315) )\"\n    ).eval(\n    \"SouthSlope = cos(South*Slope*3.141592653\/180)\"\n    ).eval(\n    \"WestSlope = cos(West*Slope*3.141592653\/180)\"\n    ).eval(\n    \"EastSlope = cos(East*Slope*3.141592653\/180)\"\n    ).eval(\n    \"NorthSlope = cos(North*Slope*3.141592653\/180)\"\n    )\n\n    \n    \n    new_db[\"accumulate\"]=0\n    soils = new_db.columns.values[\n                             new_db.columns.str.startswith(\"Soil_Type\")]\n    for i,name in enumerate(soils):\n            new_db.accumulate = new_db.accumulate + new_db[name]*int(name.replace('Soil_Type',''))\n        \n      \n    #return new_db.drop(columns=soils)\n    return new_db\n\ntransformed_training_db = transform_db(training_db)","f1a26b6d":"def analyse_db(training_db):\n   analysis_db = training_db.eval(\"wilderness = Wilderness_Area1+Wilderness_Area2*2+Wilderness_Area3*4+Wilderness_Area4*8\") \n   analysis_db[\"Cover_Type\"] = \"c\"+analysis_db.Cover_Type.astype(str)\n   analysis_db[\"wilderness\"] = \"w\"+analysis_db.wilderness.astype(str)\n   analysis_db[\"Grp_Flags\"] = analysis_db.Cover_Type.replace([\"c5\",\"c7\",\"c2\",\"c1\"],\"High\").replace([\"c4\",\"c3\",\"c6\"],\"Low\")\n   analysis_db = pd.concat((analysis_db,pd.get_dummies(analysis_db.Cover_Type,prefix='',dtype=int)),axis=1)\n   analysis_db[\"accumulate\"]=0\n   soils = analysis_db.columns.values[\n                             analysis_db.columns.str.startswith(\"Soil_Type\")]\n   for i,name in enumerate(soils):\n            analysis_db.accumulate = analysis_db.accumulate + analysis_db[name]*(2<<(i+1))\n\n   return analysis_db.eval(\"total_light = Hillshade_9am+Hillshade_3pm+Hillshade_Noon\"\n                                                               ).eval(\n    \"morning=Hillshade_9am\/(total_light+0.01)\"\n    ).eval(\n    \"noon=Hillshade_Noon\/(total_light+0.01)\"\n    ).eval(\n    \"afternoon=Hillshade_3pm\/(total_light+0.01)\"\n    ).eval(\n    \"lightdiff1 = Hillshade_9am -Hillshade_3pm\"\n    ).eval(\n    \"lightdiff2 = Hillshade_9am -Hillshade_Noon\"\n    ).eval(\n    \"lightdiff3 = Hillshade_Noon -Hillshade_3pm\"\n    ).eval(\n    \"lightsum1 = Hillshade_9am +Hillshade_3pm\"\n    ).eval(\n    \"lightsum2 = Hillshade_9am +Hillshade_Noon\"\n    ).eval(\n    \"lightsum3 = Hillshade_Noon +Hillshade_3pm\"\n    ).eval(\n    \"North = ((Aspect >= 0) & (Aspect <=45) ) | ((Aspect>=315)&(Aspect<=360) )\"\n    ).eval(\n    \"East = ((Aspect > 45) & (Aspect <=135) )\"\n    ).eval(\n    \"South = ((Aspect > 135) & (Aspect <=225) )\"\n    ).eval(\n    \"West = ((Aspect > 225) & (Aspect <=315) )\"\n    ).eval(\"direction = North + West*2 + South*3 +East*4\").eval(\n    \"SouthSlope = cos(South*Slope*3.141592653\/180)\"\n    ).eval(\n    \"WestSlope = cos(West*Slope*3.141592653\/180)\"\n    ).eval(\n    \"EastSlope = cos(East*Slope*3.141592653\/180)\"\n    ).eval(\n    \"NorthSlope = cos(North*Slope*3.141592653\/180)\"\n    )\n\nanalysis_db = analyse_db(training_db)","91f57d10":"pd.crosstab(index=analysis_db.Cover_Type,columns=analysis_db.wilderness)","636c0773":"sns.boxplot(x=\"Cover_Type\",y=\"Horizontal_Distance_To_Hydrology\",data=analysis_db)","13a73d16":"sns.boxplot(x=\"Cover_Type\",y=\"Vertical_Distance_To_Hydrology\",data=analysis_db)","fda6545e":"sns.scatterplot(x=\"Elevation\",y=\"Vertical_Distance_To_Hydrology\",hue=\"Grp_Flags\", \n                data=analysis_db)","c04312ff":"\ng = sns.FacetGrid(col=\"wilderness\", data=analysis_db,hue=\"Grp_Flags\")\ng.map(sns.scatterplot, \"Elevation\",\"Horizontal_Distance_To_Hydrology\")\ng.add_legend();\n","ce0be584":"g = sns.FacetGrid(row=\"wilderness\",col=\"Cover_Type\" ,data=analysis_db)\ng.map(sns.boxplot, \"Elevation\")\ng.add_legend();","6503404f":"\n#transformed_training_db.reset_index(inplace=True)\nIds_train,Ids_validate, y_train,y_validate = train_test_split(transformed_training_db['Id'],\n                                                          transformed_training_db['Cover_Type'],\n                                                          train_size=0.8,\n                                                          random_state=42,\n                                                         stratify=transformed_training_db['Cover_Type']\n                                                         ) \n\ntransformed_training_db.set_index('Id',inplace=True)\nX_train=transformed_training_db.drop(columns=['Cover_Type','index'],errors='ignore').loc[Ids_train]\nprint(X_train.head())\nX_validate=transformed_training_db.drop(columns=['Cover_Type','index'],errors='ignore').loc[Ids_validate]\n\n","82839637":"\n# A little biased but I don't think nitpicking will help\ntypes = {}\n\nweights = {}\nfor cover,grp in transformed_training_db.loc[Ids_train].groupby('Cover_Type'):\n     types[cover] = [ 'Soil_Type'+str(soil_type) for soil_type in grp.accumulate.unique()]\n     weights[cover] = grp[types[cover]].sum()\/grp.shape[0]\n\n    \ndef soil_types(df):\n    for cover in types:   \n        df[\"soil_for\"+str(cover)]= (df[types[cover]]*weights[cover]).sum(axis=1)\n        \n    df[\"Low_soils\"] = df[[\"soil_for1\",\"soil_for2\",\"soil_for7\",\"soil_for5\"]].sum(axis=1)\n    df[\"High_soils\"] = df[[\"soil_for4\",\"soil_for3\",\"soil_for6\"]].sum(axis=1)\n    \nsoil_types(X_train)\nsoil_types(X_validate)\nsoil_types(analysis_db)","e21abca5":"#Just for Wilderness Area 3\nW2_train=X_train.Wilderness_Area3.values==1\nW2_validate=X_validate.Wilderness_Area3.values==1\n#high_low_model = LogisticRegressionCV(penalty='l1',random_state=13,solver=\"liblinear\",cv=5,max_iter=400)\n#high_low_model = SVC(probability=True,gamma=1.0e-12)\n#high_low_model = LogisticRegression()\nhigh_low_model = xgb.XGBClassifier(n_estimators=900,n_jobs=4,learning_rate=0.1,colsample_bytree=0.4,max_depth=5)\ny_high_low_train=y_train.replace([5,7,2,1],\"High\").replace([4,3,6],\"Low\").loc[W2_train]\ny_high_low_validate=y_validate.replace([5,7,2,1],\"High\").replace([4,3,6],\"Low\").loc[W2_validate]\nhigh_low_predictors = [\"Elevation\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Hydrology\",\n                       \"High_soils\",\n                       \"Low_soils\",\n                       \"soil_for1\",\n                       \"soil_for2\",\n                       \"soil_for3\",\n                       \"soil_for4\",\n                       \"soil_for5\",\n                       \"soil_for6\",\n                       \"soil_for7\",\n                       \"diff1\",\n                       \"diff2\",\n                       \"diff3\",\n                       \"diff4\"\n                       #,\"Elevation2\"\n                       #,\"interact2\"\n                       #,\"Vertical_Distance_To_Hydrology2\"\n                       #,\"Horizontal_Distance_To_Hydrology2\"\n                       #,\"interact1\"\n                       #,\"interact3\"\n                      ]\nhigh_low_model.fit(X_train.loc[W2_train,high_low_predictors].values,y_high_low_train)\nhigh_low_model.score(X_validate.loc[W2_validate,high_low_predictors].values,y_high_low_validate)\nhigh_low_scores= high_low_model.predict_proba(X_validate.loc[W2_validate,high_low_predictors].values)[:,1]\ntpr,fpr,thresh = roc_curve(y_high_low_validate,\n                           high_low_scores,\n                           pos_label=\"High\") \nplt.plot(fpr,tpr)\nprint(roc_auc_score(y_high_low_validate,\n                           high_low_scores))","68416768":"spreads = pd.DataFrame({'target':y_validate.loc[W2_validate].values,\n'scores':high_low_scores})\nspreads[\"grpd_score\"]=pd.cut(spreads.scores,10)\ng = sns.FacetGrid(col=\"target\", data=spreads,sharey=False,col_wrap=2)\ng.map(sns.distplot, \"scores\")\ng.add_legend();\n","4fde4db7":"#krummholz_model = LogisticRegression()\nkrummholz_model = xgb.XGBClassifier(n_estimators=1400,max_depth=20,n_jobs=4,learning_rate=0.07)\n\nkrummholz_train = y_train.isin([5,7,2,1]).values\nkrummholz_validate = y_validate.isin([5,7,2,1]).values\ny_krummholz_train=  y_train[krummholz_train] ==7 #y_train ==7\ny_krummholz_validate=  y_validate[krummholz_validate] == 7 #y_validate == 7\n\npredictors_krummholz=['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', \n            'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n            'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n            'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', \n            'Wilderness_Area2', 'Wilderness_Area3', \"soil_for1\",\n                       \"soil_for2\",\n                       \"soil_for3\",\n                       \"soil_for4\",\n                       \"soil_for5\",\n                       \"soil_for6\",\n                       \"soil_for7\",\n                        'h_ratio1', 'hratio2', 'hratio3', \n            'diff1', 'diff2', 'diff3', 'diff4', 'sum1', 'sum2', 'sum3', 'sum', \n            'rockoutcropcomplex', 'rubbly', 'Bulwark', 'Leighcan', 'ext_stony', 'stony', 'accumulate',\n            'Wilderness_Area1',\n            'Wilderness_Area2',\n            'Wilderness_Area3',\n            'Wilderness_Area4'\n           ]\nkrummholz_model.fit(X_train.loc[krummholz_train,\n                                predictors_krummholz].values,y_krummholz_train.values)\nkrummholz_model.score(X_validate.loc[krummholz_validate,\n                                     predictors_krummholz].values,\n                      y_krummholz_validate)\n\nscores =  krummholz_model.predict_proba(X_validate.loc[krummholz_validate,\n                                                    predictors_krummholz].values)[:,1]\ntpr,fpr,thresh = roc_curve(y_krummholz_validate,\n                          scores,\n                           pos_label=False\n                          ) \nplt.plot(fpr,tpr)\n\nprint(roc_auc_score(y_krummholz_validate,\n                    scores\n     ))\n\n\n\nspreads = pd.DataFrame({'target':y_krummholz_validate,\n'scores':scores,'raw_target': y_validate[krummholz_validate]})\nspreads[\"grpd_score\"]=pd.cut(spreads.scores,10)\ng = sns.FacetGrid( row='raw_target', data=spreads,sharey=False)\ng.map(sns.distplot, \"scores\")\ng.add_legend();","69ce389c":"Low_predictors = ['Elevation', 'Aspect', 'Slope', \n                  'Horizontal_Distance_To_Hydrology', \n                  'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n                  'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'calc_slope', 'calc_slope2', 'calc_slope3', 'sin_aspect', 'h_ratio1', \n                  'hratio2', 'hratio3', 'diff1', 'diff2', 'diff3', 'diff4', 'sum1', 'sum2', 'sum3', 'sum', \n                  'rockoutcropcomplex', 'rubbly', 'Vanet', 'Bulwark', 'Leighcan', 'ext_stony', 'very_stony', 'stony',\n                   'interact1', 'interact2', 'interact3',\n      'Vertical_Distance_To_Hydrology2',\n       'total_light', 'morning',\n       'noon', 'afternoon', 'lightdiff1', 'lightdiff2', 'lightdiff3',\n       'lightsum1', 'lightsum2', 'lightsum3', 'North',  \n        'SouthSlope', 'WestSlope', 'EastSlope', 'NorthSlope',\n                  \"soil_for1\",\n                       \"soil_for2\",\n                       \"soil_for3\",\n                       \"soil_for4\",\n                       \"soil_for5\",\n                       \"soil_for6\",\n                       \"soil_for7\",\n                  \n                       \n                 #,'Wilderness_Area4'\n                  #, 'Wilderness_Area3'\n                 ]","a556c043":"#Set up datasets \nLow_train_indices = y_train.isin([4,3,6]).values\nLow_validate_indices = y_validate.isin([4,3,6]).values\n\n#Low_predictors = X_train.columns[~(X_train.columns.str.startswith(\"Soil\") | (X_train.columns == \"accumulate\"))]\n\nX_train_low = X_train.loc[Low_train_indices,Low_predictors]\nX_validate_low = X_validate.loc[Low_validate_indices,Low_predictors]\ny_train_low = y_train.loc[Low_train_indices]\ny_validate_low = y_validate.loc[Low_validate_indices]\n\n#Low_model = OneVsOneClassifier(xgb.XGBClassifier(n_estimators=1200,max_depth=10,n_jobs=4,learning_rate=0.05))\n\nLow_model = xgb.XGBClassifier(n_estimators=4000,max_depth=5,n_jobs=4,learning_rate=0.1)\nLow_model = LGBMClassifier(n_estimators=2000,max_depth=5,n_jobs=4,learning_rate=0.1)\n#low_weights = pd.DataFrame({'weight':[10000,1,1000]},index=pd.Index([3,4,6]))\nLow_model.fit(X_train_low.values,y_train_low.values\n              #,sample_weight=low_weights.loc[y_train_low].values\n             )\npredictions = Low_model.predict(X_validate_low.values)\n\nmisclass(y_validate_low,predictions,analyse_db(training_db.loc[Ids_validate].loc[Low_validate_indices]).wilderness.values)\nprint(confusion_matrix(y_validate_low,predictions))\n\n","4400a455":"\nscores = Low_model.predict_proba(X_validate_low.values)\nfor i in range(scores.shape[1]):\n  spreads = pd.DataFrame({\n  'scores':scores[:,i],'raw_target':y_validate_low  })\n  g = sns.FacetGrid( row='raw_target', data=spreads,sharey=False)\n  g.map(sns.distplot, \"scores\")\n  g.add_legend();","263da45b":"low_importances = pd.DataFrame({'importances':Low_model.feature_importances_,\n                             'names':X_train_low.columns})\nlow_importances.sort_values('importances')","a12b2497":"print(low_importances.names[low_importances.importances>0.001].tolist())","eabd422f":"#Set up datasets \n#high_columns=X_train.columns[~X_train.columns.str.startswith('Soil_Type')]\nhigh_columns=['Elevation', 'Aspect', 'Slope',\n              'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', \n              'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', \n              'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', \n              'Wilderness_Area3', 'calc_slope', 'calc_slope2', 'calc_slope3', 'sin_aspect', \n              'h_ratio1', 'hratio2', 'hratio3', 'diff1', 'diff2', 'diff3', 'diff4',\n              'sum1', 'sum2', 'sum3', 'sum', 'rockoutcropcomplex', 'rubbly',\n              'Vanet', 'Bulwark', 'Leighcan', 'ext_stony', 'very_stony', \n              'interact1', 'interact2', 'interact3', 'Vertical_Distance_To_Hydrology2',\n              'total_light', 'morning', 'noon', 'afternoon', 'lightdiff1', 'lightdiff2',\n              'lightdiff3', 'lightsum1', 'lightsum2', 'lightsum3', 'North', 'SouthSlope', \n              'WestSlope', 'EastSlope', 'NorthSlope', 'accumulate', 'soil_for1', 'soil_for2', \n              'soil_for3', 'soil_for4', 'soil_for5', 'soil_for6', 'soil_for7', \n              'Low_soils', 'High_soils']\nHigh_train_indices = y_train.isin([1,2,5]).values\nHigh_validate_indices = y_validate.isin([1,5,2]).values\nX_train_high = X_train.loc[High_train_indices,high_columns]\nX_validate_high = X_validate.loc[High_validate_indices,high_columns]\ny_train_high = y_train.loc[High_train_indices].replace([2],1)\ny_validate_high = y_validate.loc[High_validate_indices].replace([2],1)\n#high_weights = pd.DataFrame({'weight':[1000,1000,1]},index=pd.Index([1,2,5]))\nhigh_weights = {1:1000}#,5:1}\n\nHigh_model = LGBMClassifier(n_estimators=2000,max_depth=10,n_jobs=4,\n                                                  learning_rate=0.1\n                            #,class_weight=high_weights\n                            #objective='multiclass'\n                           )\n\n\nHigh_model.fit(X_train_high.values,y_train_high.values,\n               #sample_weight=high_weights.loc[y_train_high].values\n              )\npredictions = High_model.predict(X_validate_high.values)\n\n\nmisclass(y_validate_high,predictions,analyse_db(training_db.loc[Ids_validate].loc[High_validate_indices]).wilderness.values)\n\n","312c04d7":"scores = High_model.predict_proba(X_validate_high.values)\nfor i in range(scores.shape[1]):\n  spreads = pd.DataFrame({\n  'scores':scores[:,i],'raw_target':y_validate_high  })\n  g = sns.FacetGrid( row='raw_target', data=spreads,sharey=False)\n  g.map(sns.distplot, \"scores\")\n  g.add_legend();","e6d086a9":"tpr,fpr,thresh = roc_curve(y_validate_high,\n                           scores[:,0],\n                           pos_label=5\n                          ) \nplt.plot(fpr,tpr)\nprint(roc_auc_score(y_validate_high,\n                           scores[:,1],\n     ))","ee714b03":"high_importances = pd.DataFrame({'importances':High_model.feature_importances_,\n                             'names':X_train_high.columns})\nhigh_importances.sort_values('importances')","7869a327":"print(high_importances.names[high_importances.importances>0].to_list())","92981c46":"keep_preds=['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n            'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n            'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', \n            'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', \n            'calc_slope', 'calc_slope2', 'calc_slope3', 'sin_aspect', 'h_ratio1', 'hratio2', 'hratio3', 'diff1', 'diff2', 'diff3', 'diff4', 'sum1', 'sum2', 'sum3', 'sum', 'Leighcan', 'ext_stony', 'interact1', 'interact2', 'interact3', 'Vertical_Distance_To_Hydrology2', 'total_light', 'morning', 'noon', 'afternoon', 'lightdiff1', 'lightdiff2', 'lightdiff3', 'lightsum1', 'lightsum2', 'lightsum3', 'SouthSlope', 'WestSlope', 'EastSlope', 'NorthSlope', 'accumulate', 'soil_for1', 'soil_for2', 'soil_for3', 'soil_for5', 'soil_for6', 'soil_for7', 'Low_soils', 'High_soils']","7c12b4bb":"#Set up datasets \npine_train_indices = (y_train.isin([1,2]).values ) & (X_train.Wilderness_Area4 == 0 ).values\npine_validate_indices = (y_validate.isin([1,2]).values) & (X_validate.Wilderness_Area4 == 0 ).values\n#Cover_Type2 in Wilderness Area 4 looks wrong\nX_train_pine = X_train.loc[pine_train_indices]\nX_validate_pine = X_validate.loc[pine_validate_indices]\ny_train_pine = y_train.loc[pine_train_indices]\ny_validate_pine = y_validate.loc[pine_validate_indices]\n\n#pine_model = xgb.XGBClassifier(n_estimators=4000,max_depth=12,n_jobs=4,learning_rate=0.01\n#                               )\npine_model = LGBMClassifier(n_estimators=4000,max_depth=5,n_jobs=4,learning_rate=0.1\n                               )\n\n#High_model = OneVsOneClassifier(RandomForestClassifier(n_estimators=1600,n_jobs=4,oob_score=True))\n#predictors = [\"diff1\",\"stony\",\"Hillshade_Noon\",\"Leighcan\",\"Wilderness_Area2\",\n#              \"Wilderness_Area1\",\"Wilderness_Area3\",\"Wilderness_Area4\",\n#             \"Bulwark\",\"Horizontal_Distance_To_Roadways\"\n#             ] + X_train_pine.columns[X_train.columns.str.startswith(\"Soil\")].values.tolist()\n#pine_predictors = X_train.columns[~X_train.columns.str.startswith('Soil_Type')]\npine_predictors=keep_preds\n#predictors = X_train_pine.columns.values\npine_model.fit(X_train_pine[pine_predictors].values,y_train_pine.values)\npredictions = pine_model.predict(X_validate_pine[pine_predictors].values)\n\n\n","a8e45c51":"\n\nmisclass(y_validate_pine,predictions,analyse_db(training_db.loc[Ids_validate].loc[pine_validate_indices]).wilderness.values)\nprint(confusion_matrix(y_validate_pine,predictions))\nscores =  pine_model.predict_proba(X_validate_pine[#.loc[krummholz_validate,\n                                                    pine_predictors].values)[:,0]\n\ntpr,fpr,thresh = roc_curve(y_validate_pine,\n                          scores,\n                           pos_label=2\n                          ) \nplt.plot(fpr,tpr)\n\nprint(roc_auc_score(y_validate_pine,\n                    1-scores\n     ))\n\n","d0a0ec91":"importances = pd.DataFrame({'importances':pine_model.feature_importances_,\n                             'names':pine_predictors})\nimportances.sort_values('importances')","bf5ee556":"keep_preds = importances.names[importances.importances >30]","c70bd1ef":"print(keep_preds.to_list())","0725f7bf":"print(X_validate.eval(\"wilderness= Wilderness_Area1 + 2*Wilderness_Area2 +3*Wilderness_Area3 + 4*Wilderness_Area4\",inplace=True))","a26b8e8f":"def  score_dset(df,high_low_cutoff=0.45,krummholz_cutoff=0.98,cutoff_5=0.98):\n    results = df.copy()\n    results[\"HighvsLow\"] = 'High'\n    results.loc[(df.Wilderness_Area4==1).values,\"HighvsLow\"] = \"Low\"\n    high_low_flag = high_low_model.predict_proba(df[high_low_predictors].values)[:,0]> high_low_cutoff\n    print(high_low_flag.shape)\n    results.loc[(df.Wilderness_Area3==1).values & high_low_flag] = \"High\" \n    results[\"Krummholz\"] = krummholz_model.predict_proba(df[predictors_krummholz].values)[:,1]\n    results.loc[(df.Wilderness_Area4==1).values,\"Krummholz\"] =0.0\n    results[\"Low\"] = Low_model.predict(df[X_train_low.columns].values)\n    results[\"High\"] = High_model.predict_proba(df[X_train_high.columns].values)[:,1]\n    results[\"Pine\"] = pine_model.predict(df[pine_predictors].values)\n    results[\"final_result\"] = results.Low\n    \n    results.loc[(results.HighvsLow =='High') & (results.Krummholz>=krummholz_cutoff),\"final_result\"] = 7\n    #results.loc[(results.HighvsLow =='High') & (results.Krummholz>=0.5),\"final_result\"] = results.High\n    #results.loc[(results.HighvsLow =='Low'),\"final_result\"] = results.Low\n    \n    results.loc[(results.HighvsLow =='High') & (results.Krummholz<krummholz_cutoff) & (results.High > cutoff_5) ,\n                \"final_result\"] = 5\n    results.loc[(results.HighvsLow =='High')  & (results.Krummholz<krummholz_cutoff) & (results.High <= cutoff_5) \n                ,\"final_result\"] = results.Pine\n  \n    return results\n\n\nresult = score_dset(X_validate)\n\n\nmisclass(y_validate.values,result.final_result.values,X_validate.wilderness)\nclass_weights=np.array([0,0.5,0.4,0.1,0.0,0.1,0.1,0.0,0.0])\nprint(class_weights[y_validate.values])\nmax_score=0 \nfor kc in np.linspace(0.9,0.98,5):\n    for hlc in np.linspace(0.3,0.7,8):\n        for c5 in np.linspace(0.9,0.98,8):        \n            result = score_dset(X_validate,krummholz_cutoff=kc,high_low_cutoff=hlc,cutoff_5=c5)\n            print(kc,hlc,c5)\n            acc=accuracy_score(y_validate.values,result.final_result.values,sample_weight=class_weights[y_validate.values])\n            if (acc >max_score):\n                max_score=acc\n                max_state={'krummholz_cutoff':kc,'high_low_cutoff':hlc,'cutoff_5':c5}\n                print(acc,max_state)","78940968":"test_db = pd.read_csv(\"\/kaggle\/input\/learn-together\/test.csv\",index_col='Id')\nprediction_db = transform_db(test_db)\nsoil_types(prediction_db)\n#prediction_db = leave1out.transform(transform_db(test_db))\nprediction_db.head()","cdc7b891":"result_test = score_dset(prediction_db[X_validate.drop(columns=['wilderness']).columns],**max_state)","41f60723":"predictions = result_test.final_result","77b8a920":"ids = prediction_db.index.values","67270871":"output_frame = pd.DataFrame({'Id':ids,'Cover_Type':predictions})","a3c4d7f6":"weights = output_frame.Cover_Type.value_counts()\/output_frame.shape[0]\nprint(weights)","cbbd357d":"pd.crosstab(index=analysis_test.Cover_Type,columns=analysis_test.wilderness)","c35f24bc":"output_frame.head()","024ef067":"output_frame.to_csv('submission.csv',index=False)","488be23f":"# Differentiate High and Low groups","d525effa":"Cover types \"c5\",\"c7\",\"c2\",\"c1\" vs  \"c3\",\"c6\",\"c4\" is easy (Elevation, Distance to Hydrology)\n\nCover type  \"c7\" vs \"c5\",\"c2\",\"c1\" based on Elevation should be easy.(Elevation)\n\nThen need to differentiate \"c3\",\"c6\",\"c4\" and \"c5\",\"c2\",\"c1\"\n","8d23f7ba":"# Differentiate between 5 and pines (1,2) within High ","f290389d":"# Generate submission","50957b15":"# Feature generation","7057ba40":"# Strategy\n\nWhile I could just build the build the best machine learning model possible, it doesn't give a way forward.\nMy way forward is:\n* Understand the data\n* Identify high level groupings\n* Identify easy and problematic groups within the high level groupings\n* Work on separating the problematic groups","3437ff46":"# Manual adjustments","4e850949":"# Within \"High\" Group distinguish Krumholz from everything else","737ad456":"# Differentiate between 4,3,6 within Low","c4bfd51c":"# Pines - 1 vs 2","32c67651":"# Split into training and validation","0ee8352b":"So \"w1\" and w3 have no Low group\nW8 has no high \nw4 (Wilderness_Area2) has mixed ","79c43240":"# Soil weights","83c022a6":"# EDA"}}