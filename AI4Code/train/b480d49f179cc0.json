{"cell_type":{"efd5e730":"code","5d1dbb87":"code","82db07ea":"code","722a9a6a":"code","68c3e56f":"code","ced16ab1":"code","8cabbeac":"code","3dea5e87":"code","c82186c4":"code","beb264cc":"code","21a539e5":"markdown","11ce0d01":"markdown"},"source":{"efd5e730":"from pandas import Series, DataFrame\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pylab as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split","5d1dbb87":"def one_d_2_d_mul(a, b):\n    c = np.zeros(b.shape[1])\n    for i in range(len(c)):\n        for j in range(len(a)):\n            print(a[j])\n            c[i] += a[j] * b[j, i]\n            \n    return c","82db07ea":"def activation(x, act):\n    if (act == 'sigmoid') :\n        x = 1\/(1 + np.exp((-0.4)*x))\n    elif (act == 'tanh'):\n        x = (2\/(1 + np.exp((-2)*x))) - 1\n    elif (act == 'RLE'):\n        x[x<0] = 0\n    return x","722a9a6a":"def one_d_sub(a, b):\n    c = np.zeros(len(b))\n    for i in range(len(a)):\n        c[i] = a[i] - b[i]    \n    return c ","68c3e56f":"def scalar_multiplication(scalar, arr):\n    c = np.zeros(len(arr))\n    for i in range(len(c)):\n        c[i] = arr[i] * scalar\n            \n    return c","ced16ab1":"def partial_derivative(error, y, x, learning_rate, act):\n    if(act == 'sigmoid'):\n        c = scalar_multiplication(x * learning_rate, y)\n        c = np.multiply(c, 1 - y)\n        c = np.multiply(error, c)\n    elif(act == 'tanh'):\n        y_2 = y**2\n        c = scalar_multiplication(x * learning_rate, 1 - y_2)\n        c = np.multiply(error, c)\n    elif (act == 'RLE'):\n        if(x < 0):\n            c = scalar_multiplication(0, error)\n        else:\n            c = scalar_multiplication(x * learning_rate, error)\n    return c","8cabbeac":"class neural_network:\n    def __init__(self, input_parameters, output_parameters, hidden_node_count, hidden_layers, activation_function):\n        self.number_of_input_parameters      = input_parameters\n        self.number_of_output_parameters     = output_parameters\n        self.number_of_nodes_in_hidden_layer = hidden_node_count\n        self.number_of_hidden_layers         = hidden_layers\n        self.act                             = activation_function\n        self.input_weight_matrix = \\\n                             np.random.rand( \\\n                                  self.number_of_input_parameters, \\\n                                  self.number_of_nodes_in_hidden_layer\n                                  ) - 0.5\n        self.hidden_layer_weight_matrix = \\\n                              np.random.rand(\\\n                                  self.number_of_nodes_in_hidden_layer, \\\n                                  self.number_of_nodes_in_hidden_layer, \\\n                                  self.number_of_hidden_layers -1) - 0.5\n\n        self.output_weight_matrix = \\\n                              np.random.rand(\\\n                                  self.number_of_nodes_in_hidden_layer, \\\n                                  self.number_of_output_parameters) - 0.5\n        self.threshold           = 0.5\n    \n    def query(self, input_data):\n        hidden_nodes        = np.zeros(shape = ( \\\n                                        self.number_of_hidden_layers, \\\n                                        self.number_of_nodes_in_hidden_layer))\n        if((not  input_data.shape[1] == None) and input_data.shape[1] > 0):\n            output_array    = np.zeros(shape = len(input_data))\n        else:\n            output_array    = np.zeros(shape = 1)\n        \n        for input_idx in range(len(input_data)):\n            input_nodes                     = input_data[input_idx, :]\n            target                          = output_data[input_idx]\n\n            #feedforward\n            hidden_nodes[0, :] = np.matmul(input_nodes, self.input_weight_matrix)\n            hidden_nodes[0, :] = activation(hidden_nodes[0, :], 'sigmoid')\n\n            for i in range(1, self.number_of_hidden_layers, 1):\n                hidden_nodes[i, :] = \\\n                                np.matmul(hidden_nodes[i - 1, :], \\\n                                          self.hidden_layer_weight_matrix[:, :, i-1])\n                hidden_nodes[i, :] = activation(hidden_nodes[i, :], self.act)\n\n            output_nodes = np.matmul(hidden_nodes[self.number_of_hidden_layers - 1, :],\\\n                                     self.output_weight_matrix)\n            sigmoid_output = activation(output_nodes, \"sigmoid\")\n\n            if(output_nodes > self.threshold):\n                output_array[input_idx] = 1\n            else:\n                output_array[input_idx] = 0\n        \n        return output_array\n    \n    def train(self, input_data, output_data, epochs, learning_rate):\n        hidden_nodes        = np.zeros(shape = ( \\\n                                        self.number_of_hidden_layers, \\\n                                        self.number_of_nodes_in_hidden_layer))\n        hidden_layer_error = np.zeros(shape = ( \\\n                                        self.number_of_hidden_layers, \\\n                                        self.number_of_nodes_in_hidden_layer))\n        input_layer_error = np.zeros(shape = self.number_of_input_parameters)\n        \n        for epoch in range(epochs):\n            for input_idx in range(len(input_data)):\n                input_nodes                     = input_data[input_idx, :]\n                target                          = output_data[input_idx]\n\n                #feedforward\n                hidden_nodes[0, :] = np.matmul(input_nodes, self.input_weight_matrix)\n                hidden_nodes[0, :] = activation(hidden_nodes[0, :], 'sigmoid')\n\n                for i in range(1, self.number_of_hidden_layers, 1):\n                    hidden_nodes[i, :] = \\\n                                    np.matmul(hidden_nodes[i - 1, :], \\\n                                              self.hidden_layer_weight_matrix[:, :, i-1])\n                    hidden_nodes[i, :] = activation(hidden_nodes[i, :], self.act)\n\n                output_nodes = np.matmul(hidden_nodes[self.number_of_hidden_layers - 1, :],\\\n                                         self.output_weight_matrix)\n                sigmoid_output = activation(output_nodes, 'sigmoid')\n\n                if(output_nodes > self.threshold):\n                    output_nodes = 1\n                else:\n                    output_nodes = 0\n\n                ### back propogation\n\n                ##error calculation\n                output_error = (output_nodes - target)\n\n                hidden_layer_error[self.number_of_hidden_layers -1, :] = \\\n                                                      scalar_multiplication( \\\n                                                        output_error, \\\n                                                        self.output_weight_matrix)\n\n                for i in range(self.number_of_hidden_layers - 2, -1, -1):\n                    hidden_layer_error[i, :] = np.matmul( \\\n                                                         self.hidden_layer_weight_matrix[:, :, i], \\\n                                                         hidden_layer_error[i+1, :])\n                \n                ## weight update\n                \n                #output layer\n                self.output_weight_matrix = one_d_sub(self.output_weight_matrix, \\\n                                                      scalar_multiplication( \\\n                                                                            output_error * sigmoid_output * \\\n                                                                            (1 - sigmoid_output) * learning_rate, \\\n                                                                            hidden_nodes[self.number_of_hidden_layers - 1, :]))\n                #hidden layers\n                for layer_idx in range(self.number_of_hidden_layers - 1):\n                    for node_idx in range(self.number_of_nodes_in_hidden_layer):\n                        self.hidden_layer_weight_matrix[node_idx, :, layer_idx] = \\\n                                                        one_d_sub( \\\n                                                                  self.hidden_layer_weight_matrix[node_idx, :, layer_idx], \\\n                                                                  partial_derivative(\\\n                                                                        hidden_layer_error[layer_idx + 1, :], \\\n                                                                        hidden_nodes[layer_idx + 1, :], \\\n                                                                        hidden_nodes[layer_idx, node_idx], \\\n                                                                        learning_rate, \\\n                                                                        self.act))\n                #input layer\n                for idx in range(self.number_of_input_parameters):\n                    self.input_weight_matrix[idx, :] = one_d_sub( \\\n                                                                 self.input_weight_matrix[idx, :], \\\n                                                                 partial_derivative(\\\n                                                                        hidden_layer_error[0, :], \\\n                                                                        hidden_nodes[0, :], \\\n                                                                        input_nodes[idx], \\\n                                                                        learning_rate, \\\n                                                                        'sigmoid'))\n    def print_nerual_net(self):\n        print(\"input weight\", self.input_weight_matrix)\n        print(\"hidden weight\", self.hidden_layer_weight_matrix)\n        print(\"output weight\", self.output_weight_matrix)","3dea5e87":"validation_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ntrain_data = train_data.fillna(-1)\nvalidation_data = validation_data.fillna(-1)\n\ntrain_data = train_data.replace('male', 0)\ntrain_data = train_data.replace('female', 1)\n\nvalidation_data = validation_data.replace('male', 0)\nvalidation_data = validation_data.replace('female', 1)\n\ntrain_data, test_data = train_test_split(train_data, test_size = 0.2)\n\nprint(train_data.head())","c82186c4":"explanatory_variables = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']\nresponse_variables = ['Survived']\n\ninput_data = train_data[explanatory_variables].to_numpy()\noutput_data = train_data[response_variables].to_numpy()\n\nvalidation_input_data = validation_data[explanatory_variables].to_numpy()\ntest_input_data = test_data[explanatory_variables].to_numpy()\n\ntest_output_data = test_data['Survived'].to_numpy()\n\n#preprocessing of the data\nfor input_parameter in range(input_data.shape[1]):\n    input_data[:, input_parameter] = input_data[:, input_parameter]\/max(input_data[:, input_parameter])\n    test_input_data[:, input_parameter] = test_input_data[:, input_parameter]\/max(test_input_data[:, input_parameter]) \n    validation_input_data[:, input_parameter] = \\\n                             validation_input_data[:, input_parameter]\/ \\\n                                max(validation_input_data[:, input_parameter])\n    ","beb264cc":"fig, plots = plt.subplots(4)\nepoch_step = 50\nepoch_list = [* range(50, 300, epoch_step)]\naccuracy_sigmoid = np.zeros(shape = len(epoch_list))\naccuracy_tanh = np.zeros(shape = len(epoch_list))\naccuracy_RLE = np.zeros(shape = len(epoch_list))\n\nfor plott in plots:\n    neural_net_sigmoid = neural_network(input_parameters = 5, output_parameters = 1,\\\n                                hidden_node_count = 2, hidden_layers = 2, activation_function = 'sigmoid')\n    neural_net_tanh = neural_network(input_parameters = 5, output_parameters = 1,\\\n                                hidden_node_count = 2, hidden_layers = 2, activation_function = 'tanh')\n    neural_net_RLE = neural_network(input_parameters = 5, output_parameters = 1,\\\n                                hidden_node_count = 2, hidden_layers = 2, activation_function = 'RLE')\n\n    for idx in range(len(epoch_list)):\n        neural_net_RLE.train(input_data = input_data, output_data = output_data, \\\n                                       learning_rate = 0.3, epochs = epoch_step)\n        predictions = neural_net_RLE.query(test_input_data)\n        accuracy_RLE[idx] = sklearn.metrics.accuracy_score(test_output_data, predictions)\n        \n        \n        neural_net_sigmoid.train(input_data = input_data, output_data = output_data, \\\n                                       learning_rate = 0.03, epochs = epoch_step)\n        predictions = neural_net_sigmoid.query(test_input_data)\n        accuracy_sigmoid[idx] = sklearn.metrics.accuracy_score(test_output_data, predictions)\n\n        \n        neural_net_tanh.train(input_data = input_data, output_data = output_data, \\\n                                       learning_rate = 0.03, epochs = epoch_step)\n        predictions = neural_net_tanh.query(test_input_data)\n        accuracy_tanh[idx] = sklearn.metrics.accuracy_score(test_output_data, predictions)\n\n    plott.plot(epoch_list, accuracy_sigmoid, label = 'sigmoid')\n    plott.plot(epoch_list, accuracy_tanh, label = 'tanh')\n    plott.plot(epoch_list, accuracy_RLE, label = 'RLE')\n\nplt.legend(loc=\"upper left\") \nplt.show()\nplt.savefig('foo.png')","21a539e5":"# Introduction to data\n\nUnderstanding each variable.","11ce0d01":"## Lessons learnt\n\n### Writing clean code\nMaking class for neural network reduced time for coding and made life much easier.\n\n### Why only 2 hidden layers and 2 hidden nodes?\nMore neurons do NOT mean more accuracy.\nRan multiple iterations with different number of neurons\n\nThe number of features in this data is few. Therefore, I used few number of neurons.\n\n### Perprocessing data is very important\nI didn't scale my input in the start this lead to following issues:\n1. Saturation of weights when using sigmoid activation function\n2. Calculation time was too much\n3. Exp function started having overflow issues\n4. biasing --> inputs with higher values were getting more importance\n\n### Activation fucntions\nAll the activation functions are performing similar because number of features is very less.\n"}}