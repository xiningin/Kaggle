{"cell_type":{"ba339641":"code","74914d3b":"code","08a4136e":"code","9371f87d":"code","13b316b7":"code","0cc251fd":"code","417ec06d":"code","9d1ea059":"code","cf300a4f":"code","e2cdd211":"code","53a56f87":"code","c7d4ab3d":"code","9841c8da":"code","ddda99a0":"code","999c5ed8":"code","8c0ca89d":"code","e2d9f0fe":"code","e07c8b76":"code","962b9c47":"markdown","f997c175":"markdown","f7439d0e":"markdown","3d870bd6":"markdown","63adf761":"markdown","f39ecb30":"markdown","4cec0f39":"markdown","d734adae":"markdown","cba79f2a":"markdown","17ff493c":"markdown","a915b4d5":"markdown"},"source":{"ba339641":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.layers import *\nfrom keras.models import Model\n\n\n# For reproducibility.\nseed = 7\nnp.random.seed(seed)\ntf.set_random_seed(seed)\nsession_conf = tf.ConfigProto(\n    intra_op_parallelism_threads=1,\n    inter_op_parallelism_threads=1\n)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","74914d3b":"DATA_DIR = '..\/input\/movie-review-sentiment-analysis-kernels-only'\ntrain_file = os.path.join(DATA_DIR, 'train.tsv')\ntest_file  = os.path.join(DATA_DIR, 'test.tsv')\ndf_train = pd.read_table(train_file)\ndf_test  = pd.read_table(test_file)","08a4136e":"EMBEDDING_FILE =  '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n\ndef load_embeddings(filename):\n    embeddings = {}\n    with open(filename) as f:\n        for line in f:\n            values = line.rstrip().split(' ')\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')   \n            embeddings[word] = vector # Ex ~> word: 12321 12331 13454 2323\n    return embeddings\n\nembeddings = load_embeddings(EMBEDDING_FILE)","9371f87d":"df_train.Phrase = df_train.Phrase.str.replace(\"n't\", 'not')\ndf_test.Phrase = df_test.Phrase.str.replace(\"n't\", 'not')","13b316b7":"df_train.Phrase = df_train.Phrase.apply(lambda x: re.sub(r'[0-9]+', '0', x))\ndf_test.Phrase = df_test.Phrase.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n\nx_train = df_train['Phrase'].values\nx_test  = df_test['Phrase'].values\ny_train = df_train['Sentiment'].values\nx = np.r_[x_train, x_test]","0cc251fd":"print(x_train[0])\nprint(y_train[0])","417ec06d":"tokenizer = Tokenizer(lower=True, filters='\\n\\t')\ntokenizer.fit_on_texts(x)\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test  = tokenizer.texts_to_sequences(x_test)\nvocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\nprint('vocabulary size: {}'.format(vocab_size))","9d1ea059":"maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\nprint('maxlen: {}'.format(maxlen))\nprint(x_train.shape)\nprint(x_test.shape)","cf300a4f":"a = (embeddings.get('god'))\na.shape","e2cdd211":"def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n    embedding_matrix = np.zeros([vocab_size, dim])  # 2000000 * 300 Matrix\n    for word, i in word_index.items():\n        if i >= vocab_size:\n            continue\n        vector = embeddings.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector\n    return embedding_matrix # Return 19262 * 300\n\nembedding_size = 300\nembedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n                                     vocab_size, embedding_size)\nprint('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings)))) # Out of Vocab","53a56f87":"embedding_matrix.shape","c7d4ab3d":"class Attention(Layer):\n    \"\"\"\n    Keras Layer that implements an Attention mechanism for temporal data.\n    Supports Masking.\n    Follows the work of Raffel et al. [https:\/\/arxiv.org\/abs\/1512.08756]\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Attention())\n    \"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","9841c8da":"def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n    input_words = Input((maxlen, ))\n    x_words = Embedding(vocab_size,\n                        embedding_size,\n                        weights=[embedding_matrix],\n                        mask_zero=True,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(0.3)(x_words)\n    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n    x = Attention(maxlen)(x_words)\n    x = Dropout(0.2)(x)\n    x = Dense(50, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    pred = Dense(5, activation='softmax')(x)\n\n    model = Model(inputs=input_words, outputs=pred)\n    return model\n\n\n","ddda99a0":"from keras import optimizers\nmodel = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\nmodel.compile(optimizer=optimizers.RMSprop(lr = 0.004 , decay = 0.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n# model.summary()","999c5ed8":"save_file = 'model1.h5'\nhistory = model.fit(x_train, y_train,\n                    epochs=30, verbose=1,\n                    batch_size=2048, shuffle=True,)","8c0ca89d":"from keras.models import load_model","e2d9f0fe":"y_pred = model.predict(x_test, batch_size=512)\ny_pred = y_pred.argmax(axis=1).astype(int)\ny_pred.shape","e07c8b76":"mapping = {phrase: sentiment for _, _, phrase, sentiment in df_train.values}\n\n# Overlapping\nfor i, phrase in enumerate(df_test.Phrase.values):\n    if phrase in mapping:\n        y_pred[i] = mapping[phrase]\n\ndf_test['Sentiment'] = y_pred\ndf_test[['PhraseId', 'Sentiment']].to_csv('submission.csv', index=False)","962b9c47":"## Zero padding","f997c175":"# Training the model","f7439d0e":"# Preprocessings\n\nAfter loading the datasets, we will preprocess the datasets. In this time, we will apply following techniques:\n\n* Nigation handling\n* Replacing numbers\n* Tokenization\n* Zero padding\n\nNegation handling is the process of converting negation abbreviation to a canonical format. For example, \"aren't\" is converted to \"are not\". It is helpful for sentiment analysis.\n\nReplacing numbers is the process of converting numbers to a specific character. For example, \"1924\" and \"123\" are converted to \"0\". It is helpful for reducing the vocabulary size.\n\nTokenization is the process of taking a text or set of texts and breaking it up into its individual words. In this step, we will tokenize text with the help of splitting text by space or punctuation marks.\n\nZero padding is the process of pad \"0\" to the dataset for the purpose of ensuring that all sentences has the same length.","3d870bd6":"# Making a submission file\n\nAfter training the model, we make a submission file by predicting for the test dataset.","63adf761":"# Building a model\n\nIn this time, we will use attention based LSTM model. First of all, we should define the attention layer as follows:","f39ecb30":"## Replacing numbers","4cec0f39":"In addition to the datasets, we load a pretrained word embeddings.","d734adae":"## Negation handling","cba79f2a":"After defining the attention layer, we will define the entire model:","17ff493c":"## Tokenization","a915b4d5":"# Data Loading\n\nwe load the dataset and apply some transformations to use it in a deep learning model."}}