{"cell_type":{"cb7d25ae":"code","06ab1b92":"code","9c1d8bb5":"code","0b1e6c0e":"code","1aceceb2":"code","5441f821":"code","4ea3f0d3":"code","be8fdb0d":"code","b773dcfd":"code","88f63b62":"code","8087e81e":"code","cdf66518":"code","c2e7200b":"code","a89ff660":"code","1bdd39f3":"code","6a80935e":"code","46861c07":"code","fe032b66":"code","e7d17c8c":"code","52bacc30":"code","108e7790":"code","9da52847":"code","5c711f74":"code","301462ad":"code","aa053d82":"code","67401eb6":"code","9844da23":"code","119bcecb":"code","bc49cc6d":"code","375bc56e":"code","aa96a8e3":"code","8fee0b3e":"code","1b43f00a":"code","d65e8469":"code","43da620c":"code","a5234445":"code","a7af1db0":"code","b4587c45":"code","7d964c07":"code","0cabd0fd":"code","4db8ec6c":"code","55278a2a":"code","80d02518":"code","299a1412":"code","7699e637":"code","21a79ba4":"code","8e6e5b9d":"code","80078412":"code","69774909":"code","4947f371":"markdown","2fe3ec5e":"markdown","5a735b25":"markdown","e632cd4f":"markdown","9984724a":"markdown","a2cd15b7":"markdown","225634c2":"markdown","f1127af2":"markdown","09a8dbbd":"markdown","360732cc":"markdown","6fa8beae":"markdown","cbb5918a":"markdown","1285f380":"markdown","29f21f24":"markdown","bbdde7db":"markdown","9be3c1b4":"markdown","9775d93d":"markdown","bd36ede9":"markdown","10a70eab":"markdown","50089a6f":"markdown"},"source":{"cb7d25ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport xgboost as xg\nimport tables as tb\nfrom tqdm import tqdm\nfrom itertools import cycle, islice\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import make_blobs\nfrom IPython import display\nfrom sklearn.neighbors import BallTree, KDTree, DistanceMetric\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import Normalizer\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom keras.layers.core import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nimport utils\n\n%matplotlib inline\n%pylab inline\n\n# Make the images larger\nplt.rcParams['figure.figsize'] = (16, 9)\nfigsize = (10,10)\npoint_size=150\npoint_border=0.8\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","06ab1b92":"def my_percentile(arr, w, q):\n    left = 0.\n    right = (w).sum()\n    sort_inds = np.argsort(arr, axis=0)\n    if left\/right >= q\/100.:\n        return arr[0]\n    for i in sort_inds:\n        left += w[i]\n        if left\/right >= q\/100.:\n            return arr[i]\n\ndef plot(prediction, spectator, cut, percentile=True, weights=None, n_bins=100,\n              color='b', marker='o', ms=4, label=\"MVA\", fmt='o', markeredgecolor='b', markeredgewidth=2, ecolor='b'):\n    if weights is None:\n        weights = np.ones(len(prediction))\n\n    if percentile:\n        if weights is None:\n            cut = np.percentile(prediction, 100-cut)\n        else:\n            cut = my_percentile(prediction, weights, 100-cut)\n    \n    edges = np.linspace(spectator.min(), spectator.max(), n_bins)\n    \n    xx = []\n    yy = []\n    xx_err = []\n    yy_err = []\n    \n    for i_edge in range(len(edges)-1):\n\n        left = edges[i_edge]\n        right = edges[i_edge + 1]\n        \n        N_tot_bin = weights[((spectator >= left) * (spectator < right))].sum()\n        N_cut_bin = weights[((spectator >= left) * (spectator < right) * (prediction >= cut))].sum()\n        \n        if N_tot_bin != 0:\n            \n            x = 0.5 * (right + left)\n            y = 1. * N_cut_bin \/ N_tot_bin\n            \n            if y > 1.:\n                y = 1.\n            if y < 0:\n                y = 0\n            \n            xx.append(x)\n            yy.append(y)\n            \n            x_err = 0.5 * (right - left)\n            y_err = np.sqrt(y*(1-y)\/N_tot_bin)\n            \n            xx_err.append(x_err)\n            yy_err.append(y_err)\n        \n        else:\n            pass\n\n    plt.errorbar(xx, yy, yerr=yy_err, xerr=xx_err, fmt=fmt, color=color, marker=marker, ms=ms, label=label, markeredgecolor=markeredgecolor, markeredgewidth=markeredgewidth, ecolor=ecolor)\n    \n    return cut","9c1d8bb5":"# Get numeric labels for each of the string labels, to make them compatible with our model\nlabel_class_correspondence = {'Electron': 0, 'Ghost': 1, 'Kaon': 2, 'Muon': 3, 'Pion': 4, 'Proton': 5}\nclass_label_correspondence = {0: 'Electron', 1: 'Ghost', 2: 'Kaon', 3: 'Muon', 4: 'Pion', 5: 'Proton'}\n\ndef get_class_ids(labels):\n    return np.array([label_class_correspondence[alabel] for alabel in labels])","0b1e6c0e":"def roc_curves(predictions, labels):\n    plt.figure(figsize=(9, 6))\n    u_labels = np.unique(labels)\n    for lab in u_labels:\n        y_true = labels == lab\n        y_pred = predictions[:, lab]\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        auc = roc_auc_score(y_true, y_pred)\n        plt.plot(tpr, 1-fpr, linewidth=3, label=class_label_correspondence[lab] + ', AUC = ' + str(np.round(auc, 4)))\n        plt.xlabel('Signal efficiency (TPR)', size=15)\n        plt.ylabel(\"Background rejection (1 - FPR)\", size=15)\n        plt.xticks(size=15)\n        plt.yticks(size=15)\n        plt.xlim(0., 1)\n        plt.ylim(0., 1)\n        plt.legend(loc='lower left', fontsize=15)\n        plt.title('One particle vs rest ROC curves', loc='right', size=15)\n        plt.grid(b=1)","1aceceb2":"def efficiency(predictions, labels, spectator, eff=60, n_bins=20, xlabel='Spectator'):\n    plt.figure(figsize=(5.5*2, 3.5*3))\n    u_labels = np.unique(labels)\n    for lab in u_labels:\n        y_true = labels == lab\n        pred = predictions[y_true, lab]\n        spec = spectator[y_true]\n        plt.subplot(3, 2, lab+1)\n        plot(pred, spec, cut=eff, percentile=True, weights=None, n_bins=n_bins, color='1', marker='o', \n                  ms=7, label=class_label_correspondence[lab], fmt='o')\n        \n        plt.plot([spec.min(), spec.max()], [eff \/ 100., eff \/ 100.], label='Global signal efficiecny', color='r', linewidth=3)\n        plt.legend(loc='best', fontsize=12)\n        plt.xticks(size=12)\n        plt.yticks(size=12)\n        plt.ylabel('Signal efficiency (TPR)', size=12)\n        plt.xlabel(xlabel,size=12)\n        plt.ylim(0, 1)\n        plt.xlim(spec.min(), spec.max())\n        plt.grid(b=1)\n    plt.tight_layout()\n        \n\ndef efficiency_on_p(predictions, labels, spectator, eff=60, n_bins=20):\n    sel = spectator < 200 * 10**3\n    efficiency(predictions[sel], labels[sel], spectator[sel] \/ 10**3, eff, n_bins, 'Momentum, GeV\/c')","5441f821":"def efficiency_on_pt(predictions, labels, spectator, eff=60, n_bins=20):\n    sel = spectator < 10 * 10**3\n    efficiency(predictions[sel], labels[sel], spectator[sel] \/ 10**3, eff, n_bins, 'Transverse momentum, GeV\/c')","4ea3f0d3":"def nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(100, input_dim=input_dim))\n    model.add(Activation('tanh'))\n\n    model.add(Dense(6))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n    return model","be8fdb0d":"# release unreferenced memory to ensure we don't run out of memory\nimport gc\ngc.collect()","b773dcfd":"train = pd.read_hdf('..\/input\/dark-matter-from-opera-experiments\/open30.h5') # pick just a single brick\ntest = pd.read_hdf('..\/input\/dark-matter-from-opera-experiments\/test.h5')","88f63b62":"train.shape","8087e81e":"test = test.reset_index(drop=True)\ntest.shape","cdf66518":"train.head()","c2e7200b":"train.columns","a89ff660":"train.signal.value_counts()","1bdd39f3":"test.head()","6a80935e":"train_signal = train.copy()\ntrain_signal.head()","46861c07":"train_signal = train_signal[train['signal']==1]\ntrain_signal.signal.value_counts()","fe032b66":"train_signal.head()","e7d17c8c":"CMAP = sns.diverging_palette(220, 20, s=99, as_cmap=True, n=2500)\n\ndef plot3D(X, target, elev=0, azim=0, title=None, sub=111):\n    x = X[:, 0]\n    y = X[:, 1]\n    z = X[:, 2]\n    \n    fig = plt.figure(figsize=(12, 8))\n    ax = Axes3D(fig)\n    mappab = ax.scatter(x, y, z, c=target, cmap=CMAP)\n\n    if title is not None:\n        ax.set_title(title)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n    ax.set_zlabel('Component 3')\n\n    ax.view_init(elev=elev, azim=azim)\n    fig.colorbar(mappable=mappab, label='Target variable')\n    plt.show()\n    \nfeat_XY = ['TX', 'TY', 'X', 'Y']\nfirst = train.loc[train.data_ind == 21, :]\nplot3D(first.loc[first.signal==1, ['Z', 'X', 'Y']].values,\n       first.loc[first.signal==1].signal.values, elev=20, azim=60)","52bacc30":"plot3D(first.loc[first.signal==1, ['Z', 'X', 'Y']].values,\n       first.loc[first.signal==1].signal.values, elev=45, azim=0)","108e7790":"axis = 'X'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","9da52847":"axis = 'Y'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","5c711f74":"axis = 'Z'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","301462ad":"def add_neighbours(df, k, metric='minkowski'):\n    res = []\n    \n    for data_ind in tqdm(np.unique(df.data_ind)):\n        ind = df.loc[df.data_ind == data_ind].copy()\n        ind[['TX', 'TY']] *= 1293\n        values = np.unique(ind.Z)\n        \n        for j in range(1, len(values)):\n            z, z_next = (ind.loc[ind.Z == values[j-1]].copy(),\n                         ind.loc[ind.Z == values[j]].copy())\n            \n            b_tree = BallTree(z_next[feat_XY], metric=metric)\n            d, i = b_tree.query(z[feat_XY], k=min(k, len(z_next)))\n            \n            for m in range(i.shape[1]):\n                data = z_next.iloc[i[:, m]]\n                z_copy = z.copy()\n                for col in feat_XY + ['Z']:\n                    z_copy[col + '_pair'] = data[col].values\n                res.append(z_copy)\n            \n        res.append(z_next)\n        \n    res = pd.concat(res)\n    for col in feat_XY + ['Z']:\n        res['d' + col] = res[col].values - res[col + '_pair'].values\n    return res\n\ndef balance_train(df, k):\n    data = add_neighbours(df, k=k)\n    noise = data.event_id == -999\n    signal, not_signal = data.loc[np.logical_not(noise)], data.loc[noise]\n    noise_part = not_signal.sample(len(signal))\n    return pd.concat([signal, noise_part]).reset_index(drop=True)\ntrain = []\nfor file in glob.glob('..\/input\/dark-matter-from-opera-experiments\/open*.h5')[:5]: # just 5 bricks\n    train.append(balance_train(pd.read_hdf(file), k=3))\ntrain = pd.concat(train)","aa053d82":"y_train = train.signal\nX_train = train.drop(['event_id', 'signal', 'data_ind'], axis=1)","67401eb6":"transformer = Normalizer()\nX_train_norm = transformer.fit_transform(X_train.fillna(0))","9844da23":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\ntrain_index, val_index = next(sss.split(X_train_norm, y_train))","119bcecb":"def nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_dim))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer=Adam())\n    return model","bc49cc6d":"callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto'),\n            ModelCheckpoint('{val_loss:.4f}.hdf5', monitor='val_loss', verbose=2, save_best_only=True, mode='auto')]","375bc56e":"nn = nn_model(X_train_norm.shape[1])\nnn.fit(X_train_norm, y_train, validation_split=0.2, epochs=20, verbose=2, batch_size=256, shuffle=True, callbacks=callbacks)","aa96a8e3":"prepared_test = add_neighbours(test, k=3)\nX_test = prepared_test.drop(['data_ind'], axis=1)","8fee0b3e":"X_test_norm = transformer.transform(X_test.fillna(0))\nX_test = transformer.transform(X_test.fillna(0))","1b43f00a":"X_test_norm[:5]","d65e8469":"probas = nn.predict_proba(X_test_norm)","43da620c":"probas = np.squeeze(probas)","a5234445":"df = pd.DataFrame({'id': prepared_test.index, 'signal': probas}).groupby('id')\nagg = df.aggregate(('mean')).loc[:, ['signal']]","a7af1db0":"agg.head()","b4587c45":"agg.to_csv('submission.csv.gz', index=True, compression='gzip')","7d964c07":"train_signal.fillna(0, inplace=True)","0cabd0fd":"kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(train_signal)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nk = [inertias.index(i) for i in inertias]","4db8ec6c":"plt.plot(k, inertias, linewidth=2.0)\nline, = plt.plot(k, inertias, 'o')\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia\", fontsize=14)","55278a2a":"kmeans = KMeans(n_clusters=6, random_state=42).fit(train_signal)\nclustering_labels = kmeans.labels_","80d02518":"train_signal.shape","299a1412":"clustering_labels.shape","7699e637":"clusters = train_signal\nclusters['cluster'] = clustering_labels","21a79ba4":"X_sample = train_signal.sample(frac=0.1, random_state=42)","8e6e5b9d":"X_sample.head()","80078412":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_sample.X, X_sample.Y, X_sample.Z, c=X_sample.cluster)","69774909":"agg.to_csv('submission2.csv.gz', index=True, compression='gzip')","4947f371":"Group base tracks from neighboring plates (see blog post for intuition behind why we do this)","2fe3ec5e":"# Particle Identification","5a735b25":"## Using a Neural Net to separate signal from background","e632cd4f":"## Explore the dataset","9984724a":"# Searching for dark matter","a2cd15b7":"## Train an AdaBoost model ","225634c2":"## Explore dataset and find neighboring base tracks","f1127af2":"Closer look at base track distribution along the axes","09a8dbbd":"Create a dataframe of signals for clustering later on","360732cc":"Training a second, neural network model and comparing performance via ROC curves","6fa8beae":"Just to compare performances, here are the ROC curves for our AdaBoostClassifier model.","cbb5918a":"## Using KMeans to find signal clusters","1285f380":"The inertia's rate of decline flattens around k=6 clusters. So we'll train a KMeans with 6 clusters.","29f21f24":"From the dataset description, we know that the features describe particle responses in the detector systems, and represent the following:\n\n- ID - id value for tracks (presents only in the test file for the submitting purposes)\n- Label - string valued observable denoting particle types. Can take values \"Electron\", \"Muon\", \"Kaon\", \"Proton\", \"Pion\" and \"Ghost\". This column is absent in the test file.\n- FlagSpd - flag (0 or 1), if reconstructed track passes through Spd\n- FlagPrs - flag (0 or 1), if reconstructed track passes through Prs\n- FlagBrem - flag (0 or 1), if reconstructed track passes through Brem\n- FlagEcal - flag (0 or 1), if reconstructed track passes through Ecal\n- FlagHcal - flag (0 or 1), if reconstructed track passes through Hcal\n- FlagRICH1 - flag (0 or 1), if reconstructed track passes through the first RICH detector\n- FlagRICH2 - flag (0 or 1), if reconstructed track passes through the second RICH detector\n- FlagMuon - flag (0 or 1), if reconstructed track passes through muon stations (Muon)\n- SpdE - energy deposit associated to the track in the Spd\n- PrsE - energy deposit associated to the track in the Prs\n- EcalE - energy deposit associated to the track in the Hcal\n- HcalE - energy deposit associated to the track in the Hcal\n- PrsDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Prs\n- BremDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Brem\n- TrackP - particle momentum\n- TrackPt - particle transverse momentum\n- TrackNDoFSubdetector1  - number of degrees of freedom for track fit using hits in the tracking sub-detector1\n- TrackQualitySubdetector1 - chi2 quality of the track fit using hits in the tracking sub-detector1\n- TrackNDoFSubdetector2 - number of degrees of freedom for track fit using hits in the tracking sub-detector2\n- TrackQualitySubdetector2 - chi2 quality of the track fit using hits in the  tracking sub-detector2\n- TrackNDoF - number of degrees of freedom for track fit using hits in all tracking sub-detectors\n- TrackQualityPerNDoF - chi2 quality of the track fit per degree of freedom\n- TrackDistanceToZ - distance between track and z-axis (beam axis)\n- Calo2dFitQuality - quality of the 2d fit of the clusters in the calorimeter \n- Calo3dFitQuality - quality of the 3d fit in the calorimeter with assumption that particle was electron\n- EcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Ecal\n- EcalDLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from Ecal\n- EcalShowerLongitudinalParameter - longitudinal parameter of Ecal shower\n- HcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Hcal\n- HcalDLLbeMuon - delta log-likelihood for a particle candidate to be using information from Hcal\n- RICHpFlagElectron - flag (0 or 1) if momentum is greater than threshold for electrons to produce Cherenkov light\n- RICHpFlagProton - flag (0 or 1) if momentum is greater than threshold for protons to produce Cherenkov light\n- RICHpFlagPion - flag (0 or 1) if momentum is greater than threshold for pions to produce Cherenkov light\n- RICHpFlagKaon - flag (0 or 1) if momentum is greater than threshold for kaons to produce Cherenkov light\n- RICHpFlagMuon - flag (0 or 1) if momentum is greater than threshold for muons to produce Cherenkov light\n- RICH_DLLbeBCK  - delta log-likelihood for a particle candidate to be background using information from RICH\n- RICH_DLLbeKaon - delta log-likelihood for a particle candidate to be kaon using information from RICH\n- RICH_DLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from RICH\n- RICH_DLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from RICH\n- RICH_DLLbeProton - delta log-likelihood for a particle candidate to be proton using information from RICH\n- MuonFlag - muon flag (is this track muon) which is determined from muon stations\n- MuonLooseFlag muon flag (is this track muon) which is determined from muon stations using looser criteria\n- MuonLLbeBCK - log-likelihood for a particle candidate to be not muon using information from muon stations\n- MuonLLbeMuon - log-likelihood for a particle candidate to be muon using information from muon stations\n- DLLelectron - delta log-likelihood for a particle candidate to be electron using information from all subdetectors\n- DLLmuon - delta log-likelihood for a particle candidate to be muon using information from all subdetectors\n- DLLkaon - delta log-likelihood for a particle candidate to be kaon using information from all subdetectors\n- DLLproton - delta log-likelihood for a particle candidate to be proton using information from all subdetectors\n- GhostProbability - probability for a particle candidate to be ghost track. This variable is an output of classification model used in the tracking algorithm.\n\nSpd stands for Scintillating Pad Detector, Prs - Preshower, Ecal - electromagnetic calorimeter, Hcal - hadronic calorimeter, Brem denotes traces of the particles that were deflected by detector.","bbdde7db":"## Train a Neural Network","9be3c1b4":"Plot showers in the brick","9775d93d":"Since we're trying to predict the label (or class), we remove them from our list of features.","bd36ede9":"Split the training data into training and validation sets.","10a70eab":"**Find the optimal number of clusters**","50089a6f":"As we can see, AdaBoostClassifier performs slightly better than the neural net across the board for all particles."}}