{"cell_type":{"98e8b8ff":"code","1b0768fe":"code","8ee977d5":"code","30d1b5a2":"code","60692ed3":"code","9139c542":"code","7a285151":"code","eea26463":"code","ddc8e500":"code","b79f90eb":"code","fc88d1d5":"code","88122804":"code","6e615969":"code","afd48ffa":"code","20c7db0f":"code","7d8e6157":"code","7242b191":"code","b9a39c72":"code","80142124":"code","949bbf09":"code","0a425fac":"code","bfd2d4d9":"code","3a62a8d4":"code","2d50bbe5":"code","672a906d":"code","d69d2923":"code","3dc8bd7c":"code","6bb8438f":"code","6747cdf8":"code","96f8a195":"code","46aee23d":"markdown","8c5c339d":"markdown","172bfa4d":"markdown","da5f19bf":"markdown","d3736a34":"markdown","d443d9bb":"markdown","7505d646":"markdown","4b15f919":"markdown","a212bc6a":"markdown","4bbc2686":"markdown","93851526":"markdown","70348808":"markdown","08241456":"markdown","c3be8295":"markdown","0ae850e4":"markdown","2b335edc":"markdown","7124765d":"markdown","94fafdfe":"markdown","ff74278a":"markdown","558ab460":"markdown","78530760":"markdown","5460250e":"markdown","9d682611":"markdown","92f0a561":"markdown","89636560":"markdown","0b985c18":"markdown","abf9ae5b":"markdown","55019f32":"markdown","55176a36":"markdown","b8927cc8":"markdown","69e5a709":"markdown","e115dfeb":"markdown","03eaa14d":"markdown","48adcea1":"markdown","641330af":"markdown","8db11a4c":"markdown"},"source":{"98e8b8ff":"# importing packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport numpy as np\nimport pandas as pd\n\n# import ML packages\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import datasets, linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom scipy import stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","1b0768fe":"# save filepath to variable for easier access\nbike_file_path = '..\/input\/london-bike-sharing-dataset\/london_merged.csv' \n\n# read the data and store data in DataFrame titled bike_data\nbike_data = pd.read_csv(bike_file_path)\n\n# inspect data\nbike_data.head()","8ee977d5":"# inspect variables\nbike_data.info()","30d1b5a2":"# check the sum of null records\nbike_data.isnull().sum()","60692ed3":"# plot distribution of cnt target variable\nsns.distplot(bike_data['cnt'])\nplt.show()","9139c542":"# inspect description of variables\nbike_data.describe()","7a285151":"# create correlation matrix displaying pearson correlation coefficients for all variables\ncorr_matrix = bike_data.corr()\ncorr_matrix","eea26463":"# plot pair grid with histograms and scatterplots\nbike_data_sample = bike_data.sample(1000)\np = sns.PairGrid(data=bike_data_sample, vars=['t1', 't2', 'hum', 'wind_speed', 'cnt'])\np.map_diag(plt.hist)\np.map_offdiag(plt.scatter)","ddc8e500":"# plot pair grid with histograms and scatterplots using season as hue\nbike_data_sample = bike_data.sample(1000)\np = sns.PairGrid(data=bike_data_sample, vars=['t1', 't2', 'hum', 'wind_speed', 'cnt'], hue='season')\np.map_diag(plt.hist)\np.map_offdiag(plt.scatter)\nplt.legend(title='Season', loc='center right', bbox_to_anchor=(1.65, 0.5), ncol=1, labels=['Spring', 'Summer', 'Fall', 'Winter'])","b79f90eb":"# convert timestamp string to datetime format for entire timestamp column\nbike_data['timestamp'] = pd.to_datetime(bike_data['timestamp']) \n# retrieving timestamp column by iloc method\ntype(bike_data['timestamp'].iloc[0]) \n\n# create hour, month, and day of week variables from timestamp data\nbike_data['hour']=bike_data['timestamp'].apply(lambda time: time.hour) \nbike_data['month']=bike_data['timestamp'].apply(lambda time: time.month)\nbike_data['day_of_week']=bike_data['timestamp'].apply(lambda time: time.dayofweek)\n\n# creating mapping variable for day of week labels\ndate_names = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'} \nbike_data['day_of_week'] = bike_data['day_of_week'].map(date_names)\n\nbike_data.head()","fc88d1d5":"# create box plots for time related variables\nfigure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\nfigure.set_size_inches(24, 8)\n\nsns.boxplot(data=bike_data, x='month', y='cnt', ax=ax1)\nsns.boxplot(data=bike_data, x='hour', y='cnt', ax=ax2)\nsns.boxplot(data=bike_data, x='day_of_week', y='cnt', ax=ax3)","88122804":"# create point plot comparing cnt by hour for is_holiday variable\nfig,(ax1)= plt.subplots(nrows=1)\nfig.set_size_inches(18,5)\nsns.pointplot(data=bike_data, x='hour', y='cnt', ci=\"sd\", hue='is_holiday', ax=ax1, palette='YlGnBu')","6e615969":"# create point plot comparing cnt by hour for is_weekend variable\nfig,(ax1)= plt.subplots(nrows=1)\nfig.set_size_inches(18,5)\nsns.pointplot(data=bike_data, x='hour', y='cnt', ci=\"sd\", hue='is_weekend', ax=ax1, palette='YlGnBu')","afd48ffa":"# creating mapping variable for season labels\nseason_names = {0:'Spring',1:'Summer',2:'Fall',3:'Winter'} \nbike_data['season'] = bike_data['season'].map(season_names) \n\n# create point plot comparing cnt by hour for season variable\nfig,(ax1)= plt.subplots(nrows=1)\nfig.set_size_inches(18,5)\nsns.pointplot(data=bike_data, x='hour', y='cnt', ci=\"sd\", hue='season', ax=ax1, palette='YlGnBu')\nplt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)","20c7db0f":"# creating mapping variable for weather labels\nweather_names = {1:'Clear',2:'Scattered Clouds',3:'Broken Clouds',4:'Cloudy',7:'Light Rain',10:'Thunderstorm',26:'Snowing',94:'Freezing Fog'}\nbike_data['weather_code'] = bike_data['weather_code'].map(weather_names)\n\n# create point plot comparing cnt by hour for weather variable\nfig,(ax1)= plt.subplots(nrows=1)\nfig.set_size_inches(18,5)\nsns.pointplot(data=bike_data, x='hour', y='cnt', ci=\"sd\", hue='weather_code',ax=ax1, palette='YlGnBu')\nplt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)","7d8e6157":"# reset data to prepare for building regression model\nbike_data = pd.read_csv(bike_file_path)\n\n# convert float variables to int\nbike_data.weather_code = bike_data.weather_code.astype(int)\nbike_data.is_holiday = bike_data.is_holiday.astype(int)\nbike_data.is_weekend = bike_data.is_weekend.astype(int)\nbike_data.season = bike_data.season.astype(int)\n\n# convert timestamp string to datetime format for entire timestamp column\nbike_data['timestamp'] = pd.to_datetime(bike_data['timestamp']) \n\n# retrieving timestamp column by iloc method\ntype(bike_data['timestamp'].iloc[0]) \n\n# create new variables from timestamp data\nbike_data['hour']=bike_data['timestamp'].apply(lambda time: time.hour) \nbike_data['month']=bike_data['timestamp'].apply(lambda time: time.month)\n\n# inspect data\nbike_data.head()","7242b191":"# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\nbike_data = pd.get_dummies(bike_data, columns = ['weather_code', 'season','hour','month'],drop_first = True)\n\n# drop timestamp\nbike_data.drop('timestamp', axis=1, inplace=True)\n\n# inspect bike_data df with added dummy variables\nbike_data.head(5)","b9a39c72":"# inspect variables\nbike_data.info()","80142124":"# set limit for correlation coefficient\ndrop_corr = .95\n\n# select only upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# drop any variables with correlation coefficient greater than drop_corr value\nto_drop = [column for column in upper.columns if any(upper[column] > drop_corr)]\nbike_data = bike_data.drop(to_drop,axis=1)\nprint(\"Dropping: \" + str(to_drop) + \" variable(s) for exceeding correlation of \" + str(drop_corr))\n      \n# display remaining variables represented as dataframe columns\nbike_data.columns","949bbf09":"# set the target variable\ny = bike_data['cnt']\n\n# set the independent predictor variables\nX = bike_data.drop('cnt', axis=1)\n\n# split data into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\nX_train = sm.add_constant(X_train)\nX_train.head()","0a425fac":"# fit data to linear regression\nmlr1 = sm.OLS(y_train, X_train).fit()\n\n# view OLS regression results\nprint(mlr1.summary())","bfd2d4d9":"X_train2 = X_train.drop(['month_2','month_3','month_4','month_5','month_6','month_7','month_8','month_9','month_10','month_11','month_12'], axis=1)\nmlr2 = sm.OLS(y_train, X_train2).fit()\nprint(mlr2.summary())","3a62a8d4":"X_train3 = X_train2.drop(['season_1','season_2','season_3'], axis=1)\nmlr3 = sm.OLS(y_train, X_train3).fit()\nprint(mlr3.summary())","2d50bbe5":"X_train4 = X_train3.drop(['weather_code_2','weather_code_3','weather_code_4','weather_code_7','weather_code_10','weather_code_26'], axis=1)\nmlr4 = sm.OLS(y_train, X_train4).fit()\nprint(mlr4.summary())","672a906d":"# create dataframe to calculate and display VIF for each variable\nvif = pd.DataFrame()\nvif['Features'] = X_train4.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train4.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d69d2923":"# drop predictive variables to prepare final model\nX_test = sm.add_constant(X_test)\nX_test_1 = X_test[X_train4.columns] \n\n# fit data to linear regression for final model using test data\nmlr_test = sm.OLS(y_test, X_test_1).fit()\n\n# inspect X_test data\nX_test_1.head()","3dc8bd7c":"# Making predictions using the final model\ny_pred = mlr_test.predict(X_test_1)","6bb8438f":"# distribution plot of predicted y values vs test y values\nsns.distplot((y_test - y_pred), bins=50);","6747cdf8":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)   \nplt.xlabel('y_test ', fontsize=18)                       \nplt.ylabel('y_pred', fontsize=16) ","96f8a195":"# create R2 score\nr2 = r2_score(y_test, y_pred)\n\n# create Adjusted R2 Score\np = len(X_test_1.columns)\nn = y_test.shape[0]\nadj_r2 = 1 - (1 - r2) * ((n - 1)\/(n-p-1))\n\n# create RMSE score\nrmse = mean_squared_error(y_test, y_pred, squared = False)\n\n\n# print final model performance stats\nprint(str(p) + \" Predictors in Test Set\")\nprint(str(n) + \" Records in Test Set\")\nprint(\"R2: \" + str(r2))\nprint(\"Adj R2: \" + str(adj_r2))\nprint(\"RMSE: \" + str(rmse))\n","46aee23d":"Let's compare the R2 and Adj R2 values from the training dataset vs the test dataset.\n\n**Training Dataset**\n* R2 - 0.707\n* Adj R2 - 0.706\n\n**Test Dataset**\n* R2 - 0.717\n* Adj R2 - 0.715","8c5c339d":"**Model 3**\n\nFor our third model, we see that the adjusted R2 has stayed the same after dropping the season variables.\n\n* Adj R2: 0.712\n\nThe **weather_code_3** variable has a P-value over the .05 threshold, so we will remove all of the **weather_code** variables to see how our model is affected.","172bfa4d":"# Evaluating Predictions of Final Model","da5f19bf":"# Final Model Conclusion\n\n**Here is the equation for the final model:**\n\nCount = 1228.6733 + (41.1958 * t1) + (-14.9461 * hum) + (-10.3165 * wind_speed) + (-310.1950 * is_holiday) + (-227.6981 * is_weekend) + (-80.2197 * hour_1) + (-121.5762 * hour_2) + (-151.5846 * hour_3) + (-151.7127 * hour_4) + (-110.8856 * hour_5) + (233.1219 * hour_6) + (1208.3575 * hour_7) + (2438.8380 * hour_8) + (1246.4129 * hour_9) + (574.6209 * hour_10) + (628.7589 * hour_11) + (862.8595 * hour_12) + (841.0589 * hour_13) + (833.7588 * hour_14) + (900.3832 * hour_15) + (1258.4986 * hour_16) + (2199.0846 * hour_17) + (2110.0713 * hour_18) + (1123.3729 * hour_19) + (593.3988 * hour_20) + (316.1232 * hour_21) + (218.6951 * hour_22) + (94.2308 * hour_23)\n\nLet's make a final future prediction using the model with the following values:\n\n* t1: 18.0\n* hum: 70.0\n* wind_speed: 6.0\n* is_holiday: 0\n* is_weekend: 0\n* hour_1: 0\n* hour_2: 0\n* hour_3: 0\n* hour_4: 0\n* hour_5: 0\n* hour_6: 0\n* hour_7: 0\n* hour_8: 0\n* hour_9: 0\n* hour_10: 1\n* hour_11: 0\n* hour_12: 0\n* hour_13: 0\n* hour_14: 0\n* hour_15: 0\n* hour_16: 0\n* hour_17: 0\n* hour_18: 0\n* hour_19: 0\n* hour_20: 0\n* hour_21: 0\n* hour_22: 0\n* hour_23: 0\n\nWe plug in the above values into the formula:\n\nCount = 1228.6733 + (41.1958 * 18.0) + (-14.9461 * 70.0) + (-10.3165 * 6.0) + (-310.1950 * 0) + (-227.6981 * 0) + (-80.2197 * 0) + (-121.5762 * 0) + (-151.5846 * 0) + (-151.7127 * 0) + (-110.8856 * 0) + (233.1219 * 0) + (1208.3575 * 0) + (2438.8380 * 0) + (1246.4129 * 0) + (574.6209 * 1) + (628.7589 * 0) + (862.8595 * 0) + (841.0589 * 0) + (833.7588 * 0) + (900.3832 * 0) + (1258.4986 * 0) + (2199.0846 * 0) + (2110.0713 * 0) + (1123.3729 * 0) + (593.3988 * 0) + (316.1232 * 0) + (218.6951 * 0) + (94.2308 * 0)\n\nCount = 1228.6733 + (41.1958 * 18.0) + (-14.9461 * 70.0) + (-10.3165 * 6.0) + (574.6209 * 1)\n\nCount = 1228.6733 + 741.5244 -1046.227 -61.899 + 574.6209\n\n**Count = 1,436.6926**\n\n\n# Thank you!\n\nThank you for reading and please leave a comment below if you have a question or suggestion for improvement!\n","d3736a34":"As expected, the season has an effect on count but mainly from 8am until midnight, with the biggest effects found in the afternoon during what would be considered peak commute times. ","d443d9bb":"Lastly, we will look at the R Squared (R2), Adjusted R Squared (Adj R2), and Root Mean Square Error (RMSE) values for the predictions. \n\nThe RMSE is the standard deviation of the prediction errors also known as residuals. This helps us understand how well the actual data fits our model. ","7505d646":"# Exploratory Data Analysis\n\nFirst we want to view a description of the Metadata as provided by the London Bike Sharing Data Set:\n\n*timestamp* - timestamp field for grouping the data  \n*cnt* - the count of new bike shares  \n*t1* - real temperature in C  \n*t2* - temperature in C \"feels like\"  \n*hum* - humidity in percentage  \n*windspeed* - wind speed in km\/h  \n*weathercode* - category of the weather  \n*isholiday* - boolean field - 1 holiday \/ 0 non holiday  \n*isweekend* - boolean field - 1 if the day is weekend  \n*season* - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.  \n\n*weather_code* category description:  \n1 = Clear ; mostly clear but have some values with haze\/fog\/patches of fog\/ fog in vicinity 2 = scattered clouds \/ few clouds 3 = Broken clouds 4 = Cloudy 7 = Rain\/ light Rain shower\/ Light rain 10 = rain with thunderstorm 26 = snowfall 94 = Freezing Fog\n\n<br\/>\n\nLet's take a look at the distribution of the cnt variable which will be our dependent variable we will be using in our model. We will also take a look at the description of the data which contains several standard statistical measures. ","4b15f919":"We can observe a roughly normal distribution for **t1**, **t2**, **hum**, and **wind_speed**. Collinearity is observed between **t1** (real temperature in C) and **t2** (temperature in C \"feels like\") which is not suprising given that **t2** uses **t1** as a starting point and is potentially modified by other conditions such as **wind_speed** and **humidity** which can be observed in the scatterplots. A decision will need to be made as to which should be used when we create the linear regression model.\n\n<br\/><br\/>\n\nWe also examined histograms and scatterplots comparing cnt, t1, hum, and wind_speed, color coded by season. A couple of observations that stand out are a moderate negative correlation between t1 and hum and a mild negative correlation between t1 and wind_speed.","a212bc6a":"The dataset has 10 initial columns and 17,414 records. Most columns are of floats, including several categorical variables interestingly, and there is a timestamp object. ","4bbc2686":"We can also visually examine the relationships between the measurable variables via a scatterplot using a randomly selected sample size of 1,000. We also see the histograms for each of the measurable variables displayed across the diagonal. ","93851526":"# Problem Statement\n\nThis notebook is based on the London Bike Sharing dataset. \n\nThere are various weather, season, and time related data associated with bike sharing counts and we are looking to discover a pattern that can be used for prediction. This is useful because given a set of inputs, a bike sharing business can predict future demand and determine the necessary inventory levels to sustain that demand. This information can also be used to derive revenue and expense forecasts which is useful for business planning and forecasting. ","70348808":"We have several categorical variables and will need to transform them into dummy variables with binary values in order to incorporate them into our model. This is done because despite having int values, they are not ordinal variables. In order to avoid multicollinearity we will also have to drop one of the dummy variables from each set. For example, if we create four season variables season_1, season_2, season_3, season_4, the first variable will be dropped. In our regression, it will be assumed that if variables for seasons 2 through 4 have values of 0, then our missing dummy variable of season_1 is being represented. The same will apply to weather_code, hour, and month.","08241456":"# Setup Environment\n\nWe need to import that various packages we will be using for preparing the dataframe, creating visualizations, and creating a multiple linear regression model. ","c3be8295":"# Variance Inflation Factors (VIF)\n\nVariance inflation factors range from a value of 1.0 and upwards. The VIF helps you quantify the severity of multicollinearity in an OLS regression. The VIF value tells you how much larger the standard error increases compared to if that variable had 0 correlation to other independent predictor variables in your model. ","0ae850e4":"After creating the dummy variables and dropping the timestamp variables that is no longer needed, we want to inspect the new full list of variables.","2b335edc":"Let's examine the pearson correlation coefficients again given that we have added a significant amount of variables. We will be dropping any that display multicolinearity. We have segtt a correlation coefficient threshold and will automatically calculate the coefficients and drop from the DataFrame any variables that exceed our threshold. ","7124765d":"# Creating the Multiple Regression and Evaluating the Model\n\nThere are many methods for selecting predictors in the process of creating a multiple regeression model.\n\n* Backward Selection\n* Forward Selection\n* Stepwsie Regression\n* Best Subsets\n\n\nTo start we will create a multiple regression using all of the variables to get a baseline. \n\nWe will be using backward selection where we start with all of the predictor variables in the model, and after evaluating the variables, remove less useful predictors one at a time or in batches for categorical variables. We will take a look at the strength of the model (adj r2) and the statistical significance (P-value) of the indepenent variables and determine how to proceed in revising the model. \n\nR Squared, also known as the coefficient of determination, displays the variation in the depdendent target variable y as explained by the independent predictor x variables. In other words, the percentage of the prediction outcome that can be attributed to the predictor variables of the model. Another way to look at it is R2 = explained variation \/ total variation. The higher the number the better the data fits the model in question. \n\nAdjusted R Squared is used when multiple independent predictor variables exist and includes a penalty for adding additional predictors. This allows you to compare the effectiveness of different models with differing numbers of predictor variables. This is required because as you add predictors, R2 will always continue to increase, even if there is just a chance correlation between variables. In general a parsimonious model is preferred given that it meets reasonable Adjusted R Squared and statistical significance criteria. \n\nP-Values measure the statistical significance of each variable in the model within the context of all variables in the model. It is essentially a measure of the liklihood of achieving results as extreme as were observed given a null hypothesis. In other words, the likeliness of the results being explained by random chance. A very low P-value is desired with .05 or .01 often being used as the standard depending on the context and several factors.\n\nWe begin by creating our first model.","94fafdfe":"Weather, however, had a much more pronounced difference than season, with severe weather conditions drastically decreasing ridership as one would expect. ","ff74278a":"# Making Predictions With Final Model\n\nWe are going to create our final model for prediction using the final set of variables that we had in our final training model. ","558ab460":"# Splitting Data into Training and Test Sets\n\nWe will be creating a multiple regression model which is a supervised machine learning parametric method. To do that we are going to split our data into a training set (60%) and a test set(40%). We will be fiting the multiple linear regression to the variables and data from the training set and then testing the performance of the model against the test set.","78530760":"**Model 2**\n\nFor our second model, we see that the adjusted R2 has only decreased  slightly after dropping the month variables.\n\n* Adj R2: 0.712\n\nThere are still several variables that have an unacceptably high P-value, particularly the **season** variables. We will be dropping all of the season variables as well for our third model and observing the change.  ","5460250e":"All of the variables have relatively low VIF values with the exception of t1 which is still below our cuttoff of 5.0 which is acceptable. ","9d682611":"As you can see, both the weekend and holiday variable have a similar effect. One thing to note is that a non holiday can still also be a weekend, which is why you see a slightly weaker contrast on the is_holiday graph.","92f0a561":"The cnt is not normally distributed and contains a positive skew.","89636560":"# Loading Data\n\nFirst we import the dataset from the csv file into our DataFrame and inspect the data. ","0b985c18":"# Preparing Variables for Regression Model\n\nFirst we will prepare the data and inspect it to quickly verify things look good. This step is needed because values for season and weather_code have been rewritten as categorical names for use as labels within visualizations for the exploratory data analysis to be more useful. We will start fresh here by pulling in the raw data again and starting fresh.","abf9ae5b":"We can now look for patterns in the hour, month, and day of week data and see if there are periods of higher usage. A hypothesis I will be testing is the assumption that the **cnt** will be higher during daylight hours, weekdays, and summer months. \n\nWe will now visualize the **Hour**, **Month**, and **Day of Week** data using boxplot graphs.","55019f32":"No null exists in the data, contributing to the high usability score in Kaggle. No further cleanup of the dataset is required, as there are no rows to drop or null values to fill in. ","55176a36":"Let's take a look at the pearson correlation coefficients. This helps us understand the extent to which two variables are correlated. We will be able to see both the strength of the correlation as well as the direction and use that to make a decision on the exclusion of predictive variables that display multicollinearity.","b8927cc8":"Now we will make predictions of the y variable which is **cnt** using our final rest model.","69e5a709":"**Model 4**\n\nFor our fourth, we see that the adjusted R2 has only decreased slightly after dropping the weather variables. We can see that all remaining p-values are below 0.05 and thus we have satisfied our desired criteria. We will keep this as our final model for making predictions. \n\n* Adj R2: 0.706","e115dfeb":"**Model 1**\n\nFor our first model, we have already eliminated **t2** due to multicollinearity at an earlier step. Here we take a look at the strength of the model (adj r2) and the statistical significance (P-value) of the indepenent variables.\n\n* Adj R2: 0.713\n\nThere are many variables that have an unacceptably high P-value, particularly several of the **month** variables. Given that they are binary categorical dummy variables, it doesn't make much sense to keep some of them and drop only the problematic ones.  For our second model we will be dropping all of the month variables. ","03eaa14d":"# Adding Time and Date Variables\n\nThe **timestamp** data is not very useful to us in it's current format. In order to incorporate it into our linear regression model, we will need to extract the hour, day, and month from the **timestamp**.","48adcea1":"To evaluate our final model we will look a number of data points and visualizations. First we start off with a distribution plot of predicted y values subtracted from the actual y values from the test dataset. This will help us visualize the distribution of errors. We also take a look at a scatter plot of predicted y values vs actuals from the test data set. ","641330af":"Reviewing the month box plot, the peak is during the summer, particularly in July with the months of April through October (when the weather is presumably warmer) having higher counts than the rest of the year. The hour box plot demonstrates the highest traffic during commuter hours in the morning (7am-9am) and afternoon (4pm-7pm), and a steep decrease during the night when its dark.  The Day of Week boxplot shows a modest decrease on Saturday and Sunday due to the weekend. \n\n<br\/>\n\nNext we will visualize the effect of **is_weekend** and **is_holiday**, **season**, and **weather_code** on the **cnt** by **Hour** using a point plot. A hue will be used to see the effect of these variables.","8db11a4c":"From the **cnt** we can again confirm that there are no missing values for any of the variables. We can also see the **mean**, **std** (standard deviation), **min** (mininum), **max** (maximum), and the various quartiles (**25%**, **50%**, **75%**). "}}