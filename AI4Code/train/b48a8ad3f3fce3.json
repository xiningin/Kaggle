{"cell_type":{"8a9e5c95":"code","e37029e9":"code","8ca7f337":"code","1bdd9fbf":"code","b533469b":"code","6b70d016":"code","c028ea60":"code","b2b00c6d":"code","2cec12c9":"code","de4b1935":"code","0c6f2969":"code","0b6ba9aa":"markdown","e3241edf":"markdown","5c7271c8":"markdown","40290af8":"markdown","d1c8ccf5":"markdown","ea7dcac9":"markdown","6596bd0f":"markdown","3fd2dd2e":"markdown","fb24c2af":"markdown","c5a799b7":"markdown","b9a0c6b4":"markdown","240d764c":"markdown","13d12168":"markdown","af72eecf":"markdown","039fcff4":"markdown"},"source":{"8a9e5c95":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py  > \/dev\/null\n!python pytorch-xla-env-setup.py --version nightly  > \/dev\/null","e37029e9":"!pip install timm  > \/dev\/null","8ca7f337":"import gc\nimport os\nimport time\nimport torch\nimport albumentations\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nfrom PIL import Image\n\nimport torch.nn as nn\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser\n\n\nimport timm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nos.environ['XLA_USE_BF16']=\"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'","1bdd9fbf":"FLAGS = {\n    \n    'fold': 0,\n    'model': 'resnext50_32x4d',\n    'pretrained': True,\n    'batch_size': 128,\n    'num_workers': 4,\n    'lr': 3e-4,\n    'epochs': 10\n}","b533469b":"# Using Ross Wightman's timm package\nclass TimmModels(nn.Module):\n    def __init__(self, model_name,pretrained=True, num_classes=5):\n        super(TimmModels, self).__init__()\n        self.m = timm.create_model(model_name,pretrained=pretrained)\n        model_list = list(self.m.children())\n        model_list[-1] = nn.Linear(\n            in_features=model_list[-1].in_features, \n            out_features=num_classes, \n            bias=True\n        )\n        self.m = nn.Sequential(*model_list)\n        \n    def forward(self, image):\n        out = self.m(image)\n        return out","6b70d016":"# Image Dataset class taken from Abhishek's tez package\n\nclass ImageDataset:\n    def __init__(\n        self,\n        image_paths,\n        targets,\n        resize,\n        augmentations=None,\n        backend=\"pil\",\n        channel_first=True,\n    ):\n        \"\"\"\n        :param image_paths: list of paths to images\n        :param targets: numpy array\n        :param resize: tuple or None\n        :param augmentations: albumentations augmentations\n        \"\"\"\n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n        self.backend = backend\n        self.channel_first = channel_first\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, item):\n        targets = self.targets[item]\n        if self.backend == \"pil\":\n            image = Image.open(self.image_paths[item])\n            if self.resize is not None:\n                image = image.resize(\n                    (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n                )\n            image = np.array(image)\n            if self.augmentations is not None:\n                augmented = self.augmentations(image=image)\n                image = augmented[\"image\"]\n        elif self.backend == \"cv2\":\n            image = cv2.imread(self.image_paths[item])\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            if self.resize is not None:\n                image = cv2.resize(\n                    image,\n                    (self.resize[1], self.resize[0]),\n                    interpolation=cv2.INTER_CUBIC,\n                )\n            if self.augmentations is not None:\n                augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        else:\n            raise Exception(\"Backend not implemented\")\n        if self.channel_first:\n            image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        return {\n            \"image\": torch.tensor(image),\n            \"targets\": torch.tensor(targets),\n        }","c028ea60":"# create folds\ndf = pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/train.csv\")\ndf[\"kfold\"] = -1    \ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.label.values\nkf = model_selection.StratifiedKFold(n_splits=5)\n\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[v_, 'kfold'] = f\n\ndf.to_csv(\"train_folds.csv\", index=False)","b2b00c6d":"MX = xmp.MpModelWrapper(TimmModels(FLAGS['model'],pretrained=FLAGS['pretrained'], num_classes=5))","2cec12c9":"def train_loop_fn(data_loader, loss_fn, model, optimizer, device, scheduler=None):\n    model.train() # put model in training mode\n    for bi, d in enumerate(data_loader): # enumerate through the dataloader\n        \n        images = d['image'] # obtain the ids\n        targets = d['targets'] # obtain the target\n\n        # pass image to model\n        optimizer.zero_grad()\n        outputs = model(images)\n        \n        # calculate loss\n        loss = loss_fn(outputs, targets)\n        \n        # backpropagate\n        loss.backward()\n        \n        # Use PyTorch XLA optimizer stepping\n        xm.optimizer_step(optimizer)\n        \n        # Step the scheduler\n        if scheduler is not None: scheduler.step()\n    \n    # since the loss is on all 8 cores, reduce the loss values and print the average\n    loss_reduced = xm.mesh_reduce('loss_reduce',loss, lambda x: sum(x) \/ len(x)) \n    # master_print will only print once (not from all 8 cores)\n    xm.master_print(f'bi={bi}, train loss={loss_reduced}')\n        \n    model.eval() # put model in eval mode for later use\n    \ndef eval_loop_fn(data_loader, loss_fn, model, device):\n    fin_targets = []\n    fin_outputs = []\n    for bi, d in enumerate(data_loader): # enumerate through dataloader\n        \n        images = d['image'] # obtain the ids\n        targets = d['targets']# # obtain the targets\n\n\n        # pass image to model\n        with torch.no_grad(): outputs = model(images)\n\n        # Add the outputs and targets to a list \n        targets_np = targets.cpu().detach().numpy().tolist()\n        outputs_np = outputs.cpu().detach().numpy().tolist()\n        fin_targets.extend(targets_np)\n        fin_outputs.extend(outputs_np)    \n        del targets_np, outputs_np\n        gc.collect() # delete for memory conservation\n                \n    o,t = np.array(fin_outputs), np.array(fin_targets)\n    \n    # calculate loss\n    loss = loss_fn(torch.tensor(o), torch.tensor(t))\n    # since the loss is on all 8 cores, reduce the loss values and print the average\n    loss_reduced = xm.mesh_reduce('loss_reduce',loss, lambda x: sum(x) \/ len(x)) \n    # master_print will only print once (not from all 8 cores)\n    xm.master_print(f'val. loss={loss_reduced}')\n    \n    acc = metrics.accuracy_score(t,o.argmax(axis=1))\n    acc_reduced = xm.mesh_reduce('acc_reduce', acc, lambda x: sum(x) \/ len(x))\n        \n    xm.master_print(f'val. accuracy = {acc_reduced}')","de4b1935":"def run(rank, flags):\n    global FLAGS\n    torch.set_default_tensor_type('torch.FloatTensor')\n    \n    training_data_path = \"..\/input\/cassava-jpeg-256x256\/kaggle\/train_images_jpeg\" #define the dataset path\n    df = pd.read_csv(\"\/kaggle\/working\/train_folds.csv\") #read train csv created earlier\n    device = xm.xla_device() #device, will be different for each core on the TPU\n    epochs = FLAGS['epochs']\n    fold = FLAGS['fold']\n    \n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n    train_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0, \n                always_apply=True\n            ),\n            albumentations.Transpose(p=0.5),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.ShiftScaleRotate(p=0.5),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, \n                sat_shift_limit=0.2, \n                val_shift_limit=0.2, \n                p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n            albumentations.CoarseDropout(p=0.5),\n            albumentations.Cutout(p=0.5)\n        ]\n    )\n\n    valid_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0,\n                always_apply=True\n            )\n        ]\n    )\n\n    train_images = df_train.image_id.values.tolist()\n    train_images = [\n        os.path.join(training_data_path, i) for i in train_images\n    ]\n    train_targets = df_train.label.values\n\n    valid_images = df_valid.image_id.values.tolist()\n    valid_images = [\n        os.path.join(training_data_path, i) for i in valid_images\n    ]\n    valid_targets = df_valid.label.values\n    \n    \n    train_dataset = ImageDataset(\n        image_paths=train_images,\n        targets=train_targets,\n        resize=None,\n        augmentations=train_aug,\n    )\n    \n    valid_dataset = ImageDataset(\n        image_paths=valid_images,\n        targets=valid_targets,\n        resize=None,\n        augmentations=valid_aug,\n    )\n    \n    # special sampler needed for distributed\/multi-core (divides dataset among the replicas\/cores\/devices)\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(), #divide dataset among this many replicas\n        rank=xm.get_ordinal(), #which replica\/device\/core\n        shuffle=True)\n    \n    # define DataLoader with the defined sampler\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=FLAGS['batch_size'],\n        sampler=train_sampler,\n        num_workers=FLAGS['num_workers'],\n        drop_last=True)\n    \n    # same as train but with valid data\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False)\n\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=FLAGS['batch_size'],\n        sampler=valid_sampler,\n        num_workers=FLAGS['num_workers'],\n        drop_last=False)\n    \n    train_loader = pl.MpDeviceLoader(train_loader, device) # puts the train data onto the current TPU core\n    valid_loader = pl.MpDeviceLoader(valid_loader, device) # puts the valid data onto the current TPU core\n\n    model = MX.to(device) # put model onto the current TPU core\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=FLAGS['lr']*xm.xrt_world_size()) # often a good idea to scale the learning rate by number of cores\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*FLAGS['epochs']) #let's use a scheduler\n\n    gc.collect()\n    \n    xm.master_print(f'========== training fold {FLAGS[\"fold\"]} for {FLAGS[\"epochs\"]} epochs ==========')\n    for i in range(FLAGS['epochs']):\n        xm.master_print(f'EPOCH {i}:')\n        # train one epoch\n        train_loop_fn(train_loader, loss_fn, model, optimizer, device, scheduler)\n                \n        # validation one epoch\n        eval_loop_fn(valid_loader, loss_fn, model, device)\n\n        gc.collect()\n    \n    xm.rendezvous('save_model')\n    \n    xm.master_print('save model')\n    \n    xm.save(model.state_dict(), f'xla_trained_model_{FLAGS[\"epochs\"]}_epochs_fold_{FLAGS[\"fold\"]}.pth')","0c6f2969":"for i in range(5):\n    FLAGS['fold'] = i\n    start_time = time.time()\n    xmp.spawn(run, args=(FLAGS,), nprocs=8, start_method='fork')\n    print('time taken: ', time.time()-start_time)\n    print('==============================================================================')","0b6ba9aa":"# Cassava PyTorch XLA\/TPU starter\n![image.png](attachment:image.png)\n\n### If you found this helpful, please give it an upvote!\n\n## Introduction\n\n[PyTorch XLA](https:\/\/pytorch.org\/xla\/master) is a PyTorch library for XLA support. XLA (Accelerated Linear Algebra) is a domain-specific compiler that was originally meant for compiling and accelerating TensorFlow models. However, other packages, like JAX and now PyTorch XLA can compile program with XLA to accelerate code. TPUs can be programmed with XLA programs and PyTorch XLA provides this interface with TPUs by compiling our PyTorch code as XLA programs to run on TPU devices.\n\nIn this kernel, I provide an in-depth look into how you can use PyTorch XLA to **train a PyTorch model on the TPU** for the Cassava Leaf Disease classification competition.","e3241edf":"Now, **WE ARE DONE!**\n\nIf you enjoyed this kernel, please give it an upvote. If you have any questions or suggestions, please leave a comment!\n\nAlso, check out my [related kernel](https:\/\/www.kaggle.com\/tanlikesmath\/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r) with more detailed information on PyTorch XLA\/TPU training.","5c7271c8":"The below cell will install the [timm]() library, which is what we will use to define our models and get pretrained weights.","40290af8":"Here, I define a class for the PyTorch Dataset (taken from @abhishek's amazing [Tez package](https:\/\/github.com\/abhishekkrthakur\/tez)).","d1c8ccf5":"Finally, we define a main function that we will run on each of the 8 cores of the TPU.","ea7dcac9":"## Training code","6596bd0f":"Let's train all 5 folds!","3fd2dd2e":"## Installs & Imports\n\nThe below cell will install the PyTorch XLA package.","fb24c2af":"These are the flags for training. When you fork (after you upvote of course, ;) ), feel free to play around with these flags!","c5a799b7":"Let's now define our training and validationfunctions. ","b9a0c6b4":"Here are all of our imports!","240d764c":"I now create my folds. Of course, you can replace with your own folds here.","13d12168":"Here, I define a model class for the timm models.","af72eecf":"## Definitions\n\nNow let's define the necessary functions and variables needed for training.","039fcff4":"Let's start training! To do so, we start by initializing the model. We use the `xmp.MpModelWrapper` provided by PyTorch XLA to save memory when initializing the model."}}