{"cell_type":{"ed9a7dc5":"code","26ff2ef7":"code","bb2d13f1":"code","8b87343b":"code","bc744499":"code","e04da9e3":"code","9b59504b":"code","25922dcc":"code","fd4f4a0a":"code","c3920309":"code","cc79e2ef":"code","010fc919":"code","f689beed":"code","536f2f8d":"code","19c6dbe8":"code","fc2d14ac":"code","72536c9e":"code","3278d369":"code","66b4d549":"code","252768d1":"code","e9cd7765":"code","61e2fd3a":"code","807a2029":"code","a4e0ef22":"code","0b994fb2":"code","92e088a6":"code","bd105110":"code","d0e7d1d3":"code","9f257d5a":"code","d1f2f572":"code","31097f4c":"code","e145b0eb":"code","ae7c7279":"code","da854485":"code","cc31349f":"code","fc9e9c30":"code","6550bb55":"code","fd4d2a0b":"code","5251b6b0":"code","9ba0f364":"code","63f47b05":"code","2f4ace0a":"code","dd9f5d9e":"markdown","5f48d492":"markdown","96c328dd":"markdown","1ed06559":"markdown","d61f5233":"markdown","94f9bf19":"markdown","a7666969":"markdown","7f3c2b16":"markdown","907e3c8e":"markdown","bf57c318":"markdown","d887c615":"markdown","fc3e7518":"markdown","5267bd8f":"markdown"},"source":{"ed9a7dc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.compose import ColumnTransformer\nfrom catboost import CatBoostClassifier, Pool\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.pipeline import Pipeline \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26ff2ef7":"train_data = pd.read_csv('\/kaggle\/input\/massp-housing-prices-in-melbourne\/train.csv')\nval_data = pd.read_csv('\/kaggle\/input\/massp-housing-prices-in-melbourne\/test.csv')","bb2d13f1":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","8b87343b":"train_data = reduce_mem_usage(train_data, verbose = True)\nval_data = reduce_mem_usage(val_data, verbose = True)","bc744499":"train_data.shape","e04da9e3":"val_data.shape","9b59504b":"#from pandas_profiling import ProfileReport\n#profile = ProfileReport(train_data, title=\"Pandas Profiling Report\")\n#profile","25922dcc":"from pandas_profiling import ProfileReport\nprofile_train = ProfileReport(train_data, minimal=True)\nprofile_train","fd4f4a0a":"y = train_data['Price']","c3920309":"from sklearn.metrics import mean_squared_log_error\n#eveluation = np.sqrt(mean_squared_log_error( y_test, predictions))","cc79e2ef":"numerical = train_data.select_dtypes(include='number')\n#id kh\u00f4ng li\u00ean quan n\u00ean drop\nnumerical = numerical.drop(columns=['id'])\nnumerical['YearBuiltDur'] = 2021 - train_data['YearBuilt']\nnumerical.head(5)","010fc919":"numerical = numerical.drop(columns = ['YearBuilt','Lattitude','Longtitude'])","f689beed":"#s\u1eed d\u1ee5ng simpleimputer \u0111\u1ec3 fill mean value\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_numerical = pd.DataFrame(my_imputer.fit_transform(numerical))\nimputed_numerical.columns = numerical.columns\nimputed_numerical.head(5)","536f2f8d":"import matplotlib.pyplot as plt\n  \nfig, axes = plt.subplots(2, 2, figsize=(18, 10))\n  \nfig.suptitle('Some Numerical Data vs Price')\n  \nsns.boxplot(ax=axes[0, 0], data= imputed_numerical, x='Rooms', y='Price')\nsns.boxplot(ax=axes[0, 1], data= imputed_numerical, x='Distance', y='Price')\nsns.boxplot(ax=axes[1, 0], data= imputed_numerical, x='Bathroom', y='Price')\nsns.boxplot(ax=axes[1, 1], data= imputed_numerical, x='Landsize', y='Price')\n\n","19c6dbe8":"fig, axes = plt.subplots(3, 2, figsize=(18, 10))\nsns.boxplot(ax=axes[0, 0], data= imputed_numerical, x='Postcode', y='Price')\nsns.boxplot(ax=axes[0, 1], data= imputed_numerical, x='Bedroom2', y='Price')\nsns.boxplot(ax=axes[1, 0], data= imputed_numerical, x='Car', y='Price')\nsns.boxplot(ax=axes[1, 1], data= imputed_numerical, x='Propertycount', y='Price')\nsns.boxplot(ax=axes[2, 0], data= imputed_numerical, x='YearBuiltDur', y='Price')\nsns.boxplot(ax=axes[2, 1], data= imputed_numerical, x='BuildingArea', y='Price')","fc2d14ac":"categorical = train_data[['Price','Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea',\n       'Regionname']]\ncategorical.columns","72536c9e":"categorical","3278d369":"categorical['Regionname'].count","66b4d549":"cate = categorical.drop(columns = ['Price','Suburb','Address', 'SellerG', 'Date',\n       'Regionname'])","252768d1":"from sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(cate))\nOH_cols_train.index = cate.index\ncate.index","e9cd7765":"#ph\u00e2n ra categorical v\u00e0 numerical columns cho test data\nnumerical = numerical.drop(columns=['Price','Bedroom2'])\nnumerical_cols = numerical.columns\ncategorical_cols = cate.columns\n","61e2fd3a":"categorical_cols_1 = categorical[['Suburb','Regionname']]\ncategorical_cols_1 = categorical_cols_1.columns\ncategorical_cols_1","807a2029":"categorical_cols","a4e0ef22":"train_data = train_data.drop(train_data[(train_data['Rooms']>10)|(train_data['Bathroom']>7)|(train_data['Car']>9)].index)","0b994fb2":"train_data['YearBuiltDur'] = 2021 - train_data['YearBuilt']","92e088a6":"#vi\u1ebft pipeline \u0111\u1ec3 l\u1eb7p l\u1ea1i c\u00e1c b\u01b0\u1edbc tr\u00ean v\u1edbi test data\n\n#c\u00e1c b\u01b0\u1edbc preprocessing numerical data\n#c\u00f3 x\u00e0i SimpleImputer fill mean\nnumerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='constant'))])#,('scale',StandardScaler()),('PCA',StandardScaler())])\n\n#c\u00e1c b\u01b0\u1edbc preprocessing categorical data\n#c\u00f3 x\u00e0i onehotencode\n#th\u1eed th\u00eam imputer coi ntn\ncategorical_transformer = Pipeline(steps = [('imputer',SimpleImputer(strategy='constant')),('onehot',OneHotEncoder(handle_unknown = 'ignore'))])\n#categorical_transformer_1 = Pipeline(steps = [('imputer',SimpleImputer(strategy='most_frequent')),('label',LabelEncoder())])\n#g\u00f3i 2 th\u1eb1ng l\u1ea1i\npreprocessor = ColumnTransformer(transformers = [('cat',categorical_transformer,categorical_cols),('num',numerical_transformer,numerical_cols)])\n","bd105110":"#ph\u00e2n data\n#from sklearn.model_selection import train_test_split\n#X_train , X_valid, y_train, y_valid = train_test_split(train_data, y)\n#X_valid.shape","d0e7d1d3":"y = train_data.Price","9f257d5a":"train_data = train_data.drop(columns=['id','Price','Lattitude','Longtitude','YearBuilt','Date'])","d1f2f572":"#y = np.log1p(y)","31097f4c":"#x\u1eed d\u1ee5ng xgboost\nfrom catboost import CatBoostRegressor\n#from sklearn.model_selection import train_test_split\n#X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.2)\n#X_train, X_learn, y_train, y_learn = train_test_split(X_train,y_train,test_size = 0.2)\n#x_dummy = encoder.transform(X_learn)\nfrom xgboost import XGBRegressor\n#my_model = XGBRegressor(n_estimators = 1000, learning_rate = 0.01)\nmy_model= XGBRegressor(n_estimators = 2000,\n                            max_depth = 12,\n                            learning_rate = 0.02,\n                            subsample = 0.8,\n                          \n                            random_state = 42,\n                           tree_method = 'gpu_hist'\n                           )\n#my_model = CatBoostRegressor()\nmy_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',my_model)])\n#my_pipeline.fit(X_train, y_train, model__early_stopping_rounds = 5, model__eval_set=[(x_dummy,y_learn)], model__verbose= False)","e145b0eb":"train_data ","ae7c7279":"train_data[numerical.columns]","da854485":"#ch\u1ea1y cross validation \u0111\u1ec3 check \u0111i\u1ec3m \u0111\u00fang h\u01a1n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = -1 * cross_val_score(my_pipeline, train_data ,y,cv=3,scoring = 'neg_mean_squared_log_error')\nprint(np.sqrt(scores.mean()))","cc31349f":"my_pipeline.fit(train_data,y)","fc9e9c30":"val_data['YearBuiltDur'] = 2021 - val_data['YearBuilt']","6550bb55":"val_data = val_data.drop(columns=['Lattitude','Longtitude','YearBuilt','Date','id'])","fd4d2a0b":"print(train_data.shape)\nprint(val_data.shape)","5251b6b0":"#predict test set\npredicted_X_test = my_pipeline.predict(val_data)","9ba0f364":"#predicted_X_test = np.expm1(predicted_X_test)","63f47b05":"test  = pd.read_csv('..\/input\/massp-housing-prices-in-melbourne\/test.csv')\nmy_submission = pd.DataFrame({'id': test.id, 'Price': predicted_X_test})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('.\/submission.csv', index=False)","2f4ace0a":"my_submission.head(10)","dd9f5d9e":"feature = 'Type'\nX = train_data[feature].dropna()\nfig , (ax1, ax2) = plt.subplots(1,2, figsize=(14,7), sharex=True)\nplt.subplot(ax1)\nplot_feature(X)\nplt.subplot(ax2)\nif train_data[feature].dtype == np.dtype('object'):\n    sns.countplot(X)\nelse:\n    sns.distplot(X)","5f48d492":"#multicollinearity, \u1ea3nh h\u01b0\u1edfng t\u1edbi Logistic Regression\n#s\u1eed d\u1ee5ng Variance Inflation Factor \u0111\u1ec3 check multicollinearity\n#VIF= 1, model ngon l\u00e0nh\n#VIF<5, t\u1ea1m \u1ed5n\n#VIF>5, sai\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n#function for VIF\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = OH_cols_train.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(OH_cols_train.values, i)\n                          for i in range(len(OH_cols_train.columns))]\n  \nprint(vif_data)","96c328dd":"# 1. GI\u1edaI THI\u1ec6U DATA","1ed06559":"**1.3 Metric \u0111\u00e1nh gi\u00e1**\n    - S\u1eed d\u1ee5ng Root Mean Squared Logarithmic Error","d61f5233":"X\u1eed d\u1ee5ng Pandas Report \u0111\u1ec3 nh\u00ecn l\u1eb9 Data 2 file","94f9bf19":"# 3. Feature Engineering","a7666969":"T\u1ed5ng qu\u00e1t:\n- **1. GI\u1edaI THI\u1ec6U DATA:**\n    - 1.1 Th\u00f4ng tin t\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u\n    - 1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t v\u1ec1 Target\n    - 1.3 Metric \u0111\u00e1nh gi\u00e1\n- **2. KHAI PH\u00c1 D\u1eee LI\u1ec6U**\n    - 2.1 D\u1eef li\u1ec7u Numerical\n    - 2.2 D\u1eef li\u1ec7u Categorical\n    - 2.3 T\u1ed5ng h\u1ee3p c\u00e1c d\u1eef li\u1ec7u l\u1ea5y l\u00e0m Feature\n- **3. FEATURE ENGINEERING**\n    - 3.1 X\u1eed l\u00fd d\u1eef li\u1ec7u numerical\n    - 3.2 X\u1eed l\u00fd d\u1eef li\u1ec7u Categorical\n- **4. X\u00c2Y D\u1ef0NG M\u00d4 H\u00ccNH**","7f3c2b16":"**1.1 T\u1ed5ng qu\u00e1t th\u00f4ng tin d\u1eef li\u1ec7u**","907e3c8e":"**2.1 D\u1eef li\u1ec7u Numerical**:\n- C\u00f3 14 h\u00e0ng d\u1eef li\u1ec7u Numerical\n- Ta s\u1ebd \u0111i t\u1eebng h\u00e0ng v\u00e0 ph\u00e2n t\u00edch mqh gi\u1eefa ch\u00fang v\u00e0 Target","bf57c318":"# 2. KHAI PH\u00c1 D\u1eee LI\u1ec6U","d887c615":"- X\u1eed d\u1ee5ng One Hot Encoder x\u1eed l\u00fd","fc3e7518":"**1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t Target**","5267bd8f":"**2.2 D\u1eef li\u1ec7u Categorical**\n - 8 tr\u01b0\u1eddng categorical"}}