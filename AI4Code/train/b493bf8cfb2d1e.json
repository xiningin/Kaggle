{"cell_type":{"8c6511c3":"code","436b5fcc":"code","cb2234df":"code","b657fb45":"code","04c27080":"code","c9d6c2a4":"code","da158d9d":"code","b454afcc":"code","cfb4e2a3":"code","8ba255aa":"code","55add404":"code","915d0d3d":"code","7870e73b":"code","868f1558":"code","4f3f4f9b":"code","b40d33ca":"code","65c5286a":"code","04d3f9b5":"markdown","ca7944ca":"markdown","8bee3445":"markdown","48588bee":"markdown","43808493":"markdown","800e6cc8":"markdown","b1ddd7c7":"markdown","5a870f73":"markdown","0195da42":"markdown","9d2dad9e":"markdown","22db3646":"markdown","10a60371":"markdown","6da8fc55":"markdown","17a49d33":"markdown","c38c8a52":"markdown","e67ecc25":"markdown","5bfa2f3e":"markdown"},"source":{"8c6511c3":"import os\nfrom IPython.display import Image\nImage(filename=\"..\/input\/images\/fake.jpg\", width= 1000, height=1000)","436b5fcc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport re, string\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\n","cb2234df":"data_news = pd.read_csv('..\/input\/fake-news-detection\/data.csv')\ndata_news.head()","b657fb45":"print(data_news.shape)\nprint(50*'-')\nprint(data_news.info())","04c27080":"data_news['Body'] = data_news['Headline']+data_news['Body']\ndata_news = data_news.drop(['URLs', 'Headline'], axis=1)\ndata_news.head()","c9d6c2a4":"data_news.isna().sum()","da158d9d":"data_news.dropna(inplace=True)","b454afcc":"data_news.isna().sum()","cfb4e2a3":"plt.figure(figsize=(6,4))\nsns.countplot(data_news['Label'], palette='pastel')","8ba255aa":"def clean_data(news):\n    punc = set(string.punctuation)\n    news = ''.join(ch for ch in news if ch not in punc)\n    return news\n    \n    news = news.lower()\n    news = str(text).lower()\n    news = re.sub('\\[.*?\\]', '', news)\n    news = re.sub('https?:\/\/\\S+|www\\.\\S+', '', news)\n    news = re.sub('<.*?>+', '', news)\n    news = re.sub('[%s]' % re.escape(string.punctuation), '', news)\n    news = re.sub('\\n', '', news)\n    news = re.sub('\\w*\\d\\w*', '', news)\n    news = re.sub('Reuters','',news)\n    news = re.sub(r\"\\d+\", \"\", news)\n    \ndata_news['Body'] = data_news['Body'].apply(lambda x: clean_data(x))\ndata_news['Body']","55add404":"stopword = stopwords.words('english')\ndata_news['Body'] = data_news['Body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopword]))\ndata_news['Body']","915d0d3d":"def lemmatize_word(news):\n    lem = WordNetLemmatizer()\n    lemmatizer = ''.join([lem.lemmatize(i) for i in news.split()])\n    return news\n\ndata_news['Body'] = data_news['Body'].apply(lambda x: lemmatize_word(x))","7870e73b":"#Define the label and feature\nX = data_news['Body']\ny = data_news['Label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","868f1558":"vectorizer = CountVectorizer(stop_words='english')\ncount_train = vectorizer.fit_transform(X_train.values)\ncount_test = vectorizer.transform(X_test.values)\nprint(count_test)\nprint(count_train)","4f3f4f9b":"# Model 1 - default parameter \n\nnb_classifier1 = MultinomialNB()\nnb_classifier1.fit(count_train, y_train)\n\npred1 = nb_classifier1.predict(count_test)\n\nprint(classification_report(y_test, pred1, target_names = ['Fake','True']))","b40d33ca":"# 1\n\nsvc_model1 = SVC(C=1, kernel='linear', gamma= 1)\nsvc_model1.fit(count_train, y_train)\n\nprediction1 = svc_model1.predict(count_test)\n\nprint(classification_report(y_test, prediction1, target_names = ['Fake','True']))","65c5286a":"# 2\nsvc_model2 = SVC(C= 100, kernel='linear', gamma= 1)\nsvc_model2.fit(count_train, y_train)\n\nprediction2 = svc_model2.predict(count_test)\n\nprint(classification_report(y_test, prediction2, target_names = ['Fake','True']))","04d3f9b5":"*Let's clean the dataset, especially for the feature one.*","ca7944ca":"**Lemmatize Words**","8bee3445":"So, here we go with 97% Accuracy, yeay!\nThanks for your attention! ","48588bee":"*Before we start to preprocessing the dataset, we have to make sure that there is no any missing values. It could be worst our preprocessing process.*","43808493":"# Explanatory Data Analysis","800e6cc8":"# Visualization","b1ddd7c7":"*So, we're gonna merge the Headline and Body columns and remove the unnecessary feature.*","5a870f73":"I think to celebrate **World Press Freedom Day (WPFD) on 03 May 2021**, it will be fun to do something about press. Since we know that hoax or fake pers is almost around the world and give wors impact. So, I think it will be nice if we learn about predicting fake or real news, especially for me as a beginner in this field.\nI choose this dataset that completely easy to use for me as well.\n*Let's do it!*","0195da42":"**Let's Train Our Feature**","9d2dad9e":"**Don't forget to vote this code.**\n**Hope you have a nice day!**","22db3646":"*Looks there is a lot missing values, 21 missing values in Body columns. Here we go to drop it.*","10a60371":"# Missing Values","6da8fc55":"**Remove the Stopwords**","17a49d33":"# Wellcome, Guys!","c38c8a52":"**Import Library Package**","e67ecc25":"# Model","5bfa2f3e":"# Data Cleaning"}}