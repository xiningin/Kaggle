{"cell_type":{"c203dd0d":"code","01c62cc2":"code","7ba24352":"code","f3fa5b04":"code","cb7dab77":"code","747db641":"code","c632e936":"code","7b06cfd2":"code","28cdec20":"code","95e2a403":"code","2191abe5":"code","ecc2e86b":"code","7513b398":"code","a4ddfccb":"code","c319a386":"code","a2eeba89":"code","433362b6":"code","adfbcb70":"code","07b35c04":"code","0e89336d":"code","c8e4ef56":"code","6b8fa7c4":"code","58f48fcd":"code","654ceafd":"code","fb6f0763":"code","7044c6f5":"code","c981d22a":"code","de93787b":"code","598b7be9":"code","31b9746c":"code","562a5502":"code","c9f25d4f":"code","b63c8780":"code","5323a14a":"code","9e481c7d":"code","65e416d5":"code","4bb86d61":"code","c528d037":"code","09b07713":"code","f5917caf":"code","0e338521":"code","1fab3a6e":"code","abfae3dd":"code","e8fb73ad":"code","00637f19":"code","304c1d08":"code","eef85208":"code","82796e80":"code","07492bd4":"code","bcb828c2":"code","7f5cffe1":"code","dd91ea8e":"code","c2265c7a":"code","9f8b64a4":"code","e6bc49aa":"code","b4c742b6":"code","4ced0039":"code","5c5b35a7":"code","c7caa902":"code","26ccddb3":"code","929b2c23":"code","719ac90a":"code","93fd9020":"code","4cc65024":"code","030f7946":"code","86e3aabc":"code","4f111ff2":"code","08517233":"code","d103d6da":"code","79eb756c":"code","06493ca8":"code","4701461a":"code","52de9784":"code","442a3abe":"code","23a6f237":"code","80d089a0":"code","15266b55":"code","8456818a":"code","94158e40":"code","4a39a95c":"code","619704e4":"code","b8cef2c7":"code","7018cff1":"code","f6b1ebff":"code","d2654836":"code","e2b2e6f8":"code","759f1109":"code","c1277fc0":"code","c705be81":"code","0bb9725d":"code","f88ec4ee":"code","b722fd84":"code","9c201295":"code","aac7104e":"code","a86beb63":"code","a546da58":"code","12d54a08":"code","ed434729":"code","56231aef":"code","a27797c2":"code","10707fe7":"code","c7dcc176":"code","48809754":"code","31818d10":"code","0ae735ff":"code","b9b269fb":"code","b6d322db":"code","e3b3b42e":"code","5d46e107":"code","8c676757":"code","9e5c1e7e":"code","13ce67b5":"code","d625d956":"code","924c85f5":"code","d0affc0d":"code","6423f5e3":"code","f24ad974":"code","4cdc4ff9":"code","05ec2fdb":"code","c1d79378":"code","c89b174f":"code","e8d4dc3d":"code","9ba074bf":"code","c2177cf2":"code","e7754f87":"code","e44846bb":"code","064a2bb2":"code","428a90f9":"code","7fad9057":"code","ffbf693e":"code","d7e59740":"code","9b8507bc":"markdown","cd48573f":"markdown","9d0c5e05":"markdown","5351b693":"markdown","996c7b95":"markdown","4a781116":"markdown","45bcf143":"markdown","4c5b3f67":"markdown","0e086ade":"markdown","737901ae":"markdown","c6575a03":"markdown","6c12e0c2":"markdown","b620cc2b":"markdown","230aa7ba":"markdown","16df7a71":"markdown","62c0eeb9":"markdown","dcbf867b":"markdown","1f72c226":"markdown","09c8b400":"markdown","b659f667":"markdown","e1aad375":"markdown","bff15e24":"markdown","ad2331a8":"markdown","e758032b":"markdown","c99130ca":"markdown","a717930f":"markdown","ee20eeb7":"markdown","28176557":"markdown","2c1514b6":"markdown","2cffd7bc":"markdown","41f5bb6e":"markdown","23fec384":"markdown","797209a2":"markdown","3a10171a":"markdown","b5484892":"markdown","efaeed56":"markdown","c5df89e9":"markdown","cf15176a":"markdown","d043673b":"markdown","c87aa73d":"markdown","c03e638a":"markdown","51931472":"markdown","dd822cc8":"markdown"},"source":{"c203dd0d":"# importing  necesary files & libraries ","01c62cc2":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport scipy.stats as ss\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\n%timeit \n%matplotlib inline","7ba24352":"dftrain=pd.read_csv(\"..\/input\/train.csv\")\ndftest=pd.read_csv(\"..\/input\/test.csv\")\ntest=dftest.copy()","f3fa5b04":"dftrain.info()","cb7dab77":"dftrain.head()","747db641":"dftrain.info()","c632e936":"# analysis of dtypes ","7b06cfd2":"plt.figure(figsize=(5,5))\nsns.set(font_scale=2)\nsns.countplot(y=dftrain.dtypes ,data=dftrain)\nplt.xlabel(\"count of each data type\")\nplt.ylabel(\"data types\")\nplt.show()","28cdec20":"import missingno as msno","95e2a403":"msno.bar(dftrain.sample(890))","2191abe5":"msno.matrix(dftrain)","ecc2e86b":"msno.heatmap(dftrain)","7513b398":"msno.dendrogram(dftrain)","a4ddfccb":"df=dftrain.copy()\ndf.head()","c319a386":"\nmale1=df.loc[(df.Survived==1) &(df.Sex=='male'),:].count()\nfemale1=df.loc[(df.Survived==1) & (df.Sex=='female'),:].count()\n","a2eeba89":"print(male1)","433362b6":"print(female1)","adfbcb70":"sns.factorplot(x=\"Sex\",col=\"Survived\", data=df , kind=\"count\",size=6, aspect=.7,palette=['crimson','lightblue'])\nmalecount=pd.value_counts((df.Sex == 'male') & (df.Survived==1))\nfemalecount=pd.value_counts((df.Sex=='female') & (df.Survived==1))\ntotalmale,totalfemale=pd.value_counts(df.Sex)\nprint(\"male survived {} , female survived {}\".format(malecount\/totalmale,femalecount\/totalfemale))","07b35c04":"plt.figure(figsize=(12,12))\nsns.swarmplot(x=\"Sex\",y=\"Age\",hue='Pclass',data=df,size=10 ,palette=['pink','lightgreen','purple'])","0e89336d":"plt.figure(figsize=(12,12))\nsns.swarmplot(x=\"Sex\",y=\"Age\",hue='Survived',data=df,size=10)","c8e4ef56":"\nsns.factorplot(x=\"Sex\", hue = \"Pclass\" , col=\"Survived\", data=df , kind=\"count\",size=7, aspect=.7,palette=['crimson','orange','lightblue'])","6b8fa7c4":"pd.crosstab([df.Sex,df.Survived],df.Pclass, margins=True).style.background_gradient(cmap='autumn_r')","58f48fcd":"pd.crosstab([df.Survived,df.Pclass],df.Age,margins=True).style.background_gradient(cmap='autumn_r')","654ceafd":"sns.factorplot(x=\"Survived\",col=\"Embarked\",data=df ,hue=\"Pclass\", kind=\"count\",size=8, aspect=.7,palette=['crimson','darkblue','purple'])","fb6f0763":"pd.crosstab([df.Survived],[df.Sex,df.Pclass,df.Embarked],margins=True).style.background_gradient(cmap='autumn_r')","7044c6f5":"sns.factorplot(x=\"Sex\", y=\"Survived\",col=\"Embarked\",data=df ,hue=\"Pclass\",kind=\"bar\",size=7, aspect=.7)","c981d22a":"context1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ndf['Sex_bool']=df.Sex.map(context1)\ndf[\"Embarked_bool\"] = df.Embarked.map(context2)\nplt.figure(figsize=(20,20))\ncorrelation_map = df[['PassengerId', 'Survived', 'Pclass', 'Sex_bool', 'Age', 'SibSp',\n       'Parch', 'Fare' , 'Embarked_bool']].corr()\nsns.heatmap(correlation_map,vmax=.7, square=True,annot=True,fmt=\".2f\")","de93787b":"df.groupby(\"Pclass\").Age.mean()","598b7be9":"df.isnull().sum()\n","31b9746c":"for x in [dftrain, dftest,df]:\n    x['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        x.loc[ x['Age'] <= i*10, 'Age_bin'] = i","562a5502":"df[['Age','Age_bin']].head(20)","c9f25d4f":"plt.figure(figsize=(20,20))\nsns.set(font_scale=1)\nsns.factorplot('Age_bin','Survived', col='Pclass' , row = 'Sex',kind=\"bar\", data=df)","b63c8780":"df.describe()","5323a14a":"for x in [dftrain, dftest , df]:\n    x['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        x.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i","9e481c7d":"fig, axes = plt.subplots(2,1)\nfig.set_size_inches(20, 18)\nsns.kdeplot(df.Age_bin , shade=True, color=\"red\" , ax= axes[0])\nsns.kdeplot(df.Fare , shade=True, color=\"red\" , ax= axes[1])","65e416d5":"df.isnull().sum()","4bb86d61":"\n\nmodel= ols('Age~ Pclass + Survived + SibSp',data=df).fit()\nprint(model.summary())","c528d037":"dftrain.info()","09b07713":"dftest.info()","f5917caf":"np.where(dftrain[\"Embarked\"].isnull())[0]","0e338521":"sns.factorplot(x='Embarked',y='Fare', hue='Pclass', kind=\"box\",order=['C', 'Q', 'S'],data=dftrain, size=7,aspect=2)\n\n# ... and median fare\nplt.axhline(y=80, color='r', ls='--')","1fab3a6e":"dftrain.loc[[61,829],\"Embarked\"] = 'C'","abfae3dd":"dftrain.info()","e8fb73ad":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndftrain['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# get average, std, and number of NaN values\naverage_age = dftrain[\"Age\"].mean()\nstd_age = dftrain[\"Age\"].std()\ncount_nan_age = dftrain[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_age = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n\n# fill NaN values in Age column with random values generated\nage_slice = dftrain[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\n# plot imputed Age values\nage_slice.astype(int).hist(bins=70, ax=axis2)","00637f19":"dftrain[\"Age\"] = age_slice","304c1d08":"dftrain.info()","eef85208":"dftrain=dftrain.drop('Age_bin',axis=1)","82796e80":"dftrain.info()","07492bd4":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndftest['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# get average, std, and number of NaN values\naverage_age = dftest[\"Age\"].mean()\nstd_age = dftest[\"Age\"].std()\ncount_nan_age = dftest[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_age = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n\n# fill NaN values in Age column with random values generated\nage_slice = dftest[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\n# plot imputed Age values\nage_slice.astype(int).hist(bins=70, ax=axis2)","bcb828c2":"dftest[\"Age\"] = age_slice","7f5cffe1":"dftest.info()","dd91ea8e":"dftest.info()","c2265c7a":"plt.figure(figsize=(20,20))\nsns.factorplot(x='Fare',y='Cabin',data=dftrain,size=20)","9f8b64a4":"family_df = dftrain.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df[\"Fsize\"] = family_df.SibSp + family_df.Parch + 1\n\nfamily_df.head()","e6bc49aa":"plt.figure(figsize=(15,5))\n\n# visualize the relationship between family size & survival\nsns.countplot(x='Fsize', hue=\"Survived\", data=family_df)","b4c742b6":"dftrain['Fsize']=family_df['Fsize']","4ced0039":"dftrain.info()\n","5c5b35a7":"family_df_t= dftest.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df_t[\"Fsize\"] = family_df_t.SibSp + family_df_t.Parch + 1\n\nfamily_df_t.head()","c7caa902":"dftest['Fsize']=family_df_t['Fsize']","26ccddb3":"dftest.info()","929b2c23":"#dftest=dftest.drop('Cabin',axis=1)","719ac90a":"dftest.info()","93fd9020":"np.where(dftest[\"Fare\"].isnull())[0]","4cc65024":"dftest.ix[[152]]","030f7946":"dftest.loc[[152],\"Fare\"] = 10","86e3aabc":"dftest.ix[[152]]","4f111ff2":"dftest.info()","08517233":"dftrain.info()","d103d6da":"family_df_tr= dftrain.loc[:,[\"Parch\", \"SibSp\", \"Survived\"]]\n\n# Create a family size variable including the passenger themselves\nfamily_df_tr[\"Fsize\"] = family_df_tr.SibSp + family_df_tr.Parch + 1\n\nfamily_df_tr.head()","79eb756c":"dftrain['Fsize']=family_df_tr['Fsize']\n","06493ca8":"dftrain['Fsize'].dtype","4701461a":"dftrain.info()","52de9784":"dftest.info()","442a3abe":"import scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\n#Initialize ChiSquare Class\ncT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Embarked','Cabin','Pclass','Age','Name','Fare','Fare_bin','Fsize']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Survived\" )  ","23a6f237":"# Make a copy of the titanic data frame\ndftrain['Title'] = dftrain['Name']\n\n# Grab title from passenger names\ndftrain[\"Title\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)","80d089a0":"rare_titles = ['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\ndftrain['Title'].replace(rare_titles, \"Rare title\", inplace=True)\n\n# Also reassign mlle, ms, and mme accordingly\ndftrain['Title'].replace([\"Mlle\",\"Ms\", \"Mme\"], [\"Miss\", \"Miss\", \"Mrs\"], inplace=True)","15266b55":"dftrain.info()","8456818a":"cT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Embarked','Cabin','Pclass','Age','Name','Fare','Fare_bin','Fsize','Title','SibSp','Parch']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Survived\" )  ","94158e40":"dftest.info()","4a39a95c":"dftrain.info()","619704e4":"dftest=dftest.drop(['Ticket','PassengerId'],axis=1)\n","b8cef2c7":"dftest.info()","7018cff1":"dftest['Title'] = dftest['Name']\n\n# Grab title from passenger names\ndftest[\"Title\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)\n","f6b1ebff":"dftest.info()","d2654836":"dftrain.info()","e2b2e6f8":"dftrain.head()","759f1109":"dftrain=dftrain.drop('Name',axis=1)","c1277fc0":"dftrain.head()","c705be81":"context1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ndftrain['Sex_bool']=dftrain.Sex.map(context1)\ndftrain[\"Embarked_bool\"] = dftrain.Embarked.map(context2)","0bb9725d":"dftrain.head()","f88ec4ee":"#dftrain=dftrain.drop(['Sex','Embarked'],axis=1)\ncontext3= {\"Mr\":0 , \"Mrs\":1 , \"Miss\":2,'Master':3}\ndftrain['Title']=dftrain.Title.map(context3)","b722fd84":"dftrain.head()","9c201295":"dftrain=dftrain.drop(['PassengerId','Cabin','Ticket'],axis=1)\nplt.figure(figsize=(14,4))\nsns.boxplot(data=dftrain)","aac7104e":"reserve=dftrain.copy()\nreserve.shape","a86beb63":"dftrain.head()","a546da58":"dftrain=dftrain.drop(['Embarked','Sex'],axis=1)\ndftrain.head()","12d54a08":"#dftrain=dftrain[np.abs(zscore(dftrain)<3).all(axis=1)]","ed434729":"dftest.head()","56231aef":"context1 = {\"female\":0 , \"male\":1}\ncontext2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ndftest['Sex_bool']=dftest.Sex.map(context1)\ndftest[\"Embarked_bool\"] = dftest.Embarked.map(context2)\ncontext3= {\"Mr\":0 , \"Mrs\":1 , \"Miss\":2,'Master':3}\ndftest['Title']=dftest.Title.map(context3)","a27797c2":"dftest.head()","10707fe7":"dftest=dftest.drop(['Name','Sex','Embarked'],axis=1)","c7dcc176":"dftest.head()","48809754":"for x in [dftrain, dftest,df]:\n    x['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        x.loc[ x['Age'] <= i*10, 'Age_bin'] = i","31818d10":"dftrain.head()","0ae735ff":"dftest.head()","b9b269fb":"#dftrain=dftrain.drop(['Fare_bin'],axis=1)\n#dftest=dftest.drop(['Fare_bin'],axis=1)\nfor x in [dftrain, dftest,df]:\n    x['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        x.loc[ x['Fare'] <= i*10, 'Fare_bin'] = i","b6d322db":"dftrain.head()","e3b3b42e":"dftest.head()","5d46e107":"dftrain=dftrain.drop('Age',axis=1)\ndftest=dftest.drop('Age',axis=1)","8c676757":"dftrain.head()\ndftrain=dftrain.convert_objects(convert_numeric=True)\n","9e5c1e7e":"def change_type(df):\n    float_list=list(df.select_dtypes(include=[\"float\"]).columns)\n    print(float_list)\n    for col in float_list:\n        df[col]=df[col].fillna(0).astype(np.int64)\n        \n    return df    \nchange_type(dftrain)    \ndftrain.dtypes","13ce67b5":"#dftrain=dftrain.drop(['Fare'],axis=1)\n#dftest=dftest.drop(['Fare','Cabin'],axis=1)\nx=dftrain.iloc[:,1:].values\ny=dftrain.iloc[:,0].values\nprint(dftrain.columns)\nprint(dftest.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)","d625d956":"dftest=dftest.convert_objects(convert_numeric=True)\nchange_type(dftest)    \ndftest.dtypes","924c85f5":"MLA = []\nZ = [LinearSVC() , DecisionTreeClassifier() , LogisticRegression() , KNeighborsClassifier() , GaussianNB() ,\n    RandomForestClassifier() , GradientBoostingClassifier()]\nX = [\"LinearSVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"KNeighborsClassifier\" , \"GaussianNB\" ,\n    \"RandomForestClassifier\" , \"GradientBoostingClassifier\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    MLA.append(accuracy_score(pred , y_test))","d0affc0d":"MLA","6423f5e3":"sns.kdeplot(MLA , shade=True, color=\"red\")","f24ad974":"d = { \"Accuracy\" : MLA , \"Algorithm\" : X }\ndfm = pd.DataFrame(d)","4cdc4ff9":"dfm","05ec2fdb":"sns.barplot(x=\"Accuracy\", y=\"Algorithm\", data=dfm)","c1d79378":"# imporvsing the model first logistic Regression\nparams={'C':[1,100,0.01,0.1,1000],'penalty':['l2','l1']}\nlogreg=LogisticRegression()\ngscv=GridSearchCV(logreg,param_grid=params,cv=10)\n%timeit gscv.fit(x,y)","c89b174f":"gscv.best_params_","e8d4dc3d":"logregscore=gscv.best_score_\nprint(logregscore)","9ba074bf":"gscv.predict(X_test)\ngscv.score(X_test,y_test)","c2177cf2":"rfcv=RandomForestClassifier(n_estimators=500,max_depth=6)\nrfcv.fit(X_train,y_train)\nrfcv.predict(X_test)\nrfcv.score(X_test,y_test)\n","e7754f87":"gbcv=GradientBoostingClassifier(learning_rate=0.001,n_estimators=2000,max_depth=5)\ngbcv.fit(X_train,y_train)\ngbcv.predict(X_test)\ngbcv.score(X_test,y_test)","e44846bb":"param={'n_neighbors':[3,4,5,6,8,9,10],'metric':['euclidean','manhattan','chebyshev','minkowski'] }       \nknn = KNeighborsClassifier()\ngsknn=GridSearchCV(knn,param_grid=param,cv=10)\ngsknn.fit(x,y)                         \n                                                ","064a2bb2":"gsknn.best_params_","428a90f9":"gsknn.best_score_","7fad9057":"gsknn.predict(X_test)","ffbf693e":"gsknn.score(X_test,y_test)\n","d7e59740":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, gscv.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rfcv.predict_proba(X_test)[:,1])\nknn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, gsknn.predict_proba(X_test)[:,1])\ngbc_fpr, gbc_tpr, ada_thresholds = roc_curve(y_test, gbcv.predict_proba(X_test)[:,1])\n\nplt.figure(figsize=(9,9))\nlog_roc_auc = roc_auc_score(y_test, gscv.predict(X_test))\nprint (\"logreg model AUC = {} \" .format(log_roc_auc))\nrf_roc_auc = roc_auc_score(y_test, rfcv.predict(X_test))\nprint (\"random forest model AUC ={}\" .format(rf_roc_auc))\nknn_roc_auc = roc_auc_score(y_test, gsknn.predict(X_test))\nprint (\"KNN model AUC = {}\" .format(knn_roc_auc))\ngbc_roc_auc = roc_auc_score(y_test, gbcv.predict(X_test))\nprint (\"GBC Boost model AUC = {}\" .format(gbc_roc_auc))\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression')\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest')\n\n# Plot Decision Tree ROC\nplt.plot(knn_fpr, knn_tpr, label=' KnnClassifier')\n\n# Plot GradientBooseting Boost ROC\nplt.plot(gbc_fpr, gbc_tpr, label='GradientBoostingclassifier')\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate' 'k--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()\n","9b8507bc":"# and conversion of object data type to categorical is necessary for reducing the space in memory and decrease  time in computation ","cd48573f":"### cleanning the dftrain","9d0c5e05":"# our finding says that when cabin and age values will come and will be null together where as in case of emabarked it is reverse ","5351b693":"# Embarked  S survived more in female in every passenger class, in male also same the same \n# Least is from Q Embarked from both the case","996c7b95":"### first filling in train set 1> embarked 2> then age and after that i will create a new varibale family size let's see what this do","4a781116":"# Now a pie chart percentage of Categories of people travelling survived ","45bcf143":"# Correlation Analysis ","4c5b3f67":"# OSEMN approach known as awesome , readers when read this notebook will get to know about this approach ","0e086ade":"# data cleanning jobs are pending 1> filling of null values , imputaion 3> oulier cleaning","737901ae":"# so we can see from red line that those who embarked from C HAS PAY 80 NEARLY , i was droppin these values , but now now ","c6575a03":"# now with dealing with age values ","6c12e0c2":"# Titanic In depth Analysis , my aims will be like getting insigts from stats test , some useful knowledge to show and atlast which machine learning algorithm is best and why graphs , learning curves visualization , and most common facing Data Leaking will not be happen , which ","b620cc2b":"### overall the males and females of Pclass 3 died more than others\n### the males of Pclass 3 showed a remarkable increase in death and shoots the graph up , same goes to the females in\n### same goes to the females in survived = 0\n### in survived = 0 , showing increasing trend in death as class shifts down\n### In survived = 1 females showed a near fall down trend as expected but pclass=2 females survived less than the Pclass=3 females\n### But the males on contrary showed a dip in between i.e.\n### in males who survived , Plass --> 3 > 1 > 2\n### i.e Survived Pclass=3 males survived more than the survived Pclass=1 males and survived Pclass=2 males\n### the above is evident from the following inspection","230aa7ba":"# from this we are concluding a fact that only 38.8 % people survived , and even most young people died in this disaster about age of 30 ","16df7a71":"#  Embarked Analysis\nmost male survived from emabarked C > S > Q\n\nfrom  Q emabarked on 3 class passengers werae travelling and only 20 % were got saved and Q emabarked is the only were females from 1 class and 2 class not died ","62c0eeb9":"# so we can males which are less suvived were lower class people , labours , and bachelors but in first class as we can see out 77 male  45 sruvived nearly 60 percent  , money vs humanity , sad reality money wins always","dcbf867b":"# swarmplot inferences - \n# 1> like there age group from 0 to 10 , 20 to 40 , 40 above till 52 then 55 to 65 then most likely grand parents\n\n# 2> males are comparetively less in 1st class than female , females of higher class were most of them alone or not couple , travelling with friends , very few couples\n# 3> couples are very less since male red marks are more , so we can these were mostly labours , small kids, bachelors and they were in class 3 passengers\n# 4> most aged group were more in special 1st class than  2nd class","1f72c226":"# Now going for KNN","09c8b400":"# now appending both the test and train frames first ","b659f667":"# so female survived more in than male since , during disaster , females are send from the ship first and siblings too. ","e1aad375":"# nullity analysis ","bff15e24":"# now inference with respect to survivebiltity and age factor","ad2331a8":"# How to deal with null values as we can see cabin , AGE , EMBARKED are having most null values ","e758032b":"# Random forest accuracy increased to 82.4%","c99130ca":"### Now we go for Feature Engg finally as we have far much analysis and intitution is coming a use polynomial features too like\n### features like Pclass with fare -> survived and many more combos with another\n","a717930f":"# now nultity correlation wehave to see between age , cabin and embarked","ee20eeb7":"# as we can see that object data types are more and with equal to int , can increase computation time , need conversion in cat.dtype","28176557":"# Gradient Boosting Classifier increased to 1.3% accuracy","2c1514b6":"# The above heatmap shows the overall picture very clearly\n### PassengerId is a redundant column as its very much less related to all other attributes , we can remove it .\n### Also , Survived is related indirectly with Pclass and also we earlier proved that as Pclass value increases Survival decreases\n### Pclass and Age are also inversely related and can also be proven by the following cell that as Pclass decreases , the mean of the Age increases , means the much of the older travellers are travelling in high class .\n### Pclass and fare are also highly inversely related as the fare of Pclass 1 would obviously be higher than corresponding Pclass 2 and 3 .\n### Also , people with lower ages or children are travelling with their sibling and parents more than higher aged people (following an inverse relation) , which is quite a bit obvious .\n### Parch and SibSp are also highly directly related\n### Sex_bool and Survived people are highly inversely related , i.e. females are more likely to survive than men","2cffd7bc":"# kids on 1st class was one found and died , even 1st class children weren't there \n# richer famlies were without chidren and even in that too many were bachelors.\n# 28 children were died in Plcass 3 and children 1st class 99% were saved , in second class 100% saved \n# youth from 3 class died more than any class","41f5bb6e":"# 18% of male survived and 74% percent female survived ","23fec384":"# Most of the youth male died , siblings survived (both) , only i can say that high class passengers in male might got save ","797209a2":"# now both frames are filled up now we are left with cabin ","3a10171a":"# now we focus on which group from each Passenger class survived more and less \n","b5484892":"## KNN With neigbhour = 5 and metric = euclidean  and accuracy score is 85.074 and it is cross validated ","efaeed56":"# Now go for analysis for features and there data types ","c5df89e9":"# as we can family with greater has less chance of survival ","cf15176a":"# 25% QUARTILE IS 7 THEN 50 IS 14 , 75 IS 31 MAX 512 ","d043673b":"# as here we have stop because each cabin has unique but very since , each cabin were alloted in different way , unique things , and to make bins we need titanic full map , ship architecture","c87aa73d":"# logistic Regressor increased by 1%","c03e638a":"# hence it was said that it will never sink but after more than 100 years data sciencetist from around the gathering to predict this passenger will survive or not ? on basis various features that passenger posses ","51931472":"### most passengers were from 20 to 40 \n### most passengers paid nearly 40 rupees , one thing i can is that in case of fare we are facing left skewed graph need of coversion to log_scale or sqrt scale is necessary depending upon what box-cox transform value come up","dd822cc8":"# how story TIMELINE WILL WORK "}}