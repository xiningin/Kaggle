{"cell_type":{"66082275":"code","09d75655":"code","fe52553c":"code","4ded3684":"code","f0087094":"code","c5c7c8ff":"code","4663929c":"code","a344c300":"code","9b44dc13":"code","d72c4209":"code","97873769":"code","4a4fe1c9":"code","a6761f6e":"code","3fa560ee":"code","7e36eb91":"code","0ab6f75f":"code","3d24eda1":"code","c12484c5":"markdown","bde7158f":"markdown","3679be81":"markdown","b17bf40d":"markdown","d3e002c2":"markdown","e153c13b":"markdown","883db068":"markdown","e373ef61":"markdown","c622a86d":"markdown","ec6b0992":"markdown","b2dad5e6":"markdown","f5e314e9":"markdown","736b2737":"markdown","ab7702fd":"markdown"},"source":{"66082275":"import time\n\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import random\n\n%config IPCompleter.use_jedi = False","09d75655":"# We will create two arrays, one with numpy and other with jax\n# to check the common things and the differences\n\narray_numpy = np.arange(10, dtype=np.int32)\narray_jax = jnp.arange(10, dtype=jnp.int32)\n\nprint(\"Array created using numpy: \", array_numpy)\nprint(\"Array created using JAX: \", array_jax)","fe52553c":"# What types of array are these?\nprint(f\"array_numpy is of type : {type(array_numpy)}\")\nprint(f\"array_jax is of type : {type(array_jax)}\")","4ded3684":"# Find the max element. Similarly you can find `min` as well\nprint(f\"Maximum element in ndarray: {array_numpy.max()}\")\nprint(f\"Maximum element in DeviceArray: {array_jax.max()}\")","f0087094":"# Reshaping\nprint(\"Original shape of ndarray: \", array_numpy.shape)\nprint(\"Original shape of DeviceArray: \", array_jax.shape)\n\narray_numpy = array_numpy.reshape(-1, 1)\narray_jax = array_jax.reshape(-1, 1)\n\nprint(\"\\nNew shape of ndarray: \", array_numpy.shape)\nprint(\"New shape of DeviceArray: \", array_jax.shape)","c5c7c8ff":"# Absoulte pairwise difference\nprint(\"Absoulte pairwise difference in ndarray\")\nprint(np.abs(array_numpy - array_numpy.T))\n\nprint(\"\\nAbsoulte pairwise difference in DeviceArray\")\nprint(jnp.abs(array_jax - array_jax.T))\n\n# Are they equal?\nprint(\"\\nAre all the values same?\", end=\" \")\nprint(jnp.alltrue(np.abs(array_numpy - array_numpy.T) == jnp.abs(array_jax - array_jax.T)))","4663929c":"# Matrix multiplication\nprint(\"Matrix multiplication of ndarray\")\nprint(np.dot(array_numpy, array_numpy.T))\n\nprint(\"\\nMatrix multiplication of DeviceArray\")\nprint(jnp.dot(array_jax, array_jax.T))","a344c300":"array1 = np.arange(5, dtype=np.int32)\narray2 = jnp.arange(5, dtype=jnp.int32)\n\nprint(\"Original ndarray: \", array1)\nprint(\"Original DeviceArray: \", array2)\n\n# Item assignment\narray1[4] = 10\nprint(\"\\nModified ndarray: \", array1)\nprint(\"\\nTrying to modify DeviceArray-> \", end=\" \")\n\ntry:\n    array2[4] = 10\n    print(\"Modified DeviceArray: \", array2)\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","9b44dc13":"# Modifying DeviceArray elements at specific index\/indices\narray2_modified = array2.at[4].set(10)\n\n# Equivalent => array2_modified = jax.ops.index_update(array2, 4, 10)\n\nprint(\"Original DeviceArray: \", array2)\nprint(\"Modified DeviceArray: \", array2_modified)","d72c4209":"# Of course, updates come in many forms!\n# Of course, updates come in many forms!\nprint(array2.at[4].add(6))\nprint(array2.at[4].max(20))\nprint(array2.at[4].min(-1))\n\n# Equivalent but depecated. Just to showcase the similarity to tf scatter_nd_update\nprint(\"\\nEquivalent but deprecatd\")\nprint(jax.ops.index_add(array2, 4, 6))\nprint(jax.ops.index_max(array2, 4, 20))\nprint(jax.ops.index_min(array2, 4, -1))","97873769":"# Create two random arrays sampled from a uniform distribution\narray1 = np.random.uniform(size=(8000, 8000)).astype(np.float32)\narray2 = jax.random.uniform(jax.random.PRNGKey(0), (8000, 8000), dtype=jnp.float32) # More on PRNGKey later!\nprint(\"Shape of ndarray: \", array1.shape)\nprint(\"Shape of DeviceArray: \", array2.shape)","4a4fe1c9":"# Dot product on ndarray\nstart_time = time.time()\nres = np.dot(array1, array1)\nprint(f\"Time taken by dot product op on ndarrays: {time.time()-start_time:.2f} seconds\")\n\n# Dot product on DeviceArray\nstart_time = time.time()\nres = jnp.dot(array2, array2)\nprint(f\"Time taken by dot product op on DeviceArrays: {time.time()-start_time:.2f} seconds\")","a6761f6e":"# First we will time it by converting the computation results to ndarray\n%time np.asarray(jnp.dot(array2, array2))","3fa560ee":"# Now let's time it using the blocking method\n%time jnp.dot(array2, array2).block_until_ready()","7e36eb91":"print(\"Types promotion in numpy =>\", end=\" \")\nprint((np.int8(32) + 4).dtype)\n\nprint(\"Types promtoion in JAX =>\", end=\" \")\nprint((jnp.int8(32) + 4).dtype)","0ab6f75f":"array1 = np.random.randint(5, size=(2), dtype=np.int32)\nprint(\"Implicit numpy casting gives: \", (array1 + 5.0).dtype)\n\n# Check the difference in semantics of the above function in JAX\narray2 = jax.random.randint(jax.random.PRNGKey(0),\n                            minval=0,\n                            maxval=5,\n                            shape=[2],\n                            dtype=jnp.int32\n                           )\nprint(\"Implicit JAX casting gives: \", (array2 + 5.0).dtype)","3d24eda1":"def squared(x):\n    return x**2\n\nx = 4.0\ny = squared(x)\n\ndydx = jax.grad(squared)\nprint(\"First order gradients of y wrt x: \", dydx(x))\nprint(\"Second order gradients of y wrt x: \", jax.grad(dydx)(x))","c12484c5":"So, `array_numpy` is an object of **`ndarray`** while `array_jax` is an object of **`DeviceArray`**. Before discussing anything else, let's dive into **`DeviceArray`** and see what makes **DeviceArray** so special.\n","bde7158f":"# Immutability\n\nJAX arrays are **immutable**, just like [**TensorFlow tensors**](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1). Meaning, JAX arrays don't support `item assignment` as you do in `ndarray`. Let's take an example!","3679be81":"# What is JAX anyway?\n\n**JAX** is a framework that is specifically suited for Machine Learning Research (more on this later). A few points about JAX:\n1. It's just like `numpy` but uses a compiler (XLA) to compile native Numpy code, and runs on acceleartors (GPU\/TPU)\n2. For automatic differentiation, JAX uses `Autograd`. It automatically differentiate native Python and Numpy code.\n3. JAX is used to express numerical programs as compositions but with certain constraints e.g. JAX transformation and compilation are designed to work only on Python functions that are functionally pure. A function is pure if it always returns the same value when invoked with same arguments, and the function has no-side affect e.g. chaning the state of a non-local variables\n4. In terms of syntax, JAX is very very similar to numpy but there are subtle differences that you should be aware of. We will look into it in a bit.\n\nLet's take a few examples to see JAX in-play!","b17bf40d":"# Numpy vs JAX-numpy\n\n`jax numpy` is very very similar to `numpy` in terms of API. *Most of the operations* that you do in numpy are also available in jax numpy with similar semantics. I am just listing down a few operations to showcase this but there are many more. Please check the [docs](https:\/\/jax.readthedocs.io\/en\/latest\/jax.numpy.html) to see the list of functions that are available.\n\n**Note:** Not all Numpy functions are implemented in JAX numpy (..yet)","d3e002c2":"# DeviceArray\n\nFollowing are the points that you should know about **`DeviceArray`**:\n1. It is the core underlying JAX array object, similar to `ndarray` but with subtle differences (more on this in the examples below)\n2. Unlike `ndarray`, `DeviceArray` is backed by a memory buffer on a single device (CPU\/GPU\/TPU)\n3. It is **device-agnostic** i.e. JAX doesn't need to track the device on which the array is present, and can avoid data transfers\n4. Because it is device agnostic, this makes it easy to run the same JAX code on CPU, GPU, or TPU with no code changes\n5. `DeviceArray` is **lazy** i.e. the value of a JAX `DeviceArray` isn't immediately available and is only pulled when requested.\n6. Even though `DeviceArray` is lazy, you can still do operations like inspecting the shape or type of a DeviceArray without waiting for the computation that produced it to complete. We can even pass it to another JAX computation. (The examples will make it more clear)\n\nThe two properties **lazy evaluation**, and being **device-agnostic** give **`DeviceArray`** a huge advantage. You will see this in the future tutorials as we dive deeper and deeper into complex things like model building, optimization, etc.","e153c13b":"This situation is exactly the same as we have with TensorFlow Tensors. Similar to `tf.tensor_scatter_nd_update` in TensorFlow, we have [Indexed update operators](https:\/\/jax.readthedocs.io\/en\/latest\/jax.ops.html#indexed-update-operators)( earlier there used to be [**jax.ops.index_update(..)**](https:\/\/jax.readthedocs.io\/en\/latest\/_autosummary\/jax.ops.index_update.html#jax.ops.index_update) but it's deprecated now). The syntax is pretty simple e.g. `DeviceArray.at[idx].op(val)`. This doesn't modify the original array though, it returns a new array with the elements updated as specified\n\n\nA question that naturally comes to mind? **Why immutability?** The thing is that JAX relies on **pure functions**. Allowing item-assignment or in-place updates is the opposite of that philosophy.\n\nBut then why TF Tensors are immutable as it doesn't need pure functions? If you are doing any optimization on a DAG, it is highly advisable to avoid things that change the state of an op used in the computation to avoid any side effects.","883db068":"# Types promotion\n\nThis is another aspect to keep in mind. `dtype` promotion in JAX is less aggressve as compared to numpy. A few things:\n1. JAX always prefers the precision of the JAX value when promoting a Python scalar\n2. JAX always prefers the type of the floating-point or complex type when promoting an integer or boolean type against floating or complex type\n3. JAX uses floating point promotion rules that are more suited to modern accelerator devices like GPUs\/TPUs\n\nLet's take an example to see these in action","e373ef61":"Now, let's do some computation on each array to see what happens and how much time does each computation take","c622a86d":"# Automatic Differentiation\n\nAutomatic Differentiation is one of my favorite topics to cover. It is beautiful and demands a full tutorial. Similar to how we covered AD in depth in [TensorFlow tutorial](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3), we will cover it in-depth here in future tutorials. Here we will take a look at a simple example to see how tightly it is integrated into JAX.","ec6b0992":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n---\n\n<img src=\"https:\/\/raw.githubusercontent.com\/google\/jax\/main\/images\/jax_logo_250px.png\" width=\"300\" height=\"300\" align=\"center\"\/>\n\nI hope you have been liking the tutorials so far. This is the fourth tutorial in this series and today, we will be starting with **JAX**. If you haven't looked at the previous tutorials, I highly suggest going through them once. Here are the links:\n\n1. [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n2. [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n3. [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)","b2dad5e6":"# Asynchronous dispatch\n\nOne of the biggest differences between `ndarrays` and `DeviceArrays` is in their execution and their availability. JAX uses asynchronous dispatch to hide Python overheads. Let's take an example to understand what it means.","f5e314e9":"Wow! Seems that the `DeviceArray` computation finished in no time. This is where you should remember this:\n1. Unlike the result of `ndarray`, the result of the computation done on DeviceArray isn't available yet. This is a **future** value that will be available on the accelerator \n2. You can retrieve the value of this computation by **printing** it or by converting it into a plain old numpy `ndarray`\n3. The above timing for DeviceArray is the time taken to **dispatch** the work, not the time taken for actual computation\n4. Asynchronous dispatch is useful since it allows Python code to \u201crun ahead\u201d of an accelerator device, keeping Python code out of the critical path. Provided the Python code enqueues work on the device faster than it can be executed, and that Python code does not actually need to inspect the output of a computation on the host, then a Python program can enqueue arbitrary amounts of work and avoid having the accelerator wait.\n5. To measure the true cost of any such operation:\n     - Either convert it to plain numpy ndarray (not preferred)\n     - Use `block_until_ready()` to wait for the computation that produced it to complete (preferred way for benchmarking)\n     \nLet's take a look at the above two methods again to measure the correct computation time","736b2737":"That's it for part 1! We will be looking at other things, especially the **`Automatic Differentiation`** in the next tutorial!<br>\n\n\n**References**:\n1. https:\/\/jax.readthedocs.io\/en\/latest\/\n2. https:\/\/colinraffel.com\/blog\/you-don-t-know-jax.html\n3. https:\/\/flax.readthedocs.io\/en\/latest\/notebooks\/jax_for_the_impatient.html\n\n\nLet me know in the comments if you have any suggestions\/queries","ab7702fd":"Now, let's take a look at some of the things that you can do in Numpy but not in Jax-numpy and vice-versa"}}