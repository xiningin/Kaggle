{"cell_type":{"a89fa8af":"code","41550c6c":"code","7be17ce4":"code","35647136":"code","eba1aa17":"code","23b19378":"code","415e35ee":"code","26e1c200":"code","576d8869":"code","9a698062":"code","5f7d2a33":"code","2dbd4919":"code","293892f0":"code","ba9437bc":"code","b0fc70ce":"code","9d10e7d4":"code","8408749a":"code","6f3957ed":"code","65420d1c":"code","8dd43d21":"code","4297d72f":"code","62f51354":"code","e7fc12cc":"code","1b713c3d":"code","fdd0ae9b":"code","b33ef80c":"code","560c2a30":"code","713f0d71":"code","451b8f92":"code","904419cc":"code","2c246b27":"code","752d33d8":"code","72b6c2d8":"code","6e53af71":"code","eb14d5b2":"code","8932fe03":"code","31755d75":"code","478a1e75":"code","266e4c21":"code","6be75424":"code","42847421":"code","9f5a4a5f":"code","af83b447":"code","660c4bac":"code","b257d25b":"code","caabd58a":"code","ff6b10f6":"code","a44c688b":"code","38f7d521":"code","b081015a":"code","e5b0f00d":"code","01409e57":"code","b0dad9da":"code","70b62505":"code","c549c66a":"code","f8ee2ecf":"code","149c86af":"code","b1c5bddb":"code","03f43491":"code","d046a34e":"code","1b0c9374":"code","5de5ae3b":"code","9f657779":"code","a6238352":"code","41889fea":"code","caae2f71":"code","1f0fd078":"code","59fe6e96":"code","f71648eb":"code","fe62dbb6":"code","b853568b":"code","e5cf4977":"code","90ed26f4":"code","b96e337c":"code","9ce9ed4d":"code","680fb3d4":"code","56e692f7":"code","1c602933":"code","f792d98f":"code","e9162fbe":"code","64045e34":"code","7d8d0d3e":"code","229b228b":"code","8acd5364":"code","076cf274":"code","00729fa4":"markdown","aa8c0f84":"markdown","c370e5eb":"markdown","38d088b5":"markdown","b673ee1d":"markdown","a15b6226":"markdown","12210bef":"markdown","2cb4dcde":"markdown","2e338296":"markdown","1b0a90fc":"markdown","29ee2eb7":"markdown","17d65864":"markdown","f5d118b7":"markdown","e2afd9dd":"markdown","8b825364":"markdown","64a2b78e":"markdown","fea950d3":"markdown","ce5775eb":"markdown","f5c3a828":"markdown","40b777ac":"markdown","d274a5c9":"markdown","b059b49a":"markdown","b28bd973":"markdown","20c3cb44":"markdown","0987851b":"markdown","48b8de29":"markdown","297f4134":"markdown","7a9d8920":"markdown","a7211ac3":"markdown","5853425d":"markdown","b9414182":"markdown","0f342f8f":"markdown","1cfdf224":"markdown","d227ab3e":"markdown","22610537":"markdown","13370105":"markdown","e832296b":"markdown","d9a1ab1d":"markdown","5956f374":"markdown","b70e958a":"markdown","aaedaf72":"markdown","ac301727":"markdown","6dc40402":"markdown","b68b4a63":"markdown","d02dad10":"markdown","c9941b12":"markdown","4ad69e0f":"markdown","2de19d1b":"markdown","2287c722":"markdown","40dadcb9":"markdown","8661a50c":"markdown"},"source":{"a89fa8af":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport xgboost as xgb\nimport random\nfrom scipy import stats\n\nwarnings.filterwarnings('ignore')","41550c6c":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.ensemble import GradientBoostingClassifier ,RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.decomposition import PCA","7be17ce4":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","35647136":"from imblearn.over_sampling import SMOTE","eba1aa17":"from keras import models\nfrom keras import layers","23b19378":"data_  = pd.read_csv('..\/input\/GiveMeSomeCredit\/cs-training.csv')","415e35ee":"data_.columns","26e1c200":"print(data_['SeriousDlqin2yrs'].value_counts()\/data_.shape[0] *100)\n\n## Pie Chart\nlabels = 'Default', 'Non-Defaults'\nsizes = [6.684, 93.316]\nexplode = (0.2, 0)\ncols    = ['#00FFFF', '#008080']\n\nfig = plt.figure(figsize = (4,4))\nplt.pie(sizes, explode=explode, colors = cols, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nplt.axis('equal')\nplt. title(\"Percentage of Defaults and Non-Defaults\")\nplt.show()","576d8869":"def missing_vals(data_):\n    miss_     = data_.isnull().sum()\n    miss_pct  = data_.isnull().sum()\/data_.shape[0]\n    \n    miss_pct  = pd.concat([miss_, miss_pct], axis =1)\n    miss_pct.reset_index(inplace=True)\n    miss_cols = miss_pct.rename(columns={'index':'Column Name', 0:'Missings', 1:'Missing_pct'})\n    \n    miss_cols = miss_cols[miss_cols.iloc[:,1]!=0].sort_values('Missing_pct', ascending=False).round(1)\n    miss_cols.reset_index(inplace=True, drop=True)\n    \n    return miss_cols ","9a698062":"miss = missing_vals(data_)\nmiss","5f7d2a33":"data_.describe()","2dbd4919":"cols = list(data_.columns)\ncols = cols[1:]\ncolors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'b', 'g', 'r', 'c']","293892f0":"fig = plt.figure(figsize=(15, 12))\nfor i in range(0, len(cols)):\n    plt.subplot(5, 4, i+1)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n\n    plt.hist(data_[cols[i]], bins=30, color=colors[i])\n    plt.title(cols[i])\n\nplt.tight_layout()","ba9437bc":"df = data_.copy()\ndf['MonthlyIncome'] =df['MonthlyIncome'].transform(lambda x: x.fillna(x.mean()))\ndf['NumberOfDependents'] =df['NumberOfDependents'].transform(lambda x: x.fillna(x.mean()))","b0fc70ce":"miss = missing_vals(df)\nmiss","9d10e7d4":"random.seed(32)\npca = PCA(n_components = 2)\npca.fit(df)\n\nscores = pca.transform(df)\n\nx,y = scores[:,0] , scores[:,1]\ndf_ = pd.DataFrame({'x': x, 'y':y, 'clusters':df['SeriousDlqin2yrs']})\ngrouping_ = df_.groupby('clusters')\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nnames = {0: 'Non-Defaults', \n         1: 'Defaults'}\n\nfor name, grp in grouping_:\n    ax.plot(grp.x, grp.y, marker='o', label = names[name], linestyle='')\n    ax.set_aspect('auto')\n    ax.set_ylim([0,200000])     ### I have just kept a upper cap on the axis to see the distribution of them\n    \nax.legend()\nplt.title('Plot showing Defaults and Non-Defaults')\nplt.show()","8408749a":"df = pd.get_dummies(df)\ndf = df[[c for c in df if c not in ['SeriousDlqin2yrs']]+['SeriousDlqin2yrs']]\ndf = df.drop(['Unnamed: 0'], axis=1)","6f3957ed":"df.head()","65420d1c":"correlations = pd.DataFrame(df.corr()['SeriousDlqin2yrs'].sort_values())\ncorrelations = correlations.rename(columns = {'SeriousDlqin2yrs':'Correlation value'})","8dd43d21":"correlations","4297d72f":"correlations.plot(kind=\"bar\", color=\"olive\")","62f51354":"corr_ = df.corr()\nfig= plt.figure(figsize=(15,7))\nsns.heatmap(corr_, cmap = plt.cm.RdYlBu_r, vmin = -0.9, annot = True, vmax = 0.9)","e7fc12cc":"#### Here we will drop variables to better our model\ndf           = df.drop(['NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate'], axis=1)","1b713c3d":"X_train, X_test, y_train, y_test = train_test_split(df.drop(['SeriousDlqin2yrs'], axis=1), df['SeriousDlqin2yrs'], test_size=0.2,random_state = 72)","fdd0ae9b":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n  \nsm = SMOTE(random_state = 2) \nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n  \nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))","b33ef80c":"def model_fit_reports(algo,X_,y_,performCV=True,printFeatureImportance=True, cv_folds=5):\n    \n    #Accuracy, Precision, Recall, F1 Score\n    pred = algo.predict(X_)\n    accu = accuracy_score(y_, pred)\n    f1_  = f1_score(y_, pred)\n    rec  = recall_score(y_, pred)\n    prec = precision_score(y_, pred)\n\n    \n    #GINI & AUC\n    fpr, tpr, thresholds = roc_curve(y_, pred)\n    roc_auc = auc(fpr, tpr)\n    Gini   = 2*roc_auc - 1   \n    labels  = ['Accuracy','F1 Score', 'Recall', 'Precision', 'Gini', 'AUC']\n    values  = [accu,f1_,rec,prec,Gini,roc_auc]\n    \n    all_    = pd.Series(values,labels)  \n    print(all_)\n    all_.plot(kind='bar', title='Model Fit Report')   \n\n\n    if performCV:\n        cv_score = cross_val_score(algo, X_, y_, cv=cv_folds, scoring='roc_auc')\n        GINI     = 2 * cv_score -1\n        print(\"AUC : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n        print(\"GINI : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(GINI),np.std(GINI),np.min(GINI),np.max(GINI)))\n\n    cols = list(X_.columns)\n    if printFeatureImportance:\n        feat_imp = pd.Series(algo.feature_importances_, cols).sort_values(ascending=False)\n        feat_imp.plot(kind='bar', title='Feature Importances')\n        plt.ylabel('Feature Importance Score')    \n    \n    return all_","560c2a30":"regressor = LogisticRegression(random_state =2, solver='sag', max_iter = 10**2)\nregressor.fit(X_train_res, y_train_res)","713f0d71":"train = model_fit_reports(algo =regressor ,X_ = X_train,y_ = y_train, performCV=True, printFeatureImportance=False, cv_folds=5)","451b8f92":"test  = model_fit_reports(algo =regressor ,X_ = X_test,y_ = y_test, performCV=True, printFeatureImportance=False, cv_folds=5)","904419cc":"rfc = RandomForestClassifier(random_state=8, n_estimators=500)\nrfc.fit(X_train_res, y_train_res)","2c246b27":"train = model_fit_reports(algo =rfc ,X_ = X_train,y_ = y_train, performCV=True, printFeatureImportance=True, cv_folds=5)","752d33d8":"test = model_fit_reports(algo =rfc ,X_ = X_test,y_ = y_test, performCV=True, printFeatureImportance=True, cv_folds=5)","72b6c2d8":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train_res, y_train_res)","6e53af71":"train = model_fit_reports(algo =gbc ,X_ = X_train,y_ = y_train, performCV=True, printFeatureImportance=False, cv_folds=5)","eb14d5b2":"test = model_fit_reports(algo =gbc ,X_ = X_test,y_ = y_test, performCV=True, printFeatureImportance=False, cv_folds=5)","8932fe03":"param_test1 = {'n_estimators':range(20,81,10)}\ngsearch1    = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, \n                                                               min_samples_split=500,\n                                                               min_samples_leaf=50,\n                                                               max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n                                                               param_grid = param_test1, scoring='roc_auc',\n                                                               n_jobs=4,iid=False, cv=5)\n\ngsearch1.fit(X_train_res,y_train_res)","31755d75":"gsearch1.best_params_, gsearch1.best_score_","478a1e75":"param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,400,600)}\ngsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80, max_features='sqrt', subsample=0.8, random_state=10), \nparam_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n\ngsearch2.fit(X_train_res,y_train_res)","266e4c21":"gsearch2.best_params_, gsearch2.best_score_","6be75424":"param_test3 = {'min_samples_split':range(1000,2100,200), 'min_samples_leaf':range(30,71,10)}\n\ngsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80,max_depth=15,max_features='sqrt', subsample=0.8, random_state=10), \nparam_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train_res,y_train_res)","42847421":"gsearch3.best_params_, gsearch3.best_score_","9f5a4a5f":"param_test4 = {'max_features':range(7,20,2)}\ngsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80,max_depth=15, min_samples_split=1000, min_samples_leaf=30, subsample=0.8, random_state=10),\nparam_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(X_train_res,y_train_res)","af83b447":"gsearch4.best_params_, gsearch4.best_score_","660c4bac":"param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\ngsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80, max_depth= 15, min_samples_split= 1000, min_samples_leaf=30,max_features=7, random_state = 10),\nparam_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(X_train_res,y_train_res)","b257d25b":"gsearch5.best_params_, gsearch5.best_score_","caabd58a":"gbc = GradientBoostingClassifier(n_estimators=80, max_depth= 15, min_samples_split= 1000, min_samples_leaf=30,subsample=0.9,max_features=7)\ngbc.fit(X_train_res, y_train_res)","ff6b10f6":"train = model_fit_reports(gbc,X_train,y_train,performCV=True,printFeatureImportance=False, cv_folds=5)","a44c688b":"test = model_fit_reports(gbc,X_test,y_test,performCV=True,printFeatureImportance=False, cv_folds=5)","38f7d521":"xgb1 = XGBClassifier(\n learning_rate =0.001,\n n_estimators=1000,\n max_depth=9,\n min_child_weight=1,\n gamma=0.2,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n reg_alpha = 0.1,\n scale_pos_weight=1,\n seed=27)","b081015a":"xgb1.fit(X_train_res, y_train_res)","e5b0f00d":"train = model_fit_reports(xgb1,X_train,y_train,performCV=True,printFeatureImportance=False, cv_folds=5)","01409e57":"test = model_fit_reports(xgb1,X_test,y_test,performCV=True,printFeatureImportance=False, cv_folds=5)","b0dad9da":"param_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(X_train_res, y_train_res)","70b62505":"gsearch1.best_params_, gsearch1.best_score_","c549c66a":"param_test2 = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2.fit(X_train_res, y_train_res)","f8ee2ecf":"gsearch2.best_params_, gsearch2.best_score_","149c86af":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=9,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train_res, y_train_res)","b1c5bddb":"gsearch3.best_params_, gsearch3.best_score_","03f43491":"param_test6 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n min_child_weight=1, gamma=0.2, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n\ngsearch6.fit(X_train_res, y_train_res)","d046a34e":"gsearch6.best_params_, gsearch6.best_score_","1b0c9374":"n_inputs = X_train_res.shape[1]\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation ='relu', input_shape =(n_inputs, )))\nmodel.add(layers.Dense(32,activation = 'relu'))\nmodel.add(layers.Dense(1,activation ='sigmoid'))","5de5ae3b":"model.compile(optimizer = 'rmsprop',\n             loss= 'binary_crossentropy',\n             metrics = ['accuracy'])","9f657779":"history = model.fit(X_train_res,\n                   y_train_res,\n                   epochs=150,\n                   batch_size=512,\n                   validation_data=(X_test,y_test))","a6238352":"score = model.evaluate(X_test, y_test)","41889fea":"#GINI & AUC \npred  = model.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, pred)\nroc_auc = auc(fpr, tpr)\nprint(\"The AUC of the Test model is \", roc_auc)\nGini   = 2*roc_auc - 1\nprint(\"The Gini of the Test model is \", Gini)","caae2f71":"log_pred                   = regressor.predict(X_test)\nrfc_pred                       = rfc.predict(X_test)\ngbm_pred                       = gbc.predict(X_test)\nxgb_pred                       = xgb1.predict(X_test)\ndeepl_pred                     = model.predict(X_test)","1f0fd078":"log_fpr, log_tpr, log_threshold   = roc_curve(y_test, log_pred)\nrfc_fpr, rfc_tpr, rfc_threshold   = roc_curve(y_test, rfc_pred)\ngbm_fpr, gbm_tpr, gbm_threshold   = roc_curve(y_test, gbm_pred)\nxgb_fpr, xgb_tpr, xgb_threshold   = roc_curve(y_test, xgb_pred)\ndeepl_fpr, deepl_tpr, deepl_threshold   = roc_curve(y_test, deepl_pred)","59fe6e96":"# Plot ROC curves\nfig  = plt.figure(figsize=(10,6))\nplt.title('ROC Curve \\n Comparison of Classifiers')\nplt.plot(log_fpr, log_tpr, label ='Logistic Regression AUC: {:.2f}'.format(roc_auc_score(y_test, log_pred)))\nplt.plot(rfc_fpr, rfc_tpr, label ='Random Forest AUC: {:.2f}'.format(roc_auc_score(y_test, rfc_pred)))\nplt.plot(gbm_fpr, gbm_tpr, label ='GBM AUC: {:.2f}'.format(roc_auc_score(y_test, gbm_pred)))\nplt.plot(xgb_fpr, xgb_tpr, label ='XgBoost AUC: {:.2f}'.format(roc_auc_score(y_test, xgb_pred)))\nplt.plot(deepl_fpr, deepl_tpr, label ='Deep Learning AUC: {:.2f}'.format(roc_auc_score(y_test, deepl_pred)))\n\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","f71648eb":"## Predict for the whole of the dataset\nactual_   = df['SeriousDlqin2yrs']\ndf_new    = df.copy()\ndf_new    = df.drop(['SeriousDlqin2yrs'], axis=1)","fe62dbb6":"pred_all  = model.predict(df_new)","b853568b":"fpr, tpr, thresholds = roc_curve(actual_, pred_all)\nroc_auc = auc(fpr, tpr)\nprint(\"The AUC of the overall model is: {:.2f}\".format(roc_auc))\nGini   = 2*roc_auc - 1\nprint(\"The Gini of the overall model is: {:.2f}\".format(Gini))","e5cf4977":"## Convert the probability into a score\nBase_Score = 600\npdo        = 120\nGood_Bads  = 10\n\n## Creating a function to calculate a Score\ndef score_(x, Offset, Factor):\n    score_ = Offset - Factor * np.log(x)\n    return score_","90ed26f4":"Factor          = pdo\/np.log(2)\nOffset          = Base_Score - Factor * np.log(Good_Bads)\nScore_          = score_(pred_all, Offset=Offset, Factor=Factor)","b96e337c":"Actual_vals      = pd.DataFrame(actual_).reset_index(drop=True)\nScore            = pd.DataFrame(Score_)\npred_all         = pd.DataFrame(pred_all)\ncombine          = [Actual_vals,pred_all, Score]\ncombine_         = pd.concat(combine, axis=1)","9ce9ed4d":"combine_.columns  = ['Default', 'Predicted_prob', 'Risk_Score']","680fb3d4":"combine_              = combine_.replace([np.inf, -np.inf], np.nan)\ncombine_              = combine_[combine_.isna()==False]","56e692f7":"combine_.loc[(combine_.Risk_Score>900), ['Risk_Score']] = 899","1c602933":"fig = plt.figure(figsize = (5,5))\nax = sns.distplot(combine_['Risk_Score'], hist=True, kde=True,\n                        bins=100, color = 'blue',hist_kws={'edgecolor':'black'},\n                         kde_kws={'linewidth': 4})\n\nax.set_xlabel(\"Application Score\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Score Distribution\")","f792d98f":"max_                         = max(combine_['Risk_Score'])\nmin_                         = min(combine_['Risk_Score'])\ncombine_['Score_decile']     = pd.cut(combine_['Risk_Score'], bins=[min_,300,415,490,555,600,690,730,max_],labels = [min_,300,415,490,555,600,690,730], include_lowest= True)","e9162fbe":"no_of_defaults                = combine_.groupby('Score_decile', as_index=False).agg({'Default':'sum', 'Predicted_prob':'count'})","64045e34":"no_of_defaults['default_rate'] = (no_of_defaults['Default']\/no_of_defaults['Predicted_prob'] * 100)\nno_of_defaults['Score_decile'] = round(no_of_defaults['Score_decile'],0)","7d8d0d3e":"no_of_defaults","229b228b":"fig = plt.figure(figsize = (8,6))\nplt.plot(no_of_defaults['Score_decile'], no_of_defaults['default_rate'], color = 'c')\nplt.title(\"Default Rates across Score Range\", fontsize = 15)\nplt.xlabel(\"Score\", fontsize = 12)\nplt.ylabel(\"Default Rate\", fontsize=12)\nplt.annotate(\"A rank order is seen \\n for all the Score Bands\",xy= (350,7), xytext =(380,8), arrowprops = dict(facecolor = \"black\", shrink = 0.05))\nplt.legend()","8acd5364":"Final_data     = pd.concat([df,combine_], axis=1)","076cf274":"Final_data.head()","00729fa4":"We see the Logistic Regression performs very poorly in comparison to Random Forest. Random Forest is performing decent when we see the Train Data Resuls but if you compare it with Test Data, it is a clear case of overfitting. We also know that Random Forest is prone to high variance or overfitting.\n\nNow, we can't use Random Forest as our go to metric because of this reason. We have to go for boosting algorithms which are not prone to overfitting and uses a different technique","aa8c0f84":"### Tuning Max_depth & min_samples_split","c370e5eb":"**Result**: As can be seen above, there is no rank ordering break; which is basically the Default rate for low score buckets is not lower than the Default rate for high score bucket","38d088b5":"#### Combining all the tuned parameter values and runnning them as a whole","b673ee1d":"### Checking for correlation \nWe will be checking for two type of correlations:\n* Correlation with the dependent variable to check which variable correlates better with the dependent variable\n* Correlations among different variables which is basically checking for mutli-collinearity\n\nThis is the step where we reduce the variables and as a part of the feature engineering section. In our case, we are having a smaller number of variables but it is just a use case. Feature Engineering can really break or make the model.","a15b6226":"### Reading the Data","12210bef":"#### Missing value function","2cb4dcde":"### Default and Non-Default Visualization\nWe will try to visualize the Default and Non-Defaults on a 2axis framework and see how much of the overlap they have. For this we will use PCA to get two principal components which are a combination of all the variables. This will help us understand the distribution of Defaults and Non-Defaults","2e338296":"#### Check for missing values again","1b0a90fc":"#### Result on Train data","29ee2eb7":"We see that that the correlation values are a little less across all the variables. None of the variables have a correlation of more than 0.5. The variable _NumberOfTime30-59DaysPastDueNotWorse_ has the highest correlation with the dependent variable","17d65864":"**Conclusion**: We see that there are very less missing values in two of the columns. Then also, we will be replacing the missing values with mean. Missing value treatment is another area which has so many options but the quickest is replacing it by mean or median (when the missing percentage is really less)","f5d118b7":"#### Result on the Test Data","e2afd9dd":"### Create Deciles and check default rate by score bands\n\nThis is an idea exercise which every Credit Risk Manager does to understand if the scores created by him is rank ordering as per the actual defaults. We would create 10 groups in the Score we have created and then check the overall defauls and default rates in that particular Score Band","8b825364":"### Function to measure the Model fit\nBelow is the function which we will use to measure the fitness of the model. The function contains metrics like Accuracy, Precision, Recall and F1 Score. It also calcualtes the GINI and AUC of the model and throws us the Feature Importance as per the respective model (only if the particular Machine Learning algorithm allows to do so)","64a2b78e":"### Give me Some Credit: Predicting Default\n\n**Note**: There are certain improvisations which needs to be done and is in my mind but I have tried this as my 1st version of notebook. Also, please upvote this kernel since it will also help my enthusiasm to provide more learning to the larger group and deeply understand the nuances of predicting a default","fea950d3":"### Check the percentage of Defaults and Non-Defaults\nIt is important to check the Default Rates in case you are creating a scorecard. What is does help in understanding about the data that whether we have a imbalanced data. An imbalanced data is basically where the event rate is really low; in the range of 0-5%. Usually, the defaults are a rare scenarios in case of Home Loans & Auto Loans. When you have a default rate in this range, we should be able to balance the data first and then run any model on the same. To balance a data set, there are a different appraoches but the most common and the one which we will use are:\n\n* SMOTE (Synthetic Minority Over-sampling Technique)\n* Near Miss (Under sampling Technique)\n\nBelow image clearly explains How Undersampling and Oversampling works:\n![Undersampling and Oversampling](https:\/\/oralytics.files.wordpress.com\/2019\/05\/screenshot-2019-05-20-15.34.14.png?w=705)\nImage Source: https:\/\/oralytics.com\/2019\/07\/01\/managing-imbalanced-data-sets-with-smote-in-python\/\n\n**SMOTE** works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line. Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n\n**NEAR MISS** is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process. To prevent problem of information loss in most under-sampling techniques, near-neighbor methods are widely used.","ce5775eb":"### Gradient Boosting","f5c3a828":"#### Result on Test data","40b777ac":"As we see that there are no null values left. Many of the Machine Learning algorithms do take missing values in their analysis but it is always a better strategy to treat them before reaching the modelling stage.\n\nThere are certain instances when you can't treat them:\n1. Missing values take away some 30-50% of observations where you can't delete them\n1. In the above case, you can't even treat them with mean, median as a variables with so many similar values doesn't bring a lot of variance in the variable","d274a5c9":"### Tuning n_estimators","b059b49a":"### Initializing the Score variable to predict the score","b28bd973":"### Tuning subsample","20c3cb44":"#### Result on the Train Data","0987851b":"### Modeling","48b8de29":"### Logistic Regression","297f4134":"### Developing our model with various Algorithms\nWe will be developing our model with various ML algorithms and compare it with Traditional Alogithms. Also, while it seems easy to tun these ML algos but it is quite tough optimize and tune such alogithms. In a classifier, we had to use some metrics on which basis we can tune and improve our classification. These metrics are:\n\n* Precision\n* Recall\n* F1 Score\n* GINI\n* AUC\n\nWe will also run a CV to check if our results are consistent when measured on metrics like GINI or AUC","7a9d8920":"#### Descriptive Statistics\nWe start with the Descriptive statistics by checking the basic statistics of the variables. Then we start developing histograms for all the variables. These will help us understand the distributions of each of the variable.","a7211ac3":"#### Tuning XGboost\n\nWe will be tuning some of the hyperparameters of XgBoost:\n\n* **max_depth**: The maximum depth of a tree\n* **min_child_weight**: It is the minimum sum of weights of all observations required in a child\n* **gamma**: A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split\n* **subsample**: Denotes the fraction of observations to be randomly samples for each tree","5853425d":"### Now doing the one-hot encoding\nOne hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction\n\nSome algorithms can work with categorical data directly. For example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation). Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric. In general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves. This means that categorical data must be converted to a numerical form.\n\nBelow image clearly explains How one-hot encoding works:\n![Undersampling and Oversampling](https:\/\/naadispeaks.files.wordpress.com\/2018\/04\/mtimfxh.png?w=371&h=146)","b9414182":"### Oversampling and Undersampling\nSince, we have already figured out that there is an unbalanced data, we will have to do a Oversampling and Undersampling. We will follow both the procedures and see which one we should follow to create a model","0f342f8f":"### Creating a Score from Probability\n\nNext we have is how do we consume these probabilities. Since, predicting a default is under a Credit Risk team of a Bank, we need to provide these probabilities into a more consumable format which will be like a Score; something similar to a FICO Score or a CIBIL Score or a Experian Score. These are all the Buereau Score on which basis one can predict the likeliness of a person not paying back the loan.\n\nWe will also convert this probability into our internal Bank Score which will be used to give away loan once a customer applies for the same. This is called as an Application Score basis the information provided by the customer.\n\nSome of the terms which will be used are:\n* **Base Score**: This is the score on which we will start for each customer and then add\/subtract points (absolute value) basis his probability to default (predicted). \n\n* **pdo**(Points to Double the odds): These are the points by which odds will be doubled. Odds here are (Good\/Bads) ratio. For example, suppose at a score of 620, the odds ratio is 300\/100 (3) and now if pdo is 100 then at a score of 720, the odds ratio will be 6.\n\n* **Goods\/Bads**: This is the initial Goods\/Bads ratio to be taken\n","1cfdf224":"### ROC curve comparison on the Test Data","d227ab3e":"**As we can see the GINI of our test data is even less than 50%, we will have to tune it so that we attain a far better GINI on our Test data**","22610537":"### Tuning max_features","13370105":"### Please upvote my kernel and share your feedback. I will be improving the kernel overtime","e832296b":"#### Tuning the Gradient Boosting parameters\n\nWe will be tuning the various hyperparameters of Gradient Boosting to improve our first cut results. The below image shows you what are the different pararmeters:\n\n* **n_estimators** : The number of sequential trees to be modeled. Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n\n* **max_depth** : This is the number of maximum depth of the tree. Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n\n* **min_samples_split** : This is the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n\n* **min_samples_leaf** : This is the minimum samples (rows or observations) required in a terminal node or leaf\n\n* **max_features** : The number of features to be considered while searching for the best split. A random selection is done when we fix the number of features to avoid any bias. Higher values can lead to over-fitting\n\n* **subsample** : The fraction of observations to be selected for each tree. Selection is done by random sampling.","d9a1ab1d":"### Checking for the Default rate\nAs we see that it is definitely a case of imbalanced data. So, we had to follow one of the approach discussed above; so either we will be oversampling or undersampling. There is a trade-off between having to chooses undersampling or oversampling:\n\n1. **With undersampling**, there is a big loss of data as the total number of rows will be reduced to the number of rows for event rate (Defaults). So, there is a high possibility of High Bias,\n1. **With oversampling**, there will be synthetic data added to make the Defaults observations equal to the Non-Defaults. The synthetic data is not the exact replica of the observations under Default but these are very much the same as chosen from their nearest neighbours. This can really increase the variance in our model which leads to overfitting. The other disadvantage of oversmapling is that it can increasre the model run time if we are dealing with large number of Non-Defaults","5956f374":"### Deep Learning\n\nKERAS is an Open Source Neural Network library written in Python that runs on top of Theano or Tensorflow. Keras doesn't handle low-level computation. Instead, it uses another library to do it, called the \"Backend. So Keras is high-level API wrapper for the low-level API, capable of running on top of TensorFlow, CNTK, or Theano.\n\nKeras High-Level API handles the way we make models, defining layers, or set up multiple input-output models. In this level, Keras also compiles our model with loss and optimizer functions, training process with fit function. Keras doesn't handle Low-Level API such as making the computational graph, making tensors or other variables because it has been handled by the \"backend\" engine.\n\nWe will be using deep learning to predict the default. Deep Learning an Advanced ML algorithm that uses the power of Neural Netowrks.","b70e958a":"### Predict the probability for the full dataset with the Deep Learning Model\n\nWe will use the Deep Learning model to predict the probability for all the customer. We have also tested our model on the KPIs like GINI and AUC.","aaedaf72":"### Import Libraries","ac301727":"**Result** : As we clearly see that the Deep Learning model works best on the Test data. It has a AUC score of around 81% which is a good classifer score for a first-second cut version. We can definitely improve a lot on this by tuning our keras model ","6dc40402":"### Correlation as heat map\nWe will check here the multi-collinearity between all the variable and understand which variable can be removed to avoid any multi-collinearity in the model","b68b4a63":"#### Replace the missings with the mean","d02dad10":"### Tuning min_samples_split & min_samples_leaf","c9941b12":"As you can see there is a lift in GINI by around 5% from tuning the hyperparameters. So, in this case we have been able to reduce the bias from the model by increasing the lift in GINI","4ad69e0f":"**Results** : We see that variable; **NumberOfTime60-89DaysPastDueNotWorse**, **NumberOfTimes90DaysLate** & **NumberOfTime30-59DaysPastDueNotWorse** have a high correlation among themselves. To overcome the issue of multi-collinearity we can remove the two variables and keep only one of them. This is again a part of feature engineering. \n\nNow, why did I choose the variable **NumberOfTime60-89DaysPastDueNotWorse**, **NumberOfTimes90DaysLate** to be removed as firstly the variable _NumberofTime90DaysLate_ is a collection related variable which is like heavily used when a person is not able to pay back the loan and the collection (team which collects back money) reaches the customer to force him to pay or else a legal action is taken against him","2de19d1b":"### Random Forest","2287c722":"### XgBoost","40dadcb9":"#### We will cap the Score to 900 as some of the customers have a Probability of almost equal to zero which is there is almost no chance they will miss any loan payments","8661a50c":"### Exploratory Data Analysis (EDA)\nThese will be the steps we will follow before trying to develop any model out of the model:\n\n* Identification of Missing values\n* Treatment of Missing values\n* Univariate Analysis for all the variables\n* Bivariate Analysis\n* Correlations\n\nThese will help us to strengthen our understanding on the data. We will be knowing about which variables have missing values, outliers if any, the general distribution. Also, we would know about the correlations between the independent and dependent variables."}}