{"cell_type":{"940ec66e":"code","6590e81a":"code","58cf9182":"code","43bf520d":"code","b70834da":"code","f49836f0":"code","f34873c3":"code","51da25e4":"code","b1d9c065":"code","ef56a4c1":"code","7c0686fc":"code","fdea8243":"code","69fdf946":"code","b908cdbc":"code","becb2a7e":"code","10fcd42b":"code","210e058f":"code","1b31fb46":"code","ea68f1af":"code","52f4a70e":"code","2eadd1e6":"code","020e53ca":"code","4aef4502":"markdown","79b10f5f":"markdown","1aa45436":"markdown","24bd07e9":"markdown","8e01dd53":"markdown","29bf06f5":"markdown","10ff1ab6":"markdown","34cf7f15":"markdown","b7e940b0":"markdown","05540d3b":"markdown","04f03881":"markdown","ae9d9539":"markdown","6299b0c5":"markdown","4a507e4a":"markdown","45860877":"markdown","ccc47db2":"markdown","4b42bdca":"markdown","d10dd83e":"markdown","64d179d1":"markdown","f9d6a499":"markdown","3ebffc8a":"markdown","31c0b35d":"markdown","90ec88a7":"markdown","48a3d98c":"markdown","7c153c38":"markdown","ec2ed79b":"markdown","e4517ca6":"markdown","1a5ef9c2":"markdown","84ccb27c":"markdown"},"source":{"940ec66e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n#import warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6590e81a":"#read train \ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nprint(\"Train Shape --> \",train.shape)\n# 28*28 = 784","58cf9182":"train.head()","43bf520d":"#read test\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nprint(\"Test Shape --> \",test.shape)","b70834da":"y_train = train[\"label\"]\nX_train = train.drop(labels=[\"label\"],axis=1)","f49836f0":"# how many of which numbers are there?\nplt.figure(figsize=(15,7))\ng = sns.countplot(y_train, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\nprint(\" \\t # y_train value counts # \\n\",y_train.value_counts())","f34873c3":"# plot some samples\nplt.figure(figsize=(8,6))\nimg = X_train.iloc[8].to_numpy()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train.iloc[8,0])\nplt.grid()\nplt.show()","51da25e4":"# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\nprint(\"x_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)\n\n","b1d9c065":"# Reshape\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nprint(\"x_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","ef56a4c1":"# Label Encoding \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nY_train = to_categorical(y_train, num_classes = 10)\n","7c0686fc":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_val shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_val shape\",Y_val.shape)","fdea8243":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n# fully connected\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","69fdf946":"# Define the optimizer\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)","b908cdbc":"model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","becb2a7e":"epochs = 100  \nbatch_size = 250","10fcd42b":"# data augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False, \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False, \n        rotation_range=5,  \n        zoom_range = 0.1, \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip=False,  \n        vertical_flip=False)  \n\ndatagen.fit(X_train)","210e058f":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=6)","1b31fb46":"history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=[early_stopping])","ea68f1af":"print(\"Accuracy of the model is --> \" , model.evaluate(X_val, Y_val, batch_size=batch_size)[1]*100 , \"%\")\nprint(\"Loss of the model is --> \" , model.evaluate(X_val, Y_val, batch_size=batch_size)[0])","52f4a70e":"plt.figure()\nplt.plot(history.history[\"loss\"],label = \"Train Loss\", color = \"black\")\nplt.plot(history.history[\"val_loss\"],label = \"Validation Loss\", color = \"darkred\", marker = \"+\", linestyle=\"dashed\",markeredgecolor = \"purple\", markeredgewidth = 2)\nplt.title(\"Model Loss\", color = \"darkred\", size = 13)\nplt.legend()\nplt.show()","2eadd1e6":"plt.figure()\nplt.plot(history.history[\"accuracy\"],label = \"Train Accuracy\", color = \"black\")\nplt.plot(history.history[\"val_accuracy\"],label = \"Validation Accuracy\", color = \"darkred\", marker = \"+\", linestyle=\"dashed\",markeredgecolor = \"purple\", markeredgewidth = 2)\nplt.title(\"Model Accuracy\", color = \"darkred\", size = 13)\nplt.legend()\nplt.show()","020e53ca":"# We make predictions using the model we have created.\nY_pred = model.predict(X_val)\n# argmax = To briefly mention it, it will give the index of the value with the highest value.\nY_pred_classes = np.argmax(Y_pred,axis = 1) \n\n# We do the same for the y_val values. because we will compare these values. \nY_true = np.argmax(Y_val,axis = 1) \n\n\"\"\"\nLet's explain the process we have done above with an example so that it sits on our minds.\n\nY_pred = [0.12, 1.258, 3,4448], Let's assume we have such an array.\nargmax -- > it gives us the index of the largest value here.your biggest value here is our value \nstarting with 3. its index is 2. argmax gives us this.\n\"\"\"\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n\n# define the colors\ncmap = mpl.colors.ListedColormap(['#f8f8ff', 'darkred'])\n\n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=cmap,linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\", color = \"blue\")\nplt.ylabel(\"True Label\", color = \"green\")\nplt.title(\"Confusion Matrix\", color = \"darkred\", size = 15)\nplt.show()","4aef4502":"<a id ='11' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Non-linearity \u2753 <\/h3>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > The Non-Linearity layer usually develops after all the Convolutional layers. So why is linearity in the image a problem? The problem is that since all layers can be a linear function, the Neural Network behaves like a single perception, that is, the result can be calculated as a linear combination of outputs. This layer is called the activation layer (Activation Layer) because it uses one of the activation functions. <\/p>\n\n<a id ='12' ><\/a>\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Let's take a look at the ReLU function, which is the most used of these functions. <\/p>\n\n![relu.PNG](attachment:9d3b0e6f-628c-41a8-b869-4c3d807d277b.PNG)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > As seen in the picture, ReLU reflects positive inputs as they are, while negative inputs as 0.Let's finish this part by looking at how the ReLU function looks on the real picture. <\/p>\n\n![non_linear.PNG](attachment:23799359-ddb3-42ed-9e66-09421bef55ca.PNG)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > When the ReLu function is applied to the Feature Map, a result as above is produced.Black values \u200b\u200bin Feature Map are negative. After the Relu function is applied, the black values \u200b\u200bare removed and 0 is replaced. <\/p>","79b10f5f":"![cat-removebg-preview.png](attachment:e2c74a77-7137-4f9f-8f3e-bb58857cf237.png)\n\n<center><h1 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Introduction \ud83d\udcd6 <\/h1><\/center>\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\">This data set consists of hand drawn numbers from 0 to 9. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.<\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > What are we going to do in this notebook? <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\">First, we will examine our data very briefly before CNN passes. Later, we'll talk about what is CNN. We will learn CNN step by step and encode it using keras.We will talk about CNN concepts and support them with images so that it fits better.We will try to explain and embody all of the steps you see below. <\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > So, let's get started. <\/p>\n    \n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Content :<\/h2>\n\n<ul>\n    <li style = \"color:gray;font-size:16px\"> <a href = \"#1\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Load and Check Data \ud83d\uddf8 <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#2\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Normalization \u2049\ufe0f <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#3\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Reshape \u2049\ufe0f <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#4\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Label Encoding \u2049\ufe0f <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#5\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Train -Test Split <\/a> <\/li> \n        <li style = \"color:gray;font-size:16px\"> <a href = \"#6\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Convolution Neural Network (CNN) \ud83d\udcda <\/a>\n            <ul>\n                <li style = \"color:black;font-size:16px\" ><a href = \"#7\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> What does CNN do? <\/a><\/li>\n                <li style = \"color:black;font-size:16px\" ><a href = \"#8\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Convolutional Layer \u2754 <\/a><\/li>\n                <li style = \"color:black;font-size:16px\" ><a href = \"#9\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Filter <\/a><\/li>\n                <li style = \"color:black;font-size:16px\" ><a href = \"#10\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Padding \u2753 <\/a><\/li>\n                <li style = \"color:black;font-size:16px\" ><a href = \"#11\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Non-linearity \u2753 <\/a>\n                    <ul>\n                        <li style = \"color:grey;font-size:16px\" ><a href = \"#12\" style = \"color:#321414;font-family:Segoe Print;font-weight:bold\"> Let's take a look at the ReLU function, which is the most used of these functions. <\/a><\/li>\n                    <\/ul>\n                <\/li>\n                <li style = \"color:black;font-size:16px\" ><a href = \"#13\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Pooling Layer \u2754 <\/a><\/li>\n              <li style = \"color:black;font-size:16px\" ><a href = \"#14\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Flattening Layer \u2754 <\/a><\/li>\n               <li style = \"color:black;font-size:16px\" ><a href = \"#15\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Fully-Connected Layer \u2754 <\/a><\/li>\n                      <li style = \"color:black;font-size:16px\" ><a href = \"#16\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Dropout \u2754 <\/a><\/li>\n            <\/ul>\n    <\/li> \n         <li style = \"color:gray;font-size:16px\" ><a href = \"#17\" style = \"color:black;font-family:Segoe Print;font-weight:bold\"> Implementing with Keras \u2754 <\/a>\n       <ul>\n            <li style = \"color:black;font-size:16px\" ><a href = \"#18\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Create Model <\/a><\/li>\n           <li style = \"color:black;font-size:16px\" ><a href = \"#19\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\">Define Optimizer \u2753 <\/a><\/li>\n           <li style = \"color:black;font-size:16px\" ><a href = \"#20\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Compile Model \u2753  <\/a><\/li>\n           <li style = \"color:black;font-size:16px\" ><a href = \"#21\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Epochs and Batch Size \u2753 <\/a><\/li>\n           <li style = \"color:black;font-size:16px\" ><a href = \"#22\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Data Augmentation \u2753 <\/a><\/li>\n                      <li style = \"color:black;font-size:16px\" ><a href = \"#23\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Callbacks \u2753 <\/a><\/li>\n                      <li style = \"color:black;font-size:16px\" ><a href = \"#24\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Fit the Model \u2753 <\/a><\/li>\n                      <li style = \"color:black;font-size:16px\" ><a href = \"#25\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Evaluate the model \u2754  <\/a>\n           <ul>\n                        <li style = \"color:grey;font-size:16px\" ><a href = \"#26\" style = \"color:#321414;font-family:Segoe Print;font-weight:bold\"> Model Success <\/a><\/li>\n                                       <li style = \"color:grey;font-size:16px\" ><a href = \"#27\" style = \"color:#321414;font-family:Segoe Print;font-weight:bold\"> Loss Chart <\/a><\/li>\n                                       <li style = \"color:grey;font-size:16px\" ><a href = \"#28\" style = \"color:#321414;font-family:Segoe Print;font-weight:bold\"> Accuracy Chart <\/a><\/li>\n                                   <li style = \"color:grey;font-size:16px\" ><a href = \"#29\" style = \"color:#321414;font-family:Segoe Print;font-weight:bold\"> Model Confusion Matrix \u2753 <\/a><\/li>\n                    <\/ul>                      \n           <\/li>\n                                 <li style = \"color:black;font-size:16px\" ><a href = \"#30\" style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> CONCLUSION <\/a><\/li>\n       <\/ul>\n    <\/li>\n<\/ul>\n","1aa45436":"<a id ='17' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Implementing with Keras \u2754 <\/h2>\n\n<a id ='18' ><\/a>\n<h4 style = \"color:darkred;border:0;font-family:Segoe Print;font-weight:bold\"> Create Model  <\/h4>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > We are starting to build our model using the keras library. <\/p>","24bd07e9":"<a id ='27' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Loss Chart \u2753 <\/h4>","8e01dd53":"<a id ='24' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Fit the model \u2753 <\/h2>","29bf06f5":"<a id ='1' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Load and Check Data \ud83d\uddf8<\/h2>","10ff1ab6":"<a id ='25' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Evaluate the model \u2754 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > In this section, we will find the success of our model. We will examine this with the loss vs accuracy chart. <\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >Model Success <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Loss Chart <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >Accuracy Chart <\/li>    \n<\/ul>","34cf7f15":"<a id ='14' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Flattening Layer \u2754 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > The task of this layer is simply to prepare the data at the input of the last and most important layer, the Fully Connected Layer. Generally, neural networks receive input data from a one-dimensional array. The data in this neural network are the matrixes coming from the Convolutional and Pooling layers are converted into a one-dimensional array. <\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > As we always do, let's examine it by visualizing it in order to better visualize it and understand it better. <\/p>\n\n![flattening.PNG](attachment:2996e08f-4b72-4ae9-829c-1887f3702f04.PNG)","b7e940b0":"<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Import Libraries \ud83d\udd16<\/h2>","05540d3b":"<a id ='28' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Accuracy Chart \u2753 <\/h4>","04f03881":"![CNN.png](attachment:6f7bae0e-33b1-45ee-9e12-77cf1478e617.png)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > CNN is a very effective mechanism often used for image recognition. <\/p>\n\n<a id ='7' ><\/a>\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:700\" >What does CNN do? <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > CNN uses their unique properties to distinguish pictures or images. Isn't this actually the same in our brains ? <br> Example : <br> When we look at a cat, our brains use features like her ears tail and her cat\nWe can define that. CNN does just that. <\/p> \n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:700\" > First, let's look at its structure before we get started. <\/p>\n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" > <strong> Convolutional Layer --> <\/strong> Used to determine features  <\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" > <strong> Non-Linearity Layer --> <\/strong> Introduction of nonlinearity to the system  <\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" > <strong> Pooling (Downsampling) Layer --> <\/strong> Reduces the number of weights and checks fit  <\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" > <strong> Flattening Layer --> <\/strong> Prepares data for the Classical Neural Network  <\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" > <strong> Fully-Connected Layer --> <\/strong> Standard Neural Network used in classification.  <\/li>\n<\/ul>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >So we can say ! CNN classification uses the normal neural network to solve the problem.\nHowever, up to that part, other layers are used to determine the properties.<\/p>\n","ae9d9539":"<a id ='20' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">  Compile Model \u2753 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Yes, now we need to compile our model. Compiling the model requires 3 parameters. <\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >For our missing function we will use 'categorical_crossentropy'.<\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >We use the \"accuracy\" metric to see the success of our model.<\/li>\n<\/ul>","6299b0c5":"<a id ='21' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Epochs and Batch Size \u2753 <\/h2>\n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Epochs : It is the forward and reverse processing of the images in the data set one by one. In other words, we can simply call it a tour of the complete education.Training is determined by the number of epochs. <\/li>\n        <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Batch Size : Basically, it takes a long time if we train individual pictures during the epochs I mentioned above. He runs away during the training and says that the pictures will be educated. <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >To give an example of this, imagine we have 10 loaves of bread. If we select batch_size as 2, imagine that we eat these breads 2 times and 2 times.<\/li>  \n<\/ul>","4a507e4a":"<a id ='16' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Dropout \u2754 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Before coming to the keras-application part, I want to talk about the DropOut concept. Dropout is a technique where randomly selected neurons are ignored during training <\/p>\n\n![drop\u0131ut.png](attachment:d61031fe-0eff-4faf-bf90-b7fca3498c41.png)","45860877":"<a id ='15' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Fully-Connected Layer \u2754 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Fully Connected layers in a neural networks are those layers where all the inputs from one layer are connected to every activation unit of the next layer. In most popular machine learning models, the last few layers are full connected layers which compiles the data extracted by previous layers to form the final output. It is the second most time consuming layer second to Convolution Layer. <\/p>\n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >Neurons in a fully connected layer have connections to all activations in the previous layer<\/li>\n<\/ul>\n\n![fulll.png](attachment:ec9c4657-14d9-4e1d-92c5-974e38d7a346.png)","ccc47db2":"<a id ='3' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Reshape \u2049\ufe0f <\/h2>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Why do we need to reshape the pictures? <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Keras works this way. 2D does not want data. <\/p>\n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >We have (28,28) pictures. We need to make them (28,28,1) 3-dimensional.<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >where 1 = black and white, 3 = rgb means that it is colored.<\/li>\n<\/ul>","4b42bdca":"<a id ='6' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Convolution Neural Network (CNN) \ud83d\udcda <\/h2>","d10dd83e":"<a id ='10' ><\/a>\n<h3 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Padding \u2753 <\/h3>\n\n<p  style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Before we get to the concept of padding, let's take a look at what <strong> stride? <\/strong> is. This term is often used in conjunction with the term padding. Stride controls how the filter evolves around the input image. In the example above, Stride was 1 pixel, but it could be larger. This affects the size of the Feature Map's output.<\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > In the early stages of Cnn, while applying the initial filters, we need to preserve as much information as possible for the other Convolutional Layers. This is why padding is used. You may have noticed that the Feature Map is smaller than the original login image. So Padding will add zero values to this map to maintain the size of the image. <\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Let's examine this on the picture. <\/p>\n\n![same_padding.PNG](attachment:91e9b688-9723-426d-8099-f5c3e928f268.PNG)","64d179d1":"<a id ='23' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Callbacks \u2753 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >\nInstead of going all the way to the end of the training for better results during the training of our model, we can stop it when success decreases.There are many ways to do this, we will use EarlyStopping for this.\n<\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > monitor --> we tell you the amount to be monitored, that is, which option to choose to stop our model. <\/li>\n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >patience --> times when there is no improvement. how many in a row. <\/li>\n<\/ul>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > For more detailed information : \n    <a href = \"https:\/\/keras.io\/api\/callbacks\/early_stopping\/\" >https:\/\/keras.io\/api\/callbacks\/early_stopping\/<\/a>\n<\/p>","f9d6a499":"<a id ='22' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Data Augmentation \u2753 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >\nIn order for our model to learn well, we need to have enough data. If we have less data, we should not use data increase methods.\n<\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >Taking symmetries of the image with respect to various axes, <\/li>\n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >Cutting out a random sample piece, <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >turning it to the right or left,<\/li>  \n        <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >zoom in on the picture or just take a part of it,<\/li>  \n            <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >. . .<\/li> \n<\/ul>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > By creating new pieces of data obtained from the data in many different ways, it can be duplicated \/ increased. Thus, the model moves away from overfitting.We can understand better by examining the picture below. <\/p>\n\n![data_aug.png](attachment:e4caf969-0a26-48ae-bc60-ba7de4dff66b.png)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Let's write the code for this section we learned. <\/p>","3ebffc8a":"<a id ='4' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Label Encoding \u2049\ufe0f <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > we have numbers. We are doing this for Keras to better understand these numbers. <\/p>\n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" > <strong> Example : <\/strong> <br> 2 = > [0,0,1,0,0,0,0,0,0,0] <br> 5 = > [0,0,0,0,0,1,0,0,0,0] <\/li>\n<\/ul>","31c0b35d":"<a id ='8' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Convolutional Layer \u2754 <\/h2>\n\n\n<ul>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >This layer is the main building block of CNN. It is responsible for perceiving the features of the picture.<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >This layer applies some filters to the image to extract low and high level features in the image.\nFor example, this filter can be a filter that will detect edges.<\/li>\n<\/ul>\n\n<a id ='9' ><\/a>\n<h4 style = \"color:darkred;font-family:Segoe Print;font-weight:bold\"> Filter <\/h4>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > Now let's see how the filter is applied. <\/p>\n\n![filter.PNG](attachment:aef13d3d-3929-4915-a128-53e59a74362d.PNG)\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Let's take a look at how the filter is applied to our picture. <\/p>\n\n![Filter.gif](attachment:c33d4751-3e71-4c26-abd2-720ec98dde54.gif)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > First, the filter is positioned in the upper left corner of the image. Here, the indices between the two matrices (picture and filter) are multiplied by each other and all results are summed, then the result is stored in the output matrix. Then move this filter to the right by 1 pixel (also known as a \"step\") and repeat the process. After the end of the 1st line, 2 lines are passed and the operations are repeated. After all operations are completed, an output matrix is created. The reason why the output matrix is 3 \u00d7 3 here is because in the 5 \u00d7 5 matrix the 3 \u00d7 3 filter moves 3 times horizontally and vertically. <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > So what does the output matrix tell us? This matrix is often called Feature Map. It shows where the image is located in the property represented by the filter. <\/p>\n\n","90ec88a7":"<a id ='2' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Normalization \u2049\ufe0f <\/h2>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Why do we want to normalize the pictures? <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > If we do not normalize, there may be errors due to certain colors. So, we first need to normalize. <\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" >For example, if you think of this page as a picture, there are different colors such as blue, white, black, so we normalize these colors to bring them into the black and white range.\nIn short, we can say that we will make that picture in black and white. (Values between 0 and 1)<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >This increases the speed of CNN.<\/li>\n    <li style = \"color:black;font-family:Segoe Print;font-weight:bold\" >The maximum color a picture can take is 255, and we divide this floating by 255.<\/li>\n<\/ul>","48a3d98c":"<a id ='13' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Pooling Layer \u2754 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > This layer is a layer that is often added between successive convolutional layers in CovNet. The task of this layer is to reduce the shear size of the representation and the number of parameters and calculations within the network. In this way, incompatibility in the network is checked. There are many pooling operations, but the most popular is max pooling. There are also average pooling and L2-norm pooling algorithms that work on the same principle. <\/p>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > Let's examine the most preferred Maxpooling process. <\/p>\n\n![maxpo.PNG](attachment:bb6b0af1-a14e-4e8a-b232-297f266a11e3.PNG)\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" > We created a (2x2) size filter and applied it to our image. He writes what he is doing here, taking the greatest value there. <\/p>","7c153c38":"<a id ='19' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Define Optimizer \u2753 <\/h2>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. <\/p>\n\n<ul>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <strong> beta_1 : <\/strong> The exponential decay rate for the first moment estimates.\n <\/li>\n    <li style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > <strong> beta_2 : <\/strong> The exponential decay rate for the second-moment estimates. This value should be set close to 1.0 on problems with a sparse gradient.<\/li>\n<\/ul>\n\n<strong> referance : <\/strong> https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/ ","ec2ed79b":"<a id ='5' ><\/a>\n<h2 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\">Train -Test Split <\/h2>","e4517ca6":"<a id ='30' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> CONCLUSION<\/h4>\n\n<p style = \"color:darkred;font-family:Segoe Print;font-weight:bold\" > I am waiting for them if you have any questions or suggestions. <\/p>\n\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Seaborn Tutorial -->  \n    <a href = \"https:\/\/www.kaggle.com\/rafetcan\/visualization-tutorial-with-seaborn\" >https:\/\/www.kaggle.com\/rafetcan\/visualization-tutorial-with-seaborn<\/a>\n<\/p>\n<p style = \"color:black;font-family:Segoe Print;font-weight:bold\" >  Plotly Tutorial -->  \n    <a href = \"https:\/\/www.kaggle.com\/rafetcan\/plotly-tutorial-for-beginners\" >https:\/\/www.kaggle.com\/rafetcan\/plotly-tutorial-for-beginners<\/a>\n<\/p>","1a5ef9c2":"<a id ='29' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Model Confusion Matrix \u2753 <\/h4>","84ccb27c":"<a id ='26' ><\/a>\n<h4 style = \"background:#0C0C0C ;color:white;border:0;font-family:Segoe Print;font-weight:bold\"> Model Success \u2753 <\/h4>"}}