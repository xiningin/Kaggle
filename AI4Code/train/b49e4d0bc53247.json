{"cell_type":{"76d39db4":"code","6989747d":"code","bbc2afa8":"code","3064ae85":"code","969e3181":"code","ac6ba48d":"code","862dfac0":"code","a1885e48":"code","493fd1be":"code","b78e9d93":"code","18652d90":"code","48cdd19d":"code","1aa423c3":"code","87860317":"code","c84348d1":"code","ef77e795":"code","438fed34":"code","f1b78f09":"code","ce14e41c":"code","b2dcfd08":"code","51a9fabb":"code","8d7904ec":"code","ea23cfa2":"code","ee60eb68":"code","88ea7309":"code","e5bc85ab":"code","730c7aa2":"code","fee01b82":"markdown","785be6bf":"markdown","65da1078":"markdown","065d4a97":"markdown","ffdd2e08":"markdown","f27ed98a":"markdown","d9e787d2":"markdown","40fe994f":"markdown","d9c24cbc":"markdown","8411d40d":"markdown"},"source":{"76d39db4":"import pandas as pd \nfrom collections import defaultdict\nimport string\nfrom gensim.models import CoherenceModel\nimport gensim\nfrom pprint import pprint\nimport spacy,en_core_web_sm\nfrom nltk.stem import PorterStemmer\nimport os\nimport json\nfrom gensim.models import Word2Vec\nimport nltk\nimport re\nimport collections\nfrom sklearn.metrics import cohen_kappa_score\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom collections import defaultdict\nimport string","6989747d":"class MetaData:\n    def __init__(self):\n        \"\"\"Define varibles.\"\"\"\n        # path and data\n        self.path = '..\/input\/CORD-19-research-challenge\/'\n        self.meta_data = pd.read_csv(self.path + 'metadata.csv')\n\n    def data_dict(self):\n        \"\"\"Convert df to dictionary. \"\"\"\n        mydict = lambda: defaultdict(mydict)\n        meta_data_dict = mydict()\n\n        for cord_uid, abstract, title, sha in zip(self.meta_data['cord_uid'], self.meta_data['abstract'], self.meta_data['title'], self.meta_data['sha']):\n            meta_data_dict[cord_uid]['title'] = title\n            meta_data_dict[cord_uid]['abstract'] = abstract\n            meta_data_dict[cord_uid]['sha'] = sha\n\n        return meta_data_dict","bbc2afa8":"class ExtractText:\n    \"\"\"Extract text according to keywords or phrases\"\"\"\n\n    def __init__(self, metaDict, keyword, variable):\n        \"\"\"Define varibles.\"\"\"\n        self.path = '..\/input\/CORD-19-research-challenge\/'\n        self.metadata = metaDict\n        self.keyword = keyword\n        self.variable = variable\n\n\n    def simple_preprocess(self):\n        \"\"\"Simple text process: lower case, remove punc. \"\"\"\n        mydict = lambda: defaultdict(mydict)\n        cleaned = mydict()\n        for k, v in self.metadata.items():\n            sent = v[self.variable]\n            sent = str(sent).lower().translate(str.maketrans('', '', string.punctuation))\n            cleaned[k]['processed_text'] = sent\n            cleaned[k]['sha'] = v['sha']\n            cleaned[k]['title'] = v['title']\n\n        return cleaned\n\n    def very_simple_preprocess(self):\n        \"\"\"Simple text process: lower case only. \"\"\"\n        mydict = lambda: defaultdict(mydict)\n        cleaned = mydict()\n        for k, v in self.metadata.items():\n            sent = v[self.variable]\n            sent = str(sent)\n            #sent = str(sent).lower()\n            cleaned[k]['processed_text'] = sent\n            cleaned[k]['sha'] = v['sha']\n            cleaned[k]['title'] = v['title']\n\n        return cleaned\n     \n\n    def extract_w_keywords(self):\n        \"\"\"Select content with keywords.\"\"\"\n        ps = PorterStemmer()\n        mydict = lambda: defaultdict(mydict)\n        selected = mydict()\n        textdict = self.simple_preprocess()\n        \n        for k, v in textdict.items():\n            if self.keyword in v['processed_text'].split():\n                #print(ps.stem(str(self.keyword)))\n                selected[k]['processed_text'] = v['processed_text']\n                selected[k]['sha'] = v['sha']\n                selected[k]['title'] = v['title']\n        return selected\n\n    def extract_w_keywords_punc(self):\n        \"\"\"Select content with keywords, with punctuations in text\"\"\"\n        ps = PorterStemmer()\n        mydict = lambda: defaultdict(mydict)\n        selected = mydict()\n        textdict = self.very_simple_preprocess()\n        \n        for k, v in textdict.items():\n            #keywords are stemmed before matching\n            if ps.stem(str(self.keyword)) in ps.stem(str(v['processed_text'].split())):\n                selected[k]['processed_text'] = v['processed_text']\n                selected[k]['sha'] = v['sha']\n                selected[k]['title'] = v['title']\n        return selected\n\n    def get_noun_verb(self, text):\n        \"\"\"get noun trunks for the lda model,\n        change noun and verb part to decide what\n        you want to use as input for LDA\"\"\"\n        ps = PorterStemmer()\n      \n        #find nound trunks\n        nlp = en_core_web_sm.load()\n        all_extracted = {}\n        for k, v in text.items():\n            #v = v.replace('incubation period', 'incubation_period')\n            doc = nlp(v)\n            nouns = ' '.join(str(v) for v in doc if v.pos_ is 'NOUN').split()\n            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n            all_w = nouns + verbs + adj\n            all_extracted[k] = all_w\n      \n        return all_extracted\n\n    def get_noun_verb2(self, text):\n        \"\"\"get noun trunks for the lda model,\n        change noun and verb part to decide what\n        you want to use as input for LDA\"\"\"\n        ps = PorterStemmer()\n      \n        #find nound trunks\n        nlp = en_core_web_sm.load()\n        all_extracted = {}\n        for k, v in text.items():\n            #v = v.replace('incubation period', 'incubation_period')\n            doc = nlp(v['processed_text'])\n            nouns = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'NOUN').split()\n            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n            all_w = nouns + verbs + adj\n            all_extracted[k] = all_w\n      \n        return all_extracted\n\n    def tokenization(self, text):\n        \"\"\"get noun trunks for the lda model,\n        change noun and verb part to decide what\n        you want to use as input for the next step\"\"\"\n        nlp = spacy.load(\"en_core_web_sm\")\n\n        all_extracted = {}\n        for k, v in text.items():\n            doc = nlp(v)\n            all_extracted[k] = [w.text for w in doc]\n      \n        return all_extracted\n\n","3064ae85":"class LDATopic:\n    def __init__(self, processed_text, topic_num, alpha, eta):\n        \"\"\"Define varibles.\"\"\"\n        self.path = '..\/input\/CORD-19-research-challenge\/'\n        self.text = processed_text\n        self.topic_num = topic_num\n        self.alpha = alpha\n        self.eta = eta\n\n    def get_lda_score_eval(self, dictionary, bow_corpus):\n        \"\"\"LDA model and coherence score.\"\"\"\n\n        lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=self.topic_num, id2word=dictionary, passes=10,  update_every=1, random_state = 300, alpha=self.alpha, eta=self.eta)\n        #pprint(lda_model.print_topics())\n\n        # get coherence score\n        cm = CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n        coherence = cm.get_coherence()\n        print('coherence score is {}'.format(coherence))\n\n        return lda_model, coherence\n\n    def get_score_dict(self, bow_corpus, lda_model_object):\n        \"\"\"\n        get lda score for each document\n        \"\"\"\n        all_lda_score = {}\n        for i in range(len(bow_corpus)):\n            lda_score ={}\n            for index, score in sorted(lda_model_object[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n                lda_score[index] = score\n                od = collections.OrderedDict(sorted(lda_score.items()))\n            all_lda_score[i] = od\n        return all_lda_score\n\n\n    def topic_modeling(self):\n        \"\"\"Get LDA topic modeling.\"\"\"\n        # generate dictionary\n        dictionary = gensim.corpora.Dictionary(self.text.values())\n        bow_corpus = [dictionary.doc2bow(doc) for doc in self.text.values()]\n        # modeling\n        model, coherence = self.get_lda_score_eval(dictionary, bow_corpus)\n\n        lda_score_all = self.get_score_dict(bow_corpus, model)\n\n        all_lda_score_df = pd.DataFrame.from_dict(lda_score_all)\n        all_lda_score_dfT = all_lda_score_df.T\n        all_lda_score_dfT = all_lda_score_dfT.fillna(0)\n\n        return model, coherence, all_lda_score_dfT\n\n    def get_ids_from_selected(self, text):\n        \"\"\"Get unique id from text \"\"\"\n        id_l = []\n        for k, v in text.items():\n            id_l.append(k)\n            \n        return id_l","969e3181":"# Now we extract articles contain the most relevant topic\n\ndef selected_best_LDA(keyword, varname):\n        \"\"\"Select the best lda model with extracted text \"\"\"\n        # convert data to dictionary format\n        m = MetaData()\n        metaDict = m.data_dict()\n\n        #process text and extract text with keywords\n        et = ExtractText(metaDict, keyword, varname)\n        text1 = et.extract_w_keywords()\n\n\n        # extract nouns, verbs and adjetives\n        text = et.get_noun_verb2(text1)\n\n        # optimized alpha and beta\n        alpha = [0.1, 0.3, 0.5, 0.7, 0.9]\n        beta = [0.1, 0.3, 0.5, 0.7, 0.9]\n\n        mydict = lambda: defaultdict(mydict)\n        cohere_dict = mydict()\n        for a in alpha:\n            for b in beta:\n                lda = LDATopic(text, 20, a, b)\n                model, coherence, scores = lda.topic_modeling()\n                cohere_dict[coherence]['a'] = a\n                cohere_dict[coherence]['b'] = b\n    \n        # sort result dictionary to identify the best a, b\n        # select a,b with the largest coherence score \n        sort = sorted(cohere_dict.keys())[0] \n        a = cohere_dict[sort]['a']\n        b = cohere_dict[sort]['b']\n        \n        # run LDA with the optimized values\n        lda = LDATopic(text, 20, a, b)\n        model, coherence, scores_best = lda.topic_modeling()\n        pprint(model.print_topics())\n\n        # select merge ids with the LDA topic scores\n        id_l = lda.get_ids_from_selected(text)\n        scores_best['cord_uid'] = id_l\n\n        return scores_best\n\n\n\n\ndef select_text_from_LDA_results(keyword, varname, scores_best, topic_num):\n        # choose papers with the most relevant topic\n        # convert data to dictionary format\n        m = MetaData()\n        metaDict = m.data_dict()\n\n        # process text and extract text with keywords\n        et = ExtractText(metaDict, keyword, varname)\n        # extract text together with punctuation\n        text1 = et.extract_w_keywords_punc()\n        # need to decide which topic to choose after training\n        sel = scores_best[scores_best[topic_num] > 0] \n        \n        mydict = lambda: defaultdict(mydict)\n        selected = mydict()\n        for k, v in text1.items():\n            if k in sel.cord_uid.tolist():\n                selected[k]['title'] = v['title']\n                selected[k]['processed_text'] = v['processed_text']\n                selected[k]['sha'] = v['sha']\n    \n        return selected\n\ndef extract_relevant_sentences(cor_dict, search_keywords, filter_title=None):\n    \"\"\"Extract sentences contain keyword in relevant articles. \"\"\"\n    #here user can also choose whether they would like to only select title contain covid keywords\n\n    mydict = lambda: defaultdict(mydict)\n    sel_sentence = mydict()\n    filter_w = ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n    \n    for k, v in cor_dict.items():\n        keyword_sentence = []\n        sentences = v['processed_text'].split('.')\n        for sentence in sentences:\n            # for each sentence, check if keyword exist\n            # append sentences contain keyword to list\n            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n            if keyword_sum > 0:\n                keyword_sentence.append(sentence)         \n\n        # store results\n        if not keyword_sentence:\n            pass\n        elif filter_title is not None:\n            for f in filter_w:\n                title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n                abstract = v['processed_text'].lower().translate(str.maketrans('', '', string.punctuation))\n                if (f in title) or (f in abstract):\n                    sel_sentence[k]['sentences'] = keyword_sentence\n                    sel_sentence[k]['sha'] = v['sha']\n                    sel_sentence[k]['title'] = v['title'] \n        else:\n            sel_sentence[k]['sentences'] = keyword_sentence\n            sel_sentence[k]['sha'] = v['sha']\n            sel_sentence[k]['title'] = v['title'] \n            \n    print('{} articles are relevant to the topic you choose'.format(len(sel_sentence)))\n\n    path = '..\/input\/CORD-19-research-challenge\/'\n    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n    #df.to_csv(path + 'search_results_{}.csv'.format(search_keywords))\n    #sel_sentence_df = pd.read_csv(path + 'search_results_{}.csv'.format(search_keywords))\n    return sel_sentence, df\n\ndef extract_relevant_sentences2(cor_dict, search_keywords, filter_title=None):\n    \"\"\"Extract sentences contain keyword in relevant articles for system evaluation. \"\"\"\n    #here user can also choose whether they would like to only select title contain covid keywords\n    #difference from the previous one is where we store the result\n\n    mydict = lambda: defaultdict(mydict)\n    sel_sentence = mydict()\n    filter_w = ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n    \n    for k, v in cor_dict.items():\n        keyword_sentence = []\n        sentences = v['processed_text'].split('.')\n        for sentence in sentences:\n            # for each sentence, check if keyword exist\n            # append sentences contain keyword to list\n            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n            if keyword_sum > 0:\n                keyword_sentence.append(sentence)         \n\n        # store results\n        if not keyword_sentence:\n            pass\n        \n        elif filter_title is not None:\n            for f in filter_w:\n                title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n                abstract = v['processed_text'].lower().translate(str.maketrans('', '', string.punctuation))\n                if (f in title) or (f in abstract):\n                    sel_sentence[k]['sentences'] = keyword_sentence\n                    sel_sentence[k]['sha'] = v['sha']\n                    sel_sentence[k]['title'] = v['title'] \n        else:\n            sel_sentence[k]['sentences'] = keyword_sentence\n            sel_sentence[k]['sha'] = v['sha']\n            sel_sentence[k]['title'] = v['title'] \n    print('{} articles contain keyword {}'.format(len(sel_sentence),  search_keywords))\n\n    path = '..\/input\/CORD-19-research-challenge\/'\n    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n    df.to_csv(path + 'eval_results_{}.csv'.format(search_keywords))\n    sel_sentence_df = pd.read_csv(path + 'eval_results_{}.csv'.format(search_keywords))\n    return sel_sentence, sel_sentence_df\n","ac6ba48d":"#here we select the LDA model with the lowe\nscores_best_mask = selected_best_LDA('mask', 'abstract')","862dfac0":"scores_best_mask.shape","a1885e48":"# topic number 10 is most relevant to public wearing mask\n# which topic do you think is most relevant to your search\ncor_dict_mask = select_text_from_LDA_results('mask', 'abstract', scores_best_mask, 10)\nprint (\"There are {} abstracts selected\". format(len(cor_dict_mask)))","493fd1be":"# extract relevant sentences  #search keywords can be a list\nsel_sentence_mask, sel_sentence_df_mask = extract_relevant_sentences(cor_dict_mask, ['mask'])","b78e9d93":"#read extracted article\nsel_sentence_df_mask.head(20)","18652d90":"#here we annotated a sample of 40 abstracts\npath = '..\/input\/annotation\/'\nannotation_mask = pd.read_csv(path + 'wear_mask.csv')","48cdd19d":"# view file\nannotation_mask.head(5)\nprint('there are {} articles relevant to the topic'.format(annotation_mask.shape[0]))","1aa423c3":"annotation_mask['stance'].value_counts()","87860317":"print('there are {} papers support using a mask during a pandemic is useful, {} assume masks as useful and examine the public\u2019s willingness to comply the rules,  {} papers show no obvious evidence that shows using mask is protective or the protection is very little'. format(str(annotation_mask['stance'].value_counts()[1]), str(annotation_mask['stance'].value_counts()[2]), annotation_mask['stance'].value_counts()[0]) )\n          ","c84348d1":"scores_best_incu = selected_best_LDA('incubation', 'abstract')","ef77e795":"# topic number 0 is most relevant to public wearing mask\n# which topic do you think is most relevant to your search\ncor_dict_incu = select_text_from_LDA_results('incubation', 'abstract', scores_best_incu, 0)\nprint (\"There are {} abstracts selected\". format(len(cor_dict_incu)))","438fed34":"# extract relevant sentences  #search keywords can be a list\nsel_sentence_incu, sel_sentence_df_incu = extract_relevant_sentences(cor_dict_incu, ['incubation','day'], 'title')","f1b78f09":"#read extracted article\nsel_sentence_df_incu.head(10)","ce14e41c":"#here we need to add the stats analysis \npath = '..\/input\/annotation\/'\nannotation_incubation = pd.read_csv(path + 'incubation.csv')\nprint('there are {} articles relevant to the topic'.format(annotation_incubation.shape[0]))","b2dcfd08":"incubation = annotation_incubation['stance'].value_counts()\nprint('there are {} paper shows the incubation period is 2-14 days with mean 5 days, {} papers shows a different number'. format(incubation[1], incubation[0])\n     )","51a9fabb":"incubation = annotation_incubation['relevance'].value_counts()\nprint('there are {} papers relevant to the topic, {} papers not relevant to the topic'. format(incubation[1], incubation[0]))","8d7904ec":"scores_best_asym = selected_best_LDA('asymptomatic', 'abstract')","ea23cfa2":"# topic number 8 is most relevant to public wearing mask\n# which topic do you think is most relevant to your search\ncor_dict_asym = select_text_from_LDA_results('asymptomatic', 'abstract', scores_best_asym, 8)\nprint (\"There are {} abstracts selected\". format(len(cor_dict_asym)))","ee60eb68":"# extract relevant sentences  #search keywords can be a list\nsel_sentence_asym, sel_sentence_df_asym = extract_relevant_sentences(cor_dict_asym, ['asymptomatic','transmission'], 'title')","88ea7309":"sel_sentence_df_asym.tail(10)","e5bc85ab":"#here we need to add the stats analysis \nannotation_asymptomatic = pd.read_csv(path + 'asymtomatic.csv')\nprint('there are {} articles relevant to the topic'.format(annotation_asymptomatic.shape[0]))","730c7aa2":"asymptomatic = annotation_asymptomatic['stance'].value_counts()\nprint('{} papers show that there is clear evidence show that asymtomatic cases contribute to the spread of the virus, {} papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus'.format(asymptomatic[1], asymptomatic[0]))","fee01b82":"## Question 3: Are asymptomatic patients infectious?\n\n### Annotation guideline for question 3:\nHere we want to identify whether asymtomatic cases contribute to the spread of the virus\n\n#### stance annotation\n* \u20181\u2019  there is clear evidence show that asymtomatic cases contribute to the spread of the virus\n* \u20180\u2019  it is unlikely that asymtomatic cases contribute to the spread of the virus\n* '3' Not relevant to the question\n\n#### relevance annotation\n* '1' the result is relevent to the question  \n* '0' the result is not relevant to the question","785be6bf":"# Fighting COVID-19 Infodemics \n## Project Design\n\nKnowledge gap between public and specialists and uncertainties are an important factors that drive pandemic anxiety, in this task, we will examine papers that discuss some of the controversial topics that contribute to rumours and anxiety\n\nDuring covid-19, there are lots of invalifated statements circulating in the public. One way to validate these statements is to examine the academic papers. However, most of the public struggle to find the right keywords to search these papers and they do not want to go through the entire paper to find the relevant information. Here I develop a search system to extract sentences from abstracts that are relevant to a statement.\n\nWe extract topic relevant sentences from paper abstracts then manually annotate whether these sentences support or against the statement.\n\nHere are some of the statements:\n\n1. Wearing mask is an effective way to stop the spreading of the virus\n\n2. The incubation period range from 2-14 days with a media of 5 days\n\n3. Asymptomatic patients drive the spread of the virus\n\nFor each question, we define at least two stance according to whether the sentences in the abstract support or against the statement. Here are the steps to achieve this goal:\n\n### Step 1: Developing a topic search system \n\nThe search system first extract abstract contains a keyword (e.g. \u2018mask\u2019), then we use LDA to group the abstract topics. We identify a topic that is  most relevant to the question and we extract abstracts that contain the target topic. The system sentences that contain the keyword from the relevant abstracts. The standard apporach of a search system is to used TFIDF to rank documents, here we use LDA topic modeling on nouns, verbs and adjectives of the abstract. Users can decide the relevant information when they know what are the most frequent keywords in each topic. For some queries, users want to identify articles for covid-19 only. Therefore, users are opt to add title filer for different queries in our system.\n\nThe benefit of this approach is that when we want to know the relevant content for a question, we don't know what are the keywords in the article are more relevant to the question we ask, because the users are usually not farmiliar with academic papers. In our system, the topic keywords serve as prime for the query in the next step for extracting sentences in the abstract.\n\nHowever, there are still a lot of space for improvement in this system. For example, we will need to optimize the topic selection process and combine selected topics.\n\n### Step 2: Mannual annotation of stance and relevance\n\nWe manually annotate the key sentences to identify the stance of the result sentences and whether these sentences are relevant to the question asked. Relevance annotation is a important part for evaluating a search system.\n\n\n#### Annotation results\nTo understand the answer to the relevant question, we need to annotate the stance of the results, such as, does the abstract for \/ against the statement. \n\nTo evaluate the search system, we need to annotate the relevance of the retrieved result. Please refer to each section for annotation guildline\n\nRetrieved results and annotations are in this document \nhttps:\/\/docs.google.com\/spreadsheets\/d\/1-eWEqji7mLXNF0Z9KH8RE5djcxK-97dUHzPWY7GEhI8\/edit?usp=sharing\n\nThe document contains:\n\n1. annotation of stance:\n\nsheet: mask, incubation, asymtomatic, seasonality, column 'stance'\n\n2. annotation for relevance\n\nsheet: mask, incubation, asymtomatic, seasonality, column 'relevance'\n\n3. annotation for system evaluation \n\nsheet: system_eval_varname, column 'relevance'\n\n\n### Result:\n\n### Here we only annotated a subset of the retrieved result. Further analysis need to be done after we annotate all the retrieved reuslts\n\n#### statement: Wearing mask is an effective way to stop the spreading of the virus\n\nAccording to the key sentences in 40 paper abstracts that discuss the topic of public using masks, there are 12 papers support using a mask during a pandemic is useful, 18 assume masks as useful and examine the public\u2019s willingness to comply the rules,  1 papers show no obvious evidence that shows using mask is protective or the protection is very little\n\nWe also see that governments in some regions advocate using masks as a standard approach to reduce risk of infection, papers in these regions focus on whether people comply to the rules. Some government advocate that there is little evidence show that mask is effective in controlling the pandemic, whereas nearly half of the academic papers from our search result consider wearing masks as a standard practice and these paper examine whether the public comply to the recommended practice. Another half of the papers found evidence to support that wearing masks is effective in controlling the pandemic.\n\n\n### statement 2: The incubation period range from 2-14 days with a media of 5 days\n\nThere are 16 paper shows the incubation period is 2-14 days with mean 5 days, 51 papers shows a different number. We can see that majority of the paper shows a slightly different result than what the authorities reported. \n\n#### statement 3: Asymptomatic patients drive the spread of the virus\n\nAccording to 53 papers relevant to this topic, 28 papers show that there is clear evidence support that asymtomatic cases contribute to the spread of the virus, 25 papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus. Therefore, there is no clear evidence show that asymptomatic patients drive the spread of the virus.\n\n\n### Step 3: Evaluation of the system:\n\nThe evaluation of the system is currently being implemented. A test collection will be annotated in order to compare our search system with a baseline system\n","65da1078":"### Annotation guidline for question 1\nWe extracted 33 papers that are supposed to discuss whether using masks is useful. We annotate  whether the key sentences suggest using mask can reduce the risk of infection.\n\n#### Stance Annotation \n* \u20181\u2019 sentences that support using a mask during a pandemic is useful \n* \u20182\u2019  papers that assume masks as useful and examine the public\u2019s willingness to comply the rules,\n* \u20190\u2019 no obvious evidence that shows using mask is protective or the protection is very little\n* '3' Not relevant to the above stance\n\n#### relevance annotation\n* '1' the result is relevent to the question  \n* '0' the result is not relevant to the question","065d4a97":"## Incubation period statistical analysis","ffdd2e08":"### Question 2: How long in incubation period? In some region (e.g. China), there\u2019s rumour circulating that the incubation period is longer than 14 days\n\n### Annotation guideline for question 2:\n\n#### stance annotation\nHere we want to identify papers that report a result aligns with the incubation period reported by the governments\nUK government advocate: 2-14 days, mean 5\n* \u20181\u2019  same as government advocate \n* \u20180\u2019  different from what the government\n*  Not relevant to the question \n\n#### relevance annotation\n* '1' the result is relevent to the question  \n* '2' the result is not relevant to the question","f27ed98a":"### Extract documents contain keywords, preprocessing ","d9e787d2":"## Using LDA to rank documents\nLDA is optimized by coherence score u_mass","40fe994f":"## Question 1: Is wearing mask an effective way to control pandemic?","d9c24cbc":"## Asymptomatic Result","8411d40d":"### Read metadata into dictionary format"}}