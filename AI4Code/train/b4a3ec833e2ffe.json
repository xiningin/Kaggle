{"cell_type":{"dd5c5f65":"code","79bcc821":"code","9e95688c":"code","0efe3dae":"code","3dfa8c75":"code","e8bf4285":"code","0e8d84ba":"code","863d0c78":"code","2552169b":"code","8496ff76":"code","351bb8b1":"code","438433eb":"code","adc58f8a":"code","7210c423":"code","13c8b230":"code","2efac1be":"code","b9b5aab6":"markdown","185cb284":"markdown","2fb356c2":"markdown","d654b006":"markdown","2a23e1cf":"markdown","226fc867":"markdown","dba7e349":"markdown","c5857601":"markdown","ef03eff3":"markdown","72201bec":"markdown","15611c2c":"markdown","b1569ce4":"markdown","104c299a":"markdown","0902db17":"markdown","10470ca3":"markdown","492f285d":"markdown"},"source":{"dd5c5f65":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.tree.export import export_text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib as plt\nfrom sklearn.impute import SimpleImputer\n","79bcc821":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(train.head())","9e95688c":"train.loc[train['Sex']=='male', 'Sex_bin'] = 1\ntrain.loc[train['Sex']=='female', 'Sex_bin'] = 0\n","0efe3dae":"X = train[['Pclass','Sex_bin','Age','SibSp','Parch','Fare']]\nX_for_cols = X.copy()\nsi = SimpleImputer(missing_values=np.nan, strategy='median')\nsi.fit(X)\nX = si.transform(X)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","3dfa8c75":"dt = DecisionTreeClassifier(max_depth = 6)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n\nprint(\"Accuracy score for imputed Decision Tree model: \" + str(accuracy_score(y_test,y_pred)))","e8bf4285":"rf = RandomForestClassifier(max_depth=4, n_estimators = 2000)\nrf.fit(X_train, y_train)\ny_rf_pred = rf.predict(X_test)\n\nprint(\"Accuracy score for imputed Random Forest model: \" + str(accuracy_score(y_test, y_rf_pred)))","0e8d84ba":"# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_for_cols.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')","863d0c78":"train['Title'] = train['Name'].str.split(pat = \"(\\.|\\,)\", expand=True)[2]\ntrain['Title'] = [string.strip() for string in train['Title']]\n\ntest['Title'] = test['Name'].str.split(pat = \"(\\.|\\,)\", expand=True)[2]\ntest['Title'] = [string.strip() for string in test['Title']]\n\nprint(train['Title'].value_counts())\nprint(test['Title'].value_counts())\n","2552169b":"train.loc[train['Title'] == 'Mme', 'Title'] = 'Mlle'\ntrain.loc[train['Title'] == 'Ms', 'Title'] = 'Miss'\ntrain.loc[train['Title'] == 'Major', 'Title'] = 'Officer'\ntrain.loc[train['Title'] == 'Col', 'Title'] = 'Officer'\ntrain.loc[train['Title'] == 'Capt', 'Title'] = 'Officer'\ntrain.loc[train['Title'] == 'the Countess', 'Title'] = 'Countess'\n\nprint(train['Title'].value_counts())\n\ndummies = pd.get_dummies(train['Title'])\ntrain = pd.concat([train, dummies], axis=1)","8496ff76":"test.loc[test['Sex']=='male', 'Sex_bin'] = 1\ntest.loc[test['Sex']=='female', 'Sex_bin'] = 0\n\ntest.loc[test['Title'] == 'Mme', 'Title'] = 'Mlle'\ntest.loc[test['Title'] == 'Dona', 'Title'] = 'Mlle'\ntest.loc[test['Title'] == 'Ms', 'Title'] = 'Miss'\ntest.loc[test['Title'] == 'Major', 'Title'] = 'Officer'\ntest.loc[test['Title'] == 'Col', 'Title'] = 'Officer'\ntest.loc[test['Title'] == 'Capt', 'Title'] = 'Officer'\ntest.loc[test['Title'] == 'the Countess', 'Title'] = 'Countess'\n\nprint(train['Title'].value_counts())\ntest_dummies = pd.get_dummies(test['Title'])\ntest = pd.concat([test, test_dummies], axis=1)","351bb8b1":"cols = ['Pclass','Sex_bin','Age','SibSp','Parch','Fare','Officer', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Master', 'Mlle', 'Mr',\n       'Miss', 'Rev', 'Sir', 'Countess']\n\nX_title = train[cols]\nX_title_for_cols = X_title.copy()\nsi_t = SimpleImputer(missing_values=np.nan, strategy='median')\nsi_t.fit(X_title)\nX_title = si_t.transform(X_title)\ny = train['Survived']\nX_title_train, X_title_test, y_train, y_test = train_test_split(X_title, y, test_size=0.2, random_state=42)","438433eb":"rf = RandomForestClassifier(max_depth=6, n_estimators = 2000)\nrf.fit(X_title_train, y_train)\n\ny_rf_pred = rf.predict(X_title_test)\n\nprint(accuracy_score(y_test, y_rf_pred))","adc58f8a":"# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_title_for_cols.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')","7210c423":"test['Jonkheer'] = 0\ntest['Don'] = 0\ntest['Lady'] = 0\ntest['Countess'] =0\ntest['Sir']=0","13c8b230":"X_output = test[cols]\nsi_output = SimpleImputer(missing_values=np.nan, strategy='median')\nsi_output.fit(X_output)\nX_output = si_output.transform(X_output)\ny_output = rf.predict(X_output)\nprint(y_output)","2efac1be":"output = pd.DataFrame(columns = ['PassengerId', 'Survived'])\noutput['PassengerId'] = test['PassengerId']\noutput['Survived'] = y_output\n\noutput['PassengerId'] = pd.to_numeric(output['PassengerId'], downcast='integer')\noutput['Survived'] = pd.to_numeric(output['Survived'], downcast='integer')\n\noutput.to_csv('output.csv', index=False)\n","b9b5aab6":"Rinse and repeat from above code to get predictions for the competition.","185cb284":"Firring a RandomForestClassifier to this new data, we see that the accuracy has jumped up to 81.56%! All that work wasn't for nothing.","2fb356c2":"The score this submission received was 0.79904! ","d654b006":"\nNow that we know what we're working with there are some decisions to be made. Unfortunately, the sklearn DecisionTreeClassifier and RandomForestClassifier don't handle categorical data, so we either have to clean it up or get rid of it. I think the 'Sex' column is going to be important. I've recoded that into 'Sex_bin.' 'Cabin' and 'Embarked' seem like they're going to be less useful. It's possible that 'Cabin' contains some interesting information, but only about 200 out of 891 possible rows have data for this column. Since 'Embarked' only tells us where a person boarded the Titanic, I'm making the assumption that 'Embarked' isn't telling us something that another column can't. ","2a23e1cf":"Next, I do the same cleaning on the test dataset.","226fc867":"I create the data and labels (X and Y) and then split them into a test\/train set using an 80\/20 split. I'm using a SimpleImputer to fill NaN's with the median from their respective column.","dba7e349":"I start with a decision tree model, which is scoring somewhere in the neighborhood of ~77%. The results here are okay but I think we can do better considering that solely guessing by gender gets you 73% accuracy","c5857601":"I want to clean up 'Title' a bit so I lumped some of the values together. There's possibly some room for improvement here since I was doing minimal Wikipedia research to find the relative importance of some of these titles. \n\nI'm choosing to use One-Hot encoding here since RandomForestClassifier can't handle categorical data.","ef03eff3":"One of the nice things about the Random Forest is that we can see what columns were important to the analysis.In this model Gender was the most important, but Fare, Class, and Age were also important. ","72201bec":"There is one piece of data that I didn't use initially- Name. While I don't think being named \"Alex\" instead of \"John\" would boost anyone's chances of survival, there is information about a person's gender and societal standing encoded into the Name column via the Prefix of the name. I'm taking this prefix and calling it 'Title'. ","15611c2c":"To verify that the 'Title' column and it's dummies actually had an impact we take a look at the importances. As expected, we see Mr, Miss, Master, and Rev all fairly high on the list. ","b1569ce4":"# Titanic Survival Predictor\n\nWe start by importing the packages we're going to need","104c299a":"I throw the column titles into a list and then create my data and labels. This is exactly the same as the code used above. ","0902db17":"Next, we bring in the training data and test data. We aren't going to do anything with the test data for the time being. Print the head of the train data to get an idea of what we're working with.","10470ca3":"The test data had less data than the training data and so a few columns didn't automatically get created. This probably isn't the ideal way to do it, but it stops my model from breaking.  ","492f285d":"The logical next step is a Random Forest Classifier, made from a \"forest\" of random decision trees. This model gives us *slightly* more accuracy, up to about 79%"}}