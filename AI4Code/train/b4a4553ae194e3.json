{"cell_type":{"79971b6e":"code","1f4475f9":"code","a352b995":"code","7631306b":"code","d337e430":"code","3ccbfa2d":"code","5abd9379":"code","000d36db":"markdown","bfacb3c9":"markdown","a048a4f2":"markdown","7f100aad":"markdown","b9677975":"markdown","9b7c8232":"markdown","32cdf017":"markdown","e3fc94a3":"markdown","5fac6e1a":"markdown","ac3efdff":"markdown"},"source":{"79971b6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1f4475f9":"df2 = pd.read_csv(\"..\/input\/3x_univariate_linear_reg.csv\")\nplt.scatter(df2['X'], df2['Y'])","a352b995":"m_tot = df2['X'].count()\n#****************\nteta0_v = -25\nlearning_rate_teta0 = 1.5\nlearning_rate_teta1 = 1.5\n#****************\nY_pred = []\nError = []\nteta1 = []\nteta0 = []\nderivative_teta1 = []\nderivative_teta0 = []\n\n# loop teta0\nfor j in range(0, 50):\n  #*****************\n  teta1_v = -25\n  #*****************\n  \n  # loop teta1\n  for i in range(0, 50):\n    Y_pred = []\n    \n    # loop Xi\n    for m in range(0, m_tot):\n      Y_pred.append(teta0_v + df2['X'][m]*teta1_v)\n\n    df2['Y_pred'] = Y_pred\n    df2['SE'] = (df2['Y_pred'] - df2['Y'])**2\n    df2['SE_derivative_teta1'] = (df2['Y_pred'] - df2['Y'])*df2['X']\n    df2['SE_derivative_teta0'] = (df2['Y_pred'] - df2['Y'])\n  #  plt.figure()\n    plt.scatter(df2['X'], df2['Y'])\n    plt.scatter(df2['X'], df2['Y_pred'])\n    Error.append(sum(df2['SE'])\/(2*m_tot))\n    derivative_teta1.append(sum(df2['SE_derivative_teta1'])\/(m_tot))\n    derivative_teta0.append(sum(df2['SE_derivative_teta0'])\/(m_tot))\n    teta0.append(teta0_v)\n    teta1.append(teta1_v)\n    teta1_v = teta1_v + learning_rate_teta1\n  \n  teta0_v = teta0_v + learning_rate_teta0\n\nJ = pd.DataFrame()\nJ['teta1'] = teta1\nJ['teta0'] = teta0\nJ['error'] = Error\nJ['derivative_teta1'] = derivative_teta1\nJ['derivative_teta0'] = derivative_teta0\nplt.figure()\nplt.plot(J['teta1'], J['error'])\nplt.plot(J['teta0'], J['error'])","7631306b":"J.round(2)\nplt.figure()\nplt.plot(J['teta1'], J['derivative_teta1'])\nplt.plot(J['teta0'], J['derivative_teta0'])","d337e430":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax1 = fig.add_subplot(111,projection='3d')\nax1.scatter(J['teta1'],J['teta0'],J['error'])\nplt.xlabel('teta1', fontsize=16)\nplt.ylabel('error', fontsize=16)\nplt.ylabel('teta0', fontsize=16)\nax1.view_init(30, 30)\n\n# rotate the axes and update\n#for angle in range(0, 360):\n#    ax1.view_init(30, angle)\n#    plt.draw()\n#    plt.pause(.001)","3ccbfa2d":"import scipy.interpolate\n\nN = 500 #number of points for plotting\/interpolation    \nx = J['teta0']\ny = J['teta1']\nz = J['error']\nxll = x.min();  xul = x.max();  yll = y.min();  yul = y.max()\n\nxi = np.linspace(xll, xul, N)\nyi = np.linspace(yll, yul, N)\nzi = scipy.interpolate.griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n\ncontours = plt.contour(xi, yi, zi, 6, colors='white')\nplt.clabel(contours, inline=True, fontsize=13)\nplt.imshow(zi, extent=[xll, xul, yll, yul], origin='lower', cmap=plt.cm.jet, alpha=0.9)\nplt.xlabel(r'$teta0$')\nplt.ylabel(r'$teta1$')\nplt.clim(0, 10)\nplt.colorbar()\nplt.show()","5abd9379":"df2['x0'] = 1\nXarray = np.array([df2['x0'],df2['X']])\nyarray = np.array([df2['Y']])\n\nX = np.mat(Xarray).T\ny = np.mat(yarray).T\n\nfrom numpy.linalg import inv\n\nA = X.T.dot(X)\nAinv = inv(A)\nTHETA = Ainv.dot(X.T).dot(y)\n\nparam = pd.DataFrame(THETA)\nparam.head()","000d36db":"The above graph is a 3D plot of the Cost function according to the 2 parameters **\u03b80** and **\u03b81**.\n\nWith this graph, it is quite difficult to see the minima of this surface.\n\nWe will have a better view of the minima by plotting the contour plot of the above 3D function.","bfacb3c9":"The above 2 graphs are showing respectively : \n- All the hypothesis function we computed starting with **\u03b80 = -25** and **\u03b81 = -25** until **\u03b80 = 50** and **\u03b81 = 50**.\n- The error function for each **\u03b80** and **\u03b81** with **\u03b80** in green and **\u03b81** in blue.","a048a4f2":"In this notebook, we will compute the hypothesis and the Cost function for severral **\u03b80** and **\u03b81**.\n\nWe will then plot :\n    - The hypothesis in order to find a fit between actual output and our predicted output.\n    - The Cost function in order the find the minima of the error.","7f100aad":"**The model is a Supervised Learning - Univariate linear regression with 2 parameters.**\n\nI remind you that the hypothesis function is : $$h_\u03b8(x)=\u03b8_0+\u03b8_1x$$ where **\u03b80** and **\u03b81** have to be defined.\n\nThe Cost function to be minimized is the Mean Squared Error :\n\n$$J(\u03b8)=\\frac{1}{2m}\\sum_{i}^{m}\\left[h_\u03b8(x_i)-y_i \\right]^2$$\n\n**m : number of examples in the data set**\n\n**y : value to be predicted**\n\n**xi : the ith example in the data set**\n","b9677975":"The above graph is showing the derivative of **\u03b80** and **\u03b81**.","9b7c8232":"The above graph looks like linear relationship with following equation : $$y(x)=3x$$\n\nMeans our hypothesis should looks like : $$h_\u03b8(x)=\u03b8_0+\u03b8_1x$$ with **\u03b80 = 0** and **\u03b81 = 3**","32cdf017":"The above graph is a simple contour plot which is the same than the 3D plot.\n\nNow it is easy to read and we can see that the minima of the function is for **\u03b80 = 0** and **\u03b81 = 3**\n\nFinally we retrieve our parameters which minimized the Cost function.\n\nLet's compute the normal equation to confirm our parameters :\n\n**Normal equation :**  $\\theta=(X^TX)^{-1}X^T\\vec{y}$","e3fc94a3":"First, let's have a look on the data in order to see the trend and the model of the variable we want to predict.","5fac6e1a":"Let's see if we can retrieve this parameters computing a Cost function and then minimized this error.\n\nWe will compute this function for several values of **\u03b80** and **\u03b81** and then plot the graphs.","ac3efdff":"Following the gradient descent algorithm, we will also compute and plot the gradient of the Cost function.\n\n$$\\frac{dJ}{d\u03b8_1}=\\frac{1}{m}\\sum_{i}^{m}\\left[(h_\u03b8(x_i)-y_i \\right)x_i]$$\n\n$$\\frac{dJ}{d\u03b8_0}=\\frac{1}{m}\\sum_{i}^{m}\\left[h_\u03b8(x_i)-y_i \\right]$$"}}