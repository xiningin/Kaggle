{"cell_type":{"5bb59030":"code","2b3b38d9":"code","04a2ee83":"code","9351effd":"code","5cf63051":"code","6443978b":"code","591f0db6":"code","5285f71a":"code","0eef1859":"code","5a8028cf":"code","5163127d":"code","97c1cd0a":"code","92338b86":"code","30e1a9e1":"code","3af5aec7":"code","9a6fbbe6":"code","2973f9a1":"code","bb624ba8":"code","202423e6":"code","d434c4a1":"code","064b17f1":"code","0d7147ff":"code","d105d465":"code","108d09cb":"code","b3310dc8":"code","585a3e6a":"code","15dce662":"code","326bfe62":"code","9c45634e":"code","9f17162a":"markdown","04a5c736":"markdown","3705bef5":"markdown","b6a0e400":"markdown","da293d18":"markdown","6e11378e":"markdown","67193ab4":"markdown","0f02c9d9":"markdown","5b6b732e":"markdown","be291e7f":"markdown","10dbda81":"markdown","001b537b":"markdown","78fbfe5a":"markdown","811c7e1b":"markdown","8c936636":"markdown","3aac1a5e":"markdown","ce58bee9":"markdown","f2b2ba25":"markdown","f334422c":"markdown","d5e6bee5":"markdown","6d428afe":"markdown","a6699e12":"markdown","78362c3f":"markdown"},"source":{"5bb59030":"import pandas as pd\nimport numpy as np\nimport keras\nimport os\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nimport random\nimport pickle\nfrom keras import models, layers, callbacks\nimport shutil\nimport cv2\nfrom math import sqrt, floor\nfrom prettytable import PrettyTable","2b3b38d9":"#move to directory containing data\nos.chdir('..\/input\/plant-seedlings-classification')","04a2ee83":"def print_bold(text):\n    print('\\033[1m{}\\033[0m'.format(text))\n\ndef list_files(startpath):\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * (level)\n        \n        dir_name= '{}{}\/'.format(indent, os.path.basename(root))\n        if dir_name.strip().startswith('.'):\n            continue\n        \n        print_bold('\\n'+dir_name)\n            \n        subindent = ' ' * 4 * (level + 1)\n        if level==0:\n            for f in files:\n                if f.startswith('.'):\n                    continue\n                print('{}{}'.format(subindent, f))\n        else:\n            #if len(files)>0:\n            #   print('{}File Count: {}'.format(subindent, len(files)))\n            for i, f in enumerate(files):\n                print('{}{}'.format(subindent, f))\n                if i==2:\n                    print('{}{}'.format(subindent, '...'))\n                    break\n\nlist_files(os.getcwd())","9351effd":"def copyDirectory(src, dest):\n    try:\n        shutil.copytree(src, dest)\n    # Directories are the same\n    except shutil.Error as e:\n        print('Directory not copied. Error: %s' % e)\n    # Any error saying that the directory doesn't exist\n    except OSError as e:\n        print('Directory not copied. Error: %s' % e)\n\nos.chdir('\/kaggle')\nos.mkdir('new_working_dir')\n\nfor d in ['train', 'test']:\n    copyDirectory(os.path.join('input\/plant-seedlings-classification', d), os.path.join('new_working_dir', d))\n\nos.chdir('new_working_dir')","5cf63051":"classes= []\nsample_counts= []\n\nfor f in os.listdir('train'):\n    train_class_path= os.path.join('train', f)\n    if os.path.isdir(train_class_path):\n        classes.append(f)\n        sample_counts.append(len(os.listdir(train_class_path)))\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\ny_pos = np.arange(len(classes))\n\nax.barh(y_pos, sample_counts, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(classes)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Sample Counts')\nax.set_title('Sample Counts Per Class')\n\nplt.show()","6443978b":"fig= plt.figure(figsize= (10, 15))\nfig.suptitle('Random Samples From Each Class', fontsize=14, y=.92, horizontalalignment='center', weight='bold')\n\ncolumns = 5\nrows = 12\nfor i in range(12):\n    sample_class= os.path.join('train',classes[i])\n    for j in range(1,6):\n        fig.add_subplot(rows, columns, i*5+j)\n        plt.axis('off')\n        if j==1:\n            plt.text(0.0, 0.5,str(classes[i]).replace(' ','\\n'), fontsize=13, wrap=True)\n            continue\n        random_image= os.path.join(sample_class, random.choice(os.listdir(sample_class)))\n        #from keras.preprocessing.image\n        img = image.load_img(random_image, target_size=(150, 150))\n        img= image.img_to_array(img)\n        img\/=255.\n        plt.imshow(img)\n        \n        \n\nplt.show()\n","591f0db6":"#create validation set\ndef create_validation(validation_split=0.2):\n    if os.path.isdir('validation'):\n        print('Validation directory already created!')\n        print('Process Terminated')\n        return\n    os.mkdir('validation')\n    for f in os.listdir('train'):\n        train_class_path= os.path.join('train', f)\n        if os.path.isdir(train_class_path):\n            validation_class_path= os.path.join('validation', f)\n            os.mkdir(validation_class_path)\n            files_to_move= int(0.2*len(os.listdir(train_class_path)))\n            \n            for i in range(files_to_move):\n                random_image= os.path.join(train_class_path, random.choice(os.listdir(train_class_path)))\n                shutil.move(random_image, validation_class_path)\n    print('Validation set created successfully using {:.2%} of training data'.format(validation_split))\n                ","5285f71a":"create_validation()","0eef1859":"sample_counts= {}\n\nfor i, d in enumerate(['train', 'validation']):\n\n    classes= []\n    sample_counts[d]= []\n\n    for f in os.listdir(d):\n        train_class_path= os.path.join(d, f)\n        if os.path.isdir(train_class_path):\n            classes.append(f)\n            sample_counts[d].append(len(os.listdir(train_class_path)))\n\n    #fig, ax= plt.subplot(221+i)\n    fig, ax = plt.subplots()\n\n    # Example data\n    y_pos = np.arange(len(classes))\n\n    ax.barh(y_pos, sample_counts[d], align='center')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(classes)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.set_xlabel('Sample Counts')\n    ax.set_title('{} Sample Counts Per Class'.format(d.capitalize()))\n\nplt.show()","5a8028cf":"def pull_random_pixels(samples_per_class, pixels_per_sample):\n    total_pixels= 12*samples_per_class*pixels_per_sample\n    random_pixels= np.zeros((total_pixels, 3), dtype=np.uint8)\n    for i in range(12):\n        sample_class= os.path.join('train',classes[i])\n        for j in range(samples_per_class):\n            \n            random_image= os.path.join(sample_class, random.choice(os.listdir(sample_class)))\n            img= cv2.imread(random_image)\n            img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img=np.reshape(img, (img.shape[0]*img.shape[1], 3))\n            new_pixels= img[np.random.randint(0, img.shape[0], pixels_per_sample)]\n            \n            start_index=pixels_per_sample*(i*samples_per_class+j)\n            random_pixels[start_index:start_index+pixels_per_sample,:]= new_pixels\n\n    h= floor(sqrt(total_pixels))\n    w= total_pixels\/\/h\n    \n    random_pixels= random_pixels[np.random.choice(total_pixels, h*w, replace=False)]\n    random_pixels= np.reshape(random_pixels, (h, w, 3))\n    return random_pixels\n    \nrandom_pixels= pull_random_pixels(10, 50)\n\nplt.figure()\nplt.suptitle('Random Samples From Each Class', fontsize=14, horizontalalignment='center')\nplt.imshow(random_pixels)\nplt.show()\n","5163127d":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib import colors\n\nr, g, b = cv2.split(random_pixels)\nfig = plt.figure(figsize=(8, 8))\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\naxis.view_init(20, 120)\n\npixel_colors = random_pixels.reshape((np.shape(random_pixels)[0]*np.shape(random_pixels)[1], 3))\nnorm = colors.Normalize(vmin=-1.,vmax=1.)\nnorm.autoscale(pixel_colors)\npixel_colors = norm(pixel_colors).tolist()\n\n\naxis.scatter(r.flatten(), g.flatten(), b.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Red\")\naxis.set_ylabel(\"Green\")\naxis.set_zlabel(\"Blue\")\nplt.show()","97c1cd0a":"hsv_img = cv2.cvtColor(np.uint8(random_pixels), cv2.COLOR_RGB2HSV)\n\nh, s, v = cv2.split(hsv_img)\nfig = plt.figure(figsize=(8,8))\naxis = fig.add_subplot(1, 1, 1, projection=\"3d\")\naxis.view_init(50, 240)\n\n\n\naxis.scatter(h.flatten(), s.flatten(), v.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Hue\")\naxis.set_ylabel(\"Saturation\")\naxis.set_zlabel(\"Value\")\nplt.show()","92338b86":"hsv_img = cv2.cvtColor(np.uint8(random_pixels), cv2.COLOR_RGB2HSV)\n\nh, s, v = cv2.split(hsv_img)\nfig = plt.figure(figsize=(6,6))\naxis = fig.add_subplot(1, 1, 1)\n\naxis.scatter(h.flatten(), s.flatten(), facecolors=pixel_colors, marker=\".\")\naxis.set_xlabel(\"Hue\")\naxis.set_ylabel(\"Saturation\")\nplt.show()","30e1a9e1":"lower_bound= (24, 50, 0)\nupper_bound= (55, 255, 255)\n\nfig= plt.figure(figsize=(10, 10))\nfig.suptitle('Random Pre-Processed Image From Each Class', fontsize=14, y=.92, horizontalalignment='center', weight='bold')\n\nfor i in range(12):\n    sample_class=os.path.join('train',classes[i])\n    random_image= os.path.join(sample_class, random.choice(os.listdir(sample_class)))\n    img= cv2.imread(random_image)\n    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img= cv2.resize(img, (150, 150))\n    \n    hsv_img= cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    mask = cv2.inRange(hsv_img, lower_bound, upper_bound)\n    result = cv2.bitwise_and(img, img, mask=mask)\n\n    fig.add_subplot(6, 4, i*2+1)\n    plt.imshow(img)\n    plt.axis('off')    \n\n    fig.add_subplot(6, 4, i*2+2)\n    plt.imshow(result)\n    plt.axis('off')\n    \nplt.show()","3af5aec7":"def color_segment_function(img_array):\n    img_array= np.rint(img_array)\n    img_array= img_array.astype('uint8')\n    hsv_img= cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n    mask = cv2.inRange(hsv_img, (24, 50, 0), (55, 255, 255))\n    result = cv2.bitwise_and(img_array, img_array, mask=mask)\n    result= result.astype('float64')\n    return result","9a6fbbe6":"#files moved from \"test\/\" to \"test\/test_images\" since ImageDataGenerator requires files to be within a subfolder\nos.rename('test', 'test_images')\nshutil.move('test_images', 'test\/test_images')\n\n","2973f9a1":"#image function from keras.preprocessing\ntrain_datagen = image.ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=40,\n      width_shift_range=0.0,\n      height_shift_range=0.0,\n      shear_range=0.0,\n      zoom_range=0.0,\n      horizontal_flip=True,\n      vertical_flip= True,\n    preprocessing_function=color_segment_function,\n      fill_mode='nearest')\n\ntest_datagen = image.ImageDataGenerator(rescale=1.\/255, preprocessing_function=color_segment_function)","bb624ba8":"train_generator = train_datagen.flow_from_directory(\n  'train',\n  target_size=(150, 150),\n  batch_size=20,\n  class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'validation',\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(\n        'test',\n        target_size=(150,150),\n        batch_size=1,\n        class_mode='categorical',\n        shuffle=False)","202423e6":"#get class indices and labels. calculate class weight\nlabel_map = {}\nfor k, v in train_generator.class_indices.items():\n    label_map[v]=k\n\nclass_counts= pd.Series(train_generator.classes).value_counts()\nclass_weight= {}\n\nfor i, c in class_counts.items():\n    class_weight[i]= 1.0\/c\n    \nnorm_factor= np.mean(list(class_weight.values()))\n\nfor k in class_counts.keys():\n    class_weight[k]= class_weight[k]\/norm_factor\n\nt = PrettyTable(['class_index', 'class_label', 'class_weight'])\nfor i in sorted(class_weight.keys()):\n    t.add_row([i, label_map[i], '{:.2f}'.format(class_weight[i])])\nprint(t)","d434c4a1":"model = models.Sequential()\n\nmodel.add(layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.1))\n\n\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.1))\n\n\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.1))\n\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.1))\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.4))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.4))\n\nmodel.add(layers.Dense(12, activation='softmax'))","064b17f1":"model.summary()","0d7147ff":"best_cb= callbacks.ModelCheckpoint('model_best.h5', \n                                         monitor='val_accuracy', \n                                         verbose=1, \n                                         save_best_only=True, \n                                         save_weights_only=False, \n                                         mode='auto', \n                                         period=1)\n\nopt= keras.optimizers.Adam(lr=0.0005, amsgrad=True)","d105d465":"model.compile(optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\nhistory = model.fit_generator(\n                    train_generator,\n                    class_weight= class_weight,\n                    steps_per_epoch= 190,\n                    epochs=60,\n                    validation_data=validation_generator,\n                    validation_steps= 48,\n                    verbose=1,\n                    callbacks= [best_cb])","108d09cb":"#load best model from training\nmodel= models.load_model('model_best.h5')","b3310dc8":"#save history\nwith open('model_history.pkl', 'wb') as f:\n    pickle.dump(history, f)","585a3e6a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.figure()\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","15dce662":"pred= model.predict_generator(test_generator, steps= test_generator.n, verbose=1)","326bfe62":"predicted_class_indices=np.argmax(pred,axis=1)\n\nprediction_labels = [label_map[k] for k in predicted_class_indices]\nfilenames= test_generator.filenames","9c45634e":"import csv\ncsvfile= open('ray_reed_submission.csv', 'w', newline='')\nwriter= csv.writer(csvfile)\n\nheaders= ['file', 'species']\n\nwriter.writerow(headers)\nt = PrettyTable(headers)\nfor i, f, p in zip(range(len(filenames)), filenames, prediction_labels):\n    writer.writerow([os.path.basename(f),p])\n    if i <10:\n        t.add_row([os.path.basename(f), p])\n    elif i<13:\n        t.add_row(['.', '.'])\ncsvfile.close()\nprint(t)","9f17162a":"As seen above, there is some class inbalance. I will account for this by defining class weights at the time of training. Lets have a look at a few random images from each class.","04a5c736":"I will now construct a set of class weights to be provided during training in order to account for the class imbalance I've noted earlier. The result will be a value will be centered around 1, which represents the weight that will be applied to the loss value for the corresponding class. A higher weight value will  essentially tell the optimizer to treat these class as more important when adjusting trainable parameters. \n\nUsing a normalized inverse frequency will produce higher weights for under-represented classes, which will avoid situations in which our CNN gives preference to certain classes just because there are more records from that class. This set of class weights are provided to the model's fit_generator method.  ","3705bef5":"First, lets have a look at the directory structure seen below. The three dots (...) indicate that there were too many files in the directory to list","b6a0e400":"Load the best model which ws saved by our checkpoint callback.","da293d18":"I define my model below. I use 4 convolutional layers followed by a densely connected layer. I include dropout for each layer in order to avoid overfitting. Relu activation functions are used for all layers except the last one which requires a softmax activation in order to produce a result appropriate for multiclass classification.\n\nA fair amount of experimentation has been done, and the architecture below has yeilded the best results. Although I have not saved the configuration and results of previous models in any systematic way, these experiments have involved modifying things such as:\n\n-Number of layers<br \/>\n-Use of batch normalization<br \/>\n-Learning rate schedules<br \/>\n-Different optimizers (rmsprop, SGD, adadelta, etc.)<br \/>\n-Use of dropout<br \/>\n-Nodes per layer<br \/>\n-Activation function (leaky relu)<br \/>\n-Training image size (refit to 300X300 instead of 150X150<br \/>\n-Inclusion\/exclusion of background removal<br \/>\n-Using a pre-trained model<br \/>\n\nA more systematic approach to will be a goal for future projects, perhaps employing a random search across hyperparameters. This will become more practical with access to GPUs and multiple nodes for parallel training. \n","6e11378e":"I train the model below over 50 epochs. Note that the steps per epoch must be defined explicitely when using generators.","67193ab4":"I use the Adam optimizer along with ModelCheckpoint callback to save the best model during training. The model checkpoint considers the model with the highest accuracy value to be the best. Although I initially used validation loss as the monitored quantity when running locally, I found that the loss value is much more volatile when running in the kaggle environment, even though validation accuracy increases smoothly.\n\nAs for the optimizer- The Adam optimizer will dynamically adjust its learning rate throughout training, it is not advised to define a learning rate scheduler as I've done in earlier trials with SGD. ","0f02c9d9":"In HSV space, it looks like our clusters are more neatly seperable by choosing upper and lower bounds of HSV values. We can also clearly see that the green cluster comes from the plants and the brown, white, and grey clusters must come from the dirt, rocks, and other things in the background. It looks like there is little variance along the V (value) axis, so lets plot this in 2 dimensions (H & S).","5b6b732e":"The class with the fewest number of records in our validation set has just over 40 records. Although this is less than ideal, it should still be sufficient to get a decent measure of our model's accuracy.\n\nNow I will attempt to remove the background from the images to see if can find a method which generalizes well across all images, then this can be used to accelerate training by isolating the important part of our data. The strategy will be to find upper and lower bounds within a color space which will only contain the green part of the plants. We will then turn the rest of the background black. In order to find the best values for these upper and lower bounds, I grab random pixels from random training images from each of my 12 classes. I will then take this random collection of pixels and plot it in color space i hopes that I can find upper and lower bounds which cleanly seperate the green part of the plants. ","be291e7f":"From an initial view, it looks like the green regions may be seperable from the rest, but simply coosing bounds of RGB values will not work due to the shape of the distribution. Before resorting to more sophisticated methods to isolate these pixels, lets try a differe color space basis (HSV).","10dbda81":"From here, we can point to an upper and lower bound. I will isolate pixels with Hue values ranging from 24 to 55 and Saturation values ranging from 50 to 255.\n\nI will grab a random image from each class and use the cv2 library to map the pixels in HSV space and black out the background using the bounds I've identified previously. I will display the original and transformed images next to eachother.","001b537b":"I will take advantage of data augmentation during training. Our training generator will apply random flips and rotations to our images in addition to the background removal function I've defined above. I will also define a validation and testing generator. The advantage of using generators is that there we can do our training and validation in batches while also avoiding the need to load all of our data into memory at once. This is especially important when working with large datasets which may be too large to even store locally.\n\nFirst, I make a change to the test directory in order to make the directory tree in the format expected by the ImageDataGenerator. ","78fbfe5a":"I condider this project to be a success. As mentioned above a major goal of the next project is to establish access to more powerful hardware (multiple nodes with GPU) in order to perform a more systemaic search for hyperparameter tuning. I would also like to explore other computer vision techniques to improve pre-processing efforts and accelerate training process even further. I would also like to better understand why batch normalization and some of the other architectures have yeilded such poor results.","811c7e1b":"Now lets have a look at a bar chart showing counts for each class in our training and validation set.","8c936636":"This approach seems to have worked well. I will use this in my model. Its important to note that this should <em>not<\/em> be done if we expect the background to vary in real data. For example, if we were developing an ML application for these specific gardeners, then this pre-processing should be fine since we expect real life data to have a background consistent with what we see here. If we were to generalize this application to seedling images with different backgrounds, using different cameras, etc. then our pre-processing technique may cause issues.\n\nI will create a function to make the above transformation compatible with the ImageDataGenerator object from Keras, which I will be using in our model. ","3aac1a5e":"I generate a submission file in the required format. I also print a preview of this table in the cell below to show this format.","ce58bee9":"I also have the history of the model. I plot the loss and history of the model below over the 50 training epochs. ","f2b2ba25":"The parent folder of our training images indicates the class label. Lets have a look at the count of records for each class","f334422c":"There are some important things to note here. We see that the resolution of these images can vary significantly. We also see that images for a particular specied seem to have been taken at various points of its life cycle. Luckily, all of the images share a roughly similar background which may allow for us to remove it from the image in order to accelerate training.\n\nBefore doing this, we must address the fact that there is no validation dataset yet. I will construct a validation set using 20% of the training set. In order to maintain the same distribution, I will randomly select 20% from each class.","d5e6bee5":"The goal of this project is to classify images of seedlings into 1 of 12 possible species. After some initial data exploration, I will perform some pre-processing using computer vision techniques in order to accelerate the training process. Finally, I will train a CNN model and generate a submission file containing predictions for the test set provided by Kaggle.","6d428afe":"Since we cannot modify directories within the \"input\" folder, we will copy the files to a new working directory which allow for modification. ","a6699e12":"The image above is a random sampling of 50 pixels from each of 10 training images from each of 12 classes. I will now plot these pixels in color space (RGB).","78362c3f":"We see that the accuracy of the training and validation set increased smoothly, plateauing around 90%. We also see that the training and validation accuracy remained roughly equal throughout the process, suggesting that overfitting was not an issue. \n\nWe do see that the validation loss is very noisy, however. It\u2019s unclear why the validation loss is so turbulent despite the smoothly increasing validation accuracy. Furthermore, I found that the validation loss smoothly decreased when training the same model on my local machine. Some searching suggests that this may be a bug which is specific to the ImageDataGenerator function, the machine being used, or perhaps the version of Keras (I was using a different version on my local machine). I have linked the open ticket raising this bug:\n\nhttps:\/\/github.com\/keras-team\/keras\/issues\/8273"}}