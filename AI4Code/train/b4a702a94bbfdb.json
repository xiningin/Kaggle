{"cell_type":{"fdcdcfea":"code","ee3b3903":"code","29fba843":"code","471cedb7":"code","3963d87a":"code","eb16cfc8":"code","48ee68cc":"code","79c3ee85":"code","7bf15955":"markdown","a77228ee":"markdown","dc30e27e":"markdown"},"source":{"fdcdcfea":"import numpy as np\nfrom sklearn.datasets import make_blobs","ee3b3903":"class LogisticRegression:\n    \"\"\"\n    1. i need LogisticRegression class\n    2. I need __init__ function with learning rate and #iteration parameter\n    3. I need train function with X,y parameter\n    4. I need predict function\n    5. I need sigmoid function for gradiant decent and hypothesis\n    \"\"\"\n    def __init__(self, learning_rate = 0.1, iteration = 10000):\n        \"\"\"\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.lr = learning_rate\n        self.it = iteration\n    \n    def cost_function(self, y, y_pred):\n        \"\"\"\n        :param y: Original target value.\n        :param y_pred: predicted target value.\n        \"\"\"\n        return -1 \/ self.m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n\n    # hypothesis function.   \n    def sigmoid(self, z):\n        \"\"\"\n        :param z: Value to calculate sigmoid.\n        \"\"\"\n        return 1 \/ (1 + np.exp(-z))\n\n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        # Target value should be in the shape of (n, 1) not (n, ).\n        # So, this will check that and change the shape to (n, 1), if not.\n        try:\n            y.shape[1]\n        except IndexError as e:\n            # we need to change it to the 1 D array, not a list.\n            print(\"ERROR: Target array should be a one dimentional array not a list\"\n                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n            return \n        # m is number of training samples.\n        self.m  = X.shape[0]\n        # n is number of features\/columns\/dependant variables.\n        self.n = X.shape[1]\n\n        # Set the initial weight.\n        self.w = np.zeros((self.n , 1))\n        # bias.\n        self.b = 0\n\n        for it in range(1, self.it+1):\n            # 1. Find the predicted value.\n            # 2. Find the Cost function.\n            # 3. Find the derivation of weights and bias.\n            # 4. Apply Gradient Decent.\n\n            y_pred = self.sigmoid(np.dot(X, self.w) + self.b)\n\n            cost = self.cost_function(y, y_pred)\n\n            # Derivation of w and b.\n            dw = 1 \/ self.m * np.dot(X.T , (y_pred - y))\n            db = 1 \/ self.m * np.sum(y_pred - y)\n\n            # Chnage the parameter value\/ apply Gradient decent.\n            self.w = self.w - (self.lr * dw)\n            self.b = self.b - (self.lr * db)\n\n            if it % 1000 == 0:\n                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n    \n    def predict(self, test_X):\n        \"\"\"\n        :param: test_X: Values need to be predicted.\n        \"\"\"\n        y_pred = self.sigmoid(np.dot(test_X, self.w) + self.b)\n        # output of the sigmoid function is between [0 - 1], then need to convert it to class values either 0 or 1.\n        y_pred_class = y_pred >=0.5\n\n        return y_pred_class","29fba843":"# Define the traning data.\nX, y = make_blobs(n_samples=5000, centers=2)\n\n# Chnage the shape of the target to 1 dimentional array.\ny = y[:, np.newaxis]\n\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\nprint(\"Shape of the target value ----------> {}\".format(y.shape))","471cedb7":"#define the parameters\nparam = {\n    \"learning_rate\" : 0.1,\n    \"iteration\" : 10000\n}\nprint(\"=\"*100)\nlog_reg = LogisticRegression(**param)\n\n# Train the model.\nlog_reg.train(X, y) \n\n# Predict the values.\ny_pred = log_reg.predict(X)\n\n#calculate accuracy.\nacc = np.sum(y==y_pred)\/X.shape[0]\nprint(\"=\"*100)\nprint(\"Accuracy of the prediction is {}\".format(acc))","3963d87a":"from sklearn.linear_model import LogisticRegression as LogisticRegression_sklearn\nfrom sklearn.metrics import accuracy_score","eb16cfc8":"# data is already defined, going to use the same data for comparision.\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))","48ee68cc":"log_reg_sklearn = LogisticRegression_sklearn()\nlog_reg_sklearn.fit(X, y)\n\n# predict the value\ny_pred_sklearn = log_reg_sklearn.predict(X)\nacc = accuracy_score(y, y_pred_sklearn)\nprint(\"=\"*100)\nprint(\"Accuracy of the prediction is {}\".format(acc))","79c3ee85":"# The main different between this scratch and sklearn model is speed.\n# Sklearn model train and predict more faster than sractch code.\n#  anyways this scracth code is maily for understanding the behind math and logics :) :)","7bf15955":"# Supervised Machine Learning models scratch series....\nyou can also check....\n\n- 1) Linear Regression         ---> https:\/\/www.kaggle.com\/ninjaac\/linear-regression-from-scratch\n- 2) Lasso Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch \n- 3) Ridge Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch\n- 4) ElasticNet Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/elasticnet-regression-from-scratch \n- 5) Polynomail Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch \n- 5) PolynomailRidge Regression---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch \n- 6) KNN Classifier            ---> https:\/\/www.kaggle.com\/ninjaac\/knnclassifier-from-scratch \n- 6) Logistic Regression       ---> https:\/\/www.kaggle.com\/ninjaac\/logistic-regression-from-scratch (Same Notebook you are looking now)\nNext :<br\/>\n# why we are using binary cross entropy  as a cost function for logistric regression model.","a77228ee":"# Logistic Regression From Scratch","dc30e27e":"# Logistic Regression Using sklearn"}}