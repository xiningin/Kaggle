{"cell_type":{"8bf47411":"code","2241c239":"markdown"},"source":{"8bf47411":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom keras.utils import Sequence\nimport numpy as np\nimport pandas as pd\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers.merge import concatenate\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nclass DataGenerator(Sequence):\n    def __init__(self, in1, in2, in3, in4, in5, ySet):\n        self.in1, self.in2, self.in3, self.in4, self.in5 = in1, in2, in3, in4, in5\n        self.y = ySet\n\n    def __len__(self):\n        return len(self.in1)\n\n    def __getitem__(self, idx):\n        batch1 = self.in1[idx]\n        batch1 = batch1[np.newaxis, :, :]\n        batch2 = self.in2[idx]\n        batch2 = batch2[np.newaxis, :, :]\n        batch3 = self.in3[idx]\n        batch3 = batch3[np.newaxis, :, :]\n        batch4 = self.in4[idx]\n        batch4 = batch4[np.newaxis, :, :]\n        batch5 = self.in5[idx]\n        batch5 = batch5[np.newaxis, :, :]\n        if self.y.empty:   # condition for predict generator.\n            return [batch1,batch2,batch3,batch4,batch5]\n        else:\n            batchY = self.y[idx]\n            return [batch1,batch2,batch3,batch4,batch5], np.atleast_2d(batchY)\n\napptrain = pd.read_csv('..\/input\/application_train.csv', index_col='SK_ID_CURR')\napptest = pd.read_csv('..\/input\/application_test.csv', index_col='SK_ID_CURR')\napptrain = apptrain.select_dtypes(exclude='object')\napptest = apptest.select_dtypes(exclude='object')\nfor col in apptrain.columns:\n    if col != 'TARGET':\n        apptrain[col] = scaler.fit_transform(apptrain[[col]])\n        apptest[col] = scaler.transform(apptest[[col]])\nfilelist = ['credit_card_balance','previous_application','POS_CASH_balance','installments_payments','bureau']\nprint(filelist[0])\ndf_dict = dict()\ndf_dict_train = dict()\ndf_dict_test = dict()\ndef myfunc(grObj):\n    arr1 = np.array(grObj)\n    return arr1\nfor file in filelist:\n    file_str = \"..\/input\/%s.csv\" % (file)\n    df_dict[file] = pd.read_csv(file_str, index_col='SK_ID_CURR')\n    df_dict[file] = df_dict[file].select_dtypes(exclude='object')\n    df_dict[file] = df_dict[file].loc[:, ~df_dict[file].columns.isin(['SK_ID_PREV','SK_ID_BUREAU'])] ## removing extra indices\n    ### merging the input file with train and test indices to handle missing indices and sorting.\n    df_dict_train[file] = pd.merge(apptrain[apptrain.columns[0:2]], df_dict[file], how='left',\n                              on=['SK_ID_CURR'])\n    df_dict_test[file] = pd.merge(apptest[apptest.columns[0:2]], df_dict[file], how='left',\n                              on=['SK_ID_CURR'])\n    df_dict_train[file].drop(df_dict_train[file].columns[0:2], axis=1, inplace=True)\n    df_dict_test[file].drop(df_dict_test[file].columns[0:2], axis=1, inplace=True)\n    ### scaling each column\n    for col in df_dict[file].columns:\n        df_dict_train[file][col] = scaler.fit_transform(df_dict_train[file][[col]])\n        df_dict_test[file][col] = scaler.transform(df_dict_test[file][[col]])\n    df_dict_train[file] = df_dict_train[file].fillna(0)\n    # Creating groupby object and making dataframe of arbitrary length numpy arrays.\n    df_dict_train[file] = df_dict_train[file].groupby('SK_ID_CURR')\n    df_dict_train[file] = df_dict_train[file].apply(myfunc)\n    ###\n    df_dict_train[file] = df_dict_train[file].reset_index(drop=True)\n    \n    df_dict_test[file] = df_dict_test[file].fillna(0)\n    df_dict_test[file] = df_dict_test[file].groupby('SK_ID_CURR')\n    df_dict_test[file] = df_dict_test[file].apply(myfunc)\n    df_dict_test[file] = df_dict_test[file].reset_index(drop=True)\nyTrain = apptrain['TARGET']    \nyTrain = yTrain.reset_index(drop=True)\n# Network definition:\n#Simple RNN for converting the arbitrary length of batch size to single length.\n#Each input node represents one input file.\ninput1 = Input(shape=(None, df_dict_train[filelist[0]][0].shape[1]))\nrnn1 = SimpleRNN(df_dict_train[filelist[0]][0].shape[1])(input1)\ndense1_1 = Dense(8, activation='relu')(rnn1)\ninput2 = Input(shape=(None, df_dict_train[filelist[1]][0].shape[1]))\nrnn2 = SimpleRNN(df_dict_train[filelist[1]][0].shape[1])(input2)\ndense2_1 = Dense(8, activation='relu')(rnn2)\ninput3 = Input(shape=(None, df_dict_train[filelist[2]][0].shape[1]))\nrnn3 = SimpleRNN(df_dict_train[filelist[2]][0].shape[1])(input3)\ndense3_1 = Dense(8, activation='relu')(rnn3)\ninput4 = Input(shape=(None, df_dict_train[filelist[3]][0].shape[1]))\nrnn4 = SimpleRNN(df_dict_train[filelist[3]][0].shape[1])(input4)\ndense4_1 = Dense(8, activation='relu')(rnn4)\ninput5 = Input(shape=(None, df_dict_train[filelist[4]][0].shape[1]))\nrnn5 = SimpleRNN(df_dict_train[filelist[4]][0].shape[1])(input5)\ndense5_1 = Dense(8, activation='relu')(rnn5)\n\nmerge = concatenate([dense1_1,dense2_1,dense3_1,dense4_1,dense5_1])\ndense = Dense(50, activation='relu')(merge)\noutput = Dense(1)(dense)\nmodel = Model(inputs=[input1,input2,input3,input4,input5],outputs=output)\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc', 'mse'])\ntrainGenerator = DataGenerator(df_dict_train[filelist[0]], df_dict_train[filelist[1]], df_dict_train[filelist[2]], df_dict_train[filelist[3]], df_dict_train[filelist[4]], yTrain)\ntestGenerator = DataGenerator(df_dict_test[filelist[0]], df_dict_test[filelist[1]], df_dict_test[filelist[2]], df_dict_test[filelist[3]], df_dict_test[filelist[4]], pd.Series())\nmodel.fit_generator(generator=trainGenerator,steps_per_epoch=len(yTrain))\ny = model.predict_generator(generator=testGenerator)\nyhat = pd.DataFrame()\nyhat['SK_ID_CURR'] = appTrain.index\nyhat['TARGET'] = y.flatten()\nyhat.to_csv('yout.csv')\nprint(yhat)","2241c239":"![model_plot.png](attachment:model_plot.png)"}}