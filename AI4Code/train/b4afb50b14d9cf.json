{"cell_type":{"9f4eabb0":"code","7e379204":"code","bf78b57e":"code","9d8c39a3":"code","b5882f19":"code","6cfd3385":"code","97e8a147":"code","e9800d22":"code","02dda33d":"code","11dfba9f":"code","f75c53ee":"code","169c0ee1":"code","5e93a791":"code","8b45924f":"code","19fc2282":"code","01c60ef3":"code","f1a82046":"code","623f96a3":"code","b948e3b4":"code","6dc0fc0f":"code","e6a930f6":"code","dab74fdd":"code","e4b23116":"code","b5cf508b":"code","e3cca640":"code","2f44efef":"code","d5c221f2":"code","aa36a51d":"code","18a52e97":"code","5fc1973b":"code","4fb5f655":"code","f3464b75":"code","8bbc9bf9":"code","9cb06ef9":"code","8351ff6d":"code","a8c94c75":"code","7c363e3b":"code","7945af5e":"code","16b0f937":"code","3b1cb75d":"code","861642aa":"code","82603bb5":"code","c93841d6":"code","180a0e47":"code","edd38acc":"code","c0d48d25":"code","fa822e90":"code","5cbb8601":"code","8c79d833":"code","abe60d5c":"code","f83734d9":"code","1e9eb3b7":"code","2f49b36b":"code","2190f924":"code","eac669b4":"code","c53f453f":"code","f94bbb50":"code","d564b8b1":"code","5775e496":"code","cd009746":"code","21a24b3a":"code","c34e8343":"markdown","5ac8e7d1":"markdown","b5581978":"markdown","1bc4b876":"markdown","7e1b9dd8":"markdown","7f30744f":"markdown","a4727ffe":"markdown","293f08bc":"markdown","06910891":"markdown","c58f0429":"markdown","5f33cd85":"markdown","944e9f62":"markdown","d39d349c":"markdown","1f8a991b":"markdown","870f8bc8":"markdown","7f78c236":"markdown","e10f8dbe":"markdown","eb1fdc1c":"markdown","7df80b40":"markdown","b3ed2712":"markdown","ede87df0":"markdown","bfb98987":"markdown","ce75b6c0":"markdown","ec3b3b69":"markdown","f686e0c4":"markdown","9a0f5d1d":"markdown"},"source":{"9f4eabb0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport json\nfrom time import time\nimport pickle\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM\nfrom keras.layers.merge import add","7e379204":"def readTextFile(path):\n    \n    with open(path) as f:\n        \n        # We are used read() because readlines() will give a separate lines to a big caption\n        captions = f.read()\n        \n    return captions","bf78b57e":"captions = readTextFile(\"..\/input\/flikr8k\/Flickr8k_text\/Flickr8k.token.txt\")\n# print(captions)\n# Data is large","9d8c39a3":"captions = captions.split(\"\\n\")[:-1]\nlen(captions) # total lines","b5882f19":"captions[0]","6cfd3385":"# Image_Id : [ list of captions ]\n\ndescriptions = {}\n\nfor x in captions:\n    \n    first, second = x.split(\"\\t\")\n    img_name = first.split('.')[0]\n    \n    # if img_name doesn't exist\n    if descriptions.get(img_name) is None:\n        descriptions[img_name] = []\n        \n    descriptions[img_name].append(second)","97e8a147":"descriptions[\"1000268201_693b08cb0e\"]","e9800d22":"## Image\nsample = image.load_img(\"..\/input\/flikr8k\/Flickr8k_Dataset\/Flicker8k_Dataset\/1000268201_693b08cb0e.jpg\")\nplt.imshow(sample)\nplt.axis(\"off\")\nplt.show()","02dda33d":"def clean_text(sentence):\n    \n    sentence = sentence.lower()\n    sentence = re.sub(\"[^a-z]+\", \" \", sentence) # Removing all the non-alphabetical characters\/words\n    sentence = sentence.split()\n    \n    sentence = [w for w in sentence if len(w)>1 ]\n    \n    sentence = \" \".join(sentence)\n    \n    return sentence\n    ","11dfba9f":"clean_text(\"A Cat is sitting over the house #64\")","f75c53ee":"# Clean all the captions\nfor key,caption_list in descriptions.items():\n    \n    for i in range(len(caption_list)):\n        \n        caption_list[i] = clean_text(caption_list[i])","169c0ee1":"descriptions[\"1000268201_693b08cb0e\"]","5e93a791":"# Storing these descriptions in a text file\nwith open(\"descriptions.txt\", \"w\") as f:\n    f.write(str(descriptions))","8b45924f":"descriptions = None\n# Directly reading the preprocessed file\nwith open(\"descriptions.txt\",\"r\") as f:\n    descriptions = f.read()\n    \njson_acceptable_string = descriptions.replace(\"'\", \"\\\"\")\ndescriptions = json.loads(json_acceptable_string)","19fc2282":"print(type(json_acceptable_string)) # Which we read from the file\nprint(type(descriptions)) # which we converted into a dict using json.loads()","01c60ef3":"# Vocab\n\nvocab = set()\n\nfor key in descriptions.keys():\n    \n    [vocab.update(sentence.split()) for sentence in descriptions[key] ]\n\nprint(\"Size of Vocab :\", len(vocab))\n","f1a82046":"# Total number of words across all the sentences\n\ntotal_words = []\n\nfor key in descriptions.keys():\n    \n    [total_words.append(i) for des in descriptions[key] for i in des.split() ]\n    \nprint(\"Total Words : \", len(total_words))","623f96a3":"# Filter words from the vocab according to ceratin threshold frequency\nimport collections\n\n# word : freq\ncounter = collections.Counter(total_words)\n\n# as a dict\nfreq_count = dict(counter)\n\nprint(freq_count)","b948e3b4":"# Sorting the dict according to freq count\n\n# x[0] : key || x[1] : value\nsorted_freq_count = sorted( freq_count.items(), reverse=True, key=lambda x : x[1])\n\n# Filtering according to threshold\nthreshold = 10\nsorted_freq_count = [ x for x in sorted_freq_count if x[1] > threshold]\ntotal_words = [ x[0] for x in sorted_freq_count ]","6dc0fc0f":"len(total_words)","e6a930f6":"# These are strings of all the image names\ntrain_file_data = readTextFile(\"..\/input\/flikr8k\/Flickr8k_text\/Flickr_8k.trainImages.txt\")\ntest_file_data = readTextFile(\"..\/input\/flikr8k\/Flickr8k_text\/Flickr_8k.testImages.txt\")","dab74fdd":"# Splitting the file to retrieve the image names\n\ntrain = [ row.split(\".\")[0] for row in train_file_data.split(\"\\n\")[:-1]]\ntest = [ row.split(\".\")[0] for row in test_file_data.split(\"\\n\")[:-1]]","e4b23116":"print(len(train))\nprint(len(test))","b5cf508b":"train_descriptions = {}\n\nfor img_id in train:\n    \n    train_descriptions[img_id] = []\n    \n    for caption in descriptions[img_id]:\n        \n        caption_to_append = \"startseq \" + caption + \" endseq\"\n        train_descriptions[img_id].append(caption_to_append)\n        ","e3cca640":"train_descriptions[\"1000268201_693b08cb0e\"]","2f44efef":"model = ResNet50(weights=\"imagenet\", input_shape=(224, 224, 3))\nmodel.summary()","d5c221f2":"# (m X 2048) : output [ ENCODING ]\n# 2048 represents the number of features\nmodel_new = Model(model.input, model.layers[-2].output)","aa36a51d":"def preprocess_img(img):\n    \n    img = image.load_img(img, target_size=(224, 224))\n    img = image.img_to_array(img)\n    \n    # Converted from 3D to a 4D tensor\n    # EX: (224, 224, 3) -- axis=0 --> (1, 224, 224, 3)\n    # We can also use resape\n    img = np.expand_dims(img, axis=0)\n    \n    # We need to feed this image to the ResNet-50 model\n    # Normalizing according to how it was trained\n    img = preprocess_input(img)\n    \n    return img","18a52e97":"# Sample\nsample = preprocess_img(\"..\/input\/flikr8k\/Flickr8k_Dataset\/Flicker8k_Dataset\/1000268201_693b08cb0e.jpg\")\nplt.imshow(sample[0]) # as it is a tensor\nplt.axis(\"off\")\nplt.show()\n# The image seems different due to the Normalization technique used by ResNet-50\n# It subtracts the channel mean ( R G and B ) from each corresponding channel","5fc1973b":"# Encoding an Image\ndef encode_image(img):\n    img = preprocess_img(img)\n    # (1 X 2048)\n    feature_vector = model_new.predict(img)\n    # 2048\n    feature_vector = feature_vector.reshape((-1,))\n    return feature_vector","4fb5f655":"encode_image(\"..\/input\/flikr8k\/Flickr8k_Dataset\/Flicker8k_Dataset\/1000268201_693b08cb0e.jpg\")","f3464b75":"# Encoding all Images\nstart = time()\nIMAGE_PATH = \"..\/input\/flikr8k\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"\n\n# (image_id, feature_vector)\nencoding_train = {}\n\nfor ix, img_id in enumerate(train):\n    img_path = IMAGE_PATH + img_id + \".jpg\"\n    encoding_train[img_id] = encode_image(img_path)\n    \n    # Tracking the progress\n    if ix%100 == 0:\n        print(\"Encoding in progress || Time step {}\".format(ix))\n        \nend = time()\nprint(\"Total time {}\".format(end - start))","8bbc9bf9":"with open(\"encoded_train_features.pkl\", \"wb\") as f:\n    pickle.dump(encoding_train, f)","9cb06ef9":"# Encoding all Images\nstart = time()\nIMAGE_PATH = \"..\/input\/flikr8k\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"\n\n# (image_id, feature_vector)\nencoding_test = {}\n\nfor ix, img_id in enumerate(test):\n    img_path = IMAGE_PATH + img_id + \".jpg\"\n    encoding_test[img_id] = encode_image(img_path)\n    \n    # Tracking the progress\n    if ix%100 == 0:\n        print(\"Encoding in progress || Time step {}\".format(ix))\n        \nend = time()\nprint(\"Total time {}\".format(end - start))","8351ff6d":"with open(\"encoded_test_features.pkl\", \"wb\") as f:\n    pickle.dump(encoding_test, f)","a8c94c75":"word_to_idx = {}\nidx_to_word = {}\n\nfor i,word in enumerate(total_words):\n    \n    # We are not using 0 as it is reserved for extrapolating short sentences\n    word_to_idx[word] = i+1\n    idx_to_word[i+1] = word","7c363e3b":"# Adding <s> and <e>\nprint(len(idx_to_word))","7945af5e":"idx_to_word[1846] = 'startseq'\nword_to_idx['startseq'] = 1846\n\nidx_to_word[1847] = 'endseq'\nword_to_idx['endseq'] = 1847\n\nvocab_size = len(word_to_idx) + 1 # 1 for 0 that is reserved\nprint(\"Vocab Size : {}\".format(vocab_size))","16b0f937":"max_len = 0 # for training batches to the model\nfor key in train_descriptions.keys():\n    for cap in train_descriptions[key]:\n        max_len = max(max_len, len(cap.split()))\n        \nprint(max_len)","3b1cb75d":"def data_generator(train_descriptions, encoding_train, word_to_idx, max_len, batch_size):\n    X1, X2, y = [],[],[]\n    \n    n = 0\n    while True:\n        \n        # for image_id, caption_list in train_descriptions\n        for key, desc_list in train_descriptions.items():\n            n += 1\n            \n            # (2048,) vector of the image\n            photo = encoding_train[key]\n            \n            # for caption in caption_list\n            for desc in desc_list:\n                \n                # Each word contributes to a training point\n                # If the word is not present in the vocab, then ignore it\n                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n                \n                # Setting up as a SUPERVISED LEARNING PROBLEM\n                for i in range(1, len(seq)):\n                    xi = seq[0:i]\n                    yi = seq[i]\n                    \n                    # Remember, we reserved 0 [ padding word ]\n                    # Because, we needed to pad the sentence so as it's length = max_len\n                    # It accepts a 2d matrix\n                    xi = pad_sequences([xi], maxlen=max_len, value=0, padding='post')[0]\n                    \n                    # one hot encoding\n                    yi = to_categorical([yi], num_classes=vocab_size)[0]\n                    \n                    X1.append(photo)\n                    X2.append(xi)\n                    y.append(yi)\n                    \n                if n == batch_size:\n                    # Here we use generator\n                    yield ([np.array(X1), np.array(X2)], np.array(y))\n                    \n                    X1, X2, y = [],[],[]\n                    n = 0","861642aa":"f = open(\"..\/input\/glove-embeddings\/glove.6B.50d.txt\", encoding='utf8')","82603bb5":"embedding_index = {}\n\nfor line in f:\n    \n    values = line.split()\n    word = values[0]\n    word_embedding = np.array(values[1:], dtype='float')\n    embedding_index[word] = word_embedding","c93841d6":"f.close()","180a0e47":"# 50 dimensional glove vector for \"apple\"\nprint(embedding_index[\"apple\"])","edd38acc":"def get_embedding_matrix():\n    \n    emb_dim = 50\n    matrix = np.zeros((vocab_size, emb_dim))\n    \n    for word,idx in word_to_idx.items():\n        \n        # 50d vector from GLOVE embeddings\n        embedding_vector = embedding_index.get(word)\n        \n        if embedding_vector is not None:\n            matrix[idx] = embedding_vector\n            \n    return matrix","c0d48d25":"embedding_matrix = get_embedding_matrix()\nprint(embedding_matrix.shape)","fa822e90":"embedding_matrix[1847] # start and end sequence are not in Glove embeddings\n# Thus, it will be a 0 vector","5cbb8601":"# Image Model\n\n# ResNet50's output for image\ninput_img_features = Input(shape=(2048,))\ninp_img1 = Dropout(0.3)(input_img_features)\ninp_img2 = Dense(256,activation='relu')(inp_img1)","8c79d833":"# Captions as Input\n\ninput_captions = Input(shape=(max_len,)) #35\n\n# Embeddings (batch_size,35,) --> (batch_size,35,50)\ninp_cap1 = Embedding(input_dim=vocab_size, output_dim=50, mask_zero=True)(input_captions)\n\ninp_cap2 = Dropout(0.3)(inp_cap1)\ninp_cap3 = LSTM(256)(inp_cap2) # 256 : output activation vector\/ Hidden vector","abe60d5c":"# Concatenating Image and Captions\n# Combining both of their outputs\n\n# 2 (256,) tensors as input\ndecoder1 = add([inp_img2, inp_cap3])\n\n# Those 2 tensors are combined to create a (256,) tensor\ndecoder2 = Dense(256, activation='relu')(decoder1)\n\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n","f83734d9":"# Combined Model\nmodel = Model(inputs=[input_img_features, input_captions], outputs=outputs)","1e9eb3b7":"model.summary()","2f49b36b":"model.layers[2].set_weights([embedding_matrix])\n# As we are using a pretrained model,\n# Therefore, no need to train the Embedding Layer\nmodel.layers[2].trainable = False","2190f924":"# Compiling the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","eac669b4":"epochs = 30\nbatch_size = 3\nsteps = len(train_descriptions)\/\/batch_size","c53f453f":"import os\nos.mkdir(\"model_weights\")","f94bbb50":"def train():\n    \n    for i in range(epochs):\n        print(i)\n        generator = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, batch_size)\n        # Each batch will be passed for 1 epoch\n        model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n        model.save('.\/model_weights\/model_' + str(i) + \".h5\")","d564b8b1":"train()","5775e496":"# Loading the weights\nmodel = load_model(\".\/model_weights\/model_29.h5\")","cd009746":"def predict_caption(photo):\n    \n    # We will feed 2 things to the model\n    # 1. Feed the image vector (2048,)\n    # 2. Provide the start sequence \"startseq\" (<s>)\n    \n    inp_text = \"startseq\"\n    \n    for i in range(max_len):\n        \n        sequence = [word_to_idx[w] for w in inp_text.split() if w in word_to_idx]\n        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n        \n        ypred = model.predict([photo,sequence])\n        \n        # Greedy Sampling : Word with max probability always\n        ypred = ypred.argmax()\n        \n        # retreiving the word\n        word = idx_to_word[ypred]\n        \n        # adding it to the sequence\n        inp_text += (' ' + word)\n        \n        # If <e>\/end sequence is encountered\n        if word == \"endseq\":\n            break\n            \n    # removing <s> and <e>\n    final_caption = inp_text.split(' ')[1:-1]\n    final_caption = ' '.join(final_caption)\n    \n    return final_caption","21a24b3a":"for i in range(15):\n    \n    # We have 1000 testing images\n    idx = np.random.randint(0, 1000)\n    \n    all_test_images = list(encoding_test.keys())\n    \n    img_name = all_test_images[idx]\n    \n    test_photo = encoding_test[img_name].reshape((1, 2048))\n    \n    i = plt.imread(IMAGE_PATH + img_name + \".jpg\")\n    \n    caption = predict_caption(test_photo)\n    plt.imshow(i)\n    plt.title(caption)\n    plt.axis(\"off\")\n    plt.show()\n    \n    ","c34e8343":"## Data Preprocessing for Captions","5ac8e7d1":"## Thanks for Reading the Notebook\n## ANY IMPROVEMENTS AND SUGGESTIONS ARE WELCOMED\n## Have a Nice Day :)","b5581978":"### __Pre-Initializing Embedding Layer__\n- with Glove Vectors matrix, which we have already created","1bc4b876":"### Prepare Descriptions for the training data\n- We need to add < s > ( start token ) and an < e > ( end token )\n- < s > is needed to begin the sent, along with the image vector\n- < e > is needed to be generated to end the sentence\n- That is why, these special tokens are added so that the model learns when to start and end","7e1b9dd8":"### Read the captions from the file","7f30744f":"## Image Captioning","a4727ffe":"### Steps\n1. Data Collection\n2. Understanding the data\n3. Data Cleaning\n4. Loading the training set\n5. Data Preprocessing of Images\n6. Data Preprocessing of Captions\n7. Data Preparation using Generator Function\n8. Word Embeddings\n9. Model Architecture\n10. Inference\n11. Evaluation","293f08bc":"## Prepare Train and Test Data\nThe motive is to create :\n- train_description\n- test_description\n- These are ( image_name, list of captions )","06910891":"## Model Architecture","c58f0429":"### Final Vocab Size : 1845 !!\n- total words : 3,73,837\n- unique words : 8,424\n- filtered vocab : 1,845","5f33cd85":"## Training Of Model","944e9f62":"## Storing the features to the disk\n- We will use __PICKLE__\n- RAM <=> DISK\n- for storing data in disk : __dump__\n- for loading data in RAM : __load__\n","d39d349c":"## Testing\n- Testing on some random images","1f8a991b":"### Extraction of features from images\n1. Image is preprocessed\n2. Passed to ResNet-50 Conv Base\n3. Receives a (m X 2048) tensor [ m : number of examples ]\n4. Store these feature vectors","870f8bc8":"## Word Embeddings\n- Embedding layer can be trained\n- Use a pre-trained Embedding Layer ( TRANSFER LEARNING )\n- We will use GLOVE embeddings ( Glove6b50D.txt )","7f78c236":"### Note: We will preprocess only the train captions","e10f8dbe":"## Creating a Data Loader\/Generator","eb1fdc1c":"## Data Cleaning\n- don't remove stopwords : the model should know how to insert stopwords in a sentence\n- don't perform stemming : for proper English\n- remove non-alphabetical symbols : reduces vocab size","7df80b40":"### TRANSFER LEARNING\n- Images --> Features\n- Text --> Features","b3ed2712":"### Building Embedding matrix for our Vocab\n- If the word is not found in embedding_index, then that word will be a 0 vector","ede87df0":"### Doing the same for extracting Test features","bfb98987":"### Dictionary to map each image to its corresponding caption","ce75b6c0":"## Image Feature Extraction\n- ResNet50 Model\n- It has __skip connections__ and branching\n- That is why, there is separate column for __\"Connected to\"__\n- Skip Connections helps in tackling with Vanishing Gradients","ec3b3b69":"### Creating Vocabulary\n- model gives an output in the form of a vector\n- we need to convert that vector into a specific word\n- therefore, we need a vocab","f686e0c4":"## Predictions","9a0f5d1d":"## Flikr8k Dataset\n- 6000 images for training\n- 1000 for validation\n- 1000 for testing\n\n- 5 captions to describe an image"}}