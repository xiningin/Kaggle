{"cell_type":{"e09d4fef":"code","076b63db":"code","d3753605":"code","290782a9":"code","f6cbfcad":"code","0b299844":"code","b628bb99":"code","d821bb98":"code","853d7d00":"code","7739f1f9":"code","9e07a9e4":"code","6574fd85":"code","11bb599a":"code","b0ea7540":"code","1b5c96f4":"code","b7eba12c":"code","f2b8fd2b":"code","5d4b19b8":"code","da2ec972":"code","0a58c355":"code","046ea6b4":"code","1a04eb62":"code","163c4d4c":"code","43bb8e1e":"code","51b1dac3":"code","caaeaf49":"code","91ecfd3c":"code","69e24d5a":"code","4457d5aa":"code","ccdd351d":"code","f45deeeb":"markdown","6fb5ae0f":"markdown","a38f472f":"markdown","2c2a0a02":"markdown","96239c10":"markdown","0c48b14c":"markdown","282ba4ad":"markdown","ba00a6dd":"markdown","57b1180e":"markdown","276edac2":"markdown","1cc7d531":"markdown","fe320207":"markdown","014f9512":"markdown","8700b1c5":"markdown","8cea8d1b":"markdown","e18555e6":"markdown","b512b85d":"markdown","786d4c7f":"markdown","cd41006c":"markdown","cdaf9817":"markdown","54f54e2b":"markdown","498d5b03":"markdown","3e622142":"markdown","49e1c98a":"markdown","10d51799":"markdown","417cb8c1":"markdown","1e89f395":"markdown","02d858d7":"markdown"},"source":{"e09d4fef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","076b63db":"import tensorflow as tf\nimport pandas as pd\nimport seaborn as sb\n\nsb.set()","d3753605":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain = train.drop(['PassengerId'], 1)","290782a9":"train.columns","f6cbfcad":"train = train.drop(['Name'],1)","0b299844":"train['FamilySize'] = train['SibSp'] + train['Parch']\ntrain = train.drop(['SibSp','Parch'],1)\ntrain = train.drop(['Ticket'],1)","b628bb99":"print('FamilySize: ' + str(train['FamilySize'].unique())) \nprint('Embarked: ' + str(train['Embarked'].unique())) \nprint('Pclass: ' + str(train['Pclass'].unique()))\nprint('Sex: ' + str(train['Sex'].unique())) ","d821bb98":"print(train['Cabin'].isnull().sum())","853d7d00":"train['Cabin']=train['Cabin'].fillna('NA')","7739f1f9":"print(train['Cabin'].unique())","9e07a9e4":"train['Cabin']=train['Cabin'].astype(str).str[0]","6574fd85":"print(train.isnull().sum())","11bb599a":"train['Embarked'] = train['Embarked'].fillna(train['Embarked'].value_counts().index[0])","b0ea7540":"for i in ['male','female']:\n    for j in [3,1,2]:\n        temp_dataset=train[(train['Sex']==i) &  (train['Pclass']==j)]['Age'].dropna()\n        train.loc[(train.Age.isnull()) & (train.Sex==i) & (train.Pclass==j),'Age']=int(temp_dataset.median())","1b5c96f4":"train.head()","b7eba12c":"sb.catplot(x = 'Sex', y = 'Survived', data = train, kind = 'bar')","f2b8fd2b":"train.loc[(train['Sex'] == 'male'),'Sex']=1\ntrain.loc[(train['Sex'] == 'female'),'Sex']=2","5d4b19b8":"sb.catplot(y=\"Embarked\",x='Survived',data=train,kind='bar')","da2ec972":"train.loc[(train['Embarked'] == 'S'),'Embarked']=1\ntrain.loc[(train['Embarked'] == 'Q'),'Embarked']=2\ntrain.loc[(train['Embarked'] == 'C'),'Embarked']=3","0a58c355":"sb.catplot(y='Cabin',x='Survived',data=train,kind='bar')","046ea6b4":"train.loc[(train['Cabin'] == 'T'),'Cabin']=0\ntrain.loc[(train['Cabin'] == 'N'),'Cabin']=1\ntrain.loc[(train['Cabin'] == 'A'),'Cabin']=2\ntrain.loc[(train['Cabin'] == 'G'),'Cabin']=3\ntrain.loc[(train['Cabin'] == 'C'),'Cabin']=4\ntrain.loc[(train['Cabin'] == 'F'),'Cabin']=5\ntrain.loc[(train['Cabin'] == 'E'),'Cabin']=6\ntrain.loc[(train['Cabin'] == 'B'),'Cabin']=7\ntrain.loc[(train['Cabin'] == 'D'),'Cabin']=8","1a04eb62":"train.head()","163c4d4c":"train['Age_Band']=pd.cut(train['Age'],5)\nprint(train['Age_Band'].unique())\n\ntrain['Fare_Band']=pd.cut(train['Fare'],3)\nprint(train['Fare_Band'].unique())\n","43bb8e1e":"train = train.drop(['Age_Band'], 1)\ntrain = train.drop(['Fare_Band'], 1)","51b1dac3":"train.loc[(train['Age']<=16.136),'Age']=1\ntrain.loc[(train['Age']>16.136) & (train['Age']<=32.102),'Age']=2\ntrain.loc[(train['Age']>32.102) & (train['Age']<=48.068),'Age']=3\ntrain.loc[(train['Age']>48.068) & (train['Age']<=64.034),'Age']=4\ntrain.loc[(train['Age']>64.034) & (train['Age']<=80.),'Age']=5\n\ntrain.loc[(train['Fare']<=170.776),'Fare']=1\ntrain.loc[(train['Fare']>170.776) & (train['Fare']<=314.553),'Fare']=2\ntrain.loc[(train['Fare']>314.553) & (train['Fare']<=513),'Fare']=3","caaeaf49":"train.head()","91ecfd3c":"train_x = train.drop(['Survived'],1)\ntrain_x = train_x.astype('float64').to_numpy()\ntrain_labels = train['Survived'].to_numpy()","69e24d5a":"model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(32, activation = tf.nn.relu),\n        tf.keras.layers.Dense(16, activation = tf.nn.relu),\n        tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n    ])","4457d5aa":"model.compile(optimizer = tf.optimizers.Adam(),\n              loss = 'binary_crossentropy',\n              metrics=['accuracy'])","ccdd351d":"model.fit(train_x, train_labels, epochs=25)","f45deeeb":"This implies, that majority of the entries are null in the training data. Let's, replace null entries with the word 'NA'.","6fb5ae0f":"Now, let's split the train set into input data and labels","a38f472f":"Now, let's take a look at the unique elements contained in each column feature. ","2c2a0a02":"According to the survival values found, we'll encode our Cabin classes.","96239c10":"We'll be using Adam optimization and binary_crossentropy loss, as this is a binary classification problem.","0c48b14c":"We also, see that it might be better to split the ***Fare*** and ***Age*** categories into groups. Let's split ***Age*** into 5 and ***Fare*** into 3 groups respectively. Here are the category ranges:","282ba4ad":"Now, that we've succesfully encoded all our non-numeric values, let's again take a look at out dataset:","ba00a6dd":"As, we work with numerical values, The feature values for the fields:\n1. Sex\n2. Cabin\n3. Embarked \nneed to be coverted to numeric values","57b1180e":"Now, in order to resolve the 177 null entries for the feature ***Age*** let's take median of the given ***Sex*** in a ***Pclass*** and replace *null* elements with it  \n*( the median value )*.","276edac2":"Let's start by examining *null* entries in the Database","1cc7d531":"**All Set!!!, Let's run the model!** *(25 epochs is an experimental value, and not precisely accurate)*","fe320207":"# Embarked\nLet's first visualize the survival rate vs Embark:","014f9512":"# The Model\nWe'll be using a Fully Connected Sequential model having two hidden layers with ( 32 & 16 neurons respectively ) and 1 output layer. ","8700b1c5":"Clearly, as the ***Embarked*** feature has only two *null* entries, let's replace them with the most frequently occuring value in the column.","8cea8d1b":"Let's read in the train dataset. ( Dropping PassengerId, as it's just a primary key )","e18555e6":"Let's see, how many null entries we have left:","b512b85d":"Let's have a look at the column headers available in the dataframe","786d4c7f":"Let's first have a look at our train dataset:","cd41006c":"# Sex\nLet's first visualize the survival rate for the either gender:","cdaf9817":"Survival rate for C > Q > S. Let's accordingly encode 'C': 3, 'Q': 2 & 'S': 1","54f54e2b":"Clearly, as the feature ***Name*** wouldn't be having much impact on survival chances, let's drop it too.","498d5b03":"Let's replace these values with their respective group characters, i.e. (A, B, C, .. T)","3e622142":"The Cabin names are of vast variety, however it can be seen, that they effectively belong to groups (A, B, C, .. T):","49e1c98a":"# Feature Engineering","10d51799":"# Cabin\nLet's visualize Cabin vs Survival Rate:","417cb8c1":"Also, the features ***Parch*** and ***SibSp*** collectively represent the number of family members. Hence, let's club them into a single feature column.","1e89f395":"Final values:","02d858d7":"Clearly, females have a higher survival possibility than males. Let's, encode Male with 1 & Female with 2."}}