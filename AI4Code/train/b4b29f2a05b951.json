{"cell_type":{"512b0c74":"code","a18030d7":"code","9d1ef9a9":"code","c15f757b":"code","51c7a3d4":"code","2f0d85dd":"code","855555b0":"code","1bfdf0c7":"code","c84248a0":"code","ac73f31c":"code","d1e4e725":"code","102072d5":"code","25cc075a":"code","4444078b":"code","a98f7549":"code","0326deb0":"code","69bcc52e":"code","1eba0102":"code","591fa131":"code","0fd00616":"code","d46b2c84":"code","8f2aec19":"code","ed9c35c3":"code","e89c2e8d":"code","971b7c52":"code","6102c9f0":"code","72fa4e16":"code","5f562280":"code","5d5ea1fb":"code","81cb0ff3":"code","fc586992":"markdown","78168a9c":"markdown","77bc3008":"markdown","33b586cf":"markdown","d6fe1f7b":"markdown","7010a691":"markdown","3fdfee11":"markdown","5f840c38":"markdown","2a68fe57":"markdown","1cb5920f":"markdown","f4155424":"markdown","2d853d57":"markdown","bc130ebc":"markdown","05d7dab7":"markdown","b8d5c158":"markdown","7aeeed4c":"markdown","60535fd8":"markdown","a7c1fd69":"markdown","c6c441f9":"markdown","f2be00d1":"markdown","d006454a":"markdown","d0a1a6c5":"markdown","4e51d2c0":"markdown","162ab17f":"markdown","fcad8be3":"markdown","0f2eab51":"markdown","86df26da":"markdown","bc306d62":"markdown","ad6eb350":"markdown","9ba1bf7a":"markdown","815966a6":"markdown","89f824cf":"markdown","7cf7857e":"markdown","3d506ffb":"markdown"},"source":{"512b0c74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom wordcloud import WordCloud\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a18030d7":"# import train and test set\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","9d1ef9a9":"train.head()","c15f757b":"test.head()","51c7a3d4":"# visualise the occurances\/distribution of all the class types.\nnum_c = train['type'].value_counts()\nf, ax = plt.subplots(figsize=(12,6))\nsns.barplot( num_c.values, num_c.index, palette=\"Blues_d\")\nax.xaxis.grid(False)\nax.set(xlabel=\"Number of Occurrences\")\nax.set(ylabel=\"Personality Types\")\nax.set(title=\"Personality Occurances\")\nsns.despine(trim=True, left=True, bottom=True)","2f0d85dd":"# drop the  'Id' colum since it's unnecessary for the prediction process.\ntrain_type = train[['type']]\ntest_ID = test['id']\ntrain.drop(['type'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","855555b0":"# put all the features together to enable quick transformation process\npost_features = pd.concat([train, test],sort=False).reset_index(drop=True)\npost_features.head()","1bfdf0c7":"# using regular expressions for dealing with special patterns of noise.\npattern = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\npost_features = post_features.replace(to_replace = pattern, value = ' ', regex = True)","c84248a0":"post_features.head()","ac73f31c":"# creating a cleaned corpus\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nlem = WordNetLemmatizer() # returns the actual root word of a text token\n# ps = PorterStemmer() # cuts off the surfix but doesnt necessarily return the root word\ncorpus =[]\n# cleaning text\nfor i in range(0,len(post_features['posts'])):\n\n    post = re.sub('[^a-zA-Z]', ' ',str(post_features.iloc[i].values)) \n    post = post.lower()\n    post = post.split()\n    # ps.stem(word) #MethodAttempt\n    stop_wrds = set(stopwords.words('english'))\n    post = [lem.lemmatize(word, \"v\") for word in post if not word in stop_wrds]\n    post = ' '.join(post)\n    corpus.append(post)\n\npost_features['posts_clean'] = corpus\npost_features.drop(['posts'], axis=1, inplace=True)              ","d1e4e725":"post_features.head()","102072d5":"# Create Ranked Statistical Features with TfidfVectorizer, best choice because insignificant words recieve a lower rank\nobj = TfidfVectorizer()\npost_final_features = obj.fit_transform(post_features['posts_clean'])\nprint(post_final_features[0])\n\n#AnotherMethodAttempt\n# Create Ranked Statistical Features with CountVectorizer has no ranking machanism\n# obj = CountVectorizer() \n# post_final_features = obj.fit_transform(post_features['posts_clean']).toarray()\n# print(post_final_features[0:5])","25cc075a":"# label extraction and creating our target variables from personality types\npd.options.mode.chained_assignment = None\nyc = pd.DataFrame(train_type['type'])\ntrain_type['I-E'] = train_type['type'].astype(str).str[0]\ntrain_type['I-E'] = train_type['I-E'].map({\"I\": 0, \"E\": 1})\ntrain_type['S-N'] = train_type['type'].astype(str).str[1]\ntrain_type['S-N'] = train_type['S-N'].map({\"S\": 0, \"N\": 1})\ntrain_type['F-T'] = train_type['type'].astype(str).str[2]\ntrain_type['F-T'] = train_type['F-T'].map({\"F\": 0, \"T\": 1})\ntrain_type['P-J'] = train_type['type'].astype(str).str[3]\ntrain_type['P-J'] = train_type['P-J'].map({\"P\": 0, \"J\": 1})\ntrain_type.drop('type', axis=1, inplace=True) \ny = train_type\nprint(y[0:5])","4444078b":"y = np.array(y)\nprint(y[0:5])","a98f7549":"# Spliting the data back to train(X,y) and test(X_sub)\nX = post_final_features[:len(y), :]\nX_final_test = post_final_features[len(y):, :]\nprint('Features size for train(X,y) and test(X_final_test):')\nprint('X', X.shape, 'y', y.shape, 'X_final_test', X_final_test.shape)","0326deb0":"# wordcloud of the most frequently used words by each personality\nyc['posts_clean'] = post_features['posts_clean']\nlabels = yc['type'].unique()\nrow, col = 5, 3\nwc = WordCloud(stopwords = ['infj','entp','intp','intj', 'isfps','istps','isfjs','istjs',\n                             'entjs','enfjs','infps','enfps','entj','enfj','infp','enfp',\n                             'estps','esfps','estjs','esfjs','isfp','istp','isfj','istj',\n                             'estp','esfp','estj','esfj','infjs','entps','intps','intjs'])\nfig, ax = plt.subplots(5, 3, figsize=(20,15))\nfor i in range(5):\n    for j in range(3):\n        c_type = labels[i*col+j]\n        c_ax = ax[i][j]\n        df = yc[yc['type'] == c_type]\n        wordc = wc.generate(df['posts_clean'].to_string())\n        c_ax.imshow(wordc)\n        c_ax.axis('off')\n        c_ax.set_title(label=c_type,fontdict = {'fontsize': 20})","69bcc52e":"# Predicting model\nmodel = OneVsRestClassifier(LogisticRegressionCV(Cs=30, solver = 'saga',\n                                                       multi_class = 'multinomial', cv=5), n_jobs =-1)","1eba0102":"# split the train set to create a validation set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","591fa131":"# train model with 80% of train set\nmodel.fit(X_train, y_train)","0fd00616":"# predict 20% of train set\ny_pred = model.predict(X_test)","d46b2c84":"# performance matrics and model eveluation using sklearn.metrics inbuilt classification metric\nprint(classification_report(y_test, y_pred, target_names=['Mind', 'Energy', 'Nature', 'Tactics']))","8f2aec19":"\n#AnotherMethodAttempt\n# from sklearn.naive_bayes import MultinomialNB\n# model = OneVsRestClassifier(MultinomialNB())","ed9c35c3":"#AnotherMethodAttempt\n# from sklearn.neighbors import KNeighborsClassifier\n# model = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=3))","e89c2e8d":"# Reinitialise final model\nmodel = OneVsRestClassifier(LogisticRegressionCV(Cs=30, solver = 'saga',\n                                                 multi_class = 'multinomial', cv=10), n_jobs =-1)","971b7c52":"# train final model with full train set \nmodel.fit(X, y)","6102c9f0":"# pridicting the actual test set (X_final_test)\ny_predicted = model.predict(X_final_test)","72fa4e16":"# Final model results\ny_predicted[0:5]","5f562280":"# Plot the accuracy for each classifier\nf, ax = plt.subplots(figsize=(20, 10))\nsns.set_color_codes(palette='deep')\nplt.subplots_adjust(wspace = 0.5)\nax = plt.subplot(2, 4, 1)\nplt.pie([sum(y_predicted[:,0]), len(y_predicted[:,0]) - sum(y_predicted[:,0])],\n        labels = ['Extraverted', 'Introverted'],explode = (0, 0.1),autopct='%1.1f%%', colors=['y','orange'])\nax.set(title=\"Mind\")\n\nax = plt.subplot(2, 4, 2)\nplt.pie([sum(y_predicted[:,1]), len(y_predicted[:,1]) - sum(y_predicted[:,1])], \n        labels = ['Sensing', 'Intuitive'],explode = (0, 0.1),autopct='%1.1f%%', colors=['y','orange'])\nax.set(title=\"Energy\")\n\nf, ax2 = plt.subplots(figsize=(20, 10))\nplt.subplots_adjust(wspace = 0.5)\n\nax2 = plt.subplot(2, 4, 1)\nplt.pie([sum(y_predicted[:,2]), len(y_predicted[:,2]) - sum(y_predicted[:,2])], \n        labels = ['Thinking', 'Feeling'],explode = (0, 0.1),autopct='%1.1f%%', colors=['salmon','y'])\nax2.set(title=\"Nature\")\n\nax2 = plt.subplot(2, 4, 2)\nplt.pie([sum(y_predicted[:,3]),  len(y_predicted[:,3]) - sum(y_predicted[:,3])], \n        labels = ['Judging', 'Perceiving'], explode = (0, 0.1), autopct='%1.1f%%', colors=['y','salmon'])\nax2.set(title=\"Tactics\")\nplt.show()\n","5d5ea1fb":"# format submission of the predicted classes\nsubmission = pd.DataFrame({'id' : np.array(test_ID),'mind' : y_predicted[:,0], \n                           'energy' : y_predicted[:,1], 'nature' : y_predicted[:,2], \n                           'tactics' : y_predicted[:,3]})\nprint('Save submission')","81cb0ff3":"# save DataFrame to csv file for submission\n# submission.to_csv(\"new_submission.csv\", index=False)","fc586992":"# <center> Personality Profile Prediction <\/center>","78168a9c":"Here we are going to go through the three steps of **Noise Removal**(Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise), **Lexicon Normalization**(Another type of textual noise is about the multiple representations exhibited by single word), **Object Standardization**(Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by models). These will enable us to get our data into a format that we can work with.","77bc3008":"<a id=\"1\"><\/a>","33b586cf":"# 1. Introduction","d6fe1f7b":"* **Model Bulding and Validation**","7010a691":"Separating the data into a proper training(X,y) and the testing(X_final_test) set.","3fdfee11":"# Table of contents","5f840c38":"<a id=\"7\"><\/a>","2a68fe57":"<a id=\"5\"><\/a>","1cb5920f":"# 3. Text Preprocessing","f4155424":"Since, our text is in an unstructured form, various types of noise are present in it and the data is not readily analyzable without any pre-processing. In this section we go through the entire process of cleaning and standardization of text, making it noise-free and ready for analysis.","2d853d57":"![](https:\/\/collegepublicspeakingresources.files.wordpress.com\/2014\/09\/myersbriggs1.jpg)","bc130ebc":"To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques. Here we are going to use **Statistical features**. Here text data is quantified directly into numbers using several techniques implemented in this section. **Term Frequency \u2013 Inverse Document Frequency (TF \u2013 IDF**) and **CountVectorizer**.","05d7dab7":"# 5. Text Classification","b8d5c158":"# 7. Conclusion","7aeeed4c":"# 4. Text to Features (Feature Engineering on text data)","60535fd8":"Importing all the necessary packages and input data thats going to be necessary for our predictions.","a7c1fd69":"Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. In these case we have 8 categories(**(I-E)**, **(S-N)**, **(F-T)**, **(P-J)**) that we are classifiying. ","c6c441f9":"These models did not work very well compared to the LogisticRegressionCV. These was based on accuracy score and execution time. LogisticRegressionCV had a much better accuracy and with its build in cross validation method, helped to avoid overfitting.","f2be00d1":"<a id=\"3\"><\/a>","d006454a":"Submission of the predicted results from our test data. The predicted array is sliced into 4 categories(**mind** (I-E), **energy** (S-N), **nature** (F-T), **tactics** (P-J) to match the required prediction format.","d0a1a6c5":"[](http:\/\/)1. [Introduction](#1)\n2. [Importing Packages & Data](#2)\n3. [Text Preprocessing](#3)\n4. [Text to Features ](#4)\n5. [Text Classification](#5)\n    * [Model Validation](#)\n    * [Other Models Used](#)\n    * [Final model](#)\n    \n6. [Submission](#6)\n7. [Conclusion](#7)\n","4e51d2c0":"<a id=\"2\"><\/a>","162ab17f":"# <center> Classify a person's MBTI personality type using text from what they post online. <\/center><a id=\"index\"><\/a>","fcad8be3":"* **Final Model**","0f2eab51":"# 6. Submission","86df26da":"*  **Other Models Used**","bc306d62":"# 2. Importing Packages & Data","ad6eb350":"The hyperparameters where selected manually through trial and error. Other hyperparameter tuning models like GridSearchCV did not work due to multi-class classification nature of our model.","9ba1bf7a":"<a id=\"4\"><\/a>","815966a6":"<a id=\"6\"><\/a>","89f824cf":"In this challenge, you will be required to build and train a model (or many models) capable of predicting a person's MBTI label using only what they post in online forums.\n\nThis challenge will require the use of Natural Language Processing to convert the data into machine learning format. This data will then be used to train a classifier capable of assigning MBTI labels to a person's online forum posts.\n\nEach person will have only one of the two categories for each variable above. Combining the four variables gives the final personality type. For example, a person who is **Extraverted**, **Intuitive**, **Thinking** and **Judging** will get the ENTJ personality type.\n\nYou will need to build and train a model that is capable of predicting labels for each of the four MBTI variables - i.e. predict four separate labels for each person which, when combined, results in that person's personality type.","7cf7857e":"Visualising Accuracy and Distribution of each class\/category of our test data. This is the best alternative to visualise the model accuracy as we cannot do it with the usual accuracy matrics methods as they require the target variable y which we do not have for the test dataset.","3d506ffb":"From our analysis we can conclude that the most common personality type based on our test set is **ISFP** (Introverted Sensing Feeling Perceiving). These is based on comments that are obtained from the test set that does not necessarily represent the whole population of users because we have a limited dataset and its only those who participate in comments."}}