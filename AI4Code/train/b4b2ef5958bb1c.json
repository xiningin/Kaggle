{"cell_type":{"91a388f8":"code","f6964b44":"code","d141bcd6":"code","4fb0dc65":"code","6607c2ed":"code","2c2a3891":"markdown","07ae577f":"markdown","7629a383":"markdown","36170fdc":"markdown","9185f2d8":"markdown","2d310d11":"markdown","514023a0":"markdown","f8078776":"markdown","c42862cb":"markdown","e37ca985":"markdown"},"source":{"91a388f8":"import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\n\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\nplt.plot(X, y, \"b.\")\nplt.axis([0, 2, 0, 15])\nplt.show()","f6964b44":"X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\nbeta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)","d141bcd6":"beta_best","4fb0dc65":"X_new = np.array([[0], [2]])\nX_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\ny_predict = X_new_b.dot(beta_best)\ny_predict","6607c2ed":"plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\nplt.axis([0, 2, 0, 15])\nplt.legend()\nplt.show()","2c2a3891":"## 2. The proposed solution\n\nTo solve this problem we have to introduce two big assumptions:\n\n1- The $p$ predictors are independent variables.\n\n2- The underlying relationship between $X$ and $Y$ is linear ($f$ is linear).\n\nThe second assumption means that we are looking for linear approximation of the real function. Our assumption is that the function $f$ take the following linear form in $X$:\n\n$f(X) = {\\beta}_{0} + \\sum \\limits _{j=1} ^{p} X_{j}{\\beta}_{j} $\n\nThis reduces our problem from learning an arbitrary function $f(X)$ in $p+1$ dimensions to a problem of only estimating $p+1$ coefficients $\\beta_{0}, \\beta_{1}, ..., \\beta_{p}$\n\nSweet! The next step is to fit the linear model, in other words estimating the coefficients. To find the optimal values for the coefficients we need to define a criterion, and the residual sum of squares is a popular and convenient one. For $N$ observations (this would be the number of data points in the training dataset) and $p$ different predictors (the number of features), the residual sum of squares is defined by the following formula:\n\n$RSS(\\beta) = \\sum \\limits _{i=1} ^{N} (y_{i} - \\beta_{0} - \\sum \\limits _{j=1} ^{p} x_{ij}\\beta_{j})^2$\n\nWhere $x_{i} = (x_{i1}, x_{i2}, ..., x_{ip})^T$ is a vector of features for the *i*th data point.\n\nTo simplify the formula for $RSS$, let's introduce a new $N \u00d7 (p+1)$ matrix $X$ with each row as an input vector with $p$ entries of features and one entry (the first entry) of value $1$. Also let's introduce a column vector $\\beta = (\\beta_{0}, \\beta_{1}, ..., \\beta_{p})$ of $p+1$ coefficients,and $y$ as a vector of $N$ outputs. Now the residual sum of squares will take the following form:\n\n$RSS(\\beta) = (y - X\\beta)^{T}(y - X\\beta)$\n\nCool! Now we have well-defined criteria for choosing the best values for coefficients $\\beta$ which minimize the squared sum of the prediction errors (minimize the function RSS). This approach of estimating $\\beta$ is called the Least Squares.\n\nThere is a beautiful method of solving the least-squares using linear algebra, but it requires an explanation of the four fundamental subspaces of a matrix. To keep things simple, I will use calculus to solve for $\\beta$. RSS is a quadratic function of $\\beta$, and to find the minimum we set the derivative to zero:\n\n$\\frac{\\partial RSS(\\beta)}{\\partial \\beta} = -2X^{T}y + 2X^{T}X\\beta = 0 $\n\nSo our estimation for $\\beta$ is $\\hat{\\beta}$ (beta hat) $= (X^{T}X)^{-1}X^{T}y$  And the predicted value of $y$ for $X$ is:\n\n$\\hat{y} = X\\hat{\\beta}$\n\n\n\n\n","07ae577f":"We now what the actual $\\beta$ is because we simulate the data using the function  $y = 4 + 3X + Gaussian noise$. So $\\beta_{0} = 4; \\beta_{1} = 3$\n\nNow let's see what our best estimation $\\hat{\\beta}$ is","7629a383":"## 3. Implementing the solution\n\n**Now let's code our solution using python!**\n\n**First, Let\u2019s generate some linear-looking data**\n","36170fdc":"Close enough! Now you can make predictions using $\\hat{\\beta}$","9185f2d8":"## 1. The problem\n\nThe problem that you and me are going to tackle is **regression**. It's a big part of machine learning and data science. The problem is stated as follow:\n\nWe observed a quantitative response\/output $Y$ (e.g. the price of a house) and $p$ different predictors\/inputs $X=(X_{1}, X_{2}, ..., X_{p})$ (e.g. number of kitchens, construction date, etc) where $X$ is a column vector and we assume a relation exist between $Y$ and $X$. We want to use this relation to predict $Y$ for new observations of $X$. This relation could be written in the following general form:\n\n$Y=f(X)+ \\epsilon$\n\nWhere $\\epsilon$ is an error term and $f$ is a function of the predictors $X$. Our goal is to learn\/estimate $f$ and use it to predict the value of $Y$.\n\nIf we are lucky, the function would be linear. Linear functions are simple, easy to learn, and Interpretable. This helps us understanding how predictors are related to the response. However, the function $f$ is usually none linear (see FIGURE 2.2).","2d310d11":"![](https:\/\/i.ibb.co\/dQVHT1D\/2-2.jpg)\n\nFIGURE 2.2 (An Introduction to Statistical Learning) $^{[1]}$\n\n**Left:** The red dots are the observed values of income (in tens of thousands of dollars) and years of education for 30 individuals. \n\n**Right:** The blue curve represents the true underlying relationship between income and years of education, which is generally unknown (but is known in this case because the data were simulated). The black lines represent the error associated with each observation. Note that some errors are positive (if an observation lies above the blue curve) and some are negative (if an observation lies below the curve). Overall, these errors have approximately mean zero.","514023a0":"# Least Squares Approximations\n\nThis notebook aims to introduce you to a simple but powerful approach to prediction. It will be the base for building more sophisticated approaches.","f8078776":"**Finally, let's plot the estimated linear model predictions.**","c42862cb":"**Now let's compute $\\hat{\\beta}$**","e37ca985":"## References\n\n[1] James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning.\n\n[2] Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction.\n\n[3] G\u00e9ron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems."}}