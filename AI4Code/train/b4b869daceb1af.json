{"cell_type":{"dd60780d":"code","0eaeb1ff":"code","3d5cf969":"code","61a9261c":"code","c8e86b13":"code","0ff344f5":"code","c7616a83":"code","132ef5ca":"code","2667eede":"code","1cf67547":"code","2fe3bffc":"code","fd20062d":"code","f582606a":"code","fbe8487c":"code","08fba00c":"code","c9130c01":"code","ce22831b":"code","0ea0fe1a":"code","a33df63a":"code","b0035ab5":"code","11451a7d":"code","f539acdd":"code","f95c52a3":"code","ef080157":"code","3b5a837b":"code","235143d2":"code","57d00799":"code","f64a94b4":"code","b9238ad2":"code","a311318d":"code","eb346cd4":"code","3b98aa65":"code","0fccb1b4":"code","98b93892":"markdown","88bc2bc7":"markdown","f6a3f07a":"markdown","3b5f3a68":"markdown","4418ac17":"markdown","5f1b82d1":"markdown","e745f4e2":"markdown","5ca54de5":"markdown","3c47d5e8":"markdown","544c6e07":"markdown","ce507cd7":"markdown","639e9195":"markdown","371a822f":"markdown","3eca2bb8":"markdown","467e7079":"markdown","4c29eae2":"markdown","c145f258":"markdown","dde754c6":"markdown","5bf35a57":"markdown","6d6e637b":"markdown","97594980":"markdown","7bccb0f7":"markdown","099f946a":"markdown"},"source":{"dd60780d":"import warnings\nimport seaborn as sns\nsns.set(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", color_codes=True, font=\"sans- serif\", font_scale=1,)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import linear_model\nfrom xgboost import XGBRegressor\nfrom statsmodels.api import OLS\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom colorama import Fore\nimport pandas as pd","0eaeb1ff":"data = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\ndata.head()","3d5cf969":"data.isnull().sum()","61a9261c":"data.columns","c8e86b13":"data.info()","0ff344f5":"cc = data.copy()","c7616a83":"clean_date = []\nfor i,j in enumerate(cc[\"date\"]):\n    a = j[:-7]\n    clean_date.append(a)\ncc[\"clean_date\"] =  clean_date\ncc = cc.drop([\"date\",\"id\"],axis=1)\ncc.head()  ","132ef5ca":"cc[\"clean_date\"] = cc[\"clean_date\"].astype(str).astype(int)","2667eede":"cc.info()","1cf67547":"cc.columns","2fe3bffc":"cc.corr()","fd20062d":"plt.figure(figsize  = (15,6))\nsns.heatmap(cc.isnull(),cmap=\"viridis\")","f582606a":"# Compute the correlation matrix\nimport numpy as np\nmask = np.zeros_like(cc.corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \ncorr = cc.corr()\nplt.figure(figsize=(15, 18))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=0.8, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True,mask=mask,annot_kws={\"size\":8},linecolor='g')\nplt.tight_layout()\nplt.show()","fbe8487c":"feature_1= ['price','sqft_living15','lat','sqft_basement','grade', 'sqft_above','bedrooms', 'bathrooms', 'sqft_living']\n\nsns.pairplot(data=cc[feature_1], palette='tab20')","08fba00c":"def Box(a):\n    plt.figure(figsize=(16,4))\n    sns.boxplot(x=cc[a])\n    plt.title(\"Boxplot\")\n    plt.show()\n\ncol = cc.columns\nfor i,j in enumerate(col):\n    Box(col[i])","c9130c01":"def countplot(a):\n    plt.figure(figsize=(15,6))\n    sns.countplot(data=cc,x=cc[a],order=cc[a].value_counts().index)\n\nfeature_2 = [\"bedrooms\",\"bathrooms\",\"grade\",\"floors\"]\nfor i,j in enumerate(feature_2):\n    countplot(feature_2[i])\n    ","ce22831b":"def dist(a):\n    plt.figure(figsize=(16,6))\n    sns.distplot(cc[a],hist=True,kde=True,rug=False,label=a,norm_hist=True,color=\"r\")\n    plt.show()\n    \ncol = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built',\n       'sqft_living15', 'sqft_lot15']\nfor i,x in  enumerate(col):\n    dist(col[i])","0ea0fe1a":"plt.figure(figsize=(15,6))\nsns.barplot(data=cc,x=\"price\",y=\"sqft_above\")\nplt.title(\"house prices by sqft_above\")\nplt.xlabel('sqft_above')\nplt.ylabel('house prices')\nplt.legend()","a33df63a":"h = cc.hist(bins=20,figsize=(16,16),xlabelsize='10',ylabelsize='10',xrot=-15,color=\"r\")\nsns.despine(left=True, bottom=True)\n[x.title.set_size(12) for x in h.ravel()];\n[x.yaxis.tick_left() for x in h.ravel()];","b0035ab5":"plt.figure(figsize=(16,6))\nfacet = sns.FacetGrid(cc,hue=\"bedrooms\",aspect=4)\nfacet.map(sns.kdeplot,\"price\",shade=True)\nfacet.add_legend()\nplt.title(\"grade\")","11451a7d":"def boxp(a):\n    plt.figure(figsize = (16,5))\n    sns.boxplot(x=cc[a],y=cc[\"price\"])\nfea_3  =[\"bedrooms\",\"grade\",\"bathrooms\",'waterfront',\"view\",]\nfor i, j in enumerate(fea_3):\n    boxp(fea_3[i])","f539acdd":"fig = plt.figure(figsize=(15,6))\nax = fig.add_subplot(1,1,1,projection=\"3d\")\nax.scatter(cc[\"view\"],cc[\"grade\"],cc[\"yr_built\"],color=\"green\",alpha=0.6)\nax.set(xlabel='\\nView',ylabel='\\nGrade',zlabel='\\nYear Built');","f95c52a3":"plt.figure(figsize=(15,5))\na= sns.FacetGrid(data=cc,hue=\"condition\",aspect=0.5,size=15)\na.map(plt.scatter,\"sqft_living\", \"price\")\nplt.legend(loc=\"best\")\nplt.show()","ef080157":"g = sns.factorplot(x=\"yr_built\", y = \"price\", data=cc[cc['price'] < 1000000], size= 7, aspect = 2, kind=\"box\" )\ng.set_xticklabels(rotation=90)\nplt.show()","3b5a837b":"import folium\nfrom folium.plugins import HeatMap\nmaxpr = cc.loc[cc[\"price\"].idxmax()]\n\ndef generateBaseMap(default_location=[47.7210,-122.319],default_zoom_start=10):\n    base_map = folium.Map(location=default_location,zoom_start=default_zoom_start,control_scale=True)\n    return base_map\ndf_copy = cc.copy()\n# select a zipcode for the heatmap\n#set(df['zipcode'])\n#df_copy = df[df['zipcode']==98001].copy()\ndf_copy['count'] = 1\nbasemap = generateBaseMap()\n# add carton position map\nfolium.TileLayer('cartodbpositron').add_to(basemap)\ns=folium.FeatureGroup(name='icon').add_to(basemap)\n# add a marker for the house which has the highest price\nfolium.Marker([maxpr['lat'], maxpr['long']],popup='Highest Price: $'+str(format(maxpr['price'],'.0f')),icon=folium.Icon(color='green')).add_to(s)\n# add heatmap\nHeatMap(data=df_copy[['lat','long','count']].groupby(['lat','long']).sum().reset_index().values.tolist(), radius=8,max_zoom=13,name='Heat Map').add_to(basemap)\nfolium.LayerControl(collapsed=False).add_to(basemap)\nbasemap","235143d2":"## XGBRegressor\nX = cc.drop([\"price\"],axis=1)\nY = cc[\"price\"]\n\nmodel = XGBRegressor()\nmodel.fit(X,Y)\n\nfeature_importance = pd.Series(model.feature_importances_,index=X.columns)\nfeature_importance = pd.DataFrame(feature_importance,columns=[\"Score\"])\nfeature_importance = feature_importance.sort_values([\"Score\"])\n\nplt.figure(figsize=(15,6))\nsns.barplot(y = feature_importance.index, x=feature_importance.Score)\nplt.title(\"Best features plot by using XGBRegressor method\")\nplt.show()\n\n##  DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\nmodel.fit(X,Y)\n\nfeature_importance = pd.Series(model.feature_importances_,index=X.columns)\nfeature_importance = pd.DataFrame(feature_importance,columns=[\"Score\"])\nfeature_importance = feature_importance.sort_values([\"Score\"])\n\nplt.figure(figsize=(15,6))\nsns.barplot(y = feature_importance.index, x=feature_importance.Score)\nplt.title(\"Best features plot by using DecisionTreeRegressor method\")\nplt.show()\n\n## SelectKBest\nk1 = SelectKBest(score_func = f_classif, k='all')\n\nfit1 = k1.fit(X,Y)\nfeature_importance = pd.Series(fit1.scores_,index=X.columns)\nfeature_importance = pd.DataFrame(feature_importance,columns=[\"Score\"])\nfeature_importance = feature_importance.sort_values([\"Score\"])\n\nplt.figure(figsize=(15,6))\nsns.barplot(y = feature_importance.index, x=feature_importance.Score)\nplt.title(\"Best features plot by using SelectKBest method\")\nplt.show()","57d00799":"cc = cc.drop(cc.index[[3914,7252,12777,15870]])","f64a94b4":"def Model_Result(x,y,m):\n    X_1 = x\n    y_1 = y\n    X1_train,X1_test,y1_train,y1_test = train_test_split(X_1,y_1,test_size=0.3,random_state=123)\n    print(\"Shape of X_train : {}\\nShape of X_test : {}\\nShape of y_train : {}\\nShape of y_test : {}\\n\".format(X1_train.shape,X1_test.shape,y1_train.shape,y1_test.shape))\n    \n    Root_mean_squared_Error = []\n    mean_absolute_Error = []\n    mean_squared_Error = []\n    r2 = []\n    Adj_r2 = []\n   \n    ## OLS method\n    print(Fore.WHITE + \"Result with OLS method :\")\n    model_1 = OLS(y1_train,X1_train).fit()\n    print(model_1.summary())\n    pred_1 = model_1.predict(X1_test)\n    \n    Root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y1_test,pred_1))\n    Root_mean_squared_Error.append(Root_mean_squared_error) \n                                      \n    mean_absolute_error = metrics.mean_absolute_error(y1_test, pred_1) \n    mean_absolute_Error.append(mean_absolute_error)    \n                                      \n    mean_squared_error = metrics.mean_squared_error(y1_test, pred_1) \n    mean_squared_Error.append(mean_squared_error)   \n                                      \n    R2 = r2_score(y1_test, pred_1)  \n    r2.append(R2)        \n                                      \n    Adj_R2 = 1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1) \n    Adj_r2.append(Adj_R2)\n                                      \n    print(\"Report of Testing data : \\n\")\n    print('Root Mean Squared Error:', float(format(np.sqrt(metrics.mean_squared_error(y1_test,pred_1)),'.3f')))\n    print('Mean Absolute Error:', float(format(metrics.mean_absolute_error(y1_test, pred_1),\".3f\")))\n    print('Mean Squared Error:', float(format(metrics.mean_squared_error(y1_test, pred_1),\".3f\")))\n    print(\"R2 :\",float(format(r2_score(y1_test, pred_1),'.3f')))\n    print(\"Adjusted R2 :\",float(format(1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1),\".3f\")))\n    print(\"\\n\")\n    \n    ## Sklearn Linear Regression method\n    print(Fore.RED + \"Result with Sklearn Linear Regression method :\")\n    model_2 = LinearRegression()\n    model_2.fit(X1_train,y1_train)\n    pred_2 = model_2.predict(X1_test)\n    \n    Root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y1_test,pred_2))\n    Root_mean_squared_Error.append(Root_mean_squared_error) \n                                      \n    mean_absolute_error = metrics.mean_absolute_error(y1_test, pred_2) \n    mean_absolute_Error.append(mean_absolute_error)    \n                                      \n    mean_squared_error = metrics.mean_squared_error(y1_test, pred_2) \n    mean_squared_Error.append(mean_squared_error)   \n                                      \n    R2 = r2_score(y1_test, pred_2)  \n    r2.append(R2)        \n                                      \n    Adj_R2 = 1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1) \n    Adj_r2.append(Adj_R2)\n    \n    print(\"Report of Testing data : \\n\")\n    print('Root Mean Squared Error:', float(format(np.sqrt(metrics.mean_squared_error(y1_test,pred_2)),'.3f')))\n    print('Mean Absolute Error:', float(format(metrics.mean_absolute_error(y1_test, pred_2),\".3f\")))\n    print('Mean Squared Error:', float(format(metrics.mean_squared_error(y1_test, pred_2),\".3f\")))\n    print(\"R2 :\",float(format(r2_score(y1_test, pred_2),'.3f')))\n    print(\"Adjusted R2 :\",float(format(1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1),\".3f\")))\n    print(\"\\n\")\n    \n    ## Ridge regression technique (Regularization)\n    print(Fore.BLUE + \"Result with Ridge regression technique :\")\n    model_3 = linear_model.Ridge(alpha=100)\n    model_3.fit(X1_train,y1_train)\n    pred_3 = model_3.predict(X1_test)\n    \n    Root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y1_test,pred_3))\n    Root_mean_squared_Error.append(Root_mean_squared_error) \n                                      \n    mean_absolute_error = metrics.mean_absolute_error(y1_test, pred_3) \n    mean_absolute_Error.append(mean_absolute_error)    \n                                      \n    mean_squared_error = metrics.mean_squared_error(y1_test, pred_3) \n    mean_squared_Error.append(mean_squared_error)   \n                                      \n    R2 = r2_score(y1_test, pred_3)  \n    r2.append(R2)        \n                                      \n    Adj_R2 = 1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1) \n    Adj_r2.append(Adj_R2)\n    \n    print(\"Report of Testing data : \\n\")\n    print('Root Mean Squared Error:', float(format(np.sqrt(metrics.mean_squared_error(y1_test,pred_3)),'.3f')))\n    print('Mean Absolute Error:', float(format(metrics.mean_absolute_error(y1_test, pred_3),\".3f\")))\n    print('Mean Squared Error:', float(format(metrics.mean_squared_error(y1_test, pred_3),\".3f\")))\n    print(\"R2 :\",float(format(r2_score(y1_test, pred_3),'.3f')))\n    print(\"Adjusted R2 :\",float(format(1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1),\".3f\")))\n    print(\"\\n\")\n    \n    ## Polynomial Regression(degree=2)\n    print(Fore.YELLOW + \"Result with Polynomial Regression (degree=2):\")\n    polyfeat = PolynomialFeatures(degree=2)\n    X_trainpoly = polyfeat.fit_transform(X1_train)\n    X_testpoly = polyfeat.fit_transform(X1_test)\n    model_4 = linear_model.LinearRegression().fit(X_trainpoly, y1_train)\n    pred_4 = model_4.predict(X_testpoly)\n    \n    Root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y1_test,pred_4))\n    Root_mean_squared_Error.append(Root_mean_squared_error) \n                                      \n    mean_absolute_error = metrics.mean_absolute_error(y1_test, pred_4) \n    mean_absolute_Error.append(mean_absolute_error)    \n                                      \n    mean_squared_error = metrics.mean_squared_error(y1_test, pred_4) \n    mean_squared_Error.append(mean_squared_error)   \n                                      \n    R2 = r2_score(y1_test, pred_4)  \n    r2.append(R2)        \n                                      \n    Adj_R2 = 1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1) \n    Adj_r2.append(Adj_R2)\n    \n    print(\"Report of Testing data : \\n\")\n    print('Root Mean Squared Error:', float(format(np.sqrt(metrics.mean_squared_error(y1_test,pred_4)),'.3f')))\n    print('Mean Absolute Error:', float(format(metrics.mean_absolute_error(y1_test, pred_4),\".3f\")))\n    print('Mean Squared Error:', float(format(metrics.mean_squared_error(y1_test, pred_4),\".3f\")))\n    print(\"R2 :\",float(format(r2_score(y1_test, pred_4),'.3f')))\n    print(\"Adjusted R2 :\",float(format(1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1),\".3f\")))\n    print(\"\\n\")\n    \n    ## Polynomial Regression(degree=3)\n    print(Fore.MAGENTA + \"Result with Polynomial Regression (degree=3):\")\n    polyfeat3 = PolynomialFeatures(degree=3)\n    X_trainpoly = polyfeat3.fit_transform(X1_train)\n    X_testpoly = polyfeat3.fit_transform(X1_test)\n    model_5 = linear_model.LinearRegression().fit(X_trainpoly, y1_train)\n    pred_5 = model_5.predict(X_testpoly)\n    \n    Root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y1_test,pred_5))\n    Root_mean_squared_Error.append(Root_mean_squared_error) \n                                      \n    mean_absolute_error = metrics.mean_absolute_error(y1_test, pred_5) \n    mean_absolute_Error.append(mean_absolute_error)    \n                                      \n    mean_squared_error = metrics.mean_squared_error(y1_test, pred_5) \n    mean_squared_Error.append(mean_squared_error)   \n                                      \n    R2 = r2_score(y1_test, pred_5)  \n    r2.append(R2)        \n                                      \n    Adj_R2 = 1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1) \n    Adj_r2.append(Adj_R2)\n    \n    print(\"Report of Testing data : \\n\")\n    print('Root Mean Squared Error:', float(format(np.sqrt(metrics.mean_squared_error(y1_test,pred_5)),'.3f')))\n    print('Mean Absolute Error:', float(format(metrics.mean_absolute_error(y1_test, pred_5),\".3f\")))\n    print('Mean Squared Error:', float(format(metrics.mean_squared_error(y1_test, pred_5),\".3f\")))\n    print(\"R2 :\",float(format(r2_score(y1_test, pred_5),'.3f')))\n    print(\"Adjusted R2 :\",float(format(1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1),\".3f\")))\n    print(\"\\n\")\n    \n    ## Decision Tree Regressor \n    print(Fore.CYAN + \"Result with Decision Tree Regressor:\")\n    model_6 = DecisionTreeRegressor()\n    model_6.fit(X1_train,y1_train)\n    pred_6 = model_6.predict(X1_test)\n    \n    Root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y1_test,pred_6))\n    Root_mean_squared_Error.append(Root_mean_squared_error) \n                                      \n    mean_absolute_error = metrics.mean_absolute_error(y1_test, pred_6) \n    mean_absolute_Error.append(mean_absolute_error)    \n                                      \n    mean_squared_error = metrics.mean_squared_error(y1_test, pred_6) \n    mean_squared_Error.append(mean_squared_error)   \n                                      \n    R2 = r2_score(y1_test, pred_6)  \n    r2.append(R2)        \n                                      \n    Adj_R2 = 1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1) \n    Adj_r2.append(Adj_R2)\n    \n    print(\"Report of Testing data : \\n\")\n    print('Root Mean Squared Error:', float(format(np.sqrt(metrics.mean_squared_error(y1_test,pred_6)),'.3f')))\n    print('Mean Absolute Error:', float(format(metrics.mean_absolute_error(y1_test, pred_6),\".3f\")))\n    print('Mean Squared Error:', float(format(metrics.mean_squared_error(y1_test, pred_6),\".3f\")))\n    print(\"R2 :\",float(format(r2_score(y1_test, pred_6),'.3f')))\n    print(\"Adjusted R2 :\",float(format(1-(1-R2)*(len(y1_test)-1)\/(len(y1_test)-m-1),\".3f\")))\n    print(\"\\n\")\n    \n    \n    ind = [\"OLS\",\"Sklearn_linear\",\"Ridge\",\"Poly_2\",\"Poly_3\",\"Decision_tree_regressor\"]\n    Result = pd.DataFrame()\n    Result[\"Root_mean_squared_error\"] = Root_mean_squared_Error  \n    Result[\"mean_absolute_error\"] = mean_absolute_Error\n    Result[\"mean_squared_error\"] = mean_squared_Error\n    Result[\"R2\"] = r2\n    Result[\"Adj_R2\"] =  Adj_r2   \n    Result.index = ind\n    \n    return Result","b9238ad2":"Model_Result(cc[['bedrooms', 'bathrooms', 'sqft_living','grade', 'sqft_above','sqft_living15']],cc[\"price\"],6)","a311318d":"Model_Result(cc[['price', 'bedrooms', 'bathrooms',  'grade', 'sqft_above','sqft_basement','sqft_living15', 'sqft_lot15']],cc[\"price\"],8)","eb346cd4":"Model_Result(cc[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15', 'clean_date']],cc[\"price\"],19)","3b98aa65":"##Normalization function\ndef norm_fun(i):\n    x = (i-i.min())\/(i.max()-i.min())\n    return x\nfea_n = cc[['bedrooms', 'bathrooms', 'sqft_living','grade', 'sqft_above','sqft_living15']]\nNorm_feat = norm_fun(fea_n)\nNorm_feat.head()","0fccb1b4":"Model_Result(Norm_feat,cc[\"price\"],6)","98b93892":"# <a id=\"res1\">8. Results with 6 Highly Corelated Features<\/a>","88bc2bc7":"# <a id=\"drop\">6. Dropping Influencers\u261d<\/a>","f6a3f07a":"# <a id=\"model\">7. Model Function\u2708\ud83d\ude80<\/a>","3b5f3a68":"# <a id=\"norm\">11. Results with Normalized features<\/a>","4418ac17":" <button type=\"button\" class=\"btn btn-success\"><a href=\"https:\/\/www.youtube.com\/watch?v=EqLBAmtKMnQ\"><p style=\"color:black\"><b>1st Reference<\/b><\/p><\/a> <span class=\"badge\"><\/span><\/button>\n<button type=\"button\" class=\"btn btn-success\"><a href=\"https:\/\/machinelearningmastery.com\/calculate-feature-importance-with-python\/\"><p style=\"color:yellow\"><b>2nd Reference<\/b><\/p><\/a> <span class=\"badge\"><\/span><\/button>","5f1b82d1":"# <a id=\"fimp\">5. Feature Importance\ud83d\udc8e<\/a>","e745f4e2":"# <a id=\"res2\">9. Results with 8 Features<\/a>","5ca54de5":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"style=\"background-color:#5EAFF2; color:#FFE200\">Table of Contents<\/h3>\n<ul class=\"nav flex-column\">\n  <li class=\"nav-item\">    \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#lib\" role=\"tab\" aria-controls=\"profile\"style=\"color:#ff4f00\">Libraries<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>         <li class=\"nav-item\">\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#data\" role=\"tab\" aria-controls=\"profile\">Data<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>  \n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#eda\" role=\"tab\" aria-controls=\"profile\"style=\"color:#ff4f00\">EDA<span class=\"badge badge-primary badge-pill\">3<\/span><\/a> \n  <li class=\"nav-item\">\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#viz\" role=\"tab\" aria-controls=\"profile\">Visualizations<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#map\" role=\"tab\" aria-controls=\"profile\">MAP<span class=\"badge badge-primary badge-pill\">4.1<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fimp\" role=\"tab\" aria-controls=\"profile\"style=\"color:#ff4f00\">Feature Importance<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#drop\" role=\"tab\" aria-controls=\"profile\">Dropping Influencers<span class=\"badge badge-primary badge-pill\">6<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#model\" role=\"tab\" aria-controls=\"profile\"style=\"color:#ff4f00\">Model Function<span class=\"badge badge-primary badge-pill\">7<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#res1\" role=\"tab\" aria-controls=\"profile\">Results With 6 Highly Corelated features<span class=\"badge badge-primary badge-pill\">8<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#res2\" role=\"tab\" aria-controls=\"profile\"style=\"color:#ff4f00\">Results With 8 features<span class=\"badge badge-primary badge-pill\">9<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#res3\" role=\"tab\" aria-controls=\"profile\">Results With all features<span class=\"badge badge-primary badge-pill\">10<\/span><\/a>\n  <li class=\"nav-item\">  \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#norm\" role=\"tab\" aria-controls=\"profile\"style=\"color:#ff4f00\">Results With normalized features<span class=\"badge badge-primary badge-pill\">11<\/span><\/a>\n  <li class=\"nav-item\">\n<\/ul>","3c47d5e8":"Creating a copy of Original dataset","544c6e07":"# <a id=\"map\">4.1. Map\ud83d\uddfa\ud83c\udf0d<\/a>","ce507cd7":"*Some visualizations of my notebook are inspired by the below notebook.*\n\n<button type=\"button\" class=\"btn btn-success\"><a href=\"https:\/\/www.kaggle.com\/burhanykiyakoglu\/predicting-house-prices\"><p style=\"color:yellow\"><b>Reference<\/b><\/p><\/a> <span class=\"badge\"><\/span><\/button>","639e9195":"# <a id = \"viz\">4. Visualizations\ud83d\udc53\ud83d\udcc8\ud83d\udcc9\ud83d\udcca<\/a>","371a822f":"# <a id=\"eda\">3. Basic EDA\ud83e\ude94<\/a>","3eca2bb8":"# \ud83e\udd47\ud83e\udd48\ud83e\udd49\ud83c\udfc5\ud83c\udfc6","467e7079":"# <a id=\"lib\">1. Libraries\ud83d\udcd6<\/a>","4c29eae2":"# <a id=\"res3\">10. Results with all Features<\/a>","c145f258":"![image.png](attachment:image.png)","dde754c6":"# Please Do give an Upvote If you liked the Notebook.","5bf35a57":"# <a id=\"data\">2. Data\ud83d\udcbc<\/a>","6d6e637b":"* After analysing all the above results we can say that Ploynomial with 2 degree is giving the best result.","97594980":"Here I am creating a function which will give you the result of 6 models.\n\n**Models are : OLS, Sklearn Linear, Ridge, Ploynimial(Degree = 2), Ploynimial(Degree = 3),Decision Tree Regressor**","7bccb0f7":"<img src=\"https:\/\/media.giphy.com\/media\/JDTsqJhvLOq9G\/giphy.gif\">","099f946a":"#                      House Price Predictions"}}