{"cell_type":{"1a1d554c":"code","35bfe25a":"code","f6b227aa":"code","b9b1b757":"code","b6c1b446":"code","15ba791c":"code","cc742e97":"code","106929ec":"markdown","fdbfe65b":"markdown","dfb834a7":"markdown","65a82fa5":"markdown","7f35669e":"markdown","c121837d":"markdown","b4cbecfb":"markdown","6af6db05":"markdown"},"source":{"1a1d554c":"!pip install -U scikit-learn","35bfe25a":"import pandas as pd\n\ncustomers_data = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\norder_items_data = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\nproducts_data = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\ntranslation_data = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/product_category_name_translation.csv')\norders_data = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\n\n# Replacing PT categories with EN\ncategory_translations = dict(zip(translation_data['product_category_name'],translation_data['product_category_name_english']))\nproducts_data = products_data.replace({'product_category_name':category_translations})\n\n# Joining datasets to get only data required for segmentation\nsegment_data = pd.merge(customers_data.drop(['customer_zip_code_prefix','customer_city'],axis=1),orders_data[['order_id','customer_id']],on='customer_id')\nsegment_data = pd.merge(segment_data,order_items_data[['order_id','product_id','price']],on='order_id')\nsegment_data = pd.merge(segment_data,products_data[['product_id','product_category_name']])\nsegment_data = segment_data.dropna()\n\n# No. of products ordered per customer_unique_id\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nproducts_by_unique = segment_data['customer_unique_id'].value_counts()\nproducts_by_unique = products_by_unique.value_counts().sort_index().to_frame().reset_index()\nproducts_by_unique = products_by_unique.rename(columns={'index':'Total # of products purchased','customer_unique_id':'# of unique customers'})\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n\nplt.suptitle(\"Number of Products Purchased per Unique Customer (Linear and Log Scale)\", fontsize=14)\n\nsns.barplot(data=products_by_unique, x='Total # of products purchased', y='# of unique customers',ax=ax[0], palette='copper')\nsns.barplot(data=products_by_unique, x='Total # of products purchased', y='# of unique customers',ax=ax[1], palette='copper')\nax[1].set_yscale('log')","f6b227aa":"# From the perspective of a unique customer, inc. no of orders, amount spent, total products purchased by product category.\nsegment_data_by_unique = segment_data[['customer_unique_id','order_id','price','product_category_name']]\n\n# One hot encode all categories purchased by unique id\none_hot = pd.get_dummies(segment_data_by_unique['product_category_name'])\nsegment_data_by_unique = segment_data_by_unique.drop('product_category_name',axis=1)\nsegment_data_by_unique = segment_data_by_unique.join(one_hot)\n\n# Replace order_id with 1 (so groupby can sum total orders)\nsegment_data_by_unique['order_id'] = 1\n\n# Groupby and sum\nsegment_data_by_unique = segment_data_by_unique.groupby(['customer_unique_id']).sum()\n\n# Remove order_id and price\nsegment_data_by_unique = segment_data_by_unique.drop(['order_id','price'],axis=1)\n\n# Keeping only customers that purchased more than 2 categories\nunique_single = segment_data_by_unique\nunique_single['Total'] = segment_data_by_unique.sum(1)\nunique_single['Single Category only'] = unique_single.drop('Total',1).isin(unique_single['Total']).any(1)\nunique_single = unique_single.loc[unique_single['Single Category only'] == False].drop(['Single Category only','Total'],1)\n\n# Remove all columns for which sum() = 0\nunique_single = unique_single.loc[:, (unique_single.sum(axis=0) != 0)]\n\n# Used to work out number of users purchasing multiple items\nlen(segment_data_by_unique.loc[segment_data_by_unique.sum(1).eq(5)])\n\n# Used to work out number of users purchasing single category\nunique_single.sum(1).value_counts()\n\nsingle_category_users = pd.DataFrame({'Total Orders': [2,3,4,5],\n                                      'Total customers': [8854,1638,623,251],\n                                     '# ordering just 1 category': [8854 - 1577, 1638 - 390, 623 - 136, 251 - 58]})\n\n# Show users sticking to one category\nsns.set(rc={'figure.figsize':(10,7)})\ntidy = single_category_users.melt(id_vars='Total Orders').rename(columns=str.title)\nax = sns.barplot(x='Total Orders', y='Value', hue='Variable', data=tidy)\nax.set(ylabel='# of Customers')\nplt.show()","b9b1b757":"# Import libraries for different preprocessing, dimensionality reduction and clustering techniques\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.decomposition import PCA,TruncatedSVD, NMF\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom kmodes.kmodes import KModes\n\nn_clusters = 8\n\n# Define techniques\nstandard = StandardScaler()\nnormalizer = Normalizer()\npca = PCA()\nsvd = TruncatedSVD(n_components=20)\nnmf = NMF()\nagglomerative = AgglomerativeClustering(n_clusters=n_clusters, compute_distances=True, linkage='ward')\nkmeans = KMeans(n_clusters=n_clusters, n_init=50, max_iter=300)\nkmodes = KModes(n_clusters=n_clusters, n_init=4, init='Huang')\n\n# Fit whichever produces cleanest clusters\npipe = Pipeline([('scaling',normalizer),('reduce_dim',svd),('clustering',agglomerative)])\nclusters = pipe.fit_predict(unique_single)\n\nprint('Clustering Successful!')","b6c1b446":"# Visualization using top 3 (of 20) components to show clustering\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\n\noutput = pipe.named_steps['reduce_dim'].fit_transform(unique_single)\n\ndf = pd.DataFrame(output)\ndf = df[[0,1,2]]\ndf['Clusters'] = clusters\ndf['Clusters'].value_counts()\n\n# axes instance\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig)\n\n# get colormap from seaborn\ncmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n\n# plot\nsc = ax.scatter(df[0], df[1], df[2], s=40, c=df['Clusters'], marker='o', cmap=cmap, alpha=1)\nax.set_xlabel('1st Component')\nax.set_ylabel('2nd Component')\nax.set_zlabel('3rd Component')\n\n# legend\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n\n# save\nplt.savefig(\"scatter_hue\", bbox_inches='tight')","15ba791c":"# Get 8 top categories for each cluster \nunique_single['clusters'] = df['Clusters'].values\nunique_single.reset_index(drop=True,inplace=True)\n\ncluster_cats = unique_single.groupby('clusters').sum().T\n\nhighest_categories = pd.DataFrame()\ncat_numbers = pd.DataFrame()\n\nfor i in cluster_cats.columns:\n    highest_categories[i] = list(cluster_cats[i].nlargest(5).index)\n\nfor i in cluster_cats.columns:\n    cat_numbers[i] = list(cluster_cats[i].nlargest(5))\n    \n# Generate 8 separate dfs, each with top 8 categories and number of product orders for each\nd = {}\n\nfor i in highest_categories.columns:\n    cats = pd.DataFrame(highest_categories[i])\n    cats = cats.rename(columns={i:'Categories'})\n    cats['Total'] = cat_numbers[i].values\n    d.update({i:cats})\n\n# Barplots showing the number of product categories purchased within each cluster\n\nrows = round(len(np.unique(clusters)) \/ 2)\n\nf, ax = plt.subplots(nrows=rows, ncols=2, figsize=(20, 18))\n\nfor i in list(d.keys()):\n    if i < rows:\n        sns.barplot(data=d[i], x='Categories', y='Total',ax=ax[i,0], palette='copper')\n        ax[i,0].set_title('Cluster {} (n={})'.format(i,df['Clusters'].value_counts().at[i]))\n        ax[i,0].set_ylabel('')\n        ax[i,0].set_xlabel('')\n        ax[i,0].set_xticklabels(ax[i,0].get_xticklabels(), \n                          rotation=30,\n                          fontsize=8,\n                          horizontalalignment='right')\n    else:\n        sns.barplot(data=d[i], x='Categories', y='Total',ax=ax[i-rows,1], palette='copper')\n        ax[i-rows,1].set_title('Cluster {} (n={})'.format(i,df['Clusters'].value_counts().at[i]))\n        ax[i-rows,1].set_ylabel('')\n        ax[i-rows,1].set_xlabel('')\n        ax[i-rows,1].set_xticklabels(ax[i-rows,1].get_xticklabels(), \n                          rotation=30,\n                          fontsize=8,  \n                          horizontalalignment='right')\n\nplt.suptitle(\"Top 5 Purchased Categories by Cluster\", fontsize=20, y=1.02)\nplt.tight_layout()","cc742e97":"# Implement Scipy dendrogram using AgglomerativeClustering child attribute\nfrom scipy.cluster.hierarchy import dendrogram\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(agglomerative.children_.shape[0])\n    n_samples = len(agglomerative.labels_)\n    for i, merge in enumerate(agglomerative.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([agglomerative.children_, agglomerative.distances_,\n                                      counts]).astype(float)\n    \n    # Plot the corresponding dendrogram\n    plt.figure(figsize=(10,7))\n    ddata = dendrogram(linkage_matrix, **kwargs)\n    plt.show()\n\nplot_dendrogram(agglomerative, truncate_mode='lastp', p=n_clusters)","106929ec":"## Feasibility of Clustering on Purchase Data\n\nThere are 94,108 unique customers, purchasing a total of 111,047 products. 87.6% (82,433 people) of those unique customers only purchased one product within the period of 2016 - 2018.","fdbfe65b":"# Dimensionality Reduction & Clustering\n\n\nA typical process for clustering involves reducing the number of features through dimensionality reduction (i.e. Principal Component Analysis (PCA), or Singular value decomposition(SVD)), then applying a clustering algorithm (i.e. KMeans, t-SNE, Agglomerative Clustering).\n\nGiven the exploratory nature of clustering, it makes sense to try a few combinations of different dimensionality reduction and clustering techniques to get an intuition of the output, then choosing the combination that provides the most representative\/cleanest clusters.\n\nI eventually settled on a combination of SVD and Agglomerative Clustering.","dfb834a7":"### Visualizing Clusters\n\nThe following is a 3D representation of the distance between customers and their purchases in the latent space, coloured according to the cluster to which they were allocated. \n\nImplementing SVD reduced the dimensionality from 70 features to 20 (still not visible in a chart), so the representation shows the 3 components that capture the most variance in the data. ","65a82fa5":"Of the remaining customers that purchased more than one product, the vast majority still stick to one category. \nWhile this limits the amount of data that can be used to cluster, you can hypothesize that:\n\n1) Customers are hesitant to order from a new category<br\/>\n2) Once they have ventured into a new category, they are more likely to repeat purchase in that category\n\nIf this is true, helping customers shift to a new category could generate significant additional orders.<br\/>\n**Note:** Testing this hypothesis could be an interesting additional project.","7f35669e":"## Results\n\nAfter playing around with several hyperparameters (i.e. # of clusters, cluster sizes etc.), connections between related product categories consistently emerged.\n\nWhile the the final iteration yielded 8 separate clusters, some category connections were spread across several different clusters.\nWe can consolidate these into the following 4 \"personas\":\n\n- Bed, Bath & Table, Furniture & Decor, Garden Tools and Housewares\n- Toys, Baby, Cool Stuff, Watches & Gifts (maybe + Telephony, although this didn't always cluster together neatly)\n- Health & Beauty, Sports & Leisure (maybe + Perfumery)\n- Computers & Accessories\n\nFor the final cluster \"Computers & Accessories\", several clustering iterations didn't find a strong connection between this product category and others.\n\nWhen iterating through different values for clustering hyperparameters, setting a high # of clusters yielded a mix of weak category connections across several clusters (rendering results difficult to interpret) and some interesting connections among less popular categories. The following connections were consistently observed, adding 2 more \"personas\" :\n\n- Telephony, Auto\n- Perfumery, Stationery","c121837d":"# Creating Personas from Purchase Data\n\n## Motivation\n\nAs a marketer, the process of understanding your target audience often starts with defining a set of \"personas\" - a proxy for subsets of your audience that have similar interests, priorities and problems to be solved.\n\nTraditionally, defining personas has been an analytical process through a mixture of focus groups, surveys and workshops - often without stopping to look at the clues embedded within customer data that already exists.\n\nIn this exercise, we will attempt to derive personas empirically from purchase data using a \"clustering\" algorithm.\n\n## Datasets\n\nThis project makes use of the \"[Brazilian E-Commerce Public Dataset by Olist](http:\/\/https:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce)\", which contains information of over 100,000 orders from 2016 to 2018. The datasets are structured as follows:\n\n<img src=\"http:\/\/i.imgur.com\/HRhd2Y0.png\" width=\"600\">\n\nNot all of this data is required, so we can pick and choose only the columns from each table that are required and join them accordingly.\n\n![Webp.net-resizeimage.png](attachment:Webp.net-resizeimage.png)","b4cbecfb":"The additional clusters of Telephony, Auto and Perfumery, Stationery.\n\n![Screenshot%202021-03-04%20at%209.38.21.png](attachment:Screenshot%202021-03-04%20at%209.38.21.png)\n![Screenshot%202021-03-04%20at%209.38.07.png](attachment:Screenshot%202021-03-04%20at%209.38.07.png)","6af6db05":"## Visualizing the Cluster Hierarchy\n\n**Note:** Not that insightful given the sparsity of the data, but good practice nonetheless.\n\nSince the clustering method ultimately used was a hierarchical clustering method, it is possible to visualize the connections between each cluster using the dendrogram feature in the Scipy library.\n\nThe number of observations (n) in the 8 cluster breakdown can be matched with the numbers for each leaf in the dendrogram."}}