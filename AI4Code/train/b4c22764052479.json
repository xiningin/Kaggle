{"cell_type":{"fe166f5d":"code","8d22dba6":"code","a62b9b3a":"code","be2254d6":"code","c1837e6d":"code","4f804d7f":"code","52ec4f8f":"code","6fd50944":"code","854e394a":"code","48b6cf00":"code","36cf647b":"code","fbab289b":"code","c52473a6":"code","68155a48":"code","403eb17c":"code","15a78d05":"code","9b05b8a4":"code","84b9328d":"code","9c104886":"code","a68758ef":"code","770d790b":"code","9ba9cc47":"code","7d838c21":"code","af2f62d1":"code","0cd72b7d":"code","732098a0":"markdown","0fa50271":"markdown","31dece0a":"markdown","d773a6d6":"markdown","e4de4665":"markdown","bdd1d8ee":"markdown","0185cf6a":"markdown","44e39d9c":"markdown","e0dc372d":"markdown","38158084":"markdown","95f47aac":"markdown"},"source":{"fe166f5d":"#!git clone https:\/\/github.com\/ayulockin\/SwAV-TF.git","8d22dba6":"# https:\/\/github.com\/ayulockin\/SwAV-TF\/tree\/master\/utils\n# multicrop_dataset.py\nimport tensorflow as tf\nimport random\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Reference: https:\/\/github.com\/google-research\/simclr\/blob\/master\/data_util.py\n\n@tf.function\ndef gaussian_blur(image, kernel_size=23, padding='SAME'):\n    sigma = tf.random.uniform((1,))* 1.9 + 0.1\n\n    radius = tf.cast(kernel_size \/ 2, tf.int32)\n    kernel_size = radius * 2 + 1\n    x = tf.cast(tf.range(-radius, radius + 1), tf.float32)\n    blur_filter = tf.exp(\n        -tf.pow(x, 2.0) \/ (2.0 * tf.pow(tf.cast(sigma, tf.float32), 2.0)))\n    blur_filter \/= tf.reduce_sum(blur_filter)\n    # One vertical and one horizontal filter.\n    blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n    blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n    num_channels = tf.shape(image)[-1]\n    blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n    blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n    expand_batch_dim = image.shape.ndims == 3\n    if expand_batch_dim:\n        image = tf.expand_dims(image, axis=0)\n    blurred = tf.nn.depthwise_conv2d(\n        image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n    blurred = tf.nn.depthwise_conv2d(\n        blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n    if expand_batch_dim:\n        blurred = tf.squeeze(blurred, axis=0)\n    return blurred\n\n@tf.function\ndef color_jitter(x, s=0.5):\n    x = tf.image.random_brightness(x, max_delta=0.8*s)\n    x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n    x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n    x = tf.image.random_hue(x, max_delta=0.2*s)\n    x = tf.clip_by_value(x, 0, 1)\n    return x\n\n@tf.function\ndef color_drop(x):\n    x = tf.image.rgb_to_grayscale(x)\n    x = tf.tile(x, [1, 1, 3])\n    return x\n\n@tf.function\ndef custom_augment(image):\n    # Random flips\n    image = random_apply(tf.image.flip_left_right, image, p=0.5)\n    # Randomly apply gausian blur\n    image = random_apply(gaussian_blur, image, p=0.5)\n    # Randomly apply transformation (color distortions) with probability p.\n    image = random_apply(color_jitter, image, p=0.8)\n    # Randomly apply grayscale\n    image = random_apply(color_drop, image, p=0.2)\n\n    return image\n\n@tf.function\ndef random_resize_crop(image, min_scale, max_scale, crop_size):\n    # Conditional resizing\n    if crop_size == 224:\n        image_shape = 260\n        image = tf.image.resize(image, (image_shape, image_shape))\n    else:\n        image_shape = 160\n        image = tf.image.resize(image, (image_shape, image_shape))\n    # Get the crop size for given min and max scale\n    size = tf.random.uniform(shape=(1,), minval=min_scale*image_shape,\n        maxval=max_scale*image_shape, dtype=tf.float32)\n    size = tf.cast(size, tf.int32)[0]\n    # Get the crop from the image\n    crop = tf.image.random_crop(image, (size,size,3))\n    crop_resize = tf.image.resize(crop, (crop_size, crop_size))\n\n    return crop_resize\n\n@tf.function\ndef random_apply(func, x, p):\n    return tf.cond(\n        tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n                tf.cast(p, tf.float32)),\n        lambda: func(x),\n        lambda: x)\n\n@tf.function\ndef scale_image(image):\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    return image\n\n@tf.function\ndef tie_together(image, min_scale, max_scale, crop_size):\n    # Retrieve the image features\n    image = image['image']\n    # Scale the pixel values\n    image = scale_image(image)\n    # Random resized crops\n    image = random_resize_crop(image, min_scale,\n        max_scale, crop_size)\n    # Color distortions & Gaussian blur\n    image = custom_augment(image)\n\n    return image\n\ndef get_multires_dataset(dataset,\n    size_crops,\n    num_crops,\n    min_scale,\n    max_scale,\n    options=None):\n    loaders = tuple()\n    for i, num_crop in enumerate(num_crops):\n        for _ in range(num_crop):\n            loader = (\n                    dataset\n                    .shuffle(1024)\n                    .map(lambda x: tie_together(x, min_scale[i],\n                        max_scale[i], size_crops[i]), num_parallel_calls=AUTO)\n                )\n            if options!=None:\n                loader = loader.with_options(options)\n            loaders += (loader, )\n\n    return loaders\n\ndef shuffle_zipped_output(a,b,c,d,e):\n    listify = [a,b,c,d,e]\n    random.shuffle(listify)\n\n    return listify[0], listify[1], listify[2], \\\n        listify[3], listify[4]","a62b9b3a":"# https:\/\/github.com\/ayulockin\/SwAV-TF\/tree\/master\/utils\n# architecture.py\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n\ndef get_resnet_backbone():\n    base_model = tf.keras.applications.ResNet50(\n        include_top=False, weights=None, input_shape=(None, None, 3)\n    )\n    base_model.trainabe = True\n\n    inputs = layers.Input((None, None, 3))\n    h = base_model(inputs, training=True)\n    h = layers.GlobalAveragePooling2D()(h)\n    backbone = tf.keras.models.Model(inputs, h)\n\n    return backbone\n\ndef get_projection_prototype(dense_1=1024, dense_2=96, prototype_dimension=10):\n    inputs = layers.Input((2048, ))\n    projection_1 = layers.Dense(dense_1)(inputs)\n    projection_1 = layers.BatchNormalization()(projection_1)\n    projection_1 = layers.Activation(\"relu\")(projection_1)\n\n    projection_2 = layers.Dense(dense_2)(projection_1)\n    projection_2_normalize = tf.math.l2_normalize(projection_2, axis=1, name='projection')\n\n    prototype = layers.Dense(prototype_dimension, use_bias=False, name='prototype')(projection_2_normalize)\n\n    return tf.keras.models.Model(inputs=inputs,\n        outputs=[projection_2_normalize, prototype])","be2254d6":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport time\nimport os\n\nfrom itertools import groupby\nfrom tqdm import tqdm\n\ntf.random.set_seed(666)\nnp.random.seed(666)\n\ntfds.disable_progress_bar()\n\nprint(\"Tensorflow version \" + tf.__version__)","c1837e6d":"#epochs = 40\nepochs = 25  # 9\u6642\u9593\u4ee5\u5185\u306b\u53ce\u3081\u308b\u305f\u3081\u30021epoch~20\u5206\n\nBATCH_SIZE = 32\n\nn_classes = 5  # \u30ad\u30e3\u30c3\u30b5\u30d0\u30c7\u30fc\u30bf\u306f5\u30af\u30e9\u30b9\nn_prototype = n_classes * (3000 \/ 1000)  # \u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u306e\u6570.\u8ad6\u6587\u306e\u6bd4\u7387\u3068\u5408\u308f\u305b\u308b\n\n# \u30a8\u30dd\u30c3\u30af\u6570\u6e1b\u3089\u3057\u3066\u5b9f\u884c\u30c6\u30b9\u30c8\n#DEBUG = True\nDEBUG = False\nif DEBUG:\n    epochs = 2\n    print(\"DEBUG\")","4f804d7f":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    return image","52ec4f8f":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return {\"image\": image, \"label\": label}\n    idnum = example['image_name']\n    return {\"image\": image, \"label\": idnum}","6fd50944":"from functools import partial\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","854e394a":"import re\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","48b6cf00":"from kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\n\nGCS_PATH = KaggleDatasets().get_gcs_path()\n\ntrain_tfrecs = tf.io.gfile.glob(GCS_PATH + '\/train_tfrecords\/ld_train*.tfrec')\n    \nTRAINING_FILENAMES, _ = train_test_split(train_tfrecs,\n                                         test_size=0.35, \n                                         random_state=5)\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n\nprint(f\"NUM_TRAINING_IMAGES: {NUM_TRAINING_IMAGES}\")","36cf647b":"# Configs\nSIZE_CROPS = [224, 96]\nNUM_CROPS = [2, 3]\nMIN_SCALE = [0.14, 0.05] \nMAX_SCALE = [1., 0.14]\n\n# Experimental options\noptions = tf.data.Options()\noptions.experimental_optimization.noop_elimination = True\noptions.experimental_optimization.map_vectorization.enabled = True\noptions.experimental_optimization.apply_default_optimizations = True\noptions.experimental_deterministic = False\noptions.experimental_threading.max_intra_op_parallelism = 1","fbab289b":"dataset = load_dataset(TRAINING_FILENAMES, labeled=True) \n\ntrainloaders = get_multires_dataset(dataset,\n    size_crops=SIZE_CROPS,\n    num_crops=NUM_CROPS,\n    min_scale=MIN_SCALE,\n    max_scale=MAX_SCALE,\n    options=options)\n\ntrainloaders","c52473a6":"# Prepare the final data loader\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Zipping \ntrainloaders_zipped = tf.data.Dataset.zip(trainloaders)\n\n# Final trainloader\ntrainloaders_zipped = (\n    trainloaders_zipped\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nim1, im2, im3, im4, im5 = next(iter(trainloaders_zipped))\nprint(im1.shape, im2.shape, im3.shape, im4.shape, im5.shape)","68155a48":"plt.figure(figsize=(15, 15))\nfor i, image_batch  in enumerate([im1, im2, im3, im4, im5]):\n    ax = plt.subplot(4, 5, i + 1)\n    plt.imshow(image_batch[0])\n    ax = plt.subplot(4, 5, 5 + i + 1)\n    plt.imshow(image_batch[1])\n    ax = plt.subplot(4, 5, 10 + i + 1)\n    plt.imshow(image_batch[2])\n    ax = plt.subplot(4, 5, 15 + i + 1)\n    plt.imshow(image_batch[3])","403eb17c":"feature_backbone = get_resnet_backbone()\nfeature_backbone.summary()","15a78d05":"projection_prototype = get_projection_prototype(n_prototype)\nprojection_prototype.summary()","9b05b8a4":"embedding_batch = feature_backbone(im1)\nembedding_batch.shape","84b9328d":"projection, prototype = projection_prototype(embedding_batch)\nprojection.shape, prototype.shape","9c104886":"def sinkhorn(sample_prototype_batch, n_iters=3):\n    Q = tf.transpose(tf.exp(sample_prototype_batch\/0.05))\n    Q \/= tf.keras.backend.sum(Q)\n    K, B = Q.shape\n\n    u = tf.zeros_like(K, dtype=tf.float32)\n    r = tf.ones_like(K, dtype=tf.float32) \/ K\n    c = tf.ones_like(B, dtype=tf.float32) \/ B\n\n    for _ in range(n_iters):\n        u = tf.keras.backend.sum(Q, axis=1)\n        Q *= tf.expand_dims((r \/ u), axis=1)\n        Q *= tf.expand_dims(c \/ tf.keras.backend.sum(Q, axis=0), 0)\n\n    final_quantity = Q \/ tf.keras.backend.sum(Q, axis=0, keepdims=True)\n    final_quantity = tf.transpose(final_quantity)\n\n    return final_quantity","a68758ef":"# Check\nfinal_q = sinkhorn(prototype)\nfinal_q.shape","770d790b":"# @tf.function\n# Reference: https:\/\/github.com\/facebookresearch\/swav\/blob\/master\/main_swav.py\ndef train_step(input_views, feature_backbone, projection_prototype, \n               optimizer, crops_for_assign, temperature):\n    # ============ retrieve input data ... ============\n    im1, im2, im3, im4, im5 = input_views \n    inputs = [im1, im2, im3, im4, im5]\n    batch_size = inputs[0].shape[0]\n\n    # ============ create crop entries with same shape ... ============\n    crop_sizes = [inp.shape[1] for inp in inputs] # list of crop size of views\n    unique_consecutive_count = [len([elem for elem in g]) for _, g in groupby(crop_sizes)] # equivalent to torch.unique_consecutive\n    idx_crops = tf.cumsum(unique_consecutive_count)\n    \n    # ============ multi-res forward passes ... ============\n    start_idx = 0\n    with tf.GradientTape() as tape:\n        for end_idx in idx_crops:\n            concat_input = tf.stop_gradient(tf.concat(inputs[start_idx:end_idx], axis=0))\n            _embedding = feature_backbone(concat_input) # get embedding of same dim views together\n            if start_idx == 0:\n                embeddings = _embedding # for first iter\n            else:\n                embeddings = tf.concat((embeddings, _embedding), axis=0) # concat all the embeddings from all the views\n            start_idx = end_idx\n        \n        projection, prototype = projection_prototype(embeddings) # get normalized projection and prototype\n        projection = tf.stop_gradient(projection)\n\n        # ============ swav loss ... ============\n        # https:\/\/github.com\/facebookresearch\/swav\/issues\/19\n        loss = 0\n        for i, crop_id in enumerate(crops_for_assign): # crops_for_assign = [0,1]\n            with tape.stop_recording():\n                out = prototype[batch_size * crop_id: batch_size * (crop_id + 1)]\n                \n                # get assignments\n                q = sinkhorn(out) # sinkhorn is used for cluster assignment\n            \n            # cluster assignment prediction\n            subloss = 0\n            for v in np.delete(np.arange(np.sum(NUM_CROPS)), crop_id): # (for rest of the portions compute p and take cross entropy with q)\n                p = tf.nn.softmax(prototype[batch_size * v: batch_size * (v + 1)] \/ temperature) \n                subloss -= tf.math.reduce_mean(tf.math.reduce_sum(q * tf.math.log(p), axis=1))\n            loss += subloss \/ tf.cast((tf.reduce_sum(NUM_CROPS) - 1), tf.float32)\n        \n        loss \/= len(crops_for_assign)\n\n    # ============ backprop ... ============\n    variables = feature_backbone.trainable_variables + projection_prototype.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return loss","9ba9cc47":"def train_swav(feature_backbone, \n               projection_prototype, \n               dataloader, \n               optimizer, \n               crops_for_assign,\n               temperature, \n               epochs=50):\n  \n    step_wise_loss = []\n    epoch_wise_loss = []\n    \n    for epoch in tqdm(range(epochs)):\n        w = projection_prototype.get_layer('prototype').get_weights()\n        w = tf.transpose(w)\n        w = tf.math.l2_normalize(w, axis=1)\n        projection_prototype.get_layer('prototype').set_weights(tf.transpose(w))\n\n        for i, inputs in enumerate(dataloader):\n            loss = train_step(inputs, feature_backbone, projection_prototype, \n                              optimizer, crops_for_assign, temperature)\n            step_wise_loss.append(loss)\n\n        epoch_wise_loss.append(np.mean(step_wise_loss))\n        #wandb.log({'epoch': epoch, 'loss':np.mean(step_wise_loss)})\n        \n        if epoch % 5 == 0:\n            print(\"epoch: {} loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n\n    return epoch_wise_loss, [feature_backbone, projection_prototype]","7d838c21":"%%time\n# ============ initialize the networks and the optimizer ... ============\nfeature_backbone = get_resnet_backbone()\nprojection_prototype = get_projection_prototype(10)\n\nlr_decayed_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=0.1,\n    decay_steps=500,\n    end_learning_rate=0.001,\n    power=0.5)\nopt = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn)\n\n# ================= initialize wandb ======================\n#wandb.init(entity='authors', project='swav-tf', id='40-epochs')\n\n# ============ train for 40 epochs ... ============\nepoch_wise_loss, models = train_swav(feature_backbone, \n    projection_prototype, \n    trainloaders_zipped, \n    opt, \n    crops_for_assign=[0, 1],\n    temperature=0.1, \n    epochs=epochs\n)","af2f62d1":"plt.plot(epoch_wise_loss)\nplt.show()","0cd72b7d":"# Serialize the models\nfeature_backbone, projection_prototype = models\nfeature_backbone.save_weights('feature_backbone.h5')\nprojection_prototype.save_weights('projection_prototype.h5')","732098a0":"# Sinkhorn Knopp for Cluster Assignment\nReference: A.1 from https:\/\/arxiv.org\/abs\/2006.09882","0fa50271":"# SwAV-TF\n- \u672c\u5bb6\u306e\u5b9f\u88c5\u3068\u306f\u5c11\u3057\u9055\u3046\n\n    - \u30ad\u30e5\u30fc\u3092\u4f7f\u7528\u3057\u306a\u3044\u3002\u672c\u5bb6\u306e\u5b9f\u88c5\u3067\u306f\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u306e\u6570\u304c\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3088\u308a\u3082\u5927\u304d\u304f\u306a\u3063\u305f\u3068\u304d\u306b\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u3001\u672c\u5bb6\u306f\u30ad\u30e5\u30fc\u3092\u7dad\u6301\n    - 224x224\u306e\u89e3\u50cf\u5ea6\u306e2\u3064\u306e\u30af\u30ed\u30c3\u30d7\u3068\u300196x96\u306e\u89e3\u50cf\u5ea6\u306e3\u3064\u306e\u30af\u30ed\u30c3\u30d7\u3092\u4f7f\u7528\u3002\u30de\u30eb\u30c1\u30af\u30ed\u30c3\u30d7\u306e\u63d0\u6848\u3055\u308c\u305f\u8a2d\u5b9a\u3068\u306f\u9055\u3046\n    - 15\u500b\u306e\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u3092\u4f7f\u7528\u3002\u5143\u306e\u8ad6\u6587\u3067\u306f\u3001\u8457\u8005\u306fImageNet\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b3000\u306e\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u3092\u4f7f\u7528 \n    - \u57fa\u672c\u5b66\u7fd2\u73870.1\u306e\u30b3\u30b5\u30a4\u30f3\u6e1b\u8870\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3068\u3068\u3082\u306bSGD\u3092\u4f7f\u7528\u3002\u672c\u5bb6\u306e\u5b9f\u88c5\u306f\u7fd2\u7387\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u306b\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\u3068\u30b3\u30b5\u30a4\u30f3\u6e1b\u8870\u306e\u7d44\u307f\u5408\u308f\u305b\n","31dece0a":"# Multi Crop Resize Data Augmentation","d773a6d6":"https:\/\/wandb.ai\/authors\/swav-tf\/reports\/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg\n\nSwAV\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\n\n![swav](https:\/\/i.ibb.co\/TtSW4Fd\/figure-3.png)\n\n![swav2](https:\/\/i.ibb.co\/2FGDvd6\/figure-6.png)\n\n![swav3](https:\/\/i.ibb.co\/jgm7J81\/figure-7.png)\n\n- \u540c\u3058\u753b\u50cf\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u30c8\u30ea\u30df\u30f3\u30b0\u3057\u3066\u9ad8\u89e3\u50cf\u5ea6\uff08\u4f8b\uff1a224x224\uff09\u4f4e\u89e3\u50cf\u5ea6\uff08\u4f8b\uff1a96x96\uff09\u306e\u30de\u30eb\u30c1\u30af\u30ed\u30c3\u30d7\u753b\u50cf\u306bcolor distortion, random flipping, and random grayscaling\u306a\u3069\u306eaugmentation \u3092\u9806\u756a\u306b\u9069\u7528\uff08SimCLR \u306eaugmentation\uff09\n- CNN\uff08ResNet50\uff09\u3067\u57cb\u3081\u8fbc\u307f\uff08\u6700\u5f8c\u306e\u30b0\u30ed\u30fc\u30d0\u30eb\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc\u304b\u3089\u306e\u51fa\u529b\uff09\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97\n- \u3053\u306e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u3092\u6d45\u3044\u975e\u7dda\u5f62\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u9001\u308a\u3001\u305d\u306e\u51fa\u529b\u304c\u5c04\u5f71\u30d9\u30af\u30c8\u30ebZ\n- \u5c04\u5f71\u30d9\u30af\u30c8\u30ebZ\u3092\u5358\u4e00\u306e\u7dda\u5f62\u5c64\u306b\u6e21\u3059\u3002\u3064\u307e\u308a\u3001\u3053\u306e\u30ec\u30a4\u30e4\u30fc\u306b\u306f\u975e\u7dda\u5f62\u6027\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u305b\u3093\u3002\u30ec\u30a4\u30e4\u30fc\u306e\u51fa\u529b\u306f\u3001Z\u3068\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u306e\u9593\u306e\u5185\u7a4d\u3002\u3053\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u95a2\u9023\u3059\u308b\u300c\u91cd\u307f\u300d\u30de\u30c8\u30ea\u30c3\u30af\u30b9\uff08\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\u4e2d\u306b\u66f4\u65b0\u3055\u308c\u305f\u3082\u306e\uff09\u306f\u3001\u5b66\u7fd2\u53ef\u80fd\u306a\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u30d0\u30f3\u30af\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\n- Sinkhorn Knopp \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\u3057\u3066\u540c\u3058\u753b\u50cf\u306e2\u3064\u306e\u5225\u3005\u306e\u30d3\u30e5\u30fc\u9593\u3067\u30b9\u30ef\u30c3\u30d7\u3055\u308c\u305f\u4e88\u6e2c\u554f\u984c\u3092\u8a2d\u5b9a","e4de4665":"# Training Loop","bdd1d8ee":"# Model Architecture","0185cf6a":"# Train Step","44e39d9c":"\nSwAV\u306f\u3001\u540c\u3058\u753b\u50cf\u304b\u3089\u306e\u30af\u30ed\u30c3\u30d7\u753b\u50cf\u305f\u3061\u304c\u5c5e\u3059\u308b\u30af\u30e9\u30b9\u30bf\u30fc(\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u30d9\u30af\u30c8\u30eb)\u306f\u540c\u3058\u3001\u3068\u3044\u3046\u306e\u3092\u5b66\u7fd2\n\n\u3053\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30e9\u30d9\u30eb(\u30bd\u30d5\u30c8\u30e9\u30d9\u30eb)\u306fSinkhorn-Knopp\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u547c\u3070\u308c\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u5404\u30d0\u30c3\u30c1\u3067\u6bce\u56de\u751f\u6210\n\nhttps:\/\/qiita.com\/omiita\/items\/a7429ec42e4eef4b6a4d","e0dc372d":"https:\/\/github.com\/ayulockin\/SwAV-TF  \nhttps:\/\/github.com\/ayulockin\/SwAV-TF\/blob\/master\/Train_SwAV_40_epochs.ipynb","38158084":"# params","95f47aac":"# Cassava data\n- https:\/\/www.kaggle.com\/jessemostipak\/getting-started-tpus-cassava-leaf-disease"}}