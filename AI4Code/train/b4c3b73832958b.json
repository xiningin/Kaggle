{"cell_type":{"e12d96c4":"code","2c32c64f":"code","5ab39355":"code","f743a0da":"code","c1500a83":"code","4ae0e868":"code","8ede1612":"code","2c06e127":"code","ad98d27e":"code","efeade29":"code","73398dac":"code","3b39f1f7":"code","2a86b2dc":"code","b0c48b80":"code","042d1c1d":"code","74b1d8a7":"code","9d82b160":"code","c0db5dd5":"code","3ce6389f":"markdown","57de9eaf":"markdown","02928754":"markdown","068fa2c4":"markdown","beb68593":"markdown","27c68c0c":"markdown","b72f0dbc":"markdown","b9f5165d":"markdown","ddf60bae":"markdown"},"source":{"e12d96c4":"import os\nimport random\nimport difflib\nfrom binascii import crc32\nimport sys \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATA_PATH = \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\"\nval_df = pd.read_csv(os.path.join(DATA_PATH, \"validation.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\ntest_en_df = pd.read_csv(\"..\/input\/test-en-df\/test_en.csv\")\n\nval_ids_df = pd.read_csv(os.path.join(DATA_PATH, \"validation-processed-seqlen128.csv\"))\ntest_ids_df = pd.read_csv(os.path.join(DATA_PATH, \"test-processed-seqlen128.csv\"))\ntest_ids_df = pd.merge(test_ids_df, test_df[[\"id\", \"lang\"]], on=\"id\")","2c32c64f":"target_column = \"comment_text\"\ntest_ids_df.loc[test_ids_df[target_column].isin(val_ids_df[target_column]), ]","5ab39355":"target_column = \"input_word_ids\"\noverlapped_test_df = test_ids_df.loc[test_ids_df[target_column].isin(val_ids_df[target_column]), ]\noverlapped_val_df = val_ids_df.loc[val_ids_df[target_column].isin(test_ids_df[target_column]), ]\noverlapped_test_df.nunique()","f743a0da":"test_ids_df[\"token_hash\"] = test_ids_df[\"input_word_ids\"].apply(lambda x: crc32(x.encode()) & 0xffffffff)\nval_ids_df[\"token_hash\"] = val_ids_df[\"input_word_ids\"].apply(lambda x: crc32(x.encode()) & 0xffffffff)\ntarget_column = \"token_hash\"\noverlapped_hash = test_ids_df.loc[test_ids_df[target_column].isin(val_ids_df[target_column]), target_column]\noverlapped_hash = np.unique(overlapped_hash.values)\noverlapped_hash.sort()\noverlapped_hash.shape","c1500a83":"def display_test_val_comment(test_ids_df, val_ids_df, target_column=\"token_hash\", target_value=0, test_en_df=None, ind_=0):\n    test_sample = test_ids_df.query(f\"{target_column} == {target_value}\")\n    val_sample = val_ids_df.query(f\"{target_column} == {target_value}\")\n    \n    test_comment = test_sample.comment_text.values[ind_]\n    val_comment = val_sample.comment_text.values[0]\n    print(\"{}\".format(\"-\"*80))\n\n    print(f\"{target_column}:{target_hash}\")\n    print(f\">> TEST \\n ID:{test_sample.id.values[ind_]}, LANG:{test_sample.lang.values[ind_]}, Duplicated Num:{len(test_sample)}\")\n    print(f\"COMMET_TEXT:{test_comment}\\n\")\n\n    print(f\">> VALIDATION \\n ID:{val_sample.id.values[0]}, LANG:{val_sample.lang.values[0]}, Duplicated Num:{len(val_sample)}, TOXIC:{val_sample.toxic.values[0]}\")\n    print(f\"COMMET_TEXT:{val_comment}\\n\")\n\n    # diff = difflib.unified_diff(test_comment.replace(\" \", \"\\n \").split(), val_comment.replace(\" \", \"\\n \").split(), \"TEST\", \"VAL\", lineterm='\\n')\n    diff = difflib.context_diff(test_comment.replace(\" \", \"\\n \").split(), val_comment.replace(\" \", \"\\n \").split(), \"TEST\", \"VAL\", lineterm='\\n')\n    print(\">> diff result\")\n    sys.stdout.writelines(diff)\n\n    if test_en_df is not None:\n        test_en_comment = test_en_df.query(f\"id == {test_sample.id.values[ind_]}\").content_en.values[0]\n        print(\"\\n\\n>> TEST ENGLISH Translation\")\n        print(f\"{test_en_comment}\\n\")","4ae0e868":"for i, target_hash in enumerate(overlapped_hash):\n    display_test_val_comment(test_ids_df, val_ids_df, target_column=\"token_hash\", target_value=target_hash, test_en_df=test_en_df)\n    # comment out lines below if you want to check all samples\n    if i >= 2:\n        break","8ede1612":"# submission file from https:\/\/www.kaggle.com\/hamditarek\/ensemble by Tarek Hamdi, its LB score is 0.9462.\nsub_df = pd.read_csv(\"..\/input\/ensemble\/submission.csv\")\nsub_df = pd.merge(sub_df, test_ids_df, on=\"id\")\nsub_df.head()","2c06e127":"non_toxic_hash = val_ids_df.loc[val_ids_df.token_hash.isin(overlapped_hash), :].query(\"toxic == 0\").token_hash\ntoxic_hash = val_ids_df.loc[val_ids_df.token_hash.isin(overlapped_hash), :].query(\"toxic == 1\").token_hash\nnon_toxic_hash = np.unique(non_toxic_hash)\ntoxic_hash = np.unique(toxic_hash)","ad98d27e":"sub_df.loc[sub_df.token_hash.isin(non_toxic_hash), \"toxic\"].hist()","efeade29":"sub_df.loc[sub_df.token_hash.isin(non_toxic_hash), :].sort_values(\"toxic\").tail()","73398dac":"sub_df.query(\"token_hash == 3880127965\")","3b39f1f7":"non_toxic_hash = non_toxic_hash[non_toxic_hash != 3880127965]","2a86b2dc":"sub_df.loc[sub_df.token_hash.isin(non_toxic_hash), \"toxic\"].hist()","b0c48b80":"sub_df.loc[sub_df.token_hash.isin(toxic_hash), \"toxic\"].hist()","042d1c1d":"sub_df.loc[sub_df.token_hash.isin(non_toxic_hash), \"toxic\"] = 0.0\nsub_df.loc[sub_df.token_hash.isin(toxic_hash), \"toxic\"]= 1.0","74b1d8a7":"sub_df[[\"id\", \"toxic\"]].to_csv(\"submission.csv\", index=False)","9d82b160":"test_ids_df.duplicated(\"input_word_ids\", keep=False).sum()","c0db5dd5":"val_ids_df.duplicated(\"input_word_ids\", keep=False).sum()","3ce6389f":" # Appendix: Similar samples for each validatoin and test data.","57de9eaf":"#### There are 641 test \"input_word_ids\" samples which are confirmed at validation data. And there are also deplicated samples.","02928754":"Let's check these samples.","068fa2c4":"# Data leak? similar\/same validataoin samples on test data\nThere migiht be leaked samples from validation data, ~600 samples on test data.  \nI could confirm some minor changed validation samples on test data or the exact same samples on test data like below.  \nPlease check this notebook.\n\nAnd also I tried whether LB score is improved with this leak.  \nBut I got worse result if I use this leak.(0.9462 -> 0.9459: base submission file from https:\/\/www.kaggle.com\/hamditarek\/ensemble ).  \nOne possible reason for this is my rough treatment after 128 tokens. \n\n```\n--------------------------------------------------------------------------------\n>> TEST \n ID:8667, LANG:tr, Duplicated Num:2\nCOMMET_TEXT:Vikipedi de deneme yapt\u0131\u011f\u0131n\u0131z i\u00e7in te\u015fekk\u00fcrler. Denemeniz \u00e7al\u0131\u015ft\u0131, ancak \u015fu anda ya    geri al\u0131nd\u0131    ya da    silindi   . Ba\u015fka bir deneme yapmak istiyorsan\u0131z l\u00fctfen    deneme tahtas\u0131n\u0131    kullan\u0131n. Ansiklopedimize nas\u0131l katk\u0131da bulunabilece\u011finiz hakk\u0131nda daha fazla bilgi edinmek istiyorsan\u0131z    ho\u015fgeldin    sayfas\u0131na bir g\u00f6z at\u0131n. Te\u015fekk\u00fcrler.\n\n>> VALIDATION \n ID:2162, LANG:tr, Duplicated Num:1, TOXIC:0\nCOMMET_TEXT:Vikipedi de deneme yapt\u0131\u011f\u0131n\u0131z i\u00e7in te\u015fekk\u00fcrler. Denemeniz \u00e7al\u0131\u015ft\u0131, ancak \u015fu anda ya geri al\u0131nd\u0131 ya da silindi. Ba\u015fka bir deneme yapmak istiyorsan\u0131z l\u00fctfen deneme tahtas\u0131n\u0131 kullan\u0131n. Ansiklopedimize nas\u0131l katk\u0131da bulunabilece\u011finiz hakk\u0131nda daha fazla bilgi edinmek istiyorsan\u0131z ho\u015fgeldin sayfas\u0131na bir g\u00f6z at\u0131n. Te\u015fekk\u00fcrler.\n\n>> diff result\n*** TEST\n--- VAL\n***************\n*** 14,21 ****\n  al\u0131nd\u0131  ya  da! silindi! .  Ba\u015fka  bir  deneme--- 14,20 ----\n  al\u0131nd\u0131  ya  da! silindi.  Ba\u015fka  bir  deneme\n\n>> TEST ENGLISH Translation\nThank you for doing experiments in Wikipedia. Your experiment worked, but at the moment either rolled back or deleted. If you want to make another attempt, please use the sandbox. If you want to learn more about how you might be contributing to our encyclopedia Take a look at the welcome page. Thank you.\n\n```","beb68593":"At \"token_hash == 3880127965\" these samples share the header, \"........\", and not leak.","27c68c0c":"There are no ovelappings with \"comment_text\" between test and validation.","b72f0dbc":"# Display all overlapped samples.","b9f5165d":"# Make a submission\nLet's check this leak with LB score.  But current apporach doen't work well, you will get worse result... 0.9462 -> 0.9459","ddf60bae":"Split the overlapped samples with validation toxicity, \"toxic\" and \"non_toxic\"."}}