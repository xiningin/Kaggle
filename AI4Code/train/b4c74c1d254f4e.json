{"cell_type":{"c47a7d27":"code","a8a80faa":"code","3ed3764a":"code","537654e6":"code","62543205":"code","82a643bc":"code","8a95f4a2":"code","cc07fb5c":"code","a82fc17c":"code","308d6842":"code","ba516bd4":"code","c5d24901":"code","12e4389c":"code","24c52d15":"code","c489d8db":"code","0983844d":"code","cc53b6e0":"code","6aa87524":"code","d991f8b2":"code","a53c7252":"code","5c3484e8":"code","cf21a047":"code","5a520a30":"code","026e6c68":"code","2f498e8a":"code","497b879f":"code","4e01beef":"code","74f586c3":"code","98f7f360":"code","214afcfa":"code","b5674f47":"code","a394bde2":"code","04694c04":"code","fae20d77":"code","f5f93f62":"code","dd7b29bb":"code","fec4e485":"code","a54e1744":"markdown","96cb302e":"markdown","5269a12f":"markdown","a28a4f5a":"markdown","7543bd94":"markdown","cde40ff2":"markdown","17f1427a":"markdown","8675491d":"markdown","c8624cd7":"markdown","0acdcbe8":"markdown","05933a8b":"markdown","a052bbed":"markdown","bcdd1a40":"markdown","8f69be27":"markdown","18444af2":"markdown","93b7dd2e":"markdown","29de0cb7":"markdown","aff6df2d":"markdown","24992723":"markdown","ae023d31":"markdown","398874dd":"markdown","429f2010":"markdown","8e84ce46":"markdown","d5cd6857":"markdown","34b28384":"markdown","f3afcab9":"markdown","21da5d8f":"markdown"},"source":{"c47a7d27":"# Import the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tabulate import tabulate\nnp.random.seed(0)\n#==========================================================================\n#==========================================================================\ndata = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv',\n                   index_col = 'RowNumber')\ndata = data.sample(5000)\ndata.head()","a8a80faa":"plt.figure(1, dpi=100)\nvalues = data['Exited'].values\n# Analysis\nplt.text(\n    x=0.2,\n    y=7.0,\n    s = \"80%\",\n    fontsize=44,\n    c='#ff8c00'\n)\n# text\nplt.text(\n    x=0.2,\n    y=6.0,\n    s = \"of data points are non churn category\\npoints, suggesting imbalance\",\n    c=\"gray\"\n)\n# Hist\nplt.hist(\n    values,\n    density=True,\n    color='gray'\n)\n# 0 label\nplt.annotate(\n    s = \"0\",\n    xy = (0.05, 7),\n    fontsize=12,\n    c='white'\n)\n# 1 label\nplt.annotate(\n    s = \"1\",\n    xy = (0.95, 1.5),\n    fontsize = 12,\n    c='white'\n)\nplt.box(on=None)\nplt.xlabel('Customer Distribution')\n\nplt.yticks([])\nplt.xticks([])\nplt.title('Need to implement Data Imbalance Measures')\nplt.show();","3ed3764a":"# check for missing values\ndata.isnull().any()","537654e6":"# Import necessary plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make figures inline\n%matplotlib inline","62543205":"data.info()","82a643bc":"plt.figure(figsize=(8, 8))\nsns.set()\nsns.boxplot(y = 'CreditScore', x = 'Exited', data = data, palette = 'husl');","8a95f4a2":"plt.figure(figsize=(8, 5))\nsns.violinplot(y = 'Exited' , x = 'Gender' , data = data, kind='boxen', palette = 'hot');","cc07fb5c":"plt.figure(figsize=(8, 5))\nsns.countplot(x = 'Geography' , data = data);","a82fc17c":"plt.figure(figsize=(10, 10))\nsns.set(style = 'white')\nsns.heatmap(data.select_dtypes(include='number').corr(), annot = True, cmap = 'magma', square = True);","308d6842":"# Pairplot\ndata_random_sample = data.sample(frac = 0.4).reset_index()\n\nplt.figure(figsize=(12, 8))\nsns.pairplot(data_random_sample, corner = True, hue = 'Exited');","ba516bd4":"# Drop a useless feature\ndata.drop(['CustomerId', 'Surname'], axis = 1, inplace = True)","c5d24901":"# Get dependent and independent features\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1].astype('float')\nX.head()","12e4389c":"# Splitting to train test dataset\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 1)\nlen(y_train), len(y_val)","24c52d15":"# Reset the indexes of the splitted data frames\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)","c489d8db":"categorical_cols = [col for col in X_train.columns if X_train[col].dtypes == object]","0983844d":"# Label encoder object\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Create two empty data frames\nX_train_categorical, X_val_categorical = pd.DataFrame(), pd.DataFrame()\n\n# Label Encode the features\nfor col in categorical_cols:\n    X_train_categorical[col] = label_encoder.fit_transform(X_train[col])\n    X_val_categorical[col] = label_encoder.transform(X_val[col])\n\n# Drop the non required columns\nX_train.drop(categorical_cols, axis = 1, inplace = True)\nX_val.drop(categorical_cols, axis = 1, inplace=True)\n\n# put new colums in dataframe\nX_train = X_train.join(X_train_categorical)\nX_val = X_val.join(X_val_categorical)","cc53b6e0":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_val, y_val = smk.fit_sample(X_val, y_val)","6aa87524":"X_train.shape, X_val.shape","d991f8b2":"X_train[:5]","a53c7252":"y_train.value_counts()","5c3484e8":"columns = ['Balance', 'EstimatedSalary']  ## Columns to modify\n\n## Subtract the mean, divide by standard deviation.\nfor col in columns:\n    colMean = X_train[col].mean()\n    colStdDev = X_train[col].std()\n    X_train[col] = X_train[col].apply(lambda x : (x - colMean) \/ colStdDev)\n    X_val[col] = X_val[col].apply(lambda x : (x - colMean) \/ colStdDev)    ","cf21a047":"X_train.head()","5a520a30":"# metric\nfrom sklearn.metrics import f1_score","026e6c68":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = 'lbfgs', max_iter = 300)\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_preds, y_val)","2f498e8a":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_preds, y_val)","497b879f":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(class_weight='balanced')\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)","4e01beef":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)","74f586c3":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)\n","98f7f360":"from tensorflow import keras as K","214afcfa":"model = K.Sequential()\n\nmodel.add(K.layers.Dense(512, input_dim = 10, activation = 'relu'))\n\nmodel.add(K.layers.Dense(256, activation = 'relu'))\nmodel.add(K.layers.BatchNormalization())\n\nmodel.add(K.layers.Dense(64, activation = 'relu'))\nmodel.add(K.layers.Dropout(0.4))\n\nmodel.add(K.layers.Dense(8, activation = 'relu'))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(1, activation = 'sigmoid'))\n\nmodel.summary()","b5674f47":"opt = K.optimizers.Adam(learning_rate=0.00001)\n\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","a394bde2":"history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=(X_val, y_val))","04694c04":"y_preds = model.predict_classes(X_val)","fae20d77":"f1_score(y_val, y_preds)","f5f93f62":"from sklearn.model_selection import RandomizedSearchCV\n\nmodel = XGBClassifier()  ## Model to tune\n\n# define a parameters dictionary, which contains the search space to see\nparamSearchSpace = {\n    'n_estimators' : [10, 25, 70],  ## Number of trees\n    'gamma' : [1, 0.05, 0.1],    ## Regularisation parameter\n    'max_depth' : [2, 3, 5, 7],    ## max depth of tree\n    'scale_pos_weight' : [60, 70, 80] # Num pos \/ num Neg\n}\n\n# make Grid Search CV object\nclf = RandomizedSearchCV(model, param_distributions=  paramSearchSpace)\n\n# Fit with data\nclf.fit(X_train, y_train)\n\n# See the best values we obtain\nclf.best_params_, clf.best_score_","dd7b29bb":"finalModel = XGBClassifier(**clf.best_params_)\nfinalModel.fit(X_train, y_train)\ny_preds = finalModel.predict(X_val)\n\n## Final f1 score\nf1_score(y_preds, y_val)","fec4e485":"import pickle\n# Dump the model\npickle.dump(finalModel, open('ChurnModelFinal.pkl', 'wb'))","a54e1744":"# Neural Network(TensorFlow)","96cb302e":"# Plotting with Matplotlib and Seaborn","5269a12f":"As we can see, **Exited** is our dependent feature. Other columns are independent features\n\nLet us check how many values of __Exited__ columns are there so that we can figure out if there is class imbalance or not","a28a4f5a":"# Data Visualization\nHere we are going to plot graphs regarding the data to get a deeper insight.","7543bd94":"Let us plot a heatmap of the correlations of the features with each other. That will help us discard non useful features.\nIt also gives us some idea as to what features predict dependent column best","cde40ff2":"# Models\nWe will be using the following models \n* Logistic Regression\n* Decision Tree\n* Random Forest Classifier\n* Extra Trees Classifier\n* XGBClassifier\n* ANN","17f1427a":"# Scaling\nWe scale the data so that datapoints are on the same level\n\n### Note: we have labelled data, so we should not scale all the data.Otherwise meaning will be lost","8675491d":"Pairplot - This plots graphs between every two variables. This is useful for visualisation","c8624cd7":"# Checking for Missing Values(NaN)","0acdcbe8":"# Hyperparameter Tuning\nLet us tune hyperparameters of XGBoost to further improve our results.\n\nWe will be using RandomisedSearchCV for this. This searches randomly through a search space and gets the best parameters\n","05933a8b":"# Decision Tree","a052bbed":"# First Steps","bcdd1a40":"Randomized Search CV takes time!! Please wait!","8f69be27":"# Final check at the dataset before putting in model\nNow we take a final look at the dataset","18444af2":"# Converting non numeric features to numeric features\nWe convert non numeric features to numeric features.\nAlso we drop columns which do not seem to contribute anything useful like **CustomerId**, **Surname**.\n\nBut first we will split the dataset into train and test dataset.","93b7dd2e":"# Generating new data by oversampling\nSince we have an imbalanced dataset, we will increase the number of samples by SMOTE technique","29de0cb7":"**Geography, Gender, Surname** are object data-types, while others are either int \/ float.","aff6df2d":"Phew! We are lucky we did not get any null values. \nUsually there are null values in the dataset and we need to remove them.\n\nUsually, There are various techniques to handle missing values. \n\n[This awesome notebook by Kaggle Grandmaster Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python) helped me learn a lot. Do check out if you like it!","24992723":"Let us get a list of columns in the data so that we can predict better. \nWe use the .info() method to get the datatypes too","ae023d31":"# Extra Trees Classifier","398874dd":"# Saving Best Model","429f2010":"# Logistic Regression","8e84ce46":"# Scope for improvement\n\n* Better Hyperparameter Tuning\n* Change optimizer for model\n\n## Thank you!","d5cd6857":"# Data Preprocessing\nIn this step, we are going to preprocess our data so that we can use it on our models.\n\nPreprocessing involves the following:\n* Checking for NaN values that is missing values in the data\n* Visualise the data so that we can derive meaningful insights\n* Split to training and test datasets\n* Fill in NaN Values\n* Convert non numeric features to numeric features so that we can do predictions\n* Scale the data \n\nLet us go ahead with the first step, __checking for NaN\/missing values__","34b28384":"# XGBoost","f3afcab9":"# Random Forest Classifier","21da5d8f":"# Let us begin with a dilemma...\n\n> Java has a startup, but lately he finds that his customers are leaving the services he provides. So he call's us to help him. As a data scientist, we need to look into data about his customers and find out which customers are likely to leave.\n\nLet us go about this task\n\n# What we will go through in this notebook:\n* [First Steps - Preliminary work](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#First-Steps)\n* [Data Preprocessing](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Data-Preprocessing)\n* [Checking for Missing Values](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Checking-for-Missing-Values(NaN))\n* [Visualization of Data](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Data-Visualization)\n* [Converting Non numeric features to numeric features](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Converting-non-numeric-features-to-numeric-features)\n* [Oversampling](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Generating-new-data-by-oversampling)\n* [Scaling](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Scaling)\n* [Various Models](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Models)\n* [Hyperparameter Tuning](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Hyperparameter-Tuning)\n* [Scope For Improvement](https:\/\/www.kaggle.com\/duttasd28\/java-s-dilemma?scriptVersionId=50296668#Scope-for-improvement)"}}