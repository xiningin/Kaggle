{"cell_type":{"09a6ea5d":"code","f2bf618a":"code","06a1d38d":"code","91199acc":"code","bc379fe6":"code","4bedc997":"code","cbcd1eb2":"code","4dbccdc7":"code","265654c1":"code","6fa1005a":"code","0629b16a":"code","840f4a8f":"code","7f51be8b":"code","2430f52f":"code","cc2dcf82":"code","824e2694":"code","6680996f":"code","4dd665f6":"code","f166291a":"code","a4424695":"code","b39040e5":"code","502fc33c":"code","a7ec823a":"code","627f8e55":"code","0f983fb7":"code","3f50649f":"code","2ba38f17":"code","501c10ad":"code","7d93f7e6":"code","82c4fd27":"code","0e3140d0":"code","c8031ea6":"code","fe0d7c00":"code","ce87fac7":"code","9fcb9349":"code","ba25b03d":"code","400a66b3":"code","68e72f8c":"code","e60c704e":"markdown","6151eed7":"markdown","a4dd4a2e":"markdown","63e210c2":"markdown","b9d44008":"markdown","14a2d271":"markdown","d7218766":"markdown","0e63942a":"markdown","8bdce7bb":"markdown","982ef3c0":"markdown","7c9ccdc7":"markdown","041045c4":"markdown","6ebcd890":"markdown","c24339e1":"markdown","b0323e65":"markdown","5d14b8b1":"markdown","3175ac01":"markdown","7fce7295":"markdown","2b38fafa":"markdown","6479b65d":"markdown","ab46d9ea":"markdown","17dc3826":"markdown","4b48f3d8":"markdown","2d9d35db":"markdown","718385f9":"markdown","58878c99":"markdown","c7b8d739":"markdown","ccd886eb":"markdown","ba42cbb1":"markdown","de114c7a":"markdown","f9b48f64":"markdown"},"source":{"09a6ea5d":"import torch\nimport os\nimport shutil\nfrom IPython.display import Image\nfrom torchvision.utils import save_image \nfrom fastai.vision.data import ImageDataLoaders\nfrom fastai.learner import Learner\nfrom fastai.callback.all import *\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nimport cv2\nimport random\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom fastai.metrics import accuracy","f2bf618a":"path='\/kaggle\/input\/gender-classification-dataset'\ntrain_valid_data_path=os.path.join(path,'Training')\ntest_data_path=os.path.join(path,'Validation')\ntargets=os.listdir(train_valid_data_path)\nprint(targets)","06a1d38d":"data_dir='\/kaggle\/data'\ntrain_path=os.path.join(data_dir,'train') #To store training images\nvalid_path=os.path.join(data_dir,'valid') # To store validation images\ntest_path=os.path.join(data_dir,'test') # To store testing images\n# display_path will be used to display images in this notebook\ndisplay_path=os.path.join(data_dir,'display')","91199acc":"def create_dir():\n    if not os.path.isdir(data_dir):\n        os.mkdir(data_dir)\n        os.mkdir(train_path)\n        os.mkdir(valid_path)\n        os.mkdir(test_path)\n        os.mkdir(display_path)\n        for target in targets:\n            os.mkdir(os.path.join(train_path,target))\n            os.mkdir(os.path.join(valid_path,target))\n            os.mkdir(os.path.join(test_path,target))\ndef check_dir():\n    print(f'{data_dir}: {os.path.isdir(data_dir)}')\n    print(f'{train_path}: {os.path.isdir(train_path)}')\n    print(f'{valid_path}: {os.path.isdir(valid_path)}')\n    print(f'{test_path}: {os.path.isdir(test_path)}')\n    print(f'{display_path}: {os.path.isdir(display_path)}')\n    for target in targets:\n        print(f'{os.path.join(train_path,target)}: {os.path.isdir(os.path.join(train_path,target))}')\n        print(f'{os.path.join(valid_path,target)}: {os.path.isdir(os.path.join(valid_path,target))}')\n        print(f'{os.path.join(test_path,target)}: {os.path.isdir(os.path.join(test_path,target))}')","bc379fe6":"create_dir()","4bedc997":"check_dir()","cbcd1eb2":"img_size=64\ndef find_img_size():\n    for folder in os.listdir(train_valid_data_path):\n        folder_path=os.path.join(train_valid_data_path,folder)\n        for file in os.listdir(folder_path):\n            file_path=os.path.join(folder_path,file)\n            image=cv2.imread(file_path)\n            print(image)\n            print(image.shape)\n            break\n        break\nfind_img_size()","4dbccdc7":"def load_train_images(n=5000):\n    for folder in os.listdir(train_valid_data_path):\n        folder_path=os.path.join(train_valid_data_path,folder)\n        dest=os.path.join(train_path,folder)\n        image_set=random.sample(os.listdir(folder_path),n)\n        print(f'Loading the training images for {folder}')\n        for file in tqdm(image_set):\n            file_path=os.path.join(folder_path,file)\n            shutil.copy(file_path,dest)\ndef load_valid_images(n=2500):\n    for folder in os.listdir(train_valid_data_path):\n        folder_path=os.path.join(train_valid_data_path,folder)\n        dest=os.path.join(valid_path,folder)\n        image_set=random.sample(os.listdir(folder_path),n)\n        print(f'Loading the validation images for {folder}')\n        for file in tqdm(image_set):\n            file_path=os.path.join(folder_path,file)\n            shutil.copy(file_path,dest)\ndef load_test_images(n=1000):\n    for folder in os.listdir(test_data_path):\n        folder_path=os.path.join(test_data_path,folder)\n        dest=os.path.join(test_path,folder)\n        image_set=random.sample(os.listdir(folder_path),n)\n        print(f'Loading the testing images for {folder}')\n        for file in tqdm(image_set):\n            file_path=os.path.join(folder_path,file)\n            shutil.copy(file_path,dest)","265654c1":"load_train_images()","6fa1005a":"load_valid_images()","0629b16a":"load_test_images()","840f4a8f":"class MyDataLoader(torch.utils.data.DataLoader):\n    def __init__(self,dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=True, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False,device):\n        # Initialise the parent DataLoader class with the values provided\n        super().__init__(dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler,\n           batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn,\n           pin_memory=pin_memory, drop_last=drop_last, timeout=timeout,\n           worker_init_fn=worker_init_fn, prefetch_factor=prefetch_factor,\n           persistent_workers=persistent_workers)\n        # Add the device property(for which I created this class) \n        self.device=device","7f51be8b":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","2430f52f":"def move_to_device(data,device):\n    #Check if the data is a list or a tuple\n    if isinstance(data,(list,tuple)):\n        #If it is then recursively call the function on its individual elements\n        #This will move the list or tuple on the GPU, one element at a time\n        return [move_to_device(x,device) for x in data] \n    return data.to(device)\nclass DeviceDataLoader:\n    def __init__(self,dl,device):\n        self.dl=dl\n        self.device=device\n    def __iter__(self):\n        for b in self.dl:\n            yield move_to_device(b,self.device)\n    def __len__(self):\n        return len(self.dl)","cc2dcf82":"train_tfms=tt.Compose([\n    tt.RandomHorizontalFlip(),\n    tt.Resize((img_size,img_size)),\n    tt.ToTensor()\n])\nvalid_test_tfms=tt.Compose([\n    tt.Resize((img_size,img_size)),\n    tt.ToTensor()\n])","824e2694":"train_ds=ImageFolder(root=train_path,transform=train_tfms)\nvalid_ds=ImageFolder(root=valid_path,transform=valid_test_tfms)\ntest_ds=ImageFolder(root=test_path,transform=valid_test_tfms)","6680996f":"batch_size=25\nepochs=35","4dd665f6":"train_dl=MyDataLoader(dataset=train_ds,shuffle=True,batch_size=batch_size,num_workers=8,device=device)\nvalid_dl=MyDataLoader(dataset=valid_ds,batch_size=batch_size,device=device,num_workers=8)\ntest_dl=MyDataLoader(dataset=test_ds,batch_size=batch_size,device=device,num_workers=8)","f166291a":"def show_batch(dl,mode):\n    for images,_ in dl:\n        #Save the batch of images in the display path under the file name shown\n        #Specify the number of rows in nrow\n        save_image(images,os.path.join(display_path,f'display_images_{mode}.png'),nrow=5)\n        break\n    # return the grid of images\n    return Image(os.path.join(display_path,f'display_images_{mode}.png'))","a4424695":"show_batch(train_dl,'train')","b39040e5":"show_batch(valid_dl,'valid')","502fc33c":"show_batch(test_dl,'test')","a7ec823a":"for images,labels in train_dl:\n    print(images.shape)\n    print(torch.max(images))\n    break","627f8e55":"train_dl=DeviceDataLoader(train_dl,device)\nvalid_dl=DeviceDataLoader(valid_dl,device)\ntest_dl=DeviceDataLoader(test_dl,device)","0f983fb7":"class ResidualUnit(nn.Module):\n    def __init__(self,ni,nf,n=3,ks=3,st=1):\n        super().__init__()\n        self.layers=[]\n        self.preprocess=[\n            nn.Conv2d(in_channels=ni,out_channels=nf,kernel_size=1,stride=1),\n            nn.ReLU(inplace=False),\n        ]\n        for i in range(n):\n            self.layers+=[\n                nn.Conv2d(in_channels=nf,out_channels=nf,kernel_size=ks,stride=st,padding=ks\/\/2,padding_mode='reflect'),\n                nn.ReLU(inplace=False),\n                nn.BatchNorm2d(nf)\n            ]\n        self.preprocess=nn.Sequential(*self.preprocess)\n        self.layers=nn.Sequential(*self.layers)\n    def forward(self,x):\n        r=self.preprocess(x)\n        out=self.layers(r)*0.3\n        return F.relu(out+r)","3f50649f":"def make_model(img_size=64):\n    steps=int(np.log2(img_size))\n    ni=3\n    nf=16\n    model=[]\n    model.append(ResidualUnit(ni=ni,nf=nf))\n    model.append(nn.AvgPool2d(kernel_size=2,stride=1))\n    for i in range(steps-1):\n        model.append(ResidualUnit(ni=nf,nf=nf*2))\n        model.append(nn.AvgPool2d(kernel_size=2,stride=1))\n        nf*=2\n    model+=[\n        nn.AdaptiveAvgPool2d(1),\n        nn.Flatten(),\n        nn.Linear(in_features=nf,out_features=4096),\n        nn.ReLU(inplace=True),\n        nn.Linear(in_features=4096,out_features=4096),\n        nn.ReLU(inplace=True),\n        nn.Linear(in_features=4096,out_features=len(targets)),\n        nn.Softmax(dim=1)\n    ]\n    return nn.Sequential(*model)","2ba38f17":"model=make_model()","501c10ad":"model","7d93f7e6":"def cntParam(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(cntParam(model))","82c4fd27":"data=ImageDataLoaders(train_dl,valid_dl)","0e3140d0":"learner=Learner(data,model,loss_func=F.cross_entropy,metrics=[accuracy])","c8031ea6":"learner.lr_find(start_lr=1e-9,end_lr=1e-2,num_it=1000)","fe0d7c00":"learner.fit(n_epoch=epochs,lr=5.584702012129128e-05)","ce87fac7":"learner.recorder.plot_loss()","9fcb9349":"def print_accuracy(dl,string):\n    preds,labels=learner.get_preds(dl=dl)\n    _,preds=torch.max(preds,dim=1)\n    print(f'The {string} accuracy of the model is {(torch.sum(preds==labels).item()\/len(preds)*100)}')","ba25b03d":"print_accuracy(train_dl,'training')","400a66b3":"print_accuracy(valid_dl,'validation')","68e72f8c":"print_accuracy(test_dl,'test')","e60c704e":"create_dir() will create all the directoried mentioned above and check_dir() will check whether the directories have been successfully created","6151eed7":"Making my datasets","a4dd4a2e":"Printing the total number of parameters in the model","63e210c2":"Printing a batch from validation set","b9d44008":"## Checking the performance of the model","14a2d271":"Printing a batch from test set","d7218766":"Loading my train and valid dataloaders in the FastAI ImageDataLoaders class, this will later be given to the learner class which we will use to train our model","0e63942a":"Printing the test accuracy(~ 97%)","8bdce7bb":"Making the residual unit of my model","982ef3c0":"I made functions to load the training,validation and testing images to their respective paths in the data_dir","7c9ccdc7":"Prining my model to view whether its architecture is appropriate","041045c4":"Checking the size of the images and the pixel values(The images have been resized appropriately and pixel values are between 0 and 1)","6ebcd890":"Setting up the paths to the provided dataset","c24339e1":"Printing my training accuracy","b0323e65":"Making a function that shows a batch of images. The reason I have **mode** as an arguement to the function is to have different images for the training,validation and testing data","5d14b8b1":"Setting up the paths where I will store the train,valid and test data","3175ac01":"## Setting up paths and directories","7fce7295":"Printing a batch from the training set","2b38fafa":"Printing the validation accuracy","6479b65d":"Making a function to return a Sequential model","ab46d9ea":"I had to make my own dataloader by extending the DataLoader class otherwise **learner.fit_one_cycle()** that I use later on gives me an error that my data has not been shifted to GPU","17dc3826":"## Loading and preprocessing the data","4b48f3d8":"Finding the appropriate learning rate using lr_find","2d9d35db":"Making my dataloaders","718385f9":"Training my model","58878c99":"img_size is the size to which my data will get resized to. I check the size of the images to make sure that img_size is less than the given image size. I also print out the images to see the range of pixel values(0-255) in this case","c7b8d739":"Moving my input dataloaders on to the GPU","ccd886eb":"## Making the model","ba42cbb1":"I have to make a DeviceDataLoader class otherwise **ImageDataLoaders** that I use below gives an error","de114c7a":"Making the learner object","f9b48f64":"Making the transformations I will apply to my training,validation and testing data"}}