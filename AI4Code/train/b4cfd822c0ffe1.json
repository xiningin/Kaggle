{"cell_type":{"502c2561":"code","297d336d":"code","e8eac2bf":"code","cea4fa9e":"code","e39f6edf":"code","01673969":"code","4b266626":"code","d19128d6":"code","efe7e9e1":"code","e913577d":"code","abfd86f9":"code","86848a2a":"code","2fb8f4de":"code","ef8efec2":"code","ab4693fe":"code","215e90d1":"code","1c9820c1":"code","87646e12":"code","522db059":"code","c20d2fb9":"code","db6c13a7":"code","031daa52":"markdown","dd16cad3":"markdown","cda3e0de":"markdown","69b4ec9c":"markdown","97af379f":"markdown","5b7e6725":"markdown","5ad6d301":"markdown"},"source":{"502c2561":"\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.metrics import categorical_accuracy\nimport numpy as np\nimport os\nimport collections\n\n","297d336d":"#import spacy, and english model\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")","e8eac2bf":"data_dir = '..\/input\/'# data directory containing input.txt\nsave_dir = '' # directory to store models\nseq_length = 10 # sequence length\nsequences_step = 1 #step to create sequences","cea4fa9e":"def create_wordlist(doc):\n    wl = []\n    ignore=('0','1','2','3','4','5','6','7','8','9','_','--')\n    for word in doc:\n        if word.text.isdigit()==False and word.text.startswith(\"\\n\")==False and word.text.startswith(ignore)==False:\n            wl.append(word.text.lower())\n            \n    return wl","e39f6edf":"wordlist = []\ninput_file = os.path.join('..\/input\/abcd.txt')\nf= open(input_file,'r',errors='ignore',encoding='utf-8')\ndata = f.read()\ndoc=nlp(data)\n\nwl = create_wordlist(doc)\n\nwordlist = wordlist + wl\n\n","01673969":"# count the number of words\nword_counts = collections.Counter(wordlist)\n\n# Mapping from index to word : that's the vocabulary\nvocabulary_inv = [x[0] for x in word_counts.most_common()]\nvocabulary_inv = list(sorted(vocabulary_inv))\n\n# Mapping from word to index\nvocab = {x: i for i, x in enumerate(vocabulary_inv)}#with index\nwords = [x[0] for x in word_counts.most_common()]\n\n#size of the vocabulary\nvocab_size = len(words)\nprint(\"vocab size: \", vocab_size)\n\n","4b266626":"#create sequences\nsequences = []\nnext_words = []\nfor i in range(0, len(wordlist) - seq_length, sequences_step):\n    \n    sequences.append(wordlist[i: i + seq_length])\n    next_words.append(wordlist[i + seq_length])\n\nprint('nb sequences:', len(sequences))","d19128d6":"X_NPP = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)","efe7e9e1":"y_NPP = np.zeros((len(sequences), vocab_size), dtype=np.bool)","e913577d":"\n\nfor i, sentence in enumerate(sequences):\n    for t, word in enumerate(sentence):\n        X_NPP[i, t, vocab[word]] = 1\n    y_NPP[i, vocab[next_words[i]]] = 1","abfd86f9":"rnn_size = 256 # size of RNN\nbatch_size = 64 # minibatch size\nseq_length = 10 # sequence length\nsequences_step = 1 #step to create sequences","86848a2a":"\ndef bidirectional_lstm_model(seq_length, vocab_size):\n    print('Build LSTM model.')\n    model_np = Sequential()\n    model_np.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n    model_np.add(Dropout(0.3))\n    model_np.add(Dense(vocab_size))\n    model_np.add(Activation('softmax'))\n    model_np.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n    \n    return model_np","2fb8f4de":"md = bidirectional_lstm_model(seq_length, vocab_size)\nmd.summary()","ef8efec2":"history = md.fit(X_NPP, y_NPP,\n                 batch_size=64,\n                 epochs=20,\n                 validation_split=0.1\n                )","ab4693fe":"#save_dir","215e90d1":"\nscore,accuracy=md.evaluate(X_NPP,y_NPP)\nprint(\"score\",score)\nprint(\"accuracy: \",accuracy)","1c9820c1":"#save the model\nmd.save('my_model_gen_sentences_lstm.final.h5')","87646e12":"from keras.models import load_model\n# load the model\nprint(\"loading model...\")\nmodel = load_model('my_model_gen_sentences_lstm.final.h5')","522db059":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","c20d2fb9":" #initiate sentences\nseq_length=10\nseed_sentences = \"how\"\nseed_sentences=seed_sentences.lower()\ngenerated = ''\nsentence = []\nfor i in range (seq_length):\n        sentence.append(\".\")\n    \nseed = seed_sentences.split()\n\n\nfor i in range(len(seed)):\n    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n\ngenerated += ' '.join(sentence)\nstart=len(generated)-len(seed_sentences)\nprint('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n\nwords_number = 10\n    #generate the text\nfor i in range(words_number):\n        #create the vector\n    x = np.zeros((1, seq_length, vocab_size))\n       \n    for t, word in enumerate(sentence):\n        x[0, t, vocab[word]] = 1.\n        \n\n        #calculate next word\n    preds = md.predict(x, verbose=0)[0]\n  \n    next_index = sample(preds, 0.34)\n    next_word = vocabulary_inv[next_index]\n\n        #add the next word to the text\n    generated += \" \" + next_word\n\n        # shift the sentence by one, and and the next word at its end\n    sentence = sentence[1:] + [next_word]\n\n","db6c13a7":"print(generated[start:])\n","031daa52":"# Generate phrase","dd16cad3":"# Build Model","cda3e0de":"If a print the summary of this model, you can see it has close to 61 millions of trainable parameters. It is huge, and the compute will take some time to complete.","69b4ec9c":"Create the list of sentences:","97af379f":"## train data","5b7e6725":" we train the model now. We extract 20% of it as validation sample. We simply run :","5ad6d301":"# parameters"}}