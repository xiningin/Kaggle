{"cell_type":{"bd19dc50":"code","a7fa8a32":"code","dfc4305a":"code","10844b0b":"code","9c054c40":"code","17a7fa6a":"code","78ef9f4a":"code","301fd70f":"code","f859c16c":"code","0fa5099e":"code","b5b7fa3e":"code","4d35b0c1":"code","ceb80046":"code","db736fbf":"code","6120f65c":"code","4ac2e544":"code","5ae7fa09":"code","3e4f24c8":"code","a71774f2":"code","e5664950":"code","9a3d9d7a":"code","46fcdb88":"code","522a0b6c":"code","1aaabcee":"code","3b091162":"code","29c0fac7":"code","3aaf02f4":"code","246c633a":"code","f0af30d6":"code","390d7643":"code","2dd238a1":"code","821c1e46":"code","4bdefb96":"code","27fbf670":"code","6c33c371":"code","fa9a4afd":"code","918d7b2c":"code","e723e086":"code","8f396538":"code","3c04a790":"code","11899d1a":"code","05a0116b":"code","14d508a8":"code","79e65aff":"code","f1c568fb":"code","e752202e":"code","505f0432":"code","2636eb08":"code","5e6e8201":"code","9344c072":"code","c8558dd9":"code","08da3e2b":"code","6aeebd8f":"code","3ff0a1a0":"code","c42fdbea":"code","be170d4f":"code","0deb4f93":"code","1ad60274":"markdown","a5d0a949":"markdown","07a0a559":"markdown","757fba96":"markdown","9530843a":"markdown","ca1dca22":"markdown","7484e004":"markdown","fa0bfab7":"markdown","c6aa8c90":"markdown","c25296d1":"markdown","345cfe4c":"markdown","2e053aca":"markdown","7ea88077":"markdown","f438262c":"markdown","63cfa01e":"markdown","cc0c1246":"markdown","462b420f":"markdown","052cd6f6":"markdown","27e14920":"markdown","ec4c1416":"markdown","c16f6569":"markdown","ed6639f8":"markdown","1ee4cbd6":"markdown","3349bb30":"markdown","746f11d9":"markdown","1c1ae328":"markdown","d1291549":"markdown","f1281064":"markdown","468820fd":"markdown","10031e86":"markdown","34e9d79b":"markdown","812ddf9b":"markdown","800dbf35":"markdown","9a2cfc25":"markdown","8861b461":"markdown","b0a4f5bc":"markdown","fc92115b":"markdown","72e440e4":"markdown","b2b7f41e":"markdown","36398556":"markdown","d70e719a":"markdown","04f26525":"markdown","e75498df":"markdown","f14f3e1c":"markdown","e5fb4b9c":"markdown","919a5194":"markdown","febf54cc":"markdown","54a35f5a":"markdown","338e264e":"markdown","475ce196":"markdown"},"source":{"bd19dc50":"# !pip install tokenizers==0.9.4\n# !pip install tokenizers==0.9.4\n# !pip install simpletransformers\n!pip install transformers==3\n# !pip install simpletransformers","a7fa8a32":"# !pip install transformers -U","dfc4305a":"!pip install -q -U watermark\n%reload_ext watermark\n%watermark -v -p numpy,pandas,torch,transformers","10844b0b":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nimport os\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","9c054c40":"device","17a7fa6a":"!export CUDA_LAUNCH_BLOCKING=1\n","78ef9f4a":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","301fd70f":"df1 = pd.read_csv('\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/consumer-reviews-of-amazon-products\/1429_1.csv')\ndf3 = pd.read_csv('\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')","f859c16c":"df1.head()\ndf2.head()\ndf3.head()","0fa5099e":"df1.shape\ndf2.shape\ndf3.shape","b5b7fa3e":"df = df2\ndf.isnull().sum()","4d35b0c1":"df = df[df['reviews.rating'].notnull() & df['reviews.text'].notnull()]\ndf.isnull().sum()","ceb80046":"df = df[['reviews.text', 'reviews.rating']]\ndf.columns = ['review_text', 'review_rating']\ndf.shape","db736fbf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(df['review_rating'])\nplt.xlabel('Rating Count')","6120f65c":"def to_sentiment(rating):\n    \"\"\" Convert rating into sentiment\"\"\"\n    if rating == 5:\n        sentiment = 'positive'\n    elif rating == 4:\n        sentiment = 'neutral'\n    else:\n        sentiment = 'negative'\n    return sentiment","4ac2e544":"df['sentiment'] = df['review_rating'].apply(to_sentiment)","5ae7fa09":"df.head()","3e4f24c8":"(df.sentiment.value_counts() *100 \/ len(df), 2)\nsns.countplot(df.sentiment)\nplt.xlabel('Class Count')","a71774f2":"PRE_TRAINED_MODEL_NAME = 'bert-base-cased'","e5664950":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","9a3d9d7a":"sample_txt = 'Machine Learning is so much fun!! I love Machine Learning apps and practice it twice a week.'","46fcdb88":"tokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","522a0b6c":"tokenizer.sep_token, tokenizer.sep_token_id","1aaabcee":"tokenizer.cls_token, tokenizer.cls_token_id","3b091162":"tokenizer.pad_token, tokenizer.pad_token_id","29c0fac7":"tokenizer.unk_token, tokenizer.unk_token_id","3aaf02f4":"encoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  truncation=True,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\n\nencoding.keys()","246c633a":"print(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","f0af30d6":"print(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","390d7643":"tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])","2dd238a1":"token_lens = []\n\nfor txt in df.review_text:\n    tokens = tokenizer.encode(txt,truncation=True, max_length=512)\n    token_lens.append(len(tokens))","821c1e46":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count');","4bdefb96":"MAX_LEN = 160","27fbf670":"class ReviewsDataset(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    \n\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n          review,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          truncation=True,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          return_tensors='pt'\n    )\n        return {\n          'review_text': review,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","6c33c371":"sentiment_int = {'positive' : 0, 'neutral': 1,  'negative': 2}\ndf['sentiment_int'] = df.sentiment.map(sentiment_int)\ndf.head()","fa9a4afd":"df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\ndf_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)","918d7b2c":"df_train.shape, df_val.shape, df_test.shape","e723e086":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = ReviewsDataset(\n    reviews=df.review_text.to_numpy(),\n    targets=df.sentiment_int.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n\n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4\n    )","8f396538":"BATCH_SIZE = 16\n\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","3c04a790":"data = next(iter(train_data_loader))\ndata.keys()\n","11899d1a":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","05a0116b":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","14d508a8":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encoding['input_ids'], \n  attention_mask=encoding['attention_mask']\n)","79e65aff":"bert_model.config.hidden_size","f1c568fb":"class SentimentClassifier(nn.Module):\n\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n  \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n    )\n        output = self.drop(pooled_output)\n        return self.out(output)","e752202e":"model = SentimentClassifier(len(sentiment_int))\nmodel = model.to(device)","505f0432":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\n\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","2636eb08":"F.softmax(model(input_ids, attention_mask), dim=1)","5e6e8201":"EPOCHS = 5\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\n\nloss_fn = nn.NLLLoss().to(device)","9344c072":"def train_epoch(\n      model,\n      data_loader,\n      loss_fn,\n      optimizer,\n      device,\n      scheduler,\n      n_examples\n    ):\n    \n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:   \n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n    \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","c8558dd9":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    model = model.eval()\n\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:        \n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n              )\n            _, preds = torch.max(outputs, dim=1)\n\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","08da3e2b":"%%time\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    \n\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,    \n    loss_fn, \n    optimizer, \n    device, \n    scheduler, \n    len(df_train)\n  )\n\n    print(f'Train loss {train_loss}')\n\n    val_acc, val_loss = eval_model(\n    model,\n    val_data_loader,\n    loss_fn, \n    device, \n    len(df_val)\n  )\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","6aeebd8f":"plt.plot(history['train_loss'], label='train loss')\nplt.plot(history['val_loss'], label='validation loss')\n\nplt.title('Training history')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\n# plt.ylim([0, 1]);","3ff0a1a0":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\ntest_acc.item()","c42fdbea":"def get_predictions(model, data_loader):\n\n    model = model.eval()\n\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"review_text\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n    outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n    _, preds = torch.max(outputs, dim=1)\n\n    probs = F.softmax(outputs, dim=1)\n\n    review_texts.extend(texts)\n    predictions.extend(preds)\n    prediction_probs.extend(probs)\n    real_values.extend(targets)\n\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    return review_texts, predictions, prediction_probs, real_values","be170d4f":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","0deb4f93":"print(classification_report(y_test, y_pred))","1ad60274":"The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it'll be easier to evaluate the predictions from our model. Let's split the data:","a5d0a949":"Some basic operations can convert the text to tokens and tokens to unique integers (ids):","07a0a559":"As we see, we have a huge imbalance and this will affect the model for sure but we will try to see how `BERT` will deal with this imblance. ","757fba96":"Let's load a pre-trained [BertTokenizer](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#berttokenizer):","9530843a":"We'll define a helper function to get the predictions from our model:","ca1dca22":"All of that work can be done using the [`encode_plus()`](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus) method:","7484e004":"## Evaluation\n\nSo how good is our model on predicting sentiment? Let's start by calculating the accuracy on the test data:","fa0bfab7":"## Setup\n\nWe'll need [the Transformers library](https:\/\/huggingface.co\/transformers\/) by Hugging Face:","c6aa8c90":"The token ids are now stored in a Tensor and padded to a length of 32:","c25296d1":"Most of the reviews seem to contain less than 128 tokens, but we'll be on the safe side and choose a maximum length of 160.","345cfe4c":"We also need to create a couple of data loaders. Here's a helper function to do \n","2e053aca":"There is also a special token for padding:","7ea88077":"Lets see the class distrbution right now.","f438262c":"Now, I will combine the `1`, `2`, and `3` ratings in a class and keep the `4` and `5` classes as is. Lets call our new classes `negative`, `neutral`, and `positive`.","63cfa01e":"## Data Preprocessing\n\nYou might already know that Machine Learning models don't work with raw text. You need to convert text to numbers (of some sort). BERT requires even more attention (good one, right?). Here are the requirements: \n\n- Add special tokens to separate sentences and do classification\n- Pass sequences of constant length (introduce padding)\n- Create array of 0s (pad token) and 1s (real token) called *attention mask*\n\nThe Transformers library provides (you've guessed it) a wide variety of Transformer models (including BERT). It works with TensorFlow and PyTorch! It also includes prebuild tokenizers that do the heavy lifting for us!\n","cc0c1246":"The task will be using the column `reviews.text` to predict `reviews.rating`","462b420f":"`[CLS]` - we must add this token to the start of each sentence, so BERT knows \n","052cd6f6":"Let's have a look at the classification report","27e14920":"This is similar to the evaluation function, except that we're storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs):","ec4c1416":"We'll move the example batch of our training data to the GPU:","c16f6569":"# Sentiment Analysis with BERT\n\n> In this tutorial, you'll learn how to fine-tune BERT for sentiment analysis. You'll do the required text preprocessing (special tokens, padding, and attention masks) and build a Sentiment Classifier using the amazing Transformers library by Hugging Face!\n\n\n\nYou'll learn how to:\n\n- Preprocess text data for BERT and build PyTorch Dataset (tokenization, attention masks, and padding)\n- Use Transfer Learning to build Sentiment Classifier using the Transformers library by Hugging Face\n- Evaluate the model on test data\n- Predict sentiment on raw text\n\nLet's get started!","ed6639f8":"### Choosing Sequence Length\n\nBERT works with fixed-length sequences. We'll use a simple strategy to choose the max length. Let's store the token length of each review:","1ee4cbd6":"Let's have a look at an example batch from our training data loader:","3349bb30":"Now, the two columns are clean. Lets take a subset of the addressed columns\n","746f11d9":"To get the predicted probabilities from our trained model, we'll apply the softmax function to the outputs:","1c1ae328":"We'll use this text to understand the tokenization process:\n\n\n","d1291549":"Note that we're storing the state of the best model, indicated by the highest validation accuracy.\n\nWhoo, this took some time! We can look at the training vs validation accuracy:","f1281064":"Now, lets convert our target to a numeric representation","468820fd":"### Special Tokens\n\n`[SEP]` - marker for ending of a sentence\n","10031e86":"## What is BERT?\n\nBERT (introduced in [this paper](https:\/\/arxiv.org\/abs\/1810.04805)) stands for Bidirectional Encoder Representations from Transformers. If you don't know what most of that means - you've come to the right place! Let's unpack the main ideas:\n\n- Bidirectional - to understand the text  you're looking you'll have to look back (at the previous words) and forward (at the next words)\n- Transformers - The [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762) paper presented the Transformer model. The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words (e.g. `his` in a sentence refers to Jim).\n- (Pre-trained) contextualized word embeddings - [The ELMO paper](https:\/\/arxiv.org\/abs\/1802.05365v2) introduced a way to encode words based on their meaning\/context. Nails has multiple meanings - fingernails and metal nails.\n\nBERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence. Let's look at examples of these tasks:\n\n### Masked Language Modeling (Masked LM)\n\nThe objective of this task is to guess the masked tokens. Let's look at an example, and try to not make it harder than it has to be:\n\nThat's `[mask]` she `[mask]` -> That's what she said\n\n### Next Sentence Prediction (NSP)\n\nGiven a pair of two sentences, the task is to say whether or not the second follows the first (binary classification). Let's continue with the example:\n\n*Input* = `[CLS]` That's `[mask]` she `[mask]`. [SEP] Hahaha, nice! [SEP]\n\n*Label* = *IsNext*\n\n*Input* = `[CLS]` That's `[mask]` she `[mask]`. [SEP] Dwight, you ignorant `[mask]`! [SEP]\n\n*Label* = *NotNext*\n\nThe training corpus was comprised of two entries: [Toronto Book Corpus](https:\/\/arxiv.org\/abs\/1506.06724) (800M words) and English Wikipedia (2,500M words). While the original Transformer has an encoder (for reading the input) and a decoder (that makes the prediction), BERT uses only the decoder.\n\nBERT is simply a pre-trained stack of Transformer Encoders. How many Encoders? We have two versions - with 12 (BERT base) and 24 (BERT Large).\n\n### Is This Thing Useful in Practice?\n\nThe BERT paper was released along with [the source code](https:\/\/github.com\/google-research\/bert) and pre-trained models.\n\nThe best part is that you can do Transfer Learning (thanks to the ideas from OpenAI Transformer) with BERT for many NLP tasks - Classification, Question Answering, Entity Recognition, etc. You can train with small amounts of data and achieve great performance!","34e9d79b":"Now, lets view the dataset in its new format.","812ddf9b":"The attention mask has the same length:","800dbf35":"> You can use a cased and uncased version of BERT and tokenizer. I've experimented with both. The cased version works better. Intuitively, that makes sense, since \"BAD\" might convey more sentiment than \"bad\".","9a2cfc25":"## Import Packages","8861b461":"BERT understands tokens that were in the training set. Everything else can be encoded using the `[UNK]` (unknown) token:","b0a4f5bc":"We can use all of this knowledge to create a classifier that uses the BERT model:\n","fc92115b":"The `last_hidden_state` is a sequence of hidden states of the last layer of the \nmodel. Obtaining the `pooled_output` is done by applying the [BertPooler](https:\/\/github.com\/huggingface\/transformers\/blob\/edf0582c0be87b60f94f41c659ea779876efc7be\/src\/transformers\/modeling_bert.py#L426) on `last_hidden_state`:","72e440e4":"Our classifier delegates most of the heavy lifting to the BertModel. We use a dropout layer for some regularization and a fully-connected layer for our output. Note that we're returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.\n\nThis should work like any other PyTorch model. Let's create an instance and move it to the GPU:","b2b7f41e":"We will use the not null values only","36398556":"We can inverse the tokenization to have a look at the special tokens:","d70e719a":"Let's investigate the 3 files","04f26525":"We have all building blocks required to create a PyTorch dataset. Let's do it:","e75498df":"## Sentiment Classification with BERT and Hugging Face\n\nThere are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task you might want to use [BertForSequenceClassification](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertforsequenceclassification), [BertForQuestionAnswering](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertforquestionanswering) or something else. \n\nBut who cares, right? We're *hardcore*! We'll use the basic [BertModel](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertmodel) and build our sentiment classifier on top of it. Let's load the model:","f14f3e1c":"How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning:\n\n- Batch size: 16, 32\n- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n- Number of epochs: 2, 3, 4\n\nWe're going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy.\n\nLet's continue with writing a helper function for training our model for one epoch:","e5fb4b9c":"And try to use it on the encoding of our sample text:","919a5194":"### Training\n\nTo reproduce the training procedure from the BERT paper, we'll use the [AdamW](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#adamw) optimizer provided by Hugging Face. It corrects weight decay, so it's similar to the original paper. We'll also use a linear scheduler with no warmup steps:","febf54cc":"## Data Exploration\n","54a35f5a":"Lets continue working with `df2`. I will rename it to `df` for simplicity.","338e264e":"and plot the distribution:\n","475ce196":"Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We're avoiding exploding gradients by clipping the gradients of the model using [clip_grad_norm_](https:\/\/pytorch.org\/docs\/stable\/nn.html#clip-grad-norm).\n\nLet's write another one that helps us evaluate the model on a given data loader:"}}