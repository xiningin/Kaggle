{"cell_type":{"8961cd6c":"code","e9852042":"code","5f6419d0":"code","81d4d785":"code","c226a85e":"code","38a643db":"code","0cc1de65":"code","8abe404f":"code","5fb888af":"code","23a4f0e7":"code","65f11120":"code","37e66818":"code","6e7bce99":"code","195760e6":"code","def5217a":"code","ebe3a554":"code","d6ce620f":"code","db2ffb77":"code","a5248d69":"code","e02561b2":"markdown","42d4b882":"markdown","cdb00657":"markdown","d30b4131":"markdown","da964f5e":"markdown"},"source":{"8961cd6c":"import numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e9852042":"data = pd.read_csv(\"..\/input\/column_2C_weka.csv\",sep = \",\")\n","5f6419d0":"data.head()","81d4d785":"data.info()","c226a85e":"data.describe()","38a643db":"color_list = ['red' if i == \"Abnormal\" else 'green' for i in data.loc[: ,'class']]\npd.plotting.scatter_matrix(data.loc[:,data.columns != 'class'],\n                          c = color_list,\n                          figsize = [15,15],\n                          diagonal = 'hist',\n                          alpha = 0.5,\n                          s = 200,\n                          marker = '*',\n                          edgecolor = \"black\")\nplt.show()\n","0cc1de65":"sns.countplot(x = \"class\",data=data)\ndata.loc[:,'class'].value_counts()","8abe404f":"\nx  = data.loc[:,data.columns != 'class']\ny = data.loc[:,\"class\"]\n\n# x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\n\n\nprint(x.shape)\nprint(y.shape)\n\n","5fb888af":"from sklearn.model_selection import  train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3 , random_state = 1)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\n\nprint(\"{} knn score : {}\".format(3,knn.score(x_test,y_test)))\n\n\n","23a4f0e7":"print(len(x_train),len(y_train))","65f11120":"# model complexity\nneig = np.arange(1,25)\ntrain_accuracy = []\ntest_accuracy = []\n\nfor i ,k in enumerate(neig):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    train_accuracy.append(knn.score(x_train,y_train))\n    test_accuracy.append(knn.score(x_test,y_test))\n    \n#plot\n\nplt.figure(figsize = [13,8])\nplt.plot(neig,test_accuracy,label='testing accuracy')\nplt.plot(neig,train_accuracy,label='train accuracy')\nplt.legend()\nplt.title('values vs accuracy')\nplt.xlabel('number of neighbours')\nplt.ylabel('accuracy')\nplt.xticks(neig)\nplt.show()\nprint('best accuracy is {} with k = {}'.format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))\n\n\n\n","37e66818":"data.loc[:,\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data.loc[:,\"class\"]]\ndata.loc[:,\"class\"]","6e7bce99":"y = data.loc[:,\"class\"]\nx_data = data.drop([\"class\"],axis = 1)\n\nprint(x_data.shape)\nprint(y.shape)\n","195760e6":"x = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\nx\n","def5217a":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T.values.reshape(248,1)\ny_test = y_test.T.values.reshape(62,1)\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)\n\n","ebe3a554":"#parameter initialize and sigmoid function\n#dimension = 30\n\ndef initialize_weights_and_bias(dimension):\n    w =np.full((dimension,1),0.01)\n    #np.full i\u00e7ine ald\u0131\u011f\u0131 (x,y),z ile x e y boyutlu z lerden olu\u015fan bir arrray yapar\n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z)) #ayn\u0131 zamanda bu sigmoid fonksiyon \n    #np.exp e \u00fczeri demek\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z = np.dot(w.T,x_train)+b #?\n    #np.dot matrix \u00e7arp\u0131m\u0131nda kullan\u0131l\u0131yor\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1] #?\n    \n    #backward propagaiton\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\":derivative_bias}\n    return cost, gradients\n\n    \n    \n","d6ce620f":"#update\ndef update (w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    #updating(learning) parameters is number_of_iteration time\n    for i in range(number_of_iteration):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n    #lets upgrade\n    \n    w=w-learning_rate*gradients[\"derivative_weight\"]\n    b=b-learning_rate*gradients[\"derivative_bias\"]\n    \n    if i%10 == 0:\n        cost_list2.append(cost)\n        index.append(i)\n        print(\"cost after iteration %i:%f\" %(i,cost))\n        \n    #we update(learn) parameters weight and bias\n\n    parameters = {\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"number of iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    return parameters,gradients,cost_list\n","db2ffb77":"#prediction\ndef predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_predction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n        return y_prediction\n    ","a5248d69":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n   \n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    #print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, num_iterations = 150)","e02561b2":"**NORMAL\u0130ZAT\u0130ON**","42d4b882":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    dimension=x_train.shape[0]\n    w,b=initialize_weight_and_bias(dimension)\n    parameters,gradients,cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","cdb00657":"logistic regression","d30b4131":"**LINEAR REGRESSION**","da964f5e":"SUPPORT VECTOR MACHINE"}}