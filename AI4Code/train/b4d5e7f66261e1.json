{"cell_type":{"5e7d5b01":"code","04a1951c":"code","3e89598e":"code","d2caa35b":"code","801bf948":"code","1fcc65eb":"code","b4f85d5f":"code","48f2a735":"code","8859116e":"code","88bc9d3e":"code","e9cc78d7":"code","bf486bb6":"code","7201ce95":"code","0425fb71":"code","e7dd50de":"code","169afbcf":"code","b7b82995":"code","fff50d4a":"code","fa36bb3b":"code","006f5dad":"code","b61838fb":"code","1d058e45":"code","c5094d1e":"code","f2aca3b6":"code","08b059ee":"code","6ba1ee2e":"code","99f92135":"code","5b2ab8e3":"code","3dd224c4":"code","1d144575":"code","499cd6a3":"code","f807f993":"code","f723d5f8":"code","5e4a0875":"code","bdc0ebd5":"code","4566d984":"code","bdcfb04a":"code","0de41e1e":"code","9ab41df3":"markdown","3f644dc4":"markdown","d6ef00bd":"markdown","eab413dd":"markdown","c2406c2b":"markdown","ab3b7773":"markdown","bb04f303":"markdown","a98cc032":"markdown","d69e96fa":"markdown","b3cdd365":"markdown","408c4c97":"markdown","51225a8a":"markdown","cf2d5fe8":"markdown","f04d3542":"markdown","0b4ad4a1":"markdown","41876fc6":"markdown","f01752f8":"markdown","82854293":"markdown","580b7c82":"markdown","a05c45a0":"markdown","5b4a222d":"markdown","2d87650c":"markdown","f979e3bf":"markdown","7a498a16":"markdown","9d5c2088":"markdown","fda667d5":"markdown","36709a52":"markdown","1d31586a":"markdown","391e1d66":"markdown"},"source":{"5e7d5b01":"!pip install sweetviz","04a1951c":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport math\nimport random\nimport seaborn as sns\nimport sweetviz as sv\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport missingno as msno\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics, pipeline\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom time import time\n# models\nfrom sklearn.linear_model import LogisticRegression, LogisticRegression, Perceptron, RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3e89598e":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmit=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntrain.head()","d2caa35b":"train.isnull().sum()","801bf948":"msno.matrix(train)","1fcc65eb":"msno.matrix(test)","b4f85d5f":"dataset = [train,test]\n\nfor data in dataset:\n    # coplete missing age with median\n    data['Age'].fillna(data['Age'].median(),inplace = True)\n    \n    # complete Embarked with mode\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n    \n    # complete missing Fare with median\n    data['Fare'].fillna(data['Fare'].median(),inplace = True)","48f2a735":"print(\"Train info:\")\nprint(train.isnull().sum())\nprint()\nprint()\nprint(\"Test info:\")\nprint(test.isnull().sum())","8859116e":"train.drop(['Cabin','PassengerId'], axis=1, inplace = True)\ntest.drop(['Cabin','PassengerId'],axis=1,inplace=True)","88bc9d3e":"train.head()","e9cc78d7":"print(\"Train info:\")\nprint(train.isnull().sum())\nprint()\nprint()\nprint(\"Test info:\")\nprint(test.isnull().sum())","bf486bb6":"report=pp.ProfileReport(train)","7201ce95":"report","0425fb71":"#analyzing the dataset\nanalysis=sv.analyze(train)","e7dd50de":"analysis.show_html('train_analysis.html')","169afbcf":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","b7b82995":"numVar = [\"Fare\", \"Age\"]\nfor n in numVar:\n    plot_hist(n)","fff50d4a":"sns.pairplot(train)\nplt.show()","fa36bb3b":"sns.heatmap(train.corr(), annot = True, fmt = \".2f\")\nplt.show()","006f5dad":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","b61838fb":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", kind = \"bar\", data = train, size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","1d058e45":"g = sns.FacetGrid(train, col = \"Survived\",size=6)\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","c5094d1e":"train.drop(['Embarked'], axis=1, inplace = True)\ntest.drop(['Embarked'],axis=1,inplace=True)","f2aca3b6":"genders = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(genders)\ntest['Sex'] = test['Sex'].map(genders)\ntrain['Sex'].value_counts()","08b059ee":"for data in dataset:\n    data['Age'] = data['Age'].astype(int)\n    data.loc[ data['Age'] <= 15, 'Age'] = 0\n    data.loc[(data['Age'] > 15) & (data['Age'] <= 20), 'Age'] = 1\n    data.loc[(data['Age'] > 20) & (data['Age'] <= 26), 'Age'] = 2\n    data.loc[(data['Age'] > 26) & (data['Age'] <= 28), 'Age'] = 3\n    data.loc[(data['Age'] > 28) & (data['Age'] <= 35), 'Age'] = 4\n    data.loc[(data['Age'] > 35) & (data['Age'] <= 45), 'Age'] = 5\n    data.loc[ data['Age'] > 45, 'Age'] = 6\ntrain['Age'].value_counts()","6ba1ee2e":"train.head()","99f92135":"for data in dataset:\n    data['Family'] = data['SibSp'] + data['Parch'] + 1\ntrain.head()    ","5b2ab8e3":"for data in dataset:\n    drop_column = ['Fare','Name','Ticket','SibSp','Parch']\n    data.drop(drop_column, axis=1, inplace = True)","3dd224c4":"train.head()","1d144575":"test.head()","499cd6a3":"X_train,X_val,y_train,y_val=train_test_split(train.iloc[:,1:],train['Survived'],test_size=0.2,random_state=2)","f807f993":"def acc_summary(pipeline, X_train, y_train, X_val, y_val):\n    t0 = time()\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_val)\n    train_test_time = time() - t0\n    accuracy = accuracy_score(y_val, y_pred)*100\n    print(\"accuracy : {0:.2f}%\".format(accuracy))\n    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n    print(\"-\"*80)\n    return accuracy, train_test_time\n","f723d5f8":"names = [ \n        'Logistic Regression',\n        'Ridge Classifier',\n        'SGD Classifier',\n        'SVC',\n        'Gradient Boosting Classifier', \n        'Extra Trees Classifier', \n        \"Bagging Classifier\",\n        \"AdaBoost Classifier\", \n        \"K Nearest Neighbour Classifier\",\n         \"Decison Tree Classifier\",\n         \"Random Forest Classifier\",\n         'GaussianNB',\n        \"Gaussian Process Classifier\",\n        \"MLP Classifier\",\n        \"XGB Classifier\",\n        \"LGBM Classifier\"\n         ]\nclassifiers = [\n    LogisticRegression(),\n    RidgeClassifier(),\n    SGDClassifier(),\n    SVC(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(), \n    BaggingClassifier(),\n    AdaBoostClassifier(),\n    KNeighborsClassifier(n_neighbors=3),\n    DecisionTreeClassifier(max_depth=3),\n    RandomForestClassifier(n_estimators=100),\n    GaussianNB(),\n    GaussianProcessClassifier(),\n    MLPClassifier(),\n    XGBClassifier(booster= 'dart', max_depth=2,n_estimators=500),\n    LGBMClassifier()\n        ]\n\nzipped_clf = zip(names,classifiers)","5e4a0875":"def classifier_comparator(X_train,y_train,X_val,y_val,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([\n            ('classifier', c)\n        ])\n        print(\"Validation result for {}\".format(n))\n        print(c)\n        clf_acc,tt_time = acc_summary(checker_pipeline,X_train, y_train, X_val, y_val)\n        result.append((n,clf_acc,tt_time))\n    return result","bdc0ebd5":"classifier_comparator(X_train,y_train,X_val,y_val)","4566d984":"model=RandomForestClassifier(n_estimators=200)\nmodel.fit(train.iloc[:,1:],train['Survived'])\ny_pred=model.predict(test)","bdcfb04a":"submit['Survived']=y_pred","0de41e1e":"#submit.to_csv('submission_file.csv',index=False)","9ab41df3":"## Check the output files for the eda report.","3f644dc4":"## Features Survival Statistics","d6ef00bd":"# Feature Selection and model building.","eab413dd":"# Importing The Required Libraries.","c2406c2b":"# Thankyou For Reading and Upvote If you Liked my notebook !!!","ab3b7773":"### Selecting Random Forest for final prediction.","bb04f303":"# Judgement Time,all eyes on last two classifiers.","a98cc032":"### Parch & Survived","d69e96fa":"- age <= 10 has a high survival rate,\n- oldest passengers (80) survived,\n- large number of 20 years old did not survive,\n- most passengers are in 15-35 age range,","b3cdd365":"The Cabin column has more than 75% missing values,so we are going to drop that column along with passenger id column because it is not needed.","408c4c97":"# EDA using Sweetviz","51225a8a":"### Age & Survived","cf2d5fe8":"### Plotting Correlation Heatmap","f04d3542":"### Creating new feature Family Size as a combination of SibSp and Parch","0b4ad4a1":"# Splitting The data into train and test set.","41876fc6":"- Having a lot of SibSp have less chance to survive.\n- if sibsp == 0 or 1 or 2, passenger has more chance to survive","f01752f8":"We can see that Age, Embarked and cabin  columns have a lot of missing values. now, lets check missing values for test data.","82854293":"### Lets Deal with these missing values first. ","580b7c82":"### SibSp & Survived","a05c45a0":"# Creating submission file.","5b4a222d":"We can use Embarked as feature here for getting high accuracy but logically its doesn't matter. so we drop it out.","2d87650c":"## Creating a Custom function to compare accuracies of all models.","f979e3bf":"# EDA using Pandas Profiling and then seaborn.","7a498a16":"## Lets see the number of missing values in the data.","9d5c2088":"## EDA of Numeric Variables.","fda667d5":"## Final check of missing values.","36709a52":"cabin, age and fare columns have a lot of missing values in test data.","1d31586a":"As we created new fetures form existing one, so we remove the old ones.\n\nDropping SibSp & Parch because we have family now. same way Age. We also going to remove some other features like  Ticket number and Name.","391e1d66":"### Final check of train and test set."}}