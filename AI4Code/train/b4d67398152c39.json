{"cell_type":{"15cb34ed":"code","78ce56bc":"code","aa5ddd11":"code","6b17c5dd":"code","9d8aff38":"code","6cf65599":"code","e25b4e7f":"code","01bae638":"code","bd0ec630":"code","39d34c01":"code","0a99d786":"code","e23332bb":"code","6643654d":"code","6659b9c2":"code","8691e765":"code","97468862":"code","e89ee175":"code","54e8f420":"code","ff9e5c9f":"code","b11ff889":"code","53e76481":"code","22acf39f":"code","4a9ff571":"code","b3a23a62":"code","10fcd85d":"code","b66fd28e":"code","04101028":"code","a656c54e":"code","533fdc2f":"code","4c848771":"markdown","0eb1668b":"markdown","fbcf5de0":"markdown","da4ecc4c":"markdown","d5b628a0":"markdown","e80146ca":"markdown","2a09ed2a":"markdown","3a5ec932":"markdown","c8a613a6":"markdown","ad28b5d8":"markdown","7cc7af6e":"markdown","09924a74":"markdown","a5b1068d":"markdown","ade5a11a":"markdown"},"source":{"15cb34ed":"!pip install --upgrade efficientnet-pytorch","78ce56bc":"from efficientnet_pytorch import EfficientNet\nimport numpy as np \nimport pandas as pd\nimport shutil\nimport os\nimport zipfile\nimport torch\nimport random\nfrom torch import nn, optim\nimport csv\nimport torch.nn as nn\nimport cv2\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import transforms\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image\n\nimport albumentations\nfrom albumentations import pytorch as AT\n\n%matplotlib inline","aa5ddd11":"pretrained=False\n\nbatch_size = 200\nnum_workers = 0\nimg_size = 256\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n!mkdir predictions","6b17c5dd":"from google.colab import drive\ndrive.mount('\/content\/drive')","9d8aff38":"with zipfile.ZipFile('\/content\/drive\/My Drive\/diag-22-02-2020.zip', 'r') as zip_ref:\n    zip_ref.extractall('\/content')\nprint('ds_1 complete.')\n\nwith zipfile.ZipFile('\/content\/drive\/My Drive\/diag_2.zip', 'r') as zip_ref:\n    zip_ref.extractall('\/content')\nprint('ds_2 complete.')\n\nwith zipfile.ZipFile('\/content\/drive\/My Drive\/test_diagrams.zip', 'r') as zip_ref:\n    zip_ref.extractall('\/content\/diagram_test')\nprint('ds_3 complete.')","6cf65599":"train_dir_1 = 'train_data\/'\ntrain_dir_2 = 'All1\/'\n\ntest_dir = 'diagram_test\/test\/'\n\ntrain_files_1 = os.listdir(train_dir_1)\ntrain_files_2 = os.listdir(train_dir_2)\n\ntest_files = os.listdir(test_dir)\nprint(len(train_files_1), len(train_files_2), len(test_files))","e25b4e7f":"class DiagramDataset(Dataset):\n    def __init__(self, file_list, dir, transform, classes, mode='train'):\n        self.file_list = file_list\n        self.dir = dir\n        self.mode = mode\n        self.transform = transform\n        self.classes=classes\n\n    def __len__(self):\n        return len(self.file_list)\n\n    #\u043c\u0435\u0442\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0430\u043c \u0438\u043d\u0434\u0435\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n    def __getitem__(self, idx):\n        image = cv2.imread(os.path.join(self.dir, self.file_list[idx]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        if self.mode == 'train':\n            for label, name in enumerate(self.classes):\n                if name in self.file_list[idx]:\n                    return image, label\n        else:\n            return image, self.file_list[idx]","01bae638":"data_transforms_1 = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.HorizontalFlip(),\n    albumentations.CenterCrop(244,232),\n    albumentations.ChannelShuffle(),\n    albumentations.ShiftScaleRotate(rotate_limit=25, scale_limit=0.25),\n    albumentations.Normalize(),\n    AT.ToTensor()\n])\ndata_transforms_2 = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.HorizontalFlip(),\n    albumentations.VerticalFlip(),\n    albumentations.HueSaturationValue(),\n    # albumentations.RandomBrightnessContrast(),\n    albumentations.RandomCrop(244,232),\n    albumentations.ChannelShuffle(),\n    albumentations.RGBShift(),\n    # albumentations.ToGray(p=0.3),\n    albumentations.Normalize(),\n    AT.ToTensor()\n])\n\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.ChannelShuffle(),\n    albumentations.HorizontalFlip(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n])","bd0ec630":"my_classes = ['other','histogram','rose','block','linear','growth','circle', 'table']\nclasses_2 = ['just_image','bar_chart','diagram','flow_chart','graph','growth_chart','pie_chart', 'table']\n\ntrainset_1 = DiagramDataset(train_files_1, train_dir_1, classes=my_classes, mode='train', transform = data_transforms_1)\ntrainset_2 = DiagramDataset(train_files_2, train_dir_2, classes=classes_2, mode='train', transform = data_transforms_1)\ntrainset_3 = DiagramDataset(train_files_1, train_dir_1, classes=my_classes, mode='train', transform = data_transforms_2)\ntrainset_4 = DiagramDataset(train_files_2, train_dir_2, classes=classes_2, mode='train', transform = data_transforms_2)\n\ntestset = DiagramDataset(test_files, test_dir, classes=[], mode='test', transform=data_transforms_test)","39d34c01":"valid_size_1 = int(len(train_files_1) * 0.15)\nvalid_size_2 = int(len(train_files_2) * 0.12)\n\n#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0434\u043b\u044f \u0442\u0440\u0435\u0439\u043d \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\nindices = torch.randperm(len(trainset_1))\ntrain_indices_1 = indices[:len(indices)-valid_size_1]\nvalid_indices_1 = indices[len(indices)-valid_size_1:]\n\nindices = torch.randperm(len(trainset_2))\ntrain_indices_2 = indices[:len(indices)-valid_size_2]\nvalid_indices_2 = indices[len(indices)-valid_size_2:]\n\nindices = torch.randperm(len(trainset_3))\ntrain_indices_3 = indices[:len(indices)-valid_size_1]\nvalid_indices_3 = indices[len(indices)-valid_size_1:]\n\nindices = torch.randperm(len(trainset_4))\ntrain_indices_4 = indices[:len(indices)-valid_size_2]\nvalid_indices_4 = indices[len(indices)-valid_size_2:]\n\n\ntrainloader_1 = torch.utils.data.DataLoader(trainset_1, pin_memory=False, \n                                        batch_size=batch_size,\n                                        sampler=SubsetRandomSampler(train_indices_1))\n# trainloader_2 = torch.utils.data.DataLoader(trainset_2, pin_memory=False, \n#                                         batch_size=batch_size,\n#                                         sampler=SubsetRandomSampler(train_indices_2))\ntrainloader_3 = torch.utils.data.DataLoader(trainset_3, pin_memory=False, \n                                        batch_size=batch_size,\n                                        sampler=SubsetRandomSampler(train_indices_3))\n# trainloader_4 = torch.utils.data.DataLoader(trainset_4, pin_memory=False, \n#                                         batch_size=batch_size,\n#                                         sampler=SubsetRandomSampler(train_indices_4))\n\n\nvalidloader_1 = torch.utils.data.DataLoader(trainset_1, pin_memory=True, \n                                        batch_size=batch_size,\n                                        sampler=SubsetRandomSampler(valid_indices_1))\n# validloader_2 = torch.utils.data.DataLoader(trainset_2, pin_memory=True, \n#                                         batch_size=batch_size,\n#                                         sampler=SubsetRandomSampler(valid_indices_2))\nvalidloader_3 = torch.utils.data.DataLoader(trainset_3, pin_memory=True, \n                                        batch_size=batch_size,\n                                        sampler=SubsetRandomSampler(valid_indices_3))\n# validloader_4 = torch.utils.data.DataLoader(trainset_4, pin_memory=True, \n#                                         batch_size=batch_size,\n#                                         sampler=SubsetRandomSampler(valid_indices_4))\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)","0a99d786":"def validate(model, validloader, valid_size):\n    correct = 0\n    with torch.no_grad():\n        for data, target in validloader:\n            data = data.to(device=device)\n            target = target.to(device=device)\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == target).sum().item()\n\n    return correct \/ valid_size","e23332bb":"def train_model(model, model_name, trainloader, validloader, valid_size, criterion, optimizer, scheduler, n_epoch, default_scheduler=True):\n    acc_max = 0\n    for epoch in range(n_epoch):\n        model.train()\n        train_loss = []\n        train_losses = []\n        for data, target in tqdm(trainloader):\n            data = data.to(device=device)\n            target = target.to(device=device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()       \n            optimizer.step()\n            train_loss.append(loss.item())\n        if default_scheduler:\n            scheduler.step(np.mean(train_loss))\n        else:\n            scheduler.step()\n        print(f'epoch #{epoch+1}:', loss.item())\n        train_losses.append(np.mean(train_loss))\n\n        model.eval()\n        correct = 0\n\n        acc = validate(model, validloader, valid_size)\n\n        if acc >= acc_max:\n            print('Saving model...')\n            torch.save(model.state_dict(), f'{model_name}.pt')\n            acc_max = acc\n        print('validation accuracy:', acc)\n\n    return model","6643654d":"def make_predict(model, model_name, testloader):\n    try:\n        model.load_state_dict(torch.load(f'{model_name}.pt'))\n    except:\n        model.load_state_dict(torch.load(f'\/content\/drive\/My Drive\/{model_name}.pt'))\n    answer = {}\n    with torch.no_grad():\n        model.eval()\n        for samples, name in testloader:\n            data = samples.to(device=device)\n            outputs = model(data)\n            _, predicted = torch.max(outputs.data, 1)\n            for r in zip(name, predicted):\n                answer.update({r[0]:int(r[1])})\n    return answer","6659b9c2":"def save_predict(predict, model_name='model'):\n    if predict.get('image_name'):\n        del predict['image_name']\n    with open(f'predictions\/{model_name}.csv', 'w', newline='') as csvfile:\n        fieldnames = ['image_name', 'label']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writerow({'image_name': 'image_name', 'label': 'label'})\n        for k,v in predict.items():\n            writer.writerow({'image_name': k, 'label': int(v)})","8691e765":"def cmp(predict_1, predict_2, diff=False):\n    t=0\n    for k in predict_1.keys():\n        if str(predict_1[k]) != str(predict_2[k]):\n            if(diff):\n                print(k, predict_1[k], predict_2[k])\n            t+=1\n    acc = 1-t\/float(len(predict_1.keys()))\n    return acc","97468862":"predict_98 = {}\nwith open('drive\/My Drive\/answers.csv', newline='') as csvfile:\n    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n    for k,v in spamreader:\n        predict_98.update({k:v})","e89ee175":"model_1 = torchvision.models.resnet50(pretrained=True)\n\n# model_1 = EfficientNet.from_pretrained(\"efficientnet-b7\", advprop=False)\nfor param in model_1.parameters():\n    param.requires_grad = False\n\n# in_ = model_1._fc.in_features\n# model_1._fc = nn.Linear(in_, 8)\n\nmodel_1.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\nin_ = model_1.fc.in_features\nmodel_1.fc = nn.Linear(in_features=in_, out_features=8, bias=True)\nmodel_1.to(device)\n\nmodel_name='resnet_1'\nlr = 0.01\nn_epoch = 50\ncriterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.RMSprop(model_1.parameters(), lr=lr, alpha=0.33, eps=1e-4, weight_decay=1e-6, centered=True)\noptimizer = torch.optim.RMSprop(model_1.parameters(), lr=lr, centered=True)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=0)\nif pretrained:\n    try:\n        model_1.load_state_dict(torch.load(f'\/content\/drive\/My Drive\/{model_name}.pt'))\n    except:\n        model_1.load_state_dict(torch.load(f'{model_name}.pt'))\n    else:\n        model_1 = train_model(model_1, model_name, trainloader_3, validloader_3, valid_size_1, criterion, optimizer, scheduler, n_epoch)\n# model_1 = torchvision.models.densenet161(pretrained=True)\n\n# ct = 0\n# for old in model_1.children():\n#   for but in old.children():\n#     ct+=1\n#     if ct < 10:\n#       for gold in but.parameters():\n#         gold.requires_grad = False\n\n# model_1.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n# in_ = model_1.classifier.in_features\n# model_1.classifier = nn.Linear(in_features=in_, out_features=8, bias=True)\n# model_1.to(device)\n\n# model_name='dense_3'\n\n# lr = 0.001\n# n_epoch = 25\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.SGD(model_1.parameters(), lr=lr, momentum=0.7)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.42, patience=0)\n\n# if pretrained:\n#   try:\n#     model_1.load_state_dict(torch.load(f'\/content\/drive\/My Drive\/{model_name}.pt'))\n#   except:\n#     model_1.load_state_dict(torch.load(f'{model_name}.pt'))\n# else:\n#   model_1 = train_model(model_1, model_name, trainloader_3, validloader_3, valid_size_1, criterion, optimizer, scheduler, n_epoch)","54e8f420":"predict_1 = make_predict(model_1, model_name, testloader)\nsave_predict(predict_1, model_name)\ncmp(predict_1, predict_98, diff=False)","ff9e5c9f":"model_2 = torchvision.models.densenet161(pretrained=True)\n\nct = 0\nfor old in model_2.children():\n    for but in old.children():\n        ct+=1\n        if ct < 10:\n            for gold in but.parameters():\n                gold.requires_grad = False\n\nmodel_2.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\nin_ = model_2.classifier.in_features\nmodel_2.classifier = nn.Linear(in_features=in_, out_features=8, bias=True)\nmodel_2.to(device)\n\nmodel_name='dense_1'\n\nlr = 0.001\nn_epoch = 25\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.RMSprop(model_2.parameters(), lr=lr, centered=True)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.228, patience=1)\n\nif pretrained:\n    try:\n        model_2.load_state_dict(torch.load(f'\/content\/drive\/My Drive\/{model_name}.pt'))\n    except:\n        model_2.load_state_dict(torch.load(f'{model_name}.pt'))\nelse:\n    model_2 = train_model(model_2, model_name, trainloader_3, validloader_3, valid_size_1, criterion, optimizer, scheduler, n_epoch)","b11ff889":"predict_2 = make_predict(model_2, model_name, testloader)\nsave_predict(predict_2, model_name)\ncmp(predict_2, predict_98, diff=False)","53e76481":"model_3 = torchvision.models.densenet161(pretrained=True)\nfor param in model_3.parameters():\n    param.requires_grad = False\n\nmodel_3.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\nin_ = model_3.classifier.in_features\nmodel_3.classifier = nn.Linear(in_features=in_, out_features=8, bias=True)\n\nmodel_3.to(device)\n\nmodel_name='dense_2'\n\n# lr = 0.1\n# n_epoch = 20\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.SGD(model_3.parameters(), lr=lr, momentum=0.9)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=0)\n\nlr = 0.001\nn_epoch = 25\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.RMSprop(model_2.parameters(), lr=lr, weight_decay=1e-5, centered=True)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.333, patience=1)\n\nif pretrained:\n    try:\n        model_3.load_state_dict(torch.load(f'\/content\/drive\/My Drive\/{model_name}.pt'))\n    except:\n        model_3.load_state_dict(torch.load(f'{model_name}.pt'))\nelse:\n    model_3 = train_model(model_2, model_name, trainloader_2, validloader_2, valid_size_2, criterion, optimizer, scheduler, n_epoch)","22acf39f":"predict_3 = make_predict(model_3, model_name, testloader)\nsave_predict(predict_3, model_name)\ncmp(predict_3, predict_98, diff=False)","4a9ff571":"model_4 = torchvision.models.resnet50(pretrained=True)\nfor param in model_4.parameters():\n    param.requires_grad = False\n\nmodel_4.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\nin_ = model_4.fc.in_features\nmodel_4.fc = nn.Linear(in_features=in_, out_features=8, bias=True)\nmodel_4.to(device)\n\nmodel_name='resnet_2'\n# lr = 0.01\n# n_epoch = 20\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.SGD(model_4.parameters(), lr=lr, weight_decay=1e-6, momentum=0.8)\n# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, lr*1e-5, lr*100, 5)\nlr = 0.01\nn_epoch = 25\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_4.parameters(), lr, weight_decay=1e-6, amsgrad=True)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n\nif pretrained:\n    try:\n    model_4.load_state_dict(torch.load(f'\/content\/drive\/My Drive\/{model_name}.pt'))\n    except:\n    model_4.load_state_dict(torch.load(f'{model_name}.pt'))\nelse:\n    model_4 = train_model(model_4, model_name, trainloader_4, validloader_4, valid_size_2, criterion, optimizer, scheduler, n_epoch)","b3a23a62":"predict_4 = make_predict(model_4, model_name, testloader)\nsave_predict(predict_4, model_name)\ncmp(predict_4, predict_98, diff=False)","10fcd85d":"\nfor k in predict_1.keys():\n    s = [0,0,0,0,0,0,0,0]\n    for predict in [predict_1, predict_2, predict_3, predict_4]:\n        s[int(predict[k])]+=1\n    general[k] = np.argmax(s)\nsave_predict(general, model_name='final')","b66fd28e":"import matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.metrics import confusion_matrix","04101028":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    print(cm)\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, float(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a656c54e":"def draw_confusion_matrix(predict):\n    cm = torch.zeros(8, 8, dtype=torch.int32)\n    for key in predict_98.keys():\n        if key == 'image_name':\n            continue\n        cm[int(predict_98[key]), int(predict[key])] += 1\n\n    font = {'size' : 15}\n    plt.rc('font', **font)\n    plt.figure(figsize=(8, 8))\n    plot_confusion_matrix(cm, my_classes)","533fdc2f":"draw_confusion_matrix(predict_1)\ndraw_confusion_matrix(predict_2)\ndraw_confusion_matrix(predict_4)","4c848771":"## VotingClassifier hard","0eb1668b":"## densenet161, dataset 2","fbcf5de0":"## Loading data\n","da4ecc4c":"# Functions:","d5b628a0":"## Creating datasets","e80146ca":"# Start","2a09ed2a":"# Models:","3a5ec932":"## resnet50, dataset 1","c8a613a6":"## resnet50, dataset 2","ad28b5d8":"# Confusion matrix","7cc7af6e":"## densenet161, dataset 1","09924a74":"# Ensemble:","a5b1068d":"# Datasets","ade5a11a":"## VotingClassifier soft \n### (not realised)"}}