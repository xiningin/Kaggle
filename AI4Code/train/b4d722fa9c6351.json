{"cell_type":{"a3752428":"code","4d4dbe74":"code","13f2e414":"code","9ed04795":"code","d639c7a5":"code","ce1186c8":"code","893d24a1":"code","e36c418b":"code","508e3ff3":"code","9f41cc7d":"code","dfee7903":"code","8cd2fd3d":"code","54bf1375":"code","eaa63e26":"code","9c910254":"code","89660d25":"code","e54bcf10":"code","3031ad91":"code","57f08798":"code","72ed4dc9":"code","3adf1dfa":"code","22075947":"code","f46a3d18":"code","261e5e9e":"code","0815cd10":"code","9c474547":"code","a7439712":"code","e2ca1dde":"code","eb55ebf7":"code","a4f806e1":"code","034a393c":"code","ca30f1b8":"code","6b2d76a8":"code","bbc641ca":"code","fceaa21e":"code","67fe418f":"code","420b0477":"code","d89de80f":"code","ea946f49":"code","07d67b02":"code","8d1e357a":"code","60be6dd6":"code","59901d85":"code","9fc404cf":"code","c32bd99d":"code","20ea4f76":"code","d4b12ff3":"code","2494eac2":"code","df326df4":"code","583fe1b7":"code","3260dbff":"code","067f55db":"code","d42f9486":"code","59d49ff9":"code","bb521068":"code","d8081df7":"code","b1e12449":"code","52bcd507":"code","5aaea69a":"code","3db2d8fc":"markdown","0d1f0bde":"markdown","3106738c":"markdown","a7d7d3ab":"markdown","418219fb":"markdown","0a722eaf":"markdown","7f71a1a9":"markdown","2184238e":"markdown","caccb0c3":"markdown","5abd390d":"markdown","4f7ff35d":"markdown","bb39d4d9":"markdown","f0abd67d":"markdown","90ab2814":"markdown","bb1b77a8":"markdown","abe57d4b":"markdown","a1534681":"markdown","feb9b6e4":"markdown","04258636":"markdown","24c373b1":"markdown","7df299c4":"markdown","0fc3cbf0":"markdown","abd5b48e":"markdown","a7c6fc78":"markdown","080b5758":"markdown","1aa3712e":"markdown","d1c13efd":"markdown","91784693":"markdown","b212874f":"markdown","65b62980":"markdown","e176693b":"markdown","a50f53b4":"markdown","b1b4a872":"markdown","54223b5a":"markdown","6fa951f7":"markdown","7d2a8e12":"markdown","f77bee0b":"markdown","895b213e":"markdown","f2d14ed2":"markdown","2bd31e9c":"markdown","7dcd2956":"markdown","74a8969c":"markdown","4a97b46f":"markdown"},"source":{"a3752428":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d4dbe74":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt","13f2e414":"dataset = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","9ed04795":"dataset","d639c7a5":"dataset.info()","ce1186c8":"dataset.isnull().sum()","893d24a1":"dataset.describe()","e36c418b":"dataset.corr()","508e3ff3":"# Compute the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","9f41cc7d":"fig = px.violin(dataset, y= 'Age')\nfig.show()","dfee7903":"fig = px.violin(dataset, x=\"Age\", y=\"Gender\", orientation= 'h')\nfig.show()","8cd2fd3d":"fig = px.violin(dataset, x=\"Age\", y=\"Geography\", orientation= 'h')\nfig.show()","54bf1375":"fig = px.violin(dataset, x=\"Geography\", y=\"EstimatedSalary\", color = 'Gender', violinmode='overlay', hover_data=dataset.columns)\nfig.show()","eaa63e26":"fig = px.violin(dataset, x=\"Geography\", y=\"Balance\", color = 'Gender', violinmode='overlay', hover_data=dataset.columns)\nfig.show()","9c910254":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.EstimatedSalary, color=\"green\", label=\"Estimated Salary\", kde= True)\nplt.legend()","89660d25":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.Balance, color=\"blue\", label=\"Estimated Salary\", kde= True)\nplt.legend()","e54bcf10":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.CreditScore, color=\"red\", label=\"Estimated Salary\", kde= True)\nplt.legend()","3031ad91":"sns.pairplot(dataset)","57f08798":"x = dataset.iloc[:, 3:-1].values\ny = dataset.iloc[:, -1].values","72ed4dc9":"x","3adf1dfa":"y","22075947":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx[:, 2] = le.fit_transform(x[:, 2])","f46a3d18":"x","261e5e9e":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))","0815cd10":"x","9c474547":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","a7439712":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","e2ca1dde":"from keras.regularizers import l2","eb55ebf7":"ann = tf.keras.models.Sequential()","a4f806e1":"ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))","034a393c":"tf.keras.layers.Dropout(0.6)","ca30f1b8":"ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))","6b2d76a8":"tf.keras.layers.Dropout(0.6)","bbc641ca":"ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))","fceaa21e":"ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])","67fe418f":"ann_history = ann.fit(x_train, y_train, batch_size= 32, epochs= 100, validation_split= 0.3)","420b0477":"loss_train = ann_history.history['loss']\nloss_val = ann_history.history['val_loss']\nepochs = range(1,101)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d89de80f":"loss_train = ann_history.history['accuracy']\nloss_val = ann_history.history['val_accuracy']\nepochs = range(1,101)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","ea946f49":"from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay","07d67b02":"# Predicting the Test set results\ny_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate the Accuracy\naccuracy = accuracy_score(y_pred,y_test)","8d1e357a":"cm","60be6dd6":"accuracy","59901d85":"#Predicting on new data\nprint(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)","9fc404cf":"cmd = ConfusionMatrixDisplay(cm, display_labels=['Stay','Leave'])\ncmd.plot()","c32bd99d":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score","20ea4f76":"# Builing the function\ndef ann_classifier():\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    tf.keras.layers.Dropout(0.6)\n    ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\n    ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return ann","d4b12ff3":"# Passing values to KerasClassifier \nann = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 100)","2494eac2":"# We are using 5 fold cross validation here\naccuracies = cross_val_score(estimator = ann, X = x_train, y = y_train, cv = 5)","df326df4":"# Checking the mean and standard deviation of the accuracies obtained\nmean = accuracies.mean()\nstd_deviation = accuracies.std()\nprint(\"Accuracy: {:.2f} %\".format(mean*100))\nprint(\"Standard Deviation: {:.2f} %\".format(std_deviation*100))","583fe1b7":"from sklearn.model_selection import GridSearchCV","3260dbff":"# Builing the function\ndef ann_classifier(optimizer = 'adam'):\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    tf.keras.layers.Dropout(0.6)\n    ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\n    ann.compile(optimizer= optimizer, loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return ann","067f55db":"# Passing values to KerasClassifier \nann = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 100)","d42f9486":"# Using Grid Search CV to getting the best parameters\nparameters = {'batch_size': [25, 32],\n             'epochs': [100, 150],\n             'optimizer': ['adam', 'rmsprop']}\n\ngrid_search = GridSearchCV(estimator = ann, param_grid = parameters, scoring = 'accuracy', cv = 5, n_jobs = -1)\n\ngrid_search.fit(x_train, y_train)","59d49ff9":"best_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_","bb521068":"print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","d8081df7":"# defining the layers\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\nann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\nann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\nann.fit(x_train, y_train, batch_size= 32, epochs= 150)","b1e12449":"# Predicting the Test set results\ny_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate the Accuracy\naccuracy = accuracy_score(y_pred,y_test)","52bcd507":"print('Confusion Matrix after tuning the ANN:\\n', cm)","5aaea69a":"print('Accuracy after tuning the ANN:', (accuracy)*100,'%')","3db2d8fc":"# **Feature Scaling**","0d1f0bde":"**Adding the output layer**","3106738c":"**Distribution Plot**","a7d7d3ab":"**Label Encoding the \"Gender\" column**","418219fb":"# **Running the ANN again based on parameters obtained above**","0a722eaf":"***Therefore, after tuning the ANN model, accuracy reaches 86.3% and increases by only 1%.***","7f71a1a9":"**Geography vs Balance**","2184238e":"**Displaying info of dataset.**","caccb0c3":"# **Conclusion**","5abd390d":"**Adding the input layer and the first hidden layer**","4f7ff35d":"**Plotting with Age**","bb39d4d9":"**Check if there are any NULL values present.**","f0abd67d":"**Heat Map Correlation**","90ab2814":"# **Importing Libraries**","bb1b77a8":"**Adding first Dropout layer**","abe57d4b":"# **Data Preprocessing**","a1534681":"**Adding the second hidden layer**","feb9b6e4":"# **Importing Dataset**","04258636":"**Age vs Gender**","24c373b1":"# **Training the ANN**","7df299c4":"# **Building the ANN**","0fc3cbf0":"**Age vs Geography**","abd5b48e":"# **Data Visualization**","a7c6fc78":"# **Splitting the dataset into the Training set and Test set**","080b5758":"**Initializing the ANN**","1aa3712e":"**Performing the Cross Validation**","d1c13efd":"**Plotting with Geography**","91784693":"# **Visualizing Confusion Matrix**","b212874f":"# **Visualizing Training and Validation Accuracy**","65b62980":"# **Encoding Categorical Data**","e176693b":"# **Visualizing Training and Validation Loss**","a50f53b4":"**Geography vs Estimated Salary**","b1b4a872":"**One Hot Encoding the \"Geography\" column**","54223b5a":"**Wrapping k-fold cross validation into keras model**","6fa951f7":"**Adding second Dropout layer**","7d2a8e12":"**We use the Grid Search method for this task**","f77bee0b":"# **What is Churn Model?**","895b213e":"**Importing Regularizers to add a penalty for weight size to the loss function and avoiding overfitting**","f2d14ed2":"**Pair Plot**","2bd31e9c":"# **Tuning the ANN**","7dcd2956":"**Check correlation in your dataset.**","74a8969c":"# **Evluating the ANN (Cross Validation)**","4a97b46f":"*A churn model is a mathematical representation of how churn impacts your business. Churn calculations are built on existing data (the number of customers who left your service during a given time period). A predictive churn model extrapolates on this data to show future potential churn rates.*\n\n*In its simplest form, churn rate is calculated by dividing the number of customer cancellations within a time period by the number of active customers at the start of that period. Very valuable insights can be gathered from this simple analysis \u2014 for example, the overall churn rate can provide a benchmark against which to measure the impact of a model. And knowing how churn rate varies by time of the week or month, product line, or customer cohort can help inform simple customer segments for targeting as well.*"}}