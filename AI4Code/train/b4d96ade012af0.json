{"cell_type":{"514bfd04":"code","692d2939":"code","a90e72e2":"code","a10ec5e9":"code","61314565":"code","b9eb2032":"code","74153e7e":"code","3e26cd38":"code","22a51ecc":"code","305f8021":"code","70e63c96":"code","b7adabda":"code","eb39a1a1":"code","067d7a95":"code","61c31704":"code","5dfb75ae":"code","31dda0a7":"code","fa76e13d":"code","ee24b1c9":"code","bb6b8bc9":"code","81c2cf7b":"code","abcb7dd3":"markdown","1e084e0f":"markdown","f523acb7":"markdown","2880cd79":"markdown","acc7fa2a":"markdown","9b9878bc":"markdown","93ddf491":"markdown","ea22ac99":"markdown","f90a1780":"markdown","79b815f8":"markdown","eca829bd":"markdown","81504455":"markdown","6d45e5f2":"markdown"},"source":{"514bfd04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#importing all neccesary libraries and data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","692d2939":"#get data frame and display it\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf","a90e72e2":"#display columns and check for any null values\ndf.columns","a10ec5e9":"#check the presence of NaN values\ndf.isnull().sum()","61314565":"#drop'Unamed'column\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv').drop([\"Unnamed: 32\"], axis = 1)\ndf\n","b9eb2032":"#labeling the diagnosis as '1' and '0'\ndf[\"diagnosis\"] = df[\"diagnosis\"].map({'M':1, 'B':0})","74153e7e":"df","3e26cd38":"#Assigning our input and output data\nX = df[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean','fractal_dimension_mean',\n                 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n                 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst','fractal_dimension_worst']]\n\nY = df['diagnosis']","22a51ecc":"#plot a countplot\nsns.countplot(df[\"diagnosis\"])\nplt.show()","305f8021":"cor= df.corr()\nsns.heatmap(cor,cmap='coolwarm')\nplt.savefig('heatmap.png')\nplt.show()","70e63c96":"df.corr()","b7adabda":"#Get the training and test data\nX_train_orig, X_test_orig, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)","eb39a1a1":"#Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train_orig)\nX_test = sc.transform(X_test_orig)","067d7a95":"#training the model\nfrom sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression(random_state = 0)\nlr_classifier.fit(X_train, Y_train)\n#predicting the model\nY_pred = lr_classifier.predict(X_test)\n#Accuracy\nlr_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\",lr_score)\nprint(classification_report(Y_test, Y_pred))\n","61c31704":"#training the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_classifier.fit(X_train, Y_train)\nY_pred = knn_classifier.predict(X_test)\n#Accuracy\nknn_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\",knn_score)\nprint(classification_report(Y_test, Y_pred))","5dfb75ae":"#training\nfrom sklearn.svm import SVC\nsvm_classifier = SVC(kernel = 'linear', random_state = 0)\nsvm_classifier.fit(X_train, Y_train)\nY_pred = svm_classifier.predict(X_test)\n#Accuracy\nsvm_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\", svm_score)\nprint(classification_report(Y_test, Y_pred))","31dda0a7":"#training the model\nfrom sklearn.svm import SVC\nksvm_classifier = SVC(kernel = 'rbf', random_state = 0)\nksvm_classifier.fit(X_train, Y_train)\nY_pred = ksvm_classifier.predict(X_test)\n#accuracy\nksvm_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\", ksvm_score)\nprint(classification_report(Y_test, Y_pred))","fa76e13d":"#training the model\nfrom sklearn.naive_bayes import GaussianNB\ngnb_classifier = GaussianNB()\ngnb_classifier.fit(X_train, Y_train)\nY_pred = gnb_classifier.predict(X_test)\n#accuracy\ngnb_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\", gnb_score)\nprint(classification_report(Y_test, Y_pred))","ee24b1c9":"#training the model\nfrom sklearn.tree import DecisionTreeClassifier\ndectree_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndectree_classifier.fit(X_train, Y_train)\nY_pred = dectree_classifier.predict(X_test)\n#Accuracy\ndectree_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\", dectree_score)\nprint(classification_report(Y_test, Y_pred))\n","bb6b8bc9":"#training the model\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf_classifier.fit(X_train, Y_train)\nY_pred = rf_classifier.predict(X_test)\n#accuracy\nrf_score = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of the model:\",rf_score )\nprint(classification_report(Y_test, Y_pred))","81c2cf7b":"    models_initial = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'Kernel SVM', \n                     'Random Forest', 'K-Nearest Neighbors', 'Gaussian Naive Bayes'],\n    'Accuracy'    : [lr_score, dectree_score, svm_score, ksvm_score, rf_score, knn_score, gnb_score],\n    }, columns = ['Model', 'Accuracy'])\n    \n    models_initial","abcb7dd3":"# Decision Tree Algorithm","1e084e0f":"# Data Visualization\n### The following count plot shows the number of Benign and Malignant cells in graphical form","f523acb7":"# Guassian Naive Bayes\n","2880cd79":"# K Nearest Neighbour model","acc7fa2a":"# Correlation\n### Shows the degree of linear relation between the variables present in the dataset","9b9878bc":"### This dataset consists of digital images of fine needle aspirate (**FNA**, a biopsy procedure) of breast mass and describes the characteristics of the cell nuclei present. \n### The diagnosis i.e. whether the nuclei is **Benign** **OR** **Malignant** is displayed in the 'diagnosis' column in the data frame.Breast cancer is the most common type of cancer affecting women among which **Invasive Ductal Carcinoma** (IDC) is the most common form of breast cancer representing 80% of all breast cancer diagnosis.\n### By detecting the malignant\/benign cell nuclei classification becomes easier as well as faster resulting in early diagnosis and preventing fatality.\n\n### The data also consists of various other characteristic features of the nuclei, this notebook shows various classification techniques performed on the dataset along with the comparison of thier accuracy, aiming to provide suitable solution.\n","93ddf491":"### As we can see in the above table, The model created with **Random forest algorithm** gives the highest accuracy as compared to other models.","ea22ac99":"## Comparision Table","f90a1780":"# Random Forest Classifier\n","79b815f8":"# **Logistic Regression Model**","eca829bd":"# Standard Scaling","81504455":"# Support Vector Machine Kernel (radial basis function)","6d45e5f2":"# Support Vector Machine (SVM)"}}