{"cell_type":{"70830060":"code","71dba8b4":"code","2f17e366":"code","d0545d33":"code","cbf823b2":"code","4b2995db":"code","af4a28f4":"code","5bb0730b":"code","7acd27ea":"code","902691e6":"code","5708778d":"code","4380ac1a":"code","06953f39":"code","a2a73717":"code","694c955a":"code","c4da7cba":"code","fc71d7a6":"code","330a5dd6":"code","3c168f71":"code","5939c81b":"code","48cf27a0":"code","932c5c63":"code","c17dfdbc":"code","99d3558c":"code","6f37811f":"code","786ba62c":"code","26682565":"code","52afaa51":"code","5a8ef887":"code","63577acd":"code","4f47d86a":"code","e4c3a20c":"code","c50d5e92":"code","a6c09b96":"code","4f55ef38":"code","c640bebd":"code","ff5c644a":"code","5c7239e2":"code","fcfb17eb":"code","6d649c0b":"code","87bc1e60":"code","c6746968":"code","a47da80b":"code","e9101a3b":"code","19bf1f58":"code","18e277c1":"code","b4048872":"code","167570bf":"code","7768cab3":"code","9d3c1bee":"code","c9608aca":"code","b9eb3940":"code","28e6b4cc":"code","4bfae1ad":"code","5b31a67c":"code","9f230791":"code","027de437":"code","f9ee288a":"code","794a58fc":"code","659669af":"code","721402cd":"code","27b1baa5":"code","12d1df2d":"code","2b63b244":"code","94f4a307":"code","3b464701":"code","acb0a54b":"code","6c60e652":"code","a60cceae":"code","637647ca":"code","e775b52c":"code","47ca7cac":"code","2e066ffe":"code","a738475b":"code","415a7171":"code","0d64e98c":"code","81daf900":"code","6e60e308":"code","30787207":"code","064c42a5":"code","27d952c4":"code","0211920c":"code","2517f0fb":"code","b6bbd544":"code","2b0091bd":"code","a613e895":"code","16aeb00c":"code","3d355b34":"code","f5570e43":"code","668a9c9e":"code","97ff76ff":"code","771c43af":"code","6f1f2925":"code","b0ec0ea3":"code","322bec2a":"code","f1f72bad":"code","6e411955":"code","56b891ae":"code","8f5087bd":"code","24da7d4d":"code","16615469":"code","e6ec704a":"code","06a5dc1f":"code","4ce3056e":"code","b3b6e44e":"code","87e91c92":"code","069344ba":"code","b594b7ea":"code","a23b7c70":"code","af41332a":"code","47897750":"code","24d97c92":"code","a335fde5":"code","d62796b4":"code","a7a8b4b5":"code","1370e06a":"code","d004269e":"code","8e154024":"code","75860ae4":"markdown","281e11f7":"markdown","22a5689b":"markdown","3c6fc62b":"markdown","6dce800c":"markdown","092ba6dc":"markdown","b1ebc59c":"markdown","3b5a4bd2":"markdown","21529416":"markdown","5f20bdc1":"markdown","5cbf0296":"markdown","6fb47c6b":"markdown","a48f14f7":"markdown","ceb55e7b":"markdown","7df7c349":"markdown","8f6c6859":"markdown","7353424b":"markdown","5ae46054":"markdown","9fa4af5a":"markdown","008671af":"markdown","3b61a6be":"markdown","b6c5f65a":"markdown","ca471a3c":"markdown","730b4dc1":"markdown","5d41544c":"markdown","412b8070":"markdown","1b4b2102":"markdown","cc2f965b":"markdown","4318fdc0":"markdown","dc857bd8":"markdown","b288460a":"markdown","c9a29ec0":"markdown","ab7af0a5":"markdown","b76cab05":"markdown","6c22d79b":"markdown","c4398ce8":"markdown","9df8d906":"markdown","9f56ae04":"markdown","653f0a84":"markdown","50c69460":"markdown","523923be":"markdown","62013774":"markdown","643d8185":"markdown","6ee64424":"markdown","61fd083f":"markdown","1d5c9217":"markdown","d2831e40":"markdown","3bb226d0":"markdown","c80e1ef9":"markdown","b278592b":"markdown","4156a41d":"markdown","389a8cbb":"markdown"},"source":{"70830060":"%load_ext autoreload\n%autoreload 2\n","71dba8b4":"import shap\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd  \nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom math import floor, ceil\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.dates as mdates\nfrom itertools import combinations\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import silhouette_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, plot_precision_recall_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV, train_test_split, cross_val_score, StratifiedKFold","2f17e366":"# General setup\n#plt.style.use('fivethirtyeight')\nplt.style.use('fivethirtyeight')\n\nplt.rcParams.update({'figure.figsize': (8, 4)})\n\n# To be able to reproduce results\nrandom.seed(42)\n\n# Today variable, useful for calculating ages\ntoday = datetime.datetime.now()","d0545d33":"# Create an array with the colors you want to use\ncolors = ['#004792', '#AFC4DB']\n# Set your custom color palette\nsns.set_palette(sns.color_palette(colors))","cbf823b2":"# MATPLOTLIB CONFIGS\nplt.rcParams['axes.facecolor']='white'\nplt.rcParams['figure.facecolor']='white'\nplt.rcParams['savefig.facecolor']='white'","4b2995db":"default_color='#004792'","af4a28f4":"def corr_plot(df, figsize=(11, 9), cmap='Blues'):\n    # Compute the correlation matrix\n    corr = df.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=figsize)\n\n    # Generate a custom diverging colormap\n    #cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n                square=False, linewidths=.5, cbar_kws={\"shrink\": .7}, annot=True, fmt=\".1%\")\n\n    plt.xticks(rotation=60)\n    plt.tight_layout()\n\ndef plot_cluster_var_comparison(df, exclude, cluster_var='Cluster'):\n    if cluster_var not in exclude:\n        exclude.append(cluster_var)\n\n    columns = df.columns.difference(exclude)\n\n    var_combinations = list(combinations(columns, 2))\n\n    rows = ceil(len(var_combinations)\/2)\n\n    fig, axes = plt.subplots(rows, 2, figsize=(14, rows*5))\n\n    axes = axes.ravel()\n\n    axis_to_off = [i-1 for i in list(range(rows*2, len(var_combinations), -1))]\n\n    for combination, ax in zip(var_combinations, axes):\n        x_axis = combination[0]\n        y_axis = combination[1]\n\n        sns.scatterplot(data=df, x=x_axis, y=y_axis, hue=cluster_var,\n                        legend='brief', ax=ax, s=70)\n\n    for ax_i in axis_to_off:\n        axes[ax_i].axis('off')\n\n    plt.tight_layout()\n\ndef drop_highly_correlated(df, threshold=.95):\n    init_shape = df.shape\n    # Create correlation matrix\n    corr_matrix = df.corr().abs()\n\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(\n        np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find index of feature columns with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(\n        upper[column] > threshold)]\n\n    # Drop features\n    df = df.drop(df[to_drop], axis=1)\n\n    print(f'Dropping columns: {to_drop}')\n    print(f'Initial df Shape: {init_shape}\\nFinal df Shape: {df.shape}')\n\n    return df\n\n# Create a function for computing and plotting the ECDF with default parameters\ndef plot_ecdf(data, ax, xlabel='Data Values', ylabel='FDEC', color='#FF3030'):\n    \"\"\" \n    Function to plot ecdf taking a column of data as input.\n    \"\"\"\n    # ECF cacl\n    xaxis = np.sort(data)\n    yaxis = np.arange(1, len(data)+1)\/len(data)\n\n    ax.plot(xaxis, yaxis, linestyle='none', marker='.', color=color)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    plt.margins(0.02)\n\ndef plot_dist(df, columns, hist=True, ecdf=True, figsize=(8, 4), color=default_color):\n    \"\"\"\n    Plots histogram and ecdf plots for columns passed\n    \"\"\"\n    for col in columns:\n        f, ax = plt.subplots(1, 2, figsize=figsize)\n        plt.suptitle(f'Histograma e FDEC da vari\u00e1vel {col}', y=1.035)\n        if hist:\n            sns.distplot(df[col], ax=ax[0], color=color)\n        if ecdf:\n            plot_ecdf(df[col], ax=ax[1], xlabel=col, color=color)\n        plt.tight_layout()\n        plt.show()\n\ndef accpt_bar_plot(df, column, title='', xlabel='', figsize=(10, 5), ylim_offset=5, ax=None):\n    df_bar = (df.groupby(['accpt', column])[\n              'ID'].nunique().to_frame('perc_customer')\/df.shape[0])*100\n    df_bar = df_bar.reset_index().sort_values(by=column, ascending=True)\n    \n    \n    plt.figure(figsize=figsize)\n    \n    ax = sns.barplot(x=column, y='perc_customer', hue='accpt',\n                     hue_order=[\"N\u00e3o aceitou nenhuma campanha\",\n                                'Aceitou pelo menos uma campanha'],\n                     data=df_bar, alpha=1, saturation=1, edgecolor='k', linewidth=.7)\n    \n    \n    # Add this loop to add the annotations\n    for p in ax.patches:\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy()\n        ax.annotate('%.1f' % height + '%', (x + width \/\n                                            1.8, y + height*1.01), ha='center')\n\n    ax.xaxis.set_tick_params(labelsize=15)\n    ax.yaxis.set_tick_params(labelsize=15)\n    plt.legend(fontsize='medium', loc='best', title='')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('% de clientes')\n    plt.ylim(0, df_bar.perc_customer.max() + ylim_offset)\n    plt.tight_layout()\n    plt.savefig(f'.\/images\/{column}_accpt.png', dpi=300)\n    \n    #plt.show()\n# f, ax = plt.subplots(2, 2, figsize=(16, 8), constrained_layout=True)\n# plt.grid(False)\n# accpt_bar_plot(df, 'Education', xlabel='Escolaridade', ax=ax[0][1])\n# accpt_bar_plot(df, 'Marital_Status', xlabel='Estado civil', ylim_offset=5, ax=ax[0][0])\n\n# accpt_bar_plot(df, 'AgeGroup', xlabel='Faixa et\u00e1ria', ylim_offset=10, ax=ax[1][0])\n# accpt_bar_plot(df, 'Kidhome', xlabel='N\u00famero de crian\u00e7as em casa', ylim_offset=5, ax=ax[1][1])\n\n# plt.savefig('images\/com_profile.png', dpi=300, bbox_inches='tight')\n#plt.tight_layout()\n\n# f, ax = plt.subplots(1, 3, figsize=(18, 4.5), constrained_layout=True)\n# plt.grid(False)\n\n# accpt_bar_plot(df, 'r_score', title='Recency', xlabel='Score', ylim_offset=8, ax=ax[0])\n# accpt_bar_plot(df, 'f_score', title='Frequency', xlabel='Score', ylim_offset=8, ax=ax[1])\n# accpt_bar_plot(df, 'm_score', title='Monetary', xlabel='Score', ylim_offset=8, ax=ax[2])\n\nplt.savefig('images\/com_rfm.png', dpi=300, bbox_inches='tight')\n    \ndef campaign_success(df_, index):\n    custo_total = df_['Z_CostContact'].sum()\n    revenue = (df_[df_.Response == 1]['Z_Revenue'].sum())\n    roi = str(round((revenue -\n                 custo_total)\/custo_total, 2)*100) + '%'\n    success_rate = str(round(df_[df_.Response == 1].shape[0]\/df_.shape[0], 2)*100) + '%'\n    customers = df_.ID.nunique()\n    \n\n    return pd.DataFrame([[customers, df_[df_.Response == 1].ID.nunique(), success_rate, custo_total, revenue, revenue-custo_total, roi]], \n                        columns=['# total de clientes', '# que aceitaram', 'Pct. Ades\u00e3o', 'Custo total', 'Receita', 'Lucro', 'ROI'], index=[index])","5bb0730b":"# Reading data\ndf = pd.read_csv('data\/ml_project1_data.csv')","7acd27ea":"# Age column\ndf['Age'] = today.year - df['Year_Birth']","902691e6":"# Identifying out of scope variables\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nplt.suptitle('Distribution of Z annotated variables')\nsns.distplot(df['Z_CostContact'], ax=ax[0], kde=False, color=default_color)\nsns.distplot(df['Z_Revenue'], ax=ax[1], kde=False, color=default_color)\nplt.show()","5708778d":"roi = round((df[df.Response==1]['Z_Revenue'].sum() - df['Z_CostContact'].sum())\/df['Z_CostContact'].sum(), 2)\nsuccess_rate = round(df[df.Response==1].shape[0]\/df.shape[0], 2)*100\nprint(f\"Campaign success rate: {success_rate}%\\nReturn on investment (ROI): {roi}%\")","4380ac1a":"df.Response.value_counts()","06953f39":"# Non dummy variables like accepted campaign and response\nnon_dummy = ['Age', 'Income', 'Kidhome', 'Teenhome', \n              'Recency', 'MntWines', 'MntFruits',\n              'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n              'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n              'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']","a2a73717":"corr_plot(df[non_dummy], figsize=(18, 7), cmap='coolwarm')\n# Checking for sanity if any correlation above .9 threshold\n_ = drop_highly_correlated(df[df.columns.difference(['ID', 'Year_Birth'])], threshold=.9)","694c955a":"# Checking for customers with multiple rows\ndf['ID'].value_counts().sort_values(ascending=False).to_frame('Customer Frequency').head()","c4da7cba":"df.isnull().sum()","fc71d7a6":"plot_dist(df, \n          columns=['Teenhome', 'Kidhome', 'Recency', 'MntWines', 'MntFruits',\n                       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n                       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n                       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Age', 'Income'], \n          figsize=(12, 4), color=default_color)","330a5dd6":"# Clipping age to 80, no need for lower bound because min age=24\ndf['Age'] = df['Age'].clip(upper=80)\n# Replacing by income by the median\ndf.loc[df['Income'] > 6e5, 'Income'] = np.nan","3c168f71":"plot_dist(df, columns=['Age', 'Income'], figsize=(12, 4), color=default_color)","5939c81b":"df['Marital_Status'].value_counts().sort_values().plot(kind = 'barh', color=default_color, alpha=.95, figsize=(8, 5), fontsize=13)\nplt.tight_layout()\nplt.savefig('images\/marti_outlier.png', dpi=300)\nplt.show()","48cf27a0":"df['Education'].value_counts().sort_values().plot(kind = 'barh', color=default_color, alpha=.95, figsize=(8, 5), fontsize=13)\nplt.tight_layout()\nplt.show()","932c5c63":"# Other replace\ndf.loc[df['Marital_Status'].isin(['Alone',\n       'Absurd', 'YOLO']), 'Marital_Status'] = 'Other'\n","c17dfdbc":"df['Marital_Status'].value_counts().sort_values().plot(kind = 'barh', color=default_color, alpha=.95, figsize=(8, 5), fontsize=13)\nplt.tight_layout()\nplt.show()","99d3558c":"f, ax = plt.subplots(1, 1, )\n\nfreq_campaign = df[['Response', 'AcceptedCmp3', 'AcceptedCmp4',\n                    'AcceptedCmp5', 'AcceptedCmp1',\n                    'AcceptedCmp2']].sum(axis=1). \\\n    value_counts().sort_values(ascending=False).to_frame('freq')\n\nax = freq_campaign.freq.plot(kind='bar', color=default_color, \n                        alpha=.95, fontsize=13, \n                        figsize=(8, 5))\n\nplt.title('Number of campaingns accepted per customer')\nplt.xlabel('Number of campaigns accepted')\nplt.ylabel('Number of customers')\n\n# Add this loop to add the annotations\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    perc = p.get_height()\/df.shape[0]\n    x, y = p.get_xy()\n    ax.annotate(f'{perc:.02%}', (x + width\/2, y + height*1.01), ha='center')\n    \n\nplt.xticks(rotation=0)\nplt.ylim(0, 1800)\n\nplt.tight_layout()\nplt.show()","6f37811f":"df_cmp = df[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].sum()\ndf_cmp.index = [f'{i}' for i in range(1, 7)]\ndf_cmp.sort_values('index')\nax = df_cmp.plot(kind='bar', color=default_color, \n                        alpha=.95, fontsize=13, \n                        figsize=(8, 5))\n\nplt.title('Campaign success rate')\nplt.ylabel('Number of customers')\n\n# Add this loop to add the annotations\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    perc = p.get_height()\/df.shape[0]\n    x, y = p.get_xy()\n    ax.annotate(f'{perc:.02%}', (x + width\/2, y + height*1.01), ha='center')\n\nplt.xlabel('Campaign')\nplt.xticks(rotation=0)\nplt.ylim(0, 370)\nplt.tight_layout()\nplt.show()","786ba62c":"df['accpt'] = \"N\u00e3o aceitou nenhuma campanha\"\n\naccpt_mask = df[['Response', 'AcceptedCmp3', \n                 'AcceptedCmp4', 'AcceptedCmp5', \n                 'AcceptedCmp1', 'AcceptedCmp2']].sum(axis=1) > 0\n\ndf.loc[accpt_mask, 'accpt'] = 'Aceitou pelo menos uma campanha'","26682565":"accpt_bar_plot(df, 'Education', title='Education')","52afaa51":"# Creating age groups\nlabels = ['20-40 Anos', '41-60 Anos', '60+ Anos']\nbins = [20, 40, 60, 100]\ndf['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels,)\naccpt_bar_plot(df, 'AgeGroup', xlabel='Faixa et\u00e1ria', ylim_offset=10)\n","5a8ef887":"# Tenure, i.e, time as consumer\ndf['Tenure'] = (today - pd.to_datetime(df.Dt_Customer)).dt.days\/365\nplot_dist(df, columns=['Tenure'], figsize=(12, 4), color=default_color)\n","63577acd":"# Creating age groups\nlabels = ['6-7 Anos', '7-9 Anos']\nbins = [6, 7, 9]\ndf['TenureGroup'] = pd.cut(df['Tenure'], bins=bins, labels=labels,)\naccpt_bar_plot(df, 'TenureGroup', xlabel='Tempo como consumidor', ylim_offset=10)","4f47d86a":"accpt_bar_plot(df, 'Kidhome', xlabel='N\u00famero de crian\u00e7as em casa', ylim_offset=5)","e4c3a20c":"accpt_bar_plot(df, 'Teenhome', xlabel='N\u00famero de adolescentes em casa', ylim_offset=5)","c50d5e92":"# Number of children at home\ndf['Number_Children'] = df['Kidhome'] + df['Teenhome']","a6c09b96":"accpt_bar_plot(df, 'Number_Children', xlabel='Number of children at home', ylim_offset=5)","4f55ef38":"accpt_bar_plot(df, 'Marital_Status', ylim_offset=5)\n","c640bebd":"f, ax = plt.subplots(1, 2, figsize=(16, 5))\n\n(df[df.Response == 1][['MntWines', 'MntFruits', \n                     'MntMeatProducts', 'MntFishProducts',\n                     'MntSweetProducts', 'MntGoldProds']]\\\n                    .sum()\/1e3).sort_values().plot(kind='barh', \n                                              color=default_color, \n                                              alpha=.95, \n                                              ax=ax[1])\nax[1].xaxis.set_tick_params(labelsize=14)\nax[1].yaxis.set_tick_params(labelsize=14)\nax[1].set_xlabel('Quantity in thousands')\nax[1].set_title('Quantity that respondents bought from a product category')\n\n# Second plot\n\ndf[df.Response == 1][['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']]\\\n                .sum().sort_values().plot(kind='barh', \n                                          color=default_color, \n                                          alpha=.95, \n                                          ax=ax[0])\nax[0].xaxis.set_tick_params(labelsize=14)\nax[0].yaxis.set_tick_params(labelsize=14)\nax[0].set_xlabel('Quantity')\nax[0].set_title(\"Respondent's usage of different sales channels\")\nplt.tight_layout()\nplt.savefig('images\/buying_behavior.png', dpi=300, bbox_inches='tight')\nplt.show()","ff5c644a":"plot_dist(df[df.Response==1], columns=['Income'], figsize=(12, 4), color=default_color)","5c7239e2":"df[df.Response==1]['Income'].agg([min, max, 'mean', 'median'])","fcfb17eb":"# Adding var to non dummy variables\nnon_dummy.append('Tenure')","6d649c0b":"variation_df = df.groupby('accpt')[non_dummy].mean().T\nvariation_df.columns = ['mean_accpt', 'mean_non_accpt']","87bc1e60":"variation_df['mean_variation'] = ((variation_df['mean_accpt'] - variation_df['mean_non_accpt']) \/\n                variation_df['mean_non_accpt']).round(2).to_frame('Variation_Accpt_NonAccpt')\\\n\nvariation_df = variation_df.drop(['Kidhome', 'Teenhome', 'Age', 'Recency', 'Tenure', 'NumWebVisitsMonth'])","c6746968":"variation_df\n","a47da80b":"ax = variation_df['mean_variation'].plot(kind='barh', color=default_color, alpha=.95, figsize=(10, 7), legend=None)\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\n#plt.title('Comparison between the means of respondents and non respondents', loc='left', x=-0.18049, y=1.005)\nplt.title('Compara\u00e7\u00e3o entre as m\u00e9dias dos clientes positivos e negativos', loc='left', x=-0.18049, y=1.005)\n\n\nplt.xlabel(r'Varia\u00e7\u00e3o (%)')\nplt.tight_layout()\nplt.savefig('images\/comparison_of_means.png', dpi=300, bbox_inches='tight')\nplt.show()","e9101a3b":"# Gold products have higher \"monetary\" value\ndf['MntGoldProds'] *= 2\n","19bf1f58":"df['Monetary'] = df[['MntWines', 'MntFruits', \n                    'MntMeatProducts', 'MntFishProducts', \n                    'MntSweetProducts', 'MntGoldProds']].sum(axis=1)","18e277c1":"# +1 because if customer is in dataset he must have bought something at least one time\ndf['Frequency'] = 1 + df[['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']].sum(axis=1)","b4048872":"plot_dist(df, columns=['Recency', 'Frequency', 'Monetary'], figsize=(12, 4), color=default_color)","167570bf":"# Assigning scores to the variables\ndf['r_score'] = pd.qcut(df['Recency'], q=4, labels=range(4, 0, -1)).astype(int)\ndf['f_score'] = pd.qcut(df['Frequency'], q=4, labels=range(1, 5))\ndf['m_score'] = pd.qcut(df['Monetary'], q=4, labels=range(1, 5))","7768cab3":"plot_dist(df, columns=['r_score', 'f_score', 'm_score'], figsize=(12, 4), color=default_color)","9d3c1bee":"f, ax = plt.subplots(1, 3, figsize=(16, 5), constrained_layout=True, sharey=True)\nplt.grid(False)\n\naccpt_bar_plot(df, 'r_score', title='Recency', xlabel='Score', ylim_offset=8, ax=ax[0])\naccpt_bar_plot(df, 'f_score', title='Frequency', xlabel='Score', ylim_offset=8, ax=ax[1])\naccpt_bar_plot(df, 'm_score', title='Monetary', xlabel='Score', ylim_offset=8, ax=ax[2])\n\nplt.grid(True, axis='y')\n\nplt.savefig('images\/com_rfm.png', dpi=300, bbox_inches='tight')","c9608aca":"accpt_bar_plot(df, 'r_score', title='Recency', xlabel='Score', ylim_offset=8)","b9eb3940":"accpt_bar_plot(df, 'f_score', title='Frequency', xlabel='Score', ylim_offset=8)","28e6b4cc":"accpt_bar_plot(df, 'm_score', title='Monetary', xlabel='Score', ylim_offset=8)","4bfae1ad":"rfm_columns = ['Recency', 'Frequency', 'Monetary']\n\ndf_rfm = df.set_index('ID')[rfm_columns].copy()","5b31a67c":"# Before log transform plot\nf, ax = plt.subplots(1, len(rfm_columns), figsize=(16, 4))\n\nplt.suptitle('RFM distributions before skewness treatment', y=1.035)\nfor col, i in zip(rfm_columns, range(len(rfm_columns))):\n    sns.distplot(df_rfm[col], ax=ax[i], color=default_color)\n    \nplt.tight_layout()\n\n\n\nplt.show()","9f230791":"# Before log transform plot\nf, ax = plt.subplots(1, len(rfm_columns), figsize=(16, 4))\n\nplt.suptitle('RFM distributions before skewness treatment', y=1.035)\nfor col, i in zip(rfm_columns, range(len(rfm_columns))):\n    sns.distplot(df_rfm[col], ax=ax[i], color=default_color)\n    \nplt.tight_layout()\n\n\nplt.show()","027de437":"rfm_columns = ['Recency', 'Frequency', 'Monetary']\n\ndf_rfm = df.set_index('ID')[rfm_columns].copy()\n\n# Before log transform plot\nf, ax = plt.subplots(2, len(rfm_columns), figsize=(16, 8))\n\nplt.subplots_adjust(wspace=0.2, hspace=0.4) \n\nfor col, i in zip(rfm_columns, range(len(rfm_columns))):\n    sns.distplot(df_rfm[col], ax=ax[0][i], color=default_color)\n    \n# Scaling the variables so they have same avg and std values, only Recency because the other ones were powertransformed\n#df_rfm[['Recency']] = StandardScaler().fit_transform(df_rfm[['Recency']])\n\n# Skewennes treatment is to take the log (Values must be positive)\n# Applying log + 1 (because of zeros), to treat skewness\ndf_rfm[['Frequency', 'Monetary', 'Recency']] = PowerTransformer(method='yeo-johnson').fit_transform(df_rfm[['Frequency', 'Monetary', 'Recency']])\n\nfor col, i in zip(rfm_columns, range(len(rfm_columns))):\n    sns.distplot(df_rfm[col], ax=ax[1][i], color=default_color)\n\nax[0][0].text(134., ax[0][0].get_ylim()[1] + .001, 'Vari\u00e1veis RFM antes do processamento', color='k',fontsize=20)\nax[1][0].text(3., .4, 'Vari\u00e1veis RFM ap\u00f3s o processamento', color='k',fontsize=20)\n#plt.tight_layout()\n\n\n#plt.savefig('images\/tratamento_rfm.png', dpi=300)","f9ee288a":"# Skewennes treatment is to take the log (Values must be positive)\n# Applying log + 1 (because of zeros), to treat skewness\ndf_rfm[['Frequency', 'Monetary', 'Recency']] = PowerTransformer(method='yeo-johnson').fit_transform(df_rfm[['Frequency', 'Monetary', 'Recency']])\n\n#df_rfm[['Frequency', 'Monetary']].transform(lambda v: np.log(v))\n\n# After log transform plot\nf, ax = plt.subplots(1, len(rfm_columns), figsize=(16, 4))\n\nplt.suptitle('RFM distributions after skewness treatment', y=1.035)\nfor col, i in zip(rfm_columns, range(len(rfm_columns))):\n    sns.distplot(df_rfm[col], ax=ax[i], color=default_color)\n    \nplt.tight_layout()\nplt.show()","794a58fc":"df_rfm.skew()","659669af":"# Scaling data so mean and variance of variables are the same\n# Use StandardScaler\ndf_rfm.describe().round(3)","721402cd":"# Scaling the variables so they have same avg and std values, only Recency because the other ones were powertransformed\n#df_rfm[['Recency']] = StandardScaler().fit_transform(df_rfm[['Recency']])","27b1baa5":"# Describe after scaling\n# Variables have the same avg and std values\ndf_rfm.describe().round(5).append(df_rfm.skew().to_frame('Skewness').T)","12d1df2d":"# Instantiate the clustering model and visualizer\nkmeans = KMeans(random_state=42)\nvisualizer = KElbowVisualizer(kmeans, k=(2, 10))\n\n\nvisualizer.fit(df_rfm[rfm_columns].values)        # Fit the data to the visualizer\nax = visualizer.show()        # Finalize and render the figure","2b63b244":"kmeans = KMeans(random_state=42, n_clusters=int(visualizer.elbow_value_))","94f4a307":"df_rfm_copy = df_rfm.copy()","3b464701":"df_rfm = df_rfm_copy.copy()\ndf_rfm['Cluster'] = kmeans.fit_predict(df_rfm[rfm_columns].values)\n\ndf_rfm['Cluster'] = df_rfm['Cluster'].astype(int)\ndf_rfm = df_rfm.reset_index()\n\n# Assigning labels to original dataframe\ndf['Cluster'] = df_rfm['Cluster']","acb0a54b":"# Silhouette score to see how well K-Means performed\ns_score = silhouette_score(df_rfm[rfm_columns].values, df_rfm['Cluster'].values, random_state=42)\nprint(f'Silhoutte Score: {s_score}')","6c60e652":"# Melt the normalized dataset and reset the index\ndf_melt = pd.melt(\n                df_rfm, \n# Assign ID and Cluster as ID variables                  \n                id_vars=['ID', 'Cluster'],\n\n# Assign RFMT values as value variables\n                value_vars=['Recency', 'Frequency', 'Monetary'], \n# Name the variable and value\n                var_name='Metric', value_name='Value'\n                )\n\ndf_piv = df_melt.pivot_table(\"Value\", \"Metric\", \"Cluster\")\ndf_piv = df_piv.reindex(['Recency', 'Frequency', 'Monetary'])\nax = df_piv.plot(marker=\"o\", figsize=(10, 4.8), colormap='seismic')\n\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nplt.xlabel('Metric')\nplt.ylabel('Value')\nplt.title('Snake plot das vari\u00e1veis RFM')\nplt.savefig('images\/snake_plot.png', dpi=300)\nplt.show()","a60cceae":"# Calculate average RFM values for each cluster\ncluster_avg = df.groupby(['Cluster'])[rfm_columns].mean() \n\n# Calculate average RFM values for the total customer population\npopulation_avg = df[rfm_columns].mean()\n\n# Calculate relative importance of cluster's attribute value compared to population\nrelative_imp = cluster_avg \/ population_avg - 1\n\n# Initialize a plot with a figure size of 8 by 2 inches \nplt.figure(figsize=(8, 4))\n\n\n# Add the plot title\nplt.title('Relative importance of attributes')\n\n# Plot the heatmap\nax = sns.heatmap(data=relative_imp, annot=True, cmap='coolwarm', linewidths=.5, cbar_kws={\"shrink\": 1}, fmt=\".1%\")\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\n\nplt.show()","637647ca":"# Assigning each cluster to its respective profile\n#df_rfm['Cluster'] = df_rfm['Cluster'].map({2: 'Alto', 1: 'Melhor', 3: 'Medio', 0: 'Pior'})\n# Assigning each cluster to its respective profile\ndf_rfm['Cluster'] = df_rfm['Cluster'].map({3: 'Alto', 1: 'Melhor', 2: 'Medio', 0: 'Pior'})","e775b52c":"# Same plot but with renaming of clusters\n# Melt the normalized dataset and reset the index\ndf_melt = pd.melt(\n                df_rfm, \n# Assign ID and Cluster as ID variables                  \n                id_vars=['ID', 'Cluster'],\n\n# Assign RFMT values as value variables\n                value_vars=['Recency', 'Frequency', 'Monetary'], \n# Name the variable and value\n                var_name='Metric', value_name='Value'\n                )\n\ndf_piv = df_melt.pivot_table(\"Value\", \"Metric\", \"Cluster\")\ndf_piv = df_piv.reindex(['Recency', 'Frequency', 'Monetary'])\nax = df_piv.plot(marker=\"o\", figsize=(10, 5), colormap='seismic')\n\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nax.legend(fontsize=15)\n#plt.xlabel('Metric')\nplt.ylabel('Valor')\nplt.title('Snake plot das vari\u00e1veis RFM')\n#plt.tight_layout()\nplt.savefig('images\/snakeplot_rfm.png', dpi=300, bbox_inches='tight')\n\nplt.show()","47ca7cac":"# For reassigning purposes after mapping\ndf['Cluster'] = df_rfm['Cluster']","2e066ffe":"# Assigning each cluster to its respective profile\n#df['Cluster'] = df['Cluster'].map({0: 'High', 2: 'Highest', 1: 'Medium', 3: 'Lowest'})","a738475b":"cat_dtype = pd.api.types.CategoricalDtype(categories=['Pior', 'Medio', 'Alto', 'Melhor'], ordered=True)","415a7171":"df_rfm.Cluster = df_rfm['Cluster'].astype(cat_dtype)","0d64e98c":"df.Cluster = df['Cluster'].astype(cat_dtype)","81daf900":"accpt_bar_plot(df, 'Cluster', title='Aceita\u00e7\u00e3o por segmento RFM')","6e60e308":"campaign_success(df, 'Piloto')\\\n.append(campaign_success(df[df.Cluster.isin(['Melhor', 'Alto'])], 'Somente segmentos RFM Alto e Melhor'))\\\n.append(campaign_success(df[df.Cluster.isin(['Melhor'])], 'Somente o Melhor segmento RFM'))","30787207":" df.Response.replace({0: 'N\u00e3o aceitou', 1: 'Aceitou'}).value_counts().to_frame('Response Count').T","064c42a5":" df.Response.replace({0: 'N\u00e3o aceitou', 1: 'Aceitou'}).value_counts()","27d952c4":"df.Response.replace({0: 'N\u00e3o aceitou', 1: 'Aceitou'}).value_counts().to_frame('Response Count').reset_index()","0211920c":"imb = round(df.Response.replace({0: 'N\u00e3o aceitou', 1: 'Aceitou'}).value_counts().to_frame('Response Count')\/df.shape[0], 2)*100","2517f0fb":"# Checking for imbalanced dataset\nax = imb.plot(kind='bar', figsize=(6, 4), legend=False)\nplt.xticks(rotation=0)\nplt.title('')\nplt.ylabel('% de clientes')\nax.xaxis.set_tick_params(labelsize=15)\nax.yaxis.set_tick_params(labelsize=15)\nplt.savefig('images\/imb.png', dpi=300, bbox_inches='tight')","b6bbd544":"# Columns that will be used to train the classification model\ndata_columns = ['Education', 'Marital_Status', 'Income', \n           'Kidhome', 'Teenhome', 'MntWines', 'MntFruits',\n           'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n           'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', \n           'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n           'Complain', 'Age', 'Tenure', 'Recency', 'Cluster', 'Response', 'AgeGroup', ]","2b0091bd":"df_clean = df.set_index('ID')[data_columns].copy()","a613e895":"# Enconding RFM clusters, greater the cluster number better this client is in terms of RFM\ndf_clean['Cluster'] = df_clean['Cluster'].map({'Medio': 2, 'Alto': 3, 'Melhor': 4, 'Pior': 1})","16aeb00c":"# Preserving these columns\n# They will be used for inputing NA values for the variable income\n# Before training they will be dropped and replaced by the dummy variables\ndf_clean['raw_Education'] = df_clean['Education']\ndf_clean['raw_Marital_Status'] = df_clean['Marital_Status']","3d355b34":"# One hot encoding of categorical values\ndf_clean = pd.get_dummies(df_clean, columns=['Education', 'Marital_Status'])","f5570e43":"# Dropping redudant columns created because of get_dummies\n# Could have been any of the columns for each categorical columns\ndf_clean = df_clean.drop(['Marital_Status_Other', 'Education_2n Cycle'], axis=1)","668a9c9e":"def process_data(df, target='Response', skew_threshold=.3):\n    df_proc = df.copy()\n    #['Education', 'Marital_Status', 'AgeGroup']\n    # inputing income\n    # Tries higher granular level and if fails uses population median\n    df_proc.loc[:, 'Income'] = df_proc['Income'].fillna(df_proc.groupby(['raw_Education', 'raw_Marital_Status', 'AgeGroup'])\\\n                        ['Income'].transform('median')).fillna(df_proc['Income'].median())\n    \n    # Dropping columns used for inputting income NaN\n    df_proc = df_proc.drop(['raw_Education', 'raw_Marital_Status', 'AgeGroup'], axis=1)\n    \n    # Finding columns to power transform using yeo-johnson\n    # And finding columns that will be scaled using StandardScaler\n    skew = df_proc[df_proc.columns.difference([target])].skew()\n    pt_columns = skew[np.abs(skew) > skew_threshold].index\n    scale_columns = skew[np.abs(skew) < skew_threshold].index\n    \n    df_proc[pt_columns] = PowerTransformer(method='yeo-johnson').fit_transform(df_proc[pt_columns])\n    df_proc[scale_columns] = StandardScaler().fit_transform(df_proc[scale_columns])\n    \n    print(f'Mean Skew value of DF: {df_proc.skew().mean()}\\nMedian Skew value of DF: {df_proc.skew().median()}')\n    display(df_proc.agg(['mean', 'std']).round(4))\n    \n    \n    return df_proc[df_proc.columns.difference([target])], df_proc[target]","97ff76ff":"# Stratified split, because of imbalanced data\n# Splitting data before any scaling or sampling to avoid data leakage\ndf_train, df_test = train_test_split(df_clean, test_size=.3, stratify=df_clean.Response.values, random_state=42)","771c43af":"(df_train.Response.replace({0: 'N\u00e3o aceitou', 1: 'Aceitou'}).value_counts()\/df_train.shape[0]).to_frame('Conjunto de Treino').T","6f1f2925":"(df_test.Response.replace({0: 'N\u00e3o aceitou', 1: 'Aceitou'}).value_counts()\/df_test.shape[0]).to_frame('Conjunto de Teste').T","b0ec0ea3":"X_train, y_train = process_data(df_train)","322bec2a":"rfe_selector = RFECV(estimator=LogisticRegression(solver='liblinear'), step=2, cv=StratifiedKFold(n_splits=5), n_jobs=-1)\nrfe_selector.fit(X_train, y_train)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X_train.loc[:, rfe_support].columns.tolist()\nprint(f'Number of selected features: {len(rfe_feature)}')\nrfe_feature","f1f72bad":"X_test, y_test = process_data(df_test)","6e411955":"# Features that were selected\nrfe_feature","56b891ae":"# Using only the selected features using RFECV\nX_test_slc = X_test[rfe_feature]\nX_train_slc = X_train[rfe_feature]","8f5087bd":"# Models and param grids to use on GridSearch\nmodels = [{'name': 'logreg','label': 'Logistic Regression',\n           'classifier': LogisticRegression(random_state=42),\n           'grid': {\"clf__C\": np.logspace(-3,3,7), \"clf__penalty\": [\"l1\",\"l2\"], \"clf__solver\": ['liblinear'], 'clf__random_state': [42]}}]","24da7d4d":"# from sklearn.metrics import balanced_accuracy_score, average_precision_score, make_scorer\n\n# scorer = make_scorer(balanced_accuracy_score)\n# p_scorer = make_scorer(average_precision_score, average='weighted')","16615469":"def model_selection(classifier, name, grid, X_train, y_train, X_test, y_test,\n                    scoring, features_info='all_features', cv=StratifiedKFold(n_splits=5), n_jobs=-1):\n    \n    # Oversampling with smote\n    smote = SMOTE(random_state=42)\n    # Pipeline, every fold does as sampling\n    pipeline = Pipeline([('sampling', smote), ('clf', classifier)])\n    \n    # GridSearch init\n    gridsearch_cv=GridSearchCV(pipeline, \n                               grid,\n                               cv=cv, \n                               scoring=scoring, \n                               n_jobs=n_jobs, \n                               verbose=2)\n    \n    gridsearch_cv.fit(X_train, y_train)\n    \n    # Creates the results dataframe\n    results_dict = {}\n    results_dict['classifier_name'] = name    \n    results_dict['classifier'] = gridsearch_cv.best_estimator_\n    results_dict['best_params'] = gridsearch_cv.best_params_\n    results_dict['ROC_AUC_TRAIN'] = gridsearch_cv.best_score_\n    results_dict['features_info'] = features_info\n    results_dict['refit_time'] = gridsearch_cv.refit_time_\n    \n    # Compute the ROC_AUC score in the never seen test test\n    y_pred = gridsearch_cv.best_estimator_.predict(X_test)\n    results_dict['ROC_AUC_TEST'] = roc_auc_score(y_test, y_pred)\n    \n    return(results_dict)\n\nresults = []\nfor m in models:    \n    results.append(model_selection(m['classifier'], \n                                   m['name'],\n                                   m['grid'],\n                                   X_train, \n                                   y_train,\n                                   X_test,\n                                   y_test,\n                                   'roc_auc', 'all_features'))      \n    \n    results.append(model_selection(m['classifier'], \n                                   m['name'],\n                                   m['grid'],\n                                   X_train_slc, \n                                   y_train,\n                                   X_test_slc,\n                                   y_test,\n                                   'roc_auc', 'selected_features'))   \n    \nresults = pd.DataFrame.from_dict(results)\nresults","e6ec704a":"pd.set_option('display.max_colwidth', None)\nresults[['classifier_name', 'best_params', 'features_info', 'ROC_AUC_TEST', 'ROC_AUC_TRAIN', 'refit_time']].round(3)","06a5dc1f":"# Getting the best estimator\nbest = results.sort_values(by=['ROC_AUC_TEST', 'ROC_AUC_TRAIN'], ascending=False).head(1)\nclf = best.iloc[0, 1]['clf']\nfeature = best.iloc[0, 4]\ndisplay(clf)","4ce3056e":"X_test_final = X_test_slc if feature=='selected_features' else X_test","b3b6e44e":"# Predicting if a customer will accept or not the campaing\n# This test has never been used\ny_pred = clf.predict(X_test_final)","87e91c92":"def report_to_df(report):\n    report = [x.split(' ') for x in report.split('\\n')]\n    header = ['Resposta']+[x for x in report[0] if x!='']\n    values = []\n    for row in report[1:-5]:\n        row = [value for value in row if value!='']\n        if row!=[]:\n            values.append(row)\n    df = pd.DataFrame(data = values, columns = header)\n    return df","069344ba":"report = report_to_df(classification_report(y_test, y_pred))\nreport['Resposta'] = report['Resposta'].replace({'0': 'N\u00e3o aceitou', '1': 'Aceitou'})","b594b7ea":"report","a23b7c70":"from sklearn.metrics import plot_roc_curve","af41332a":"plot_roc_curve(clf, X_test_final, y_test)\n","47897750":"# Confusion matrix\nf, ax = plt.subplots(1, 1, figsize=(7, 5))\nplot_confusion_matrix(clf, X_test_final, y_test, normalize='true', display_labels=[\"N\u00e3o aceitou\", 'Aceitou'], cmap='Blues', ax=ax, values_format='.1%', )\nax.set_title('Matriz de confus\u00e3o do modelo final')\nax.xaxis.set_tick_params(labelsize=14)\nax.yaxis.set_tick_params(labelsize=14)\nax.set_ylabel('Real')\nax.set_xlabel('Previsto')\nplt.grid(None)\nplt.tight_layout()\nplt.savefig('images\/cmatrix_model.png', dpi=300, bbox_inches='tight')\nplt.show()","24d97c92":"# Assigning pred\nX_test['accepted'] = y_pred","a335fde5":"# original dataframe containing customers which the model said they would accept\ndf_model_accepted = df[df.ID.isin(X_test[X_test['accepted'] == 1].index)].copy()","d62796b4":"#campaign_success(df[df.ID.isin(df_test.index) & (df.Response == 1)], index='Modelo perfeito').append(\n(campaign_success(df[df.ID.isin(df_test.index)], index='Piloto com todos clientes do conjunto de teste'))\\\n.append(campaign_success(df_model_accepted, index='Clientes selecionados pelo modelo'))","a7a8b4b5":"pd.set_option(\"display.max_columns\",None)\nshap.initjs()","1370e06a":"explainer = shap.LinearExplainer(clf, X_train_slc)","d004269e":"shap_values = explainer.shap_values(X_test_slc)","8e154024":"plt.figure()\nshap.summary_plot(shap_values, X_test_slc)","75860ae4":"* Model performance:\n    * The model does a good job of predicting customers who would accept the campaign.\n    * Correctly predicted that 82% accepted the campaign and 76.9% did not.\n    * This model is good enough for a pilot run over the current approach.","281e11f7":"## Duplicates","22a5689b":"## K-Means Clustering\n\n* The data will be preprocessed so K-Means have a better chance to converge to an optimal solution.\n* The number of clusters will be found using the elbow-method with the silhouette score as the metric of evaluation.\n* An analysis of the clusters and how they behave will be done later on","3c6fc62b":"## Feature Selection\n\n* Variables will be selected using a recursive approach.\n* RFECV with logistic regression.\n* RFECV: recursive feature elimination and cross-validated selection of the best number of features.\n* To prevent any leakage this will be done using the training set.\n* This helps to prevent overfitting and reduces dimensionality resulting in less time fit our estimator.","6dce800c":"* From the plot above:\n    * The last campaign was the most successful in terms of customer adhesion, doubling last campaigns success rate.\n    * Campaign 2 was the worst one with 1.34% customer adhesion","092ba6dc":"### Evaluating ROI and Success Rate for test set","b1ebc59c":"* From the plot above we can notice that clusters that holds the customers with higher RFM values tend to accept more campaigns than those with low RFM scores.\n* Knowing this we could use this simple segmentation to choose possible customers from clusters High, and Highest to contact.","3b5a4bd2":"## Campaign success rate","21529416":"## Outlier detection\n\n* Using histograms and ecdf plots to identify possible outliers in numerical columns\n* Checking possible outlier in categorical columns like Marital Status, Education","5f20bdc1":"## Profiling respondents\n\n* In this section an analysis will be made to better understand the costumers who positively responded to the campaigns","5cbf0296":"### RFM approach key takeaways ","6fb47c6b":"* Seems like most of data is skewed this is a point of concern if we are going to use K-means, the data must not be skewed.\n* Age and Income columns have outliers clear seen by its ECDF plots\n\n* We will not remove customers with outlier columns instead we are going to clip it, all customers are eligible to receive any kind of deals. \n* Transformations:\n    Age will be clipped with upper = 80 which is the maximum age before outliers occur\n    The Income outlier which seems to be > 600000 will be replace with the median income of the population    ","a48f14f7":"* Frequency and Monetary variables distributions are skewed, this will be treated\n","ceb55e7b":"### Finding K\n\n* Using elbow-method to find the K number of clusters","7df7c349":"* The are no duplicates in this dataset","8f6c6859":"### RFM Cluster characteristics\n\n* With an graphical analysis using the Snake plot and the importance heatmap we have the following profiling of clusters:\n    * **Lowest**: High Recency and low Frequency and Monetary values (Non active and non money spenders)\n    * **Medium**: Low Recency, Frequency, and Monetary values (Active but are not money spenders)\n    * **High**: High Recency, Frequency, and Monetary Value (Non active but are money spenders)\n    * **Highest**: Low Recency and high Frequency and Monetary values (Active and money spenders) ","7353424b":"## Checking for outliers","5ae46054":"# EDA\n\n## To look for:\n\n* Profiling the respondents:\n    * Education \n    * Income\n    * Age\n    * Time as costumer\n    * Buying behavior\n    * RFM score of respondents\n    \n\n* As we don't know the kind of campaign that the marketing team realize, we will focus on understanding which consumers are more likely to accept any campaign. If we knew the scope of the campaign we could have done a more specific investigation, for example, if the next campaign would focus on alcoholic beverages, then having this knowledge we could search for patterns and discover customers which are more likely to buy alcoholic products when they have discounted prices or special deals.","9fa4af5a":"* No apparent outliers in the RFMT variables, so we continue with the segmentation","008671af":"* We can notice that higher scores tend to have higher acceptance. \n* Knowing this we could formulate a simple approach for the marketing team which uses these **RFM** scores.","3b61a6be":"# Helper Functions\n","b6c5f65a":"### K-Means assumption and data preprocessing\n\n* Data will be processed to satisfy K-Means assumptions which are:\n    * Symmetric distributions of variables (not skewed)\n    * Variables with same average values. This makes ensures that each metric receives an equal weight in the K-Means calculation.\n    * Variables with same variance, this also ensures equal importance in the clustering calculation.","ca471a3c":"### Imbalanced Dataset\n\n* Only 15% (last campaign's success rate) of the data is labeled with 1.\n* To overcome this problem the data will be split in a stratified manner, so that the training and test sets have the same percentage of classes occurrences.\n* Also an oversampling technique called SMOTE will be used during training.","730b4dc1":"## Quantiles cutting\n* RFM variables will cut in quartiles\n* 1 - Low Score\n* 4 - High Score","5d41544c":"* Notice that Other marital status has a lower frequency in the customer dataset\n","412b8070":"* Using the plot above we can see which RFM characteristics influence on the cluster score","1b4b2102":"* There were no variables with correlation above .9","cc2f965b":"## Analyzing acceptance over RFM variables","4318fdc0":"## Z variables\n* From the case .pdf file the Z variables are related to the campaign costs and success rates\n* The campaign success rate and return of investment can be calculated through them","dc857bd8":"* Using the elbow method the best K segmentations is 4","b288460a":"## Model Evaluation","c9a29ec0":"## ARRANJAR TITULO MELHOR PRA FIGURA","ab7af0a5":"# Predictive Model - Binary Classification\n* This section is deserved to show the improvements of using a classifier over the simple RFM approach\n\n* Section Divided in:\n    * Data Cleaning and Feature Engineering\n    * Feature Selection\n    * Model Selection\n    * Model Evaluation","b76cab05":"## Model Selection\n\n* The model chose was LogistRegression because of its simplicity and efficiency with binary classification tasks.\n* Using GridSearch greedy framework to find the best parameters of the estimator, i.e, hyperparameter tuning.\n* The training is done using StratifiedKFold to prevent overfitting.\n* Every fold is oversampled using SMOTE so they have the same number of classes.","6c22d79b":"# RFM Segmentation\n\n* Recency, frequency, monetary, and tenure segmentation of customers\n* Recency: days since last purchase\n* Frequency: how many times has the customer used any of the sales channels\n* Monetary: In this case we don't have the price spent so we are going to assume a price unit for every kind of product except for gold products which will have a price unit of 2","c4398ce8":"* If we used this simple linear model on the test set we would increase our ROI in 85% and have a 38% campaign success rate.","9df8d906":"* From the summary plot we have great insights on how the variables are affecting our model, for example:\n    * Lower values o recency tend increase the chance predicting that a customer will accept the campaign\n    * The longer the time as customer also increases the chances of predicting that it will accept\n    * A relation on the number of catalog purchases and store purchases can be noticed, customers that have low purchases in store and high purchases in catalog may been more susceptible to campaigns.","9f56ae04":"* Only the income column has missing values:\n    * Values will be imputed using the mean of their group represented by: ['Education', 'Marital_Status', 'AgeGroup']\n    * Age category is divided as:\n        * 20-30\n        * 30-50\n        * 50 >","653f0a84":"### Insights over respondents:\n\n* Looking over the education of respondents, most of them have higher education, is the campaign somehow discriminating?\n* However most of the customer in the dataset have higher education, why is that?\n* Education: majority of them have higher education \n* Income: average income of 60000\n* Age: the biggest age group with respondent is 41-60 Years\n* Time as costumer: majority of them have been a customer for at least 7 years \n* People at home: most of the respondents don't have children or teenagers at home\n* Marital Status: majority is either married or living together\n* Buying behavior: biggest sale channel is the physical one and the most bought product is wine\n* RFM score of respondents: expected to be really high","50c69460":"* From the plot above we can notice that:\n * ~73% of the customers didn't accept any campaign\n * ~27% of the customers accepted at least one campaign\n * And there is a few number of customers which accepted all of the campaigns, 10 customers to be exact","523923be":"### Categorical outlier","62013774":"## Feature Importance using SHAP","643d8185":"* The time as costumer (tenure) is well spread across customers\n","6ee64424":"* Creating age column from Year_Birth ","61fd083f":"## Segmentation using scores\n* Usually at this stage we would define segments based on business knowledge and the sum of the RFM scores.\n* A different approach using K-Means to find clusters will be followed.","1d5c9217":"## Data Cleaning and Feature Engineering\n\n* Income variable will be inputed as discussed before.\n* Variables that have and absolute skewness value >.4 will be transformed using yeo-johnson transform\n* Variables that weren't transformed will be scaled using StandardScaler\n* Logistic vs SVM\n* To prevent data leakage a data preprocessing will be constructed and ran separately in the training and test set.\n* To also prevent any leaks in information variables such as AcceptedCmp will be removed. Also, new customers or other customers may not have these information.","d2831e40":"* Seems only the Marital Status columns has weird values such as Alone, Absurd, and Yolo.\n* These values will be categorized as Other","3bb226d0":"## Using a simple RFM segmentation in order to increase ROI","c80e1ef9":"* With less and more relevant features we have almost the same score.","b278592b":"* Using the RFM segmentation proposed as we select better customers the ROI and Success Rates tend to increase, however the number of customers decreases, i.e, the reach of the campaign decreases.\n* It could be a safer and simpler approach to reduce campaign costs.\n* With better business insights the RFM segmentation can improve.","4156a41d":"## Missing Values","389a8cbb":"### Numeric outlier"}}