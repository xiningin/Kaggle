{"cell_type":{"a9303bdd":"code","25e714a3":"code","824b370f":"code","4e0b9399":"code","ca011762":"code","0c6d780b":"code","0267b53c":"code","2dd0984e":"code","b25a1cb1":"code","1cb7d7b4":"code","103a2729":"code","54d410ae":"code","4eab109c":"code","2bde40d5":"code","271c9a65":"code","f2811411":"code","370b515a":"code","e7890065":"code","12ee3d53":"code","607d4fc6":"code","072c47fd":"code","0b78375c":"code","ee28e5c7":"code","cf7de91f":"code","3edcda96":"code","e5aaa87f":"code","72270ee8":"code","bba7b9ea":"code","a877f89c":"code","b16271c2":"code","ef407247":"code","ec5799df":"code","65a91dcb":"code","442ca0b7":"code","eed08380":"code","bdeb6fdb":"code","5926a68e":"code","ecbb8563":"code","6891f610":"code","842d01b2":"code","f52e3e45":"code","95a72647":"code","77c0a761":"code","a8c21e46":"code","0a2e0a1e":"code","652e0a76":"code","59dd0fcc":"code","68f49241":"code","a6a9b3be":"code","1d080d56":"code","1b5eb0a6":"code","829c5c87":"code","5e22f966":"code","00c13bcd":"code","524318bc":"code","4f0c3712":"code","51b2b1dd":"code","d1c8c38a":"code","98db66e1":"code","a00b775a":"code","c5163606":"code","8ec9a875":"code","cf9b1f9c":"code","8658d92f":"code","8a6d8d1d":"code","3d2f7b95":"code","e17b8cd4":"code","ed666ccb":"code","26dfbd1c":"code","6938a350":"code","2d34df7f":"code","4b235d0e":"code","7fbf5d61":"code","51f9f8f8":"markdown","387a581d":"markdown","92b3370f":"markdown","5ef85635":"markdown","8bbcd40c":"markdown","aba93d53":"markdown","0be89888":"markdown","f5df1e10":"markdown","acce3d1e":"markdown","625399f6":"markdown","4cbef56f":"markdown","37935f0f":"markdown","51643c6e":"markdown","1c75ec8f":"markdown","bd979249":"markdown","26cf2216":"markdown","d9731859":"markdown","f22ba330":"markdown","1899ced0":"markdown","435d9811":"markdown"},"source":{"a9303bdd":"#Import Libraries\n\n#General Libraries\nimport pandas as pd\nimport numpy as np\nimport os\n\n#Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Prepossing Libraries\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport pickle\n\n#Models\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n\n#Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","25e714a3":"#Load Data\n\n#train data\ntrain = pd.read_csv('..\/input\/competitive-data-science-final-project\/sales_train.csv.gz', compression='gzip')\n\n#supplement data\nitems           = pd.read_csv('..\/input\/competitive-data-science-final-project\/items.csv') #item id and its category id\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-final-project\/item_categories.csv') #item category name\nshops           = pd.read_csv('..\/input\/competitive-data-science-final-project\/shops.csv') #shop name\ntest            = pd.read_csv('..\/input\/competitive-data-science-final-project\/test.csv.gz') #test data\nsample_submission = pd.read_csv('..\/input\/competitive-data-science-final-project\/sample_submission.csv.gz') #sample_submission","824b370f":"#Transaction Data\nprint(\"Shape of train: \"+ str(train.shape))\ntrain.head()","4e0b9399":"test.head()","ca011762":"sample_submission.head()","0c6d780b":"train.item_price.value_counts()\nprint(\"Number of prices:\",train.item_price.nunique())\nprint(\"Maximum of prices:\",train.item_price.max())\nprint(\"Data with maximum price:\")\ntrain[train.item_price==train.item_price.max()]","0267b53c":"max_id = train[train.item_price==train.item_price.max()].item_id.values[0]\nitems[items.item_id==max_id]","2dd0984e":"train[train.item_id==max_id]","b25a1cb1":"min_id = train[train.item_price==train.item_price.min()].item_id.values[0]\nitems[items.item_id==min_id]","1cb7d7b4":"train[train.item_id==min_id]","103a2729":"train[train.item_price>=1000].item_price.hist()","54d410ae":"#Translate the item category name and create meta category, subtype\nsymbols=(u\"\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f\u0410\u0411\u0412\u0413\u0414\u0415\u0401\u0416\u0417\u0418\u0419\u041a\u041b\u041c\u041d\u041e\u041f\u0420\u0421\u0422\u0423\u0424\u0425\u0426\u0427\u0428\u0429\u042a\u042b\u042c\u042d\u042e\u042f\", u\"abvgdeejzijklmnoprstufhzcss_y_euaABVGDEEJZIJKLMNOPRSTUFHZCSS_Y_EUA\")\nenglish = {ord(a):ord(b) for a, b in zip(*symbols)}\n\nitem_categories['items_english'] = item_categories['item_category_name'].apply(lambda x: x.translate(english))\nitem_categories['meta_category'] = item_categories['items_english'].apply(lambda x: x.split('-')[0])\nitem_categories['subtype'] = item_categories['items_english'].apply(lambda x: x.split('-')) \\\n                                                             .map(lambda x:x[1].strip() if len(x)>1 else x[0].strip())\nitem_categories.head(10)","4eab109c":"print(item_categories.meta_category.value_counts())","2bde40d5":"#Label Encoding\nlabelencoder = LabelEncoder()\nitem_categories[\"meta_category_code\"] = labelencoder.fit_transform(item_categories[\"meta_category\"])\nitem_categories[\"subtype_code\"] = labelencoder.fit_transform(item_categories[\"subtype\"])\nitem_categories = item_categories.drop([\"items_english\", \"meta_category\", \"subtype\"], axis=1)","271c9a65":"#Let's see what it looks like now\n\nitem_categories.head(5)","f2811411":"#Translate the shop name and identify towns\nshops[\"shops_english\"] = shops[\"shop_name\"].apply(lambda x: x.translate(english))\nshops[\"town\"] = shops[\"shops_english\"].apply(lambda x: x.split()[0])\nshops.head(5)","370b515a":"#Clean Up Data - Label encoding\nshops[\"town_code\"] = labelencoder.fit_transform(shops[\"town\"])\nshops = shops.drop([\"shops_english\", \"town\"], axis=1)","e7890065":"#Let's see if it did what it was suppose to\nshops.head(5)","12ee3d53":"# Remove dates\nnewcols = train[\"date\"].str.split(\".\", expand=True)\nls = [\"day\", \"month\", \"year\"]\nfor i in range(len(ls)):\n    train.insert(i, ls[i], newcols[i])\ntrain['weekday'] = pd.to_datetime(train['date'], format = '%d.%m.%Y').dt.dayofweek\ntrain.pop(\"date\")\n\n# Add item category for each item\ntrain['item_category_id'] = [items['item_category_id'].values[i] for i in train['item_id'].values]\n\n# Add revenues\ntrain[\"revenue\"] = train[\"item_price\"]*train[\"item_cnt_day\"]","607d4fc6":"#Let's confirm it worked\ntrain.head(5)","072c47fd":"train.date_block_num.plot()\ntrain.shop_id.plot()","0b78375c":"train=train.merge(items, how='left')\ntrain=train.merge(item_categories, how='left')\ntrain=train.merge(shops, how='left')","ee28e5c7":"#Remove columns\ntrain.drop(\"item_name\",axis=1,inplace=True)\ntrain.drop(\"shop_name\",axis=1,inplace=True)\ntrain.drop(\"item_category_name\",axis=1,inplace=True)\n\n#Confirm it worked\ntrain.head()","cf7de91f":"#Revenue seems to increase during Christmas time period and decrease during summer\ntrain.groupby(\"date_block_num\")[\"revenue\"].sum().plot()","3edcda96":"#Week Day vs Weekend\ntrain.groupby(\"weekday\")[\"revenue\"].sum().plot()","e5aaa87f":"train.groupby(\"shop_id\")[\"revenue\"].sum().plot.bar()\n#Not super helpful but there does seem to be some spikes","72270ee8":"#Look into Spike\ntrain[train[\"shop_id\"]==31][\"revenue\"].plot()","bba7b9ea":"#Revenue is increasing because the price has gone up not because sales have\ntrain.groupby(\"date_block_num\").count().revenue.plot()","a877f89c":"train.groupby(\"date_block_num\").item_price.mean().plot()","b16271c2":"train.groupby(\"date_block_num\").revenue.mean().plot()","ef407247":"#Plot\ntrain.groupby([\"date_block_num\",\"item_category_id\"]).sum()[\"revenue\"].unstack().plot(figsize=(20,20))\nplt.title('Item Category Revenue')","ec5799df":"#Look at shop dates (Min and max)\nshop_life=pd.DataFrame(columns=[\"shop_id\",\"Start\", \"Stop\"])\nshop_life[\"shop_id\"]=np.arange(60)\nshop_life[\"Start\"]=train.groupby(\"shop_id\")[\"date_block_num\"].min()\nshop_life[\"Stop\"]=train.groupby(\"shop_id\")[\"date_block_num\"].max()\nshop_life.merge(shops, how=\"left\").drop(\"shop_name\",axis=1)\nshop_life.head(10)","65a91dcb":"#It seems that shop 10 and 11 are the same, shop 1 and 58 are the same, shop 0 and 57 are the same\ntrain.loc[train[\"shop_id\"]==11,\"shop_id\"]=10\ntrain.loc[train[\"shop_id\"]==1,\"shop_id\"]=58\ntrain.loc[train[\"shop_id\"]==0,\"shop_id\"]=57","442ca0b7":"#Check whether all the shops are both in train and test sets\ntest_list = list(sorted(test[\"shop_id\"].unique()))\nout_of_list = [i for i in list(range(60)) if i not in test_list]\nprint(out_of_list)","eed08380":"shop_not_active = shop_life[shop_life[\"Stop\"]<33].shop_id\nout_of_list_active = [i for i in out_of_list if i not in shop_not_active]\nprint(out_of_list_active)","bdeb6fdb":"# 9 and 20 only have limited transactions\ntrain[(train[\"shop_id\"]==9) | (train[\"shop_id\"]==20)].groupby([\"shop_id\",\"date_block_num\"]).sum()[\"revenue\"]","5926a68e":"#Remove outliers (You can't always do this in the real world)\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","ecbb8563":"#Match new training data to cartesian each shop\/item pair\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix.sort_values(cols, inplace=True)","6891f610":"#Merge item count month\ngroup = train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum'})\ngroup.columns = ['item_cnt_month']\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","842d01b2":"#Merge test into matrix\ntest[\"date_block_num\"] = 34\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","f52e3e45":"#Merge encoded label into matrix\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, item_categories, on=['item_category_id'], how='left')","95a72647":"# Make lags features\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\n# Make lag encoded features for item_cnt_month\ndef encoded(matrix, cols, name, lags):\n    a = matrix.copy()\n    group = a.groupby(cols).agg({'item_cnt_month': ['mean']})\n    group.columns = [name]\n    group.reset_index(inplace=True)\n\n    a = pd.merge(a, group, on=cols, how='left')\n    a[name] = a[name].astype(np.float16)\n    a = lag_feature(a, lags, name)\n    a.drop([name], axis=1, inplace=True)\n    return a","77c0a761":"#Create matrix\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","a8c21e46":"#Show matrix\nmatrix","0a2e0a1e":"%%time\n# Make lag encoded features\n\"\"\"\n1) monthly\n2) monthly every item\n3) monthly every shop\n4) monthly every item category\n5) monthly every shop every item category\n6) monthly every shop every meta category\n7) monthly every shop every subtype category\n8) monthly every town\n9) monthly every item every town\n10) monthly every meta category\n11) monthly every subtype category\n\"\"\"\nmatrix1 = encoded(matrix, ['date_block_num'], 'date_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'item_id'], 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix1 = encoded(matrix1, ['date_block_num', 'shop_id'], 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix1 = encoded(matrix1, ['date_block_num', 'item_category_id'], 'date_category_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_category_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'shop_id', 'meta_category_code'], 'date_shop_meta_category_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'town_code'], 'date_town_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'item_id', 'town_code'], 'date_item_town_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'meta_category_code'], 'date_meta_category_avg_item_cnt', [1])\nmatrix1 = encoded(matrix1, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_cnt', [1])","652e0a76":"matrix1 = matrix1.drop([\"shop_name\", \"item_name\", \"item_category_name\"], axis=1)\nmatrix1.head(10)","59dd0fcc":"%%time\n# Make lag encoded features for delta item price\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix1 = pd.merge(matrix1, group, on=['item_id'], how='left')\nmatrix1['item_avg_item_price'] = matrix1['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix1 = pd.merge(matrix1, group, on=['date_block_num','item_id'], how='left')\nmatrix1['date_item_avg_item_price'] = matrix1['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix1 = lag_feature(matrix1, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix1['delta_price_lag_'+str(i)] = \\\n        (matrix1['date_item_avg_item_price_lag_'+str(i)] - matrix1['item_avg_item_price']) \/ matrix1['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix1['delta_price_lag'] = matrix1.apply(select_trend, axis=1)\nmatrix1['delta_price_lag'] = matrix1['delta_price_lag'].astype(np.float16)\nmatrix1['delta_price_lag'].fillna(0, inplace=True)","68f49241":"fetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix1.drop(fetures_to_drop, axis=1, inplace=True)","a6a9b3be":"# Make lag encoded features for delta shop revenue\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix2 = pd.merge(matrix1, group, on=['date_block_num','shop_id'], how='left')\nmatrix2['date_shop_revenue'] = matrix2['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix2 = pd.merge(matrix2, group, on=['shop_id'], how='left')\nmatrix2['shop_avg_revenue'] = matrix2['shop_avg_revenue'].astype(np.float32)\n\nmatrix2['delta_revenue'] = (matrix2['date_shop_revenue'] - matrix2['shop_avg_revenue']) \/ matrix2['shop_avg_revenue']\nmatrix2['delta_revenue'] = matrix2['delta_revenue'].astype(np.float16)\n\nmatrix2 = lag_feature(matrix2, [1], 'delta_revenue')\n\nmatrix2.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","1d080d56":"#show matrix\nmatrix2.columns","1b5eb0a6":"%%time\n# months between the two sales of same products in all shops \/ same products in every shop\ndef interval_calc(im, name, shop):\n    \n    matrix = im.copy()\n    cache = {}\n    matrix[name] = -1\n    \n    for idx, row in matrix.iterrows():\n        key = str(row.item_id)+' '+str(row.shop_id) if shop else row.item_id\n        if key not in cache:\n            if row.item_cnt_month != 0:\n                cache[key] = row.date_block_num\n        else:\n            matrix.at[idx, name] = row.date_block_num - cache[key]\n            cache[key] = row.date_block_num\n    \n    return matrix\n\nmatrix3 = interval_calc(matrix2, 'item_shop_interval', 1)\nmatrix3 = interval_calc(matrix3, 'item_interval', 0)","829c5c87":"matrix3.tail(10)","5e22f966":"#Months between the first sale and current sale of same products in every shop \/ same products in same shop\nmatrix3['item_shop_interval_first'] = matrix3['date_block_num'] - \\\n                                      matrix3.groupby(['item_id', 'shop_id'])['date_block_num'].transform('min')\nmatrix3['item_interval_first'] = matrix3['date_block_num'] - \\\n                                      matrix3.groupby('item_id')['date_block_num'].transform('min')","00c13bcd":"#Lags are set 12, so remove data where date_block_num > 11\nmatrix3 = matrix3[matrix3['date_block_num']>11]\n\n# Fill nan\nfor col in matrix3.columns:\n    matrix3[col].fillna(0, inplace=True)         \n\nmatrix3.tail(10)","524318bc":"# Add its month and days in that month\nmatrix3['month'] = matrix3['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix3['days'] = matrix3['month'].map(days).astype(np.int8)","4f0c3712":"matrix3.to_pickle('data.pkl')","51b2b1dd":"#View Columns\nmatrix3.columns","d1c8c38a":"#Retrieve data\ndata = pd.read_pickle('data.pkl')","98db66e1":"train = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'town_code',\n    'item_category_id',\n    'meta_category_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_category_avg_item_cnt_lag_1',\n    'date_shop_category_avg_item_cnt_lag_1',\n    'date_town_avg_item_cnt_lag_1',\n    'date_item_town_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'item_shop_interval', \n    'item_interval',\n    'item_shop_interval_first', \n    'item_interval_first',\n    'month',\n    'days'\n]]","a00b775a":"#Split Train\/Test\n\nX_train = train[train.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = train[train.date_block_num < 33]['item_cnt_month']\nX_val = train[train.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = train[train.date_block_num == 33]['item_cnt_month']\nX_test = train[train.date_block_num == 34].drop(['item_cnt_month'], axis=1)","c5163606":"# For saving data & output results \/ models\ndef post_processing(model,model_name,X_train,X_val,X_test,y_train,y_val,test):\n    # Here we once again clip the output to 0~20\n    train_pred = model.predict(X_train).clip(0, 20)\n    val_pred = model.predict(X_val).clip(0, 20)\n    test_pred = model.predict(X_test).clip(0, 20)","8ec9a875":"%%time\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)","cf9b1f9c":"#Predict\ny_pred = model.predict(X_val).clip(0, 20)\ny_test = model.predict(X_test).clip(0, 20)\n","8658d92f":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","8a6d8d1d":"# save predictions for an ensemble\npickle.dump(y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(y_test, open('xgb_test.pickle', 'wb'))","3d2f7b95":"rf = RandomForestRegressor(\n    bootstrap=True,\n    max_depth=30,\n    max_features=3,\n    min_samples_leaf=5,\n    min_samples_split=12,\n    n_estimators=200,\n    random_state=42,\n    verbose=1,\n    n_jobs=-1\n)\nrf.fit(X_train, y_train)","e17b8cd4":"#Predict\ny_pred = rf.predict(X_val).clip(0, 20)\ny_test = rf.predict(X_test).clip(0, 20)","ed666ccb":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv('rf_submission.csv', index=False)","26dfbd1c":"# save predictions for an ensemble\npickle.dump(y_pred, open('rf_train.pickle', 'wb'))\npickle.dump(y_test, open('rf_test.pickle', 'wb'))","6938a350":"#Super Basic Linear Model, I really did this to make sure I covered all aspects of the required assignment\nlm=LinearRegression()","2d34df7f":"lm.fit(X_train,y_train)\n\ny_pred2 = lm.predict(X_val).clip(0, 20)\ny_test2 = lm.predict(X_test).clip(0, 20)","4b235d0e":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv('lm_submission.csv', index=False)","7fbf5d61":"# save predictions for an ensemble\npickle.dump(y_pred2, open('lm_train.pickle', 'wb'))\npickle.dump(y_test2, open('lm_test.pickle', 'wb'))","51f9f8f8":"****XGB and Random Forest****","387a581d":"**1) Import Libraries and Files**","92b3370f":"Revenue","5ef85635":"Revenue Summary\n* Revenue seems to peak around the Christmas holiday time period and slow down during the summer\n* Revenue seems to be higher on the weekend then during the week\n* Some shops\/locations perform better than others (in some cases significantly)\n* Revenue is increasing even though transactions are falling most likely due to increase in price","8bbcd40c":"Objective - Predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.","aba93d53":"Linear Model","0be89888":"XG Boost","f5df1e10":"**4) Model Time**","acce3d1e":"**3) Prepossing and Final Prepping of Data**","625399f6":"Random Forest","4cbef56f":"**Item Category**","37935f0f":"**2) Explore Data**","51643c6e":"I tried both XGB Regression and Random Forest.  With Random Forest I got a score of 0.93189.  With XGB I got a score of 0.94078.  I simple did the linear regression to make sure I met the assignment requirements.","1c75ec8f":"**Description about Fields**\n","bd979249":"**Item Price Investigation**","26cf2216":"Transactions","d9731859":"**File Description**\n\n* sales_train.csv - The training set. Daily historical data from January 2013 to October 2015.\n* test.csv - The test set. You need to forecast the sales for these shops and products for November 2015.\n* sample_submission.csv - A sample submission file in the correct format.\n* items.csv - Supplemental information about the items\/products.\n* item_categories.csv - Supplemental information about the items categories.\n* shops.csv- Supplemental information about the shops.","f22ba330":"**Shop**","1899ced0":"**Analysis**","435d9811":"* ID - Id that represents a (Shop, Item) tuple within the test set \n* shop_id - Unique identifier of a shop\n* item_id - Unique identifier of a product\n* item_category_id - Unique identifier of item category\n* item_cnt_day - Number of products sold. You are predicting a monthly amount of this measure\n* item_price - Current price of an item\n* date - Date in format dd\/mm\/yyyy\n* date_block_num - A consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* item_name - Name of item\n* shop_name - Name of shop\n* item_category_name - Name of item category"}}