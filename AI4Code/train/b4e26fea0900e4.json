{"cell_type":{"07f6c574":"code","9670b0a1":"code","68412e6f":"code","6be3fd25":"code","bbb55f0f":"code","7f173b11":"code","4fec3dd9":"code","fe3fb119":"code","448d1f2c":"code","9a043b52":"code","db62b2b7":"code","8e3fd18f":"code","687e08f5":"code","51735277":"code","f101b1f8":"code","0d30e001":"code","65968740":"code","4ad217f8":"code","2cac998c":"code","5a52145c":"code","2876a86d":"code","4641028a":"code","6efe4989":"code","dd3815af":"markdown","466eb347":"markdown","c263b898":"markdown","2f7486b1":"markdown","3a56a03b":"markdown","d6825944":"markdown","7b39cd99":"markdown"},"source":{"07f6c574":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport plotly.figure_factory as ff\n\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertConfig, AdamW, BertForSequenceClassification\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9670b0a1":"import nltk\ntext = nltk.word_tokenize(\"And now for something completely different\")\nnltk.pos_tag(text)","68412e6f":"class SSTDataset:\n    \n    def __init__(self, sentences, tokenizer, max_length, sentiments):\n        \n        self.inputs = sentences\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = sentiments\n        \n    def __getitem__(self, index):\n        \n        sntc = str(self.inputs[index])\n        bert_inputs = self.tokenizer.encode_plus(sntc,\n                                   None,\n                                   add_special_tokens = True,\n                                   max_length = self.max_length,\n                                   pad_to_max_length=True\n                                  )\n        \n        token_ids = bert_inputs['input_ids']\n        token_type = bert_inputs['token_type_ids']\n        token_mask = bert_inputs['attention_mask']\n        \n        return {\n            'token_ids': torch.tensor(token_ids, dtype=torch.long),\n            'token_type': torch.tensor(token_type, dtype=torch.long),\n            'token_mask': torch.tensor(token_mask, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }\n        \n    def __len__(self):\n        \n        return len(self.inputs)","6be3fd25":"base_path = '..\/input\/binary-sst2-dataset'\nbert_path = '..\/input\/bert-base-uncased'\n\ntrain_df = pd.read_csv(base_path+'\/train.csv')\nvalid_df = pd.read_csv(base_path+'\/val.csv')\ntest_df = pd.read_csv(base_path+'\/test.csv')","bbb55f0f":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = BertForSequenceClassification.from_pretrained(bert_path, output_hidden_states=True, output_attentions=True)\ntokenizer = BertTokenizer.from_pretrained(bert_path, do_lower_case = True)\n","7f173b11":"tokenizer.convert_ids_to_tokens(101)","4fec3dd9":"device","fe3fb119":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n    \nnums = test_df[\"sentence\"].apply(new_len) \nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All sentences\"],\n                         colors=[\"coral\"])\nfig.update_layout(title_text=\"distibution of number of words.\", xaxis_title=\"words\", template=\"simple_white\", showlegend=False)\nfig.show()","448d1f2c":"test_sentences = test_df.sentence.values\ntest_targets = test_df.label.values\n\ntest_maxlen = 57 # a little extra for tokenization process. (52+5)\nbs = 64\ntest_dataset = SSTDataset(test_sentences,\n                           tokenizer,\n                           test_maxlen,\n                           test_targets)\ntest_dataLoader = torch.utils.data.DataLoader(test_dataset,\n                                              batch_size = bs)","9a043b52":"def eval_loop(data_loader, model, device):\n    \n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    \n    for batch, batch_data in enumerate(data_loader):\n        \n        ids = batch_data['token_ids'].to(device, dtype=torch.long)\n        type_ids = batch_data['token_type'].to(device, dtype=torch.long)\n        mask_ids = batch_data['token_mask'].to(device, dtype=torch.long)\n        targets = batch_data['targets'].to(device, dtype=torch.long)\n\n        outputs = model(input_ids = ids,\n                        token_type_ids = type_ids,\n                        attention_mask = mask_ids,\n                        labels = targets.view(-1, 1)\n                       )\n        loss, logits, hidden_states, attention = outputs[:4]\n\n        targets_np = targets.cpu().detach().numpy()\n        outputs_np = logits.cpu().detach().numpy()\n\n        fin_targets.extend(targets_np)\n        fin_outputs.extend(outputs_np)    \n        \n    return fin_outputs, fin_targets , attention           ","db62b2b7":"def get_pred_classes(y_preds):\n    return [ row.argmax() for row in y_preds]\ndef calc_acc(y_preds, y_target):\n\n    pred_classes = get_pred_classes(y_preds)\n    return sum(1 for x,y in zip(pred_classes, y_target) if x == y) \/ len(y_target)","8e3fd18f":"os.listdir('..\/input\/sst-models')","687e08f5":"model.load_state_dict(torch.load('..\/input\/sst-models\/tuned_bert_base_model1.bin')) # get best model\nmodel.to(device)\nopt, tgt, attn = eval_loop(test_dataLoader, model, device)\ntest_acc = calc_acc(opt, tgt)\n\nprint(f'Accuracy for test-set of SST2 : {test_acc*100}')","51735277":"def max_length(sentences):\n    max_len = -1\n    for s in sentences:\n        l = len(s.split())\n        if l>max_len:\n            max_len = l\n    return max_len","f101b1f8":"def get_inputs(sentences, targets, model, tokenizer, batch_size):\n    \n    max_len = max_length(sentences) + 8 # in case tokenization breaks a word. \n    exp_dataset = SSTDataset(np.array(sentences),\n                           tokenizer,\n                           max_len,\n                           np.array(targets))\n    \n    exp_dataLoader = torch.utils.data.DataLoader(exp_dataset,\n                                                 batch_size = batch_size)\n    \n    fin_targets = []\n    fin_outputs = []\n    attentions = []\n    tokens = []\n    \n    for batch, batch_data in enumerate(exp_dataLoader):\n\n        ids = batch_data['token_ids'].to(device, dtype=torch.long)\n        type_ids = batch_data['token_type'].to(device, dtype=torch.long)\n        mask_ids = batch_data['token_mask'].to(device, dtype=torch.long)\n        targets = batch_data['targets'].to(device, dtype=torch.long)\n    \n        outputs = model(input_ids = ids,\n                        token_type_ids = type_ids,\n                        attention_mask = mask_ids,\n                        labels = targets.view(-1, 1)\n                       )\n        \n        loss, logits, hidden_states, attn = outputs[:4]\n\n\n        attentions.extend(attn)\n        tokens.extend(ids.cpu().detach().numpy().tolist())\n        \n        targets_np = targets.cpu().detach().numpy()\n        outputs_np = logits.cpu().detach().numpy()\n\n        fin_targets.extend(targets_np)\n        fin_outputs.extend(outputs_np)\n        \n    return fin_outputs, attentions, fin_targets, tokens","0d30e001":"sentences = ['I am so happy boy',\n            'Worst day of my life, I wanna kill  you.',\n             'Fuck Yeahh!!, I am so happy. This is a good day for me.',\n             'Ok so I am about to go on a ride, are you coming?',\n             'It is a plain movie with an average direction.',\n             'Shit I have never watched a movie better than this, I loved the ending.',\n             'Shit I have never watched a movie better than this.',\n             'Shit I have never watched a movie like than this, wow',\n             'The movie just could not be any better.'\n            ] # last 3 are tricky for the model, second last one maybe for humans too lol.\n# first 4 are not movie reviews tho, while sst2 is all about movies. hmm\nlabels = [1, 0, 1, 1, 1, 1, 1, 1, 1]\nbatch_size = 2 # < len of sentences\n\ny_preds, attentions, y_targets, tokens = get_inputs(sentences, labels, model, tokenizer, batch_size)\nprint(f'Accuracy is : {calc_acc(y_preds, y_targets)*100}%')\nprint(f'Probability Distribution for class format - [0(-ve), 1(+ve)] :\\n {torch.softmax(torch.tensor(y_preds), dim = 1)}')\nprint(f'Final predicted classes are : {get_pred_classes(y_preds)}\\nTrue_classes were :           {labels}')","65968740":"s0 = train_df[['sentence', 'label']].query('label == 0')\ns1 = train_df[['sentence', 'label']].query('label == 1')\nexp_df = pd.concat([s0,s1]).reset_index().drop(['index'], axis = 1)\n\nsentences = list(exp_df.sentence.values)\nlabels = list(exp_df.label.values)\nprint(exp_df)","4ad217f8":"# just gives attention map for one sentence, so batch_size = 1, mnakes things simple, can be modified later to incorporate batches. little complex\n\n#attentions shape - [layer(12), batch_size(1), head(12), seq_len(max), seq_len(max)] -- only taking last layer into account here. should i take all layers?? donno, but abhi yahi dekjhre h.\ndef get_attn_map(tokens, pos_2_tok, attentions, tokenizer):\n    for head_j,_ in enumerate(range(12)):\n        for i,_ in enumerate(tokens[0]):\n            attns_word = torch.softmax(attentions[11][0][head_j][i], dim = 0).tolist()\n            max_3_pos = sorted(range(len(attns_word)), key=lambda i: attns_word[i])[-3:]\n#             max_3_attns = [attns_word[pos] for pos in max_3_pos]\n            max_3_ids = [pos_2_tok[pos] for pos in max_3_pos]\n            max_3_tokens = tokenizer.convert_ids_to_tokens(max_3_ids)\n\n            for tok in max_3_tokens:\n                tok_2_attn[tok] += 1\n    \n    return tok_2_attn","2cac998c":"def prune(tok_2_attn):\n    ignore = set(['[CLS]', '[SEP]', '[PAD]', ',', '.', \"'\", ')', '(', ':' , 'a', 'an', 'the', 'on', '-'])\n    keys = list(tok_2_attn.keys())\n    for i in range(len(tok_2_attn)-1, -1, -1):\n        tok = keys[i]\n        if tok in ignore:\n            del(tok_2_attn[tok])\n        elif i < len(tok_2_attn)-1 and keys[i+1][:2] == \"##\":\n            added_attn = tok_2_attn[tok] + tok_2_attn[keys[i+1]]\n            del(tok_2_attn[tok])\n            del(tok_2_attn[keys[i+1]])\n            tok += keys[i+1][2:]\n            keys[i] = tok\n            tok_2_attn[tok] = added_attn","5a52145c":"#use train_df here instead \nbatch_size = 1\npositives = []\nnegatives = []\nfor i in range(100):#len(exp_df)):\n    sentence = [exp_df.iloc[i][0]]\n    label = [exp_df.iloc[i][1]]\n    y_preds, attentions, y_targets, tokens = get_inputs(sentence, label, model, tokenizer, batch_size) #labels is superfluous, but needed tokens and the function already calc. it soo... \n    \n    pos_2_tok = {pos: token for pos, token in enumerate(tokens[0])}\n    tok_2_attn = {tok : 0 for tok in tokenizer.convert_ids_to_tokens(tokens[0])}\n    \n    tok_2_attn = get_attn_map(tokens, pos_2_tok, attentions, tokenizer)\n#     print(tok_2_attn, '\\n')\n    prune(tok_2_attn)\n    tok_2_attn = {k: v for k, v in sorted(tok_2_attn.items(), key=lambda item: -item[1])} #sort by attention\n    top_k = 1 #change this if you want to change. @tenzin\n    i = 0\n#     print(tok_2_attn, '\\n')\n    for tok in tok_2_attn:\n        if i >= top_k:\n            break\n        i += 1\n        if label[0]==1:\n            positives.append(tok)\n        else:\n            negatives.append(tok)\n        \n    #here u get how much a token was attended to, take out top k, except [sep], [cls], punctuations and stopwords (is, the, a etc.)\n    #then can add that in a pool corresponding to label ifelse.\nprint(positives)\nprint(negatives)\n## save bags of words to files here if you want @tenzin","2876a86d":"os.listdir('..\/')\n#save the output pool words in whatever form in the working directory, then when you will commit notebook, commited ntbk ke outputs sec. me miljayega, data(input) dir. is edit only.","4641028a":"import nltk\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))","6efe4989":"if 'a' in stopwords:\n    print('ok')","dd3815af":"## Done\nMajor base work\n\ntoken_id 2 word and various ither mappings.\n\nfunction for finding relevant word to perturb via attention\n\n## TODO\nWordPool\n\nthen perturbation algo stuff (chunk)*","466eb347":"tokenizer sometimes breaks word too, like yuks -> yu ##ks. so isko bhi handle karna padega i guess. hmm maybe whenever you get a high attended word check 2 others, one back and one forwrd, if they have hashes, (aage ya ppeche of broken token depending on token being checked is before ya after), then add that to form a word. ya position bhi use hosakta h initially. tok2pos stuff, - CP challenge for dhiman lol main chala ipr, dwdm karne :\/ ","c263b898":"1. Experiment 2 - Specialized (attention based) manual testing + attention map -> Dhiman work from here","2f7486b1":"stopwords","3a56a03b":"Getting attention maps - main part","d6825944":"Esperiment 1 - basic","7b39cd99":"this was just for me, when i was doing manual perturbation, but you consider train_df(loaded initally) directly"}}