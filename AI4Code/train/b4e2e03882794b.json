{"cell_type":{"05efa1ab":"code","298030bc":"code","669d4602":"code","dd60137c":"code","c6d77ba4":"code","45abc043":"code","b942ce9a":"code","b301bcc6":"code","d4f4f82b":"code","46b65178":"code","5270447a":"code","ef3f5553":"code","542ecaa6":"code","eca69685":"code","c2ea9c80":"code","63f49f7d":"code","0d393c75":"code","97c7cde1":"code","3150459c":"code","1be253e9":"code","31536a19":"code","76d407fa":"code","41cf1e2c":"code","51160c0c":"code","72f95903":"code","2b503f79":"code","c79e0c0e":"code","8008c4a1":"code","da1fbaec":"code","514fae80":"code","4f889a2c":"markdown","a51e5491":"markdown","33088800":"markdown","bca050b4":"markdown","e8e2e625":"markdown","de0d60fa":"markdown","e74afd0d":"markdown","6d673a11":"markdown","ed1234c3":"markdown","aff0e17b":"markdown","a9422765":"markdown","1d1add81":"markdown","f3c2dd36":"markdown","ad0e6206":"markdown","85f4aba1":"markdown","68ff8abe":"markdown","6750e52c":"markdown","6942916c":"markdown"},"source":{"05efa1ab":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n# Limit numeric output to 3 decimal points\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) ","298030bc":"ls ..\/*","669d4602":"# Load data\ntrain_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_raw.shape, test_raw.shape","dd60137c":"train_raw.head()","c6d77ba4":"train_raw.info()","45abc043":"# Check missing values in columns\nnum_missing = train_raw.isnull().mean()\nnum_missing = num_missing[num_missing != 0].sort_values(ascending = False)\nnum_missing","b942ce9a":"# Display the distribution of missing value percentages for the variable that have missing values\nplt.subplots(figsize=(12, 5))\nsns.barplot(num_missing.index, num_missing.values)\nplt.xticks(rotation = '45')\nplt.show()","b301bcc6":"num_missing = train_raw.isnull().sum()\nnum_missing[num_missing != 0]","d4f4f82b":"price = train_raw.SalePrice","46b65178":"sns.distplot(price)","5270447a":"price.describe()","ef3f5553":"log_price = np.log(price)\nsns.distplot(log_price)","542ecaa6":"# Look at numeric features\nnumeric_train = train_raw.select_dtypes(exclude=['object']).drop('Id', axis = 1)\nnumeric_train.describe()","eca69685":"# Use heat map to show the correlations between different numeric variables and sales price, \n# as well as the correlations between each pair of themselves.\nplt.subplots(figsize = (10,10))\nsns.heatmap(numeric_train.corr(), cmap=sns.color_palette('coolwarm'), square=True)\nplt.title('Correlations of numeric features')\nplt.show()","c2ea9c80":"# Look at categorical features\ncate_train = train_raw.select_dtypes(include=['object'])\ncate_train.head()","63f49f7d":"# Count unique values in each categorical feature\nunique_count_cate = cate_train.apply(lambda x: len(x.unique())).sort_values(ascending=False)\n\n# Display them in descending order\nplt.subplots(figsize = (15, 6))\nsns.barplot(unique_count_cate.index, unique_count_cate.values)\nplt.title('Number of unique values for each categorical features')\nplt.xticks(rotation = '90')\nplt.show()","0d393c75":"class Engineer:\n    def fit(self, X, y = None):\n        df = X.copy()\n        self.LotFrontage_mean = df.LotFrontage.mean()\n        self.ords = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, \n                     'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, \n                    'Fin': 3, 'RFn': 2, 'Unf': 1, \n                    'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, \n                    'SBrkr': 4, 'FuseA': 3, 'FuseF': 2, 'FuseP': 1, 'Mix': 2.5, \n                    'Shed': 1, 'Gar2': 1, 'TenC': 1, 'Othr': 0.5}\n        \n       \n    def transform(self, X, y = None):\n        df = X.copy()\n        \n        df['LotFrontage'] = df['LotFrontage'].fillna(self.LotFrontage_mean)\n        df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)       \n        df['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n        \n        ordinal_cols = ['PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', \n                       'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'Electrical']\n        dummy_cols = ['Alley', 'GarageType', 'MasVnrType']\n        df[ordinal_cols] = df[ordinal_cols].fillna('None')\n        df[dummy_cols] = df[dummy_cols].fillna('None')\n        \n        for l in ordinal_cols:\n            df[l] = df[l].map(lambda x: self.ords.get(x, 0))\n            \n        df = pd.get_dummies(df, drop_first=True)  \n        \n        return df\n            \n    \n    def fit_transform(self, X, y = None):\n        self.fit(X)\n        return self.transform(X)\n    ","97c7cde1":"Engineer().fit_transform(train_raw).head()","3150459c":"# Split the traning data into X (feature matrix) and Y (target variable)\nX = train_raw.drop('SalePrice', axis=1)\ny = log_price\n\nX.shape, y.shape","1be253e9":"# Create engineer object to transform data\nen = Engineer()\nX = en.fit_transform(X)\ntest = en.transform(test_raw)\n\n# To keep the training and test data have the same columns\nX, test = X.align(test, join='outer', axis=1, fill_value=0)\n\nX.shape, test.shape","31536a19":"# Use a 80 vs. 20 split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 2020)\n\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","76d407fa":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","41cf1e2c":"rf = RandomForestRegressor(random_state=2020)\nrf.fit(X_train, y_train)","51160c0c":"preds_train = rf.predict(X_train)\npreds_valid = rf.predict(X_valid)","72f95903":"print('Training RMSE: {:.3f}'.format(rmse(y_train, preds_train)))\nprint('Validation RMSE: {:.3f}'.format(rmse(y_valid, preds_valid)))","2b503f79":"xgb = XGBRegressor(random_state=2020)\nxgb.fit(X_train, y_train)\n\npreds_train = xgb.predict(X_train)\npreds_valid = xgb.predict(X_valid)","c79e0c0e":"print('Training RMSE: {:.3f}'.format(rmse(y_train, preds_train)))\nprint('Validation RMSE: {:.3f}'.format(rmse(y_valid, preds_valid)))","8008c4a1":"# Predict test data\ntest_results = xgb.predict(test)\ntest_results = np.exp(test_results)","da1fbaec":"# Save the results to csv file\npredict_submission = pd.DataFrame()\npredict_submission['Id'] = [1461 + i for i in range(test.shape[0])]\npredict_submission['SalePrice'] = test_results\n","514fae80":"predict_submission.to_csv('house_submission.csv', header=True, index = False)","4f889a2c":"## Model building","a51e5491":"### Random Forest","33088800":"### Explore missing values","bca050b4":"### Explore categorical features","e8e2e625":"## Some simple EDA","de0d60fa":"## Use XGBoost to predict test data","e74afd0d":"## Feature engineering","6d673a11":"### Transform the training data and test data using the Engineer class above","ed1234c3":"### XGBoost","aff0e17b":"The distribution of the sale price is right skewed by some extremely large numbers, apparently deviates from normal distribution. \n\nI will take the log of sales price.","a9422765":"### Explore target variable: SalePrice","1d1add81":"### Split training data into training set and validation set","f3c2dd36":"### Explore numeric features","ad0e6206":"XGBoost has a less RMSE on the validation set and less overfitting phenomenon compared with random forest.","85f4aba1":"The minimum value is greater than 0, thus no need to plus 1 when take the log of sales price.","68ff8abe":"The distribution of the log sale price looks much more like a normal distribution.","6750e52c":"* Most of the numeric features are positevely corelated with sale price. \n* `OverallQual` has the strongest correlation with sale price.\n* Some of the numeric features are correlated with each other.\n","6942916c":"Numeric features are varied in scales."}}