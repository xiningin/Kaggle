{"cell_type":{"b54cf740":"code","8aa7b0bc":"code","84e4e697":"code","7cf9ca5e":"code","53387722":"code","ac96d6d2":"code","cc26f10f":"code","b55ea7f4":"code","66f7a740":"code","bf3d8bb9":"code","24ff515e":"code","71dabe04":"code","d6e95579":"code","61c85b7b":"markdown","a293c4d4":"markdown","62df05bc":"markdown","e51cec83":"markdown","498138e5":"markdown","a48d77a4":"markdown","231262f7":"markdown","7546b210":"markdown","371e357f":"markdown","83949fc0":"markdown","8619778f":"markdown","9ae0e22f":"markdown","9cb93006":"markdown","3d3f87c1":"markdown","2341e931":"markdown","8d400dbb":"markdown","04c1e5de":"markdown","550b5468":"markdown","8de4129b":"markdown","0bd6a4bc":"markdown","ea6bc1c3":"markdown","8cbe584d":"markdown","f23c9468":"markdown"},"source":{"b54cf740":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8aa7b0bc":"fakedataset = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake = fakedataset[:5000]","84e4e697":"realdataset = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")\nreal = realdataset[:5000]","7cf9ca5e":"real[\"class\"] = 1\nfake[\"class\"] = 0","53387722":"real[\"text\"] = real[\"title\"] + \" \" + real[\"text\"]\nfake[\"text\"] = fake[\"title\"] + \" \" + fake[\"text\"]\n\nreal.drop([\"subject\", \"date\", \"title\"], axis = 1)\nfake.drop([\"subject\", \"date\", \"title\"], axis = 1)","ac96d6d2":"dataset = real.append(fake, ignore_index = True)","cc26f10f":"del real, fake","b55ea7f4":"import nltk\n\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")","66f7a740":"import re\nimport string\nstopwords = nltk.corpus.stopwords.words('english')\nstemmer = nltk.PorterStemmer()\n\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndataset['body_len'] = dataset['text'].apply(lambda x: len(x) - x.count(\" \"))\ndataset['punct%'] = dataset['text'].apply(lambda x: count_punct(x))\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [stemmer.stem(word) for word in tokens if word not in stopwords]\n    return text\n    ","bf3d8bb9":"from sklearn.model_selection import train_test_split\n\nX=dataset[['text', 'body_len', 'punct%']]\ny=dataset['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=42)","24ff515e":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['text'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['text'])\ntfidf_test = tfidf_vect_fit.transform(X_test['text'])\n\nX_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)\n\nX_train_vect.head()","71dabe04":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import accuracy_score as acs\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d6e95579":"rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\nrf_model = rf.fit(X_train_vect, y_train)\n\ny_pred = rf_model.predict(X_test_vect)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average='binary')\nprint('Precision: {} \/ Recall: {} \/ F1-Score: {} \/ Accuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), round(acs(y_test,y_pred), 3)))\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nclass_label = [0, 1]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n","61c85b7b":"We build a RandomForestClassifier instance and then train it on our training subset and then evaluate it over our test subset. And then make a confusion matrix using the [heatmap function](https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html#seaborn.heatmap) from seaborn","a293c4d4":"# Basic Imports","62df05bc":"The dataset doesn't have any classes associated with it, so to enable Supervised Learning I've given a class of 1 to real news and 0 to fake news","e51cec83":"**Initialising the Fake News Dataset into a DataFrame**","498138e5":"**Initialising the Real News Dataset into a DataFrame**","a48d77a4":"Here, I've used the [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) function from sklearn to create random train and test subsets of the dataset. ","231262f7":"Here, we import the basic packages like **pandas** and **numpy** for I\/O and Linear Algebra. Link to the documentation of the packages can be found here:\n* [numpy](https:\/\/numpy.org\/doc\/stable\/)\n* [pandas](https:\/\/pandas.pydata.org\/docs\/reference\/index.html#api)\n* [os](https:\/\/docs.python.org\/3\/library\/os.html)","7546b210":"**Concatenating Text and Title into a Single Column (to improve performance)**","371e357f":"**Creating Train, Test and Split Datasets**","83949fc0":"**!! Download stopwords and punkt from nltk**","8619778f":"**Creating a Single DataFrame**","9ae0e22f":"**Adding Classes to the Datasets**","9cb93006":"# Pre-Processing","3d3f87c1":"Here, I've used the [Porter Stemmer](https:\/\/www.nltk.org\/howto\/stem.html) from the nltk module to stem the words. Also, I've converted everything into lowercase and removed stopwords.","2341e931":"# Training The Model","8d400dbb":"**Performing basic pre-processing on the Corpus**","04c1e5de":"I tried Training using the full corpus but even Kaggle Kernels ran out of memory, so I've shortened both the corpus to only have 5000 instances of each class","550b5468":"We, use the [TfidfVectoriser](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) from sklearn module to transform the matrix into a tf-idf representation. This is representation is commonly used in document classification and information retrieval ","8de4129b":"**Vectorising the Corpus using TfidfVectorizer**","0bd6a4bc":"**Importing Necessary Packages from sklearn**","ea6bc1c3":"We use the following functions for building our model:\n* [Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) : A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n* [precision_recall_fscore_support](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) : To compute precision, recall, F-measure and support for each class\n* [accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) : To compute the accuracy classification score\n* [matplotlib](https:\/\/matplotlib.org\/api\/index.html)\n* [seaborn](https:\/\/seaborn.pydata.org\/api.html)","8cbe584d":"Just Predicting with the Title\/Headlines wouldn't give good results so, I've Concatenated both (Title and Text) into a single column of the individual dataframes. Also, I haven't tried to exploit the relationship between subjects so I've dropped those columns.","f23c9468":"Joined both dataframes to just have a single dataframe"}}