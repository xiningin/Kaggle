{"cell_type":{"b3ff93b2":"code","52966065":"code","e5ec0229":"code","a06fc044":"code","91388408":"code","f2e083c3":"code","3506a98e":"code","5d9fe249":"code","80881cea":"markdown"},"source":{"b3ff93b2":"import torch\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd","52966065":"# Hyperparameters\nNUM_EPOCHS = 5000\nHIDDEN_SIZE = 100\nBATCH_SIZE = 6\nLEARNING_RATE = 0.01","e5ec0229":"# Open dataset\ndata = open('\/kaggle\/input\/dinosaur-island\/dinos.txt', 'r').read()\ndata = data.lower()\nexamples = data.split('\\n')\nchars = sorted(set(' '.join(examples)))\ndata_size, vocab_size, max_len = len(examples), len(chars), len(max(examples, key=len))\nch_to_idx = {c:i for i, c in enumerate(chars)}\nidx_to_ch = {i:c for i, c in enumerate(chars)}\nprint('Dataset Size: {} Vocabulary Size: {} | Max Length: {}'.format(len(examples), vocab_size, max_len))\nprint(ch_to_idx)\nprint(idx_to_ch)","a06fc044":"# Construct training set. Y is simply X moved by one place\nX = torch.zeros(size=(data_size, max_len, vocab_size))\nY = torch.zeros(size=(data_size, max_len), dtype=torch.long)\nprint_once = False\n\n# Padding\nfor i in range(len(examples)):\n    while len(examples[i]) < max_len:\n        examples[i] += ' '\n\nfor example_idx, example in enumerate(examples):\n    # Remove last character for input sequence\n    text_x = example[:-1]\n    for i, char in enumerate(text_x):\n        X[example_idx][i][ch_to_idx[char]] = 1\n\n    # Remove first character for target sequence\n    text_y = example[1:]\n    for i, char in enumerate(text_y):\n        Y[example_idx][i] = ch_to_idx[char]\n    \n    if print_once:\n        print(f'Example: {example} | {text_x} {text_y} | X: {X[example_idx]} | Y: {Y[example_idx]}')\n        print_once = False\n\nprint('X: {} | Y: {}'.format(X.size(), Y.size()))","91388408":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        # RNN Layer\n        self.rnn = nn.RNN(input_size=vocab_size, hidden_size=HIDDEN_SIZE, batch_first=True)\n        # Fully connected layer\n        self.fc = nn.Linear(HIDDEN_SIZE, vocab_size)\n    \n    def forward(self, x):\n        # Initializing hidden state for first input using method defined below\n        batch_size = x.size(0)\n        hidden = self.init_hidden(batch_size)\n\n        # Passing in the input and hidden state into the model and obtaining outputs\n        out, hidden = self.rnn(x, hidden)\n        \n        # Reshaping the outputs such that it can be fit into the fully connected layer\n        out = out.contiguous().view(-1, HIDDEN_SIZE)\n        out = self.fc(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        # This method generates the first hidden state of zeros which we'll use in the forward pass\n        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n        hidden = torch.zeros(1, batch_size, HIDDEN_SIZE)\n        return hidden\n\nmodel = Model()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","f2e083c3":"for epoch_idx in range(NUM_EPOCHS):\n    model.train()\n    optimizer.zero_grad()\n    output, hidden = model(X)\n    loss = loss_fn(output, Y.view(-1).long())\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch_idx + 1) % 100 == 0:\n        print('Epoch: {} | Loss: {}'.format(epoch_idx + 1, loss))\n","3506a98e":"import string\nwith torch.no_grad():\n    for word in range(10):\n        model.eval()\n        word = random.choice(string.ascii_lowercase)\n        \n        for letter_idx in range(max_len):\n            start = torch.zeros(size=(1, len(word), vocab_size))\n            for w_idx, w in enumerate(word):\n                start[0][w_idx][ch_to_idx[w]] = 1\n                    \n            out, hid = model(start)\n            prob = nn.functional.softmax(out[-1], dim=0).data\n#             high_prob, char_idx = torch.max(prob, dim=0)\n            prob = prob.data.numpy()\n            probs = np.array(prob)\n            probs \/= probs.sum()\n            char_idx = np.random.choice(range(0, 27), p=probs)\n            letter = idx_to_ch[char_idx.item()]\n            if letter == ' ':\n                break\n            else:            \n                word += letter\n\n        print(word)","5d9fe249":"def predict(model, characters):\n    # One-hot encoding our input to fit into the model\n    start = torch.zeros(size=(1, len(characters), vocab_size))\n    for idx, ch in enumerate(characters):\n        start[0][idx][ch_to_idx[ch]] = 1\n    out, hidden = model(start)\n\n    prob = nn.functional.softmax(out[-1], dim=0).data\n    # Taking the class with the highest probability score from the output\n    char_ind = torch.max(prob, dim=0)[1].item()\n\n    return idx_to_ch[char_ind], hidden\n\ndef sample(model, start='h'):\n    model.eval() # eval mode\n    start = start.lower()\n    # First off, run through the starting characters\n    chars = [ch for ch in start]\n    size = 20 - len(chars)\n    # Now pass in the previous characters and get a new one\n    for ii in range(size):\n        char, h = predict(model, chars)\n        chars.append(char)\n\n    return ''.join(chars)\n\nsample(model, 'a')","80881cea":"**Resources**\n\nhttps:\/\/blog.floydhub.com\/a-beginners-guide-on-recurrent-neural-networks-with-pytorch\/"}}