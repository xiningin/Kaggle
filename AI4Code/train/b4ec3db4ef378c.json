{"cell_type":{"0162eee5":"code","3df3da5e":"code","b10479a7":"code","ae655988":"code","fcdcf2fb":"code","486f9461":"code","8ef54ee9":"code","44451634":"code","3f67483d":"code","ccfe453a":"code","8585673e":"code","36e29ac9":"code","50b666c7":"code","b3c4abca":"code","1047d5a4":"code","7acb6aff":"code","e1b64fdb":"code","590481f7":"code","30508842":"code","12e9241c":"code","9869cb2c":"code","672fb156":"code","6269a200":"code","efec901d":"code","eedb0bed":"code","1fa76a0a":"code","84bbaa13":"code","1c85adfb":"code","ed69570c":"code","6c67c773":"code","7186c89f":"code","4ef3b662":"code","35fd19a5":"code","5559d28e":"code","dd9bf9e4":"code","42bf5324":"code","c2842b81":"code","e019afd3":"code","f44dda0e":"code","eb9b81ef":"code","0be646ce":"code","d871c7a2":"code","9ff468e0":"code","c45a77a6":"code","c611a7de":"code","c97f5905":"code","6bf21868":"code","dc63e53a":"code","bb06080e":"code","2a832a37":"code","595582d1":"code","e041a7f2":"code","acca16bf":"code","2c46b659":"code","1b763b13":"markdown","0430c8f5":"markdown","0fadf0a7":"markdown","d35d93de":"markdown","c69ed75e":"markdown","39c13a6c":"markdown","c686e0a7":"markdown","1c7bc473":"markdown","6daa11df":"markdown","a83f1045":"markdown","a1df26d8":"markdown","7c318ab9":"markdown","5e388d7e":"markdown","c703afd5":"markdown","1d3581b6":"markdown","ac9defda":"markdown","794894cf":"markdown","be70d4e6":"markdown","44406e20":"markdown"},"source":{"0162eee5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3df3da5e":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')","b10479a7":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","ae655988":"train.shape","fcdcf2fb":"test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(test.shape)\ntest.head(5)","486f9461":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\nprint(train.shape,test.shape)","8ef54ee9":"train.columns","44451634":"train.describe()","3f67483d":"train['SalePrice'].describe()","ccfe453a":"#histogram and normal probability plot\nplt.figure(figsize=(12,8))\nsns.distplot(train['SalePrice'], fit=norm);\n\nplt.figure(figsize=(12,8))\nres = stats.probplot(train['SalePrice'], plot=plt)","8585673e":"#applying log transformation\ntarget = train['SalePrice']\ntarget = np.log(target)","36e29ac9":"#transformed histogram and normal probability plot\nfig = plt.figure(figsize=(12,8))\nsns.distplot(target, fit=norm);\nfig = plt.figure(figsize=(12,8))\nres = stats.probplot(target, plot=plt)","50b666c7":"feature_num = train.select_dtypes(include=[np.number])\nfeature_num.columns\n#sns.heatmap(train.corr(), annot = True)","b3c4abca":"feature_cat = train.select_dtypes(include=[np.object])\nfeature_cat.columns","1047d5a4":"#Finding Correlation coefficients between numeric features and SalePrice\ncorrelation = feature_num.corr()\nprint(correlation['SalePrice'].sort_values(ascending=False))","7acb6aff":"sns.set(font_scale=2)\nplt.figure(figsize = (50,35))\nax = sns.heatmap(feature_num.corr(), square = True, vmax = .8, annot = True, \n                 annot_kws={\"size\": 25},fmt='.1f',cmap='PiYG', linewidths=.5)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","e1b64fdb":"k = 10\ncols = correlation.nlargest(k, 'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train[cols].values.T)\nf, ax = plt.subplots(figsize = [14,12])\nsns.heatmap(cm, square = True, vmax = 0.8, annot = True, cmap = 'viridis', linewidths=0.01,\n            xticklabels = cols.values, yticklabels = cols.values, linecolor = 'white', annot_kws={'size':12})\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","590481f7":"#Here we can see how each feature is correlated with SalePrice.\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","30508842":"sns.scatterplot(x = train['GarageCars'], y = train['SalePrice'])","12e9241c":"sns.regplot(x = train['GarageCars'], y = train['SalePrice'], scatter = True, fit_reg = True)","9869cb2c":"sns.boxplot(train['SalePrice'])","672fb156":"fig, ((ax1,ax2),(ax3,ax4), (ax5,ax6)) = plt.subplots(ncols = 2, nrows = 3, figsize=(12,10))\nsns.regplot(x = train['OverallQual'], y = train['SalePrice'], scatter = True, fit_reg = True, ax = ax1)\nsns.regplot(x = train['GrLivArea'], y = train['SalePrice'], scatter = True, fit_reg = True, ax = ax2)\nsns.regplot(x = train['GarageArea'], y = train['SalePrice'], scatter = True, fit_reg = True, ax = ax3)\nsns.regplot(x = train['FullBath'], y = train['SalePrice'], scatter = True, fit_reg = True, ax = ax4)\nsns.regplot(x = train['YearBuilt'], y = train['SalePrice'], scatter = True, fit_reg = True, ax = ax5)\nsns.regplot(x = train['TotalBsmtSF'], y = train['SalePrice'], scatter = True, fit_reg = True, ax = ax6)","6269a200":"f,ax = plt.subplots(figsize=[12,10])\nsns.boxplot(x = train['SaleType'], y = train['SalePrice'])","efec901d":"f,ax = plt.subplots(figsize=[12,10])\nfig = sns.boxplot(x = train['OverallQual'], y = train['SalePrice'])","eedb0bed":"first_quartile = train['SalePrice'].quantile(0.25)\nthird_quartile = train['SalePrice'].quantile(0.75)\nIQR = third_quartile-first_quartile\nnew_boundary = third_quartile+3*IQR\n#train.drop(train[train['SalePrice']>new_boundary].index, axis = 0, inplace = True)","1fa76a0a":"sns.boxplot(train['SalePrice'])","84bbaa13":"#Concatenate train and test\n# train.drop(\"SalePrice\", axis = 1, inplace = True)\ntotal = pd.concat((train, test)).reset_index(drop=True)\nprint(total.shape, total.columns)\ntotal_df = [train, test]","1c85adfb":"for dataset in total_df:\n    dataset.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)","ed69570c":"#missing value analysis\nmissing = total.isnull().sum().sort_values(ascending=False)\nmissing = missing[missing>0]\nmissing ","6c67c773":"plt.figure(figsize=(15,8))\nmissing.plot.bar()","7186c89f":"feature_tot_num = total.select_dtypes(include=[np.number])\nmissing_tot_numeric = feature_tot_num.isnull().sum().sort_values(ascending=False)\nmissing_tot_numeric_percent = (feature_tot_num.isnull().sum()\/feature_tot_num.isnull().count()).sort_values(ascending=False)\nmissing_num_data = pd.concat([missing_tot_numeric, missing_tot_numeric_percent], axis=1,join='outer', keys=['Total Missing Count', '% of Total Observations'])\nmissing_num_data.index.name =' Numeric Feature'\n\nprint(missing_num_data.head(20), feature_tot_num.shape)\nprint(feature_tot_num['LotFrontage'].isnull().sum())","4ef3b662":"missing_num_data = missing_num_data[missing_num_data>0]\n\nplt.figure(figsize=(15,8))\nmissing_num_data.head(10).plot.barh()","35fd19a5":"feature_tot_cat = total.select_dtypes(include=[np.object])\nmissing_tot_cat = feature_tot_cat.isnull().sum().sort_values(ascending=False)\nmissing_tot_cat_percent = (feature_tot_cat.isnull().sum()\/feature_tot_cat.isnull().count()).sort_values(ascending=False)\nmissing_cat_data = pd.concat([missing_tot_cat, missing_tot_cat_percent], axis=1,join='outer', keys=['Total Missing Count', '% of Total Observations'])\nmissing_cat_data.index.name =' Categorical Feature'\nmissing_cat_data.head(25)\nprint(feature_tot_cat.shape)","5559d28e":"missing_tot_cat = feature_tot_cat.isnull().sum(axis=0).reset_index()\nmissing_tot_cat.columns = ['column_name', 'missing_count']\nmissing_tot_cat = missing_tot_cat.loc[missing_tot_cat['missing_count']>0]\nmissing_tot_cat = missing_tot_cat.sort_values(by='missing_count')\n\nind = np.arange(missing_tot_cat.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_tot_cat.missing_count.values, color='b')\nax.set_yticks(ind)\nax.set_yticklabels(missing_tot_cat.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Observations Count - Categorical Features\")\nplt.show()\n","dd9bf9e4":"for dataset in total_df:\n    dataset['LotFrontage'] = dataset['LotFrontage'].fillna(dataset['LotFrontage'].mean())#float\n    dataset['BsmtQual'] = dataset['BsmtQual'].fillna(dataset['BsmtQual'].mode()[0])\n    dataset['BsmtCond'] = dataset['BsmtCond'].fillna(dataset['BsmtCond'].mode()[0])\n    dataset['BsmtExposure'] = dataset['BsmtExposure'].fillna(dataset['BsmtExposure'].mode()[0])\n    dataset['BsmtFinType1'] = dataset['BsmtFinType1'].fillna(dataset['BsmtFinType1'].mode()[0])\n    dataset['MasVnrType'] = dataset['MasVnrType'].fillna(dataset['MasVnrType'].mode()[0])\n    dataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(dataset['MasVnrArea'].mean())#float\n    dataset['BsmtFinType2'] = dataset['BsmtFinType2'].fillna(dataset['BsmtFinType2'].mode()[0])\n    dataset['Electrical'] = dataset['Electrical'].fillna(dataset['Electrical'].mode()[0])\n    dataset['FireplaceQu'] = dataset['FireplaceQu'].fillna(dataset['FireplaceQu'].mode()[0])\n    dataset['GarageType'] = dataset['GarageType'].fillna(dataset['GarageType'].mode()[0])\n    dataset['GarageYrBlt'] = dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mean())#float\n    dataset['GarageFinish'] = dataset['GarageFinish'].fillna(dataset['GarageFinish'].mode()[0])\n    dataset['GarageQual'] = dataset['GarageQual'].fillna(dataset['GarageQual'].mode()[0])\n    dataset['GarageCond'] = dataset['GarageCond'].fillna(dataset['GarageCond'].mode()[0])\n    ########\n    dataset['GarageCars'] = dataset['GarageCars'].fillna(dataset['GarageCars'].mean())\n    dataset['GarageArea'] = dataset['GarageArea'].fillna(dataset['GarageArea'].mean())\n    dataset['SaleType'] = dataset['SaleType'].fillna(dataset['SaleType'].mode()[0])\n    dataset['Functional'] = dataset['Functional'].fillna(dataset['Functional'].mode()[0])\n    dataset['KitchenQual'] = dataset['KitchenQual'].fillna(dataset['KitchenQual'].mode()[0])\n    dataset['BsmtHalfBath'] = dataset['BsmtHalfBath'].fillna(dataset['BsmtHalfBath'].mean())\n    dataset['BsmtFullBath'] = dataset['BsmtFullBath'].fillna(dataset['BsmtFullBath'].mean())\n    dataset['TotalBsmtSF'] = dataset['TotalBsmtSF'].fillna(dataset['TotalBsmtSF'].mean())\n    dataset['BsmtUnfSF'] = dataset['BsmtUnfSF'].fillna(dataset['BsmtUnfSF'].mean())\n    dataset['BsmtFinSF2'] = dataset['BsmtFinSF2'].fillna(dataset['BsmtFinSF2'].mean())\n    dataset['BsmtFinSF1'] = dataset['BsmtFinSF1'].fillna(dataset['BsmtFinSF1'].mean())\n    dataset['Exterior2nd'] = dataset['Exterior2nd'].fillna(dataset['Exterior2nd'].mode()[0])\n    dataset['Exterior1st'] = dataset['Exterior1st'].fillna(dataset['Exterior1st'].mode()[0])\n    dataset['Utilities'] = dataset['Utilities'].fillna(dataset['Utilities'].mode()[0])\n    dataset['MSZoning'] = dataset['MSZoning'].fillna(dataset['MSZoning'].mode()[0])","42bf5324":"#feature_tot_cat = pd.get_dummies(feature_tot_cat)\n#feature_tot_cat.shape\n#str(feature_tot_cat.isnull().values.sum())","c2842b81":"train_df = pd.get_dummies(train, drop_first=True)","e019afd3":"df = pd.concat([train, test])\ndf1 = pd.get_dummies(df, drop_first=True)","f44dda0e":"train = df1.iloc[: 1460, :]\ntest = df1.iloc[1460: , :]\nprint(train.shape,test.shape)","eb9b81ef":"X = train.drop('SalePrice', axis=1)\ny = train['SalePrice']\ntest = test.drop('SalePrice', axis=1)","0be646ce":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer","d871c7a2":"#split the data to train the model \nX_train,X_test,y_train,y_test = train_test_split(X, target,test_size = 0.3,random_state= 0)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","9ff468e0":"n_folds = 5\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_test,y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","c45a77a6":"lr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)\nprint('rmse on train',rmse_CV_train(lr).mean())\nprint('rmse on train',rmse_CV_test(lr).mean())","c611a7de":"#plot between predicted values and residuals\nplt.scatter(train_pre, train_pre - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_pred,y_pred - y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","c97f5905":"# Plot predictions - Real values\nplt.scatter(train_pre, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_pred, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","6bf21868":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","dc63e53a":"tree_regressor = DecisionTreeRegressor()\ntree_regressor.fit(X, y)\ntree_regressor.score(X, y)","bb06080e":"forest_regressor = RandomForestRegressor()\nforest_regressor.fit(X, y)\nforest_regressor.score(X, y)","2a832a37":"Xgb_regressor = XGBRegressor()\nXgb_regressor.fit(X, y)","595582d1":"y_predict = Xgb_regressor.predict(test)","e041a7f2":"submission_sample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n","acca16bf":"dataset = pd.DataFrame({\n    'Id': submission_sample['Id'],\n    'SalePrice': y_predict\n})","2c46b659":"dataset.to_csv('output.csv', index=False)","1b763b13":"At initial glance it is observed that there are two green colored squares that get attention.\n\nThe first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables.\nSecond one refers to the 'GarageCars' and 'GarageArea'. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs.\nHeatmaps are great to detect this kind of multicollinearity situations and in problems related to feature selection like this project, it comes as an excellent exploratory tool.\n\nAnother aspect I observed here is the 'SalePrice' correlations. As it is observed that 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' are shaking hands firmly with SalePrice, however we cannot exclude the fact that rest of the features have some level of correlation to the SalePrice.  So, for a clear vision lets limit it to the top most numerical variables.","0430c8f5":"# Separating Numerical and Categorical variables","0fadf0a7":"### Defining cross_val_score function for both train and test sets separately","d35d93de":"# Categorical Features\nLet us look at the missing values in categorical features in detail","c69ed75e":"# EDA\n\nThere are 1460 instances of training data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\nQuantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\nQualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,","39c13a6c":"## Importing Libraries ","c686e0a7":"## Relationship of the target variable with numerical variables","1c7bc473":"# Locating missing values","6daa11df":"# Modeling","a83f1045":"## Analysing the target feature","a1df26d8":"# Preparing the data","7c318ab9":"This notebook is written by a beginner for beginners. I hope it may help.","5e388d7e":"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line. A simple data transformation can solve the problem.","c703afd5":"## Load datasets","1d3581b6":"## Features in the dataset","ac9defda":"# Visualization between 'SalePrice' and correlated variables\n\nVisualisation of 'OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt' features with respect to SalePrice in the form of scatter plot for better understanding.","794894cf":"#### Linear model without Regularization","be70d4e6":"it is observed that GarageCars & GarageArea are closely correlated . Similarly TotalBsmtSF and 1stFlrSF are also closely correlated. So far we can conclude that\n\n1. OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n2. 'GarageCars' and 'GarageArea' are strongly correlated variables. It is because the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. So it is hard to distinguish between the two. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n3. 'TotalBsmtSF' and '1stFloor' also seem to be twins. In this case let us keep 'TotalBsmtSF'\n4. 'TotRmsAbvGrd' and 'GrLivArea', twins\n5. 'YearBuilt' it appears like is slightly correlated with 'SalePrice'.\n\nNow that we have gathered some basic information on the data, it\u2019s a good idea to just go a little bit deeper into the challenges that the data might pose.\n\nThere are two factors mostly observed in EDA exercise which are missing values and outliers.","44406e20":"# House Price Prediction"}}