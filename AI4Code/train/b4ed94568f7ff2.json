{"cell_type":{"146c1ece":"code","37b13595":"code","a1603f3b":"code","6f709868":"code","8d566770":"code","f147c89c":"code","768e35c3":"code","154fea3a":"code","29e08957":"code","cfb0f198":"code","6bcf85c1":"code","95c7e5f3":"code","921ad0cc":"code","73cf8bcd":"code","0307dd35":"markdown","7d5fbe33":"markdown","f174c6e1":"markdown","1c176103":"markdown"},"source":{"146c1ece":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","37b13595":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df  = pd.read_csv('..\/input\/test.csv')","a1603f3b":"def corr_plot(v1, v2, n=10):\n    new = pd.DataFrame()\n    new[v1] = pd.cut(train_df[v1], 10, labels=[v1 + '_' + str(i) for i in range(1, 11)])\n    new[v2] = pd.cut(train_df[v2], 10, labels=[v2 + '_' + str(i) for i in range(1, 11)])\n    new['target'] = train_df['target']\n    new = new.groupby([v1, v2])['target'].mean().reset_index()\n    new = pd.pivot_table(index=[v1], columns=[v2], values=['target'], data=new)\n#     plt.figure(figsize=(10,8))\n    sns.heatmap(new)\n    plt.show()\n    return new","6f709868":"corr_plot('var_108', 'var_154')","8d566770":"df = pd.concat([train_df, test_df], axis=0, sort=False).reset_index(drop=True)\n# add variable category decode\ndf_decode = pd.DataFrame()\nn_cut = 10\nfeatures = [c for c in train_df.columns if c not in ['ID_code', 'target']]\nfor col in features:\n    col_decode = col + '_' + 'decode'\n    df_decode[col_decode] = pd.cut(df[col], n_cut, labels=range(0, n_cut))","f147c89c":"def add_joint(v1, v2):\n    return df_decode[v1 + '_' + 'decode'].astype(str) + df_decode[v2 + '_' + 'decode'].astype(str)","768e35c3":"def entropy(x):\n    uniq, counts = np.unique(x, return_counts=True)\n    uniq_prob = counts \/ counts.sum()\n    entr = -np.sum(uniq_prob * np.log2(uniq_prob))\n    return entr\n\ndef condEntropy(cond, target):\n    cond_df = pd.DataFrame({'cond': cond, 'target': target}).dropna()\n    entr = cond_df.groupby('cond')['target'].apply(entropy)\n    prob = cond_df.groupby('cond')['target'].apply(lambda x: x.count() \/ cond_df.shape[0])\n    return np.sum(entr * prob)","154fea3a":"origin_target_entropy = entropy(train_df.target)\nvar108_cond_entropy = condEntropy(df_decode['var_108_decode'].iloc[:200000], train_df.target)\nvar154_cond_entropy = condEntropy(df_decode['var_154_decode'].iloc[:200000], train_df.target)\njoint_cond_entropy = condEntropy(add_joint('var_108', 'var_154').iloc[:200000], train_df.target)","29e08957":"print(origin_target_entropy, var108_cond_entropy, var154_cond_entropy, joint_cond_entropy)","cfb0f198":"from itertools import combinations\ncombs = list(combinations([col for col in train_df if col not in ['ID_code', 'target']], 2))","6bcf85c1":"print(combs[0])\nprint(len(combs))","95c7e5f3":"%%time\njoint_cond_entropy = condEntropy(add_joint('var_108', 'var_154').iloc[:200000], train_df.target)","921ad0cc":"df_decode = df_decode.iloc[:2000000]","73cf8bcd":"%%time\nfrom tqdm import tqdm\n\nresult = dict()\nfor c in tqdm(combs):\n    ce  = condEntropy(add_joint(c[0], c[1]), train_df.target)\n    result[c] = ce\nresult = pd.Series(result).reset_index()\nresult.columns = ['v1', 'v2', 'entropy']\nresult = result.sort_values('entropy', ascending=True)\nresult.to_csv('joint_cond_entropy.csv')","0307dd35":"## It seems work, some joint encode have a high targets mean value.\n## 2. Let's check the information gains from joint encode.","7d5fbe33":"## emmmm  have a higher information gain indeed\n## 3. find the Top K combination","f174c6e1":"## 1. Take a look at effect of joint distribution on Targets","1c176103":"### This will take nearly 3.5 hours"}}