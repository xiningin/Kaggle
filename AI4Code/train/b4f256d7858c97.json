{"cell_type":{"b6068fc3":"code","9966f584":"code","25719f9a":"code","3208529f":"code","21855e49":"code","404dd8a0":"code","4375cdc9":"code","4104df94":"code","5cc19568":"code","dab87444":"code","8eae07d5":"code","377f6e97":"code","e8d9d834":"code","7d43f08b":"code","941c8e88":"code","df435aff":"code","6b03f996":"code","da1ee467":"code","5c68f1f1":"code","69ee0fba":"code","3ebc5487":"code","f0ce494a":"code","20fc52f0":"code","22aceb1e":"code","ec951955":"code","b9af004f":"code","af472a1c":"code","357333d1":"code","214b1273":"code","c9a847f8":"code","32a107d0":"code","9883eac0":"code","52dc26de":"code","c2e833c3":"code","17e89ae6":"code","db54075e":"code","670f66c7":"code","2f00d31d":"code","7736b952":"code","9bf64d35":"code","ec51ba04":"code","af2becf9":"code","e9b7a21b":"code","364a75de":"code","d0b7ed12":"code","0eea4eed":"code","3918027c":"code","0ff46f2c":"code","cc46ca03":"code","efb80753":"code","4148ec8b":"code","f619463c":"code","0a3ddc7b":"code","39538ae3":"code","41404afe":"code","4386f615":"code","e68d36af":"markdown","58078305":"markdown","67cc4f95":"markdown","ed4e7707":"markdown","3e923432":"markdown","d383644a":"markdown","0e936134":"markdown","4f53a7ab":"markdown","dee4bc23":"markdown","a70200dc":"markdown","f9f89958":"markdown","eca92fd9":"markdown","3c323b1c":"markdown","4c13a69f":"markdown","a3672a00":"markdown"},"source":{"b6068fc3":"cd \/kaggle\/input\/","9966f584":"pwd","25719f9a":"ls","3208529f":"ls quickdraw-doodle-recognition\/train_simplified\/","21855e49":"ls quickdraw2\/","404dd8a0":"import pandas as pd\ndf_11_category = pd.read_csv('quickdraw2\/340classes_11_category_labeling_libre.csv')\ndisplay(df_11_category)","4375cdc9":"pwd","4104df94":"%matplotlib inline\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom keras.metrics import top_k_categorical_accuracy\ndef top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom glob import glob\nimport gc\ngc.enable()\ndef get_available_gpus():\n    from tensorflow.python.client import device_lib\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n\n# base \ub514\ub809\ud1a0\ub9ac\uc640 test \uc6a9 csv \ud30c\uc77c\uc758 \uacbd\ub85c\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\nbase_dir = os.path.join('.', 'quickdraw-doodle-recognition')\ntest_path = os.path.join(base_dir, 'test_simplified.csv')","5cc19568":"ls","dab87444":"from ast import literal_eval\nALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\nCOL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n\ndef _stack_it(raw_strokes):\n    \"\"\"preprocess the string and make \n    a standard Nx3 stroke vector\"\"\"\n    stroke_vec = literal_eval(raw_strokes) # string->list\n    # unwrap the list\n    in_strokes = [(xi,yi,i)  \n     for i,(x,y) in enumerate(stroke_vec) \n     for xi,yi in zip(x,y)]\n    c_strokes = np.stack(in_strokes)\n    # replace stroke id with 1 for continue, 2 for new\n    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n    c_strokes[:,2] += 1 # since 0 is no stroke\n    # pad the strokes with zeros\n    return pad_sequences(c_strokes.swapaxes(0, 1), \n                         maxlen=STROKE_COUNT, \n                         padding='post').swapaxes(0, 1)\n\ndef read_batch(var_l_category_list=[],\n               samples=5, \n               start_row=0,\n               max_rows = 1000):\n    \"\"\"\n    load and process the csv files\n    this function is horribly inefficient but simple\n    \"\"\"\n    out_df_list = []\n    if len(var_l_category_list) != 0:\n        for c_path in ALL_TRAIN_PATHS:\n            if c_path.split('\/')[-1] in var_l_category_list:\n                c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n                c_df.columns=COL_NAMES\n                out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n    elif len(var_l_category_list) == 0:\n        for c_path in ALL_TRAIN_PATHS:\n            c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n            c_df.columns=COL_NAMES\n            out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n    full_df = pd.concat(out_df_list)\n    full_df['drawing'] = full_df['drawing'].\\\n        map(_stack_it)\n    \n    return full_df","8eae07d5":"print(len(ALL_TRAIN_PATHS))\nprint(base_dir)","377f6e97":"word_encoder = LabelEncoder()\ndef f_preparing_dataset(var_list=[]):\n    train_args = dict(var_l_category_list=var_list,\n                      samples=TRAIN_SAMPLES, \n                      start_row=0, \n                      max_rows=int(TRAIN_SAMPLES*1.5))\n    valid_args = dict(var_l_category_list=var_list,\n                      samples=VALID_SAMPLES, \n                      start_row=train_args['max_rows']+1, \n                      max_rows=VALID_SAMPLES+25)\n    test_args = dict(var_l_category_list=var_list,\n                     samples=TEST_SAMPLES, \n                     start_row=valid_args['max_rows']+train_args['max_rows']+1, \n                     max_rows=TEST_SAMPLES+25)\n    train_df, valid_df, test_df = [read_batch(**train_args), read_batch(**valid_args), read_batch(**test_args)]\n    \n    word_encoder.fit(train_df['word'])\n    print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))\n\n    time_f = time.time()\n    time_d = time_f - time_i\n    print(\"{:2.0f}mins {:2.2f}sec\".format(time_d\/\/60, time_d%60))\n    return [train_df, valid_df, test_df]","e8d9d834":"import time\ntime_i = time.time()\n\nbatch_size = 4096\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 1    # \uc774\ubbf8\uc9c0 plot\uc744 \uc704\ud574 \uac01 dataset\ub9c8\ub2e4 1\uac1c sample\ub9cc \uc77d\uc5b4\uc635\ub2c8\ub2e4.\nVALID_SAMPLES = 1    # \uc774\ubbf8\uc9c0 plot\uc744 \uc704\ud574 \uac01 dataset\ub9c8\ub2e4 1\uac1c sample\ub9cc \uc77d\uc5b4\uc635\ub2c8\ub2e4.\nTEST_SAMPLES = 1     # \uc774\ubbf8\uc9c0 plot\uc744 \uc704\ud574 \uac01 dataset\ub9c8\ub2e4 1\uac1c sample\ub9cc \uc77d\uc5b4\uc635\ub2c8\ub2e4.\n\ntrain_df, valid_df, test_df = f_preparing_dataset()\n\ntime_f = time.time()\ntime_d = time_f - time_i\nprint(\"{:2.0f}mins {:2.2f}sec\".format(time_d\/\/60, time_d%60))","7d43f08b":"def get_Xy(in_df):\n    X = np.stack(in_df['drawing'], 0)\n    y = to_categorical(word_encoder.transform(in_df['word'].values))\n    return X, y\ntrain_X, train_y = get_Xy(train_df)\nvalid_X, valid_y = get_Xy(valid_df)\ntest_X, test_y = get_Xy(test_df)\nprint(train_X.shape)\nprint(valid_y.shape)","941c8e88":"import time\n\ndef f_plot_img_X(var_arr_X, nb_samples, n_img=1, n_class=1):\n    time_i = time.time()\n    # rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n    \n    n_tot = n_img * n_class\n    n_wide = 10\n    n_height = n_tot\/\/10 + (n_tot%10 != 0)\n    print(n_tot)\n    print(n_height)\n    print(n_wide*n_height)\n    rand_idxs = np.array([nb_samples*i+j for i in range(n_class) for j in range(n_img) ])\n    fig, m_axs = plt.subplots(n_height, n_wide, figsize = (16, int(1.6*n_height)))\n\n    for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n        test_arr = var_arr_X[c_id]\n        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n        lab_idx = np.cumsum(test_arr[:,2]-1)\n\n\n        for i in np.unique(lab_idx):\n            c_ax.plot(test_arr[lab_idx==i,0], \n                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n        c_ax.axis('off')\n        c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])\n\n    plt.show()\n    time_f = time.time()\n    time_d = time_f - time_i\n    print(\"{:2.0f}mins {:2.2f}sec\".format(time_d\/\/60, time_d%60))","df435aff":"f_plot_img_X(train_X, nb_samples=TRAIN_SAMPLES, n_img=1, n_class=340)\nf_plot_img_X(valid_X, nb_samples=VALID_SAMPLES, n_img=1, n_class=340)\nf_plot_img_X(test_X, nb_samples=TEST_SAMPLES, n_img=1, n_class=340)","6b03f996":"# 11\uac00\uc9c0\uc758 \uce74\ud14c\uace0\ub9ac\ub85c \ub098\ub204\uace0 \uac01 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c 3\uac1c\uc758 class\ub97c \uac00\uc9c0\uace0 \uc0ac\uc804\ud6c8\ub828\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4.\n# 11\uac00\uc9c0 \uce74\ud14c\uace0\ub9ac\n# \uad50\ud1b5 \/\uc0ac\ubb3c\/ \ub3d9\ubb3c\/ \ub3d9\uc791\/ \uc74c\uc2dd\/ \uc2e0\uccb4,\uc778\ubb3c\/ \uc2dd\ubb3c\/ \uacfc\uc77c\/ \uc7a5\uc18c,\uac74\ubb3c\/ \uc5ed\ud560\/ \uac1c\ub150\n# pretraining\uc758 \ud559\uc2b5\ub370\uc774\ud130\ub294 \ucd1d 32\uac1c csv \ud30c\uc77c\nstr_11_category_list = \\\n'''airplane.csv\nambulance.csv\nbicycle.csv\nalarm clock.csv\nanvil.csv\nbackpack.csv\nant.csv\nbat.csv\nbear.csv\nanimal migration.csv\ncamouflage.csv\nsnorkel.csv\nbanana.csv\nbirthday cake.csv\nbread.csv\nangel.csv\narm.csv\nbeard.csv\nbroccoli.csv\nbush.csv\ncactus.csv\napple.csv\nbanana.csv\nblueberry.csv\nbeach.csv\ncastle.csv\ngarden.csv\ncampfire.csv\nfire hydrant.csv\npool.csv\ncircle.csv\nhexagon.csv\nhurricane.csv'''\nl_category_list = str_11_category_list.split('\\n')","da1ee467":"# \ubd84\ub2f9 900 TRAIN_SAMPLES \ucc98\ub9ac \ud568\nimport time\n\nbatch_size = 4096\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 7500\nVALID_SAMPLES = 75\nTEST_SAMPLES = 50\n\nword_encoder = LabelEncoder()\ntrain_df, valid_df, test_df = f_preparing_dataset(l_category_list)","5c68f1f1":"from keras.preprocessing import sequence\n\n# main_training \uc758 \ubaa8\ub378 \uad6c\uc870\uc640 \ub9de\ucd94\uae30 \uc704\ud574 340 class \uac1c\uc218\ub85c post padding\uc744 \uc900\ub2e4.\ntrain_X, train_y = get_Xy(train_df)\ntrain_y = sequence.pad_sequences(train_y, maxlen=340, padding='post')\nvalid_X, valid_y = get_Xy(valid_df)\nvalid_y = sequence.pad_sequences(valid_y, maxlen=340, padding='post')\ntest_X, test_y = get_Xy(test_df)\ntest_y = np.array(sequence.pad_sequences(test_y, maxlen=340, padding='post'))\n\nprint(train_X.shape)\nprint(train_y.shape)\nprint(test_y.shape)","69ee0fba":"f_plot_img_X(train_X, nb_samples=TRAIN_SAMPLES, n_img=1, n_class=32)\nf_plot_img_X(valid_X, nb_samples=VALID_SAMPLES, n_img=1, n_class=32)\nf_plot_img_X(test_X, nb_samples=TEST_SAMPLES, n_img=1, n_class=32)","3ebc5487":"from keras.models import Sequential\nfrom keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\nif len(get_available_gpus())>0:\n    # https:\/\/twitter.com\/fchollet\/status\/918170264608817152?lang=en\n    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\nstroke_read_model = Sequential()\n# from keras.utils.training_utils import multi_gpu_model\n\n\nstroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n# filter count and length are taken from the script https:\/\/github.com\/tensorflow\/models\/blob\/master\/tutorials\/rnn\/quickdraw\/train_model.py\nstroke_read_model.add(Conv1D(48, (5,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Conv1D(64, (5,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Conv1D(96, (3,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(LSTM(128, return_sequences = True))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(LSTM(128, return_sequences = False))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Dense(512))\nstroke_read_model.add(Dropout(0.3))\n\n# main trainig \uc5d0\uc11c len(word_encoder.classes_)=340 \uc774\ubbc0\ub85c Dense(340)\uc73c\ub85c \ub9de\ucdb0\uc900\ub2e4.\nstroke_read_model.add(Dense(340, activation = 'softmax'))\n\n# \uba40\ud2f0 GPU \uc0ac\uc6a9\ud558\uae30\n# stroke_read_model = multi_gpu_model(stroke_read_model, gpus=1)\nstroke_read_model.compile(optimizer = 'adam', \n                          loss = 'categorical_crossentropy', \n                          metrics = ['categorical_accuracy', top_3_accuracy])\nstroke_read_model.summary()","f0ce494a":"weight_path=\"pretrain_weights.best.hdf5\".format('stroke_lstm_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n#                       patience=5) # probably needs to be more patient, but kaggle time is limited\n                      patience=5*2)\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","20fc52f0":"history = stroke_read_model.fit(train_X, train_y,\n                      validation_data = (valid_X, valid_y), \n                      batch_size = batch_size,\n                      epochs = 100,\n                      callbacks = callbacks_list,\n                      shuffle = True)\nstroke_read_model.save_weights('pretrain_weights.hd')\n\nlstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\nprint('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))","22aceb1e":"df_era1_pretraining = pd.DataFrame(history.history)","ec951955":"# \ubd84\ub2f9 30 TRAIN_SAMPLES \ucc98\ub9ac \ud568\nimport time\n\nbatch_size = 4096\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 750\nVALID_SAMPLES = int(TRAIN_SAMPLES\/10)\nTEST_SAMPLES = int(TRAIN_SAMPLES\/75*50)\n\nword_encoder = LabelEncoder()\ntrain_df, valid_df, test_df = f_preparing_dataset()","b9af004f":"from keras.preprocessing import sequence\n\n# main_training \uc758 \ubaa8\ub378 \uad6c\uc870\uc640 \ub9de\ucd94\uae30 \uc704\ud574 340 class \uac1c\uc218\ub85c post padding\uc744 \uc900\ub2e4.\ntrain_X, train_y = get_Xy(train_df)\ntrain_y = sequence.pad_sequences(train_y, maxlen=340, padding='post')\nvalid_X, valid_y = get_Xy(valid_df)\nvalid_y = sequence.pad_sequences(valid_y, maxlen=340, padding='post')\ntest_X, test_y = get_Xy(test_df)\ntest_y = np.array(sequence.pad_sequences(test_y, maxlen=340, padding='post'))\n\nprint(train_X.shape)\nprint(train_y.shape)\nprint(test_y.shape)","af472a1c":"weight_path=\"era2_maintraining_weights.best.hdf5\".format('stroke_lstm_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n#                       patience=5) # probably needs to be more patient, but kaggle time is limited\n                      patience=5*2)\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","357333d1":"history2 = stroke_read_model.fit(train_X, train_y,\n                      validation_data = (valid_X, valid_y), \n                      batch_size = batch_size,\n                      epochs = 20,\n                      callbacks = callbacks_list,\n                      shuffle = True)\nstroke_read_model.save_weights('era2_maintraining_weights.hd')\n\nlstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\nprint('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))","214b1273":"display(pd.DataFrame(history2.history))","c9a847f8":"# \ubd84\ub2f9 60 TRAIN_SAMPLES \ucc98\ub9ac \ud568\nimport time\n\nbatch_size = 4096\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 1500\nVALID_SAMPLES = int(TRAIN_SAMPLES\/10)\nTEST_SAMPLES = int(TRAIN_SAMPLES\/75*50)\n\nword_encoder = LabelEncoder()\ntrain_df, valid_df, test_df = f_preparing_dataset()","32a107d0":"from keras.preprocessing import sequence\n\n# main_training \uc758 \ubaa8\ub378 \uad6c\uc870\uc640 \ub9de\ucd94\uae30 \uc704\ud574 340 class \uac1c\uc218\ub85c post padding\uc744 \uc900\ub2e4.\ntrain_X, train_y = get_Xy(train_df)\ntrain_y = sequence.pad_sequences(train_y, maxlen=340, padding='post')\nvalid_X, valid_y = get_Xy(valid_df)\nvalid_y = sequence.pad_sequences(valid_y, maxlen=340, padding='post')\ntest_X, test_y = get_Xy(test_df)\ntest_y = np.array(sequence.pad_sequences(test_y, maxlen=340, padding='post'))\n\nprint(train_X.shape)\nprint(train_y.shape)\nprint(test_y.shape)","9883eac0":"weight_path=\"era3_maintraining_weights.best.hdf5\".format('stroke_lstm_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n#                       patience=5) # probably needs to be more patient, but kaggle time is limited\n                      patience=5*2)\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","52dc26de":"history3 = stroke_read_model.fit(train_X, train_y,\n                      validation_data = (valid_X, valid_y), \n                      batch_size = batch_size,\n                      epochs = 20,\n                      callbacks = callbacks_list,\n                      shuffle = True)\nstroke_read_model.save_weights('era3_maintraining_weights.hd')\n\nlstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\nprint('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))","c2e833c3":"display(pd.DataFrame(history3.history))","17e89ae6":"# # \ubd84\ub2f9 60 TRAIN_SAMPLES \ucc98\ub9ac \ud568\n# import time\n\n# batch_size = 4096\n# STROKE_COUNT = 196\n# TRAIN_SAMPLES = 3000\n# VALID_SAMPLES = int(TRAIN_SAMPLES\/10)\n# TEST_SAMPLES = int(TRAIN_SAMPLES\/75*50)\n\n# word_encoder = LabelEncoder()\n# train_df, valid_df, test_df = f_preparing_dataset()","db54075e":"# from keras.preprocessing import sequence\n\n# # main_training \uc758 \ubaa8\ub378 \uad6c\uc870\uc640 \ub9de\ucd94\uae30 \uc704\ud574 340 class \uac1c\uc218\ub85c post padding\uc744 \uc900\ub2e4.\n# train_X, train_y = get_Xy(train_df)\n# train_y = sequence.pad_sequences(train_y, maxlen=340, padding='post')\n# valid_X, valid_y = get_Xy(valid_df)\n# valid_y = sequence.pad_sequences(valid_y, maxlen=340, padding='post')\n# test_X, test_y = get_Xy(test_df)\n# test_y = np.array(sequence.pad_sequences(test_y, maxlen=340, padding='post'))\n\n# print(train_X.shape)\n# print(train_y.shape)\n# print(test_y.shape)","670f66c7":"# weight_path=\"era3_maintraining_weights.best.hdf5\".format('stroke_lstm_model')\n\n# checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n#                              save_best_only=True, mode='min', save_weights_only = True)\n\n\n# reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n#                                    verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n# early = EarlyStopping(monitor=\"val_loss\", \n#                       mode=\"min\", \n# #                       patience=5) # probably needs to be more patient, but kaggle time is limited\n#                       patience=5*2)\n# callbacks_list = [checkpoint, early, reduceLROnPlat]","2f00d31d":"# history4 = stroke_read_model.fit(train_X, train_y,\n#                       validation_data = (valid_X, valid_y), \n#                       batch_size = batch_size,\n#                       epochs = 20,\n#                       callbacks = callbacks_list,\n#                       shuffle = True)\n# stroke_read_model.save_weights('era4_maintraining_weights.hd')\n\n# lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n# print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))","7736b952":"# display(pd.DataFrame(history4.history))","9bf64d35":"# # \ubd84\ub2f9 60 TRAIN_SAMPLES \ucc98\ub9ac \ud568\n# import time\n\n# batch_size = 4096\n# STROKE_COUNT = 196\n# TRAIN_SAMPLES = 6000\n# VALID_SAMPLES = int(TRAIN_SAMPLES\/10)\n# TEST_SAMPLES = int(TRAIN_SAMPLES\/75*50)\n\n# word_encoder = LabelEncoder()\n# train_df, valid_df, test_df = f_preparing_dataset()","ec51ba04":"# from keras.preprocessing import sequence\n\n# # main_training \uc758 \ubaa8\ub378 \uad6c\uc870\uc640 \ub9de\ucd94\uae30 \uc704\ud574 340 class \uac1c\uc218\ub85c post padding\uc744 \uc900\ub2e4.\n# train_X, train_y = get_Xy(train_df)\n# train_y = sequence.pad_sequences(train_y, maxlen=340, padding='post')\n# valid_X, valid_y = get_Xy(valid_df)\n# valid_y = sequence.pad_sequences(valid_y, maxlen=340, padding='post')\n# test_X, test_y = get_Xy(test_df)\n# test_y = np.array(sequence.pad_sequences(test_y, maxlen=340, padding='post'))\n\n# print(train_X.shape)\n# print(train_y.shape)\n# print(test_y.shape)","af2becf9":"# weight_path=\"era3_maintraining_weights.best.hdf5\".format('stroke_lstm_model')\n\n# checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n#                              save_best_only=True, mode='min', save_weights_only = True)\n\n\n# reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n#                                    verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n# early = EarlyStopping(monitor=\"val_loss\", \n#                       mode=\"min\", \n# #                       patience=5) # probably needs to be more patient, but kaggle time is limited\n#                       patience=5*2)\n# callbacks_list = [checkpoint, early, reduceLROnPlat]","e9b7a21b":"# history5 = stroke_read_model.fit(train_X, train_y,\n#                       validation_data = (valid_X, valid_y), \n#                       batch_size = batch_size,\n#                       epochs = 20,\n#                       callbacks = callbacks_list,\n#                       shuffle = True)\n# stroke_read_model.save_weights('era5_maintraining_weights.hd')\n\n# lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n# print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))","364a75de":"# display(pd.DataFrame(history5.history))","d0b7ed12":"from sklearn.metrics import confusion_matrix, classification_report\ntest_cat = np.argmax(test_y, 1)\npred_y = stroke_read_model.predict(test_X, batch_size = 4096)\npred_cat = np.argmax(pred_y, 1)\nplt.matshow(confusion_matrix(test_cat, pred_cat))\nprint(classification_report(test_cat, pred_cat, \n                            target_names = [x for x in word_encoder.classes_]))","0eea4eed":"points_to_use = [5, 15, 20, 30, 40, 50]\npoints_to_user = [108]\nsamples = 12\nword_dex = lambda x: word_encoder.classes_[x]\nrand_idxs = np.random.choice(range(test_X.shape[0]), size = samples)\nfig, m_axs = plt.subplots(len(rand_idxs), len(points_to_use), figsize = (24, samples\/8*24))\nfor c_id, c_axs in zip(rand_idxs, m_axs):\n    res_idx = np.argmax(test_y[c_id])\n    goal_cat = word_encoder.classes_[res_idx]\n    \n    for pt_idx, (pts, c_ax) in enumerate(zip(points_to_use, c_axs)):\n        test_arr = test_X[c_id, :].copy()\n        test_arr[pts:] = 0 # short sequences make CudnnLSTM crash, ugh \n        stroke_pred = stroke_read_model.predict(np.expand_dims(test_arr,0))[0]\n        top_10_idx = np.argsort(-1*stroke_pred)[:10]\n        top_10_sum = np.sum(stroke_pred[top_10_idx])\n        \n        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n        lab_idx = np.cumsum(test_arr[:,2]-1)\n        for i in np.unique(lab_idx):\n            c_ax.plot(test_arr[lab_idx==i,0], \n                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], # flip y\n                      '.-')\n        c_ax.axis('off')\n        if pt_idx == (len(points_to_use)-1):\n            c_ax.set_title('Answer: %s (%2.1f%%) \\nPredicted: %s (%2.1f%%)' % (goal_cat, 100*stroke_pred[res_idx]\/top_10_sum, word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]\/top_10_sum))\n        else:\n            c_ax.set_title('%s (%2.1f%%), %s (%2.1f%%)\\nCorrect: (%2.1f%%)' % (word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]\/top_10_sum, \n                                                                 word_dex(top_10_idx[1]), 100*stroke_pred[top_10_idx[1]]\/top_10_sum, \n                                                                 100*stroke_pred[res_idx]\/top_10_sum))","3918027c":"sub_df = pd.read_csv(test_path)\nsub_df['drawing'] = sub_df['drawing'].map(_stack_it)","0ff46f2c":"sub_vec = np.stack(sub_df['drawing'].values, 0)\nsub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=4096)","cc46ca03":"top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in sub_pred]","efb80753":"top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\ntop_3_pred[:3]","4148ec8b":"fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\nrand_idxs = np.random.choice(range(sub_vec.shape[0]), size = 9)\nfor c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n    test_arr = sub_vec[c_id]\n    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n    lab_idx = np.cumsum(test_arr[:,2]-1)\n    for i in np.unique(lab_idx):\n        c_ax.plot(test_arr[lab_idx==i,0], \n                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n    c_ax.axis('off')\n    c_ax.set_title(top_3_pred[c_id])","f619463c":"pwd","0a3ddc7b":"sub_df['word'] = top_3_pred\nsub_df[['key_id', 'word']].to_csv('\/kaggle\/working\/submission.csv', index=False)","39538ae3":"ls \/kaggle\/working\/","41404afe":"ls","4386f615":"df_submission = pd.read_csv('submission.csv')\ndf_submission.head()","e68d36af":"# (3) 340\uac1c class\uc758 \uc774\ubbf8\uc9c0 plotting \ubc0f pretraining \ub370\uc774\ud130\ub85c \ub2e8\uc21c\ud55c \uadf8\ub9bc \ucc3e\uae30","58078305":"## (6).1. \ubd84\uc11d (precision, recall, f1-score, support, confusion_matrix)","67cc4f95":"# (1)\uc0ac\uc804\ud559\uc2b5, \uc810\uc9c4\uc801 \ud559\uc2b5\ub370\uc774\ud130 \ud655\ub300\ub97c \ud1b5\ud55c \uae30\uc874 LSTM \ubaa8\ub378\uc758 \uac1c\uc120","ed4e7707":"## (5).4. main training (era5_maintraining\/20epoch\/6000img\/ %cat_acc\/240mins)","3e923432":"# (2) 340\uac1c class\uc758 11 \uc885 \ub300\ubd84\ub958\n\uc0ac\ubb3c\uc774 161\uac1c\ub85c \uac00\uc7a5 \ub9ce\uace0, \ub3d9\ubb3c\uc774 53\uac1c, \uad50\ud1b5\uc774 29\uac1c\ub85c \ub9ce\uc2b5\ub2c8\ub2e4.","d383644a":"### (3).1. 340class \uc774\ubbf8\uc9c0 plotting\n340class\uc5d0 \ub300\ud55c \uadf8\ub9bc\uc744 train_X, test_X, valid_X\ub85c\ubd80\ud130 plot\ud558\uc600\uc2b5\ub2c8\ub2e4.","0e936134":"## (5).3. main training (era4_maintraining\/20epoch\/3000img\/ 56.9%cat_acc\/120mins)","4f53a7ab":"## (5).2. main training (era3_maintraining\/ 20epoch\/ 1500img\/ 49.3%cat_acc\/ 60m)","dee4bc23":"## (7).2. \ubaa8\ub378\uc758 submission \ub370\uc774\ud130\uc14b\uc744 \ud1b5\ud55c \uc608\uce21","a70200dc":"```\nEpoch 20\/20\n255000\/255000 [==============================] - 25s 96us\/step - loss: 3.8444 - categorical_accuracy: 0.1638 - top_3_accuracy: 0.3205 - val_loss: 3.5913 - val_categorical_accuracy: 0.2022 - val_top_3_accuracy: 0.3750\n\nEpoch 00020: val_loss improved from 3.72432 to 3.59134, saving model to pretrain_weights.best.hdf5\n17000\/17000 [==============================] - 1s 35us\/step\nAccuracy: 20.4%, Top 3 Accuracy 37.4%\n\nEpoch 20\/20\n255000\/255000 [==============================] - 25s 96us\/step - loss: 2.2690 - categorical_accuracy: 0.4540 - top_3_accuracy: 0.6680 - val_loss: 2.0096 - val_categorical_accuracy: 0.5108 - val_top_3_accuracy: 0.7212\n\nEpoch 00020: val_loss improved from 2.01422 to 2.00963, saving model to pretrain_weights.best.hdf5\n17000\/17000 [==============================] - 1s 34us\/step\nAccuracy: 50.5%, Top 3 Accuracy 71.7%\n\nEpoch 20\/20\n255000\/255000 [==============================] - 25s 96us\/step - loss: 2.0509 - categorical_accuracy: 0.5009 - top_3_accuracy: 0.7121 - val_loss: 1.8247 - val_categorical_accuracy: 0.5543 - val_top_3_accuracy: 0.7570\n\nEpoch 00020: val_loss improved from 1.83847 to 1.82466, saving model to pretrain_weights.best.hdf5\n17000\/17000 [==============================] - 1s 35us\/step\nAccuracy: 54.7%, Top 3 Accuracy 75.2%\n\nEpoch 20\/20\n255000\/255000 [==============================] - 25s 96us\/step - loss: 1.9077 - categorical_accuracy: 0.5315 - top_3_accuracy: 0.7380 - val_loss: 1.7446 - val_categorical_accuracy: 0.5709 - val_top_3_accuracy: 0.7711\n\nEpoch 00020: val_loss did not improve from 1.72911\n17000\/17000 [==============================] - 1s 35us\/step\nAccuracy: 56.6%, Top 3 Accuracy 76.5%\n```","f9f89958":"## 19.11.05. \ub0b4\uc6a9 \uc218\uc815\n1. \uc624\ud0c0 \uc218\uc815(2.1.1. \uc5d0\uc11c) \"10epoch\uc815\ub3c4\uc5d0\uc11c 0.33%\" >>> \"10epoch\uc815\ub3c4\uc5d0\uc11c 33%\"\n\n## \ubaa9\ucc28\n- \uc694\uc57d\n- \uac00\uc124 \ubc0f \uc2e4\ud5d8 \ub0b4\uc6a9\n- \uacb0\uacfc \ubd84\uc11d \ubc0f \uace0\ucc30\n\n## 1. \uc694\uc57d\n\ubcf8 \"Quick, Draw!\" \ubb38\uc81c\uc5d0\uc11c\ub294 \uc774\ubbf8\uc9c0 \ubd84\ub958 \ubb38\uc81c\ub85c\uc11c, CIFAR\uc758 \uc774\ubbf8\uc9c0 \ubd84\ub958\ubb38\uc81c\uac00 \ub2e8\uc21c\ud788 \uc774\ubbf8\uc9c0 \uc815\ubcf4\ub9cc \uc8fc\uc5b4\uc9c4 \uac83\uc5d0 \ube44\ud574, \n\ud68d\uc21c\uc815\ubcf4\ub77c\ub294 \ubd80\uac00\uc801\uc778 \uc815\ubcf4\uac00 \ub4e4\uc5b4\uac00\uc788\uc5b4 \ud68d\uc21c\uc815\ubcf4\uc758 \ud65c\uc6a9 \ubc29\uc2dd\uc774 \uae30\ub300\uac00 \ub418\ub294 \ud765\ubbf8\ub85c\uc6b4 \uc8fc\uc81c\uc785\ub2c8\ub2e4.\n\ubcf8 competition\uc758 Notebook\uc5d0\uc11c stroke \uc815\ubcf4\ub97c \ud65c\uc6a9\ud558\uace0\uc790 LSTM\uc744 \uc0ac\uc6a9\ud55c \n\uc544\ub798\uc758 notebook\uc744 \uae30\ucd08\ub85c \ud558\uc5ec pretraining \ubc29\ubc95\uacfc \ud559\uc2b5\ub370\uc774\ud130\uc758 \uc810\uc9c4\uc801 \uc99d\uac00\ub97c \ud1b5\ud574 \ud574\ub2f9 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uace0\uc790 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n\nhttps:\/\/www.kaggle.com\/kmader\/quickdraw-baseline-lstm-reading-and-submission\n\nLSTM\uc740 sequence\uc5d0 \ub300\ud558\uc5ec \uc801\uc751\ud560 \uc218 \uc788\ub294 \ubaa8\ub378\uc774\uae30 \ub54c\ubb38\uc5d0, \ud68d\uc21c \uc815\ubcf4\ub97c \ub2e4\ub8e8\ub294\ub370 \uc801\ud569\ud55c \ubaa8\ub378\ub85c \ud30c\uc545\ub429\ub2c8\ub2e4.\n\n## 2.1.1. \uac00\uc1241. \ub2e8\uc21c\ud55c \uadf8\ub9bc\uc73c\ub85c pretraining\ud558\uba74 \ud68d\uc21c \uac1c\ub150\uc744 \ud559\uc2b5\ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4.\n\n\ub3c4\ud615(line.csv, triangle.csv, square.csv, otagon.csv, hexagon.csv, circle.csv, squiggle.csv) \uc744 \uad6c\ubd84\ud558\ub294 \ubaa8\ub378\uc744 pretraining\ud558\uace0\n\ubcf5\uc7a1\ud55c \uadf8\ub9bc\uc744 \uc810\ucc28 \ubd84\uc11d\ud574\ub098\uac00\ub294 \ubc29\uc2dd\uc73c\ub85c \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uba74, \ubaa8\ub378\uc758 layer\ub97c \uc904\uc774\uace0 \ud6a8\uacfc\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \uad6c\ud604\ud560 \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c\ub77c\ub294 \ucc29\uc548\uc744 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uac04\ub2e8\ud558\uace0 \uba85\ud655\ud55c \uc774\ubbf8\uc9c0\ub97c \uac16\ub294 class\uc5d0 \ub300\ud574\uc11c pretraining\uc744 \uc9c4\ud589\ud558\uc5ec \ucda9\ubd84\ud788 \uc815\ud655\ub3c4\ub97c \ub192\uc774\uace0, \n\uc0bc\uac01\ud615, \uc0ac\uac01\ud615, \uc6d0\ub9cc\uc744 \ud559\uc2b5\ub370\uc774\ud130\ub85c \uc0ac\uc6a9\ud588\uc744 \ub54c, early stopping \uc870\uac74\uc5d0 \uc758\ud574 10epoch\uc815\ub3c4\uc5d0\uc11c 33% cat_acc(categorical accuracy)\ub85c \ud559\uc2b5\uc774 \uc885\ub8cc\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\nclass\ub2f9 7500 \uc774\ubbf8\uc9c0\ub85c \ud558\uc600\ub294\ub370\ub3c4, 33%\ub97c \ub118\uc744 \uc218\uac00 \uc5c6\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\ud559\uc2b5\ub370\uc774\ud130\uc5d0 \ub3c4\ud615\uc744(\uc9c1\uc120, \uc0bc\uac01\ud615, \uc0ac\uac01\ud615, \uc624\uac01\ud615, \uc721\uac01\ud615, \uc6d0, \uad6c\ubd88\uad6c\ubd88\ud55c \uc120(squiggle)) \ub354 \ucd94\uac00\ud558\uace0 \ud6c8\ub828\ub370\uc774\ud130\ub97c class \ub2f9 7500\uac1c \uc774\ubbf8\uc9c0\ub85c \ud558\uc5ec \ud6c8\ub828\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\uc774 \uacbd\uc6b0\uc5d0\ub294 100epoch\uc5d0\uc11c 84.7% cat_acc(85.3% val_cat_acc)\ub85c \ud559\uc2b5\uc774 \uc885\ub8cc\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c, class \uc885\ub958\uac00 \ub298\uc5b4\ub098\uba74 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uac1c\uc120\ub41c \uac83\uc73c\ub85c \ubcf4\uc5ec\uc9d1\ub2c8\ub2e4.\n\n\n340 \uc885\uc758 class\ub97c \ub2e4\uc2dc 11 \uac1c\uc758 \ub300\ubd84\ub958(\uad50\ud1b5, \uc0ac\ubb3c, \ub3d9\ubb3c \ub4f1)\ub85c \ubd84\ub958\ud558\uace0, \uac01 \ub300\ubd84\ub958\uc5d0\uc11c \ub2e8\uc21c\ud55c \uc774\ubbf8\uc9c0\ub97c \uac16\ub294 3\uac00\uc9c0\uc758 class\ub9cc \ud0dd\ud558\uc5ec pretraining\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n73% cat_acc\uae4c\uc9c0 pretraining\uc744 \ud55c \ud6c4, \ud574\ub2f9\uc2dc\uc810\uc758 \uac00\uc911\uce58\ub97c \uc800\uc7a5\ud558\uc5ec 340 \uc885 \ubaa8\ub4e0 class\ub97c \ud559\uc2b5\ub370\uc774\ud130\ub85c \ud558\uc5ec main training\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\npretraining \ud6c4 main training\uc744 \uc2dc\uc791\ud560 \ub54c cat_acc\ub294 73%\uc5d0\uc11c 0.003%\ub85c \uac10\uc18c\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n## 2.1.2. \uac00\uc1242. \ud559\uc2b5\ub370\uc774\ud130\ub97c \uc810\uc9c4\uc801\uc73c\ub85c \ub298\ub9ac\uba74 \ud559\uc2b5\uc774 \uc798 \ub420 \uac83\uc774\ub2e4.\n\npretraining(32 class, 7500 \uc774\ubbf8\uc9c0\/class) \uc774\ud6c4 main training\uc5d0\uc11c \ud559\uc2b5\ub370\uc774\ud130\ub97c 340 class\ub97c \uac01 class \ub2f9 750\uac1c\uc758 \uc774\ubbf8\uc9c0\ub85c 20 epoch\uc744 \ud559\uc2b5 \ud6c4 \ud559\uc2b5 \uc774\ubbf8\uc9c0\uc758 \uac1c\uc218\ub97c \ub450 \ubc30\ub85c(1500\uac1c) \ub298\ub824\uc11c \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\uc774 \ud6c4 20 epoch \ub2f9 \uc774\ubbf8\uc9c0\uc758 \uac1c\uc218\ub97c \ub450 \ubc30\uc529 \ub298\ub824 \ud559\uc2b5\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4.\n\npretraining\uc5d0\uc11c main training\uc73c\ub85c \ub118\uc5b4\uac08 \ub54c cat_acc\uac00 73% \uc5d0\uc11c 3%\ub85c \uae09\uaca9\ud788 \ub5a8\uc5b4\uc9c4 \uac83\uc5d0 \ube44\ud558\uc5ec, main training\uc5d0\uc11c\ub294 20 epoch\uc5d0\uc11c \ub2e4\uc74c 20 epoch \uc73c\ub85c \ub118\uc5b4\uac08 \ub54c,\n41.7% \uc5d0\uc11c 35.1%\ub85c \uc815\ud655\ub3c4\uac00 \uc5b4\ub290\uc815\ub3c4 \uc720\uc9c0\ub418\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610 era\uac00 \uc99d\uac00\ud560\uc218\ub85d \uadf8 \uac10\uc18c\ud3ed\uc774(-6.6p%, -0.2p%) \uc904\uc5b4\ub4e4\ub2e4\uac00 era5\ub85c \ub118\uc5b4\uac08 \ub54c\ub294 \uc624\ud788\ub824 \uc99d\uac00\ud568\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n||||||||||\n|--- |--- |--- |--- |--- |--- |--- |--- |--- |\n|era|era\ub2f9<br>epoch \uc218|class \ub2f9<br> \uc774\ubbf8\uc9c0 \uc218|\uccab-\ub9c8\uc9c0\ub9c9<br> cat_acc|\uccab<br> cat_acc|\ub9c8\uc9c0\ub9c9<br> cat_acc|\ub9c8\uc9c0\ub9c9-\ub9c8\uc9c0\ub9c9<br> cat_acc|epoch\ub2f9<br> \uc18c\uc694\uc2dc\uac04|era<br> \ucd1d \uc18c\uc694\uc2dc\uac04|\n|era1_pretraining|100|7500|-|0%|73%|-|120s|40m|\n|era2_maintraining|20|750|-70p%|3%|41.7%|-31.3p%|120s|40m|\n|era3_maintraining|20|1500|-6.6p%|35.1%|49.3%|+7.6p%|180s|60m|\n|era4_maintraining|20|3000|-0.2p%|49.1%|56.9%|+7.6p%|6m|120m|      \n|era5_maintraining|20|6000|+1.5p%|58.4%|%|p%|12.5m|240m?|      \n\n## 3. \uacb0\uacfc\ubd84\uc11d \ubc0f \uace0\ucc30\nera5\uc758 \ubaa8\ub378\ub85c late submission\uc744 \ud55c \uacb0\uacfc 0.74238\ub97c \ud68d\ub4dd\ud588\uc2b5\ub2c8\ub2e4.\nera5 \ubaa8\ub378\uc740 \ucd1d epoch\uc774 \ub9ce\uc774 \ubd80\uc871\ud558\uae30 \ub54c\ubb38\uc5d0, score\uac00 \ub192\uc9c0 \ubabb\ud55c \uac83\uc73c\ub85c \ud30c\uc545\ub429\ub2c8\ub2e4. \n\ucda9\ubd84\ud788 \ub9ce\uc740 epoch\uc744 \ub3cc\ub9ac\uace0, \ub300\uc870\uad70\uc744 \ud65c\uc6a9\ud558\uc5ec \ube44\uad50\ub97c \ud55c\ub2e4\uba74 \ubcf8 \uac00\uc124\uc5d0 \ub300\ud574\uc11c \ub354\uc6b1 \uc815\ud655\ud55c \uace0\ucc30\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","eca92fd9":"## (4) \uc0ac\uc804\ud559\uc2b5 era1-pretraining(100epoch\/7500img\/40mins)","3c323b1c":"## (5).1. main training (era2_maintraining\/20epoch\/750img\/ 41.7%cat_acc\/40mins)","4c13a69f":"## (7).1. Late Submission","a3672a00":"## (6).2. \ubd84\uc11d (\uadf8\ub9ac\ub294 \ub3c4\uc911\uc758 \ubaa8\ub378\uc758 \uc608\uce21)"}}