{"cell_type":{"10bfcf12":"code","22e69be3":"code","d469a2b5":"code","1f147984":"code","7e7e77a5":"code","0a5df199":"code","83ede9a2":"code","5c33ab5e":"code","e854458d":"code","104c4b8f":"code","f8dd8da8":"code","ca702bbe":"code","f9d38a94":"markdown","ff838005":"markdown","d742c812":"markdown","c9e8c194":"markdown","b29f47b5":"markdown","565d9ea7":"markdown","8ba03346":"markdown","d863c7aa":"markdown","eea9d5a8":"markdown","d4b33685":"markdown","2569b94f":"markdown"},"source":{"10bfcf12":"#Importanto as bibliotecas e definindo padr\u00f5es de sa\u00edda\nimport math,sys,os,numpy as np\nnp.set_printoptions(precision=4, linewidth=100)\nfrom numpy.random import random\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nimport matplotlib.animation\n\n# from matplotlib import pyplot as plt, rcParams, animation, rc\nfrom ipywidgets import interact, interactive, fixed\nplt.rcParams['figure.figsize'] = 8, 6\nfrom time import sleep\nfrom IPython.display import HTML","22e69be3":"# Fun\u00e7\u00e3o para a equa\u00e7\u00e3o y = ax + b\ndef lin(a,b,x): \n    return a*x+b","d469a2b5":"a=3.\nb=8.","1f147984":"np.random.seed(5)\nn=30\nx = random(n)\ny = lin(a,b,x)","7e7e77a5":"plt.scatter(x,y)","0a5df199":"def sse(y,y_pred): \n    return ((y-y_pred)**2).sum()\n\ndef mse(y,a,b,x): \n    return sse(y, lin(a,b,x))\/n","83ede9a2":"from mpl_toolkits.mplot3d import Axes3D  \nl_boudary = -5\nr_boudary = 20\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d', )\na = b = np.arange(l_boudary, r_boudary, 0.5)\nmse_vec = []\nfor i in np.arange(l_boudary, r_boudary, 0.5): \n    for j in np.arange(l_boudary, r_boudary, 0.5):\n        mse_vec.append(mse(y, i, j, x))\n\n\nA, B = np.meshgrid(a, b)\nZ = np.array(mse_vec).reshape(A.shape)\n\n\nax.plot_surface(A, B, Z)\n\nax.set_xlabel('A')\nax.set_ylabel('B')\nax.set_zlabel('MSE')\n\nplt.show()","5c33ab5e":"a_guess=-1.\nb_guess=1.\nmse(y, a_guess, b_guess, x)","e854458d":"# taxa de aprendizagem\nlr=0.1","104c4b8f":"def back_propagation():\n    global a_guess, b_guess\n    y_pred = lin(a_guess, b_guess, x)\n    dydb = 2 * (y_pred - y)\n    dyda = x*dydb\n    a_guess -= lr*dyda.mean()\n    b_guess -= lr*dydb.mean()","f8dd8da8":"%matplotlib inline\n\nprev_error = 1000\na_guess=-5.\nb_guess=8.\nit = 0\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax.scatter(x,y)\nplt_line = ax.plot(x, lin(a_guess, b_guess, x), 'r', label = 'a=%.4f, b=%.4f, mse=%.4f, it=%d' % (a_guess, b_guess, prev_error,it))[0]\nplt_legend = ax.legend(loc='upper left',prop={'size': 14})\nplt.close(fig)\n\ndef animate(step):\n    global prev_error, y, a_guess, b_guess, x, it\n    it = it + 1\n    prev_error = mse(y, a_guess, b_guess, x)\n    back_propagation()\n\n    plt_line.set_data(x, lin(a_guess, b_guess, x))\n    plt_legend.texts[0].set_text('a=%.4f, b=%.4f, mse=%.4f, it=%d' % (a_guess, b_guess, prev_error,it))\n\nani = matplotlib.animation.FuncAnimation(fig, animate, frames=300, interval=50, repeat=False)\n\nHTML(ani.to_jshtml())","ca702bbe":"print('a=%.4f, b=%.4f' % (a_guess, b_guess))","f9d38a94":"Aqui definimos a taxa de aprendizado, que \u00e9 o quanto vamos \"caminhar\" na dire\u00e7\u00e3o oposta ao gradiente, que define o vetor de crescimento de uma fun\u00e7\u00e3o. \n\nO gradiente \u00e9 calculado pela derivada parcial da equa\u00e7\u00e3o em rela\u00e7\u00e3o a cada par\u00e2metro:\n","ff838005":"Abaixo mostramos a superf\u00edcie da fun\u00e7\u00e3o de custo (MSE) em fun\u00e7\u00e3o de valores de <b>a<\/b> e <b>b<\/b> entre [-5,5]","d742c812":"# Demonstra\u00e7\u00e3o - Regress\u00e3o Linear - Gradiente Descente\n ","c9e8c194":"A fun\u00e7\u00e3o `back_propagation` atualiza os par\u00e2metros `a_guess` e `b_guess`","b29f47b5":"$$\\frac{\\partial{(y-(a*x + b))^2}}{\\partial{b}} = 2*(b + a*x - y)$$\n\n$$\\frac{\\partial{(y-(a*x + b))^2}}{\\partial{a}} = 2*x*(b + a*x - y) = x* \\frac{\\partial{y}}{\\partial{b}}$$","565d9ea7":"A fun\u00e7\u00e3o abaixo representa uma fun\u00e7\u00e3o linear (uma reta) com os par\u00e2metros a e b com valor x e retorna y <br\/><br\/>\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/2f\/Line_gen_m_no_slope.svg\/220px-Line_gen_m_no_slope.svg.png\" widht=80>","8ba03346":"Nessa demonstra\u00e7\u00e3o explicamos com um exemplo de uma equa\u00e7\u00e3o linear simples, como funciona a otimiza\u00e7\u00e3o dos par\u00e2metros de um modelo matem\u00e1tico pelo m\u00e9todo do Gradiente Descendente. Mais informa\u00e7\u00f5es podem ser encontradas aqui: <a href=\"https:\/\/en.wikipedia.org\/wiki\/Gradient_descent\">https:\/\/en.wikipedia.org\/wiki\/Gradient_descent<\/a>","d863c7aa":"Agora inicializamos nossos par\u00e2metros \"aleatoriamente\" com os valores -1 e +1 e calculamos a m\u00e9trica de custo","eea9d5a8":"Imagine que a fun\u00e7\u00e3o que nosso algoritmo quer \"descobrir\" \u00e9 <b>y=3x+8<\/b>. Vamos definir os par\u00e2metros abaixo:","d4b33685":"Aqui definimos a nossa fun\u00e7\u00e3o de custo, a que desejamos minimizar, definida por $MSE = {\\frac{1}{n}*{\\sum(Yreal - Ypred)^2}}$","2569b94f":"Gerando 30 pontos aleatoriamente para X e encontrando o Y correspondente usando <b>y=3x+8<\/b>"}}