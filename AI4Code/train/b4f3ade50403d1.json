{"cell_type":{"2e39e4ec":"code","09da4072":"code","6ee5e5e3":"code","3ed2c139":"code","f5115906":"code","103d9642":"code","7a33b157":"code","78a28409":"code","5ca80d44":"code","f24715a0":"code","33f54bb3":"code","f7d1bdcc":"code","2bde53d2":"code","3ee851df":"code","d8ccf25d":"code","972ea6b9":"code","c4e69544":"code","d90cf22b":"code","d8844a40":"code","3d740b81":"code","ffbc5e5a":"code","8943e9fc":"code","9579d96d":"code","50247806":"code","a756ecba":"code","e117a14b":"markdown","32e60a09":"markdown","cf330930":"markdown","99c30c8c":"markdown","9276b872":"markdown","c955eb1e":"markdown","02bfcf6a":"markdown","679140f2":"markdown","dd9145d1":"markdown","b81616c7":"markdown","fc99e48a":"markdown","2167df8f":"markdown","00cb3586":"markdown","aeaebb09":"markdown","6b9231e5":"markdown","ca10b501":"markdown","576f197a":"markdown","bf13cd91":"markdown"},"source":{"2e39e4ec":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn import preprocessing\n","09da4072":"data = pd.read_csv('..\/input\/datat3rr\/DataT3RR.csv')\nprint('Size of weather data frame is :',data.shape)\ndata.info()\ndata[0:10]","6ee5e5e3":"data.count().sort_values()","3ed2c139":"data = data.drop(columns=['Stasiun','Tanggal'],\n\n                 axis=1)\ndata = data.dropna(how='any')\nprint(data.shape)","f5115906":"print(data.shape)\ndata.head()","103d9642":"cnt_pro = data['Besok_hujan'].value_counts()\nplt.figure(figsize=(6,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Kelas', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","7a33b157":"sns.set_style(\"whitegrid\")\nsns.pairplot(data,hue=\"Besok_hujan\",size=3);\nplt.show()","78a28409":"# Calculate the correlation matrix\ncorr = data.corr()\ncorr1 = pd.DataFrame(abs(corr['Besok_hujan']),columns = ['Besok_hujan','Tn','Tx','Tavg'])\nnonvals = corr1.loc[corr1['Besok_hujan'] < 0.005]\nprint('Var correlation < 0.5%',nonvals)\nnonvals = list(nonvals.index.values)\n\n# We extract variables with correlation less than 0.5%\ndata1 = data.drop(columns=nonvals,axis=1)\nprint('Data Final',data1.shape)","5ca80d44":"data = data[['Tn','Tx','Tavg','RR','Hari_hujan','Besok_hujan']] #Subsetting the data\ncor = data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","f24715a0":"from sklearn.model_selection import train_test_split\nY = data1['Besok_hujan']\nX = data1.drop(columns=['Besok_hujan'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=9)","33f54bb3":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","f7d1bdcc":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","2bde53d2":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","3ee851df":"test_acc_knncla = round(knncla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_knncla = round(knncla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","d8ccf25d":"#Accuracy\nmodel1 = pd.DataFrame({\n    'Model': ['KNN'],\n    'Train Score': [train_acc_knncla],\n    'Test Score': [test_acc_knncla]\n})\nmodel1.sort_values(by='Test Score', ascending=False)","972ea6b9":"#Precision, Recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict6)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","c4e69544":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(knncla,X_train, Y_train)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","d90cf22b":"Y1 = data['Besok_hujan']\nX1 = data.drop(columns=['Besok_hujan','Hari_hujan'])\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False,random_state=9).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","d8844a40":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Hujan_besok, Tn, Tx, RR')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","3d740b81":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(1, 5))) \nprint(variance[30:70])","ffbc5e5a":"X1 = data[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.3, random_state=9)","8943e9fc":"# K-Nearest Neighbor classification\nknncla.fit(X1_train, Y1_train)\nY1_predict6 = knncla.predict(X1_test)\nknncla_cm = confusion_matrix(Y1_test, Y1_predict6)\nscore1_knncla= knncla.score(X1_test, Y1_test)","9579d96d":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('KNN Classification')\nsns.heatmap(data=knncla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"BuPu\" ,fmt='g', ax=ax1)\nplt.show()","50247806":"Testscores1 = pd.Series([score1_knncla], index=[ 'K-Nearest Neighbour Score']) \nprint(Testscores1)","a756ecba":"#Precision, Recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y1_test, Y1_predict6)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","e117a14b":"Confusion Matrix is commonly used for a summarization of prediction results on a classification problem.The number of correct and incorrect predictions is summarized with counting values and each value broken down for each class. Each of them is the key to the confusion matrix. It shows the classification model is confused when it makes predictions, at this point in here it gives us insight not only into the errors being made by a classifier but also show the types of errors that are being made [[2](https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/)].\n\n\n![https:\/\/miro.medium.com\/max\/712\/1*Z54JgbS4DUwWSknhDCvNTQ.png](https:\/\/miro.medium.com\/max\/712\/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n\nWhere,\n\n\u25cf Class 1 is represent Positive Class\n\n\u25cf Class 0 is represent Negative Class\n\nDefinition of the Terms:\n\n\u25cf Positive (P) is an observation of positive\n(for example: is a laptop).\n\n\u25cf Negative (N) is an observation of not positive\n(for example: is not a laptop).\n\n\u25cf True Positive (TP) is an observation of positive, and is predicted\nto be positive.\n\n\u25cf False Negative (FN) is an observation of positive, but is predicted\nnegative.\n\n\u25cf True Negative (TN) is an observation of negative, and is predicted\nto be negative.\n\n\u25cf False Positive (FP) is an observation of negative, but is predicted\npositive.","32e60a09":"Eliminate irrelevant variables in analysis such as  Stasiun(Station), Tanggal(Date)","cf330930":"This work used a dataset from daily meteorological observations for 5 stations, the aim is to predict whether or not it will rain tomorrow using the binary classification model K-Nearest Neighbour (KNN). Then use the confusion matrix to measure the quality of the model that was built. \n\n**Variable prediction:**\n\n1. Stasiun: Weather Station\n1. Tanggal : Date\n1. Tn: Minimum temperature (\u00b0 C)\n1. Tx: Maximum temperature (\u00b0 C)\n1. Tavg: Average temperature (\u00b0 C)\n1. RR: Rainfall (mm)\n1. Hari_Hujan: Ground truth rainy today\n\n\n**Variable target:**\n\n1. Besok_Hujan: Ground truth tomorrow weather forecast\n\n","99c30c8c":"### Data for training and testing\n\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 30%, assumed it ideal ratio between training and testing","9276b872":"# classification","c955eb1e":"Accuracy, Precision, Recall","02bfcf6a":"# Plotting Heatmap\n\nHeatmap can be defined as a method of graphically representing numerical data where individual data points contained in the matrix are represented using different colors. The colors in the heatmap can denote the frequency of an event, the performance of various metrics in the data set, and so on. Different color schemes are selected by varying businesses to present the data they want to be plotted on a heatmap [2].The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation.","679140f2":"# Practical example","dd9145d1":"Import Library","b81616c7":"# What is Confusion Matrix ?","fc99e48a":"![https:\/\/brilliant-staff-media.s3-us-west-2.amazonaws.com\/tiffany-wang\/TGbEPp4Cad.png](https:\/\/brilliant-staff-media.s3-us-west-2.amazonaws.com\/tiffany-wang\/TGbEPp4Cad.png)","2167df8f":"## Features Selection\n\n","00cb3586":"**To calculate the Confusion Matrix, the formula is defined as follow:**\n\n![https:\/\/www.nosimpler.me\/wp-content\/uploads\/2017\/03\/precision-recall-2.png](https:\/\/www.nosimpler.me\/wp-content\/uploads\/2017\/03\/precision-recall-2.png)\n\nwhere, \n\n**Accuracy**\n\nis closeness of the measurements to a specific value. Accuracy has two definitions:\n\n1. More commonly, it is a description of systematic errors, a measure of statistical bias; low accuracy causes a difference between a result and a \"true\" value. ISO calls this trueness.\n2. Alternatively, ISO defines[[3](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision)] accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness.\n\n\n**Precision and Recall**\n\nAccording to [[3](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision)] Precision is a description of random errors, a measure of statistical variability. In simpler terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if the values are close to each other. \n\n\n**Recall**\n1. \nis defined as the fraction of relevant documents retrieved compared to the total number of relevant documents (true positives divided by true positives+false negatives).\n\n","aeaebb09":"VISUALIZING THE DATA","6b9231e5":"### Classification\n\nThe classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","ca10b501":"##### The confusion matrix","576f197a":"Read Data","bf13cd91":"# What is K-Nearest Neighbour (KNN) ?\n\nis a non-parametric method proposed by Thomas Cover used for classification and regression.[[1](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n* In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n* In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, normalizing the training data can improve its accuracy dramatically. \n\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1\/d, where d is the distance to the neighbor. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n\n**\nExample**\n\n![https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e7\/KnnClassification.svg\/220px-KnnClassification.svg.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e7\/KnnClassification.svg\/220px-KnnClassification.svg.png)\n\n\nAs you can see above on the Example of k-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).\n\n\n**Parameter selection**\n\nThe best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification[[1](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)] but make boundaries between classes less distinct. \n\n\n**The 1-nearest neighbor classifier**\n\n\nThe most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is ![https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f815904edbff2ce82502172ec0dce3311d57f2bb](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f815904edbff2ce82502172ec0dce3311d57f2bb)\n\nAs the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data)."}}