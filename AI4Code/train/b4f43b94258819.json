{"cell_type":{"e2f909b9":"code","3acd94a6":"code","cb62d9e9":"code","6f28dfd3":"code","b27fb9f7":"code","fd7006c8":"code","934da937":"code","8329a307":"code","29d0879a":"code","c6728f49":"code","39bc9260":"code","14f6b278":"code","ea5533eb":"code","52c359a5":"code","233d9110":"code","e27b0a5d":"code","c4185024":"code","785c33fa":"markdown","9698a368":"markdown","a56eca09":"markdown","afba724d":"markdown","89ee54ba":"markdown","88a80616":"markdown"},"source":{"e2f909b9":"# Loading all packages \nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow_hub as hub\nimport tensorflow_text\nimport glob\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport warnings\nimport faiss  \nimport requests\nimport pickle\nfrom sklearn.metrics.pairwise import cosine_similarity\nplt.style.use('ggplot')\nimport re\n\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\nfrom dash.dependencies import Input, Output, State\nfrom flask import Flask\nimport os\nimport requests\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport dash_bootstrap_components as dbc","3acd94a6":"## Helper Functions\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        if 'abstract' in file:\n            features = [\n                file['paper_id'],\n                file['metadata']['title'],\n                format_authors(file['metadata']['authors']),\n                format_authors(file['metadata']['authors'], \n                               with_affiliation=True),\n                format_body(file['abstract']),\n                format_body(file['body_text']),\n                format_bib(file['bib_entries']),\n                file['metadata']['authors'],\n                file['bib_entries']\n            ]\n        else:\n            features = [\n                file['paper_id'],\n                file['metadata']['title'],\n                format_authors(file['metadata']['authors']),\n                format_authors(file['metadata']['authors'], \n                               with_affiliation=True),\n                format_body(file['body_text']),\n                format_body(file['body_text']),\n                format_bib(file['bib_entries']),\n                file['metadata']['authors'],\n                file['bib_entries']\n            \n            ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df\n\n","cb62d9e9":"biorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))\n\nall_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\n","6f28dfd3":"### Biorxiv: Generate CSV\n\ncleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)\n\ncol_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head()","b27fb9f7":"#Reading all CSV files and Concatenating final result\npmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)\n# pmc_df.to_csv('clean_pmc.csv', index=False)\n\n\npmc_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pmc_json\/'\npmc_files_1 = load_files(pmc_dir_1)\n\npmc_df_1 = generate_clean_df(pmc_files_1)\n# pmc_df.to_csv('clean_pmc.csv', index=False)\n\n\ncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\ncomm_files = load_files(comm_dir)\ncomm_df = generate_clean_df(comm_files)\n\n\n\ncomm_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pmc_json\/'\ncomm_files_1 = load_files(comm_dir_1)\ncomm_df_1 = generate_clean_df(comm_files_1)\n\n\n\nnoncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\nnoncomm_files = load_files(noncomm_dir)\nnoncomm_df = generate_clean_df(noncomm_files)\n\n\n\nnoncomm_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pmc_json\/'\nnoncomm_files_1 = load_files(noncomm_dir_1)\nnoncomm_df_1 = generate_clean_df(noncomm_files_1)\n\n\n\ndf_covid_new = pd.concat([clean_df,pmc_df,pmc_df_1,comm_df,comm_df_1,noncomm_df,noncomm_df_1],axis=0,ignore_index=True)\n","fd7006c8":"#Reading Metadata file\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\ndf_covid_old = pd.merge(df_covid_new,meta_df[['sha','url']],left_on='paper_id',right_on='sha',how='left')\n\n## Saving the Doc information with their URLs.\ndf_covid_new[['paper_id','title','url']].to_csv('\/kaggle\/output\/df_docid_with_url.csv')\n","934da937":"#Reading all CSV files and Concatenating final result\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")","8329a307":"## Creating a dictionary with Document as Key and Paragraphs as text\nshort_paragraph=[]\ndict1 = {}\nfor i in range(len(df_covid_old)):\n#     if dict1[df_covid.loc[i,'paper_id']] is not null:\n    dict1[df_covid_old.loc[i,'paper_id']] = re.split(r'(?:\\r?\\n){1,}', df_covid_old.loc[i,'text'])","29d0879a":"## Create Vector Embedding for all the Text Documents and storing in a dictionary with key as docId and values as paragraph embeddings\ndict_vector_old = {}\nfor key in list(dict1.keys()):\n    try:\n        dict_vector_old[key] = embed(dict1[key])\n        print(len(dict_vector_old))\n    except:\n        continue","c6728f49":"## Matching Vector and Text Documents (if embedding vector generation fails, we are ignoring the document)        \ndict1_old= {}\nfor key in list(dict_vector_old.keys()):\n    if key in dict1:\n        dict1_old[key]=dict1[key]\n    print(len(dict1_old))","39bc9260":"## Storing the paragraph embeddings as pickle file for further use at \/kaggle\/output\/\nwith open('\/kaggle\/output\/dict1_text_v6.pickle', 'wb') as handle:\n    pickle.dump(dict1_old, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('\/kaggle\/output\/dict_vector_v6.pickle', 'wb') as handle:\n    pickle.dump(dict_vector_old, handle, protocol=pickle.HIGHEST_PROTOCOL)","14f6b278":"df_covid = df_covid_old\n\n## Reading the paragraph embeddings pickle files\nwith open('\/kaggle\/output\/dict1_text_v6.pickle', 'rb') as handle:\n    dict1_text_v1 = pickle.load(handle)\n\nwith open('\/kaggle\/output\/dict_vector_v6.pickle', 'rb') as handle:\n    dict_vector = pickle.load(handle)","ea5533eb":"\n## Building the index for semantic search for faiss\nindex = faiss.IndexFlatL2(512)   # build the index\nfor vector in list(dict_vector.keys()):\n    index.add(dict_vector[vector].numpy())                  # add vectors to the index\n","52c359a5":"## Creating a list of documents with their docid and paragraph text to get the results\ntext=[]\ndocId=[]\nfor key in list(dict1_text_v1.keys()):\n\n    text.extend(dict1_text_v1[key])\n    doc=[key]*len(dict1_text_v1[key])\n    docId.extend(doc)\n    ","233d9110":"## Enter the required query to be searched upon\n## Below is the query to search \"Seasonality of transmission of corona virus\"\nsearch = [''' Seasonality of transmission of corona virus''']\n\n## Creating the embedding vectors for the query\npredictions = embed(search)\n\nk = 10                         # we want to see 10 nearest neighbors\nD, I = index.search(np.array(predictions,dtype='float32'), k) # sanity check\nprint(I)\nprint(D)\n\nfor i in range(k):\n    print(text[I[0][i]])\n    print(docId[I[0][i]])\n    print('\\n')\n    ","e27b0a5d":"# Supporting function for calculation of Text summarization using Extraction-based text summarization\nexternal_stylesheets=[dbc.themes.BOOTSTRAP]\n\nserver = Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', 'secret')\napp = dash.Dash(name = __name__, server = server, external_stylesheets = external_stylesheets)\n#external_stylesheets = ['https:\/\/codepen.io\/chriddyp\/pen\/bWLwgP.css']\n\n#app.config['suppress_callback_exceptions'] = True\n\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/rahulpoddar\/dash-deploy-exp\/master\/TASK1_annotated_1_v3.csv', encoding='latin1')\n\ntasks = df['Task Name'].unique().tolist()\n\ndef data_prep(inpt): \n    clean_data = []\n    article3 = ' '.join(inpt)\n    result=re.sub(\"\\d+\\.\", \" \", article3)\n    clean_data.append(result)\n            \n    clean_data = pd.DataFrame(clean_data)\n    clean_data.columns = ['Remediation']\n    clean_data['Remediation'] = clean_data['Remediation'].astype('str')\n\n    clean_data1 = clean_data['Remediation']\n    clean_data2 = []\n    regex = r\"(?<!\\d)[-,_;:()](?!\\d)\"\n    for i in range(1):\n        result2 = re.sub(regex,'',clean_data1.loc[i])\n        clean_data2.append(result2)\n    clean_data2 = pd.DataFrame(clean_data2)\n    clean_data2.columns = ['Remediation']\n    clean_data2['Remediation'] = clean_data2['Remediation'].astype('str')\n    \n    return (clean_data2)\n\ndef _create_dictionary_table(text_string) -> dict:\n   \n    # Removing stop words\n    stop_words = set(stopwords.words(\"english\"))\n        \n    words = word_tokenize(text_string)\n    \n    # Reducing words to their root form\n    stem = PorterStemmer()\n    \n    # Creating dictionary for the word frequency table\n    frequency_table = dict()\n    for wd in words:\n        wd = stem.stem(wd)\n        if wd in stop_words:\n            continue\n        if wd in frequency_table:\n            frequency_table[wd] += 1\n        else:\n            frequency_table[wd] = 1\n\n    return frequency_table\n\ndef _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n\n    # Algorithm for scoring a sentence by its words\n    sentence_weight = dict()\n\n    for sentence in sentences:\n        sentence_wordcount = (len(word_tokenize(sentence)))\n        sentence_wordcount_without_stop_words = 0\n        for word_weight in frequency_table:\n            if word_weight in sentence.lower():\n                sentence_wordcount_without_stop_words += 1\n                if sentence[:7] in sentence_weight:\n                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n                else:\n                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n\n        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] \/(sentence_wordcount_without_stop_words)\n      \n    return sentence_weight\n\ndef _calculate_average_score(sentence_weight) -> int:\n   \n    # Calculating the average score for the sentences\n    sum_values = 0\n    for entry in sentence_weight:\n        sum_values += sentence_weight[entry]\n\n    # Getting sentence average value from source text\n    average_score = (sum_values \/ (len(sentence_weight)))\n\n    return average_score\n\ndef _get_article_summary(sentences, sentence_weight, threshold):\n    sentence_counter = 0\n    article_summary = ''\n\n    for sentence in sentences:\n        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n            article_summary += \" \" + sentence\n            sentence_counter += 1\n\n    return article_summary\n\ndef _run_article_summary(article):\n    \n    #creating a dictionary for the word frequency table\n    frequency_table = _create_dictionary_table(article)\n\n    #tokenizing the sentences\n    sentences = sent_tokenize(article)\n\n    #algorithm for scoring a sentence by its words\n    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n\n    #getting the threshold\n    threshold = _calculate_average_score(sentence_scores)\n\n    #producing the summary\n    article_summary = _get_article_summary(sentences, sentence_scores, 1 * threshold)\n\n    return article_summary\n\ndef _output(inpt):\n    new = []\n    df = data_prep(inpt)\n    df_rem = df['Remediation']\n    #sentences = sent_tokenize(df_rem[0])\n    summary_results = _run_article_summary(df_rem[0])\n    new.append(summary_results)\n    return(new)\n'''\ndef generate_summary(task):\n    return 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n'''","c4185024":"def generate_table(dff):\n    rows = []\n    for i in range(len(dff)):\n        row = []\n        for col in ['Title', 'Output']:\n            value = dff.iloc[i][col]\n            url = dff.iloc[i]['URL']\n            if col == 'Title':\n                cell = html.Td(html.A(href=url, children = value))\n            else:\n                cell = html.Td(children = value)\n            row.append(cell)\n        rows.append(html.Tr(row))\n    return dbc.Table(\n        # Header\n        [html.Tr([html.Th(col,  style={'text-align':'center'}) for col in ['Title', 'Search Output']]) ] +\n        # Body\n        rows,\n        bordered=True,\n        dark=False,\n        hover=True,\n        responsive=True,\n        striped=True,\n    )\n\n\napp.layout = html.Div([\n        html.Div([\n        html.H1('COVID-19 Open Research Dataset Challenge (CORD-19)', style = {'margin-left': '10%', 'margin-top': '5%'}),\n        html.Hr(),\n        html.Div([\n        html.H3('Type a general query (e.g. \"What is Corona Virus?\"):'),\n        html.Br(),\n        dbc.Input(id = 'general-search', type = 'text', placeholder = 'Type a query', value = ''),\n        html.Br(),\n        dbc.Button(id='submit-button-state', n_clicks=0, children='Submit', color = \"primary\", className=\"mr-2\", style = {'margin-left': '46%'}),\n        ], style = {'width': '80%', 'margin': 'auto'}),\n        html.Hr(),\n        html.Div([html.H3('OR')],style = {'margin-left': '48%'}),\n        html.Hr(),\n        html.Div([\n        html.H3('Select a task:'),\n        dcc.Dropdown(\n        id='task-dropdown',\n        options=[\n            {'label': i, 'value': i} for i in tasks \n        ],\n        placeholder=\"Select a task\",\n    ),\n        html.Br(),\n        html.Div([\n                html.H3('Select a sub-task:'),\n                dcc.Dropdown(\n                        id='sub-task-dropdown',\n                        placeholder = \"Select a sub-task\",\n                        ),\n                ], id = 'sub-task'),\n                ], style = {'margin-left': '10%','margin-right': '10%'}),\n    ]),\n    html.Hr(),\n    html.Div([\n                html.H3('Response Summary'),\n                html.Div(id = 'task-summary'),\n                html.Div(id = 'query-summary')\n                        ], style = {'margin-left': '10%','margin-right': '10%'}),\n    html.Hr(),\n    html.Div([\n            html.H3('Search Results'),\n            html.Div(id = 'task-results'),\n            html.Div(id = 'query-results')\n            ], id = 'search-results-main', style = {'margin-left': '10%','margin-right': '10%'}),\n    html.Hr(),\n])\n\n@app.callback(\n    Output('sub-task-dropdown', 'options'),\n    [Input('task-dropdown', 'value')])\ndef set_subtask_options(selected_task):\n    if selected_task != None:\n        dff = df[df['Task Name'] == selected_task]\n        options = dff['Sub-tasks'].unique().tolist()\n        return [{'label': i, 'value': i} for i in options]\n    else:\n        return [{'label': i, 'value': i} for i in []]\n    \n@app.callback(\n    Output('sub-task-dropdown', 'value'),\n    [Input('sub-task-dropdown', 'options')])\ndef set_subtask_value(available_options):\n    if available_options != []:\n        return available_options[0]['value']\n    else:\n        return ''\n    \n@app.callback(\n    Output('task-summary', 'children'),\n    [Input('sub-task-dropdown', 'value')])\ndef update_taks_summary(value):\n    if value != '':\n        dff = df[df['Sub-tasks'] == value]\n        return _output(dff['Output'].tolist())[0]\n    else:\n        return ''\n\n\n@app.callback(\n    Output('task-results', 'children'),\n    [Input('sub-task-dropdown', 'value')])\ndef update_taks_results(value):\n    if value != '':\n        dff = df[df['Sub-tasks'] == value]\n        return generate_table(dff)\n    else:\n        return ''\n    \n@app.callback(\n        Output('query-results', 'children'),\n         [Input('submit-button-state', 'n_clicks')],\n         [State('general-search', 'value')]\n         )\ndef populate_search_results(n_clicks, value):\n    if value != '':\n        query = value\n        response = requests.post(\"https:\/\/nlp.biano-ai.com\/develop\/test\", json={\"texts\": [query]})\n        predictions = response.json()['predictions']\n        pred_df = pd.DataFrame(predictions[0])\n        pred_df.columns = ['Distance', 'Document ID', 'Output', 'Title', 'URL']\n        return generate_table(pred_df)\n    else:\n        return ''\n\n@app.callback(\n        Output('query-summary', 'children'),\n         [Input('submit-button-state', 'n_clicks')],\n         [State('general-search', 'value')]\n         )\ndef generate_search_summary(n_clicks, value):\n    if value != '':\n        query = value\n        response = requests.post(\"https:\/\/nlp.biano-ai.com\/develop\/test\", json={\"texts\": [query]})\n        predictions = response.json()['predictions']\n        pred_df = pd.DataFrame(predictions[0])\n        return _output(pred_df['text'].tolist())[0]\n    else:\n        return ''\n\nif __name__ == '__main__':\n    app.run_server(debug=True)","785c33fa":"# Results and Outcome: <br>\nAge is a primary risk factor for SARS-CoV-2\/COVID-19 disease severity with higher fatality risk above 62 years of age, with 20% increased risk of mortality for every 5-year increase in age.  Early hospitalization (within 2 days of onset) can reduce mortality.  History of comorbidities (respiratory, cardiovascular, and diabetes) significantly increase fatality risk, with 40% increase of mortality for one additional preexisting comorbidity.  Patients with a history of coronary heart disease (CHD) have a three-fold increased mortality risk.  Underlying condition contributions for individuals 30 years or older to mortality risk include cardiovascular disease, diabetes, steroid therapy, severe obesity, chronic kidney disease, and chronic obstructive pulmonary disease.  Pulmonary inflammation (as measured by CT-scan lung abnormalities including ground glass opacities, consolidation, and interlobular septal thickening) is associated with clinical severity and viral load (as measured by RT-PCR).  Based on SARS-CoV-1, residual damage in the second week (i.e., without detectable viral load in the lung tissue) is attributed to excessive host response causing hypercytokinemia and pulmonary fibrosis.  Pregnancy may increase vulnerability to infection, is associated with preterm birth, and newborns may be considered an at-risk population.  Lacking vaccination or pharmaceutical availabilities, consensus strategies focus on community mitigation including mass screening, mandatory quarantine of exposed individuals, and social distancing interventions.\n\n\n <br>\n\n2) Dash Link : https:\/\/dash-app-deploy.herokuapp.com\/ <br>","9698a368":"# Objective: Task2 - What do we know about COVID-19 risk factors?","a56eca09":"# Methodology :  <br>\n\n**1) Reading datasets and creating embedding**\n                  a. Parsed 59,000 documents (source: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge)\n                  b. Created embedding vectors for each paragraph (~10million), using universal sentence encoder, (https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/5) <br>\n**2) Predicting results**\n                  a. Task based specific search using \u201cfaiss\u201d library.\n                  b. Addition of covid-19 like terms (Coronavirus disease 2019, COVID-19, SARS-CoV-2, Severe acute respiratory syndrome coronavirus 2, 2019-nCoV, SARSr-CoV) in order to get results more specific to COVID-19.\n                  c. Experts validation of the results. <br>\n**3) Text Summarization and Visualization**\n                  a. Extraction-based text summarization performed on the results found, in an automated way. (source: https:\/\/blog.floydhub.com\/gentle-introduction-to-text-summarization-in-machine-learning\/)\n                     i) Convert Paragraph to sentences and calculate word weightage.\n                     ii) Calculate the average word weightage by dividing the sum of weightage by total number of words.\n                     iii) Select the sentence with the highest average weight.\n                  b. Creating an API using dash in order to view the results.**","afba724d":"# 3. Text Summarization and Vizualization \n","89ee54ba":"# 1. Reading Datasets and creating Embeddings","88a80616":"# 2. Predicting results"}}