{"cell_type":{"a0b616d0":"code","ca37d986":"code","8515c44f":"code","e8d62121":"code","6964a7ba":"code","e6acb02f":"code","4f828f78":"code","d2090c67":"code","421cd343":"code","14a95aff":"code","46b5b24f":"code","1c75e3e0":"code","876cee88":"code","b9cb394c":"code","175a3a0c":"code","922a81eb":"code","1e9cfa82":"code","9564eef5":"code","38c3404f":"code","01e0a86b":"code","aa4bee3c":"code","74e39a00":"code","36e4f041":"code","d21fd8d6":"code","c37831dc":"code","99c6a823":"code","69d41aca":"code","90718444":"code","ed5a4263":"code","e75d0ab0":"code","5058ee1f":"code","d62d91ca":"code","08d28def":"code","bf70e7df":"code","4ef3a5a6":"code","7d675777":"code","bd5210a7":"code","1c56cf77":"code","35d57ee6":"code","5dcc536b":"code","a89a94dc":"code","2d3ffce7":"code","8a019fe6":"code","e704e49f":"code","35fdd24d":"code","63b076ff":"code","bb875409":"code","6800c56a":"code","740faded":"code","6b7ba3f5":"code","090ae4e7":"code","baf7b2f8":"code","b21b2d89":"code","f63a1bed":"code","2f20e1d4":"code","244049b0":"code","ceff493f":"code","8a06f53b":"code","7e31c366":"code","6e03a839":"code","8c0f70ee":"code","b59ffc43":"code","dd64ba56":"code","550965c3":"code","9fc23501":"code","aa181ae7":"code","1b43b11c":"code","f3bd49fd":"code","4702ad9c":"code","61580401":"code","d0c40a66":"code","1f288e5b":"code","7794af27":"code","24302a33":"code","2f3908a2":"code","8ff11216":"code","9fb2e7a8":"code","e7c6ab4d":"code","85ed47eb":"code","a7297394":"code","8bf8f61f":"code","96f2c7b9":"code","288c546a":"code","385c4f3d":"code","252d63b6":"code","728dd3a7":"markdown","42f62997":"markdown","14d529fa":"markdown","54af70f1":"markdown","30e4f325":"markdown","6c5e0689":"markdown","7b501e17":"markdown","89f0ae5c":"markdown","ec1618dd":"markdown","314cc147":"markdown","6e8bc395":"markdown","412b1a09":"markdown","d895f747":"markdown","608d61f6":"markdown","b682ade9":"markdown","e723215f":"markdown","d7f2163b":"markdown","c3907377":"markdown","c8787615":"markdown","8df34777":"markdown","f65a3a16":"markdown","7507e069":"markdown","b47440f4":"markdown","891b0862":"markdown","8bce431e":"markdown","28d2061a":"markdown","f58302f6":"markdown","0d4a110a":"markdown","f61aade5":"markdown","b1384fd8":"markdown","dbada9ed":"markdown","360c5dc4":"markdown","117366cc":"markdown","5622a78c":"markdown","216d72c9":"markdown","50715faa":"markdown","c8072988":"markdown","0eb3b034":"markdown","c7ee3f65":"markdown","35a190f2":"markdown","cae230e0":"markdown","b7329dcf":"markdown","2fa24f62":"markdown","fb4ea8b9":"markdown","b931c74d":"markdown","089f53d5":"markdown","e1faafe1":"markdown","83fcb1a6":"markdown","6b106a52":"markdown","8f4e2761":"markdown","3a1e05d1":"markdown","6ebcb9b2":"markdown","59a604c6":"markdown","2db67318":"markdown","34e9f881":"markdown","c79efcd4":"markdown","e22d2435":"markdown","89a96d40":"markdown","2bbc664a":"markdown","886e930e":"markdown","019a489f":"markdown","ee1b2f1a":"markdown","d49b13a8":"markdown","d64f5bed":"markdown","379c743b":"markdown","bc86dd90":"markdown","cdb55732":"markdown","8fdf89ac":"markdown","7a0fbb29":"markdown","b7046b7c":"markdown"},"source":{"a0b616d0":"import gc\nimport sys\nimport warnings\nfrom pathlib import Path\n\nimport os\n\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nwarnings.simplefilter(\"ignore\")\nimport datetime as dt\nfrom sklearn.preprocessing import LabelEncoder \n\nimport pickle\nfrom tqdm import tqdm\nimport random\n\nfrom datetime import date, timedelta","ca37d986":"train = pd.read_pickle(\"..\/input\/eda-of-mlb-for-starter-version\/player_engagement_with_info.pkl\")","8515c44f":"train","e8d62121":"features=train.columns[9:].to_list()\nfeatures","6964a7ba":"cols = []\n\nfor col in tqdm(features):\n    if train[col].dtype == \"object\":\n        \n        train[col] = np.where(train[col].isna(),\"NaN\",train[col]) \n        \n       \n        train[col] = train[col].astype(\"category\")\n\n        le=LabelEncoder()\n        le.fit(train[col])\n        train[col] = le.transform(train[col])\n        with open( col + '_encoder.txt', 'wb') as f:\n              pickle.dump(le, f)\n\n\n        cols.append(col)\n\n","e6acb02f":"targets = [\"target1\", \"target2\", \"target3\", \"target4\"]","4f828f78":"train['date'] = pd.to_datetime(train['date'], format=\"%Y%m%d\")","d2090c67":"from sklearn import datasets\nfrom sklearn import model_selection\n\ndef create_folds(data, num_splits,target):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[target], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","421cd343":"train.head(3)","14a95aff":"folds = create_folds(train, 5,\"targetAvg\")","46b5b24f":"folds.head(3)","1c75e3e0":"import lightgbm as lgb\nimport optuna.integration.lightgbm as lgbo","876cee88":"\"\"\"\n## optuna ##\n    \ntraindf = folds[folds[\"kfold\"]!=0]\nvaliddf = folds[folds[\"kfold\"]==0]\n\ntraindf = traindf.reset_index(drop=True)\nvaliddf = validdf.reset_index(drop=True)\n\nlgb_train = lgbo.Dataset(traindf[features],traindf[\"targetAvg\"],categorical_feature=cols)\nlgb_valid = lgbo.Dataset(validdf[features],validdf[\"targetAvg\"],categorical_feature=cols)\nlgbm_params_optuna = {\n    'objective': 'mae', \n        \"seed\":42,\n    'metric': 'mae',\n\"verbose\":-1}\n\nres = {}\nmodel = lgbo.train(lgbm_params_optuna, lgb_train, valid_sets=lgb_valid,\n                  verbose_eval=50,  \n                  num_boost_round=1000,  \n                   early_stopping_rounds=10, \n                 evals_result=res)\n\nlgb_params2 = model.params\n\nwith open(\"LGBM_optuna.bin', 'wb') as f:\n          pickle.dump(model2, f)\n\n\"\"\"","b9cb394c":"\"\"\"\n## lgbm ####\n    \nfor fold in range(5):\n    print(\"--fole-----{}---start-----\".format(str(fold)))\n\n    traindf = folds[folds[\"kfold\"]!=fold]\n    validdf = folds[folds[\"kfold\"]==fold]\n\n    traindf = traindf.reset_index(drop=True)\n    validdf = validdf.reset_index(drop=True)\n\n\n    scores = []\n    res = {}\n    \n    for a in targets:\n\n        lgb_train = lgb.Dataset(traindf[features],traindf[a],categorical_feature=cols)\n        lgb_valid = lgb.Dataset(validdf[features],validdf[a],categorical_feature=cols)\n\n        model2 = lgb.train(lgb_params2, lgb_train, valid_sets=lgb_valid,\n                      verbose_eval=50,  \n                      num_boost_round=1000,  \n                      early_stopping_rounds=10, \n                     evals_result=res)\n\n        tmpdf = pd.DataFrame(res)\n        scores.append(np.min(tmpdf[\"valid_0\"][0]))\n\n        with open(\"LGBM_fold\"+str(fold) +\"_\" + a + '.bin', 'wb') as f:\n                  pickle.dump(model2, f)\n\n\"\"\"","175a3a0c":"del folds\ngc.collect()","922a81eb":"train.head(3)","1e9cfa82":"dfgroup = train.groupby(\"playerId\")","9564eef5":"train_feats = dfgroup[targets].agg([\"cumsum\",\"cumcount\"])\ntrain_feats","38c3404f":"for a in targets:\n    train_feats[( a,   'mean')] = np.where(train_feats[(a,   'cumcount')]==0,np.nan,  train_feats[(a,   'cumsum')]\/train_feats[( a,   'cumcount')] )\n    ","01e0a86b":"train_feats","aa4bee3c":"train_feats[\"ct\"] = train_feats[('target1', 'cumcount')]\ntrain_feats","74e39a00":"train_feats = train_feats.iloc[:,-5:]\ntrain_feats","36e4f041":"train_feats.columns = [\"target1_mean\",\"target2_mean\",\"target3_mean\",\"target4_mean\",\"ct\"]\ntrain_feats","d21fd8d6":"train_feats[\"dailyDataDate\"] = train[\"dailyDataDate\"]\ntrain_feats[\"playerId\"] = train[\"playerId\"]\ntrain_feats.head(3)","c37831dc":"period = 31\ntrain_feats[\"dailyDataDate2\"] = train_feats[\"dailyDataDate\"]\ntrain_feats[\"dailyDataDate\"] = train_feats[\"dailyDataDate2\"] + timedelta(days=period)","99c6a823":"train_feats.head(3)","69d41aca":"targets2 = targets.copy()\ntargets2.append(\"targetAvg\")\ntargets2","90718444":"for a in targets2:\n    train_feats[a] = train[a]\ntrain_feats.head(3)","ed5a4263":"LAGS=[31,45,60,180]","e75d0ab0":"def makeLAG(num):\n    tmp1 = train_feats.groupby([\"playerId\"])[\"target1\",\"target2\",\"target3\",\"target4\"].rolling(num).agg([\"median\",\"max\"]).reset_index()\n    \n    cols = []\n    cols.append(\"playerId\")\n    cols.append(\"level_1\")\n    for a in targets2[:-1]:\n        for b in [\"median\",\"max\"]:\n            cols.append(a+\"_\" + b)\n            \n    tmp1.columns=cols\n\n    tmp1.columns = [\"Lag\" + str(num) + \"_\" + str(s) for s in tmp1.columns]\n    \n    return tmp1","5058ee1f":"lag1 = makeLAG(LAGS[0])","d62d91ca":"lag1.tail(3)","08d28def":"lag2 = makeLAG(LAGS[1])\nlag3 = makeLAG(LAGS[2])\nlag4 = makeLAG(LAGS[3])","bf70e7df":"lag2.tail(3)","4ef3a5a6":"lagall = pd.concat([lag1,lag2.iloc[:,2:],lag3.iloc[:,2:],lag4.iloc[:,2:]],axis=1)\nlagall.tail(3)","7d675777":"del lag1,lag2,lag3,lag4\nimport gc\n_ = gc.collect()","bd5210a7":"lagall = lagall.iloc[:,1:]\nlagall","1c56cf77":"lagall = lagall.set_index(\"Lag31_level_1\")","35d57ee6":"lagall","5dcc536b":"train_feats = pd.concat([train_feats,lagall],axis=1)","a89a94dc":"del lagall\n_ = gc.collect()","2d3ffce7":"train_feats","8a019fe6":"train_feats = train_feats.drop(targets2,axis=1)\ntrain_feats","e704e49f":"train_feats_fin = train_feats[train_feats[\"dailyDataDate\"]<\"20210501\"].reset_index(drop=True)\nfutureLAG = train_feats[train_feats[\"dailyDataDate\"]>=\"20210501\"].reset_index(drop=True)","35fdd24d":"del train_feats\n_ = gc.collect()","63b076ff":"train = pd.merge(train,train_feats_fin,on=[\"dailyDataDate\",\"playerId\"],how=\"left\")","bb875409":"del train_feats_fin\n_ = gc.collect()","6800c56a":"train","740faded":"train = train.drop(\"dailyDataDate2\",axis=1)","6b7ba3f5":"features = train.columns[9:].to_list()\nfeatures","090ae4e7":"features.remove('numGamesTeam')","baf7b2f8":"\"\"\"\n## kfold ##\n\nfolds = create_folds(train, 5,\"targetAvg\")\n\n\n## optuna ##\n    \ntraindf = folds[folds[\"kfold\"]!=0]\nvaliddf = folds[folds[\"kfold\"]==0]\n\ntraindf = traindf.reset_index(drop=True)\nvaliddf = validdf.reset_index(drop=True)\n\nlgb_train = lgbo.Dataset(traindf[features],traindf[\"targetAvg\"],categorical_feature=cols)\nlgb_valid = lgbo.Dataset(validdf[features],validdf[\"targetAvg\"],categorical_feature=cols)\nlgbm_params_optuna = {\n    'objective': 'mae', \n        \"seed\":42,\n    'metric': 'mae',\n\"verbose\":-1}\n\nres = {}\nmodel = lgbo.train(lgbm_params_optuna, lgb_train, valid_sets=lgb_valid,\n                  verbose_eval=50,  \n                  num_boost_round=1000,  \n                   early_stopping_rounds=10, \n                 evals_result=res)\n\nlgb_params2 = model.params\n\nwith open(\"LGBM_optuna.bin', 'wb') as f:\n          pickle.dump(model2, f)\n\n\n## lgbm ####\n    \nfor fold in range(5):\n    print(\"--fole-----{}---start-----\".format(str(fold)))\n\n    traindf = folds[folds[\"kfold\"]!=fold]\n    validdf = folds[folds[\"kfold\"]==fold]\n\n    traindf = traindf.reset_index(drop=True)\n    validdf = validdf.reset_index(drop=True)\n\n\n    scores = []\n    res = {}\n    \n    for a in targets:\n\n        lgb_train = lgb.Dataset(traindf[features],traindf[a],categorical_feature=cols)\n        lgb_valid = lgb.Dataset(validdf[features],validdf[a],categorical_feature=cols)\n\n        model2 = lgb.train(lgb_params2, lgb_train, valid_sets=lgb_valid,\n                      verbose_eval=50,  \n                      num_boost_round=1000,  \n                      early_stopping_rounds=10, \n                     evals_result=res)\n\n        tmpdf = pd.DataFrame(res)\n        scores.append(np.min(tmpdf[\"valid_0\"][0]))\n\n        with open(\"LGBM_fold\"+str(fold) +\"_\" + a + '.bin', 'wb') as f:\n                  pickle.dump(model2, f)\n\n\"\"\"\n\n","b21b2d89":"Vcount = train[\"target1\"].value_counts().reset_index()\nVcount.columns = [\"target1\",\"count\"]\nVcount","f63a1bed":"analize_t1 = train[\"target1\"][(train[\"target1\"]!=0)&(train[\"target1\"]!=100)].reset_index()\nanalize_t1","2f20e1d4":"plt.figure(figsize=(25,5))\nfor num,a in enumerate(targets):\n\n    plt.subplot(1,4,num+1)\n    plt.hist(train[a])\n    plt.xlabel(a)","244049b0":"plt.figure(figsize=(25,5))\nfor num,a in enumerate(targets):\n    \n    analize_t1 = train[a][(train[a]!=0)&(train[a]!=100)].reset_index()\n\n    plt.subplot(1,4,num+1)\n    plt.hist(np.log(analize_t1[a]))\n    plt.xlabel(\"log \" + a)","ceff493f":"\"\"\"\n\nfor a in targets:\n\n    ### pull out target 0, target 100\n    \n    train0 = train[(train[a]!=0) & (train[a]!=100)].reset_index(drop=True)\n\n    ### kfold ###\n    \n    folds = create_folds(train0, 5,a)\n\n    folds[a] = np.log10(folds[a])\n\n    ## optuna ##\n    \n    traindf = folds[folds[\"kfold\"]!=0]\n    validdf = folds[folds[\"kfold\"]==0]\n\n    traindf = traindf.reset_index(drop=True)\n    validdf = validdf.reset_index(drop=True)\n\n    lgb_train = lgbo.Dataset(traindf[features],traindf[a],categorical_feature=cols)\n    lgb_valid = lgbo.Dataset(validdf[features],validdf[a],categorical_feature=cols)\n    lgbm_params_optuna = {\n        'objective': 'mae', # Binary classification : 2\u5024\u5206\u985e\u3067\u306f\u3053\u308c\u3092\u4f7f\u3046\n            \"seed\":42,\n        'metric': 'mae',\n    \"verbose\":-1}\n\n    res = {}\n    model = lgbo.train(lgbm_params_optuna, lgb_train, valid_sets=lgb_valid,\n                      verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                      num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                      early_stopping_rounds=10, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                     evals_result=res)\n\n    model.params\n\n    lgb_params2 = model.params\n    \n    ## lgbm ####\n    \n    for fold in range(5):\n        print(\"--fole-----{}---start-----\".format(str(fold)))\n        \n        traindf = folds[folds[\"kfold\"]!=fold]\n        validdf = folds[folds[\"kfold\"]==fold]\n\n        traindf = traindf.reset_index(drop=True)\n        validdf = validdf.reset_index(drop=True)\n\n\n        scores = []\n        res = {}\n\n       \n        lgb_train = lgb.Dataset(traindf[features],traindf[a],categorical_feature=cols)\n        lgb_valid = lgb.Dataset(validdf[features],validdf[a],categorical_feature=cols)\n\n        model2 = lgb.train(lgb_params2, lgb_train, valid_sets=lgb_valid,\n                      verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                      num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                      early_stopping_rounds=10, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                     evals_result=res)\n\n        tmpdf = pd.DataFrame(res)\n        scores.append(np.min(tmpdf[\"valid_0\"][0]))\n\n        with open(savepath + \"\/LGBM_fold\"+str(fold) +\"_\" + a + '.bin', 'wb') as f:\n                  pickle.dump(model2, f)\n\n\n\"\"\"","8a06f53b":"train.groupby(\"noHitter\")[targets].median().reset_index()","7e31c366":"april = pd.read_csv(\"..\/input\/optuna16\/optuna16_aprildiff.csv\")\napril[\"targetAve_diff\"] = april.iloc[:,1:].mean(axis=1)\napril = april.sort_values(\"targetAve_diff\").reset_index(drop=True)\napril","6e03a839":"targets2 = april.columns[1:5].to_list()\nnum = 1\ntargets2\nfor a in targets2:\n    april[a] = np.where(april.index>=num,0,april[a])\n\napril","8c0f70ee":"del train\n_ = gc.collect()","b59ffc43":"futureLAG = pd.read_pickle(\"..\/input\/optuna9\/lag_evalsheet_light20210706.pkl\")\nfutureLAG","dd64ba56":"futureLAG = futureLAG.drop(\"dailyDataDate2\",axis=1)","550965c3":"training = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/train.csv\",nrows=100)","9fc23501":"# Helper function to unpack json found in daily data\ndef unpack_json(json_str):\n    return np.nan if pd.isna(json_str) else pd.read_json(json_str)\n\ndf_names = ['seasons', 'teams', 'players', 'awards']\n\npath = \"..\/input\/mlb-player-digital-engagement-forecasting\"\n\nfor name in df_names:\n    globals()[name] = pd.read_csv(os.path.join(path,name)+ \".csv\")\n","aa181ae7":"#### Unnest various nested data within training (daily) data ####\ndaily_data_unnested_dfs = pd.DataFrame(data = {\n  'dfName': training.drop('date', axis = 1).columns.values.tolist()\n  })\n\ndaily_data_unnested_dfs['df'] = [pd.DataFrame() for row in \n  daily_data_unnested_dfs.iterrows()]\n\nfor df_index, df_row in daily_data_unnested_dfs.iterrows():\n    nestedTableName = str(df_row['dfName'])\n    \n    date_nested_table = training[['date', nestedTableName]]\n    \n    date_nested_table = (date_nested_table[\n      ~pd.isna(date_nested_table[nestedTableName])\n      ].\n      reset_index(drop = True)\n      )\n    \n    daily_dfs_collection = []\n    \n    for date_index, date_row in date_nested_table.iterrows():\n        daily_df = unpack_json(date_row[nestedTableName])\n        \n        daily_df['dailyDataDate'] = date_row['date']\n        \n        daily_dfs_collection = daily_dfs_collection + [daily_df]\n\n    unnested_table = pd.concat(daily_dfs_collection,\n      ignore_index = True).set_index('dailyDataDate').reset_index()\n\n    # Creates 1 pandas df per unnested df from daily data read in, with same name\n    globals()[df_row['dfName']] = unnested_table    \n    \n    daily_data_unnested_dfs['df'][df_index] = unnested_table\n\ndel training\ngc.collect()","1b43b11c":"def preparation(training,sample_sub):\n    training['date'] = pd.to_datetime(training['date'], format=\"%Y%m%d\")\n    sample_sub['date'] = pd.to_datetime(sample_sub['date'], format=\"%Y%m%d\")\n    sample_sub = sample_sub[[\"date\",\"date_playerId\"]]\n    sample_sub[\"playerId\"] = [s.split(\"_\")[-1] for s in sample_sub[\"date_playerId\"]]\n    sample_sub[\"playerId\"] = sample_sub[\"playerId\"].astype(\"int\")\n    sample_sub.columns = ['dailyDataDate',\"date_playerId\",\"playerId\"]\n    \n    \n\n\n\n    #### Unnest various nested data within training (daily) data ####\n    daily_data_unnested_dfs = pd.DataFrame(data = {\n      'dfName': training.drop('date', axis = 1).columns.values.tolist()\n      })\n\n    daily_data_unnested_dfs['df'] = [pd.DataFrame() for row in \n      daily_data_unnested_dfs.iterrows()]\n\n    for df_index, df_row in daily_data_unnested_dfs.iterrows():\n        nestedTableName = str(df_row['dfName'])\n\n        date_nested_table = training[['date', nestedTableName]]\n\n        date_nested_table = (date_nested_table[\n          ~pd.isna(date_nested_table[nestedTableName])\n          ].\n          reset_index(drop = True)\n          )\n\n        daily_dfs_collection = []\n\n        for date_index, date_row in date_nested_table.iterrows():\n            daily_df = unpack_json(date_row[nestedTableName])\n\n            daily_df['dailyDataDate'] = date_row['date']\n\n            daily_dfs_collection = daily_dfs_collection + [daily_df]\n            \n        # adding\n        if len(daily_dfs_collection) == 0:\n\n            unnested_table = pd.DataFrame(columns=eval(nestedTableName).columns)\n            unnested_table['dailyDataDate'] = testdf['date']\n\n        else:\n            unnested_table = pd.concat(daily_dfs_collection,\n          ignore_index = True).set_index('dailyDataDate').reset_index()\n\n\n\n        # Creates 1 pandas df per unnested df from daily data read in, with same name\n        globals()[df_row['dfName']] = unnested_table.copy()    \n\n        daily_data_unnested_dfs['df'][df_index] = unnested_table.copy()\n\n    del training\n    gc.collect()\n\n\n\n    #### Get some information on each date in daily data (using season dates of interest) ####\n    dates = pd.DataFrame(data = \n      {'dailyDataDate': games['dailyDataDate'].unique()})\n\n    dates['date'] = pd.to_datetime(dates['dailyDataDate'].astype(str))\n\n    dates['year'] = dates['date'].dt.year\n    dates['month'] = dates['date'].dt.month\n\n    dates_with_info = pd.merge(\n      dates,\n      seasons,\n      left_on = 'year',\n      right_on = 'seasonId'\n      )\n\n    dates_with_info['inSeason'] = (\n      dates_with_info['date'].between(\n        dates_with_info['regularSeasonStartDate'],\n        dates_with_info['postSeasonEndDate'],\n        inclusive = True\n        )\n      )\n\n    dates_with_info['seasonPart'] = np.select(\n      [\n        dates_with_info['date'] < dates_with_info['preSeasonStartDate'], \n        dates_with_info['date'] < dates_with_info['regularSeasonStartDate'],\n        dates_with_info['date'] <= dates_with_info['lastDate1stHalf'],\n        dates_with_info['date'] < dates_with_info['firstDate2ndHalf'],\n        dates_with_info['date'] <= dates_with_info['regularSeasonEndDate'],\n        dates_with_info['date'] < dates_with_info['postSeasonStartDate'],\n        dates_with_info['date'] <= dates_with_info['postSeasonEndDate'],\n        dates_with_info['date'] > dates_with_info['postSeasonEndDate']\n      ], \n      [\n        'Offseason',\n        'Preseason',\n        'Reg Season 1st Half',\n        'All-Star Break',\n        'Reg Season 2nd Half',\n        'Between Reg and Postseason',\n        'Postseason',\n        'Offseason'\n      ], \n      default = np.nan\n      )\n\n    #### Add some pitching stats\/pieces of info to player game level stats ####\n\n    player_game_stats = (playerBoxScores.copy().\n      # Change team Id\/name to reflect these come from player game, not roster\n      rename(columns = {'teamId': 'gameTeamId', 'teamName': 'gameTeamName'})\n      )\n\n    # Adds in field for innings pitched as fraction (better for aggregation)\n    \n    # add chumajin I don't know why np.where becomes error... this is repaired version\n    if pd.isna(player_game_stats['inningsPitched'])[0]:\n        player_game_stats['inningsPitchedAsFrac'] = np.nan\n    else:\n        player_game_stats['inningsPitchedAsFrac'] = np.where(\n          pd.isna(player_game_stats['inningsPitched']),\n          np.nan,\n          np.floor(player_game_stats['inningsPitched']) +\n            (player_game_stats['inningsPitched'] -\n              np.floor(player_game_stats['inningsPitched'])) * 10\/3\n          )\n\n    # Add in Tom Tango pitching game score (https:\/\/www.mlb.com\/glossary\/advanced-stats\/game-score)\n    player_game_stats['pitchingGameScore'] = (40\n    #     + 2 * player_game_stats['outs']\n        + 1 * player_game_stats['strikeOutsPitching']\n        - 2 * player_game_stats['baseOnBallsPitching']\n        - 2 * player_game_stats['hitsPitching']\n        - 3 * player_game_stats['runsPitching']\n        - 6 * player_game_stats['homeRunsPitching']\n        )\n\n    # Add in criteria for no-hitter by pitcher (individual, not multiple pitchers)\n    player_game_stats['noHitter'] = np.where(\n      (player_game_stats['gamesStartedPitching'] == 1) &\n      (player_game_stats['inningsPitched'] >= 9) &\n      (player_game_stats['hitsPitching'] == 0),\n      1, 0\n      )\n\n    player_date_stats_agg = pd.merge(\n      (player_game_stats.\n        groupby(['dailyDataDate', 'playerId'], as_index = False).\n        # Some aggregations that are not simple sums\n        agg(\n          numGames = ('gamePk', 'nunique'),\n          # Should be 1 team per player per day, but adding here for 1 exception:\n          # playerId 518617 (Jake Diekman) had 2 games for different teams marked\n          # as played on 5\/19\/19, due to resumption of game after he was traded\n          numTeams = ('gameTeamId', 'nunique'),\n          # Should be only 1 team for almost all player-dates, taking min to simplify\n          gameTeamId = ('gameTeamId', 'min')\n          )\n        ),\n      # Merge with a bunch of player stats that can be summed at date\/player level\n      (player_game_stats.\n        groupby(['dailyDataDate', 'playerId'], as_index = False)\n        [['runsScored', 'homeRuns', 'strikeOuts', 'baseOnBalls', 'hits',\n          'hitByPitch', 'atBats', 'caughtStealing', 'stolenBases',\n          'groundIntoDoublePlay', 'groundIntoTriplePlay', 'plateAppearances',\n          'totalBases', 'rbi', 'leftOnBase', 'sacBunts', 'sacFlies',\n          'gamesStartedPitching', 'runsPitching', 'homeRunsPitching', \n          'strikeOutsPitching', 'baseOnBallsPitching', 'hitsPitching',\n          'inningsPitchedAsFrac', 'earnedRuns', \n          'battersFaced','saves', 'blownSaves', 'pitchingGameScore', \n          'noHitter'\n          ]].\n        sum()\n        ),\n      on = ['dailyDataDate', 'playerId'],\n      how = 'inner'\n      )\n\n    #### Turn games table into 1 row per team-game, then merge with team box scores ####\n    # Filter to regular or Postseason games w\/ valid scores for this part\n    games_for_stats = games[\n      np.isin(games['gameType'], ['R', 'F', 'D', 'L', 'W', 'C', 'P']) &\n      ~pd.isna(games['homeScore']) &\n      ~pd.isna(games['awayScore'])\n      ]\n\n    # Get games table from home team perspective\n    games_home_perspective = games_for_stats.copy()\n\n    # Change column names so that \"team\" is \"home\", \"opp\" is \"away\"\n    games_home_perspective.columns = [\n      col_value.replace('home', 'team').replace('away', 'opp') for \n        col_value in games_home_perspective.columns.values]\n\n    games_home_perspective['isHomeTeam'] = 1\n\n    # Get games table from away team perspective\n    games_away_perspective = games_for_stats.copy()\n\n    # Change column names so that \"opp\" is \"home\", \"team\" is \"away\"\n    games_away_perspective.columns = [\n      col_value.replace('home', 'opp').replace('away', 'team') for \n        col_value in games_away_perspective.columns.values]\n\n    games_away_perspective['isHomeTeam'] = 0\n\n    # Put together games from home\/away perspective to get df w\/ 1 row per team game\n    team_games = (pd.concat([\n      games_home_perspective,\n      games_away_perspective\n      ],\n      ignore_index = True)\n      )\n\n    # Copy over team box scores data to modify\n    team_game_stats = teamBoxScores.copy()\n\n    # Change column names to reflect these are all \"team\" stats - helps \n    # to differentiate from individual player stats if\/when joining later\n    team_game_stats.columns = [\n      (col_value + 'Team') \n      if (col_value not in ['dailyDataDate', 'home', 'teamId', 'gamePk',\n        'gameDate', 'gameTimeUTC'])\n        else col_value\n      for col_value in team_game_stats.columns.values\n      ]\n\n    # Merge games table with team game stats\n    team_games_with_stats = pd.merge(\n      team_games,\n      team_game_stats.\n        # Drop some fields that are already present in team_games table\n        drop(['home', 'gameDate', 'gameTimeUTC'], axis = 1),\n      on = ['dailyDataDate', 'gamePk', 'teamId'],\n      # Doing this as 'inner' join excludes spring training games, postponed games,\n      # etc. from original games table, but this may be fine for purposes here \n      how = 'inner'\n      )\n\n    team_date_stats_agg = (team_games_with_stats.\n      groupby(['dailyDataDate', 'teamId', 'gameType', 'oppId', 'oppName'], \n        as_index = False).\n      agg(\n        numGamesTeam = ('gamePk', 'nunique'),\n        winsTeam = ('teamWinner', 'sum'),\n        lossesTeam = ('oppWinner', 'sum'),\n        runsScoredTeam = ('teamScore', 'sum'),\n        runsAllowedTeam = ('oppScore', 'sum')\n        )\n       )\n\n    # Prepare standings table for merge w\/ player digital engagement data\n    # Pick only certain fields of interest from standings for merge\n    standings_selected_fields = (standings[['dailyDataDate', 'teamId', \n      'streakCode', 'divisionRank', 'leagueRank', 'wildCardRank', 'pct'\n      ]].\n      rename(columns = {'pct': 'winPct'})\n      )\n\n    # Change column names to reflect these are all \"team\" standings - helps \n    # to differentiate from player-related fields if\/when joining later\n    standings_selected_fields.columns = [\n      (col_value + 'Team') \n      if (col_value not in ['dailyDataDate', 'teamId'])\n        else col_value\n      for col_value in standings_selected_fields.columns.values\n      ]\n\n    standings_selected_fields['streakLengthTeam'] = (\n      standings_selected_fields['streakCodeTeam'].\n        str.replace('W', '').\n        str.replace('L', '').\n        astype(float)\n        )\n\n    # Add fields to separate winning and losing streak from streak code\n    standings_selected_fields['winStreakTeam'] = np.where(\n      standings_selected_fields['streakCodeTeam'].str[0] == 'W',\n      standings_selected_fields['streakLengthTeam'],\n      np.nan\n      )\n\n    standings_selected_fields['lossStreakTeam'] = np.where(\n      standings_selected_fields['streakCodeTeam'].str[0] == 'L',\n      standings_selected_fields['streakLengthTeam'],\n      np.nan\n      )\n\n    standings_for_digital_engagement_merge = (pd.merge(\n      standings_selected_fields,\n      dates_with_info[['dailyDataDate', 'inSeason']],\n      on = ['dailyDataDate'],\n      how = 'left'\n      ).\n      # Limit down standings to only in season version\n      query(\"inSeason\").\n      # Drop fields no longer necessary (in derived values, etc.)\n      drop(['streakCodeTeam', 'streakLengthTeam', 'inSeason'], axis = 1).\n      reset_index(drop = True)\n      )\n\n    #### Merge together various data frames to add date, player, roster, and team info ####\n    # Copy over player engagement df to add various pieces to it\n\n    # remove chumajin\n    #player_engagement_with_info = nextDayPlayerEngagement.copy()\n    player_engagement_with_info = sample_sub.copy()\n\n\n    # Take \"row mean\" across targets to add (helps with studying all 4 targets at once)\n    \"\"\"\n    player_engagement_with_info['targetAvg'] = np.mean(\n      player_engagement_with_info[['target1', 'target2', 'target3', 'target4']],\n      axis = 1)\n    \"\"\"\n\n\n    # Merge in date information\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      dates_with_info[['dailyDataDate', 'date', 'year', 'month', 'inSeason',\n        'seasonPart']],\n      on = ['dailyDataDate'],\n      how = 'left'\n      )\n\n    # Merge in some player information\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      players[['playerId', 'playerName', 'DOB', 'mlbDebutDate', 'birthCity',\n        'birthStateProvince', 'birthCountry', 'primaryPositionName']],\n       on = ['playerId'],\n       how = 'left'\n       )\n\n\n    # Merge in some player roster information by date\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      (rosters[['dailyDataDate', 'playerId', 'statusCode', 'status', 'teamId']].\n        rename(columns = {\n          'statusCode': 'rosterStatusCode',\n          'status': 'rosterStatus',\n          'teamId': 'rosterTeamId'\n          })\n        ),\n      on = ['dailyDataDate', 'playerId'],\n      how = 'left'\n      )\n\n    # Merge in team name from player's roster team\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      (teams[['id', 'teamName']].\n        rename(columns = {\n          'id': 'rosterTeamId',\n          'teamName': 'rosterTeamName'\n          })\n        ),\n      on = ['rosterTeamId'],\n      how = 'left'\n      )\n\n    # Merge in some player game stats (previously aggregated) from that date\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      player_date_stats_agg,\n      on = ['dailyDataDate', 'playerId'],\n      how = 'left'\n      )\n\n    # Merge in team name from player's game team\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      (teams[['id', 'teamName']].\n        rename(columns = {\n          'id': 'gameTeamId',\n          'teamName': 'gameTeamName'\n          })\n        ),\n      on = ['gameTeamId'],\n      how = 'left'\n      )\n\n    # Merge in some team game stats\/results (previously aggregated) from that date\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      team_date_stats_agg.rename(columns = {'teamId': 'gameTeamId'}),\n      on = ['dailyDataDate', 'gameTeamId'],\n      how = 'left'\n      )\n\n    # Merge in player transactions of note on that date\n\n    # Merge in some pieces of team standings (previously filter\/processed) from that date\n    player_engagement_with_info = pd.merge(\n      player_engagement_with_info,\n      standings_for_digital_engagement_merge.\n        rename(columns = {'teamId': 'gameTeamId'}),\n      on = ['dailyDataDate', 'gameTeamId'],\n      how = 'left'\n      )\n\n    #display(player_engagement_with_info)\n    \n    return player_engagement_with_info\n","f3bd49fd":"features3 = [#'dailyDataDate',\n    #'engagementMetricsDate',\n    #'playerId', 'target1','target2', 'target3', 'target4', 'targetAvg', \n    \n    #'date',\n    'year',\n    'month',\n    'inSeason',\n    'seasonPart',\n    'playerName',\n    'DOB',\n    'mlbDebutDate',\n     'birthCity',\n    'birthStateProvince',\n    'birthCountry',\n    'primaryPositionName',\n    'rosterStatusCode',\n    'rosterStatus',\n     'rosterTeamId',\n    'rosterTeamName', \n    'numGames',\n    'numTeams', \n    'gameTeamId',\n    'runsScored',\n    'homeRuns',\n    \n    'strikeOuts',\n    'baseOnBalls', \n    'hits',\n    'hitByPitch',\n    'atBats', \n    'caughtStealing',\n    'stolenBases',\n    'groundIntoDoublePlay',\n    'groundIntoTriplePlay',\n    'plateAppearances',\n     'totalBases', \n    'rbi', \n    'leftOnBase',\n   'sacBunts',\n    'sacFlies',\n     'gamesStartedPitching',\n    'runsPitching',\n    'homeRunsPitching',\n      'strikeOutsPitching',\n    'baseOnBallsPitching',\n    'hitsPitching',\n       'inningsPitchedAsFrac',\n    'earnedRuns',\n    'battersFaced',\n    'saves',\n    'blownSaves',\n    'pitchingGameScore',\n    'noHitter',\n    'gameTeamName',\n     'gameType',\n    'oppId',\n    'oppName',\n    'numGamesTeam',\n    'winsTeam',\n    'lossesTeam',\n    'runsScoredTeam',\n    'runsAllowedTeam', \n    'divisionRankTeam',\n    'leagueRankTeam', \n    'wildCardRankTeam', \n    'winPctTeam', \n    'winStreakTeam',\n    'lossStreakTeam']","4702ad9c":"path3 = \"..\/input\/mlbsharemodela\"\n\ntarget1_models3 = [pickle.load(open(os.path.join(path3,s), 'rb')) for s in os.listdir(path3) if \"target1\" in s]\ntarget2_models3 = [pickle.load(open(os.path.join(path3,s), 'rb')) for s in os.listdir(path3) if \"target2\" in s]\ntarget3_models3 = [pickle.load(open(os.path.join(path3,s), 'rb')) for s in os.listdir(path3) if \"target3\" in s]\ntarget4_models3 = [pickle.load(open(os.path.join(path3,s), 'rb')) for s in os.listdir(path3) if \"target4\" in s]\n\nallmodels3 = [target1_models3,target2_models3,target3_models3,target4_models3]","61580401":"def prediction3(models):\n    preds=[]\n    for model in models:\n        preds.append(model.predict(test_feats[features3]))\n    preds = np.mean(preds,axis=0)\n    preds = np.clip(preds,0,100)\n    return preds","d0c40a66":"features = ['year',\n 'month',\n 'inSeason',\n 'seasonPart',\n 'playerName',\n 'DOB',\n 'mlbDebutDate',\n 'birthCity',\n 'birthStateProvince',\n 'birthCountry',\n 'primaryPositionName',\n 'rosterStatusCode',\n 'rosterStatus',\n 'rosterTeamId',\n 'rosterTeamName',\n 'numGames',\n 'numTeams',\n 'gameTeamId',\n 'runsScored',\n 'homeRuns',\n 'strikeOuts',\n 'baseOnBalls',\n 'hits',\n 'hitByPitch',\n 'atBats',\n 'caughtStealing',\n 'stolenBases',\n 'groundIntoDoublePlay',\n 'groundIntoTriplePlay',\n 'plateAppearances',\n 'totalBases',\n 'rbi',\n 'leftOnBase',\n 'sacBunts',\n 'sacFlies',\n 'gamesStartedPitching',\n 'runsPitching',\n 'homeRunsPitching',\n 'strikeOutsPitching',\n 'baseOnBallsPitching',\n 'hitsPitching',\n 'inningsPitchedAsFrac',\n 'earnedRuns',\n 'battersFaced',\n 'saves',\n 'blownSaves',\n 'pitchingGameScore',\n 'noHitter',\n 'gameTeamName',\n 'gameType',\n 'oppId',\n 'oppName',\n 'numGamesTeam',\n 'winsTeam',\n 'lossesTeam',\n 'runsScoredTeam',\n 'runsAllowedTeam',\n 'divisionRankTeam',\n 'leagueRankTeam',\n 'wildCardRankTeam',\n 'winPctTeam',\n 'winStreakTeam',\n 'lossStreakTeam',\n 'target1_mean',\n 'target2_mean',\n 'target3_mean',\n 'target4_mean',\n 'ct',\n 'Lag31_target1_median',\n 'Lag31_target1_max',\n 'Lag31_target2_median',\n 'Lag31_target2_max',\n 'Lag31_target3_median',\n 'Lag31_target3_max',\n 'Lag31_target4_median',\n 'Lag31_target4_max',\n 'Lag45_target1_median',\n 'Lag45_target1_max',\n 'Lag45_target2_median',\n 'Lag45_target2_max',\n 'Lag45_target3_median',\n 'Lag45_target3_max',\n 'Lag45_target4_median',\n 'Lag45_target4_max',\n 'Lag60_target1_median',\n 'Lag60_target1_max',\n 'Lag60_target2_median',\n 'Lag60_target2_max',\n 'Lag60_target3_median',\n 'Lag60_target3_max',\n 'Lag60_target4_median',\n 'Lag60_target4_max',\n 'Lag180_target1_median',\n 'Lag180_target1_max',\n 'Lag180_target2_median',\n 'Lag180_target2_max',\n 'Lag180_target3_median',\n 'Lag180_target3_max',\n 'Lag180_target4_median',\n 'Lag180_target4_max',\n 'Ability_target1_mean',\n 'Ability_target1_median',\n 'Ability_target1_max',\n 'Ability_target1_min',\n 'Ability_target1_std',\n 'Ability_target2_mean',\n 'Ability_target2_median',\n 'Ability_target2_max',\n 'Ability_target2_min',\n 'Ability_target2_std',\n 'Ability_target3_mean',\n 'Ability_target3_median',\n 'Ability_target3_max',\n 'Ability_target3_min',\n 'Ability_target3_std',\n 'Ability_target4_mean',\n 'Ability_target4_median',\n 'Ability_target4_max',\n 'Ability_target4_min',\n 'Ability_target4_std',\n 'Ability_targetAvg_mean',\n 'Ability_targetAvg_median',\n 'Ability_targetAvg_max',\n 'Ability_targetAvg_min',\n 'Ability_targetAvg_std']","1f288e5b":"path = \"..\/input\/optuna9\"\n\ntarget1_models = [pickle.load(open(os.path.join(path,s), 'rb')) for s in os.listdir(path) if \"target1\" in s]\ntarget2_models = [pickle.load(open(os.path.join(path,s), 'rb')) for s in os.listdir(path) if \"target2\" in s]\ntarget3_models = [pickle.load(open(os.path.join(path,s), 'rb')) for s in os.listdir(path) if \"target3\" in s]\ntarget4_models = [pickle.load(open(os.path.join(path,s), 'rb')) for s in os.listdir(path) if \"target4\" in s]\n\nallmodels = [target1_models,target2_models,target3_models,target4_models]","7794af27":"def prediction(models):\n    preds=[]\n    for model in models:\n        preds.append(model.predict(test_feats[features]))\n    preds = np.mean(preds,axis=0)\n    preds = np.clip(preds,0,100)\n    return preds","24302a33":"features2 = [#'dailyDataDate',\n    #'engagementMetricsDate',\n    #'playerId', 'target1','target2', 'target3', 'target4', 'targetAvg', \n    \n    #'date',\n    'year',\n    'month',\n    'inSeason',\n    'seasonPart',\n    'playerName',\n    'DOB',\n    'mlbDebutDate',\n     'birthCity',\n    'birthStateProvince',\n    'birthCountry',\n    'primaryPositionName',\n    'rosterStatusCode',\n    'rosterStatus',\n     'rosterTeamId',\n    'rosterTeamName', \n    'numGames',\n    'numTeams', \n    'gameTeamId',\n    'runsScored',\n    'homeRuns',\n    \n    'strikeOuts',\n    'baseOnBalls', \n    'hits',\n    'hitByPitch',\n    'atBats', \n    'caughtStealing',\n    'stolenBases',\n    'groundIntoDoublePlay',\n    'groundIntoTriplePlay',\n    'plateAppearances',\n     'totalBases', \n    'rbi', \n    'leftOnBase',\n   'sacBunts',\n    'sacFlies',\n     'gamesStartedPitching',\n    'runsPitching',\n    'homeRunsPitching',\n      'strikeOutsPitching',\n    'baseOnBallsPitching',\n    'hitsPitching',\n       'inningsPitchedAsFrac',\n    'earnedRuns',\n    'battersFaced',\n    'saves',\n    'blownSaves',\n    'pitchingGameScore',\n    'noHitter',\n    'gameTeamName',\n     'gameType',\n    'oppId',\n    'oppName',\n   # 'numGamesTeam',\n    'winsTeam',\n    'lossesTeam',\n    'runsScoredTeam',\n    'runsAllowedTeam', \n    'divisionRankTeam',\n    'leagueRankTeam', \n    'wildCardRankTeam', \n    'winPctTeam', \n    'winStreakTeam',\n    'lossStreakTeam',\n    'target1_mean', 'target2_mean', 'target3_mean',\n       'target4_mean', 'ct', 'Lag31_target1_median',\n       'Lag31_target2_median', 'Lag31_target3_median',\n       'Lag31_target4_median', 'Lag45_target1_median',\n       'Lag45_target2_median', 'Lag45_target3_median',\n       'Lag45_target4_median', 'Lag60_target1_median',\n       'Lag60_target2_median', 'Lag60_target3_median',\n       'Lag60_target4_median', 'Lag180_target1_median',\n       'Lag180_target2_median', 'Lag180_target3_median',\n       'Lag180_target4_median'\n]","2f3908a2":"path2 = \"..\/input\/optuna16\"\n\ntarget1_models2 = [pickle.load(open(os.path.join(path2,s), 'rb')) for s in os.listdir(path2) if (\"target1\" in s) & ('numGamesTeam' in s)]\ntarget2_models2 = [pickle.load(open(os.path.join(path2,s), 'rb')) for s in os.listdir(path2) if (\"target2\" in s)  & ('numGamesTeam' in s)]\ntarget3_models2 = [pickle.load(open(os.path.join(path2,s), 'rb')) for s in os.listdir(path2) if (\"target3\" in s)  & ('numGamesTeam' in s)]\ntarget4_models2 = [pickle.load(open(os.path.join(path2,s), 'rb')) for s in os.listdir(path2) if (\"target4\" in s)  & ('numGamesTeam' in s)]\n\nallmodels2 = [target1_models2,target2_models2,target3_models2,target4_models2]","8ff11216":"def prediction2(models):\n    preds=[]\n    for model in models:\n        preds.append(model.predict(test_feats[features2]))\n    preds = np.mean(preds,axis=0)\n    preds = np.clip(preds,0,100)\n    return preds","9fb2e7a8":"path4 = \"..\/input\/optuna25log\"\n\n\ntarget1_models4 = [pickle.load(open(os.path.join(path4,s), 'rb')) for s in os.listdir(path4) if (\"target1\" in s)]\ntarget2_models4 = [pickle.load(open(os.path.join(path4,s), 'rb')) for s in os.listdir(path4) if (\"target2\" in s)]\ntarget3_models4 = [pickle.load(open(os.path.join(path4,s), 'rb')) for s in os.listdir(path4) if (\"target3\" in s)]\ntarget4_models4 = [pickle.load(open(os.path.join(path4,s), 'rb')) for s in os.listdir(path4) if (\"target4\" in s)]\n\nallmodels4 = [target1_models4,target2_models4,target3_models4,target4_models4]","e7c6ab4d":"def prediction4(models):\n    preds=[]\n    for model in models:\n        preds.append(model.predict(test_feats[features2]))\n    preds = np.mean(preds,axis=0)\n    \n    preds = np.power(10,preds) \n    \n    preds = np.clip(preds,0,100)\n    return preds","85ed47eb":"def prediction22(models,test_feats):\n    preds=[]\n    for model in models:\n        preds.append(model.predict(test_feats[features2]))\n    preds = np.mean(preds,axis=0)\n    \n    preds = np.power(10,preds)\n    \n    preds = np.clip(preds,0,100)\n    return preds","a7297394":"def returnpreddf(test_feats2,allmodels2):\n    \n    indexes = test_feats2.index\n    \n    test_feats2 = test_feats2.reset_index(drop=True)\n\n    allpreds2=[]\n    for models in allmodels2:\n            allpreds2.append(prediction22(models,test_feats2))\n\n    preddf2 = pd.DataFrame(allpreds2)\n    preddf2 = preddf2.T\n    \n    preddf2[\"index\"] = indexes\n\n    \n    return preddf2","8bf8f61f":"path5 = \"..\/input\/optuna32pitcher\"\n\n\ntarget1_models5 = [pickle.load(open(os.path.join(path5,s), 'rb')) for s in os.listdir(path5) if (\"target1\" in s)]\ntarget2_models5 = [pickle.load(open(os.path.join(path5,s), 'rb')) for s in os.listdir(path5) if (\"target2\" in s)]\ntarget3_models5 = [pickle.load(open(os.path.join(path5,s), 'rb')) for s in os.listdir(path5) if (\"target3\" in s)]\ntarget4_models5 = [pickle.load(open(os.path.join(path5,s), 'rb')) for s in os.listdir(path5) if (\"target4\" in s)]\n\nallmodels5 = [target1_models5,target2_models5,target3_models5,target4_models5]","96f2c7b9":"path6 = \"..\/input\/optuna32other\"\n\n\ntarget1_models6 = [pickle.load(open(os.path.join(path6,s), 'rb')) for s in os.listdir(path6) if (\"target1\" in s)]\ntarget2_models6 = [pickle.load(open(os.path.join(path6,s), 'rb')) for s in os.listdir(path6) if (\"target2\" in s)]\ntarget3_models6 = [pickle.load(open(os.path.join(path6,s), 'rb')) for s in os.listdir(path6) if (\"target3\" in s)]\ntarget4_models6 = [pickle.load(open(os.path.join(path6,s), 'rb')) for s in os.listdir(path6) if (\"target4\" in s)]\n\nallmodels6 = [target1_models6,target2_models6,target3_models6,target4_models6]","288c546a":"path7 = \"..\/input\/optuna32hd\"\n\n\ntarget1_models7 = [pickle.load(open(os.path.join(path7,s), 'rb')) for s in os.listdir(path7) if (\"target1\" in s)]\ntarget2_models7 = [pickle.load(open(os.path.join(path7,s), 'rb')) for s in os.listdir(path7) if (\"target2\" in s)]\ntarget3_models7 = [pickle.load(open(os.path.join(path7,s), 'rb')) for s in os.listdir(path7) if (\"target3\" in s)]\ntarget4_models7 = [pickle.load(open(os.path.join(path7,s), 'rb')) for s in os.listdir(path7) if (\"target4\" in s)]\n\nallmodels7 = [target1_models7,target2_models7,target3_models7,target4_models7]","385c4f3d":"if 'kaggle_secrets' in sys.modules:  # only run while on Kaggle\n    import mlb\n\n    env = mlb.make_env()\n    iter_test = env.iter_test()","252d63b6":"for (testdf, sample_sub) in iter_test:\n    \n\n    testdf = testdf.reset_index()\n    testdf = testdf.rename(columns={\"index\":\"date\"})\n\n    sample_sub2 = sample_sub.reset_index()\n    \n    ## adding test_feats if you want to see, look at the below test feats.###\n\n    test_feats = preparation(testdf,sample_sub2)\n    test_feats[\"rosterTeamId\"] = np.where(pd.isna(test_feats[\"rosterTeamId\"]),np.nan,test_feats[\"rosterTeamId\"])\n    \n   \n        \n    test_feats[\"dailyDataDate\"] = pd.to_datetime(test_feats[\"dailyDataDate\"], format=\"%Y%m%d\")\n    futureLAG[\"dailyDataDate\"] = pd.to_datetime(futureLAG[\"dailyDataDate\"], format=\"%Y%m%d\")\n\n    test_feats[\"playerId\"] = test_feats[\"playerId\"].astype(\"int\")\n    futureLAG[\"playerId\"] = futureLAG[\"playerId\"].astype(\"int\")\n    \n    test_feats = pd.merge(test_feats,futureLAG,on=[\"dailyDataDate\",\"playerId\"],how=\"left\")\n    \n     ### Fill in nan when there are not enough features in test_feats##\n\n\n    tcols = test_feats.columns[4:].to_list()\n\n    for a in features:\n        if np.isin(a,tcols) == False:\n            test_feats[a] = np.nan\n\n        #############Label encoding########################\n    \n    for col in cols:\n        \n        test_feats[col] = np.where(test_feats[col].isna(),\"NaN\",test_feats[col])\n        \n        with open(os.path.join(\".\/\",col + '_encoder.txt'), 'rb') as f:\n              le = pickle.load(f)\n        test_feats[col] = le.transform(test_feats[col])\n    \n    #############Maybe I don't need it.########################\n    \n    for feature in features:\n        if test_feats[feature].dtype==\"object\":\n            test_feats[feature] = test_feats[feature].astype(\"float\")\n        \n\n  #### model A ####\n        \n    allpreds=[]\n    for models in allmodels:\n        allpreds.append(prediction(models))\n\n    preddf = pd.DataFrame(allpreds)\n    preddf = preddf.T\n\n\n    ##### model B #######\n\n    allpreds2=[]\n    for models in allmodels2:\n        allpreds2.append(prediction2(models))\n\n    tmpdf2 = pd.DataFrame(allpreds2)\n    tmpdf2 = tmpdf2.T\n    \n    \n   \n    ##### model C#######\n\n    allpreds3=[]\n    for models in allmodels3:\n        allpreds3.append(prediction3(models))\n\n    tmpdf3 = pd.DataFrame(allpreds3)\n    tmpdf3 = tmpdf3.T\n    \n    ##### model D ###log##\n    \n\n    allpreds4=[]\n    for models in allmodels4:\n        allpreds4.append(prediction4(models))\n\n    tmpdf4 = pd.DataFrame(allpreds4)\n    tmpdf4 = tmpdf4.T\n    \n    ##### model E pitcher #####\n    test_feats2 = test_feats[test_feats['primaryPositionName']==6]\n    preddf2 = returnpreddf(test_feats2,allmodels5)\n    \n\n    ##### model E hd #####\n    test_feats4 = test_feats[test_feats['primaryPositionName']==1]\n    preddf4 = returnpreddf(test_feats4,allmodels7)\n\n    ##### model E other #####\n    test_feats3 = test_feats[(test_feats['primaryPositionName']!=6)]\n    test_feats3 = test_feats3[(test_feats3['primaryPositionName']!=1)]\n    \n    preddf3 = returnpreddf(test_feats3,allmodels6)\n\n    #############################\n    \n        \n    preddf5 = pd.concat([preddf2,preddf3,preddf4])\n    \n    \n    preddf5 = preddf5.sort_values(\"index\").reset_index(drop=True)\n    \n    preddf5 = preddf5.drop(\"index\",axis=1)\n    \n    \n\n    ##### merge ########\n\n    ens = [preddf,tmpdf2,tmpdf3]\n    ens = np.mean(ens,axis=0)\n    preddf = pd.DataFrame(ens)\n    \n    ##### merge2 ########\n\n    ens2 = [preddf5,tmpdf4]\n    ens2 = np.mean(ens2,axis=0)\n    preddf6 = pd.DataFrame(ens2)\n\n    ##### merge3 ########\n\n    ens3 = [preddf,preddf6]\n    ens3 = np.mean(ens3,axis=0)\n    preddf = pd.DataFrame(ens3)\n    \n    \n     ### Shohei Otani ###\n    \n    tmpdf2 = preddf.copy()\n    \n    tmpdf2.columns = targets\n    tmpdf2[\"playerId\"] = test_feats[\"playerId\"]\n\n    tmpdf2 = pd.merge(tmpdf2,april,on=\"playerId\",how=\"left\")\n\n\n    for a in targets:\n        tmpdf2[a] = tmpdf2[a] - tmpdf2[a+\"_diff\"]\n        tmpdf2[a] = np.clip(tmpdf2[a],0,100)\n    tmpdf2 = tmpdf2[targets]\n    \n    preddf = tmpdf2.copy()\n    \n    \n    ### no hitter ###\n    playervalue = []\n\n    for num,a in enumerate(test_feats[\"noHitter\"]):\n        if a==1:\n\n            playervalue.append([100,62.182615,93.614439,100])\n\n        else:\n            playervalue.append([np.nan,np.nan,np.nan,np.nan])\n\n    playerdf = pd.DataFrame(playervalue)\n\n    playerdf[0] = np.where(pd.isna(playerdf[0]),preddf[targets[0]],playerdf[0])\n    playerdf[1] = np.where(pd.isna(playerdf[1]),preddf[targets[1]],playerdf[1])\n    playerdf[2] = np.where(pd.isna(playerdf[2]),preddf[targets[2]],playerdf[2])\n    playerdf[3] = np.where(pd.isna(playerdf[3]),preddf[targets[3]],playerdf[3])\n\n\n    preddf = playerdf.copy()\n    \n    \n    \n\n\n    ###### Clean columns ####\n\n    tmpdf = sample_sub.iloc[:,:1]\n    preddf = preddf.set_index(tmpdf.index)\n\n\n    tmpdf = pd.concat([tmpdf,preddf],axis=1)\n    tmpdf.columns = sample_sub.columns\n\n    # Submit \n    env.predict(tmpdf)","728dd3a7":"Only the right 5 columns are used.\n\n\u53f3\u50745\u5217\u3060\u3051\u4f7f\u7528\u3002","42f62997":"If you pull out 0,100 and log this, you can see that it has a clean distribution (especially target4).\n\n0,100\u3092\u629c\u3044\u3066\u3001\u3053\u308c\u306elog\u3092\u53d6\u308b\u3068\u3001\u304d\u308c\u3044\u306a\u5206\u5e03\uff08\u7279\u306btarget4)\u306b\u306a\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002","14d529fa":"Prepare LAG data to merge with the test data created earlier\n\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u30de\u30fc\u30b8\u3059\u308b\u3088\u3046\u306eLAG\u30c7\u30fc\u30bf\u3092\u6e96\u5099","54af70f1":"concat to train_feats\n\ntrain_feats\u306b\u304f\u3063\u3064\u3051\u307e\u3059\u3002\n","30e4f325":"# MLB submission is complete! \n\n* First of all, thank you very much to those who supported me, upvoted EDA, and excited me! I enjoyed experiencing various things.The top ones are really amazing. I respect them.\n\n* There was no leak just before train_updated.csv came out, and I shared the code that was in 12th place (experienced 1st to 12th place on the way) with explanations.\n\n* If I get worse score or submission error, please laugh me ! (I also laugh at my own...)\n\n## **I share this notebook, but it became long. I'm sorry if it feels hard to read. (Especially, infrence)**\n\n## But, if it is helpful for you, I'm grad if you **upvote**!","6c5e0689":"# 7. Correction\nI found that no hitter (perfect game) is obviously a high number, so I corrected it using median values of all train data.\n\nno hitter(\u5b8c\u5168\u8a66\u5408)\u306e\u65e5\u3060\u3051\u660e\u3089\u304b\u306b\u98db\u3073\u5024\u304c\u51fa\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u306e\u3067\u3001\u7f6e\u304d\u63db\u3048\u307e\u3057\u305f\u3002","7b501e17":"See the code from EDA's kaggle staff for adjustments in this area. Anyway, I adjusted it so that the test data also flows.\n(This is the code below. Thank you.)\n\n\u3053\u306e\u8fba\u306e\u8abf\u6574\u306f\u3001EDA\u306ekaggle\u306e\u30b9\u30bf\u30c3\u30d5\u3055\u3093\u306e\u30b3\u30fc\u30c9\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002\u3068\u306b\u304b\u304f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u3082\u6d41\u308c\u308b\u3088\u3046\u306b\u8abf\u6574\u3057\u307e\u3057\u305f\u3002\n(\u2193\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002)\n\nhttps:\/\/www.kaggle.com\/ryanholbrook\/getting-started-with-mlb-player-digital-engagement","89f0ae5c":"targetAvg\u30925\u5206\u5272\u3057\u307e\u3057\u305f\u3002","ec1618dd":"Thank you for reading to the end. It was difficult to explain and it became long. I'm sorry it's hard to read.\n\nI've done a lot of debugging in the last week, but I'm still afraid of submission errors.\nBut I enjoyed it anyway. Thank you so much for everyone!\n\nI hope your good luck in this competition!","314cc147":"## 7.1.2 making test features\n","6e8bc395":"\u6700\u5f8c\u307e\u3067\u8aad\u3093\u3067\u3082\u3089\u3063\u3066\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\u8aac\u660e\u304c\u96e3\u3057\u304f\u3066\u3001\u9577\u304f\u306a\u308a\u307e\u3057\u305f\u3002\u8aad\u307f\u3065\u3089\u304f\u3066\u3059\u307f\u307e\u305b\u3093\u3002\n\n\u6700\u5f8c\u306e1\u9031\u9593\u306f\u30c7\u30d0\u30c3\u30b0\u3092\u305f\u304f\u3055\u3093\u3057\u307e\u3057\u305f\u304c\u3001\u305d\u308c\u3067\u3082\u3001submission error\u3092\u6050\u308c\u3066\u3044\u307e\u3059\u3002\n\u3051\u3069\u3001\u3069\u3061\u3089\u306b\u3057\u3066\u3082\u697d\u3057\u3081\u307e\u3057\u305f\u3002\u672c\u5f53\u306b\u7686\u69d8\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01\n\n\u7686\u69d8\u306e\u5e78\u904b\u3092\u304a\u7948\u308a\u3057\u3066\u3044\u307e\u3059\u3002","412b1a09":"## Model B \n\nIt is a model using features that are not explained\n\n(\u8aac\u660e\u3057\u3066\u3044\u306a\u3044\u7279\u5fb4\u91cf\u3092\u4f7f\u3063\u305f\u30e2\u30c7\u30eb\u3067\u3059)","d895f747":"The number of 0 and 100 is very large. I guessed it was clipped at 0 and 100, so I removed 0 and 100.\n\n0\u3068100\u306e\u6570\u304c\u4ed6\u306b\u6bd4\u3079\u3066\u7570\u69d8\u306b\u591a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u30020\u3068100\u3067\u30af\u30ea\u30c3\u30d7\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u63a8\u6e2c\u3057\u307e\u3057\u305f\u306e\u3067\u30010\u3068100\u306f\u9664\u53bb\u3057\u307e\u3057\u305f\u3002","608d61f6":"concat all LAG dataframes\n\n\u5168\u90e8\u304f\u3063\u3064\u3051\u307e\u3059","b682ade9":"## This notebook cannot cover everything, but I will publish 1-8 above with explanations.\n\n## The part of creating optuna and model is omitted.\n\n\n\u3053\u306enotebook\u3067\u306f\u3001\u5168\u90e8\u306f\u307e\u304b\u306a\u3048\u307e\u305b\u3093\u304c\u3001\u4e0a\u8a181-8\u3092\u89e3\u8aac\u3092\u5165\u308c\u306a\u304c\u3089\u516c\u958b\u3057\u3066\u3044\u304d\u307e\u3059\u3002optuna,model\u4f5c\u6210\u306e\u3068\u3053\u308d\u306f\u30b3\u30fc\u30c9\u3060\u3051\u306b\u3057\u3066\u7701\u7565\u3057\u307e\u3059\u3002","e723215f":"## 6.1 Analysis of Target1","d7f2163b":"# 9. A postscript","c3907377":"Clean the column name\n\n\u30ab\u30e9\u30e0\u540d\u3092\u304d\u308c\u3044\u306b","c8787615":" # 5. In the 2nd phase, we know the correct answer (target value) 31 days ago, so I added it in the features. \n \n With this, optuna + Light GBM with 5kfold and about 1.3373\n\n(Actually, in addition to this, the feature amount was added to the ensemble and submitted.)","8df34777":"\u6700\u5f8c\u306e3rd phase\u306f\u3001\u7d71\u8a08\u7684\u306b\u307f\u3066\u3001target1\uff5e4\u306f\u3001\u305d\u308c\u305e\u308c\u30010\u3068100\u306e\u500b\u6570\u304c\u591a\u304f\u3066\u3001np.clip\u3055\u308c\u3066\u3044\u305d\u3046\u3002\u305d\u3053\u3067\u3001target\u5024\u306e0\u3068100\u306e\u3068\u3053\u308d\u306f\u629c\u3044\u3066\u3001\u305d\u306e\u9593\u306etarget\u5024\u306flog\u3092\u53d6\u308b\u3068\u3001\u304d\u308c\u3044\u306a\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306b\u306a\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u3053\u308c\u3092\u5229\u7528\u3057\u3066\u3001target\u306elog10\u3092\u4e88\u60f3\u3057\u3066\u3001\u6700\u5f8c\u300110\u306epred\u4e57\u306b\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u3084\u308a\u307e\u3057\u305f\u3002optuna\u3068light GBM\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u3067\u30015k fold\u30671.3256\u3002(Light GBM\u3060\u3051\u3067\u3053\u306e\u30b9\u30b3\u30a2\u3060\u304b\u3089\u3001\u3051\u3063\u3053\u3046\u826f\u3044\u304b\u3068)","f65a3a16":"# 6. In the last 3rd phase, I used log scale of targets for learning.\n\nStatistically, targets1 to 4 have a large number of 0s and 100s, respectively, and are likely to be np.clip. Therefore, I found that if the target values 0 and 100 are omitted and the target value between them is logged, a clean histogram will be obtained. Using this, I predicted log10 of target and finally made it 10 to the pred power. optuna and light GBM. Now it's 1.3256 with 5k fold. (Because Light GBM alone has this score, is it pretty good?) ","7507e069":"Separate for train data and test data.\n\ntrain\u30c7\u30fc\u30bf\u7528\u3068test\u30c7\u30fc\u30bf\u7528\u306b\u5206\u96e2\u3057\u307e\u3059\u3002","b47440f4":"This gives a score of 1.3256. After that, I ensembled using the ones created using other features.\n\n\u3053\u308c\u3067\u30b9\u30b3\u30a21.3256\u3002\u3042\u3068\u306f\u3001\u4ed6\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u305f\u3082\u306e\u306a\u3069\u3092\u4f7f\u3063\u3066\u3001\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u307e\u3057\u305f\u3002","891b0862":"Cross Validation\u306f\u30012021\u5e744\u6708\u3060\u3051\u3060\u3068\u4e0d\u5b89\u5b9a\u3060\u3068\u611f\u3058\u3066\u3001\u5168\u4f53\u3092target Average\u30675\u5206\u5272\u3057\u3066kfold\u3057\u307e\u3057\u305f\u3002\n\n\nCommonLit Readability Prize\u3092\u3084\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u4ee5\u4e0b\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002upvote\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\n\nhttps:\/\/www.kaggle.com\/abhishek\/step-1-create-folds","8bce431e":"This will be about 1.3370.","28d2061a":"2nd phase\u306f\u300131\u65e5\u524d\u306e\u6b63\u89e3(target\u5024)\u306f\u77e5\u3063\u3066\u3044\u308b\u304b\u3089\u305d\u308c\u3092\u7279\u5fb4\u91cf\u306b\u5165\u308c\u3066\u307f\u307e\u3057\u305f\u3002\u3000\u3053\u308c\u3067optuna + Light GBM\u3067 5kfold\u30671.3373 \n \n (\u5b9f\u969b\u306f\u3053\u308c\u306b\u52a0\u3048\u3066\u7279\u5fb4\u91cf\u52a0\u3048\u305f\u3082\u306e\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u51fa\u3057\u3066\u3044\u307e\u3057\u305f\u3002 )","f58302f6":"The feature is that log is returned by np.power.\n\nnp.power\u3067log\u3092\u623b\u3057\u3066\u3044\u308b\u306e\u304c\u7279\u5fb4\u3067\u3059\u3002","0d4a110a":"# 8. About real submission","f61aade5":"### Label encoding","b1384fd8":"targets\u306f\u3082\u3046\u4f7f\u308f\u306a\u3044\u306e\u3067\u3001\u843d\u3068\u3057\u307e\u3059\u3002","dbada9ed":"## 4.2 model making","360c5dc4":"1st phase\u306f\u30011.\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3048\u308b\u3082\u306e\u306f\u5168\u90e8\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002(\u516c\u958b\u3055\u308c\u3066\u3044\u308bnotebook\u3068\u306f\u3001\u30e1\u30b8\u30e3\u30fc\u30ea\u30fc\u30b0\u306b\u30c7\u30d3\u30e5\u30fc\u3057\u305f\u65e5\u3068\u304b\u305d\u306e\u8fba\u306e\u6587\u5b57\u5217\u3082Label encoding\u3057\u3066\u5168\u90e8\u5165\u308c\u305f\u306e\u304c\u3001\u9055\u3044\u3067\u3059\u3002\u3053\u306e\u8fba\u306e\u6587\u5b57\u5217\u306fEDA\u3057\u3066\u52b9\u679c\u3042\u308a\u305d\u3046\u3060\u306a\u3068\u601d\u3063\u3066\u3044\u307e\u3057\u305f\u3002)\u3042\u3068\u3001\u3053\u306e\u307e\u307e\u3060\u3068test\u30c7\u30fc\u30bf\u3092\u5909\u63db\u3059\u308b\u3068\u304d\u306b\u3001\u3044\u308d\u3044\u308d\u3068\u4e0d\u5177\u5408\u304c\u3042\u3063\u305f\u306e\u3067\u3001original\u3067\u30b3\u30fc\u30c9\u3092\u4fee\u6b63\u3057\u307e\u3057\u305f\u3002","117366cc":"The basic idea is mentioned above. You may not be able to understand from here, but this is the actual inference code.\n\n\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9\u3092\u4e0a\u306b\u8ff0\u3079\u307e\u3057\u305f\u3002\u3053\u3053\u304b\u3089\u306f\u898b\u3066\u3082\u308f\u304b\u3089\u306a\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u5b9f\u969b\u306einference\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002late submission\u3067\u304d\u306a\u3044\u307f\u305f\u3044\u3067\u3059\u3002","5622a78c":"Therefore, I predicted the target with log and returned it to the end, and the score went up.\n\n\u305d\u306e\u305f\u3081\u3001target\u3092log\u3067\u4e88\u60f3\u3057\u3066\u3001\u6700\u5f8c\u306b\u623b\u3057\u3066\u3042\u3052\u305f\u3089\u30b9\u30b3\u30a2\u304c\u4e0a\u304c\u308a\u307e\u3057\u305f\u3002(1.3373\u21921.3256)","216d72c9":"Next, target LAGS In this case, I took the statics (median,max) values for 31 , 45, 60, and 180 days, respectively.\n\n\u6b21\u306btarget\u306eLAGS \u3053\u306e\u5834\u5408\u300131\u65e5\u300145\u65e5\u300160\u65e5\u3001180\u65e5\u306e\u7d71\u8a08\u5024(median,max)\u3092\u305d\u308c\u305e\u308c\u53d6\u3063\u3066\u3044\u304d\u307e\u3059\u3002","50715faa":"submit\u306f\u2191\u306e5\u3067\u8ff0\u3079\u305f\u7279\u5fb4\u91cf\u3060\u3068\u30019\/15\u65e5\u306f\u300131\u65e5\u524d\u304c8\/15\u304f\u3089\u3044\u3067\u6b63\u89e3\u304c\u306a\u3044\u306e\u3067\u300131\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u306f\u4f7f\u7528\u3067\u304d\u305a\u3002\n47\u65e5\u524d\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u3066\u3001\u30e2\u30c7\u30eb\u4f5c\u6210\u3002 \u203b 9\/16\u65e5(\u30de\u30fc\u30b8\u30f3\u3092\u306a\u305c\u304b1\u65e5\u5165\u308c\u3066\u3057\u307e\u3063\u305f\u3002)\u306e47\u65e5\u524d\u306e7\/31\u307e\u3067\u306e\u6b63\u89e3\u304c\u3042\u308b\u3068\u3053\u308d\u304b\u3089\u30c7\u30fc\u30bf\u304b\u3089\u4f5c\u6210\u3002\n\n1\u3064\u76ee\u306esubmission1\u305d\u306e47\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u3092\u4f7f\u7528(\u88dc\u6b63\u7121\u3057\u306e\u5b89\u5b9aversion)\u3002\n2\u3064\u76ee\u306esubmission2\u306f15\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u300131\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u300147\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u3092\u305d\u308c\u305e\u308c\u4f5c\u6210\u3057\u3001\u65e5\u4ed8(8\/1,8\/16,9\/1)\u306b\u306a\u3063\u305f\u3089\u3001\u30e2\u30c7\u30eb\u3092\u5909\u3048\u308b\u307f\u305f\u3044\u306a\u611f\u3058\u3067submit\u3057\u307e\u3057\u305f(\u3055\u3089\u306b\u88dc\u6b63\u3042\u308a)\u3002\ntrain_updata.csv\u3092\u30d5\u30eb\u4f7f\u7528\u3057\u305f\u306e\u3067\u3001\u5b9f\u969b\u306e\u30b9\u30b3\u30a2\u306f\u4fe1\u3058\u308b\u306e\u307f\u3067\u3059\u3002submission error\u8d77\u3053\u3057\u3066\u305f\u3089out\u30fb\u30fb\u30fb","c8072988":"I also want to use cumcount (cumulative number of appearances) as a feature.\n\ncumcount\uff08\u7d2f\u7a4d\u767b\u5834\u56de\u6570\uff09\u3082\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u3044\u305f\u3044\u3002","0eb3b034":"## Model E : log learning by position. Pitcher, HD, and others. score 1.3250","c7ee3f65":"I pulled out 0 and 100 for each target, did optuna with log, and created a model with lgbm.\u203b Using GCP\n\n\u5404target\u30670\u3068100\u3092\u629c\u3044\u3066\u3001log\u3067optuna\u3092\u884c\u3044\u3001lgbm\u3067\u30e2\u30c7\u30eb\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002 \u203b GCP\u4f7f\u7528\u3002","35a190f2":"Load models","cae230e0":"Next, add the correct answer of targets to train_feats.\n\n\n\u6b21\u306b\u5927\u672c\u306e\u6b63\u89e3\u3092train_feats\u306b\u8ffd\u8a18\u3057\u307e\u3059\u3002","b7329dcf":"Add dailtyDataDate, playerId","2fa24f62":"Only one person will pull from pred later.\n\n\u4e00\u4eba\u3060\u3051\u5f8c\u3067pred\u304b\u3089\u5f15\u304d\u307e\u3059\u3002","fb4ea8b9":"---------------------------------------------------","b931c74d":"## 5.1.1 Calculate the average value at that time for each playerId (the one used in the riiid competition)\n\nplayerId\u3054\u3068\u306b\u305d\u306e\u6642\u70b9\u3067\u306e\u5e73\u5747\u5024\u3092\u7b97\u51fa\u3059\u308b (riiid\u30b3\u30f3\u30da\u3067\u4f7f\u3063\u305f\u3084\u3064)","089f53d5":"So at the end, it was 1.3019, 12th place before leak.\n\n\u305d\u308c\u3067\u6700\u5f8c\u306f1.3019. leak\u524d\u306712\u4f4d\u3067\u3057\u305f\u3002","e1faafe1":"Eventually, it will be merged with the data one month later, so dailyDataDate2 is the original date. The dailyDataDate is the data one month later.\n\n\n\u6700\u7d42\u7684\u306b\u306f\u30011\u30ab\u6708\u5f8c\u306e\u30c7\u30fc\u30bf\u3068\u30de\u30fc\u30b8\u3059\u308b\u306e\u3067\u3001dailyDataDate2\u306f\u5143\u306e\u65e5\u4ed8\u3002dailyDataDate\u306f1\u30ab\u6708\u5f8c\u306e\u30c7\u30fc\u30bf\u3068\u3057\u307e\u3057\u305f\u3002","83fcb1a6":"From the experimental results, the score increased a little except for numGamesTeam.\n\n\n\u5b9f\u9a13\u7d50\u679c\u304b\u3089\u3001numGamesTeam\u3092\u9664\u304f\u3068\u5c11\u3057\u30b9\u30b3\u30a2\u304c\u4e0a\u304c\u308a\u307e\u3057\u305f\u3002","6b106a52":"The features from the 9th row are used.\n\n\n9\u5217\u76ee\u304b\u3089\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u7528\u3002","8f4e2761":"merge train data. futureLAG will be used in inferece.\n\ntrain\u30c7\u30fc\u30bf\u3068merge\u3057\u307e\u3059\u3002futureLAG\u306finference\u3067\u4f7f\u7528\u3057\u307e\u3059\u3002","3a1e05d1":"## 5.1 feature engineering","6ebcb9b2":"## Model A : maybe I explained in chapter 2\uff5e4 score 1.3490","59a604c6":"\nSince Lag31_level_1 is the row number of train_feats, use it to attach lagall to train_feats.\n\n\nLag31_level_1\u304ctrain_feats\u306erow\u756a\u53f7\u306a\u306e\u3067\u3001\u305d\u308c\u3092\u5229\u7528\u3057\u3066train_feats\u306blagall\u3092\u304f\u3063\u3064\u3051\u307e\u3059\u3002\n","2db67318":"## 5.1.2 Adding statics (median,max) values for LAGS","34e9f881":"Calculate the date when shifting by 31 days (we know the correct answer 31 days ago)\n\n31\u65e5\u5206\u30b7\u30d5\u30c8\u3057\u305f\u3068\u304d\u306e\u65e5\u4ed8\u3092\u8a08\u7b97\uff08\u79c1\u305f\u3061\u306f\u3001\uff13\uff11\u65e5\u524d\u306e\u6b63\u89e3\u3092\u77e5\u3063\u3066\u3044\u307e\u3059)","c79efcd4":"## Model C ( I explained chapter 5 ) score : 1.3373","e22d2435":"Next, when writing a histogram for normal targets 1 to 4, there are many averages near 0, which is not beautiful.\n\n\u6b21\u306b\u3001\u901a\u5e38\u306etarget1\uff5e4\u306f\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u66f8\u304f\u3068\u30010\u4ed8\u8fd1\u306b\u5e73\u5747\u304c\u591a\u304f\u3001\u304d\u308c\u3044\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002","89a96d40":"## 5.2 2nd phase Modeling (I used GCP because the memory is insufficient.)","2bbc664a":"# 1. The basic data used was the one merged by the kaggle staff.\n\u57fa\u672c\u30c7\u30fc\u30bf\u306f\u3001kaggle\u30b9\u30bf\u30c3\u30d5\u3055\u3093\u304cmerge\u3057\u3066\u304f\u308c\u305f\u3082\u306e\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002","886e930e":"# 4. Basically, it's optuna + LightGBM. Score about 1.3490 with 5kfold. \n(I was surprised that it moved up to No. 1 at that time!) Considering the code published now, it is good.\n\n\u57fa\u672c\u7684\u306b\u306f\u3001optuna + LightGBM\u3067\u3059\u3002\n\n\ntargetAvg\u3092\u3082\u3068\u306b\u3001kfold = 0\u306e\u307foptuna\u3002\u305d\u306e\u3042\u3068\u30011\uff5e4\u306e\u500b\u5225\u306e\u30e2\u30c7\u30eb\u4f5c\u6210\u30025kfold\u3067\u30b9\u30b3\u30a2 1.3490 \u7a0b\u5ea6\u3002(\u3053\u308c\u3067\u5f53\u66421\u4f4d\u306b\u6d6e\u4e0a\u3057\u3066\u3073\u3063\u304f\u308a\uff01\uff09\u4eca\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u30b3\u30fc\u30c9\u3092\u8003\u3048\u308b\u3068\u307e\u3041\u307e\u3041\u3067\u3059\u304b\u306d\u3002","019a489f":"## 7.1 Final inference ( But late submission is not active. just to see)","ee1b2f1a":"## 4.1 optuna\nIf optuna did too much, the score was bad. I made it 1000 round.\n\n\noptuna\u306f\u3084\u308a\u3059\u304e\u308b\u3068\u30b9\u30b3\u30a2\u304c\u60aa\u304b\u3063\u305f\u30021000 round\u306b\u3057\u307e\u3057\u305f\u3002","d49b13a8":"# MLB\u63d0\u51fa\u7d42\u308f\u308a\u307e\u3057\u305f\uff01\u3000\n\n* \u307e\u305a\u306f\u3001\u5fdc\u63f4\u3057\u3066\u304f\u308c\u305f\u65b9\u3001EDA\u306eupvote\u9802\u3044\u305f\u65b9\u3001\u9806\u4f4d\u4e89\u3044\u3067\u71b1\u304f\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\u65b9\u3001\u672c\u5f53\u306b\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01 \u3044\u308d\u3044\u308d\u7d4c\u9a13\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u3066\u697d\u3057\u304b\u3063\u305f\u3067\u3059\u3002\u4e0a\u4f4d\u306e\u65b9\u3001\u672c\u5f53\u306b\u3059\u3054\u3044\u3067\u3059\u3002\u5c0a\u656c\u3057\u307e\u3059\u3002\n\n* train_updated.csv\u304c\u51fa\u308b\u76f4\u524d\u306e\u30ea\u30fc\u30af\u7121\u3057\u3067\u300112\u4f4d\u3060\u3063\u305f\uff08\u9014\u4e2d1\u4f4d~12\u4f4d\u3092\u7d4c\u9a13)\u30b3\u30fc\u30c9\u3092\u89e3\u8aac\u8fbc\u307f\u3067share\u3057\u305f\u3082\u306e\u3067\u3059\u3002\n\n* \u3042\u3068\u3001\u3082\u3057\u7d50\u679c\u304c\u60aa\u304b\u3063\u305f\u308a\u3001submission error\u3057\u3066\u3044\u305f\u3089\u3001\u7b11\u3063\u3066\u304f\u3060\u3055\u3044\u3002\u79c1\u3082\u81ea\u8eab\u3092\u7b11\u3044\u307e\u3059\u3002\n\n## **notebook\u516c\u958b\u3057\u307e\u3057\u305f\u304c\u3001\u9577\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u8aad\u307f\u306b\u304f\u304b\u3063\u305f\u3089\u3059\u307f\u307e\u305b\u3093(\u7279\u306binference)**\n\n## \u3067\u3082\u3001\u3054\u53c2\u8003\u306b\u306a\u308c\u3070\u3001**upvote**\u9802\u3051\u305f\u3089\u5b09\u3057\u3044\u3067\u3059!","d64f5bed":"# 2. For the 1st phase, all the data that can be used in 1. were used as features. \n(Maybe the difference from the published notebook is that the character strings like the date of debut in Major League Baseball and so on are also Label encoded and put in all. )\n\n","379c743b":" If submit is the feature amount mentioned in 5 of \u2191, on 9\/15, 31 days ago is about 8\/15 and there is no correct answer, so 31-day shift data cannot be used. Create a model by creating shift data 47 days ago. * Created from the data from the place where there is a correct answer up to 7\/31 47 days before 9\/16 (I put a margin for some reason).\n The first submission uses the 47-day shift data without correction for stable submit. The second submission created 15-day shift data, 31-day shift data, and 47-day shift data, respectively, and submitted it as if changing the model when the date (8\/1,8\/16,9\/1) came. I've used train_updata.csv fully, so I can only believe the actual score without submission error!","bc86dd90":"# 3. kfold\n\nI felt Cross Varidation was unstable if I use only in April 2021, so I divided the whole into 5 by target average and kfolded it.\n\nSince I was doing the CommonLit Readability Prize, I referred to the following. upvote Thank you.\n\n\n","cdb55732":"# 0. Summary\n(If I divide it roughly, there were 3 stages of score up, so I call each one the 1st phase, 2nd phase and 3rd phase.)\n\n1. The basic data used was the one merged by the kaggle staff who output it at https:\/\/www.kaggle.com\/chumajin\/eda-of-mlb-for-starter-version.\nref)  https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-with-mlb-player-digital-engagement (thank you and please upvote their work!)\n\n2. For the 1st phase, all the data that can be used in 1. were used as features. \n(Maybe the difference from the published notebook is that the character strings like the date of debut in Major League Baseball and so on are also Label encoded and put in all. )\n\n3. I felt Cross Varidation was unstable if I use only in April 2021, so I divided the whole into 5 by target average and kfolded it.\n\n4. Basically, it's optuna + LightGBM. Score about 1.3490 with 5kfold. (I was surprised that it moved up to No. 1 at that time!) Considering the code published now, it is good.\n\n5. In the 2nd phase, I know the correct answer (target value) 31 days ago, so I put it in the feature quantity. With this, optuna + Light GBM with 5kfold and 1.3373\n(Actually, in addition to this, the feature amount was added to the ensemble and put out.) I used the GCP because of memory insufficient.\n\n6. In the last 3rd phase, Statistically, targets1 to 4 have a large number of 0s and 100s, respectively, and are likely to be np.clip. Therefore, I found that if the target values 0 and 100 are omitted and the target value between them is logged, a clean histogram will be obtained. Using this, I predicted log10 of target and finally made it 10 to the pred power. optuna and light GBM. Now it's 1.3256 with 5k fold. (Because Light GBM alone has this score, is it pretty good?) \n(Actually, in addition to this, the ones with features added, the ones for each position, the ones without log, etc. were ensembled and put out.)\nEnsemble using various features, about 1.3144.)\n\n7. From there, no hitter (perfect game) is obviously a high number, so I corrected it. Also, Shohei Ohtani did not follow the result of LGBM and corrected it(this may be overfit). So at the end, it was 1.3019, 12th place before leak.\n\n8. If submit is the feature amount mentioned in 5 of \u2191, on 9\/15, 31 days ago is about 8\/15 and there is no correct answer, so 31-day shift data cannot be used. Create a model by creating shift data 47 days ago. * Created from the data from the place where there is a correct answer up to 7\/31 47 days before 9\/16 (I put a margin for some reason).\n The first submission uses the 47-day shift data. The second submission created 15-day shift data, 31-day shift data, and 47-day shift data, respectively, and submitted it as if changing the model when the date (8\/1,8\/16,9\/1) came. I've used train_updata.csv fully, so I can only believe the actual score without submission error!","8fdf89ac":"####  (\u5927\u304d\u304f\u5206\u3051\u308b\u30683\u6bb5\u968e\u306e\u30b9\u30b3\u30a2\u30a2\u30c3\u30d7\u304c\u3042\u3063\u305f\u306e\u3067\u3001\u305d\u308c\u305e\u308c1st phase,2nd phase, 3rd phase\u3068\u304b\u547c\u3093\u3067\u3044\u307e\u3059\u3002)\n\n\n1. \u3000\u57fa\u672c\u30c7\u30fc\u30bf\u306f\u3001https:\/\/www.kaggle.com\/chumajin\/eda-of-mlb-for-starter-version \u3067output\u3057\u305fkaggle\u30b9\u30bf\u30c3\u30d5\u3055\u3093\u304cmerge\u3057\u3066\u304f\u308c\u305f\u3082\u306e\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\n ref) https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-with-mlb-player-digital-engagement (\u826f\u304b\u3063\u305f\u3089\u3001\u5f7c\u3089\u306e\u30b3\u30fc\u30c9\u3092upvote\u3057\u3066\u3042\u3052\u3066\u304f\u3060\u3055\u3044)\n\n2. \u30001st phase\u306f\u30011.\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3048\u308b\u3082\u306e\u306f\u5168\u90e8\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002(\u516c\u958b\u3055\u308c\u3066\u3044\u308bnotebook\u3068\u306f\u3001\u30e1\u30b8\u30e3\u30fc\u30ea\u30fc\u30b0\u306b\u30c7\u30d3\u30e5\u30fc\u3057\u305f\u65e5\u3068\u304b\u305d\u306e\u8fba\u306e\u6587\u5b57\u5217\u3082Label encoding\u3057\u3066\u5168\u90e8\u5165\u308c\u305f\u306e\u304c\u3001\u9055\u3044\u3067\u3059\u3002\u3053\u306e\u8fba\u306e\u6587\u5b57\u5217\u306fEDA\u3057\u3066\u52b9\u679c\u3042\u308a\u305d\u3046\u3060\u306a\u3068\u601d\u3063\u3066\u3044\u307e\u3057\u305f\u3002)\u3042\u3068\u3001\u3053\u306e\u307e\u307e\u3060\u3068test\u30c7\u30fc\u30bf\u3092\u5909\u63db\u3059\u308b\u3068\u304d\u306b\u3001\u3044\u308d\u3044\u308d\u3068\u4e0d\u5177\u5408\u304c\u3042\u3063\u305f\u306e\u3067\u3001original\u3067\u30b3\u30fc\u30c9\u3092\u4fee\u6b63\u3057\u307e\u3057\u305f\u3002\n \n3.  CV\u306f\u30012021\u5e744\u6708\u3060\u3051\u3060\u3068\u4e0d\u5b89\u5b9a\u3060\u3068\u611f\u3058\u3066\u3001\u5168\u4f53\u3092target Average\u30675\u5206\u5272\u3057\u3066kfold\u3057\u307e\u3057\u305f\u3002\n\n4.  \u57fa\u672c\u7684\u306b\u306f\u3001optuna + LightGBM\u3067\u3059\u30025kfold\u3067\u30b9\u30b3\u30a2 1.3490 \u7a0b\u5ea6\u3002(\u3053\u308c\u3067\u5f53\u66421\u4f4d\u306b\u6d6e\u4e0a\u3057\u3066\u3073\u3063\u304f\u308a\uff01\uff09\u4eca\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u30b3\u30fc\u30c9\u3092\u8003\u3048\u308b\u3068\u307e\u3041\u307e\u3041\u3067\u3059\u304b\u306d\u3002\n\n5. 2nd phase\u306f\u300131\u65e5\u524d\u306e\u6b63\u89e3(target\u5024)\u306f\u77e5\u3063\u3066\u3044\u308b\u304b\u3089\u305d\u308c\u3092\u7279\u5fb4\u91cf\u306b\u5165\u308c\u3066\u307f\u307e\u3057\u305f\u3002\u3000\u3053\u308c\u3067optuna + Light GBM\u3067 5kfold\u30671.3373 \n  (\u5b9f\u969b\u306f\u3053\u308c\u306b\u52a0\u3048\u3066\u7279\u5fb4\u91cf\u52a0\u3048\u305f\u3082\u306e\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u51fa\u3057\u3066\u3044\u307e\u3057\u305f\u3002 \u3053\u306e\u8fba\u304b\u3089\u30e1\u30e2\u30ea\u304c\u8db3\u308a\u306a\u304f\u3066\u3001GCP\u4f7f\u3044\u307e\u3057\u305f\u3002)\n\n6.  \u6700\u5f8c\u306e3rd phase\u306f\u3001\u7d71\u8a08\u7684\u306b\u307f\u3066\u3001target1\uff5e4\u306f\u3001\u305d\u308c\u305e\u308c\u30010\u3068100\u306e\u500b\u6570\u304c\u591a\u304f\u3066\u3001np.clip\u3055\u308c\u3066\u3044\u305d\u3046\u3002\u305d\u3053\u3067\u3001target\u5024\u306e0\u3068100\u306e\u3068\u3053\u308d\u306f\u629c\u3044\u3066\u3001\u305d\u306e\u9593\u306etarget\u5024\u306flog\u3092\u53d6\u308b\u3068\u3001\u304d\u308c\u3044\u306a\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306b\u306a\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u3053\u308c\u3092\u5229\u7528\u3057\u3066\u3001target\u306elog10\u3092\u4e88\u60f3\u3057\u3066\u3001\u6700\u5f8c\u300110\u306epred\u4e57\u306b\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u3084\u308a\u307e\u3057\u305f\u3002optuna\u3068light GBM\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u3067\u30015k fold\u30671.3256\u3002(Light GBM\u3060\u3051\u3067\u3053\u306e\u30b9\u30b3\u30a2\u3060\u304b\u3089\u3001\u3051\u3063\u3053\u3046\u826f\u3044\u304b\u3068)\n(\u5b9f\u969b\u306f\u3053\u308c\u306b\u52a0\u3048\u3066\u7279\u5fb4\u91cf\u52a0\u3048\u305f\u3082\u306e\u3084\u3001\u30dd\u30b8\u30b7\u30e7\u30f3\u3054\u3068\u306e\u3082\u306e\u3001log\u3092\u4f7f\u308f\u306a\u304b\u3063\u305f\u3082\u306e\u306a\u3069\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u51fa\u3057\u3066\u3044\u307e\u3057\u305f\u3002)\n\n7.  \u3044\u308d\u3044\u308d\u306a\u7279\u5fb4\u91cf\u4f7f\u7528\u3057\u305f\u3082\u306e\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u30011.3144\u7a0b\u5ea6\u3002\u305d\u3053\u304b\u3089\u3001no hitter(\u5b8c\u5168\u8a66\u5408)\u306f\u660e\u3089\u304b\u306b\u6570\u5b57\u304c\u9ad8\u3044\u306e\u3067\u3001\u88dc\u6b63\u3002\u3042\u3068\u5927\u8c37\u7fd4\u5e73\u9078\u624b\u3082LGBM\u306e\u7d50\u679c\u306b\u8ffd\u5f93\u305b\u305a\u3001\u88dc\u6b63\u3002\n \u305d\u308c\u3067\u6700\u5f8c\u306f\u30011.3019\u3067leak\u524d12\u4f4d\u3002\n   \n8.  submit\u306f\u2191\u306e5\u3067\u8ff0\u3079\u305f\u7279\u5fb4\u91cf\u3060\u3068\u30019\/15\u65e5\u306f\u300131\u65e5\u524d\u304c8\/15\u304f\u3089\u3044\u3067\u6b63\u89e3\u304c\u306a\u3044\u306e\u3067\u300131\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u306f\u4f7f\u7528\u3067\u304d\u305a\u3002\n\u300047\u65e5\u524d\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u3066\u3001\u30e2\u30c7\u30eb\u4f5c\u6210\u3002 \u203b 9\/16\u65e5(\u30de\u30fc\u30b8\u30f3\u3092\u306a\u305c\u304b1\u65e5\u5165\u308c\u3066\u3057\u307e\u3063\u305f\u3002)\u306e47\u65e5\u524d\u306e7\/31\u307e\u3067\u306e\u6b63\u89e3\u304c\u3042\u308b\u3068\u3053\u308d\u304b\u3089\u30c7\u30fc\u30bf\u304b\u3089\u4f5c\u6210\u3002\n1\u3064\u76ee\u306esubmission\u306f\u305d\u306e47\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3002submission2\u306f15\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u300131\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u300147\u65e5\u30b7\u30d5\u30c8\u30c7\u30fc\u30bf\u3092\u305d\u308c\u305e\u308c\u4f5c\u6210\u3057\u3001\u65e5\u4ed8(8\/1,8\/16,9\/1)\u306b\u306a\u3063\u305f\u3089\u3001\u30e2\u30c7\u30eb\u3092\u5909\u3048\u308b\u307f\u305f\u3044\u306a\u611f\u3058\u3067submit\u3057\u307e\u3057\u305f\u3002train_updata.csv\u3092\u30d5\u30eb\u4f7f\u7528\u3057\u305f\u306e\u3067\u3001\u5b9f\u969b\u306e\u30b9\u30b3\u30a2\u306f\u4fe1\u3058\u308b\u306e\u307f\u3067\u3059\u3002submission error\u8d77\u3053\u3057\u3066\u305f\u3089out\u30fb\u30fb\u30fb","7a0fbb29":"Also, Shohei Ohtani(playerId 660271) did not follow the result of LGBM and corrected it by using the difference of LGBM pred and correct on April.\n(It has a possibility of overfit. Other players got worse score.)\n\n\u5927\u8c37\u7fd4\u5e73\u3060\u3051LGBM\u306e\u4e88\u6e2c\u5024\u304c\u6700\u3082\u5408\u308f\u305a\u3001\u5dee\u5206\u3092\u4e88\u6e2c\u5024\u306b\u8db3\u3057\u305f\u3089\u3001\u30b9\u30b3\u30a2\u304c\u826f\u304f\u306a\u3063\u305f\u306e\u3067\u3001\u88dc\u6b63\u3057\u307e\u3057\u305f(\u305f\u3076\u3093\u3001overfit)\u30020.05\u6539\u5584\u3059\u308b\u30ec\u30d9\u30eb\u3002\u4ed6\u306e\u30d7\u30ec\u30a4\u30e4\u30fc\u306f\u3053\u308c\u3092\u3084\u308b\u3068\u9006\u306b\u30b9\u30b3\u30a2\u304c\u60aa\u304f\u306a\u308a\u307e\u3057\u305f\u3002\n\n\n","b7046b7c":"## Model D (I explained in chapter 6) score 1.3256"}}