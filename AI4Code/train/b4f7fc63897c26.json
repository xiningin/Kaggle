{"cell_type":{"c7c7c2cd":"code","4982d739":"code","052c22fd":"code","e2877ebe":"code","39de0352":"code","ea87c20c":"code","d0fd151a":"code","8b5391c8":"code","fb2a0941":"code","af85006c":"code","e93ce17f":"code","ea1ee3b2":"code","fec1b440":"code","7081363b":"code","ccd795e2":"code","5213dbcf":"code","65965d0d":"code","4c220bbf":"code","193a22ee":"code","bc3ea2e8":"code","e3905c87":"code","a08419ad":"code","ab1646ee":"code","57319c23":"code","0cfd0648":"code","e1e32f0c":"code","046fdb2a":"code","27c7b796":"code","7e015936":"code","ed242916":"code","9a48aa62":"code","6fe41f47":"code","0fc686a6":"code","9df83d12":"code","6cc2d1f0":"code","b526724b":"code","78f7fc67":"code","7c86132a":"code","10947cd1":"code","44e693f6":"code","d6e9811a":"code","c56529f5":"code","2b162fea":"code","3c9e0ce3":"code","956ed18a":"code","b9885de1":"code","0abecf4e":"code","3e60ad1f":"code","8321a707":"code","3f62078e":"code","b5ca56ec":"code","d3641c78":"code","fbf50144":"code","840ca42c":"code","78a03252":"code","f7974d45":"code","418477db":"code","1d030dd4":"code","7e00f4be":"code","4afdd386":"code","c9117d8c":"code","5ad2cf82":"code","f2f0f901":"code","3108b52e":"code","f127a9ba":"code","2f027a88":"code","4510e490":"code","8f48f3fc":"code","1c122923":"code","42a2c0f8":"code","5635a2b4":"code","272c6150":"code","2c0b6869":"code","c73efc76":"code","e2d9c40c":"code","76cd1d19":"code","3677be3f":"code","fcea30e3":"code","3f482f0a":"code","4f4c080b":"code","12af6e57":"code","606cbb4f":"code","c7f19de6":"code","21f5e487":"code","1b1e06ef":"code","cdb54dce":"code","c6c40fdf":"code","b4be459f":"code","c01cb812":"code","f0fe3f4a":"code","fa1bd94a":"code","db234477":"code","966a8177":"code","db97d01f":"code","3c0be95a":"code","f085f00d":"code","224d0057":"code","d2c915d7":"code","fac452fa":"code","29b5c987":"code","2b5fc85b":"code","5c5e8970":"code","08c9885e":"code","419a20ac":"code","2e4dc0f3":"code","d44c89de":"code","75512927":"code","4ba63045":"code","d0e1b542":"code","1f18a08e":"code","dca27918":"code","dc9255e2":"markdown","1d61e6e9":"markdown","cad4e920":"markdown","d6f7d2c9":"markdown","7daefc4b":"markdown","798ca68a":"markdown","dc7ce302":"markdown","16073537":"markdown","ab1b8158":"markdown","aa4cddae":"markdown","ee9f0641":"markdown","5d3011b5":"markdown","a4850470":"markdown","5826436c":"markdown","24749348":"markdown","d7ae71a7":"markdown","7a570290":"markdown","c1837c56":"markdown","9a647f12":"markdown","ebb23953":"markdown","d654fb0b":"markdown","e7af5fc1":"markdown","86c87117":"markdown","668cc4a2":"markdown","3668bb60":"markdown","d8aef480":"markdown","fa2e5ccc":"markdown","8c02cef7":"markdown","6cfee7e0":"markdown","db2b5e12":"markdown","49f16cb2":"markdown","c9ac8b60":"markdown","1dc10b55":"markdown","c484a6bf":"markdown","b7d6a1d4":"markdown","54aeaae6":"markdown","b2c660ad":"markdown","eeb2b3bb":"markdown","0423ac49":"markdown","90ca6d99":"markdown","836ef7d6":"markdown","388c0953":"markdown","2be4e0ca":"markdown","ac049dac":"markdown","0bbae2d5":"markdown","e06c0c86":"markdown","5eda513b":"markdown"},"source":{"c7c7c2cd":"# surpass warnings\nimport warnings\nwarnings.filterwarnings('ignore')","4982d739":"# import all libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","052c22fd":"# Importing data set\nhouse = pd.read_csv('..\/input\/house-price-prediction\/House_Price.csv')\nhouse.head()","e2877ebe":"# Check the columns\nhouse.columns","39de0352":"# Checking the information of the dataset\nhouse.info()","ea87c20c":"#Check the descriptive statistics\nhouse.describe().T","d0fd151a":"# Check the shape of the dataset\nhouse.shape","8b5391c8":"# Drop unwanted columns\nhouse = house.drop(['Id'], axis=1)\nhouse.shape","fb2a0941":"# check for duplicate rows\nhouse.duplicated().sum()","af85006c":"# Missing Value Count Function\ndef show_missing():\n    missing = house.columns[house.isnull().any()].tolist()\n    return missing\n\n# Missing data counts and percentage\nprint('Missing Data Count')\nprint(house[show_missing()].isnull().sum().sort_values(ascending = False))\nprint('--'*40)\nprint('Missing Data Percentage')\nprint(round(house[show_missing()].isnull().sum().sort_values(ascending = False)\/len(house)*100,2))","e93ce17f":"# Features with over 90% of its observations missings will be removed\nhouse = house.drop(['PoolQC','MiscFeature','Alley'],axis = 1)\nhouse.shape","ea1ee3b2":"# data distribution in MasVnrType column\nhouse['MasVnrType'].astype('category').value_counts()","fec1b440":"# data distribution in BsmtQual column\nhouse['BsmtQual'].astype('category').value_counts()","7081363b":"# data distribution in BsmtCond column\nhouse['BsmtCond'].astype('category').value_counts()","ccd795e2":"# data distribution in BsmtExposure column\nhouse['BsmtExposure'].astype('category').value_counts()","5213dbcf":"# data distribution in BsmtFinType1 column\nhouse['BsmtFinType1'].astype('category').value_counts()","65965d0d":"# data distribution in BsmtFinType2 column\nhouse['BsmtFinType2'].astype('category').value_counts()","4c220bbf":"# data distribution in FireplaceQu column\nhouse['FireplaceQu'].astype('category').value_counts()","193a22ee":"# data distribution in GarageType column\nhouse['GarageType'].astype('category').value_counts()","bc3ea2e8":"# data distribution in GarageFinish column\nhouse['GarageFinish'].astype('category').value_counts()","e3905c87":"# data distribution in GarageQual column\nhouse['GarageQual'].astype('category').value_counts()","a08419ad":"# data distribution in GarageCond column\nhouse['GarageCond'].astype('category').value_counts()","ab1646ee":"# data distribution in Fence column\nhouse['Fence'].astype('category').value_counts()","57319c23":"# data distribution in Electrical column\nhouse['Electrical'].astype('category').value_counts()","0cfd0648":"# impute the miningful missing values\nNG = ['GarageCond', 'GarageQual', 'GarageType', 'GarageFinish','GarageType',]\nNB = ['BsmtFinType1', 'BsmtFinType2','BsmtExposure', 'BsmtCond', 'BsmtQual']\nhouse[NG] = house[NG].replace(np.nan, \"NG\")\nhouse[NB] = house[NB].replace(np.nan, \"NB\")\n\n# imputing 'LotFrontage' with median\nhouse[['LotFrontage']] = house[['LotFrontage']].fillna(house['LotFrontage'].median())\nhouse[['Electrical']] = house[['Electrical']].fillna(house['Electrical'].mode()[0])\nhouse[['MasVnrType']] = house[['MasVnrType']].fillna(house['MasVnrType'].mode()[0])\n\n# impute mising GarageYrBlt to current year 2020\nhouse['GarageYrBlt'] = house['GarageYrBlt'].fillna(2020)\nhouse['MasVnrArea'] = house['MasVnrArea'].fillna(0)\n\n# handing missing values for 'Fence','FireplaceQu'\nhouse[['Fence']] = house[['Fence']].replace(np.nan, \"No_Fence\")\nhouse[['FireplaceQu']] = house[['FireplaceQu']].replace(np.nan, \"No\")\n","e1e32f0c":"## Let's check percentage of null values again after imputation\n# You won't see any data now as all the columns are imputed and there is no column with null values\nprint(round(100*(house.loc[:, house.isnull().any()].isnull().sum())\/len(house.index), 2))","046fdb2a":"# Checking shape\nhouse.shape","27c7b796":"# Let's drop duplicate columns if any present in data set \nhouse = house.drop_duplicates()\nhouse.shape","7e015936":"# Deriving prperty age from YearBuit\nCurrent_Year = 2020\n\nhouse['Property_Age'] = Current_Year - house['YearBuilt'] \n\n# Derived Modification Age\nhouse['RemodAdd_Age'] = house['YearRemodAdd'] - house['YearBuilt']\nhouse['RemodAdd_Age'] = house['Property_Age'] - house['RemodAdd_Age']\n\n# Derived GarageAge\nhouse['GarageBlt_Age'] = Current_Year - house['GarageYrBlt']\n\n# Build to sold age\nhouse['Build_Sold_age'] = house['YrSold'] - house['YearBuilt']\n\n\n# drop the year columns\nhouse.drop('YearBuilt', axis =1, inplace = True)  # drop YearBuilt\nhouse.drop('YearRemodAdd', axis =1, inplace = True)  # drop YearRemodAdd\nhouse.drop('GarageYrBlt', axis =1, inplace = True)  # drop GarageYrBlt\nhouse.drop('YrSold', axis =1, inplace = True)  # drop YrSold\nhouse.drop('MoSold', axis =1, inplace = True)  # drop MoSold\n","ed242916":"# Derive Categorical features\n\nhouse['haspool'] = house['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse['has2ndfloor'] = house['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse['hasgarage'] = house['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse['hasbsmt'] = house['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)","9a48aa62":"# Total floor space finished and unfinished\nhouse['TotalSF'] = (house['TotalBsmtSF'] \n                       + house['1stFlrSF'] \n                       + house['2ndFlrSF'])\n\n# Total Finished floor space\nhouse['Total_finished_SF'] = (house['BsmtFinSF1'] \n                                 + house['BsmtFinSF2'] \n                                 + house['1stFlrSF'] \n                                 + house['2ndFlrSF']\n                                )\n\n# drop redundant\nhouse.drop(['TotalBsmtSF','1stFlrSF','2ndFlrSF','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'], axis =1, inplace = True)","6fe41f47":"# porch\nhouse['PorchType'] = np.select([house['OpenPorchSF'] > 0 , house['3SsnPorch'] > 0, house['EnclosedPorch'] > 0,\n                               house['ScreenPorch'] > 0, house['WoodDeckSF'] > 0 ],\n                               [\"OpenPorchSF\", \"3SsnPorch\", \"EnclosedPorch\", \"ScreenPorch\", \"WoodDeckSF\"], default= \"No Porch\")\n\n\nhouse[\"PorchType\"].value_counts()","0fc686a6":"# Total Porch Area\nhouse['Total_Porch_SF'] = (house['OpenPorchSF'] \n                              + house['3SsnPorch'] \n                              + house['EnclosedPorch'] \n                              + house['ScreenPorch'] \n                              + house['WoodDeckSF']\n                             )\n# derive new feature as porch type\nhouse['PorchType'] = np.select([house['OpenPorchSF'] > 0 , house['3SsnPorch'] > 0, house['EnclosedPorch'] > 0,\n                               house['ScreenPorch'] > 0, house['WoodDeckSF'] > 0 ],\n                               [\"OpenPorchSF\", \"3SsnPorch\", \"EnclosedPorch\", \"ScreenPorch\", \"WoodDeckSF\"], default= \"No Porch\")\n\n\nprint(house[\"PorchType\"].value_counts())\n\n# drop redundant\nhouse.drop(['OpenPorchSF','3SsnPorch','EnclosedPorch','ScreenPorch','WoodDeckSF'], axis =1, inplace = True)","9df83d12":"# Bathroom types\nhouse[['FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']].info()","6cc2d1f0":"# Total Bathrroms\nhouse['Total_Bathrooms'] = (house['FullBath'] \n                               + (0.5 * house['HalfBath']) \n                               + house['BsmtFullBath'] \n                               + (0.5 * house['BsmtHalfBath'])\n                              )                      \n\n# drop redundant\nhouse.drop(['FullBath','HalfBath','BsmtFullBath','BsmtHalfBath'], axis =1, inplace = True)","b526724b":"# Drop unwanted columns\nhouse.drop(['Build_Sold_age'], axis=1, inplace = True)","78f7fc67":"# Checking the data type\nhouse.select_dtypes(['int64','float64']).columns","7c86132a":"# checking Categorical data \ncols = ['MSSubClass','OverallQual','OverallCond', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces','GarageCars',\n       'haspool','has2ndfloor', 'hasgarage', 'hasbsmt',\"Total_Bathrooms\"]\n# converting data types to category\nhouse[cols] = house[cols].astype('category')","10947cd1":"# Select numerical and categorical data types\nnum_var = house.select_dtypes(['int64','float64']).columns\ncat_var = house.select_dtypes(exclude=['int64','float64']).columns","44e693f6":"# look for basic statistics of numerical variable\nhouse[num_var].describe().T","d6e9811a":"# drop the variables having no variance\nhouse.drop(['LowQualFinSF', 'PoolArea','MiscVal'], axis =1, inplace = True)","c56529f5":"# Distribution plot for numerical varibles\nnum_var = house.select_dtypes([\"int64\",\"float64\"]).columns\ncol = 4\nrow = len(num_var)\/\/col+1\nplt.figure(figsize=(20,18))\nfor i in enumerate(num_var):\n    plt.subplot(row, col, i[0]+1)\n    sns.distplot(house[i[1]])\n    plt.tight_layout(pad = 1)\nplt.show()","2b162fea":"# Functions to FaceGrid plots for numericl and categorical variable\n# Visualising with box plot\ndef facetgrid_scaterplot(x, y, **kwargs):\n    sns.scatterplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \n# Visualising with scaterplot\ndef facetgrid_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)","3c9e0ce3":"# Visualising Numerical predictor variables with Target Variables  \nf = pd.melt(house, id_vars=['SalePrice'], value_vars=house[num_var].drop('SalePrice', axis=1))\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=4, sharex=False, sharey=False, size=5)\ng = g.map(facetgrid_scaterplot, \"value\", \"SalePrice\")","956ed18a":"# skew of all numerical varibles\nhouse[num_var].skew().sort_values(ascending = False)","b9885de1":"# kurt of all numerical varibles\nhouse[num_var].kurt().sort_values(ascending = False)","0abecf4e":"# Statistical Summary\nprint(\"SalePrice Statistical Summary:\\n\")\nprint(house['SalePrice'].describe())\nprint(\"Median Sale Price:\", house['SalePrice'].median(axis = 0))\nprint('Skewness:',house['SalePrice'].skew())\nskew = house['SalePrice'].skew()\n\n# mean distribution\nmu = house['SalePrice'].mean()\n\n# std distribution\nsigma = house['SalePrice'].std()\nnum_bins = 40\n\n# Plotting 'SalePrice' before and after the transformation\nplt.figure(figsize=(15,5))\n\nax = plt.subplot(1, 2, 1)\nsns.distplot(house['SalePrice'])\nplt.title('Before Transformation', size=15)\nplt.show()\n\n# Checking skewness & Kurtosis\nprint('skewness %f'%house['SalePrice'].skew())\nprint('Kurtosis %f'%house['SalePrice'].kurt())\n","3e60ad1f":"# Transforming 'SalePrice' into normal distribution\nimport scipy.stats as st\n\n\nplt.figure(figsize=(25,5))\n\nax1 = plt.subplot(1, 3, 1)\nsns.distplot(house['SalePrice'], kde=False, fit=st.norm)\nplt.title('Normal', size = 15)\n\nax2 = plt.subplot(1, 3, 2)\nsns.distplot(house['SalePrice'], kde=False, fit=st.lognorm)\nplt.title('Log Normal', size = 15)\n\nax3 = plt.subplot(1, 3, 3)\nsns.distplot(house['SalePrice'], kde=False, fit=st.johnsonsu)\nplt.title('Johnson SU', size = 15)\n\nplt.show()","8321a707":"# Transformation of data using PowerTransformer\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nhouse[num_var] = pd.DataFrame(pt.fit_transform(house[num_var]))\nhouse[num_var].skew().sort_values(ascending=False)","3f62078e":"# basic statistics of numerical variable\nhouse[cat_var].describe()","b5ca56ec":"# Percentage of most frequent value with respect to total counts\nmost_cat = house[cat_var].describe().loc['freq']\/house[cat_var].describe().loc['count']*100\n# most_cat = most_cat.round()\nmost_cat = most_cat.sort_values(ascending = False)\nmost_cat","d3641c78":"# categorical variables having high skew\nmost_cat_80 = most_cat[most_cat>80].index\nmost_cat_80","fbf50144":"# drop categorical variable those are highly skewed\nhouse.drop(most_cat_80, axis=1, inplace = True)\n\n# update cat_var dataframe\ncat_var = house.select_dtypes(exclude=['int64','float64']).columns","840ca42c":"# Visualising Categorical predictor variables with Target Variables\nf = pd.melt(house, id_vars=['SalePrice'], value_vars=sorted(house[cat_var]))\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, size=5)\ng = g.map(facetgrid_boxplot, \"value\", \"SalePrice\")","78a03252":"# Heat Map\nplt.figure(figsize= (12,10))\nsns.heatmap(house.corr(),annot=True,cmap='YlOrRd', linewidths=0.2,annot_kws={'size':10})\nplt.show()","f7974d45":"# Categorical variables statistics\nhouse[cat_var].describe()","418477db":"# ExterQual = Evaluates the quality of the material on the exterior: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor)\nhouse[\"ExterQual\"] = house['ExterQual'].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n\n# BsmtQual = Evaluates the height of the basement: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor), NA(No Basement)\nhouse[\"BsmtQual\"] = house['BsmtQual'].map({'NB':1, 'Po':2,'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n# BsmtExposure = Refers to walkout or garden level walls: Gd(Good), Av(Average), Mn(Minimum), No(No Exposure), NA(No Basement)\nhouse[\"BsmtExposure\"] = house['BsmtExposure'].map({'NB':1, 'No':2, 'Mn':3, 'Av':3, 'Gd':4})\n\n# # BsmtCond = Evaluates the general condition of the basement: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor), NA(No Basement)\n# house[\"BsmtCond\"] = house['BsmtCond'].map({'None':1, 'Po':2, 'Fa':3, 'TA':3, 'Gd':4})\n\n# HeatingQC = Heating quality and condition: Ex(Excellent), Gd(Good), TA(Average), Fa(Fair), Po(Poor)\nhouse[\"HeatingQC\"] = house['HeatingQC'].map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n# KitchenQual: Kitchen quality: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor)\nhouse[\"KitchenQual\"] = house['KitchenQual'].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n\n# FireplaceQu: Fireplace quality: Ex(Excellent), Gd(Good), TA(Average), Fa(Fair), Po(Poor), NA(No Fireplace)\nhouse[\"FireplaceQu\"] = house['FireplaceQu'].map({'No':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n# GarageFinish: Interior finish of the garage: Fin(Finished), RFn(Rough Finished), Unf(Unfinished), NA(No Garage)\nhouse[\"GarageFinish\"] = house['GarageFinish'].map({'NG':1, 'Unf':2, 'RFn':3, 'Fin':4})","1d030dd4":"# Categorical ordinal variables\ncat_ord = ['ExterQual','BsmtQual','BsmtExposure','HeatingQC','KitchenQual','FireplaceQu','GarageFinish','OverallQual',\n           'OverallCond','Total_Bathrooms']\ndummy_list = cat_var.drop(cat_ord)","7e00f4be":"house.head()","4afdd386":"# convert into dummies\ncat_dummies = pd.get_dummies(house[dummy_list], drop_first=True)\nhouse = pd.concat([house,cat_dummies],axis=1)\n\nhouse.drop(dummy_list, axis=1, inplace =True)\nhouse.shape","c9117d8c":"# Dropping Salesprice from numerical variables\nnum_var = num_var.drop(\"SalePrice\")","5ad2cf82":"from sklearn.model_selection import train_test_split\n\n# test train split\ny = house[\"SalePrice\"]\nX= house.drop('SalePrice', axis = 1)\n\n#train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=40)","f2f0f901":"# Checking shape\nX_train.shape, X_test.shape","3108b52e":"#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()","f127a9ba":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","2f027a88":"# scale test train data\n\nX_train[num_var] = scaler.fit_transform(X_train[num_var])\nX_test[num_var] = scaler.transform(X_test[num_var])\n\n\nX_train[cat_ord] = scaler.fit_transform(X_train[cat_ord])\nX_test[cat_ord] = scaler.transform(X_test[cat_ord])","4510e490":"# Checking shape of the dataset\nX_train.shape, X_test.shape","8f48f3fc":"# Train dataset check\nX_train.head()","1c122923":"#Test dataset check\nX_test.head()","42a2c0f8":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","5635a2b4":"# Model Selection based on RFE\n# linear regression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# rfe\nrfe = RFE(lm,25)\nrfe.fit(X_train,y_train)","272c6150":"rfe_scores = pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))\nrfe_scores.columns = ['Column_Names','Status','Rank']\n\nrfe_cols = list(rfe_scores[rfe_scores.Status==True].Column_Names)\nprint(rfe_cols)","2c0b6869":"# Select variables based on RFE\nX_train_rfe = X_train[rfe_cols]\nX_test_rfe = X_test[rfe_cols]","c73efc76":"# model coefficients\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","e2d9c40c":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","76cd1d19":"\n## list of alphas to tune\nparams = {'alpha': [1e-8,1e-7, 1e-06, 1e-05, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,1.0, 2.0, 3.0,\n                    0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]}\n\n\nlasso = Lasso()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train_rfe, y_train)","3677be3f":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","fcea30e3":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","3f482f0a":"#checking the value of optimum number of parameters\nalpha_lasso =  model_cv.best_params_[\"alpha\"]\n\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","4f4c080b":"# Lasso model parameters\nlasso = Lasso(alpha=  model_cv.best_params_[\"alpha\"])\nlasso.fit(X_train_rfe, y_train)","12af6e57":"# Lasso coefficient\nlasso.coef_","606cbb4f":"# Train & test prediction \ny_train_pred = lasso.predict(X_train_rfe)\nr2_score_train= metrics.r2_score(y_true=y_train, y_pred=y_train_pred)\n\ny_test_pred = lasso.predict(X_test_rfe)\nr2_score_test = metrics.r2_score(y_true=y_test, y_pred=y_test_pred)","c7f19de6":"# Evaluation Metrices\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nprint ('------- Train-------')\nprint(\"R2 Train:\", round(r2_score_train, 3))\nprint ('MAE:', mean_absolute_error(y_train, y_train_pred))\nprint ('MSE is:', mean_squared_error(y_train, y_train_pred))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n\n\nprint (' ')\n\nprint ('------- Test-------')\nprint(\"R2 Test:\", round(r2_score_test, 3))\nprint ('MAE:', mean_absolute_error(y_test, y_test_pred))\nprint ('MSE is:', mean_squared_error(y_test, y_test_pred))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","21f5e487":"# function to calculate adjusted R2\ndef adj_r2_score(X, y, y_pred):\n    n = len(y)\n    p = len(X.columns)\n    R2= round(metrics.r2_score(y_true=y, y_pred=y_pred),3)\n\n    adj_r2 = round(1-(1-R2)*(n-1)\/(n-p-1),3)\n    return R2, adj_r2","1b1e06ef":"# EValuation of metrices\nprint(\"Train: (R2 , Adjusted R2):\", adj_r2_score(X_train_rfe, y_train, y_train_pred))\nprint(\"Test: (R2 , Adjusted R2):\", adj_r2_score(X_test_rfe, y_test, y_test_pred))","cdb54dce":"# Model paramenetrs\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","c6c40fdf":"# Parameters with coefficient\nlasso_params = pd.Series(lasso.coef_, X_train_rfe.columns)\nlasso_params","b4be459f":"# lasso parmas having zero coeffiecient\nlasso_params[abs(lasso_params)<0.00001]","c01cb812":"# drop insignificant Features for lasso with threshold of 0.05\ndrop_vars = lasso_params[abs(lasso_params)<0.05].index\nX_train_lasso = X_train_rfe.drop(drop_vars, axis =1 )\nX_test_lasso = X_test_rfe.drop(drop_vars, axis =1 )","f0fe3f4a":"# update loasso model\nlasso = Lasso(alpha=  model_cv.best_params_[\"alpha\"])\nlasso.fit(X_train_lasso, y_train)\n\ny_train_pred = lasso.predict(X_train_lasso)\nr2_score_train= metrics.r2_score(y_true=y_train, y_pred=y_train_pred)\n\ny_test_pred = lasso.predict(X_test_lasso)\nr2_score_test = metrics.r2_score(y_true=y_test, y_pred=y_test_pred)","fa1bd94a":"# Evaluation Metrices\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nprint ('------- Train-------')\nprint(\"R2 Train:\", round(r2_score_train, 3))\nprint ('MAE:', mean_absolute_error(y_train, y_train_pred))\nprint ('MSE is:', mean_squared_error(y_train, y_train_pred))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n\n\nprint (' ')\n\nprint ('------- Test-------')\nprint(\"R2 Test:\", round(r2_score_test, 3))\nprint ('MAE:', mean_absolute_error(y_test, y_test_pred))\nprint ('MSE is:', mean_squared_error(y_test, y_test_pred))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","db234477":"# EValuation of metrices\nprint(\"Train: (R2 , Adjusted R2):\", adj_r2_score(X_train_rfe, y_train, y_train_pred))\nprint(\"Test: (R2 , Adjusted R2):\", adj_r2_score(X_test_rfe, y_test, y_test_pred))","966a8177":"# Lasso Parameters\nlasso_params = pd.Series(lasso.coef_, X_train_lasso.columns)\nlasso_params","db97d01f":"#Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\nlasso_feature_importance = lasso_params\nlasso_feature_importance = 100.0 * (lasso_feature_importance \/ lasso_feature_importance.max())\nlasso_feature_importance.sort_values(ascending = False)","3c0be95a":"# Feature Importance\nlasso_feature_importance = lasso_feature_importance.sort_values()\nplt.figure(figsize=(8,10))\nlasso_feature_importance.plot.barh(align='center', color = 'tab:red',alpha=0.8, fontsize = 12)\nplt.title('Relative Feature Importance', fontsize=14)\nplt.show()","f085f00d":"# To tap 10 features\nabs_lasso = abs(lasso_params)\nfeatures_lasso = pd.DataFrame({\"Coefficients\": lasso_params, \"Absolute_Coeffients\": abs_lasso})\nfeatures_lasso_top10 = features_lasso.sort_values(by = 'Absolute_Coeffients', ascending = False ).head(10)\nfeatures_lasso_top10","224d0057":"## list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0,  4.0, 5.0, \n                    6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train_rfe, y_train)","d2c915d7":"# results data frame\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=200]\ncv_results.head()","fac452fa":"# plotting mean test and train scoes with alpha to tune hyper parameter\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('r2 score')\nplt.title(\"r2 score and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","29b5c987":"#checking the value of optimum number of parameters\nalpha_ridge =  model_cv.best_params_[\"alpha\"]\n\nprint(model_cv.best_params_)\nprint(model_cv.best_score_)","2b5fc85b":"# Ridge model fitting\nridge = Ridge(alpha=  model_cv.best_params_[\"alpha\"])\nridge.fit(X_train_rfe, y_train)","5c5e8970":"# Coefficient\nridge.coef_","08c9885e":"# Test & Train prediction\ny_train_pred = ridge.predict(X_train_rfe)\nr2_score_train= metrics.r2_score(y_true=y_train, y_pred=y_train_pred)\n\ny_test_pred = ridge.predict(X_test_rfe)\nr2_score_test = metrics.r2_score(y_true=y_test, y_pred=y_test_pred)","419a20ac":"# Evaluation Metrices\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nprint ('------- Train-------')\nprint(\"R2 Train:\", round(r2_score_train, 3))\nprint ('MAE:', mean_absolute_error(y_train, y_train_pred))\nprint ('MSE is:', mean_squared_error(y_train, y_train_pred))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n\n\nprint (' ')\n\nprint ('------- Test-------')\nprint(\"R2 Test:\", round(r2_score_test, 3))\nprint ('MAE:', mean_absolute_error(y_test, y_test_pred))\nprint ('MSE is:', mean_squared_error(y_test, y_test_pred))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","2e4dc0f3":"# EValuation of metrices\nprint(\"Train: (R2 , Adjusted R2):\", adj_r2_score(X_train_rfe, y_train, y_train_pred))\nprint(\"Test: (R2 , Adjusted R2):\", adj_r2_score(X_test_rfe, y_test, y_test_pred))","d44c89de":"# Parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","75512927":"#Predictor Variables from the Model built using Lasso Regression:\nprint(list(zip(cols, model_parameters)))","4ba63045":"# Parameters with coefficients\nridge_params = pd.Series(ridge.coef_, X_train_rfe.columns)\nridge_params","d0e1b542":"#Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\nridge_feature_importance = ridge_params\nridge_feature_importance = 100.0 * (ridge_feature_importance \/ ridge_feature_importance.max())\nridge_feature_importance.sort_values(ascending = False)","1f18a08e":"# Feature Importance\nridge_feature_importance = ridge_feature_importance.sort_values()\nplt.figure(figsize=(8,10))\nridge_feature_importance.plot.barh(align='center', color = 'tab:red',alpha=0.8, fontsize = 12)\nplt.title('Relative Feature Importance', fontsize=14)\nplt.show()","dca27918":"# To tap 10 features\nabs_ridge = abs(ridge_params)\nfeatures_ridge = pd.DataFrame({\"Coefficients\": ridge_params, \"Absolute_Coeffients\": abs_ridge})\nfeatures_ridge_top10 = features_ridge.sort_values(by = 'Absolute_Coeffients', ascending = False ).head(10)\nfeatures_ridge_top10","dc9255e2":"#### 9.1 Lasso Regression","1d61e6e9":"**Lets filter the train and test set for the RFE selected columns**","cad4e920":"**2.2 Data Preparation**","d6f7d2c9":"Normal distribution doesn't fit, so SalePrice need to be transformed before creating the model. Best fit is unbounded Johnson distribution, altough log normal distribution also fits well","7daefc4b":"### Step:-7 Feature Scaling:","798ca68a":"**4.2 Target Variable**","dc7ce302":"We have from Lasson Rigression with alpha = 0.001 and from Ridge Regression with alpha = 0.6. These are the optimal values of lambda or the hyerparameters for regularization of the regression model.\n\n**1. Lasso:** Optimal value of alpha of lasso regression is 0.001 with 24 Variables and evaluation metrics for optimal value of alpha is given below\n            - R2 Score    Train  : 0.894    Test  : 0.891\n            - Adj.R2      Train  : 0.892    Test  : 0.881\n            - MAE         Train  : 0.22     Test  : 0.232\n            - MSE         Train  : 0.12     Test  : 0.099\n            - RMSE        Train  : 0.33     Test  : 0.316\n           \n\n**2. Ridge:** Optimal value of alpha of ridge regression is 0.6 with 25 Variables and evaluation metrics for optimal value of alpha is given below\n            - R2 Score   Train  : 0.898      Test  : 0.889\n            - Adj. R2    Train  : 0.896      Test  : 0.879\n            - MAE        Train  : 0.218      Test  : 0.238\n            - MSE        Train  : 0.104      Test  : 0.1014\n            - RMSE       Train  : 0.323      Test  : 0.323\n                  \n    \n\n**3. Important Features:** Based on both the Ridge and Lasso Models, the key top 10 predictor variables for impacting the Sale Price of Houses in Australia are as follows:\n\n - **Lasso Model Top 10 Predictor Variables**:     `'GrLivArea', 'Total_finished_SF', 'OverallQual', 'LotArea', 'Property_Age', 'OverallCond', 'MSZoning_FV', 'MSZoning_RL','Neighborhood_StoneBr', 'Neighborhood_NoRidge'`\n                     \n - **Ridge Model Top 10 Predictor Variables**:    `'GrLivArea', 'Total_finished_SF', 'OverallQual', 'LotArea','Property_Age', 'MSZoning_FV', 'OverallCond', 'MSZoning_RL', 'MSZoning_RH', 'MSZoning_RM'`\n\n\n\n**Based on above analysis, both the models prediction capability is more than  88%. Although the predictibility of both models are almost similar however Lasso model is relatively similer with 24 number of predictors in comparision to 25 of Ridge model. So it is recomended to select Lasso model.**","16073537":"Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n\n###### Loss function = OLS + alpha * summation (squared coefficient values)\n\nIn the above loss function, alpha is the parameter we need to select. A low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.","ab1b8158":"Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).\n\nThe loss function for Lasso Regression can be expressed as below:\n\n###### *Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)*\n\nIn the above loss function, alpha is the penalty parameter we need to select. Using an l1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values.","aa4cddae":"**4.1 Numerical Variables**","ee9f0641":"**2.1 Handling Missing Values:**\n\nLet's now conduct some data cleaning steps.  We've seen that there are some missing values in the dataset. ","5d3011b5":"There isn't much Kurtosis in the data columns, but Skewness is very present, meaning that distribution is not symetrical.","a4850470":"**Ridge Conclusion:**\n\n1. From the R square value we see that the model:\n    - Built using Ridge regression can have morethan 88% accuracy in its prediction capability.\n    - R-squared (R2) explains the proportion of variation in the outcome that is explained by the predictor variables. \n\n\n2. With optimal value of alpha = 0.6 for ridge regression, we got following score for train and test data set.\n\n     **------- Train-------**\n    - R2 Train: 0.898\n    - MAE: 0.21836680029445973\n    - MSE is: 0.10407516587965289\n    - RMSE 0.3226068286314673\n \n    **------- Test-------**\n    - R2 Test: 0.889\n    - MAE: 0.23792667695969663\n    - MSE is: 0.10149098890998756\n    - RMSE 0.31857650401432236\n   \n\n3. Top 10 features\n   \n      `'GrLivArea', 'Total_finished_SF', 'OverallQual', 'LotArea','Property_Age', 'MSZoning_FV', 'OverallCond', 'MSZoning_RL', 'MSZoning_RH', 'MSZoning_RM'`\n    \n*This score looks good and we can conclude that model is not overfitting.*","5826436c":"### Step-2: Data Inspection","24749348":"### Step-6: Test Train Split:","d7ae71a7":"Insight:\n\n- `'MSSubClass_40'` Features are having Zero coefficient. So we will drop these feature and update lasso model with optimum alpha.\n","7a570290":"**2.3 Check Duplicates**","c1837c56":"Insights:\nfor distribution plot and scater plot we could find that\n- Follwing features are right skewed and it shows presence of outlier. \n    `'LotFrontage', 'LotArea', 'BsmtFinSF1', 'BsmtFinSF2','2ndFlrSF', 'LowQualFinSF'`","9a647f12":"Insight:\n\n- following categocal varibales represent one data having more than aprox 90% of time. So these varibales are going to contribute to our model.\n\n `'Utilities', 'Street', 'haspool', 'Condition2', 'RoofMatl', 'Heating','hasbsmt', 'KitchenAbvGr', 'LandSlope', 'hasgarage', 'CentralAir','Functional', 'PavedDrive', 'Electrical', 'GarageCond', 'GarageQual','BsmtCond', 'LandContour', 'ExterCond', 'SaleType', 'Condition1','BsmtFinType2', 'BldgType', 'SaleCondition', 'Fence'`","ebb23953":"### Step-1: Import and inspect data frame","d654fb0b":"Insights:\n- Lot Frontage:- \n    - LotFrontage is missing 259 observations.  First, I will check to see if there are other variables that are strongly correlated with LotFrontage I can use for imputation.  Otherwise, I will impute with the median LotFrontage value.  \n- Garage Features:-  \n    - GarageYrBlt, GarageType, GarageFinish,GarageQual, and GarageCond are all missing 81 observations. These null values are assumed to be in the same rows for each column and associated with homes that do not have garages at all. If these assumptions are correct,  the nulls can be inputed with zero as these are properties without garages. \n- Basement Features\n    - BsmtFinType2 and 'BsmtExposure are both missing 38 observations.  It is suspected that these observations are in the same rows for both columns and associated with homes that do not have basements.  If these assumptions are true, we can impute the nulls with zero as we have for other missing values previously.\n    - Most of the nulls are homes without basements; There is another set of basement features that are missing the same number of observations. BsmtFinType1, BsmtCond, BsmtQual all have 37 missing values.  It is assumed that these observations are in the same rows and correspond to homes that do not have basements.  If these assumptions are true, we will impute nulls with 'None', otherwise impute using the most frequent value for each feature.  \n- Masonry Features:- \n    - MasVnrArea and MasVnrType are both missing 8 observations.  Again, we'll check to see they are missing in the same rows and then impute with the most frequent value for each column.  \n- Electrical:- \n    - Electrical is only missing one observation, which can be imputed with the most frequent value in the column.  ","e7af5fc1":"#### Skewness and Kurtosis\n\n**Skewness** is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined. For a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. \n\n**Kurtosis** is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution and there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Different measures of kurtosis may have different interpretations.","86c87117":"Missing in Random; \n- `LotFrontage`: We will impute with median.\n- `'Electrical'`: We will impute with mode\n- `MasVnrType: We will impute these features with mode\n\nFollwing features have missingful missing value:\n- `GarageCond, GarageQual, GarageType, GarageFinish`: Missing due to no Garage\n- `BsmtFinType1, BsmtFinType2,BsmtExposure, BsmtCond, BsmtQual` : Missing due to no basement\n- `'GarageYrBlt'` : If garage built year is missing we will impute it to current year 2020\n-  `'MasVnrArea'` : As we are imputing MasVnrType to mode which is None we shall impute this feature to Zero.","668cc4a2":"Insights:\n- From the above plot we have seen that optimal value for alpha is 0.6. So let's use this value of alpha for regularization.","3668bb60":"Insights:\n- We shall use `Iterative imputer` to handle rest of the missing values in later stage","d8aef480":"### Step-9: Model Building:","fa2e5ccc":"Insights:\n- Correlected variables with salesprice is -'TotalSF','Total_finished_SF, 'GrLivArea' and 'GrageArea'","8c02cef7":"### Step-5: Dummy Creation\n\nLabeling Ordinal Variables: ","6cfee7e0":"It's clearly visible that this model is overfitting and accuracy is very poor on test data set. So let's build regression model using regularization.","db2b5e12":"## <font color= Blue> Advanced Regression- House Price Prediction using Lasso and Ridge Models\n\n\n\n#### Problem Statement:\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual value and flip them at a higher price. For the same purpose, the company has collected a data set from house sales in Australia.  \n\nThe company is looking at prospective properties to buy to enter the market.\n\nRequired to build a regression model using regularization, so as to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n \nThe company wants to know:\n    - Which variables are significant in predicting the price of a house\n    - How well those variables describe the price of a house\n    - Also, determine the optimal value of lambda for ridge and lasso regression. \n\n#### Business Goal\n\nRequired to model the price of houses with the available independent variables. It will be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high rewards. Further, the model will be a good way for management to understand the pricing dynamics of a new market.","49f16cb2":"Insights:\n- The majority of homes are between 1 - 2 with a median home price.  \n- The distribution appears to have a right skew (mean > median).   \n- There are no regression assumptions that require the independent or dependent variables to be normal.  ","c9ac8b60":"Insight:\n- Following Numerical variables are have no variance:  `'LowQualFinSF', 'PoolArea', 'MiscVal'`","1dc10b55":"**Lasso Conclusion:**\n\n1. From the R square value we see that the model**\n    - built using lasso regression can have morethan 88% accuracy in its prediction capability.\n    - R-squared (R2) explains the proportion of variation in the outcome that is explained by the predictor variables. \n\n2. With optimal value of alpha = 0.001, we got following score for train and test data set.\n\n  **------- Train-------**\n    - R2 Train: 0.894\n    - MAE: 0.21851880685120664\n    - MSE is: 0.10851861545066735\n    - RMSE 0.3294216377997465\n \n   **------- Test-------**\n    - R2 Test: 0.891\n    - MAE: 0.23164127459252518\n    - MSE is: 0.0996805872430409\n    - RMSE 0.3157223261713382\n   \n   \n   \n3. Top 10 features\n\n    `'GrLivArea', 'Total_finished_SF', 'OverallQual', 'LotArea','Property_Age', 'OverallCond', 'MSZoning_FV', 'MSZoning_RL', 'Neighborhood_StoneBr', 'Neighborhood_NoRidge`\n    \n*This score looks good and we can conclude that model is not overfitting.*","c484a6bf":"From the plots we have seen that optimal value for alpha is 0.001. So let's use this value of alpha for regularization.","b7d6a1d4":"**9.1.1 Re-Build Lasso**","54aeaae6":"**Property Age**","b2c660ad":"### Step-8: RFE:","eeb2b3bb":"#### 9.2 Ridge Regression","0423ac49":"Instead of dropping the null values which will result in a data loss, we will impute the null values according to the domain \nunderstanding and the data dictionary provided with the data.","90ca6d99":"### Import Required Libraries","836ef7d6":"**4.3 Categorical Variables**","388c0953":"### <font color = Green>Step-10: Conclusion","2be4e0ca":"Insights:\n- `'PoolQC', 'MiscFeature', 'Alley'` are having missig values more that 90%. So imputing them will not give us any meaning full insights. We shall drop these columns.\n\n- `'Fence'`: Not missing at random. 80% of property does not have fence. We will impute this variable.\n- `'FireplaceQu'`: Not missing at random. The fireplace for the building is not available so the quality rating is mssing. So we will impute the missing values.","ac049dac":"**Building Floors**","0bbae2d5":"### Step-4: EDA","e06c0c86":"### Step-3: Feature Engineering","5eda513b":"Insights:\n- There are 43 object type columns and 38 numerical columns in the dataset\n- SalesPrice is the target variable\n- ID - the identification number will not impact in the model, so can be dropped\n- Here, although some variables are numeric (int), we'd rather treat them as categorical since they have discrete values."}}