{"cell_type":{"4264f6f7":"code","e72412d1":"code","b22a414b":"code","29fd9d8a":"code","9b8ab41c":"code","c42d237b":"code","4c751b01":"code","3b5dab75":"code","e2fe9fb8":"code","58c5a95d":"code","cc99b1f2":"code","6fec93d2":"code","c058d227":"code","680fc817":"code","0c55b587":"code","d21a3e54":"code","c4acfab6":"code","93f1e513":"code","142b111f":"code","636de0bf":"code","7fcc15ba":"markdown","da26c015":"markdown","afdde137":"markdown","c044ed83":"markdown","e5995f3c":"markdown","1d0084da":"markdown","2b8da476":"markdown","02c2c4a2":"markdown","4683c531":"markdown","543f5bc3":"markdown","93616100":"markdown","9eca719d":"markdown","521c0642":"markdown","d8424571":"markdown","49ed3505":"markdown","4252cdbd":"markdown","d87b0646":"markdown"},"source":{"4264f6f7":"!pip install pyspark","e72412d1":"import os\nimport pyspark.sql.types as typ\nimport pyspark.sql.functions as fn\nfrom enum import Enum\nfrom pyspark import SparkContext\nfrom pyspark.sql import DataFrame, SparkSession\nfrom pyspark.ml import Transformer, Pipeline, PipelineModel\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom typing import List, Callable\n","b22a414b":"spark = SparkSession.builder.appName(\"loan_default_prediction\").getOrCreate()","29fd9d8a":"labels = [\n  ('ID', typ.IntegerType()),\n  ('DISBURSED_VALUE', typ.IntegerType()),\n  ('ASSET_COST', typ.IntegerType()),\n  ('LOAN_TO_VALUE', typ.DoubleType()),\n  ('BRANCH_ID', typ.IntegerType()),\n  ('SUPPLIER_ID', typ.IntegerType()),\n  ('MANUFACTURER_ID', typ.IntegerType()),\n  ('PINCODE', typ.IntegerType()),\n  ('DOB', typ.StringType()),\n  ('EMP_TYPE', typ.StringType()),\n  ('DISBURSED_DATE', typ.StringType()),\n  ('REGION_ID', typ.IntegerType()),\n  ('EMPLOYEE_CODE_ID', typ.IntegerType()),\n  ('MOBILENO_AVL_FLAG', typ.IntegerType()),\n  ('ID1_FLAG', typ.IntegerType()),\n  ('ID2_FLAG', typ.IntegerType()),\n  ('ID3_FLAG', typ.IntegerType()),\n  ('ID4_FLAG', typ.IntegerType()),\n  ('ID5_FLAG', typ.IntegerType()),\n  ('BUREAU_SCORE', typ.IntegerType()),\n  ('SCORE_CATEGORY', typ.StringType()),\n  ('PRI_ACCS', typ.IntegerType()),\n  ('ACTIVE_ACCS', typ.IntegerType()),\n  ('OVERDUE_ACCS', typ.IntegerType()),\n  ('TOTAL_BALANCE_OUTSTANDING', typ.IntegerType()),\n  ('TOTAL_SANCTIONED_AMT', typ.IntegerType()),\n  ('TOTAL_DISBURSED_AMT', typ.IntegerType()),\n  ('SEC_ACCS', typ.IntegerType()),\n  ('SEC_ACTIVE_ACCS', typ.IntegerType()),\n  ('SEC_OVERDUE_ACCS', typ.IntegerType()),\n  ('SEC_TOTAL_BALANCE_OUTSTANDING', typ.IntegerType()),\n  ('SEC_TOTAL_SANCTIONED_AMT', typ.IntegerType()),\n  ('SEC_TOTAL_DISBURSED_AMT', typ.IntegerType()),\n  ('PRI_EMI', typ.IntegerType()),\n  ('SEC_EMI', typ.IntegerType()),\n  ('LOANS_6_MTHS', typ.IntegerType()),\n  ('LOANS_DEFAULT_6_MTHS', typ.IntegerType()),\n  ('AVG_LOAN_TENURE', typ.StringType()),\n  ('CREDIT_HIST_LEN', typ.StringType()),\n  ('INQUIRIES', typ.IntegerType()),\n  ('DEFAULT', typ.IntegerType())\n]","9b8ab41c":"schema = typ.StructType([\n  typ.StructField(e[0], e[1], True)\n  for e in labels\n])\n\ndf = spark.read.csv(\"..\/input\/vehicular-loan\/train.csv\",\n                    header=True,\n                    schema=schema)","c42d237b":"config = {\n    \"id_col\": \"ID\",\n    \"impute_cat_cols\": {\"EMP_TYPE\": \"missing\"},\n    \"impute_numerical_cols\": {},\n    \"date_str_cols\": {\n        \"DOB\": \"19\",\n        \"DISBURSED_DATE\": \"20\"\n    },\n    \"age_cols\": [\n        {\n            \"start\": \"DOB\",\n            \"end\": \"DISBURSED_DATE\",\n            \"output_col\": \"BORROWER_AGE\"\n        }\n    ],\n    \"tenure_cols\": [\"CREDIT_HIST_LEN\", \"AVG_LOAN_TENURE\"],\n    \"str_replace_cols\": {\n        \"SCORE_CATEGORY\": {\n            \"pattern\": \"Not Scored: (.*)\",\n            \"replacement\": \"Not Scored\"\n        }\n    },\n    \"drop_cols\": [\"ID\", \"MANUFACTURER_ID\", \"BRANCH_ID\", \"SUPPLIER_ID\",\n                  \"PINCODE\", \"EMPLOYEE_CODE_ID\", \"DISBURSED_DATE\",\n                  \"DOB\"],\n    \"processed_data_dir\": \"processed\/\",\n    \"str_cat_cols\": [\"SCORE_CATEGORY\", \"EMP_TYPE\"],\n    \"test_size\": 0.2,\n    \"seed\": 42,\n    \"cat_cols\": [\"REGION_ID\"],\n    \"target_col\": \"DEFAULT\",\n    \"model_hyperparams\": {\n        \"maxIter\": 100,\n        \"featuresCol\": \"features\"\n    },\n    \"model_path\": \"models\/models\/gbt\",\n    \"model_log_dir\": \"models\/logs\/\"\n}","4c751b01":"folders = [config['processed_data_dir'], config['model_path'], config['model_log_dir']]\nfor folder in folders:\n    if not os.path.exists(folder):\n        os.makedirs(folder)","3b5dab75":"class RemoveDuplicates(Transformer):\n    \"\"\"Drops duplicate records\"\"\"\n\n    def __init__(self, id_col: str = None):\n        super(RemoveDuplicates, self).__init__()\n        self._id_col = id_col\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        # Drop exact duplicates\n        df = df.dropDuplicates()\n        # Drop duplicates that are exact duplicates apart from ID\n        df = df.dropDuplicates(subset=[c for c in df.columns if c != self._id_col])\n        if self._id_col == '' or self._id_col is None:\n            return df\n        # Provide monotonically increasing, unique ID (in case of duplicate ID)\n        df = df.withColumn(self._id_col, fn.monotonically_increasing_id())\n        return df","e2fe9fb8":"class ConvertStrToDate(Transformer):\n    \"\"\"Converts date string column to date column\"\"\"\n\n    def __init__(self, date_str_cols: dict = None):\n        super(ConvertStrToDate, self).__init__()\n        self._date_str_cols = date_str_cols\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        if self._date_str_cols == {} or self._date_str_cols is None:\n            return df\n        # Converts \"1\/1\/99\" --> \"01\/01\/1999\", then convert to date object\n        for column, year_prefix in self._date_str_cols.items():\n            df = df.withColumn(column, fn.to_date(column, \"d\/M\/yy\"))\n            df = df.withColumn(column, fn.when(fn.year(column) > (int(year_prefix) + 1)*100, \n                                            fn.add_months(column, -12*100)) \\\n                                        .when(fn.year(column) == 1900, \n                                            fn.add_months(column, 12*100)) \\\n                                        .otherwise(fn.col(column)))\n        return df","58c5a95d":"class GetAge(Transformer):\n    \"\"\"Get Age in years based on 2 date columns\"\"\"\n\n    def __init__(self, age_cols: List[dict] = None):\n        super(GetAge, self).__init__()\n        self._age_cols = age_cols\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        if self._age_cols == {} or self._age_cols is None:\n            return df\n        for column_dict in self._age_cols:\n            start_col = column_dict[\"start\"]\n            end_col = column_dict[\"end\"]\n            output_col = column_dict[\"output_col\"]\n            df = df.withColumn(output_col, \n                               fn.floor(fn.datediff(end_col, start_col)\/365.25))\n        return df","cc99b1f2":"class ExtractTimePeriodMths(Transformer):\n    \"\"\"\n    Extract Period of time from string\n    for eg: 2yrs 2mon --> 26\n    \"\"\"\n\n    def __init__(self, tenure_cols: List[str] = None):\n        super(ExtractTimePeriodMths, self).__init__()\n        self._tenure_cols = tenure_cols\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        if self._tenure_cols == [] or self._tenure_cols is None:\n            return df\n        for column in self._tenure_cols:\n            df = df.withColumn(f\"{column}_YRS\", fn.regexp_extract(column, \"(\\d)(yrs)\", 1))\n            df = df.withColumn(f\"{column}_MTHS\", fn.regexp_extract(column, \"(\\d)(mon)\", 1))\n            df = df.withColumn(column, (fn.col(f\"{column}_YRS\")*12 + fn.col(f\"{column}_MTHS\")))\n            df = df.drop(*[f\"{column}_YRS\", f\"{column}_MTHS\"]) \n        return df","6fec93d2":"class ReplaceStrRegex(Transformer):\n    \"\"\"Replace string in a String column using regex\"\"\"\n\n    def __init__(self, str_replace_cols: dict = None):\n        super(ReplaceStrRegex, self).__init__()\n        self._str_replace_cols = str_replace_cols\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        if self._str_replace_cols == {} or self._str_replace_cols is None:\n            return df\n        for column, val in self._str_replace_cols.items():\n            pattern = val[\"pattern\"]\n            replacement = val[\"replacement\"]\n            df = df.withColumn(column, fn.regexp_replace(column, pattern, replacement))\n        return df","c058d227":"class ImputeCategoricalMissingVals(Transformer):\n    \"\"\"Impute missing values in categorical columns\"\"\"\n\n    def __init__(self, cat_cols_dict: dict = None):\n        super(ImputeCategoricalMissingVals, self).__init__()\n        self._cat_cols_dict = cat_cols_dict\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        for column, value in self._cat_cols_dict.items():\n            df = df.fillna({column: value})\n        return df","680fc817":"class DropColumns(Transformer):\n    \"\"\"Converts date string column to date column\"\"\"\n\n    def __init__(self, cols: List[str] = None):\n        super(DropColumns, self).__init__()\n        self._cols = cols\n\n    def _transform(self, df: DataFrame) -> DataFrame:\n        if self._cols == [] or self._cols is None:\n            return df\n        df = df.drop(*self._cols)\n        return df","0c55b587":"class Pipe(Transformer):\n    \"\"\"\n    Conditional pipeline which runs one or another list of transformers based on condition\n\n    original author: https:\/\/github.com\/bbiletskyy\/pipeline-oriented-analytics\n    \"\"\"\n\n    def __init__(self, stages: List[Transformer]):\n        super(Pipe, self).__init__()\n        self._pipeline = PipelineModel(stages)\n\n    def _transform(self, dataset: DataFrame) -> DataFrame:\n        return self._pipeline.transform(dataset)","d21a3e54":"class IF(Transformer):\n    \"\"\"\n    Conditional pipeline which runs one or another list of transformers based on condition\n    \n    original author: https:\/\/github.com\/bbiletskyy\/pipeline-oriented-analytics\n    \"\"\"\n\n    class Predicate:\n        @classmethod\n        def const(cls, value: bool) -> Callable[[DataFrame], bool]:\n            def predicate(df: DataFrame) -> bool:\n                return value\n            return predicate\n\n        @classmethod\n        def has_column(cls, name: str) -> Callable[[DataFrame], bool]:\n            def predicate(df: DataFrame) -> bool:\n\n                return name in set(df.columns)\n\n            return predicate\n\n    @classmethod\n    def condition(cls, condition: bool) -> Callable[[DataFrame], bool]:\n        def constant_predicate(df: DataFrame) -> bool:\n            return condition\n        return constant_predicate\n\n    @classmethod\n    def has_columns(cls, columns: List[str]) -> Callable[[DataFrame], bool]:\n        def predicate(df: DataFrame) -> bool:\n            return set(columns).issubset(df.columns)\n\n        return predicate()\n\n    def __init__(self, condition: Callable[[DataFrame], bool],\n                 then: List[Transformer], otherwise: List[Transformer] = None):\n        super(IF, self).__init__()\n        self._condition = condition\n        self._then = Pipe(then)\n        self._otherwise = Pipe([])\n        if otherwise is not None:\n            self._otherwise = Pipe(otherwise)\n\n    def _transform(self, dataset: DataFrame) -> DataFrame:\n        if self._condition(dataset) is True:\n            return self._then.transform(dataset)\n        else:\n            return self._otherwise.transform(dataset)","c4acfab6":"def preprocess_data(spark, config, raw_df):\n    \"\"\" Runs Data Preparation job\"\"\"\n    phase = 'train' if \"DEFAULT\" in raw_df.columns else 'test'\n    df = Pipe([\n        IF(IF.Predicate.has_column('DEFAULT'), then=[\n            RemoveDuplicates(config['id_col'])\n        ]),\n        ConvertStrToDate(config['date_str_cols']),\n        GetAge(config['age_cols']),\n        ExtractTimePeriodMths(config['tenure_cols']),\n        ReplaceStrRegex(config['str_replace_cols']),\n        ImputeCategoricalMissingVals(config['impute_cat_cols']),\n        DropColumns(config['drop_cols']),\n    ]).transform(raw_df)\n    df.write.parquet(config['processed_data_dir'] + f\"{phase}.parquet\", mode='overwrite')\n    \npreprocess_data(spark, config, df)","93f1e513":"def _read_data(spark, config):\n    return spark.read.parquet(config['processed_data_dir'] + \"train.parquet\")\n\n\ndef _train_test_split(df, config):\n    \"\"\"\n    Split dataset (df) into train and test data\n    using stratified split:\n    \"\"\"\n    fractions = df.select(\"DEFAULT\").distinct() \\\n                  .withColumn(\"fraction\", fn.lit(1-config['test_size'])).rdd.collectAsMap()\n    train_df = df.stat.sampleBy(config['target_col'], fractions, config['seed'])\n    test_df = df.subtract(train_df)\n    return train_df, test_df\n\n\ndef _get_string_indexers(config):\n    string_indexers = [StringIndexer(inputCol=column, outputCol=column+\"_INDEX\") \\\n                        for column in config['str_cat_cols']]\n    return string_indexers\n\n\ndef _get_missing_value_imputers(df, config):\n    impute_dict = config.get(\"impute_numerical_cols\", None)\n    if impute_dict == {} or impute_dict is None:\n        return []\n    imputers = []\n    for column, val in impute_dict.items():\n        check_numerical_dtype(df, column)\n        imputer = sf.Imputer(inputCols=[column], \n                             outputCols=[column],\n                             strategy=val)\n        imputers.append(imputer)\n    return imputers\n\n\ndef _get_one_hot_encoders(config):\n    cat_cols = [f'{column}_INDEX' for column in config['str_cat_cols']]\n    cat_cols += config['cat_cols']\n    ohe_encoders = []\n    for column in cat_cols:\n        ohe_encoder = OneHotEncoder(inputCol=column,\n                                    outputCol=\"{0}_ENCODED\".format(column))\n        ohe_encoders.append(ohe_encoder)\n    return ohe_encoders\n\n\ndef _get_vector_assembler(df, config):\n    vars_exclude = [config['target_col']] + config['cat_cols'] + config['str_cat_cols']\n    assembler_inputs = [c for c in df.columns if c not in vars_exclude]\n    vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n    return vec_assembler\n\n\ndef _evaluate_results(config, predictions):\n    evaluator = BinaryClassificationEvaluator(labelCol=config['target_col'])\n    auc_roc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n    auc_pr = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'})\n    preds_and_labels = predictions.select(['prediction', config['target_col']]) \\\n                                  .withColumn('label', fn.col(config['target_col']) \\\n                                  .cast(typ.FloatType())).orderBy('prediction')\n    #select only prediction and label columns\n    preds_and_labels = preds_and_labels.select(['prediction', 'label'])\n    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n    with open(config['model_log_dir'] + 'train_results.txt', 'w') as writer:\n        writer.write(f\"AUC ROC: {auc_roc} \\n\")\n        writer.write(f\"AUC PR: {auc_pr} \\n\")\n        writer.write(f\"Confusion Matrix: {metrics.confusionMatrix().toArray()} \\n\")","142b111f":"def train_model(spark, config):\n    \"\"\" Runs Model Training job\"\"\"\n    df = _read_data(spark, config)\n    train_df, test_df = _train_test_split(df, config)\n    string_indexers = _get_string_indexers(config)\n    imputers = _get_missing_value_imputers(train_df, config)\n    ohe_encoders = _get_one_hot_encoders(config)\n    vec_assembler = _get_vector_assembler(train_df, config)\n    gbt_clf = GBTClassifier(**config['model_hyperparams'], labelCol=config['target_col'])\n    stages = string_indexers + imputers + ohe_encoders + [vec_assembler] + [gbt_clf]\n    pipeline = Pipeline(stages=stages).fit(train_df)\n    model_path = config['model_path']\n    print('Saving model to {}'.format(model_path))\n    pipeline.write().overwrite().save(model_path)\n    print('Model saved...')\n    model = PipelineModel.load(model_path)\n    predictions_df = model.transform(test_df)\n    _evaluate_results(config, predictions_df)","636de0bf":"train_model(spark, config)","7fcc15ba":"Specifically, the flow of model training is as such:  <br\/>\n![image.png](attachment:image.png)","da26c015":"<a id=\"3\"><\/a>\n## 3. Solution\n---","afdde137":"Chain the transformers together for preprocessing","c044ed83":"<a id=\"2\"><\/a>\n## 2. Solution Overview\n---\n\nTo tackle this problem, we will train a machine learning model in Spark, specifically using Pyspark.\n\n\ud83d\udd14<ins>Why Spark?<\/ins>\n\n1. Simplicity: API\u2019s are well-documented and structured\n2. Speed: Spark designed for speed by operating both in-memory. Lower I\/O costs as compared to Hadoop\n\nRead this informative blog post for more information: https:\/\/medium.com\/javarevisited\/why-industries-running-behind-spark-50-reasons-why-spark-is-important-83473477b41d\n\nThe above benefits makes Spark scalable and stable","e5995f3c":"After defining Estimators, we can chain them together in a Pipeline. By doing so, we don't have to call .fit method multiple times. We just do it once on the Pipeline Object and it will return a chained Pipeline.\n\n\ud83d\udd14<ins>What is a Pipeline?<\/ins>\n\nA Pipeline is a workflow which consists of a sequence of PipelineStages (Transformers\/Estimators) to be run in a specific order\n\nPlease refer to spark documentation for more details: https:\/\/spark.apache.org\/docs\/1.6.0\/ml-guide.html#pipeline","1d0084da":"Here, we first perform a train-test split, then we get the Estimators and chain them to form a Pipeline.\n\n\ud83d\udd14<ins>What is an Estimator?<\/ins>\n\nAn Estimator takes a DataFrame as input and returns a Transformer after we call .fit() on a spark DataFrame.\n\nPlease refer to spark documentation for more details: \nhttps:\/\/spark.apache.org\/docs\/1.6.0\/ml-guide.html#estimators","2b8da476":"Here is the chain of transformations we will be doing:\n\n![image.png](attachment:image.png)","02c2c4a2":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n    \n[0. Domain Background](#0)\n    \n[1. Data](#1)\n    \n[2. Solution Overview](#2)\n    \n[3. Solution](#3)\n* [3.1 Configuration](#3.1)\n* [3.2 Defining Transformers](#3.2)\n* [3.3 Building a machine learning pipeline](#3.3)\n    \n[4. Conclusion](#4)","4683c531":"## Smart Lending Default Prediction\n---\n\n![image.png](attachment:image.png)","543f5bc3":"To run pyspark code, we have to first initialize the SparkContext.\n\n\ud83d\udd14<ins>What is a SparkContext?<\/ins>\n\nSparkContext is the entry point of Spark functionality. The most important step of any Spark driver application is to generate SparkContext. It allows your Spark Application to access Spark Cluster with the help of Resource Manager","93616100":"<a id=\"3.2\"><\/a>\n\n## 3.2 Defining Transformers\n---\n\nHere we define pyspark transformers for our preprocessing step.\n\n\ud83d\udd14<ins>What is a transformer in Spark?<\/ins>\n\nA transformer takes in a Spark Dataframe, performs some transformation on its column(s) and return a Dataframe.\n\nRefer to Spark documentation for a detailed explanation:\nhttps:\/\/spark.apache.org\/docs\/1.6.0\/ml-guide.html#transformers","9eca719d":"Define a schema to read in our data:","521c0642":"<a id=\"3.1\"><\/a>\n\n## 3.1 Configuration\n---\n\nTypically this would be in a config.json file. In this case, we define it in a dict.","d8424571":"<a id=\"0\"><\/a>\n## 0. Domain Background\n---\n\n* Financial institutions incur significant losses due to the default of vehicle loans every year.\n\n* <ins>Goal:<\/ins> To accurately pre-empt problematic borrowers who are likely to default in future, our goal is to build a machine learning model that can accurately predict borrowers that are likely to default on first EMI (Equated Monthly Instalments)","49ed3505":"<a id=\"4\"><\/a>\n\n## 4. Conclusion\n---\n\nThat's the end for our spark machine learning pipeline. We have successfully built a pyspark ml pipeline from scratch to predict vehicular loan default. This persisted Pipeline can be used for inferencing unseen, real world data.\n\nIf this notebook has benefitted you, please give an upvote, I will greatly appreciate it.\n\nIn addition, please feel free to refer to my github repo at: https:\/\/github.com\/alanchn31\/Loan-Default-Prediction on how to structure pyspark code to run as spark jobs \/ scripts.\n\nThank you!","4252cdbd":"<a id=\"3.3\"><\/a>\n\n## 3.3 Building a Model Training Pipeline\n---\n\n\nNow that data is transformed, we can build our model training pipeline","d87b0646":"<a id=\"1\"><\/a>\n## 1. Data\n\n* We used a public dataset that can be found on Kaggle: https:\/\/www.kaggle.com\/avikpaul4u\/vehicle-loan-default-prediction.\n\n* Dataset has 50,000 rows with 41 columns\n\n* As data is imbalanced, (about 80% of labels are non-default while 20% are defaults), our main metric of evaluating our trained model will be the AUC_ROC"}}