{"cell_type":{"1b067f98":"code","3c876a8f":"code","4ecd4ae7":"code","b12407dd":"markdown","e1a0dac9":"markdown","9987e983":"markdown","cc89faf2":"markdown","4ac251e2":"markdown","d8efaa22":"markdown","e307063b":"markdown","fb048bc4":"markdown","56ac7bfb":"markdown","6b56235a":"markdown","a0ea980a":"markdown"},"source":{"1b067f98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom IPython.core.display import display, HTML, Javascript\nfrom string import Template\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# import json, random\nimport IPython.display\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3c876a8f":"htmlpros=\"\"\"\n<style>\n .l th { text-align:left;}\n  .l td { text-align:left;}\n   .l tr { text-align:left;}\n<\/style>\n<p>\n<h2>Pros and cons of my approach<\/h2>\n<table class=l border=1>\n<tr><th>Risk<\/th><th>Example Failure<\/th><th>Design<\/th>\n<tr><td>\t1.1 Non-trustworthy sources<td>Unscientific or fictitious source<td>Use CORD-19 dataset.<br>Pro: which are primarily sources from a PUBMED search of documents that were\nwritten and published for professionals (in medicine, government, etc.). They are not all peer-reviewed (not a panacaea anyway) but they are expected to be scientifically trustworthy.<br>Con: CORD-19 are \"open\" sources. Better sources may be excluded.\n<tr><td rowspan=4>1.2 Inappropriate use of trustworthy sources<td rowspan=4>Result out of context. (E.g. incubation period from a paper about mice, or SARS, which is a different virus)\n\t<td>Use results from titles and abstracts, not full text. <br>Pro: Full-text search is likely to find many documents with the terms that were intended to be mentioned only in passing, and in a different context, unnecessarily multiplying the quantity, but not the quality of results.<br>Pro: Titles (and most abstracts) are most often written from a \"low-context\" perspective using wording that is precise and most carefully considered. This is in contrast to sentences from full text which are very high-context. We avoid excerpting high-context sentences for presentation in search results so that we avoid the risk of misinterpretation due to taking them out of the author's context and presenting them in a very different context.<br>Con: Important findings may not be described in the abstract using search terms.\n<tr><td>Avoid use of secondary MeSH headings.<br>Pro: A source with trustworthy conclusions should be categorized under a primary MeSH heading. Secondary headings are inconsistently applied and using them decreases reliability.<br>Con: Secondary headings, if they are accurate, could be useful in limiting search results further than just primary Headings. (Due to the inconsistent application of secondary headings, there are better ways of doing this than secondary headings, though. Using the alternative primary heading is better, for example.)\n<tr><td>Use Human-in-the-loop to show context and interactively limit sources (e.g. by MeSH header, date range, paper type.)\n<br>Pro: Human can apply and review individual criteria for their circumstances\n<br>Con: \"Cluttered\" user interface that requires some experience to become familiar and use well. \n<tr><td>Date range limit and sort by date range descending. [See note 1 below.]\n<tr><td rowspan=7>1.3 Missed sources<td rowspan=4>Search terms too narrow, lacking synonyms\n\t<td>Show search results within MeSH subject context.\n\t<br>Pro: Sources with similar topics are grouped into a category. Finding one document finds the category, and that means other articles that are similar. Synonym terms will be in those documents.\n\t<br>Con: MeSH subjects are too broad.\n<tr><td>Use synonyms in search terms Caveat: (Currently human-directed, as was done in CORD-19 query)\n\t<br>Pro: avoids creating a good synonym dictionary\n\t<br>Con: Extra work and likely to lead non-experts to searches that are too narrow.\n<tr><td>Show links to nearby categories that were used, because MeSH tagging for articles is not 100% reliable.\n\t<br>Pro: Tolerates the categorization imprecision that exists in any subject index, by showing \"adjacent\" categories, parents, and children.\n\t<br>Con: MeSH trees are not without flaws, and may be unfamiliar to users. Makes searching harder because several pages need to be consulted, not just one.\n<tr><td>Show articles at multiple MeSH tree locations, not just one.\n\t<br>Pro: Articles are multi-faceted and should not go into a subject tree at one heading.\n\t<br>Con: Articles will be seen more than once when searching several subject pages.\n<tr><td rowspan=2>Sources outside CORD-19<td>Show concept keywords for making external searches\n\t<br>Pro: Allows consulting other systems\n\t<br>Con: Would be better to integrate the other sources into one system.\n<tr><td>Deep link to full PubMed search so it can be edited and re-run\n\t<br>Pro: Allows consulting other systems\n\t<br>Con: Makes it look like that consulting other systems is usual and necessary, rather than for rare needs.\n<tr><td>Missing because terms appear in body, not title and abstract<td>Provide link to PubMed with full-text search\n\t<br>Pro: Allows expanding to full-text search without needing a reverse index.\n\t<br>Con: Harder to use than clicking a checkbox and re-running\n<tr><td>2. Too many results<td>Imposed ordering of paginated results<td>Present unranked, unpaginated results under categories. See Note 2\n\t<br>Pro: Makes it easy to use the full result set\n\t<br>Con: Requires using the full, unranked result set.\n\n<tr><td rowspan=2>3. Unknown confidence in results<td>Accepting narrow search results that look good<td>Human-in-the-loop to explore \"what could I be missing\" using designs to avoid missed sources in 1.3\n\t<br>Pro: Human in the loop is good control\t\n\t<br>Con: Training is necessary to get consistent results across people.\n<tr><td>Lack of consensus on source quality<td>Show all results that match, without imposing filtering\n\t<br>Pro: This is not a \"solvable\" problem. Humans have individual opinions. There should be deep links from results to pages with mechanisms to encourage easy customization and re-resulting with different source filters.\n\t<br>Con: Non-experts will get different results and take extra work to compare.\n<tr><td colspan=3>Note 1: The importance of considering date in evaluating results cannot be over-emphasized given two facts:<ol><li>In the abstract of the Review article <A href=\"https:\/\/pubmed.ncbi.nlm.nih.gov\/32166607\/\">A Review of Coronavirus Disease-2019 (COVID-19)<\/A>\", (2020\/03) the authors report \"[SARS-CoV-2] spreads faster than its two ancestors the SARS-CoV and Middle East respiratory syndrome coronavirus (MERS-CoV), but has lower fatality.\" <li>The CORD-19 dataset includes search terms for MERS and SARS and since SARS-CoV-2 is so new, there are likely to be a disproportionate majority of articles which could answer CORD-19 questions unreliably.\n<\/ol>NLP and automated text processing that estimates consensus will be working with a very large data set of possibly inappropriate sources. SARS-CoV-2 must is distinct from other coronaviruses at least in several important aspects. General conclusions made in 2019 about \"coronavirus\" will need to be re-evaluated and not used indiscriminately. In a research field that is so new, a human would deem the consideration of date of publication as an essential part of identifying trustworthy sources. Using date-range descending order of presenting a list of results is a simple way to help with this task.\n<tr><td colspan=3>Note 2: The problem of too many search results is so common that we have become comfortable with partial and risky coping strategies. The most common is that some form of automatic ranking is used to show results in a sorted list with pagination. This is bad because even if all users agreed on the ranking criteria, it is not within the current capabilities of automated text processing systems to reliably evaluate sources for useful criteria, such as \"trustworthiness.\" There are many criteria that might seem to be proxies for trustworthiness (such as type of publication systematic review vs. study, number of citations) they are not reliable for many reasons, including: \n<ul><li>dependence on reliable metadata creation. (There are gaps and limitations in the MEDLINE metadata.)\n<li>different users will have different ranking criteria they would prefer. What most people would expect would be a composite multi-faceted analysis of several base criteria that are not consistent across all the articles in a large data set.\n<li>in \"cutting edge\" research, there has not been enough time to establish a body of research with scientific consensus so new and old articles need different criteria applied.\n<li>it is not simple to indicate the valuation of trustworthiness or other ranking criteria to a user, even if it were reliable. \n<\/ul>\n In most systems, the ranking criteria is imposed, not selected by the individual. This is risky because sorted results and pagination train and encourage people to be satisfied with looking through some of the results and stopping when they find something \"sufficient\" rather than \"better\" and \"optimal.\" When the answer is critical, all the relevant results should be reviewed, and if filtering and ranking is to be employed, it should be directed by the user.\n<p>The CORD-19 dataset has about 50,000 items, which is certainly too many items to display if a stop-word (or word from the orginal search criteria) is used for filtering.  But if articles are segregated by MeSH subject heading, the MeSH heading with the largest number of articles in the CORD-19 data set is Coronavirus Infections, with 2,485 items. Most users will want to apply criteria or at least one additional text search term to limit the set. But even if they do not, the 9.6MB metadata file and use of modern desktop web browsers easily show 2,485 in one list.\n<\/table>\n\n\"\"\"\n\nh = display(HTML(htmlpros))","4ecd4ae7":"htmltask1a=\"\"\"\n<style>\n .l th { text-align:left;}\n  .l td { text-align:left;}\n   .l tr { text-align:left;}\n<\/style>\nSearching for (incubation period) in<br>[recent CORD-19 titles and abstracts]<br> finds documents at the following Medical Subject (MeSH) headings\n<blockquote>\n<p><span style=\"font-size:large\"><a href=\"http:\/\/www.softconcourse.com\/CORD19\/?filterText=incubation+period&amp;from=CORD19#\/C\/CO\/Coronavirus Infections\">Coronavirus Infections<\/a><\/span>\n<\/p><p><span style=\"font-size:large\"><a href=\"http:\/\/www.softconcourse.com\/CORD19\/?filterText=incubation+period&amp;from=CORD19#\/B\/BE\/Betacoronavirus\">Betacoronavirus<\/a><\/span>\n<\/p><p><span style=\"font-size:large\"><a href=\"http:\/\/www.softconcourse.com\/CORD19\/?filterText=incubation+period&amp;from=CORD19#\/P\/PN\/Pneumonia, Viral\">Pneumonia, Viral<\/a><\/span>\n<\/p><\/blockquote><p>Click a MeSH heading to review these results in a WebApp where you can view full abstracts, modify the search terms, include documents published before 2019\/12, etc.\n<table class=l><tbody><tr><th>MeSH heading<\/th><th>Recent Titles and matching excerpts from Abstracts<\/th><th>Pub.Type<\/th><th>Date<\/th>\n<\/tr><tr valign=\"top\"><td rowspan=\"4\"><a href=\"http:\/\/www.softconcourse.com\/CORD19\/?filterText=incubation+period&amp;from=CORD19#\/C\/CO\/Coronavirus Infections\">Coronavirus Infections<\/a>\n<\/td><\/tr><tr valign=\"top\"><td>\n <a target=\"_blank\" href=\"https:\/\/pubmed.ncbi.nlm.nih.gov\/32112886\">\nCharacteristics of COVID-19 infection in Beijing.\n<\/a>\n<small>(PMID32112886<\/small>)\n<br>...The median <b>incubation<\/b> <b>period<\/b> was 6.7 days, the interval time from between illness onset and seeing a doctor was 4.5 days.\n<\/td><td>Journal Article; Research Support, Non-U.S. Gov't<\/td>\n<td>2020\/04<\/td>\n<\/tr>\n<tr valign=\"top\"><td>\n <a target=\"_blank\" href=\"https:\/\/pubmed.ncbi.nlm.nih.gov\/31995857\">\nEarly Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.\n<\/a>\n<small>(PMID31995857<\/small>)\n<br>...The mean <b>incubation<\/b> <b>period<\/b> was 5.2 days (95% confidence interval [CI], 4.1 to 7.0), with the 95th percentile of the distribution at 12.5 days.\n<\/td><td>Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't<\/td>\n<td>2020\/03<\/td>\n<\/tr>\n<tr valign=\"top\"><td>\n <a target=\"_blank\" href=\"https:\/\/pubmed.ncbi.nlm.nih.gov\/32046816\">\nEffectiveness of airport screening at detecting travellers infected with novel coronavirus (2019-nCoV).\n<\/a>\n<small>(PMID32046816<\/small>)\n<br>...In our baseline scenario, we estimated that 46% (95% confidence interval: 36 to 58) of infected travellers would not be detected, depending on <b>incubation<\/b> <b>period<\/b>, sensitivity of exit and entry screening, and proportion of asymptomatic cases.\n<\/td><td>Journal Article<\/td>\n<td>2020\/02<\/td>\n<\/tr>\n<tr valign=\"top\"><td rowspan=\"2\"><a href=\"http:\/\/www.softconcourse.com\/CORD19\/?filterText=incubation+period&amp;from=CORD19#\/B\/BE\/Betacoronavirus\">Betacoronavirus<\/a>\n<\/td><\/tr><tr valign=\"top\"><td>\n <a target=\"_blank\" href=\"https:\/\/pubmed.ncbi.nlm.nih.gov\/32046819\">\nIncubation period of 2019 novel coronavirus (2019-nCoV) infections among travellers from Wuhan, China, 20-28 January 2020.\n<\/a>\n<small>(PMID32046819<\/small>)\n<br>...Using the travel history and symptom onset of 88 confirmed cases that were detected outside Wuhan in the early outbreak phase, we  estimate the mean <b>incubation<\/b> <b>period<\/b> to be 6.4 days (95% credible interval: 5.6-7.7), ranging from 2.1 to 11.1 days (2.5th to 97.5th percentile).\n<\/td><td>Journal Article<\/td>\n<td>2020\/02<\/td>\n<\/tr>\n<tr valign=\"top\"><td rowspan=\"2\"><a href=\"http:\/\/www.softconcourse.com\/CORD19\/?filterText=incubation+period&amp;from=CORD19#\/P\/PN\/Pneumonia, Viral\">Pneumonia, Viral<\/a>\n<\/td><\/tr><tr valign=\"top\"><td>\n <a target=\"_blank\" href=\"https:\/\/pubmed.ncbi.nlm.nih.gov\/32166607\">\nA Review of Coronavirus Disease-2019 (COVID-19).\n<\/a>\n<small>(PMID32166607<\/small>)\n<br>...The disease is transmitted by inhalation or contact with infected droplets and the <b>incubation<\/b> <b>period<\/b> ranges from 2 to 14 d.\n<\/td><td>Journal Article; Review<\/td>\n<td>2020\/04<\/td>\n<\/tr>\n<\/tbody><\/table>\n<\/p><hr><p>There are also 131 matches before 2019\/12\n<\/p>\n\"\"\"\n\nh = display(HTML(htmltask1a)) ","b12407dd":"## Improved Methodology\n![methodology.png](attachment:methodology.png)","e1a0dac9":"# Example Uses\nOne notebook per CORD-19 task details the search results.  Here is one example from [CORD-19 Task](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=568)\n\n> What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n\n> Specifically, we want to know what the literature reports about:\n\n>     Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery\n\n","9987e983":"# Avoiding Search Term Insufficiency in CORD-19\n\n## Abstract\nThe \"COVID-19 Open Research Dataset Challenge\" (CORD-19) is a call to action \"to develop text and data mining tools that can help the medical community develop answers to high priority scientific questions\" from a rapidly growing body of literature. Using \"bag-of-word\" text searches on large document sets is very popular, but often produces very large result sets. Coping with large result sets by using a one-size-fits-all ranking algorithm and paginated display of results is inferior to using more targeted search terms which are sufficient to limit results without missing relevant documents.  We introduce an improved methodology designed to provide searchers \n- a means to search titles and abstracts of the CORD-19 dataset, (which avoids the risks and limitations of bag-of-words searches on high-context technical prose in most scientific literature), \n- a means to view collated results displayed under MeSH subject headings, \n- a means to deliberately and confidently choose to exclude documents under subject headings that are irrelevant to their purposes, \n- a means to locate other documents at the relevant MeSH subject headings which can be used to suggest additional or more targeted search terms, and \n- a means to consider documents that were were omitted.\n\nThe ability to produce a manageable, custom-tailored full result set without the risks of general-purpose ranking and pagination allows the quality of the search to be evaluated rather than assumed.\n\n## Background\nMechanical algorithms that attempt to estimate relevance (such as TF-IDF, number of citations) are popular and well-tolerated, but they always come with the risk of over-weighting irrelevant results, marginalizing relevant results, and failing to show what similar results were omitted. \n\nI consider the CORD-19 success critiera to mean \"locating a precise, trustworthy answer in the CORD-19 dataset.\"  It is not enough to produce \"an answer\", it must be a \"correct answer\" and there must be some way to be confident it is correct. Establishing confidence requires human evaluation, first in selecting which search results to access articles later, reading and understanding the article and evaluating it for suitability. In this notebook, we introduce tools to leverage the human activities which are already present that are not amenable to being easily automated:\n\n- The author's effort to write careful titles and abstracts with descriptive terms used in low-context.\n- The publisher's effort to identify and provide accurate and appropriate MeSH subject headings and metadata to MEDLINE\n- The searcher's effort to select accurate and appropriate descriptive terms for searching.\n- The searcher's effort to review results for relevancy and consider what was missed and what was inadvertently included.\n\nMethodology and a short example are provided in this notebook. Longer examples are provided in the separate notebooks being used to make CORD-19 task submissions using this methodology, in order to avoid duplicated content.\n\n## Description of Existing (unimproved) Search Methodology\n\nA diagram of the existing search methodology can be presented as follows:\n\n","cc89faf2":"## Example of Formatted search results","4ac251e2":"![existingmethodology.png](attachment:existingmethodology.png)","d8efaa22":"While the author and publishing efforts are depicted in the same column in this diagram, they are certainly distinct in many respects. Prior to 2015, the MeSH tagging was performed by trained librarians at the NLM, which generally results in tighter control over vocabulary and subject usage. But the tagging done since then appears to maintain a high-level of consistency and accuracy.\n\nThe NLM still maintains the MeSH subject headings, and the current version (2020) was used for this work.\n\n## New Improvements to the Retrieval Process\nMy approach is to focus on improving the efficiency and outcomes in the searcher's side of the retrieval process. (While future work could improve the author and publisher efforts, that is outside the scope of the Kaggle project, which must work with the existing dataset.) After considering the task and the state-of-the-art in natural language understanding, we tend to favor improvements which augment and enhance human capabilities rather than automate and eliminate them entirely. \n\n\nI focus on the following aspects of the searcher's efforts:\n<ol><li> reduce risks that a non-trustworthy source is used to provide an answer. This might happen in three ways:\n\t<ol><li>A source can be non-trustworthy (e.g. unscientific, or even a work of complete fiction), or\n\t<li>A source can be trustworthy, but used in a way that is not trustworthy (e.g. taking a result out of context.)\n\t<li>Superior trustworthy sources are missed and inferior sources are located, (e.g. best source inadvertently ranked too low)\n\t<\/ol>\n<li> provide means to work with an entire result set, not a ranked, paginated result set\n<li> provide means to locate alternative resources which may have confirmation or alternative (potentially competing) answers and possibly suggest improved search terms \n<\/ol>\n\n","e307063b":"# Technical Summary: Dataset acquisition and preparation\n<h2>1. Obtaining MEDLINE metadata for CORD-19 data set<\/h2>\nThe MEDLINE metadata which includes MH-type (MeSH heading) records was obtained as follows:\n<br>\n1. By re-running the Pubmed <a href=\"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/?term=%22COVID-19%22+OR+Coronavirus+OR+%22Corona+virus%22+OR+%222019-nCoV%22+OR+%22SARS-CoV%22+OR+%22MERS-CoV%22+OR+%E2%80%9CSevere+Acute+Respiratory+Syndrome%E2%80%9D+OR+%E2%80%9CMiddle+East+Respiratory+Syndrome%E2%80%9D\">CORD-19<\/a>\n<p>\n2. The details shown in the side bar are:\n<\/p><blockquote>\n\"COVID-19\"[All Fields] OR (\"coronavirus\"[MeSH Terms] OR \"coronavirus\"[All Fields]) OR \"Corona virus\"[All Fields] OR \"2019-nCoV\"[All Fields] OR \"SARS-CoV\"[All Fields] OR \"MERS-CoV\"[All Fields] OR \"Severe Acute Respiratory Syndrome\"[All Fields] OR \"Middle East Respiratory Syndrome\"[All Fields]\n<\/blockquote>\n<p>\n\n3. The sidebar has \"Find Related Data\". This truncates at 10000 items, so we need to use date ranges to limit the results.\nThe date ranges used were: 1900-2006, 2007-2011, 2012-2014, 2015-2017, 2018-2020, and all are less than 10000 items.\n<\/p><p>\n\nClicking \"Find Related Data\" and asking for MEDLINE format download gets the format we want.\n<\/p><p>\n\n4. The files were concatenated to a 130MB file for use in later steps.\n<\/p>","fb048bc4":"The changes to search methodology compared to the previous diagram are an additional column for pre-processing the CORD-19 metadata in bulk, and augmentation of the human-controlled search process on the right side. The most important change is that instead of getting a paginated set of search results, results are collated and displayed within MeSH headings.  \n\n","56ac7bfb":"# Process to responding to CORD-19 Task Questions\nThe specific responses are created by doing a term search on the AB (Abstract) and TI (Title) fields of the MEDLINE metadata. Devising search terms is still a manual process. Once the terms are determined, there is no web interface to do this search. (The web interface is for showing results is limited to one MeSH descriptor at a time.)\n\nA javascript program (searchPubmed3.js, run using NodeJS) loads in the MEDLINE concatenated dataset and creates an array of objects in memory. The stdin stream is read for lines of search terms. As each line is read in, a linear scan of the objects is performed. Search results are written out to the console, grouping results by most common to least common primary MeSH headings. In this way, results are displayed next to adjacent results. As discussed in Pros and Cons, date range limits are very important.\n\nThe results are formatted with deep links to PubMed (for articles) and the MeSH results explorer (for MeSH descriptors.)\n\nThe process to confirm that a result is trustworthy requires that the user be diligent to review the formatted results in the context of other articles in the same subject area, spending some time with the results browser to verify that there was nothing missed in the subject, and considering what related subjects would also be candidate places to look. ","6b56235a":"# Collation of CORD-19 MEDLINE data to separate files for each MeSH Descriptor\n\nThe results explorer is a javascript application that runs in a web browser. The full set of capabilities and requirements is discussed in a separate section, but some description is necessary here in order to explain the purpose of having a collated datafile per MeSH dscriptor.\n\nThe results explorer always shows results inside the context of one MeSH subject descriptor. This supports the ability to see what alternative results could have been provided. All of the MEDLINE data needed to display the results is collated to one datafile per MeSH descriptor name, and this datafile is fetched at the time the browser page is loaded and when the window.location is changed to show a different MeSH description.\n\nStarting at the top of the explorer page, there are definitions and notes for the MeSH descriptors. These come from the NLM data. Below that are links to other MeSH descriptor pages that are related, narrower, and wider (more general). (These appear above the results section.)\n\nIt is important to keep in mind that a MeSH descriptor exists in more than one MeSH tree simultaneously. The user interface must support and indicate that there are multiple parents for many of the descriptors. (It is not just a simple \"bread-crumb\" path to arrive at a descriptor.)\n\nThe count of items at the target link is shown in parenthesis at the link.\n\nSo in addition to the MEDLINE article data, the metadata for the MeSH subject and related links must be available to display the page. All of this extra datails placed at the head of the data file which has all of the MEDLINE metadata. In short, everything that is needed to be displayed is in that one file, which are plain text following MEDLINE formats. I used a file extension of .pmidx.\n\nThe .files are created by a custom javascript program (parsePubmed2.js, run with NodeJS) which takes the source data (downloaded MEDLINE data, the processed MeSH descriptor data (desc2020d.tsv) and article count data (pubmed.tally from a previous run)) and produces individual files under a simple tree of folder-names (E.g. \/C\/CO\/Cornavirus Infections.pmidx.)\n\nThe largest datafile for CORD-19 is \"\/C\/CO\/Coronavirus Infections.pmidx\". It is 9.6MB and there are 2485 items in the file. Most index files are much smaller than this.\n\nWriting .pmidx files is implemented a single-pass operation. The tallies of article counts and children are not available until the end of processing, and these are written to a small index file in tab-separated-value format. If the content of this output file has changed compared to previous runs, the file needs to be copied to pubmed.tally and the operation must be re-run. (If this is not done, the article counts displayed at MeSH subject links in the browser will be wrong.) ","a0ea980a":"# Preparation of MeSH descriptor data\nAs part of our approach we want to display responses to questions within the context of MeSH descriptors, alongside other articles from the dataset.\n\nThe MeSH subject headings were obtained by downloading the 2020 .XML file https:\/\/www.ncbi.nlm.nih.gov\/mesh\n\nXML file is a verbose file format and includes fields which are not useful. A javascript program (parseMesh.js) was run (using NodeJS) to converted the XML to a custom-format Tab-separated-value (TSV) file. The output has four different record types, and is saved to desc2020d.tsv\n\nThis is used a the MeSH index in later processing steps. \n\nThe XML original is 290MB, and the TSV index is 20MB. "}}