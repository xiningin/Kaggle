{"cell_type":{"40d68307":"code","f7683482":"code","7bc8c4e0":"code","3ec1622c":"code","f7268e72":"code","f1fa2235":"code","b51f6b52":"code","34fa3af9":"code","a2f1299d":"code","b2a82671":"code","69759421":"code","d4ca4190":"code","df709ae8":"code","74051244":"code","788358e4":"code","6629ea7a":"code","1dd90024":"code","79e5fc16":"code","8260bdf9":"code","581019ba":"code","57c75da2":"code","113c5655":"code","5def7f57":"code","68dcb820":"code","111bd442":"code","bab6d133":"code","26abdb63":"code","b1638b91":"code","1bbbfcd2":"code","1fa7afa5":"code","5fa85850":"code","b5644814":"code","78b0b30e":"code","97b02c21":"code","69f6295c":"code","6ee24b63":"code","285fd7db":"code","8298ba6b":"code","f355f446":"code","c775455e":"markdown","104855ae":"markdown","5d24645e":"markdown","dac4e6c1":"markdown","bca475a4":"markdown","b01f8074":"markdown","fb8bb28c":"markdown","263838a1":"markdown","230f95ad":"markdown","76371afb":"markdown","a4717cb0":"markdown","6c3a4e16":"markdown","82eb19d5":"markdown","c6f8bc83":"markdown","c3e115c8":"markdown","9c5e78f0":"markdown","315b05bf":"markdown","7ea9f132":"markdown","c741e7f9":"markdown","fff561cb":"markdown","5de396e9":"markdown","3d51f407":"markdown","4c91fc8e":"markdown","dd34e39f":"markdown","d16f52a7":"markdown","214881a2":"markdown","64147cb5":"markdown","cf616665":"markdown","34d40828":"markdown","416b51c1":"markdown","3dc6282c":"markdown","1503f1cd":"markdown","1297d621":"markdown"},"source":{"40d68307":"!pip install tensorflow==1.11.0\n!pip install --upgrade git+https:\/\/github.com\/goolig\/dsClass.git\nimport dsClass","f7683482":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nrootdir_prefix = '\/' if os.getcwd()=='\/kaggle\/working' else ''  # in kaggle os.getcwd() should be '\/kaggle\/working'\n    \nfrom dsClass.align_custom import AlignCustom\nfrom dsClass.face_feature import FaceFeature\nfrom dsClass.mtcnn_detect import MTCNNDetect\nfrom dsClass.tf_graph import FaceRecGraph\nfrom dsClass.path_helper import *\n#from helper import vid2vid_audio_transfer\n\nimport sys\nimport glob\nimport cv2\nimport json\nimport time\nimport scipy\nimport urllib\nimport matplotlib.pyplot as plt","7bc8c4e0":"# Fixes the issue with np.load version above 1.16 that defaults np.load allow_pickle to False.\n# this snippet makes old calls to np.load which doesn't specify allow_pickle=True to work with newer versions of numpy.\nfrom functools import partial\ndef np_load_fix(load_func):\n    return partial(load_func , allow_pickle=True)\n\nnp.load = np_load_fix(np.load)","3ec1622c":"from IPython.display import YouTubeVideo\nYouTubeVideo('NuhCoO6GO5U')","f7268e72":"def annotate_face(rect, recog_data, frame):\n    \"\"\"\n    Draw a box around the face and label the person\n    :param rect : the face bounding box\n    :param recog_data : tuple of person name and confidence percentage (e.g: ('Jaime', 0.8))\n    :param frame : the frame to draw on\n    \"\"\"\n    shrtname = short_name(recog_data[0])\n    acc = round(recog_data[1], 1)\n    bbox_color = (255, 255, 255) if \"Unknown\" in recog_data[0] else (124,252,0)  # RGB\n\n    #draw bounding box \/ fancy border for the face\n    #cv2.rectangle(frame,(rect[0],rect[1]),(rect[0] + rect[2],rect[1]+rect[3]),bbox_color) \n    draw_border(frame, (rect[0],rect[1]), (rect[0] + rect[2],rect[1]+rect[3]), bbox_color, 1, 10, 10)  \n    anot_text = shrtname + \"-\" + str(acc) + \"%\"\n    cv2.putText(frame, anot_text,\n                (rect[0]-4,rect[1]-4), cv2.FONT_HERSHEY_SIMPLEX,0.35,\n                bbox_color, 1, cv2.LINE_AA)                        \n\n    \ndef short_name(name):\n    \"\"\"e.g: Jaime Lannister -> Jaime.L\"\"\"\n    if not name.startswith(\"Unknown\"):\n        name_split = name.split(\" \")\n        short_name = name_split[0]\n        if len(name_split) > 1:\n            short_name = short_name + \".\" + name_split[1][0]\n        return(short_name)\n    else:\n        return name\n    \n    \ndef draw_border(img, pt1, pt2, color, thickness, r, d):\n    \"\"\"\n    Fancy box drawing function by Dan Masek\n    Code in: https:\/\/www.codemade.io\/fast-and-accurate-face-tracking-in-live-video-with-python\/\n    \"\"\"\n    x1, y1 = pt1\n    x2, y2 = pt2\n \n    # Top left drawing\n    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)\n    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\n \n    # Top right drawing\n    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)\n    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\n \n    # Bottom left drawing\n    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)\n    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\n \n    # Bottom right drawing\n    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)\n    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness) \n    \n    \ndef read_image_from_url(url2read):\n    if url2read.startswith('http') or url2read.startswith('data'):\n        req = urllib.request.urlopen(url2read, timeout=5)\n        arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n        img = cv2.imdecode(arr, -1) # 'Load it as it is'\n    else:\n        img = cv2.imread(url2read)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return(img)","f1fa2235":"if not os.path.exists(os.path.join(rootdir_prefix, 'kaggle\/working', 'haarcascade_frontalface_default.xml')):\n    !wget https:\/\/github.com\/opencv\/opencv\/raw\/master\/data\/haarcascades\/haarcascade_frontalface_default.xml -P \/kaggle\/working","b51f6b52":"#Create the haar cascade\nface_cascade = cv2.CascadeClassifier(os.path.join(rootdir_prefix, 'kaggle\/working', 'haarcascade_frontalface_default.xml'))\n\ndef find_faces_in_image(orig_img, scaleFactor, minNeighbors, minSize, maxSize):\n    orig_img_copy = orig_img.copy()\n    gray = cv2.cvtColor(orig_img_copy, cv2.COLOR_BGR2GRAY)\n    #plt.imshow(gray) \n    \n    # Detect faces in the image\n    faces = face_cascade.detectMultiScale(\n        gray,           \n        scaleFactor=scaleFactor, \n        minNeighbors=minNeighbors,  \n        minSize=minSize, \n        maxSize=maxSize \n    )\n    \n    print(\"Found {0} faces!\".format(len(faces)))\n\n    # Draw a rectangle around the faces\n    for (x, y, w, h) in faces:\n        cv2.rectangle(orig_img_copy, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n    plt.imshow(orig_img_copy)","34fa3af9":"img  = read_image_from_url(\"https:\/\/static.independent.co.uk\/s3fs-public\/thumbnails\/image\/2018\/05\/10\/12\/game-of-thrones-finale.jpg?w968h681\")\nscaleFactor = 1.3\nminNeighbors = 5\nminSize = (60, 60)   \nmaxSize = (70, 70)\nfind_faces_in_image(img, scaleFactor, minNeighbors, minSize, maxSize)","a2f1299d":"def find_faces_in_frame_of_video(frame, scaleFactor, minNeighbors, minSize, maxSize):\n    \"\"\"Find faces in an image and returns a list of (x, y, w, h) tuples (the bounding boxes of them)\"\"\"\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        \n    # Detect faces in the image\n    faces = \n    return faces","b2a82671":"def video_file_recog_haar(src_filename, output_filename=rootdir_prefix+'kaggle\/working\/output_haar.mp4', framerate=None, scaleFactor=1.3, minNeighbors=5, minSize=60, maxSize=70):\n    print(\"[INFO] Reading video file...\")\n    if glob.glob(src_filename):\n        vs = cv2.VideoCapture(src_filename); #get input from file\n    else:\n        print(\"file does not exist\")\n        return\n    \n    print(\"[INFO] Initializing video writer...\")\n    frame_width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # DIVX, XVID, MJPG, X264, WMV1, WMV2, mp4v\n    if framerate is None:\n        framerate = vs.get(cv2.CAP_PROP_FPS) # use same as input video, or can set to 20.0 \/ 30.0\n    out = cv2.VideoWriter(output_filename, fourcc, framerate, (frame_width,frame_height))\n    \n    recog_list = []\n    frame_counter = 0\n    t0 = time.time()\n    while True:        \n        ret, frame = vs.read();\n        if ret:\n            frame_counter += 1\n            if frame_counter%(30\/framerate)==0:\n                min_face_size = 60 #min face size is set to 60x60\n                rects = find_faces_in_frame_of_video(frame, scaleFactor, minNeighbors, (minSize, minSize), (maxSize,maxSize))\n                print(\"\\rNumber of faces found in frame \" + str(frame_counter) + \":\",len(rects), end='')\n                aligns = []\n                positions = []\n                for (i, rect) in enumerate(rects):\n                    draw_border(frame, (rect[0],rect[1]), (rect[0] + rect[2],rect[1]+rect[3]), (255,255,255), 1, 10, 10)\n                    cv2.putText(frame,\"Unknown\",\n                                        (rect[0]-4,rect[1]-4),cv2.FONT_HERSHEY_SIMPLEX,0.35,\n                                        (255,255,255),1,cv2.LINE_AA)\n\n\n                out.write(frame)\n        else:  # end of video, no more frames\n            break\n    \n    elapsed_time = time.time() - t0\n    print()\n    print(\"[exp msg] elapsed time for going over the video: \" + str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n    vs.release()\n    out.release()\n    cv2.destroyAllWindows()\n    print(\"Done\")","69759421":"video_file_recog_haar(src_filename = get_file_path(\"Game of Thrones 7x07 - Epic Daenerys Dragonpit Entrance.mp4\"), \n                      framerate=10,\n                      scaleFactor=1.3,\n                      minNeighbors=5,\n                      minSize=60,\n                      maxSize=70)","d4ca4190":"#Global Variables\nperson_embeddings = None","df709ae8":"def augment_image(img):\n    aug_images = []\n    flip_img = cv2.flip(img, 1)  #https:\/\/docs.opencv.org\/2.4\/modules\/core\/doc\/operations_on_arrays.html?highlight=flip#cv2.flip\n    # for more example see https:\/\/github.com\/aleju\/imgaug\n    aug_images.append(flip_img)\n    return(aug_images)\n\n\ndef get_person_imgs(urls, min_face_size=40):\n    \"\"\"\n    Given a list of URLs of a person images, this function will extract and align the faces within the image and detect its pose (left\/right\/center)\n    \"\"\"\n    person_imgs = {\"Left\" : [], \"Right\": [], \"Center\": []};\n    person_imgs_count = {\"Left\" : 0, \"Right\": 0, \"Center\": 0};\n    \n    counter_break = 0\n    while True:    \n        for url2read in urls:\n            #print(file)\n            #ret, frame = vs.read()\n            #img = cv2.imread(file)\n            img = read_image_from_url(url2read) # ****** file = url2read\n            if img is None:\n                print(\"********************* image was not loaded ***********************\")\n                continue\n\n            # Augmenting the data - add a flipped version of the image to add more data\n            frames = [img]\n            frames.extend(augment_image(img))\n\n            for frame in frames:\n                if True: #ret:\n                    rects, landmarks = face_detect.detect_face(frame, min_face_size)\n                    #print(\"rects\", rects)\n                    for (i, rect) in enumerate(rects):\n                        aligned_frame, pos = aligner.align(160, frame,landmarks[i]);\n                        #print(pos)\n                        person_imgs_count[pos]+=1\n                        if len(aligned_frame) == 160 and len(aligned_frame[0]) == 160:\n                            person_imgs[pos].append(aligned_frame)\n                            #cv2.imshow(\"Captured face\", aligned_frame)\n                            #cv2.imwrite(\"..\/data2\/frame%d.jpg\" % count, aligned_frame)\n                else:\n                    break\n            \n        if person_imgs_count[\"Left\"] == 0 or person_imgs_count[\"Right\"] == 0 or person_imgs_count[\"Center\"] == 0:\n            counter_break+=1\n            if counter_break > 0:\n                print(person_imgs_count) \n                assert 0==1, \"Must get all poses of a face: Left, Right and Center, try adding more images\"\n                return None\n        else:\n            break\n                            \n    print(person_imgs_count)    \n    return(person_imgs)  ","74051244":"def extract_embeddings_from_images(min_face_size=40, embeddings_filename=rootdir_prefix+'kaggle\/working\/facerec_128D.txt'):\n    \"\"\" \n    Go over all urls, extract and align faces, feed each face to the embeddings net,\n    and saves an embedding vector for each person-position pair.\n    Save all embeddings to a .txt file\n    \"\"\"\n    print()\n    print(\"[INFO] Extracting data from images ...\")\n    data_set = dict()\n\n    for new_name in dict_faces.keys():\n        person_features = {\"Left\" : [], \"Right\": [], \"Center\": []};\n        print(\"Extracting:\", new_name)\n        print(\"number of img files:\",len(dict_faces[new_name]))\n        person_imgs = get_person_imgs(dict_faces[new_name], min_face_size=min_face_size) \n        if person_imgs is None:\n            print(\"extraction of:\",new_name, \" failed\")\n            continue\n        \n        print(\"extracted person_imgs from:\",new_name)\n        print(\"-------------------------------------\")\n\n        for pos in person_imgs: # there are some exceptions here, but I'll just leave it as this to keep it simple\n            person_features[pos] = [np.mean(extract_feature.get_features(person_imgs[pos]), axis=0).tolist()]\n        data_set[new_name] = person_features;\n    \n    global person_embeddings\n    person_embeddings = data_set\n    with open(embeddings_filename, 'w+') as f:\n        f.write(json.dumps(data_set))\n    \n\ndef load_embeddings_from_file(embeddings_filename=rootdir_prefix+'kaggle\/working\/facerec_128D.txt'):\n    global person_embeddings\n    with open(embeddings_filename, 'r') as f:\n        person_embeddings = json.loads(f.read());\n\n        \ndef identifyPerson(features_arr, position, thres = 0.6, percent_thres = 70):\n    '''\n    :param features_arr: a list of 128d Features of a face\n    :param position: face position types (Left\/Right\/Center)\n    :param thres: distance threshold\n    :param percent_thres : minimum confidence required to identify a person\n    :return: tuple of person name and confidence of detection\n    '''\n    assert person_embeddings is not None, \"Must load or extract persons embeddings in order to recgonize persons\"\n    result = \"Unknown\"\n    smallest = sys.maxsize  # initialize with a large number\n    for person in person_embeddings.keys():\n        person_data = person_embeddings[person][position]\n        for data in person_data:  # in our case there's only one embedding per person-position pair\n            distance = scipy.spatial.distance.euclidean(data, features_arr)  # same as: np.sqrt(np.sum(np.square(data-features_arr)))\n            #distance = scipy.spatial.distance.cosine(data, features_arr)  # if using cosine distance it is recommended to lower the thres to ~0.4\n            \n            if(distance < smallest):\n                smallest = distance\n                result = person\n    percentage =  min(100, 100 * thres \/ smallest)\n    if percentage <= percent_thres:\n        result = \"Unknown (%s)\" % result.split(' ')[0]  # show highest score person for debug purposes\n    return (result, percentage)","788358e4":"model_path = rootdir_prefix + 'kaggle\/input\/model-backup2\/model-20170512-110547.ckpt-250000' ","6629ea7a":"# initalize\nFRGraph = FaceRecGraph();\naligner = AlignCustom();\nextract_feature = FaceFeature(FRGraph, model_path = model_path);\nface_detect = MTCNNDetect(FRGraph, scale_factor=2); #scale_factor, rescales image for faster detection","1dd90024":"dict_faces = dict()\ndict_faces[\"Jaime Lannister\"] = [\"https:\/\/s2.r29static.com\/\/bin\/entry\/97f\/340x408,85\/1832698\/image.jpg\",\n                                 \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/b\/b4\/Jaime_Lannister-Nikolaj_Coster-Waldau.jpg\/220px-Jaime_Lannister-Nikolaj_Coster-Waldau.jpg\",\n                                 \"https:\/\/upload.wikimedia.org\/wikipedia\/pt\/thumb\/0\/06\/Nikolaj-Coster-Waldau-Game-of-Thrones.jpg\/220px-Nikolaj-Coster-Waldau-Game-of-Thrones.jpg\",\n                                 \"https:\/\/purewows3.imgix.net\/images\/articles\/2017_09\/jaime-lannister-season-7-game-of-thrones-finale1.jpg?auto=format,compress&cs=strip&fit=min&w=728&h=404\",\n                                 \"https:\/\/cdn.newsday.com\/polopoly_fs\/1.13944684.1502107079!\/httpImage\/image.jpeg_gen\/derivatives\/landscape_768\/image.jpeg\",\n                                 \"https:\/\/www.cheatsheet.com\/wp-content\/uploads\/2017\/08\/Jaime-Lannister-Game-of-Thrones.png\",\n                                 \"https:\/\/fsmedia.imgix.net\/9c\/c0\/27\/10\/15e0\/44a4\/8ecb\/9339993b563d\/nikolaj-coster-waldau-as-jaime-lannister-in-game-of-thrones-season-7.png?rect=0%2C0%2C1159%2C580&dpr=2&auto=format%2Ccompress&w=650\",\n                                 \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQrIQuBKKUocAizwfWtIdhAcvfowLJatKqqDsO3ywYdh3rv-mBk\",\n                                ]\n\ndict_faces['Cersei Lannister'] = ['https:\/\/cdn.pastemagazine.com\/www\/articles\/CERSEI-LANNISTER-quotes-list.jpg',\n                                 ]","79e5fc16":"# Check urls and print last image\n# if any image causes problems (like error 403 forbidden) it will remove it and choose another\nfor p in dict_faces.keys():\n    urls = dict_faces[p]\n    valid_urls = []\n    for url2read in urls:\n        print('\\rChecking: %s' % url2read[:100], ' '*100, end='')\n        try:\n            img = read_image_from_url(url2read)\n            valid_urls.append(url2read)\n        except:\n            print(\"\\nERROR: Couldn't fetch %s, skipping this one\" % url2read)\n    dict_faces[p] = valid_urls\nplt.imshow(img);","8260bdf9":"#load_embeddings_from_file()\nextract_embeddings_from_images(min_face_size=40)","581019ba":"url = 'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/best-game-of-thrones-season-8-fan-theories-1554917935.jpg'\nframe = read_image_from_url(url)\nframe = cv2.resize(frame, (640, 480))\nrects, landmarks = face_detect.detect_face(frame, minsize=40);  # min face size is set to 80x80\n\nfor i, rect in enumerate(rects):\n    #draw_border(frame, (rect[0],rect[1]), (rect[0] + rect[2],rect[1]+rect[3]), (255,255,255), 2, 10, 10)\n    annotate_face(rect, ('Face %i' % i, 100), frame)\n    \nplt.subplots(figsize=(15,10))\nplt.imshow(frame);","57c75da2":"idx = 0\nrect = rects[idx]\nplt.imshow(frame[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]])\nfor k in range(int(len(landmarks[idx]) \/ 2)):\n    plt.plot(landmarks[idx][k]-rect[0], landmarks[idx][k+5]-rect[1], 'r+', markersize=20)","113c5655":"for (i, rect) in enumerate(rects):\n    aligned_frame, pos = aligner.align(160, frame, landmarks[i])\n    plt.subplot(121)\n    plt.imshow(frame[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]])\n    plt.title('Original')\n    plt.subplot(122)\n    plt.imshow(aligned_frame)\n    plt.title('Aligned')\n    plt.suptitle('Position: %s' % pos)\n    plt.show()","5def7f57":"features_vector = extract_feature.get_features([aligned_frame])","68dcb820":"features_vector.shape","111bd442":"features_vector","bab6d133":"def frame_face_recog(frame, min_face_size=80, percent_thres = 70, verbose=False):\n    \"\"\" \n    Detect faces in a frame, try to recgonize them, and draws a box around the face with predicted person + % confidence\n    :param frame : the frame to indentify faces in. an array with shape of width X height X channels\n    :param min_face_size : minimum size of face to detect. integer. e.g: value of 80 is set to 80x80 pixels\n    :param verbose : True to print debug information while running\n    \n    Alters inplace the frame with predictions annotations\n    returns a list of (person,confidence) tuples\n    \"\"\"\n    \n    # Detect all faces in frame and get their bounding-rectangles and landmarks\n    rects, landmarks = \n    \n    # Go through each face in frame and perform:\n    recog_list = []\n    for <something> in <something>:\n        if verbose: print('BBox %i:' % i, end=' ')\n        #  1) align the face (using aligner.align() function). remember that aligner.align() returns the aligned-face and face-pose (left\/right\/center)\n        aligned_face, face_pos = \n        #  2) extract aligned face features (embeddings)\n        features_arr = \n        #  3) find the person the face belongs to\n        recog_data = \n        recog_list.append(recog_data)\n        if verbose: print(\"recog_data\", str(recog_data))\n        #  4) draw a box around the face and label the person    \n            \n    return recog_list","26abdb63":"url = 'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/best-game-of-thrones-season-8-fan-theories-1554917935.jpg'\nframe = read_image_from_url(url)\nframe = cv2.resize(frame, (640, 480))\nframe_face_recog(frame, min_face_size=80, percent_thres = 70)\nplt.subplots(figsize=(15,10))\nplt.imshow(frame);","b1638b91":"def video_file_recog(src_filename, output_filename=rootdir_prefix+'kaggle\/working\/output.mp4', percent_thres = 70, verbose=False):\n    print(\"[INFO] Reading video file...\")\n    if glob.glob(src_filename):\n        vs = cv2.VideoCapture(src_filename); #get input from file\n    else:\n        print(\"file does not exist\")\n        return\n    \n    print(\"[INFO] Initializing video writer...\")\n    frame_width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # DIVX, XVID, MJPG, X264, WMV1, WMV2, mp4v\n    framerate = vs.get(cv2.CAP_PROP_FPS) # use same as input video, or can set to 20.0 \/ 30.0\n    out = cv2.VideoWriter(output_filename, fourcc, framerate, (frame_width,frame_height))\n    \n    recog_list = []\n    frame_counter = 0\n    t0 = time.time()\n    while True:        \n        ret, frame = vs.read();\n        if ret:\n            frame_counter += 1\n            print('\\rProcessing Frame %i\/%i' % (frame_counter, total_frames), end=' ')\n            recog_data = frame_face_recog(frame, min_face_size=40, percent_thres=percent_thres, verbose=verbose)\n            recog_list.extend(recog_data)\n            #cv2.imshow(\"Frame\",frame)\n            #cv2.imwrite(\"..\/data3\/frame%d.jpg\" % count, frame)\n            out.write(frame)\n        else:  # end of video, no more frames\n            break\n    \n    elapsed_time = time.time() - t0\n    print()\n    print(\"[exp msg] elapsed time for going over the video: \" + str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n    vs.release()\n    out.release()\n    cv2.destroyAllWindows()\n    \n    known_counter = len([1 for recog in recog_list if recog[1] > percent_thres])\n    unknown_counter = len(recog_list) - known_counter\n    print(\"known_counter:\", known_counter, \"unknown_counter:\", unknown_counter)\n    \n    print()\n    print(\"Done\")","1bbbfcd2":"video_file_recog(src_filename = get_file_path(\"Game of Thrones 7x07 - Epic Daenerys Dragonpit Entrance.mp4\"), percent_thres=70, verbose=False)","1fa7afa5":"# the output file from opencv is just video frames (Without audio). lets add the original audio track to the output movie\n# if ffmpeg is not installed then it will return False\n#vid2vid_audio_transfer('Game of Thrones 7x07 - Epic Daenerys Dragonpit Entrance.mp4', 'output.mp4', 'output_w_audio.mp4');","5fa85850":"!ls ..\/working\/ -ashl","b5644814":"from dsClass.srganUnified import SRGAN\nimport tensorflow as tf","78b0b30e":"def infer(x_test, ground_truth, titles, save_results=False, display=True, model_file=rootdir_prefix+'kaggle\/input\/srgan-models-tutorial\/srganA-epoch300'):\n    \"\"\" x_test should be in shape of (batch_size, 24, 24, 3). images in BGR (not RGB)\n    and ground_truth is same as x_test, just (batch_size, 96, 96, 3)\n    ground_truth is only used for displaying, and not for inferring.\n    \"\"\"\n    x = tf.placeholder(tf.float32, [None, 24, 24, 3])\n    is_training = tf.placeholder(tf.bool, [])\n\n    print('Initializing Model')\n    model = SRGAN(x, is_training, batch_size=len(x_test), infer=True)\n\n    print('Loading model checkpoint')\n    # Restore the SRGAN network\n    saver = tf.train.Saver()\n    saver.restore(sess, model_file)\n\n    print('Inferring')\n    # Infer\n    raw = x_test.astype('float32')\n    fake = sess.run(\n        model.imitation,\n        feed_dict={x: raw, is_training: False})\n    save_img([raw, fake, ground_truth], ['Input', 'Output', 'Ground Truth'], titles, save=save_results, display=display)\n    print('Done')\n\n    \ndef save_img(imgs, label, titles, save=False, display=True):\n    for i in range(len(imgs[0])):\n        seq_ = \"{0:04d}\".format(i+1)\n        fig = plt.figure()\n        for j, img in enumerate(imgs):\n            im = np.uint8((img[i]+1)*127.5)\n            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n            fig.add_subplot(1, len(imgs), j+1)\n            plt.imshow(im)\n            plt.tick_params(labelbottom='off')\n            plt.tick_params(labelleft='off')\n            plt.gca().get_xaxis().set_ticks_position('none')\n            plt.gca().get_yaxis().set_ticks_position('none')\n            plt.xlabel(label[j])\n            if j==1:\n                plt.title(titles[i])\n    \n        path = os.path.join('result', '{}.jpg'.format(titles[i]))\n        if save:\n            plt.savefig(path)\n        if display:\n            plt.show()\n        if save:\n            plt.close()","97b02c21":"def downscale_func(x):\n        if len(x.shape)==3:\n            x = np.expand_dims(x, axis=0)\n        K = 4\n        arr = np.zeros([K, K, 3, 3])\n        arr[:, :, 0, 0] = 1.0 \/ K ** 2\n        arr[:, :, 1, 1] = 1.0 \/ K ** 2\n        arr[:, :, 2, 2] = 1.0 \/ K ** 2\n        weight = tf.constant(arr, dtype=tf.float32)\n        downscaled = tf.nn.conv2d(\n            x, weight, strides=[1, K, K, 1], padding='SAME')\n        return downscaled\n\n\ndef process_file(filename, downscale='cv2'):\n    \"\"\" downscale can be 'cv2' \/ 'conv' \n    \"\"\"\n    img = cv2.imread(filename)\n    return process_img_arr(img[:,:,::-1], filename, downscale)\n\n\ndef process_img_arr(img, name, downscale='cv2'):\n    face = img[:,:,::-1].copy()\n      \n    if face.shape[0] > 96:\n        ground_truth = cv2.resize(face, (96, 96))\n    else:\n        ground_truth = face\n    \n    if downscale=='cv2':\n        face = cv2.resize(face, (24, 24))\n    elif downscale=='conv':\n        gt4conv = cv2.resize(face, (96, 96))\n        downs = downscale_func(gt4conv.astype('float32'))\n        face = sess.run(downs)\n    \n    ground_truth = ground_truth \/ 127.5 - 1\n    face = face \/ 127.5 - 1\n    input_ = np.zeros((1, 24, 24, 3))\n    input_[0] = face\n    \n    return input_, ground_truth","69f6295c":"face_urls = [\n             rootdir_prefix+'kaggle\/input\/srgan-models-tutorial\/srgan-pic1.jpg',\n             rootdir_prefix+'kaggle\/input\/srgan-models-tutorial\/srgan-pic2.jpg',\n             rootdir_prefix+'kaggle\/input\/srgan-models-tutorial\/srgan-pic3.jpg',\n             'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/best-game-of-thrones-season-8-fan-theories-1554917935.jpg',\n             'https:\/\/i.redd.it\/mm9sgp28ri811.jpg',\n             'https:\/\/cdn.pastemagazine.com\/www\/articles\/CERSEI-LANNISTER-quotes-list.jpg',\n]","6ee24b63":"def urls2imgarr(urls, crop_faces=True):\n    \"\"\" \n    Go over all urls. for each url, fetch the image, and if crop_faces is True then extract and crop all faces in image.\n    returns a list of image arrays. each item in the list is of shape of (width, height, 3). width and height of images can vary, they will be resized later on in process_img_arr()\n    \"\"\"\n    all_faces = []\n    for url in face_urls:\n        print('Fetching', url)\n        frame = \n        if crop_faces:\n            rects, _ = \n            for (i, rect) in enumerate(rects):\n                img_arr = frame[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]]\n                all_faces.append(img_arr)\n        else:\n            all_faces.append(frame)\n    print('Done.')\n    return all_faces\n\nall_faces = urls2imgarr(face_urls, crop_faces=True)\nprint('Fetched total of %i faces' % len(all_faces))","285fd7db":"tf.reset_default_graph()\nsess = tf.Session()\ninit = tf.global_variables_initializer() \nsess.run(init)\n\nx_test = []\nground_truth = []\ntitles = []\nfor (i, img_arr) in enumerate(all_faces):\n    x, gt = process_img_arr(img_arr, i, downscale='conv')\n    x_test.append(x)\n    ground_truth.append(gt)\n    titles.append(i)\n\nx_test = np.concatenate(x_test)","8298ba6b":"x_test.shape","f355f446":"tf.reset_default_graph()\nsess = tf.Session()\ninit = tf.global_variables_initializer() \nsess.run(init)\n\ninfer(x_test, ground_truth, titles, save_results=False, display=True, model_file=rootdir_prefix+'kaggle\/input\/srgan-models-tutorial\/srganA-epoch300')","c775455e":"Based and Inspired by:\n- **https:\/\/github.com\/vudung45\/FaceRec\n- Augmentation code: https:\/\/github.com\/vxy10\/ImageAugmentation\n- Fancy borders: https:\/\/www.codemade.io\/fast-and-accurate-face-tracking-in-live-video-with-python\/","104855ae":"# Part 2 - Super Resolution\n\n- OBJECTIVE - reconstruct a face from a low resolution image","5d24645e":"## Detect Faces Using MTCNN\n\n### Full process stages overview:\n- <b><u>Creating persons database (from google images) to compare our video faces to:<\/u><\/b>\n    - Extract faces using MTCNN (replacing the Haar features we used earlier)\n        - extracting from each frame the faces position and pose - center\/left\/right \n    - Augment images (synthetically creating more data)\n    - Aligning the images (for better face comparing matches)\n    - Feed faces to neural network to get 128D vector representation of the face (embeddings)\n    - Store the results (person name, face pose, 128D vector) in a list of all persons\n        - so later we will compare our video faces to that database\n\n\n- <b><u>Go over video frame by frame:<\/u><\/b>\n    - Extract faces using MTCNN\n    - Aligning the images (for better face comparing matches)\n    - Feed faces to neural network to get 128D vector representation of the face (embeddings)\n    - Compare each face vector to the vectors we have in the faces database we created earlier to find the best match\n        - if the match confidence (some distance metric) passes a chosen threshold then flag that face as the person we matched to","dac4e6c1":"### Landmarks","bca475a4":"Heavily based on https:\/\/github.com\/tadax\/srgan, which is an implementation of the SRGAN model proposed in the paper (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, https:\/\/arxiv.org\/abs\/1609.04802) with TensorFlow.  \nTrained on Labeled Faces in the Wild - http:\/\/vis-www.cs.umass.edu\/lfw\/","b01f8074":"## Fetching the data\nfeed in urls of images of faces","fb8bb28c":"## Setup and main functions","263838a1":"----------------","230f95ad":"## Face Recognition in Video using MTCNN, aligner and embedded vectors","76371afb":"### Face Detection","a4717cb0":"## Q4: Mini assignment\nComplete the following urls2imgarr() function.  \nuse the face detection methods you've used in part 1","6c3a4e16":"## Detection and Recgnition","82eb19d5":"# Part 1 - Objective - Face Detection in Video\n\nDetect and recognize the faces in the following youtube video:","c6f8bc83":"### Q2: Mini assignment\nComplete the following find_faces_in_frame_of_video() function so we can do face detection on the video (tip: see find_faces_in_image() function above)","c3e115c8":"## Detect Faces Using Haar Cascades","9c5e78f0":"### RUN Video generation","315b05bf":"# You can find more images yourself, otherwise, more images are available in Moodle","7ea9f132":"Description:\n- Images from Video Capture -> detect faces' regions -> crop those faces and align them \n- each cropped face is categorized in 3 types: Center, Left, Right \n- Extract 128D vectors( face features)\n- Search for matching subjects in the dataset based on the types of face positions. \n- The preexisitng face 128D vector with the shortest distance to the 128D vector of the face on screen is most likely a match\n(Distance threshold is 0.6, percentage threshold is 70%)\n    ","c741e7f9":"### Align Face","fff561cb":"## Questions and Instructions - part 1. Not for **submission**\n\n### Haar Cascade\n- Change paramaters of (scaleFactor, minNeighbors, minSize, maxSize) to find all faces in the GOT image, using find_faces_in_image().\n    - gray is the input grayscale image.\n    - scaleFactor is the parameter specifying how much the image size is reduced at each image scale. It is used to create the scale pyramid.\n    - minNeighbors is a parameter specifying how many neighbors each candidate rectangle should have, to retain it. A higher number gives \n      lower false   positives.\n    - minSize is the minimum rectangle size to be considered a face.\n    - More help can be found in: https:\/\/docs.opencv.org\/2.4\/modules\/objdetect\/doc\/cascade_classification.html\n- Which parameters did the best work?\n- complete function find_faces_in_frame_of_video() to run face detection using  video_file_recog_haar() on the GOT video\n-  Change paramaters of (scaleFactor, minNeighbors, minSize, maxSize) to find as many TRUE faces as possible in video using\n    video_file_recog_haar()\n    - To download output video file (output_haar.mp4): click on the download icon near its name in the right pane. click once and wait, as sometimes it takes it a bit of time to process the download.\n    - Which parameters did the best work?\n- Check your parameters with framerate of 30 when you think it is good enough\n\n### MTCNN and Face Vector Search\n- Do mini-assignment and complete frame_face_recog() function\n- Run video_file_recog() to create the output movie (code already there, just need to run it)\n- Download movie output.mp4 and check who was recognized and how many times?\n- Add more individulas to the database so you could recgnize more individuals in video (notice you need images with center, right, left angles)\n- What led you to choose the images you chose? what was your thought process?\n- Try to augment the images using the augment_image function, does that improves the accuracy?\n   - For help check opencv image manipulaions and https:\/\/github.com\/aleju\/imgaug\n- Try to change min_face_size and see if you can recgnize faces in more frames\n- How would you increase the accuracy of the recognition?\n- How would you increase the consistency of the recognition (like when the same face is recognized in one frame and not in the next)?\n- How would you make the entire process run faster?\n- what will happen if we will reshape the face images (that the aligner returns) to something other than 160x160 pixels?\n- what should you do if you want to methodically test your model on the given video file?\n- in second 55, Cersei isn't recognized well? which images you should include in person-images to improve that?","5de396e9":"### Initialize","3d51f407":"## Questions and Instructions - Part 2\n\n- Complete urls2imgarr() function. use the face detection methods you've used in part 1.\n- Q5: What happens when we are not cropping the images to only the faces? Why?\n- Q6: Which downscaling method brings better results ('conv' or 'cv2')? why do the downscaling method affects the performance of the model?\n- Go to Labeled Faces in the Wild website and inspect it a bit (http:\/\/vis-www.cs.umass.edu\/lfw\/)\n  - Q7: What are the caveats of this dataset and what we should look out for when using it?\n- run the inference one time with 'srganA-epoch300' as the model_file, and again with 'srganB-epoch300'\n  - Q8: both srganA and srganB models have the same architechture. What do you think is the difference between them?\n- Q9: How many faces were identified using the algorithm in part 1? - \n- Q10: Suggest a way to improve the detection rate of the images that came from the superresolution model.","4c91fc8e":"## RUN","dd34e39f":"## Helper functions","d16f52a7":"### Q1: Mini Assignemnt\nChange paramaters of (scaleFactor, minNeighbors, minSize, maxSize) above to find all faces in the GOT image,\nuse information in: https:\/\/docs.opencv.org\/2.4\/modules\/objdetect\/doc\/cascade_classification.html","214881a2":"### Generate Faces Database functions\n- Extract and crop faces from each given image\n- convert faces to embedded vector of 128D","64147cb5":"### Generate face database","cf616665":"## Part by Part Walkthrough","34d40828":"### Test your function","416b51c1":"### Q3 Mini assignment\nComplete the following frame_face_recog() function so we can do face detection on the video\n - Detect all faces in frame and get their bounding-rectangles and landmarks using face_detect.detect_face() function\n - Go through each face bounding box in the frame and perform for each one:\n    - align the face (using aligner.align() function). remember that aligner.align() returns the aligned-face and face-pose (left\/right\/center) that you'll need for identifyPerson() function\n    - extract aligned face features (embeddings) using extract_feature.get_features() function. (make sure you pass the function a list object, just wrap the input with [ ]\n    - find the person the face belongs to using identifyPerson() function.\n    - draw a box around the face and label the person using annotate_face() function","3dc6282c":"![example](https:\/\/github.com\/tadax\/srgan\/raw\/master\/results\/000000010.jpg)","1503f1cd":"## Face Detection in Video using Haar","1297d621":"### Feature extraction"}}