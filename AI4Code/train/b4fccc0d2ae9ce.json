{"cell_type":{"bf461206":"code","1d4876be":"code","c58731f9":"code","a0b00ab2":"code","cfe399db":"code","1bce353c":"code","5134d852":"code","ff763eca":"code","7fac1495":"code","92b70784":"code","b54d2c1e":"code","43860c00":"code","7cc6d7d9":"code","a8b692d2":"code","609d7126":"code","18b7c20c":"code","05f26a57":"code","e7bc2bc8":"code","f6d3fd90":"code","c305c240":"code","edc34518":"code","255878cc":"code","313b6bbe":"code","cca8a526":"code","4fd2fab6":"code","18375f62":"code","9db39082":"markdown","f5d2546f":"markdown","3d8b1860":"markdown","52985a3c":"markdown","c16035f7":"markdown","38e0d03e":"markdown","acae6499":"markdown","bbf9fa52":"markdown","df8950b5":"markdown","8b78718e":"markdown","36e24f03":"markdown"},"source":{"bf461206":"import pandas as pd\nimport numpy as np\nimport datetime\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, confusion_matrix, classification_report\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import mode, skew, kurtosis\n\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#----------\npd.options.display.max_rows = 50\npd.options.display.max_columns = 50\n\nimport warnings\nwarnings.simplefilter('ignore')\nfrom IPython.display import clear_output ","1d4876be":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n\nall_df = pd.concat([train, test]).reset_index(drop=True)","c58731f9":"train.value_counts('target')","a0b00ab2":"all_features = ['feature_' + str(i) for i in range(0,75)]","cfe399db":"res = np.array(np.ones((9,9), int))\nnp.fill_diagonal(res, 0)\n\nmap = [{}, {}, {},  {}, {}, {}, {}, {}, {}]\nfor i in range(0,9):\n    map = {'Class_' + str(j+1) : res[i][j] for j in range(0,9)}\n    all_df['target' + str(i)] = all_df.replace({'target': map})['target']\n\nall_df","1bce353c":"X = (all_df.iloc[:train.shape[0]])[all_features].to_numpy()\ntest_npy = all_df.iloc[train.shape[0]:][all_features].to_numpy()\nt0 = all_df[['target0']].to_numpy()\nt1 = all_df[['target1']].to_numpy()\nt2 = all_df[['target2']].to_numpy()\nt3 = all_df[['target3']].to_numpy()\nt4 = all_df[['target4']].to_numpy()\nt5 = all_df[['target5']].to_numpy()\nt6 = all_df[['target6']].to_numpy()\nt7 = all_df[['target7']].to_numpy()\nt8 = all_df[['target8']].to_numpy()","5134d852":"def create_model(shape=(75,)):\n    cat_input = tf.keras.layers.Input(shape=shape, name='cat_input')\n    \n    x = tf.keras.layers.Embedding(400, 16, name='Embedding')(cat_input)\n    x = tf.keras.layers.Flatten(name='Flatten')(x)\n    \n    x = tf.keras.layers.Dropout(0.4, name='dropout_concatenated')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(128, activation='relu', name='dense1')(x)\n    x = tf.keras.layers.Dense(64, activation='relu', name='dense2')(x)\n    x = tf.keras.layers.Dense(32, activation='relu', name='dense3')(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n    \n    model = tf.keras.Model(cat_input, outputs)\n    \n    metrics = ['accuracy', tf.keras.metrics.BinaryCrossentropy(\n        from_logits=False,\n        label_smoothing=0,\n        name='binary_crossentropy'\n    )]\n    \n    loss = tf.keras.losses.BinaryCrossentropy(\n                from_logits=False,\n                label_smoothing=0,\n                reduction='auto',\n                name='binary_crossentropy'\n    )\n    \n    optimizer = tfa.optimizers.AdamW(\n        weight_decay=1e-7,\n        learning_rate=0.0001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-07,\n        amsgrad=True,\n        name='AdamW',\n    )\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\nmodel().summary()","ff763eca":"scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=2,\n    verbose=0,\n    mode='auto',\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0\n)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=5,\n    verbose=1,\n    mode='auto',\n    baseline=None,\n    restore_best_weights=True\n)","7fac1495":"history = []\ny = [t0, t1, t2, t3, t4, t5, t6, t7, t8]\ndf = pd.DataFrame()\nK.clear_session()\n\nfor i in range(0,9):\n    print('Training for Class_' + str(i+1) + \" :\")\n    model = create_model(shape=(75,))\n\n    history.append(\n        model.fit(\n            x=X[:, :(len(all_features))],\n            y=y[i],\n            steps_per_epoch=1000,\n            batch_size=256,\n            epochs=50,\n            validation_split=0.2,\n            callbacks=[scheduler_cb, early_stopping_cb]\n        )\n    )\n    \n    print('Predicting for Class_' + str(i+1) + \"\\n\\n\")\n    df['Class_' + str(i+1)] = 1 - model.predict(all_df[all_features].to_numpy()).flatten()\n\ndf","92b70784":"class_probs = pd.read_csv('\/kaggle\/input\/max-score-ensemble\/ensemble_top_scores4.csv')\n#classes = ['Class_' + str(i) for i in range(1,10)] \n#result = pd.DataFrame(tf.keras.utils.to_categorical(class_probs[classes].to_numpy(), axis=1), num_classes=9), columns=classes)\n#log_loss(result, class_probs[classes])\n#display(class_probs)\n#display(df[200000:])","b54d2c1e":"rdf = df.iloc[200000:]\nrdf['id'] = [i for i in range(0,100000)]\nrdf = rdf.set_index('id')\ndisplay(rdf)\nresult = pd.DataFrame()\nresult['Class_1'] =  10 * (class_probs['Class_1']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_2'] =  10 * (class_probs['Class_2']) * (1 - rdf['Class_1']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_3'] =  10 * (class_probs['Class_3']) * (1 - rdf['Class_2']) * (1 - rdf['Class_1']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_4'] =  10 * (class_probs['Class_4']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_1']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_5'] =  10 * (class_probs['Class_5']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_1']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_6'] =  10 * (class_probs['Class_6']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_1']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_7'] =  10 * (class_probs['Class_7']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_1']) * (1 - rdf['Class_8']) * (1 - rdf['Class_9'])\nresult['Class_8'] =  10 * (class_probs['Class_8']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_1']) * (1 - rdf['Class_9'])\nresult['Class_9'] =  10 * (class_probs['Class_9']) * (1 - rdf['Class_2']) * (1 - rdf['Class_3']) * (1 - rdf['Class_4']) * (1 - rdf['Class_5']) * (1 - rdf['Class_6']) * (1 - rdf['Class_7']) * (1 - rdf['Class_8']) * (1 - rdf['Class_1'])\n#result['tg'] = all_df['target']\ndisplay(result)\nclass_probs","43860c00":"class display(object):\n    \"\"\"Display HTML representation of multiple objects\"\"\"\n    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n    <p style='font-family:\"Courier New\", Courier, monospace'>{0}<\/p>{1}\n    <\/div>\"\"\"\n    def __init__(self, *args):\n        self.args = args\n        \n    def _repr_html_(self):\n        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n                         for a in self.args)\n    \n    def __repr__(self):\n        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n                           for a in self.args)","7cc6d7d9":"#test_pred.shape\n#sample = read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n#print(sample)","a8b692d2":"#sub = pd.DataFrame(test_pred.data, columns=['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\nsub = result\nsub.insert(0, 'id', [id for id in range(200000,300000,1)])\nprint(sub)\ncsv = 'submission_ovr.csv'\nsub.to_csv(csv, index = False)","609d7126":"from IPython.display import FileLink\nFileLink(csv)","18b7c20c":"#from scipy.special import softmax\n#newresult = softmax(10 * result[['Class_' + str(i) for i in range(1,10)]], axis=1)\n#display('rdf', 'result', 'newresult', 'class_probs')\n#log_loss(tf.keras.utils.to_categorical((train['target'].str.split('_', expand=True)[1].astype(int) -1), num_classes=9), myresult.to_numpy())\n#myresult","05f26a57":"#scaler = MinMaxScaler()\n#classes = ['Class_' + str(i) for i in range(1,10)]\n#result = pd.DataFrame((scaler.fit_transform(result.T)).T, columns=classes)\n\n#display(result[result['Class_5'] > 0].head(20))\n#display(class_probs.head(20))\n#df['tg'] = all_df['target']\n#display(df.iloc[:200000])","e7bc2bc8":"#pd.options.display.max_rows = 2000\n#train_result = pd.DataFrame()\n#result['Class_8'] = 1 - df['86']\n#result['Class_6'] = df['86']\n#result['Class_2'] = 1 - df['29']\n#result['Class_9'] = df['29']\n#result['Class_7'] = 1 - df['73']\n#result['Class_3'] = df['73']\n#result['Class_5'] = 1 - df['54']\n#result['Class_4'] = df['54']\n#result['Class_1'] = df['101']\n#train_result['tg'] = train['target'].apply(lambda x: int(x.split(\"_\")[-1])-1).to_numpy()\n#train_result[classes] = result[classes]\n#df = df.drop('id')\n#train_result.head(50)","f6d3fd90":"#result['tg'] = train['target'].apply(lambda x: int(x.split(\"_\")[-1])-1).to_numpy()\n#result.head(50)","c305c240":"#tmp = pd.DataFrame()\n#tmp['pred'] = (((rdf[classes])[rdf['tg'] == 0])['Class_1']).astype(float) \n#tmp['tg'] = rdf[rdf['tg'] == 0]['tg'].astype(float) + 1\n#(tmp['tg'] - tmp['pred']).describe()","edc34518":"#!pip3 install -U lightautoml","255878cc":"#from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n#from lightautoml.tasks import Task\n#from sklearn.metrics import log_loss","313b6bbe":"#N_THREADS = 4 # threads cnt for lgbm and linear models\n#N_FOLDS = 5 # folds cnt for AutoML\n#RANDOM_STATE = 1 # fixed random state for various reasons\n#TEST_SIZE = 0.2 # Test size for metric check\n#TIMEOUT =  60 * 60 # Time in seconds for automl run","cca8a526":"#automl = TabularUtilizedAutoML(task = Task('multiclass',), \n#                               timeout = TIMEOUT,\n#                               cpu_limit = N_THREADS,\n#                               reader_params = {'n_jobs': N_THREADS},\n#                              )","4fd2fab6":"#target_column = 'tg'\n\n#roles = {\n#    'target': target_column,\n#}\n\n#lightml_pred = automl.fit_predict(rdf, roles = roles)\n#print('lightml_pred:\\n{}\\nShape = {}'.format(lightml_pred[:10], lightml_pred.shape))","18375f62":"#test_pred = automl.predict(test)","9db39082":"For a one vs. rest classifier, it makes sense to get probability of belonging to a class by multiplying the probabilities of not belonging to other classes.\nSince, in one vs rest classifier, there are more samples for 'rest of the classes', hence a model trained for one vs rest is good at predicting the probability of 'not belonging to a class'. Hence, I believe that to improve prediction, we can calculate the probability of belonging to a class as follows:\n\nProb['Class_1'] =  Prob['Class_1']  * (1 - Prob['Class_2']) * ( 1 - Prob['Class_3']) * ( 1 - Prob['Class_4']) * ( 1 - Prob['Class_5']) * ( 1 - Prob['Class_6']) * ( 1 - Prob['Class_7']) * ( 1 - Prob['Class_8']) * ( 1 - Prob['Class_9'])\n\nHere, Prob['Class_1'] is the probability of belonging to Class_1. ","f5d2546f":"# Assign training and test data","3d8b1860":"# One vs. Rest Classifier\n---\n\n## Reference\n* [Simple Keras embedding in 10 folds](https:\/\/www.kaggle.com\/pourchot\/simple-keras-embedding-in-10-folds) by [@pourchot](https:\/\/www.kaggle.com\/pourchot)\n* [Combining discrete and continuous features in neural networks](https:\/\/www.kaggle.com\/hiro5299834\/tps06-nn-w-discrete-and-continuous-features) by [@bizen](https:\/\/www.kaggle.com\/hiro5299834)","52985a3c":"# Train model","c16035f7":"# Submission","38e0d03e":"# Define Model","acae6499":"# Load Data","bbf9fa52":"# Get probabilities for belonging to a class","df8950b5":"# **LIBRARIES**","8b78718e":"# Create Targets for one vs. Rest learning","36e24f03":"# Multiply by probabilities for not belonging to other classes"}}