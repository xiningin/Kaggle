{"cell_type":{"1a72aa8a":"code","6c61c307":"code","b1a965b6":"code","8112e14e":"code","f8941f32":"code","d05194b3":"code","03a60356":"code","51d899bf":"code","c4c67241":"code","36338646":"code","0ca4e8b7":"code","2e094ab2":"code","5c21231b":"code","cd20c9a5":"code","5a09fc58":"code","985c7bd9":"code","a6cf1a80":"code","542f5987":"code","c3a60c30":"code","6aca5f0f":"code","7bac461a":"code","e2e4f86d":"code","c41d4599":"code","036648da":"markdown","c74e4b22":"markdown","a5070458":"markdown","d75ecd4e":"markdown","65dff6c8":"markdown","edff2e21":"markdown","bdaf2fd3":"markdown","1144dd4f":"markdown","a5895680":"markdown","72b3bc51":"markdown","79bdf3d5":"markdown","91e87b72":"markdown","62d1ebe1":"markdown","f052c270":"markdown"},"source":{"1a72aa8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\nimport pylab as plot\n\n# import datasets\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c61c307":"train.head()","b1a965b6":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","8112e14e":"train.dtypes","f8941f32":"train.describe().T","d05194b3":"test.head()","03a60356":"print(f'Number of rows: {test.shape[0]};  Number of columns: {test.shape[1]}; No of missing values: {sum(test.isna().sum())}')","51d899bf":"test.describe().T","c4c67241":"test.dtypes","36338646":"features=['f1', 'f16', 'f27', 'f55', 'f86', 'loss']\n\nfor col in features:\n    print(f'{col} unique value : {train[col].nunique()}')","0ca4e8b7":"submission.head()","2e094ab2":"train.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","5c21231b":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_count = train['loss'].value_counts().sort_index()\n\nax.bar(target_count.index, target_count, color=['#1520E6' if i%2==0 else '#93D1FF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(20):\n    ax.annotate(f'{target_count[i]\/len(train)*100:.3}', xy=(i, target_count[i]+1000),\n                   va='center', ha='center',\n               )\n#Annotate the point xy with text text.\n\n#In the simplest form, the text is placed at xy.\n\nax.set_title('Target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","cd20c9a5":"target_count = train['loss'].value_counts().sort_index()\ntarget_count_df = pd.DataFrame(target_count)\n#pd.options.display.float_format = '{:,.2f}%'.format\ntarget_count_df['loss(%)'] = (target_count_df\/target_count.sum()*100)\ntarget_count_df.sort_values('loss(%)', ascending=False, inplace=True)\ndisplay(target_count_df)","5a09fc58":"from sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","985c7bd9":"def train_model(model_cls, X, y):\n    model = model_cls() \n    train_scores, valid_scores = [], []\n    \n    skf = KFold(n_splits=5, shuffle=True)\n\n    for tr_idx, va_idx in skf.split(X, y):\n        X_train, X_val = X.iloc[tr_idx], X.iloc[va_idx]\n        y_train, y_val = y[tr_idx], y[va_idx]\n        model.fit(X_train, y_train)\n        \n        pred = model.predict(X_train)\n        train_score = mean_squared_error(y_train, pred)\n        \n        pred = model.predict(X_val)\n        valid_score = mean_squared_error(y_val, pred)\n        \n        train_scores.append(train_score)    \n        valid_scores.append(valid_score)\n        \n    \n    print('train score mean : ',np.mean(train_scores))\n    print('valid score mean : ',np.mean(valid_scores))\n    return train_scores, valid_scores","a6cf1a80":"\ntrain = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv').sample(1000)\ntarget = train['loss']\ntrain = train.drop(['loss'], axis=1)\n","542f5987":"dt_train, dt_val = train_model(DecisionTreeRegressor, train, target.values)\nsvm_train, svm_val = train_model(SVR, train, target.values)\nrf_train, rf_val = train_model(RandomForestRegressor, train, target.values)\nada_train, ada_val = train_model(AdaBoostRegressor, train, target.values)","c3a60c30":"train2=train.iloc[:,[2,17,28,56,87]]\ntrain2.head()","6aca5f0f":"dt_train2, dt_val2 = train_model(DecisionTreeRegressor, train2, target.values)\nsvm_train2, svm_val2 = train_model(SVR, train2, target.values)\nrf_train2, rf_val2 = train_model(RandomForestRegressor, train2, target.values)\nada_train2, ada_val2 = train_model(AdaBoostRegressor, train2, target.values)","7bac461a":"raw_data = np.vstack([dt_train, dt_train2,dt_val, dt_val2,\n           svm_train,svm_train2, svm_val, svm_val2,\n           rf_train,rf_train2, rf_val, rf_val2,\n           ada_train, ada_train2,ada_val, ada_val2]).T\n\nraw_data = np.vstack([raw_data, raw_data.mean(axis=0)])\ndf = pd.DataFrame(raw_data,\n                  index=pd.Index([f'Fold {idx}' for idx in range(5)]+['Mean'], name='#:'),\n                  columns=pd.MultiIndex.from_product([['Decision Tree', 'SVM', 'Random Forest', 'AdaBoost'],\n                                                     ['Train', 'Train1','Valid','Valid1']], \n                                                     names=['Model:', 'Train\/Split']))\ndisplay(df)\ns = df.style.format('{:.3f}')","e2e4f86d":"main_color = '#00539C'\nsub_color = '#FFD662'\n\n# Cell\ncell_hover = {\n    'selector': 'td:hover',\n    'props': [('background-color', sub_color),\n              ('color', main_color),\n              ('font-weight', 'bold')\n             ]\n}\n\n# Index Explaination\nindex_names = {\n    'selector': '.index_name',\n    'props': [('font-style', 'italic'), \n              ('color', 'darkgrey'),  \n              ('font-weight', 'normal')]\n}\n\n# header\nheaders = {\n    'selector': 'th:not(.index_name)',\n    'props': [('background-color', main_color),\n              ('color', 'white')]\n}\n\nheaders_head = {\n    'selector': 'th.col_heading', \n    'props': [('text-align', 'center')]\n}\n\n# border\nborder_head1 = {\n    'selector': 'th.col_heading.level0', \n    'props': [\n        ('font-weight', 'bold'),\n        ('color', sub_color),\n        ('border-left', '1px solid white'),\n    \n    ]\n}\n\nborder_head2 = {\n    'selector': 'th:nth-child(2n+2)', \n    'props': [('border-left', '1px solid white')]\n}\n\nborder_body = {\n    'selector': 'td:nth-child(2n+2)', \n    'props': [('border-left', f'1px solid {main_color}')]\n}\n\nborder_footer1 = {\n    'selector': 'tr:last-child td', \n    'props': [('border-top', f'1px solid {main_color}')]\n}\n\nborder_footer2 = {\n    'selector': 'tr:last-child td', \n    'props': [('border-top', f'1px solid {main_color}')]\n}\n\nborder_footer3 = {\n    'selector': 'tr:last-child', \n    'props': [('background-color', main_color+'20')]\n}","c41d4599":"s =s.set_table_styles([cell_hover, index_names, headers, headers_head, \n                    border_head1, border_head2, border_body, \n                    border_footer1, border_footer2, border_footer3])\ns","036648da":"** Basic statistics:\n**Below is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.","c74e4b22":"There are a total of 43 discrete losses.\nThe top 12 distributions account for 80% of the total.\nAll except the order of 2 and 1 are in increasing order.","a5070458":"The dimension and number of missing values in the train dataset is as below:","d75ecd4e":"Below is the first 5 rows of test dataset:","65dff6c8":"K-Fold Benchmark Visualization\nLet's create a total of 4 models.\n\nFor demonstration, I used Decistion Tree, SVM, RandomForest, AdaBoost.\n\nThe score is RMSE.","edff2e21":"1. Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\nObservations:\n\n","bdaf2fd3":"The result is bundled using numpy's stack and overlaid with a DataFrame.","1144dd4f":"3.  Submission\nThe submission file is expected to have an id and loss columns.\n\nBelow is the first 5 rows of submission file:","a5895680":"** Data types\n**Except for column id, f1, f16, f27, f55, f60, f86 and loss column which are in int64 type, other columns are in float64 which is consistent with the train dataset. (to see the details, please expand)","72b3bc51":"**Data types:**\nExcept for column id, f1, f16, f27, f55, f60, f86 and loss column which are in int64 type, other columns are in float64. (to see the details, please expand)","79bdf3d5":"For fast implementation, only 1000 random data were used.\n","91e87b72":"\nBelow is the first 5 rows of test dataset:","62d1ebe1":"2. Test dataset\nTest dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it\u2019s similiarity with the train dataset.","f052c270":"Number of features available to be used to create a prediction model are 100. The analysis is started by looking on number of uniques value on integer features which are f1, f16, f27, f55, f60 and f86."}}