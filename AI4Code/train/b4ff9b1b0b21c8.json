{"cell_type":{"ecd29632":"code","be6cb9b3":"code","7b1195b7":"code","fdc60113":"code","10417948":"code","f4cf3cc1":"code","63df5673":"code","b08d58a1":"code","1a8d1d1a":"code","e1af8a54":"code","f8bc9d5f":"code","0033e733":"code","b21216c8":"code","43380e7b":"code","04e99c60":"code","47a5eec8":"code","7b7db03f":"code","9a910b31":"code","e7b0d8d1":"markdown","ba425bea":"markdown","220cee03":"markdown","652b6ffd":"markdown","057d3747":"markdown","91b2cbf7":"markdown","dfb794bd":"markdown","f2078198":"markdown","932c9006":"markdown","c1e3c739":"markdown","3a03b634":"markdown","733b4e06":"markdown"},"source":{"ecd29632":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer","be6cb9b3":"train = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n\nsample_submission = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\n\n#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\ntrain_txt = glob('..\/input\/feedback-prize-2021\/train\/*.txt') \ntest_txt = glob('..\/input\/feedback-prize-2021\/test\/*.txt')","7b1195b7":"!cat ..\/input\/feedback-prize-2021\/train\/423A1CA112E2.txt","fdc60113":"train.head(15)","10417948":"cols_to_display = ['discourse_text', 'predictionstring']\n\ntext = train[cols_to_display].values[0][0].split(' ')\npredict_string = train[cols_to_display].values[0][1].split(' ')\nprint(text, len(text))\nprint()\nprint(predict_string, len(predict_string))","f4cf3cc1":"# this code chunk is copied from Rob Mulla\nlen_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"\/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)\n\n#add columns\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\n#initialize column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n        #minus 2 as the previous end is always -1 and the previous start always +1\n    #gap if the first discourse of an new essay does not start at 0\n    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n\n\n #is there any text after the last discourse of an essay?\nlast_ones = train.drop_duplicates(subset=\"id\", keep='last')\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id', 'discourse_id', 'gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")","63df5673":"def add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)\n\n\ndef print_colored_essay(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"..\/input\/feedback-prize-2021\/train\/\" + essay + \".txt\"\n\n    ents = []\n    for i, row in df_essay.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(essay_file, 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n    }\n\n    colors = {'Lead': '#EE11D0','Position': '#AB4DE1','Claim': '#1EDE71','Evidence': '#33FAFA','Counterclaim': '#4253C1','Concluding Statement': 'yellow','Rebuttal': 'red'}\n    options = {\"ents\": df_essay.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);\n    \nprint_colored_essay(\"423A1CA112E2\")","b08d58a1":"print(f\"The total number of discourses is {len(train)}\")\n\nprint(len(train.query('discourse_len != pred_len')[cols_to_display]))\ntrain.query('discourse_len != pred_len')[cols_to_display].head(5)","1a8d1d1a":"train.groupby('discourse_type')['discourse_type'].count()","e1af8a54":"train.groupby('discourse_type')['discourse_len'].mean()","f8bc9d5f":"fig = plt.figure(figsize=(12,8))\n\nax1 = fig.add_subplot(211)\nax1 = train.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind=\"barh\")\nax1.set_title(\"Average number of words versus Discourse Type\", fontsize=14, fontweight = 'bold')\nax1.set_xlabel(\"Average number of words\", fontsize = 10)\nax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(212)\nax2 = train.groupby('discourse_type')['discourse_type'].count().sort_values().plot(kind=\"barh\")\nax2.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ','))) #add thousands separator\nax2.set_title(\"Frequency of Discourse Type in all essays\", fontsize=14, fontweight = 'bold')\nax2.set_xlabel(\"Frequency\", fontsize = 10)\nax2.set_ylabel(\"\")\n\nplt.tight_layout(pad=2)\nplt.show()","0033e733":"fig = plt.figure(figsize=(12,8))\nav_per_essay = train['discourse_type_num'].value_counts(ascending = True).rename_axis('discourse_type_num').reset_index(name='count')\nav_per_essay['perc'] = round((av_per_essay['count'] \/ train.id.nunique()),3) * 100\nav_per_essay = av_per_essay.set_index('discourse_type_num')\nax = av_per_essay.query('perc > 3')['perc'].plot(kind=\"barh\")\nax.set_title(\"discourse_type_num: Percent present in essays\", fontsize=20, fontweight = 'bold')\nax.bar_label(ax.containers[0], label_type=\"edge\")\nax.set_xlabel(\"Percent\")\nax.set_ylabel(\"\")\nplt.show()","b21216c8":"#display an example\n#how many pieces of tekst are not used as discourses?\nprint(f\"Besides the {len(train)} discourse texts, there are {len(train.query('gap_length.notna()', engine='python'))+ len(train.query('gap_end_length.notna()', engine='python'))} pieces of text not classified.\")\n\ncols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\ntrain[cols_to_display].query('id == \"AFEC37C2D43F\"')","43380e7b":"total_gaps = train.groupby('id').agg({'essay_len': 'first',\\\n                                               'gap_length': 'sum',\\\n                                               'gap_end_length': 'sum'})\ntotal_gaps['perc_not_classified'] = round(((total_gaps.gap_length + total_gaps.gap_end_length)\/total_gaps.essay_len),2)\n\ntotal_gaps.sort_values(by = 'perc_not_classified', ascending = False).head()","04e99c60":"train.sort_values(by = \"gap_length\", ascending = False)[cols_to_display].head()","47a5eec8":"all_gaps = (train.gap_length[~train.gap_length.isna()]).append((train.gap_end_length[~train.gap_end_length.isna()]), ignore_index= True)\n#filter outliers\nall_gaps = all_gaps[all_gaps<300]\nfig = plt.figure(figsize=(12,6))\nall_gaps.plot.hist(bins=100)\nplt.title(\"Histogram of gap length (gaps up to 300 characters only)\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Length of gaps in characters\")\nplt.show()","7b7db03f":"add_gap_rows(\"7330313ED3F0\")","9a910b31":"print_colored_essay(\"7330313ED3F0\")","e7b0d8d1":"# \ubd84\ub958 \ud558\uace0\uc790 \ud558\ub294 Discourse_type\uc774 \ub9c8\ud0b9 \ub418\uc5b4 \uc788\uc9c0 \uc54a\ub294 Gap \ubd84\uc11d","ba425bea":"# visualization","220cee03":"* (Lead) \ub9ac\ub4dc - \ub3c5\uc790\uc758 \uad00\uc2ec\uc744 \ub04c\uace0 \ub17c\ubb38\uc744 \uac00\ub9ac\ud0a4\ub3c4\ub85d \ud558\uae30 \uc704\ud574 \ud1b5\uacc4, \uc778\uc6a9\ubb38, \uc124\uba85 \ub610\ub294 \uae30\ud0c0 \uc7a5\uce58\ub85c \uc2dc\uc791\ud558\ub294 \uc18c\uac1c \n* (Position) \uc785\uc7a5 - \uc8fc\uc694 \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc758\uacac \ub610\ub294 \uacb0\ub860\n* (Claim) \uc8fc\uc7a5 - \uc785\uc7a5\uc744 \ub4b7\ubc1b\uce68\ud558\ub294 \uc8fc\uc7a5\n* (Counterclaim) \ubc18\ub300 \uc8fc\uc7a5 - \ub2e4\ub978 \uc8fc\uc7a5\uc744 \ubc18\ubc15\ud558\uac70\ub098 \ubc18\ub300 \uc774\uc720\ub97c \uc81c\uc2dc\ud558\ub294 \uc8fc\uc7a5\n* (Rebuttal) \ubc18\ubc15 - \ubc18\ub300 \uc8fc\uc7a5\uc744 \ubc18\ubc15\ud558\ub294 \uc8fc\uc7a5\n* (Evidence) \uc99d\uac70 - \uc8fc\uc7a5, \ubc18\ub300 \uc8fc\uc7a5 \ub610\ub294 \ubc18\ubc15\uc744 \ub4b7\ubc1b\uce68\ud558\ub294 \uc544\uc774\ub514\uc5b4 \ub610\ub294 \uc608.\n* (Concluding Statement) \uacb0\ub860 \uc9c4\uc220 - \uc8fc\uc7a5\uc744 \ub2e4\uc2dc \uc124\uba85\ud558\ub294 \uacb0\ub860 \uc9c4\uc220","652b6ffd":"# \uac01 discourse_type_num\uc774 Essay\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294 \ube44\uc728","057d3747":"# discourse type \uc885\ub958 \ubcc4 \ud3c9\uade0 \uae38\uc774","91b2cbf7":"\ub370\uc774\ud130 \uc774\uc288 \uc810\uac80","dfb794bd":"# discourse type \uc885\ub958 \ubcc4 \ub370\uc774\ud130 \uac1c\uc218","f2078198":"# \ud559\uc2b5\uc2dc \ucc38\uace0\ud560 raw datas \uc5d0 \ub300\ud55c \uba54\ud0c0 \uc815\ubcf4 (train.csv)","932c9006":"* id\n* discourse_id\n* discourse_start, discourse_end\n* discourse_text\n* discourse_type, discourse_type_num\n* predictionstring","c1e3c739":"# 1. \ud559\uc2b5, \ud3c9\uac00\uc5d0 \uc0ac\uc6a9\ud560 raw datas (train dir, test dir)\n# 2. \ud559\uc2b5\uc2dc \ucc38\uace0\ud560 raw datas \uc5d0 \ub300\ud55c \uba54\ud0c0 \uc815\ubcf4 (train.csv)\n# 3. \ucd5c\uc885 \uc81c\ucd9c \ud30c\uc77c (sample_submission.csv)\n","3a03b634":"# \ud559\uc2b5 \ub370\uc774\ud130, \uc815\ub2f5 \ub370\uc774\ud130\uc640 \uad00\ub828\ub41c \uc815\ubcf4","733b4e06":"# \ub370\uc774\ud130 \uc774\uc288 \uc810\uac80 2"}}