{"cell_type":{"8eecf368":"code","7c392569":"code","e097eaf8":"code","6b829d76":"code","3ce86aa7":"code","78cf5ad7":"code","b4f1868c":"code","4c3fb5fe":"code","7c994f35":"code","ed79235f":"code","25099939":"code","785fb520":"code","74b97e2f":"code","c7049ec9":"code","4a244bf7":"code","3d1a66cd":"code","1ec0ba64":"markdown","e617c89f":"markdown","5fddba54":"markdown","745f31a2":"markdown","034c32e4":"markdown","f413656c":"markdown","dffdfbcf":"markdown","0d4a4d97":"markdown","f0c4eb44":"markdown","983729f2":"markdown"},"source":{"8eecf368":"import numpy as np\n\nimport matplotlib as mpl\n\nimport IPython.display as display\nimport PIL.Image\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport cv2","7c392569":"Image.open(\"..\/input\/deep-dream-ds\/deep_dream.jpg\")","e097eaf8":"dim = (256,256)\noriginal_img = np.array(cv2.imread('..\/input\/deep-dream-ds\/shiba.jpg'))\nimg = np.array(cv2.imread('..\/input\/deep-dream-ds\/shiba.jpg'))","6b829d76":"Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))","3ce86aa7":"base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')","78cf5ad7":"# list of all layers\nlayers = [i.name for i in base_model.layers]\nprint(layers[:10])","b4f1868c":"# Maximize the activations of these layers\nnames = ['mixed3', 'mixed5']\nlayers = [base_model.get_layer(name).output for name in names]\n\n# Create the feature extraction model\ndream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n","4c3fb5fe":"def calc_loss(img, model):\n    # Convert image into a batch of size 1\n    img_batch = tf.expand_dims(img, axis=0)\n    # forward passing\n    prediction = model(img_batch)\n    if len(prediction) == 1:\n        prediction = [prediction]\n        \n    losses = []\n    for act in prediction:\n        loss = tf.math.reduce_mean(act)\n        losses.append(loss)\n    \n    return tf.reduce_sum(losses)","7c994f35":"class DeepDream(tf.Module):\n    def __init__(self, model):\n        self.model = model\n\n    @tf.function(\n        input_signature=(\n            tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n            tf.TensorSpec(shape=[], dtype=tf.int32),\n            tf.TensorSpec(shape=[], dtype=tf.float32),)\n          )\n    \n    def __call__(self, img, steps, step_size):\n        print(\"Tracing\")\n        loss = tf.constant(0.0)\n        for n in tf.range(steps):\n            with tf.GradientTape() as tape:\n                tape.watch(img)\n                loss = calc_loss(img, self.model)\n\n            # Gradient of the loss with respect to the pixels of the input image\n            gradients = tape.gradient(loss, img)\n\n            # Normalize the gradients.\n            gradients \/= tf.math.reduce_std(gradients) + 1e-8 \n\n            # Update the image and clip (-1, 1)\n            img = img + gradients*step_size\n            img = tf.clip_by_value(img, -1, 1)\n\n        return loss, img","ed79235f":"deepdream = DeepDream(dream_model)","25099939":"def deprocess(img):\n    img = 255*(img + 1.0)\/2.0\n    return tf.cast(img, tf.uint8)","785fb520":"def func_steps_remaining(steps):\n    if steps > 100:\n        return tf.constant(100)\n    else:\n        return tf.constant(steps)","74b97e2f":"def train(img, steps=300, step_size=0.01):\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    img = tf.convert_to_tensor(img)\n    \n    step_size = tf.convert_to_tensor(step_size)\n    steps_remaining = steps\n    step = 0\n    \n    while steps_remaining:\n        run_steps = func_steps_remaining(steps_remaining)\n        steps_remaining -= run_steps\n        step += run_steps\n        \n        loss, img = deepdream(img, run_steps, tf.constant(step_size))\n        \n        print (\"Step {}, loss {}\".format(step, loss))\n\n    return deprocess(img)","c7049ec9":"result = train(img)","4a244bf7":"original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\nresult_rgb = cv2.cvtColor(np.array(result), cv2.COLOR_BGR2RGB)","3d1a66cd":"fig, ax = plt.subplots(1,2, figsize=(30,20), gridspec_kw = {'wspace':0, 'hspace':0})\nax[0].imshow(original_rgb)\nax[0].set_title('Before')\nax[0].set_axis_off()\nax[1].imshow(result_rgb)\nax[1].set_title('After')\nax[1].set_axis_off()","1ec0ba64":"## Goal\n\nThe goal of this notebook is to show how we can use Inception to create a deep dream image.","e617c89f":"# Deep Dream using Inception","5fddba54":"# Loss","745f31a2":"# Deep Dream Class","034c32e4":"# Prepare Data","f413656c":"# Result","dffdfbcf":"The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. Normally, loss is a quantity you wish to minimize via gradient descent. In DeepDream, you will maximize this loss via gradient ascent.","0d4a4d97":"The idea in DeepDream is to choose a layer (or layers) and maximize the \"loss\" in a way that the image increasingly \"excites\" the layers. The complexity of the features incorporated depends on layers chosen by you, i.e, lower layers produce strokes or simple patterns, while deeper layers give sophisticated features in images, or even whole objects.","f0c4eb44":"# Training","983729f2":"Once you have calculated the loss for the chosen layers, all that is left is to calculate the gradients with respect to the image, and add them to the original image.\n\nAdding the gradients to the image enhances the patterns seen by the network. At each step, you will have created an image that increasingly excites the activations of certain layers in the network."}}