{"cell_type":{"3bd3c555":"code","cddcdf75":"code","df727396":"code","afd4e142":"code","4120a7d7":"code","7a2e9cf9":"code","5221391c":"code","054662db":"code","71469323":"code","1bc10d68":"code","36eb0b57":"code","5384ea33":"code","fa9be7d1":"code","be1c660b":"code","b998fae7":"code","c023be42":"code","db952ace":"code","5f6acd73":"code","fac0089d":"code","24228739":"code","971f3af9":"code","cf4168bc":"code","a3fb7ef5":"code","33222a1e":"code","ba5b39fb":"code","4d0a057a":"markdown","54c00e95":"markdown","b9e3eef2":"markdown","ebfecd63":"markdown","9e3be14a":"markdown","83c426d4":"markdown","620bdf5b":"markdown","cf8ed8f2":"markdown","e759ab4e":"markdown","0b4f9b0b":"markdown"},"source":{"3bd3c555":"!pip install stable-baselines3","cddcdf75":"import gym\nfrom kaggle_environments import make, evaluate\n\nimport os\nimport numpy as np\nimport torch as th\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.monitor import load_results\nfrom stable_baselines3.common.torch_layers import NatureCNN\nfrom stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor","df727396":"# ConnectX wrapper from Alexis' notebook.\n# Changed shape, channel first.\n# Changed obs\/2.0\nclass ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n        self.action_space = gym.spaces.Discrete(self.columns)\n        self.observation_space = gym.spaces.Box(low=0, high=1, \n                                            shape=(1,self.rows,self.columns), dtype=np.float)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\/2\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1\/42\n            return 1\/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\/2, reward, done, _","afd4e142":"# Create ConnectFour environment\nenv = ConnectFourGym()\nenv","4120a7d7":"# from stable_baselines3.common.env_checker import check_env\n# check_env(env)","7a2e9cf9":"# Create directory for logging training information\nlog_dir = \"log\/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nenv = Monitor(env, log_dir, allow_early_resets=True)\nenv","5221391c":"env = DummyVecEnv([lambda: env])\nenv","054662db":"env.observation_space.sample()","71469323":"class Net(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n        super(Net, self).__init__(observation_space, features_dim)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc3 = nn.Linear(384, features_dim)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = nn.Flatten()(x)\n        x = F.relu(self.fc3(x))\n        return x","1bc10d68":"policy_kwargs = {\n    'activation_fn':th.nn.ReLU, \n    'net_arch':[64, dict(pi=[32, 16], vf=[32, 16])],\n    'features_extractor_class':Net,\n}\nlearner = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs)\n\nlearner.policy","36eb0b57":"%%time\nlearner.learn(total_timesteps=100_000)","5384ea33":"df = load_results(log_dir)['r']\ndf.rolling(window=1000).mean().plot()","fa9be7d1":"learner.predict(env.reset())","be1c660b":"def testagent(obs, config):\n    import numpy as np\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)\/2\n    action, _ = learner.predict(obs)\n    return int(action)","b998fae7":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds\/\/2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds\/\/2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])\/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])\/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","c023be42":"get_win_percentages(agent1=testagent, agent2=\"random\")","db952ace":"env = make(\"connectx\", debug=True)\n\n# Two random agents play one game round\nenv.run([testagent, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","5f6acd73":"%%writefile submission.py\ndef agent(obs, config):\n    import numpy as np\n    import torch as th\n    from torch import nn as nn\n    import torch.nn.functional as F\n    from torch import tensor\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n            self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n            self.fc3 = nn.Linear(384, 512)\n            self.shared1 = nn.Linear(512, 64)\n            self.policy1 = nn.Linear(64, 32)\n            self.policy2 = nn.Linear(32, 16)\n            self.action = nn.Linear(16, 7)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n            x = nn.Flatten()(x)\n            x = F.relu(self.fc3(x))\n            x = F.relu(self.shared1(x))\n            x = F.relu(self.policy1(x))\n            x = F.relu(self.policy2(x))\n            x = self.action(x)\n            x = x.argmax()\n            return x\n        ","fac0089d":"learner.policy.state_dict().keys()","24228739":"th.set_printoptions(profile=\"full\")\n\nagent_path = 'submission.py'\n\nstate_dict = learner.policy.to('cpu').state_dict()\nstate_dict = {\n    'conv1.weight': state_dict['features_extractor.conv1.weight'],\n    'conv1.bias': state_dict['features_extractor.conv1.bias'],\n    'conv2.weight': state_dict['features_extractor.conv2.weight'],\n    'conv2.bias': state_dict['features_extractor.conv2.bias'],\n    'fc3.weight': state_dict['features_extractor.fc3.weight'],\n    'fc3.bias': state_dict['features_extractor.fc3.bias'],\n    \n    'shared1.weight': state_dict['mlp_extractor.shared_net.0.weight'],\n    'shared1.bias': state_dict['mlp_extractor.shared_net.0.bias'],\n    \n    'policy1.weight': state_dict['mlp_extractor.policy_net.0.weight'],\n    'policy1.bias': state_dict['mlp_extractor.policy_net.0.bias'],\n    'policy2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\n    'policy2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\n    \n    'action.weight': state_dict['action_net.weight'],\n    'action.bias': state_dict['action_net.bias'],\n}\n\nwith open(agent_path, mode='a') as file:\n    #file.write(f'\\n    data = {learner.policy._get_data()}\\n')\n    file.write(f'    state_dict = {state_dict}\\n')","971f3af9":"%%writefile -a submission.py\n\n    model = Net()\n    model = model.float()\n    model.load_state_dict(state_dict)\n    model = model.to('cpu')\n    model = model.eval()\n    obs = tensor(obs['board']).reshape(1, 1, config.rows, config.columns).float()\n    obs = obs \/ 2\n    action = model(obs)\n    return int(action)","cf4168bc":"# load submission.py\nf = open(agent_path)\nsource = f.read()\nexec(source)","a3fb7ef5":"# simple test agent\nagent(env.reset()[0]['observation'], env.configuration)","33222a1e":"get_win_percentages(agent1=agent, agent2=\"random\")","ba5b39fb":"env = make(\"connectx\", debug=True)\n\n# Two random agents play one game round\nenv.run([agent, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","4d0a057a":"# Model","54c00e95":"# Install stable-baselines3\n\nI happened to find this while looking at the documentation, they use pytorch.","b9e3eef2":"# Env","ebfecd63":"# Import modules","9e3be14a":"I learned a lot from Alexis's notebook. But one question remains, how do I create a submission file?\nI asked Google and managed to create a submission file, so I'll share it with you.\n\nthanks to:\n* https:\/\/www.kaggle.com\/alexisbcook\/deep-reinforcement-learning\n\n* https:\/\/stable-baselines3.readthedocs.io\/en\/master\/\n* https:\/\/github.com\/DLR-RM\/stable-baselines3\n\n* Stable Baselines RL https:\/\/www.kaggle.com\/c\/connectx\/discussion\/128591\n\n- Hard-coding PyTorch weights into a script\nhttps:\/\/www.kaggle.com\/c\/connectx\/discussion\/126678","83c426d4":"UPDATE: \n- Version1: make submission with stable-baseline3\/PPO('MlpPolicy')\n- Version2: using custom CNN.\n- Version3: tried using the GPU, but it didn't work at submission.\n- Version4: using GPU only train.","620bdf5b":"# Write submission.py","cf8ed8f2":"# Test submission.py","e759ab4e":"# Validation","0b4f9b0b":"# Train"}}