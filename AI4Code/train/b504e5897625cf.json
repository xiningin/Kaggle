{"cell_type":{"d41c1610":"code","dd413582":"code","2f668979":"code","bdf02a68":"code","211a2731":"code","134299e0":"code","7414a17e":"code","ce72c73d":"code","04b81177":"code","8dbffe0f":"code","5430b9db":"code","f2333ac8":"code","ca08b4ac":"code","18d0e2df":"code","e6565a1a":"code","68031f5a":"code","ff52c886":"code","d646066a":"code","b6ee2e9f":"code","392afb70":"code","f946d81e":"code","066d5d73":"code","93693166":"code","55527701":"code","77565940":"code","8c0f2b3d":"code","803ce85d":"code","579e54e6":"code","500165df":"code","531de47d":"code","77754d97":"code","421c77b6":"code","dbdbf3b6":"code","40ec4f19":"code","38480d52":"code","a6cc1dde":"code","fff6d9fa":"code","b9fb118c":"code","8b73d204":"code","37884c73":"code","72462b0f":"code","62d18750":"code","83b02d3d":"code","0d993349":"code","8e7d0abe":"code","9888c5d3":"code","b4606688":"code","72e0ad70":"code","ec5e424a":"code","271ba188":"code","7bd58201":"code","66fa7e94":"code","e303cc5b":"code","593a9cf3":"code","a91a26cc":"code","5446f592":"code","8e752a0c":"code","6ef0e19a":"code","d598e56a":"code","6257e9f1":"code","1631c9e7":"code","2a0614c8":"code","2ee6a44d":"code","a0b42acb":"code","81103a0f":"code","7a597472":"code","e827798d":"code","3dea5726":"code","70f7bb72":"code","7ba6c6a5":"code","fa169b2f":"code","3a5fe582":"code","bab13c56":"code","1049d27c":"code","6fc8fc84":"code","711985c6":"code","adeac11e":"code","8b33c0d4":"code","ce673fdf":"code","af5f4649":"code","f52ce2d4":"code","cad0ce24":"code","249c78c6":"code","d96e501c":"code","7b37c8a3":"code","6f78efa6":"code","8e767fe1":"code","10aa64e9":"code","a12e1cba":"code","ee37b1ce":"code","1959a123":"code","5288c0db":"code","05261c0e":"code","d014e52e":"code","f9dedf2c":"code","7dad22b9":"code","338fe206":"code","f059bcc8":"code","fc4e9c7f":"code","fb65415c":"code","b60e52a1":"code","4fb81e37":"code","ae364e3c":"code","7e5e7b2c":"code","f2ba8ec0":"code","8aa6f744":"code","b1827113":"code","0c446e27":"code","fbbca0aa":"code","2eef158d":"code","703a7a7d":"markdown","c411edbd":"markdown","d468ab6d":"markdown","10e15a1b":"markdown","8e9ab3a3":"markdown","e4b1685e":"markdown","02123971":"markdown","b29475c3":"markdown","4ac54b1a":"markdown","16c17eec":"markdown","18099a00":"markdown","b4ca8a68":"markdown","d27823ca":"markdown","655f8001":"markdown","e96aebf9":"markdown","b43e3808":"markdown","a2f6c811":"markdown","0130b6b9":"markdown","3a2742a6":"markdown","83fabde9":"markdown","d8f0e67e":"markdown","7ca03d10":"markdown","09f9a002":"markdown","8b17acb8":"markdown"},"source":{"d41c1610":"import numpy as np \nimport pandas as pd \nimport warnings # Just to remove verbose maybe...\nwarnings.filterwarnings('ignore')","dd413582":"train_df = pd.read_csv('..\/input\/dasprodatathon\/train.csv')\ntest_df = pd.read_csv('..\/input\/dasprodatathon\/test.csv')\n\ntrain_df.shape, test_df.shape","2f668979":"13743 \/ 5727","bdf02a68":"test_df.head()","211a2731":"test_df.isnull().sum().sum()","134299e0":"test_df.info()","7414a17e":"test_df.describe().T","ce72c73d":"train_df.head()","04b81177":"train_df.isnull().sum().sum()","8dbffe0f":"train_df.info()","5430b9db":"train_df.describe().T","f2333ac8":"train_df.columns","ca08b4ac":"# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.offline as offln\nfrom plotly import tools\n\noffln.init_notebook_mode(connected=False)\n\nimport cufflinks as cf\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# cf.go_offline()\npio.renderers.default = 'kaggle'\n\n# Statistics\nfrom scipy.stats import norm, skew \nfrom scipy import stats ","18d0e2df":"from sklearn.preprocessing import RobustScaler, robust_scale, MinMaxScaler, StandardScaler","e6565a1a":"fig, axs = plt.subplots(ncols=2, figsize=(17,3))\nsns.stripplot(x='Waterfront', data=train_df, ax=axs[0])\nsns.stripplot(x='Year Renovated', data=train_df, ax=axs[1])","68031f5a":"plt.figure(figsize=(19,7))\nsns.stripplot(x='Zipcode', hue='Waterfront', data=train_df);","ff52c886":"plt.figure(figsize=(10,6))\nsns.distplot(train_df['Price'], color='Purple', fit=norm)\nmu, sigma = norm.fit(train_df['Price'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} \/\/ $\\sigma=$ {:.2f} \/\/ \\nSkewness: {:.2f} \/\/ Kurtosis: {:.2f} )'\n            .format(mu, sigma, train_df['Price'].skew(), train_df['Price'].kurt())]);","d646066a":"sns.boxplot(train_df['Price'])","b6ee2e9f":"# n_rows = 6\n# n_cols = 2\n\n# plot_feat = np.array([ 'Price', 'Living Area', 'Total Area', 'Above the Ground Area',\n#        'Basement Area', 'Neighbors Living Area', 'Neighbors Total Area',\n#        'Bedrooms', 'Bathrooms', 'Floors', 'Year Built', 'Zipcode'])\n\n# fig, axs = plt.subplots(n_rows, n_cols, figsize=(15,25))\n\n# for r in range(0,n_rows):\n#     for c in range(0,n_cols):  \n#         i = r*n_cols+c\n#         if i < len(plot_feat):\n#             sns.distplot(train_df[plot_feat[i]], color='Purple', ax = axs[r][c], fit=norm)\n#             mu, sigma = norm.fit(train_df[plot_feat[i]])\n#             axs[r][c].legend(['Normal dist. ($\\mu=$ {:.2f} \/\/ $\\sigma=$ {:.2f} \/\/ \\nSkewness: {:.2f} \/\/ Kurtosis: {:.2f} )'\n#                         .format(mu, sigma, train_df[plot_feat[i]].skew(), train_df[plot_feat[i]].kurt())])\n\n# plt.tight_layout()","392afb70":"scaler = StandardScaler()\npric_scl = scaler.fit_transform(train_df['Price'].values.reshape(-1, 1)).reshape(-1,)","f946d81e":"fig = plt.figure(figsize = (17,10))\n\nfig.add_subplot(1,3,1)\nres = stats.probplot(train_df['Price'], plot=plt)\nplt.title('Default')\nplt.legend(['Std: {}'.format(train_df['Price'].std())])\n\nfig.add_subplot(1,3,2)\nres = stats.probplot(pric_scl, plot=plt)\nplt.title('Scaling')\nplt.legend(['Std: {}'.format(pric_scl.std())])\n\nfig.add_subplot(1,3,3)\nres = stats.probplot(np.log1p(train_df['Price']), plot=plt)\nplt.title('Log1p')\nplt.legend(['Std: {}'.format(np.log1p(train_df['Price']).std())]);","066d5d73":"scales=pd.array(robust_scale(train_df['Price']))\nscls = pd.array(pric_scl)\nlogss=np.log1p(train_df['Price'])\n\nfig = plt.figure(figsize=(15,7))\n\nfig.add_subplot(1,2,1)\nsns.distplot(scls, color='Red', fit=norm)\n\nmu, sigma = norm.fit(scls)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} \/\/ $\\sigma=$ {:.2f} \/\/ \\nSkewness: {:.2f} \/\/ Kurtosis: {:.2f} )'\n            .format(mu, sigma, scls.skew(), scls.kurt())])\nplt.title('Scaling')\nplt.xlabel('Price')\n\nfig.add_subplot(1,2,2)\nsns.distplot(logss, color='Red', fit=norm)\n\nmu, sigma = norm.fit(logss)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} \/\/ $\\sigma=$ {:.2f} \/\/ \\nSkewness: {:.2f} \/\/ Kurtosis: {:.2f} )'\n            .format(mu, sigma, logss.skew(), logss.kurt())], loc='upper right')\nplt.title('Log1p');","93693166":"# Interactive visualization\ntrace = dict(type = 'scattergeo',\n             lon = train_df['Longitude'],\n             lat = train_df['Latitude'],\n             marker = dict(size=12),\n             text = train_df['Zipcode'],\n             mode = 'markers')\n\ndata = [trace]\nlayout = dict(title='House Location (King County)',\n              showlegend=False,\n              geo=dict(\n                  scope='usa',\n                  showland=True)\n             )\n\nfig=dict(data=data,\n         layout=layout)\n\noffln.iplot(fig)","55527701":"train_df.plot(kind='scatter',x='Longitude', y='Latitude', c='Price', alpha=0.4, title='Price Distribution', \n                cmap='terrain', figsize=(10,7), colorbar=True, sharex=False);","77565940":"# plt.figure(figsize=(10,7))\n# sns.scatterplot(x='Longitude', y='Latitude', hue='Zipcode', data=train_data, c='Zipcode')\ntrain_df.plot(kind='scatter',x='Longitude', y='Latitude', c='Zipcode', alpha=0.4, title='Zipcode', \n                 cmap='terrain', figsize=(10,7), colorbar=True, sharex=False);","8c0f2b3d":"fig1, axs = plt.subplots(ncols=3, figsize=(20, 4))\n\ntrain_df['Bathrooms'].value_counts().plot(kind='bar', ax=axs[0], title='Num of Bathrooms')\nsns.countplot(x='Bedrooms', data=train_df, ax=axs[1])\nsns.countplot(x='Floors', data=train_df, ax=axs[2]);\n\n# plt.tight_layout()","803ce85d":"# Interactive visualization\ntrain_unique = []\ncols = ['Bedrooms','Bathrooms','Floors','Waterfront','View','Condition','Grade', 'Year Built', 'Zipcode', 'Year Renovated']\n\nfor i in cols:\n    train_unique.append(train_df[i].nunique())\nunique_train = pd.DataFrame()\nunique_train['Columns'] = cols\nunique_train['Unique_Value'] = train_unique\n\ndata = [\n    go.Bar(\n        x = unique_train['Columns'],\n        y = unique_train['Unique_Value'],\n        name = 'Unique value in features',\n        textfont=dict(size=17),\n        marker=dict(\n        line=dict(\n            color= 'yellow',\n            #width= 2,\n        ), opacity = 0.45\n    )\n    ),\n    ]\nlayout= go.Layout(\n        title= \"Unique Value By Column\",\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis= dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n)\nfig = go.Figure(data=data, layout=layout)\noffln.iplot(fig, filename='skin');","579e54e6":"corr_matrix = train_df.corr(method='spearman')\nfig, axes = plt.subplots(figsize=(17,17))\nsns.heatmap(corr_matrix.sort_values(by='Price', ascending=False).sort_values(by='Price', ascending=False, axis=1), \n            cmap='YlGnBu', annot=True,linewidths= .5, vmax=1, ax=axes)\n\ncorr_matrix['Price'].sort_values(ascending=False);","500165df":"num_feat = np.array([ 'Living Area','Neighbors Living Area','Above the Ground Area', 'Basement Area', 'Total Area', 'Neighbors Total Area'])\ncat_feat = np.array(['Grade','Bathrooms','Bedrooms', 'Floors', 'View', 'Condition'])\n\ndef plot_bivar(features, the_plot, nrows, ncols):\n    '''\n    Use to plot some categorical features that seems look high correlated to the label ('Price')\n    with 4 parameters:\n    * features : The feature for visualization (pass as string in List)\n    * the_plot : the type of the graph\n    * nrows : rows number (for subplotting)\n    * ncols : cols number (for subplotting)\n    '''\n    fig, axs = plt.subplots(nrows, ncols, figsize=(20,15))\n\n    for r in range(0,nrows):\n        for c in range(0,ncols):  \n            i = r*ncols+c\n            if i < len(features):\n                the_plot(x = train_df[features[i]], y=train_df['Price'], ax = axs[r][c])\n\n    sns.despine()\n    plt.tight_layout()\n    \nplot_bivar(num_feat,sns.regplot,3,2);","531de47d":"test_df['Total Area'].max(), test_df['Living Area'].max(), test_df['Above the Ground Area'].max(), test_df['Neighbors Total Area'].max()","77754d97":"plot_bivar(cat_feat,sns.boxplot,3,2);","421c77b6":"test_df['Floors'].min(), test_df['Floors'].max()","dbdbf3b6":"test_df['Bathrooms'].min(), test_df['Bathrooms'].max()","40ec4f19":"test_df['Bedrooms'].min(), test_df['Bedrooms'].max()","38480d52":"from sklearn.feature_selection import SelectKBest, f_regression, RFE, RFECV\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder","a6cc1dde":"train_temp = train_df.copy()\ntest_temp = test_df.copy()","fff6d9fa":"train_temp.head()","b9fb118c":"# areas\ntrain_df['yard_area'] = train_df['Total Area'] - train_df['Living Area']\ntrain_df['neighbors_yard_area'] = train_df['Neighbors Total Area'] - train_df['Neighbors Living Area']\ntrain_df['ratio_total_area'] = train_df['Total Area'] \/ train_df['Neighbors Total Area']\ntrain_df['ratio_living_area'] = train_df['Living Area'] \/ train_df['Neighbors Living Area']\ntrain_df['room_ratio_per_floor'] = (train_df['Bedrooms'] + train_df['Bathrooms']) \/ train_df['Floors']\n\ntest_df['yard_area'] = test_df['Total Area'] - test_df['Living Area']\ntest_df['neighbors_yard_area'] = test_df['Neighbors Total Area'] - test_df['Neighbors Living Area']\ntest_df['ratio_total_area'] = test_df['Total Area'] \/ test_df['Neighbors Total Area']\ntest_df['ratio_living_area'] = test_df['Living Area'] \/ test_df['Neighbors Living Area']\ntest_df['room_ratio_per_floor'] = (test_df['Bedrooms'] + test_df['Bathrooms']) \/ test_df['Floors']","8b73d204":"# bathroom type\ntrain_df['bathroom_full'] = train_df['Bathrooms'].apply(lambda x: int(str(x).split('.')[0]))\ntrain_df['bathroom_partial'] = train_df['Bathrooms'].apply(lambda x: int(str(x).split('.')[1]))\ntrain_df['bathroom_partial'] = train_df['bathroom_partial'].apply(lambda x: 50 if x==5 else x)\n\ntest_df['bathroom_full'] = test_df['Bathrooms'].apply(lambda x: int(str(x).split('.')[0]))\ntest_df['bathroom_partial'] = test_df['Bathrooms'].apply(lambda x: int(str(x).split('.')[1]))\ntest_df['bathroom_partial'] = test_df['bathroom_partial'].apply(lambda x: 50 if x==5 else x)\n\ntrain_df['have_bath_partial'] = train_df['Bathrooms'].apply(lambda x: 1 if int(str(x).split('.')[1])!=0 else 0)\ntest_df['have_bath_partial'] = test_df['Bathrooms'].apply(lambda x: 1 if int(str(x).split('.')[1])!=0 else 0)","37884c73":"# floors special feature like loft\/attic\ntrain_df['have_more_floor'] = train_df['Floors'].apply(lambda x: 0 if int(str(x).split('.')[1])==0 else 1)\ntest_df['have_more_floor'] = test_df['Floors'].apply(lambda x: 0 if int(str(x).split('.')[1])==0 else 1)","72462b0f":"# is_renovated\ntrain_df['is_renovated'] = train_df['Year Renovated'].apply(lambda x: 1 if x>0 else 0)\n\ntest_df['is_renovated'] = test_df['Year Renovated'].apply(lambda x: 1 if x>0 else 0)","62d18750":"yr_blt_list = sorted(train_df['Year Built'].value_counts().keys())\nyr_blt_list","83b02d3d":"## --- year built ---\n\ndef period_mcrel(col):\n    '''\n    McRel_Standards:\n    * ...\n    * The Development of the Industrial United States (1870-1900)\n    * The Emergence of Modern America (1890-1930)\n    * The Great Depression and World War II (1929-1945)\n    * Postwar United States (1945 to early 1970s)\n    * Contemporary United States (1968 to the present)\n    '''\n#     if col<1931:\n#         return 'Development and Emergence'\n#     elif col<1945:\n#         return 'Great Depress'\n#     elif col<1970:\n#         return 'Postwar'\n#     return 'Contemporary'\n    if col<1931:\n        return 0\n    elif col<1945:\n        return 1\n    elif col<1970:\n        return 2\n    return 3\n    \ndef period_history(col):\n    '''\n    US History timeline:\n    * ...\n    * Progressive Era -> 1896\u20131916\n    * World War I -> 1917\u20131919\n    * Roaring Twenties -> 1920\u20131929\n    * Great Depression -> 1929\u20131941\n    * World War II -> 1941\u20131945\n    * Post-war Era -> 1945\u20131964\n    * Civil Rights Era -> 1965\u20131980\n    * Reagan Era -> 1981\u20131991\n    * Post-Cold War Era -> 1991\u20132008\n    * Modern Day -> 2008\u2013present\n    '''\n#     if col<1917:\n#         return 'Progressive Era'\n#     elif col<1920:\n#         return 'WW I'\n#     elif col<1930:\n#         return 'Roaring 20'\n#     elif col<1942:\n#         return 'Great Depress'\n#     elif col<1946:\n#         return 'WW II'\n#     elif col<1965:\n#         return 'Post War'\n#     elif col<1981:\n#         return 'Civil Rights'\n#     elif col<1992:\n#         return 'Reagan'\n#     elif col<2009:\n#         return 'Post Cold'\n#     return 'Modern Day'\n    if col<1917:\n        return 0\n    elif col<1920:\n        return 1\n    elif col<1930:\n        return 2\n    elif col<1942:\n        return 3\n    elif col<1946:\n        return 4\n    elif col<1965:\n        return 5\n    elif col<1981:\n        return 6\n    elif col<1992:\n        return 7\n    elif col<2009:\n        return 8\n    return 9\n    \ndef period_presidency(col):\n    '''\n    US Presidency:\n    * ...\n    * William McKinley: 1897\u20131901\n    * Theodore Roosevelt: 1901\u20131909\n    * William H. Taft: 1909\u20131913\n    * Woodrow Wilson: 1913\u20131921\n    * Warren Harding: 1921\u20131923\n    * Calvin Coolidge: 1923\u20131929\n    * Herbert Hoover: 1929\u20131933\n    * Franklin D. Roosevelt: 1933\u20131945\n    * Harry S. Truman: 1945\u20131953\n    * Dwight D. Eisenhower: 1953\u20131961\n    * John F. Kennedy: 1961\u20131963\n    * Lyndon B. Johnson: 1963\u20131969\n    * Richard M. Nixon: 1969\u20131974\n    * Gerald Ford: 1974\u20131977\n    * Jimmy Carter: 1977\u20131981\n    * Ronald Reagan: 1981\u20131989\n    * George H. W. Bush: 1989\u20131993\n    * Bill Clinton: 1993\u20132001\n    * George W. Bush: 2001\u20132009\n    * Barack Obama: 2009\u20132017\n    * ...\n    '''\n#     if col<1902:\n#         return 'W McKinley'\n#     elif col<1910:\n#         return 'T Roosevelt'\n#     elif col<1914:\n#         return 'W H Taft'\n#     elif col<1922:\n#         return 'Woodrow W'\n#     elif col<1924:\n#         return 'W Harding'\n#     elif col<1930:\n#         return 'C Coolidge'\n#     elif col<1934:\n#         return 'H Hoover'\n#     elif col<1946:\n#         return 'F D Roosevelt'\n#     elif col<1954:\n#         return 'H S Truman'\n#     elif col<1962:\n#         return 'D D Eisenhower'\n#     elif col<1964:\n#         return 'J F Kennedy'\n#     elif col<1970:\n#         return 'L B Johnson'\n#     elif col<1975:\n#         return 'R M Nixon'\n#     elif col<1978:\n#         return 'G Ford'\n#     elif col<1982:\n#         return 'J Carter'\n#     elif col<1990:\n#         return 'R Reagan'\n#     elif col<1994:\n#         return 'G H W Bush'\n#     elif col<2002:\n#         return 'B Clinton'\n#     elif col<2010:\n#         return 'G W Bush'\n#     return 'B Obama'\n    if col<1902:\n        return 0\n    elif col<1910:\n        return 1\n    elif col<1914:\n        return 2\n    elif col<1922:\n        return 3\n    elif col<1924:\n        return 4\n    elif col<1930:\n        return 5\n    elif col<1934:\n        return 6\n    elif col<1946:\n        return 7\n    elif col<1954:\n        return 8\n    elif col<1962:\n        return 9\n    elif col<1964:\n        return 10\n    elif col<1970:\n        return 11\n    elif col<1975:\n        return 12\n    elif col<1978:\n        return 13\n    elif col<1982:\n        return 14\n    elif col<1990:\n        return 15\n    elif col<1994:\n        return 16\n    elif col<2002:\n        return 17\n    elif col<2010:\n        return 18\n    return 19\n    \ndef period_political(col):\n    '''\n    US Political Parties history:\n    * ...\n    * Fourth Party System (c. 1896\u2013c. 1932)\n    * Fifth Party System (c. 1933\u20131964)\n    * Sixth Party System (c.1964(?)\u2013present)\n    '''\n#     if col<1933:\n#         return '4th Party System'\n#     elif col<1965:\n#         return '5th Party System'\n#     return '6th Party System'\n    if col<1933:\n        return 0\n    elif col<1965:\n        return 1\n    return 2","0d993349":"# action\ntrain_df['build_period_McRel'] = train_df['Year Built'].apply(period_mcrel)\ntrain_df['build_period_US_history'] = train_df['Year Built'].apply(period_history)\ntrain_df['build_period_Presidency'] = train_df['Year Built'].apply(period_presidency)\ntrain_df['build_period_Political'] = train_df['Year Built'].apply(period_political)\n\ntest_df['build_period_McRel'] = test_df['Year Built'].apply(period_mcrel)\ntest_df['build_period_US_history'] = test_df['Year Built'].apply(period_history)\ntest_df['build_period_Presidency'] = test_df['Year Built'].apply(period_presidency)\ntest_df['build_period_Political'] = test_df['Year Built'].apply(period_political)\n\n# train_df['build_4_period'] = train_df.build_period_Presidency+train_df.build_period_US_history+train_df.build_period_McRel+train_df.build_period_Political\n# test_df['build_4_period'] = test_df.build_period_Presidency+test_df.build_period_US_history+test_df.build_period_McRel+test_df.build_period_Political\n\n# train_df = train_df.drop(['build_period_McRel','build_period_US_history','build_period_Presidency','build_period_Political'], axis=1)\n# test_df = test_df.drop(['build_period_McRel','build_period_US_history','build_period_Presidency','build_period_Political'], axis=1)\n\n# le = LabelEncoder()\n# train_df['build_4_period'] = le.fit_transform(train_df[['build_4_period']])\n# test_df['build_4_period'] = le.transform(test_df[['build_4_period']])","8e7d0abe":"train_df.head()","9888c5d3":"zipcode_list = sorted(train_df.Zipcode.value_counts().keys())\nzipcode_list","b4606688":"# import requests\n# from bs4 import BeautifulSoup as bs\n\n# url = 'http:\/\/www.ciclt.net\/sn\/clt\/capitolimpact\/gw_ziplist.aspx?FIPS=53033'\n# r = requests.get(url) \n# soup = bs(r.content,'lxml')\n# table = soup.find('table')\n\n# output_rows = []\n# for table_row in table.findAll('tr'):\n#     columns = table_row.findAll('td')\n#     output_row = []\n#     for column in columns:\n#         output_row.append(column.text)\n#     output_rows.append(output_row)","72e0ad70":"zipcode_city = '''98001 Algona 98001 Auburn 98001 Federal_Way 98002 Auburn 98003 Auburn 98003 Federal_Way 98004 Beaux_Arts_Village \\\n98004 Bellevue 98004 Clyde_Hill 98004 Hunts_Point 98004 Yarrow_Point 98005 Bellevue 98006 Bellevue 98007 Bellevue 98008 Bellevue \\\n98009 Bellevue 98010 Black_Diamond 98011 Bothell 98013 Burton 98013 Vashon 98014 Carnation 98015 Bellevue 98019 Duvall 98022 Enumclaw \\\n98023 Auburn 98023 Federal_Way 98024 Fall_City 98025 Hobart 98027 Issaquah 98028 Kenmore 98028 Bothell 98029 Issaquah 98030 Kent \\\n98031 Kent 98032 Kent 98033 Kirkland 98034 Kirkland 98035 Kent 98038 Maple_Valley 98039 Medina 98040 Mercer_Island 98041 Bothell \\\n98042 Covington 98042 Kent 98045 North_Bend 98047 Auburn 98047 Pacific 98050 Preston 98051 Ravensdale 98052 Redmond 98053 Redmond \\\n98054 Redondo 98055 Renton 98056 Newcastle 98056 Renton 98057 Renton 98058 Renton 98059 Newcastle 98059 Renton 98062 Seahurs \\\n98063 Auburn 98063 FederalWay 98064 Kent 98065 Snoqualmie 98068 Snoqualmie_Pass 98068 Snoqualmie 98070 Vashon 98071 Auburn \\\n98072 Woodinville 98073 Redmond 98074 Sammamish 98074 Redmond 98075 Sammamish 98075 Issaquah 98083 Kirkland 98092 Auburn \\\n98093 Auburn 98093 Federal_Way 98101 Seattle 98102 Seattle 98103 Seattle 98104 Seattle 98105 Seattle 98106 Seattle 98107 Seattle \\\n98108 Seattle 98108 Tukwila 98109 Seattle 98111 Seattle 98112 Seattle 98114 Seattle 98115 Seattle 98116 Seattle 98117 Seattle \\\n98118 Seattle 98119 Seattle 98121 Seattle 98122 Seattle 98124 Seattle 98125 Seattle 98126 Seattle 98131 Seattle 98132 Seattle \\\n98133 Seattle 98133 Shoreline 98134 Seattle 98136 Seattle 98138 Seattle 98138 Tukwila 98144 Seattle 98145 Seattle 98146 Burien \\\n98146 Seattle 98148 Burien 98148 Des_Moines 98148 Normandy_Park 98148 Seatac 98148 Seattle 98154 Seattle 98155 Lake_Forest_Park \\\n98155 Lake_Forest_Park 98155 Lake_Forest_Park 98155 Seattle 98155 Shoreline 98158 Seatac 98158 Seattle 98160 Seattle 98161 Seattle \\\n98164 Seattle 98166 Burien 98166 Normandy_Park 98166 Seattle 98168 Burien 98168 Seatac 98168 Seattle 98168 Tukwila 98171 Seattle \\\n98174 Seattle 98177 Seattle 98177 Shoreline 98178 Seattle 98178 Tukwila 98188 Seatac 98188 Seattle 98188 Tukwila 98198 Des_Moines \\\n98198 Normandy_Park 98198 Seatac 98198 Seattle 98199 Seattle 98224 Baring 98288 Skykomish 98077 Cottage_Lake'''","ec5e424a":"z_city_list = zipcode_city.split(' ')","271ba188":"zcity_df = pd.DataFrame({'zipcode':z_city_list[::2],\n                        'city':z_city_list[1::2]})","7bd58201":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\n# pd.reset_option('display.max_rows')\n# pd.reset_option('all')","66fa7e94":"grp_zp = zcity_df.groupby('zipcode')['city'].apply(','.join).reset_index()\ngrp_zp['zipcode'] = grp_zp['zipcode'].astype(int)","e303cc5b":"grp_zp","593a9cf3":"# zipcode city\ntrain_df = train_df.merge(grp_zp.set_index('zipcode'), how='left', left_on='Zipcode', right_on='zipcode')\ntest_df = test_df.merge(grp_zp.set_index('zipcode'), how='left', left_on='Zipcode', right_on='zipcode')\n\ntrain_df['city>1'] = train_df.city.apply(lambda x: 1 if len(x.split(','))>1 else 0)\ntest_df['city>1'] = test_df.city.apply(lambda x: 1 if len(x.split(','))>1 else 0)","a91a26cc":"train_df.head()","5446f592":"train_df.shape, test_df.shape","8e752a0c":"train_df.city.nunique()","6ef0e19a":"oh = OneHotEncoder(drop='first')","d598e56a":"pd.DataFrame(oh.fit_transform(train_df[['city']]).toarray()).head()","6257e9f1":"le = LabelEncoder()\ntrain_df['city'] = le.fit_transform(train_df[['city']])\ntest_df['city'] = le.transform(test_df[['city']])","1631c9e7":"test_df.head()","2a0614c8":"corr_matrix = train_df.corr(method='spearman')\nplt.figure(figsize=(21,17))\nsns.heatmap(corr_matrix.sort_values(by='Price', ascending=False).sort_values(by='Price', ascending=False, axis=1), \n            cmap='PuBuGn', annot=True,linewidths= .5, vmax=1, cbar=False);","2ee6a44d":"corr_matrix = train_df.corr(method='spearman')\ncorr_matrix['Price'].sort_values(ascending=False)","a0b42acb":"# First candidate (Nope because we also have it in test data)\ntrain_df[train_df['Neighbors Total Area'] > 60000]","81103a0f":"test_df[test_df['Neighbors Total Area'] == 80937]","7a597472":"# second candidate (nope-biased)\ntrain_df[train_df['Total Area'] > 110000]","e827798d":"# third candidate (nope-biased)\ntrain_df[train_df['Living Area'] > 900]","3dea5726":"test_df[test_df['Above the Ground Area'] > 700]","70f7bb72":"test_df[test_df['Living Area'] > 800]","7ba6c6a5":"# Fourth candidate (outliers)\ntrain_df[train_df['Bedrooms'] > 11]","fa169b2f":"# train_df = train_df[train_df['Bedrooms'] < 10]","3a5fe582":"from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nnp.random.seed(101)","bab13c56":"# trt_df = train_df.replace([np.inf, -np.inf], np.nan)","1049d27c":"train_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","6fc8fc84":"X = train_df.drop(['ID', 'Price'], axis=1)\ny = train_df['Price']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","711985c6":"#Validation function\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=101).get_n_splits(X)\n\ndef rmse(y_true, y_pred, sample_weight=None):\n    return np.sqrt(mean_squared_error(y_true, y_pred, sample_weight=sample_weight))\n\ndef rmse_cv(model, X, y, fit_params=None):\n    rmse_score= cross_val_score(model, X, y, scoring=make_scorer(rmse, greater_is_better=False), cv = skf, fit_params=fit_params)\n    return print(f'{rmse_score}\\n--> {rmse_score.mean():.4f}')\n\ndef acc_cv(model, X, y, fit_params=None):\n    score = cross_val_score(model, X, y, cv = skf, fit_params=fit_params)\n    return print(f'{score}\\n--> {score.mean()*100:.2f}%')\n\ndef get_best_param(model, param, X, y, theSearch='Random', n_iter=10):\n    if theSearch == 'Grid':\n        grSCV = GridSearchCV(model, param, scoring=make_scorer(rmse, greater_is_better=False), cv=skf, verbose=0)\n        grSCV.fit(X, y)\n        return grSCV.best_params_, '--> {:.4f}'.format(grSCV.best_score_)\n    elif theSearch == 'Random':\n        ranSCV = RandomizedSearchCV(model, param, scoring=make_scorer(rmse, greater_is_better=False), cv=skf, verbose=0, n_iter=n_iter, refit=True, return_train_score=False)\n        ranSCV.fit(X, y)\n        return ranSCV.best_params_, '--> {:.4f}'.format(ranSCV.best_score_)\n    return None","adeac11e":"param_gridsearch = {\n    'learning_rate' : [0.01, 0.1, 1],\n    'max_depth' : [5, 10, 15],\n    'n_estimators' : [5, 20, 35], \n    'num_leaves' : [5, 25, 50],\n    'boosting_type': ['gbdt', 'dart'],\n    'colsample_bytree' : [0.6, 0.75, 1],\n    'reg_lambda': [0.01, 0.1, 1],\n}\n\nparam_random = {\n    'learning_rate': list(np.logspace(np.log(0.01), np.log(1), num = 500, base=3)),\n    'max_depth': list(range(5, 15)),\n    'min_child_weight': randint(30, 60),\n    'subsample': uniform(0.4, 0.2),\n    'n_estimators': list(range(5, 35)),\n    'num_leaves': list(range(5, 50)),\n    'boosting_type': ['gbdt', 'dart'],\n    'colsample_bytree': list(np.linspace(0.6, 1, 500)),\n    'reg_lambda': list(np.linspace(0, 1, 500)),\n    'reg_alpha': uniform(1, 2)\n}\n\nparams = {\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'mse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }","8b33c0d4":"lm= make_pipeline(RobustScaler(), LinearRegression(normalize=True))\n\nlasR = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, normalize=True))\n\neNet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, normalize=True))\n\nxgbMod = XGBRegressor(max_depth = 3, n_estimators = 1000, colsample_bytree=0.5, missing=True, verbosity=0)\n\nlgbMod = LGBMRegressor(num_leaves=10,learning_rate=0.05, n_estimators=3500,max_bin = 500, bagging_fraction = 0.9,\n                       bagging_freq = 5, feature_fraction = 0.3,feature_fraction_seed=1, bagging_seed=3,min_data_in_leaf =1, \n                       min_sum_hessian_in_leaf = 11, tree_learner='data', force_col_wise=True, max_depth=500)","ce673fdf":"np.any(np.isnan(X)),np.all(np.isfinite(X)), np.any(np.isnan(y)),np.all(np.isfinite(y))","af5f4649":"select_feature = SelectKBest(f_regression, k=15).fit(X, y)\n\nprint('Score list:', select_feature.scores_)\n# print('Feature list:', X_train.columns)\n# print(select_feature.get_support(indices=False))\n# print(select_feature.get_params(deep=True))\nX_selected = select_feature.fit_transform(X, y)\n# print(X_selected.shape)\nprint(X.columns[select_feature.get_support()])","f52ce2d4":"slc_kbest = pd.DataFrame([i for i in zip(X.columns, select_feature.scores_)])\nslc_kbest.columns = ['features', 'score']\nslc_kbest = slc_kbest.sort_values(by='score', ascending=False).reset_index(drop=True)\nslc_kbest","cad0ce24":"rfe = RFE(estimator=lgbMod, n_features_to_select=15, step=5)\nrfe = rfe.fit(X, y)\n\nprint('Chosen best 15 feature by rfe:', X.columns[rfe.support_])","249c78c6":"slc_rfe = pd.DataFrame([i for i in zip(X.columns, rfe.ranking_)])\nslc_rfe.columns = ['features', 'rank']\nslc_rfe = slc_rfe.sort_values(by='rank', ascending=True).reset_index(drop=True)\nslc_rfe","d96e501c":"# clf_rf_4 = RandomForestRegressor() \nrfecv = RFECV(estimator=lgbMod, step=10, cv=skf, scoring=make_scorer(rmse, greater_is_better=False)) \nrfecv = rfecv.fit(X, y)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X.columns[rfecv.support_])\nprint('Features Ranking :', rfecv.ranking_)","7b37c8a3":"plt.figure()\nplt.xlabel('Number of features selected')\nplt.ylabel('Cross validation score of number of selected features')\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show();","6f78efa6":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif['variables'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","8e767fe1":"calc_vif(X.drop(['is_renovated', 'build_period_Presidency'], axis=1))","10aa64e9":"calc_vif(X)","a12e1cba":"# p={'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n#  get_best_param(lm, p, x_train, y_train, theSearch='sd')\n# par={'min_sum_hessian_in_leaf':np.array([0,1,11,20,50,30])}\n# get_best_param(lgbMod, par, x_train, y_train)","ee37b1ce":"X_oth = X.drop(['is_renovated', 'build_period_Presidency'], axis=1)","1959a123":"# acc_cv(lm)\n# rmse_cv(lm)\n\n# acc_cv(lasR)\n# rmse_cv(lasR)\n\n# acc_cv(eNet)\n# rmse_cv(eNet)\n\n# acc_cv(xgbMod)\n# rmse_cv(xgbMod)\n\nacc_cv(lgbMod, X_oth, y)\nrmse_cv(lgbMod, X_oth, y)","5288c0db":"from sklearn.ensemble import StackingRegressor\n\nlevel0 = list()\nlevel0.append(('lasso', lasR))\nlevel0.append(('eNet', eNet))\nlevel0.append(('xgb', xgbMod))\nlevel0.append(('lgb', lgbMod))\n\n# define the stacking ensemble\nstack_model = StackingRegressor(estimators=level0, final_estimator=lm, cv=5)","05261c0e":"acc_cv(stack_model, X_oth, y)\nrmse_cv(stack_model, X_oth, y)","d014e52e":"full_train = X.drop(['is_renovated', 'build_period_Presidency'], axis=1)\nX_test = test_df[full_train.columns]\nid_test = test_df['ID']","f9dedf2c":"full_train.shape, X_test.shape","7dad22b9":"from sklearn.ensemble import StackingRegressor\n\nlevel0 = list()\nlevel0.append(('lasso', lasR))\nlevel0.append(('eNet', eNet))\nlevel0.append(('xgb', xgbMod))\nlevel0.append(('lgb', lgbMod))\n\nstack_model_r = StackingRegressor(estimators=level0, final_estimator=lm, cv=5)\n\nstack_model_r.fit(full_train, y)","338fe206":"stack_pred = stack_model_r.predict(X_test)","f059bcc8":"# sns.distplot((y_test-y_predict), bins=30)\n# coeffecients = pd.DataFrame(lm.coef_,x_train.columns)\n# coeffecients.columns = ['Coeffecient']\n# coeffecients.sort_values(by='Coeffecient', ascending=False)","fc4e9c7f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.preprocessing import Normalizer\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as krs\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","fb65415c":"scaler = RobustScaler()\nnormalz = Normalizer()","b60e52a1":"X_train_scaled = scaler.fit_transform(X_oth)\nX_test_scaled = scaler.transform(X_test)\n\nX_train_norm = normalz.fit_transform(X_oth)\nX_test_norm = normalz.transform(X_test)","4fb81e37":"plt.figure(figsize=(10,6))\nsns.distplot(X_train_norm, color='Purple', fit=norm)\nmu, sigma = norm.fit(X_train_norm)\n# plt.legend(['Normal dist. ($\\mu=$ {:.2f} \/\/ $\\sigma=$ {:.2f} \/\/ \\nSkewness: {:.2f} \/\/ Kurtosis: {:.2f} )'\n#             .format(mu, sigma, X_train_norm.skew(), X_train_norm.kurt())]);","ae364e3c":"early_stop = EarlyStopping(monitor='val_loss', \n                           patience=5, \n                           mode='min', \n                           restore_best_weights=False)\n\ncheck_point = ModelCheckpoint('house_price_pred_z.h5', \n                              monitor='val_loss', \n                              save_best_only=False)\n\nlr_plateau = ReduceLROnPlateau(monitor='val_loss', \n                               patience=3,\n                               factor=.2, \n                               min_lr=1e-6)","7e5e7b2c":"opt_adam = Adam(learning_rate=1e-5)\n\ndef rmse_ann(y_true, y_pred):\n    return krs.sqrt(krs.mean(krs.square(y_pred - y_true))) ","f2ba8ec0":"def build_ann_regressor():\n    model = Sequential()\n\n    model.add(Dense(18, activation='relu', kernel_initializer='random_uniform', input_dim=32))\n    model.add(Dense(14, activation='relu', kernel_initializer='random_uniform'))\n    model.add(Dropout(.2))\n    model.add(Dense(9, activation='relu', kernel_initializer='random_uniform'))\n    model.add(Dense(7, activation='relu', kernel_initializer='random_uniform'))\n    model.add(Dropout(.2))\n    model.add(Dense(4, activation='relu', kernel_initializer='random_uniform'))\n    model.add(Dropout(.2))\n    model.add(Dense(2, activation='relu', kernel_initializer='random_uniform'))\n\n    model.add(Dense(1, kernel_initializer='random_uniform'))\n\n    model.compile(optimizer=opt_adam, loss=rmse_ann, metrics=['accuracy'])\n\n    return model","8aa6f744":"plot_model(build_ann_regressor(), show_shapes=True, show_layer_names=True)","b1827113":"ann_reg = KerasRegressor(build_fn=build_ann_regressor, batch_size=16, nb_epoch=175)","0c446e27":"acc_cv(ann_reg, X_train_norm, y, fit_params={'callbacks': [lr_plateau, early_stop]})\nrmse_cv(ann_reg, X_train_norm, y, fit_params={'callbacks': [lr_plateau, early_stop]})","fbbca0aa":"submission_data = pd.read_csv('..\/input\/dasprodatathon\/sample_submission.csv')\nsubmission_data['ID'] = id_test\nsubmission_data['Price'] = stack_pred\n\nsubmission_data.to_csv('submit_house_price_predict.csv', index=False)","2eef158d":"submission_data.tail()","703a7a7d":"#### Evaluate Machine Learning Models","c411edbd":"# The Datas","d468ab6d":"#### Feature Engineering","10e15a1b":"# Price Predict House (Regression Problem)","8e9ab3a3":"Preprocess for use in the Deep Learning","e4b1685e":"The data to submit","02123971":"#### Stacking (Ensemble) Regressor","b29475c3":"# Predict Real Test Data","4ac54b1a":"Hyperparameter Tuning","16c17eec":"#### Multicollinearity","18099a00":"Encoding","b4ca8a68":"# EDA","d27823ca":"# Evaluate ANN","655f8001":"In this notebook we use to method: \n* By traditional Machine Learning\n* Try to use Deep Learning (ANN)","e96aebf9":"#### Assume that we have outliers","b43e3808":"Check again","a2f6c811":"# Use Deep Learning (ANN)","0130b6b9":"Actually, the datasets provided for this notebook are quite similar to [House Sales in King County, USA](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction) but without time(date) relation and also a little bit customization value in the datas.","3a2742a6":"#### Feature Selection","83fabde9":"Do it before train the data with full model and predict on real test set","d8f0e67e":"# Validation model","7ca03d10":"# Finishing","09f9a002":"# Preprocessing","8b17acb8":"# ANN Model"}}