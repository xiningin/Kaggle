{"cell_type":{"01c646d5":"code","f50df3f4":"code","698fe1b6":"code","5b8470d7":"code","87a52889":"code","69486fe2":"code","f3a3e8da":"code","053339a3":"code","7543a392":"code","3fde972c":"code","d16a4303":"code","82d8c509":"code","64ab3ed8":"code","ec253e28":"code","0a8357ac":"code","3ded2644":"code","1862f827":"code","d5db0fe1":"code","bc758154":"code","4279d5c4":"code","fb0e1437":"code","97d49875":"code","7cf0da46":"code","02e29c16":"code","3aecc55d":"code","67726426":"code","4dbba5bf":"code","ddbf9a77":"code","b493b9e1":"code","bc4599d1":"code","6900b1b6":"code","0f90d1e2":"code","7d645d8f":"code","b2cda723":"code","18a97b9e":"code","c37dfd43":"code","cd53f834":"code","3382f162":"code","dd6f2a84":"code","0a868892":"code","f4ce18f3":"code","6e2cf017":"code","def606c7":"code","901e13e0":"code","d65f076f":"code","65330cac":"code","a680633a":"code","ee443a28":"code","6b397cd6":"code","8a94498f":"code","c327729e":"code","615ee455":"code","ab038325":"code","483a3790":"code","6ceb3811":"code","3295d6ed":"code","b11b592e":"code","6a3693ef":"code","f534bcfa":"code","d8dc160c":"code","bad214cf":"code","685fbb29":"code","e9881f76":"code","37d83211":"code","b0e45cf9":"code","a85dc5b8":"code","85882902":"code","dae9dc9d":"code","7fded1af":"code","40e6ae12":"code","a52cf57a":"code","f661131e":"code","8367db45":"code","f23dc4ee":"code","5488cad0":"code","3c9c8342":"code","94ba65d8":"code","fd1f850c":"code","bfb7ccef":"code","929d0e79":"code","8c02fcd0":"code","ed25ea22":"code","56df84f0":"code","31fda351":"code","96c8ef4d":"code","b2915c1c":"code","65fef30c":"code","1b88c283":"code","6a4272fa":"code","98352c6b":"code","97dded89":"markdown","abce6013":"markdown","dbe10a01":"markdown","355aa09e":"markdown","c8411713":"markdown","f90ed1ca":"markdown","128ed207":"markdown","55920dc2":"markdown","8285b77b":"markdown","5160e1a9":"markdown","aa7188fe":"markdown","fcb00a6a":"markdown","be4dee54":"markdown","21fcd31b":"markdown","e7d49164":"markdown","5dd28b17":"markdown","3f02b57d":"markdown","007c7951":"markdown","358d47cc":"markdown","ff0d988a":"markdown","17f86005":"markdown","1a31c1f5":"markdown","e1b4a00d":"markdown","21cfbb30":"markdown","6eae9291":"markdown","77e34cd3":"markdown","f7ff573e":"markdown","fadc2e28":"markdown","9b104497":"markdown","58f95767":"markdown","3a9164fc":"markdown","e82938d3":"markdown","e4f6a674":"markdown"},"source":{"01c646d5":"#import libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f50df3f4":"#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","698fe1b6":"#import data\ncar_data = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')","5b8470d7":"#look at formatting of entries\ncar_data.head()","87a52889":"#look at null count and dtype\ncar_data.info()","69486fe2":"#numerical features\nnumerical = [\n    'Year',\n    'Present_Price',\n    'Kms_Driven',\n    'Selling_Price'\n]\n\n#categorical features\ncategorical = [\n    'Car_Name',\n    'Fuel_Type',\n    'Seller_Type',\n    'Transmission',\n    'Owner'\n]","f3a3e8da":"#look at distribution of data\ncar_data.describe()","053339a3":"#look at outliers in selling price as a percentage\npercentage=(len(car_data.Selling_Price[np.abs(stats.zscore(car_data.Selling_Price)) >= 3])\/len(car_data))*100\nprint('Percentage of Selling_Price outliers >= 3 std from the mean: {}%'.format(percentage))","7543a392":"#look at number of outliers greater than or equal to 3 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 3]","3fde972c":"#look at number of outliers greater than or equal to 4 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 4]","d16a4303":"#look at number of outliers greater than or equal to 5 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 5]","82d8c509":"#look at number of outliers greater than or equal to 6 std from mean\ncar_data[numerical][np.abs(stats.zscore(car_data[numerical])) >= 6]","64ab3ed8":"#selling price outliers visualized\nsns.boxplot(x=car_data['Selling_Price'])\nplt.xlabel('Selling_Price')","ec253e28":"#present price outlier visualized\nsns.boxplot(x=car_data['Present_Price'])\nplt.xlabel('Present_Price')","0a8357ac":"#kms driven outlier visualized\nsns.boxplot(x=car_data['Kms_Driven'])\nplt.xlabel('Kms_Driven')","3ded2644":"#capitalize all car names\nfor name in car_data['Car_Name']:\n    car_data = car_data.replace(name,name.title())","1862f827":"#look for anything that needs to be fixed\nvalues,counts=np.unique(car_data['Car_Name'],return_counts=True)\nunique_cars_counts = pd.DataFrame({'car names':values, 'counts':counts})\nvalues","d5db0fe1":"#reassign categorical names to numbers\ncar_data = car_data.replace('Petrol',0)\ncar_data = car_data.replace('Diesel',1)\ncar_data = car_data.replace('CNG',2)\n\ncar_data = car_data.replace('Dealer',0)\ncar_data = car_data.replace('Individual',1)\n\ncar_data = car_data.replace('Manual',0)\ncar_data = car_data.replace('Automatic',1)","bc758154":"#replace car names with numbers\nfor i in unique_cars_counts['car names']:\n    idx = pd.Index(unique_cars_counts['car names'])\n    car_data = car_data.replace(i,idx.get_loc(i))","4279d5c4":"#assign categorical variables to int dtype\ncar_data[categorical].astype('int64')","fb0e1437":"#look at numerical data distribution\nfor i in car_data[numerical].columns:\n    plt.hist(car_data[numerical][i], edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of cars')\n    plt.show()","97d49875":"#look at categorical data distribution\nfor i in car_data[categorical].columns:\n    plt.hist(car_data[categorical][i], edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of cars')\n    plt.show()","7cf0da46":"#heat map to find extreme positive and negative correlations in numerical data\nplt.figure(figsize=(16, 6))\nsns.heatmap(car_data[numerical].corr(), annot=True)\nplt.title('Correlation Heatmap for Numerical Variables', fontdict={'fontsize':12}, pad=12);","02e29c16":"#look at how target is distributed among variables\nsns.pairplot(car_data)\nplt.legend()\nplt.show()","3aecc55d":"#lmplot comparing year and kms driven (-0.52 corr)\nsns.lmplot(x='Year', y='Kms_Driven',data=car_data)\n\n#settings to display all markers\nxticks, xticklabels = plt.xticks()\nxmin = 2002\nxmax = 2019\nplt.xlim(xmin, xmax)\nplt.xticks(xticks)\n\nplt.show()","67726426":"#violin plot comparing selling price and seller type\nsns.violinplot(y='Selling_Price',\n              x='Seller_Type', data = car_data)\nplt.show()","4dbba5bf":"#stripplot comparing selling price and fuel type\nsns.stripplot(y=car_data['Selling_Price'],\n              x=car_data['Fuel_Type'])\nplt.show()","ddbf9a77":"#lmplot comparing selling price and present price (0.88 corr)\nsns.lmplot(x='Selling_Price', y='Present_Price',data=car_data)\n\n#settings to display all markers\nxmin = -2\nxmax = 37\nplt.xlim(xmin, xmax)\n\nplt.show()","b493b9e1":"#vionlinplot comparing present price and seller type\nsns.violinplot(y='Present_Price',\n              x='Seller_Type', data = car_data)\nplt.show()","bc4599d1":"#change dtype of categorical features to object\ncar_data[categorical]=car_data[categorical].astype('object')\n\n#copy of variables and target\nX = car_data.copy().drop('Selling_Price', axis=1)\ny = car_data.pop('Selling_Price')\n\n#remove Selling_Price from numerical variables\nnumerical.remove('Selling_Price')","6900b1b6":"X.info()","0f90d1e2":"#create dummy variables for categorical variables\ncar_data_dum = pd.get_dummies(X, drop_first=True)","7d645d8f":"#generate OLS Regression Results\nimport statsmodels.api as sm\n\nX_sm = sm.add_constant(car_data_dum)\nmodel = sm.OLS(y,X_sm)\nmodel.fit().summary()","b2cda723":"X_mi = X.copy()","18a97b9e":"#label encoding for categorical variables\nfor colname in X_mi.select_dtypes(\"object\"):\n    X_mi[colname], _ = X_mi[colname].factorize()\n\n#all discrete features have int dtypes\ndiscrete_features = X_mi.dtypes == int","c37dfd43":"#some continuous variables also have int dtypes\ndiscrete_features[['Year','Kms_Driven']] = False","cd53f834":"#use regression since the target variable is continuous\nfrom sklearn.feature_selection import mutual_info_regression\n\n#define a function to produce mutual information scores\ndef make_mi_scores(X_mi, y, discrete_features):\n    mi_scores = mutual_info_regression(X_mi, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_mi.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n#compute mutual information scores\nmi_scores = make_mi_scores(X_mi, y, discrete_features)\nmi_scores","3382f162":"#define a function to plot mutual information scores\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n#plot the scores\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","dd6f2a84":"#plot selling_price against car_name\nfig, ax = plt.subplots(figsize=(12,4))\nsns.scatterplot(x=X_mi.Car_Name, y=y, ax=ax)\n\n# ax.text(15,33,\"{}\".format(values[15]))\n# ax.text(24,35,\"{}\".format(values[24]))\n\n#add names and arrows to highest values\nax.annotate(\"{}\".format(values[15]), xy=(15,33), xytext=(30,15), arrowprops=dict(facecolor='black',shrink=0.05))\nax.annotate(\"{}\".format(values[24]), xy=(24,35), xytext=(39,30), arrowprops=dict(facecolor='black',shrink=0.05))\n\n\nplt.show()","0a868892":"#import ML preprocessing packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler","f4ce18f3":"#one hot encoder for categorical variables\nencoder=OneHotEncoder(handle_unknown='error', drop='first')\nX = pd.concat([X[numerical],pd.get_dummies(X[categorical], drop_first=True)],axis=1)\nfeature_names = X.columns\n\n# train\/test split with stratify making sure classes are evenlly represented across splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n\n#numerical pipeline\nscaler=MinMaxScaler()\n\n#apply scaler to numerical data\nX_train[numerical] = scaler.fit_transform(X_train[numerical])\nX_test[numerical] = scaler.transform(X_test[numerical])","6e2cf017":"#import ML packages\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std","def606c7":"#LinearRegression mean cross-validation\nlm = LinearRegression()\nlm.fit(X_train, y_train)\ncv = cross_val_score(lm,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('LinearRegression')\nprint(mean(cv), '+\/-', std(cv))","901e13e0":"#Lasso mean cross-validation\nlm_l = Lasso(random_state = 1)\ncv = cross_val_score(lm_l,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('Lasso')\nprint(mean(cv), '+\/-', std(cv))","d65f076f":"#Ridge mean cross-validation\nrid = Ridge(random_state = 1)\ncv = cross_val_score(rid,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('Ridge')\nprint(mean(cv), '+\/-', std(cv))","65330cac":"#ElasticNet mean cross-validation\nenr = ElasticNet(random_state = 1)\ncv = cross_val_score(enr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('ElasticNet')\nprint(mean(cv), '+\/-', std(cv))","a680633a":"#RandomForestRegressor mean cross-validation\nrf = RandomForestRegressor(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('RandomForestRegressor')\nprint(mean(cv), '+\/-', std(cv))","ee443a28":"#GradientBoostingRegressor mean cross-validation\ngbr = GradientBoostingRegressor(random_state = 1)\ncv = cross_val_score(gbr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('GradientBoostingRegressor')\nprint(mean(cv), '+\/-', std(cv))","6b397cd6":"#SVR mean cross-validation\nsvr = SVR()\ncv = cross_val_score(svr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('SVR')\nprint(mean(cv), '+\/-', std(cv))","8a94498f":"#ml algorithm tuner\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\n#performance reporting function\ndef clf_performance(regressor, model_name):\n    print(model_name)\n    print('Best Score: {} +\/- {}'.format(str(regressor.best_score_),str(regressor.cv_results_['std_test_score'][regressor.best_index_])))\n    print('Best Parameters: ' + str(regressor.best_params_))","c327729e":"#LinearRegression GridSearchCV\nlm = LinearRegression()\nparam_grid = {\n                'fit_intercept':[True,False],\n                'normalize':[True,False],\n                'copy_X':[True, False]\n}\nclf_lm = GridSearchCV(lm, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_lm = clf_lm.fit(X_train,y_train)\nclf_performance(best_clf_lm,'LinearRegressor')","615ee455":"#determine optimal lasso alpha value\nalpha = []\nerror = []\n\nfor i in range(1,100):\n    alpha.append(i\/5000)\n    lm_l = Lasso(random_state = 1,alpha=(i\/5000))\n    error.append(np.mean(cross_val_score(lm_l,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)))\n    \nplt.plot(alpha,error)\n\nplt.show()","ab038325":"#print optimal alpha value\nerr = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns=['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","483a3790":"#Lasso GridSearchCV\nlm_l = Lasso(random_state = 1)\nparam_grid = {\n                'alpha':[0.0038],\n                'fit_intercept':[True,False],\n                'normalize':[True, False],\n                'copy_X':[True, False]\n}\nclf_lm_l = GridSearchCV(lm_l, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_lm_l = clf_lm_l.fit(X_train,y_train)\nclf_performance(best_clf_lm_l,'Lasso')","6ceb3811":"#Ridge GridSearchCV\nrid = Ridge(random_state = 1)\nparam_grid = {\n                'fit_intercept':[True,False],\n                'normalize':[True, False],\n                'copy_X':[True, False],\n                'solver': ['auto','svd','cholesky','lsqr','sparse_cg','sag','saga']\n}\nclf_rid = GridSearchCV(rid, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_rid = clf_rid.fit(X_train,y_train)\nclf_performance(best_clf_rid,'Ridge')","3295d6ed":"#determine optimal elasticnet alpha value\nalpha = []\nerror = []\n\nfor i in range(1,100):\n    alpha.append(i\/10000)\n    enr = ElasticNet(random_state = 1,alpha=(i\/10000))\n    error.append(np.mean(cross_val_score(enr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)))\n    \nplt.plot(alpha,error)\n\nplt.show()","b11b592e":"#print optimal alpha value\nerr = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns=['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","6a3693ef":"#ElasticNet GridSearchCV\nenr = ElasticNet(random_state = 1)\nparam_grid = {\n                'alpha':[0.0018],\n                'fit_intercept':[True,False],\n                'normalize':[True, False],\n                'copy_X':[True, False],\n}\nclf_enr = GridSearchCV(enr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_enr = clf_enr.fit(X_train,y_train)\nclf_performance(best_clf_enr,'ElasticNet')","f534bcfa":"#RanddomForestRegressor GridSearchCV\nrf = RandomForestRegressor(random_state = 1)\nparam_grid = {\n                'n_estimators': [385] , \n                'bootstrap': [True],\n                'max_depth': [9],\n                'max_features': ['auto'],\n                'min_samples_leaf': [1,],\n                'min_samples_split': [2]\n              }\nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,y_train)\nclf_performance(best_clf_rf,'RandomForestRegressor')","d8dc160c":"#determine optimal gbr alpha value\nalpha = []\nerror = []\n\nfor i in range(1,10):\n    alpha.append(i\/10000)\n    gbr = GradientBoostingRegressor(random_state = 1,alpha=(i\/10000))\n    error.append(np.mean(cross_val_score(gbr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)))\n    \nplt.plot(alpha,error)\n\nplt.show()","bad214cf":"#print optimal alpha value\nerr = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns=['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","685fbb29":"#GradientBoostingRegressor GridSearchCV\ngbr = GradientBoostingRegressor(random_state = 1)\nparam_grid = {\n                'n_estimators': [20], \n                'max_depth': [7],\n                'max_features': ['auto'],\n                'learning_rate': [0.2],\n#                 'alpha': [0.0001],\n                'min_samples_leaf': [3],\n                'min_samples_split': [2]\n              }\nclf_gbr = GridSearchCV(gbr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_gbr = clf_gbr.fit(X_train,y_train)\nclf_performance(best_clf_gbr,'GradientBoostingRegressor')","e9881f76":"#SVR GridSearchCV\nsvr = SVR()\nparam_grid = {\n                'kernel' : ['poly'],\n                'C' : [24],\n                'coef0' : [0.9],\n                'gamma' : ['scale','auto']\n}\nclf_svr = GridSearchCV(svr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_svr = clf_svr.fit(X_train,y_train)\nclf_performance(best_clf_svr,'SVR')","37d83211":"#import ensemble packages and numpy functions\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor, BaggingRegressor, AdaBoostRegressor","b0e45cf9":"#StackingRegressor mean cross-validation\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('lm', LinearRegression()))\n    level0.append(('lm_l', Lasso(random_state = 1)))\n    level0.append(('rid', Ridge(random_state = 1)))\n    level0.append(('enr', ElasticNet(random_state = 1)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1)))\n    level0.append(('svr', SVR()))\n    # define meta learner model\n    level1 = LinearRegression()\n    # define the stacking ensemble\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model\n\ndef get_models():\n    models = dict()\n    models['lm'] = LinearRegression()\n    models['lm_l'] = Lasso(random_state = 1)\n    models['rid'] = Ridge(random_state = 1)\n    models['enr'] = ElasticNet(random_state = 1)\n    models['rf'] = RandomForestRegressor(random_state = 1)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1)\n    models['svr'] = SVR()\n    models['stacking'] = get_stacking()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","a85dc5b8":"#StackingRegressor mean cross-validation\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    #level0.append(('lm', LinearRegression(copy_X= True, fit_intercept= True, normalize= True)))\n    level0.append(('lm_l', Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)))\n    level0.append(('rid', Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')))\n    #level0.append(('enr', ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)))\n    level0.append(('svr', SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')))\n    # define meta learner model\n    level1 = LinearRegression()\n    # define the stacking ensemble\n    stacking_model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return stacking_model\n\ndef get_models():\n    models = dict()\n    #models['lm'] = LinearRegression(copy_X= True, fit_intercept= True, normalize= True)\n    models['lm_l'] = Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)\n    models['rid'] = Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')\n    #models['enr'] = ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)\n    models['rf'] = RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)\n    models['svr'] = SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')\n    models['stacking'] = get_stacking()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","85882902":"#VotingRegressor mean cross-validation\ndef get_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('lm', LinearRegression()))\n    level0.append(('lm_l', Lasso(random_state = 1)))\n    level0.append(('rid', Ridge(random_state = 1)))\n    level0.append(('enr', ElasticNet(random_state = 1)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1)))\n    level0.append(('svr', SVR()))\n    # define the stacking ensemble\n    voting_model = VotingRegressor(estimators=level0)\n    return voting_model\n\ndef get_models():\n    models = dict()\n    models['lm'] = LinearRegression()\n    models['lm_l'] = Lasso(random_state = 1)\n    models['rid'] = Ridge(random_state = 1)\n    models['enr'] = ElasticNet(random_state = 1)\n    models['rf'] = RandomForestRegressor(random_state = 1)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1)\n    models['svr'] = SVR()\n    models['voting'] = get_voting()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","dae9dc9d":"#VotingRegressor mean cross-validation\ndef get_voting():\n    # define the base models\n    level0 = list()\n    #level0.append(('lm', LinearRegression(copy_X= True, fit_intercept= True, normalize= True)))\n    #level0.append(('lm_l', Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)))\n    #level0.append(('rid', Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')))\n    #level0.append(('enr', ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)))\n    level0.append(('rf', RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)))\n    level0.append(('gbr', GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)))\n    level0.append(('svr', SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')))\n    # define the stacking ensemble\n    voting_model = VotingRegressor(estimators=level0)\n    return voting_model\n\ndef get_models():\n    models = dict()\n    #models['lm'] = LinearRegression(copy_X= True, fit_intercept= True, normalize= True)\n    #models['lm_l'] = Lasso(random_state = 1, alpha=0.0038, copy_X=True, fit_intercept=True, normalize=False)\n    #models['rid'] = Ridge(random_state = 1, copy_X=True, fit_intercept=False, normalize=True, solver='cholesky')\n    #models['enr'] = ElasticNet(random_state = 1,alpha=0.0018, copy_X=True,fit_intercept=True, normalize= False)\n    models['rf'] = RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)\n    models['gbr'] = GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)\n    models['svr'] = SVR(C=24, coef0=0.9, gamma='scale', kernel='poly')\n    models['voting'] = get_voting()\n    return models\n\nmodels = get_models()\nresults, names = list(),list()\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","7fded1af":"#BaggingRegressor mean cross-validation\nbagging_model = BaggingRegressor(\n                                     bootstrap=True,\n                                     random_state=1,\n                                     n_jobs=-1\n                                     )\n\nbagging_model.fit(X_train, y_train)\n\ncv = cross_val_score(bagging_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+\/-', std(cv))","40e6ae12":"#BaggingRegressor mean cross-validation\nbagging_model = BaggingRegressor(\n#                                     base_estimator=RandomForestRegressor(),\n                                     bootstrap=True,\n                                     random_state=1,\n                                     n_estimators=20,\n                                     n_jobs=-1\n                                     )\n\nbagging_model.fit(X_train, y_train)\n\ncv = cross_val_score(bagging_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+\/-', std(cv))","a52cf57a":"#BaggingRegressor (pasting) mean cross-validation\npasting_model = BaggingRegressor(\n                                     bootstrap=False,\n                                     random_state=1,\n                                     n_jobs=-1\n                                     )\n\npasting_model.fit(X_train, y_train)\n\ncv = cross_val_score(pasting_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+\/-', std(cv))","f661131e":"#BaggingRegressor (pasting) mean cross-validation\npasting_model = BaggingRegressor(\n                                     base_estimator=RandomForestRegressor(),\n                                     bootstrap=False,\n                                     random_state=1,\n                                     n_estimators=40,\n                                     n_jobs=-1\n                                     )\n\npasting_model.fit(X_train, y_train)\n\ncv = cross_val_score(pasting_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+\/-', std(cv))","8367db45":"#AdaBoostRegressor mean cross-validation\nadaboost_model = AdaBoostRegressor(\n                                       random_state=1)\n\nadaboost_model.fit(X_train , y_train)\n\ncv = cross_val_score(adaboost_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+\/-', std(cv))","f23dc4ee":"#AdaBoostRegressor mean cross-validation\nadaboost_model = AdaBoostRegressor(\n                                       base_estimator=RandomForestRegressor(),\n                                       learning_rate=0.01,\n                                       random_state=1)\n\nadaboost_model.fit(X_train , y_train)\n\ncv = cross_val_score(adaboost_model, X_train, y_train,scoring='neg_mean_absolute_error', cv=5)\nprint(mean(cv), '+\/-', std(cv))","5488cad0":"#import metrics packages\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score","3c9c8342":"#VotingRegressor metrics\nvoting_model = get_voting()\nvoting_model.fit(X_train,y_train)\ntpred_voting=voting_model.predict(X_test)\nprint('VotingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_voting)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_voting))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_voting)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_voting)))","94ba65d8":"#StackingRegressor metrics\nstacking_model = get_stacking()\nstacking_model.fit(X_train,y_train)\ntpred_stack=stacking_model.predict(X_test)\nprint('StackingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_stack)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_stack))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_stack)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_stack)))","fd1f850c":"#BaggingRegressor (pasting) metrics\npasting_model.fit(X_train,y_train)\ntpred_pasting=pasting_model.predict(X_test)\nprint('BaggingRegressor (Pasting)')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_pasting)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_pasting))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_pasting)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_pasting)))","bfb7ccef":"#RandomForestRegressor metrics\nrf = RandomForestRegressor(random_state = 1,bootstrap=True, max_depth=9, max_features='auto', min_samples_leaf=1, min_samples_split= 2, n_estimators=385)\nrf.fit(X_train,y_train)\ntpred_rf=rf.predict(X_test)\nprint('RandomForestRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_rf)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_rf))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_rf)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_rf)))","929d0e79":"#BaggingRegressor metrics\nbagging_model.fit(X_train,y_train)\ntpred_bagging=bagging_model.predict(X_test)\nprint('BaggingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_bagging)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_bagging))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_bagging)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_bagging)))","8c02fcd0":"#AdaBoostRegressor metrics\nadaboost_model.fit(X_train,y_train)\ntpred_adaboost=adaboost_model.predict(X_test)\nprint('AdaBoostRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_adaboost)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_adaboost))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_adaboost)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_adaboost)))","ed25ea22":"#GradientBoostingRegressor metrics\ngbr = GradientBoostingRegressor(random_state = 1,learning_rate= 0.2, max_depth= 7, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 20)\ngbr.fit(X_train,y_train)\ntpred_gbr=gbr.predict(X_test)\nprint('GradientBoostingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_gbr)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_gbr))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_gbr)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_gbr)))","56df84f0":"#SVR metrics\nsvr = SVR(C=24, coef0= 0.9, gamma='scale', kernel='poly')\nsvr.fit(X_train,y_train)\ntpred_svr=svr.predict(X_test)\nprint('SVR')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_svr)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_svr))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_svr)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_svr)))","31fda351":"#import packages for explaining feature importance\nfrom pdpbox import pdp, get_dataset, info_plots\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap","96c8ef4d":"#preparing data for shap\nX_shap = pd.DataFrame(X_train)\nX_shap.columns = feature_names\n\npred_data = pd.DataFrame(X_test)\npred_data.columns = feature_names","b2915c1c":"#create object that can calculate shap values\nexplainer = shap.Explainer(svr.predict, X_shap)\nshap_values = explainer(pred_data)","65fef30c":"#summary_plot using SVR\nshap.initjs()\nshap.summary_plot(shap_values, pred_data)","1b88c283":"#car that has the most impact on SVR model: car_name_37\nvalues[37]","6a4272fa":"#permutation importance from Voting Regressor\nperm = PermutationImportance(voting_model).fit(pred_data, y_test)\neli5.show_weights(perm, feature_names = list(feature_names), top=len(feature_names))","98352c6b":"#cars that has the most impact on Voting Regressor model: car_name_27\nprint(values[27])\nprint(values[28])","97dded89":"# Productionization\n\nI created a [front-end](https:\/\/recommend-vehicle-price.herokuapp.com\/) using this model using Flask and Heroku to recommend vehicle sales prices.\n\nSee the [GitHub repo](https:\/\/github.com\/MichaelBryantDS\/vehicle-price-rec) for more information.","abce6013":"**Data distribution and outliers**\n","dbe10a01":"Hyperparameter tuning pasting","355aa09e":"**Applying linear model to better understand feature relationship with selling price**","c8411713":"**Peparing data for ML**","f90ed1ca":"**Data cleaning**","128ed207":"Baseline","55920dc2":"Hyperparameter tuning","8285b77b":"**Data distributions**","5160e1a9":"# ML Modeling","aa7188fe":"**Import libraries and data**","fcb00a6a":"**AdaBoostRegressor**","be4dee54":"Baseline","21fcd31b":"**Tuning model performance**","e7d49164":"**Mutual information**","5dd28b17":"Pasting baseline","3f02b57d":"Hyperparameter tuning bagging","007c7951":"# EDA","358d47cc":"**Untuned model prerformance**","ff0d988a":"**Finding correlations with a heat map and visualizations**","17f86005":"**VotingRegressor**","1a31c1f5":"Bagging baseline","e1b4a00d":"Baseline","21cfbb30":"**MSE, RMSE, MAE, and R-squared values for best models using test set**","6eae9291":"# Conclusions","77e34cd3":"**Best model**\n- SVR\n- MSE: 0.61430\n- RMSE: 783.77 USD\n- MAE: 502.69 USD\n- R-squared: 0.97032\n\n**Most important features**\n- Present_Price\n- Year\n- Seller_Type\n","f7ff573e":"**BaggingRegressor: Bagging and Pasting**","fadc2e28":"**Defining variables and cleaning data**","9b104497":"# Feature Importance","58f95767":"Hyperparameter tuning","3a9164fc":"Hyperparameter tuning","e82938d3":"# End-to-end Vehicle Sales Price Recommendation Project\n\n[Front-end](https:\/\/recommend-vehicle-price.herokuapp.com\/)\n\n[GitHub repo](https:\/\/github.com\/MichaelBryantDS\/vehicle-price-rec)","e4f6a674":"**StackingRegressor**"}}