{"cell_type":{"e85a3678":"code","d81ced67":"code","07806a3e":"code","426e1c2f":"code","1c6ff174":"code","f22ee081":"code","eb6d8285":"code","994dfb4e":"code","2abc1c1b":"code","df50af2c":"code","3e5da7e0":"code","131983bc":"code","23b8e236":"code","5d8ccd24":"code","b9065d6f":"code","fd9897ea":"code","a458ca21":"code","8e699ed4":"code","720486fe":"code","19b184db":"code","95b79a2c":"code","22a30f94":"code","ed47e974":"code","6799746e":"code","28e0f613":"code","56938a59":"code","9410c60b":"code","9328a3a6":"code","85d55841":"code","47b6be31":"code","6b1b514f":"code","955094a1":"markdown","792dcd1f":"markdown","cfbc7f58":"markdown","5f5d5d14":"markdown","4e9ef42d":"markdown","ba50bc55":"markdown","cc34c654":"markdown","58a266c1":"markdown","c2972762":"markdown","aabe9b9b":"markdown","fedbb900":"markdown","973e0728":"markdown","ebd3d161":"markdown"},"source":{"e85a3678":"# to read and manipulate data\nimport pandas as pd\nimport numpy as np\n\n# to add a general preprocessing step to the data\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# to prepare the model\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\n\n# to model the data\nfrom sklearn.metrics import accuracy_score, log_loss, roc_curve, fbeta_score\nfrom sklearn.linear_model import LogisticRegression\n\n# for hyperparameter optimization\nimport optuna\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\n\n# to warnings off\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = None","d81ced67":"# function to create a model instance given the hyperparameters\ndef instantiate_model(hyperparameters):\n    # creating an instance of the model\n    model = LogisticRegression(**hyperparameters)\n    # returning the model\n    return model\n\n\n\n# function to fit an instantiated model\ndef fit_model(model, X, y):\n    # fitting the model to the data\n    model.fit(X, y)\n    # returning the trained model\n    return model\n\n\n# function to extract the best probability threshold according to the roc curve\ndef extract_roc_threshold(y_obs, probs):\n    # extracting fpr, tpr and thresholds from the roc curve\n    fpr, tpr, thresholds = roc_curve(y_true = y_obs, y_score = probs)\n    # calculating the gmeans for each of the fpr and tpr from the roc curve\n    gmeans = np.sqrt(tpr * (1 - fpr))\n    # identifying the element with the highest gmeans - best trade-off between fpr and tpr\n    best_threshold = np.argmax(gmeans)\n    # returning the probability threshold that maximizes the gmeans\n    return thresholds[best_threshold]\n\n\n# function to encode categories for a selected probability threshold\ndef move_threshold(probabilities, threshold):\n    return (probabilities >= threshold).astype('int')\n\n\n# function to extract the best probability threshold according to the aucpr curve\ndef extract_fbeta_threshold(y_obs, probs, beta_value):\n    # defining an array of thresholds\n    thresholds = np.arange(0, 1, 0.01)\n    # calculating the fbeta score for each threshold\n    f_scores = [fbeta_score(y_true = y_obs, y_pred = move_threshold(probs, threshold), beta = np.float(beta_value)) for threshold in thresholds]\n    # identifying the element with the highest fbeta score - best trade-off between precision and recall\n    best_threshold = np.argmax(f_scores)\n    # returning the probability threshold that maximizes the fbeta score\n    return thresholds[best_threshold]\n\n\n# function to evaluate a trained model\ndef score_model(model, X_eval, y_eval, scoring):\n    \n    # fitting the model trained model to the data to obtain predicted probabilities\n    predicted_probabilities = model.predict_proba(X_eval)[:, 1]\n    \n    # use the traditional approach\n    if scoring == 'traditional':\n        # predicting the class label with the traditional approach\n        selected_threshold = 0.5\n        predicted_classes = model.predict(X_eval)\n    # use the ROC AUC curve to define the threshold and set the class\n    elif scoring == 'auc':\n        selected_threshold = extract_roc_threshold(y_obs = y_eval, probs = predicted_probabilities)\n        predicted_classes = move_threshold(probabilities = predicted_probabilities, threshold = selected_threshold)\n    # use the F-beta define the threshold and set the class\n    else:\n        selected_threshold = extract_fbeta_threshold(y_obs = y_eval, probs = predicted_probabilities, beta_value = scoring)\n        predicted_classes = move_threshold(probabilities = predicted_probabilities, threshold = selected_threshold)\n    \n    # calculating the accuracy score of the model\n    acc = accuracy_score(y_true = y_eval, y_pred = predicted_classes)\n    \n    # calculating the log loss of the model\n    ll = log_loss(y_true = y_eval, y_pred = predicted_probabilities)\n    \n    # returning the trained model\n    return acc, ll, selected_threshold\n\n\n# objective function for optuna to evaluate\ndef objective(trial):\n    \n    # search space\n    search_space = {'hyperparams': {'C': trial.suggest_float('C', 0.001, 1.0),\n                                    'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])},\n                    'imputation': {'fare_imputation': trial.suggest_categorical('fare_imputation', ['mean', 'median']),\n                                   'age_imputation': trial.suggest_categorical('age_imputation', ['mean', 'median'])},\n                    'transform': {'log_fare': trial.suggest_categorical('log_fare', ['yes', 'no']),\n                                  'ohe': trial.suggest_categorical('ohe', ['yes', 'no']),\n                                  'interactions': trial.suggest_categorical('interactions', ['yes', 'no'])},\n                    'scoring': trial.suggest_categorical('scoring', ['traditional', 'auc', '0.5', '1.0', '1.5']),\n                    'pseudo_label': trial.suggest_categorical('pseudo_label', [None, 'lr', 'tabnet'])\n                   }\n    \n    # fit SKF\n    skf_evaluated = evaluate_stratifield_kfold(X, y, kfolds = kfolds, hyperpars = search_space)\n    \n    # unpack accuracy and loss\n    acc = skf_evaluated['val_accuracy'].mean()\n    \n    # return scores\n    return acc\n\n\n# function to create a model pipeline\ndef create_pipeline(hyperpars):\n    # imputing the defined fare imputation strategy and the knn strategy if this is the case\n    if hyperpars['imputation']['age_imputation'] == 'knn':\n        imputation = ColumnTransformer([('imputation_fare', SimpleImputer(missing_values = np.nan, strategy=hyperpars['imputation']['fare_imputation'], copy=False), ['Fare']),\n                                        ('imputation_age', KNNImputer(missing_values = np.nan, copy=False), ['Age'])],\n                                       remainder = 'passthrough')\n    # otherwise, input the defined strategy for the age imputation   \n    else:\n        imputation = ColumnTransformer([('imputation_fare', SimpleImputer(missing_values = np.nan, strategy=hyperpars['imputation']['fare_imputation'], copy=False), ['Fare']),\n                                        ('imputation_age', SimpleImputer(missing_values = np.nan, strategy=hyperpars['imputation']['age_imputation'], copy=False), ['Age'])],\n                                       remainder = 'passthrough')\n    \n    # instantiating a standard scaler for the numerical columns\n    scaler = ColumnTransformer([('scaler', StandardScaler(), num_columns_indexes)], remainder = 'passthrough')\n    \n    # defining whether we are transforming the fare variable\n    if hyperpars['transform']['log_fare'] == 'yes':\n        transformation = ColumnTransformer([('log_fare', FunctionTransformer(np.log1p), [0])], remainder = 'passthrough')\n    else: \n        transformation = ColumnTransformer([('log_fare', FunctionTransformer(), [0])], remainder = 'passthrough')\n    \n    # defining whether we are ohe hot encoding variables or not\n    if hyperpars['transform']['ohe'] == 'yes':\n        ohe = ColumnTransformer([('ohe', OneHotEncoder(handle_unknown = 'error', drop = 'first'), cat_columns_indexes)], remainder = 'passthrough')\n    else:\n        ohe = ColumnTransformer([('ohe', FunctionTransformer(), cat_columns_indexes)], remainder = 'passthrough')\n        \n    # defining whether we are considering interactions or not\n    if hyperpars['transform']['interactions'] == 'yes':\n        poly = ColumnTransformer([('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True), num_columns_indexes)], remainder = 'passthrough')\n    else:\n        poly = ColumnTransformer([('poly', FunctionTransformer(), num_columns_indexes)], remainder = 'passthrough')\n    \n    # instantiating the model\n    model = instantiate_model(hyperparameters = hyperpars['hyperparams'])\n    \n    # instantiating the pipeline\n    pipeline = Pipeline(steps = [('imputation', imputation), ('scaler', scaler), ('log_fare', transformation), ('encoding', ohe), ('poly', poly), ('model', model)])\n    \n    # returning the pipeline\n    return pipeline\n\n\n\n# function to run the stratified kfold with the selected hyperparameters\ndef evaluate_stratifield_kfold(X, y, kfolds, hyperpars):\n    print(f'\\n------------- Initializing the Stratified {kfolds}-fold evaluation -------------')\n    # creating empty lists to store the accuracy and log loss\n    fold_scores = []\n    \n    # printing the hyperparameters under evaluation\n    print('Evaluating the following hyperparameters:')\n    print(hyperpars)\n    \n    # instanting the stratified k-fold\n    skf = StratifiedKFold(n_splits = kfolds, random_state = 42, shuffle = True)\n    \n    # deciding whether past submissions should be used and which ones\n    if hyperpars['pseudo_label'] == 'lr': # logistic regression submission labels\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test.shape[0]), size = np.int(X_test.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test.iloc[test_idx_leak], y_lr[test_idx_leak]\n    if hyperpars['pseudo_label'] == 'tabnet':\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test.shape[0]), size = np.int(X_test.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test.iloc[test_idx_leak], y_tabnet[test_idx_leak]\n        \n    # unpacking the statified k-fold\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        # separating the train from the test folds\n        X_train_fold, X_val_fold, y_train_fold, y_val_fold = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]\n     \n        # mergining the test dataset leak to the train set if this approach should be used\n        if hyperpars['pseudo_label'] is not None:\n            # merging and replacing the fold objects\n            X_train_fold, y_train_fold = pd.concat([X_train_fold, X_leak]), np.hstack((y_train_fold, y_leak))\n            # shuffling the sequence of observations\n            shuffled_indices = np.random.choice(a = range(X_train_fold.shape[0]), size = X_train_fold.shape[0], replace = False)\n            # rearranging x and y\n            X_train_fold, y_train_fold = X_train_fold.iloc[shuffled_indices], y_train_fold[shuffled_indices]\n        \n        # pipeline\n        model = create_pipeline(hyperpars = hyperpars)\n        \n        # fitting the model to the data\n        model = fit_model(model = model, X = X_train_fold, y = y_train_fold)\n        \n        # extracting the train score\n        train_acc, train_loss, _ = score_model(model = model, X_eval = X_train_fold, y_eval = y_train_fold, scoring = hyperpars['scoring'])\n        # extracting the validation score\n        val_acc, val_loss, _ = score_model(model = model, X_eval = X_val_fold, y_eval = y_val_fold, scoring = hyperpars['scoring'])\n        \n        # appending accuracy and log loss to the list\n        fold_scores.append((train_acc, val_acc, train_loss, val_loss))\n        \n        # printing the results of this fold\n        print(f\"> Fold {fold + 1}: Training loss: {train_loss} | Training accuracy: {train_acc} | Validation loss: {val_loss} | Validation accuracy: {val_acc}\")\n    \n    # structuring the training and validation scores in a dataframe\n    scores = pd.DataFrame(fold_scores, columns=['train_accuracy', 'val_accuracy', 'train_loss', 'val_loss'])\n    \n    # printing average scores for this run\n    print(f\"End of Run!\\nTraining loss: {scores['train_loss'].mean()} | Training accuracy: {scores['train_accuracy'].mean()} | Validation loss: {scores['val_loss'].mean()} | Validation accuracy: {scores['val_accuracy'].mean()}\")\n    \n    # returning the scores\n    return scores\n\n\n# function to create the pipeline that will be used to make the submission\ndef final_pipeline(hyperpars):\n    # imputing the defined fare imputation strategy and the knn strategy if this is the case\n    if hyperpars['age_imputation'] == 'knn':\n        imputation = ColumnTransformer([('imputation_fare', SimpleImputer(missing_values = np.nan, strategy=hyperpars['fare_imputation'], copy=False), ['Fare']),\n                                        ('imputation_age', KNNImputer(missing_values = np.nan, copy=False), ['Age'])],\n                                       remainder = 'passthrough')\n    # otherwise, input the defined strategy for the age imputation   \n    else:\n        imputation = ColumnTransformer([('imputation_fare', SimpleImputer(missing_values = np.nan, strategy=hyperpars['fare_imputation'], copy=False), ['Fare']),\n                                        ('imputation_age', SimpleImputer(missing_values = np.nan, strategy=hyperpars['age_imputation'], copy=False), ['Age'])],\n                                       remainder = 'passthrough')\n    \n    # instantiating a standard scaler for the numerical columns\n    scaler = ColumnTransformer([('scaler', StandardScaler(), num_columns_indexes)], remainder = 'passthrough')\n    \n    # defining whether we are transforming the fare variable\n    if hyperpars['log_fare'] == 'yes':\n        transformation = ColumnTransformer([('log_fare', FunctionTransformer(np.log1p), [0])], remainder = 'passthrough')\n    else: \n        transformation = ColumnTransformer([('log_fare', FunctionTransformer(), [0])], remainder = 'passthrough')\n    \n    # defining whether we are ohe hot encoding variables or not\n    if hyperpars['ohe'] == 'yes':\n        ohe = ColumnTransformer([('ohe', OneHotEncoder(handle_unknown = 'error', drop = 'first'), cat_columns_indexes)], remainder = 'passthrough')\n    else:\n        ohe = ColumnTransformer([('ohe', FunctionTransformer(), cat_columns_indexes)], remainder = 'passthrough')\n    \n    # defining whether we are considering interactions or not\n    if hyperpars['interactions'] == 'yes':\n        poly = ColumnTransformer([('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True), num_columns_indexes)], remainder = 'passthrough')\n    else:\n        poly = ColumnTransformer([('poly', FunctionTransformer(), num_columns_indexes)], remainder = 'passthrough')\n        \n    # instantiating the model\n    model = LogisticRegression(C = hyperpars['C'], class_weight = hyperpars['class_weight'])\n    \n    # instantiating the pipeline\n    pipeline = Pipeline(steps = [('imputation', imputation), ('scaler', scaler), ('log_fare', transformation), ('encoding', ohe), ('poly', poly), ('model', model)])\n    \n    # returning the pipeline\n    return pipeline\n\n\n# function to run the model with the best hyperparameters and submit it\ndef submit_stratifield_kfold(X, y, kfolds, hyperpars):\n    print(f'\\n------------- Initializing the Stratified {kfolds}-fold evaluation -------------')\n    # creating empty lists to store the accuracy and log loss\n    fold_scores = []\n    \n    # creating an empty dictionary to store the models\n    fitted_models = {}\n    \n    # creating empty lists to store the thresholds\n    threshold_list = []\n    \n    # creating a numpy array to store probabilities\n    probas = np.zeros(len(X_test))\n    \n    # printing the hyperparameters under evaluation\n    print('Evaluating the following hyperparameters:')\n    print(hyperpars)\n    \n    # instanting the stratified k-fold\n    skf = StratifiedKFold(n_splits = kfolds, random_state = 42, shuffle = True)\n    \n    # deciding whether past submissions should be used and which ones\n    if hyperpars['pseudo_label'] == 'lr': # logistic regression submission labels\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test.shape[0]), size = np.int(X_test.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test.iloc[test_idx_leak], y_lr[test_idx_leak]\n    if hyperpars['pseudo_label'] == 'tabnet':\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test.shape[0]), size = np.int(X_test.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test.iloc[test_idx_leak], y_tabnet[test_idx_leak]\n        \n    # unpacking the statified k-fold\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        # separating the train from the test folds\n        X_train_fold, X_val_fold, y_train_fold, y_val_fold = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]\n        \n        # mergining the test dataset leak to the train set if this approach should be used\n        if hyperpars['pseudo_label'] is not None:\n            # merging and replacing the fold objects\n            X_train_fold, y_train_fold = pd.concat([X_train_fold, X_leak]), np.hstack((y_train_fold, y_leak))\n            # shuffling the sequence of observations\n            shuffled_indices = np.random.choice(a = range(X_train_fold.shape[0]), size = X_train_fold.shape[0], replace = False)\n            # rearranging x and y\n            X_train_fold, y_train_fold = X_train_fold.iloc[shuffled_indices], y_train_fold[shuffled_indices]\n            \n        # pipeline\n        model = final_pipeline(hyperpars = hyperpars)\n        \n        # fitting the model to the data\n        model = fit_model(model = model, X = X_train_fold, y = y_train_fold)\n        \n        # extracting the train score\n        train_acc, train_loss, _ = score_model(model = model, X_eval = X_train_fold, y_eval = y_train_fold, scoring = hyperpars['scoring'])\n        # extracting the validation score\n        val_acc, val_loss, val_threshold = score_model(model = model, X_eval = X_val_fold, y_eval = y_val_fold, scoring = hyperpars['scoring'])\n        \n        # appending accuracy and log loss to the list\n        fold_scores.append((train_acc, val_acc, train_loss, val_loss))\n        \n        # appending the threshold to the list\n        threshold_list.append(val_threshold)\n        \n        # printing the results of this fold\n        print(f\"> Fold {fold + 1}: Training loss: {train_loss} | Training accuracy: {train_acc} | Validation loss: {val_loss} | Validation accuracy: {val_acc}\")\n        \n        # generating predictions on the test set with the model\n        probas += model.predict_proba(X_test)[:, 1]\n        \n        # saving the fitted model\n        fitted_models[f'model_{fold}'] = model\n        \n    # structuring the training and validation scores in a dataframe\n    scores = pd.DataFrame(fold_scores, columns=['train_accuracy', 'val_accuracy', 'train_loss', 'val_loss'])\n    \n    # printing average scores for this run\n    print(f\"End of Run!\\nTraining loss: {scores['train_loss'].mean()} | Training accuracy: {scores['train_accuracy'].mean()} | Validation loss: {scores['val_loss'].mean()} | Validation accuracy: {scores['val_accuracy'].mean()}\")\n    \n    # returning the score\n    return scores, probas, threshold_list, fitted_models","07806a3e":"# train data\ntrain = pd.read_csv(filepath_or_buffer = '..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain.shape","426e1c2f":"# test data\ntest = pd.read_csv(filepath_or_buffer = '..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest.shape","1c6ff174":"# sample submission data\nsubmission = pd.read_csv(filepath_or_buffer = '..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\nsubmission.shape","f22ee081":"# submission from a previous logistic regression\nlr_submission = pd.read_csv(filepath_or_buffer = '..\/input\/lr-submission-tps202104\/lr_submission.csv')\nlr_submission.shape","eb6d8285":"# submission from a previous logistic regression\ntabnet_submission = pd.read_csv(filepath_or_buffer = '..\/input\/tabnetsubmissiontps202104\/tabnet_submission-tps202104.csv')\ntabnet_submission.shape","994dfb4e":"# putting the train on top of the test set in order to create some general features\ndf = pd.concat([train, test])\ndf.shape","2abc1c1b":"## adding number of missing values in the rows\ndf['nMissing'] = df.isnull().sum(axis = 1)\n\n# encoding the deck to which de cabin belongs to\ndf['Deck'] = df.Cabin.str.extract(pat = r'(^[A-Z])')\n# filling missing value for deck\ndf['Deck'] = df.Deck.fillna('U')\n\n# encoding whether the passenger travels alone\ndf['TravelsAlone'] = ((df.SibSp == 0) & (df.Parch == 0)).astype('int')\n\n# encoding whether the passenger travels accompanied by somebody\ndf['TravelsTwo'] = ((df.SibSp == 1) & (df.Parch == 0) | (df.SibSp == 0) & (df.Parch == 1)).astype('int')\n\n# calculating the size of the family\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\n# creating flags for the other columns that will be imputed\ndf['AgeImputed'] = (~df.Age.isnull()).astype('int')\ndf['FareImputed'] = (~df.Fare.isnull()).astype('int')\ndf['EmbarkedImputed'] = (~df.Embarked.isnull()).astype('int')\n\n# parsing Pclass to category\ndf['Pclass'] = df.Pclass.astype('object')","df50af2c":"# creating a variable to encode the deviation of the fare by class\n## log scaling the fare\ndf['FareClass'] = np.log(df.Fare)\n\n## adding the average fare by class\ndf['AvgFare'] = df.Pclass.map(df.groupby('Pclass').FareClass.mean())\n\n## calculating the deviation for the mean\ndf['FareClass'] = df.FareClass - df.AvgFare\n\n## adding a flag to the deviation\ndf['FareClassFlag'] = (df.FareClass > 0).astype('int')","3e5da7e0":"# fixing possible typo on the ticket column \ndf['Ticket'] = df.Ticket.str.replace(pat=r'\\bSTON\\b', repl='SOTON', regex=True)\n\n# extracting the letters from the ticket column\ndf['TicketLetters'] = df.Ticket.str.findall(pat = r'([A-Za-z]+)')\n\n# filling NaNs with an empty list\ndf['TicketLetters'] = df.TicketLetters.fillna('')\n\n# parsing the list column to a string column\ndf['TicketLetters'] = df.TicketLetters.apply(lambda x: ' '.join([letras for letras in x]))\n\n# filling missing ticket letters if the string is empty\ndf['TicketLetters'] = df.TicketLetters.apply(lambda x: 'N' if x == '' else x)\n\n# putting all letters to upper\ndf['TicketLetters'] = df.TicketLetters.str.upper()\n\n# fixing some tickets that seems to be the same\ndf['TicketLetters'] = df.TicketLetters.replace(to_replace = {'C A': 'CA', 'W E P': 'WE P', 'SOTON O Q': 'SOTON OQ', 'S W PP': 'SW PP', \n                                                             'S O C': 'SO C', 'C A SOTON': 'SOTON CA', 'S C PARIS': 'SC PARIS', 'P PP': 'PP',\n                                                             'S C A': 'SC A', 'S O P P': 'S O P'})","131983bc":"# extracting the ticket number\ndf['TicketNumber'] = df.Ticket.str.extract(pat=r'(?<=\\s)?([0-9]+)$').astype('float')\n\n# filling missing values in the ticket number\ndf['TicketNumber'] = df.TicketNumber.fillna(0)\n\n# number of digits\ndf['TicketDigits'] = df.TicketNumber.astype('int').astype('str').str.count(pat='[0-9]')\n\n# rounding the ticket number to an integer\ndf['TicketNumber'] = (df.TicketNumber \/ (df.TicketDigits * 1000)).astype('int')\n\n# scaling the ticket back to its original scale\ndf['TicketNumber'] = df.TicketNumber * (df.TicketDigits * 1000)\n\n# calculating the frequency of ticket numbers\nfrequencies_tnb = df.TicketNumber.value_counts(normalize=False)\n\n# mapping the frequencies to ticket numbers\nmask_TcktNmbFreq = df.TicketNumber.map(frequencies_tnb)\n\n# replacing ticket numbers that are very rare by a random number\ndf['TicketNumber'] = df.TicketNumber.mask(mask_TcktNmbFreq < 10, 999999)","23b8e236":"# extracting the numeric part of the cabin\ndf['CabinNb'] = df.Cabin.fillna('0').str.extract(pat=r'([0-9]+)').astype('int')\n\n# extracting the count of digits in the cabin number\ndf['CabinDigits'] = df.CabinNb.astype('str').str.count(pat='[0-9]').astype('int')\n\n# rounding the ticket number to an integer\ndf['CabinNb'] = (df.CabinNb \/ (df.CabinDigits * 10)).astype('int')\n\n# scaling the ticket back to its original scale\ndf['CabinNb'] = df.CabinNb * (df.CabinDigits * 10)\n\n# calculating the frequency of cabin numbers\nfrequencies_cbn = df.CabinNb.value_counts(normalize=False)\n\n# mapping the frequencies to cabin numbers\nmask_CabinNbFreq = df.CabinNb.map(frequencies_cbn)\n\n# replacing cabin numbers that are very rare by a random number\ndf['CabinNb'] = df.CabinNb.mask(mask_CabinNbFreq < 10, 99999)","5d8ccd24":"# filling NaNs\ndf['Embarked'] = df.Embarked.fillna('U')","b9065d6f":"# list of columns that will be encoded\ncolumns_to_encode = ['Sex', 'Embarked', 'Deck', 'Pclass', 'TicketLetters']\n\n# encoding columns\nfor column in columns_to_encode:\n    # instantiating the encoder\n    le = LabelEncoder()\n    # transforming the column\n    df[column] = le.fit_transform(df[column])","fd9897ea":"## dropping unwanted columns\n# list of columns to drop\ncolumns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n\n# dropping selected columns\ndf = df.drop(columns = columns_to_drop)","a458ca21":"## arranging columns of the dataframe\nnum_columns = ['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize', 'nMissing']\ncat_columns = ['Pclass', 'Sex', 'Embarked', 'Deck', 'TravelsAlone', 'TravelsTwo', 'AgeImputed', 'FareImputed', 'EmbarkedImputed', 'FareClassFlag', 'TicketLetters', 'TicketDigits', 'CabinDigits']\n\n# arranging the columns\ndf = df[num_columns + cat_columns + ['Survived']]","8e699ed4":"# extracting the training dataset\ntrain_df = df[0:train.shape[0]]\n\n# separating inputs and targets for training data\nX_train, y_train = train_df.drop(columns='Survived'), train_df.Survived\n\n## encoding target\ny_train = LabelEncoder().fit_transform(y_train)\n\n# encoding the target from the lr submission\ny_lr = LabelEncoder().fit_transform(lr_submission.Survived.values)\n\n# encoding the target from the tabnet submission\ny_tabnet = LabelEncoder().fit_transform(tabnet_submission.Survived.values)","720486fe":"# extracting the test dataset\ntest_df = df[train.shape[0]:]\n\n# separating inputs and targets for training data\nX_test = test_df.drop(columns='Survived')","19b184db":"# setting up the variables to run the objective\nX = X_train\ny = y_train\nkfolds = 8\n\n# indexes of the categorical columns\ncat_columns_indexes = [cat_index for cat_index, cat_column in enumerate(X_train.columns) if cat_column in cat_columns]\nnum_columns_indexes = [num_index for num_index, num_column in enumerate(X_train.columns) if num_column in num_columns]","95b79a2c":"# creating a study\nstudy = optuna.create_study(directions = ['maximize'], pruner = optuna.pruners.MedianPruner())","22a30f94":"# adding a baseline to the study\nstudy.enqueue_trial(\n    {\n        'hyperparams': {'C': 1.0,\n                        'class_weight': None},\n        'imputation': {'fare_imputation': 'mean',\n                       'age_imputation': 'mean'},\n        'transform': {'log_fare': 'no',\n                      'ohe': 'no',\n                      'interactions': 'no'},\n        'scoring': 'traditional',\n        'pseudo_label': None\n    }\n)","ed47e974":"# running the study\nstudy.optimize(func = objective, n_trials = 30, timeout = 60 * 30)","6799746e":"plot_optimization_history(study)","28e0f613":"plot_param_importances(study)","56938a59":"plot_parallel_coordinate(study)","9410c60b":"# extracting the best hyperparameters\nbest_hyperparameters = study.best_trials[0].params\nprint(f'Best hyperparameter combination evaluated: {best_hyperparameters}')","9328a3a6":"# fitting the model on the best combination of hyperparameters\nlast_scores, probas, thrs, model_list = submit_stratifield_kfold(X, y, kfolds = kfolds, hyperpars = best_hyperparameters)","85d55841":"print(f'Thresholds used: {thrs}.')","47b6be31":"# adding predictions to the submission dataframe\nsubmission['Survived'] = ((probas \/ kfolds) >= np.mean(thrs)).astype('int')","6b1b514f":"submission.to_csv('submission.csv', index=False)","955094a1":"# Feature Engineering","792dcd1f":"# Defining utility functions","cfbc7f58":"# Separating the two datasets, inputs and outputs","5f5d5d14":"# Encoding categorical data","4e9ef42d":"# Hyperparameter Optimization","ba50bc55":"# Importcat_columnsthe data","cc34c654":"Encoding ticket information.","58a266c1":"# Importing modules and libraries","c2972762":"# Evaluating trials","aabe9b9b":"# Merging the datasets","fedbb900":"# Saving predictions","973e0728":"Encoding cabin data.","ebd3d161":"# Fitting with the best trial"}}