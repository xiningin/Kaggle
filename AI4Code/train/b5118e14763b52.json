{"cell_type":{"367fa1c0":"code","8e78dd6e":"code","23e2d0a0":"code","c0bb7a75":"code","fde2fb9c":"code","f523e799":"code","533fc3b7":"code","677e6e19":"code","a598f06b":"code","006a3430":"code","e763ca19":"markdown","83a53840":"markdown","e0f97a27":"markdown","f1b381f7":"markdown","f76429fa":"markdown","03c7b7a7":"markdown","39165e3b":"markdown","827784cf":"markdown","d64310b1":"markdown"},"source":{"367fa1c0":"!pip install -q keras-tcn --no-dependencies\nfrom tcn import TCN, tcn_full_summary","8e78dd6e":"import pandas as pd\nimport numpy  as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras\nimport tensorflow as tf","23e2d0a0":"train_data = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest_data  = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","c0bb7a75":"for df in (train_data, test_data):\n    df['u_in_lag'] = df.groupby('breath_id')['u_in'].shift(2).fillna(method=\"backfill\")\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['last_value_u_in'] = df.groupby('breath_id')['u_in'].transform('last')\n    df['u_in_mean'] = df.groupby('breath_id')['u_in'].transform('mean')\n    df['u_in_median'] = df.groupby('breath_id')['u_in'].transform('median')\n    df['first_value_u_in'] = df.groupby('breath_id')['u_in'].transform('first')\n    df['u_in_min'] = df.groupby('breath_id')['u_in'].transform('min')\n    df['u_in_max'] = df.groupby('breath_id')['u_in'].transform('max')\n    df['u_in_delta'] = df['u_in_max'] - df['u_in_min']","fde2fb9c":"targets = train_data[['pressure']].to_numpy().reshape(-1, 80)\n\n# drop the unwanted features\ntrain_data.drop(['pressure', 'id', 'breath_id', 'u_out'], axis=1, inplace=True)\ntest_data =  test_data.drop(['id', 'breath_id', 'u_out'], axis=1)","f523e799":"from sklearn.preprocessing import RobustScaler\nRS = RobustScaler()\ntrain_data = RS.fit_transform(train_data)\ntest_data  = RS.transform(test_data)","533fc3b7":"n_features = train_data.shape[-1]\n\ntrain_data = train_data.reshape(-1, 80, n_features)\ntest_data  = test_data.reshape(-1, 80, n_features)\n\nn_epochs = 50\nn_splits =  5","677e6e19":"kf = KFold(n_splits=n_splits, shuffle=False)\ntest_preds = []\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train_data, targets)):\n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    X_train, X_valid = train_data[train_idx], train_data[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    \n    scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 200*((len(test_data)*0.8)\/1024), 1e-5)\n    \n    model = keras.models.Sequential([\n        TCN(input_shape=(80, n_features), nb_filters=256, return_sequences=True, dilations=[1, 2, 4, 8, 16, 32]),\n        keras.layers.Dense(1)\n    ])\n    \n    model.compile(optimizer=\"adam\", loss=\"mae\",\n                  metrics=keras.metrics.MeanAbsoluteError())\n    \n    history = model.fit(X_train, y_train, \n                        validation_data=(X_valid, y_valid), \n                        epochs=n_epochs, \n                        batch_size=1024, \n                        callbacks=[tf.keras.callbacks.LearningRateScheduler(scheduler)])\n    \n    model.save(f'Fold{fold+1} weights')\n    test_preds.append(model.predict(test_data).squeeze().reshape(-1, 1).squeeze())","a598f06b":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[1:,\"loss\"], lw=2, label='training loss')\nplt.plot(logs.loc[1:,\"val_loss\"], lw=2, label='validation loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[1:,\"mean_absolute_error\"], lw=2, label='training MAE')\nplt.plot(logs.loc[1:,\"val_mean_absolute_error\"], lw=2, label='validation MAE')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MAE\")\nplt.legend(loc='upper right')\nplt.show()","006a3430":"submission[\"pressure\"] = sum(test_preds)\/n_splits\nsubmission.to_csv('submission.csv', index=False)","e763ca19":"# Read in the data","83a53840":"This notebook is heavily based on the following two LSTM notebooks:\n* [Tensorflow LSTM Baseline](https:\/\/www.kaggle.com\/ryanbarretto\/tensorflow-lstm-baseline), written by [Ryan Barretto](https:\/\/www.kaggle.com\/ryanbarretto)\n* [Tensorflow Bidirectional LSTM (0.234)](https:\/\/www.kaggle.com\/tolgadincer\/tensorflow-bidirectional-lstm-0-234), by [Tolga Dincer](https:\/\/www.kaggle.com\/tolgadincer)","e0f97a27":"# Submission\nThe submission is created from the average over each fold","f1b381f7":"# Get things ready","f76429fa":"# Plot a learning curve","03c7b7a7":"# Some feature engineering\nThese ideas are from various sources, not all of them are necessarily useful and are just here for demonstration purposes:","39165e3b":"# [Ventilator Pressure Prediction](https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction): \n# Temporal Convolutional Network using Keras-TCN\n\nIn this simple \"starter\" notebook we shall be using a **Temporal Convolutional Network** layer, thanks to the [Keras-TCN](https:\/\/github.com\/philipperemy\/keras-tcn) package written by Philippe R\u00e9my, which is based on the work in the paper [\"*An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling*\"](https:\/\/arxiv.org\/pdf\/1803.01271.pdf).\n\n![](https:\/\/raw.githubusercontent.com\/philipperemy\/keras-tcn\/master\/misc\/Dilated_Conv.png)\n\nFirstly, install `keras-tcn`:","827784cf":"# Related kaggle notebooks\n* [\"Temporal CNN\"](https:\/\/www.kaggle.com\/christofhenkel\/temporal-cnn) by [Dieter](https:\/\/www.kaggle.com\/christofhenkel)\n* [\"Temporal Convolutional Network\"](https:\/\/www.kaggle.com\/christofhenkel\/temporal-convolutional-network) by [Dieter](https:\/\/www.kaggle.com\/christofhenkel)\n* [\"(PyTorch) Temporal Convolutional Networks\"](https:\/\/www.kaggle.com\/ceshine\/pytorch-temporal-convolutional-networks) by [Ceshine Lee](https:\/\/www.kaggle.com\/ceshine)","d64310b1":"# Calculation"}}