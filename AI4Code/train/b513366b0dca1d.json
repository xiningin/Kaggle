{"cell_type":{"674d05c8":"code","fa7e949c":"code","02ffe313":"code","ed1ab45b":"code","c129add2":"code","9b9d53c9":"code","f0b746ef":"code","4b132280":"code","44176d20":"code","1074477b":"code","b23d2d7e":"code","61e8ad80":"code","f536a128":"code","c59008df":"code","c7c4f391":"code","cc14ac8b":"code","c22d3f11":"code","d82efe8b":"code","6b58cf4c":"code","6663df04":"code","a1729204":"code","feccbb9b":"code","c6e7755e":"code","5f27b2f3":"code","177ce35a":"code","53c90593":"code","b3acced2":"code","f687ac1e":"code","a2edb76d":"code","90d7c070":"code","50989148":"code","42c7d59d":"code","a65ebe78":"code","794f7f92":"code","13df3c0b":"code","b6c34e36":"code","9ee249f0":"code","e5a52278":"code","191f9c62":"code","0fd2bf08":"code","8883add2":"code","8b5d38d9":"code","b24f3d29":"code","6b618297":"code","fb2654d3":"code","4a637f06":"code","7ed023a9":"code","9d43728a":"code","7f0faf68":"code","129e5f5d":"code","8bfdb3cd":"code","450ea178":"code","f9a47376":"code","5159eb58":"code","e64f74d6":"code","464abe01":"code","a410048f":"code","a4f80137":"code","184da11d":"code","c005aa86":"code","80069cfd":"code","862f8326":"code","e103cbad":"code","f9190393":"code","5275a1d7":"code","7b449e23":"code","1489a883":"code","3bb0e3c8":"code","0b89e5f5":"code","2d8351b8":"code","7166ecd7":"code","632373f5":"code","472898d1":"code","167d4e27":"code","44898789":"code","03f9f9f3":"code","f82a9360":"code","c11efda2":"code","9f0b7d5a":"code","98e3768c":"code","05b52c17":"code","66d8edbd":"code","a995c282":"code","35846916":"code","c719f46b":"code","3a0af4a0":"code","d8abeb87":"code","88126cec":"code","c96ade0c":"code","f661819a":"code","4cfc6cb1":"code","1ed860d3":"code","ddc0a291":"code","a368f71f":"code","2cb21990":"code","33fb5924":"code","1ff324ea":"markdown","21d0f587":"markdown","93313388":"markdown","695c35f9":"markdown","3c75a24a":"markdown","17e75768":"markdown","3a7a1593":"markdown","71f463f3":"markdown","9eaff5d1":"markdown","0000e0b9":"markdown","3b9a455e":"markdown","b95b5a14":"markdown","4ffeea74":"markdown","99ec1a3c":"markdown","c1e463cf":"markdown","b4f4b837":"markdown","d3ffb6a7":"markdown","6db59be1":"markdown","854cd574":"markdown","34099a9b":"markdown","d2bfed72":"markdown","99bfae1b":"markdown","9b875658":"markdown","3520f1d0":"markdown","d7d25d30":"markdown","e764bbba":"markdown","b2ca2f1c":"markdown","1da04085":"markdown","0558d483":"markdown","57d6dd4c":"markdown","95d9166f":"markdown","ce7b141d":"markdown","2b3c7b91":"markdown","26c372b4":"markdown","5e344ba5":"markdown","8a07e10b":"markdown","609de6e0":"markdown","d51623fa":"markdown","1b1ddd27":"markdown","a3f5ff8a":"markdown","ad1174b6":"markdown","de5f7b3f":"markdown","46422d1c":"markdown","85b7065e":"markdown","031e9145":"markdown","a83313a1":"markdown","de82d917":"markdown","f4e14345":"markdown","5757d65a":"markdown","a3f29ec1":"markdown","6e5695e9":"markdown","35a3911b":"markdown","fb70d9a8":"markdown","f1922e14":"markdown","4ad4e307":"markdown","2154ef6d":"markdown","1575d5b0":"markdown","697c8231":"markdown","143f28d7":"markdown","90f5b190":"markdown","e0c018c6":"markdown","7c07ae44":"markdown","3854a29d":"markdown","79af9d95":"markdown","1c2b355d":"markdown","3fd4c060":"markdown","3c6c428e":"markdown","52288c21":"markdown","025baf59":"markdown","4ab0dad1":"markdown","b981eaa8":"markdown","7d7ab485":"markdown","33fc5081":"markdown","59db762d":"markdown","1fac1132":"markdown","eb267816":"markdown","1551f34c":"markdown","af1a95c7":"markdown","ca585c18":"markdown","e7445171":"markdown","6195fb9f":"markdown","720ec1b8":"markdown"},"source":{"674d05c8":"import numpy as np # numerical computation\nimport pandas as pd # # data processing\/manipulation\nimport matplotlib.pyplot as plt # basic data visualization\nimport seaborn as sns # nicer data visualization\nimport time\n\n# stopwords, tokenizer, stemmer\nimport nltk  \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import wordnet\n\nnltk.download('averaged_perceptron_tagger')\nnltk.download('vader_lexicon')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nimport re # regular expressions\n\n!pip install gensim\nimport gensim\nfrom gensim.parsing.preprocessing import remove_stopwords # we also use gensim for stopwords removal\n\nfrom textblob import TextBlob\n\n!pip install flair\nfrom flair.models import TextClassifier\nfrom flair.data import Sentence\nfrom segtok.segmenter import split_single","fa7e949c":"from pathlib import Path\n\nout_folder = '..\/output' # folder to save intermediary data to\nPath(out_folder).mkdir(parents=True, exist_ok=True)","02ffe313":"# Loading each dataset\ntrump_df = pd.read_csv('..\/input\/us-election-2020-tweets\/hashtag_donaldtrump.csv', lineterminator='\\n')\nbiden_df = pd.read_csv('..\/input\/us-election-2020-tweets\/hashtag_joebiden.csv', lineterminator='\\n')","ed1ab45b":"print('Total number of records in Trump dataset: ', trump_df.shape)\nprint('Total number of records in Biden dataset: ', biden_df.shape)\n\ntrump_initial_count = trump_df.shape[0]\nbiden_initial_count = biden_df.shape[0]","c129add2":"trump_df.columns","9b9d53c9":"# Remove unneeded columns\nirrelevant_columns = ['source','user_name','user_screen_name','user_description','user_join_date','collected_at']\n\n#a more conservative way of considering which columns are not relevant\n#irrelevant_columns = ['collected_at']\ntrump_df = trump_df.drop(columns=irrelevant_columns)\nbiden_df = biden_df.drop(columns=irrelevant_columns)\n\n# Renaming columns\n#trump_df = trump_df.rename(columns={\"likes\": \"Likes\", \"retweet_count\": \"Retweets\", \n#                                    \"state\": \"State\", \"user_followers_count\": \"Followers\"})\n#biden_df = biden_df.rename(columns={\"likes\": \"Likes\", \"retweet_count\": \"Retweets\", \n#                                    \"state\": \"State\", \"user_followers_count\": \"Followers\"})\n\n# Drop null rows\ntrump_df = trump_df.dropna()\nbiden_df = biden_df.dropna()","f0b746ef":"biden_df.country.unique()","4b132280":"trump_usa_df = trump_df[trump_df.country == \"United States of America\"]\nbiden_usa_df = biden_df[biden_df.country == \"United States of America\"]\n\ndel trump_df\ndel biden_df","44176d20":"print('Total number of records in Trump USA dataset: ', trump_usa_df.shape)\nprint('Total number of records in Biden USA dataset: ', biden_usa_df.shape)","1074477b":"tids = trump_usa_df.tweet_id\nbids = biden_usa_df.tweet_id\n\nids_tweets_in_common = set(trump_usa_df.tweet_id).intersection(set(biden_usa_df.tweet_id))\nlen(ids_tweets_in_common)","b23d2d7e":"pd.options.display.max_colwidth = 1000 #by default, Python will likely display only the first 50 characters from a long text\n\nbiden_usa_df.tweet.loc[biden_usa_df.tweet_id.isin(list(ids_tweets_in_common))].head(5)","61e8ad80":"trump_usa_unique_df = trump_usa_df[~trump_usa_df['tweet_id'].isin(ids_tweets_in_common)]\nbiden_usa_unique_df = biden_usa_df[~biden_usa_df['tweet_id'].isin(ids_tweets_in_common)]","f536a128":"print('Total number of unique records in Trump USA dataset: ', trump_usa_unique_df.shape)\nprint('Total number of unique records in Biden USA dataset: ', biden_usa_unique_df.shape)","c59008df":"del trump_usa_df\ndel biden_usa_df","c7c4f391":"example_tweet = '#Wisconsin podr\u00eda ser el punto de inflexi\u00f3n en la carrera entre #Trump y #Biden https:\/\/t.co\/WFf8A1hAn7'","cc14ac8b":"#try out TextBlob on a sample tweet\n'''from textblob import TextBlob\nb = TextBlob(example_tweet)\nprint(f'Language of text: \\n{example_tweet}\\nis: {b.detect_language()}')\n'''","c22d3f11":"import chardet\nprint(chardet.detect(\"\u042f \u043b\u044e\u0431\u043b\u044e \u0432\u043a\u0443\u0441\u043d\u044b\u0435 \u043f\u0430\u043c\u043f\u0443\u0448\u043a\u0438\".encode('cp1251')))\nprint(chardet.detect(\"Et le fran\u00e7ais? Pouvons-nous obtenir une identification correcte?\".encode('cp850')))\nprint(chardet.detect(\"\u00a1Qu\u00e9 pena! pero todo el mundo ama el espa\u00f1ol, \u00bfverdad?\".encode('utf-8')))","d82efe8b":"#try out langdetect on a sample tweet\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\ndetect(\"#Wisconsin podr\u00eda ser el punto de inflexi\u00f3n en la carrera entre #Trump y #Biden https:\/\/t.co\/WFf8A1hAn7\")","6b58cf4c":"from langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\n\ndef get_language(tweet):\n    try:                                                          \n        lang=detect(tweet)                                      \n    except:                                                       \n        lang='no'\n        # for some tweets, detect will throw an error.\n        # uncomment the line below if you want to look further into this behavior\n        #print(\"This tweet throws an error:\", tweet)  \n    return lang","6663df04":"import time\n\nstart_time = time.time()\n\ntest_df = trump_usa_unique_df.iloc[:1000].copy()\ntest_df['lang'] = test_df.tweet.apply(lambda x: get_language(x))\n\nstop_time = time.time()\n\nprint(f'It took {np.around((time.time() - start_time),decimals=1)} seconds')","a1729204":"import time\nstart_time = time.time()\n\ntrump_usa_unique_df['lang'] = trump_usa_unique_df.tweet.apply(lambda x: get_language(x))\n\nstop_time = time.time()\nprint(f'It took {np.around((time.time() - start_time), decimals=1)} seconds')","feccbb9b":"start_time = time.time()\n\nbiden_usa_unique_df['lang'] = biden_usa_unique_df.tweet.apply(lambda x: get_language(x))\n\nstop_time = time.time()\nprint(f'It took {np.around((time.time() - start_time), decimals=1)} seconds')","c6e7755e":"# language detection may take 20-30 minutes, depending on your device\n# better run this cell to save the data, in case you walk away from your computer\n# and the Notebook enter sleep mode\n#biden_usa_unique_df.to_csv(out_folder + '\/biden_usa_unique_df.csv', index=False)\n#trump_usa_unique_df.to_csv(out_folder + '\/trump_usa_unique_df.csv', index=False)","5f27b2f3":"# if you used the cell above toout_foldersave to file the intermediary results\n# uncomment the lines below in case you need to load from file now\n#biden_usa_unique_df = pd.read_csv(out_folder + '\/biden_usa_unique_df.csv', lineterminator='\\r')\n#trump_usa_unique_df = pd.read_csv(out_folder + '\/trump_usa_unique_df.csv', lineterminator='\\r')","177ce35a":"biden_usa_unique_df.columns","53c90593":"plt.figure(figsize=(20,5))\nax = biden_usa_unique_df.lang.value_counts().plot.bar(rot=0)\nplt.setp(ax.get_xticklabels(), fontsize=16)\nplt.title('Frequency of languages in Biden tweets')\nplt.show()\n\nplt.figure(figsize=(20,5))\nax = trump_usa_unique_df.lang.value_counts().plot.bar(rot=0)\nplt.setp(ax.get_xticklabels(), fontsize=16)\nplt.title('Frequency of languages in Trump tweets')\nplt.show()","b3acced2":"biden_df = biden_usa_unique_df.copy()\ndel biden_usa_unique_df\n\ntrump_df = trump_usa_unique_df.copy()\ndel trump_usa_unique_df","f687ac1e":"biden_df = biden_df[biden_df.lang == 'en']\ntrump_df = trump_df[trump_df.lang == 'en']","a2edb76d":"print('Total number of records in Trump dataset: ', trump_df.shape)\nprint('Total number of records in Biden dataset: ', biden_df.shape)","90d7c070":"print(f'We retained {np.around(trump_df.shape[0]*100\/trump_initial_count, decimals=1)}% of the initial Trump dataset')\nprint(f'And {np.around(biden_df.shape[0]*100\/biden_initial_count, decimals=1)}% from Biden')","50989148":"trump_df['ds'] = 'trump'\nbiden_df['ds'] = 'biden'\n\n# Combine the filtered on United States Trump and Biden Datasets \ntweets_df = pd.concat([biden_df, trump_df],ignore_index=True)\n\n# covnert date to something we can work with easily\ntweets_df['created_at'] =  pd.to_datetime(tweets_df['created_at'])","42c7d59d":"start_time = time.time()\n\nplt.figure(figsize=(15,5))\n\ntweets_df.created_at.dt.date.value_counts().sort_index().plot.bar(rot=90, alpha=0.3,color='green')\n\nplt.setp(ax.get_xticklabels(), fontsize=16)\nplt.title('Frequency of tweets per day')\nplt.show()\n\nstop_time = time.time()\nprint(f'It took {(time.time() - start_time)} seconds')","a65ebe78":"most_popular_tweet = tweets_df.loc[tweets_df['retweet_count'].idxmax()]\nprint(f\" The tweet:\\n'{most_popular_tweet.tweet}'\\nwas retweeted the most ({most_popular_tweet.retweet_count} times).\")","794f7f92":"tweets_df[['tweet_id','user_id','created_at', 'likes', 'retweet_count', 'tweet', 'ds']].iloc[tweets_df.retweet_count.sort_values(ascending=False).head(5).index]","13df3c0b":"print(f'Our 2nd most popular tweet was retweeted for number of times equal to {np.around(13500*100\/tweets_df.shape[0], decimals=1)}% of our dataset size')","b6c34e36":"tweets_df[tweets_df.tweet.str.contains('Are you there')][['created_at', 'tweet', 'user_id']]","9ee249f0":"print(f'There are {tweets_df.retweet_count.nunique()} different amounts of retweets')","e5a52278":"sns.kdeplot(x='retweet_count', data=tweets_df)","191f9c62":"fig, ax=plt.subplots(1,1, figsize=(12,6))\n\nax.set_title('Frequency distribution of number of tweets per user', fontsize = 16)\nsns.kdeplot(trump_df.groupby(['user_id'])['tweet'].count(), shade=True, color='r', label='Trump', ax = ax)\nsns.kdeplot(biden_df.groupby(['user_id'])['tweet'].count(), shade=True, color='b', label='Biden', ax = ax)\nlabels= [\"Trump\", \"Biden\"]\nax.legend(labels)\n#ax.set_ylim(0, .005)\nplt.show()","0fd2bf08":"#pd.options.display.max_colwidth = 1000\ntweets_df.tweet.head(5)","8883add2":"def get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef clean_text(tweet, lemmatize = 'nltk'):\n    \"\"\"\n    Inputs:\n    tweet - a string representing the text we need to clean\n    lemmatize - one of two possible values {spacy, nltk} \n      two lemmatization methods\n      with our dataset, we got the best results with nltk\n      but Spacy also did a good job, hence you might \n      try both and compare results for your own data\n    \n    Output: \n    tokenized - the cleaned text, tokenized (a list of string words)\n    \"\"\"\n\n    tweet = tweet.lower() # lowercase\n    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE) # remove urls\n    tweet = re.sub(r'\\@\\w+|\\#','', tweet) # remove mentions of other usernames and the hashtag character\n    tweet = remove_stopwords(tweet) # remove stopwords with Gensim\n\n    if (lemmatize == 'spacy'):\n        # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n        nlp = spacy.load('en', disable=['parser', 'ner'])\n        doc = nlp(tweet)\n        tokenized = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n    elif (lemmatize == 'nltk'): \n        '''\n        lemmatization works best when WordNetLemmatizer receives both the text and the part of speech of each word\n        the code below assigns POS (part of speech) tag on a per word basis (it does not infer POS from contenxt \/ sentence), which might not be optimal\n        '''\n        lemmatizer = WordNetLemmatizer()\n        tokenized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(tweet)]\n    \n    # remove left over stop words with nltk\n    tokenized = [token for token in tokenized if token not in stopwords.words(\"english\")] \n\n    # remove non-alpha characters and keep the words of length >2 only\n    tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]\n\n    return tokenized\n\ndef combine_tokens(tokenized): \n    non_tokenized = ' '.join([w for w in tokenized])\n    return non_tokenized","8b5d38d9":"start =  time.time()\n\ntweets_df['tokenized_tweet_nltk'] = tweets_df['tweet'].apply(lambda x: clean_text(x, 'nltk'))\ntweets_df['clean_tweet_nltk'] = tweets_df['tokenized_tweet_nltk'].apply(lambda x: combine_tokens(x))\n\nstop = time.time()\nprint(f'Cleaning all tweets takes ~{round((stop-start)\/60, 3)} minutes: ')","b24f3d29":"tweets_df.to_csv(out_folder + '\/clean_tweets_df.csv', index=False)","6b618297":"tweets_df[tweets_df.tweet_id.isin(list(trump_df.tweet_id))].ds = 'trump'\ntweets_df[tweets_df.tweet_id.isin(list(biden_df.tweet_id))].ds = 'biden'","fb2654d3":"tweets_df.head(10)[['tweet', 'clean_tweet_nltk']]","4a637f06":"# Helper Function to assign Label for Sentiment Analysis with TextBlob\ndef create_sentiment_labels(df, feature,value):\n    '''\n    in: \n        dataframe \n        value on which to classify\n        feature - column name of the feature that receives the label\n    out: \n        does not return a value\n        modifies the dataframe received as parameter\n    '''\n\n    df.loc[df[value] > 0,feature] = 'positive'\n    df.loc[df[value] == 0,feature] = 'neutral'\n    df.loc[df[value] < 0,feature] = 'negative'","7ed023a9":"# Polarity and subjectivity\ndef sentiment_analysis(dataframe):\n    dataframe['blob_polarity'] = dataframe['clean_tweet_nltk'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    dataframe['blob_subjectivity'] = dataframe['clean_tweet_nltk'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n\n    create_sentiment_labels(dataframe, 'blob_sentiment','blob_polarity')\n    \n    return dataframe[['clean_tweet_nltk','blob_polarity','blob_subjectivity','blob_sentiment']].head()   ","9d43728a":"import time\nstart =  time.time()\n\nsentiment_analysis(tweets_df)\n\nstop = time.time()\nprint(f'Sentiment analysis with TextBlob took: {round((stop-start)\/60, 3)} minutes')","7f0faf68":"tweets_df.head(10)[['blob_polarity','blob_subjectivity', 'blob_sentiment']]","129e5f5d":"#update the divided dataset\ntrump_df = tweets_df[tweets_df.ds=='trump']\nbiden_df = tweets_df[tweets_df.ds=='biden']\n\nfig, axes = plt.subplots(1, 2, figsize=(8,5))\n\nfig.suptitle('TextBlob analysis: nmean polarity (-1.0, 1.0) and mean subjectivity (0.0, 1.0) per candidate (one tweet, one sentiment)', fontsize=14)\n\nfeatures = ['blob_polarity', 'blob_subjectivity']\nvalues = [trump_df.groupby(['user_id'])['blob_polarity'].mean().mean(), trump_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\naxes[0].bar(features,values, width=0.2)\naxes[0].set_ylim(0, .5)\naxes[0].set_title('Trump', fontsize = 14)\naxes[0].set_ylabel('Value', fontsize = 12)\n\nvalues = [biden_df.groupby(['user_id'])['blob_polarity'].mean().mean(), biden_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\naxes[1].bar(features,values, width=0.2)\naxes[1].set_ylim(0, .5)\naxes[1].set_title('Biden', fontsize = 14)\naxes[1].set_ylabel('Value', fontsize = 12)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()\n\ntrump_usa_pol_tweet =trump_df['blob_polarity'].mean()\ntrump_usa_subj_tweet = trump_df['blob_subjectivity'].mean()\nbiden_usa_pol_tweet = biden_df['blob_polarity'].mean()\nbiden_usa_subj_tweet = biden_df['blob_subjectivity'].mean()","8bfdb3cd":"# the below gives us a mean per user\n# trump_usa_df[['user_id', 'Polarity']].groupby(['user_id'])['Polarity'].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 5))\n\nfig.suptitle('TextBlob analysis: mean polarity (-1.0, 1.0) and mean subjectivity (0.0, 1.0)\\nper candidate (one user, one sentiment)', fontsize=14)\n\nfeatures = ['blob_polarity', 'blob_subjectivity']\nvalues = [trump_df.groupby(['user_id'])['blob_polarity'].mean().mean(), trump_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\naxes[0].bar(features,values, width=0.2,)\naxes[0].set_ylim(0, .5)\naxes[0].set_title('Trump', fontsize = 14)\naxes[0].set_ylabel('Value', fontsize = 12)\n\nvalues = [biden_df.groupby(['user_id'])['blob_polarity'].mean().mean(), biden_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()]\naxes[1].bar(features,values, width=0.2,)\naxes[1].set_ylim(0, .5)\naxes[1].set_title('Biden', fontsize = 14)\naxes[1].set_ylabel('Value', fontsize = 12)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()\n\ntrump_usa_pol_user = trump_df.groupby(['user_id'])['blob_polarity'].mean().mean()\ntrump_usa_subj_user = trump_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()\nbiden_usa_pol_user = biden_df.groupby(['user_id'])['blob_polarity'].mean().mean()\nbiden_usa_subj_user = biden_df.groupby(['user_id'])['blob_subjectivity'].mean().mean()","450ea178":"fig, axes = plt.subplots(1, 2, figsize=(10,6))\n\nfig.suptitle('TextBlob analysis: \\nmean polarity and mean subjectivity\\n (tweet level = one tweet, one sentiment) vs (user level = one user, one sentiment)', fontsize=16)\n\n#features = ['Polarity', 'Subjectivity']\nfeatures = np.array([1, 2])\nvalues_tweet = [ trump_usa_pol_tweet, trump_usa_subj_tweet]\nvalues_user = [ trump_usa_pol_user, trump_usa_subj_user]\n\n#values = [[trump_usa_pol_tweet, trump_usa_subj_tweet], \n#[trump_usa_pol_user, trump_usa_subj_user]]\n\naxes[0].bar(features-0.2, values_tweet, width=0.2, align = 'center', color = 'y')\naxes[0].bar(features, values_user, width=0.2, align = 'center', color = 'g')\n#axes[0].bar(features,values)\naxes[0].set_ylim(0, .5)\naxes[0].set_title('Trump', fontsize = 16)\naxes[0].set_xlabel('Feature', fontsize = 14)\naxes[0].set_ylabel('Average value', fontsize = 14)\naxes[0].set_xticklabels(['', 'Polarity', '', '', '', '', '', 'Subjectivity'])\nlabels= [\"tweet level\", \"user level\"]\naxes[0].legend(labels)\n\nvalues_tweet = [ biden_usa_pol_tweet, biden_usa_subj_tweet]\nvalues_user = [ biden_usa_pol_user, biden_usa_subj_user]\naxes[1].bar(features-0.2,values_tweet, width=0.2, align = 'center', color = 'y')\naxes[1].bar(features,values_user, width=0.2, align = 'center', color = 'g')\naxes[1].set_ylim(0, .5)\naxes[1].set_title('Biden', fontsize = 16)\naxes[1].set_xlabel('Feature', fontsize = 14)\naxes[1].set_ylabel('Average value', fontsize = 14)\naxes[1].set_xticklabels(['', 'Polarity', '', '', '', '', '', 'Subjectivity'])\n\nlabels= [\"tweet level\", \"user level\"]\naxes[1].legend(labels)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()","f9a47376":"plt.figure(figsize=(6,5))\n\nax = plt.gca()\nax.set_title('--Relative--\\nTextBlob sentiment analysis - \\nrelative frequency per valence type for each candidate', fontsize=16)\n\nfeatures = np.array([1,2,3])\ntrump = (trump_df['blob_sentiment'].sort_values().value_counts()\/trump_df['blob_sentiment'].shape[0])[['negative', 'neutral', 'positive']]\nax.bar(features-0.3, trump.values, width=0.3, align = 'center', color = 'r', alpha= .6)\n\nbiden = (biden_df['blob_sentiment'].sort_values().value_counts()\/biden_df['blob_sentiment'].shape[0])[['negative', 'neutral', 'positive']]\nax.bar(features, biden.values, width=0.3, align = 'center', color = 'b', alpha= .6)\n\nax.set_ylim(0, .5)\nax.set_xlabel('Valence', fontsize = 14)\nax.set_ylabel('Relative frequency', fontsize = 14)\n\nax.set_xticklabels(['', '', 'Negative', '', 'Neutral', '', 'Positive'])\n\nlabels= [\"Trump\", \"Biden\"]\nax.legend(labels)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()","5159eb58":"plt.figure(figsize=(6,5))\n\nax = plt.gca()\nax.set_title('--Absolute--\\nTextBlob sentiment analysis - \\nabsolute frequency per valence type for each candidate', fontsize=16)\n\nfeatures = np.array([1,2,3])\ntrump = (trump_df['blob_sentiment'].sort_values().value_counts())[['negative', 'neutral', 'positive']]\nax.bar(features-0.3, trump.values, width=0.3, align = 'center', color = 'r', alpha=0.6)\n\nbiden = (biden_df['blob_sentiment'].sort_values().value_counts())[['negative', 'neutral', 'positive']]\nax.bar(features, biden.values, width=0.3, align = 'center', color = 'b', alpha = 0.6)\n\n#ax.set_ylim(0, .5)\nax.set_xlabel('Valence', fontsize = 14)\nax.set_ylabel('Absolute frequency', fontsize = 14)\n\nax.set_xticklabels(['', '', 'Negative', '', 'Neutral', '', 'Positive'])\n\nlabels= [\"Trump\", \"Biden\"]\nax.legend(labels)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()","e64f74d6":"sid = SentimentIntensityAnalyzer()","464abe01":"def sentiment_analysis_vader(df, clean = True):\n    if clean:\n        target_col = 'clean_tweet_nltk'\n        prefix = 'vader_clean_'\n    else:\n        target_col = 'tweet'\n        prefix = 'vader_'\n    \n    scores_col = prefix+'scores'\n    \n    #let's make it vader_sentiment, so that it has the same naming convention as TextBlob and Flair sentiment score\n    #compound_col = prefix+'compound'\n    compound_col = prefix+'polarity'\n    \n    #comp_score_col = prefix+'comp_score'\n    comp_score_col = prefix+'sentiment' \n    \n    df[scores_col] = df[target_col].apply(lambda tweet: sid.polarity_scores(tweet))\n    df[compound_col] = df[scores_col].apply(lambda d: d['compound'])\n    create_sentiment_labels(df,comp_score_col,compound_col)","a410048f":"start = time.time()\n\nsentiment_analysis_vader(tweets_df)\nsentiment_analysis_vader(tweets_df, clean = False)\n\nstop = time.time()\nprint(f'Sentiment analysis with VADER took: {round((stop-start)\/60, 3)} minutes')\n\n#update the divided dataset\ntrump_df = tweets_df[tweets_df.ds=='trump']\nbiden_df = tweets_df[tweets_df.ds=='biden']","a4f80137":"from sklearn.metrics import accuracy_score\n\nstart = time.time()\n\nprint(f\"Accuracy score for our cleaning vs vader tweet cleaning for Trump: {accuracy_score(trump_df['vader_sentiment'],trump_df['vader_clean_sentiment']):.4}\")\nprint(f\"Accuracy score for our cleaning vs vader tweet cleaning for Biden: {accuracy_score(biden_df['vader_sentiment'],biden_df['vader_clean_sentiment']):.4}\")\n\nstop = time.time()\nprint(f'This took: {round((stop-start)\/60, 3)} minutes')","184da11d":"#Uncomment the code below if you want to visualize some tweets yourself\n#We decided based on analysis to feed the raw tweet to VADER and let it run the cleaning itself\n\n#Some tweets arel long. This setting makes sure Python does not truncate the output. Default value is 50.\n'''\npd.options.display.max_colwidth = 300\ntrump_df[['tweet', 'clean_tweet_nltk', 'vader_sentiment', 'vader_clean_sentiment']].head(20)\n'''","c005aa86":"def get_valence_relative_freq(df):\n    #grouped = df.sort_values('comp_score').groupby(['comp_score'], sort=False)\n    grouped = df.sort_values('vader_sentiment').groupby(['vader_sentiment'], sort=False)\n    valence = grouped['vader_sentiment'].value_counts(normalize=False, sort=False)\n    valence = valence.droplevel(0)\n    valence = valence \/ valence.sum()\n    return valence","80069cfd":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\ntrump_tmp = get_valence_relative_freq(trump_df)\nbiden_tmp = get_valence_relative_freq(biden_df)\n\n#plt.figure(figsize=(8,6))\nfig, axes = plt.subplots(1, 2, figsize=(8,5))\nfig.suptitle('Vader sentiment analysis - \\nrelative frequency per valence type for each candidate', fontsize=16)\n#fig.tight_layout()\n\n#sns.barplot(trump_tmp.index, trump_tmp.values, ax=axes[0])\n(trump_tmp).plot(kind='bar', ax = axes[0])\naxes[0].set_title('Trump', fontsize = 16)\naxes[0].set_xlabel('Valence', fontsize = 14)\naxes[0].set_ylabel('Relative frequency', fontsize = 14)\naxes[0].set_ylim(0, .5)\n\n#ax2 = sns.countplot(x=\"comp_score\", data=biden_tmp)\n#sns.barplot(biden_tmp.index, biden_tmp.values,  ax=axes[1])\n(biden_tmp).plot(kind='bar', ax = axes[1])\naxes[1].set_title('Biden', fontsize = 16)\naxes[1].set_xlabel('Valence', fontsize = 14)\naxes[1].set_ylabel('Relative frequency', fontsize = 14)\naxes[1].set_ylim(0, .5)\n\nplt.tight_layout()\nplt.show()","862f8326":"plt.figure(figsize=(6,5))\nsns.set_style(\"white\")\n\nax = plt.gca()\nax.set_title('--Relative--\\nTextBlob sentiment analysis - \\nrelative frequency per valence type for each candidate', fontsize=16)\n\nfeatures = np.array([1,2,3])\n\ntrump = get_valence_relative_freq(trump_df)\nax.bar(features-0.3, trump.values, width=0.3, align = 'center', color = 'r', alpha= .6)\n\nbiden = get_valence_relative_freq(biden_df)\nax.bar(features, biden.values, width=0.3, align = 'center', color = 'b', alpha= .6)\n\nax.set_ylim(0, .5)\nax.set_xlabel('Valence', fontsize = 14)\nax.set_ylabel('Relative frequency', fontsize = 14)\n\nax.set_xticklabels(['', '', 'Negative', '', 'Neutral', '', 'Positive'])\n\nlabels= [\"Trump\", \"Biden\"]\nax.legend(labels)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()","e103cbad":"print(trump_df['vader_polarity'].mean())\nprint(biden_df['vader_polarity'].mean())\n\nfig = plt.figure(figsize=(5,5))\n\nfig.suptitle('Mean VADER compund score (between -1.0 and 1.0)\\nfor Trump and Biden', fontsize=16)\n\nfeatures = ['Mean Valence Trump', 'Mean Valence Biden']\nvalues = [trump_df['vader_polarity'].mean(), biden_df['vader_polarity'].mean()]\n\nplt.bar(features,values, width=0.2)\n\naxes = plt.gca()\naxes.set_ylim(-.3, .3)\naxes.set_xlabel('Feature', fontsize = 14)\naxes.set_ylabel('Value', fontsize = 14)\n\nplt.show()","f9190393":"bins = 50\n\nfig = plt.figure(figsize=(8,5))\nfig.suptitle('Histograms of tweets polarity per candidate (VADER)', fontsize=16)\n\nplt.hist(trump_df['vader_polarity'], bins = bins, alpha = 0.5, color = 'r')\nplt.hist(biden_df['vader_polarity'], bins = bins, alpha = 0.5, color = 'b')\n\naxes = plt.gca()\naxes.set_ylim(0, 4000)\n\nlabels= [\"Trump\", \"Biden\"]\naxes.legend(labels)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.88])\nplt.show()","5275a1d7":"classifier = TextClassifier.load('en-sentiment')","7b449e23":"sentence = Sentence('The food was not horrible!')\nclassifier.predict(sentence)\n\nprint('Sentence above is: ', sentence.labels)","1489a883":"def flair_make_sentences(text):\n    \"\"\" Break apart text into a list of sentences \"\"\"\n    sentences = [sent for sent in split_single(text)]\n    return sentences\n\ndef flair_predict_sentences(sentence):\n    \"\"\" Predict the sentiment of a sentence \"\"\"\n    if sentence == \"\":\n        return 0\n    text = Sentence(sentence)\n    # stacked_embeddings.embed(text)\n    classifier.predict(text)\n    value = text.labels[0].to_dict()['value'] \n    if value == 'POSITIVE':\n        result = text.to_dict()['labels'][0]['confidence']\n    else:\n        result = -(text.to_dict()['labels'][0]['confidence'])\n    return round(result, 3)\n\ndef flair_get_scores_per_sentences(sentences):\n    \"\"\" Call predict on every sentence of a text \"\"\"\n    results = []\n    \n    for i in range(0, len(sentences)): \n        results.append(flair_predict_sentences(sentences[i]))\n    results.append(flair_predict_sentences(sentences[0]))\n    return results\n\ndef flair_get_sum(scores):\n    result = round(sum(scores), 3)\n    return result\n\ndef flair_get_avg_from_sentences(scores):\n    result = round(np.mean(scores), 3)\n    return result\n\ndef flair_get_score_tweet(text):\n  if not text:\n    return 0\n  s = Sentence(text)\n  classifier.predict(s)\n  value = s.labels[0].to_dict()['value'] \n  if value == 'POSITIVE':\n    result = s.to_dict()['labels'][0]['confidence']\n  else:\n    result = -(s.to_dict()['labels'][0]['confidence'])\n  return round(result, 3)\n\ndef sentiment_analysis_flair(polarity):\n  if polarity > 0:\n    return 'positive'\n  if polarity == 0:\n    return 'neutral'\n  if polarity < 0:\n    return 'positive'","3bb0e3c8":"records = 1000\ntemp = tweets_df[tweets_df.ds=='trump'][:records].copy()\n\nimport time\nstart = time.time()\n\n#flair sentiment by diving tweet into sentenes and averaging\ntemp['sentences'] = temp['clean_tweet_nltk'].apply(flair_make_sentences)\ntemp['scores'] = temp['sentences'].apply(flair_get_scores_per_sentences)\ntemp['flair_scores_avg'] = temp.scores.apply(flair_get_avg_from_sentences)\n\n#flair sentiment on the whole tweet\ntemp['flair_one_score'] = temp['clean_tweet_nltk'].apply(flair_get_score_tweet)\n\nstop = time.time()\nprint(round((stop-start)\/60, 3))","0b89e5f5":"bins = 50\nalpha = 0.6\nfig = plt.plot(figsize=(6,5))\n\nplt.title('Flair polarity: per sentence versus per tweet')\nax = plt.gca()\n\nax.hist(temp['flair_scores_avg'], bins = bins, alpha = alpha, color = 'r')\nax.hist(temp['flair_one_score'], bins = bins, alpha = alpha, color = 'g')\n\nax.set_ylim(0, 100)\nlabels= [\"Flair sentences\", \"Flair tweet\"]\nax.legend(labels)\nplt.show()","2d8351b8":"bins = 50\nalpha = 0.6\nfig = plt.plot(figsize=(8,7))\n\nplt.title('Distribution of sentiment scores\\nTextBlob vs VADER vs Flair', fontsize=16)\n\nax = plt.gca()\n\nax.hist(temp['blob_polarity'], bins = bins, alpha = alpha, color = 'r')\nax.hist(temp['vader_polarity'], bins = bins, alpha = alpha, color = '#ffd343')\nax.hist(temp['flair_one_score'], bins = bins, alpha = alpha, color = 'g')\nax.set_ylim(0, 100)\nlabels= [\"TextBlob\", \"VADER\", \"Flair\"]\nax.legend(labels)\n\nax.set_ylabel('Frequency', fontsize = 14)\n\n#fig.tight_layout(rect=[0, 0.03, 1, 0.9])\nplt.show()","7166ecd7":"temp['flair_sentiment'] = temp['flair_one_score'].apply(sentiment_analysis_flair)","632373f5":"print(f\"Consensus TextBlob - VADER: {accuracy_score(temp['blob_sentiment'],temp['vader_sentiment']):.4}\")\nprint(f\"Consensus TextBlob - Flair: {accuracy_score(temp['blob_sentiment'],temp['flair_sentiment']):.4}\")\nprint(f\"Consensus VADER - Flair: {accuracy_score(temp['vader_sentiment'],temp['flair_sentiment']):.4}\")","472898d1":"fig = plt.figure(figsize=(6,6))\n\nax = plt.gca()\nax.set_title('Agreement between TextBlob, VADER and Flair predictions', fontsize=16)\n\nfeatures = np.array([1,2,3])\nvalues = [accuracy_score(temp['blob_sentiment'],temp['vader_sentiment']), accuracy_score(temp['blob_sentiment'],temp['flair_sentiment']), accuracy_score(temp['vader_sentiment'],temp['flair_sentiment'])]\n\nax.bar(features, values, width=0.3, align = 'center', color = 'g', alpha= .6)\n\nax.set_ylim(0, .6)\n#ax.set_xlabel('Valence', fontsize = 14)\nax.set_ylabel('% of agreement', fontsize = 14)\n\nax.set_xticklabels(['', 'Consensus TextBlob\\n-VADER', '', 'Consensus TextBlob\\n-Flair', '', 'Consensus VADER\\n- Flair'])\n\n#labels= [\"Trump\", \"Biden\"]\n#ax.legend(labels)\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.80])\nplt.show()","167d4e27":"def consensus(row):\n    count = 0\n    count += row['blob_sentiment']==row['vader_sentiment']\n    count += row['blob_sentiment']==row['flair_sentiment']\n    count += row['vader_sentiment']==row['flair_sentiment']\n\n    return count\n\ntemp['consensus'] = temp.apply(lambda row: consensus(row), axis=1)\n\nprint(temp['consensus'].value_counts())","44898789":"# Change the number of displayd results in the code below if you want to visually inspect more of them yourself\n(temp[temp['consensus']==0])[['tweet', 'clean_tweet_nltk', 'blob_sentiment', 'vader_sentiment', 'flair_sentiment']].tail(5)\n","03f9f9f3":"rel_senti = tweets_df[['state','vader_clean_polarity','ds']].groupby(['state','ds']).mean()\nrel_senti.reset_index(inplace = True)","f82a9360":"rel_senti.head(6)","c11efda2":"rel_senti.shape","9f0b7d5a":"tmp = (rel_senti.state.value_counts()!=2).reset_index()\ntmp[tmp.state == True]","98e3768c":"rel_senti[rel_senti.state=='South Dakota']","05b52c17":"rel_senti = rel_senti.append({'state': 'South Dakota', 'ds':'trump', 'vader_clean_polarity':0}, ignore_index=True)\nrel_senti.sort_values('state', ascending=True, inplace=True)\nrel_senti.reset_index(drop=True)","66d8edbd":"rel_senti = rel_senti[['state','vader_clean_polarity']].groupby('state')\nrel_senti = rel_senti.diff(periods=-1)\n\n#uncomment to see what we've done so far\n#rel_senti","a995c282":"rel_senti.dropna(inplace=True)\nrel_senti.reset_index(inplace = True, drop=True)\nrel_senti.head(4)","35846916":"states_df = pd.DataFrame(data=np.sort(tweets_df.state.unique()), columns=['state'])\nrel_senti = rel_senti.join(states_df)\nrel_senti.head(3)","c719f46b":"geo_usa_df = pd.read_json('..\/input\/a-simple-geojson-file-for-us-state\/geo_usa.json')\ngeo_usa_df.head()","3a0af4a0":"GEO_ID = []\nSTATE = []\nNAME = []\nCENSUSAREA = []\ngeometry = []\n\nfor row in geo_usa_df.features.iteritems():\n    GEO_ID.append(row[1][\"properties\"][\"GEO_ID\"])\n    STATE.append(row[1][\"properties\"][\"STATE\"])\n    NAME.append(row[1][\"properties\"][\"NAME\"])\n    CENSUSAREA.append(row[1][\"properties\"][\"CENSUSAREA\"])\n    geometry.append(row[1][\"geometry\"])","d8abeb87":"geo_usa_df.head(1)","88126cec":"geo_usa_df['GEO_ID'] = GEO_ID\ngeo_usa_df['STATE'] = STATE\ngeo_usa_df['NAME'] = NAME\ngeo_usa_df['CENSUSAREA'] = CENSUSAREA\ngeo_usa_df['geometry'] = geometry","c96ade0c":"#Rename NAME feature to State\ngeo_usa_df = geo_usa_df.rename(columns={'NAME': 'state'})","f661819a":"row = rel_senti[rel_senti.state=='Maine']\nbiden_score = row.vader_clean_polarity","4cfc6cb1":"def assign_candidate(x):\n    row = rel_senti[rel_senti.state==x]\n    biden_score = row.vader_clean_polarity\n    \n    if biden_score.iloc[0]>0:\n        return 'biden'\n    else:\n        return 'trump'\n\ngeo_usa_df['winner'] = geo_usa_df['state'].apply(lambda x: assign_candidate(x))","1ed860d3":"def assign_biden_score(x):\n    row = rel_senti[rel_senti.state==x]\n    biden_score = row.vader_clean_polarity\n    return biden_score.iloc[0]\n    \ngeo_usa_df['biden_score'] = geo_usa_df['state'].apply(lambda x: assign_biden_score(x))","ddc0a291":"def Diff(li1, li2):\n    return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))\n \n# Driver Code\nli1 = NAME\nli2 = list(tweets_df.state.unique())\n\nprint(Diff(li1, li2)) #an empty output means the two lists of states are identical","a368f71f":"# Read in the lat and long of states captials \ngeo_us_capitals_df = pd.read_csv('..\/input\/usa-state-capitals-lat-long\/usa-state-capitals.csv')\n\n#Rename 'origin' feature to state\n#geo_us_capitals_df = geo_us_capitals_df.rename(columns={'origin': 'state'})\ngeo_us_capitals_df.head()","2cb21990":"geo_usa_df = pd.merge(geo_usa_df, geo_us_capitals_df, on='state', how='left')","33fb5924":"geo_usa_df.to_json(out_folder+'\/geo_usa_senti.json',orient='records')","1ff324ea":"## 2. Text pre-processing\n\nBefore we begin to use the text of the tweets, we need to perform some **transformations on the text**.  \nLet's examine a few tweets to see why.","21d0f587":"### Let's see how our results are influenced by choosing either of the two options mentioned above\n","93313388":"Helper functions for performing the sentiment analysis using Flair","695c35f9":"So, our most popular tweet contains irony. I'm curious to see how sentiment analysis libraries will perform on this.","3c75a24a":"Cleaning our tweets.","17e75768":"## Average sentiment score per candidate","3a7a1593":"In the kdeplot below we explore the retweeting behavior. It looks like a huge amount of tweets are never retweeted. And then we have a tiny number of tweets that get retweeted all the way up to ~ 17.500 times. This was to be expected.","71f463f3":"Multiply the result above with 100 and we have a rough estimation of how long we'll have to wait per dataset. ","9eaff5d1":"Let's explore further the differences between sentiment for the two candidates.  \nWe continue with visual inspection of the distribution of sentiment scores.","0000e0b9":"![Relative sentiment towards Biden](https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/03\/relative_senti.jpg)","3b9a455e":"## 4. Sentiment analysis with TextBlob\n\nAccording to [TextBlob's official website](https:\/\/textblob.readthedocs.io\/en\/dev\/), TextBlob \"provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\"\n\nTextBlob library will output something like this for each snippet of text that it analyzes:  \n_Sentiment(polarity=-0.125, subjectivity=0.5916666666666667)_\n\nThat is, TextBlob will output:\n- a measure of polarity, which can have values in the interval \\[-1, 1\\]\n- an estimation of subjectivity, ranging is \\[0.0, 1.0\\] where 0.0 is very objective (dealing with facts) and 1.0 is very subjective (opinions). \n\nInterpreting the results:  \n- TextBlob documentation does not give precise guidelines for interpreting the numberic output ([see TextBlob ReadTheDocs section for Sentiment analysis here](https:\/\/textblob.readthedocs.io\/en\/dev\/quickstart.html#sentiment-analysis))\n- We will adopt the approach we encountered in this [KDNuggets post](https:\/\/www.kdnuggets.com\/2018\/08\/emotion-sentiment-analysis-practitioners-guide-nlp-5.html):  \n  - we will label <0 values as 'negative' \n  - 0 values as 'neutral' \n  - and \\>0 values as 'positive'","b95b5a14":"Let's have a look at number of tweets per valence for both candidates and see if we spot anything interesting. ","4ffeea74":"We begi by computing mean polarity per candidate per state.","99ec1a3c":"For better visualizing these results, I will use [Kepler](http:\/\/kepler.gl\/) to display them overlaid on a map showing USA states. \n\nFor this purpose, I will also need a geoJSON file which I obtained from <a href='https:\/\/eric.clst.org\/tech\/usgeojson\/'>this link<\/a>\n\nIf you're not familiar with GeoJSON, I found <a href=\"https:\/\/developer.here.com\/blog\/an-introduction-to-geojson\">this<\/a> to be a good introduction.\n\nWe are interested in the public opinion of users that are tweeting about the election from the USA.\n\n### Visualizations\n\nTo better visualize our data we will use the geographic map of the United States in our visualizations. More specifically we will use a geojson file of the US states that we can then process with [Kepler](http:\/\/kepler.gl\/). The .json file that we are using can be retrieved [here](https:\/\/eric.clst.org\/tech\/usgeojson\/). \n\nFist we load the json file into a DataFrame.","c1e463cf":"The absolute frequency plot is relevant because all those negative tweets could potentially be support votes for the other candidate, since in presidential elections people only have 2 options. If they hate one candidate, that could be enough reason to vote for the other one. ","b4f4b837":"## 6. Sentiment analysis with Flair\n\nFlair is a pre-trained character-level LSTM (recurrent neural networks) classifier which takes into account:\n- the sequence of words\n- the sequence of letters \n- intensifiers ('so', 'very' etc)\nAdvantage over VADER: by looking at character level, it can recognize and correct for typos (e.g. it will recognize that 'anoy' means 'annoy'), which for VADER would just be an OOV (Out Of Vocabulary) word (and thus ignored).\n\nAdvantage over VADER: by looking at character level, it can recognize and correct for typos (e.g. it will recognize that 'anoy' means 'annoy'), which for VADER would just be an OOV (Out Of Vocabulary) word (and thus ignored).\n\n### Pre-trained Flair models\nAs we lack computing power, we will use a freely available pre-trained Flair model.  \n\nFor sentiment classification, Flair offers two pre-trained models (as mentioned in one of the [official Flair tutorials on their github here](https:\/\/github.com\/flairNLP\/flair\/blob\/master\/resources\/docs\/TUTORIAL_2_TAGGING.md)):\n\n<style type=\"text\/css\">\n\ttable.tableizer-table {\n\t\tfont-size: 12px;\n\t\tborder: 1px solid #CCC; \n\t\tfont-family: Arial, Helvetica, sans-serif;\n\t} \n\ttable.tableizer-table td {\n\t\tpadding: 4px;\n\t\tmargin: 3px;\n\t\tborder: 1px solid black;\n\t}\n\ttable.tableizer-table th {\n\t\tbackground-color: #104E8B; \n\t\tcolor: #FFF;\n\t\tfont-weight: bold;\n\t}\n<\/style>\n\n<table class=\"tableizer-table\" style=\"border: 1px solid #CCC;\">\n<thead><tr class=\"tableizer-firstrow\"><th>ID<\/th><th>Language<\/th><th>Task<\/th><th>Training Dataset<\/th><th>Accuracy<\/th><\/tr><\/thead><tbody>\n <tr><td>sentiment'<\/td><td>English<\/td><td>detecting positive and negative sentiment (transformer-based)<\/td><td>movie and product reviews<\/td><td>98.87<\/td><\/tr>\n <tr><td>sentiment-fast'<\/td><td>English<\/td><td>detecting positive and negative sentiment (RNN-based)<\/td><td>movie and product reviews<\/td><td>96.83<\/td><\/tr>\n<\/tbody><\/table>","d3ffb6a7":"## 5. Sentiment analysis with VADER\n\nVADER (Valence Aware Dictionary for Sentiment Reasoning) was developed in 2014.   \nYou can check [Vader's official github](https:\/\/github.com\/cjhutto\/vaderSentiment) for details of how the tool was designed and how to use it. \n\nAccording to [VADER's github](https:\/\/github.com\/cjhutto\/vaderSentiment), VADER is \"Empirically validated by multiple independent human judges, VADER incorporates a \"gold-standard\" sentiment lexicon that is especially attuned to microblog-like contexts.\"\n\nVader is a pre-trained model. If you want to read about the model in detail, the official website recommends [2]\n\n\nVader **outputs** something like this:   \n_{'neg': 0.0, 'neu': 0.436, 'pos': 0.564, 'compound': 0.3802}_\n\n**Negative**, **neutral** and **positive** are scores between 0 and 1.  \nThe **compound** value reflects the overall sentiment of the text. It's computed based on the values of negative, neutral and positive. It ranges from -1 (maximum negativity) to 1 (maximum positivity). \n\nThe is no standard way to interpret compound. \nOne can decide that whatever is larger than 0 is positive and lower is negative, while 0 means neutral.    \nBut we can also decide to look only at more extreme values, like above or below +\/- 0.8, for example.   \nIt really depends on the kind of data you have.\n\n**\\[2]** Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.","6db59be1":"Each dataset has 21 columns (21 features).  \nAnd we have ~970k tweets in the Trump dataset and ~776k for Biden.","854cd574":"The above plot shows that the two methods for computing polarity of a tweet with Flair produce exactly the same results. It might sound silly, but worth trying out because this level of details are not easily accessible from the documentation.","34099a9b":"The results above indicate two possibilities:\n\n    - either all algorithms are wrong a lot of times\n    - or just two of them are mostly wrong and one does a good job.\n\nUnfortunately, I know of no other way to automatically compare performance, so we will visualize a few examples where they all **disagree** and try to eyeball who's the right one.\n","d2bfed72":"![States by most liked candidate](https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/03\/winners.jpg)\n\nVisualization of most liked candidate per state, as assessed by the relative average sentiment score. \n\nI got the idea of showing geospatial data using Kepler from my colleague, <a href='https:\/\/www.linkedin.com\/in\/lara-wagner-8446789a\/'>Lara Wagner<\/a>","99bfae1b":"Let's explore the Flair results in the context of a comparison between all three methods (TextBlob, VADER and Flair).","9b875658":"Let's just make sure the json we used has the exact same name for states as our DataFrame. Just to prevent silly errors further down the road.","3520f1d0":"For which candidate are we missing tweets in South Dakota ?","d7d25d30":"For each state we should have two entries: one for the average sentiment towards Biden and one for Trump. The code cell above indicates one datapoint is missing (because we have the odd number of entries 103 instead of 104).  \nWhat info is missing ?","e764bbba":"### 1.6 \"Vocal minority\" and \"silent majority\" effect\n\nMustafaraj et al. 2011 \\[1] showed evidence of the existance on social media of a minority of users which are very vocal, while there is a majority of users which hardly produce content.\n\nWe explore this phenomenon in our dataset by looking at the distribution of the number of tweets per user.  \nThe frequency distribution we obtain confirms that there are a small number of users producing a large portion of the tweets for both candidates (the trend is stronger for Biden).  \nThis indicates that:    \n- drawing conclusions about which candidate is preferred based on the number of tweets would be influenced strongly by this small number of very active users.  \n\nIn the section dedicated to 'predicting' election results from Tweets, we will see how we can enforce a policy of 'one vote per person' when analysing tweets.  \n\n**\\[1]** Eni Mustafaraj, Samantha Finn, Carolyn Whitlock, and Panagiotis Takis Metaxas. **Vocal minority versus silent majority: Discovering the opionions\nof the long tail.** In SocialCom\/PASSAT, pages 103\u2013110, 2011.","b2ca2f1c":"## 1. Exploratory Data Analysis\n\nThis section will contain both data exploration as well as some data wrangling. As we explore, we realize there are things we need to remove, add, change.   \n\nOur data is separates into two csv files. ","1da04085":"## 3. Intro to sentiment analysis\n\nSentiment analysis means computing in some way the overall valence of the text. Valence usually means whether a text is negative, neutral or positive.\n\n![image.png](attachment:image.png)\n","0558d483":"Does it matter if we clean the tweets before feeding them to Vader ? Does Vader itself perform a good enough cleaning ?\nWe answer this question by classifying tweets into positive \/ neutral \/ negative using both approaches and then looking at the accuracy_score for the labels obtained through the two methods.","57d6dd4c":"# <center>NLP for sentiment analysis of tweets: demo for using the most popular libraries\n","95d9166f":"Here's an example of how to use Flair classifier to predict for one sentence. Flair clasifier outputs the assigned label and a value between 0 and 1 indicating the confidence level for this prediction.","ce7b141d":"Our dataset has the following columns (I have higlighted what I think is relevant for our sentiment analysis):\n\n![image.png](attachment:image.png)","2b3c7b91":"Check out our language results.","26c372b4":"## 7. Which is the best sentiment analysis library ?","5e344ba5":"Around 20.000 tweets show up in both dataset. I think it doesn't make sense for the same tweet to contribute to compute the sentiment towards Biden and towards Trump. Let's say one tweet has a negative emotion, do we know if it's negative towards both candidates or only towards one of them and just mentions the second ?  \nLet's have a look at a few of the 'duplicate' tweets.","8a07e10b":"Keep only tweets from USA.","609de6e0":"Are we missing any data points ?","d51623fa":"From the tweets above, it looks like the duplicate tweets are not really helpful for a per candidate sentiment analysis, so I'll drop them.  ","1b1ddd27":"Function for labelling tweets based on classification criteria described above.","a3f5ff8a":"I prepared a screenshot of several tweets I inspected, hopefully this is easier to visualize.  \nI highlighted in green the instances where it seemed to me there was a clear winner. \n\n![image.png](attachment:image.png)","ad1174b6":"Chardet won't recognize French or Spanish, for example.","de5f7b3f":"And now lets see how many tweets we're left with.","46422d1c":"Let's display the states capitals too, just to make our plot prettier.  \nus-state-capitals.csv contains the latitude and longitude for all state capitals. ","85b7065e":"## Comparison of the three sentiment analysis libraries\n\nWe only perform this analysis on the 1000 data points because Flair is very resource intensive. But try it out on the whole dataset if you have more computing power.","031e9145":"We keep only the tweets in English.  \nFirst, I'm getting rid of the long names, as they're making it more difficult to follow along, rather than helping, now that they became quite long.","a83313a1":"## Observations from the distribution plot of sentiment produced by TextBlob, VADER and Flair\n\nWe notice that TextBlob and VADER tend to:  \n- classify a lot of the data as neutral\n- VADER has a bimodal distribution, while TextBlob is unimodal. \n\nFlair:  \n- has no predilection for neutral. \n- and it's extremly polarizing, compared to TextBlob or VADER\n\nRegarding the lack of a strong neutral category in Flair (compare it to TextBlob and VADER for example), [Flair co-creator, Alan Akbik,](https:\/\/youtu.be\/5Hg18QxU6mA?t=2481) explains that when Flair sentiment analysis model was trained on reviews dataset and there was too much variability in people's attitudes in the middle, which prevented the model from learning something useful for a rating that translates to 'average'.\nSome people would give an average rating if the product\/service had a few shortcomings, while others would punish with an average rating if it was a complete disapointment.  \nAccording to Alan Akbik, they ended up training only on more extreme reviews, to avoid the middle reviews with very low signal to noise ration.  \nThat was the best approach for movie reviews. But we don't know if it's the best for our data too, for tweets.","de82d917":"#### Option b  \n\nAnother option is to:\n- first average sentiment expressed through tweets per user id -> we will have one averge expressed sentiment per user per candidate\n- then average across the whole population for each candidate","f4e14345":"### States by most liked candidate  \n\nI mentioned earlier I was going to use <a href='https:\/\/kepler.gl\/'>Kepler<\/a> for displaying insight from the sentiment analysis of tweets. You can see that this form of visualization is much more powerful than showing the same data as a table of numbers or even various types plots.  \n\nFor an introduction to using Kepler for obtaining such an image, check out <a href=\"https:\/\/www.youtube.com\/watch?v=BEZjt08Myxs\">this video tutorial<\/a>.","5757d65a":"### States by sentiment intensity towards Biden\n\nWe know from the image above which are the states where people are more positive towards Trump than towards Biden. But we also want to know by how much. This will help our campaign strategy because we can direct our efforts where they are most needed or even where we can obtain the highest effect for the lowest 'effort'.  \n\nThe legend on the right is the default one from Kepler. If it's too small to read, mind that the whiter the shade, the more people in that state prefer Trump.  ","a3f29ec1":"### 1.1 Localization\n\nI'm interested in the Countries our users are tweeting from. I only want to keep USA data for this analysis, because my public of interest are those who will actually vote in the USA 2020 presidential elections. I can't know for sure which ones of these tweet authors are USA registered voters, but removing tweets outside the US seems like a good enough heuristic. ","6e5695e9":"### 4.1 Different ways to look at the same data.  \n\n#### Option a\n\nIn the next plot, we will compute an average polarity per candidate as follows:  \n- we simply average the polarity feature computed by TextBlob across all tweets per candidate. \n\nIssues with this approach:  \n- imagine we have 1 user who tweeted 99 times, each having polarity -1 (a candidate opposer). And 1 user who tweeted once with polarity 1 (a candidate supporter). If we average across all tweets, we obtain **-0.98**.  \n\nWhat can we infer from this result ? \n- inferring support \/ opposing for the candidate would be wrong, since in our sample we actually have one opposant and 1 supporter.\n- we can only infer the trivial: that the result is just the average sentiment across all tweets.","35a3911b":"Let's have a look at what some tweets look like after 'cleaning'. ","fb70d9a8":"## 8. Actionable insights from sentiment analysis of tweets\n\nThere are several ways to use sentiment analysis on tweets related to a political campaign.  \n\nOne is to try to predict the election results from the sentiments towards the candidated. \n\nAnother one, which I will focus on, is to gauge how the voters feel towards the candidates while the campaign is still ongoing. I will focus on this one. Our question is: in which states is candidate Biden running behind Trump (according to the average sentiment expressed by tweets from those states) ?  ","f1922e14":"The pre-trained classifier of Flair was trained on **IMDB reviews database** . We don't know how well this may generalize to our tweets data.","4ad4e307":"Let's compare the following:\n\n1. TextBlob\n2. VADER\n3. Flair per sentence\n4. Flair per tweet\n\nOur dataset is not labelled, so there is no way for us to compare predictions to some 'ground truth'.  \nWe will, instead, compare each algorithm's predictions to the ones from the other two.  ","2154ef6d":"### Agreement between TextBlob, VADER and Flair predictions\n\nWe know the three differ in how extreme their predicted sentiment value is. But do they agree on the direction of that sentiment, regardless of whether they agree on intensity. That is, if TexBlob predicts a weak sentiment and Flair a strong sentiment, are they both negative or both positive ? Or one predicts a -.2 (weak negative) and the other a .9 (strong positive) ?\n\nTo answer the question above, tet's see the percentage of times these algorithms agree with one another when classifying the sentiment of a tweet.","1575d5b0":"### 1.2 Duplicate tweets\n\nI already know (from how the dataset was collected) that the split into two datasets was made on the criterion: tweets mentioning Trump went into Trump file and tweets mentioning Biden went into the Biden csv file.  \n\nBut how about tweets mentioning both of them ? I'll check if the two datasets have any tweets in common. ","697c8231":"My Notebook went into sleep mode quite a few times so now I'm into the habbit of saving data that takes a while to compute, just in case. ","143f28d7":"We have no tweets (in English) about Trump from South Dakota. So we will assign an average sentiment of 0, which makes sense because 0 is neutral.","90f5b190":"### 1.4 Date analysis\n\nWhen were these tweets created ?","e0c018c6":"Different libraries \/ algorithms assess text valence in different ways, but this would be the gist of the sentiment analysis.\n\nThe most popular algorithms are:\n- **Rule-based models**  \n    For example, TextBlob and Vader\n    They use a bag-of-words approach: the text is considered to be the sum of its constituent words, \n- **Word-embedding-based models:**  \n    Words are represented as vectors of numbers in an n-dimensional space\n    This mapping from individual words to a continuous vector space can be generated through various methods: neural networks, dimensionality reduction, co-occurence matrix.\n\n![image.png](attachment:image.png)\n\n### Popular libraries for sentiment analysis\nFor this analysis of tweets I tried three of the currently most popular sentiment analysis libraries.  \n**TextBlob** and **Vader** use rule-based models, while **Flair** uses word embeddings.\n\nAll three output a **continuous** number between -1 and 1.  \nIf one needs a classification into **categories** instead of these numerical values, the common interpretation is that <0 is negative, 0 is neutral and >0 is positive. The cutoff points for the three categories are not set in stone and can be adapted based on the results \/ visual inspection.\n\nLet\u2019s discuss the differences now: \n1. **<a href='https:\/\/textblob.readthedocs.io\/en\/dev\/quickstart.html#sentiment-analysis'>TextBlob<\/a>**\n    is the simplest of them\n    It does estimate though how factual versus opinionated a text is\n2. **<a href='https:\/\/github.com\/cjhutto\/vaderSentiment'>Vader<\/a>**\n    The valence for the words in the dictionary was empirically validated by multiple human judges \u201cespecially attuned to microblog-like contexts\u201d  \n    Uses some heuristics to recognize word negations (\u201ccool\u201d versus \u201cnot cool\u201d) and word intensifiers (\u201ca bit sad\u201d versus \u201creally sad\u201d)  \n    Cannot recognize typos and will consider them out of vocabulary words (veri relevant for twitter, where users tend to not spell correctly)\n3. **<a href='https:\/\/github.com\/flairNLP\/flair\/blob\/master\/resources\/docs\/TUTORIAL_2_TAGGING.md'>Flair<\/a>** is a pre-trained character-level LSTM (recurrent neural networks) classifier which takes into account:  \n    the sequence of words  \n    the sequence of letters -> recognizes typos  \n    intensifiers ('so', 'very', \u2018a bit\u2019 etc)   \n    Flair is trained on IMDB movie reviews dataset and retraining is resource intensive.  \n    Very polarizing (assigns very positive or very negative scores), but not much in the middle\n","7c07ae44":"In the frequency distribution of tweets per day below we see an expected pattern. There was more activity on the 23rd of October, when the last debate between the two candidates took place. And then the activity increases again after Nov 3rd, the election day.","3854a29d":"From my initial tests with TextBlob I saw that language detection will take a long time. Let's first try it on 1.000 records to get an idea of how long we'll have to wait for our full datasets language analysis.","79af9d95":"For lemmatization, different libraries give slightly different results.\n\nFor an extensive comparison, see this article: https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/.  \n\nThe clean_text function below can use either nltk or spacy, depending on the argument it receives. \n\nI have used both and extracted a few tweets for a comparison, shown in the next image. I highlighted in blue the correct form of the words and in red the wrong ones.  \n![Lemmatization with spacey and nltk](https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/03\/lemmatization.jpg)\n\nIt looks like there's no clear winner between nltk or spacey. You can see in the image above that each of them makes mistakes.\n\nYou can use clean_text function to do an analysis yourself.","1c2b355d":"First, we clean our data:\n- we convert everything to lowercase\n- we remove punctuation, links, @mentions and # hashtags\n- we remove stop words - stop words are a set of commonly used words in any language. For example, in English, \u201cthe\u201d, \u201cis\u201d and \u201cand\u201d. These don't add any meaningful information for our analysis\n- lemmatization - reduces inflected words to the root of that word (e.g. 'pursuing' becomes 'pursue')\n- tokenization - split each tweet into a list of individual words","3fd4c060":"Now we know where we need to intervene to win people over without running expensive polls where hundreds or thousands of people get called and surveyed.  \n\nWe can pair this analysis with a topic modelling analysis of the same tweets to see which topic are hot in the respective states, so that we know what to discuss about for the highest effect in each of these states. I will dedicate a separate Notebook to topic modelling using Latent Dirichlet allocation.","3c6c428e":"## Flair: predict sentiment per sentence versus sentiment per tweet\n\nBut first we will explore and compare sentiment labelling for two ways to use Flair and here is why.  \nWe asked oursleved what is the best way to perform prediction for Tweet ?   \n- option 1: Should we predict on the whole Tweet ?  \n- option 2: Should we split into sentences, predict for each sentence and then make an average ?\n\nAs we did not find clear indications for this in the official [Flair documentation](https:\/\/github.com\/flairNLP\/flair), we will explore both methods. ","52288c21":"<center>\nIn this notebook I will use three of the most popular libraries for <strong>sentiment analysis<\/strong> on text data.<br\/><br\/>\nIt's not any text data, though. I'll be using a dataset of tweets, collected for a period of three weeks around the <a href=\"https:\/\/www.kaggle.com\/manchunhui\/us-election-2020-tweets\/code\">2020 US presidential elections<\/a>.<br\/><br\/>\nI downloaded the dataset directly from the link above, but if you want to tackle another topic, you can open a <a href=\"https:\/\/developer.twitter.com\/en\/apply-for-access\">Twitter developer account<\/a> and start collecting your own tweets using <a href=\"https:\/\/developer.twitter.com\/en\/docs\/twitter-api\/tweets\/lookup\/introduction\">Tweet lookup<\/a> or <a href=\"https:\/\/github.com\/JustAnotherArchivist\/snscrape\">snscrape<\/a>.<br\/><br\/>\nI start with some classic exploratory data analysis (to answer questions like how many tweets we have or what is the date range ?), I proceed to text pre-processing (text data has a lot of extra material like stop words and many words are not in the most useful form (e.g. plurals will be converted to singular etc), then I do sentiment analysis using three libraries (TextBlob, VADEDR and Flair) and compare the results to see which is the most suited for our dataset.<br\/><br\/>\nAnd because I am using data related to a political campaign, I want to see what actionable insights we can draw based on the sentiment analysis results. After all, the purpose of any Data Science analysis is to find out how to solve a problem. In this case, I'm part of the campaign management for one of the candidates and we use Twitter sentiment analysis to see how to increase our voter base. #This is a hypothetical problem; I am not actually involved in Politics and hope to never be.<br\/><br\/>","025baf59":"Observations from the above plot:\n- the ratio of positive:negative is higher for Biden than for Trump. When people tweet about Biden, they tend to be less negative than when they tweet about Trump.","4ab0dad1":"It looks like there is 84% consensus for Trump and 88% consensus for Biden for sentiment per tweet when VADER is fed the raw data versus the cleaned data.  \nSo that means the decision to feed raw or cleaned data should be given some thought.  \nSince we don't have labelled data, the only way to decide which method is best is by visual inspection. ","b981eaa8":"![image.png](attachment:image.png)","7d7ab485":"### Some observations we can make based on plot above:\n- within-candidates: Trump has a ratio of 1:1 for positive to negative tweets, while for Biden, it's almost 2:1\n- between-candidates: \n    - higher % of positive tweets for Biden\n    - higher % of negative tweets for Trump","33fc5081":"The geo_usa_senti.json file created by running the code cell above is the one I imported in <a href='https:\/\/kepler.gl\/'>Kepler<\/a> to produce the following two visualizations.  \n\nIf you want to try out Kepler yourself and produce your own geospatial visualization, <a href='https:\/\/www.youtube.com\/watch?v=BEZjt08Myxs&ab_channel=LeighHalliday'>this turorial<\/a> is a good place to get your started.","59db762d":"Assign one candidate to each state depending who is more 'liked'.","1fac1132":"But langdetect seems to do a good job.","eb267816":"## Table of contents\n\n1. [Exploratory Data Analysis](#1.-Exploratory-Data-Analysis)\n2. [Text pre-processing](#2.-Text-pre-processing)\n3. [Intro to sentiment analysis](#3.-Intro-to-sentiment-analysis)\n4. [Sentiment analysis with TextBlob](#4.-Sentiment-analysis-with-TextBlob)\n5. [Sentiment analysis with VADER](#5.-Sentiment-analysis-with-VADER)\n6. [Sentiment analysis with Flair](#6.-Sentiment-analysis-with-Flair)\n7. [Which is the best sentiment analysis library ?](#7.-Which-is-the-best-sentiment-analysis-library-?)\n8. [Actionable insights from sentiment analysis of tweets](#8.-Actionable-insights-from-sentiment-analysis-of-tweets)","1551f34c":"### 1.5 Tweets popularity analysis","af1a95c7":"Interestingly, our dataset does not contain the retweets. If we would have had 10% of our dataset consisting of the same tweet retweeted by various people, we would be looking further into this phenomenon to decide if we should include all of them. But since it's not the case, we proceed to next step.  \n\nLet's see a distribution of retweets.","ca585c18":"It doesn't really matter how we average the sentiment. In any case, \"one user one sentiment\" makes much more sense for our analysis. But it's easier to code the \"one tweet one sentiment\", so we'll use this one, since it has the same result.","e7445171":"Remember when we removed from our dataset tweets in other languges than English ? If we had kept them, our 'en' classifier declared above wouldn't have been able to interpret them anyway. ","6195fb9f":"Based on the distribution plots and after inspecting a few tweets where all three algorithms applied different labels, I decided to use in further analyses the results from **VADER**\n","720ec1b8":"### 1.3 Language\n\n\nAnd the second observation is that we have Tweets in Neglish and Spanish too. Most Natural Language Processing libraries can only handle a single language. So we will keep only the tweets in English.  \n\nI tried several of the packages that can handle language detection which are mentioned in this <a href=\"https:\/\/stackoverflow.com\/questions\/39142778\/python-how-to-determine-the-language\">Stackoverflow thread<\/a>.  \n\nTextBlob unfortunately cannot handle this many requests (for each of our tweets), because it uses Google Translate API and the number of successive calls is limited in time. \n\n<a href=\"https:\/\/chardet.readthedocs.io\/en\/latest\/usage.html\">chardet<\/a> only works for Cyrillic alphabet and some other exotic languages, but not for English or Spanish.  \n\nInstalling <a href=\"https:\/\/github.com\/bsolomon1124\/pycld3\">pyCLD3<\/a> on a Windows machine (my personal laptop where I developed this Notebook) was a nightmare with no happy ending. "}}