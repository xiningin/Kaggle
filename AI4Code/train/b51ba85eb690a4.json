{"cell_type":{"1cf6c753":"code","7496f7f3":"code","740562f3":"code","79feedf7":"code","7771bbef":"code","42a9c2f6":"code","f622a36f":"code","4ea61dab":"code","88737439":"code","84c16d3e":"code","0dc02fd8":"code","b86ce3b1":"code","a74db371":"code","dc899a19":"code","c88e71cb":"code","0291122c":"code","deed1a29":"code","13047fa5":"code","c46a6356":"code","56c015f7":"code","75b130fd":"code","471fa8a5":"code","9253335d":"markdown","b1b6a61c":"markdown","70461974":"markdown","77eff271":"markdown","c94f1e08":"markdown","5ef85761":"markdown","59715e3c":"markdown","125c8849":"markdown","24df2706":"markdown","e13b351a":"markdown","f67b0c2e":"markdown","3a8562c2":"markdown","19188b10":"markdown"},"source":{"1cf6c753":"import numpy as np # linear algebra \nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","7496f7f3":"df = pd.read_csv(\"..\/input\/mushrooms.csv\")","740562f3":"df.head(10)   # as we can see whole data structured by letter instead of numbers.","79feedf7":"df.info()   # we can see that they are all objects. Which is Letters in our case","7771bbef":"df.describe()   # as we see, there are some columns with 12 unique rows while some others has only 2 unique rows.","42a9c2f6":"# let's do Label Encoding and do EDA again","f622a36f":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","4ea61dab":"# applying label encoder to whole dataset...\ndf = df.apply(label_encoder.fit_transform)\n\n# checking the result\ndf.head(10)                # which seems great.","88737439":"df.info()  # they are all int64 now and there is no null value which is very good.","84c16d3e":"y = df[\"class\"].values   # our labels.. okay to eat or poison.\n\ndf.drop([\"class\"],axis=1,inplace=True)  # dropping the lables from the data\n\nx_data = df  # our features..","0dc02fd8":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\n\nx.drop([\"veil-color\"],axis=1,inplace=True)\nx.drop([\"veil-type\"],axis=1,inplace=True)","b86ce3b1":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)   # 20% would be enough","a74db371":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","dc899a19":"def init_weight_and_bias(dimension):\n    w = np.full((dimension,1),0.01)          # just creating a dimension sized vector filled with our weight value (0.01)\n    b = 0.0                                  # smallest float value is setted to the bias (0.0)\n   # print(\"w::\",w)\n   # print(\"b::\",b)\n    return w,b;\n\ndef sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))               # implementing the sigmoid function\n   # print(\"y_head::\",y_head)\n    return y_head;\n\ndef forward_and_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b             # first phase of actual Computation of the Logistic Regression:: multiplying the weights with corresponding values then adding bias..\n    y_head = sigmoid(z)                     # second phase:: Applying the sigmoid on the result of first phase to get result between 0 and 1.\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)        # calculating loss and cost is key to optimize since they are the value of fail \n  #  print(\"loss::\",loss)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n  #  print(\"forw_bck::\",cost)\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/ x_train.shape[1]     # applying gradient descent\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    gradients = {\"deriv_weight\":derivative_weight,\"deriv_bias\":derivative_bias}  # putting all in dictionary\n    \n    return cost, gradients;","c88e71cb":"def update(w,b,x_train,y_train,learning_rate,num_of_iter):\n    cost_list = []   # empty arrays to store costs\n    index = []\n    \n    for i in range(num_of_iter):\n        cost, gradients = forward_and_backward_propagation(w,b,x_train,y_train)  # do the training as much as iteration given\n      #  print(\"update:: \",cost)\n        cost_list.append(cost)      # insert the calculated cost on array\n        index.append(i)\n        \n        w = w - learning_rate*gradients[\"deriv_weight\"]     # set new weights and bias for next iteration\n        b = b - learning_rate*gradients[\"deriv_bias\"]\n        \n    parameters = {\"weight\":w,\"bias\":b}    # save all the weights and biases on a dictionary\n    plt.plot(index,cost_list)            # draw the plot to visualize (optional)\n    plt.show()\n    \n    return parameters, gradients, cost_list;\n\n\ndef predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test) + b)           # do the first and second phase of Computation and store in array z\n    y_prediction = np.zeros((1,x_test.shape[1]))  # create empty array to fill by results of z\n   # print(\"y_pred:::\", np.zeros((1,x_test.shape[1])))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0;\n        else:\n            y_prediction[0,i] = 1;\n    \n    return y_prediction;    ","0291122c":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_of_iter):\n    \n    dimension = x_train.shape[0]\n    w,b = init_weight_and_bias(dimension)\n    \n    parameters, gradients, col_list = update(w,b,x_train,y_train,learning_rate,num_of_iter)\n    \n    y_pred_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_pred_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(y_pred_train-y_train))*100))\n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_pred_test-y_test))*100))","deed1a29":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,num_of_iter=250)","13047fa5":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\n\nlr_model.fit(x_train.T,y_train.T)\n\ny_head = lr_model.predict(x_test.T)","c46a6356":"print(\"test accuracy: \", lr_model.score(x_test.T,y_test.T))","56c015f7":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_head)","75b130fd":"import seaborn as sns\n\nplt.figure(figsize=(16,10))\nsns.heatmap(cm,annot=True,fmt='.0f')\nplt.show()","471fa8a5":"# I am currently trying to learn and improve my Machine Learning skills.\n# Your Comments,Advices and Votes are important for me. Best Regards, Efe.","9253335d":"### Pre Preocessing the data\n\n\n\ndividing the labels and the features..","b1b6a61c":"<div id=\"3\"\/>\n## Handmade Logistic Regression","70461974":"To sum up, Sklearn is much more simple, easy to code and more sophisticated based on results. (1% better)\n\nBut in handmade version, we can modify it to have better results freely. (hyperParameter Tuning)","77eff271":"<div id=\"1\"\/>\n## EDA","c94f1e08":"normalization","5ef85761":"Now, I can easily implement my Logistic Regression on the dataset and see the results..","59715e3c":"<div id=\"4\"\/>\n## SciKit-Learn Logistic Regression","125c8849":"\n## Creating our Logistic Regression","24df2706":"splitting test and train by sklearn","e13b351a":"<div id=\"2\"\/>\n## Label Encoding","f67b0c2e":"<div id=\"5\"\/>\n# Comparison and Conclusion","3a8562c2":"# - Introduction -\n\n## Hello, in this kernel you'll find:\n*  <a href=\"#1\"> EDA <\/a>\n*  <a href=\"#2\"> Label Encoding  (sklearn) <\/a>\n*  <a href=\"#3\"> Handmade Logistic Regression <\/a>  \n*  <a href=\"#4\"> SciKit-Learn Logistic Regression <\/a>\n*  <a href=\"#5\"> Comparison of the Results and Conclusion <\/a>","19188b10":"transpose of matrixes"}}