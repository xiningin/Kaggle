{"cell_type":{"3d1dd5b4":"code","cb371751":"code","6e80cc29":"code","e97e403a":"code","25233ce5":"code","7bf8be1c":"code","ad4a30d6":"code","8c990d26":"code","12863c0d":"code","e5e0a198":"code","5413397e":"code","07790328":"code","fe37eda6":"code","c4ce47d2":"code","08ef54c8":"code","ba68b60e":"code","889afea6":"code","583eb7b6":"code","a0647ee2":"code","d6f01a9b":"code","0ef87132":"code","09bc96e8":"code","c7ae7ddf":"code","0e095f98":"code","a92956b1":"code","6facd9ce":"code","28dca0e9":"code","92acc96e":"code","cc972bef":"code","4e81d537":"code","c943c3d7":"code","6b0dc030":"code","de74e6cb":"code","1802fd8d":"code","37e29670":"markdown","d3c1b64f":"markdown","050a8200":"markdown","6882efbc":"markdown","8d1ba10a":"markdown"},"source":{"3d1dd5b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb371751":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier","6e80cc29":"X = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nX.dropna(axis=0,subset=['Survived'],inplace=True)\ny = X.Survived","e97e403a":"X_copy=X.copy() ","25233ce5":"X_copy.drop(X_copy.index[2])","7bf8be1c":"num_col = [col for col in X.columns if X[col].dtypes != 'object']\nprint(num_col)\nnum_col = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']","ad4a30d6":"import seaborn as sns\nplt.figure(figsize=(10,20))\nsns.lmplot(x='Fare', y='Age', hue='Survived', data=X)","8c990d26":"#plot the countplots\nX_new = X.copy()\nX_new.Fare = X_new.Fare\/3\n#define the figure and subplotts\nfig,axs = plt.subplots(nrows = 3, ncols = 2, figsize=(20,20))\nfor i, feature in enumerate(num_col,1):\n    plt.subplot(3,2,i)\n    sns.countplot(x=feature, hue = 'Survived', data=X)\n    plt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size':20})\nplt.subplot(3,2,6)\nsns.countplot(x='Fare', hue = 'Survived', data=X_new)","12863c0d":"total_data = X.append(X_test)\ntotal_data.describe()","e5e0a198":"missing_cols = [cols for cols in total_data.columns if total_data[cols].isna().any()]\nprint(missing_cols)","5413397e":"print(total_data['Age'].isna().sum())\nprint(total_data['Cabin'].isna().sum())\nprint(total_data['Embarked'].isna().sum())\nprint(total_data['Fare'].isna().sum())","07790328":"#Predicting the unknown values of Age\n#total_data.groupby('Embarked')['Survived'].count()\ntotal_data.loc[total_data['Embarked'].isnull() == True, 'Embarked'] = 'S'\ntotal_data.loc[total_data['Fare'].isnull()==True, 'Fare'] = 14.5\n#Feature engineering for age\ntotal_data['family_size'] = (total_data['SibSp']+total_data['Parch']+1).astype(int)\ntotal_data['Is_alone'] = 1\ntotal_data['Is_alone'].loc[total_data['family_size']>1] = 0\n\ntotal_data['title'] = total_data['Name'].str.extract('([A-Za-z]+)\\.',expand = False)\ntotal_data['title'] = total_data['title'].replace('Mlle','Miss') \ntotal_data['title'] = total_data['title'].replace('Ms','Mrs') \ntotal_data['title'] = total_data['title'].replace('Mme','Mrs')\ntotal_data['title'] = total_data['title'].replace(['Mlle', 'Master','Dr','Rev','Col','Major','Dona',\n                                                'Capt','Lady','Jonkheer','Countess','Don','Sir'],'Rare') \nx=1\ntotal_data['title_num'] = 0\nfor unique in total_data['title'].unique().tolist():\n    total_data.loc[total_data['title'] == unique,'title_num'] = x\n    x+=1\ntotal_data.drop('title',axis=1,inplace=True)","fe37eda6":"#Preprocess the AGE total data\ntotal_age_data_known = total_data.loc[total_data['Age'].isnull() == False]\ntotal_age_data_unknown = total_data.loc[total_data['Age'].isnull() == True]\ntotal_age_data_unknown.drop('Age',axis=1, inplace=True)","c4ce47d2":"features = ['Pclass', 'Sex','SibSp', 'Parch', 'Fare','family_size', 'title_num']\nage_train, age_valid = train_test_split(total_age_data_known, train_size=0.6, random_state=0)\nage_train_y = age_train['Age']\nage_valid_y = age_valid['Age']\ntotal_age_known_y = total_age_data_known['Age']\nage_train = pd.get_dummies(age_train[features])\nage_valid = pd.get_dummies(age_valid[features])\ntotal_age_known = pd.get_dummies(total_age_data_known[features])\ntotal_age_unknown = pd.get_dummies(total_age_data_unknown[features])\n#total_age_unknown.drop('Age', axis=1, inplace = True)\n\ntotal_age_unknown.columns","08ef54c8":"xg_model_age = XGBRegressor(n_estimators=350, learning_rate=0.01, max_depth=4, subsample=None,\n                        colsample_bytree=0.8, gamma=10,reg_alpha = 1)\nxg_model_age.fit(age_train, age_train_y, early_stopping_rounds=5, eval_set=[(age_valid,age_valid_y)],\n             verbose=False)\npredict=np.round(xg_model_age.predict(age_valid))\nscore = metrics.mean_absolute_error(age_valid_y,predict)\nprint(score)\nscores = -1*cross_val_score(xg_model_age, total_age_known, total_age_known_y, \n                         cv = 5, scoring ='neg_mean_absolute_error')\nprint(scores)\nprint(sum(scores)\/5)\nplt.figure(figsize=(10,5)) \nxgb.plot_importance(xg_model_age, ax=plt.gca())","ba68b60e":"total_age_data_known.columns","889afea6":"#Train on whole data and get values for unknown ages\nxg_model_age.fit(total_age_known, total_age_known_y, early_stopping_rounds=5, \n             eval_set=[(age_valid,age_valid_y)], verbose=False)\npredict=np.round(xg_model_age.predict(total_age_unknown))\n#Create a new dataframe with all the features\noutput = pd.DataFrame({'PassengerId': total_age_data_unknown.PassengerId, 'Survived': total_age_data_unknown.Survived,\n                       'Pclass': total_age_data_unknown.Pclass, 'Name': total_age_data_unknown.Name, \n                       'Sex': total_age_data_unknown.Sex, 'Age': predict, 'SibSp': total_age_data_unknown.SibSp,\n                       'Parch': total_age_data_unknown.Parch, 'Ticket': total_age_data_unknown.Ticket,\n                       'Fare': total_age_data_unknown.Fare.astype(float), 'Cabin': total_age_data_unknown.Cabin,\n                       'Embarked': total_age_data_unknown.Embarked, 'family_size': total_age_data_unknown.family_size,\n                      'title_num': total_age_data_unknown.title_num, 'Is_alone': total_age_data_unknown.Is_alone})\nfull_data = total_age_data_known.append(output)","583eb7b6":"full_data.describe()","a0647ee2":"#Feature engineering on full data\n#full_data['Fare'] = pd.qcut(full_data['Fare'], 4, labels=False)\n#full_data['Age'] = pd.qcut(full_data['Age'], 10)\nfull_data['Ticket_Frequency'] = full_data.groupby('Ticket')['Ticket'].transform('count')\nfull_data['fare_ticket'] = full_data['Fare']\/full_data['Ticket_Frequency']\nfull_data['known_cabin'] = 0\nfull_data.loc[full_data['Cabin'].isnull() == False,'known_cabin']=1\nfull_data.loc[full_data['Cabin'].isnull() == True,'known_cabin']=0\n#full_data.groupby('known_cabin')['Survived'].count()\nfull_data['Age_cat']=0\nfull_data.loc[full_data['Age'] < 3, 'Age_cat'] = 0\nfull_data.loc[full_data['Age'] > 3, 'Age_cat'] = 1\nfull_data.loc[full_data['Age'] > 14, 'Age_cat'] = 2\nfull_data.loc[full_data['Age'] > 24, 'Age_cat'] = 3\nfull_data.loc[full_data['Age'] > 34, 'Age_cat'] = 4\nfull_data.loc[full_data['Age'] > 44, 'Age_cat'] = 5\nfull_data.loc[full_data['Age'] > 54, 'Age_cat'] = 6\nfull_data.loc[full_data['Age'] > 64, 'Age_cat'] = 7\n#Create a seperate feature for family size\n#full_data['family_size']=full_data['SibSp']+full_data['Parch']+1\n#family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n#full_data['Family_Size_Grouped'] = full_data['family_size'].map(family_map)\n","d6f01a9b":"full_data.columns\n","0ef87132":"sns.countplot(x='fare_ticket', hue='Survived', data=full_data)","09bc96e8":"full_data['class_fare']=0\nfull_data['class_fare'] = full_data['Fare']\/full_data['Pclass']\nprint(full_data['class_fare'].describe())","c7ae7ddf":"#full_data['Age_Fare'] = full_data.loc[full_data['Age']<6, ['Age_Fare']] = 1\nfull_data['Age_Fare'] = 1\nfull_data.loc[full_data['Fare']<60 , ['Age_Fare']] = 0\nfull_data.loc[full_data['Age']>65, ['Age_Fare']] = 0\nfull_data.loc[full_data['Age']<6, ['Age_Fare']] = 1\n#full_data['Age_Fare'] = full_data.loc[full_data['Age_Fare'].isnull()==True , ['Age_Fare']] = 1\n#full_data['Age_Fare'].sum()","0e095f98":"#Split full data into train and test sets again\nX_new = full_data.loc[full_data['Survived'].isna()==False]\nX_test_new = full_data.loc[full_data['Survived'].isna()==True]\nprint('Length of training data::',len(X_new),'\\n' 'Length of test data::',len(X_test_new))\n#X_new.groupby('Age_cat')['Survived'].count()\nX_new_copy = X_new.copy()\nX_test_new_copy=X_test_new.copy()","a92956b1":"#Drop Survived \ny_new_copy = X_new_copy.Survived\n#X_new_copy.drop(['Survived'], axis =1, inplace=True)\nmissing_cols = [cols for cols in X_new_copy.columns if X_new_copy[cols].isna().any()]\nprint(missing_cols) #Check for missing cols","6facd9ce":"X_new_copy.columns","28dca0e9":"#Target encoding for SEX feature\nimport category_encoders as ce\ncat_features = ['Sex', 'Embarked']\ntarget_enc = ce.TargetEncoder(cols=cat_features)\ntarget_enc.fit(X_new_copy[cat_features], y_new_copy)\nX_new_copy=X_new_copy.join(target_enc.transform(X_new_copy[cat_features]).add_suffix('_target'))\nX_test_new_copy=X_test_new_copy.join(target_enc.transform(X_test_new_copy[cat_features]).add_suffix('_target'))\nX_test_new_copy.columns","92acc96e":"#num_cols = [col for col in X_new_copy.columns if X_new_copy[col].dtypes in ['int64','float64']]\n#num_cols = ['Pclass', 'Fare', 'known_cabin', 'Age_cat','family_size','title_num', 'SibSp', 'Parch',\n#            'Ticket_Frequency', 'Age']\nnum_cols = ['Pclass','known_cabin', 'Age_cat','family_size','title_num', 'Sex_target',\n            'Ticket_Frequency', 'Embarked_target', 'fare_ticket']\nprint(num_cols)\nlow_card_cat_columns = [col for col in X_new_copy.columns if (X_new_copy[col].dtypes == 'object' and X_new_copy[col].nunique()<10)]\nlow_card_cat_columns = []\nprint(low_card_cat_columns)","cc972bef":"#Split data into train-test\nX_train,X_valid,y_train,y_valid = train_test_split(X_new_copy,y_new_copy,train_size=0.6, test_size=0.4, random_state=3)\n\n#OH_X_train = pd.get_dummies(X_train[low_card_cat_columns])\n#OH_X_valid = pd.get_dummies(X_valid[low_card_cat_columns])\n#OH_X_test = pd.get_dummies(X_test_new_copy[low_card_cat_columns])\n#OH_X_full = pd.get_dummies(X_new_copy[low_card_cat_columns])\n\nX_train_num = X_train[num_cols]\nX_test_num = X_test_new_copy[num_cols]\nX_valid_num = X_valid[num_cols]\nX_full_num = X_new_copy[num_cols]\n\n#X_train_num = pd.concat([X_train_num,OH_X_train], axis=1)\n#X_test_num = pd.concat([X_test_num,OH_X_test], axis=1)\n#X_valid_num = pd.concat([X_valid_num,OH_X_valid], axis=1)\n#X_full_num = pd.concat([X_full_num,OH_X_full], axis=1)\nX_full_num.describe()","4e81d537":"#RandomForest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=8, \n                                  min_samples_split=6, min_samples_leaf=5, max_leaf_nodes=None,\n                                  oob_score=True, max_samples=None)\nrf_model.fit(X_train_num, y_train)\npredict= rf_model.predict(X_valid_num)\nscore = metrics.roc_auc_score(y_valid,predict)\nprint(score)\n\n#Fine-tune the hyper-parameters using Cross-Validation\nscores = cross_val_score(rf_model, X_full_num, y_new_copy, cv = 5, scoring ='roc_auc')\nprint(scores)\nprint(sum(scores)\/5)","c943c3d7":"#XGB MODEL\nxg_model = XGBRegressor(n_estimators=350, learning_rate=0.01, max_depth=4, subsample=0.8,\n                       colsample_bytree=0.8,gamma=1, reg_lamda = 10)\nxg_model.fit(X_train_num, y_train, early_stopping_rounds=10, eval_set=[(X_valid_num,y_valid)], verbose=False)\n#xg_model = GradientBoostingClassifier(n_estimators=200, max_depth=3, learning_rate=0.05)\n#xg_model.fit(X_train_num, y_train)\npredict=np.round(xg_model.predict(X_valid_num))\n#predict=xg_model.predict(X_valid_num)\nscore = metrics.roc_auc_score(y_valid,predict)\nprint(score)\nplt.figure(figsize=(10,5)) \nxgb.plot_importance(xg_model, ax=plt.gca())\n#Fine-tune the hyper-parameters using Cross-Validation\nscores = cross_val_score(xg_model, X_full_num, y_new_copy, cv = 5, scoring ='roc_auc')\nprint(scores)\nprint(sum(scores)\/5)","6b0dc030":"metrics.SCORERS.keys()","de74e6cb":"#Fitting the model with full training data\nxg_model.fit(X_full_num, y_new_copy, early_stopping_rounds=10, eval_set=[(X_valid_num,y_valid)], verbose=False)\n#xg_model.fit(X_full_num, y_new_copy)\ntest_preds=np.round(xg_model.predict(X_test_num))\n#rf_model.fit(X_full_num, y_new_copy)\n#test_preds=rf_model.predict(X_test_num)\ntest_preds = test_preds.astype(int)","1802fd8d":"output = pd.DataFrame({'PassengerId': X_test_new_copy.PassengerId,\n                       'Survived': test_preds})\noutput.to_csv('submission.csv', index=False)","37e29670":"Very crude way to check for overfit: While tuning the above parameters try changing the random state in train_test_split and if there is drastic change in mean_absolute_error then probably its an overfit.","d3c1b64f":"The survived and not-survived are loosely distributed among all the values in the respective features. Thus, I thought of using the regression model instead of classification model of XGBoost. And, yes it performed better. Here, I rounded the probability of survival predicted by XGBoost Regressor to 0\/1.\nNote: If there is a distinct segmentation among data then classification models will work best.","050a8200":"Extracting the titles from 'Names'. This will help to determine VIP people whom will have higher probability of survival.","6882efbc":"I am a beginer in ML here and hence pardon me for the of noob level programming. Here in addition to creating new features I have concentrated on hyperparameter tuning of XGB Regressor. Please let me know if you have any suggestions to improve the score further.","8d1ba10a":"As 'Cabin' lots of NaN values it is reasonable to drop it. \n'Age' contains limited number of NaN values but it seems to be too much for imputing them with mean. Insted, I used other features predict the values of 'Age'. \nAlso I replaced unknown values in 'Embarked' and 'Fare' with the median(50%ile) values."}}