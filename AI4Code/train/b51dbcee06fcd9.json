{"cell_type":{"52b2c4bb":"code","debc642b":"code","8d6e50fa":"code","8bb29adf":"code","c5005bcd":"code","9f9da6ee":"code","3e6bcf36":"code","d46201fe":"code","79fa659f":"code","1186269f":"code","d1dba253":"code","142ded27":"code","3797d4e3":"code","a6f86de9":"code","624756ce":"code","7a389c89":"code","043e3f45":"code","92c77a4a":"code","36e69840":"code","72a4c223":"code","90295d67":"code","5609edbb":"code","c86b87ee":"code","a99c7f55":"code","3b454a50":"code","e9a1ad3d":"code","5c6c8162":"code","42785344":"code","2b49c9fc":"code","3e7721ea":"code","fbecdf5d":"code","3531fbc1":"code","0de5909c":"code","6dff3ee8":"code","8da44d89":"code","a69b191b":"code","af3787e0":"code","526e982d":"code","c032bc28":"code","5dfa1f6b":"code","2f116ec7":"code","83f8ed26":"code","c06d2939":"markdown","e7502b04":"markdown","65682cbc":"markdown","a29d8478":"markdown","0c9ba5f7":"markdown","77789bb9":"markdown","cca07c97":"markdown","d805609c":"markdown","2a6963d7":"markdown","2af5714b":"markdown","48630909":"markdown","dde10ff1":"markdown","abb27c30":"markdown","e4753835":"markdown","1be710ee":"markdown","3369516b":"markdown","c386ef76":"markdown"},"source":{"52b2c4bb":"import umap\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom numpy.random import seed\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, Conv2D, MaxPooling2D, Flatten, GaussianNoise, Reshape\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomRotation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.random import set_seed\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD, NMF, KernelPCA\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","debc642b":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf","8d6e50fa":"X = df.drop('label', axis = 1).values\nX.shape","8bb29adf":"X = X.reshape((42000, 28 * 28))\ny = df['label']\n\nX_train_sc, X_test_sc, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n\nX_train_sc = X_train_sc.astype('float32') \/ 255\nX_test_sc = X_test_sc.astype('float32') \/ 255\n\ny_cat_test = to_categorical(y_test,10)\ny_cat_train = to_categorical(y_train,10)","c5005bcd":"plt.imshow(X_train_sc[0].reshape(28, 28, 1))","9f9da6ee":"pca = PCA(n_components=2, random_state = 1)\ndf_pca = pca.fit_transform(X_train_sc)","3e6bcf36":"df_vis = pd.DataFrame(df_pca)\ndf_vis['y'] = y_train.values\n\nplt.figure(figsize = (14, 8))\nsns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","d46201fe":"svd = TruncatedSVD(n_components=2, random_state = 1)\ndf_svd = svd.fit_transform(X_train_sc)","79fa659f":"df_vis = pd.DataFrame(df_svd)\ndf_vis['y'] = y_train.values\n\nplt.figure(figsize = (14, 8))\nsns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","1186269f":"nmf = NMF(n_components=2, random_state = 1)\ndf_nmf = nmf.fit_transform(X_train_sc)","d1dba253":"df_vis = pd.DataFrame(df_nmf)\ndf_vis['y'] = y_train.values\n\nplt.figure(figsize = (14, 8))\nsns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","142ded27":"rbm = BernoulliRBM(n_components=2, random_state = 1)\ndf_rbm = rbm.fit_transform(X_train_sc)","3797d4e3":"df_vis = pd.DataFrame(df_rbm)\ndf_vis['y'] = y_train.values\n\nplt.figure(figsize = (14, 8))\nsns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","a6f86de9":"tsne = TSNE(\n    n_components=2, \n    random_state = 1,\n    n_iter = 1000,\n    n_jobs = -1\n)\ndf_tsne = tsne.fit_transform(X_train_sc)","624756ce":"df_vis = pd.DataFrame(df_tsne)\ndf_vis['y'] = y_train.values\n\nplt.figure(figsize = (14, 8))\nsns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","7a389c89":"umap_m = umap.UMAP(\n    n_components=2, \n    random_state=1, \n    n_neighbors=5\n)\n\ndf_umap = umap_m.fit_transform(X_train_sc)","043e3f45":"df_vis = pd.DataFrame(df_umap)\ndf_vis['y'] = y_train.values\n\nplt.figure(figsize = (14, 8))\nsns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","92c77a4a":"input_width = X_train_sc.shape[1]\ninput_width","36e69840":"def dim_red_analysis(n_epochs = None):\n    seed(101)\n    set_seed(101)\n\n    encoder = Sequential()\n    encoder.add(Dense(units = 256, activation = 'relu', input_shape = [input_width]))\n    encoder.add(Dropout(0.2))\n    encoder.add(Dense(units = 16, activation = 'relu'))\n    encoder.add(Dense(units = 2, activation = 'relu'))\n\n    decoder = Sequential()\n    decoder.add(Dense(units = 16, activation = 'relu', input_shape = [2]))\n    decoder.add(Dense(units = 256, activation = 'relu'))\n    decoder.add(Dense(units = input_width, activation = 'relu'))\n\n    autoencoder = Sequential([encoder, decoder])\n\n    autoencoder.compile(loss = 'mse', optimizer = SGD(lr = 12))\n    \n    autoencoder.summary()\n    \n    if n_epochs is None:\n        es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n        n_epochs = 100\n    else:\n        es = []\n    \n    autoencoder.fit(\n            X_train_sc,\n            X_train_sc,\n            epochs = n_epochs,\n            validation_data=(X_test_sc, X_test_sc), \n            callbacks=[es]\n             )\n    \n    if n_epochs > 1:\n        histo = pd.DataFrame(autoencoder.history.history)\n        for metric in ['loss', 'val_loss']:\n            plt.title(metric)\n            histo[metric].plot()\n            plt.show()\n        \n    encoded_2dim = encoder.predict(X_train_sc)\n    encoded_2dim = pd.DataFrame(encoded_2dim)\n    encoded_2dim['y'] = y_train.values\n\n    plt.figure(figsize = (12, 8))\n    sns.scatterplot(data = encoded_2dim, x = 0, y = 1, hue = 'y', palette = 'magma')\n    plt.show()","72a4c223":"dim_red_analysis(0)","90295d67":"dim_red_analysis()","5609edbb":"dim_red_analysis(30)","c86b87ee":"def eval_result(model, X_test, y_test, histo = None):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        if hasattr(model, 'predict_classes'):\n            pred = model.predict_classes(X_test)\n        else:\n            pred = model.predict(X_test)\n        \n        if histo is not None:\n            plt.title('Loss')\n            histo['loss'].plot()\n            plt.show()\n            \n            plt.title('Val loss')\n            histo['val_loss'].plot()\n            plt.show()\n            \n        print(classification_report(y_test, pred))\n        display(pd.DataFrame(confusion_matrix(y_test, pred)))\n        print(f'Accuracy: {round(accuracy_score(y_test, pred), 5)}')","a99c7f55":"lgmodel = LogisticRegression(\n    solver='lbfgs', \n    n_jobs = -1,\n    random_state = 101\n)\nlgmodel.fit(X_train_sc, y_train)\neval_result(lgmodel, X_test_sc, y_test)","3b454a50":"dtc = DecisionTreeClassifier(random_state = 101)\ndtc.fit(X_train_sc, y_train)\neval_result(dtc, X_test_sc, y_test)","e9a1ad3d":"rfc = RandomForestClassifier(random_state = 101, n_jobs = -1)\nrfc.fit(X_train_sc, y_train)\neval_result(rfc, X_test_sc, y_test)","5c6c8162":"xgbr = XGBClassifier(\n    random_state = 1,\n    n_jobs = -1,\n    eval_metric = 'logloss',\n    use_label_encoder = False\n)\nxgbr.fit(X_train_sc, y_train)\neval_result(xgbr, X_test_sc, y_test)","42785344":"seed(101)\nset_seed(101)\n\nmodel = Sequential()\nmodel.add(Dense(128, activation = 'relu', input_shape=(28 * 28,)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=10,activation='softmax'))\n\nes = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics = ['acc']\n)\n\nmodel.fit(\n    x = X_train_sc,\n    y = y_cat_train,\n    epochs = 30,\n    callbacks=[es],\n    validation_data=(X_test_sc, y_cat_test)\n)\n\nhisto = pd.DataFrame(model.history.history)\n\neval_result(model, X_test_sc, y_test.values, histo)","2b49c9fc":"X_test_sc_r = X_test_sc.reshape(12600, 28,  28, 1)\nX_train_sc_r = X_train_sc.reshape(29400, 28,  28, 1)","3e7721ea":"seed(101)\nset_seed(101)\n\nmodel_conv = Sequential()\nmodel_conv.add(Conv2D(32, (5, 5), activation='relu',\n input_shape=(28, 28, 1)))\nmodel_conv.add(MaxPooling2D((3, 3)))\n\nmodel_conv.add(Conv2D(64, (3, 3), activation='relu'))\n\nmodel_conv.add(Flatten())\nmodel_conv.add(Dense(64, activation='relu'))\nmodel_conv.add(Dense(10, activation='softmax'))\n\n\nes = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n\nmodel_conv.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics = ['acc']\n)\n\nmodel_conv.fit(\n    x = X_train_sc_r,\n    y = y_cat_train,\n    epochs = 5,\n    callbacks=[es],\n    validation_data=(X_test_sc_r, y_cat_test)\n)\n\nhisto = pd.DataFrame(model_conv.history.history)\n\neval_result(model_conv, X_test_sc_r, y_test.values, histo)","fbecdf5d":"data_augmentation = Sequential([\n  RandomRotation([-0.1, 0.1], seed = 1),\n])\n\nimage = tf.expand_dims(X_train_sc_r[0], 0)\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    augmented_image = data_augmentation(image)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_image[0])\n    plt.axis(\"off\")","3531fbc1":"seed(101)\nset_seed(101)\n\nmodel_conv_aug = Sequential()\nmodel_conv_aug.add(data_augmentation)\nmodel_conv_aug.add(Conv2D(32, (5, 5), activation='relu',\n input_shape=(28, 28, 1)))\nmodel_conv_aug.add(MaxPooling2D((3, 3)))\n\nmodel_conv_aug.add(Conv2D(64, (3, 3), activation='relu'))\n\nmodel_conv_aug.add(Flatten())\nmodel_conv_aug.add(Dense(64, activation='relu'))\nmodel_conv_aug.add(Dense(10, activation='softmax'))\n\n\nes = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n\nmodel_conv_aug.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics = ['acc']\n)\n\nmodel_conv_aug.fit(\n    x = X_train_sc_r,\n    y = y_cat_train,\n    epochs = 13,\n  #  callbacks=[es],\n    validation_data=(X_test_sc_r, y_cat_test)\n)\n\nhisto = pd.DataFrame(model_conv_aug.history.history)\n\neval_result(model_conv_aug, X_test_sc_r, y_test.values, histo)","0de5909c":"X_train_sc_r.shape","6dff3ee8":"set_seed(101)\nseed(101)\n\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28, 1]))\nencoder.add(GaussianNoise(0.2))\nencoder.add(Dense(128,activation=\"relu\"))\nencoder.add(Dense(32,activation=\"relu\"))\n\ndecoder = Sequential()\ndecoder.add(Dense(128,input_shape=[32],activation='relu'))\ndecoder.add(Dense(28 * 28, activation=\"sigmoid\"))\ndecoder.add(Reshape([28, 28, 1]))\n\n\nnoise_remover = Sequential([encoder, decoder])\n\n\n\nes = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n\n        \nnoise_remover.compile(\n    loss=\"binary_crossentropy\", \n    metrics=['accuracy'],\n    optimizer = SGD(lr = 12)\n)\n\nnoise_remover.fit(\n    X_train_sc_r, \n    X_train_sc_r, \n    epochs = 50,\n    validation_data=(X_test_sc_r, X_test_sc_r),\n    callbacks=[es]\n)","8da44d89":"histo = pd.DataFrame(noise_remover.history.history)\nfor metric in ['loss', 'val_loss']:\n    plt.title(metric)\n    histo[metric].plot()\n    plt.show()","a69b191b":"noisey_images = GaussianNoise(0.2)(X_train_sc_r[0:10],training=True)\ndenoised = noise_remover(noisey_images[0:10])","af3787e0":"n = 7\nplt.title(\"The Original:\")\nplt.imshow(X_train_sc_r[n])\nplt.show()\n\nplt.title(\"The Noisey Version:\")\nplt.imshow(noisey_images[n])\nplt.show()\n\nplt.title(\"After going through denoiser:\")\nplt.imshow(denoised[n])\nplt.show()","526e982d":"class Denoiser_classifier:\n    \n    def __init__(self, denoiser, model):\n        self.denoiser = denoiser\n        self.model = model\n\n    \n    def fit(self, x, y, epochs, callbacks, validation_data):\n        X_denoised = self.denoiser(x)\n        self.model.fit(\n            x = X_denoised,\n            y = y,\n            epochs = epochs,\n            callbacks=callbacks,\n            validation_data=validation_data\n        )\n        \n    def predict(self, X):\n        X_denoised = self.denoiser(X)\n        return self.model.predict_classes(X_denoised)\n    \n    def history(self):\n        return pd.DataFrame(self.model.history.history)\n        ","c032bc28":"seed(101)\nset_seed(101)\n\nmodel_conv2 = Sequential()\nmodel_conv2.add(Conv2D(32, (5, 5), activation='relu',\n input_shape=(28, 28, 1)))\nmodel_conv2.add(MaxPooling2D((3, 3)))\n\nmodel_conv2.add(Conv2D(64, (3, 3), activation='relu'))\n\nmodel_conv2.add(Flatten())\nmodel_conv2.add(Dense(64, activation='relu'))\nmodel_conv2.add(Dense(10, activation='softmax'))\n\n\nes = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n\nmodel_conv2.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics = ['acc']\n)\n\n\nmodel_denoised = Denoiser_classifier(noise_remover, model_conv2)\n\n\nmodel_denoised.fit(\n    x = X_train_sc_r,\n    y = y_cat_train,\n    epochs = 4,\n    callbacks=[es],\n    validation_data=(X_test_sc_r, y_cat_test)\n)\n\nhisto = model_denoised.history()\n\neval_result(model_denoised, X_test_sc_r, y_test.values, histo)","5dfa1f6b":"validation = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nvalidation = validation \/ 255","2f116ec7":"predictions = pd.Series(model_conv_aug.predict_classes(validation.values.reshape(28000, 28, 28, 1)))\npredictions","83f8ed26":"answer = pd.DataFrame(data = {'ImageId': list(range(1,28001)), 'Label':predictions})\nanswer.to_csv('digits_submission v2.csv', index=False)\nanswer","c06d2939":"#### Model itself","e7502b04":"#### Autoencoder","65682cbc":"#### Testing noise supression","a29d8478":"#### Singular Value Decomposition","0c9ba5f7":"# Commit","77789bb9":"Let's try using autoencoder for noise suppression to improve convolutional model","cca07c97":"#### Restricted Boltzmann Machine","d805609c":"# Modeling","2a6963d7":"# Data augmentation","2af5714b":"Model without this particular noise suppressor is better, so i'm going to use just convolutional model itself.","48630909":"We can see that classes are clearly separable","dde10ff1":"#### PCA","abb27c30":"#### Non-Negative Matrix Factorization (NMF)","e4753835":"# Dimensionality reduction","1be710ee":"#### t-distributed Stochastic Neighbor Embedding","3369516b":"#### UMAP (Uniform Manifold Approximation and Projection) ","c386ef76":"# Noise suppression"}}