{"cell_type":{"419e0a3e":"code","45ec7e1e":"code","0f9e5b84":"code","82f02ba5":"code","67d5aa72":"code","df2571a0":"code","02cb37d7":"code","f168538e":"code","7077f520":"code","d5e11df7":"code","f1bf8a27":"code","a778d2f7":"code","e22c211b":"code","994d64af":"code","c7f01d7d":"code","16b72859":"code","41b9838a":"code","109a1e82":"code","6cf16041":"code","efc462f4":"code","c7b4b008":"code","ca7594d7":"code","efd372b3":"code","b195b313":"code","e848d2d7":"code","9bd77045":"code","bf7d5138":"code","5287d49c":"markdown","88c7bf8b":"markdown","f9d8ee95":"markdown","17ed4939":"markdown","84023ff6":"markdown","034ca175":"markdown","31bc8aec":"markdown","af89ea52":"markdown","6b2e4ff6":"markdown","003c7c9e":"markdown","d28a6f6e":"markdown","e54bd849":"markdown","53931956":"markdown","29a8800d":"markdown","c413be30":"markdown","a556ee72":"markdown","b2915259":"markdown","bdd1093f":"markdown","ff2ac4a6":"markdown","81b75a8b":"markdown","159def73":"markdown","f056bd0e":"markdown","ffab99f9":"markdown","0974d3d9":"markdown","cd7a4eec":"markdown","e268b476":"markdown","a2c5bb6e":"markdown","a9c9a799":"markdown","7cca9909":"markdown","19ee9414":"markdown","de250473":"markdown","9c6b55a7":"markdown","19664e51":"markdown","337d38d6":"markdown","eee2d1f9":"markdown","1b8f2be1":"markdown","32dc3f93":"markdown","387448c7":"markdown","835b2639":"markdown","e499859b":"markdown","2b3ef4ae":"markdown","640a73de":"markdown","9ddbb383":"markdown","91bf1da6":"markdown","841684c8":"markdown","ee9513f4":"markdown","d72326ac":"markdown","b80c5d03":"markdown","e55b7280":"markdown","a58d3254":"markdown","1a990ba2":"markdown","5610494b":"markdown","5ff5b7fe":"markdown","ab24a41e":"markdown","d9538b09":"markdown","aa95d73c":"markdown","268a8014":"markdown"},"source":{"419e0a3e":"import os, pandas as pd, seaborn as sns, matplotlib.pyplot as plt, pydicom, numpy as np\nimport multiprocessing, warnings\nimport matplotlib.patches as patches\n\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nfrom sklearn.mixture import GaussianMixture\nfrom skimage import feature\nfrom skimage import morphology\nfrom skimage import measure\nfrom skimage import util\nfrom skimage import transform\n\nwarnings.filterwarnings('ignore')","45ec7e1e":"os.listdir(\"..\/input\")","0f9e5b84":"stage_1_detailed_class_info = pd.read_csv('..\/input\/stage_1_detailed_class_info.csv')\nprint(stage_1_detailed_class_info.iloc[2:5])\nprint(\"Rows:\", stage_1_detailed_class_info.shape[0])\nprint(\"Columns:\", stage_1_detailed_class_info.shape[1])","82f02ba5":"print(\"# of unique patient IDs: \", stage_1_detailed_class_info['patientId'].nunique())","67d5aa72":"stage_1_detailed_class_info['patientId'].value_counts().value_counts()","df2571a0":"stage_1_sample_submission = pd.read_csv('..\/input\/stage_1_sample_submission.csv')\nprint(stage_1_sample_submission.iloc[:3])\nprint(\"Rows:\", stage_1_sample_submission.shape[0])\nprint(\"Columns:\", stage_1_sample_submission.shape[1])","02cb37d7":"stage_1_train_labels = pd.read_csv('..\/input\/stage_1_train_labels.csv')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\nprint(stage_1_train_labels.iloc[2:5])\nprint(\"Rows:\", stage_1_train_labels.shape[0])\nprint(\"Columns:\", stage_1_train_labels.shape[1])\nprint(\"# of unique patient IDs: \", len(list(stage_1_train_labels.patientId.unique())))","f168538e":"patientId = stage_1_train_labels['patientId'][0]\ndicom_file = '..\/input\/stage_1_train_images\/%s.dcm' % patientId\ndicom_data = pydicom.read_file(dicom_file)\n\ndicom_data","7077f520":"stage_1_train_labels['aspect_ratio'] = (stage_1_train_labels['width'] \/ \n                                        stage_1_train_labels['height'])\nstage_1_train_labels['area'] = stage_1_train_labels['width'] * stage_1_train_labels['height']\n\ndef get_info(patientId, root_dir='..\/input\/stage_1_train_images\/'):\n    file_name = os.path.join(root_dir, f'{patientId}.dcm')\n    dicom_data = pydicom.read_file(file_name)\n    return {'age': dicom_data.PatientAge, \n            'gender': dicom_data.PatientSex,\n            'view_position': dicom_data.ViewPosition,\n            'id': os.path.basename(file_name).split('.')[0],\n            'pixel_spacing': float(dicom_data.PixelSpacing[0]),\n            'mean_black_pixels': np.mean(dicom_data.pixel_array == 0)}\n\npatient_ids = list(stage_1_train_labels.patientId.unique())\nwith multiprocessing.Pool(4) as pool:\n    result = pool.map(get_info, patient_ids)\n    \ndemo = pd.DataFrame(result)\ndemo['age'] = demo['age'].astype(int)\ndemo['gender'] = demo['gender'].astype('category')\ndemo['view_position'] = demo['view_position'].astype('category')\n\nstage_1_train_labels = (stage_1_train_labels.merge(demo, left_on='patientId', \n                                                   right_on='id', how='left')\n                        .drop(columns='id'))","d5e11df7":"stage_1_train_labels[2:5]","f1bf8a27":"sns.set_style('darkgrid')\nsns.set_context('notebook', font_scale=1.4)\n\nplt.rcParams['figure.figsize'] = [12, 3]\nplt.rcParams['lines.linewidth'] = 1\n\nboxes_per_patient = stage_1_train_labels.groupby('patientId')['Target'].sum()\n\nax = (boxes_per_patient > 0).value_counts().plot.barh(color=['teal','orange'])\n_ = ax.set_title('Pneumonia opacity present')\n_ = ax.set_xlabel('Number of patients')\n_ = ax.xaxis.set_tick_params(rotation=0)","a778d2f7":"ax = boxes_per_patient.value_counts().sort_index().plot.barh()\n_ = ax.set_title('Pneumonia opacity bounding boxes per image')\n_ = ax.set_xlabel('Number of patients')\n_ = ax.set_ylabel('Boxes')\n_ = ax.xaxis.set_tick_params(rotation=0)","e22c211b":"g = sns.FacetGrid(col='Target', hue='gender', \n                  data=stage_1_train_labels.drop_duplicates(subset=['patientId']), \n                  height=9, palette=dict(F=\"red\", M=\"blue\"))\n_ = g.map(sns.distplot, 'age', hist_kws={'alpha': 0.5}).add_legend()\n_ = g.fig.suptitle(\"What is the age distribution by gender and target?\", y=1.02, fontsize=20)","994d64af":"areas = stage_1_train_labels.dropna(subset=['area'])\ng = sns.FacetGrid(hue='gender', data=areas, height=9, palette=dict(F=\"red\", M=\"blue\"), aspect=1.4)\n_ = g.map(sns.distplot, 'area', hist_kws={'alpha': 0.5}).add_legend()\n_ = g.fig.suptitle('What are the areas of the bounding boxes by gender?', y=1.01)","c7f01d7d":"centers = (stage_1_train_labels.dropna(subset=['x'])\n           .assign(center_x=stage_1_train_labels.x + stage_1_train_labels.width \/ 2, \n                   center_y=stage_1_train_labels.y + stage_1_train_labels.height \/ 2))\nax = sns.jointplot(\"center_x\", \"center_y\", data=centers, height=6, alpha=0.03, color=\"red\")\n_ = ax.fig.suptitle(\"Where is Pneumonia located?\", y=1.01)","16b72859":"pixel_vc = stage_1_train_labels.drop_duplicates('patientId')['pixel_spacing'].value_counts()\npixel_vc.iloc[4] += pixel_vc.iloc[5] # combine into one as near identical values\nax = pixel_vc.iloc[0:5].plot.barh()\n_ = ax.set_yticklabels([f'{ps:.3f}' for ps in pixel_vc.index[:6]])\n_ = ax.set_xlabel('Count')\n_ = ax.set_ylabel('Pixel Spacing')\n_ = ax.set_title('How is the pixel spacing distributed?')","41b9838a":"areas_with_count = areas.merge(pd.DataFrame(boxes_per_patient).rename(columns={'Target': 'bbox_count'}), \n                               on='patientId')\ng = sns.FacetGrid(hue='bbox_count', data=areas_with_count, height=8, aspect=1.4)\n_ = g.map(sns.distplot, 'area').add_legend()\n_ = g.fig.suptitle(\"How are the bounding box areas distributed by the number of boxes?\", y=1.01)","109a1e82":"plt.rcParams['figure.figsize'] = [10, 6]\nclf = GaussianMixture(n_components=2)\nclf.fit(centers[['center_x', 'center_y']])\ncenter_probs = clf.predict_proba(centers[['center_x', 'center_y']])\nZ = -clf.score_samples(centers[['center_x', 'center_y']])\noutliers = centers.iloc[Z > 17]\nfig, ax = plt.subplots()\ncenters.plot.scatter('center_x', 'center_y', c=Z, alpha=0.07, cmap='viridis', ax=ax)\noutliers.plot.scatter('center_x', 'center_y', c='red', marker='x', s=100, ax=ax)\n_ = ax.set_title('Where are the outliers?', fontsize=18)","6cf16041":"def get_image(patientId, root_dir='..\/input\/stage_1_train_images\/'):\n    fn = os.path.join(root_dir, f'{patientId}.dcm')\n    dcm_data = pydicom.read_file(fn)\n    return dcm_data.pixel_array\n\ndef draw_bbs(bbs, ax):\n    for bb in bbs.itertuples():\n        rect = patches.Rectangle(\n            (bb.x, bb.y), bb.width, bb.height,\n            linewidth=2, edgecolor='red', facecolor='none')\n        ax.add_patch(rect)\n\ndef draw_image(img, bbs, ax):\n    ax.imshow(img, cmap='gray')\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if bbs is not None:\n        draw_bbs(bbs, ax)\n\noutliers_15 = outliers.drop_duplicates(subset=['patientId']).iloc[:15]\nfig, axes = plt.subplots(3, 5)\nfor row, ax in zip(outliers_15.itertuples(), axes.flatten()):\n    img = get_image(row.patientId)\n    bbs = stage_1_train_labels.loc[stage_1_train_labels.patientId == row.patientId, ['x', 'y', 'width', 'height']]\n    draw_image(img, bbs, ax)\nfig.tight_layout(pad=-0.5)","efc462f4":"plt.rcParams['figure.figsize'] = [12, 4]\nax = sns.distplot(stage_1_train_labels.mean_black_pixels)\n_ = ax.set_xlabel('Percentage of black pixels')\n_ = ax.set_title('Are there images with mostly black pixels?')\nprint(\"Images with more than 10% black pixels: \", len(stage_1_train_labels[stage_1_train_labels.mean_black_pixels > 0.1]))","c7b4b008":"high_black_pixel_patientIds = stage_1_train_labels.loc[stage_1_train_labels.mean_black_pixels > 0.1, \n                                                       'patientId'].drop_duplicates()\nfig, axes = plt.subplots(4, 5)\nfor i, (patient_id, ax) in enumerate(zip(high_black_pixel_patientIds, axes.flatten())):\n    row = stage_1_train_labels.loc[stage_1_train_labels.patientId == patient_id]\n    img = get_image(row.patientId.iloc[0])\n    bbs = row[['x', 'y', 'width', 'height']]\n    draw_image(img, bbs, ax)\nfig.tight_layout(pad=-1)","ca7594d7":"high_white_pixel_patientIds = stage_1_train_labels.loc[stage_1_train_labels.mean_black_pixels < 0.000001, 'patientId'].drop_duplicates()\nfig, axes = plt.subplots(4, 5)\nfor patient_id, ax in zip(high_white_pixel_patientIds, axes.flatten()):\n    row = stage_1_train_labels.loc[stage_1_train_labels.patientId == patient_id]\n    img = get_image(row.patientId.iloc[0])\n    bbs = row[['x', 'y', 'width', 'height']]\n    draw_image(img, bbs, ax)\nfig.tight_layout(pad=-1)","efd372b3":"high_black_pixel_images = np.empty(shape=(high_black_pixel_patientIds.shape[0], 1024, 1024))\n\nfor i, patient_id in enumerate(high_black_pixel_patientIds):\n    row = stage_1_train_labels.loc[stage_1_train_labels.patientId == patient_id]\n    img = get_image(row.patientId.iloc[0])\n    high_black_pixel_images[i] = img \n    \nhigh_black_pixel_contours = []\nfor img in high_black_pixel_images:\n    img2 = feature.canny(img != 0)\n    img2 = morphology.convex_hull_image(img2)\n    c = measure.find_contours(img2, 0)[0]\n    c = measure.approximate_polygon(c, 20)\n    high_black_pixel_contours.append(c)\n\nfig, axes = plt.subplots(4, 5)\ncontours = []\nfor c, img, ax in zip(high_black_pixel_contours, high_black_pixel_images, axes.flatten()):\n    draw_image(img, None, ax)\n    _ = ax.plot(c[:, 1], c[:, 0], '-b', linewidth=4)\nfig.tight_layout(pad=-1)","b195b313":"def order_coordinates(coords):\n    \"\"\"Returns coordinates with order:\n    (top left, top right, bottom right, bottom left)\n    \"\"\"\n    coords = coords[:-1]\n    output = np.empty((4, 2), dtype=np.float32)\n    dists = coords[:, 1]**2 + coords[:, 0]**2\n    ratios = coords[:, 1]\/np.sqrt(dists)\n    \n    tl = coords[np.argmin(dists)]\n    br = coords[np.argmax(dists)]\n    \n    tr = coords[np.argmax(ratios)]\n    bl = coords[np.argmin(ratios)]\n    \n    output[0] = tl\n    output[1] = tr\n    output[2] = br\n    output[3] = bl\n    \n    return output[:,::-1]\n\ndef _convert_bb(bb, tfm):\n    x, y, w, h = bb.x, bb.y, bb.width, bb.height\n    pts = np.array([\n        [x, y],\n        [x + w, y],\n        [x + w, y + h],\n        [x, y + h]\n    ])\n    new_pts = tfm.inverse(pts)\n    pts_min = np.min(new_pts, axis=0)\n    pts_max = np.max(new_pts, axis=0)\n    \n    x, y = pts_min\n    w, h = pts_max - pts_min\n    \n    return np.array([x, y, w, h])\n\ndef convert_bbs(bboxs, tfm):\n    output = np.empty_like(bboxs, dtype=np.float32)\n    \n    for i, bb in enumerate(bboxs.itertuples()):\n        output[i] = _convert_bb(bb, tfm)\n    \n    return pd.DataFrame(output, columns=['x', 'y', 'width', 'height'])\n\nfig, axes = plt.subplots(4, 2, figsize=(8, 10))\n\norig_coords = np.array([[0, 0], [1024, 0], [1024, 1024], [0, 1024]])\ninteresting_idices = [0, 2, 3, 17]\n\nfor i, (ax1, ax2) in zip(interesting_idices, axes):\n    patient_id = high_black_pixel_patientIds.iloc[i]\n    img = high_black_pixel_images[i]\n    contour = high_black_pixel_contours[i]\n    \n    row = stage_1_train_labels.loc[stage_1_train_labels.patientId == patient_id]\n    bbs = row[['x', 'y', 'width', 'height']]\n    ordered_coors = order_coordinates(contour)\n    tform = transform.estimate_transform('projective', orig_coords, ordered_coors)\n    img_t = transform.warp(img, tform, output_shape=(1024, 1024))\n    \n    new_bbs = convert_bbs(bbs, tform)\n    _ = draw_image(img, bbs, ax1)\n    _ = draw_image(img_t, new_bbs, ax2)\n    \nfig.tight_layout(pad=-1)","e848d2d7":"ax = sns.distplot(stage_1_train_labels['aspect_ratio'].dropna(), norm_hist=True)\n_ = ax.set_title(\"What does the distribution of bounding aspect ratios look like?\")\n_ = ax.set_xlabel(\"Aspect Ratio\")","9bd77045":"aspect_ratios = stage_1_train_labels['aspect_ratio'].dropna()\nhigh_aspect_ratio_tr = (stage_1_train_labels.iloc[aspect_ratios[aspect_ratios > aspect_ratios.quantile(q=0.99)].index]\n                          .drop_duplicates(['patientId']))\nfig, axes = plt.subplots(3, 5)\nfor row, ax in zip(high_aspect_ratio_tr.itertuples(), axes.flatten()):\n    img = get_image(row.patientId)\n    bbs = stage_1_train_labels.loc[stage_1_train_labels.patientId == row.patientId, ['x', 'y', 'width', 'height']]\n    draw_image(img, bbs, ax)\nfig.tight_layout(pad=-0.5)","bf7d5138":"g = sns.relplot(x='area', y='aspect_ratio', \n            data=stage_1_train_labels.dropna(subset=['area', 'aspect_ratio']), \n            height=8, alpha=0.8, aspect=1.4,)\n_ = g.fig.suptitle(\"Is there a relationship between the bounding box's aspect ratio and area?\", y=1.005)","5287d49c":"These ones are generally full frame. Some contain pneumonia opacities and others contain other conditions.","88c7bf8b":"## <span style=\"color:darkgreen\">Data Exploration and Visualization<\/span> <a id='data'><\/a>","f9d8ee95":"Dear Kagglers,\n\nThis kernel is intended to help beginners who are overwhelmed by this project get \"up and running.\" We hope to provide a basic understanding of the data and how to work with it to make predictions that can be submitted to the competition. If you have any questions, comments, or especially criticisms please let us know in the comments so we can address them! Have a nice day, everyone!\n\n---","17ed4939":"---\n\nWhat does the images with a high aspect ratio look like?","84023ff6":"The sample submission file will show us the format that we should be using for our competition submission after we have created our predictions.","034ca175":"---\n\nWhat is the age distribution by gender and target?","31bc8aec":"---\n\nNext up we have the training labels file.","af89ea52":"There are 28,989 rows with two columns. The first column is the `patientId` and the second is the `class`. How many of those patient IDs are unique?","6b2e4ff6":"---\n\nCan the bounding boxes be resized when cropping and resizing the cropped images?","003c7c9e":"Each row consists of the `patientId`, as well as either `NaN` (not a number) if there are no pneumonia bounding boxes indicated or values representing the `x` and `y` coordinates of the upper-left corner of the bounding box followed by the `width` and `height` of the bounding box. `Target` is 0 for no boxes and 1 when a box is present. And again, some patients are represented multiple times. Spoiler: that is because some patients have multiple bounding boxes labeled!","d28a6f6e":"### Examine Files <a id='files'><\/a>\n\nWe will start by listing the files included in the project. <a id='os'><\/a>","e54bd849":"---\n\nWhat do the outliers look like? <a id='patches'><\/a>","53931956":"---\n\nCan traditional image processing find a bounding box in the cropped images? (<a href='#skimage'>here<\/a>)","29a8800d":"---\n\nWhat is the distribution of black pixels saturation in the images? How many have more than 10% of images comprised of black pixels?","c413be30":"---\n\nWhere are the outliers? <a id='gaussianmixture'><\/a>","a556ee72":"You can also check out these excellent Medium articles by NoMonia team members:\n* [YOLO Object Detection Walkthrough for the RSNA Pneumonia Detection Challenge](https:\/\/medium.com\/@hjhuney\/yolo-object-detection-walkthrough-for-the-rsna-pneumonia-detection-challenge-123ec9a9adf2) by Jake Huneycutt: a walk-through to get a YOLOv3 model working locally\n* [Kaggle RSNA Pneumonia Detection Challenge Explained](https:\/\/medium.com\/@sebastiannorena\/c140b19bf903) by Sebastian Norena: ideas for improving upon the competition-provided starter kernel\n* [No-more-moaning\u2026ia: A Journey in Medical Imaging](https:\/\/medium.com\/@t7jackso\/26951f901707) by Robert Jackson: An overview of the competition challenge with discussion on pneumonia, neural networks, and more","b2915259":"### Other Kernels  <a id=\"kernels\"><\/a>\n\n*   [CNN with segmentation](https:\/\/www.kaggle.com\/jonnedtc\/cnn-segmentation-connected-components)\n*   [CheXNet](https:\/\/www.kaggle.com\/ashishpatel26\/chexnet-radiologist-level-pneumonia-detection)\n*   [NASNet](https:\/\/www.kaggle.com\/ashishpatel26\/beginner-tutorial-nasnet-pneumonia-detection)","bdd1093f":"(return to <a href='#contents'>Table of Contents<\/a>)\n\n---\n---","ff2ac4a6":"So 3062 patients are represented twice, 105 thrice, and 11 four times.\n\n---","81b75a8b":"Let's first look at the `csv` files in order. We'll use [`pandas.read_csv`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.read_csv.html) to read the data and then look at a few examples. <a id='pd'><\/a>","159def73":"We see that it has two columns, titled `patientId` and `PredictionString`, and 1000 rows. This tells us we do NOT want to include the row index counters on our submission. We also know, from the [competition evaluation page](https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge#evaluation), that if a patient has multiple predicted bounding boxes that the submission should include them all by listing them one after another in the prediction column like this: \n\n`00322d4d-1c29-4943-afc9-b6754be640eb,0.8 10 10 50 50 0.75 100 100 5 5`","f056bd0e":"Again, this is difficult to interpret without normalizing the location of the lungs in each image.","ffab99f9":"### Goals <a id='goals'><\/a>","0974d3d9":"---\n\nIs there a relationship between the bounding box's aspect ratio and area?","cd7a4eec":"### References <a id=\"refs\"><\/a>","e268b476":"---\n\nWhile we're at it, what do the images with mostly white pixels look like?","a2c5bb6e":"### Pneumonia Locations <a id='locations'><\/a>","a9c9a799":"Here we see that females have smaller sized pneumonia bounding boxes. This is possible because [females generally have lung volumes that are 10%-12% less than that of males](https:\/\/www.atsjournals.org\/doi\/pdf\/10.1164\/rccm.200208-876OC).","7cca9909":"## <span style=\"color:darkgreen\">Set-up<\/span> <a id='setup'><\/a>","19ee9414":"Wow. There is a lot here. The information we are most interested in at this point are the patient's sex and age, the view position of the image (PA for posterior -> anterior or AP for anterior -> posterior), the image pixel spacing, and of course the image itself, which is contained in the `Pixel Data` array.","de250473":"Some patients are represented multiple times in the `patientId` column.\n\nLet's break that down.\n\nThis line below counts up each `patientId`'s number of occurrences and then counts up how many have each number of occurrences. Got it?","9c6b55a7":"# <a href='#contents'>Table of Contents<\/a> <a id=\"contents\"><\/a> \n1. <a href='#intro'>Introduction<\/a>\n   *   <a href='#overview'>Project Overview<\/a>\n   *   <a href='#goals'>Goals<\/a>\n2. <a href='#setup'>Set-up\n   *   <a href='#dependencies'>Install Dependencies<\/a>\n   *   <a href='#files'>Examine Files<\/a>\n3. <a href='#data'>Data Exploration and Visualization<\/a>\n   *   <a href='#classes'>Classes<\/a>\n   *   <a href='#locations'>Pneumonia Locations<\/a>\n4. <a href='#conclusion'>Conclusion<\/a>\n    *   <a href='#kernels'>Other Helpful Kernels<\/a>\n    *   <a href='#refs'>References<\/a>\n\n ---\n ---","19664e51":"## <span style=\"color:darkgreen\">Introduction<\/span> <a id=\"intro\"><\/a>\nThis kernel will provide a simplified approach to loading and examining data for the RSNA Pneumonia Detection Challenge. First we will briefly discuss the challenge. Then we will look at the `csv` files and examine the [DICOM](https:\/\/en.wikipedia.org\/wiki\/DICOM) images. Next we will manipulate the images to maximize their usefulness for model training. Lastly, we will point you to models to explore, including our own version of YOLO.\n\n---","337d38d6":"---\n\nWhat are the areas of the bounding boxes by gender?","eee2d1f9":"First of all, a giant shout-out to [thomasjpfan's kernel](https:\/\/www.kaggle.com\/thomasjpfan\/q-a-with-only-pictures) for a lot of these visualizations. We've adapted some and used others as is and explained the code behind them for this tutorial.","1b8f2be1":"(return to <a href='#contents'>Table of Contents<\/a>)\n\n---\n---","32dc3f93":"### Install Dependencies <a id='dependencies'><\/a>\n\nWe need to import the libraries and packages we'll be using. These are:\n*   [os](https:\/\/docs.python.org\/3\/library\/os.html): Python module for operating system functionality (<a href='#os'>here<\/a>)\n*   [pandas](https:\/\/pandas.pydata.org\/index.html): Python data analysis and processing library, used for reading`csv`files (<a href='#pd'>here<\/a>)\n*   [seaborn](https:\/\/seaborn.pydata.org\/): Python visualization library (<a href='#seaborn'>here<\/a>)\n*   [matplotlib.pyplot](https:\/\/matplotlib.org\/users\/pyplot_tutorial.html): graphical plotting output library (<a href='#plt'>here<\/a>)\n*   [pydicom](https:\/\/github.com\/pydicom\/pydicom): inspect and modify [DICOM](https:\/\/www.dicomstandard.org\/) data (<a href='#pydicom'>here<\/a>)\n*   [numpy](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/): Python package for scientific computations, including linear algebra (<a href='#np'>here<\/a>)\n*   [multiprocessing](https:\/\/docs.python.org\/3\/library\/multiprocessing.html): supports spawning processes (<a href='#multiprocessing'>here<\/a>)\n*   [warnings](https:\/\/docs.python.org\/3\/library\/warnings.html): provides options for warning control (<a href='#warnings'>here<\/a>)\n*   [matplotlib.patches](https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.patches.Patch.html#matplotlib.patches.Patch): 2D shapes with a face color and an edge color (<a href='#patches'>here<\/a>)\n*   [imgaug](https:\/\/imgaug.readthedocs.io\/en\/latest\/): library for image augmentation in machine learning experiments (<a href='#imgaug'>here<\/a>)\n*   [tqdm](https:\/\/pypi.org\/project\/tqdm\/): fast, extensible progress meter (<a href='#tqdm'>here<\/a>)\n*   [scikit-learn GaussianMixture](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.mixture.GaussianMixture.html): allows estimation of the parameters of a Gaussian mixture distribution (<a href='#gaussianmixture'>here<\/a>)\n*   [scikit-image morphology, feature, measure, util, & transform](https:\/\/scikit-image.org\/): a collection of algorithms for image processing (<a href='#skimage'>here<\/a>)\n\nClick on `here` to the right of the library to find the code block where it is first used. <a id='warnings'><\/a>","387448c7":"---\n\nHow is the pixel spacing distributed?","835b2639":"(return to <a href='#contents'>Table of Contents<\/a>)\n\n---\n---","e499859b":"Where is the centroid of the bounding boxes of pneumonia located?","2b3ef4ae":"So there you have it. We've loaded the data, explored the files included, examined images to help us understand our goals and to brainstorm approaches, and run the provided model on the data to create our first submission file. Where do we go next? Well, there are several other models that have been shared in the kernels.","640a73de":"---\n\nLet's now look at the metadata available with each image. We'll use pydicom to read the images. <a id='pydicom'><\/a>","9ddbb383":"(return to <a href='#contents'>Table of Contents<\/a>)\n\n---\n---","91bf1da6":"---\n\nWhat do the images with mostly black pixels look like?","841684c8":"---\n### Classes <a id='classes'><\/a>","ee9513f4":"### Project Overview <a id='overview'><\/a>","d72326ac":"---\n\nHow many bounding boxes designating a pneumonia opacity are present in each image?","b80c5d03":"How many of the patients have pneumonia compared to those that don't? <a id='seaborn'><\/a> <a id='plt'><\/a>","e55b7280":"https:\/\/www.atsjournals.org\/doi\/pdf\/10.1164\/rccm.200208-876OC","a58d3254":"To classify pneumonia in medical images we will build an algorithm that detects lung opacities. We will need to differentiate between pneumonia opacities and other lung conditions that can create signals such as fluid overload, bleeding, volume loss, lung cancer, post-radiation or surgical changes, and fluid in the pleural space.\n\nThis will be done by training machine learning models on a set of images that have been labeled by experts to show a box drawn around the presumed pneumonia lung opacity or opacities, when present. Many control images with no pneumonia opacity or with other lung conditions are also present with which to train. A successful algorithm will be able to take unlabeled images and label them accurately by drawing a box around pneumonia lung opacities.\n\nScoring in the competition will be by finding the mean average precision of the predicted boxes at different intersection over union thresholds. A more detailed explanation can be found on the [challenge's evaluation page](https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge#evaluation).\n\n---","1a990ba2":"Without overlaying this over the mean lung outline it is difficult to make any strong conclusions from this, as the lungs may not be centered on all images. It would be helpful to recalculate the centroid in relation to each lung after segmentation.","5610494b":"Numerous advances in medicine have been accomplished through the use of machine learning on medical imagery. The Radiological Society of North America ([RSNA](http:\/\/www.rsna.org)) has sponsored this competition on Kaggle to incentivize the creation of new algorithms that can detect pneumonia in radiographic images. A more detailed definition of the of the competition is provided on the [Kaggle RSNA Pneumonia Detection Challenge website](https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge). \n\n---","5ff5b7fe":"---\n\nHow are the bounding box aspect ratios distributed?","ab24a41e":"---\n\nHow are the bounding box areas distributed by the number of boxes?","d9538b09":"Aha! As feared, there are images that are cropped and off-center. One thing we notice about those is that the have a large quantity of black pixels resulting from the cropping. Let's look into that.","aa95d73c":"---\n\nNext we will define functions that parse the DICOM metadata for the attributes we are interested in. We get the age, gender, view position, patient ID, pixel spacing, and a metric to determine the number of black pixels in each image (we'll see why later).\n\nWe use Python's multiprocessing package to accelerate the speed of these tasks. <a id='multiprocessing'><\/a> <a id='np'><\/a>","268a8014":"## <span style=\"color:darkgreen\">Conclusion<\/span> <a id=\"conclusion\"><\/a>"}}