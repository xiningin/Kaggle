{"cell_type":{"3b4315b9":"code","e689ca7e":"code","ffdae4c1":"code","0da6e584":"code","0fd99b84":"code","c1e00a65":"code","ad827ce2":"code","efac53fd":"code","44e32114":"code","9d5b760b":"code","d148c901":"code","fd67a732":"code","7ef2dff3":"code","890e226f":"code","eacce56d":"markdown","79885a3a":"markdown","05d70b92":"markdown","b0c2a4bd":"markdown","5a8a8ca6":"markdown","db44535a":"markdown","6d6483a0":"markdown","d654e900":"markdown","e4615bdd":"markdown","d769a0fa":"markdown","558ac295":"markdown","ad0fa90c":"markdown"},"source":{"3b4315b9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n%inline matplotlib","e689ca7e":"import os\nprint(os.listdir(\"..\/input\"))","ffdae4c1":"df_train = pd.read_csv('..\/input\/udacity-mlcharity-competition\/census.csv')\ndf_test = pd.read_csv('..\/input\/udacity-mlcharity-competition\/test_census.csv')","0da6e584":"df_train.head()","0fd99b84":"n_records = df_train.shape[0]\nn_greater_50k = len(df_train[df_train['income'] == '>50K'])\nn_at_most_50k = len(df_train[df_train['income'] == '<=50K'])\ngreater_percent = 100 * n_greater_50k \/ n_records\n\nprint(\"Total number of records: \",n_records)\nprint(\"Individuals making more than $50,000: \",n_greater_50k)\nprint(\"Individuals making at most $50,000: \",n_at_most_50k)\nprint(\"Percentage of individuals making more than $50,000: \",greater_percent)","c1e00a65":"df_train[['capital-gain','capital-loss']].hist()","ad827ce2":"# Split the data into features and target label\nincome_raw = df_train['income']\nfeatures_raw = df_train.drop('income', axis = 1)\n\n# Transform Skewed Continuous Features\nskewed = ['capital-gain', 'capital-loss']\nfeatures_log_transformed = pd.DataFrame(data = features_raw)\nfeatures_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n\n#Normalizing Numerical Features\n# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\nfeatures_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\nfeatures_final = pd.get_dummies(features_log_minmax_transform)\n\n# Encode the 'income_raw' data to numerical values\nincome = income_raw.map({'<=50K': 0, '>50K':1})\n\n# Print the final features\nfeatures_final.head(5)","efac53fd":"# check correlation between features: \nimport seaborn as sns\ndata = pd.concat([features_final, income], axis =1)\nplt.figure(figsize=(30,28))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white')","44e32114":"# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features_final, income, test_size = 0.2, random_state = 21)\n\n# Show the results of the split\nprint(\"Training set has {} samples\",X_train.shape[0])\nprint(\"Testing set has {} samples.\",X_test.shape[0])","9d5b760b":"# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier","d148c901":"# Initialize the classifier\nclf = AdaBoostClassifier(random_state=42)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {'n_estimators': [200, 300, 500, 600]}\n\n# Make an roc_auc scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv=5)\n\n# Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\nprint(\"best_estimator\", grid_fit.best_estimator_)\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"Area under curve on testing data: {:.4f}\".format(roc_auc_score(y_test, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final Area under curve on  the testing data: {:.4f}\".format(roc_auc_score(y_test, best_predictions)))","fd67a732":"from sklearn.preprocessing import MinMaxScaler\n# Replace all NaNs with forwardfilling\nfor row in df_test:\n    df_test[row].fillna(method='ffill', axis=0, inplace=True)\n# Transform Skewed Continuous Features\nskewed = ['capital-gain', 'capital-loss']\nfeatures_test_log_transformed = pd.DataFrame(data = df_test)\nfeatures_test_log_transformed[skewed] = features_test_log_transformed[skewed].apply(lambda x: np.log(x + 1))\n#Normalizing Numerical Features\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\nfeatures_test_log_minmax_transform = pd.DataFrame(data = features_test_log_transformed)\nfeatures_test_log_minmax_transform[numerical] = scaler.fit_transform(features_test_log_transformed[numerical])\n\n# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\nfeatures_test_encoded = pd.get_dummies(features_test_log_minmax_transform)\n\n# Remove the first column\nfeatures_test_final = features_test_encoded.drop('Unnamed: 0',1)","7ef2dff3":"# Make predictions using features_test_final and store it a new coulmn in test dataset\ndf_test['id'] = df_test.iloc[:,0]\ndf_test['income'] = best_clf.predict_proba(features_test_final)[:,1]\ndf_test.head()","890e226f":"# write output file\ndf_test[['id', 'income']].to_csv(\"submission.csv\", index=False)","eacce56d":"## Section 3: EDA","79885a3a":"# Section 1:","05d70b92":"# Section 2: Exploration","b0c2a4bd":"# Section 4: Feature Engineering","5a8a8ca6":"***Udacity ML Charity Competition***\n\n**Problem:-**\n\nThe training data for this competition is the same as what you used to complete the project (census.csv). The columns therefore are the same as the ones you have already been working with for the classroom project.\n- The 1 values in the test dataset indicate with incomes greater than 50K \n- while 0 values indicate that is not the case.","db44535a":"**3.1: EDA**","6d6483a0":"**1.1: Import Necessary Libraries**","d654e900":"# Section 6: Testing the Model on Test Dataset","e4615bdd":"**5.1: Shuffle and Split Data**","d769a0fa":"**2.1: Data Exploration**","558ac295":"**4.1: Feature Engineering**","ad0fa90c":"# Section 5: Modelling"}}