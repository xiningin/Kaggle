{"cell_type":{"00dd95c8":"code","fac82626":"code","b755067e":"code","be203243":"code","ce7f59c2":"code","cc344228":"code","1cc26dae":"code","f5b1cc16":"code","a05810e8":"code","2612a152":"code","a5e6dcff":"code","e2ed4b03":"code","a9593cdd":"code","0d1d3f0d":"code","2e2121b6":"code","8e81bb94":"markdown","c57034c1":"markdown","e187934e":"markdown","948b7dd8":"markdown","8bca4821":"markdown","69858c19":"markdown","e489fc0f":"markdown","be089cb1":"markdown","fcaf9ed4":"markdown","084d60f3":"markdown","cbca00c9":"markdown","932aee4a":"markdown","07566df5":"markdown","fb6c114c":"markdown","e0bacae6":"markdown","f7c00318":"markdown","9e957166":"markdown"},"source":{"00dd95c8":"import pandas as pd \nimport tensorflow as tf\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings \n%matplotlib inline\nsns.set_style('whitegrid')\nwarnings.filterwarnings(\"ignore\")","fac82626":"path  = '..\/input\/kc-housesales-data\/kc_house_data.csv'\ndf = pd.read_csv(path)\nprint(f\"Numbers of nan values are {df.isnull().any().sum()}\")\nprint(f\"Columns in this Data set {list(df.columns[:5])}..., where 'Sale Price' is what we want to Predict\")\ndf.head(3)\n#df.describe().T","b755067e":"# The only two columns to be transformed are Date , and then Drop id, Date\n\n# ---- >  Date time \ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].apply(lambda date: date.year)\ndf['month'] = df['date'].apply(lambda date: date.month)\n\n# ---- > Dropping \ndf.drop(['id','date'], axis = 1 , inplace = True)\nprint(\"-----------------Transforming Columns -------------------------\")","be203243":"f,ax = plt.subplots(2,2, figsize = (15,10))\ny = 'price'\nsns.boxplot(x='condition', y=y, data=df , ax = ax[1,1])\nsns.scatterplot(x='sqft_living',y=y ,data=df , ax = ax[0,1])\nsns.scatterplot(x='long',y='lat',hue=y ,data=df , ax = ax[1,0])\nsns.boxplot(x='bedrooms',y=y ,data=df , ax = ax[0,0]);\n\nprint('We can notice that the more bedrooms a house has the more expensive it is')\nprint('in the second graph we can notice how price is linearly related to sqft living - the bigger the house is the more expensive it will be')\nprint('In graph 3 - a combination of longitude and latitude plus price segmentation , the darker the color is the more expensive the house would be')\nprint('Graph 4 confirmes that the house condition seems not to be so significance to set the final price ')\nprint('Notice *** all those small dots in graph 1, 4 may be Outliers , there fore we have to make some transformation')","ce7f59c2":"from sklearn.neighbors import LocalOutlierFactor\n# ---> Identifying Outliers\nLOF = LocalOutlierFactor(n_neighbors=18, novelty = False)\ndf['Outliers'] = LOF.fit_predict(df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated','sqft_living15','sqft_lot15']])\n\n# Finding outliers \noutliers = df[df['Outliers']==-1].index \nprint(f\"number of outliers {len(outliers)}\")\nprice_sqft = df.loc[outliers,['price','sqft_basement','sqft_living','yr_built','sqft_lot']]\n\n#Now let's visualized where those outliers were \nf,ax = plt.subplots(2,2, figsize = (15,10))\ny = 'price'\n\nsns.scatterplot(x='sqft_basement', y=y, data=df , ax = ax[1,1])\nsns.scatterplot(x='sqft_basement', y=y, data=price_sqft , ax = ax[1,1],color = 'r')\nsns.scatterplot(x='sqft_living',y=y ,data=df , ax = ax[0,1])\nsns.scatterplot(x='sqft_living',y=y ,data=price_sqft , ax = ax[0,1],color = 'r')\nsns.scatterplot(x='yr_built',y=y ,data=df , ax = ax[1,0])\nsns.scatterplot(x='yr_built',y=y ,data=price_sqft , ax = ax[1,0],color = 'r')\nsns.scatterplot(x='sqft_lot',y=y ,data=df , ax = ax[0,0])\nsns.scatterplot(x='sqft_lot',y=y ,data=price_sqft , ax = ax[0,0],color = 'r');\n\n#Dropping Outliers\ndf.drop(df[df['Outliers']==-1].index , inplace=True)\nprint(\"the ones in red are the outliers, remember that this is two dimmensional graph\")\nprint(\"which means that in 2D doesnt make that much sense but this is only a representation\")\n","cc344228":"# ------------------------ Before Normalizing ------------------------\n\nf,ax = plt.subplots(2,4, figsize = (20,9))\ny = 'price'\nsns.histplot(df['price'], ax = ax[0,0])\nsns.histplot(df['sqft_living'], ax = ax[0,1])\nsns.histplot(df['sqft_living15'], ax = ax[0,2])\nsns.histplot(df['sqft_above'], ax = ax[0,3]);\n\n# First -- Identify numerical and categorical columns \nnumerical = df.select_dtypes(['int64','float64'])\ncategorical = df.select_dtypes('object')\n\n#Second  \nskw = numerical.skew().abs().sort_values(ascending=True) #skew the data set\ncolumns_skw = skw.loc[skw>0.75] #find those skewed to the right\n\n# ------------------------ After Normalizing ------------------------\n\n# -- Understand that not all of numerical columns must be transformed (e.x year , long, lat)\nfor i in columns_skw.index:\n  if i not in ['year','long']:\n    df[i]= np.log1p(df[i]).astype('float')\n\nsns.histplot(df['price'], ax = ax[1,0])\nsns.histplot(df['sqft_living'], ax = ax[1,1])\nsns.histplot(df['sqft_living15'], ax = ax[1,2])\nsns.histplot(df['sqft_above'], ax = ax[1,3]);","1cc26dae":"FC = numerical.corr() # Features Correlation\nTarget = numerical.corr()['price'].to_frame().reset_index() #Feature Correlation related to SalePrice\nFR = FC.unstack().to_frame(name='Correlation') # Feature Relation\nFeature = FR[(FR['Correlation']>=0.8)&(FR['Correlation']<1)].sort_values(by='Correlation', ascending = False).reset_index()\nFinal  = Feature.merge(Target,left_on='level_1', right_on='index')\nprint(\"In thisdata set these columns are strongly related , i will usually take one and drop the other one but in this case i will keep it \")\nFinal","f5b1cc16":"from sklearn.model_selection import KFold, train_test_split, cross_val_predict, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score","a05810e8":"# Base model \nX = df.drop(['price','Outliers'], axis = 1)\ny = df['price']\n\n# For SGD\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nKF = KFold(n_splits=4) # Number of split on the Data\nX_columns = X.columns.to_list()\n\nperformance = {} \nfor i in [LinearRegression(), Lasso (), Ridge (), ElasticNet ()]:\n  pipe = Pipeline([(str(i.__class__.__name__),i)]) #model name + model Options\n  model_pred = cross_val_predict(pipe, X,y, cv = KF)\n  Error = np.sqrt(mean_squared_error(np.expm1(y),np.expm1(model_pred)))\n  performance[str(i.__class__.__name__)] =round(Error)\n\n# Final Results \nprint(\"The base model - without any parameter tunning shows a MSE of 17K to 35 K\")\nprint(\"This means that the price that we predict varies +- 17K and 35K in Lasso and ElastnicNet\")\npd.DataFrame(performance, index=['Squarred_error']).T.sort_values('Squarred_error')\n","2612a152":"# Base model + StandardScaler\n\nperformance_ = {} \nprediction = []\nfor i in [LinearRegression(), Lasso (), Ridge (), ElasticNet ()]:\n  pipe = Pipeline([('Standar_Scaler',StandardScaler()),(str(i.__class__.__name__),i)]) #model name + model Options\n  model_pred = cross_val_predict(pipe, X, y, cv = KF)\n  prediction.append(model_pred)\n  Error = np.sqrt(mean_squared_error(np.expm1(y),np.expm1(model_pred)))\n  performance_[str(i.__class__.__name__)] =round(Error)\n\n# Final Results \nprint(\"There are not significant changes, except for Lasso by having 23 less MSE\")\nprint(pd.DataFrame([performance,performance_],index= ['Squarred_error','Sqrd_scaler']).T.sort_values('Sqrd_scaler'))\nplt.figure(figsize = (15,10))\nprint('\\n')\n\ncolor = (('b', 3.0,'Linear_regression'),\n         ('c', 2.0,'Lasso'),\n         ('g', 1.5,'Ridge'),\n         ('m',1.0,'ElasticNet'))\n\nfor i,j in zip(prediction,color):\n  plt.plot(y, i, marker = 'o', ls = '', c = j[0], ms =j[1], label =j[2])\n  plt.legend(frameon=True)\nprint('In here we can see that the regularization in ElastincNet and Lasso lambda where not the ebst parameter that is why we had that horizontal')\n","a5e6dcff":"MSE = {}\nbest_parameters = {}\nparams_ = {}\npredictions = []\n\n\n# ------Hyperparameters\nlasso_alpha = np.geomspace(0.0001,0.099,30)\nridge_alpha = np.geomspace(2,60,30)\nElastic_alphas = np.array([1e-5, 5e-5, 0.0001, 0.0005])\nElastic_ratios = np.linspace(0.1, 0.9, 9)\n\n# -------------Splitting Data\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nmodels = [LinearRegression(), \n          LassoCV(alphas=lasso_alpha,max_iter=5e4, cv=KF),\n          RidgeCV(alphas = ridge_alpha,cv=KF), \n          ElasticNetCV(alphas=Elastic_alphas, l1_ratio=Elastic_ratios,\n                            max_iter=1e4,cv=KF)]\n\nfor model in models:\n  pipe = Pipeline([('Standar_Scaler',StandardScaler()),(str(model.__class__.__name__),model)]) \n  pipe.fit(X,y)\n  pred_ = model.predict(X)\n  \n  # ----------- Adding Predictions\n  predictions.append(pred_)\n  rmse = np.sqrt(mean_squared_error(np.expm1(y),np.expm1(pred_)))\n\n  if pipe[str(model.__class__.__name__)].__class__.__name__ =='LinearRegression':\n    MSE[str(model.__class__.__name__)] = round(rmse)\n  else:\n    MSE[str(model.__class__.__name__)[:-2]] =round(rmse)\n    best_parameters[str(model.__class__.__name__)] = model.alpha_\n    try:\n      best_parameters[str(model.__class__.__name__)+'_ratio'] = model.l1_ratio_\n    except:\n      pass\n\n\n\n\nprint(\"***No we see some improvement ***\")\nprint(pd.DataFrame([performance,performance_,MSE],index= ['Squarred_error','MSE_Sqrd_scaler','MSE_Scaler_Hyper']).T.sort_values('MSE_Scaler_Hyper'))\nprint(\"\\n\")\nplt.figure(figsize = (15,10))\ncolor = (('b', 3.0,'Linear_regression'),\n         ('c', 2.5,'Lasso'),\n         ('g', 2.0,'Ridge'),\n         ('m', 1.0,'ElasticNet'))\n\nfor i,j in zip(predictions,color):\n  plt.plot(y, i, marker = 'o', ls = '', c = j[0], ms =j[1], label =j[2])\n  plt.legend(frameon=True)\nprint('Now we dont have the horizontal line beccause Lasso - ElastinCNet fit better the Data set due to the hyperparameter ')\n\n","e2ed4b03":"model_para_dict = {\n    'Linear':{'penalty':'none'},\n    'Lasso':{'penalty':'l2', 'alpha':best_parameters['LassoCV']},\n    'Ridge':{'penalty':'l2', 'alpha':best_parameters['RidgeCV']},\n    'ElasticNet': {'penalty':'elasticnet', \n                   'alpha':best_parameters['ElasticNetCV'],\n                   'l1_ratio':best_parameters['ElasticNetCV_ratio']}}\nNMSE = {}\nfor model_label, parameters in model_para_dict.items():\n  pipe =Pipeline([('StandarScaler',StandardScaler()),\n                  ('SGD',SGDRegressor(max_iter=4500,**parameters))])\n  model_pred = cross_val_predict(pipe, X,y, cv=KF)\n  Error = np.sqrt(mean_squared_error(np.expm1(y),np.expm1(model_pred)))\n  NMSE[str(model_label)] =round(Error)\n\nNMSE","a9593cdd":"from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping","0d1d3f0d":"def regression (initial_layers = 16 , mid_layers = 8 ,\n                drop_1 = 0 , drop_2 =0 ):\n\n  model = Sequential([\n      Dense(initial_layers, activation= 'relu'),Dropout(drop_1),\n      Dense(mid_layers, activation = 'relu'),Dropout(drop_2),\n      Dense(1)\n      ])\n  model.compile(optimizer = RMSprop(0.001),\n                loss = 'mse', \n                metrics =['mae','mse'])\n  return model \n\ndef plot_model (model):\n  hist = pd.DataFrame(model.history.history)\n  hist['epoch'] = model.history.epoch\n\n  f,ax = plt.subplots(1,2, figsize = (15,7))\n  sns.lineplot(hist.epoch, hist.mae , ax=ax[0] , label = 'Training Error')\n  sns.lineplot(hist.epoch, hist.val_mae, ax=ax[0], label = 'Validation Error')\n\n  sns.lineplot(hist.epoch, hist.mse , ax=ax[1], label = 'Training Error')\n  sns.lineplot(hist.epoch, hist.val_mse, ax=ax[1], label = 'Training Error');","2e2121b6":"BATCH_SIZE = 150\nEPOCHS = 500\n\nearly_stop = EarlyStopping(monitor = 'val_loss', mode='min' , patience = 20)\nmodel = regression(15,10,0.3,0.3)\nmodel.fit(x = X_train, y = y_train, validation_data = (X_test,y_test),\n          batch_size = BATCH_SIZE , epochs = EPOCHS, verbose = 0,\n          callbacks = [early_stop])\nplot_model(model)\nloss, mae, mse = model.evaluate(X_test,y_test, verbose = 0)\nprint(f\"Mean Abs Error: {mae}, Mean Squared Error:{mse}\")\n","8e81bb94":"#### Normalizing data\n\nIn this section we will look for some skewed columns ,skewed is when a distribution is not normalized - has bell shape -- ","c57034c1":"### Main Libraries","e187934e":"### ANN - Regression ","948b7dd8":"### Obtain\n1. Retrieve information \n2. Find Number of Null \/ Nan \/ Invaled Values \n3. Identify target value\n4. Define Objective\n\n---------------------------------\n- What are the most important variables to determine the final price\n- How could this modeling impact business positively \n- What are the future steps to improve \/  validate this model ","8bca4821":"### Basemodel + Pipeline + StandarScaler\n1. StandarScaler","69858c19":"### Hyperparameters + Pipeline + StandarScaler\n1. StandarScaler\n2. Pipeline\n3. SGD Regressor\n\n##### Notice that in this part the hyperparameter for Lasso, Ridge and ElasticNet will be extracted from the dictionary best_parametes\n- it looks like this \n* {'ElasticNetCV': 0.0005,\n 'ElasticNetCV_ratio': 0.1,\n 'LassoCV': 0.0001,\n 'RidgeCV': 40.000000000000014}","e489fc0f":"This notebook was inspired by IBM Machine learning Courses , where I learned lot of algorithms - regression - hyperparameters and the importance of using the right algorithm for the data that we have ..!!!\n\nHere there are some suggestions to those who want to take this notebook \n1. Find better parameters for Lasso, Ridge \n2. Adjust the Learning rate and Max_int in SGD\n3. ANN - find better drop out \/ epochs \/ batch size to improve \/ prevent overfitting \n4. Try another normalizatin such as MaxMin ","be089cb1":"### Explore\n1. Find trend and relation between price and other factors that might rise the prices \n2. Identify Ouliers \n3. Create Visualization that helps stakesholders to better understand the model ","fcaf9ed4":"### Scrub\n1. In this section the main objective is get familiar with the data set, make sure everything is standardized for Machine Learning\n2. We have to check date time, string, categorical values, continues values , etc\n\n--------------------\nNotice that \n- The column ID  is not important because it wont give any important contribution instead it will lead into Overfitting therefore we drop it\n- Columns Date must be transformed into Date time format and divided by year\/ month \n- We must deal with Long and Lat (Geographical Location)","084d60f3":"#### Finding Ouliers\n\nIn here we use Unsupervised Machine Learning - LocalOutlierFactor -\nLocalOutlierFactor : this algorihtm will check each point E.x Price = 20000  and will look for 20 prices near 20000 if it finds those 20 prices it will keep the the point otherwise it will be tag as outlier, of course this will happend in high dimensional space ","cbca00c9":"### In this notebook you will find:\n1. step by step Scrub methodology\n2. Outlier Detection\n3. Normalization\n4. Regression Algorithms (Lasso, Ridge, Linear Regression , SGB )","932aee4a":"### Model\n\n1. Find outliers - those houses that are super expensive \/ super cheap \n2. Apply some statical techniques to transforme some columns \n- For example bathroom = 2.5 we cannot have 2.5 bathrooms we have 2 or 3 \n- Another example could be how can we compare house with basement vs no basement\n- normallized those columns that dont have normal distribution (check some videos on youtube)\n3. Define the ML algorithm that we will use to predict the price","07566df5":"#### Finding Coorelation between variables\n\n1. We will try to reduce the curse of dimmensionality by dropping those columns that are related to each other ","fb6c114c":"### Hyperparameters + Pipeline + StandarScaler\n1. StandarScaler\n2. Pipeline\n3. Lasso ,Ridge , ElasticNet Hyperparameters","e0bacae6":"##### notice that SGD regressor can be set using all those algorithms Linear Regression ~ Elasticnet... The benefit of having this SGD is that we can adjust the max_iter 'which is the iteraction \/ running time on the data\n\n##### also notice that from here we can use GridsearchCv to find better parameters, adjust learning rate , interception and other ... from here is up to you ","f7c00318":"#### Algorithms\n\n1. Base model\n\n  *   Linear Regression\n  *   Lasso\n  *   Rige\n  *   Stochastic Gradient Regressor\n\n2. Scaling and Hyperparameters\n3. Mesuaring Performance of Artificial Neural Newtwork - regressor - ","9e957166":"### Baseline model\n1. No StandarScaler\n2. No Hyperparameters"}}