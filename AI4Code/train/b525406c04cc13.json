{"cell_type":{"7eaadc20":"code","7e3ce033":"code","755f70d3":"code","11a03824":"code","865e1560":"code","d635616e":"code","092a58df":"code","210aa6e7":"code","0d7fddc1":"code","f9acbb78":"code","cd969eed":"code","ccce4549":"code","0168b45b":"code","6b136e5f":"code","4f43a575":"code","30bab1f4":"code","8f89cb17":"code","128cd87d":"code","42dd6a63":"code","dc77a782":"code","48f3372f":"code","92b9b398":"code","c6e7dfb0":"code","07f71e85":"code","af6b694a":"code","370849e3":"code","9626f1ff":"code","faadb03f":"code","1bdcc1ee":"code","c57582b6":"code","e128549a":"code","4ebccbb1":"code","c59dabe2":"code","1d2ebe2d":"code","467a5544":"code","d294fa18":"code","168c2dbb":"code","632aa6a7":"code","4b7a2da5":"code","b1734a57":"code","f80708f4":"code","017541c3":"code","b0be52a9":"code","a676f2b4":"code","f11c1483":"code","ff8581cc":"markdown","8819c616":"markdown","056cf15b":"markdown","9dd7baea":"markdown","610b66b9":"markdown","a9589e37":"markdown","58fa048e":"markdown","9b2a56ce":"markdown","7bda0830":"markdown","46294af4":"markdown","89e506b1":"markdown","6af93c90":"markdown","b39dcc4e":"markdown","5317b89d":"markdown"},"source":{"7eaadc20":"!pip install numpy pandas tqdm sklearn transformers\n!pip install sentencepiece ","7e3ce033":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nimport json\nimport tensorflow as tf\nimport torch\nimport matplotlib.pyplot as plt\nfrom numpy.random import random\nfrom transformers import AutoTokenizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom tqdm import tqdm\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    tpu = False","755f70d3":"!cp ..\/input\/weight-model-xlm\/model_xlm.h5 .\/","11a03824":"def prepare_dataset(PATH,convert2bio=False):\n  sents = []\n  chunks = open(PATH,'r').read().split('\\n\\n')\n  for chunk in chunks:\n    lines = chunk.split('\\n')\n    sent = []\n    current_tag = None\n    previous_tag = None\n    for line in lines:\n        if line != '':\n            token = line.split('\\t')\n            previous_tag = current_tag \n            current_tag = token[1]\n            if convert2bio:\n                if previous_tag == current_tag and current_tag != 'O':\n                    sent.append((token[0],'I-'+token[1]))\n                elif previous_tag != current_tag and current_tag != 'O':\n                    sent.append((token[0],'B-'+token[1]))\n                else:\n                    sent.append((token[0],token[1]))\n            else:\n                sent.append((token[0],token[1]))\n    sents.append(sent)\n  return sents","865e1560":"samples_train = prepare_dataset('..\/input\/ner-ontonotes5\/ner_train.txt',convert2bio=True)\nsamples_valid = prepare_dataset('..\/input\/ner-ontonotes5\/ner_valid.txt',convert2bio=True)\nsamples_test = prepare_dataset('..\/input\/ner-ontonotes5\/ner_test.txt',convert2bio=True)\n\nsamples = samples_train + samples_test + samples_valid\nschema = ['_'] + sorted({tag for sentence in samples \n                             for _, tag in sentence})","d635616e":"def tokenize_sample(sample):\n  seq = [\n            (subtoken, tag)\n            for token, tag in sample\n            for subtoken in tokenizer(token)['input_ids'][1:-1]\n        ]\n  return [(3, 'O')] + seq + [(4, 'O')]\n\ndef preprocess(samples,schema,max_len_sent=None):\n    if max_len_sent != None:\n      reduced_samples = []\n      for sample in samples:\n        if len(sample) < max_len_sent:\n          reduced_samples.append(sample)\n    else:\n      reduced_samples = samples\n    \n    tag_index = {tag: i for i, tag in enumerate(schema)}\n    tokenized_samples = list(tqdm(map(tokenize_sample, reduced_samples)))\n    max_len = max(map(len, tokenized_samples))\n    X = np.zeros((len(samples), max_len), dtype=np.int32)\n    y = np.zeros((len(samples), max_len), dtype=np.int32)\n    for i, sentence in enumerate(tokenized_samples):\n        for j, (subtoken_id, tag) in enumerate(sentence):\n            X[i, j] = subtoken_id\n            y[i,j] = tag_index[tag]\n    return X, y","092a58df":"import numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nimport json\nimport tensorflow as tf\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig, TFXLMRobertaForTokenClassification\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    tpu = True\nexcept:\n    tpu = False","210aa6e7":"MODEL_NAME = 'jplu\/tf-xlm-roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","0d7fddc1":"X_valid, y_valid = preprocess(samples_valid,schema)\nX_train, y_train = preprocess(samples_train,schema)\nX_test, y_test = preprocess(samples_test,schema)","f9acbb78":"from transformers import TFAutoModelForTokenClassification\nNR_EPOCHS=2\nBATCH_SIZE=4\ncheckpoint_filepath = '.\/checkpoint\/'\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    save_freq = 'epoch',\n    monitor='val_accuracy',\n    save_best_only=False)\n\nif tpu:\n    print('with TPU')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    checkpoints_cb = tf.keras.callbacks.ModelCheckpoint('.\/checkpoints', options=save_locally)\n    with tpu_strategy.scope():\n      config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))\n      model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n                                                            config=config)\n      \n      optimizer = tf.keras.optimizers.Adam(learning_rate=3e-6)\n      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n      model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n      history = model.fit(tf.constant(X_train), tf.constant(y_train),\n                      validation_data=(X_val,y_val), epochs=NR_EPOCHS, \n                      batch_size=BATCH_SIZE,callbacks=[checkpoints_cb])\nelse:\n  print('w\\o TPU')\n  model = TFXLMRobertaForTokenClassification.from_pretrained(MODEL_NAME,num_labels=len(schema))\n  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-6)\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n  model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n  history = model.fit(tf.constant(X_train), tf.constant(y_train),\n                      validation_data=(X_val,y_val), epochs=NR_EPOCHS)\n\nmodel.save_weights('.\/model_xlm.h5')","cd969eed":"model = TFXLMRobertaForTokenClassification.from_pretrained(MODEL_NAME,num_labels=len(schema))\nmodel.load_weights(r'.\/model_xlm.h5')","ccce4549":"def aggregate(sample,schema,predictions):\n    results = []\n    i = 1\n    for token, y_true in sample:\n        nr_subtoken = len(tokenizer(token)['input_ids']) - 2\n        pred = predictions[i:i+nr_subtoken]\n        i += nr_subtoken\n        y_pred = schema[np.argmax(np.sum(pred, axis=0))]\n        results.append((token, y_true, y_pred))\n    return results","0168b45b":"y_probs = model.predict(X_test)[0]\npredictions = [aggregate(t,schema,p) for t, p in zip(samples_test, y_probs)]","6b136e5f":"predictions[17]","4f43a575":"predictions = [aggregate(t,schema,predictions) for t, predictions in zip(samples_test, y_probs)]\n\ny = []\ny_hat = []\nfor pred in predictions:\n  for token in pred:\n    y.append(token[1])\n    y_hat.append(token[2])\nprint('micro f1:',f1_score(y,y_hat,average='micro'))\nprint('macro f1:',f1_score(y,y_hat,average='macro'))\nprecision, recall, fscore, support = score(y,y_hat)\nprint('precision: {}\\n'.format(precision))\nprint('recall: {}\\n'.format(recall))\nprint('fscore: {}\\n'.format(fscore))\nprint('support: {}\\n'.format(support))","30bab1f4":"test_ru = prepare_dataset('..\/input\/test-ner-dataset\/ner_factrueval2016.txt',convert2bio=True)\nX_test_ru, y_test_ru = preprocess(test_ru,schema)","8f89cb17":"test_ru[0]","128cd87d":"y_probs_ru = model.predict(X_test_ru)[0]\npredictions_ru = [aggregate(t,schema,predictions) for t, predictions in zip(test_ru, y_probs_ru)]","42dd6a63":"predictions_ru[0]","dc77a782":"y = []\ny_hat = []\nfor pred in predictions_ru:\n  for token in pred:\n    y.append(token[1])\n    y_hat.append(token[2])\nprint('micro f1:',f1_score(y,y_hat,average='micro'))\nprint('macro f1:',f1_score(y,y_hat,average='macro'))\nprecision, recall, fscore, support = score(y,y_hat)\nprint('precision: {}\\n'.format(precision))\nprint('recall: {}\\n'.format(recall))\nprint('fscore: {}\\n'.format(fscore))\nprint('support: {}\\n'.format(support))","48f3372f":"test_gc = prepare_dataset('..\/input\/test-ner-dataset\/test_elNER18.txt',convert2bio=False)\nX_test_gc, y_test_gc = preprocess(test_gc,schema)","92b9b398":"test_gc[0]","c6e7dfb0":"y_probs_gc = model.predict(X_test_gc)[0]\npredictions_gc = [aggregate(t,schema,predictions) for t, predictions in zip(test_gc, y_probs_gc)]","07f71e85":"predictions_gc[2]","af6b694a":"y = []\ny_hat = []\nfor pred in predictions_gc:\n  for token in pred:\n    y.append(token[1])\n    y_hat.append(token[2])\nlen(y_hat)\nprint('micro f1:',f1_score(y,y_hat,average='micro'))\nprint('macro f1:',f1_score(y,y_hat,average='macro'))\nprecision, recall, fscore, support = score(y,y_hat)\nprint('precision: {}\\n'.format(precision))\nprint('recall: {}\\n'.format(recall))\nprint('fscore: {}\\n'.format(fscore))\nprint('support: {}\\n'.format(support))","370849e3":"test_tk = prepare_dataset('..\/input\/test-ner-dataset\/ner_turk.txt',convert2bio=False)\nX_test_tk, y_test_tk = preprocess(test_tk,schema)\n","9626f1ff":"test_tk[65]","faadb03f":"y_probs_tk = model.predict(X_test_tk)[0]\npredictions_tk = [aggregate(t,schema,predictions) for t, predictions in zip(test_tk, y_probs_tk)]","1bdcc1ee":"predictions_tk[2]","c57582b6":"y = []\ny_hat = []\nfor pred in predictions_tk:\n  for token in pred:\n    y.append(token[1])\n    y_hat.append(token[2])\nlen(y_hat)\nprint('micro f1:',f1_score(y,y_hat,average='micro'))\nprint('macro f1:',f1_score(y,y_hat,average='macro'))\nprecision, recall, fscore, support = score(y,y_hat)\nprint('precision: {}\\n'.format(precision))\nprint('recall: {}\\n'.format(recall))\nprint('fscore: {}\\n'.format(fscore))\nprint('support: {}\\n'.format(support))","e128549a":"test_de = prepare_dataset('..\/input\/test-ner-dataset\/de_ner_test.txt',convert2bio=False)\nX_test_de, y_test_de = preprocess(test_de,schema)\ny_probs_de = model.predict(X_test_de)[0]\npredictions_de = [aggregate(t,schema,predictions) for t, predictions in zip(test_de, y_probs_de)]","4ebccbb1":"y = []\ny_hat = []\nfor pred in predictions_de:\n  for token in pred:\n    y.append(token[1])\n    y_hat.append(token[2])\nlen(y_hat)\nprint('micro f1:',f1_score(y,y_hat,average='micro'))\nprint('macro f1:',f1_score(y,y_hat,average='macro'))\nprecision, recall, fscore, support = score(y,y_hat)\nprint('precision: {}\\n'.format(precision))\nprint('recall: {}\\n'.format(recall))\nprint('fscore: {}\\n'.format(fscore))\nprint('support: {}\\n'.format(support))","c59dabe2":"tags = ['PERSON','ORG','DATE']\nlanguages = ['ru','de','tk','gc']\nsamples = {}\nfor lang in languages:\n  samples[lang] = {}\n  for tag in tags:\n    samples[lang][tag] = []\n\nfor lang,predictions in zip(languages,[predictions_ru,predictions_de,predictions_tk,predictions_gc]): \n  for sample in predictions:\n    for token in sample:\n      if token[2] != 'O' and token[2] != '_' and token[1] == token[2]:\n        for tag in tags:\n          if tag in token[1]:\n            samples[lang][tag].append((token[0],token[2]))","1d2ebe2d":"def mean_pooling(model_output, attention_mask):\n    attention_mask = torch.Tensor(attention_mask.numpy())\n    token_embeddings = torch.Tensor(model_output[0].numpy()) #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings \/ sum_mask","467a5544":"def get_phrases(tokens):\n  phrases = {}\n  for lang in tokens.keys():\n    phrases[lang] = {}\n    for tag in tokens[lang].keys():\n      phrases[lang][tag] = []\n      for i in range(len(tokens[lang][tag])):\n        phrase = ''\n        if 'B-' in tokens[lang][tag][i][1]:\n          phrase += tokens[lang][tag][i][0] + ' '\n          for j in range(i+1,len(tokens[lang][tag])):\n            if 'I-' in tokens[lang][tag][j][1]:\n              phrase += tokens[lang][tag][j][0] + ' '\n            if 'B-' in tokens[lang][tag][j][1]:\n              break\n        if phrase != '':\n          phrases[lang][tag].append(phrase.strip(' '))\n      phrases[lang][tag] = set(phrases[lang][tag])\n  return phrases","d294fa18":"phrases = get_phrases(samples)","168c2dbb":"#import streamlit as st\ndef scatter(points,color_map,labels_tags,method):\n  '''\n  points : 2d numpy.array\n    embeddings for phrases \n  \n  color_map : dict\n    map : colors to tags\n\n  '''\n\n  if method == 'PCA':\n    two_dim = PCA(random_state=0).fit_transform(points)[:,:2]\n  else:\n    #perplexity = st.sidebar.slider('Adjust the perplexity. The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity',\n    #5, 50, (30))\n    #learning_rate = st.sidebar.slider('Adjust the learning rate',10, 1000, (200))\n    #iteration = st.sidebar.slider('Adjust the number of iteration',250, 100000, (1000))\n    two_dim = TSNE(n_components = 2, random_state=0).fit_transform(points)[:,:2]\n  \n  color_sequence = [color_map[tag] for tag in labels_tags]\n  plt.figure(figsize=(8, 8), dpi=80)\n  x = []\n  y = []\n  for point,tag in zip(two_dim,labels_tags):\n    x.append(point[0])\n    y.append(point[1])\n  PERSON  = plt.scatter(random(0), random(0), color=color_map['PERSON'])\n  ORG = plt.scatter(random(0), random(0), color=color_map['ORG'])\n  DATE = plt.scatter(random(0), random(0), color=color_map['DATE'])\n  plt.legend((PERSON,ORG,DATE),(list(color_map.keys())))\n  plt.scatter(x, y,c=color_sequence)\n\n\n\ndef visualize_by_lang(phrases,color_map):\n  points = []\n  labels_lang = []\n  for lang in phrases.keys():\n    for tag in phrases[lang].keys():\n      for phrase in phrases[lang][tag]:\n        encoded_input_gpe = tokenizer(phrase, return_tensors=\"tf\")\n        model_output_gpe=model(**encoded_input_gpe,output_hidden_states=True)\n        points.append(mean_pooling(model_output_gpe,encoded_input_gpe['attention_mask'])[0])\n        labels_lang.append(lang)\n  points = [t.numpy() for t in points]\n  PCA_scatter(points, color_map)\n\n\ndef visualize_by_tag(phrases,color_map,method='PCA'):\n  points = []\n  labels_tags = []\n  for tag in phrases.keys():\n    for phrase in tqdm(phrases[tag]):\n      encoded_input_gpe = tokenizer(phrase, return_tensors=\"tf\")\n      model_output_gpe=model(**encoded_input_gpe,output_hidden_states=True)\n      points.append(mean_pooling(model_output_gpe,encoded_input_gpe['attention_mask'])[0])\n      labels_tags.append(tag)\n  points = [t.numpy() for t in points]\n  scatter(points, color_map,labels_tags,method)\n    \n  ","632aa6a7":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['tk'],color_map)","4b7a2da5":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['de'],color_map)","b1734a57":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['ru'],color_map)","f80708f4":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['gc'],color_map)","017541c3":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['gc'],color_map,method='TSNE')","b0be52a9":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['tk'],color_map,method='TSNE')","a676f2b4":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['ru'],color_map,method='TSNE')","f11c1483":"color_map = {'PERSON':'b','ORG':'r','DATE':'g'}\nvisualize_by_tag(phrases['de'],color_map,method='TSNE')","ff8581cc":"### GERMAN\n\nhttps:\/\/github.com\/elenanereiss\/Legal-Entity-Recognition","8819c616":"#### German","056cf15b":"#### TURKISH","9dd7baea":"## PCA METHOD","610b66b9":"### RUSSIAN\n\nhttps:\/\/github.com\/dialogue-evaluation\/factRuEval-2016\/","a9589e37":"## t-SNE METHOD","58fa048e":"### TURKISH\n\nhttps:\/\/data.mendeley.com\/datasets\/cdcztymf4k\/1","9b2a56ce":"## TEST","7bda0830":"## DEFINE AND TRAIN A MODEL\n\nI use [XLM-RoBERTa](https:\/\/arxiv.org\/pdf\/1911.02116.pdf) model from huggingface hub (https:\/\/huggingface.co\/jplu\/tf-xlm-roberta-large)","46294af4":"## VISUALIZATION OF SEMANTIC SPACE","89e506b1":"### GREEK\n\nhttps:\/\/github.com\/nmpartzio\/elNER","6af93c90":"## PREPAIR DATASET\n\nTo train xlm model, I use [Ontonotes5](https:\/\/catalog.ldc.upenn.edu\/LDC2013T19) dataset, which contains English, Arabic and Chinese languages","b39dcc4e":"### Russian","5317b89d":"### Greek"}}