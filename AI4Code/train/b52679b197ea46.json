{"cell_type":{"6d20a6d1":"code","01832642":"code","0dfb56b3":"code","ca03d722":"code","624eda3a":"code","3440bf88":"code","b1cf1adc":"code","f91e226d":"code","9fe17003":"code","ff639db7":"code","c92e3b81":"code","8f0342e0":"code","5cefe1bf":"code","0e65a344":"code","6a690593":"code","6c2e0d74":"code","a1b0b473":"code","58e81158":"code","180a3668":"code","9d117dbe":"code","f8a117d8":"code","b31afa46":"code","83112c3a":"code","b4487464":"code","dc82de88":"code","9f72dbea":"code","7a49c518":"code","f6a505f0":"code","a544e9ca":"code","a909191a":"code","c5efbec6":"code","80c2b91f":"code","962ef41f":"code","1b37bd21":"code","91fb525b":"code","1d276b65":"code","ab9fe6b3":"code","90d973c2":"code","7155433d":"code","03b32a6c":"code","9fe49129":"code","0c683135":"code","c6608587":"code","8a0ca3a1":"code","9634cca7":"code","d84258e8":"code","ef8c675e":"code","422e4774":"code","e5d71f52":"code","f332d99b":"markdown","69c98e3b":"markdown","f929539b":"markdown","624448c0":"markdown","c995a66d":"markdown","d058e415":"markdown","3ad3b72b":"markdown","35d5911e":"markdown","6ba8d301":"markdown","50a2fa80":"markdown","ff2a5872":"markdown","d5fdf422":"markdown","0006aace":"markdown","3b461096":"markdown"},"source":{"6d20a6d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","01832642":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n# Load the Boston housing dataset\n\ndf = pd.read_csv('..\/input\/housing.csv',sep=\",\")","0dfb56b3":"df.head()","ca03d722":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap = 'viridis')","624eda3a":"#it state that any column doesnt have any null value.","3440bf88":"df.describe()","b1cf1adc":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f91e226d":"sns.pairplot(df,size=2);","9fe17003":"df.corr()  #for finding best feature","ff639db7":"plt.figure(figsize=(16,10))\nsns.heatmap(df.corr(),annot=True);","c92e3b81":"X = df[['MEDV','RM','PTRATIO']] #select feature\ny = df[['LSTAT']].values   #select target var\ny = y.reshape(-1,1)","8f0342e0":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","5cefe1bf":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state=42)","0e65a344":"#create linear regression object\nlm = LinearRegression()  \n","6a690593":"#train the model using training set\nlm.fit(X_train,y_train)","6c2e0d74":"#make prediction using the training set first\ny_train_pred = lm.predict(X_train)\ny_test_pred = lm.predict(X_test)","a1b0b473":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\n#the mean squared error,lower the value better, if it is a .0 means perfect prediction\ns = mean_squared_error(y_train,y_train_pred)\nprint(\"Mean Squared error of training set :%2f\"%s)\n","58e81158":"\n#the mean squared error,lower the value better it is .0 means perfect prediction\ns = mean_squared_error(y_test,y_test_pred)\nprint(\"Mean squared error of testing set: %.2f\"%s)","180a3668":"from sklearn.metrics import r2_score\n\n# Explained variance score: 1 is perfect prediction\ns = r2_score(y_train, y_train_pred)\nprint('R2 variance score of training set: %.2f' %s )\n","9d117dbe":"\n#explained the variance score :1 is perfect prediction\ns = r2_score(y_test,y_test_pred)\nprint(\"R2 variance score of testing set: %2f\"%s)\n","f8a117d8":"#calculating adjusted r2\nN = y_test.size\np = X_train.shape[1]\nadjr2score = 1 - ((1-r2_score(y_test, y_test_pred))*(N - 1))\/ (N - p - 1)\nprint(\"Adjusted R^2 Score %.2f\" % adjr2score)","b31afa46":"#import polynomial package\nfrom sklearn.preprocessing import PolynomialFeatures","83112c3a":"#creat a polynomial regression model for the given degree=2\npoly_reg = PolynomialFeatures(degree = 2)","b4487464":"#transform the existing feature to high degree features.\nX_train_poly = poly_reg.fit_transform(X_train)\nX_test_poly = poly_reg.fit_transform(X_test)","dc82de88":"#fit the transform features to linear regression\nlin_reg_2 = LinearRegression()\nlin_reg_2.fit(X_train_poly,y_train)\n","9f72dbea":"#predicting on training data set \ny_train_predict = lin_reg_2.predict(X_train_poly)\n#predicting on testing data set\ny_test_predict = lin_reg_2.predict(X_test_poly)","7a49c518":"#ealuating the model on train dataset\nrmse_train = np.sqrt(mean_squared_error(y_train,y_train_predict))\nr2_train = r2_score(y_train,y_train_predict)\nprint(\"The model performance of training set\")\nprint(\"---------------------------------------------\")\nprint(\"RMSE of training set is{}\".format(rmse_train))\nprint(\"R2 score of training set is{}\".format(r2_train))","f6a505f0":"#evaluating model on test dataset\nrmse_test = np.sqrt(mean_squared_error(y_test,y_test_predict))\nr2_test = r2_score(y_test,y_test_predict)\n\nprint(\"The model performance of training set\")\nprint(\"-----------------------------------------------\")\nprint(\"RMSE of testing set is{}\".format(rmse_test))\nprint(\"R2 score of testing set is{}\".format(r2_test))","a544e9ca":"#import polynomial package\nfrom sklearn.preprocessing import PolynomialFeatures","a909191a":"#creat a polynomial regression model for the given degree=3\npoly_reg = PolynomialFeatures(degree = 3)","c5efbec6":"#transform the existing feature to high degree features.\nX_train_poly = poly_reg.fit_transform(X_train)\nX_test_poly = poly_reg.fit_transform(X_test)","80c2b91f":"#fit the transform features to linear regression\nlin_reg_3 = LinearRegression()\nlin_reg_3.fit(X_train_poly,y_train)\n","962ef41f":"#predicting on training data set \ny_train_predict = lin_reg_3.predict(X_train_poly)\n#predicting on testing data set\ny_test_predict = lin_reg_3.predict(X_test_poly)","1b37bd21":"#ealuating the model on train dataset\nrmse_train = np.sqrt(mean_squared_error(y_train,y_train_predict))\nr2_train = r2_score(y_train,y_train_predict)\nprint(\"The model performance of training set\")\nprint(\"----------------------------------------------\")\nprint(\"RMSE of training set is{}\".format(rmse_train))\nprint(\"R2 score of training set is{}\".format(r2_train))","91fb525b":"#evaluating model on test dataset\nrmse_test = np.sqrt(mean_squared_error(y_test,y_test_predict))\nr2_test = r2_score(y_test,y_test_predict)\n\nprint(\"The model performance of testing set\")\nprint(\"--------------------------------------------\")\nprint(\"RMSE of testing set is{}\".format(rmse_test))\nprint(\"R2 score of testing set is{}\".format(r2_test))","1d276b65":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nX_std = sc_x.fit_transform(X)\ny_std = sc_y.fit_transform(y.reshape(-1,1)).flatten()","ab9fe6b3":"X_std.shape","90d973c2":"import numpy as np\nalpha = 0.0001    #learning rate\nw_ = np.zeros(1 + X_std.shape[1])    \ncost_ = [] \nn_ = 100\n \nfor i in range(n_):\n    y_pred = np.dot(X_std,w_[1:] + w_[0])\n    errors  = (y_std - y_pred)\n    \n    w_[1:] +=alpha * X_std.T.dot(errors)   #theta1\n    w_[0] +=alpha *errors.sum()        #theta0\n    \n    cost = (errors**2).sum() \/ 2.0\n    cost_.append(cost)","7155433d":"plt.figure(figsize=(10,8))  #plot the figure\nplt.plot(range(1,n_ + 1),cost_);\nplt.ylabel('SSE');\nplt.xlabel('Epoch');","03b32a6c":"w_   #gradient function (intercept and coeficient) ","9fe49129":"#accuracy of gradient function\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (w_.mean(), w_.std() * 2))\n","0c683135":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error,r2_score","c6608587":"df.head()","8a0ca3a1":"svr = SVR(kernel='linear')\nsvr.fit(X_train, y_train)","9634cca7":"y_train_pred = svr.predict(X_train)","d84258e8":"y_test_pred = svr.predict(X_test)","ef8c675e":"print(\"MSE train: {0:.4f},test: {1:.4f}\".\\\n     format(mean_squared_error(y_train,y_train_pred),\n           mean_squared_error(y_test,y_test_pred)))","422e4774":"print(\"R^2 train: {0:.4f}, test: {1:.4f}\".\\\n      format(r2_score(y_train, y_train_pred),\n             r2_score(y_test, y_test_pred)))","e5d71f52":"svr = SVR(kernel='poly', C=1e3, degree=2)\nsvr.fit(X_train, y_train)","f332d99b":"# Exploratory Data Analysis(EDA)","69c98e3b":"#Polynomial","f929539b":"# Quadratic ","624448c0":"# Polynomial Regression","c995a66d":"# Applying Gradient Descent","d058e415":"# Correlation Analysis for Feature Selection","3ad3b72b":"# Applying Support Vector Machin(SVM)","35d5911e":"# Checking if any column have null data","6ba8d301":"\nIn this first section of this project, we will make a cursory investigation about the Boston housing data and provide our observations. Familiarizing ourself with the data through an explorative process is a fundamental practice to help us better understand and justify our results.\n\nSince the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into **features and the target variable.** The features, **'RM', 'LSTAT','PTRATIO'**, give us quantitative information about each data point,they are stored in **'y'** variable. The target variable, **'MEDV'**, will be the variable we seek to predict. These are stored in **'X'** variable.\n\n","50a2fa80":"\n**Feature Observation**:\n           As a reminder, we are using three features from the Boston housing dataset: 'RM', 'LSTAT', and 'PTRATIO'. For each data point (neighborhood):\n\n**'RM'** is the average number of rooms among homes in the neighborhood.\n**'LSTAT'** is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor).\n**'PTRATIO'** is the ratio of students to teachers in primary and secondary schools in the neighborhood","ff2a5872":"# Visualization of data","d5fdf422":"# Describing housing_data for statistic metrics","0006aace":"# Applying Scikit learn Linear Regression based on 3 independent columns 'RM','LSAT','PTRATIO' to predict value of dependent variable 'MEDV'\n","3b461096":"In this first section of this project, we will make a cursory investigation about the Boston housing data and provide our observations. Familiarizing ourself with the data through an explorative process is a fundamental practice to help us better understand and justify our results.\n"}}