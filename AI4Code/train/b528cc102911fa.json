{"cell_type":{"b2978290":"code","836a197c":"code","d0578ef5":"code","c0ca39cc":"code","87d4210d":"code","e3a927d3":"code","97d7c87f":"code","4fb530de":"code","b0c37084":"code","1a07468a":"code","dda09850":"code","91a5d7c8":"code","cedea829":"code","287617e7":"code","31f1e9ae":"code","c3cdb536":"code","868ea977":"code","f409af0b":"code","da84a054":"code","4515f395":"code","b4b84655":"code","a7d1d17f":"code","3b7e9369":"code","5de57b06":"code","ff26c88a":"code","3dee1f49":"code","3e73ed86":"code","5adf96b9":"code","0d273eae":"code","5d83365b":"code","1d052579":"code","f7d69762":"code","8cd0703c":"code","de5a20f9":"code","cd53055c":"code","ebd0fef2":"code","bc8ae37c":"code","2bb5acd3":"code","c82908c1":"code","b3a6a910":"code","1a623368":"code","8b6ad604":"code","1f74b57e":"code","46f1f14e":"code","02775c52":"code","3180332d":"code","7aa50fc9":"code","1aaea7e1":"code","6325260b":"code","4d711652":"code","d78a12c6":"code","c9feeedf":"code","31dda025":"code","1b08b414":"code","b69b3bca":"code","dc8466f3":"code","d7d11f0b":"code","d30626fd":"code","9fbe7c4b":"code","f20ac7e5":"code","d5713025":"code","293d6178":"code","1204e727":"code","d8df5af6":"code","176c4be1":"code","a56bdd50":"code","5956c390":"code","76e1d04c":"code","f414e1c1":"code","99ea854c":"code","d7d754f6":"code","229007a7":"code","9b3aa32f":"code","a6894a38":"code","71afe8b4":"code","ebd7d0c3":"code","d8f0c482":"code","ad2e2886":"code","d5f71593":"code","68a2074f":"code","bbc3cf94":"code","4e69e644":"code","4e2c0909":"code","e0985910":"code","b6b76dde":"code","3fe4c1db":"code","7146b059":"code","503c52bd":"code","f3412813":"code","677d5baa":"code","8d760e91":"markdown","a7bc6045":"markdown","1411d72e":"markdown","f4721c84":"markdown","8f19a874":"markdown","2261aa54":"markdown","a73260ed":"markdown","761b1768":"markdown","cf2c86b9":"markdown","cb80b888":"markdown","b90063bf":"markdown","fc82050f":"markdown","89e55d11":"markdown","eb8bbe6d":"markdown","a16c921f":"markdown","7426e0d9":"markdown","bf830d93":"markdown","97117150":"markdown","340c72c1":"markdown","19f377b0":"markdown","c5d15345":"markdown","eca9a8ee":"markdown","37543891":"markdown","d87c4c81":"markdown","37e5e808":"markdown","8e9613b9":"markdown","8805bffb":"markdown","0900678c":"markdown","439f569d":"markdown","da07d48a":"markdown","99b8f73f":"markdown","f46f7208":"markdown","43cf7333":"markdown","820f4453":"markdown","13c641a2":"markdown","1a00e063":"markdown"},"source":{"b2978290":"!pip install numpy pandas matplotlib seaborn --quiet","836a197c":"!pip install jovian opendatasets xgboost graphviz lightgbm scikit-learn xgboost lightgbm --upgrade --quiet","d0578ef5":"import os\nimport opendatasets as od\nimport pandas as pd\npd.set_option(\"display.max_columns\", 120)\npd.set_option(\"display.max_rows\", 120)","c0ca39cc":"od.download('https:\/\/www.kaggle.com\/c\/rossmann-store-sales')","87d4210d":"os.listdir('rossmann-store-sales')","e3a927d3":"ross_df = pd.read_csv('.\/rossmann-store-sales\/train.csv', low_memory=False)\nstore_df = pd.read_csv('.\/rossmann-store-sales\/store.csv')\ntest_df = pd.read_csv('.\/rossmann-store-sales\/test.csv')\nsubmission_df = pd.read_csv('.\/rossmann-store-sales\/sample_submission.csv')","97d7c87f":"ross_df","4fb530de":"test_df","b0c37084":"submission_df","1a07468a":"store_df","dda09850":"merged_df = ross_df.merge(store_df, how='left', on='Store')\nmerged_test_df = test_df.merge(store_df, how='left', on='Store')","91a5d7c8":"merged_df","cedea829":"merged_df.info()","287617e7":"def split_date(df):\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Year'] = df.Date.dt.year\n    df['Month'] = df.Date.dt.month\n    df['Day'] = df.Date.dt.day\n    df['WeekOfYear'] = df.Date.dt.isocalendar().week","31f1e9ae":"split_date(merged_df)\nsplit_date(merged_test_df)","c3cdb536":"merged_df","868ea977":"merged_df[merged_df.Open == 0].Sales.value_counts()","f409af0b":"merged_df = merged_df[merged_df.Open == 1].copy()","da84a054":"def comp_months(df):\n    df['CompetitionOpen'] = 12 * (df.Year - df.CompetitionOpenSinceYear) + (df.Month - df.CompetitionOpenSinceMonth)\n    df['CompetitionOpen'] = df['CompetitionOpen'].map(lambda x: 0 if x < 0 else x).fillna(0)","4515f395":"comp_months(merged_df)\ncomp_months(merged_test_df)","b4b84655":"merged_df","a7d1d17f":"merged_df[['Date', 'CompetitionDistance', 'CompetitionOpenSinceYear', 'CompetitionOpenSinceMonth', 'CompetitionOpen']].sample(20)","3b7e9369":"def check_promo_month(row):\n    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',              \n                 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n    try:\n        months = (row['PromoInterval'] or '').split(',')\n        if row['Promo2Open'] and month2str[row['Month']] in months:\n            return 1\n        else:\n            return 0\n    except Exception:\n        return 0\n\ndef promo_cols(df):\n    # Months since Promo2 was open\n    df['Promo2Open'] = 12 * (df.Year - df.Promo2SinceYear) +  (df.WeekOfYear - df.Promo2SinceWeek)*7\/30.5\n    df['Promo2Open'] = df['Promo2Open'].map(lambda x: 0 if x < 0 else x).fillna(0) * df['Promo2']\n    # Whether a new round of promotions was started in the current month\n    df['IsPromo2Month'] = df.apply(check_promo_month, axis=1) * df['Promo2']","5de57b06":"promo_cols(merged_df)\npromo_cols(merged_test_df)","ff26c88a":"merged_df[['Date', 'Promo2', 'Promo2SinceYear', 'Promo2SinceWeek', 'PromoInterval', 'Promo2Open', 'IsPromo2Month']].sample(20)","3dee1f49":"merged_df.columns","3e73ed86":"input_cols = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday', \n              'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpen', \n              'Day', 'Month', 'Year', 'WeekOfYear',  'Promo2', \n              'Promo2Open', 'IsPromo2Month']\ntarget_col = 'Sales'","5adf96b9":"inputs = merged_df[input_cols].copy()\ntargets = merged_df[target_col].copy()","0d273eae":"test_inputs = merged_test_df[input_cols].copy()","5d83365b":"numeric_cols = ['Store', 'Promo', 'SchoolHoliday', \n              'CompetitionDistance', 'CompetitionOpen', 'Promo2', 'Promo2Open', 'IsPromo2Month',\n              'Day', 'Month', 'Year', 'WeekOfYear',  ]\ncategorical_cols = ['DayOfWeek', 'StateHoliday', 'StoreType', 'Assortment']","1d052579":"inputs[numeric_cols].isna().sum()","f7d69762":"test_inputs[numeric_cols].isna().sum()","8cd0703c":"max_distance = inputs.CompetitionDistance.max()","de5a20f9":"inputs['CompetitionDistance'].fillna(max_distance, inplace=True)\ntest_inputs['CompetitionDistance'].fillna(max_distance, inplace=True)","cd53055c":"from sklearn.preprocessing import MinMaxScaler","ebd0fef2":"scaler = MinMaxScaler().fit(inputs[numeric_cols])","bc8ae37c":"inputs[numeric_cols] = scaler.transform(inputs[numeric_cols])\ntest_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])","2bb5acd3":"from sklearn.preprocessing import OneHotEncoder","c82908c1":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(inputs[categorical_cols])\nencoded_cols = list(encoder.get_feature_names(categorical_cols))","b3a6a910":"inputs[encoded_cols] = encoder.transform(inputs[categorical_cols])\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])","1a623368":"X = inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]","8b6ad604":"from xgboost import XGBRegressor","1f74b57e":"model = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=20, max_depth=4)","46f1f14e":"%%time\nmodel.fit(X, targets)","02775c52":"preds = model.predict(X)","3180332d":"preds","7aa50fc9":"from sklearn.metrics import mean_squared_error\n\ndef rmse(a, b):\n    return mean_squared_error(a, b, squared=False)","1aaea7e1":"rmse(preds, targets)","6325260b":"import matplotlib.pyplot as plt\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n%matplotlib inline\n\nrcParams['figure.figsize'] = 30,30","4d711652":"plot_tree(model, rankdir='LR');","d78a12c6":"plot_tree(model, rankdir='LR', num_trees=1);","c9feeedf":"plot_tree(model, rankdir='LR', num_trees=19);","31dda025":"trees = model.get_booster().get_dump()","1b08b414":"len(trees)","b69b3bca":"print(trees[0])","dc8466f3":"importance_df = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)","d7d11f0b":"importance_df.head(10)","d30626fd":"import seaborn as sns\nplt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","9fbe7c4b":"from sklearn.model_selection import KFold","f20ac7e5":"def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n    model = XGBRegressor(random_state=42, n_jobs=-1, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    return model, train_rmse, val_rmse","d5713025":"kfold = KFold(n_splits=5)","293d6178":"models = []\n\nfor train_idxs, val_idxs in kfold.split(X):\n    X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n    X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n    model, train_rmse, val_rmse = train_and_evaluate(X_train, \n                                                     train_targets, \n                                                     X_val, \n                                                     val_targets, \n                                                     max_depth=4, \n                                                     n_estimators=20)\n    models.append(model)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","1204e727":"import numpy as np\n\ndef predict_avg(models, inputs):\n    return np.mean([model.predict(inputs) for model in models], axis=0)","d8df5af6":"preds = predict_avg(models, X)","176c4be1":"preds","a56bdd50":"model","5956c390":"def test_params_kfold(n_splits, **params):\n    train_rmses, val_rmses, models = [], [], []\n    kfold = KFold(n_splits)\n    for train_idxs, val_idxs in kfold.split(X):\n        X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n        X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n        model, train_rmse, val_rmse = train_and_evaluate(X_train, train_targets, X_val, val_targets, **params)\n        models.append(model)\n        train_rmses.append(train_rmse)\n        val_rmses.append(val_rmse)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(np.mean(train_rmses), np.mean(val_rmses)))\n    return models","76e1d04c":"from sklearn.model_selection import train_test_split","f414e1c1":"X_train, X_val, train_targets, val_targets = train_test_split(X, targets, test_size=0.1)","99ea854c":"def test_params(**params):\n    model = XGBRegressor(n_jobs=-1, random_state=42, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","d7d754f6":"test_params(n_estimators=10)","229007a7":"test_params(n_estimators=30)","9b3aa32f":"test_params(n_estimators=100)","a6894a38":"test_params(n_estimators=240)","71afe8b4":"test_params(max_depth=2)","ebd7d0c3":"test_params(max_depth=5)","d8f0c482":"test_params(max_depth=10)","ad2e2886":"test_params(n_estimators=50, learning_rate=0.01)","d5f71593":"test_params(n_estimators=50, learning_rate=0.1)","68a2074f":"test_params(n_estimators=50, learning_rate=0.3)","bbc3cf94":"test_params(n_estimators=50, learning_rate=0.9)","4e69e644":"test_params(n_estimators=50, learning_rate=0.99)","4e2c0909":"test_params(booster='gblinear')","e0985910":"model = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.2, max_depth=10, subsample=0.9, \n                     colsample_bytree=0.7)","b6b76dde":"%%time\nmodel.fit(X, targets)","3fe4c1db":"test_preds = model.predict(X_test)","7146b059":"submission_df['Sales']  = test_preds","503c52bd":"test_df.Open.isna().sum()","f3412813":"submission_df['Sales'] = submission_df['Sales'] * test_df.Open.fillna(1.)","677d5baa":"submission_df","8d760e91":"Clearly, a linear model is not well suited for this dataset.","a7bc6045":"### Visualization\n\nVisualize individual trees using `plot_tree` (note: this requires the `graphviz` library to be installed).","1411d72e":"We can now use `predict_avg` to make predictions for the test set.","f4721c84":"## Putting it Together and Making Predictions\n\nLet's train a final model on the entire training set with custom hyperparameters. ","8f19a874":"### Evaluation\n\nLet's evaluate the predictions using RMSE error.","2261aa54":"#### `n_estimators`\n\nThe number of trees to be created. More trees = greater capacity of the model.\n","a73260ed":"Now that the model is trained, we can make predictions on the test set.","761b1768":"#### `max_depth`\n\nAs you increase the max depth of each tree, the capacity of the tree increases and it can capture more information about the training set.","cf2c86b9":"### Store Open\/Closed\n","cb80b888":"Let's define a helper function `train_and_evaluate` which trains a model the given parameters and returns the trained model, training error and validation error.","b90063bf":"### Training\n\nTo train a GBM, we can use the `XGBRegressor` class from the [`XGBoost`](https:\/\/xgboost.readthedocs.io\/en\/latest\/) library.","fc82050f":"## Downloading the Data\n\nWe can download the dataset from Kaggle directly within the Jupyter notebook using the `opendatasets` library.","89e55d11":"### Prediction\n\nWe can now make predictions and evaluate the model using `model.predict`.","eb8bbe6d":"- Downloading a real-world dataset from a Kaggle competition\n- Performing feature engineering and prepare the dataset for training\n- Training and interpreting a gradient boosting model using XGBoost\n- Training with KFold cross validation and ensembling results\n- Configuring the gradient boosting model and tuning hyperparamters","a16c921f":"#### `learning_rate`\n\nThe scaling factor to be applied to the prediction of each tree. A very high learning rate (close to 1) will lead to overfitting, and a low learning rate (close to 0) will lead to underfitting.","7426e0d9":"### Scale Numeric Values\n\nLet's scale numeric values to the 0 to 1 range.","bf830d93":"### Encode Categorical Columns\n\n<img src=\"https:\/\/i.imgur.com\/n8GuiOO.png\" width=\"640\">\n\nLet's one-hot encode categorical columns.","97117150":"Let's also define a function to average predictions from the 5 different models.","340c72c1":"# Gradient Boosting Machines (GBMs) with XGBoost\n\n\n![](https:\/\/i.imgur.com\/6MYc56a.png)\n","19f377b0":"Let's train the model using `model.fit`.","c5d15345":"Here's a helper function to test hyperparameters with K-fold cross validation.","eca9a8ee":"Now, we can use the `KFold` utility to create the different training\/validations splits and train a separate model for each fold.","37543891":"\n### Date\n\nFirst, let's convert `Date` to a `datecolumn` and extract different parts of the date.","d87c4c81":"### Impute missing numerical data","37e5e808":"### Feature importance\n\nJust like decision trees and random forests, XGBoost also provides a feature importance score for each column in the input.","8e9613b9":"Let's merge the information from `store_df` into `train_df` and `test_df`.","8805bffb":"### Additional Promotion","0900678c":"Scikit-learn provides utilities for performing K fold cross validation.","439f569d":"We haven't created a validation set yet, because we'll use K-fold cross validation.","da07d48a":"Let's load the data into Pandas dataframes.","99b8f73f":"Finally, let's extract out all the numeric data for training.","f46f7208":"## Preprocessing and Feature Engineering","43cf7333":"#### `booster`\n\nInstead of using Decision Trees, XGBoost can also train a linear model for each iteration. This can be configured using `booster`.","820f4453":"Let's begin by installing the required libraries.","13c641a2":"### Input and Target Columns\n\nLet's select the columns that we'll use for training.","1a00e063":"Let's add the predictions into `submission_df`."}}