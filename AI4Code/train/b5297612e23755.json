{"cell_type":{"728aa4ea":"code","4f182061":"code","575e83c0":"code","7dc21650":"code","f874feb7":"code","b64c7392":"code","57da3c3a":"code","57965df2":"code","fe188144":"code","4fcf32a3":"code","aeb0df37":"code","4f8241ca":"code","cd209bd8":"code","e25be5d9":"code","4ba77892":"code","a4294a0d":"code","4e00a7fb":"code","86576ce4":"code","b57c4fd4":"code","8b12cb9b":"code","97c9b21b":"code","32e3f11b":"code","4850bbd8":"code","dbca296e":"code","8797f9ea":"code","ce4fabe9":"code","029a34a0":"code","ef264a1b":"code","57a99e1f":"code","9cc09ed8":"code","1e85ee87":"code","0f76cb80":"code","2efb69cc":"code","cc5d1f2c":"code","b6cdee2b":"code","17d2cfaa":"code","b81716fb":"code","21d30b48":"code","49c2623d":"code","f573966a":"code","3305d355":"code","16358ab1":"code","c8dc999a":"code","2182894e":"code","d45fd443":"code","15e83009":"code","31241c09":"code","4e65c1d2":"code","41e8db35":"code","e90ce3a6":"code","b2115446":"code","85612ab9":"code","9b899198":"code","e7997a95":"code","23190aa8":"code","8a5090f9":"code","f1f84ef1":"code","a6e4d724":"code","85b31c6a":"code","241f9e04":"markdown","4d2dc94f":"markdown","76f78e44":"markdown","17dd119a":"markdown","a2bda260":"markdown","85af4502":"markdown","b36f5bc2":"markdown","be311ab9":"markdown","a1ec51cf":"markdown","9f4f9ea8":"markdown","e80431e7":"markdown","fb8f4600":"markdown","aa5f2cce":"markdown","3f1f8c4b":"markdown","0e653677":"markdown","c938169e":"markdown","b8d38f07":"markdown","cec3dd31":"markdown","ed45870d":"markdown","f5cb6a57":"markdown","c9bbd689":"markdown","9979e70a":"markdown","c6122d7d":"markdown","ac2ae339":"markdown","9b492a5e":"markdown","17ee40b9":"markdown","e4a14bc8":"markdown","0eb8de6d":"markdown","1fdc3141":"markdown","18e7b242":"markdown","c7f55e6f":"markdown","fa9d1119":"markdown","b8fa22ad":"markdown","ad299500":"markdown","c7e0d93d":"markdown","e718ea8b":"markdown","d36f9c0e":"markdown","c6814071":"markdown","ce29ec92":"markdown","b24ceab5":"markdown","91d5ef6d":"markdown","e7045742":"markdown","8d663f60":"markdown","a0178ee8":"markdown","cd43e30c":"markdown","23ac4813":"markdown","0dc70a4f":"markdown","fb27c7af":"markdown","bfec5483":"markdown","6dd64aaa":"markdown","0ff1778c":"markdown","e9988aa1":"markdown","9f3d1eeb":"markdown","4639e0a8":"markdown","c2a3cd71":"markdown","d734502c":"markdown","e003c21b":"markdown","52536895":"markdown","45ed9731":"markdown","5e36bdd2":"markdown","509a877c":"markdown","9f060f76":"markdown","b3db1332":"markdown","5ed30bcb":"markdown","e6c9bcc2":"markdown","7362a619":"markdown","dd7285bb":"markdown","46250a50":"markdown","8ef9de97":"markdown","5ddb5c58":"markdown","e6c3304e":"markdown","186ad523":"markdown","2e5dfd62":"markdown","59646963":"markdown","505d37ad":"markdown","2614c6f5":"markdown","703ecb1c":"markdown","858b1360":"markdown"},"source":{"728aa4ea":"import pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 30)\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest  = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nboth = [train,test]","4f182061":"train.head()","575e83c0":"print(\"-\"*15 +\"Train\"+\"-\"*15)\nprint(train.info())\nprint(\"-\"*15 +\"Test\"+\"-\"*15)\nprint(test.info())","7dc21650":"percent_missing = train.isnull().mean() *100\nmissing_value_df = pd.DataFrame({\"missing\":train.isnull().sum(),\n                                 'percent_missing': percent_missing})\nprint(missing_value_df)","f874feb7":"percent_missing = test.isnull().mean() *100\nmissing_value_df = pd.DataFrame({\"missing\":test.isnull().sum(),\n                                 'percent_missing': percent_missing})\nprint(missing_value_df)","b64c7392":"import missingno as msno\nmsno.matrix(train)","57da3c3a":"train.describe()","57965df2":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","fe188144":"sns.set_style(\"darkgrid\") # Style that Seaborn will use fo the figures","4fcf32a3":"sex_surv = train.groupby(\"Sex\").mean()[\"Survived\"]\nsex_surv","aeb0df37":"sex_surv =train.groupby(\"Sex\").mean()[\"Survived\"] \n\nf_1 = round(sex_surv[0] * 100,1)\nf_0 = round((1-sex_surv[0]) * 100,1)\nm_1 = round(sex_surv[1] * 100,1)\nm_0 = round((1-sex_surv[1]) * 100,1)\n\nm_surv = [m_0,m_1]\nf_surv = [f_0,f_1]\n\nmale_df = train[train.Sex==\"male\"]\nfemale_df = train[train.Sex==\"female\"]\n\nfig,ax = plt.subplots(1,2,sharey=True)\n\naxis_1 = sns.countplot(data=male_df, x=\"Sex\",hue=\"Survived\",ax=ax[0])\naxis_2 = sns.countplot(data=female_df, x=\"Sex\",hue=\"Survived\",ax=ax[1])\n\ndef percent_label(ax,surv):\n    \n    for c,p in enumerate(ax.patches[:]):\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        ax.annotate(str(surv[c])+\"%\", xy=(x,h), xytext=(0,4), \n                    textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\npercent_label(axis_1,m_surv)\npercent_label(axis_2,f_surv)\n\nfig.suptitle(\"Sex vs survival\",y=1.07, fontsize=15)\nfig.tight_layout(pad=0)\naxis_1.set_ylim(0,500)\naxis_1.legend_.remove()\naxis_2.set_ylabel(\"\")\n\nfor ax in [axis_1,axis_2]:\n    ax.set_xlabel(\"\");","4f8241ca":"fig,ax = plt.subplots(1,2,figsize=(9,5))\n\nax[0].set_title(\"Count of survival\",{'fontsize': 13},y=1.01)\nax[1].set_title(\"Survival likelihood\",{\"fontsize\": 13},y=1.01)\n\nsns.countplot(data=train,x=\"Pclass\",hue=\"Survived\",ax=ax[0])\nsns.barplot(data=train,x=\"Pclass\",y=\"Survived\",ax=ax[1]);","cd209bd8":"#fig,ax = plt.subplots()\n\nax = sns.FacetGrid(data=train,col=\"Survived\",height=4)\nax.map(sns.distplot,\"Age\",bins=16)\nax.fig.suptitle(\"Age of survival\",y=1.1,fontsize=15)\nax.set(xlim=(0,80));","e25be5d9":"ax = sns.FacetGrid( train,hue =\"Survived\",height=4,aspect=1.5 )\nax.map(sns.kdeplot, \"Age\", shade= True )\nax.set(xlim=(0 , train[\"Age\"].max()))\nax.fig.suptitle(\"Age of survival\",y=1.05,fontsize=15)\nax.add_legend();","4ba77892":"ax = sns.FacetGrid(train, row = \"Sex\", col = \"Pclass\", hue = \"Survived\")\nax.map(plt.hist, \"Age\",alpha=0.6,edgecolor=\"none\",histtype=\"stepfilled\")\nax.fig.suptitle(\"Age,sex and class vs survival\",y=1.07,fontsize=20)\nax.add_legend();","a4294a0d":"for df in both:\n    df[\"Fare_bins\"] = pd.qcut(df[\"Fare\"],4)\n\nplt.title(\"Fare vs survival\",fontsize=15)\nsns.pointplot(data=train,x='Fare_bins',y='Survived');","4e00a7fb":"embarked_df = train[['Embarked', 'Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(\"Survived\",ascending=False)\nprint(embarked_df)\n\nplt.title(\"Embarked vs survival\",fontsize=15)\nsns.barplot(data=train, x=\"Embarked\",y=\"Survived\");","86576ce4":"ax = sns.FacetGrid(train, col = \"Embarked\",height=4)\nax.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\", order=None,hue_order=None, palette = \"deep\")\nax.fig.suptitle(\"Class & Embarked\",y=1.05,fontsize=15)\nax.add_legend();","b57c4fd4":"ax = sns.FacetGrid(data=train, col=\"Pclass\",hue=\"Pclass\",height=4)\nax.map(sns.pointplot, \"Fare_bins\",\"Survived\",order=None);","8b12cb9b":"ax = sns.FacetGrid(data=train, col=\"Pclass\",hue=\"Survived\",height=4,aspect=1.3)\nax.map(sns.kdeplot,\"Age\",shade=True)\nax.set(xlim=(0 , train['Age'].max()))\nax.add_legend();","97c9b21b":"import re\n\nfor df in both:\n    df[\"Title\"] = df[\"Name\"].apply(lambda x: re.split(\"(?<=, )(.*?)(?=\\.)\",x)[1])\n\n    df[\"Title\"] = df[\"Title\"].replace([\"Don\",\"Rev\",\"Dr\",\"Major\",\"Sir\",\"Col\",\"Capt\",\"Jonkheer\",\"Lady\",\"the Countess\"],\"Unique\")\n    df[\"Title\"] = df[\"Title\"].replace(\"Dona\",\"Mrs\")\n    df[\"Title\"] = df[\"Title\"].replace([\"Ms\",\"Mlle\",\"Mme\"],\"Miss\")\n\ntitle_surv = train[['Title', 'Survived']].groupby(['Title'],as_index=False).mean().sort_values(\"Survived\",ascending=False)\n\nsns.barplot(data=train,x=\"Title\",y=\"Survived\",order=[\"Mrs\",\"Miss\",\"Master\",\"Unique\",\"Mr\"]);","32e3f11b":"ax = sns.FacetGrid(data=train,col=\"Pclass\",height=4)\nax.map(sns.barplot,\"Title\",\"Survived\",palette=\"deep\",order=None);","4850bbd8":"for df in both:\n    df[\"Alone\"] = df.apply(lambda x: 0 if x[\"SibSp\"] + x[\"Parch\"]>=1 else 1,axis=1)\n    df[\"Family_size\"] = df.apply(lambda x: x[\"SibSp\"] + x[\"Parch\"] + 1,axis=1)\n\nalone_df = train.groupby(\"Alone\").mean()[\"Survived\"]\nprint(alone_df)","dbca296e":"alone_surv =train.groupby(\"Alone\").mean()[\"Survived\"] \n\nnot_a_1 = round(alone_surv[0] * 100,1)\nnot_a_0 = round((1-alone_surv[0]) * 100,1)\na_1 = round(alone_surv[1] * 100,1)\na_0 = round((1-alone_surv[1]) * 100,1)\n\nalone_order = [not_a_0,not_a_1,a_0,a_1]\nalone_order\n\nax = sns.countplot(data=train,x=\"Alone\",hue=\"Survived\")\n\nfor c,p in enumerate(ax.patches[:]):\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            ax.annotate(str(alone_order[c])+\"%\", xy=(x,h), xytext=(0,4), \n                       textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n\nax.set_ylim(0,400)\nplt.title(\"Alone vs survival\",y=1.01,fontsize=15);","8797f9ea":"alone_sex = train.groupby(\"Sex\").mean()[\"Alone\"]\nalone_embarked = train.groupby(\"Embarked\").mean()[\"Alone\"]\nalone_class = train.groupby(\"Pclass\").mean()[\"Alone\"]\n\nprint(alone_sex,\"\\n\")\nprint(alone_embarked,\"\\n\")\nprint(alone_class,\"\\n\")\n\nax = sns.FacetGrid( train,hue =\"Alone\",height=4,aspect=2)\nax.map(sns.kdeplot, \"Age\", shade= True)\nax.set(xlim=(0,train.Age.max()))\nax.add_legend()\nax.fig.suptitle(\"Alone age\",y=1.05);","ce4fabe9":"fig,ax = plt.subplots(1,3,figsize=(20,5))\nsns.barplot(data=train,x=\"Sex\",y=\"Survived\",hue=\"Alone\",ax=ax[0])\nsns.barplot(data=train,x=\"Pclass\",y=\"Survived\",hue=\"Alone\",ax=ax[1])\nsns.barplot(data=train,x=\"Embarked\",y=\"Survived\",hue=\"Alone\",ax=ax[2]);","029a34a0":"ax = sns.pointplot(data=train,x=\"Family_size\",y=\"Survived\");","ef264a1b":"cf = sns.FacetGrid(data=train,col=\"Pclass\",height=4,hue=\"Pclass\")\ncf.map(sns.pointplot,\"Family_size\",\"Survived\",order=None)\n\nef = sns.FacetGrid(data=train,col=\"Embarked\",height=4,hue=\"Embarked\")\nef.map(sns.pointplot,\"Family_size\",\"Survived\",order=None);","57a99e1f":"import matplotlib\nfrom matplotlib import rcParams\nimport squarify\n\ndef label(n):\n    if n[1]==1:\n        return \"1st class {0} {1} to {2}\\n{3}% Died\".format(n[0],int(n[2].left.round()),int(n[2].right.round()),round(n[4]*100))\n    elif n[1]==2:\n        return \"2nd class {0} {1} to {2}\\n{3}% Died\".format(n[0],int(n[2].left.round()),int(n[2].right.round()),round(n[4]*100))\n    elif n[1]==3:\n        return \"3rd class {0} {1} to {2}\\n{3}% Died\".format(n[0],int(n[2].left.round()),int(n[2].right.round()),round(n[4]*100))\n    \ndef fix_zeros(n):\n    if n == 0:\n        return 0.01\n    else:\n        return n\n\nfor df in both:\n    df[\"Age_bins\"] = pd.cut(df[\"Age\"],4)\n\ncomb_df = train.groupby([\"Sex\",\"Pclass\",\"Age_bins\"]).mean()[\"Survived\"].dropna().reset_index()\n\ncomb_df[\"Died\"] = 1-comb_df[\"Survived\"]\n\ncomb_df[\"Label\"] = comb_df.apply(label,axis=1)\n\ncomb_df[\"Survived\"] = comb_df[\"Survived\"].apply(fix_zeros)\ncomb_df[\"Died\"] = comb_df[\"Died\"].apply(fix_zeros)\n\ncomb_df = comb_df.sort_values(\"Died\",ascending=False)\n\nplt.figure(figsize=(15,7))\n\nnorm = matplotlib.colors.Normalize(vmin=min(comb_df.Survived), vmax=max(comb_df.Survived))\ncolors = [matplotlib.cm.Blues(norm(value)) for value in comb_df.Died]\n\nsquarify.plot(sizes=comb_df[\"Died\"], label=comb_df[\"Label\"], color=colors, alpha=.8,pad=True)\nplt.axis('off');","9cc09ed8":"def label_2(n):\n    if n[1]==1:\n        return \"1st class {0}\\n{1}% survived\".format(n[0],round(n[2]*100))\n    elif n[1]==2:\n        return \"2nd class {0}\\n{1}% survived\".format(n[0],round(n[2]*100))\n    elif n[1]==3:\n        return \"3rd class {0}\\n{1}% survived\".format(n[0],round(n[2]*100))\n    \ncomb_df2 = train.groupby([\"Sex\",\"Pclass\"]).mean()[\"Survived\"].dropna().reset_index()\\\n                                                        .sort_values(\"Survived\",ascending=False)\n\ncomb_df2[\"Label\"] = comb_df2.apply(label_2,axis=1)\ncomb_df2[\"Survived\"] = comb_df2[\"Survived\"].apply(fix_zeros)\n\nplt.figure(figsize=(10,7))\n\nnorm = matplotlib.colors.Normalize(vmin=min(comb_df2.Survived), vmax=max(comb_df2.Survived))\ncolors = [matplotlib.cm.Blues(norm(value)) for value in comb_df2.Survived]\n\n\nsquarify.plot(sizes=comb_df2[\"Survived\"], label=comb_df2[\"Label\"], color=colors, alpha=.8,pad=False)\nplt.axis('off');","1e85ee87":"train.head()","0f76cb80":"from IPython.display import Image\nImage(\"..\/input\/class-layout\/titanic class system layout.jpg\")","2efb69cc":"def split_cabin(n):\n    try:\n        return n[0]\n    except TypeError:\n        return np.nan\n    \ndef room(n):\n    try:\n        return n[1:]\n    except TypeError:\n        return np.nan\n\nfor df in both:\n    df[\"Deck\"] = df[\"Cabin\"].apply(split_cabin)\n    df[\"Room\"] = df[\"Cabin\"].apply(room)","cc5d1f2c":"train.Deck.unique()","b6cdee2b":"train.Room.unique()","17d2cfaa":"import math\nfrom statistics import mean\n\ndef multi_rooms(n):\n    try:\n        if len(n[\"Room\"])>3:\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n    \n# Function to fix multiple rooms\n\ndef fix_rooms(n):\n    try:\n        if len(n[\"Room\"])>3:\n            r_list = [int(v) for v in re.findall(\"\\d+\",n[\"Room\"])] # create a list of the multiple rooms\n            av = math.ceil(mean(r_list))                           # Take the average room number\n            return str(min(r_list, key=lambda x:abs(x-av)))        # Find the closest real room to the average\n        elif len(n[\"Room\"]) == 0:\n            return np.nan\n        else:\n            return n[\"Room\"]\n    except TypeError:\n        return np.nan\n    \nfor df in both:    \n    df[\"Multi_room\"] = df.apply(multi_rooms,axis=1)\n    df[\"Room\"] = df.apply(fix_rooms,axis=1)","b81716fb":"train.head()","21d30b48":"train.Multi_room.value_counts()","49c2623d":"train.Room.unique()","f573966a":"knn_marker = train.copy() \n    \nimpute = train.copy()\nimpute_marker = train.copy() \n\nimpute_test = [knn_marker,impute,impute_marker] # train will be knn without a marker","3305d355":"train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0],inplace=True) # Fill missing values using the mode as there are very few (2) nulls in this column\ntest[\"Fare\"].fillna(test[\"Fare\"].median(),inplace=True)\n\nfor df in both:\n    df[\"Fare_cats\"] = df[\"Fare_bins\"].cat.codes # Use cat.codes to label encode \n    df[\"Age_cats\"] = df[\"Age_bins\"].cat.codes.replace(-1, np.nan) # cat.codes encodes nan as -1 so replace with np.nan\n    df.drop(columns=[\"PassengerId\",\"Name\",\"Age\",\"Ticket\",\"Fare\",\"Cabin\",\"Age_bins\",\"Fare_bins\"],inplace=True)","16358ab1":"Image(\"..\/input\/encoding\/encoding.png\")","c8dc999a":"from sklearn.preprocessing import OrdinalEncoder\n\nle= OrdinalEncoder() # Instantiate estimator\nencode_cols = [\"SibSp\",\"Parch\",\"Deck\",\"Room\"]\n\ndef encode(data):\n    nonulls = np.array(data.dropna()) # df of non-null values\n    impute_reshape = nonulls.reshape(-1,1) # df has to be reshaped before the label encoder can be fit\n    impute_ordinal = le.fit_transform(impute_reshape)\n    data.loc[data.notnull()] = np.squeeze(impute_ordinal) # np.squeeze removes single-dimensional entries from the shape of an array\n\nfor df in both:\n    for col in encode_cols:\n        encode(df[col])","2182894e":"Image(\"..\/input\/encoding\/OHE.png\")","d45fd443":"from sklearn.preprocessing import OneHotEncoder\n\nsub = [\"Sex\",\"Embarked\",\"Title\"] # Subset of df columns to encode values\nohe = OneHotEncoder(handle_unknown = \"ignore\",sparse=False) # Instantiate estimator\n\nfor i,df in enumerate(both):\n    ohe_df = pd.DataFrame(ohe.fit_transform(both[i][sub]))  # Fit and transform OHE to the data then convert to dataframe as it returns an array\n    \n    ohe_df.columns = [\"Female\",\"Male\",\"C\",\"Q\",\"S\",\"Master\",\"Miss\",\"Mr\",\"Mrs\",\"Unique\"] # Reassign column names as OHE removes them\n    \n    both[i] = pd.concat([both[i],ohe_df],axis=1) # Concatentate  the two dataframes\n    \n    both[i].drop(columns=[\"Sex\",\"Embarked\",\"Title\"],inplace=True)","15e83009":"from sklearn.impute import KNNImputer\n\nknn_imp = KNNImputer(n_neighbors=5,weights=\"distance\")\n\nfor i,df in enumerate(both):\n    both[i] = pd.DataFrame(knn_imp.fit_transform(both[i]),columns = both[i].columns)","31241c09":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\n\nfor i,df in enumerate(both):\n    d = mm.fit_transform(both[i])\n    both[i] = pd.DataFrame(d,columns=both[i].columns)\n    \ntrain = both[0]\ntest = both[1]","4e65c1d2":"X = train.drop(columns=\"Survived\")\ny = train[\"Survived\"]","41e8db35":"X.head()","e90ce3a6":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nfor df in impute_test:\n    df[\"Fare_cats\"] = df[\"Fare_bins\"].cat.codes \n    df[\"Age_cats\"] = df[\"Age_bins\"].cat.codes.replace(-1, np.nan)\n    df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0],inplace=True)\n\nmode = [\"Deck\",\"Room\",\"Multi_room\"]\nmedian = [\"Age_cats\"]\n\nc = mode+median+list(impute.columns) # impute Column names\nc2 = mode+[n+\"_missing\" for n in mode]+median+[n+\"_missing\" for n in median]+list(impute_marker.columns) # Impute_marker column names\n\ntrans = [(\"mode\",SimpleImputer(strategy=\"most_frequent\"),mode),(\"median\",SimpleImputer(strategy=\"median\"),median)] # Estimators that we pass to our column transfomer\ntrans_m = [(\"mode\",SimpleImputer(strategy=\"most_frequent\",add_indicator=True),mode),(\"median\",SimpleImputer(strategy=\"median\",add_indicator=True),median)]\n\nt = ColumnTransformer(transformers=trans,remainder=\"passthrough\")\nt_m = ColumnTransformer(transformers=trans_m,remainder=\"passthrough\")\n\nimpute_test[1] = pd.DataFrame(t.fit_transform(impute),columns=list(dict.fromkeys(c)))\nimpute_test[2] = pd.DataFrame(t_m.fit_transform(impute_marker),columns=list(dict.fromkeys(c2)))\n\nfor df in impute_test:\n    for col in encode_cols:\n        encode(df[col])\n               \nfor i,df in enumerate(impute_test):\n    ohe_df = pd.DataFrame(ohe.fit_transform(impute_test[i][sub]))\n    ohe_df.columns = [\"Female\",\"Male\",\"C\",\"Q\",\"S\",\"Master\",\"Miss\",\"Mr\",\"Mrs\",\"Unique\"]\n    impute_test[i] = pd.concat([impute_test[i],ohe_df],axis=1)\n    \nfor df in impute_test:\n    df.drop(columns=[\"PassengerId\",\"Survived\",\"Name\",\"Ticket\",\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Title\",\"Age_bins\",\"Fare_bins\",\"Cabin\"],inplace=True)\n    \nknn_imp = KNNImputer(n_neighbors=5,weights=\"distance\",add_indicator=True)\nimpute_test[0] = pd.DataFrame(knn_imp.fit_transform(impute_test[0]),columns = list(impute_test[0].columns) + [\"Deck_missing\",\"Room_missing\",\"Multi_room_missing\",\"Age_cats_missing\"])\n    \nfor i in range(len(impute_test)):\n    d = mm.fit_transform(impute_test[i])\n    impute_test[i] = pd.DataFrame(d,columns=impute_test[i].columns)\n    \nknn_marker = impute_test[0]\nimpute = impute_test[1]\nimpute_marker = impute_test[2]\n\ndrop = X.drop(columns=[\"Deck\",\"Room\",\"Multi_room\"])\n\nimp_dict = {\"KNN\":X,\"KNN + marker\":knn_marker,\"SimpleImpute\":impute,\"SimpleImpute + marker\":impute_marker,\"Drop\":drop}","b2115446":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nalgs = [RandomForestClassifier(random_state=0),\n       GaussianNB(),\n       KNeighborsClassifier(),\n       SVC(probability=True,random_state=0),\n       XGBClassifier(random_state=0),\n       LogisticRegression(random_state=0),\n       DecisionTreeClassifier(random_state=0)]","85612ab9":"from sklearn.model_selection import cross_validate,ShuffleSplit,StratifiedKFold\n\nshuf = ShuffleSplit(n_splits=10,test_size=0.25,train_size=0.75,random_state=0)\nstratk = StratifiedKFold(n_splits=10)\nbaseline_df = pd.DataFrame()\n\nrow=0\nfor a in algs:\n    \n    results = cross_validate(a,X,y,cv=stratk,return_train_score=True)\n    \n    name = a.__class__.__name__\n    baseline_df.loc[row,\"Name\"] = name\n    baseline_df.loc[row,\"Feature_count\"] = len(X.columns)\n    baseline_df.loc[row,\"Train_score\"] = results[\"train_score\"].mean()\n    baseline_df.loc[row,\"Test_score\"] = results[\"test_score\"].mean()\n    baseline_df.loc[row,\"Time\"] = results[\"fit_time\"].mean()\n    row+=1\n    \nbaseline_df.sort_values(\"Test_score\",ascending=False)","9b899198":"cv_methods_df = pd.DataFrame()\ncvs = {\"ShuffleSplit\":ShuffleSplit(n_splits=10,test_size=0.25,train_size=0.75,random_state=0),\n       \"StratifiedKFold\":StratifiedKFold(n_splits=10)} \n\nrfc = RandomForestClassifier(random_state=0)\ni=0\nfor k,cv in cvs.items():\n       \n    results = cross_validate(rfc,X,y,cv=cv,return_train_score=True)\n    \n    name = list(cvs.keys())[i]\n    cv_methods_df.loc[i,\"cv_method\"] = name\n    cv_methods_df.loc[i,\"Train_score\"] = results[\"train_score\"].mean()\n    cv_methods_df.loc[i,\"Test_score\"] = results[\"test_score\"].mean()\n    cv_methods_df.loc[i,\"Time\"] = results[\"fit_time\"].mean()\n    i+=1\n        \ncv_methods_df.sort_values(\"Test_score\",ascending=False)","e7997a95":"imp_methods = pd.DataFrame()\ni=0\nfor a in algs:\n    n=0\n    for k,df in imp_dict.items():\n\n        results = cross_validate(a,df,y,cv=stratk,return_train_score=True)\n\n        name = list(imp_dict.keys())[n]\n        imp_methods.loc[i,\"Method\"] = name\n        imp_methods.loc[i,\"Algorithm\"] = a.__class__.__name__\n        imp_methods.loc[i,\"Test_score\"] = results[\"test_score\"].mean()\n        imp_methods.loc[i,\"Time\"] = results[\"fit_time\"].mean()\n        i+=1\n        n+=1\n    \nimp_methods.groupby(\"Algorithm\").apply(lambda x: x.sort_values(\"Test_score\", ascending=False)).drop(columns=\"Algorithm\")","23190aa8":"from sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate,ShuffleSplit\nfrom sklearn.feature_selection import RFECV\n\nFSM = [\"None\",\"RFECV\",\"PCA\",\"LDA\"]\nperformance_df = pd.DataFrame()\n\nalg_row=0\nfsm_row=0\n\nfor a in algs:\n    \n    for fsm in FSM:\n        \n        if fsm == \"None\":\n            x = X\n            \n        elif fsm == \"RFECV\":\n            try:\n                rfe = RFECV(a,scoring=\"roc_auc\")\n                x = rfe.fit_transform(X,y)   \n\n            except RuntimeError:\n                x = X\n                \n        elif fsm==\"PCA\":\n            pca = PCA(n_components=0.95,random_state=0)\n            x = pca.fit_transform(X)\n            \n        elif fsm==\"LDA\":\n            lda = LinearDiscriminantAnalysis()\n            x = lda.fit_transform(X,y)\n    \n        results = cross_validate(a,x,y,cv=stratk,return_train_score=True)\n\n        performance_df.loc[fsm_row,\"Name\"] = a.__class__.__name__\n        performance_df.loc[fsm_row,\"FSM\"] = fsm\n        performance_df.loc[fsm_row,\"Feature_count\"] = x.shape[1]\n        #performance_df.loc[fsm_row,\"Train_score\"] = results[\"train_score\"].mean()\n        performance_df.loc[fsm_row,\"Test_score\"] = results[\"test_score\"].mean()\n        performance_df.loc[fsm_row,\"Test_improvement\"] = results[\"test_score\"].mean() - baseline_df.loc[alg_row,\"Test_score\"]\n        performance_df.loc[fsm_row,\"Time\"] = results[\"fit_time\"].mean()\n        fsm_row+=1\n        \n    alg_row+=1\n    \nperformance_df.groupby(\"Name\").apply(lambda x: x.sort_values(\"Test_score\", ascending=False)).drop(columns=\"Name\")","8a5090f9":"from sklearn.model_selection import GridSearchCV\n\nrandomforest = {\"n_estimators\":[100,300,500],\n                \"criterion\":[\"gini\",\"entropy\"],\n                \"max_depth\":[2,6,10,None],\n                \"random_state\":[0],\n                \"max_features\":[\"auto\",\"sqrt\",\"log2\"],\n               \"min_samples_split\":[2,5,10],\n               \"min_samples_leaf\":[1,5,10]}\nguassianNB = {}\n\nKneighbors = {\"n_neighbors\":[5,10,15],\n              \"leaf_size\":[30,35,40],}\n\nsvc = {\"gamma\":[\"scale\",\"auto\"],\n      \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n\nxgb = {\"max_depth\":[2,4,6,8,10,None],\n      \"random_state\":[0]}\n\nlogisticregression = {\"penalty\":[\"l1\", \"l2\", \"elasticnet\", \"none\"],\n                     \"random_state\":[0],\n                     \"solver\":[\"newton-cg\",\"lbfgs\",\"liblinear\",\"sag\",\"saga\"]}\n\ndecisiontree = {\"criterion\":[\"gini\",\"entropy\"],\n               \"splitter\":[\"best\",\"random\"],\n               \"max_depth\":[2,6,10,None],\n               \"max_features\":[\"auto\",\"sqrt\",\"log2\"],\n               \"random_state\":[0],\n               \"min_samples_split\":[2,5,10],\n               \"min_samples_leaf\":[1,5,10]}\n\nparams =[randomforest,guassianNB,Kneighbors,svc,xgb,logisticregression,decisiontree]\n\nn=0\nalg_params={}\nwhile n<7:\n    for a in algs:\n        h_params = GridSearchCV(a,param_grid=params[n],scoring='roc_auc',cv=3,n_jobs=-1)\n        h_params.fit(X,y)\n        alg_params[a.__class__.__name__] = h_params.best_params_\n        n+=1\n        \nalg_params","f1f84ef1":"rfc_best = RandomForestClassifier(**alg_params[\"RandomForestClassifier\"])\nnb_best = GaussianNB()\nknn_best = KNeighborsClassifier(**alg_params[\"KNeighborsClassifier\"])\nsvc_best = SVC(probability=True,**alg_params[\"SVC\"])\nxgb_best = XGBClassifier(**alg_params[\"XGBClassifier\"])\nlr_best = LogisticRegression(**alg_params[\"LogisticRegression\"])\ndt_best = DecisionTreeClassifier(**alg_params[\"DecisionTreeClassifier\"])\n\nbest_algs = [rfc_best,nb_best,knn_best,svc_best,xgb_best,lr_best,dt_best]\n\nrfe = RFECV(a,scoring=\"roc_auc\")\nrfe.fit(X,y)\nrfe_cols = X.columns[rfe.support_]\n\nfinal = pd.DataFrame()\nrow=0\nfor a in best_algs:\n    \n    results = cross_validate(a,X[rfe_cols],y,cv=stratk,return_train_score=True)\n    \n    name = a.__class__.__name__\n    final.loc[row,\"Name\"] = name\n    final.loc[row,\"Feature_count\"] = len(X[rfe_cols].columns)\n    final.loc[row,\"Train_score\"] = results[\"train_score\"].mean()\n    final.loc[row,\"Test_score\"] = results[\"test_score\"].mean()\n    final.loc[row,\"Time\"] = results[\"fit_time\"].mean()\n    row+=1\n    \nfinal.sort_values(\"Test_score\",ascending=False)","a6e4d724":"from sklearn.ensemble import VotingClassifier\n\nrfe = RFECV(a,scoring=\"accuracy\")\nrfe.fit(X,y)\nrfe_cols = X.columns[rfe.support_]\n\nest = [('rfc', rfc_best), ('xgb', xgb_best),\n('knn', knn_best), ('svc',svc_best),('lr',lr_best),(\"nb\",nb_best),(\"dt\",DecisionTreeClassifier())]\n\nvc_hard = VotingClassifier(estimators=est,voting='hard')\nvc_hard_cv = cross_validate(vc_hard,X[rfe_cols],y,cv=stratk,return_train_score=True)\nvc_hard.fit(X[rfe_cols], y)\nprint(\"Voting classifier hard\",vc_hard_cv['test_score'].mean())\n\nvc_soft = VotingClassifier(estimators=est,voting='soft')\nvc_soft_cv = cross_validate(vc_soft,X[rfe_cols],y,cv=stratk,return_train_score=True)\nvc_soft.fit(X[rfe_cols], y)\nprint(\"Voting classifier soft\",vc_soft_cv['test_score'].mean())","85b31c6a":"test_s = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_s['Survived'] = vc_soft.predict(test[rfe_cols]).astype(int)\n\nsubmission = test_s[['PassengerId','Survived']]\nsubmission.to_csv(\"..\/working\/submission.csv\", index=False)","241f9e04":"We'll repeat the previous steps for the other imputation methods that we decided on earlier.","4d2dc94f":"It's obvious that they were not prepared any potential disaster,Thomas E. Bonsall, a historian of the disaster, has commented that the evacuation was so badly organised that \"even if they had the number of lifeboats they needed, it is impossible to see how they could have launched them\". The lack of lifeboats meant that priority would have to be given to certain people. Our intuitions would tell us women and children would most likely be first and therefore most likely to survive. Lets investigate whether that's true and what other factors played a role in peoples survival.","76f78e44":"Now that we have our new paramters lets have a look at how our model currently performs.","17dd119a":"Our final fully wrangled data is ready.","a2bda260":"# Impute missing data\nTo fill in the missing values for the remaining columns I'm going to use KNNImputer which imputes using KNN.","85af4502":"# The Titanic and its survivors","b36f5bc2":"## Fare","be311ab9":"There is a huge amount of missing data for \"Cabin\", it might be a good idea to drop this column when it comes to feature engineering but we'll see with further analysis.\nWe might be able to gleen further insights on the number of missing values using the library msno.","a1ec51cf":"The name column on its own isn't any use to us, however within the entries are titles which we can extract into a new column","9f4f9ea8":"* Out of the 891 people in this dataset only 38% survived, compared to the roughly 32% of the full actual data.\n* The ages ranged from as young as 5 months to as old as 80. with the average age being close to 30.\n* The minimum fare is 0. This could be due to people sneaking aboard, taking very poor accommodation, or most likely that the crew are part of the dataset.","e80431e7":"I am going to load in the data using Pandas a data manipulation library","fb8f4600":"### The contents\n* PassengerId - This isn't going to be helpful\n* Survived - What we are trying to predict. 0 = No, 1 = Yes\n* Pclass - The passenger class. 1 = 1st, 2 = 2nd, 3 = 3rd\n* Name - Full name inclding title\n* Sex - Male,Female\n* Age - In years\n* SibSp - The number of siblings or spouses related to this passenger on board\n* Parch - The number of parents or children related to this passenger on board\n* Ticket - Ticket number. Doesn't appear to be that useful in first glance\n* Fare - How much was paid for the ticket\n* Cabin - Letter and number indicating the posistion of the passenegr on the ship\n* Embarked - C = Cherbourg, Q = Queenstown, S = Southampton","aa5f2cce":"## Lowest mortality rate","3f1f8c4b":"It seems to be that either None or RFECV perform the best.\n\nI think I'll go with RFCEV.","0e653677":"We've seen the features age,sex and class and their correlation with surival individually, now lets see what we notice when we combine these features in a single figure. ","c938169e":"## Class age survival","b8d38f07":"There are different cross validation methods that we can use, lets see if any stand out.","cec3dd31":"# Feature engineering","ed45870d":"Reassign our data to the standard variable nomenclature X, (features) y (target).","f5cb6a57":"# Preliminary investigation of the data","c9bbd689":"Two new columns we can make are \"Alone\" and \"Family_size\" which use \"SibSp\" and \"Parch\"","9979e70a":"* Confirms intuition around the idea that children take precendence over adults\n* Children under 10 more likely to surive than other ages\n* The early twenties seems to be the worst age for surivival","c6122d7d":"* The benefit of being a 1st class citizen seems to outweigh the negative of being an adult\n* A greater amount of children survived in classes 2 and 3, perhaps due to the fact that upper classes have less children and so affects the results?","ac2ae339":"This is how the data is currently looking. During the visualisation process we created some new features that we could train our model on. We can create some more now.","9b492a5e":"# Encode data","17ee40b9":"## Alone vs class,sex,embarked","e4a14bc8":"Looking at the cabin column we can see that the values are composed of letters and numbers, after a little Googling it appears that the letters could be the deck and the number could be the room. So we could engineer a new feature by splitting the cabin column into two new columns \"Deck\" and \"Room\".","0eb8de6d":"We'll make 4 different versions of the dataframe to test:\n* knn_marker - Uses KNNimputer which works by find \"k\" nearest neighbors and then votes for the most frequent label of those neighbors. And specify \"add_indicator=True\" which adds a marker column\n* Impute - Impute using SimpleImputer (median, mode etc)\n* Impute_marker - Impute + marker\n* Drop - Drops the columns. We'll define this dataframe later","1fdc3141":"* Regardless of where you embark from a the lower your class the less likely you were to live\n* The exception to this is in Queenstown, where the survival rate of men increases slightly from 2nd to 3rd class\n* However this may be due to the fact that there were less data points for passengers in 1st or 2nd class from Queenstown","18e7b242":"* For 1st and 2nd class passengers paying more money may have led to a greater chance of suriving\n* However for 3rd class, it doesnt appear that the fare you paid would be likely to help your survival, perhaps the stigma or position of 3rd class rooms were too great of a detriment","c7f55e6f":"One last one for fun, let's look for the group with the highest mortality rate by creating a Treemap!","fa9d1119":"### The dataset that I will be exploring today is the Titanic dataset, which contains information on passengers who were aboard the RMS Titanic during its fatal voyage","b8fa22ad":"* As we have come to expect, lower class old men fair very poorly in life or death situations\n\nThe lower end of the graph becomes very difficult to read, so lets create another Treemap but this time reduce the groups and show the lowest mortality rates.","ad299500":"The task is to build a machine learning model that can predict as accurately as possible the likelihood of some given passengers to survive. We are supplied some training data with which to train the model on, and some testing data to test the models accuracy.\nBefore I jump in its probably a good idea to do a little research the historical event in question.","c7e0d93d":"I'm going to explore PCA,LDA and RFECV, so lets take a look at how these methods peform.\nOnly decision tree algorithms support RFECV so for ones that dont I set the columns to use as \"None\".","e718ea8b":"# Read in the data","d36f9c0e":"## Family size","c6814071":"# Missing values","ce29ec92":"### OneHotEncoder\nOneHotEncoder is simliar to LabelEncoder however it is more useful for data that has no order, such as \"Red\",\"Yellow\",\"Blue\".\nIt also differs from LabelEncoder in that instead of creating one new column containing numbers ranging from 0 to n-1 (where n is the length of unique items),it creates n new columns each with either a value of 0 or 1.","b24ceab5":"* As you might expect fare positively correlates with survival\n* But is this only to do with the money or is it ultimately to do with class","91d5ef6d":"Now we have our two new columns. The values for deck seem to be fine, however when we look at the unique values for room we can see that we have some odd values. There are items in this list that seem to contain multiple rooms. This could potentially be a new feature, as if a passenger has bought multiple rooms,they are likely wealthier and could be of higher class, which we know from our viualisations correlate positively with survival.","e7045742":"* Class greatly boosts mens liklihood to survive\n* Interesting to see only 10% of 2nd class \"Miss\" survived","8d663f60":"## Fare class survival","a0178ee8":"Our final score appears to be around 87%","cd43e30c":"Exlcuding \"Survived\" (as thats what I am trying to predict in test) we have the same number of columns with the same datatypes. It seems like however there quite a few nulls throughout, especially in cabin.\nLets take a better look at what null values ware present.","23ac4813":"KNN looks to have have an edge compared to the other options.","0dc70a4f":"## Class and embarked","fb27c7af":"* You were 1.5 times more likely to die if you were alone\n* Women who were alone had greater odds than those alone\n* It doesn't appear to matter what class you are, if you are alone your chances aren't that highly affected","bfec5483":"### LabelEncoder\nThis function takes categorical data and assigns a numerical value to each unique item. For example if you had the the categories \"1st Place\", \"2nd Place\", \"3rd Place\", LabelEncoder would assign values like 0, 1, 2. I decided to use LabelEncoder for these columns as they have an implicit order to them.","6dd64aaa":"Below are some images of the layout of the ship. These new columns could could have importance in predicting survival as deck and room indicate a persons posistion on the ship,and therefore their potential class or proximity to the impact of the iceberg.","0ff1778c":"# Voting classifier\nWe don't have try rely on just one model we can take the opinion of various ones using sklearn's VotingCLassifier. A collection of several models working together on a single set is called an ensemble, it works by combining the predictions from multiple machine learning algorithms.  The final output on a prediction is taken according to two different voting strategies, hard or soft. Choosing the voting paramter \"hard\" will take the majority vote of some predictions. Choosing \"soft\" will take into account the various probalities of each predcition.","e9988aa1":"Now let's see how our various imputations method ultimately performed","9f3d1eeb":"* Sex seems to be highly correlated with survival\n* 74% of women survived compared to 19% of men\n* Women were 4x as likely to survive than men","4639e0a8":"# Baseline performance\nLet's choose some various alogrithms that we think might be relevant for this classification problem, loop through them and created at table based on their performances.","c2a3cd71":"# Dealing with missing values","d734502c":"Now, we have some intial assumptions on what features might predict survival, such as age, sex etc. Lets explore those, and see what else we can find out.","e003c21b":"* Cherbourg 55% survival\n* The wealth of these ports of embarkation may be a factor in the passengers rate of survival[](http:\/\/)","52536895":"## Highest mortality rate","45ed9731":"## Alone","5e36bdd2":"<img src=\"attachment:Titanic_cutaway_diagram.png\" width=\"400\">","509a877c":"### Different feature selection methods inlcude:\n\n**Variance thresholds**\n* Variance thresholds remove features whose values don't change much from observation to observation (i.e. their variance falls below a threshold). These features provide     little value.\n\n**Correlation thresholds**\n* Correlation thresholds remove features that are highly correlated with others (i.e. its values change very similarly to another's). These features provide redundant information\n\n**PCA**\n* Is a dimensionality reduction technique that aims to find the directions of maximal variance,it is unsupervised so ignores class labels\n\n**LDA**\n* Similar to to PCA, LDA is a linear transformation technique, however LDA attempts to find a feature subspace that maximizes class separability, and requires labled data.\n\n**Recursive Feature elimination**\n* Searches for the best combination of features and removes ones that dont perform well.","9f060f76":"# Visualisations","b3db1332":"From this inital test we can see that decision tree algorithms seem to perform better, but also overfit the data to a large degree, more so than the others.","5ed30bcb":"* Class correlated with survival\n* Greater than 60% chance of survival if the passenger was in 1st class\n* Only around 25% of 3rd class passengers lived","e6c9bcc2":"## Embarked","7362a619":"## Title","dd7285bb":"* The RMS Titantic sank in the early hours of the morning on the 15th of April 1912.\n* This resulted in an estimated 1500 deaths (between 1490 and 1635 people) out the the 2224 people on board.\n* The Titanic only had enough lifeboats to carry about half of those on board. \n* Even if they had all lifeboats available, if they full capacity of the ship was met (3339) there would only be enough lifeboats to save 1\/3 of those onboard.","46250a50":"## Age","8ef9de97":"## Sex","5ddb5c58":"# Hyper-parameter tuning\nThe estimators that we choose have lots of different parameters to change that will affect our models performance. We can search for the optimal combination by using sklearn's GridSearchCV which takes a grid of paramters and tries every combination of them until it finds the combination that peforms the best. This is a very computationally expensive process and will take a long time to do. ","e6c3304e":"The final step is going to be scaling our data. Depending on which algorithm we use the inconsistent scales of our data will lead to certain features having a much greater effect than others.","186ad523":"# Feature elmination methods\nChoosing the the right features and the right amount of them is an important stage to get right in ML. As having too few features means you dont know enough about your data and too many means that your model will be worse at generalising for unseen data. It will have a high train score but a much lower test score.","2e5dfd62":"* 1st class females have the greatest chance of living, age doesn't appear to be that influencial for this group\n* Young 3rd class men at the greatest risk of perishing\n* Likelihood of surival decreases with passenger class for men, but not for women\n* The saying \"women and children first\" rings true here","59646963":"* This graph really fits well with our intuitions\n* Women rank higher than men, with married women (who probably have a higher social status than those umarried ) above unwedded women.\n* Master aka young boys is next,which makes sense as the young are usually preferred in live or death situations\n* Those with a unique title e.g. Dr, Rev etc (who are usually men) come second last, probably due to the benefit of their social status\n* Lastly regular men having a less than 20% chance of survival","505d37ad":"### Conclusions\nSome of the features that seem to have the greatest affect on surival are:\n* Sex\n* Pclass\n* Age\n* Alone\n* Fare\n* Embarked","2614c6f5":"## Who's alone","703ecb1c":"Most machine learning algorithms cannot handle missing or non numerical values, so before we can use them we need to encode the data and impute any nulls.\nThere are multiple ways of doing this, we can: \n* drop the columns which contain nulls\n* Impute missing values using mean,median or mode\n* Impute using a ML algortihm\n* Add a marker column indicating that a value is missing\n* or combinations of above\n\nLets try a few different ways and see how they play out.","858b1360":"## Passenger Class"}}