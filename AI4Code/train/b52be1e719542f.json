{"cell_type":{"f17ac3d6":"code","c512b411":"code","1779bc36":"code","32c99f8c":"code","86a2d6b0":"code","b5bd9452":"code","22f49a48":"code","cb8e129b":"code","cc36e4b1":"code","94bcdf8f":"code","ab6ba174":"code","d19d3a64":"code","d2bac818":"code","de874d0d":"code","6c08b1b6":"code","5a6edca8":"code","c9b1c036":"code","dc36ffab":"code","b57b432d":"code","c844ec9d":"code","bde25a03":"code","01d62d2c":"code","8d87b898":"code","5ba593ec":"code","19bfc5b0":"code","5440cabd":"code","aa2f6f6e":"code","14c61342":"code","f0ac15bb":"code","6d1ec873":"code","c60b2e17":"code","0379815f":"code","b0bc64a5":"code","a4e0a6eb":"code","e7406ab5":"code","d277b00b":"code","03c37b9e":"code","22f18ce7":"code","285cbfac":"code","c1efd21d":"code","b2e6f78b":"code","6771d7af":"code","687e7024":"code","bc1d8abb":"code","4dfc94cc":"code","30d07aad":"code","98689b3b":"code","8c5629a8":"code","466a7f65":"code","46636016":"code","d5862dcc":"code","7cf2a4cf":"code","a580119a":"code","51f2d87d":"code","f9669fc5":"code","6e4335b5":"code","eddada34":"code","f150f32d":"code","6be392dd":"code","8bf898c4":"code","9cac024a":"code","fec95ded":"code","e4006025":"code","c577592f":"code","d0c043f4":"code","0a29a7cc":"code","261d6263":"code","0b528ffd":"code","247803db":"code","66d0e3e4":"code","06e05013":"code","b2df2703":"code","1607314e":"code","41609c5d":"code","5227a34d":"code","32c04c47":"code","97d9896e":"code","965d5097":"code","09073a57":"code","28e07555":"code","2852315c":"code","de975e29":"code","aaec305f":"code","0ce38732":"code","825b306b":"code","c158c7e5":"code","b2ce156b":"code","2219d7bd":"markdown","8ce4260f":"markdown","521dd697":"markdown","714d14f9":"markdown","9f21041e":"markdown","abded44a":"markdown","81eb4869":"markdown","11d821ca":"markdown","6d897eb5":"markdown","950e257d":"markdown","6d722fcd":"markdown","c3b47be8":"markdown","b17994e9":"markdown","98961c88":"markdown","e4fb7e22":"markdown","ed77c5ef":"markdown"},"source":{"f17ac3d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c512b411":"import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stat\n%matplotlib inline","1779bc36":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","32c99f8c":"test.info()","86a2d6b0":"train.info()","b5bd9452":"train.columns","22f49a48":"train.head()","cb8e129b":"#lets drop id column\ntest.drop('Id',axis=1,inplace=True)\ntrain.drop('Id',axis=1,inplace=True)","cc36e4b1":"#looking at train data\ntrain.describe().transpose()","94bcdf8f":"train.shape","ab6ba174":"test.shape#only feature missing is the SalePrice","d19d3a64":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n#first plot the distribution of price column, we can see if its skewed\nsns.set_style('white')\nfig, ax =plt.subplots(nrows=1,ncols=1,figsize=(12,4))\nsns.distplot(train['SalePrice'],bins=100);","d2bac818":"#The data is centered around 150000 with some deviating sales data.\nprint('Skewness: {}'.format( train['SalePrice'].skew()))\nprint('Kurtosis: {}'.format( train['SalePrice'].kurt()))\n#as can be seen some outliers are dispersing the destribution, later on they might be better of removed.","de874d0d":"#cathegorical columns train data\ncathegorical_train=train.select_dtypes(exclude=['int64', 'float64']).columns","6c08b1b6":"#cathegorical columns test data\ncathegorical_test=test.select_dtypes(exclude=['int64', 'float64']).columns","5a6edca8":"#DataFrame cathegorical train data\nTrain_cathegorical=train[list(cathegorical_train)]","c9b1c036":"#DataFrame cathegorical test data\nTest_cathegorical=train[list(cathegorical_test)]","dc36ffab":"#numerical columns train data\nnummerical_train=train.select_dtypes(exclude=['object']).columns","b57b432d":"#numerical columns test data\nnummerical_test=test.select_dtypes(exclude=['object']).columns","c844ec9d":"#DataFrame numerical training data\nTrain_nummerical=train[list(nummerical_train)]#dataFrame containing the nummerical data","bde25a03":"#for the rest of the document im doing the same operations on test as train data\nTest_nummerical=test[list(nummerical_test)]","01d62d2c":"#Any missing data in the numeric portion?\nTrain_nummerical.isnull().sum() #seems we have some missing data on LotFrontage, GarageYrBlt Train_nummerical.isnull().sum() #seems we have some missing data on LotFrontage, GarageYrBlt, and ","8d87b898":"#The same operations used on training data will be done on the test data, \n#else we would have data leakage, for exploratory parts we will focus on train data excluding this cell. \nTest_nummerical.isnull().sum()","5ba593ec":"#lets look at nummeric data with correlation to the SalePrice\n#correlating data\nCorr_nummeric_train=Train_nummerical.corr()['SalePrice'][:-1]#droping salesprice since it will always have 1 to 1 correlation to itself.\n#lets sort the correlating values\nCorr_nummeric_train.sort_values(ascending=True,inplace=True)","19bfc5b0":"#which features correlate best with SalePrice in the numeric features lets look at top 8?\ntop8_corr=Corr_nummeric_train[-8::]\nlist(top8_corr.index)\ntop8_corr#top 8 nummeric features correlated to the SalePrice","5440cabd":"#lets make a hisplot of the most correlated features with sales price.\nTrain_nummerical[list(top8_corr.index)].hist(figsize=(16,10),bins=30);","aa2f6f6e":"#SalePrice VS OverallQual\nsns.jointplot(x='SalePrice',y='OverallQual',data=Train_nummerical,kind='scatter',stat_func=stat.pearsonr);\n#makes sense the the quality of the home is going to be very correlated to the saleprice.   ","14c61342":"#SalePrice VS GrLivArea, GrLivArea shows the living area square feet above ground\n#and is also highly related to the SalePrice. \nsns.jointplot(x='SalePrice',y='GrLivArea',data=Train_nummerical,kind='scatter',stat_func=stat.pearsonr);","f0ac15bb":"#SalePrice VS TotalBsmtSF, TotalBsmtSF shows total square feet of basement area, is  probably very related to living area\n#therfor should have a good correlation to SalesPrice.\nsns.jointplot(x='SalePrice',y='TotalBsmtSF',data=Train_nummerical,kind='scatter',stat_func=stat.pearsonr);","6d1ec873":"sns.set_style('darkgrid')\n#make boxplots of same features\nsns.boxplot(x='OverallQual',y='SalePrice',data=Train_nummerical);","c60b2e17":"#lets look for missing with count, for nummeric data \nnulls=Train_nummerical.columns[Train_nummerical.isna().any()].tolist()\nsub_set_null=Train_nummerical[nulls]\n#sub_set_null.isnull()\n#Lets look at the null columns with heatmap\nplt.figure(figsize=(16,8))\nsns.heatmap(sub_set_null.isnull(),cmap='viridis',cbar=False,yticklabels='False')\n#three features in the numeric data are missing data points LotFrontage,MassVnrArea\n#GarageYrBlt\n#Lets look at how this features correlate with SalePrice\ntrain[['LotFrontage','MasVnrArea','GarageYrBlt','SalePrice']].corr()\n#Lotfontage has alot of missing data and has correlation however 0.3 lets drop it first\ntrain=train.drop(['LotFrontage'],axis=1)\ntest=test.drop(['LotFrontage'],axis=1)","0379815f":"#MasVnrArea has some missing data points but just a few,\n#MasVnrArea is Masonry veneer area in square feet\ntrain[train['MasVnrArea'].isnull()] # 8 are missing, we can go ahead and drop this 8 data points","b0bc64a5":"#we assume Masonry veneer area in square feet is 0 where data is missing\ntrain['MasVnrArea'].fillna(0,inplace=True)\ntest['MasVnrArea'].fillna(0,inplace=True)","a4e0a6eb":"#GarageyrBLT - We have missing values, lets assume this is due to missing garage\n#we can replace nulls with earliest garage built since, lower values means higher age of garage.\ntrain['GarageYrBlt'].fillna(train['GarageYrBlt'].min(),inplace=True)\ntest['GarageYrBlt'].fillna(test['GarageYrBlt'].min(),inplace=True)","e7406ab5":"sns.heatmap(train.select_dtypes(exclude=['object']).isnull(),cmap='viridis')\n#now we have all the nummeric data, lets focus on the cathegorical data.","d277b00b":"#Lets look at nulls for the cathegorical data like we did for the nummerical \n#data, using the same method\nnulls_cat=Train_cathegorical.columns[Train_cathegorical.isna().any()].tolist()\nsub_set_null_cat=Train_cathegorical[nulls_cat]\n#sub_set_null.isnull()\n#Lets look at the null columns with heatmap\nplt.figure(figsize=(16,8))\nsns.heatmap(sub_set_null_cat.isnull(),cmap='viridis',cbar=False,yticklabels='False') \n#Alley,MasVnrType,BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinType2,Electrical\n#FireplaceQu,GarageType,GarageFinish,GarageQual,GarageCond,PoolQc,Fence,MiscFeature\n#We can see some features missing alot of data, also some features that\n#are dependent of eachother, for example if house dosent\n#have a bsmt,garage the features that are related to this shouldent have quality or cond.\n#lets start looking at each feature with nulls at a time.","03c37b9e":"#First feature is Alley, Type of alley access to property,NA \tNo alley access\n#we can assume that the house is simply missing an allay at the housing with null\ntrain['Alley']=train['Alley'].fillna('NA')\ntest['Alley']=test['Alley'].fillna('NA')","22f18ce7":"#MasVnrType MasVnrType: Masonry veneer type, \ntrain['MasVnrType'].isnull().sum() #just 8 values are missing\n#we can assume they dont have a Msonry veneer type, this is None\ntrain[train['MasVnrType'].isnull()]['MasVnrArea'] #we can also the the MasVnrArea is 0 \ntrain['MasVnrType'].fillna('None',inplace=True)\ntest['MasVnrType'].fillna('None',inplace=True)","285cbfac":"#there are some na Values where not all bsmt features are null for the same houses, this are in \n#the BsmtExposure \n#feature Gd\tGood Exposure\n#Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n#Mn\tMimimum Exposure\n#No\tNo Exposure\n#NA\tNo Basement\n#We will fill this data with Average exposure, now we will do this null change where there is bsmt qual\n#in order to make sure that this is only done on the data that has basements.\n#train data find loc where they differ\nindex_Bsmtexp=train['BsmtExposure'].isnull()\nindex_Bsmtqual=train['BsmtQual'].isnull()\nindex_loc=index_Bsmtexp != index_Bsmtqual\nidx_train=train[index_loc==True].index.tolist()\n\n#for the test data same operation\nindext_Bsmtexp=test['BsmtExposure'].isnull()\nindext_Bsmtqual=test['BsmtQual'].isnull()\nindext_loc=indext_Bsmtexp != indext_Bsmtqual\nidx_test=test[indext_loc==True].index.tolist()","c1efd21d":"#filling the null in Bsmtexposure where there is a Basement.\ntrain.loc[idx_train]['BsmtExposure'].fillna('No',inplace=True)\ntest.loc[idx_test]['BsmtExposure'].fillna('No',inplace=True)","b2e6f78b":"#for BsmtQual and all the related features with bsmt we can see the features are missing for the same houses\n#BsmtQual: Evaluates the height of the basement\n# Ex\tExcellent (100+ inches)\t\n# Gd\tGood (90-99 inches)\n# TA\tTypical (80-89 inches)\n# Fa\tFair (70-79 inches)\n# Po\tPoor (<70 inches\n# NA\tNo Basement\n#The houses missing BsmtQual inputs are assumend having no Basement. NA are used for the rest of the features.\ntrain[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']]=train[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna('NA')\ntest[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']]=test[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna('NA')","6771d7af":"train.isna().sum()","687e7024":"#Electrical only has one null value. lets just assume its standard circuit \ntrain['Electrical'].fillna('SBrkr',inplace=True)","bc1d8abb":"#FireplaceQu: Fireplace quality\n\n#Ex\tExcellent - Exceptional Masonry Fireplace\n#Gd\tGood - Masonry Fireplace in main level\n#TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n#Fa\tFair - Prefabricated Fireplace in basement\n#Po\tPoor - Ben Franklin Stove\n#NA\tNo Fireplace\n#\n\n#filling the null in FireplaceQu.\ntrain['FireplaceQu'].fillna('NA',inplace=True)\ntest['FireplaceQu'].fillna('NA',inplace=True)","4dfc94cc":"#lets check if this fixed all the missing values for fireplaceQu\ntrain['FireplaceQu'].isnull().sum()#0 no more missing data","30d07aad":"#GarageType,GarageFinish,GarageQual,GarageCond are all missing data for the same houses, \n#they can all be filled with NA\ntrain[['GarageType','GarageFinish','GarageQual','GarageCond']]=train[['GarageType','GarageFinish','GarageQual','GarageCond']].fillna('NA')\ntest[['GarageType','GarageFinish','GarageQual','GarageCond']]=test[['GarageType','GarageFinish','GarageQual','GarageCond']].fillna('NA')","98689b3b":"#PoolQC PoolQC: Pool quality\n# Ex\tExcellent\n# Gd\tGood\n# TA\tAverage\/Typical\n# Fa\tFair\n# NA\tNo Pool\n\n\n\n#filling the null values\ntrain['PoolQC'].fillna('NA',inplace=True)\ntest['PoolQC'].fillna('NA',inplace=True)\n","8c5629a8":"#lets check if this fixed all the missing values for PoolQC\ntrain['PoolQC'].isnull().sum()#0 no more missing data","466a7f65":"#Fence Fence Quality lets just assume it has no fence its just one value. \ntrain['Fence'].fillna('NA',inplace=True)\ntest['Fence'].fillna('NA',inplace=True)","46636016":"#MiscFeature, MiscFeature: Miscellaneous feature not covered in other categories\n\n# Elev\tElevator\n# Gar2\t2nd Garage (if not described in garage section)\n# Othr\tOther\n# Shed\tShed (over 100 SF)\n# TenC\tTennis Court\n# NA\tNone\n\n\n#filling the null values\ntrain['MiscFeature'].fillna('NA',inplace=True)\ntest['MiscFeature'].fillna('NA',inplace=True)","d5862dcc":"train['MiscFeature'].isnull().sum()#0 no more missing data","7cf2a4cf":"#Lets check if there is any missing cathegorical data\nsns.heatmap(train.select_dtypes(exclude=['int64', 'float64']).isnull(),cmap='viridis')","a580119a":"train.isnull().sum()[train.isnull().sum()>0]\n#test.shape","51f2d87d":"#filling nulls, if there is any left in test data \niteration=test.isnull().sum()[test.isnull().sum()>0].index.tolist()\nfor i in iteration:\n    if test[i].dtype=='O':\n        test[i].fillna('NA',inplace=True)\n    else:\n        test[i].fillna(0,inplace=True)","f9669fc5":"test.isnull().sum()[test.isnull().sum()>0]","6e4335b5":"train[train['SalePrice']>500000]#Due to the skewness lets drop this outlier values from training data, there\n#simply isent enough data.","eddada34":"#Dropping outliers or very expensive houses, we have few data points around saleprice 500000\nidx_drop=train[train['SalePrice']>500000].index\ntrain.drop(idx_drop,axis=0,inplace=True)","f150f32d":"#Lets concatenate the test and train for dummie_creation\ndf=pd.concat([train,test],ignore_index=True)","6be392dd":"df.info(verbose=True)","8bf898c4":"y_testing=train['SalePrice']\n#dummy variable, droping first to avoid collinearity\ndf=pd.get_dummies(df,drop_first=True)#we still have to drop the saleprice however this will be","9cac024a":"df.shape","fec95ded":"\n#the y value used to create the ML regression\ntrain=df.loc[df['SalePrice'].notna()]\ntest=df.loc[df['SalePrice'].isna()]\n#drop SalePrice from training data.\ntrain.drop('SalePrice',axis=1,inplace=True)\ntest.drop('SalePrice',axis=1,inplace=True)","e4006025":"test.shape\n","c577592f":"from sklearn.model_selection import train_test_split\n#We need a test train split to evaluate the model \nX=train\ny=y_testing\nX_Evaluation=test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","d0c043f4":"LRMSE_model = [] # for storing the LRMSE \nName_model=[]","0a29a7cc":"from sklearn.pipeline import make_pipeline \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\n#training model using pipline that way we get more operations at once.\nPipeline=make_pipeline(StandardScaler(), SGDClassifier())\nest=Pipeline.fit(X_train,y_train)\nprediction=est.predict(X_test)","261d6263":"est#only used standard inputs","0b528ffd":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, prediction)) #Mean absolute error\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))#Mean squared error (punishes outliers)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\nprint('Logaritmic RMSE:', np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction))))#root","247803db":"LRMSE_model.append(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction)))) # for storing the LRMSE \nName_model.append('SGD')","66d0e3e4":"#training model using pipline that way we get more operations at once.\nfrom sklearn.linear_model import Lasso\nPipeline2=make_pipeline(StandardScaler(), Lasso(alpha=0.1))\nest2=Pipeline2.fit(X_train,y_train)\nprediction=est2.predict(X_test)","06e05013":"print('MAE:', metrics.mean_absolute_error(y_test, prediction)) #Mean absolute error\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))#Mean squared error (punishes outliers)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\nprint('Logaritmic RMSE:', np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction))))","b2df2703":"LRMSE_model.append(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction)))) # for storing the LRMSE \nName_model.append('Lasso')","1607314e":"from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n#lets make a Gridsearch to try to find optimal parameters\ngrid_param={'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001],'kernel':['linear','rbf']}\n#training model using pipline that way we get more operations at once.\nclf = make_pipeline(StandardScaler(),GridSearchCV(SVR(),grid_param,refit=True))\nest3=clf.fit(X_train,y_train)\nprediction=est3.predict(X_test)","41609c5d":"print('MAE:', metrics.mean_absolute_error(y_test, prediction)) #Mean absolute error\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))#Mean squared error (punishes outliers)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\nprint('Logaritmic RMSE:', np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction))))","5227a34d":"LRMSE_model.append(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction)))) # for storing the LRMSE \nName_model.append('SVR')","32c04c47":"from sklearn.linear_model import ElasticNet\nPipeline_ela=make_pipeline(StandardScaler(), ElasticNet())\nest4=Pipeline_ela.fit(X_train,y_train)\nprediction=est4.predict(X_test)","97d9896e":"print('MAE:', metrics.mean_absolute_error(y_test, prediction)) #Mean absolute error\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))#Mean squared error (punishes outliers)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\nprint('Logaritmic RMSE:', np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction))))","965d5097":"LRMSE_model.append(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction)))) # for storing the LRMSE \nName_model.append('Elastic_Net')","09073a57":"from sklearn.linear_model import Ridge\nPipeline_ridge=make_pipeline(StandardScaler(), Ridge())\nest5=Pipeline_ridge.fit(X_train,y_train)\nprediction=est5.predict(X_test)","28e07555":"print('MAE:', metrics.mean_absolute_error(y_test, prediction)) #Mean absolute error\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))#Mean squared error (punishes outliers)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\nprint('Logaritmic RMSE:', np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction))))","2852315c":"LRMSE_model.append(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction)))) # for storing the LRMSE \nName_model.append('Ridge')","de975e29":"from sklearn.ensemble import RandomForestClassifier\nPipeline_RFC=make_pipeline(StandardScaler(), RandomForestClassifier())\nest6=Pipeline_RFC.fit(X_train,y_train)\nprediction=est6.predict(X_test)","aaec305f":"print('MAE:', metrics.mean_absolute_error(y_test, prediction)) #Mean absolute error\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))#Mean squared error (punishes outliers)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))\nprint('Logaritmic RMSE:', np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction))))","0ce38732":"LRMSE_model.append(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(prediction)))) # for storing the LRMSE \nName_model.append('RandomForestClassifier')","825b306b":"sns.set_style=('darkgrid')\nlst=[]\nfor x,y in zip(Name_model,LRMSE_model):\n    lst.append((x,y))\nEstimators=pd.DataFrame(lst,columns=['Name','LRMSE'])\nplt.figure(figsize=(16,4))\nsns.set()\nsns.lineplot(data=Estimators,x='Name',y='LRMSE');","c158c7e5":"Estimators","b2ce156b":"prediction_true=est3.predict(test)#SVR model\nsubmission_file=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission_file=submission_file.drop('SalePrice',axis=1)\nsubmission_file['SalePrice']=prediction_true\nsubmission_file.index=submission_file['Id']\nsubmission_file.drop('Id',axis=1,inplace=True)\nsubmission_file.head(10)\n#LRMSE for the test data was 0.1471","2219d7bd":"# **Starting with Stochastic Gradient Descent (SGD) regressor.**","8ce4260f":"# **Chossing the right estimator**","521dd697":"# **Model & Evaluation Elastic-Net**","714d14f9":"# **Model Evalutation SGD**","9f21041e":"# **Model & Evaluation Ridge regression**","abded44a":"# **We are going to look at the results of using different models - SGD regressor, Lasso, SVR...**","81eb4869":"# **Lets analyze the Features of the data, lets look at the nummeric data.**","11d821ca":"# **Feature Engineering Numeric Data**","6d897eb5":"# **Starting by making train_test_split, this test split is used throughout the model testing.**","950e257d":"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home","6d722fcd":"# **Model & Evaluation Random Forest**","c3b47be8":"# **Model & Evaluation SVR**","b17994e9":"# **Feature Engineering Cathegorical Data**","98961c88":"# **Lets convert cathegorical data to nummerica data with dummie variables, which is going to be needed when training the ML algorithm.**","e4fb7e22":"# **Model & Evaluation Lasso**","ed77c5ef":"# **Competition Description**"}}