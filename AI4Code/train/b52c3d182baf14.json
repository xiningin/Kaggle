{"cell_type":{"53fc659d":"code","cf2cdb00":"code","f1670c7b":"code","b387eafc":"code","64d77593":"code","b04941bf":"code","2498434e":"code","d0843f4b":"code","55752389":"code","70601466":"code","c6ed9f5c":"code","d7471653":"code","ae03af08":"code","44b314ad":"code","d4775ff7":"code","2cfcac71":"code","7166cdfc":"code","151ab65f":"code","3ef8cc79":"code","528fdf3f":"code","5ed471cd":"code","36bef6b8":"code","0bbd0429":"code","5b9fe158":"code","f09cfa5f":"code","3c71567f":"code","3b283c09":"code","cf12a7dc":"code","48da7d41":"code","30964185":"code","5eebe5a7":"code","8e1252b6":"code","ea8e3482":"code","a3b15d53":"code","d203b8e2":"code","a7c2a9b2":"code","4e226069":"code","6de41863":"code","a1d1474d":"code","df2c346f":"code","16cb8009":"code","5907b620":"code","be77546e":"code","8dcabee1":"code","b681e0b2":"code","2e606768":"code","fa97f823":"code","02ddd0cc":"code","1b7328d7":"code","7bedeea6":"code","625f0f3e":"code","7ba2e9c8":"code","4dacd16e":"code","80337a06":"code","7395a402":"code","b8b059d8":"code","678906d1":"code","1c671dd5":"code","e587082d":"code","1024bd18":"code","2e861f67":"code","3905cde2":"code","410f85ce":"code","82f24609":"code","552898a4":"code","58baa4ef":"code","bbe02e86":"code","ce7d0484":"code","b6cec669":"code","6696db70":"code","c4578dac":"code","c070ab0c":"code","7d22a211":"code","5b0b83f9":"code","1311b1de":"code","e33b8b24":"code","b7d8a330":"code","00286991":"code","af28c90e":"code","4aa749bf":"code","76dfcdc7":"code","d4075680":"code","c7ebe240":"code","b51e41a2":"code","aa0e41d5":"code","793fe0c9":"markdown","395231f8":"markdown","1d264fa7":"markdown","d2b33d91":"markdown","85c08027":"markdown","63e5bbe8":"markdown","77fbd4df":"markdown","d81e9f90":"markdown","1c7ac646":"markdown","b67522b9":"markdown","bc7f28e9":"markdown","93d43899":"markdown","f77113a4":"markdown","01ecf496":"markdown","ecb4db89":"markdown","362b26e5":"markdown","6bf8bff6":"markdown","53ed981d":"markdown","967fd014":"markdown"},"source":{"53fc659d":"import re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud \nfrom nltk.tokenize import word_tokenize \n\n\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0","cf2cdb00":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","f1670c7b":"train.head()","b387eafc":"train.shape, test.shape, test.shape[0]\/train.shape[0]","64d77593":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","b04941bf":"train.info()","2498434e":"null_counts = pd.DataFrame({\"Num_Null\": train.isnull().sum()})\nnull_counts[\"Pct_Null\"] = null_counts[\"Num_Null\"] \/ train.count() * 100\nnull_counts","d0843f4b":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","55752389":"len(train[\"keyword\"].value_counts())","70601466":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","c6ed9f5c":"armageddon_tweets = train[(train[\"keyword\"].fillna(\"\").str.contains(\"armageddon\")) & (train[\"target\"] == 0)]\nprint(\"An example tweet:\\n\", armageddon_tweets.iloc[10, 3])\narmageddon_tweets.head()","d7471653":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster \/ tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.head()","ae03af08":"keywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False).head(10)","44b314ad":"keywords_vc.sort_values(by=\"Disaster_Probability\").head(10)","d4775ff7":"locations_vc = train[\"location\"].value_counts()\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\nplt.title(\"Top 30 Locations\")\nplt.show()","2cfcac71":"len(train[\"location\"].value_counts())","7166cdfc":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:30].index, x=disaster_locations[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:30].index, x=nondisaster_locations[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","151ab65f":"train[\"tweet_length\"] = train[\"text\"].apply(len)\nsns.distplot(train[\"tweet_length\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","3ef8cc79":"min(train[\"tweet_length\"]), max(train[\"tweet_length\"])","528fdf3f":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","5ed471cd":"def count_words(x):\n    return len(x.split())\n\ntrain[\"num_words\"] = train[\"text\"].apply(count_words)\nsns.distplot(train[\"num_words\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","36bef6b8":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"num_words\")\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","0bbd0429":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) \/ len(x.split())\n\ntrain[\"avg_word_length\"] = train[\"text\"].apply(avg_word_length)\nsns.distplot(train[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","5b9fe158":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")","f09cfa5f":"def create_corpus(target):\n    corpus = []\n\n    for w in train.loc[train[\"target\"] == target][\"text\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef create_corpus_dict(target):\n    corpus = create_corpus(target)\n            \n    stop_dict = defaultdict(int)\n    for word in corpus:\n        if word in stopwords:\n            stop_dict[word] += 1\n    return sorted(stop_dict.items(), key=lambda x:x[1], reverse=True)","3c71567f":"corpus_disaster_dict = create_corpus_dict(0)\ncorpus_non_disaster_dict = create_corpus_dict(1)\n\ndisaster_x, disaster_y = zip(*corpus_disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*corpus_non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \nax[0].set_title(\"Top 30 Stop Words - Disaster Tweets\")\nax[0].set_xlabel(\"Stop Word Frequency\")\nax[1].set_title(\"Top 30 Stop Words - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Stop Word Frequency\")\nplt.tight_layout()\nplt.show()","3b283c09":"corpus_disaster, corpus_non_disaster = create_corpus(1), create_corpus(0)\ncounter_disaster, counter_non_disaster = Counter(corpus_disaster), Counter(corpus_non_disaster)\nx_disaster, y_disaster, x_non_disaster, y_non_disaster = [], [], [], []\n\ncounter = 0\nfor word, count in counter_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_disaster.append(word)\n        y_disaster.append(count)\n\ncounter = 0\nfor word, count in counter_non_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_non_disaster.append(word)\n        y_non_disaster.append(count)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\nax[0].set_title(\"Top 15 Non-Stopwords - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Non-Stopwords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","cf12a7dc":"def bigrams(target):\n    corpus = train[train[\"target\"] == target][\"text\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = count_vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","48da7d41":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","30964185":"target_vc = train[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","5eebe5a7":"train","8e1252b6":"from nltk.corpus import stopwords\n\n#function for removing pattern\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt\n\n# remove '#' handle\ntrain['tweet'] = np.vectorize(remove_pattern)(train['text'], \"#[\\w]*\")\ntest['tweet'] = np.vectorize(remove_pattern)(test['text'], \"#[\\w]*\") \ntrain.head()\n\n#Delete everything except alphabet\ntrain['tweet'] = train['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntest['tweet'] = test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntrain.head()\n\n\n#Dropping words whose length is less than 3\ntrain['tweet'] = train['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest['tweet'] = test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain.head()\n\n\n#convert all the words into lower case\ntrain['tweet'] = train['tweet'].str.lower()\ntest['tweet'] = test['tweet'].str.lower()\n\n\nset(stopwords.words('english'))\n\n# set of stop words\nstops = set(stopwords.words('english')) \n\n# tokens of words  \ntrain['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\ntest['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n\n#function to remove stop words\ndef remove_stops(row):\n    my_list = row['tokenized_sents']\n    meaningful_words = [w for w in my_list if not w in stops]\n    return (meaningful_words)\n\n#removing stop words\ntrain['clean_tweet'] = train.apply(remove_stops, axis=1)\ntest['clean_tweet'] = test.apply(remove_stops, axis=1)\ntrain.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\ntest.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\n\n#re-join the words after tokenization\ndef rejoin_words(row):\n    my_list = row['clean_tweet']\n    joined_words = ( \" \".join(my_list))\n    return joined_words\n\ntrain['clean_tweet'] = train.apply(rejoin_words, axis=1)\ntest['clean_tweet'] = test.apply(rejoin_words, axis=1)\ntrain.head()\n","ea8e3482":"all_word = ' '.join([text for text in train['clean_tweet']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_word) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off') \nplt.show()","a3b15d53":"normal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 1]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","d203b8e2":"normal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 0]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","a7c2a9b2":"import gc\nimport os\nimport time\nimport math\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom transformers import *\nfrom sklearn.metrics import *\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport re\nimport string\nimport folium\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport transformers\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.optimizers import Adam\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n\n\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,HashingVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nfrom xgboost import XGBClassifier\n\nstopword=set(STOPWORDS)\n\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)\nrandom_state = 29","4e226069":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            del obj\n    gc.collect()\n    \n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()","6de41863":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsub= pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","a1d1474d":"train.describe()\n","df2c346f":"test.describe()","16cb8009":"train.head()\n","5907b620":"test.head()\n","be77546e":"\nsub.head()\n","8dcabee1":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","b681e0b2":"# Remove all URLs, replace by URL\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'URL',text)\n\n# Remove HTML beacon\ndef remove_HTML(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# Remove non printable characters\ndef remove_not_ASCII(text):\n    text = ''.join([word for word in text if word in string.printable])\n    return text\n\n# Change an abbreviation by its true meaning\ndef word_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Replace all abbreviations\ndef replace_abbrev(text):\n    string = \"\"\n    for word in text.split():\n        string += word_abbrev(word) + \" \"        \n    return string\n\n# Remove @ and mention, replace by USER\ndef remove_mention(text):\n    at=re.compile(r'@\\S+')\n    return at.sub(r'USER',text)\n\n# Remove numbers, replace it by NUMBER\ndef remove_number(text):\n    num = re.compile(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*')\n    return num.sub(r'NUMBER', text)\n\n# Remove all emojis, replace by EMOJI\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'EMOJI', text)\n\n# Replace some others smileys with SADFACE\ndef transcription_sad(text):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]\"\n    smiley = re.compile(r'[8:=;][\\'\\-]?[(\\\\\/]')\n    return smiley.sub(r'SADFACE', text)\n\n# Replace some smileys with SMILE\ndef transcription_smile(text):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]\"\n    smiley = re.compile(r'[8:=;][\\'\\-]?[)dDp]')\n    #smiley = re.compile(r'#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}\/i')\n    return smiley.sub(r'SMILE', text)\n\n# Replace <3 with HEART\ndef transcription_heart(text):\n    heart = re.compile(r'<3')\n    return heart.sub(r'HEART', text)\n\n# Factorize elongated words, add ELONG\ndef remove_elongated_words(text):\n    rep = re.compile(r'\\b(\\S*?)([a-z])\\2{2,}\\b')\n    return rep.sub(r'\\1\\2 ELONG', text)\n\n# Factorize repeated punctuation, add REPEAT\ndef remove_repeat_punct(text):\n    rep = re.compile(r'([!?.]){2,}')\n    return rep.sub(r'\\1 REPEAT', text)\n\n\n# Remove all punctuations\ndef remove_all_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n# Remove punctuations\ndef remove_punct(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\" \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    if '...' not in text:\n        text = text.replace('..', ' ... ')   \n    return text\n\n# Remove all english stopwords\ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords)])\n    return text","2e606768":"def clean_tweet(text):\n    \n    # Remove non text\n    text = remove_URL(text)\n    text = remove_HTML(text)\n    text = remove_not_ASCII(text)\n    \n    # Lower text, replace abbreviations\n    text = text.lower()\n    text = replace_abbrev(text)  \n    text = remove_mention(text)\n    text = remove_number(text)\n    \n    # Remove emojis \/ smileys\n    text = remove_emoji(text)\n    text = transcription_sad(text)\n    text = transcription_smile(text)\n    text = transcription_heart(text)\n    \n    # Remove repeated puntuations \/ words\n    text = remove_elongated_words(text)\n    text = remove_repeat_punct(text)\n\n    #text = remove_all_punct(text)\n    #text = remove_punct(text)\n    #text = remove_stopwords(text)\n\n    return text","fa97f823":"# Clean text and add a new feature\ntrain[\"clean_text\"] = train[\"text\"].apply(clean_tweet)\ntest[\"clean_text\"] = test[\"text\"].apply(clean_tweet)\n\ntrain[\"clean_tokens\"] = train[\"clean_text\"].apply(lambda x: word_tokenize(x))\ntest[\"clean_tokens\"] = test[\"clean_text\"].apply(lambda x: word_tokenize(x))","02ddd0cc":"train[['clean_text','text']].head(10)","1b7328d7":"from gensim.models import Word2Vec\n\nskip_gram_model = Word2Vec(train['clean_tokens'],size=150,window=3,min_count=2,sg=1)\nskip_gram_model.train(train['clean_tokens'],total_examples=len(train['clean_tokens']),epochs=10)\n\ncbow_model = Word2Vec(train['clean_tokens'],size=150,window=3,min_count=2)\ncbow_model.train(train['clean_tokens'],total_examples=len(train['clean_tokens']),epochs=10)\n\n\ntrain[\"clean_tokens\"] = train[\"text\"].apply(lambda x: word_tokenize(x))\ntest[\"clean_tokens\"] = test[\"text\"].apply(lambda x: word_tokenize(x))\ndef get_word_embeddings(token_list,vector,k=150):\n    if len(token_list) < 1:\n        return np.zeros(k)\n    else:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in token_list] \n        \n    sum = np.sum(vectorized,axis=0)\n    return sum\/len(vectorized) ","7bedeea6":"max_features=5000\n\n# CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=max_features)\nsparce_matrix_train=count_vectorizer.fit_transform(train['clean_text'])\nsparce_matrix_test=count_vectorizer.fit_transform(train['clean_text'])\n\ndef count_vector(data):\n    count_vectorizer = CountVectorizer()\n    vect = count_vectorizer.fit_transform(data)\n    return vect, count_vectorizer\n\ndef tfidf_vector(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    vect = tfidf_vectorizer.fit_transform(data)\n    return vect, tfidf_vectorizer\n\ndef skip_gram_vectorizer(tokens,vector):\n        embeddings = tokens.apply(lambda x: get_word_embeddings(x, skip_gram_model))\n        return list(embeddings)\n\ndef cbow_vectorizer(tokens,vector):\n        embeddings = tokens.apply(lambda x: get_word_embeddings(x, cbow_model))\n        return list(embeddings)\n\nX_train_count, count_vectorizer = count_vector(train['clean_text'])\nX_train_tfidf, tfidf_vectorizer = tfidf_vector(train['clean_text'])\nX_train_skip_gram = skip_gram_vectorizer(train['clean_tokens'],skip_gram_model)\nX_train_cbow = cbow_vectorizer(train['clean_tokens'],cbow_model)\n\nX_test_count = count_vectorizer.transform(test['clean_text'])                                                     \nX_test_tfidf = tfidf_vectorizer.transform(test['clean_text'])\nX_test_skip_gram = skip_gram_vectorizer(test['clean_tokens'],skip_gram_model)\nX_test_cbow = cbow_vectorizer(test['clean_tokens'],cbow_model)\n\nX_train_skip_gram = np.array(X_train_skip_gram)\nX_train_cbow = np.array(X_train_cbow)\nX_test_skip_gram = np.array(X_test_skip_gram)\nX_test_cbow = np.array(X_test_cbow)\n\n\nimport pickle    \n\nwith open('count_vectorizer.pickle', 'wb') as handle:\n    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('tfidf_vectorizer.pickle', 'wb') as handle:\n    pickle.dump(tfidf_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('skip_gram_model.pickle', 'wb') as handle:\n    pickle.dump(skip_gram_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('cbow_model.pickle', 'wb') as handle:\n    pickle.dump(cbow_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('cbow_model.pickle', 'wb') as handle:\n    pickle.dump(cbow_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('cbow_model.pickle', 'wb') as handle:\n    pickle.dump(cbow_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n","625f0f3e":"metrics = pd.DataFrame(columns=['model' ,'vectoriser', 'f1 score', 'train accuracy','test accuracy'])","7ba2e9c8":"from joblib import dump\n\ndef fit_and_predict(model,x_train,x_test,y_train,y_test,vectoriser):\n    classifier = model\n    classifier_name = str(classifier.__class__.__name__)\n\n    if classifier_name==\"MultinomialNB\" and (vectoriser==\"skip gram vector\" or vectoriser==\"cbow vector\"):\n        return\n\n    classifier.fit(x_train,y_train)\n    filename = classifier_name +\" using \"+ str(vectoriser)+\".joblib\"\n    filename = filename.lower().replace(\" \",\"_\")\n    dump(model, filename=filename)\n    y_pred = classifier.predict(x_test)\n    cmatrix = confusion_matrix(y_test,y_pred)\n\n\n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(classifier))\n    plt.show()\n\n\n    f1score = f1_score(y_test,y_pred,average='weighted')\n    train_accuracy = round(classifier.score(x_train,y_train)*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n\n    global metrics\n    metrics = metrics.append({\n                              'model': classifier_name,\n                              'f1 score': f1score, \n                              'train accuracy': train_accuracy, \n                              'test accuracy': test_accuracy, \n                              'vectoriser': str(vectoriser),\n                             },\n                               ignore_index=True\n                            )\n\n    print(str(classifier.__class__.__name__) +\" using \"+ str(vectoriser))\n    print(classification_report(y_test,y_pred))    \n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))\n","4dacd16e":"models=[\n        XGBClassifier(max_depth=6, n_estimators=1000),\n        LogisticRegression(random_state=random_state),\n        SVC(random_state=random_state),\n        MultinomialNB(),\n        DecisionTreeClassifier(random_state = random_state),\n        KNeighborsClassifier(),\n        RandomForestClassifier(random_state=random_state),\n       ]","80337a06":"for model in models:\n    y = train.target\n\n    x = X_train_count\n    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)\n    fit_and_predict(model,x_train,x_test,y_train,y_test,'Count vector')\n    \n    x = X_train_tfidf\n    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)\n    fit_and_predict(model,x_train,x_test,y_train,y_test, 'Tfidf vector')\n    \n    x = X_train_skip_gram\n    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)\n    fit_and_predict(model,x_train,x_test,y_train,y_test, 'skip gram vector')\n    \n    x = X_train_cbow\n    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)\n    fit_and_predict(model,x_train,x_test,y_train,y_test, 'cbow vector')    ","7395a402":"metrics = metrics.sort_values('f1 score',ascending=False)","b8b059d8":"metrics","678906d1":"free_gpu_cache()","1c671dd5":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM,GRU, Dropout, Activation, Input, Flatten, Bidirectional, Conv1D, MaxPooling1D\nfrom tensorflow.keras import layers\nfrom keras import optimizers\nfrom functools import reduce","e587082d":"def train_lstm(x_train,x_test,y_train,y_test,vectorizer_name,vocab_size,input_length):\n    epochs = 1\n    verbose = 1\n    batch_size = 32\n    embed_dim = 32\n    optimizer = optimizers.Adam(lr=0.002)\n    \n    model = Sequential()\n    model.add(Embedding(vocab_size, embed_dim,input_length = input_length))\n    model.add(Dropout(0.2))\n    model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.4))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n    \n    print(model.summary())\n\n    model.fit(x_train, y_train, epochs = epochs, verbose = verbose, batch_size=batch_size, validation_data=(x_test, y_test))\n\n    model_name = \"LSTM using \"+ vectorizer_name \n    model.save(model_name)\n    y_pred = model.predict(x_test)\n\n    f1score = f1_score(y_test,y_pred.round(),average='weighted')\n    train_accuracy = round(accuracy_score(y_train,reduce(list.__add__, model.predict(x_train).round().tolist()))*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred.round())*100)\n\n    global metrics\n    metrics = metrics.append({\n                              'model': model_name,\n                              'f1 score': f1score, \n                              'train accuracy': train_accuracy, \n                              'test accuracy': test_accuracy, \n                              'vectoriser': vectorizer_name,\n                             },\n                               ignore_index=True\n                            )\n\n    print(model_name)\n    print(classification_report(y_test,y_pred.round()))    \n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))","1024bd18":"y = train['target'].values\n\n\n# x_train, x_test, y_train, y_test = train_test_split(X_train_cbow),y, test_size = 0.3)\n# train_lstm(x_train,x_test,y_train,y_test, 'cbow vector',5329,150)\n\nx_train, x_test, y_train, y_test = train_test_split(X_train_skip_gram,y, test_size = 0.3)\ntrain_lstm(x_train,x_test,y_train,y_test, 'skip gram vector',5329,150)\n\n# x_train, x_test, y_train, y_test = train_test_split(X_train_count,y, test_size = 0.3)\n# train_lstm(x_train,x_test,y_train,y_test,'Count vector',14455,7613)\n\n# x_train, x_test, y_train, y_test = train_test_split(X_train_tfidf,y, test_size = 0.3)\n# train_lstm(x_train,x_test,y_train,y_test, 'Tfidf vector',14455,7613)","2e861f67":"%reset -f","3905cde2":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\nimport gc\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            del obj\n    gc.collect()\n    \n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()","410f85ce":"import re\nimport torch\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\nimport torch\nfrom sklearn.metrics import classification_report\nimport random\nimport time\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split","82f24609":"if torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","552898a4":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ndf_train= train\ndf_test= test","58baa4ef":"\ndef preprocess(text):\n    text=text.lower()\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text)\n    text = re.sub(r'http?:\\\/\\\/.*[\\r\\n]*', '', text)\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>')\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n    text=re.sub(r'[:\"#$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+','',text)\n    text=re.sub(r'[!]+','!',text)\n    text=re.sub(r'[?]+','?',text)\n    text=re.sub(r'[.]+','.',text)\n    text=re.sub(r\"'\",\"\",text)\n    text=re.sub(r\"\\(\",\"\",text)\n    text=re.sub(r\"\\)\",\"\",text)\n    text=\" \".join(text.split())\n    return text\ndf_train['text'] = df_train['text'].apply(preprocess)\ndf_test['text'] = df_test['text'].apply(preprocess)\ndf_train=df_train[df_train[\"text\"]!='']","bbe02e86":"df_train=df_train[[\"text\",\"target\"]]","ce7d0484":"texts = df_train.text.values\nlabels = df_train.target.values","b6cec669":"torch.cuda.empty_cache()\ntokenizer = ElectraTokenizer.from_pretrained('google\/electra-base-discriminator')\nmodel = ElectraForSequenceClassification.from_pretrained('google\/electra-base-discriminator',num_labels=2)\nmodel.cuda()","6696db70":"indices=tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\ninput_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]","c4578dac":"train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\n\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=42, test_size=0.2)","c070ab0c":"# Converting all of our data into torch tensors\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nvalidation_labels = torch.tensor(validation_labels, dtype=torch.long)\ntrain_masks = torch.tensor(train_masks, dtype=torch.long)\nvalidation_masks = torch.tensor(validation_masks, dtype=torch.long)","7d22a211":"batch_size = 32\n\n# the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","5b0b83f9":"optimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 5\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","1311b1de":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","e33b8b24":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","b7d8a330":"# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 100 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # As we unpack the batch, we'll also copy each tensor to the GPU  \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n\n        loss = outputs[0]\n\n        total_loss += loss.item()\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n      \nprint(\"\")\nprint(\"Training complete!\")","00286991":"# Validation               \n\nprint(\"\")\nprint(\"Running Validation...\")\n\nt0 = time.time()\n\nmodel.eval()\n\npreds=[]\ntrue=[]\n\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# Evaluate data for one epoch\nfor batch in validation_dataloader:\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    \n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # Telling the model not to compute or store gradients, saving memory and\n    # speeding up validation\n    with torch.no_grad():        \n\n        # Forward pass, calculate logit predictions.\n        # This will return the logits rather than the loss because we have\n        # not provided labels.\n        # token_type_ids is the same as the \"segment ids\", which \n        # differentiates sentence 1 and 2 in 2-sentence tasks.\n\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    \n    # Get the \"logits\" output by the model. The \"logits\" are the output\n    # values prior to applying an activation function like the softmax.\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    preds.append(logits)\n    true.append(label_ids)\n    # Calculate the accuracy for this batch of test sentences.\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n    # Accumulate the total accuracy.\n    eval_accuracy += tmp_eval_accuracy\n\n    # Track the number of batches\n    nb_eval_steps += 1\n\n# Report the final accuracy for this validation run.\nprint(\"Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\nprint(\"Validation took: {:}\".format(format_time(time.time() - t0)))","af28c90e":"# report = {}\n# report['model'] = 'Electra'\n# report['test accuracy'] = 0.82\n# metrics = metrics.append(report,ignore_index=True)","4aa749bf":"# Combine the predictions for each batch into a single list of 0s and 1s.\nflat_predictions = [item for sublist in preds for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = [item for sublist in true for item in sublist]","76dfcdc7":"comments1 = df_test.text.values\n\nindices1=tokenizer.batch_encode_plus(comments1,max_length=128,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\ninput_ids1=indices1[\"input_ids\"]\nattention_masks1=indices1[\"attention_mask\"]\n\nprediction_inputs1= torch.tensor(input_ids1)\nprediction_masks1 = torch.tensor(attention_masks1)\n\n\n# Set the batch size.  \nbatch_size = 32 \n\n# Create the DataLoader.\nprediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\nprediction_sampler1 = SequentialSampler(prediction_data1)\nprediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)\n\n","d4075680":"print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions = []\n\n# Predict \nfor batch in prediction_dataloader1:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids1, b_input_mask1 = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs1 = model(b_input_ids1, token_type_ids=None, \n                      attention_mask=b_input_mask1)\n\n  logits1 = outputs1[0]\n\n  # Move logits and labels to CPU\n  logits1 = logits1.detach().cpu().numpy()\n  \n  \n  # Store predictions and true labels\n  predictions.append(logits1)\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","c7ebe240":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsubmit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':flat_predictions})","b51e41a2":"df_leak = pd.read_csv('\/kaggle\/input\/disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv', encoding ='ISO-8859-1')[['choose_one', 'text']]\n\n# Creating target and id\ndf_leak['target'] = (df_leak['choose_one'] == 'Relevant').astype(np.int8)\ndf_leak['id'] = df_leak.index.astype(np.int16)\ndf_leak.drop(columns=['choose_one', 'text'], inplace=True)\n\n# Merging target to test set\ndf_test = df_test.merge(df_leak, on=['id'], how='left')\n\nprint('Leaked Data Set Shape = {}'.format(df_leak.shape))\nprint('Leaked Data Set Memory Usage = {:.2f} MB'.format(df_leak.memory_usage().sum() \/ 1024**2))\nprint(df_test)\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = df_test['target'].values\nsubmission.to_csv('submission.csv', index=False)\nprint(submission)","aa0e41d5":"# with open('bert_model.pickle', 'wb') as handle:\n#     pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)","793fe0c9":"### Target","395231f8":"### Location","1d264fa7":"### Text aka Tweet Content\n\n**tweet length distribution**","d2b33d91":"# Preprocessing","85c08027":"## Visualization of all the words which signify unreal disaster","63e5bbe8":"* LogisticRegression\n* SVC\n* MultinomialNB\n* DecisionTreeClassifier \n* KNeighborsClassifier\n* RandomForestClassifier","77fbd4df":"## Common Bigrams","d81e9f90":"## Common Stopwords","1c7ac646":"## Common non-stopwords","b67522b9":"## Average Word Length","bc7f28e9":"### Keyword\n\nWhat are some of the most commonly used keywords?","93d43899":"## Visualization of all the words using word cloud","f77113a4":"# Models","01ecf496":"## Number of Words","ecb4db89":"### LSTM","362b26e5":"# Vectorisation","6bf8bff6":"## Electra","53ed981d":"# Exploratory Data Analysis","967fd014":"## Visualization of all the words which signify real disaster"}}