{"cell_type":{"9a054939":"code","e26faae3":"code","1a716a5c":"code","3655e497":"code","19a885e1":"code","7ddd1fc4":"code","bb556fc1":"code","6b723da0":"code","448fec3c":"code","7b8ca5f1":"code","b41b0cb7":"code","1059f76e":"code","5f160f39":"code","b463d99f":"code","7574661a":"code","7b0023c8":"code","3d6d15d9":"code","c1b63621":"code","b45eb02e":"code","729cdef7":"code","f954fdee":"code","6d9d2a32":"code","8153afe1":"code","3089124c":"code","99f22d61":"code","1b46c0e8":"code","5e60b138":"code","37f8b967":"code","9dae2765":"code","2f847f33":"code","b58a8e21":"code","ff4aade8":"code","14eb8a48":"code","ed4b9e26":"code","366a3215":"code","3d942eba":"code","eda01293":"code","63d1c508":"code","811ab805":"markdown","8be99bbb":"markdown","2e28e98d":"markdown","fbbae91a":"markdown","153f1b43":"markdown","cd013bd9":"markdown"},"source":{"9a054939":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e26faae3":"#loading dataset\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/gender_submission.csv\")\nprint(\"train data shape: {},\\ntest data shape: {} , \\nsubmission data shape: {}, \".format(train.shape, test.shape, submission.shape))","1a716a5c":"train.head()","3655e497":"test.head()","19a885e1":"#columns in train and test dataset\nprint('Columns in train: {} \\nColumns in test: {}'.format(train.columns, test.columns))","7ddd1fc4":"#checking null values\nprint(\"null values in train dataset:\\n{}\".format(train.isnull().sum()))","bb556fc1":"#checking null values\nprint(\"null values in test dataset:\\n{}\".format(test.isnull().sum()))","6b723da0":"\nprint(\"missing values of Cabin in train: {}\".format(train.Cabin.isnull().sum()\/len(train.Cabin)*100), '\\n')\n\nprint(\"missing values of Age in train: {}\".format(train.Age.isnull().sum()\/len(train.Age)*100), '\\n')\n\nprint(\"missing values of Embarked in train: {}\".format(train.Embarked.isnull().sum()\/len(train.Embarked)*100), '\\n')\n\nprint(\"missing values of Fare in test: {}\".format(test.Fare.isnull().sum()\/len(test.Fare)*100), '\\n')\n\nprint(\"missing values of Cabin in test: {}\".format(test.Cabin.isnull().sum()\/len(test.Cabin)*100), '\\n')\n\nprint(\"missing values of Age in test: {}\".format(test.Age.isnull().sum()\/len(test.Age)*100), '\\n')","448fec3c":"#filling null values\ntrain['Age'].fillna(train['Age'].mean(), inplace = True)\n\ntest['Age'].fillna(test['Age'].mean(), inplace = True)\n\ntest['Fare'].fillna(test['Fare'].mean(), inplace = True)\n\ntrain['Embarked'].fillna(train['Embarked'].fillna('S'), inplace = True)#only two values are missing that's why we can fill anything S, Q, N\n\n","7b8ca5f1":"#In Cabin 70 % test and 78% missing values so we can drop it\ntrain = train.drop(['Cabin'], axis =1)\ntest = test.drop(['Cabin'], axis =1)","b41b0cb7":"#checking again null values\nprint(\"null values in train dataset:\\n{}\".format(train.isnull().sum()))\nprint('***'*20)\n#checking again null values\nprint(\"null values in train dataset:\\n{}\".format(test.isnull().sum()))","1059f76e":"train.head()","5f160f39":"train.Sex = train.Sex.replace({\"male\":1, \"female\" : 0})\ntest.Sex = test.Sex.replace({\"male\":1, \"female\" : 0})","b463d99f":"#Dealing with Embarked\ntrain.groupby(['Embarked'], axis =0).mean()","7574661a":"train.groupby(['Survived'], axis =0).mean()","7b0023c8":"train.groupby(['Pclass'], axis =0).mean()","3d6d15d9":"train.groupby(['Parch'], axis =0).mean()","c1b63621":"#how many male and female survived in Titanic\ntrain['Survived'].value_counts().plot.bar()\n","b45eb02e":"#how many person survived in Titanic from Embarked\ntrain['Embarked'].value_counts().plot.bar()\n","729cdef7":"#how many male and female survived in Pclass\ntrain['Pclass'].value_counts().plot.bar()\n","f954fdee":"train['SibSp'].value_counts().plot.bar()","6d9d2a32":"\ntrain['Embarked'] = pd.get_dummies(train['Embarked'])\n\ntest['Embarked'] = pd.get_dummies(test['Embarked'])\n\n\n\n","8153afe1":"\nsns.pairplot(train, hue = 'Survived', diag_kind=\"kde\") ","3089124c":"X_train = train.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis =1)\ny_train = train['Survived']\nX_test = test.drop(['PassengerId','Name', 'Ticket'], axis =1)\n","99f22d61":"def acc():\n    print('score :', clf.score(X_train, y_train))\n   \n    return ","1b46c0e8":"\n#logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nacc()\n","5e60b138":"#random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, max_depth=3,random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nclf.score(X_train, y_train)\nacc()","37f8b967":"\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nclassifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nacc()","9dae2765":"#Support Vector Machine\nfrom sklearn import svm\nclf = svm.SVC()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nclf.score(X_train, y_train)\nacc()","2f847f33":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nacc()","b58a8e21":"#decision Tree\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nacc()","ff4aade8":"#xgboost\nfrom xgboost import XGBClassifier\nclf = XGBClassifier()\n\nclf.fit(X_train, y_train)\n\nY_pred = clf.predict(X_test)\nacc()","14eb8a48":"#tunning logistic regresion\nc = [1,10,1000]\nfor i in c:\n    from sklearn.linear_model import LogisticRegression\n    clf = LogisticRegression(penalty='l2', C=i)\n\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    acc()\n\n","ed4b9e26":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nimport numpy as np","366a3215":"clf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nNB = GaussianNB()\nclf3 = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=NB)","3d942eba":"for clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, X_train, y_train, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","eda01293":"NB = GaussianNB()\nclf.fit(X_train, y_train)\n\nY_pred = clf.predict(X_test)\nscores = model_selection.cross_val_score(clf, X_train, y_train, \n                                              cv=5, scoring='accuracy')\nscores","63d1c508":"\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","811ab805":"About Dataset:\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","8be99bbb":"We dont't have a survived column in test data","2e28e98d":"\n\nWe have 177 missing value in age, 2 in Embarked and 687 in Cabin","fbbae91a":"We have 86 in Age, 327 in Cabin and 1 in Fare missing  Values","153f1b43":"Looking in dataset","cd013bd9":"As a we can see when score is increasing our loss is also increasing. So in this way can learn that predction \nis not only way to measure our model. \nIn **Desicion Tree**  we have 98% score but loss is too high so our model is just a dumb model"}}