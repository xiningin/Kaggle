{"cell_type":{"1a782a40":"code","1ce6e7c3":"code","105863ce":"code","1601d376":"code","60c2b652":"code","9129033c":"code","9396b2e4":"code","811bd722":"code","f53670c5":"code","bfe0bfdc":"code","59167479":"code","68588419":"code","3e6ae23e":"code","c0bc0c72":"code","7ba51d2b":"code","aa9858f2":"code","04cf99e3":"code","bbfd6862":"code","21ec42bf":"code","1e381512":"code","054f3a89":"code","0bf187f7":"code","6d733cd5":"code","fdbe7902":"code","7a60fa8e":"code","f901c184":"code","925c9a95":"code","e7a3c1c5":"code","36166fef":"code","d4b0d025":"code","8ee473f7":"code","fbb60e75":"code","8ba93d7b":"code","9b6352a1":"code","4d340cb2":"code","c5855053":"code","aa394c17":"code","ae7ae6b6":"code","65bf171b":"code","cdc5aaa9":"code","b2145fed":"code","33cc7734":"code","4fc7fc82":"code","4b9cf5cd":"code","be401b2c":"code","4711b39d":"code","9dbb594f":"code","b29c5c47":"code","a7afc377":"code","9a2c0699":"code","407d4aeb":"code","23855bc5":"code","2e8b5c17":"code","fceb1eea":"code","a08744b3":"code","315e6625":"code","5512864d":"code","9688c5c1":"code","1f86a5d4":"code","1123026e":"code","08798f1d":"code","b1635526":"code","33c2518c":"code","5be5e7ec":"code","3be34cb0":"code","1ccdb64f":"code","6301b899":"code","1656787e":"code","26f50041":"code","141ff426":"code","21fbd7b0":"code","ee40b27c":"code","440e7326":"code","bcb65e90":"code","ff75d60f":"code","9b237ef5":"code","161465d0":"code","32f0f115":"code","3dc36f65":"code","9f5acb4c":"code","eafd28b6":"code","d2f405eb":"code","926d265c":"code","db76e7eb":"code","a36bdd46":"code","2db3e7ee":"code","cbc74a88":"code","ef2ef156":"code","9a8b4b29":"code","afa93fa4":"code","0806adfc":"code","9139e3f3":"code","3476538e":"code","cc677144":"code","cbefa913":"code","9153205e":"code","533ac765":"code","6f888a7e":"code","e4e2d0a1":"code","5b8a6074":"code","5d5b0933":"code","4944f354":"code","cc45f501":"code","47ca6be2":"code","97c86317":"code","0d11fcbe":"code","8e94d864":"code","f08ed988":"code","06fdced3":"code","5ec4d514":"code","1ab3c3a9":"code","21569ff2":"code","87e4163c":"code","a3af377c":"code","bdf343ae":"code","c4781f44":"code","9799cef4":"code","49b57e47":"code","8dd01b46":"code","c9e9eac7":"code","22ec8451":"code","fc9ae4c4":"code","18dc77f9":"code","09c3c58e":"code","4b8a86d0":"code","3d6c45b0":"code","bf9ddc19":"code","9562d005":"code","ea1ab189":"code","e12fd4b5":"code","155f8e72":"code","188f6bc6":"code","00af3f94":"code","626f1d9c":"code","929be1d2":"code","27059ed8":"code","10512fb5":"code","0481c538":"code","9d3cb65b":"code","ed2af70d":"code","c6665ae4":"code","b8eccef2":"code","c220a0ad":"code","6c7ddf51":"code","613119bc":"code","3c291b8f":"code","656830f6":"code","9e35afa7":"code","82dc3dd2":"code","147c78f5":"code","073d22ab":"code","eeebaf24":"code","815e9002":"code","f782a064":"code","f68450bd":"code","8fa3e833":"code","a6298c55":"code","21e09ff2":"code","24477b0b":"code","3aa1e2b4":"code","f9dcfd73":"code","d5526391":"code","95e564c1":"code","43c893ff":"code","9ed36230":"code","4ebc796d":"code","ce8b66f5":"code","61155cf3":"code","71f4cf71":"code","a0ac3e14":"code","96b00a11":"code","6e440dae":"code","b46c15ed":"code","628e79a1":"code","32cba0d1":"code","b3365fee":"code","2cf2640d":"code","3a233c71":"code","09a66433":"code","943a1edd":"code","220706da":"code","1ff48d1b":"code","49487d3b":"code","18f08bee":"code","c5cf92d2":"code","1d53be5b":"code","4459f039":"code","cd0abf8d":"code","7122cf14":"code","a16df6c0":"code","8637214e":"code","b3f9d484":"code","161a27c7":"code","dd8c0ae2":"code","bc03b91f":"code","33b25a82":"code","a74fe42a":"code","65bda24a":"code","f1d6f10c":"code","c0543b7e":"code","1e8e896b":"code","4f6d4dc9":"code","17b9bf9c":"code","b69af01a":"code","d6818bad":"code","c2590b3f":"markdown","85749799":"markdown","249cd1a3":"markdown","20c55dac":"markdown","f98dfba1":"markdown","658b5b90":"markdown","7881f064":"markdown","8557b8ff":"markdown","e3a46101":"markdown","abe4d3a5":"markdown","ba9baa1b":"markdown","c03c6bac":"markdown","663f7e92":"markdown","73e00f37":"markdown","4ea06143":"markdown","0a63ba12":"markdown","48cbde5c":"markdown","327744a7":"markdown","625a39e3":"markdown","7e8110fa":"markdown","44ae7b9f":"markdown","6a1e11c6":"markdown","6df5cf13":"markdown","e688ae8b":"markdown","5155e977":"markdown","94f82aad":"markdown","0fae33e8":"markdown","525748c7":"markdown","c0797e7c":"markdown","0fe77bdf":"markdown","54e6cbb2":"markdown","37ac4dfe":"markdown","371d8045":"markdown","f6d1a330":"markdown","db677104":"markdown","889b3010":"markdown","f9eeee8b":"markdown","d3f18712":"markdown","e1ebd531":"markdown","27641f24":"markdown","83901932":"markdown","3a41a69e":"markdown","de6a8fd0":"markdown","2c1aa88d":"markdown","9f43d2fe":"markdown","15245bc0":"markdown","e93f6fd0":"markdown","7ee3be55":"markdown","643178d4":"markdown","f86dea12":"markdown"},"source":{"1a782a40":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\npd.options.display.max_columns = None\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, log_loss, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n\n# import gradio as gr","1ce6e7c3":"df = pd.read_csv('..\/input\/lending-club-data\/loan_data.csv')\ndf.head()","105863ce":"df.shape\n# 14 columns, 9578 rows","1601d376":"df.info()\n# 13 numerical, 1 categorical","60c2b652":"df['credit.policy'] = df['credit.policy'].astype('object')\n# credit policy is categorical","9129033c":"# statistical description of features\ndf.describe(include = 'all')","9396b2e4":"df['credit.policy'].unique()\n# 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.","811bd722":"df['purpose'].unique()\n# 7 diff loan purposes ","f53670c5":"# interest rate of the loan\ndf['int.rate'].unique()\n\n# continuous column","bfe0bfdc":"df['installment'].unique() # monthly installments owed by the borrower if the loan is funded.\n\n# continuous column","59167479":"df['log.annual.inc'] = np.exp(df['log.annual.inc']).round(2) # annual income of the borrower.\n# converting log value to original value\ndf.head()","68588419":"df.rename({'log.annual.inc':'annual.inc'}, axis = 1, inplace = True)\n# renaming column name after log conversion","3e6ae23e":"df.head()\n# dataset after conversion of log annual income into annual income","c0bc0c72":"df['annual.inc'].unique() # annual income of the borrower.\n\n# continuous column","7ba51d2b":"df['dti'].unique()\n# The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n\n# continuous column","aa9858f2":"df['fico'].unique() # credit score of the borrower.\n\n# continuous column","04cf99e3":"df['days.with.cr.line'].unique() # number of days the borrower has had a credit line.\n\n# continuous column","bbfd6862":"df['revol.bal'].unique() # borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n\n# continuous column","21ec42bf":"df['revol.util'].unique() \n# borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n\n# continuous column","1e381512":"df['inq.last.6mths'].unique()\n# number of inquiries by creditors in the last 6 months.\n\n# continuous column","054f3a89":"df['delinq.2yrs'].unique()\n# number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n\n# continuous column","0bf187f7":"df['pub.rec'].unique()\n# borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).\n\n# continuous column","6d733cd5":"df['not.fully.paid'].unique()\n# 1 means loan was not paid back in full (the borrower either defaulted or the loan was charged off); 0 otherwise\n\n# target data","fdbe7902":"cont_col = df.select_dtypes(include = np.number)\ncont_col = cont_col.drop('not.fully.paid', axis = 1)\ncont_col.head()\n# continuous \/ numerical data","7a60fa8e":"cat_col = df.select_dtypes(include = 'object')\ncat_col.head()\n# categorical data","f901c184":"cat_col['credit.policy'].value_counts() \/ len(cat_col) * 100\n# distribution of credit policy in %\n\n# 80% of the customers meets credit policy criteria of lendingclub.com ","925c9a95":"# graphical representation\n\ncat_col['credit.policy'].value_counts().plot(kind = 'bar', figsize = (10,5))\n# most of the customers meets credit policy criteria of lendingclub.com ","e7a3c1c5":"cat_col['purpose'].value_counts() \/ len(cat_col) * 100\n# distribution of loan purpose in %\n\n# most of the loan purpose is for debt consolidation\n# least loan purpose is for educational loan","36166fef":"# graphical representation\ncat_col['purpose'].value_counts().plot(kind = 'bar', figsize = (10,5))\n\n# most of the loans was for debt consolidation\n# least loan purpose was for educational","d4b0d025":"# graphical representation of continuos \/ numeric features\n\ncont_col.plot(kind = 'kde', sharex = False, subplots = True, layout = (3,4), figsize = (15,12))\nplt.tight_layout()\nplt.show()\n\n# int.rate, dti, fico, revol.util zero skewed (normal distribution)\n# credit.policy is left skewed\n# installment, annual.inc, days.with.cr.line, revol.bal, inq.last.6mths, delinq.2yrs, pub.rec, not.fully.paid is right skewed","8ee473f7":"# skewness of continuos \/ numerical features\nfor i in cont_col.columns:\n    print(i, cont_col[i].skew().round(2))","fbb60e75":"# categorical vs target\n# credit policy vs target\n\npd.crosstab(cat_col['credit.policy'], df['not.fully.paid'])\n\n# most of the customers who met credit policy criteria of lendingclub.com are non-defaulters","8ba93d7b":"# graphical representation\npd.crosstab(cat_col['credit.policy'], df['not.fully.paid']).plot.bar(stacked = True)\n\n# only few customers who met credit policy criteria of lendingclub.com are defaulters","9b6352a1":"# purpose vs target\npd.crosstab(cat_col['purpose'], df['not.fully.paid'])\n\n# most of the loans are fully paid","4d340cb2":"# graphical representation\npd.crosstab(cat_col['purpose'], df['not.fully.paid']).plot.bar(stacked = True)\n\n# most of the loans are fully paid\n# least not paid - major purchase and educational\n# most not paid - debt consolidation and all other","c5855053":"# num vs target\n\nrow = 3\ncol = 4\ncount = 1\nplt.figure(figsize = (15,12))\nfor i in cont_col.columns:\n    plt.subplot(row, col, count)\n    sns.boxplot(x = df['not.fully.paid'], y = cont_col[i])\n    count += 1\nplt.tight_layout()\nplt.show()\n\n# relation found b\/w int rate, fico, revol.util and not fully paid","aa394c17":"# correlation matrix\ncont_col.corr()\n\n# revol.util and int rate has medium correlation","ae7ae6b6":"# graphical representation of correlation b\/w numerical columns\nplt.figure(figsize = (15,12))\nsns.heatmap(cont_col.corr(), annot = True)\nplt.show()","65bf171b":"df.isnull().sum()\n# no null values found","cdc5aaa9":"row = 3\ncol = 4\ncount = 1\nplt.figure(figsize = (15,12))\nfor i in cont_col.columns:\n    plt.subplot(row, col, count)\n    sns.boxplot(cont_col[i])\n    count += 1\nplt.tight_layout()\nplt.show()\n\n\n#  outliers in graphical presentation\n# 'int.rate', 'installment', 'annual.inc', 'fico', 'days.with.cr.line', 'revol.bal', 'inq.last.6mths' has outliers ","b2145fed":"# capping outliers\nfor i in ['int.rate', 'installment', 'annual.inc', 'fico', 'days.with.cr.line', 'revol.bal', 'inq.last.6mths']:\n    q1 = cont_col[i].quantile(0.25)\n    q3 = cont_col[i].quantile(0.75)\n    iqr = q3 - q1\n    ub = q3 + 1.5 * iqr\n    lb = q1 - 1.5 * iqr\n    uc = cont_col[i].quantile(0.99)\n    lc = cont_col[i].quantile(0.01)\n    for row in cont_col[i].index:\n        if cont_col.loc[row, i] > ub:\n            cont_col.loc[row, i] = uc\n        if cont_col.loc[row, i] < lb:\n            cont_col.loc[row, i] = lc","33cc7734":"row = 2\ncol = 4\ncount = 1\nplt.figure(figsize = (12,6))\nfor i in ['int.rate', 'installment', 'annual.inc', 'fico', 'days.with.cr.line', 'revol.bal', 'inq.last.6mths']:\n    plt.subplot(row, col, count)\n    sns.boxplot(cont_col[i])\n    count += 1\nplt.tight_layout()\nplt.show()\n# outliers removed thro capping method","4fc7fc82":"# Standard Scaler\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nfor i in cont_col:\n    cont_col[i] = sc.fit_transform(cont_col[i].values.reshape(-1,1))\ncont_col.head()","4b9cf5cd":"# one hot encoding\ncat_col = pd.get_dummies(cat_col, columns=['purpose'], drop_first = True)\ncat_col.head()","be401b2c":"cat_col['credit.policy'] = cat_col['credit.policy'].astype('int')\n\n# input variables\nx = pd.concat([cont_col, cat_col], axis = 1)\nx.head()","4711b39d":"# target variable\ny = df['not.fully.paid']\ny.head()","9dbb594f":"y.value_counts(normalize = True)\n\n# target variable distribution\n# 0 - 83% \n# 1 - 16%","b29c5c47":"y.value_counts()\n\n# target variable distribution\n# 0 - 8045 \n# 1 - 1533","a7afc377":"# SMOTE\nx_sm, y_sm = SMOTE(random_state=1).fit_resample(x,y)","9a2c0699":"y_sm.value_counts(normalize = True)\n\n# target variable distribution after SMOTE\n# 0,1 - 50% each","407d4aeb":"y_sm.value_counts()\n\n# target variable distribution after SMOTE\n# 0,1 - 8045 observations each","23855bc5":"print(x.shape)\nprint(x_sm.shape)\n\n# independant variables shape before SMOTE - 9578 rows and 18 columns\n# independant variables shape after SMOTE - 16090 rows and 18 columns","2e8b5c17":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nlr = LogisticRegression(random_state=1)\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_test)","fceb1eea":"print('Training data score:',lr.score(x_train, y_train).round(2))\nprint('Testing data score:',lr.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","a08744b3":"print(classification_report(y_test, y_pred))\n\n# 0.47 precision score\n# 0.02 recall score\n# 0.04 f1 score","315e6625":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","5512864d":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2387 are correctly classified as 0s\n# out of 477 observations - 9 are correctly classified as 1s","9688c5c1":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\nlr = LogisticRegression(random_state=1)\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_test)","1f86a5d4":"print('Training data score:',lr.score(x_train, y_train).round(2))\nprint('Testing data score:',lr.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","1123026e":"print(classification_report(y_test, y_pred))\n\n# 0.69 precision score\n# 0.67 recall score\n# 0.68 f1 score","08798f1d":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","b1635526":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1632 are correctly classified as 0s\n# out of 2448 observations - 1634 are correctly classified as 1s","33c2518c":"# cross validation on logistic regression\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\nlr_cv_score = cross_val_score(estimator=lr, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(lr_cv_score)\n\nprint('avg accuracy: ', lr_cv_score.mean())\n# on average Logistic regression gives 67% accuracy with 10 fold cross validation","5be5e7ec":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\ndt = DecisionTreeClassifier(random_state=1)\ndt.fit(x_train, y_train)\n\ny_pred = dt.predict(x_test)","3be34cb0":"print('Training data score:',dt.score(x_train, y_train).round(2))\nprint('Testing data score:',dt.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","1ccdb64f":"print(classification_report(y_test, y_pred))\n\n# 0.20 precision score\n# 0.21 recall score\n# 0.21 f1 score","6301b899":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","1656787e":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 1992 are correctly classified as 0s\n# out of 477 observations - 102 are correctly classified as 1s","26f50041":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\ndt = DecisionTreeClassifier(random_state=1)\ndt.fit(x_train, y_train)\n\ny_pred = dt.predict(x_test)","141ff426":"print('Training data score:',dt.score(x_train, y_train).round(2))\nprint('Testing data score:',dt.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","21fbd7b0":"print(classification_report(y_test, y_pred))\n\n# 0.80 precision score\n# 0.83 recall score\n# 0.81 f1 score","ee40b27c":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","440e7326":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1865 are correctly classified as 0s\n# out of 2448 observations - 2021 are correctly classified as 1s","bcb65e90":"parameters = {'max_depth': range(150,250,25), 'criterion':['entropy','gini'], 'max_leaf_nodes': range(300, 500, 30)}\n\ndt = DecisionTreeClassifier(random_state=1)\n\ntree1 = GridSearchCV(estimator = dt, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameters are 'criterion': 'gini', 'max_depth': 150, 'max_leaf_nodes': 390","ff75d60f":"# building model with tuned parameters\ndt = DecisionTreeClassifier(random_state=1, criterion='gini', max_depth=150, max_leaf_nodes=390)\ndt.fit(x_train, y_train)\n\ny_pred = dt.predict(x_test)\n\nprint('Training data score:',dt.score(x_train, y_train).round(2))\nprint('Testing data score:',dt.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","9b237ef5":"print(classification_report(y_test, y_pred))\n\n# 0.81 precision score\n# 0.78 recall score\n# 0.79 f1 score","161465d0":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","32f0f115":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1921 are correctly classified as 0s\n# out of 2448 observations - 1898 are correctly classified as 1s","3dc36f65":"# cross validation on decision tree\n\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\ndt_cv_score = cross_val_score(estimator=dt, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(dt_cv_score)\n\nprint('avg accuracy: ', dt_cv_score.mean())\n# on average, decision tree model scores 79% accuracy","9f5acb4c":"# important features for descision tree model prediction and their scores\npd.DataFrame(dt.feature_importances_, index = x_train.columns, columns=['Feature Scores']).sort_values(by = 'Feature Scores', ascending = False)\n# inq.last.6mths, int.rate, days.with.cr.line, fico, installment are the most important features ","eafd28b6":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nrf = RandomForestClassifier(random_state=1)\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_test)","d2f405eb":"print('Training data score:',rf.score(x_train, y_train).round(2))\nprint('Testing data score:',rf.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","926d265c":"print(classification_report(y_test, y_pred))\n\n# 0.59 precision score\n# 0.02 recall score\n# 0.04 f1 score","db76e7eb":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","a36bdd46":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2390 are correctly classified as 0s\n# out of 477 observations - 10 are correctly classified as 1s","2db3e7ee":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\nrf = RandomForestClassifier(random_state=1)\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_test)","cbc74a88":"print('Training data score:',rf.score(x_train, y_train).round(2))\nprint('Testing data score:',rf.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","ef2ef156":"print(classification_report(y_test, y_pred))\n\n# 0.91 precision score\n# 0.87 recall score\n# 0.89 f1 score","9a8b4b29":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","afa93fa4":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2165 are correctly classified as 0s\n# out of 2448 observations - 2134 are correctly classified as 1s","0806adfc":"parameters = {'max_depth': range(17,21), 'criterion':['entropy','gini'], 'n_estimators': range(120,151,10)}\n\nrf = RandomForestClassifier(random_state=1)\n\ntree1 = GridSearchCV(estimator = rf, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameters are 'criterion': 'gini', 'max_depth': 19, 'n_estimators': 140","9139e3f3":"# building a model with tuned parameters\nrf = RandomForestClassifier(random_state=1, max_depth=19, n_estimators = 140, criterion='gini')\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_test)","3476538e":"print('Training data score:',rf.score(x_train, y_train).round(2))\nprint('Testing data score:',rf.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","cc677144":"print(classification_report(y_test, y_pred))\n\n# 0.89 precision score\n# 0.87 recall score\n# 0.88 f1 score","cbefa913":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","9153205e":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2121 are correctly classified as 0s\n# out of 2448 observations - 2141 are correctly classified as 1s","533ac765":"# cross validation on random forest\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\nrf_cv_score = cross_val_score(estimator=rf, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(rf_cv_score)\n\nprint('avg accuracy: ', rf_cv_score.mean())\n# on average, random forest model scores 88% accuracy with 10 fold cross validation","6f888a7e":"# important features for random forest model prediction and their scores\npd.DataFrame(rf.feature_importances_, index = x_train.columns, columns=['Feature Scores']).sort_values(by = 'Feature Scores', ascending = False)\n# inq.last.6mths, int.rate, fico, days.with.cr.line, installment are the most important features ","e4e2d0a1":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_test)","5b8a6074":"print('Training data score:',knn.score(x_train, y_train).round(2))\nprint('Testing data score:',knn.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","5d5b0933":"print(classification_report(y_test, y_pred))\n\n# 0.31 precision score\n# 0.07 recall score\n# 0.11 f1 score","4944f354":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","cc45f501":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2326 are correctly classified as 0s\n# out of 477 observations - 32 are correctly classified as 1s","47ca6be2":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_test)","97c86317":"print('Training data score:',knn.score(x_train, y_train).round(2))\nprint('Testing data score:',knn.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","0d11fcbe":"print(classification_report(y_test, y_pred))\n\n# 0.71 precision score\n# 0.96 recall score\n# 0.82 f1 score","8e94d864":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","f08ed988":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1437 are correctly classified as 0s\n# out of 2448 observations - 2349 are correctly classified as 1s","06fdced3":"parameters = {'n_neighbors': np.arange(3, 20, 2), 'metric':['hamming','euclidean','manhattan','chebyshev']}\n\nknn = KNeighborsClassifier()\n\ntree1 = GridSearchCV(estimator = knn, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameters are 'metric': 'manhattan', 'n_neighbors': 3","5ec4d514":"# building a model with tuned parameters\nknn = KNeighborsClassifier(n_neighbors=3 , metric = 'manhattan')\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_test)","1ab3c3a9":"print('Training data score:',knn.score(x_train, y_train).round(2))\nprint('Testing data score:',knn.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","21569ff2":"print(classification_report(y_test, y_pred))\n\n# 0.75 precision score\n# 0.97 recall score\n# 0.84 f1 score","87e4163c":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","a3af377c":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1569 are correctly classified as 0s\n# out of 2448 observations - 2375 are correctly classified as 1s","bdf343ae":"# cross validation on KNN\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\nknn_cv_score = cross_val_score(estimator=knn, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(knn_cv_score)\n\nprint('avg accuracy: ', knn_cv_score.mean())\n# on average, knn model scores 83% accurate","c4781f44":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\n\ny_pred = gnb.predict(x_test)","9799cef4":"print('Training data score:',gnb.score(x_train, y_train).round(2))\nprint('Testing data score:',gnb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","49b57e47":"print(classification_report(y_test, y_pred))\n\n# 0.31 precision score\n# 0.34 recall score\n# 0.32 f1 score","8dd01b46":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","c9e9eac7":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2029 are correctly classified as 0s\n# out of 477 observations - 163 are correctly classified as 1s","22ec8451":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\n\ny_pred = gnb.predict(x_test)","fc9ae4c4":"print('Training data score:',gnb.score(x_train, y_train).round(2))\nprint('Testing data score:',gnb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","18dc77f9":"print(classification_report(y_test, y_pred))\n\n# 0.60 precision score\n# 0.83 recall score\n# 0.70 f1 score","09c3c58e":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","4b8a86d0":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1043 are correctly classified as 0s\n# out of 2448 observations - 2044 are correctly classified as 1s","3d6c45b0":"# cross validation on Naive Bayes\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\ngnb_cv_score = cross_val_score(estimator=gnb, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(gnb_cv_score)\n\nprint('avg accuracy: ', gnb_cv_score.mean())\n# on average, naive bayes model is 64% accurate","bf9ddc19":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nada = AdaBoostClassifier(random_state = 1)\nada.fit(x_train, y_train)\n\ny_pred = ada.predict(x_test)","9562d005":"print('Training data score:',ada.score(x_train, y_train).round(2))\nprint('Testing data score:',ada.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","ea1ab189":"print(classification_report(y_test, y_pred))\n\n# 0.52 precision score\n# 0.03 recall score\n# 0.05 f1 score","e12fd4b5":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","155f8e72":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2385 are correctly classified as 0s\n# out of 477 observations - 13 are correctly classified as 1s","188f6bc6":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\nada = AdaBoostClassifier(random_state = 1)\nada.fit(x_train, y_train)\n\ny_pred = ada.predict(x_test)","00af3f94":"print('Training data score:',ada.score(x_train, y_train).round(2))\nprint('Testing data score:',ada.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","626f1d9c":"print(classification_report(y_test, y_pred))\n\n# 0.77 precision score\n# 0.72 recall score\n# 0.75 f1 score","929be1d2":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","27059ed8":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1847 are correctly classified as 0s\n# out of 2448 observations - 1774 are correctly classified as 1s","10512fb5":"parameters = {'n_estimators': [200, 250, 300, 350], 'learning_rate':[0.1, 0.01, 0.15, 0.015]}\n\nada = AdaBoostClassifier(random_state = 1)\n\ntree1 = GridSearchCV(estimator = ada, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameters are 'learning_rate': 0.15, 'n_estimators': 350","0481c538":"# building a model with tuned parameters\nada = AdaBoostClassifier(random_state = 1, n_estimators= 350, learning_rate=0.15)\nada.fit(x_train, y_train)\n\ny_pred = ada.predict(x_test)","9d3cb65b":"print('Training data score:',ada.score(x_train, y_train).round(2))\nprint('Testing data score:',ada.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","ed2af70d":"print(classification_report(y_test, y_pred))\n\n# 0.77 precision score\n# 0.72 recall score\n# 0.75 f1 score","c6665ae4":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","b8eccef2":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1850 are correctly classified as 0s\n# out of 2448 observations - 1774 are correctly classified as 1s","c220a0ad":"# cross validation on AdaBoost\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\nada_cv_score = cross_val_score(estimator=ada, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(ada_cv_score)\n\nprint('avg accuracy: ', ada_cv_score.mean())\n# on average ada boost model is 75% accurate","6c7ddf51":"# important features for ada boost model prediction and their scores\npd.DataFrame(ada.feature_importances_, index = x_train.columns, columns=['Feature Scores']).sort_values(by = 'Feature Scores', ascending = False)\n# inq.last.6mths, purpose_debt_consolidation, purpose_credit_card, installment, annual.inc are the most important features ","613119bc":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\ngb = GradientBoostingClassifier(random_state = 1)\ngb.fit(x_train, y_train)\n\ny_pred = gb.predict(x_test)","3c291b8f":"print('Training data score:',gb.score(x_train, y_train).round(2))\nprint('Testing data score:',gb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","656830f6":"print(classification_report(y_test, y_pred))\n\n# 0.38 precision score\n# 0.02 recall score\n# 0.03 f1 score","9e35afa7":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","82dc3dd2":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2384 are correctly classified as 0s\n# out of 477 observations - 8 are correctly classified as 1s","147c78f5":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\ngb = GradientBoostingClassifier(random_state = 1)\ngb.fit(x_train, y_train)\n\ny_pred = gb.predict(x_test)","073d22ab":"print('Training data score:',gb.score(x_train, y_train).round(2))\nprint('Testing data score:',gb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","eeebaf24":"print(classification_report(y_test, y_pred))\n\n# 0.85 precision score\n# 0.73 recall score\n# 0.79 f1 score","815e9002":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","f782a064":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2056 are correctly classified as 0s\n# out of 2448 observations - 1797 are correctly classified as 1s","f68450bd":"parameters = {'n_estimators': range(800,1401,200)}\n\ngb = GradientBoostingClassifier(random_state = 1,learning_rate= 0.1)\n\ntree1 = GridSearchCV(estimator = gb, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameter is 'n_estimators': 1000","8fa3e833":"# building a model with tuned parameters\ngb = GradientBoostingClassifier(random_state = 1, n_estimators= 1000, learning_rate= 0.1)\ngb.fit(x_train, y_train)\n\ny_pred = gb.predict(x_test)","a6298c55":"print('Training data score:',gb.score(x_train, y_train).round(2))\nprint('Testing data score:',gb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","21e09ff2":"print(classification_report(y_test, y_pred))\n\n# 0.94 precision score\n# 0.81 recall score\n# 0.87 f1 score","24477b0b":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","3aa1e2b4":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2263 are correctly classified as 0s\n# out of 2448 observations - 2448 are correctly classified as 1s","f9dcfd73":"# cross validation on Gradient Boosting\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\ngb_cv_score = cross_val_score(estimator=gb, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(gb_cv_score)\n\nprint('avg accuracy: ', gb_cv_score.mean())\n# on average gradient boost model is 88% accurate","d5526391":"# important features for gradient boost model prediction and their scores\npd.DataFrame(gb.feature_importances_, index = x_train.columns, columns=['Feature Scores']).sort_values(by = 'Feature Scores', ascending = False)\n# inq.last.6mths, fico, int.rate, credit.policy, annual.inc are the most important features ","95e564c1":"# Base Model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nxgb = XGBClassifier(random_state = 1)\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_test)","43c893ff":"print('Training data score:',xgb.score(x_train, y_train).round(2))\nprint('Testing data score:',xgb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","9ed36230":"print(classification_report(y_test, y_pred))\n\n# 0.38 precision score\n# 0.08 recall score\n# 0.14 f1 score","4ebc796d":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","ce8b66f5":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2332 are correctly classified as 0s\n# out of 477 observations - 40 are correctly classified as 1s","61155cf3":"# Base Model with SMOTE\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\nxgb = XGBClassifier(random_state = 1)\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_test)","71f4cf71":"print('Training data score:',xgb.score(x_train, y_train).round(2))\nprint('Testing data score:',xgb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","a0ac3e14":"print(classification_report(y_test, y_pred))\n\n# 0.93 precision score\n# 0.83 recall score\n# 0.88 f1 score","96b00a11":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","6e440dae":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2235 are correctly classified as 0s\n# out of 2448 observations - 2039 are correctly classified as 1s","b46c15ed":"parameters = {'n_estimators': [100, 120, 140, 160], 'learning_rate':[0.1, 0.01, 0.2, 0.3], 'max_depth': [4, 5 ,6]}\n\nxgb = XGBClassifier(random_state = 1, gamma = 2)\n\ntree1 = GridSearchCV(estimator = xgb, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameters are 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 160","628e79a1":"# model built on tuned parameters\nxgb = XGBClassifier(random_state = 1, gamma = 2, learning_rate=0.2, max_depth=6, n_estimators=160)\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_test)","32cba0d1":"print('Training data score:',xgb.score(x_train, y_train).round(2))\nprint('Testing data score:',xgb.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","b3365fee":"print(classification_report(y_test, y_pred))\n\n# 0.94 precision score\n# 0.82 recall score\n# 0.88 f1 score","2cf2640d":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","3a233c71":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2245 are correctly classified as 0s\n# out of 2448 observations - 2014 are correctly classified as 1s","09a66433":"# cross validation on Extreme Gradient Boosting\n\n# considering type-2 error, base-SMOTE model gives less errors than tuned model. \n# hence cross validation done on base-SMOTE model\n\nxgb = XGBClassifier(random_state = 1)\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_test)\n\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\nxgb_cv_score = cross_val_score(estimator=xgb, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(xgb_cv_score)\n\nprint('avg accuracy: ', xgb_cv_score.mean())\n# extreme gradient boost model scores 88% accurate on average","943a1edd":"# important features for extreme gradient boost model prediction and their scores\npd.DataFrame(xgb.feature_importances_, index = x_train.columns, columns=['Feature Scores']).sort_values(by = 'Feature Scores', ascending = False)\n# inq.last.6mths, credit.policy, purpose_major_purchase, purpose_home_improvement, purpose_credit_card are the most important features ","220706da":"# instanciating estimators for stacking classifier\ngb = GradientBoostingClassifier(random_state=1)\nxgb = XGBClassifier()\nlr = LogisticRegression(random_state=1)\nada = AdaBoostClassifier(random_state=1)\nknn = KNeighborsClassifier()\ndt = DecisionTreeClassifier(random_state=1)\nrf = RandomForestClassifier(random_state=1)\nnb = GaussianNB()","1ff48d1b":"# Base Model with base learners logistic regression, naive bayes, xg boost and final estimator as decision tree\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nbase_learners = [('lr',lr), ('nb', nb), ('xgb', xgb)]\n\nstc = StackingClassifier(estimators = base_learners, final_estimator = dt, n_jobs=-1)\nstc.fit(x_train, y_train)\n\ny_pred = stc.predict(x_test)","49487d3b":"print('Training data score:',stc.score(x_train, y_train).round(2))\nprint('Testing data score:',stc.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","18f08bee":"print(classification_report(y_test, y_pred))\n\n# 0.22 precision score\n# 0.21 recall score\n# 0.22 f1 score","c5cf92d2":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","1d53be5b":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2397 observations - 2039 are correctly classified as 0s\n# out of 477 observations - 101 are correctly classified as 1s","4459f039":"# Base Model with SMOTE\n# Base Model with base learners logistic regression, naive bayes, xg boost and final estimator as decision tree\n\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size = 0.3, random_state = 1)\n\nbase_learners = [('lr',lr), ('nb', nb), ('xgb', xgb)]\n\nstc = StackingClassifier(estimators = base_learners, final_estimator = dt, n_jobs=-1)\nstc.fit(x_train, y_train)\n\ny_pred = stc.predict(x_test)","cd0abf8d":"print('Training data score:',stc.score(x_train, y_train).round(2))\nprint('Testing data score:',stc.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","7122cf14":"print(classification_report(y_test, y_pred))\n\n# 0.82 precision score\n# 0.82 recall score\n# 0.82 f1 score","a16df6c0":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","8637214e":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 1943 are correctly classified as 0s\n# out of 2448 observations - 2018 are correctly classified as 1s","b3f9d484":"parameters = {'final_estimator': [gb, xgb, lr, ada, knn, dt, rf, nb]}\n\nstc = StackingClassifier(estimators= [('lr',lr), ('nb', nb), ('xgb', xgb)], n_jobs = -1)\n\ntree1 = GridSearchCV(estimator = stc, param_grid = parameters, cv = 5, scoring = 'accuracy')\ntree1.fit(x_train, y_train)\ntree1.best_params_\n# best parameter is 'final_estimator': GradientBoostingClassifier(random_state=1)","161a27c7":"stc = StackingClassifier(estimators = [('lr',lr), ('nb', nb), ('xgb', xgb)], final_estimator = GradientBoostingClassifier(random_state=1))\nstc.fit(x_train, y_train)\n\ny_pred = stc.predict(x_test)","dd8c0ae2":"print('Training data score:',stc.score(x_train, y_train).round(2))\nprint('Testing data score:',stc.score(x_test, y_test).round(2))\nprint('Log Loss:',log_loss(y_test, y_pred).round(2))","bc03b91f":"print(classification_report(y_test, y_pred))\n\n# 0.96 precision score\n# 0.81 recall score\n# 0.88 f1 score","33b25a82":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1],'r--')\nplt.grid(True)\n\nprint('AUC Score : ',roc_auc_score(y_test, y_pred))","a74fe42a":"# confusion matrix\nconfusion_matrix(y_test, y_pred)\n# out of 2379 observations - 2304 are correctly classified as 0s\n# out of 2448 observations - 1979 are correctly classified as 1s","65bda24a":"# cross validation on Stacking Classifier\nkf = KFold(n_splits=10, shuffle = True, random_state=1)\n\nstc_cv_score = cross_val_score(estimator=stc, X = x_sm, y = y_sm, cv = kf, scoring='accuracy')\nprint(stc_cv_score)\n\nprint('avg accuracy: ', stc_cv_score.mean())\n# stacking classifier scores 86% on average","f1d6f10c":"# boxplot of cross validation scores for each model\ncv_score = pd.DataFrame({'Logistic Regression':lr_cv_score, 'Decision Tree':dt_cv_score, 'Random Forest':rf_cv_score, \n                         'KNN':knn_cv_score, 'Naive Bayes':gnb_cv_score, 'AdaBoost':ada_cv_score, \n                         'Gradient Boost':gb_cv_score, 'Extreme Gradient Boost':xgb_cv_score, \n                         'Stacking Classifier':stc_cv_score})\n\ncv_score.plot(kind='box', sharex=True, figsize=(15,10), subplots=False)\nplt.xticks(rotation=90)\nplt.show()\n# extreme gradient boosting model is the best model considering the boxplot distribution and median is not deviated much\n# no outliers found on extreme gradient boost","c0543b7e":"cv_score.mean()\n# average scores of models ","1e8e896b":"from sklearn.utils import resample\n\nsm_data = pd.concat([x_sm,y_sm], axis = 1)    # combining x & y smote data into sm_data\n\nmodel_scores = []                             # empty list to store model scores \nsamples = int(len(sm_data)*0.8)               # 80% of sm_data considered as samples\n\nfor i in range(100):                                  # 100 iterations\n    train = resample(sm_data, n_samples = samples)    # 80% of data is chosen at random using resample() as train set\n    test = sm_data.drop(train.index, axis = 0)        # left out data from sm_data is considered as test set\n\n    xgb = XGBClassifier()                             # initializing the best final model\n    xgb.fit(train.iloc[:,:-1], train.iloc[:,-1])      # training the model\n\n    score = xgb.score(test.iloc[:,:-1], test.iloc[:,-1])    # calculating model score on test data\n    model_scores.append(score)                              # appending model score to the list\n    \nsns.distplot(model_scores)    # distribution plot of the model_scores\nplt.show()\n\n# distribution plot confirms the concept of central limit theorem \n# irrespective of the population distribution, the sampling distribution will always be normally distributed\n# 100 different models are built with bootstrapped smote data and their model scores are calculated\n# these scores form a normal distribution","4f6d4dc9":"pd.Series(model_scores).skew()\n# skewness of model_scores is approx. 0 -- normal distribution","17b9bf9c":"print('Minimum score:', round(np.min(model_scores),3), '\\nMaximum score:',round(np.max(model_scores),3))\n# minimum score - 0.86 and maximum score - 0.88 from samples","b69af01a":"# 95% of data lies approximately within 2 standard deviations\n# lower range = mean - (2 * standard deviation)\n# upper range = mean + (2 * standard deviation)\nx_bar = np.mean(model_scores) \ns = np.std(model_scores)\nlower_range = x_bar - (2 * s)\nupper_range = x_bar + (2 * s)\nprint('95% confidence interval ranges between ',round(lower_range,4),'and ',round(upper_range,4))","d6818bad":"!pip install gradio\nimport gradio as gr\n\ndef loan_predict(credit_policy, loan_purpose, interest_rate, installment_amount, annual_income, debt_to_income_ratio, fico_score, days_with_credit_line, revolving_balance, revolving_utilization_rate, inquiries_last_6_months, delinquency_past_2_years, public_records):\n    \n    if credit_policy == '0 - Not Met':\n        credit_policy = 0\n    else:\n        credit_policy = 1\n    \n    purpose_credit_card, purpose_debt_consolidation, purpose_educational, purpose_home_improvement, purpose_major_purchase, purpose_small_business = 0,0,0,0,0,0\n\n    if loan_purpose == 'credit_card':\n        purpose_credit_card = 1\n    elif loan_purpose == 'home_improvement':\n        purpose_home_improvement = 1\n    elif loan_purpose == 'debt_consolidation':\n        purpose_debt_consolidation = 1\n    elif loan_purpose == 'educational':\n        purpose_educational = 1\n    elif loan_purpose == 'major_purchase':\n        purpose_major_purchase = 1\n    elif loan_purpose == 'small_business':\n        purpose_small_business = 1\n\n    prediction = xgb.predict(pd.DataFrame([[interest_rate, installment_amount, annual_income, debt_to_income_ratio, fico_score, days_with_credit_line, revolving_balance, revolving_utilization_rate, inquiries_last_6_months, delinquency_past_2_years, public_records, credit_policy, purpose_credit_card, purpose_debt_consolidation, purpose_educational, purpose_home_improvement, purpose_major_purchase, purpose_small_business]], columns = x_train.columns))\n    \n    if prediction == 1:\n        prediction = 'Defaulter'\n    else:\n        prediction = 'Non-Defaulter'\n    return prediction\n\niface = gr.Interface(fn=loan_predict,inputs=[gr.inputs.Radio(['0 - Not Met','1 - Met']), gr.inputs.Radio(['credit_card','home_improvement','debt_consolidation','educational','major_purchase','small_business','all_other']),'number', 'number', 'number', 'number', 'number', 'number', 'number', 'number','number', 'number', 'number'], outputs=['text'])\niface.launch()","c2590b3f":"### Random Forest","85749799":"#### Tuning Hyper-parameters using GridSearchCV","249cd1a3":"### Gradient Boost","20c55dac":"#### Ada Boost - Base Model with SMOTE","f98dfba1":"### SMOTE","658b5b90":"### KNN","7881f064":"### Visualizing Best Model","8557b8ff":"#### Cross Validation on Stacking Classifier","e3a46101":"### Bi-variate Analysis","abe4d3a5":"#### Cross Validation on Extreme Gradient Boost","ba9baa1b":"#### Gradient Boost - Base Model with SMOTE","c03c6bac":"### Data Dictionary\n\nThe data is publicly available which is from LendingClub.com. The dataset represents 3-year-loans that were funded through the LendingClub.com platform between May 2007 and February 2010.\n\nThe binary dependent variable <b>'not.full.paid'<\/b> indicates that the loan was not paid back in full (the borrower either defaulted or the loan was \"charged off,\" meaning the borrower was deemed unlikely to ever pay it back).\n\n\nHere are what the columns represent:\n\n<b>credit.policy<\/b>: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n\n<b>purpose<\/b>: The purpose of the loan (takes values \"creditcard\", \"debtconsolidation\", \"educational\", \"majorpurchase\", \"smallbusiness\", and \"all_other\").\n\n<b>int.rate<\/b>: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n\n<b>installment<\/b>: The monthly installments owed by the borrower if the loan is funded.\n\n<b>log.annual.inc<\/b>: The natural log of the self-reported annual income of the borrower.\n\n<b>dti<\/b>: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n\n<b>fico<\/b>: The FICO credit score of the borrower.\n\nA FICO score is a credit score created by the Fair Isaac Corporation (FICO).FICO scores take into account data in five areas to determine creditworthiness: payment history, current level of indebtedness, types of credit used, length of credit history, and new credit accounts.Scores range from 300 to 850, with scores in the 670 to 739 range considered to be \u201cgood\u201d credit history.\n\n<b>days.with.cr.line<\/b>: The number of days the borrower has had a credit line.(credit limit)\n\n<b>revol.bal<\/b>: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n\n<b>revol.util<\/b>: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available). Credit Utilization Rate\n\n<b>inq.last.6mths<\/b>: The borrower's number of inquiries by creditors in the last 6 months.\n\n<b>delinq.2yrs<\/b>: The number of times the borrower had been 30+ days past due on a payment in the past 2 years. (delinquency)\n\n<b>pub.rec<\/b>: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).","663f7e92":"### Encoding categorical features","73e00f37":"### Reading Dataset","4ea06143":"#### Cross Validation on KNN","0a63ba12":"### Range estimate for the best model using BootStrapping","48cbde5c":"### Extreme Gradient Boost","327744a7":"#### Cross Validation on Gradient Boost","625a39e3":"### Naive Bayes","7e8110fa":"#### Tuning Hyper-parameters using GridSearchCV","44ae7b9f":"### Decision Tree","6a1e11c6":"\n<center>\n    <h1> Loan Default Prediction <\/h1>\n<\/center>","6df5cf13":"#### Cross Validation on Random Forest","e688ae8b":"#### Cross Validation on Naive Bayes","5155e977":"### Logistic Regression","94f82aad":"#### Tuning Hyper-parameters using GridSearchCV","0fae33e8":"### Uni-variate Analysis","525748c7":"### Outlier Analysis","c0797e7c":"#### Cross validation on Decision Trees","0fe77bdf":"### Basic understanding of features","54e6cbb2":"#### Cross validation on Logistic Regression","37ac4dfe":"#### Logistic Regression - Base Model with SMOTE","371d8045":"#### Extreme Gradient Boost - Base Model with SMOTE","f6d1a330":"#### Stacking Classifier - Base Model with SMOTE","db677104":"### Model Deployment","889b3010":"#### Tuning Hyper-parameters using GridSearchCV","f9eeee8b":"#### Naive Bayes - Base Model with SMOTE","d3f18712":"### Scaling numerical features","e1ebd531":"### Stacking Classifier","27641f24":"### Missing Value Treatment","83901932":"#### Decision Tree - Base Model with SMOTE","3a41a69e":"### Importing Libraries","de6a8fd0":"### Ada Boosting","2c1aa88d":"#### Tuning Hyper-parameters using GridSearchCV","9f43d2fe":"#### KNN - Base Model with SMOTE","15245bc0":"### x and y variables","e93f6fd0":"#### Tuning Hyper-parameters using GridSearchCV","7ee3be55":"#### Random Forest - Base Model with SMOTE","643178d4":"#### Tuning Hyper-parameters using GridSearchCV","f86dea12":"#### Cross Validation on Ada Boost"}}