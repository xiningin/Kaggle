{"cell_type":{"d8ce334c":"code","ec5008bc":"code","45d16c1a":"code","155299b5":"code","f14db737":"code","4a644eb6":"code","201d1f20":"code","d685a5a6":"code","e938c2a9":"code","060370fa":"code","8b79e812":"code","8d69f13f":"code","f0163bbc":"code","912ae1bc":"code","fd29b490":"code","217d394d":"code","8886cf2e":"code","c6f6d558":"code","2a58cb6d":"code","93391e3b":"code","94638ac4":"code","722c8351":"code","18a16054":"code","d1f32108":"code","17551399":"code","7f6daee2":"code","99df4847":"code","35e5da20":"code","eeef457a":"code","0ad4ce2a":"code","9d74539a":"code","225e3221":"code","24a1efa6":"code","6ccfccf2":"code","239c66cf":"code","c56207bb":"code","80df0b18":"code","317991f5":"code","024d13d7":"code","6cb0075c":"code","4a06ee81":"code","b465bbf6":"markdown","2f4875c0":"markdown","f998757f":"markdown","0f037f5a":"markdown","885c9c82":"markdown","8ba02259":"markdown","47d57d98":"markdown","55d4605e":"markdown","b006badd":"markdown","ea4a5b45":"markdown","fa2b23a8":"markdown","e0fea70e":"markdown"},"source":{"d8ce334c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec5008bc":"train=pd.read_csv('\/kaggle\/input\/hackerearths-reduce-marketing-waste\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/hackerearths-reduce-marketing-waste\/test.csv')\ntrain.head()","45d16c1a":"test.head()","155299b5":"train.info()","f14db737":"train.describe()","4a644eb6":"#Probability cannot be more than 100.So, we will be dropping rows having values greater than 100\ntrain.drop(train.loc[train['Success_probability']>100].index, inplace=True)","201d1f20":"train.columns","d685a5a6":"for i in train.columns:\n    print(i,\"-->\",train[i].nunique())","e938c2a9":"for i in test.columns:\n    print(i,\"-->\",test[i].nunique())","060370fa":"train['Deal_value']=train['Deal_value'].astype('str')\ntrain['Weighted_amount']=train['Weighted_amount'].astype('str')\n\ntrain['Deal_value']=train['Deal_value'].map(lambda x:str(x).split('$')[0])\ntrain['Weighted_amount']=train['Weighted_amount'].map(lambda x:str(x).split('$')[0])\n    \n    \ntrain['Deal_value']=train['Deal_value'].astype('float64')\ntrain['Weighted_amount']=train['Weighted_amount'].astype('float64')\n","8b79e812":"test['Deal_value']=test['Deal_value'].astype('str')\ntest['Weighted_amount']=test['Weighted_amount'].astype('str')\n\ntest['Deal_value']=test['Deal_value'].map(lambda x:str(x).split('$')[0])\ntest['Weighted_amount']=test['Weighted_amount'].map(lambda x:str(x).split('$')[0])\n    \ntest['Deal_value']=test['Deal_value'].astype('float64')\ntest['Weighted_amount']=test['Weighted_amount'].astype('float64')\n","8d69f13f":"train['Date_of_creation']=train['Date_of_creation'].astype('str')\n\ntrain['Date_of_creation']=train['Date_of_creation'].map(lambda x:x.split('-')[0])\n\ntrain['Date_of_creation']=train['Date_of_creation'].astype('object')\ntrain.rename({'Date_of_creation':'Year'},axis=1,inplace=True)","f0163bbc":"test['Date_of_creation']=test['Date_of_creation'].astype('str')\n\ntest['Date_of_creation']=test['Date_of_creation'].map(lambda x:x.split('-')[0])\n\ntest['Date_of_creation']=test['Date_of_creation'].astype('object')\ntest.rename({'Date_of_creation':'Year'},axis=1,inplace=True)","912ae1bc":"train['Location']=train['Location'].fillna('IND').map(lambda x:str(x).split(',')[1].rstrip() if len(str(x).split(','))>1 else 'IND')\ntest['Location']=test['Location'].fillna('IND').map(lambda x:str(x).split(',')[1].rstrip() if len(str(x).split(','))>1 else 'IND')","fd29b490":"train.isna().sum()","217d394d":"test.isna().sum()","8886cf2e":"train['Deal_value'].fillna(train['Deal_value'].median(),inplace=True)\ntrain['Weighted_amount'].fillna(train['Weighted_amount'].median(),inplace=True)\n\ntest['Deal_value'].fillna(test['Deal_value'].median(),inplace=True)\ntest['Weighted_amount'].fillna(test['Weighted_amount'].median(),inplace=True)","c6f6d558":"#Industry column has only 1 Nan value so filling it with most appeared category\ntrain['Industry'].fillna('Banks',inplace=True)\ntest['Industry'].fillna('Banks',inplace=True)","2a58cb6d":"train['Geography'].fillna(train['Geography'].mode()[0],inplace=True)\ntest['Geography'].fillna(test['Geography'].mode()[0],inplace=True)\ntrain['Geography'].value_counts()","93391e3b":"train['Last_lead_update'].fillna(train['Last_lead_update'].mode()[0],inplace=True)\ntest['Last_lead_update'].fillna(test['Last_lead_update'].mode()[0],inplace=True)\ntrain['Last_lead_update'].value_counts()","94638ac4":"train['Resource'].fillna(train['Resource'].mode()[0],inplace=True)\ntrain.loc[0,'Resource']='No'\ntest['Resource'].fillna(test['Resource'].mode()[0],inplace=True)\ntrain['Resource'].value_counts()","722c8351":"train.drop(['Deal_title','Lead_name','Contact_no','POC_name','Lead_POC_email'],axis=1,inplace=True)\ntest.drop(['Deal_title','Lead_name','Contact_no','POC_name','Lead_POC_email'],axis=1,inplace=True)","18a16054":"s=(train.dtypes=='object')\ncategorical_features=list(s[s].index)\nprint(\"Categorical Features in the Dataset are:\")\nprint(\"\")\nprint(categorical_features)","d1f32108":"for i in categorical_features:\n    train[i]=train[i].astype('category')\n    test[i]=test[i].astype('category')","17551399":"from sklearn.preprocessing import LabelEncoder\nlb=LabelEncoder()\nfor i in categorical_features:\n    train[i]=lb.fit_transform(train[i])\n    test[i]=lb.fit_transform(test[i])","7f6daee2":"for feature in train.columns:\n    data=train.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel('count')\n    plt.title(feature)\n    plt.show()","99df4847":"for feature in train.columns:\n    data=train.copy()\n    \n    data[feature]=np.log(data[feature])\n    data['Success_probability']=np.log(data['Success_probability'])\n    plt.scatter(data[feature],data['Success_probability'])\n    plt.xlabel(feature)\n    plt.ylabel('Success_probability')\n    plt.title(feature)\n    plt.show()","35e5da20":"import matplotlib.pyplot as plt\nfor feature in train.columns:\n    data=train.copy()\n    data[feature]=np.log(data[feature])\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","eeef457a":"print(\"Train--> Internal rating\")\nprint(train['Internal_rating'].value_counts())\n\nprint(\"Test--> Internal rating\")\nprint(test['Internal_rating'].value_counts())","0ad4ce2a":"mask1=(test['Internal_rating']==-1.00)|(test['Internal_rating']==82.34)\ntest.loc[mask1,'Internal_rating']=4.00\ntest['Internal_rating']=test['Internal_rating'].astype('int64')","9d74539a":"print(\"Train--> Location\")\nprint(train['Location'].value_counts())\n\nprint(\"Test--> Location\")\nprint(test['Location'].value_counts())","225e3221":"print(\"Train--> Industry\")\nprint(train['Industry'].value_counts())\n\nprint(\"Test--> Industry\")\nprint(test['Industry'].value_counts())","24a1efa6":"print(\"Train--> Hiring_candidate_role\")\nprint(train['Hiring_candidate_role'].value_counts())\n\nprint(\"Test--> Hiring_candidate_role\")\nprint(test['Hiring_candidate_role'].value_counts())","6ccfccf2":"Y=train['Success_probability']\nX=train.drop(['Success_probability','Hiring_candidate_role','Industry','Location'],axis=1)","239c66cf":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\nparam_test = {\n 'max_depth':[3,4,5],\n'n_estimators':[9,10,11]\n}\n\ngsearch1 = GridSearchCV(estimator = XGBRegressor(objective= \"reg:linear\",learning_rate=0.11), \nparam_grid = param_test, scoring=metrics.mean_squared_error,n_jobs=-1,cv=3)\ngsearch1.fit(X,Y)","c56207bb":"gsearch1.best_params_","80df0b18":"model=XGBRegressor( learning_rate = 0.11,\n                max_depth =4,objective=\"reg:linear\",alpha =1,n_estimators=9)\nmodel.fit(X,Y)","317991f5":"test.drop(['Hiring_candidate_role','Industry','Location'],axis=1,inplace=True)\ny_pred=model.predict(test)","024d13d7":"t=pd.read_csv('..\/input\/hackerearths-reduce-marketing-waste\/test.csv')","6cb0075c":"submission=pd.DataFrame(t['Deal_title'])\nsubmission['Success_probability']=y_pred\nsubmission","4a06ee81":"submission.to_csv('Submission17.csv',index=False)","b465bbf6":"### ****The columns Hiring_candidate_role,Industry and Location have so many outlier so remove them from training set****","2f4875c0":"### Converting Deal_value and Weighted_amount columns to float type by deleting '$' character","f998757f":"## ****Looking for Outliers using Box Plots****","0f037f5a":"# ****Training Model****","885c9c82":"## ****Handling Categorical Variables****","8ba02259":"### Extracting Country code from location","47d57d98":"### Extracting year from Date_of_creation column","55d4605e":"# ****Visualising our Data using Histograms and Scatter Plots****","b006badd":"## ****Hyperparameter Tuning****","ea4a5b45":"# ****Analysis of Training Data****","fa2b23a8":"# ****EDA(Exploratory Data Analysis)****","e0fea70e":"## ****Handling Missing Values****"}}