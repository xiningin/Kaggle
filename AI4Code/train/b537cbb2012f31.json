{"cell_type":{"dbc3c1b9":"code","99a6b63c":"code","da0bede1":"code","d04ab467":"code","a5b30dc9":"code","36a4f36d":"code","1f392882":"code","7a38cd93":"code","b0f01ddb":"code","2ed10b51":"code","e0bc4a0f":"code","d51897dd":"code","22de6275":"code","84d59e24":"code","7ea3ae6c":"markdown","c8f6ccb8":"markdown","d65c52bc":"markdown","88e3fc4d":"markdown"},"source":{"dbc3c1b9":"%%time\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\n# Load data\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\n\nprint(train.shape)\nprint(test.shape)","99a6b63c":"%%time\n\n# Subset\ntarget = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","da0bede1":"# # Transfer the cyclical features into two dimensional sin-cos features\n# # https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning\n# def cyclical_encode(data, col, max_val):\n#     data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n#     data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n#     return data\n\n# train = cyclical_encode(train, 'day', 7)\n# test = cyclical_encode(test, 'day', 7) \n\n# train = cyclical_encode(train, 'month', 12)\n# test = cyclical_encode(test, 'month', 12)\n\n# train.drop(['day', 'month'], axis=1, inplace=True)\n# test.drop(['day', 'month'], axis=1, inplace=True)","d04ab467":"# First, I encode ord_1 to ord_4 since the numbers of their unique values are small \nmapper_ord_1 = {'Novice': 1, \n                'Contributor': 2,\n                'Expert': 3, \n                'Master': 4, \n                'Grandmaster': 5}\n\n# https:\/\/www.kaggle.com\/asimandia\/let-s-try-some-feature-engineering\ntraintest = pd.concat([train, test])\ntrain['ord_1_count'] = train['ord_1'].map(traintest['ord_1'].value_counts().to_dict())\ntest['ord_1_count'] = test['ord_1'].map(traintest['ord_1'].value_counts().to_dict())\n\nmapper_ord_2 = {'Freezing': 1, \n                'Cold': 2, \n                'Warm': 3, \n                'Hot': 4,\n                'Boiling Hot': 5, \n                'Lava Hot': 6}\n\nmapper_ord_3 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, \n                'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15}\n\nmapper_ord_4 = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, \n                'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15,\n                'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, \n                'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n\nfor col, mapper in zip(['ord_1', 'ord_2', 'ord_3', 'ord_4'], [mapper_ord_1, mapper_ord_2, mapper_ord_3, mapper_ord_4]):\n    train[col+'_oe'] = train[col].replace(mapper)\n    test[col+'_oe'] = test[col].replace(mapper)\n    train.drop(col, axis=1, inplace=True)\n    test.drop(col, axis=1, inplace=True)","a5b30dc9":"# # Then encode 'ord_5' using ACSII values\n\n# # Option 1: Add up the indices of two letters in string.ascii_letters\n# train['ord_5_oe_add'] = train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n# test['ord_5_oe_add'] = test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n# # Option 2: Join the indices of two letters in string.ascii_letters\n# train['ord_5_oe_join'] = train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n# test['ord_5_oe_join'] = test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n# # Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\ntrain['ord_5_oe1'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ntest['ord_5_oe1'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ntrain['ord_5_oe2'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ntest['ord_5_oe2'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\n# Option 4: Simply sort their values by string\n# https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105702#latest-607652\nord_5 = sorted(list(set(train['ord_5'].values)))\nord_5 = dict(zip(ord_5, range(len(ord_5))))\ntrain.loc[:, 'ord_5'] = train['ord_5'].apply(lambda x: ord_5[x]).astype(float)\ntest.loc[:, 'ord_5'] = test['ord_5'].apply(lambda x: ord_5[x]).astype(float)\n\n# train.drop('ord_5', axis=1, inplace=True)\n# test.drop('ord_5', axis=1, inplace=True)","36a4f36d":"# Transfer the dtypes of encoded ordinal features into float64\nfor col in ['ord_0', 'ord_1_oe', 'ord_2_oe', 'ord_3_oe', 'ord_4_oe', 'ord_5_oe1', 'ord_5_oe2', 'ord_1_count']: #, 'ord_5_oe_add', 'ord_5_oe_join'\n    train[col]= train[col].astype('float64')\n    test[col]= test[col].astype('float64')","1f392882":"# # Do some ordinal encoding\n# from sklearn.preprocessing import OrdinalEncoder\n\n# ordinal_columns = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4']\n# oe = OrdinalEncoder()\n# train_oe = oe.fit_transform(train[ordinal_columns])\n# test_oe = oe.transform(test[ordinal_columns])\n\n# ordinal_columns_oe = ['ord_0_oe_skl', 'ord_1_oe_skl', 'ord_2_oe_skl', 'ord_3_oe_skl', 'ord_4_oe_skl', 'ord_5_oe_skl']\n# train = pd.concat([train, pd.DataFrame(train_oe, columns=ordinal_columns_oe)], axis=1)\n# test = pd.concat([test, pd.DataFrame(test_oe, columns=ordinal_columns_oe)], axis=1)\n\n# train.drop(ordinal_columns, axis=1, inplace=True)\n# test.drop(ordinal_columns, axis=1, inplace=True)","7a38cd93":"# Check the data type of each feature.\ntrain.dtypes","b0f01ddb":"train.head()","2ed10b51":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom catboost import Pool, CatBoostClassifier\nfrom category_encoders import TargetEncoder\n\n# Specify the non-float features as categorical to the model.\ncategorical_features_indices = np.where(test.dtypes != np.float)[0]\nprint('Categorial Feature Indices: ', categorical_features_indices)","e0bc4a0f":"%%time\n\n# Model\ndef run_cv_model(categorical_indices, train, test, target, model_fn, params={}, eval_fn=None, label='model', n_folds=5):\n    kf = KFold(n_splits=n_folds)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0]))\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = test.columns\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('-------------------------------------------')\n        print('Started ' + label + ' fold ' + str(i) + f'\/{n_folds}')\n        dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n        dev_y, val_y = target.iloc[dev_index], target.iloc[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, fi = model_fn(categorical_indices, dev_X, dev_y, val_X, val_y, test, params2)\n        feature_importances[f'fold_{i}'] = fi\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            print(label + ' cv score {}: {}'.format(i, cv_score), '\\n')\n        i += 1\n    print('{} cv scores : {}'.format(label, cv_scores))\n    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n    pred_full_test = pred_full_test \/ n_folds\n    results = {'label': label,\n              'train': pred_train, 'test': pred_full_test,\n              'cv': cv_scores, 'fi': feature_importances}\n    return results\n\n\ndef runCAT(categorical_indices, train_X, train_y, test_X, test_y, test_X2, params):\n    # Pool the data and specify the categorical feature indices\n    print('Pool Data')\n    _train = Pool(train_X, label=train_y, cat_features = categorical_indices)\n    _valid = Pool(test_X, label=test_y, cat_features = categorical_indices)    \n    print('Train CAT')\n    model = CatBoostClassifier(**params)\n    fit_model = model.fit(_train,\n                          eval_set=_valid,\n                          use_best_model=True,\n                          verbose=1000,\n                          plot=False)\n    feature_im = fit_model.feature_importances_\n    print('Predict 1\/2')\n    pred_test_y = fit_model.predict_proba(test_X)[:, 1]\n    print('Predict 2\/2')\n    pred_test_y2 = fit_model.predict_proba(test_X2)[:, 1]\n    return pred_test_y, pred_test_y2, feature_im\n\n\n# Use some baseline parameters\ncat_params = {'loss_function': 'CrossEntropy', \n              'eval_metric': \"AUC\",\n              'task_type': \"GPU\",\n              'learning_rate': 0.01,\n              'iterations': 10000,\n              'random_seed': 42,\n              'od_type': \"Iter\",\n#               'bagging_temperature': 0.2,\n#               'depth': 10,\n              'early_stopping_rounds': 500,\n             }\n\nn_folds = 5\nresults = run_cv_model(categorical_features_indices, train, test, target, runCAT, cat_params, auc, 'cat', n_folds=n_folds)","d51897dd":"%%time\n\n# Make submission\nsubmission = pd.DataFrame({'id': test_id, 'target': results['test']})\nsubmission.to_csv('submission.csv', index=False)","22de6275":"# Calculate the average feature importance for each feature\nfeature_importances = results['fi']\nfeature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(n_folds)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\nfeature_importances.sort_values(by='average', ascending=False).head()","84d59e24":"# Plot the feature importances with min\/max\/average using seaborn\nfeature_importances_flatten = pd.DataFrame()\nfor i in range(1, len(feature_importances.columns)-1):\n    col = ['feature', feature_importances.columns.values[i]]\n    feature_importances_flatten = pd.concat([feature_importances_flatten, feature_importances[col].rename(columns={f'fold_{i}': 'importance'})], axis=0)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances_flatten.sort_values(by='importance', ascending=False), x='importance', y='feature')\nplt.title('Feature Importances over {} folds'.format(n_folds))  \nplt.savefig(\"feature_importances.png\")","7ea3ae6c":"Transfer the cyclical features into two dimensional sin-cos features","c8f6ccb8":"Just do ordinal encoding for 'ord_1' to 'ord_4' by order. 'ord_0' contains numerical values so we do not need to encode it again.","d65c52bc":"Here I have also tried to encode 'ord_5'.\n\n**Option 1**: Add up the indices of two letters in string.ascii_letters\n\n**Option 2**: Join the indices of two letters in string.ascii_letters\n\n**Option 3**: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\n\n**Option 4**: Simply sort their values by string\n","88e3fc4d":"Catboost is known as a convenient and effective tool for handling categorical features. Let's build a Catboost baseline by specifying the categorical feature indices to the model. I have also tried some cyclical and ordinal encoding methods. \n\nThis is my first try on Catboost. If you have found any mistake, please write your comments below. Thank you very much.\n\nThis notebook is modified from [Why Not Logistic Regression?][1]. Please upvote that notebook first.\n\n[1]: https:\/\/www.kaggle.com\/peterhurford\/why-not-logistic-regression"}}