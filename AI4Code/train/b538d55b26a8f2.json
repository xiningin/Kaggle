{"cell_type":{"89bb98ff":"code","63cb4c17":"code","5b3b69df":"code","7d3cc421":"code","7d8ec14f":"code","6947da80":"code","24b9f5c2":"code","21907508":"code","4792c616":"code","0f52fc54":"code","73636176":"code","bda75bef":"code","17f5aaaf":"code","0e643e37":"code","5387c5d9":"code","f8ff8862":"code","16b89503":"code","a35ad073":"code","0de4940c":"code","6a3f9680":"code","82cba290":"code","025fbe4f":"code","3c0d99e2":"code","21b887f6":"code","5c31b03a":"code","8fc08a3e":"code","574f4862":"code","15a9b69f":"code","c27ac45b":"code","56d506f3":"code","b49b4153":"code","2296ce66":"code","30120f2e":"code","8f6f59fe":"code","29d621b0":"code","71756f9f":"code","6859c148":"code","b83b74e2":"code","0ac9ecce":"code","fcde04a1":"code","fea06dc1":"code","18ca9388":"code","c2d04df8":"code","31c73ff6":"code","4c8b6422":"markdown","8eec9403":"markdown","b2970c72":"markdown","dcde20d9":"markdown","8d04f7fd":"markdown","20a8375b":"markdown","380b5340":"markdown","63fed8d3":"markdown","40fbb3b7":"markdown","b748b025":"markdown","7030e9ea":"markdown","c6bc21f8":"markdown","30bfc9a6":"markdown","0d787bda":"markdown","923c5c80":"markdown","1b8eb881":"markdown","2314a913":"markdown","c485c0e8":"markdown","6991e720":"markdown","4ebd8c0e":"markdown"},"source":{"89bb98ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63cb4c17":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/divorce-prediction\/divorce_data.csv', delimiter=';', encoding = \"ISO-8859-1\", nrows = nRowsRead)\ndf.dataframeName = 'divorce_data.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","5b3b69df":"# checking dataset\n\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())","7d3cc421":"corr = df.corr()\ncorr.style.background_gradient(cmap = 'coolwarm')","7d8ec14f":"import statsmodels.formula.api as smf\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, auc\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","6947da80":"y = df[\"Divorce\"]\nX = df.drop([\"Divorce\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","24b9f5c2":"cart = DecisionTreeClassifier(max_depth = 12)","21907508":"cart_model = cart.fit(X_train, y_train)","4792c616":"y_pred = cart_model.predict(X_test)","0f52fc54":"print('Decision Tree Model')\n\nprint('Accuracy Score: {}\\n\\nConfusion Matrix:\\n {}\\n\\nAUC Score: {}'\n      .format(accuracy_score(y_test,y_pred), confusion_matrix(y_test,y_pred), roc_auc_score(y_test,y_pred)))","73636176":"pd.DataFrame(data = cart_model.feature_importances_*100,\n                   columns = [\"Importances\"],\n                   index = X_train.columns).sort_values(\"Importances\", ascending = False)[:20].plot(kind = \"barh\", color = \"r\")\n\nplt.xlabel(\"Feature Importances (%)\")","bda75bef":"# We can use the functions to apply the models and roc curves to save space.\ndef model(algorithm, X_train, X_test, y_train, y_test):\n    alg = algorithm\n    alg_model = alg.fit(X_train, y_train)\n    global y_prob, y_pred\n    y_prob = alg.predict_proba(X_test)[:,1]\n    y_pred = alg_model.predict(X_test)\n\n    print('Accuracy Score: {}\\n\\nConfusion Matrix:\\n {}'\n      .format(accuracy_score(y_test,y_pred), confusion_matrix(y_test,y_pred)))\n    \n\ndef ROC(y_test, y_prob):\n    \n    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_prob)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    \n    plt.figure(figsize = (10,10))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, color = 'red', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1], linestyle = '--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')","17f5aaaf":"print('Model: Logistic Regression\\n')\nmodel(LogisticRegression(solver = \"liblinear\"), X_train, X_test, y_train, y_test)","0e643e37":"LogR = LogisticRegression(solver = \"liblinear\")\ncv_scores = cross_val_score(LogR, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","5387c5d9":"ROC(y_test, y_prob)","f8ff8862":"print('Model: Gaussian Naive Bayes\\n')\nmodel(GaussianNB(), X_train, X_test, y_train, y_test)","16b89503":"NB = GaussianNB()\ncv_scores = cross_val_score(NB, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","a35ad073":"ROC(y_test, y_prob)","0de4940c":"#I excluded probability in the function for SVC, also I could not use other kernel methods because it takes really long and I don't think SVC as a good model for this dateset. \nprint('Model: SVC\\n')\n\ndef model1(algorithm, X_train, X_test, y_train, y_test):\n    alg = algorithm\n    alg_model = alg.fit(X_train, y_train)\n    global y_pred\n    y_pred = alg_model.predict(X_test)\n    \n    print('Accuracy Score: {}\\n\\nConfusion Matrix:\\n {}'\n      .format(accuracy_score(y_test,y_pred), confusion_matrix(y_test,y_pred)))\n    \nmodel1(SVC(kernel = 'linear'), X_train, X_test, y_train, y_test)","6a3f9680":"print('Model: Decision Tree\\n')\nmodel(DecisionTreeClassifier(max_depth = 12), X_train, X_test, y_train, y_test)","82cba290":"DTC = DecisionTreeClassifier(max_depth = 12)\ncv_scores = cross_val_score(DTC, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","025fbe4f":"ROC(y_test, y_prob)","3c0d99e2":"print('Model: Random Forest\\n')\nmodel(RandomForestClassifier(), X_train, X_test, y_train, y_test)","21b887f6":"RFC = RandomForestClassifier()\ncv_scores = cross_val_score(RFC, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","5c31b03a":"ROC(y_test, y_prob)","8fc08a3e":"rf_parameters = {\"max_depth\": [10,13],\n                 \"n_estimators\": [10,100,500],\n                 \"min_samples_split\": [2,5]}","574f4862":"rf_model = RandomForestClassifier()","15a9b69f":"rf_cv_model = GridSearchCV(rf_model,\n                           rf_parameters,\n                           cv = 10,\n                           n_jobs = -1,\n                           verbose = 2)\n\nrf_cv_model.fit(X_train, y_train)","c27ac45b":"print('Best parameters: ' + str(rf_cv_model.best_params_))","56d506f3":"rf_tuned = RandomForestClassifier(max_depth = 13,\n                                  min_samples_split = 2,\n                                  n_estimators = 500)\n\nprint('Model: Random Forest Tuned\\n')\nmodel(rf_tuned, X_train, X_test, y_train, y_test)","b49b4153":"print('Model: XGBoost\\n')\nmodel(XGBClassifier(), X_train, X_test, y_train, y_test)","2296ce66":"XGB = XGBClassifier()\ncv_scores = cross_val_score(XGB, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","30120f2e":"ROC(y_test, y_prob)","8f6f59fe":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","29d621b0":"print('Model: Neural Network\\n')\nmodel(MLPClassifier(), X_train_scaled, X_test_scaled, y_train, y_test)","71756f9f":"ROC(y_test, y_prob)","6859c148":"mlpc_parameters = {\"alpha\": [1, 0.1, 0.01, 0.001],\n                   \"hidden_layer_sizes\": [(50,50,50),\n                                          (100,100)],\n                   \"solver\": [\"adam\", \"sgd\"],\n                   \"activation\": [\"logistic\", \"relu\"]}","b83b74e2":"mlpc = MLPClassifier()\nmlpc_cv_model = GridSearchCV(mlpc, mlpc_parameters,\n                             cv = 10,\n                             n_jobs = -1,\n                             verbose = 2)\n\nmlpc_cv_model.fit(X_train_scaled, y_train)","0ac9ecce":"print('Best parameters: ' + str(mlpc_cv_model.best_params_))","fcde04a1":"mlpc_tuned = MLPClassifier(activation = 'relu',\n                           alpha = 0.1,\n                           hidden_layer_sizes = (100,100),\n                           solver = 'adam')","fea06dc1":"print('Model: Neural Network Tuned\\n')\nmodel(mlpc_tuned, X_train_scaled, X_test_scaled, y_train, y_test)","18ca9388":"ROC(y_test, y_prob)","c2d04df8":"randomf = RandomForestClassifier()\nrf_model1 = randomf.fit(X_train, y_train)\n\npd.DataFrame(data = rf_model1.feature_importances_*100,\n                   columns = [\"Importances\"],\n                   index = X_train.columns).sort_values(\"Importances\", ascending = False)[:15].plot(kind = \"barh\", color = \"r\")\n\nplt.xlabel(\"Feature Importances (%)\")","31c73ff6":"table = pd.DataFrame({\"Model\": [\"Decision Tree (reservation status included)\", \"Logistic Regression\",\n                                \"Naive Bayes\", \"Support Vector\", \"Decision Tree\", \"Random Forest\",\n                                \"Random Forest Tuned\", \"XGBoost\", \"Neural Network\", \"Neural Network Tuned\"],\n                     \"Accuracy Scores\": [\"0.88\", \"0.98\", \"0.98\", \"1.00\", \"0.846\",\n                                         \"1.00\", \"0.851\", \"0.98\", \"0.98\", \"0.98\"],\n                     \"ROC | Auc\": [\"0.88\", \"1.00\", \"0.98\", \"0.88\",\n                                   \"0.92\", \"0.98\", \"0\", \"0.99\",\n                                   \"1.00\", \"1.00\"]})\n\n\ntable[\"Model\"] = table[\"Model\"].astype(\"category\")\ntable[\"Accuracy Scores\"] = table[\"Accuracy Scores\"].astype(\"float32\")\ntable[\"ROC | Auc\"] = table[\"ROC | Auc\"].astype(\"float32\")\n\npd.pivot_table(table, index = [\"Model\"]).sort_values(by = 'Accuracy Scores', ascending=False)","4c8b6422":"#Neural Network Model","8eec9403":"#Support Vector Classification Model","b2970c72":"#Decision Tree Model ","dcde20d9":"The conclusion above isn't mine. It's from Utku Engin  https:\/\/www.kaggle.com\/vssseel\/eda-various-ml-models-and-nn-with-roc-curves\/notebook","8d04f7fd":"As we can see from the summary table, the best algorithm is random forest for this data set.\n0 values are uncalculated ones.\nWe do not count decision tree with reservatiton status which is broken. All algorithms would give 100% accuracy scores while reservation status is included.\nTuning for XGBoost would be a good challenge too.","20a8375b":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcSmsmOyS47IpWRquRvW9jLRAnSwI1OPRs7m5g&usqp=CAU)virkpersonalinjurylawyers.com","380b5340":"#Summary Table of the Models","63fed8d3":"Tuned model has worse accuracy score than default one. In the default model there is no limit for max depth. Increasing max depth gives us better accuracy scores but may decrease generalization.","40fbb3b7":"Question1: If one of us apologizes when our discussion deteriorates, the discussion ends.\n\nQuestion2: I know we can ignore our differences, even if things get hard sometimes.\n\nQuestion3: When we need it, we can take our discussions with my spouse from the beginning and correct it.\n\nQuestion4: When I discuss with my spouse, to contact him will eventually work.\n\nQuestion5: The time I spent with my wife is special for us.\n\nQuestion6: We don't have time at home as partners.\n\nQuestion7: We are like two strangers who share the same environment at home rather than family.\n\nQuestion8: I enjoy our holidays with my wife.\n\nQuestion9: I enjoy traveling with my wife.\n\nQuestion10: Most of our goals are common to my spouse.\n\nQuestion11: I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.\n\nQuestion31: I feel aggressive when I argue with my spouse.\n\nQuestion32: When discussing with my spouse, I usually use expressions such as \u2018you always\u2019 or \u2018you never\u2019.\n\nQuestion33: I can use negative statements about my spouse's personality during our discussions.\n\nQuestion34: I can use offensive expressions during our discussions.\n\nQuestion35: I can insult my spouse during our discussions.\n\nQuestion43: I mostly stay silent to calm the environment a little bit.\n\nQuestion46: Even if I'm right in the discussion, I stay silent to hurt my spouse.\n\nQuestion53: When I discuss, I remind my spouse of her\/his inadequacy.\n\nQuestion55: Whether divorce occured or not.","b748b025":"#Gaussian Naive Bayes Model","7030e9ea":"#XGBoost Model","c6bc21f8":"#Codes from Utku Engin   https:\/\/www.kaggle.com\/vssseel\/eda-various-ml-models-and-nn-with-roc-curves\/notebook","30bfc9a6":"#Conclusion\n\n#Feature Importances","0d787bda":"#Random Forest Model Tuning","923c5c80":"#Decision Tree Model","1b8eb881":"#Model and ROC Curve Comparison","2314a913":"#Random Forest","c485c0e8":"#Logistic Regression","6991e720":"#Neural Network Model Tuning","4ebd8c0e":"Das War's Kaggle Notebook Runner: Mar\u00edlia Prata   @mpwolke"}}