{"cell_type":{"a3a3a0d8":"code","04dea1ec":"code","084b4692":"code","42474825":"code","995e6825":"code","631e1886":"code","7c87b7b8":"code","44b0bfea":"code","8dc19253":"code","5a519c47":"code","b4c45b65":"code","46aa36ed":"code","983504b5":"code","50f8ac64":"code","54b949ed":"code","6170fc59":"code","0846feab":"code","5c19b513":"code","90259f55":"code","3525ccf8":"code","2ebbc697":"code","7300ad86":"code","452d09a8":"code","8170fe8c":"code","8727186f":"code","447afdec":"code","76d523d8":"code","71a65f59":"code","55ab6a99":"code","3b367709":"markdown"},"source":{"a3a3a0d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk #NLTK(Natural Language Toolkit)\nfrom nltk.corpus import stopwords #NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages.\nfrom nltk.tokenize import word_tokenize# To tokenize words\nimport re\nimport seaborn as sns\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","04dea1ec":"# Any results you write to the current directory are saved as output.\n\ntrain_df=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n","084b4692":"## EDA\n#Examine the dataset\ntrain_df.head()\ntrain_df.shape\n\n\n","42474825":"train_df.head()","995e6825":"print (list(train_df))","631e1886":"#find out number of null values in location colum\nprint(train_df['location'].isnull().sum())\n","7c87b7b8":"#Have a look at few colums value in the dataset\ntrain_df['text'][0:2]\n","44b0bfea":"train_df['location'][0:2]","8dc19253":"#Replace # with ''\nspec_chars = [\"#\",\"%\"]\nfor char in spec_chars:\n    train_df['text'] = train_df['text'].str.replace(char, '')","5a519c47":"train_df.head(20)","b4c45b65":"sns.heatmap(train_df.isnull(), cmap='viridis')","46aa36ed":"#Below step breaks up the strings into a list of words or pieces based on a specified pattern using Regular Expressions. The pattern I chose to use (r'\\w') also removes punctuation and is a better option for this data in particular.","983504b5":"tokenizer = RegexpTokenizer(r'\\w+')\ntrain_df['text']=train_df['text'].apply(lambda x : tokenizer.tokenize(x.lower()))\n","50f8ac64":"##Removing stopwords with NTLK in python\nstop_words=set(stopwords.words('english'))\ndef remove_stopwords(text):\n    words= [w for w in text if w not in stopwords.words('english')]\n    return words\n#word_tokens = word_tokenize(train_df['text'])","54b949ed":"train_df['text']=train_df['text'].apply(lambda x : remove_stopwords(x))\n","6170fc59":"train_df['text'].head(10)","0846feab":"#Lemmatizing maps common words into one base. It returns a proper word that can be found in the dictionary.","5c19b513":"#Dataset after removing stop words\ntrain_df.head()","90259f55":"lemmatizer = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    lem_text= [lemmatizer.lemmatize(i) for i in text]\n    return lem_text \ntrain_df['text']=train_df['text'].apply(lambda x : word_lemmatizer(x))\ntrain_df.head()","3525ccf8":"#Dataset after removing stop words\ntrain_df['text'].head()","2ebbc697":"# Create y_train\ny_train=train_df['target']","7300ad86":"\ntrain_df['text']=train_df['text'].apply(lambda x: \" \".join(x) )\ncount_vectorizer= feature_extraction.text.CountVectorizer() #AKA One-hot encoding\nx=count_vectorizer.fit_transform(train_df['text'][0:5])\n\nprint(x[1].todense().shape) #There are 36 unique words (or \"tokens\") in the first five tweets.\nprint(x[1].todense())","452d09a8":"#create vectors for all of our tweets.\ntrain_vector=count_vectorizer.fit_transform(train_df['text'])\ntest_vector=count_vectorizer.transform(test_df['text'])\n\n\n","8170fe8c":"#Now we\u2019re ready to fit a Multinomial Naive Bayes classifier model to our training data and use it to predict the test data\u2019s labels:\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(train_vector, y_train)\n","8727186f":"#We are using f1.Generally, F1 Score is used when you want to seek a balance between Precision and Recall.\nf1_scores = model_selection.cross_val_score(naive_bayes, train_vector, train_df[\"target\"], cv=3, scoring=\"f1\")\nf1_scores","447afdec":"#Let's do predictions on our training set and build a submission for the competition.\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","76d523d8":"sample_submission[\"target\"] = naive_bayes.predict(test_vector)","71a65f59":"sample_submission.head()","55ab6a99":"sample_submission.to_csv(\"submission.csv\", index=False)","3b367709":"Yellow is the NaN values in each column. It doesn\u2019t mean that a column with no yellow doesn\u2019t have any NaN, it happens to have some at the top of the heatmap or at the bottom so they are confused with the graphical borders of the heatmap."}}