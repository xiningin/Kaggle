{"cell_type":{"477817f3":"code","36212837":"code","3a277f60":"code","0021d546":"code","4f7061f7":"code","0b83f2e4":"code","60633702":"code","a863407a":"code","39b0ba8b":"code","9fd2560e":"code","cd8e7615":"code","a87e8636":"code","ccbb3334":"code","97a5363d":"code","423f0168":"code","a2287312":"code","8274fffe":"code","bcff7bfa":"code","26af2678":"code","5096790d":"code","cb3d6e06":"code","00e94654":"code","48e87a94":"code","60fec181":"code","760de2c0":"code","298bd702":"code","6e69d3eb":"code","2ca8df35":"code","40a518c4":"code","a1c8058c":"code","4223be25":"code","ee84bab6":"code","ed27d850":"markdown","b2d7fda4":"markdown","1e59706a":"markdown","ba44168c":"markdown","c9394f07":"markdown","ff1bd1f5":"markdown","6ba65397":"markdown","306cde46":"markdown","7f14b150":"markdown","0ccdd2c0":"markdown","91effb80":"markdown","fa84c4d8":"markdown","58b7d508":"markdown","302dfed4":"markdown","5968a02c":"markdown","019b8819":"markdown","b9b0c63e":"markdown","6812999b":"markdown","83ac6cfb":"markdown","2bef714b":"markdown","e954e5ca":"markdown","2feac13e":"markdown","9bec348d":"markdown","6c7b799b":"markdown","cbeefcde":"markdown","f771eb30":"markdown","80f0c6ac":"markdown","72d38868":"markdown","6a3e5c5a":"markdown"},"source":{"477817f3":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error, r2_score","36212837":"#lecture du dataset\ndf = pd.read_csv(\"..\/input\/bostonhoustingmlnd\/housing.csv\")","3a277f60":"df.head(10).T","0021d546":"df.info()","4f7061f7":"df.plot(kind=\"scatter\", x=\"RM\", y=\"LSTAT\", c=\"MEDV\", cmap=\"rainbow\", s=3, figsize=(12,12))","0b83f2e4":"tabcorr = df.corr()     # on peut utiliser aussi bos.corr(method='pearson') par exemple","60633702":"tabcorr","a863407a":"plt.figure(figsize=(12,12))\nsns.heatmap(abs(tabcorr), cmap=\"coolwarm\")","39b0ba8b":"sns.clustermap(abs(tabcorr), cmap=\"coolwarm\")","9fd2560e":"from scipy.cluster import hierarchy as hc\n\ncorr = 1 - df.corr()\ncorr_condensed = hc.distance.squareform(corr)\nlink = hc.linkage(corr_condensed, method='ward')\nplt.figure(figsize=(12,12))\nden = hc.dendrogram(link, labels=df.columns, orientation='left', leaf_font_size=10)","cd8e7615":"correlations = tabcorr.MEDV\nprint(correlations)","a87e8636":"correlations = correlations.drop(['MEDV'],axis=0)","ccbb3334":"print(abs(correlations).sort_values(ascending=False))","97a5363d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler","423f0168":"X=df.drop([\"MEDV\"],axis=1)\ny=df.MEDV","a2287312":"y=y.to_numpy().reshape(-1,1)","8274fffe":"\nscaler_x = preprocessing.StandardScaler().fit(X)\nX = scaler_x.transform(X)\n# scaler_y=preprocessing.StandardScaler().fit(y)\n# y = scaler_y.transform(y)","bcff7bfa":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","26af2678":"lm = LinearRegression()\nlm.fit(X_train, y_train)            # apprentissage\ny_pred = lm.predict(X_test)         # pr\u00e9diction sur l'ensemble de test","5096790d":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_pred)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","cb3d6e06":"sns.distplot(y_test-y_pred)","00e94654":"print(np.sqrt(mean_squared_error(y_test, y_pred)))","48e87a94":"scoreR2 = r2_score(y_test, y_pred)\nprint(scoreR2)","60fec181":"lm.score(X_test,y_test)","760de2c0":"X = df.drop(['MEDV'], axis=1)\ny = df.MEDV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)","298bd702":"from sklearn import ensemble\nrf = ensemble.RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\nprint(rf.score(X_test,y_test))","6e69d3eb":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_rf)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","2ca8df35":"sns.distplot(y_test-y_rf)","40a518c4":"print(np.sqrt(mean_squared_error(y_test, y_rf)))","a1c8058c":"rf.score(X_test,y_test)","4223be25":"import xgboost as XGB\nxgb  = XGB.XGBRegressor()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\nprint(xgb.score(X_test,y_test))\n\nplt.figure(figsize=(12,12))\nplt.scatter(y_test, y_xgb)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","ee84bab6":"from sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\ndtree=DecisionTreeRegressor(random_state=1)\ndtree.fit(X_train,y_train)\ny_dtree=dtree.predict(X_test)\nprint(dtree.score(X_test,y_test))\n\n\nplt.figure(figsize=(12,12))\nplt.scatter(y_test, y_dtree)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","ed27d850":"On a vu assez clairement la corr\u00e9lation entre le nombre de pi\u00e8ces et la valeur.  \nOn va utiliser la fonction *corr* pour calculer syst\u00e9matiquement le degr\u00e9 de corr\u00e9lation entre deux param\u00e8tres :","b2d7fda4":"On peut tracer uniquement le **dendrogramme** des corr\u00e9lations entre les caract\u00e9ristiques. On utilise une forme \"condens\u00e9e\" de la matrice de corr\u00e9lation. *linkage* permet de calculer les distances entre caract\u00e9ristiques \u00e0 partir des corr\u00e9lations","1e59706a":"La standardisation de data","ba44168c":"On va utiliser les for\u00eats al\u00e9atoires pour la r\u00e9gression. On conserve l'ensemble des caract\u00e9ristiques","c9394f07":"Les donn\u00e9es originales du probl\u00e8me de pr\u00e9diction du prix des maisons \u00e9tant de type tabulaire, les r\u00e9seaux neuronaux peuvent \u00eatre moins performants que les mod\u00e8les arborescents sur ce type de probl\u00e8me.","ff1bd1f5":"On voit que la plus forte corr\u00e9lation concerne la surface","6ba65397":"Pour visualiser l'ensemble du tableau de corr\u00e9lations, on utilise une \"carte de temp\u00e9ratures\" (*heatmap*) :","306cde46":"On utilise la fonction de r\u00e9gression lin\u00e9aire multiple de *sklearn* :","7f14b150":"On s\u00e9pare les caract\u00e9ristiques continues et discr\u00e8tes :","0ccdd2c0":"Il semble que le model randomforrest est mieux ici que le model de XGBoost et de DecisionTree","91effb80":"On peut aussi regrouper les param\u00e8tres par *clusters* class\u00e9s par proximit\u00e9 :","fa84c4d8":"qu'on peut \u00e9crire plus simplement :","58b7d508":"Ou le *score R2* (rapport des variances estim\u00e9e\/r\u00e9elle) :\n(https:\/\/fr.wikipedia.org\/wiki\/Coefficient_de_d%C3%A9termination)","302dfed4":"On peut calculer l'erreur sur les moindres carr\u00e9s :","5968a02c":"ou on peut visualiser la distribution de l'erreur avec *seaborn* :","019b8819":"On \u00e9limine la ligne MV elle-m\u00eame (qui est forc\u00e9ment \u00e0 1) :","b9b0c63e":"**Exemple de r\u00e9gressions**","6812999b":"Les corr\u00e9lations fortement n\u00e9gatives sont aussi significatives que les positives ; on consid\u00e8re donc les valeurs absolues, et on trie par ordre d\u00e9croissant :","83ac6cfb":"## R\u00e9gression par for\u00eats al\u00e9atoires","2bef714b":"On trace le nuage de points pour comparer la pr\u00e9diction et les r\u00e9sultats attendus :","e954e5ca":"Tout les features sont en float, donc on ne les change pas.","2feac13e":"# Boston house price prediction in three methods","9bec348d":"# Decision Tree","6c7b799b":"## Importations des librairies courantes","cbeefcde":"## Recherche de corr\u00e9lations","f771eb30":"# Extreme Gradient Boosting","80f0c6ac":"# R\u00e9gression lin\u00e9aire","72d38868":"**Exercice** : donner l'importance des caract\u00e9ristiques","6a3e5c5a":"On s'int\u00e9resse plus pr\u00e9cis\u00e9ment \u00e0 la valeur des maisons :"}}