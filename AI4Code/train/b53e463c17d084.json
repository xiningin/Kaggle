{"cell_type":{"5e6d36a4":"code","11c29bdb":"code","1de447c0":"code","6139f0c9":"code","08a71603":"code","c47f4bac":"code","5aa2bd74":"code","c200d2e9":"code","c7a901b8":"code","95110a56":"code","cf97c7c8":"code","a3cd996b":"code","ca33cb9e":"code","f6a8ad62":"code","3c49b280":"code","432e3e8b":"code","cf98039a":"code","7e88cf5b":"code","8a798896":"code","0a3dd028":"code","5a8958c1":"code","bb0c946b":"code","a58314ac":"markdown","2f3855f1":"markdown","ff36245f":"markdown","cc34da62":"markdown","51f9f030":"markdown","02638ac7":"markdown"},"source":{"5e6d36a4":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt","11c29bdb":"train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","1de447c0":"train.describe()","6139f0c9":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['deg_C'], kde=True)\nax.set(xlabel='Degrees In Celsius', ylabel='Frequency')","08a71603":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['relative_humidity'], kde=True)\nax.set(xlabel='Relative Humidity', ylabel='Frequency')","c47f4bac":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['absolute_humidity'], kde=True)\nax.set(xlabel='Absolute Humidity', ylabel='Frequency')","5aa2bd74":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['sensor_1'], kde=True)\nax.set(xlabel='Sensor 1', ylabel='Frequency')","c200d2e9":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['sensor_2'], kde=True)\nax.set(xlabel='Sensor 2', ylabel='Frequency')","c7a901b8":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['sensor_3'], kde=True)\nax.set(xlabel='Sensor 3', ylabel='Frequency')","95110a56":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['sensor_4'], kde=True)\nax.set(xlabel='Sensor 4', ylabel='Frequency')","cf97c7c8":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['sensor_5'], kde=True)\nax.set(xlabel='Sensor 5', ylabel='Frequency')","a3cd996b":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['target_carbon_monoxide'], kde=True)\nax.set(xlabel='Carbon Monoxide', ylabel='Frequency')","ca33cb9e":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['target_benzene'], kde=True)\nax.set(xlabel='Benzene', ylabel='Frequency')","f6a8ad62":"plt.figure(figsize=(20, 10))\nax = sns.histplot(train['target_nitrogen_oxides'], kde=True)\nax.set(xlabel='Nitrogen Oxides', ylabel='Frequency')","3c49b280":"plt.figure(figsize=(20, 10))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","432e3e8b":"train_features = train.drop(['date_time', 'target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'], axis=1).values\ntest_features = test.drop('date_time', axis=1).values\ntargets = train[['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']].values","cf98039a":"print(f'The train set has {train_features.shape[0]} samples, and {train_features.shape[1]} features')\nprint(f'The test set has {test_features.shape[0]} samples, and {test_features.shape[1]} features')","7e88cf5b":"def CRMSLE(truth, pred):\n    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n    rmsle1 = (msle(truth[:,0], pred[:,0]))**(1\/2)\n    rmsle2 = (msle(truth[:,1], pred[:,1]))**(1\/2)\n    rmsle3 = (msle(truth[:,2], pred[:,2]))**(1\/2)\n    return (rmsle1 + rmsle2 + rmsle3)\/3","8a798896":"from sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.preprocessing import normalize\nimport lightgbm\nimport xgboost\nimport catboost\nlgb = MultiOutputRegressor(lightgbm.LGBMRegressor(), n_jobs=-1)","0a3dd028":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\nlr = LinearRegression()\nrd = Ridge()\nkfold = KFold(n_splits=10, random_state=2021, shuffle=True)\n\nlgb_valid_preds = []\nlgb_test_preds = []\n\ndt_valid_preds = []\ndt_test_preds = []\n\nlr_valid_preds = []\nlr_test_preds = []\nblend_preds = []\n\ntrain_features = pd.DataFrame(normalize(train_features))\ntest_features = pd.DataFrame(normalize(test_features))\ntargets = pd.DataFrame(targets)\n\nfor fold, (train_idx, test_idx) in enumerate(kfold.split(train_features, targets)):\n    \n    print('*' * 15, f'Fold {fold+1}', '*' * 15)\n    \n    X_train, X_valid = train_features.iloc[train_idx], train_features.iloc[test_idx]\n    y_train, y_valid = targets.iloc[train_idx].to_numpy(), targets.iloc[test_idx].to_numpy()\n    \n    lgb.fit(X_train, y_train)\n    lgb_valid_preds.append(lgb.predict(X_valid))\n    lgb_test_preds.append(lgb.predict(test_features))\n    \n    dt.fit(X_train, y_train)\n    dt_valid_preds.append(dt.predict(X_valid))\n    dt_test_preds.append(dt.predict(test_features))\n    \n    lr.fit(X_train, y_train)\n    lr_valid_preds.append(lr.predict(X_valid))\n    lr_test_preds.append(lr.predict(test_features))\n    \n    blend_train = np.c_[lgb_valid_preds[-1], dt_valid_preds[-1], lr_valid_preds[-1]]\n    blend_test = np.c_[lgb_test_preds[-1], dt_test_preds[-1], lr_test_preds[-1]]\n    \n    rd.fit(blend_train, y_valid)\n    blend_preds.append(rd.predict(blend_test))\n    print(f'Ridge Blend CRMSLE: {CRMSLE(rd.predict(blend_train), y_valid)}')","5a8958c1":"submission[['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']] = sum(blend_preds)\/len(blend_preds)\nsubmission.to_csv('submission.csv', index=False)","bb0c946b":"submission","a58314ac":"The code below is from [The Seaborn Docs](https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html).  It shows the feature correlation between the features of the training set.","2f3855f1":"I plot all the feature distributions to get an idea of what distribution we are dealing with and to get an idea of what the scales of the data are:","ff36245f":"Based on the histogram plots of all of the features (including the targets) we can see that most follow a Gaussian Distribution (Majority of the data (99.5%) fall between 3 standard deviations from the mean). However, it is important to note that most of the data is of different scales (e.g. The sensor features have scales from 500-1200, while humidity is between 0.25-2.25). Therefore normalizing the data will be an important preprocessing step.","cc34da62":"The metric is the column wise average of the root mean squared logarithmic error between the three predictions. The following is the implementation in tensorflow. This loss works if you were to compile a neural network via (model.compile(optimizer='adam', loss=CRMSLE)).","51f9f030":"The following is the standard blending technique which uses a decision tree, lightgbm, and linear regression model. However, before beginning training I normalize the data (If you don't normalize, the lb score is 47.93549, while with normalizing is 45.15300). A ridge regressor is then fit on the blended data.","02638ac7":"I don't believe that lightgbm supports multi-output losses so you can use scikit-learns MultiOutputRegressor class which works as a wrapper around a lightgbm regressor. The following implements it:"}}