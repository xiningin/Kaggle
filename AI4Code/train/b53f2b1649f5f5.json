{"cell_type":{"876f494f":"code","9e5313a2":"code","11c60804":"code","9edaf564":"code","b3cefff7":"code","7a02a241":"code","3f13a7d2":"code","8d43bfa1":"code","11b79845":"code","9763e3cb":"code","97bab380":"code","8a8e6d74":"code","1852421d":"code","af27b4ad":"code","7d59dc82":"code","e949b5a3":"code","e10ec54a":"code","f12ae17e":"code","cc5371ee":"code","555573af":"code","3a22702e":"code","4e1105f4":"code","b76c9e45":"code","114a8e64":"code","11e7f0fb":"code","40801bcd":"code","e19c5f24":"code","64b8d57d":"code","d1e0c0ca":"code","60eef6f9":"code","a83d0ac5":"code","f5a47597":"code","07d6ea91":"code","d0fcd9c4":"code","70c6b982":"code","4a2f4ccb":"code","4afd6e6d":"code","73a57057":"code","446e4fdb":"code","dd5dbd66":"code","649ba404":"code","64a39fa3":"code","031b45b0":"code","5e5a8429":"code","70dbfcb3":"code","74c2e123":"markdown","d847df5b":"markdown","e886a79b":"markdown","0efc2100":"markdown","d9aba986":"markdown","5fc1fb9b":"markdown","4da5d548":"markdown","0a2a5bd5":"markdown","9256b882":"markdown","e70b0002":"markdown","797ccabf":"markdown","ecbd5222":"markdown","3195f3c1":"markdown","48f46b8f":"markdown","5513eecb":"markdown","cffe0661":"markdown","37ab317c":"markdown","e09ad2bb":"markdown","e3cd0f58":"markdown","9809b45f":"markdown","ca1b76f5":"markdown","3c5bb164":"markdown","0b95824c":"markdown","910034f0":"markdown","74777ff7":"markdown","e44e4316":"markdown","3f415767":"markdown","df524f9b":"markdown","8518595e":"markdown","ce24cde7":"markdown","62be8d1d":"markdown","d8987ea9":"markdown"},"source":{"876f494f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nimport re","9e5313a2":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","11c60804":"# Concatenating the dataframes in only one\ndf_train['set'] = 'train'\ndf_test['set'] = 'test'\ndf_test[\"SalePrice\"] = np.nan\ndata = pd.concat([df_train, df_test], sort=True)","9edaf564":"#Looking  data\ntotal = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","b3cefff7":"for c in ['MiscFeature', 'Alley', 'Fence']:\n    data[c].fillna('none', inplace=True)\n    ","7a02a241":"numerical_feats = data.dtypes[data.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = data.dtypes[data.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","3f13a7d2":"print(data.shape)\n\nn = data.select_dtypes(include=object)\nfor column in n.columns:\n    print(column, ':  ', data[column].unique())","8d43bfa1":"## Let's see the distribuition of the categories: \nfor category in list(categorical_feats):\n    print('#'*35)    \n    print('Distribuition of feature:', category)\n    print(data[category].value_counts(normalize=True))\n    print('#'*35)","11b79845":"fig, axes = plt.subplots(ncols=4, nrows=4, \n                         figsize=(4 * 4, 4 * 4), sharey=True)\n\naxes = np.ravel(axes)\n\ncols = ['OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n        'BsmtCond','GarageQual','GarageCond', 'MSSubClass','MSZoning',\n        'Neighborhood','BldgType','HouseStyle','Heating','Electrical','SaleType']\n\nfor i, c in zip(np.arange(len(axes)), cols):\n    ax = sns.boxplot(x=c, y='SalePrice', data=data, ax=axes[i])\n    ax.set_title(c)\n    ax.set_xlabel(\"\")","9763e3cb":"# to categorical feature\ncols = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\n        \"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\n        \"LowQualFinSF\",\"GarageYrBlt\"]\n\nfor c in cols:\n    data[c] = data[c].astype(str)\n\n# encode quality\n# Ex(Excellent), Gd\uff08Good\uff09, TA\uff08Typical\/Average\uff09, Fa\uff08Fair\uff09, Po\uff08Poor\uff09\ncols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond','PoolQC']\nfor c in cols:\n    data[c].fillna(0, inplace=True)\n    data[c].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)","97bab380":"def pair_features_to_dummies(df, col1, col2, prefix):\n    d_1 = pd.get_dummies(df[col1].astype(str), prefix=prefix)\n    d_2 = pd.get_dummies(df[col2].astype(str), prefix=prefix)\n    for c in list(set(list(d_1.columns) + list(d_2.columns))):\n        if not c in d_1.columns: d_1[c] = 0\n        if not c in d_2.columns: d_2[c] = 0\n    return (d_1 + d_2).clip(0, 1)\n\ncond = pair_features_to_dummies(data,'Condition1','Condition2','Condition')\nexterior = pair_features_to_dummies(data,'Exterior1st','Exterior2nd','Exterior')\nbsmtftype = pair_features_to_dummies(data,'BsmtFinType1','BsmtFinType2','BsmtFinType') \n\nall_data = pd.concat([data, cond, exterior, bsmtftype], axis=1)\nall_data.drop(['Condition1','Condition2', 'Exterior1st','Exterior2nd','BsmtFinType1','BsmtFinType2'], axis=1, inplace=True)\nall_data.head()","8a8e6d74":"# fillna\nfor c in ['MiscFeature', 'Alley', 'Fence']:\n    data[c].fillna('None', inplace=True)\n    \ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\ndata.loc[data.GarageYrBlt.isnull(),'GarageYrBlt'] = data.loc[all_data.GarageYrBlt.isnull(),'YearBuilt']\n\ndata['GarageType'].fillna('None', inplace=True)\ndata['GarageFinish'].fillna(0, inplace=True)\n\nfor c in ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    data[c].fillna(0, inplace=True)","1852421d":"from sklearn.preprocessing import LabelEncoder\n\n\nfor i, t in data.loc[:, data.columns != 'SalePrice'].dtypes.iteritems():\n    if t == object:\n        data[i].fillna(data[i].mode()[0], inplace=True)\n        data[i] = LabelEncoder().fit_transform(data[i].astype(str))\n    else:\n        data[i].fillna(data[i].median(), inplace=True)","af27b4ad":"data['OverallQualCond'] = data['OverallQual'] * data['OverallCond']\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['Interaction'] = data['TotalSF'] * data['OverallQual']","7d59dc82":"df_train = data[data['SalePrice'].notnull()]\ndf_test = data[data['SalePrice'].isnull()].drop('SalePrice', axis=1)","e949b5a3":"df_test.head()","e10ec54a":"fig, axes = plt.subplots(ncols=4, nrows=9, figsize=(20, 30))\naxes = np.ravel(axes)\ncol_name = df_train.corr()['SalePrice'][1:].index\nfor i in range(36):\n    df_train.plot.scatter(ax=axes[i], x=col_name[i], \n                          y='SalePrice', c='OverallQual', \n                          sharey=True, colorbar=False, cmap='viridis')","f12ae17e":"df_train = df_train[df_train['TotalSF'] < 6000]\ndf_train = df_train[df_train['TotalBsmtSF'] < 4000]\ndf_train = df_train[df_train['SalePrice'] < 700000]","cc5371ee":"X_train = df_train.drop(['SalePrice','Id'], axis=1).values\ny_train = df_train['SalePrice'].values\nX_test  = df_test.drop(['Id'], axis=1).values\n\nprint(X_train.shape, y_train.shape, X_test.shape)","555573af":"########################################################\n######## IMPORTING NECESSARY MODULES AND MODELS ########\n########################################################\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\nfrom sklearn.metrics import explained_variance_score, median_absolute_error, r2_score, mean_squared_error #To evaluate our model\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\nfrom xgboost import XGBRegressor, plot_importance # XGBoost\nfrom sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\nfrom sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline # Streaming pipelines\nfrom sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\nfrom sklearn.feature_selection import SelectFromModel # Dimensionality reduction\nfrom sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\nfrom sklearn.base import clone # Clone estimator\nfrom sklearn.metrics import mean_squared_error as MSE","3a22702e":"thresh = 5 * 10**(-4)\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n#select features using threshold\nselection = SelectFromModel(model, threshold=thresh, prefit=True)\nselect_X_train = selection.transform(X_train)\n# eval model\nselect_X_val = selection.transform(X_test)\n# test \nselect_X_test = selection.transform(X_test)","4e1105f4":"select_X_train.shape","b76c9e45":"pipelines = []\nseed = 5\n\npipelines.append(\n                (\"Scaled_Ridge\", \n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Ridge\", Ridge(random_state=seed, alpha= 0.1, tol=0.1, solver='auto' ))]\n                 )))\n\npipelines.append(\n                (\"Scaled_Lasso\", \n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Lasso\", Lasso(random_state=seed, tol=0.1))]\n                 )))\n\npipelines.append(\n                (\"Scaled_Elastic\", \n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Lasso\", ElasticNet(random_state=seed, tol=0.1))]\n                 )))\n\npipelines.append(\n                (\"Scaled_RF_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"RF\", RandomForestRegressor(random_state=seed))]\n                 )))\n\npipelines.append(\n                (\"Scaled_ET_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"ET\", ExtraTreesRegressor(random_state=seed))]\n                 )))\n\npipelines.append(\n                (\"Scaled_BR_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"BR\", BaggingRegressor(random_state=seed))]\n                 ))) \n\npipelines.append(\n                (\"Scaled_Hub-Reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Hub-Reg\", HuberRegressor())]\n                 ))) \n\npipelines.append(\n                (\"Scaled_BayRidge\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"BR\", BayesianRidge())]\n                 ))) \n\npipelines.append(\n                (\"Scaled_XGB_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"XGBR\", XGBRegressor(seed=seed, n_estimators=300))]\n                 ))) \n\npipelines.append(\n                (\"Scaled_DT_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"DT_reg\", DecisionTreeRegressor())]\n                 ))) \n\npipelines.append(\n                (\"Scaled_SVR\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"SVR\",  SVR(kernel='linear', C=1e3, degree=2))]\n                 )))\n\npipelines.append(\n                (\"Scaled_KNN_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"KNN_reg\", KNeighborsRegressor())]\n                 )))\npipelines.append(\n                (\"Scaled_ADA-Reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"ADA-reg\", AdaBoostRegressor())\n                 ]))) \n\npipelines.append(\n                (\"Scaled_Gboost-Reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"GBoost-Reg\", GradientBoostingRegressor())]\n                 )))\n\npipelines.append(\n                (\"Scaled_RFR_PCA\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"PCA\", PCA(n_components=3)),\n                     (\"XGB\", RandomForestRegressor())]\n                 )))\n\npipelines.append(\n                (\"Scaled_XGBR_PCA\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"PCA\", PCA(n_components=3)),\n                     (\"XGB\", XGBRegressor())]\n                 )))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'r2'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in pipelines:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, select_X_train, y_train, cv= kfold,\n                                 scoring=scoring, n_jobs=1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn Name\", fontsize=20)\nax.set_ylabel(\"R Squared Score of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","114a8e64":"import featuretools as ft #importing the module\nfrom featuretools import variable_types as vtypes # importing vtypes to classify or categoricals","11e7f0fb":"es = ft.EntitySet(\"house_price\") #Creating new Entityset","40801bcd":"#Seting the categorical and ordinal variables\nhouse_variable_types = {\n    'BldgType': vtypes.Categorical, 'BsmtCond': vtypes.Categorical, 'BsmtExposure': vtypes.Categorical, \n    'BsmtFinType1': vtypes.Categorical, 'BsmtFinType2': vtypes.Categorical, 'BsmtQual': vtypes.Ordinal, \n    'CentralAir': vtypes.Categorical, 'Id':vtypes.Categorical, 'Exterior2nd': vtypes.Categorical, \n    'Condition1': vtypes.Categorical, 'Condition2': vtypes.Categorical, 'Electrical': vtypes.Categorical,\n    'ExterCond': vtypes.Categorical, 'ExterQual': vtypes.Ordinal, 'Exterior1st': vtypes.Categorical, \n    'Foundation': vtypes.Categorical, 'Functional': vtypes.Categorical, 'GarageCond': vtypes.Categorical, \n    'GarageFinish': vtypes.Categorical, 'GarageQual': vtypes.Ordinal, 'GarageType': vtypes.Categorical, \n    'Heating': vtypes.Categorical, 'HeatingQC': vtypes.Categorical, 'HouseStyle': vtypes.Categorical, \n    'LandContour': vtypes.Categorical, 'LandSlope': vtypes.Categorical, 'LotConfig': vtypes.Categorical, \n    'LotShape': vtypes.Categorical, 'MSZoning': vtypes.Categorical, 'MasVnrType': vtypes.Categorical, \n    'Neighborhood': vtypes.Categorical, 'PavedDrive': vtypes.Categorical,'RoofMatl': vtypes.Categorical,\n    'RoofStyle': vtypes.Categorical, 'SaleCondition': vtypes.Categorical, 'SaleType': vtypes.Categorical, \n    'Street': vtypes.Categorical, 'MiscFeature': vtypes.Categorical, 'KitchenQual': vtypes.Ordinal, \n    'Utilities': vtypes.Categorical, 'OverallQual': vtypes.Ordinal, 'PoolQC': vtypes.Categorical, \n    'Alley': vtypes.Categorical, 'FireplaceQu': vtypes.Categorical\n}","e19c5f24":"#Creating a new entity from our table (data) with Id and we will put the correct variable types\nes.entity_from_dataframe(entity_id=\"NewFeatures\",\n                         dataframe=data, index=\"Id\",\n                         variable_types=house_variable_types)","64b8d57d":"print(data.shape)","d1e0c0ca":"# Creating a new entity using the OverallQuality and the most correlated with our target variables\nes.normalize_entity('NewFeatures', 'Quality', 'OverallQual',\n                    additional_variables=['Neighborhood','GarageQual','SaleCondition',\n                                          'KitchenQual','HouseStyle', 'Condition1'],\n                    make_time_index=False)\n### Need I set the PriceSale in any moment? ","60eef6f9":"# es.add_interesting_values(max_values=3)","a83d0ac5":"feature_matrix, features = ft.dfs(entityset=es, \n                                  target_entity=\"NewFeatures\", \n                                  max_depth=2, verbose=True)","f5a47597":"feature_matrix.shape","07d6ea91":"#Let's drop some of outliers \nfeature_matrix = feature_matrix[feature_matrix['TotalSF'] < 6000]\nfeature_matrix = feature_matrix[feature_matrix['TotalBsmtSF'] < 4000]","d0fcd9c4":"feature_matrix.shape","70c6b982":"feature_matrix = feature_matrix.reset_index() ## I am reseting to try fix  error\nfeature_matrix = feature_matrix.fillna(-999) ## filling NA's \n\ndf_train = feature_matrix[feature_matrix['set'] == 1].copy() # spliting the data into df train\ndf_train = df_train[feature_matrix['SalePrice'] < 700000] # EXcluding some outliers \n\ndf_test = feature_matrix[feature_matrix['set'] == 0].copy() # spliting the data into df test\n\n#Deleting some inutil features (SalePrice in df_test was just to better handle with the full dataset)\ndel df_test['SalePrice']\ndel df_train['set']\ndel df_test['set']","4a2f4ccb":"## Why I got back NaN and\/or inifite values? ","4afd6e6d":"X_train = df_train.drop(['SalePrice','Id'], axis=1).values\ny_train = df_train['SalePrice'].values\nX_test  = df_test.drop(['Id'], axis=1).values\n\nprint(X_train.shape, y_train.shape, X_test.shape)","73a57057":"thresh = 5 * 10**(-4)\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n#select features using threshold\nselection = SelectFromModel(model, threshold=thresh, prefit=True)\nselect_X_train = selection.transform(X_train)\n# eval model\nselect_X_val = selection.transform(X_test)\n# test \nselect_X_test = selection.transform(X_test)","446e4fdb":"print(select_X_train.shape)","dd5dbd66":"pipelines = []\nseed = 5\n\npipelines.append(\n                (\"Scaled_Ridge\", \n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Ridge\", Ridge(random_state=seed, tol=1 ))]\n                 )))\n\npipelines.append(\n                (\"Scaled_Lasso\", \n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Lasso\", Lasso(random_state=seed, tol=0.1))]\n                 )))\n\npipelines.append(\n                (\"Scaled_Elastic\", \n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Lasso\", ElasticNet(random_state=seed, tol=0.1))]\n                 )))\n\npipelines.append(\n                (\"Scaled_SVR\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"SVR\",  SVR(kernel='linear', C=1e2, degree=5))]\n                 )))\n\npipelines.append(\n                (\"Scaled_RF_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"RF\", RandomForestRegressor(random_state=seed))]\n                 )))\n\npipelines.append(\n                (\"Scaled_ET_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"ET\", ExtraTreesRegressor(random_state=seed))]\n                 )))\n\npipelines.append(\n                (\"Scaled_BR_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"BR\", BaggingRegressor(random_state=seed))]\n                 ))) \n\npipelines.append(\n                (\"Scaled_Hub-Reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"Hub-Reg\", HuberRegressor())]\n                 ))) \n\npipelines.append(\n                (\"Scaled_BayRidge\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"BR\", BayesianRidge())]\n                 ))) \n\npipelines.append(\n                (\"Scaled_XGB_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"XGBR\", XGBRegressor(seed=seed))]\n                 ))) \n\npipelines.append(\n                (\"Scaled_DT_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"DT_reg\", DecisionTreeRegressor())]\n                 ))) \n\npipelines.append(\n                (\"Scaled_KNN_reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"KNN_reg\", KNeighborsRegressor())]\n                 )))\npipelines.append(\n                (\"Scaled_ADA-Reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"ADA-reg\", AdaBoostRegressor())\n                 ]))) \n\npipelines.append(\n                (\"Scaled_Gboost-Reg\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"GBoost-Reg\", GradientBoostingRegressor())]\n                 )))\n\npipelines.append(\n                (\"Scaled_RFR_PCA\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"PCA\", PCA(n_components=3)),\n                     (\"XGB\", RandomForestRegressor())]\n                 )))\n\npipelines.append(\n                (\"Scaled_XGBR_PCA\",\n                 Pipeline([\n                     (\"Scaler\", StandardScaler()),\n                     (\"PCA\", PCA(n_components=3)),\n                     (\"XGB\", XGBRegressor())]\n                 )))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'r2'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in pipelines:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, select_X_train, y_train, cv= kfold,\n                                 scoring=scoring)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn Name\", fontsize=20)\nax.set_ylabel(\"R Squared Score of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","649ba404":"# Importing the necessary library\nfrom tpot import TPOTRegressor","64a39fa3":"## It's a implementation of some customized models to do in future\ntpot_config = {\n    'sklearn.ensemble.GradientBoostingRegressor': {\n        ''\n    },\n    'xgboost.XGBRegressor': {\n        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n        'fit_prior': [True, False]\n    },\n    'sklearn.naive_bayes.MultinomialNB': {\n        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n        'fit_prior': [True, False]\n    }\n}\n","031b45b0":"# We will create our TPOT regressor with commonly used arguments\ntpot = TPOTRegressor(verbosity=2, scoring='r2', cv=3, \n                      n_jobs=-1, generations=6, config_dict='TPOT light',\n                      population_size=50, random_state=3,\n                      early_stop = 5)","5e5a8429":"# Fitting the auto ML model","70dbfcb3":"#fiting our tpot auto model\ntpot.fit(select_X_train, y_train)","74c2e123":"### Ok, now let's set our X and y values ","d847df5b":"Cool ! I have impelemented my first featuretools solution.\n\nWe can see a improvement in the \"naked\" models predictions. \n\nNow, let's try put it together to autoamted ML library TPOT ","e886a79b":"Knowing the type of our data","0efc2100":"## Now let's plot the categoricals and see the correlation by our target feature ","d9aba986":"Very cool and easy to implement library!!! \n\nNow, let's create some predictions to submite on the competition","5fc1fb9b":"Wow!! Excellent results. \n\nI will implement some of this models to find the best prediction to this competition; \n","4da5d548":"I will due with Nan's later, but by now I will fill with \"miss\"","0a2a5bd5":"> Very cool! We can see that some variables have influence on the SalePrice and the OverAllQuality seems the highest influence.","9256b882":"> importing necessary librarys","e70b0002":"### Seting the type of some categorical features","797ccabf":"## I will start exploring the categorical (object) variables","ecbd5222":"### Creating a normalized entity to cross throught our main interest table","3195f3c1":"## Importing the librarys","48f46b8f":"## Testing the \"Featuretools\" library to auto feature engineering","5513eecb":"<h1>Welcome to my Kernel<\/h1>\n\n### I am learning about some automated tools to Machine Learning and I will try to implement some of them on this  Kernel.\n<br> <i>*English is not my first language, sorry about any error<\/i>\n<h1>Overview<\/h1>\nThere are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.<br>\n<br>\nQuantitative: <i>1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold<\/i><br>\n<br>\n\n\n\n\nQualitative: <i>Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,<\/i>\n","cffe0661":"# Someone can clearly explain what are happen when I create the relationships? Also, when I run the DFS... What really happens? \n\nHow can I set some good practices to better feature engineering? Might can I explicitly set my target? Where I use my target ? ","37ab317c":"### Adding some interesting values ","e09ad2bb":"# IMPLEMENTING TPOT\n<b>TPOT<\/b> is a Tree-Based Pipeline Optimization Tool (TPOT) is using genetic programming to find the best performing ML pipelines, and it is built on top of scikit-learn.\n\nOnce your dataset is cleaned and ready to be used, TPOT will help you with the\nfollowing steps of your ML pipeline:\n- Feature preprocessing\n- Feature construction and selection\n- Model selection\n- Hyperparameter optimization\n","e3cd0f58":"<b>If you like my Kernel, please give me your feedback and votes up =)  <\/b>","9809b45f":"### Entities and EntitySets\nThe first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes.","ca1b76f5":"### Looking for missing values","3c5bb164":"> Very interesting. Let's plot all this values by our target value","0b95824c":"# Stay tuned because I will continue improving this models and implementing more details about automated librarys ","910034f0":"### When we invoke fit method, TPOT will create generations of populations, seeking best","74777ff7":"### Creating a new entity Id inside the created EntitySet","e44e4316":"\n<h2>I will do some exploration trough  the House Prices, prerpocessing, modeling, set the feature engineering and TPOT model. <h2>","3f415767":"## Nice, now, lets import the librarys and build the model pipeline to find the best model to our problem","df524f9b":"<b>TPOT <\/b>is very user-friendly as it's similar to using scikit-learn's API:","8518595e":"The diference of depth is that ","ce24cde7":"> Very interesting. We can see that almost all variables have high concentration in 1, 2 or 3 values.","62be8d1d":"Nice, now let's explore our features;","d8987ea9":"## Now let's use the selector in the new features"}}