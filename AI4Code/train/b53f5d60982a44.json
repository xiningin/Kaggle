{"cell_type":{"8e0c11c9":"code","635d99b8":"code","e5f5d873":"code","807f3bf6":"code","511ac4eb":"code","498faafe":"code","04735d68":"code","901e00a5":"code","3b9cc45f":"code","241fee7b":"code","b9568eff":"code","1b8f8321":"code","5dacb8a0":"code","1cf571b2":"code","900cc8ab":"code","97b43959":"code","38283496":"code","70c64688":"markdown","a6a8f60f":"markdown","854b4859":"markdown","42c77c07":"markdown","cd59c043":"markdown","c6ff6425":"markdown","05e432c5":"markdown","c56a1eca":"markdown","217a20bb":"markdown","7a15278a":"markdown","d1da5e61":"markdown","0f82b16a":"markdown","93342a90":"markdown","17554130":"markdown"},"source":{"8e0c11c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","635d99b8":"import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nX = 2 * np.random.rand(100, 1)\ny = 1 + 5 * X + np.random.randn(100, 1)","e5f5d873":"plt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.axis([0, 2, 0, 15])","807f3bf6":"X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n# Linlag is the linera algebra Library\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\ntheta_best","511ac4eb":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_","498faafe":"def plot_Lerning_rate(eta,ite):\n    n_iterations = 1000\n    m = 100\n    acc=np.empty(1000)\n    nc=np.arange(1,1001,1)\n    i=0\n    theta = np.random.randn(2,1)  # random initialization\n\n    for iteration in range(n_iterations):\n        gradients = 2\/m * X_b.T.dot(X_b.dot(theta) - y)\n        theta = theta - eta * gradients\n        acc[i]=theta[1]\n        i = i + 1\n    x=pd.Series(acc[1:ite],index=nc[1:ite])\n    x.plot()\n    # Add title and axis names\n    plt.title('Learning Growth Eta ' + str(eta))\n    plt.xlabel('Iteration')\n    plt.ylabel('Slope')\n    plt.show() ","04735d68":"plot_Lerning_rate(0.001,100)","901e00a5":"plot_Lerning_rate(0.01,100)","3b9cc45f":"plot_Lerning_rate(0.1,100)","241fee7b":"plot_Lerning_rate(0.5,100)","b9568eff":"theta_path_sgd = []\nm = len(X_b)\nnp.random.seed(42)\nn_epochs = 50\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return t0 \/ (t + t1)\n\ntheta = np.random.randn(2,1)  # random initialization\n\nfor epoch in range(n_epochs):\n    for i in range(m):\n        if epoch == 0 and i < 20:                    \n            y_predict = X_new_b.dot(theta)           \n            style = \"b-\" if i > 0 else \"r--\"         \n            plt.plot(X_new, y_predict, style)        \n        random_index = np.random.randint(m)\n        xi = X_b[random_index:random_index+1]\n        yi = y[random_index:random_index+1]\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n        eta = learning_schedule(epoch * m + i)\n        theta = theta - eta * gradients\n        theta_path_sgd.append(theta)                \n\nplt.plot(X, y, \"b.\")                                \nplt.xlabel(\"$x_1$\", fontsize=18)                    \nplt.ylabel(\"$y$\", rotation=0, fontsize=18)          \nplt.axis([0, 2, 0, 15])                              \nplt.show()                                     ","1b8f8321":"%matplotlib inline\n\nnp.random.seed(12)\nnum_observations = 500\n\n# Creating Variables for two classes\n\n# Takes mean and covariance\nx1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\nx2 = np.random.multivariate_normal([1, 4], [[1, .6],[.6, 1]], num_observations)\n\n\nsimulated_labels = np.hstack((np.zeros(num_observations),\n                              np.ones(num_observations)))\nsimulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)","5dacb8a0":"def sigmoid(scores):\n    return 1 \/ (1 + np.exp(-scores))","1cf571b2":"def log_likelihood(features, target, weights):\n    scores = np.dot(features, weights)\n    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n    return ll","900cc8ab":"def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n    if add_intercept:\n        intercept = np.ones((features.shape[0], 1))\n        features = np.hstack((intercept, features))\n        \n    weights = np.zeros(features.shape[1])\n    \n    for step in range(num_steps):\n        scores = np.dot(features, weights)\n        predictions = sigmoid(scores)\n\n        # Update weights with gradient\n        output_error_signal = target - predictions\n        gradient = np.dot(features.T, output_error_signal)\n        weights += learning_rate * gradient\n        \n    return weights","97b43959":"weights = logistic_regression(simulated_separableish_features, simulated_labels,\n                     num_steps = 200000, learning_rate = 0.01, add_intercept=True)\nprint(weights)","38283496":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(fit_intercept=True, C = 0.01)\nclf.fit(simulated_separableish_features, simulated_labels)\n\nprint(clf.intercept_, clf.coef_)","70c64688":"c# Understanding Learning Rate","a6a8f60f":"# Linear Regession by normal equation","854b4859":"# Logistic Regression <a id=\"1.3\"><\/a>","42c77c07":"![image.png](attachment:image.png)","cd59c043":"# Stochastic Gradient Descent","c6ff6425":"# Logistic from sklearn","05e432c5":"  \n# Contents\n\n* [<font size=4>Getting Started<\/font>](#1)\n    * [House Keeping](#1.1)\n    * [Linear Regression](#1.2)\n    * [Logistic Regression](#1.3) ","c56a1eca":"# Using the sklearn model","217a20bb":"![image.png](attachment:image.png)","7a15278a":"> # House Keeping <a id=\"1\"><\/a>\n> Here we describe importing the library, impoting the datset and some basic checks on the dataset","d1da5e61":"![image.png](attachment:image.png)","0f82b16a":"# Linear Regression <a id=\"1.2\"><\/a>","93342a90":"![image.png](attachment:image.png)","17554130":"![image.png](attachment:image.png)"}}