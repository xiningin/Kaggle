{"cell_type":{"b82221bc":"code","a78828ec":"code","9a901fe3":"code","338b65e5":"code","395277b8":"code","e40e059f":"code","d2d03f18":"code","1a9141af":"code","e5e1caea":"code","f4ba0e1d":"code","fd380978":"code","690630b0":"markdown","c3c8b878":"markdown","997e89d3":"markdown","e73f6002":"markdown","020c4b9c":"markdown","b69521ff":"markdown","1fd79aaa":"markdown"},"source":{"b82221bc":"import os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\n\nfrom collections import Counter","a78828ec":"root = \"\/kaggle\/input\/digit-recognizer\"\n# 42000 train 784 pixels + 1 label (take a while to load)\ntrain_data = np.loadtxt(os.path.join(root,\"train.csv\"),delimiter=\",\",skiprows=1)\n# 28000 test 784 pixels\ntest_data = np.loadtxt(os.path.join(root,\"test.csv\"),delimiter=\",\",skiprows=1)","9a901fe3":"# check gpu information\n!nvidia-smi","338b65e5":"class Dataset:\n    \"\"\"\n    build a map-style dataset\n    \"\"\"\n    def __init__(self,data,targets,transform=None):\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self,idx):\n        if self.transform == None:\n            return self.data[idx],self.targets[idx]\n        else:\n            return self.transform(self.data[idx]),self.targets[idx]","395277b8":"transform = transforms.Compose([transforms.ToPILImage(),\n                                transforms.RandomResizedCrop(size=28,scale=(0.9,1.0),ratio=(0.9,1,15)),\n                                transforms.RandomAffine(degrees=12,translate=(1\/7,1\/7),shear=12),\n                                transforms.RandomRotation(degrees=12),\n                                transforms.ToTensor()])\nx_train = train_data[:,1:].reshape(-1,28,28).astype(np.uint8)\ny_train = torch.LongTensor(train_data[:,0])\ntrain_dataset = Dataset(x_train,y_train,transform)","e40e059f":"trainloader = DataLoader(train_dataset,batch_size=512,shuffle=True,pin_memory=True)\n%matplotlib inline\nx,y = next(iter(trainloader))\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1).axis(\"off\")\n    plt.imshow(x[i].squeeze(0))\n    plt.title(str(y[i].item()))","d2d03f18":"# define a cnn classifier\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.layer1 = self.get_conv_block(1,64)\n        self.layer2 = self.get_conv_block(64,128,paddings=(0,1))\n        self.layer3 = self.get_conv_block(128,256)\n        self.fc1 = nn.Linear(256*3*3,2048)\n        self.fc2 = nn.Linear(2048,512)\n        self.fc3 = nn.Linear(512,10)\n    def forward(self,x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x).flatten(start_dim=1)\n        x = F.dropout(F.relu(self.fc1(x)),0.7,training=self.training)\n        out = self.fc3(F.relu(self.fc2(x)))\n        return out\n    def get_conv_block(self,in_chan,out_chan,strides=(1,1),paddings=(1,1)):\n        return nn.Sequential(\n            nn.Conv2d(in_chan,out_chan,3,strides[0],paddings[0]),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_chan),\n            nn.Conv2d(out_chan,out_chan,3,strides[0],paddings[1]),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_chan),\n            nn.MaxPool2d(2)\n        )","1a9141af":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nrandom.seed(1234)\nnp.random.seed(1234)\ntorch.random.manual_seed(1234)\n\nepochs = 80\nbatch_size = 512\n\ntrainloader = DataLoader(train_dataset,batch_size,shuffle=True,pin_memory=True)\n\ndef train(dataloader,net,optimizer,loss_fn,epochs=50):\n    for n in range(epochs):\n        with tqdm(dataloader,desc=f\"{n+1}\/{epochs} epochs\") as t:\n            running_loss = 0.0\n            running_correct = 0\n            running_total = 0\n            for x,y in t:\n                out = net(x.to(device))\n                pred = out.max(dim=1)[1]\n                loss = loss_fn(out,y.to(device))\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n                running_loss += loss.item()*x.size(0)\n                running_correct += (pred==y.to(device)).sum().item()\n                running_total += x.size(0)\n                t.set_postfix({\"train_loss\":running_loss\/running_total,\"train_acc\":running_correct\/running_total})\n\n# train 5 models independently (ensemble)\nmodels = []\nfor i in range(5):\n    print(\"Training {} model\".format(str(i+1)+[\"st\",\"nd\",\"rd\",\"th\",\"th\"][i]))\n    cnn = CNN()\n    cnn.to(device)\n    opt = Adam(cnn.parameters(),lr=1e-4)\n    # opt = SGD(cnn.parameters(),lr=1e-5,momentum=0.9)\n    loss_fn = CrossEntropyLoss()\n    train(trainloader,cnn,opt,loss_fn,epochs)\n    models.append(cnn)","e5e1caea":"# switch to eval mode\ncnn.eval()\n\n# disable transformation\ntrain_dataset.transform = transforms.ToTensor()\ntrainloader = DataLoader(train_dataset,batch_size=512,shuffle=False,pin_memory=True)\n\ntrain_preds = []\nfor cnn in models:\n    train_pred = []\n    with torch.no_grad():\n        for x,_ in trainloader:\n            out = cnn(x.to(device))\n            pred = out.max(dim=1)[1]\n            train_pred.append(pred.detach().cpu().numpy())\n    train_preds.append(np.concatenate(train_pred))\ntrain_preds = list(zip(*train_preds))\ntrain_pred = np.array(list(map(lambda x: Counter(x).most_common(1)[0][0],train_preds)))\ntrain_acc = (train_pred == y_train.numpy()).astype(\"float\").mean()\nprint(\"The training accurary is {}\".format(train_acc))","f4ba0e1d":"# prediction\nx_test = test_data.reshape(-1,1,28,28) # pytorch channel first\nx_test = torch.Tensor(x_test)\/255.\ntest_preds = []\nfor cnn in models:\n    test_pred = []\n    with torch.no_grad():\n        for i in range(0,len(x_test),batch_size):\n            out = cnn(x_test[i:i+batch_size].to(device))\n            pred = out.max(dim=1)[1]\n            test_pred.append(pred.detach().cpu().numpy())\n    test_preds.append(np.concatenate(test_pred))\ntest_preds = list(zip(*test_preds))\ntest_pred = list(map(lambda x: Counter(x).most_common(1)[0][0],test_preds))","fd380978":"# store predictions\nimport pandas as pd\nimageid = pd.Series(np.arange(len(test_pred)))+1\ndf = pd.DataFrame({\"ImageId\":imageid,\"Label\":test_pred})\ndf.set_index(\"ImageId\")\ndf.to_csv(\"\/kaggle\/working\/test_pred.csv\",index=False)","690630b0":"# Evaluation on the Training Set","c3c8b878":"# Define a CNN Class","997e89d3":"# Import Modules","e73f6002":"# Train the NN","020c4b9c":"# Prediction","b69521ff":"# Load Data","1fd79aaa":"# Data Augmentation"}}