{"cell_type":{"1bccbca9":"code","92789e01":"code","bb5ba8cc":"code","eefd498d":"code","1692e61e":"code","e39030b1":"code","3dbc2447":"code","c8c0c0c1":"code","ccecb43d":"code","2f5be3bd":"code","d828124c":"code","461ca9de":"code","28e30281":"code","371b7677":"code","4d959f88":"markdown","45a67706":"markdown","bf95aaba":"markdown","a2400fca":"markdown","227d59c9":"markdown","1df271bc":"markdown","b4ff32ad":"markdown"},"source":{"1bccbca9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","92789e01":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nprint(tf.__version__)\nimport glob\nimport matplotlib.pyplot as plt\n%matplotlib inline","bb5ba8cc":"imgs_path = glob.glob(r\"..\/input\/anime-sketch-colorization-pair\/data\/train\/*.png\")\nprint(len(imgs_path))\nplt.imshow(tf.keras.preprocessing.image.load_img(imgs_path[3323]))","eefd498d":"def read_jpg(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img\n\ndef normalize(mask, img):\n    # \u5f52\u4e00\u5316\n    mask = tf.cast(mask, tf.float32) \/ 127.5 - 1\n    img = tf.cast(img, tf.float32) \/ 127.5 - 1\n    return mask, img\n\ndef load_image(image_path):\n    img = read_jpg(image_path)\n    w = tf.shape(img)[1]\n    w = w\/\/2\n        # \u6574\u9664\n    input_mask = img[:, w:, :]\n        # \u5bf9\u6570\u636e\u8fdb\u884c\u5207\u7247\n    input_img = img[:, :w, :]\n    \n    input_mask = tf.image.resize(input_mask, (64, 64))\n    input_img = tf.image.resize(input_img, (64, 64))\n        # \u4f7f\u56fe\u7247\u5c3a\u5bf8\u53ef\u663e\u793a\uff0c\u786e\u4fdd\u8f93\u5165\u90fd\u4e3a(64, 64)\n    \n    # \u6570\u636e\u589e\u5f3a\uff1a\u5de6\u53f3\u7ffb\u8f6c\uff08\u9700\u6210\u5bf9\uff09\n    if tf.random.uniform(()) > 0.5:\n        # \u4ea7\u751f(0,1)\u7684\u6570\u636e\n        input_img = tf.image.flip_left_right(input_img)\n        input_mask = tf.image.flip_left_right(input_mask)\n        \n    input_mask, input_img = normalize(input_mask, input_img)\n    return input_mask, input_img","1692e61e":"dataset = tf.data.Dataset.from_tensor_slices(imgs_path)\ndataset = dataset.map(load_image)\n    # \u52a0\u8f7d\u8f6c\u6362\u56fe\u50cf\nprint(dataset)\n\nBATCH_SIZE = 1\n    # \u8bfb\u5165\u663e\u5b58\u7684\u6570\u636e\u91cf\nBUFFER_SIZE = len(imgs_path)\n    # \u6570\u636e\u5168\u90e8\u8bfb\u5165\u5185\u5b58\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    # GPU\u8bad\u7ec3\u65f6\uff0cCPU\u540c\u65f6\u52a0\u8f7d\uff0c\u53c2\u6570\u9009\u81ea\u52a8 \n    \nfor musk, img in dataset.take(1):\n    plt.subplot(1, 2, 1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(musk[0]))\n        # \u628a\u5f20\u91cf\u8f6c\u6362\u6210\u56fe\u50cf\n    plt.subplot(1, 2, 2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))","e39030b1":"imgs_test = glob.glob(r\"..\/input\/anime-sketch-colorization-pair\/data\/val\/*.png\")\ndataset_test = tf.data.Dataset.from_tensor_slices(imgs_test)\n\ndef load_image_test(image_path):\n    img = read_jpg(image_path)\n    w = tf.shape(img)[1]\n    w = w\/\/2\n        # \u6574\u9664\n    input_mask = img[:, w:, :]\n        # \u5bf9\u6570\u636e\u8fdb\u884c\u5207\u7247\n    input_img = img[:, :w, :]\n    \n    input_mask = tf.image.resize(input_mask, (64, 64))\n    input_img = tf.image.resize(input_img, (64, 64))\n        # \u4f7f\u56fe\u7247\u5c3a\u5bf8\u53ef\u663e\u793a\uff0c \u786e\u4fdd\u8f93\u5165\u90fd\u4e3a(64, 64)\n        \n    input_mask, input_img = normalize(input_mask, input_img)\n    return input_mask, input_img\n\ndataset_test = dataset_test.map(load_image_test)\ndataset_test = dataset_test.batch(BATCH_SIZE)\n\nfor musk, img in dataset_test.take(1):\n    plt.subplot(1, 2, 1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(musk[0]))\n        # \u628a\u5f20\u91cf\u8f6c\u6362\u6210\u56fe\u50cf\n    plt.subplot(1, 2, 2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))","3dbc2447":"# \u521b\u5efa\u4e0b\u91c7\u6837\u6a21\u578b\ndef down(filters, size, apply_bn=True):\n    model = keras.Sequential()\n    model.add(\n        layers.Conv2D(filters, size, strides=2, padding=\"same\", use_bias=False)\n    )\n    \n    if apply_bn:\n        model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    return model\n\n# \u521b\u5efa\u4e0a\u91c7\u6837\u6a21\u578b\ndef upsample(filters, size, apply_drop=False):\n    model = keras.Sequential()\n    model.add(\n        layers.Conv2DTranspose(filters, size, strides=2, padding=\"same\", use_bias=False)\n    )\n    \n    model.add(layers.BatchNormalization())\n    \n    if apply_drop:\n        model.add(layers.Dropout(0.5))\n        \n    model.add(layers.ReLU())\n    return model","c8c0c0c1":"def Generator():\n    inputs = layers.Input(shape=(64, 64, 3))\n    \n    down_stack = [\n        down(32, 3, apply_bn=False),\n            # 32*32*32\n        down(64, 3),\n            # 16*16*64\n        down(128, 3),\n            # 8*8*128\n        down(256, 3),\n            # 4*4*256\n        down(512, 3),\n            # 2*2*512\n        down(512, 3),\n            # 1*1*512\n    ]\n    \n    up_stack = [\n        upsample(512, 3, apply_drop=True),\n            # 2*2*1024\n        upsample(256, 3, apply_drop=True),\n            # 4*4*512\n        upsample(128, 3, apply_drop=True),\n            # 8*8*256\n        upsample(64, 3),\n            # 16*16*128\n        upsample(32, 3),\n            # 32*32*64\n    ]\n    \n    x = inputs\n    skips = []\n    for down_ in down_stack:\n        x = down_(x)\n        skips.append(x)\n            # \u4e0b\u91c7\u6837\u7ed3\u675f\n        \n    # \u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e0d\u65ad\u4e0e\u4e0b\u91c7\u6837\u7684\u5c42\u5408\u5e76\n    skips = reversed(skips[:-1])\n        # \u53bb\u9664\u6700\u540e\u8f93\u51fa\u503c\u5728\u8fdb\u884c\u7ffb\u8f6c\n    for up_, skip in zip(up_stack, skips):\n        x = up_(x)\n        x = layers.Concatenate()([x, skip])\n            # x\u4e3a128*128*128\n    x = layers.Conv2DTranspose(3, 4, strides=2, padding=\"same\", activation=\"tanh\")(x)\n    \n    return keras.Model(inputs=inputs, outputs=x)\n\ngenerator = Generator()\n# keras.utils.plot_model(generator, show_shapes=True)\n    # \u7ed8\u5236\u751f\u6210\u5668","ccecb43d":"def Discrimator():\n    inputs = layers.Input(shape=(64,64,3))\n    targets = layers.Input(shape=(64,64,3))\n    \n    x = layers.concatenate([inputs, targets])\n        # \u4ee5\u53c2\u6570\u5f62\u5f0f\u8981\u5c0f\u5199\u9996\u5b57\u6bcd\n        # 256*256*6\n        \n    x = down(32, 3, apply_bn=False)(x)\n        # 32*32*32\n    x = down(64, 3)(x)\n        # 16*16*64   \n    x = down(128, 3)(x)\n        # 8*8*128\n        \n    x = layers.Conv2D(256, 3, strides=1, padding=\"same\", use_bias=False)(x)\n        # 31*31*512\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU()(x)\n    \n    x = layers.Conv2D(1, 3, strides=1)(x)\n        # 30*30*1\n        \n    return keras.Model(inputs=[inputs, targets], outputs=x)\n\ndiscrimator = Discrimator()\n# keras.utils.plot_model(discrimator, show_shapes=True)","2f5be3bd":"loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n    # \u8f93\u5165\u6fc0\u6d3b\u7684\u6570\u636e\n    \ndef gen_loss(d_gen_output, gen_out, target):\n    gen_loss = loss_fn(tf.ones_like(d_gen_output), d_gen_output)\n    \n    l1_loss = tf.reduce_mean(tf.abs(target - gen_out))\n    \n    return gen_loss + l1_loss * 10\n\n\ndef disc_loss(d_real_output, d_gen_output):\n    real_loss = loss_fn(tf.ones_like(d_real_output), d_real_output)\n    g_loss = loss_fn(tf.zeros_like(d_gen_output), d_gen_output)\n    \n    return real_loss + g_loss","d828124c":"# \u8bbe\u7f6e\u4f18\u5316\u5668\ngenerator_opt = keras.optimizers.Adam(2e-4, beta_1=0.5)\n    # \u5b66\u4e60\u7387\u548c\u4e00\u9636\u77e9\u4f30\u8ba1\ndiscrimator_opt = keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n# \u53ef\u89c6\u5316\ndef generate_image(image, test_input, tar):\n    prediction = generator(test_input, training=True)\n        # True\u65f6\u4e3a\u8bad\u7ec3\u6a21\u5f0f\uff0c\u5f71\u54cd\u6279\u91cf\u5f52\u4e00\u5316\u5c42\n    plt.figure(figsize=(15,15))\n    \n    display_list = [test_input[0], tar[0], prediction[0]]\n        # \u8f93\u5165\u4e00\u4e2a\u6279\u6b21\u6570\u636e\uff0c\u9009\u53d6\u7b2c0\u4e2a\n    title = [\"Input Image\", \"Ground Truth\", \"Predicted Image\"]\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n            # \u5c06\u5f52\u4e00\u5316\u6570\u636e\u8fd8\u539f\u56de\u6765\n        plt.axis(\"off\")\n    plt.show()","461ca9de":"EPOCHS =  60\n\n@tf.function\ndef train_step(input_image, target, epoch):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n        \n        disc_real_output = discrimator([input_image, target], training=True)\n        disc_generated_output = discrimator([input_image, gen_output], training=True)\n        \n        gen_total_loss = gen_loss(disc_generated_output, gen_output, target)\n        d_loss = disc_loss(disc_real_output, disc_generated_output)\n        \n    generator_grad = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n    discrimator_grad = disc_tape.gradient(d_loss, discrimator.trainable_variables)\n    \n    generator_opt.apply_gradients(zip(generator_grad, generator.trainable_variables))\n    discrimator_opt.apply_gradients(zip(discrimator_grad, discrimator.trainable_variables))","28e30281":"def fit(train_ds, epochs, test_ds):\n    for epoch in range(epochs + 1):\n        if epoch % 3 == 0:\n            for example_input, example_target in test_ds.take(1):\n                generate_image(generator, example_input, example_target)\n        print(\"Epoch:\", epoch)\n        \n        for n, (input_image, target) in train_ds.enumerate():\n            train_step(input_image, target, epoch)\n        print(\"Waiting...\")","371b7677":"fit(dataset, EPOCHS, dataset_test)","4d959f88":"\u521b\u5efa\u6570\u636e\u96c6","45a67706":"\u8bad\u7ec3","bf95aaba":"\u5224\u522b\u5668\u8f93\u5165\u6210\u5bf9\u7684\u56fe\u50cf","a2400fca":"\u751f\u6210\u5668\uff08\u57fa\u4e8eUnet\uff09","227d59c9":"\u52a0\u8f7d\u56fe\u50cf\u51fd\u6570","1df271bc":"\u5f00\u59cb\u62df\u5408","b4ff32ad":"\u635f\u5931\u51fd\u6570"}}