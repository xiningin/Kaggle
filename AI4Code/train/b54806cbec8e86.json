{"cell_type":{"87db4e88":"code","b1bae3ee":"code","21cec211":"code","a8778171":"code","c3e22484":"code","bd5146d7":"code","ccbee8ff":"code","e6b93462":"code","33266e23":"code","e8923860":"code","28bfa851":"code","f2628c44":"code","ba139349":"code","6bd0b8e1":"code","260c10d0":"markdown","792fbd9b":"markdown","ffa2acb8":"markdown","cc93e797":"markdown","c3cd1a3b":"markdown","1fcf69c4":"markdown"},"source":{"87db4e88":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","b1bae3ee":"data = pd.read_csv('..\/input\/star-dataset\/6 class csv.csv')","21cec211":"data","a8778171":"data.info()","c3e22484":"data['Star color'].unique()","bd5146d7":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","ccbee8ff":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Fix color values\n    color_mapping = {\n        'white': 'White',\n        'Blue ': 'Blue',\n        'Blue white': 'Blue White',\n        'Blue-white': 'Blue White',\n        'Blue white ': 'Blue White',\n        'Blue-White': 'Blue White',\n        'yellow-white':'Yellowish White',\n        'White-Yellow':'Yellowish White',\n        'yellowish': 'Yellowish'\n    }\n    df['Star color'] = df['Star color'].replace(color_mapping)\n    \n    # One-hot encode\n    df = onehot_encode(df, column='Star color', prefix=\"Color\")\n    df = onehot_encode(df, column='Spectral Class', prefix=\"Class\")\n    \n    # Split df into X and y\n    y = df['Star type']\n    X = df.drop('Star type', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","e6b93462":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","33266e23":"X_train","e8923860":"y_train","28bfa851":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nprint(\"Test Set Accuracy: {:.2f}%\".format(model.score(X_test, y_test) * 100))","f2628c44":"kf = KFold(n_splits=5)\n\nprint(\"Split Indices\")\n\nfor i, (train_idx, test_idx) in enumerate(kf.split(X_train)):\n    print(f\"\\nSplit {i + 1}:\\n--------\")\n    print(\"\\nTrain:\\n\" + str(train_idx))\n    print(\"\\nTest:\\n\" + str(test_idx) + \"\\n\")","ba139349":"results = []\n\nfor train_idx, test_idx in kf.split(X_train):\n    train_set = (X_train.iloc[train_idx, :], y_train.iloc[train_idx])\n    test_set = (X_train.iloc[test_idx, :], y_train.iloc[test_idx])\n    \n    model = LogisticRegression()\n    model.fit(train_set[0], train_set[1])\n    results.append(model.score(test_set[0], test_set[1]))\n\nprint(\"K-Fold Accuracies:\")\nfor i, result in enumerate(results):\n    print(\"Model {}: {:.2f}%\".format(i + 1, result * 100))","6bd0b8e1":"print(\"Average K-Fold Accuracy: {:.2f}%\".format(np.array(results).mean() * 100))","260c10d0":"# Preprocessing\n","792fbd9b":"# Getting Started","ffa2acb8":"# Test Set Evaluation","cc93e797":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/75pwmIAzxKs","c3cd1a3b":"# K-Fold Evaluation","1fcf69c4":"# Task for Today  \n\n***\n\n## Star Type Prediction\n\nGiven *data about stars*, let's try to predict the **type** of a given star.\n\nWe will use a logistic regression model to make our predictions and evaluate the model performance using both test set evaluation and K-fold evaluation."}}