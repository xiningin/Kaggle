{"cell_type":{"79405273":"code","0adae29b":"code","046523bb":"code","f94cec24":"code","ee8f6e41":"code","518fe5be":"code","5ed91718":"code","c3490321":"code","cd1de2dd":"code","ae82ad42":"code","0c5c011d":"code","6031874a":"code","1fc69467":"code","014e8fa2":"code","2c5d0a6f":"code","d3be783e":"code","08be2495":"code","2bf756e0":"code","a7254263":"code","1a05c73c":"code","162256de":"code","351067bd":"code","4add05d5":"code","0be1d37d":"code","f04b1b03":"code","55c1fef4":"code","a6da3c6e":"code","d98e1ea1":"code","af095bb4":"code","e9e7b14e":"code","2b7d5f71":"code","82d38f19":"code","fd184e90":"code","5ddd02cc":"code","2d0982e9":"code","a3ce5ca4":"code","37b2d7bc":"code","096bd07b":"code","2adaa1ae":"code","50a18676":"code","f21b4b51":"code","fd86a729":"code","17c51452":"code","39310ec3":"code","ca43e999":"code","09a19d54":"code","69e2e554":"code","45a7a216":"code","52e9416c":"code","0b707846":"code","972ee29e":"code","7b4046f4":"code","3f5698d9":"code","8b8e4808":"code","f6c20e94":"code","f0a29a14":"code","8232b6cc":"code","9beb4dc0":"code","d5a25f4f":"code","0c368fe8":"code","99cad9d0":"code","cc29950a":"code","9091f090":"code","3df819cb":"code","bcef3636":"code","59154aba":"code","6ed6ad08":"code","0a1e772d":"code","9d1b063d":"code","aa23af3c":"code","ee5aaf4a":"code","74a9b1c1":"code","ceb754ce":"code","fa5a66de":"code","5bd8e241":"code","303b45cd":"code","403bea0a":"code","b624c055":"code","6fc5307b":"code","53f31128":"code","60b48a7e":"code","be0b5a8d":"code","f9488ccd":"code","5226dcd1":"code","21cd8c97":"markdown","b5c6bd4d":"markdown","f37c4e09":"markdown","0b72de39":"markdown","784a1567":"markdown","69984f23":"markdown","aff766e6":"markdown","635c4774":"markdown","9f2da930":"markdown","c6a80232":"markdown","56fc9921":"markdown","7c4327dd":"markdown","69b38694":"markdown","051de25f":"markdown","494d98b8":"markdown","4195982f":"markdown","2311ef57":"markdown","3f5ce8b2":"markdown","6c563aa4":"markdown","3546cdfa":"markdown","d5b1a804":"markdown","f0b22470":"markdown","08f76be7":"markdown","ee255d62":"markdown","02f7ef32":"markdown","3e99c54c":"markdown","82691daf":"markdown","cc414394":"markdown","ff5393cd":"markdown","d9b46d4c":"markdown","2df7d0b3":"markdown","293f8c9f":"markdown","b0a4f528":"markdown","ada2cc8f":"markdown","e68a7ba9":"markdown","b256affb":"markdown","15b96b45":"markdown","9d46bc56":"markdown","15b32984":"markdown","122dc407":"markdown","736c92ae":"markdown","9a29b372":"markdown","a39c472e":"markdown","3144bb54":"markdown","327194f2":"markdown","20ffea7a":"markdown","324c4672":"markdown","0262116b":"markdown","ce8a799c":"markdown","fae4b1cd":"markdown","b18a3387":"markdown","5a6ed91d":"markdown","942f4bee":"markdown","792d4862":"markdown","2e0004ce":"markdown","6f0988c3":"markdown","f518f798":"markdown","e1f1b188":"markdown","74090980":"markdown","18eff420":"markdown","efc92a18":"markdown","31fc73c3":"markdown","d3b56343":"markdown","f2308d74":"markdown","7d2ca206":"markdown","3f68a3dd":"markdown","ec3aba9f":"markdown"},"source":{"79405273":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly\nimport plotly.figure_factory as ff\nfrom plotly.offline import plot,iplot,download_plotlyjs\n\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport cufflinks as cf\n\ncf.set_config_file(sharing='public',theme='white',offline=True)\n\nimport plotly.io as pio\npio.renderers.default = 'colab'\n\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import  RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,recall_score,f1_score,precision_score,confusion_matrix\n\nfrom sklearn.pipeline import Pipeline","0adae29b":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndf","046523bb":"df.isnull().sum()","f94cec24":"df1 = df.iloc[:, :-2]\ndf1.info()","ee8f6e41":"Numbercial_features= df1.select_dtypes(include=['float64','int64'])\nNumbercial_features.sample()","518fe5be":"Categorical_features = df1.select_dtypes(exclude=['float64','int64'])\nCategorical_features.sample()","5ed91718":"fig = px.pie(df1,names='Attrition_Flag',title='Percentage of Existing and Attrited Customers',hole=0.3)\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()\nprint('Total number of customers:',df1['Attrition_Flag'].count())\nprint(df1['Attrition_Flag'].value_counts())","c3490321":"Gender = sns.countplot(x = 'Gender',hue = 'Attrition_Flag',data=df1,palette='Set2')\nGen_att = df1.loc[df1['Attrition_Flag']=='Attrited Customer','Gender']\nGen_ex = df1.loc[df1['Attrition_Flag']=='Existing Customer','Gender']\nprint('Gender of Attrited customer:\\n',Gen_att.value_counts())\nprint('-----------------------------------------------------------')\nprint('Gender of Existing customer:\\n',Gen_ex.value_counts())\nprint('-----------------------------------------------------------')\nprint('Gender of Total customer:\\n',df1.Gender.value_counts())","cd1de2dd":"Mar_att = df1.loc[df1['Attrition_Flag']=='Attrited Customer','Marital_Status']\nMar_ex = df1.loc[df1['Attrition_Flag']=='Existing Customer','Marital_Status']\nprint('Marital status of Attrited customer:\\n',Mar_att.value_counts())\nprint('-----------------------------------------------------------')\nprint('Marital status of Existing customer:\\n',Mar_ex.value_counts())\nprint('-----------------------------------------------------------')\nprint('Marital status of Total customer:\\n',df1.Marital_Status.value_counts())","ae82ad42":"fig = make_subplots(\n    rows=2, cols=2,subplot_titles=('Total Customer','Existing Customers','Attrited Customers','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n           [None                               ,{\"type\": \"pie\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=df1.Marital_Status.value_counts().values,\n           labels=['Married ','Single ','Unknow', 'Divorced'],\n           pull=[0,0.01,0.03,0.03],\n           hole=0.3),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(\n        labels=['Married ', 'Single ','Divorced', 'Unknown'],\n        values=df1.query('Attrition_Flag==\"Existing Customer\"').Marital_Status.value_counts().values,\n        pull=[0,0.01,0.05,0.05],\n        hole=0.3),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Pie(\n        labels=['Married ', 'Single','Unknown ','Divorced '],\n        values=df1.query('Attrition_Flag==\"Attrited Customer\"').Marital_Status.value_counts().values,\n        pull=[0,0.01,0.05,0.05],\n        hole=0.3),\n    row=2, col=2\n)\n\n\n\nfig.update_layout(\n    height=700,\n    showlegend=True,\n    title_text=\"<b>Martial Status<b>\",\n)\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","0c5c011d":"Edu_att = df1.loc[df1['Attrition_Flag']=='Attrited Customer','Education_Level']\nEdu_ex = df1.loc[df1['Attrition_Flag']=='Existing Customer','Education_Level']\nprint('Education Level of Attrited customer:\\n',Edu_att.value_counts())\nprint('-----------------------------------------------------------')\nprint('Education Level of Existing customer:\\n',Edu_ex.value_counts())\nprint('-----------------------------------------------------------')\nprint('Education Level of Total customer:\\n',df1.Education_Level.value_counts())","6031874a":"fig = make_subplots(\n    rows=2, cols=2,subplot_titles=('Total Customer','Existing Customers','Attrited Customers','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n           [None                               ,{\"type\": \"pie\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=df1.Education_Level.value_counts().values,\n           labels=['Graduate ','Hight School','Unknown','Uneducated','College','Post-Graduate','Doctorate'],\n           pull=[0,0.01,0.03,0.03],\n           hole=0.3),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(values=df1.query('Attrition_Flag==\"Existing Customer\"').Education_Level.value_counts().values,\n        labels=['Graduate ','Hight School','Unknown','Uneducated','College','Post-Graduate','Doctorate'],\n        pull=[0,0.01,0.05,0.05],\n        hole=0.3),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Pie(values=df1.query('Attrition_Flag==\"Attrited Customer\"').Education_Level.value_counts().values,\n        labels=['Graduate ','Hight School','Unknown','Uneducated','College','Doctorate','Post-Graduate'],\n        pull=[0,0.01,0.05,0.05],\n        hole=0.3),\n    row=2, col=2\n)\n\n\n\nfig.update_layout(\n    height=700,\n    showlegend=True,\n    title_text=\"<b>Education Level<b>\",\n)\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","1fc69467":"Income_att = df1.loc[df1['Attrition_Flag']=='Attrited Customer','Income_Category']\nIncome_ex = df1.loc[df1['Attrition_Flag']=='Existing Customer','Income_Category']\nprint('Income of Attrited customer:\\n',Income_att.value_counts())\nprint('-----------------------------------------------------------')\nprint('Income of Existing customer:\\n',Income_ex.value_counts())\nprint('-----------------------------------------------------------')\nprint('Income of Total customer:\\n',df1.Income_Category.value_counts())","014e8fa2":"fig = make_subplots(\n    rows=2, cols=2,subplot_titles=('Total Customer','Existing Customers','Attrited Customers','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n           [None                               ,{\"type\": \"pie\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=df1.Income_Category.value_counts().values,\n           labels=['Less than $40k ','$40k - $60k','$80k - $120k','$60k - $80k','Unknown','$120k +'],\n           pull=[0,0.01,0.03,0.03],\n           hole=0.3),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(values=Income_ex.value_counts().values,\n        labels=['Less than $40k ','$40k - $60k','$80k - $120k','$60k - $80k','Unknown','$120k +'],\n        pull=[0,0.01,0.05,0.05],\n        hole=0.3),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Pie(values=Income_att.value_counts().values,\n        labels=['Less than $40k ','$40k - $60k','$80k - $120k','$60k - $80k','Unknown','$120k +'],\n        pull=[0,0.01,0.05,0.05],\n        hole=0.3),\n    row=2, col=2\n)\n\n\n\nfig.update_layout(\n    height=700,\n    showlegend=True,\n    title_text=\"<b>Income_Category<b>\",\n)\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","2c5d0a6f":"fig = px.pie(df1,names='Card_Category',title='Percentage of Card type',hole=0.3)\nfig.update_traces(textposition='outside', textinfo='percent+label')\nfig.show()\nprint('Total number of customers:',df1['Card_Category'].count())\nprint(df1['Card_Category'].value_counts())","d3be783e":"####Let us view the age distribution.\nAge_att = df1.loc[df1['Attrition_Flag']=='Attrited Customer','Customer_Age']\nAge_exi = df1.loc[df1['Attrition_Flag']=='Existing Customer','Customer_Age']\nTotal_Age = pd.concat([Age_exi,Age_att],axis=1)\nTotal_Age.columns=['Existing Customer','Attrited Customer']\nTotal_Age","08be2495":"Total_Age.iplot(kind='hist', keys=['Attrited Customer', 'Existing Customer'],\n           colors=['grey', 'red'], histnorm='percent', opacity=0.5, bins=40,\n           title='Customers\\' age', xTitle='Age', yTitle='% customers')","2bf756e0":"fig = px.box(df, color=\"Attrition_Flag\", y=\"Total_Relationship_Count\",title='Number of products held by customer', points=\"all\")\nfig.show()","a7254263":"fig = px.histogram(df, x=\"Total_Relationship_Count\", color=\"Attrition_Flag\",title='Number of products held by customer')\nfig.show()","1a05c73c":"fig = px.box(df, color=\"Attrition_Flag\", y=\"Dependent_count\", points=\"all\",title='Number of dependents of the customer')\nfig.show()","162256de":"fig = px.histogram(df, x=\"Dependent_count\", color=\"Attrition_Flag\",title='Number of dependents of the customer')\nfig.show()","351067bd":"fig = px.box(df, color=\"Attrition_Flag\", y=\"Months_Inactive_12_mon\", points=\"all\",title='Number of months with no transactions in the last year')\nfig.show()","4add05d5":"fig = px.histogram(df, x=\"Months_Inactive_12_mon\", color=\"Attrition_Flag\",title='Number of months with no transactions in the last year')\nfig.show()","0be1d37d":"fig = px.box(df, color=\"Attrition_Flag\", y=\"Credit_Limit\", points=\"all\",title='Credit limit on the credit card')\nfig.show()","f04b1b03":"fig = px.histogram(df, x=\"Credit_Limit\", color=\"Attrition_Flag\",title='Credit limit on the credit card')\nfig.show()","55c1fef4":"fig = px.box(df,color=\"Attrition_Flag\", y=\"Months_on_book\", points=\"all\",title='Number of months since account opening')\nfig.show()","a6da3c6e":"fig = px.histogram(df, x=\"Months_on_book\", color=\"Attrition_Flag\",title='Number of months since account opening')\nfig.show()","d98e1ea1":"fig = px.box(df, color=\"Attrition_Flag\", y=\"Total_Revolving_Bal\", points=\"all\",title='Total revolving balance on the credit card')\nfig.show()","af095bb4":"fig = px.histogram(df, x=\"Total_Revolving_Bal\", color=\"Attrition_Flag\",title='Total revolving balance on the credit card')\nfig.show()","e9e7b14e":"df1.sample()","2b7d5f71":"fig = fig = px.box(df, color=\"Attrition_Flag\", y=\"Total_Trans_Ct\", points=\"all\",title='Number of transactions made in the last year')\nfig.show()","82d38f19":"fig = px.histogram(df, x=\"Total_Trans_Ct\", color=\"Attrition_Flag\",title='Number of transactions made in the last year')\nfig.update()\nfig.show()","fd184e90":"fig = fig = px.box(df, color=\"Attrition_Flag\", y=\"Total_Trans_Amt\", points=\"all\",title='Total amount of transactions made in the last year')\nfig.show()","5ddd02cc":"fig = px.histogram(df, x=\"Total_Trans_Amt\", color=\"Attrition_Flag\",title='Total amount of transactions made in the last year')\nfig.show()","2d0982e9":"fig = fig = px.box(df, color=\"Attrition_Flag\", y=\"Total_Ct_Chng_Q4_Q1\", points=\"all\",title='Change in transaction number over the last year (Q4 over Q1)')\nfig.show()","a3ce5ca4":"fig = px.histogram(df, x=\"Total_Ct_Chng_Q4_Q1\", color=\"Attrition_Flag\",title='Change in transaction number over the last year (Q4 over Q1)')\nfig.show()","37b2d7bc":"#Convert the target value to 1,0\n\ndf1.Attrition_Flag = df1.Attrition_Flag.replace({'Existing Customer':0,'Attrited Customer':1})\ndf1.Gender = df1.Gender.replace({'M':0,'F':1})\nprint(\"# existing customers: {}\\n\".format(len(df.loc[df1['Attrition_Flag']  == 0])))\nprint(\"# attrited customers: {}\\n\".format(len(df.loc[df1['Attrition_Flag']  == 1])))","096bd07b":"#convert all categorcial features to numerical\ndf1 = pd.concat([df1,pd.get_dummies(df1['Education_Level']).drop(columns = ['Unknown'])],axis= 1)\ndf1 = pd.concat([df1,pd.get_dummies(df1['Marital_Status']).drop(columns = ['Unknown'])],axis= 1)\ndf1 = pd.concat([df1,pd.get_dummies(df1['Income_Category']).drop(columns = ['Unknown'])],axis= 1)\ndf1 = pd.concat([df1,pd.get_dummies(df1['Card_Category'])],axis= 1)\n#Delete redundant columns\ndf1.drop(columns= ['Education_Level','Marital_Status','Income_Category','Card_Category','CLIENTNUM'],inplace= True)","2adaa1ae":"df1.head()","50a18676":"df1_spearman_correlation = df1.corr(method='pearson')\n\nfig = go.Figure(data=go.Heatmap(\n                   x=df1_spearman_correlation.columns,\n                   y=df1_spearman_correlation.index,\n                   z=df1_spearman_correlation.values,\n                   name='pearson',showscale=True,xgap=1,ygap=1,\n                   colorscale='Blackbody'))\nfig.update_layout(height=700, width=900, title_text=\"<b>Pearson Correlation<b>\")\nfig.show()","f21b4b51":"df1_spearman_correlation = df1.corr(method='spearman')\n\nfig = go.Figure(data=go.Heatmap(\n                   x=df1_spearman_correlation.columns,\n                   y=df1_spearman_correlation.index,\n                   z=df1_spearman_correlation.values,\n                   name='spearman',showscale=True,xgap=1,ygap=1,\n                   colorscale='Blackbody'))\nfig.update_layout(height=700, width=900, title_text=\"<b>Spearman Correlations<b>\")\nfig.show()","fd86a729":"x_RAW = df1[df1.columns[1:]]\ny_RAW = df1['Attrition_Flag']\nx_train_RAW,x_test_RAW,y_train_RAW,y_test_RAW = train_test_split(x_RAW,y_RAW,test_size = 0.2,random_state =42)","17c51452":"SC = StandardScaler()\nx_train_RAW = SC.fit_transform(x_train_RAW)\nx_test_RAW = SC.fit_transform(x_test_RAW)","39310ec3":"RF_RAW = RandomForestClassifier()\nRF_RAW.fit(x_train_RAW,y_train_RAW)","ca43e999":"RF_RAW_pre = RF_RAW.predict(x_test_RAW)","09a19d54":"sns.heatmap(confusion_matrix(y_test_RAW,RF_RAW_pre), annot=True)\nplt.show()","69e2e554":"\n\nRF_RAW_fpr, RF_RAW_tpr, RF_RAW_thresholds = metrics.roc_curve(y_test_RAW, RF_RAW_pre)\nRF_RAW_AUC = metrics.auc(RF_RAW_fpr, RF_RAW_tpr)\nprint('Random Forest Classifier : \\n', classification_report(RF_RAW_pre, y_test_RAW))\nfig = px.area(\n    x=RF_RAW_fpr, y=RF_RAW_tpr,\n    title='AUC of RandomForest on RAW Data:'' %0.4f'% RF_RAW_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","45a7a216":"SVM_RAW = Pipeline(steps =[(\"RF\",SVC(random_state=42,kernel='rbf'))])\nSVM_RAW.fit(x_train_RAW,y_train_RAW)","52e9416c":"SVM_RAW_pre = SVM_RAW.predict(x_test_RAW)\nsns.heatmap(confusion_matrix(y_test_RAW,SVM_RAW_pre), annot=True)\nplt.show()","0b707846":"SVM_RAW_fpr, SVM_RAW_tpr, SVM_RAW_thresholds = metrics.roc_curve(y_test_RAW, SVM_RAW_pre)\nSVM_RAW_AUC = metrics.auc(SVM_RAW_fpr, SVM_RAW_tpr)\nprint('Support Vector Machine : \\n', classification_report(SVM_RAW_pre, y_test_RAW))\n\nfig = px.area(\n    x=SVM_RAW_fpr, y=SVM_RAW_tpr,\n    title='AUC of SVM on RAW Data:'' %0.4f'% SVM_RAW_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","972ee29e":"GBoost_RAW = Pipeline(steps=[('RF',GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=42))])","7b4046f4":"GBoost_RAW.fit(x_train_RAW,y_train_RAW)","3f5698d9":"GBoost_RAW_pre = GBoost_RAW.predict(x_test_RAW)","8b8e4808":" sns.heatmap(confusion_matrix(y_test_RAW,GBoost_RAW_pre), annot=True)\n plt.show()","f6c20e94":"GBoost_RAW_fpr, GBoost_RAW_tpr, GBoost_RAW_thresholds = metrics.roc_curve(y_test_RAW, GBoost_RAW_pre)\nGBoost_RAW_AUC = metrics.auc(GBoost_RAW_fpr, GBoost_RAW_tpr)\nprint('Gradient Boosting Classifier : \\n', classification_report(GBoost_RAW_pre, y_test_RAW))\n\nfig = px.area(\n    x=GBoost_RAW_fpr, y=GBoost_RAW_tpr,\n    title='AUC of GBoost on RAW Data:'' %0.4f'% GBoost_RAW_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","f0a29a14":"df1.head()","8232b6cc":"x= df1[df1.columns[1:]]\ny = df1['Attrition_Flag']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state =42)  ","9beb4dc0":"#standaration of data\nx_train = SC.fit_transform(x_train)\nx_test = SC.fit_transform(x_test)","d5a25f4f":"from imblearn.over_sampling import BorderlineSMOTE\nsm = BorderlineSMOTE()\nx_SM, y_SM = sm.fit_resample(x_train, y_train)","0c368fe8":"#Check the result of SMOTE\nfrom collections import Counter\nprint(Counter(y_train))\nprint(Counter(y_SM))","99cad9d0":"SM_x = np.concatenate((x_SM, x_test))\nSM_y = np.concatenate((y_SM, y_test))\nfeature_names = list(df1.drop('Attrition_Flag', axis=1).columns)","cc29950a":"#Store the balanced data in a new dataframe.\nsm_df = pd.DataFrame(np.column_stack([SM_y, SM_x]), columns=['Attrition'] + feature_names)\nsm_df.head()","9091f090":"RF_SMOTE = RandomForestClassifier()\nSVM_SMOTE = Pipeline(steps =[(\"RF\",SVC(random_state=42,kernel='rbf'))])\nGBoost_SMOTE = Pipeline(steps=[('RF',GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=42))])","3df819cb":"# K-fold = 10\nRF_SMOTE_Cross_validation_scores = cross_val_score(RF_SMOTE,x_SM,y_SM,cv= 10,scoring='f1')\nSVM_SMOTE_Cross_validation_scores = cross_val_score(SVM_SMOTE,x_SM,y_SM,cv= 10,scoring='f1')\nGBoost_SMOTE_Cross_validation_scores = cross_val_score(GBoost_SMOTE,x_SM,y_SM,cv= 10,scoring='f1')","bcef3636":"x_axix = [x for x in range(10)] \nplt.title('Compare F1 score of Cross-validation each model')\nplt.plot(x_axix, RF_SMOTE_Cross_validation_scores, color='green', label='Random Forest F1')\nplt.plot(x_axix, SVM_SMOTE_Cross_validation_scores,  color='skyblue', label='SVM F1')\nplt.plot(x_axix, GBoost_SMOTE_Cross_validation_scores, color='blue', label='Gradient Boosting F1')\nplt.legend()\n\nplt.xlabel('iteration times')\nplt.ylabel('F1 score')\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()","59154aba":"RF_SMOTE.fit(x_SM,y_SM)","6ed6ad08":"RF_SMOTE_pre = RF_SMOTE.predict(x_test)","0a1e772d":"from sklearn import metrics\nfrom sklearn.metrics import classification_report\n\nRF_SMOTE_fpr, RF_SMOTE_tpr, RF_SMOTE_thresholds = metrics.roc_curve(y_test, RF_SMOTE_pre)\nRF_SMOTE_AUC = metrics.auc(RF_SMOTE_fpr, RF_SMOTE_tpr)\nprint('Random Forest Classifier : \\n', classification_report(RF_SMOTE_pre, y_test))\nfig = px.area(\n    x=RF_SMOTE_fpr, y=RF_SMOTE_tpr,\n    title='AUC of RandomForest on Upsample Data:'' %0.4f'% RF_SMOTE_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","9d1b063d":"SVM_SMOTE.fit(x_SM,y_SM)","aa23af3c":"SVM_SMOTE_pre = SVM_SMOTE.predict(x_test)","ee5aaf4a":"SVM_SMOTE_fpr, SVM_SMOTE_tpr, SVM_SMOTE_thresholds = metrics.roc_curve(y_test, SVM_SMOTE_pre)\nSVM_SMOTE_AUC = metrics.auc(SVM_SMOTE_fpr, SVM_SMOTE_tpr)\nprint('Support Vector Machine : \\n', classification_report(SVM_SMOTE_pre, y_test))\n\nfig = px.area(\n    x=SVM_SMOTE_fpr, y=SVM_SMOTE_tpr,\n    title='AUC of SVM on Upsample Data:'' %0.4f'% SVM_SMOTE_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","74a9b1c1":"GBoost_SMOTE.fit(x_SM,y_SM)","ceb754ce":"GBoost_SMOTE_pre = GBoost_SMOTE.predict(x_test)","fa5a66de":"GBoost_SMOTE_fpr, GBoost_SMOTE_tpr, GBoost_SMOTE_thresholds = metrics.roc_curve(y_test, GBoost_SMOTE_pre)\nGBoost_SMOTE_AUC = metrics.auc(GBoost_SMOTE_fpr, GBoost_SMOTE_tpr)\nprint('Gradient Boosting Classifier : \\n', classification_report(GBoost_SMOTE_pre, y_test))\n\nfig = px.area(\n    x=GBoost_SMOTE_fpr, y=GBoost_SMOTE_tpr,\n    title='AUC of GBoost on Upsample Data:'' %0.4f'% GBoost_SMOTE_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","5bd8e241":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Number of trees in random forest \nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","303b45cd":"#rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\n#rf_random.fit(x_SM, y_SM)\n#print(rf_random.best_params_)","403bea0a":"RF_opt= RandomForestClassifier(n_estimators=1800, min_samples_split=2, min_samples_leaf=1, \n                            max_features='auto', max_depth=20, bootstrap=False)\nRF_opt.fit(x_SM,y_SM)\nRF_opt_pre=RF_opt.predict(x_test)\nprint('Random Forest Classifier (Optimized)')\nrf_opt=plot_confusion_matrix(RF_opt, x_test, y_test)","b624c055":"RF_opt_fpr, RF_opt_tpr, RF_opt_thresholds = metrics.roc_curve(y_test, RF_opt_pre)\nRF_opt_AUC = metrics.auc(RF_opt_fpr, RF_opt_tpr)\nprint('Random Forest Classifier : \\n', classification_report(RF_opt_pre, y_test))\n\nfig = px.area(\n    x=RF_opt_fpr, y=RF_opt_tpr,\n    title='AUC of Random Forest (Optimized):'' %0.4f'% RF_opt_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","6fc5307b":"from sklearn.model_selection import GridSearchCV\n\nparam_test1 = {'n_estimators':range(20,81,10)}\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=1.0,\n                                                               min_samples_split=500,\n                                                               min_samples_leaf=50,\n                                                               max_depth=8\n                                                               ,max_features='sqrt',\n                                                               subsample=0.8,\n                                                               random_state=10), \nparam_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=10)\ngsearch1.fit(x_SM,y_SM)\nprint(gsearch1.best_params_)","53f31128":"GB_opt=GradientBoostingClassifier(n_estimators=80,learning_rate=1.0,\n                                      min_samples_split=500,min_samples_leaf=50,\n                                      max_depth=8,max_features='sqrt',\n                                      subsample=0.8,random_state=10)\nGB_opt.fit(x_SM,y_SM)\nGB_opt_pre=GB_opt.predict(x_test)\nprint('Gradient Boosting (Optimized)')\nGB_opt=plot_confusion_matrix(GB_opt, x_test, y_test)","60b48a7e":"GB_opte_fpr, GB_opt_tpr, GB_opt_thresholds = metrics.roc_curve(y_test, GB_opt_pre)\nGB_opt_AUC = metrics.auc(GB_opte_fpr, GB_opt_tpr)\nprint('Gradient Boosting Classifier : \\n', classification_report(GB_opt_pre, y_test))\n\nfig = px.area(\n    x=GB_opte_fpr, y=GB_opt_tpr,\n    title='AUC of GBoost (Optimized):'' %0.4f'% GB_opt_AUC,\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n    width=700, height=600\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=0, y1=1\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\nfig.show()","be0b5a8d":"RF_Features = RandomForestClassifier(random_state=1234)\nRF_Features.fit(x_SM, y_SM)\nRF_Features.feature_importances_","f9488ccd":"features_to_plot = 20\nimportances = RF_Features.feature_importances_\nindices = np.argsort(importances)\nbest_features = np.array(feature_names)[indices][-features_to_plot:]\nvalues = importances[indices][-features_to_plot:]\nbest_features","5226dcd1":"y_ticks = np.arange(0, features_to_plot)\nfig, ax = plt.subplots()\nax.barh(y_ticks, values)\nax.set_yticklabels(best_features)\nax.set_yticks(y_ticks)\nax.set_title(\"Random Forest Feature Importances\")\nfig.tight_layout()\nfig.set_size_inches(18.5, 10.5)\nplt.show()","21cd8c97":"# 4.5 Tunning parameters<a id = \"4.5\"><a>\n\n\n\n1.   RandomizedSearchCV\n2.   GridSearchCV\n\n","b5c6bd4d":"The value range of Pearson's correlation coefficient is [-1, 1]:\n\n\n*   When it is close to 1, it means that the two have a strong positive correlation.\n*   When it is close to -1, it indicates that there is a strong negative correlation.\n\n*   And if the value is close to 0, it means that the correlation or variable is not linearly related. But when R=0, we have to pay attention to another non-linear dependency, which can be checked with spearman coefficient.\n\n\nWe need to convert qualitative variables into quantitative variables at the very beginning.","f37c4e09":"# GridSearchCV<a id = \"4.5.2\"><a>","0b72de39":"# Gradient Boosting<a id = \"4.1.3\"><a>","784a1567":"# Number of products held by customer<a id =\"3.2.2\"><a>","69984f23":"From the level of education, we can conclude that if unknown is defined as uneducated, We can state that 70% of our customers have received formal education, and 30% of them have received higher education. Only 10% of the customers are postgraduate and doctors.","aff766e6":"# Credit limit<a id =\"3.2.5\"><a>","635c4774":"1.   Age\n2.   Number of products held by customer\n\n1.   Number of dependents of customer\n2.   Number of months with no transactions in the last year.\n\n1.   Credit limit\n2.   Months of account\n\n1.   Revolving balance on the credit card.\n\n1.   Amount of transactions made in last year\n2.   Number of transactions made in last year\n\n1.   Change in transaction number over the last year (Q4 over Q1).","9f2da930":"\n>   Obviously, this is an unbalanced data set. We will then upsample the original data to obtain a balanced data set.\n\n\n\n>   Through the previous data exploration and visualization, there is no missing vaule in this data, but some values in categorical data are Unknown. To some extent, 'Unknown' can be defined as missing values.\n\n>   For the 'Unknown' class processing depends on the actual business scenario. Sometimes it is possible to make 'Unknown' into a new class.\n\n> But in this data set, we simply delete the 'Unknown' value.\n\n\n\n\n\n","c6a80232":"# RandomizedSearchCV<a id =\"4.5.1\"><a>","56fc9921":"We can see the different AUC of each model after the Traning model in the upsampling dataset. \n\nInterestingly, except for Gradient boosting, the other two algorithms have improved.\n","7c4327dd":"For models trained on upsample data sets, \n\nFrom the AUC of the models:\n*   RandomFroest = 0.9312\n*   Support Vector Machine = 0.8611\n*   Gradient Boosting = 0.8787\n","69b38694":"1.  Training Model in  RAW Dataset\n2.  SMOTE\n3.  Cross-validation\n4.  Training Model in Upsample Dataset\n5. Tunning parameters\n6.  Feature Selection\n\nExperimental algorithm:\n*   Random Forest\n*   Support Vector Machine\n*   Gradient Boosting\n\nCompare the performance of algorithms on **raw data** and **up-sampled data**.","051de25f":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>\n\n* [1. Project Introduction](#1)\n* [2. Pre-Processing](#2)\n    * [2.1 Librarie](#2.1)\n    * [2.2 Data Loading](#2.2)\n        * [2.2.1 Data Description](#2.2.1)\n    * [2.3 Data Check](#2.3)\n* [3. Exploratory Data Analysis(EDA)](#3)\n    * [3.1 Categorcial Data Visualization](#3.1)\n        * [3.1.1 Churn Status\/Attrition_Flag](#3.1.1)\n        * [3.1.2 Gender](#3.1.2)\n        * [3.1.3 Marital Status](#3.1.3)\n        * [3.1.4 Education Level](#3.1.4)\n        * [3.1.5 Income](#3.1.5)\n        * [3.1.6 Card Category](#3.1.6)\n    * [3.2 Numercial Data Visualization](#3.2)\n        * [3.2.1 Age](#3.2.1)\n        * [3.2.2 Number of products held by customer](#3.2.2)\n        * [3.2.3 Number of dependents of customer](#3.2.3)\n        * [3.2.4 Number of months with no transactions in the last year.](#3.2.4)\n        * [3.2.5 Credit limit](#3.2.5)\n        * [3.2.6 Months of account](#3.2.6)\n        * [3.2.7 Revolving balance on the credit card.](#3.2.7)\n        * [3.2.8 Number of transactions made in last year](#3.2.8)\n        * [3.2.9 Amount of transactions made in last year](#3.2.6)\n        * [3.2.10 Change in transaction number over the last year (Q4 over Q1).](#3.2.10)\n    * [3.3 Pearson Correlation & Spearman Correlation](#3.3)\n* [4. Classification Model](#4)\n    * [4.1 Training Model in  RAW Dataset](#4.1)\n        * [4.1.1 RandomForest Classifier](#4.1.1)\n        * [4.1.2 Support Vector Machine](#4.1.2)\n        * [4.1.3 Gradient Boosting](#4.1.3)\n    * [4.2 SMOTE](#4.2)\n    * [4.3 Cross-validation](#4.3)\n    * [4.4 Training Model in Upsample Dataset](#4.4)\n        * [4.4.1 RandomForest Classifier](#4.4.1)\n        * [4.4.2 Support Vector Machine](#4.4.2)\n        * [4.4.3 Gradient Boosting](#4.4.3)\n    * [4.5 Tunning parameters](#4.5)\n        * [4.5.1 RandomizedSearchCV](#4.5.1)\n        * [4.5.2 GridSearchCV](#4.5.2)\n    * [4.6 Feature Selection](#4.6)\n* [5 Conclusion](#5)\n","494d98b8":"# Project Introduction<a id = \"1\" ><\/a>","4195982f":"![credit-cards-4-steps-1920x1152-2.jpg](attachment:6f0284c5-e60d-4b88-8a8c-aa68cd1d28d0.jpg)","2311ef57":"\n\nWe can clearly see that the variables **Total_Trans_Ct**, **Total_Trans_Ams**, **Total_Revolving_Bal**, **Contacts_Count_12_mon,** **Total_Ct_Chng_Q4_Q**1 have high importance in the random forest model. \n\nBut in order to avoid overfitting, we can't just use variables with high importance in the model training process.\n\n","3f5ce8b2":"# Education Level<a id = \"3.1.4\"><a>","6c563aa4":"# RandomForest Classifier<a id =\"4.4.1\"><a>","3546cdfa":"1. Categorical data visualization\n2. Numerical data visualization\n3. Pearson & Spearman correlation coefficient\n\nCheck the data structure of Dataset.\nThere are 20 columns in this dataset. \nWe need to know which data attributes are numerical and which data attributes are categorical.","d5b1a804":"Categorcial Data:\n\n\nAttrition_Flag, \nGender, \nEducation_Level, \nMarital_Status, \nIncome_Category and Card_Category.\n\nAll the remaining data are numerical values.","f0b22470":"# Months of account<a id =\"3.2.6\"><a>","08f76be7":"# 4.3 Cross-validation<a id =\"4.3\"><a>","ee255d62":"# Marital Status <a id = \"3.1.3\"><a>\n","02f7ef32":"# Age <a id =\"3.2.1\"><a>","3e99c54c":"# 2.1 Libraries<a id = \"2.1\" ><\/a>","82691daf":"Approximately 35%  of customers' income is less than 40K annual salary.","cc414394":"In this kernel, we will\n\n* Data set cleaning & exploration\n* Dealing with unbalanced data\n* Compare the performance of machine learning models on two data sets.\n* Tunning parameters","ff5393cd":"# 4. Classification Model<a id =\"4\"><a>\n","d9b46d4c":"# Number of months with no transactions in the last year<a id =\"3.2.4\"><a>","2df7d0b3":"# Conclusion<a id =\"5\"><a>\n\n1. On unbalanced data sets, we can only consider **AUC** as a performance indicator.\n\n*   RandomFroest = 0.8733\n*   Support Vector Machine = 0.7692\n*   Gradient Boosting = 0.8975\n\n2. On upsample data sets\n\n*   RandomFroest = 0.9315\n*   Support Vector Machine = 0.8611\n*   Gradient Boosting = 0.8593\n\n\nWe can conclude from the 10-fold cross-validation that the performance of random forest on upsample data set is better than SVM and Gradient Boosting.\n\nAfter Tunning parameters. (RandomizedSearchCV and GridSearchCV)\n*   AUC of Random Forest : 0.9321\n*   AUC of Gradient Boosting : 0.8969\n\n\nAfter several experiments, we can learn that **RandomForest** may be the best performing model for this data set.\n\nThe actual performance of the model is unknown in the real world, because we use SMOTE to adjust the data set.\n\n\n","293f8c9f":"\n> **Any data must be checked for missing values first.**\n\nEnsuring the quality of the data set greatly affects the performance of the model.\n\n\n> The filling method should be judged on the **Real-world Business Scenario.**\n\n* Filling methods\uff1a\n    *   Default value\n    *   Mean value\n    *   Mode\n    *   KNN filling\n    *   Predicting through the model(Random Forest) as label\n\n\n","b0a4f528":"# Gradient Boosting<a id =\"4.4.3\"><a>","ada2cc8f":"The interesting point is that nearly 39% of customers are single.","e68a7ba9":"# Churn Status\/Attrition_Flag <a id =\"3.1.1\"><a>","b256affb":"# Number of dependents of customer<a id =\"3.2.3\"><a>","15b96b45":"# Gender<a id = \"3.1.2\"><a>","9d46bc56":"# Revolving balance on the credit card<a id =\"3.2.7\"><a>","15b32984":"# Card Category<a id = \"3.1.6\"><a>","122dc407":"We only upsample the training set of 80% of the original data to ensure that the model has not seen the remaining 20% of the data to avoid overfitting problems.","736c92ae":"> We can conclude from the 10-fold cross-validation that the performance of random forest on this data set is better than SVM and Gradient Boosting. \n\n\n> But is this the case? Let's train and test the model on the upsample data set.\n\n\n\n","9a29b372":"#  2.3 Data Checking<a id = '2.3'><a>","a39c472e":"We can see The distribution of customer age conforms to the normal distribution.","3144bb54":"# 4.6 Feature Selelction<a id = \"4.6\"><a>\n    \nAccording to the form of feature selection, feature selection methods can be divided into three types:\n\nFilter: \n*  Variance  \n*  Correlation Coefficient\n*  chi-square test.\n\nWrapper:\n*  Recursive feature elimination.\n\nEmbedded: \n*  Based on machine learning algorithms and models.\n\n    \nWe are going to select features in RandomForest way.\n","327194f2":"# Amount of transactions made in last year<a id =\"3.2.9\"><a>","20ffea7a":"# 3.2 Numercial Data visualization<a id =\"3.2\"><a>\n","324c4672":"# 2.2 Data loading<a id = \"2.2\" ><\/a>","0262116b":"# Change in transaction number over the last year (Q4 over Q1)<a id =\"3.2.10\"><a>","ce8a799c":"# RandomForest Classifier<a id = \"4.1.1\"><a>","fae4b1cd":"# 4.2 SMOTE<a id =\"4.2\"><a>\n\n There are currently two popular methods for sampling minority categories:\n\n*  (i) Synthetic Minority Oversampling Technique (SMOTE).\n*  (ii) Adaptive Synthetic (ADASYN).\n\n\n\n> An improved algorithm of SOMTE is used here: BoderlineSMOTE.\n\n* SMOTE: For the minority sample A, randomly select a nearest neighbor sample B, and then randomly select a point C from the line between A and B as the new minority sample.\n\n* BoderlineSMOTE: All minority sample points are divided into three categories: 1) noise, that is, all nearby points are heterogeneous samples. 2) In danger, at least half of the nearby points are samples of the same kind. 3) Safe points, all nearby points are samples of the same kind. Then uniformly use the second-class in danger minority samples to generate data points.\n\n\n\n","b18a3387":"# Number of transactions made in last year<a id =\"3.2.8\"><a>","5a6ed91d":"# Support Vector Machine<a id = \"4.1.2\"><a>","942f4bee":"# 3. Exploratory Data Analysis(EDA)<a id = \"3\"><a>","792d4862":"1. CLIENTNUM - ID of the customer holding the credit card.\n\n2. Customer_Age - Age of the customer.\n\n3. Gender - Sex of the customer.\n\n4. Dependent_count - Number of dependents of the customer.\n\n5. Education_Level - Educational qualification of the customer.\n\n6. Marital_Status - Civil status of the customer.\n\n7. Income_Category - Annual income range of the customer.\n\n8. Card_Category - Type of card owned by the customer.\n\n9. Months_on_book - Number of months elapsed since the account opening.\n\n10. Total_Relationship_Count - Total number of products held by the customer.\n\n11. Months_Inactive_12_mon - Number of months with no transactions in the last year.\n\n12. Contacts_Count_12_mon - Number of contacts with the bank in the last year.\n\n13. Credit_Limit - Credit limit on the credit card.\n\n14. Total_Revolving_Bal - Total revolving balance on the credit card.\n\n15. Avg_Open_To_Buy - Average card \"Open To Buy\" (=credit limit - account balance) in the last year.\n\n16. Total_Amt_Chng_Q4_Q1 - Change in transaction amount over the last year (Q4 over Q1).\n\n17. Total_Trans_Amt - Total amount of transactions made in the last year.\n\n18. Total_Trans_Ct - Number of transactions made in the last year.\n\n19. Total_Ct_Chng_Q4_Q1 - Change in transaction number over the last year (Q4 over Q1).\n\n20. Avg_Utilization_Ratio - Average card \"Utilization ratio\" (=account balance \/ credit limit) in the last year.\n\n21. Navie Bayes\n\n22. Navie Bayes\n\n23. Attrition_Flag - Target variable. \"Attrited Customer\" if the customer closed their account, otherwise \"Existing Customer\".","2e0004ce":"# 2.2.1 Data description<a id = \"2.2.1\" ><\/a>","6f0988c3":"# 2. Pre-Processing<a id = \"2\" ><\/a>","f518f798":"# 4.1 Training Model in RAW Dataset<a id = \"4.1\"><a>","e1f1b188":"# Support Vector Machine<a id = \"4.4.2\"><a>","74090980":"Through the heat map, we can find that Arrition_Flag is related to the following features: \n* Total_Trans_Ct(-0.37)\n* Total_CT_Chang_Q4_Q1(-0.29)\n* Avg_Utilization_Ratio(-0.24)\n* Total_Revolving_Bal(-0.24)\n* Total_Trans_Amt(-0.22)\n\nLet us pay attention to these characteristics, but this does not mean that they are causally related to the target vaule.\n\nMeanwhile, we noticed that the following features have very weak correlation coefficients:\n* Months_on_book(0.015)\n* Customer_Age(0.017)\n* Dependent_Count(0.021)\n* Avg_Open_To_Buy(0.027)\n* Gender(0.037)\n\n\nThe correlation between Credit limit and Avg_Open_To_Buy is close to 1 (0.99).\nThis means that these two features are strongly correlated. Usually we can remove one of them and keep the feature with the larger Pearson coefficient of the target vaule.\n\nThe correlation coefficient of Customer age and Months On_book is 0.79, which belongs to the category of strong relationship [0.6-0.8].\n\nThe correlation coefficient of Total_Trans_Ct and Total_Trans_Amt is 0.88, which belongs to the category of extremely strong relationship [0.8-1.0].","18eff420":"# 3.3 Pearson Correlation & Spearman Correlation<a id = \"3.3\"><a>\n","efc92a18":"For models trained on unbalanced data sets, we can only consider AUC as a performance indicator.\n\nFrom the AUC of the models:\n*   RandomFroest = 0.8733\n*   Support Vector Machine = 0.7692\n*   Gradient Boosting = 0.8975\n","31fc73c3":"# 4.4 Traning Model in Upsample Dataset<a id = \"4.4\"><a>","d3b56343":"Fortunately, we don't have a null value.\n\n\nAccording to the data set description, delete the 21 and 22 columns of Naive_Bayes_Classifier","f2308d74":"# 3.1 Cegorcial Data visualization<a id = '3.1'><a>\n","7d2ca206":"We can see the length of Attrited Customers is 1627 which means there are 1627 Attrited customers in the dataset. And the length of existing customers is 8500 which means there are 8500 existing customers in the dataset. obviously, this is an unbalanced dataset.","3f68a3dd":"# Income<a id = \"3.1.5\"><a>","ec3aba9f":"\"A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction\n\nThis dataset from a website with the URL as https:\/\/leaps.analyttica.com\/home. The site explains how to solve a particular business problem.\n\nNow, this dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 23 features.\"     \n\n\n--Sakshi Goyal(Dataset uploader)\n\n"}}