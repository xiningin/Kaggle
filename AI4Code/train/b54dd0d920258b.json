{"cell_type":{"4b893337":"code","e43cc396":"code","d86fa0d5":"code","648159cd":"code","71e13503":"code","385340f7":"code","ec09faac":"code","089296c0":"code","e96a6317":"code","0244c976":"code","dace1726":"code","661c89d0":"code","4caa4939":"code","5c392389":"code","5b6f7bc7":"code","1a0eb7cb":"code","5370965a":"markdown","6abfc3f4":"markdown","2d1d6d46":"markdown","fe14e9ea":"markdown","eef12bb6":"markdown","fc4cfc8e":"markdown","6d5984ef":"markdown","6b67fdf1":"markdown","e6daec63":"markdown","2dde8b32":"markdown","80a8537d":"markdown"},"source":{"4b893337":"# Import numpy, pandas, and matplotlib using the standard aliases. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import the following tools from sklearn: \n#     Pipeline, SimpleImputer, ColumnTransformer, OneHotEncoder, StandardScaler\n#     LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Import joblib\nimport joblib","e43cc396":"# Load the training data into a DataFrame named 'train'.\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain = train.sample(frac=1, random_state=1)\n\n# Print the shape of the resulting DataFrame.\nprint(train.shape)\n# You do not need the test data in this notebook. ","d86fa0d5":"# Display the head of the train DataFrame. \ntrain.head()","648159cd":"# Calculate and print the number of missing values in each column.\ntrain.isnull().sum().to_frame().T","71e13503":"# Display a DataFrame showing the proportion of observations with each \n# possible of the target variable (which is Survived). \n(train.Survived.value_counts() \/ len(train)).to_frame()","385340f7":"\n# We will start with some feature engineering. \n\n# Add a new column named 'FamSize' to the DataFrame. \n# This should be the sum of the 'SibSp' and 'Parch' columns. \ntrain['FamSize'] = train['SibSp'] * train['Parch']\n\n# We will use the function below to determine the deck letter for each passenger:\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\n# Use the map() method of the train DataFrame to apply the function above \n# to the 'Cabin' column. Store the results in a new column named 'Deck'. \ntrain['Deck'] = train['Cabin'].map(set_deck)\n","ec09faac":"# Create a list of numberical feature names. Use the following features: 'Age', 'FamSize', 'Fare'\n# Create a list of categorical feature names. Use the following features: 'Sex', 'Pclass', 'Deck', 'Embarked'\n# Combine the two previous lists into one list named 'features'\nnum_features = ['Age', 'FamSize', 'Fare']\ncat_features = ['Sex', 'Pclass', 'Deck', 'Embarked']\nfeatures = num_features + cat_features\n\nprint(cat_features)\nprint(num_features)\nprint(features)\n\n# Create a Pipeline object for processing the numerical features. \n# This pipeline should consist of a SimpleImputer and a StandardScaler\n\nnum_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())  \n    ]\n)\n\n# Create a Pipeline object for processing the categorical features. \n# This pipeline should consist of a SimpleImputer and a OneHotEncoder\n\ncat_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\n# Create a ColumnTransformer object that combines the two pipelines created above. \n# Name this ColumnTransformer 'preprocessor'\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)","089296c0":"# Fit the preprocessor to the training data, selecting only the columns in the 'features' list. \n# Apply the fitted preprocessor to the training data, again selecting only the relevant columns. \n# Store the array created in the previous step into a variable named 'X_train'.\n\ntrain_f = train[['Age', 'FamSize', 'Fare', 'Sex', 'Pclass', 'Deck', 'Embarked']].copy()\n\npreprocessor.fit(train_f)\nX_train = preprocessor.transform(train_f)\n\n\n# Create a variable named 'y_train' that contains the training labels. \n\ny_train = train.Survived.values\n\n# Print the shapes of X_train and y_train.\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)\n\n","e96a6317":"# Select a range of parameter values of C. \n# You might need to experiment with this to find a good value for C\n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\n%time \n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.5, 1],\n    'C': [0.0001, 0.001, 0.01, 0.1, 1]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10,refit='True', n_jobs=-1, verbose=10, scoring= 'accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","0244c976":"# Run this cell without any changes to view the CV results.\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","dace1726":"# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\n%time \n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [2, 4, 6, 8, 10, 12, 14, 16],\n    'min_samples_leaf': [2, 4, 8,16]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring= 'accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","661c89d0":"# Run this cell without any changes to view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","4caa4939":"# Select a number of trees to use in your random forest.\n# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\n%time \n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=50)\n\nrf_parameters = {\n    'max_depth': [4, 8, 16, 20, 24, 28, 32],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","5c392389":"# Run this cell without any changes to view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","5b6f7bc7":"print(rf_grid.best_params_)","1a0eb7cb":"# Save your pipeline to a file. \njoblib.dump(preprocessor, 'titanic_pipeline.joblib')\n\nfinal_model = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=8, min_samples_leaf=1)\nfinal_model.fit(X_train, y_train)\n\nprint(final_model.score(X_train, y_train))\n\n\n# Determine the best model found above and save that to a file. \nprint(rf_grid.best_params_)\n\njoblib.dump(final_model, 'titanic_final_model.joblib')\n\nprint('Model written to file.')\n# Download both files to your local device and then upload them as a Kaggle dataset. ","5370965a":"# Preprocessing","6abfc3f4":"# Model Selection","2d1d6d46":"# Import Statements","fe14e9ea":"## Decicion Trees","eef12bb6":"# Titanic Dataset\n\nMost of the code cells below include comments explaining the task to be performed in those cells. Please delete the comments and add code to perform those tasks. There are a few code cells in which code has already been provided for you. In some cases, you will need to compelte this code. ","fc4cfc8e":"## Logistic Regression","6d5984ef":"# Load Training Data","6b67fdf1":"# Save Pipeline and Model","e6daec63":"# Check Label Distribution","2dde8b32":"## Random Forests","80a8537d":"# Check for Missing Values"}}