{"cell_type":{"f39e846d":"code","5dea4367":"code","4de2ed70":"code","5d69c987":"code","9ad5c8d9":"code","eb7515c8":"code","9d075b53":"code","6175c3de":"code","a9099aac":"code","fa286e54":"code","8092659d":"code","62974ec4":"code","e046dc57":"code","c98943c9":"code","c5930c47":"code","2fd69e03":"code","ff469326":"code","14918629":"code","6cce95f4":"code","779019b5":"code","6315a4df":"code","899a2ecc":"code","22d97922":"code","1320a8ad":"code","3836e897":"code","c104a539":"code","e0ddb208":"code","d068df37":"code","ee8184f4":"code","8ff774d3":"code","8d160f15":"code","3f3b30d6":"code","929c7f2f":"code","4b924bdb":"code","46cf53e7":"code","eae7aa7d":"code","98908387":"code","602005f0":"code","60e72e77":"code","a851bde5":"code","97057ac6":"markdown","57d80791":"markdown","d1bc78e7":"markdown","1655ba98":"markdown","36e68d1c":"markdown","54f25bc5":"markdown","d57ec15e":"markdown","063a1f15":"markdown","973bd585":"markdown","24dc53fb":"markdown","a7698b19":"markdown","7f8ee8bb":"markdown","2f9a5eb1":"markdown","669ed195":"markdown","77dfd659":"markdown","a908a64e":"markdown","f46d8bcf":"markdown","84a8eae3":"markdown","a6fd5d80":"markdown","806c0eb4":"markdown","8062ada2":"markdown","2cfcb848":"markdown","680cdf49":"markdown","9d25548f":"markdown","2f181156":"markdown","fda7958b":"markdown","c109dad1":"markdown","249b14a9":"markdown","4941b919":"markdown","e1330f69":"markdown","3c42dc64":"markdown","7f71c864":"markdown","432231ef":"markdown","0ca549bf":"markdown","fafa4cec":"markdown","de672ff8":"markdown","7ad4c91a":"markdown","8f878fae":"markdown","4d8dae30":"markdown","ff0163e3":"markdown","9a9aff22":"markdown","bac4c456":"markdown","3cbdc9fa":"markdown","3466d9d2":"markdown","a261fa82":"markdown","37a9ee6f":"markdown"},"source":{"f39e846d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\n\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\n","5dea4367":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_test = pd.concat([train, test], ignore_index=True, sort  = False)","4de2ed70":"train_test.head()","5d69c987":"print('Train data shape is: ', train.shape)\nprint('Test data shape is: ', test.shape)\nprint('Mixed data shape is: ', train_test.shape)","9ad5c8d9":"survivedd = train[train[\"Survived\"] == 1]\nnot_survived = train[train[\"Survived\"] == 0]\n\nprint(\"Survived count: {x} {y:1.3f} %\".format(x=len(survivedd), y=float(len(survivedd) \/ len(train)) * 100))\nprint(\"Not Survived count: {z} {n:1.3f} %\".format(z=len(not_survived), n=float(len(not_survived) \/ len(train) * 100)))","eb7515c8":"ax=sns.countplot(train[\"Survived\"])\n\n#Just fancy code to write percentages on bar.\ntotals = []\nfor i in ax.patches:\n    totals.append(i.get_height())\n\ntotal = sum(totals)\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+.25, i.get_height()-250.95, \\\n            str(round((i.get_height()\/total)*100, 2))+'%', fontsize=15,\n                color='white')\n\nplt.title('Survival Status')\n\n#Only %38.38 of passengers survived.","9d075b53":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntrain_test['FamilySize'] = train_test['SibSp'] + train_test['Parch'] + 1\n\ntrain['IsAlone'] = 0\ntrain.loc[train['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_test['IsAlone'] = 0\ntrain_test.loc[train_test['FamilySize'] == 1, 'IsAlone'] = 1\n\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1\n\n# We have two new feature.\n","6175c3de":"train_test.corr()","a9099aac":"plt.figure(figsize=(16,5))\ncorr_map = sns.heatmap(train_test.corr(),annot=True)","fa286e54":"fig, ax = plt.subplots(figsize=(12,6),ncols=2,nrows=2)\nax1 = sns.barplot(x=\"SibSp\",y=\"Survived\", data=train_test, ax = ax[0][0]);\nax2 = sns.barplot(x=\"Parch\",y=\"Survived\", data=train_test, ax = ax[0][1]);\nax3 = sns.countplot(x=\"SibSp\", data=train_test, ax = ax[1][0]);\nax4 = sns.countplot(x=\"Parch\", data=train_test, ax = ax[1][1]);\n#ax3.set_yscale('log')\n#ax4.set_yscale('log')\nncount = len(train[\"SibSp\"])\nfor p in ax3.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax3.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n            ha='center', va='bottom')\nfor p in ax4.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax4.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n            ha='center', va='bottom')","8092659d":"fig, ax = plt.subplots(figsize=(16,12),ncols=2,nrows=2)\nax1 = sns.barplot(x=\"Sex\",y=\"Survived\", data=train, ax = ax[0][0]);\nax2 = sns.barplot(x=\"Pclass\",y=\"Survived\", data=train, ax = ax[0][1]);\nax3 = sns.barplot(x=\"FamilySize\",y=\"Survived\", data=train, ax = ax[1][0]);\nax4 = sns.barplot(x=\"Embarked\",y=\"Survived\", data=train, ax = ax[1][1]);","62974ec4":"print(\"Missing Data in Train Set\"+'\\n')\ntotal = train.isnull().sum().sort_values(ascending = False)\npercentage = total\/len(train)*100\nmissing_data = pd.concat([total, percentage], axis=1, keys=['Total', '%'])\nprint(missing_data.head(3))\n\nprint('\\n')\nprint(\"Missing Data in Test Set\"+'\\n')\ntotal1 = test.isnull().sum().sort_values(ascending = False)\npercentage1 = total1\/len(test)*100\nmissing_data1 = pd.concat([total1, percentage1], axis=1, keys=['Total', '%'])\nprint(missing_data1.head(3))\n","e046dc57":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"magma\")\n#yticklabes=False removes left side labels.\n#cbar=False removes the colorbar.","c98943c9":"display(train_test[train_test.Embarked.isnull()])","c5930c47":"embarked_missing = train_test[(train_test[\"Sex\"] == \"female\") & (train_test[\"Pclass\"] ==1) & (train_test[\"Cabin\"].str.startswith('B')) \n                        & (train_test[\"Fare\"]>70) & (train_test[\"Fare\"]<100)]\n# Embarked_missing shows us passengers who have similar values.\nprint(embarked_missing[\"Embarked\"].value_counts())\n\n","2fd69e03":"train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\ntrain_test[\"Embarked\"] = train_test[\"Embarked\"].fillna(\"S\")","ff469326":"fare_missing = train_test[(train_test.Pclass == 3)]\ntest[\"Fare\"] = test[\"Fare\"].fillna(fare_missing.Fare.mean())\ntrain_test[\"Fare\"] = train_test[\"Fare\"].fillna(fare_missing.Fare.mean())\n","14918629":"fig, ax = plt.subplots(figsize=(16,12),ncols=2,nrows=1)\nax1 = sns.boxplot(x=\"Survived\", y=\"Age\", hue=\"Pclass\", data=train_test, ax = ax[0]);\nax2 = sns.boxplot(x=\"Pclass\", y=\"Age\", hue=\"Sex\", data=train_test, ax = ax[1]);","6cce95f4":"test_na_age_index = list(test[\"Age\"][test[\"Age\"].isnull()].index)\n\nfor i in test_na_age_index:\n    i_Pclass = test.iloc[i][\"Pclass\"]\n    mean_pclass = test[test.Pclass==i_Pclass]['Age'].mean()\n    age_pred = test[((test['Sex'] == test.iloc[i][\"Sex\"]) & (test['Pclass'] == test.iloc[i][\"Pclass\"]))][\"Age\"].mean()\n    if not np.isnan(age_pred) :\n        test['Age'].iloc[i] = age_pred\n    else :\n        test['Age'].iloc[i] = mean_pclass","779019b5":"train_na_age_index = list(train[\"Age\"][train[\"Age\"].isnull()].index)\n\nfor i in train_na_age_index:\n    i_Pclass = train.iloc[i][\"Pclass\"]\n    mean_pclass = train[train.Pclass==i_Pclass]['Age'].mean()\n    age_pred = train[((train['Survived'] == train.iloc[i][\"Survived\"])&(train['Sex'] == train.iloc[i][\"Sex\"]) & (train['Pclass'] == train.iloc[i][\"Pclass\"]))][\"Age\"].mean()\n    if not np.isnan(age_pred) :\n        train['Age'].iloc[i] = age_pred\n    else :\n        train['Age'].iloc[i] = mean_pclass","6315a4df":"train.drop([\"Cabin\"], axis = 1, inplace=True)\ntest.drop([\"Cabin\"], axis = 1, inplace=True)\ntrain_test.drop([\"Cabin\"], axis = 1, inplace=True)","899a2ecc":"train['Sex'] = train['Sex'].map({\"male\": 0, \"female\": 1})\ntest['Sex'] = test['Sex'].map({\"male\": 0, \"female\": 1})","22d97922":"one_hot_train = pd.get_dummies(train[\"Embarked\"], drop_first=True)\none_hot_test = pd.get_dummies(test[\"Embarked\"], drop_first=True)\n#This is called one hot encoding. It is same think with above cell. The reason behind dropping first column is effiency.\n#If both value is 0, it means it is dropped column. \none_hot_train","1320a8ad":"train.drop([\"Embarked\"], axis = 1, inplace=True)\ntrain = train.join(one_hot_train)\n\ntest.drop([\"Embarked\"], axis = 1, inplace=True)\ntest = test.join(one_hot_test)\n","3836e897":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\ntrain[\"Title\"] = pd.Series(dataset_title)\n\ndataset_title2 = [i.split(\",\")[1].split(\".\")[0].strip() for i in test[\"Name\"]]\ntest[\"Title\"] = pd.Series(dataset_title2)\n\n","c104a539":"train[\"Title\"] = train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain[\"Title\"] = train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain[\"Title\"] = train[\"Title\"].astype(int)\n\ntest[\"Title\"] = test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest[\"Title\"] = test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest[\"Title\"] = test[\"Title\"].astype(int)","e0ddb208":"pd.cut(train['Age'], 6)","d068df37":"train.loc[ train['Age'] <= 13, 'Age'] = 0,\ntrain.loc[(train['Age'] > 13) & (train['Age'] <= 27), 'Age'] = 1,\ntrain.loc[(train['Age'] > 27) & (train['Age'] <= 40), 'Age'] = 2,\ntrain.loc[(train['Age'] > 40) & (train['Age'] <= 53), 'Age'] = 3,\ntrain.loc[(train['Age'] > 53) & (train['Age'] <= 66), 'Age'] = 4,\ntrain.loc[ train['Age'] > 66, 'Age'] = 5\n","ee8184f4":"test.loc[ train['Age'] <= 13, 'Age'] = 0,\ntest.loc[(train['Age'] > 13) & (train['Age'] <= 27), 'Age'] = 1,\ntest.loc[(train['Age'] > 27) & (train['Age'] <= 40), 'Age'] = 2,\ntest.loc[(train['Age'] > 40) & (train['Age'] <= 53), 'Age'] = 3,\ntest.loc[(train['Age'] > 53) & (train['Age'] <= 66), 'Age'] = 4,\ntest.loc[ train['Age'] > 66, 'Age'] = 5\n","8ff774d3":"\ne = pd.get_dummies(train[\"FamilySize\"], drop_first=True)\ntrain.drop([\"FamilySize\"], axis = 1, inplace=True)\ntrain = train.join(e)\n\nf = pd.get_dummies(test[\"FamilySize\"], drop_first=True)\ntest.drop([\"FamilySize\"], axis = 1, inplace=True)\ntest = test.join(f)","8d160f15":"test.drop([\"Name\"], axis = 1, inplace=True)\ntrain.drop([\"Name\"], axis = 1, inplace=True)\ntest.drop([\"SibSp\"], axis = 1, inplace=True)\ntrain.drop([\"SibSp\"], axis = 1, inplace=True)\ntest.drop([\"Parch\"], axis = 1, inplace=True)\ntrain.drop([\"Parch\"], axis = 1, inplace=True)\ntest.drop([\"Ticket\"], axis = 1, inplace=True)\ntrain.drop([\"Ticket\"], axis = 1, inplace=True)\ntrain.drop([\"PassengerId\"], axis = 1, inplace=True)\ntest.drop([\"Fare\"], axis = 1, inplace=True)\ntrain.drop([\"Fare\"], axis = 1, inplace=True)","3f3b30d6":"X_train = train.drop(\"Survived\", axis=1)\ny_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()","929c7f2f":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint(logreg.score(X_train, y_train))","4b924bdb":"r_forest = RandomForestClassifier(n_estimators=200)\nr_forest.fit(X_train, y_train)\ny_pred = r_forest.predict(X_test)\nr_forest.score(X_train, y_train)\n\n","46cf53e7":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nknn.score(X_train, y_train)\n","eae7aa7d":"svc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\nsvc.score(X_train, y_train)","98908387":"d_tree = DecisionTreeClassifier()\nd_tree.fit(X_train, y_train)\ny_pred = d_tree.predict(X_test)\nd_tree.score(X_train, y_train)","602005f0":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\ngaussian.score(X_train, y_train)\n","60e72e77":"ada=AdaBoostClassifier(n_estimators=200,learning_rate=0.1)\nada.fit(X_train, y_train)\ny_pred = ada.predict(X_test)\nada.score(X_train, y_train)\n","a851bde5":"Submission = pd.DataFrame({ 'PassengerId': test[\"PassengerId\"],\n                            'Survived': y_pred })\nSubmission.to_csv(\"Submission1.csv\", index=False)","97057ac6":"Since fare is directly affected by pclass we filled with mean value of same pclass.","57d80791":"As all we know, libraries, languages are just tools. But deeper understanding can only be achieved by understanding the math behind. This notebook will not include this but I highly recommend the math behind. Basically, we need to convert our features to numbers. There are two type of features. Categorical and continuous. Since most of our features are categorical, we will convert continues one into categorical features. \n\nAlso we will create new features based by title and get rid of some of features.","d1bc78e7":"Observations\n* Pclass of passengers has significicant effect on survival rate.\n* Sex is another factor that affects survival rate. \n* Embarked may be mislead us, need deeper understanding.(Rate of people who embarked from C(Cherbourg) is using Pclass...etc)","1655ba98":"Now we will drop column embarked and add the one hot encoded data frame.","36e68d1c":"When we look back to correlation matrix, it seems, there are inverse proportion. So basically, people who are older tend to have better ticket class. When we think about what correlation matrix says, it is logical. Older people are richer so they buy higher class. Missing age values are worth more understanding, lets draw some boxplot.","54f25bc5":"It is really logical to create features from name title. I am inspired by Kaggle notebook called 'Titanic Top 4% with ensemble modeling'. Thank you for great work.","d57ec15e":"We get rid of the features that is not needed. ","063a1f15":"We have checked passengers who have similar values, and missing values are likely to be from Southampton because 7 out of 9 is embarked from Southampton. Lets assign the values.","973bd585":"As we can see, survival status is good indicator for filling ages. Since only train data have survival data. We are going to write two different filling condition. One of them is for train set, the other one is for test set.","24dc53fb":"**Reference**\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n","a7698b19":"We read the data set using pandas. After that, we've created data frame that include both data set. This will help us while we are filling blank cell in our data set.","7f8ee8bb":"In above cells, we find index of values which have missing age cell. Then, we filled the value based on our criteria. \nFor train set:\nWe filled missing values with looking Survival Status, Pclass and Sex\n\nFor train set:\nWe filled missing values with looking Pclass and Sex.\n","2f9a5eb1":"# Understanding Data","669ed195":"In our train set, we have 2 missing embarked value. We can fill them with most common one since it is not going to affect remarkably. \n\nIt is same with our test set for missing a fare value. \n\nBut I am going to fill these value with different mindset. It is not that common and it is almost impossible to use this in larger data set. ","77dfd659":"**Cabin**","a908a64e":"Before we dive in to graphs, it is a good idea to understand Correlation Matrix and Covariance Matrix. Covariance indicates the direction of the linear relation between variables.\nCorrelation measures strength and direction of linear relationship between columns we have.\n\nCorrelation is always between +1 and -1.\nIf the number is close to 1, it may have direct proportion between variables. If the number is close to -1, it may have inverse proportion between variables.\n\nCorrelation does not indicate causality, but not always. For example, lets assume we have a dataset about \ncrime rate and ice cream consumption. They are both increased in selected month so their correlation is close to 1. That doesnt mean there is a direct relation between two of them. \n\nIt is good for getting an idea. \nThe correlation matrix contains only columns with integer value. \nIf we want to contain other columns we need to convert into integers.","f46d8bcf":"Hello all,\nI am trying to use Feynman Technique to get deeper understanding the concept that I thought I know. That is why I am doing this with my broken language. Any question that will make me suffer is highly appreciated. ","84a8eae3":"We convert Name feature into a categorical data which may be understanded by machine learning algorithm. We will use get dummies to get better optimizing at the end.\n\nIt is time to convert continues data into categorical data.","a6fd5d80":"**Embarked**","806c0eb4":"It look way better. Isn't it?","8062ada2":"Since approximately 80% of cabin values are missing, it is logical to drop the feature. It will be not worth to fill and it may cause inaccuracy. ","2cfcb848":"Adding some features may be a good idea. We will come back to this part later.","680cdf49":"# Model, prediction and results","9d25548f":"Cell below is showing missing values on train set. It is not necessary to visualize but it is nice to have.","2f181156":"# Filling Values","fda7958b":"As we can see, passengers who dont have embarked data, have similarities. We can look for passengers who have similar feature values and fill embarked value according to that. ","c109dad1":"Now it is time to understand the data we are using. It is really important to analyse because causality is a concept that only people can do understand(for now). We will use that understanding while we are filling the blank datas. Lets write some code for understanding our data.","249b14a9":"# Added features explained","4941b919":"We convert family size into one hot encoding. It is not necessary but nice to have.","e1330f69":"As it can be seen, tree algorithms works better. Personally I am fan of Random Forest. All algorithms must be learned indivially. I am open for any constructive criticism. Have a wonderful journey!","3c42dc64":"This command help us to understand how to determine age categories.","7f71c864":"**Fare**","432231ef":"Now we know what features we have. In next steps, we may add or delete some of features. Who knows?","0ca549bf":"**Age**","fafa4cec":"Presentation is important so lets use seaborn library for Correlation Matrix.","de672ff8":"Sometimes, even if everything seems okay, we may not get correct results. Lets see with an example.","7ad4c91a":"We convert Sex features into 1,0.","8f878fae":"It is time to see our how well it work. \n\nSince all of the features are categorical I believe Tree Algorithms for better than the other. Lets find out which one is best.","4d8dae30":"**More visualization**","ff0163e3":" # Correlation Matrix\n","9a9aff22":"Barplots in first row, may make us think that there is relationship between Sibs, Parch and Survival Rate. BUT we neeed to look for sample size of higher parch and sibsp values. This is why we created second row.\n\nAs it can be seen, sample size is really small in higher values of parch and sibsp, so it may mislead us. \nThats why we created FamilySize feature and isAlone feature.\n\nI will drop theese columns after we fill the missing age columns.","bac4c456":"Visualization can be used to show output of above cell.","3cbdc9fa":"# Missing Data","3466d9d2":"# Converting data that machine learning can understand","a261fa82":"We convert our age feature into categorical so it may fit machine learning algorithm better.","37a9ee6f":"In above cell, we imported nesessary packages. We will understand, why they are needed."}}