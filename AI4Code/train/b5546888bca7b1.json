{"cell_type":{"d0fa02bd":"code","ea60e8ad":"code","f4a5f5f9":"code","8b215448":"code","144f6352":"code","24fb1e80":"code","5ba08c91":"code","db9bdf65":"code","862a0341":"code","8f80460f":"code","426a188a":"code","ec7b0982":"code","24654517":"code","18a13c00":"code","443d2015":"code","d8cf9939":"code","fd83df2a":"code","2c898fb2":"code","1bf969bd":"code","e2890c6e":"code","b5b9a5ab":"code","ce704264":"code","16c63cbf":"code","aecb9289":"code","72b548e9":"code","8644e2d8":"code","6ebe9f68":"code","9e2a6e67":"code","d7d07d42":"code","a82791c5":"code","08352b11":"code","ee6b20c9":"code","e9a87bc5":"code","84b8f73e":"code","660c348d":"code","995f2d6c":"code","87b4b714":"code","d8f5b802":"code","2f35cfe3":"code","8ae65a46":"code","e389ace6":"code","4867a736":"code","d23dad8e":"code","5bc89238":"code","98a3d7aa":"code","4c0aaec9":"code","184b3b3d":"code","0d8bf662":"code","ce52bae5":"code","4c750e36":"code","246e1343":"code","623b22d0":"code","b18421a4":"code","fb9d8e28":"code","5126e76d":"code","2b083f88":"code","ecc5e46f":"code","9952fb82":"code","9a619478":"code","f2d132b1":"code","77313914":"code","27f8fcba":"code","fe0a9591":"code","eaf1566d":"code","710b8ba6":"code","4d1a168a":"code","7ab199dd":"code","3c7ee211":"code","34ad8ad1":"code","76ccc5ce":"code","cdca25d9":"code","777c4fe2":"markdown","1b6b47e5":"markdown","060dfc63":"markdown","f1104314":"markdown","3a426b46":"markdown","ae3a3700":"markdown","4a0ab4cb":"markdown","6dd577f3":"markdown","46a1c0fe":"markdown","e8fdca13":"markdown","731506db":"markdown","a520a4e9":"markdown","6cd9d89c":"markdown","86f1da9e":"markdown","8633cca3":"markdown","b025c749":"markdown","27e6c198":"markdown","f4bcd822":"markdown","76b70226":"markdown","a34a7c8e":"markdown","d0519ed0":"markdown","b12dedd1":"markdown","eab95c3a":"markdown","9e1c591f":"markdown","9af0a58c":"markdown","240aee72":"markdown","fddfa7db":"markdown","a8bd64da":"markdown","399cadc5":"markdown","60d713c3":"markdown","f52151fe":"markdown","571f4a0b":"markdown","0688e581":"markdown","a0ddb63f":"markdown","85ea3d1e":"markdown","fcbbcd91":"markdown","16bbe3f7":"markdown","59433352":"markdown","1a041d63":"markdown","c57b51db":"markdown","a5444cb8":"markdown","4b7beccb":"markdown","c51a33e3":"markdown","375724c2":"markdown","3d68b3a7":"markdown","8fd8ba77":"markdown"},"source":{"d0fa02bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea60e8ad":"import matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nBATCH_SIZE = 128\nEPOCHS = 15","f4a5f5f9":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","8b215448":"train.head()","144f6352":"train.shape","24fb1e80":"test.head()","5ba08c91":"train.dtypes.unique()","db9bdf65":"# with open(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\") as fin:\n    # print(fin.read())","862a0341":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","8f80460f":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","426a188a":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","ec7b0982":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","24654517":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","18a13c00":"sns.set_theme(rc = {'grid.linewidth': 0.5,\n                    'axes.linewidth': 0.75, 'axes.facecolor': '#fff3e9', 'axes.labelcolor': '#6b1000',\n                    'figure.facecolor': '#f7e7da',\n                    'xtick.labelcolor': '#6b1000', 'ytick.labelcolor': '#6b1000'})","443d2015":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice', 'GarageArea', 'TotRmsAbvGrd'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","d8cf9939":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","fd83df2a":"data_missing = all_data.count().loc[all_data.count() < all_data.shape[0]].sort_values(ascending = False)\n\nwith plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}): \n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n    sns.barplot(x = data_missing.values, y = data_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","2c898fb2":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","1bf969bd":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","e2890c6e":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","b5b9a5ab":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","ce704264":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","16c63cbf":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","aecb9289":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","72b548e9":"for col in ('GarageYrBlt', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","8644e2d8":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","6ebe9f68":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","9e2a6e67":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","d7d07d42":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","a82791c5":"all_data = all_data.drop(['Utilities'], axis=1)","08352b11":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","ee6b20c9":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","e9a87bc5":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","84b8f73e":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","660c348d":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","995f2d6c":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","87b4b714":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","d8f5b802":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","2f35cfe3":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","8ae65a46":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","e389ace6":"# all_data.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)","4867a736":"from scipy import stats\nfrom scipy.stats import norm, skew\n\n\n# histogram and normal probability plot\nsns.distplot(y_train, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y_train, plot=plt)","d23dad8e":"# applying log transformation\ny_train = np.log(y_train)","5bc89238":"# transformed histogram and normal probability plot\nsns.distplot(y_train, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y_train, plot=plt)","98a3d7aa":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","4c0aaec9":"#histogram and normal probability plot\nsns.distplot(all_data['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data['GrLivArea'], plot=plt)","184b3b3d":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","0d8bf662":"#histogram and normal probability plot\nsns.distplot(all_data['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data['GrLivArea'], plot=plt)","ce52bae5":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","4c750e36":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","246e1343":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb","623b22d0":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","b18421a4":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","fb9d8e28":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","5126e76d":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","2b083f88":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","ecc5e46f":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","9952fb82":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","9a619478":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f2d132b1":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","77313914":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","27f8fcba":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fe0a9591":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eaf1566d":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","710b8ba6":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","4d1a168a":"averaged_models = AveragingModels(models = (ENet, GBoost, lasso, model_xgb, model_lgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\"Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7ab199dd":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","3c7ee211":"averaged_models.fit(train.values, y_train)\nav_train_pred = averaged_models.predict(train.values)\nprint(rmsle(y_train, av_train_pred))","34ad8ad1":"av_test_pred = np.expm1(averaged_models.predict(test.values))","76ccc5ce":"sample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsample_submission","cdca25d9":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = av_test_pred\nsub.to_csv('submission.csv',index=False)","777c4fe2":"Is there any remaining missing value ?","1b6b47e5":"- FireplaceQu : data description says NA means \"no fireplace\"","060dfc63":"**'SalePrice' correlation matrix**","f1104314":"## Transforms\n\nCopied from https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#2.-First-things-first:-analysing-'SalePrice'","3a426b46":"# Data processing","ae3a3700":"- KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","4a0ab4cb":"**Let's combine the data for further processing purposes**","6dd577f3":"- Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","46a1c0fe":"- Functional : data description says NA means typical","e8fdca13":"- Alley : data description says NA means \"no alley access\"","731506db":"Drop Id feature","a520a4e9":"- BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement","6cd9d89c":"## Outliers\n\nAccording to https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard there are a few outliers present in the training data.","86f1da9e":"# Submission","8633cca3":"## Skewed features\n\nLet's apply this tranform to the features too.","b025c749":"- MSSubClass : Na most likely means No building class. We can replace missing values with None","27e6c198":"- LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","f4bcd822":"## Missing data\n\nThanks a lot to https:\/\/www.kaggle.com\/anttrinh\/house-prices-regression#Stacked-Regressions-to-predict-House-Prices","76b70226":"**Getting dummy categorical features**","a34a7c8e":"We use the scipy function boxcox1p which computes the Box-Cox transformation of 1+x\n\nNote that setting \u03bb=0 is equivalent to log1p used above for the target variable.\n\nSee [this page](https:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html) for more details on Box Cox Transformation as well as the [scipy function's page](https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html)","d0519ed0":"#### Box Cox Transformation of (highly) skewed features\n\nWe don't want to fix all this features one by one, so we will apply transform to all skewed features. Also let's take a look at the 'GrLivArea' before and after transform just to be sure that it worked. ","b12dedd1":"- MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'","eab95c3a":"**Getting the new train and test sets.**","9e1c591f":"Scores","9af0a58c":"- MiscFeature : data description says NA means \"no misc feature\"","240aee72":"- GarageYrBlt and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)","fddfa7db":"- GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None","a8bd64da":"- Fence : data description says NA means \"no fence\"","399cadc5":"Let's see what happend to 'GrLivArea'.","60d713c3":"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. ","f52151fe":"#### Transforming some numerical variables that are really categorical","571f4a0b":"# Modeling","0688e581":"## Data correlation","a0ddb63f":"- Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","85ea3d1e":"Looks great!","fcbbcd91":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","16bbe3f7":"#### Label Encoding some categorical variables that may contain information in their ordering set","59433352":"- SaleType : Fill in again with most frequent which is \"WD\"","1a041d63":"### Inputting missing variables\n\nCoppied from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard. We impute them by proceeding sequentially through features with missing values.\n\n- PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","c57b51db":"#### Adding one more important feature\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","a5444cb8":"Here is some explanation provided by [this article](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#2.-First-things-first:-analysing-'SalePrice'):\n\n\"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n- 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n- 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n- 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n- 'FullBath'?? Really?\n- 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n- Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.\"\n\n**According to [this article](https:\/\/www.kaggle.com\/anttrinh\/house-prices-regression#Stacked-Regressions-to-predict-House-Prices), droping these highly correlated features is actually decreasing score, so we won't do that.**","4b7beccb":"The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\n- Histogram - Kurtosis and skewness.\n- Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.","c51a33e3":"- BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.","375724c2":"- Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","3d68b3a7":"- MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. ","8fd8ba77":"**Let's see how many skewed features are in dataset.**\n\nCoppied from https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard"}}