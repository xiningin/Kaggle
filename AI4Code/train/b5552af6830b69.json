{"cell_type":{"b5b1bfdd":"code","3b0591cd":"code","5d4de352":"code","a34477f4":"code","91f13e41":"code","9f612b4e":"code","860e80e4":"code","90822af3":"code","d52477ce":"code","dcf402aa":"code","87f36fc8":"code","5a414068":"code","5599e41a":"code","1da5eaaa":"code","37036879":"code","134e93fc":"code","009a366d":"code","86c21451":"code","550f2fd7":"code","75204efd":"code","18ff8b5c":"code","aa138a54":"code","47df7b9c":"code","a6404ba4":"code","6fd9b2f2":"code","a2bfe116":"code","79252c95":"code","a4e65647":"code","b88185ac":"markdown","b9364886":"markdown","43b808f7":"markdown","3ec3c5b9":"markdown","6ccaa108":"markdown","db67856d":"markdown","a98402fb":"markdown","8b38ae64":"markdown","f676c78c":"markdown","98b54a1a":"markdown","bf14d36e":"markdown","06f249de":"markdown","af408c48":"markdown","cd1d5dbc":"markdown","4a2ccada":"markdown","68fc8a1c":"markdown","1cb05c7d":"markdown"},"source":{"b5b1bfdd":"import pandas as pd\nimport numpy as np\nimport random as rnd\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\n","3b0591cd":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","5d4de352":"train.head(10)\n","a34477f4":"\ntrain.info()","91f13e41":"plt.figure()\n_, axis = plt.subplots(2,3,figsize=(12, 8), constrained_layout=True)\nsns.countplot(x='Survived', hue='Sex', data=train,ax=axis[0,0])\naxis[0,0].set_title('Survival by Sex')\n\nsns.countplot(x='Survived', hue='Pclass', data=train, ax=axis[0,1])\naxis[0,1].set_title('Survival by Pclass')\n\nsns.countplot(x='Survived', hue='SibSp', data=train, ax = axis[0,2])\naxis[0,2].set_title('Survival by Sibsp')\naxis[0,2].legend(loc='upper right')\n\nsns.countplot(x='Survived', hue='Parch', data=train, ax=axis[1,0])\naxis[1,0].set_title('Survival by Parch')\naxis[1,0].legend(loc='upper right')\n\nsns.countplot(x='Survived', hue='Embarked', data=train, ax=axis[1,1])\naxis[1,1].set_title('Survival by Embarked')\n\naxis[1,2].axis('off')","9f612b4e":"plt.figure()\n_, axis = plt.subplots(1,2,figsize=(10, 4), constrained_layout=True)\n\nsns.kdeplot(x='Age', hue='Survived', data=train,fill=True, ax=axis[0])\naxis[0].set_title('Survival by Age')\n\nsns.kdeplot(x='Fare', hue='Survived', data=train,fill=True, ax=axis[1])\naxis[1].set_title('Survival by Fare')","860e80e4":"age = train.groupby(['Sex', 'Pclass'])['Age']\\\n .transform(lambda x: x.fillna(x.mean()))\n\ntrain[\"Age1\"] = age","90822af3":"train[\"Title\"] = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ndef GetIsChild(x):\n    if x[\"Title\"] == \"Master\":\n       return 1\n    if pd.notnull(x[\"Age\"]):\n       if int(x[\"Age\"]) <= 18:\n            return 1\n    return 0    \n\nisChild = train[[\"Title\",\"Age\"]].apply(lambda x: GetIsChild(x), axis=1)\n\ntrain[\"IsChild\"] = isChild\n\nage = train.groupby([\"Sex\",\"IsChild\"])[\"Age\"].transform(lambda x: x.fillna(x.mean()))\n\ntrain[\"Age2\"] = age\n","d52477ce":"train.fillna(train[\"Embarked\"].value_counts().index[0], inplace=True)\n\nsex = pd.get_dummies(train[\"Sex\"])\ntrain = train.join(sex)\nembarked = pd.get_dummies(train[\"Embarked\"])\ntrain = train.join(embarked)","dcf402aa":"age_categorized = pd.cut(train[\"Age1\"],7,labels=False)\ntrain[\"Age_categorized\"] = age_categorized\nfare_categorized = pd.cut(train[\"Fare\"],10,labels=False)\ntrain[\"Fare_categorized\"] = fare_categorized","87f36fc8":"data1 = train[[\"Survived\",\"Pclass\",\"SibSp\",\"Parch\",\"Fare\",\"Age1\",\"female\",\"C\",\"Q\"]]\n\ndata2 = train[[\"Survived\",\"Pclass\",\"SibSp\",\"Parch\",\"Fare\",\"Age2\",\"female\",\"C\",\"Q\"]]\n\ndata3 = train[[\"Survived\",\"Pclass\",\"SibSp\",\"Parch\",\"Fare_categorized\",\"Age_categorized\",\"female\",\"C\",\"Q\"]]\n\ndata4 = train[[\"Survived\",\"Pclass\",\"SibSp\",\"Parch\",\"Age1\",\"female\",\"C\",\"Q\"]]","5a414068":"def LR_Adam_optimization(par, learning_rate):\n\n    W = par[\"W\"]\n    b = par[\"b\"]\n    dW = par[\"dW\"]\n    db = par[\"db\"]\n    VdW = par[\"VdW\"]\n    Vdb = par[\"Vdb\"]\n    SdW = par[\"SdW\"]\n    Sdb = par[\"Sdb\"]\n\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 0.000000001\n    \n    VdW = (beta1*VdW + (1-beta1)*dW)\n    Vdb = (beta1*Vdb + (1-beta1)*db)\n    SdW = (beta2*SdW + (1-beta2)*dW*dW)\n    Sdb = (beta2*Sdb + (1-beta2)*db*db)\n    \n    VdW_corr = VdW\/(1-beta1)\n    Vdb_corr = Vdb\/(1-beta1)\n    SdW_corr = SdW\/(1-beta2)\n    Sdb_corr = Sdb\/(1-beta2)\n    \n    W = W - learning_rate*VdW_corr\/(np.sqrt(SdW_corr+epsilon))\n    b = b - learning_rate*Vdb_corr\/(np.sqrt(Sdb_corr+epsilon))\n    \n    par[\"W\"] = W\n    par[\"b\"] = b\n    par[\"VdW\"] = VdW\n    par[\"Vdb\"] = Vdb\n    par[\"SdW\"] = SdW\n    par[\"Sdb\"] = Sdb\n    \n    return par\n\ndef NN_Adam_optimization(par, learning_rate):\n\n    W1 = par[\"W1\"]\n    b1 = par[\"b1\"]\n    dW1 = par[\"dW1\"]\n    db1 = par[\"db1\"]\n    VdW1 = par[\"VdW1\"]\n    Vdb1 = par[\"Vdb1\"]\n    SdW1 = par[\"SdW1\"]\n    Sdb1 = par[\"Sdb1\"]\n    \n    W2 = par[\"W2\"]\n    b2 = par[\"b2\"]\n    dW2 = par[\"dW2\"]\n    db2 = par[\"db2\"]\n    VdW2 = par[\"VdW2\"]\n    Vdb2 = par[\"Vdb2\"]\n    SdW2 = par[\"SdW2\"]\n    Sdb2 = par[\"Sdb2\"]\n\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 0.000000001\n    \n    VdW1 = (beta1*VdW1 + (1-beta1)*dW1)\n    Vdb1 = (beta1*Vdb1 + (1-beta1)*db1)\n    SdW1 = (beta2*SdW1 + (1-beta2)*dW1*dW1)\n    Sdb1 = (beta2*Sdb1 + (1-beta2)*db1*db1)\n    \n    VdW2= (beta1*VdW2 + (1-beta1)*dW2)\n    Vdb2 = (beta1*Vdb2 + (1-beta1)*db2)\n    SdW2 = (beta2*SdW2 + (1-beta2)*dW2*dW2)\n    Sdb2 = (beta2*Sdb2 + (1-beta2)*db2*db2)\n    \n    VdW1_corr = VdW1\/(1-beta1)\n    Vdb1_corr = Vdb1\/(1-beta1)\n    SdW1_corr = SdW1\/(1-beta2)\n    Sdb1_corr = Sdb1\/(1-beta2)\n    \n    VdW2_corr = VdW2\/(1-beta1)\n    Vdb2_corr = Vdb2\/(1-beta1)\n    SdW2_corr = SdW2\/(1-beta2)\n    Sdb2_corr = Sdb2\/(1-beta2)\n    \n    W1 = W1 - learning_rate*VdW1_corr\/(np.sqrt(SdW1_corr+epsilon))\n    b1 = b1 - learning_rate*Vdb1_corr\/(np.sqrt(Sdb1_corr+epsilon))\n    W2 = W2 - learning_rate*VdW2_corr\/(np.sqrt(SdW2_corr+epsilon))\n    b2 = b2 - learning_rate*Vdb2_corr\/(np.sqrt(Sdb2_corr+epsilon))\n    \n    \n    par[\"W1\"] = W1\n    par[\"b1\"] = b1\n    par[\"VdW1\"] = VdW1\n    par[\"Vdb1\"] = Vdb1\n    par[\"SdW1\"] = SdW1\n    par[\"Sdb1\"] = Sdb1\n    \n    par[\"W2\"] = W2\n    par[\"b2\"] = b2\n    par[\"VdW2\"] = VdW2\n    par[\"Vdb2\"] = Vdb2\n    par[\"SdW2\"] = SdW2\n    par[\"Sdb2\"] = Sdb2\n    \n    return par\n\ndef relu(x):\n    return np.maximum(x,0)\n\ndef relu_der(x):\n    x[x<=0] = 0\n    x[x>0] = 1\n    return x\n        \n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n","5599e41a":"def NN_one_step(X,Y,par, learning_rate=0.005):\n    \n    M,_ = Y.shape\n    \n    W1 = par[\"W1\"]\n    b1 = par[\"b1\"]\n    W2 = par[\"W2\"]\n    b2 = par[\"b2\"]\n    \n    Z1 = np.dot(X,W1.T)+b1\n    A1 = relu(Z1)\n    \n    Z2 = np.dot(A1,W2.T)+b2\n    A2 = sigmoid(Z2)\n    \n    L = (-1)*( Y*np.log(A2+np.finfo(float).eps) + (1 - Y)*np.log(1-A2+np.finfo(float).eps))\n    loss = np.sum(L)\/M\n    \n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2.T,A1)\/M\n    db2 = np.sum(dZ2, axis = 0, keepdims=True)\/M\n    \n    \n    dA1 = np.dot(dZ2,W2)\n    dZ1 = dA1*relu_der(Z1)\n    dW1 = np.dot(dZ1.T,X)\/M\n    db1 = np.sum(dZ1, axis = 0, keepdims=True)\/M\n    \n    par[\"dW1\"] = dW1\n    par[\"db1\"] = db1\n    par[\"dW2\"] = dW2\n    par[\"db2\"] = db2\n    \n    par = NN_Adam_optimization(par, learning_rate)\n    \n    return (par, loss)\n\ndef NN_Iterate(X,Y,iterations=3000):\n    M, N = X.shape\n    \n    neurons = 8\n    \n    par = {}\n    \n    par[\"W1\"] = np.random.rand(neurons,N)\n    par[\"b1\"] = np.zeros([1,neurons])\n    par[\"W2\"] = np.random.rand(1,neurons)\n    par[\"b2\"] = 0\n    par[\"VdW1\"] = np.zeros([neurons,N])\n    par[\"Vdb1\"] = np.zeros([1,neurons])\n    par[\"SdW1\"] = np.zeros([neurons,N])\n    par[\"Sdb1\"] = np.zeros([1,neurons])\n    par[\"VdW2\"] = np.zeros([1,neurons])\n    par[\"Vdb2\"] = 0\n    par[\"SdW2\"] = np.zeros([1,neurons])\n    par[\"Sdb2\"] = 0\n    \n    learning_rate = 0.01\n    \n    for i in range(1,iterations):\n        par,loss = NN_one_step(X, Y, par, learning_rate)\n       \n        if i%100 == 0:\n            learning_rate = learning_rate*0.98\n               \n    return par[\"W1\"],par[\"b1\"],par[\"W2\"],par[\"b2\"],loss\n\ndef NN_Prediction(X,W1,b1,W2,b2):\n    M,_ = X.shape\n    Z1 = np.dot(X,W1.T)+b1\n    A1 = relu(Z1)\n    \n    Z2 = np.dot(A1,W2.T)+b2\n    A2 = sigmoid(Z2)\n    \n    pred = np.asarray(list(map(convert, A2)))\n    \n    return pred","1da5eaaa":"def LR_one_step(X,Y, par, learning_rate=0.01):\n    \n    W = par[\"W\"]\n    b = par[\"b\"]\n    \n    M,_ = Y.shape\n    \n    Z = np.dot(X,W.T)+b\n    A = sigmoid(Z)\n    \n    dZ = A - Y\n    dW = np.dot(dZ.T,X)\/M\n    db = np.sum(dZ)\/M\n    L = (-1)*( Y*np.log(A+np.finfo(float).eps) + (1 - Y)*np.log(1-A+np.finfo(float).eps))\n    \n    loss = np.sum(L)\/M\n    \n    par[\"dW\"] = dW\n    par[\"db\"] = db\n    \n    par = LR_Adam_optimization(par, learning_rate)\n    \n    return (par,loss)\n                 \ndef LR_Iterate(X,Y,iterations=3000):\n    M, N = X.shape\n    \n    par = {}\n    \n    par[\"W\"] = np.zeros([1,N])\n    par[\"b\"] = 0\n    par[\"VdW\"] = np.zeros([1,N])\n    par[\"Vdb\"] = 0\n    par[\"SdW\"] = np.zeros([1,N])\n    par[\"Sdb\"] = 0\n    \n    \n    learning_rate=0.005\n    for i in range(1,iterations):\n        par,loss = LR_one_step(X, Y, par, learning_rate)\n        \n    return par[\"W\"],par[\"b\"],loss\n\ndef convert(x):\n    if (x>0.5):\n        return 1\n    else :\n        return 0\n\ndef LR_Prediction(X,W,b):\n    M,_ = X.shape\n    Z = np.dot(X,W.T)+b\n    A = sigmoid(Z)\n    \n    prediction = np.asarray(list(map(convert, A)))\n    \n    return prediction\n    \n","37036879":"def SplitData(data):\n    X = data.to_numpy()\n\n    M,N = X.shape\n    Y = X[:,0].reshape((M,1))\n    X = X[:,1:N]\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n    train_index, test_index = next(sss.split(X, Y))\n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]\n                                        \n    return X_train,Y_train, X_test, Y_test                                    \n    \ndef Run(data):\n    LR_accuracy=[]\n    LR_precision=[]\n    LR_recall=[]\n    LR_f1=[]\n    \n    NN_accuracy=[]\n    NN_precision=[]\n    NN_recall=[]\n    NN_f1=[]\n    \n    for i in range(0,10):\n        \n        X_train,Y_train, X_test, Y_test = SplitData(data)\n                                        \n        W,b,loss = LR_Iterate(X_train,Y_train)\n        LR_Y_prediction = LR_Prediction(X_test, W, b) \n        \n        LR_accuracy.append(accuracy_score(Y_test,LR_Y_prediction))\n        LR_precision.append(precision_score(Y_test,LR_Y_prediction))\n        LR_recall.append(recall_score(Y_test,LR_Y_prediction))\n        LR_f1.append(f1_score(Y_test,LR_Y_prediction))\n        \n        W1, b1, W2, b2, loss = NN_Iterate(X_train,Y_train)\n        NN_Y_prediction = NN_Prediction(X_test,W1,b1,W2,b2)\n        \n        NN_accuracy.append(accuracy_score(Y_test,NN_Y_prediction))\n        NN_precision.append(precision_score(Y_test,NN_Y_prediction))\n        NN_recall.append(recall_score(Y_test,NN_Y_prediction))\n        NN_f1.append(f1_score(Y_test,NN_Y_prediction))\n        \n    \n    print(\" \")\n    print(\"LR accuracy:  {:.3f}\".format(np.mean(LR_accuracy)))  \n    print(\"LR presicion: {:.3f}\".format(np.mean(LR_precision))) \n    print(\"LR recall:    {:.3f}\".format(np.mean(LR_recall)))  \n    print(\"LR f1:        {:.3f}\".format(np.mean(LR_f1)))  \n    print(\"\")\n    print(\"NN accuracy:  {:.3f}\".format(np.mean(NN_accuracy)))  \n    print(\"NN presicion: {:.3f}\".format(np.mean(NN_precision))) \n    print(\"NN recall:    {:.3f}\".format(np.mean(NN_recall)))  \n    print(\"NN f1:        {:.3f}\".format(np.mean(NN_f1))) \n    ","134e93fc":"print(\"Data1\")\nRun(data1)\n\nprint(\" \")\nprint(\"Data2\")\nRun(data2)\n\nprint(\" \")\nprint(\"Data3\")\nRun(data3)\n\nprint(\" \")\nprint(\"Data4\")\nRun(data4)","009a366d":"def DecisionTree(data):\n    X_train,Y_train,X_test,Y_test = SplitData(data)\n    DT = DecisionTreeClassifier(random_state=42)\n    DT.fit(X_train,Y_train)\n    \n    acc = list()\n    for max_depth in range(1, DT.tree_.max_depth):\n        DT.set_params(max_depth=max_depth)\n        DT.fit(X_train, np.ravel(Y_train))\n     \n        Y_prediction = DT.predict(X_test)\n        acc.append(pd.Series({'max_depth': max_depth, 'acc': accuracy_score(Y_test, Y_prediction)}))\n    \n    acc_score = pd.concat(acc, axis=1).T.set_index('max_depth')\n    return acc_score\n      \n    \ndef GradientBoosting(data):\n    X_train,Y_train,X_test,Y_test = SplitData(data)\n    GBC = GradientBoostingClassifier(random_state=42)\n    \n    acc = list()\n    for n_trees in [15, 20, 30, 40, 50, 75, 100, 125, 150, 200, 300, 400]:\n        GBC.set_params(n_estimators=n_trees)\n        GBC.fit(X_train, np.ravel(Y_train))\n     \n        Y_prediction = GBC.predict(X_test)\n        acc.append(pd.Series({'trees': n_trees, 'acc': accuracy_score(Y_test, Y_prediction)}))\n    \n    acc_score = pd.concat(acc, axis=1).T.set_index('trees')\n    return acc_score\n    \ndef AdaBoost(data):\n    X_train,Y_train,X_test,Y_test = SplitData(data)\n    ABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n\n    \n    acc = list()\n    for n_trees in [50, 75, 100,125, 150, 200, 300, 400]:\n        ABC.set_params(n_estimators=n_trees)\n        ABC.fit(X_train, np.ravel(Y_train))\n     \n        Y_prediction = ABC.predict(X_test)\n        acc.append(pd.Series({'trees': n_trees, 'acc': accuracy_score(Y_test, Y_prediction)}))\n    \n    acc_score = pd.concat(acc, axis=1).T.set_index('trees')\n    return acc_score    \n    \ndef RandomForest(data):\n    X_train,Y_train,X_test,Y_test = SplitData(data)\n    RF = RandomForestClassifier(random_state=42, \n                                warm_start=True,\n                                n_jobs=-1)\n    \n    acc = list()\n    for n_trees in [15, 20, 30, 40, 50, 75, 100, 125, 150, 200, 300, 400]:\n        RF.set_params(n_estimators=n_trees)\n        RF.fit(X_train, np.ravel(Y_train))\n     \n        Y_prediction = RF.predict(X_test)\n        acc.append(pd.Series({'trees': n_trees, 'acc': accuracy_score(Y_test, Y_prediction)}))\n    \n    acc_score = pd.concat(acc, axis=1).T.set_index('trees')\n    return acc_score\n\ndef ExtraTrees(data):\n    X_train,Y_train,X_test,Y_test = SplitData(data)\n    ET = ExtraTreesClassifier(random_state=42, \n                              warm_start=True,\n                              bootstrap=True,\n                              n_jobs=-1)\n    \n    acc = list()\n    for n_trees in [15, 20, 30, 40, 50, 75, 100, 125, 150, 200, 300, 400, 500]:\n        ET.set_params(n_estimators=n_trees)\n        ET.fit(X_train, np.ravel(Y_train))\n     \n        Y_prediction = ET.predict(X_test)\n        acc.append(pd.Series({'trees': n_trees, 'acc': accuracy_score(Y_test, Y_prediction)}))\n    \n    acc_score = pd.concat(acc, axis=1).T.set_index('trees')\n    return acc_score\n    \n    \n    \ndef PrintAccuracyGraph(ax, acc, title):\n    ax.plot(acc, marker='o', data=acc) \n    ax.set(ylabel='accuracy');\n    ax.set(xlabel=acc.index.name)\n    ax.set_title(title)\n    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n    ax.text(0.35,0.95,\"max accuracy: {0:.3f} \\nfor {2} {1} \".format(acc.max().to_numpy()[0],\n            acc.idxmax().to_numpy()[0],acc.index.name),\n            transform=ax.transAxes, fontsize=12, verticalalignment='top', bbox=props)\n    \n    \n        ","86c21451":"print(\"Data1\")    \nplt.figure()\n_, axis = plt.subplots(5,4,figsize=(15, 15), constrained_layout=True)\n\n#Data1\nacc_score = DecisionTree(data1)\nPrintAccuracyGraph(axis[0,0], acc_score, \"Decision Tree - Data1\")\nacc_score = GradientBoosting(data1)\nPrintAccuracyGraph(axis[1,0],acc_score, \"Gradient Boosting - Data1\")\nacc_score = AdaBoost(data1)\nPrintAccuracyGraph(axis[2,0],acc_score, \"Ada Boost - Data1\")\nacc_score = RandomForest(data1)\nPrintAccuracyGraph(axis[3,0],acc_score, \"Random Forest - Data1\")\nacc_score = ExtraTrees(data1)\nPrintAccuracyGraph(axis[4,0], acc_score, \"Extra Trees - Data1\")\n\n#Data2\nacc_score = DecisionTree(data2)\nPrintAccuracyGraph(axis[0,1],acc_score, \"Decision Tree - Data2\")\nacc_score = GradientBoosting(data2)\nPrintAccuracyGraph(axis[1,1],acc_score, \"Gradient Boosting - Data2\")\nacc_score = AdaBoost(data2)\nPrintAccuracyGraph(axis[2,1],acc_score, \"Ada Boost - Data2\")\nacc_score = RandomForest(data2)\nPrintAccuracyGraph(axis[3,1],acc_score, \"Random Forest - Data2\")\nacc_score = ExtraTrees(data2)\nPrintAccuracyGraph(axis[4,1],acc_score, \"Extra Trees - Data2\")\n\n#Data3\nacc_score = DecisionTree(data3)\nPrintAccuracyGraph(axis[0,2],acc_score, \"Decision Tree - Data3\")\nacc_score = GradientBoosting(data3)\nPrintAccuracyGraph(axis[1,2],acc_score, \"Gradient Boosting - Data3\")\nacc_score = AdaBoost(data3)\nPrintAccuracyGraph(axis[2,2],acc_score, \"Ada Boost - Data3\")\nacc_score = RandomForest(data3)\nPrintAccuracyGraph(axis[3,2],acc_score, \"Random Forest - Data3\")\nacc_score = ExtraTrees(data3)\nPrintAccuracyGraph(axis[4,2],acc_score, \"Extra Trees - Data3\")\n\n#Data4\nacc_score = DecisionTree(data4)\nPrintAccuracyGraph(axis[0,3],acc_score, \"Decision Tree - Data4\")\naxis[0,3].set(xlabel='max_depth')\nacc_score = GradientBoosting(data4)\nPrintAccuracyGraph(axis[1,3],acc_score, \"Gradient Boosting - Data4\")\nacc_score = AdaBoost(data4)\nPrintAccuracyGraph(axis[2,3],acc_score, \"Ada Boost - Data4\")\nacc_score = RandomForest(data4)\nPrintAccuracyGraph(axis[3,3],acc_score, \"Random Forest - Data4\")\nacc_score = ExtraTrees(data4)\nPrintAccuracyGraph(axis[4,3],acc_score, \"Extra Trees - Data4\")\nplt.show()","550f2fd7":"X_train,Y_train,X_test,Y_test = SplitData(data1)\n\nestimator = []\nestimator.append(('GBC',GradientBoostingClassifier(n_estimators=125, random_state=42)))\nestimator.append(('ABC', AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=100)))\nestimator.append(('RFC',RandomForestClassifier(n_estimators=75, random_state=42, warm_start=True,\n                        n_jobs=-1))) \n  \n# Voting Classifier with hard voting\nvot_hard1 = VotingClassifier(estimators = estimator, voting ='hard')\nvot_hard1.fit(X_train, np.ravel(Y_train))\nY_prediction = vot_hard1.predict(X_test)\n\nprint(\"Data1\")\nprint(\"Accuracy: {0:.3f}\".format(accuracy_score(Y_test, Y_prediction)))\nprint(\"f1: {0:.3f}\".format(f1_score(Y_test, Y_prediction)))","75204efd":"X_train,Y_train,X_test,Y_test = SplitData(data2)\n\nestimator = []\nestimator.append(('GBC',GradientBoostingClassifier(n_estimators=30, random_state=42)))\nestimator.append(('RFC',RandomForestClassifier(n_estimators=100,  \n                                random_state=42, \n                                warm_start=True,\n                                n_jobs=-1)))\nestimator.append(('ETC',ExtraTreesClassifier(n_estimators=15,  \n                              random_state=42, \n                              warm_start=True,\n                              bootstrap=True,\n                              n_jobs=-1)))\n  \n# Voting Classifier with osft voting\nvot_hard2 = VotingClassifier(estimators = estimator, voting ='soft')\nvot_hard2.fit(X_train, np.ravel(Y_train))\nY_prediction = vot_hard2.predict(X_test)\nacc = accuracy_score(Y_test, Y_prediction)\nprint(\"Data2\")\nprint(\"Accuracy: {0:.3f}\".format(accuracy_score(Y_test, Y_prediction)))\nprint(\"f1: {0:.3f}\".format(f1_score(Y_test, Y_prediction)))","18ff8b5c":"X_train,Y_train,X_test,Y_test = SplitData(data3)\n\nestimator = []\nestimator.append(('DTC', DecisionTreeClassifier(max_depth=5, random_state=42)))\nestimator.append(('GBC',GradientBoostingClassifier(n_estimators=40, random_state=42)))\nestimator.append(('ABC', AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=50)))\n                            \n  \n# Voting Classifier with hard voting\nvot_hard3 = VotingClassifier(estimators = estimator, voting ='hard')\nvot_hard3.fit(X_train, np.ravel(Y_train))\nY_prediction = vot_hard3.predict(X_test)\nacc = accuracy_score(Y_test, Y_prediction)\nprint(\"Data3\")\nprint(\"Accuracy: {0:.3f}\".format(accuracy_score(Y_test, Y_prediction)))\nprint(\"f1: {0:.3f}\".format(f1_score(Y_test, Y_prediction)))","aa138a54":"X_train,Y_train,X_test,Y_test = SplitData(data4)\n\nestimator = []\nestimator.append(('DTC', DecisionTreeClassifier(max_depth=13, random_state=42)))\nestimator.append(('GBC',GradientBoostingClassifier(n_estimators=20, random_state=42)))\nestimator.append(('ABC', AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=100)))\n  \n# Voting Classifier with hard voting\nvot_hard4 = VotingClassifier(estimators = estimator, voting ='hard')\nvot_hard4.fit(X_train, np.ravel(Y_train))\nY_prediction = vot_hard4.predict(X_test)\nacc = accuracy_score(Y_test, Y_prediction)\nprint(\"Data4\")\nprint(\"Accuracy: {0:.3f}\".format(accuracy_score(Y_test, Y_prediction)))\nprint(\"f1: {0:.3f}\".format(f1_score(Y_test, Y_prediction)))","47df7b9c":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.info()","a6404ba4":"\nmean = test[\"Fare\"].mean(skipna=True)\ntest.fillna(mean, inplace=True)\nage = test.groupby(['Sex', 'Pclass'])['Age']\\\n .transform(lambda x: x.fillna(x.mean()))\n\ntest[\"Age1\"] = age\n\ntest.fillna(test[\"Embarked\"].value_counts().index[0], inplace=True)\n\n\nsex = pd.get_dummies(test[\"Sex\"])\ntest = test.join(sex)\nembarked = pd.get_dummies(test[\"Embarked\"])\ntest = test.join(embarked)\n\ntest.info()\n","6fd9b2f2":"data1 = test[[\"Pclass\",\"SibSp\",\"Parch\",\"Fare\",\"Age1\",\"female\",\"C\",\"Q\"]]","a2bfe116":"test_data1 = test[[\"Pclass\",\"SibSp\",\"Parch\",\"Fare\",\"Age1\",\"female\",\"C\",\"Q\"]]\ntest_data4 = test[[\"Pclass\",\"SibSp\",\"Parch\",\"Age1\",\"female\",\"C\",\"Q\"]]\n\ntrain_data1 = train[[\"Pclass\",\"SibSp\",\"Parch\",\"Fare\",\"Age1\",\"female\",\"C\",\"Q\"]]\ntrain_data4 = train[[\"Pclass\",\"SibSp\",\"Parch\",\"Age1\",\"female\",\"C\",\"Q\"]]\n\nM,_ = train_data1.shape\ntrain_Y = train[\"Survived\"].to_numpy().reshape((M,1))\n\nY_vote1 = vot_hard1.predict(test_data1)\nY_vote1 = pd.DataFrame(data=Y_vote1.astype(int), index=test[\"PassengerId\"].values, columns=[\"Survived\"])\nY_vote1.index.name = \"PassengerId\"                                                                              \nY_vote1.to_csv(\"vote1.csv\")\n\nY_vote4 = vot_hard4.predict(test_data4)\nY_vote4 = pd.DataFrame(data=Y_vote4.astype(int), index=test[\"PassengerId\"].values, columns=[\"Survived\"])\nY_vote4.index.name = \"PassengerId\"                                                                              \nY_vote4.to_csv(\"vote4.csv\")\n        \n        \nW,b,loss = LR_Iterate(train_data1,train_Y)\nY_LR1 = LR_Prediction(test_data1, W, b) \nY_LR1 = pd.DataFrame(data=Y_LR1.astype(int), index=test[\"PassengerId\"].values, columns=[\"Survived\"])\nY_LR1.index.name = \"PassengerId\"                                                                              \nY_LR1.to_csv(\"LR1.csv\")\n\nW1, b1, W2, b2, loss = NN_Iterate(train_data1,train_Y)\nY_NN1 = NN_Prediction(test_data1,W1,b1,W2,b2)\nY_NN1 = pd.DataFrame(data=Y_NN1.astype(int), index=test[\"PassengerId\"].values, columns=[\"Survived\"])\nY_NN1.index.name = \"PassengerId\"                                                                              \nY_NN1.to_csv(\"NN1.csv\")\n\nW,b,loss = LR_Iterate(train_data4,train_Y)\nY_LR4 = LR_Prediction(test_data4, W, b) \nY_LR4 = pd.DataFrame(data=Y_LR4.astype(int), index=test[\"PassengerId\"].values, columns=[\"Survived\"])\nY_LR4.index.name = \"PassengerId\"                                                                              \nY_LR4.to_csv(\"LR4.csv\")\n\nW1, b1, W2, b2, loss = NN_Iterate(train_data4,train_Y)\nY_NN4 = NN_Prediction(test_data4,W1,b1,W2,b2)\nY_NN4 = pd.DataFrame(data=Y_NN4.astype(int), index=test[\"PassengerId\"].values, columns=[\"Survived\"])\nY_NN4.index.name = \"PassengerId\"                                                                              \nY_NN4.to_csv(\"NN4.csv\")","79252c95":"print( Y_vote1.loc[900])","a4e65647":"N,_ = Y_vote1.shape \nY = pd.DataFrame(data=np.zeros(N), index=Y_vote1.index, columns=[\"Survived\"])\n\nfor passengerId in Y_vote1.index:\n    survived = Y_vote1.loc[passengerId] + Y_vote4.loc[passengerId] + Y_LR1.loc[passengerId] + Y_NN4.loc[passengerId]+Y_LR4.loc[passengerId]\n     \n    Y.loc[passengerId] = 0\n    \n    if (survived.item() > 2):\n        Y.loc[passengerId] = 1\n        \nY[\"Survived\"] = Y[\"Survived\"].astype(int)    \nY.index.name = \"PassengerId\"\nY.to_csv(\"finalvote2.csv\")","b88185ac":"Also Age influences the survival. We see that children were saved more likely and older passanger less likely.\nFare plays a role and could be expected to be in correlation with Pclass so we should consider using either Pclass or Fare.\n","b9364886":"Let's see the data","43b808f7":"We also need to prepare categorized data for Age and Fare","3ec3c5b9":"The results show that filling Age missing values by average separately for children and for adults haven't done any good.\nAlso categorization for Age and Fare didn't contribute to better accuracy.\nFor Data4 the results are interesting. Simply excluding Fare column improved results for Neural network and didn't worsen results for Logisti regression.\n\nAll in all, there isn't any big difference between Logistic regression and Neural network. ","6ccaa108":"We run our implementation of Logistic regression and simple Neural networkfor all four data sets. The neural network has one hidden layer with 8 neurons. Activation function for firts layer is Relu and for second layer it is Sigmoid function. \n\nEvery data set is split into train and test part in proportion 70\/30. All accuracy measuremnts are done on the test data.\nBoth algorithms runs 10 times and the accuracy numbers are average of these 10 runs.\nFor both Linear regression and Neural network is used ADAM optimization algorithm as it turn out to be the most effective in the speed of finding minium value. For neural network the learning rate was decreased by multiplying it by 0.98 every 100th iteration, which also contributed to effectiveness of the algorithm. The decreasing parameters were found experimentaly.\n","db67856d":"The second one set for each passanger if it is a child according to occurence of word 'Master' in passenger's name. Then the average age for children and for adults is filled separately.\n\n","a98402fb":"Decision Trees\n\nIn this section we compared different Decision tree algorithms. We use simple Desicion tree, Gradient boosting, Adaboost, Random tree and Extra tree algorithms. We found max depth for decision tree and number of trees for the rest of the algorithms were there is best accuracy and use these for Voting Classifier.","8b38ae64":"**Preparing data**\n\nWe can see that for columns Age, Cabin and Embarked there are missing values. As there are around 200 Cabin missing values, which is significant proportion of data and there is no satisfactory way how to fill it, we don't use it.\nAge column could be filled in more ways. We use two aproaches for comparisons.\nThe first one simply adds average age value to missing values. ","f676c78c":"Now we run Voting classifier for all 4 data sets. We choose first three best decision trees algorithm for each data set. We see that the best results are again for Data1 with estimators Gradient Boosting, AdaBoost and RandomForest. The accuracy is the highest so far achieved 0.832 and f1 score is 0.767.","98b54a1a":"Now let's have a look on test data.","bf14d36e":"On graphs we see how diferrent features influence the survival.\nThe most significant are Sex and Pclass. Woman were more likely saved than men. Passangers travelled in 3rd class are more likely to die. ","06f249de":"Import Libraries","af408c48":"**Data analyse**\n","cd1d5dbc":"We use data quartet to compare impact of good cleaning and preparing of data.\n\nFirst Data Set contains columns Pclass, SibSp, Parch,Fare, Sex, Embarked and Age filled by the first way.\n\nSecond Data Set Contains columns Pclass, SibSp, Parch, Fare, Sex, Embarked and Age filled by the second way.\n\nThird Data Set Contains columns Pclass, SibSp, Parch, Sex, Embarked and categorized Fare and Age\n\nFourth Data Set is same like the first one only without Fare column.","4a2ccada":"We need to fill empty Embarked values. The Sex and Embarked columns need to be transformed\nto numeric value.","68fc8a1c":"Let's read the data","1cb05c7d":"We need to fill missing Age, Embarked and Fare same way how the train data were filled. Also Sex and Embarked need to be transform to numeric values."}}