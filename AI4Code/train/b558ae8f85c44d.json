{"cell_type":{"8faaa826":"code","834847e9":"code","a9ec1a9c":"code","a0740431":"code","d097cdda":"code","bb66752d":"code","0848a05d":"code","6d9f2cc0":"code","dddf00a3":"code","b431a681":"code","c71d528f":"code","d7602e23":"code","e41e8786":"code","0222176c":"code","812c6e1c":"code","e44122c7":"code","81e97203":"code","c6a32b02":"code","827192f2":"code","50428b39":"code","8db4a20e":"code","b3c27e61":"code","c866cd54":"code","d2dbb159":"code","b3d4aff5":"code","3951fadd":"code","71a6d5b5":"code","e639079c":"code","e092777a":"code","7e59c78b":"code","06f0c173":"code","c718d780":"code","f43414d0":"code","085c6a94":"code","932895d2":"code","209a6723":"code","a5915961":"code","426414b2":"code","560f9152":"code","d3ceaacb":"code","78d7fd3b":"code","8f0d3511":"markdown","85187efe":"markdown","6bf3d14a":"markdown","30e0a756":"markdown","1fc538eb":"markdown","a616c676":"markdown","10f7ed90":"markdown","0e4941b4":"markdown","552103f7":"markdown","0347e167":"markdown","a69d8cd9":"markdown","59b08661":"markdown","3864febb":"markdown"},"source":{"8faaa826":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","834847e9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nplt.rcParams['figure.figsize']= (12,6)\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette='dark', style=\"whitegrid\")","a9ec1a9c":"df=pd.read_csv('..\/input\/diamonds\/diamonds.csv')\ndf.head()","a0740431":"# shape of the data\ndf.shape","d097cdda":"# drop Unnamed column\ndf.drop(df.columns[0], axis=1, inplace=True)","bb66752d":"# checking data types\ndf.info()","0848a05d":"df.isnull().sum()","6d9f2cc0":"df.shape","dddf00a3":"#visualizing missing numbers\nmsno.matrix(df)","b431a681":"df.describe()","c71d528f":"df[(df['x']==0) | (df['y']==0) | (df['z']==0)]","d7602e23":"df=df[~((df['x']==0) | (df['y']==0) | (df['z']==0))]","e41e8786":"df[(df['x']==0) | (df['y']==0) | (df['z']==0)]","0222176c":"# check correlation b\/w features\nplt.figure(figsize=(12,6))\nsns.heatmap(df.corr(), annot=True, cmap='viridis', cbar=True)\nplt.show()","812c6e1c":"numerical_cols=df.select_dtypes(include=np.number).columns.to_list()\ncategorical_cols=df.select_dtypes(exclude=np.number).columns.to_list()","e44122c7":"numerical_cols","81e97203":"categorical_cols","c6a32b02":"sns.catplot('cut', data=df, kind='count',aspect=2.5)","827192f2":"sns.catplot(x='cut', y='price', kind='box', data=df, aspect=2.5)","50428b39":"sns.catplot('color', kind='count', data=df, aspect=2.5)","8db4a20e":"sns.catplot(x='color', y='price', data=df, aspect =2.5, kind='box')","b3c27e61":"numerical_cols","c866cd54":"sns.pairplot(df[numerical_cols], kind='reg')","d2dbb159":"# Let's create a new column volume\ndf['volume']=df['x']*df['y']*df['z']","b3d4aff5":"df.head()","3951fadd":"df.drop(['x', 'y', 'z'], axis=1, inplace=True)","71a6d5b5":"# Apply categorical encoding\ndf=pd.get_dummies(df, drop_first=True)","e639079c":"df.head()","e092777a":"df.info()","7e59c78b":"# Conver to X & y\nX=df.drop('price', axis=1)\ny=df['price']","06f0c173":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.2, random_state=1)\n","c718d780":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","f43414d0":"sc=StandardScaler()\nX_train_tx=sc.fit_transform(X_train)\nX_test_tx=sc.transform(X_test)","085c6a94":"dataset_1=(X_train, X_test, y_train, y_test, 'dataset_1')","932895d2":"# Blank lists for all the details\nmodel_=[]\ncv_score_test=[]\ncv_score_train=[]\nmse_=[]\nmae_=[]\nrmse_=[]\nr2_=[]\n\n","209a6723":"def run_model(model, dataset, modelname):\n    model.fit(dataset[0], dataset[2])\n    accuracies=cross_val_score(estimator=model, X=dataset[0], y=dataset[2], cv=5, verbose=1)\n    y_pred=model.predict(dataset[1])\n    print('')\n    score_1=model.score(dataset[1], dataset[3])\n    print(f'#### {modelname} ####')\n    print(\"score :%.4f\" %score_1)\n    print(accuracies)\n    \n    \n    mse=mean_squared_error(dataset[3], y_pred)\n    mae=mean_absolute_error(dataset[3], y_pred)\n    rmse=mean_squared_error(dataset[3], y_pred)**0.5\n    r2=r2_score(dataset[3], y_pred)\n    \n    \n    print('')\n    print('MSE    : %0.2f ' % mse)\n    print('MAE    : %0.2f ' % mae)\n    print('RMSE   : %0.2f ' % rmse)\n    print('R2     : %0.2f ' % r2)\n    \n    ## appending to the lists\n    \n    model_.append(modelname)\n    cv_score_test.append(score_1)\n    cv_score_train.append(np.mean(accuracies))\n    mse_.append(mse)\n    mae_.append(mae)\n    rmse_.append(rmse)\n    r2_.append(r2)","a5915961":"model_dict={'LinearRegression': LinearRegression(), 'LassoRegression': Lasso(normalize=True), \n             'AdaBoostRegressor': AdaBoostRegressor(n_estimators=1000),\n            'RidgeRegression': Ridge(normalize=True),\n            'GradientBoostingRegressor': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, verbose=1),\n           'RandomForestRegressor': RandomForestRegressor(), \n           'KNeighborsRegressor': KNeighborsRegressor()\n           }","426414b2":"run_model(model_dict['LinearRegression'], dataset_1, \"LinearRegression\")","560f9152":"for models in model_dict:\n    run_model(model_dict[models], dataset_1, models)","d3ceaacb":"accuracy_data=pd.DataFrame(zip(model_, cv_score_test, cv_score_train, mse_, mae_, rmse_, r2_), columns=['Model', 'CV Test score', 'CV Train score (mean)', '%%SVGean Squared error', 'Mean Absolute error', 'Root Mean Squared error', 'R2 Score'])","78d7fd3b":"accuracy_data","8f0d3511":"##### Let's wrap up the dataset in a tuple so that if required we can create a new feature engineered dataset to run the models again","85187efe":"Let's check the plots","6bf3d14a":"Now splitting the X & y into train and test set","30e0a756":"No Null values in the dataset","1fc538eb":"Feature \"color\" EDA","a616c676":"#### Numerical columns EDA","10f7ed90":"**Feature \"CUT\" EDA**","0e4941b4":"confirming that these rows were removed","552103f7":"### Checking categorical columns\n","0347e167":"Only few rows with such scenario, we will drop them","a69d8cd9":"x, y and z minimum values are 0 which doesn't seem realistic. Let's check\n","59b08661":"**RandomForest has 98% accuracy. Model is giving excellent results**","3864febb":"# Checking null values"}}