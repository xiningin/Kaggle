{"cell_type":{"2eb36ec9":"code","212073f6":"code","4af27878":"code","49c39e01":"code","57dbae20":"code","a5baf16a":"code","100470cc":"code","217344f3":"code","df3ba54d":"code","b356ff41":"code","bed81be6":"code","8cab0530":"code","b67584ec":"code","c1845075":"code","4ae9ad17":"code","32ec5ea7":"code","b2e62117":"code","4fb481bb":"code","db5adffb":"code","54811c71":"code","895f72ce":"code","714a3269":"code","5e86e277":"code","e8800d09":"code","87ff3f03":"code","ac9640a9":"code","b52755ad":"code","911c3471":"code","cb618c5d":"code","066016fd":"code","5cd041d1":"code","9cd99332":"code","89d03746":"code","8742d500":"code","f04c36b8":"code","fdb1ea6b":"markdown","d9cd6e40":"markdown","b3f50a93":"markdown","c4c0155c":"markdown","9faf54dd":"markdown","5da4de45":"markdown","1edf6782":"markdown","670286b2":"markdown","db96edfe":"markdown","db789b0f":"markdown","97fa0af1":"markdown","bea8e793":"markdown","a796c4b3":"markdown","21b9dd65":"markdown","9c1cd253":"markdown","085ace03":"markdown","a6dc0fe3":"markdown","3ca698a6":"markdown","1fa1d738":"markdown","d1e54d63":"markdown","e80b5604":"markdown","6e9bfe7d":"markdown","a4c47674":"markdown"},"source":{"2eb36ec9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import seed \nseed(42)","212073f6":"#Importing only the first 30000 rows\ndf = pd.read_csv('\/kaggle\/input\/prediction-of-asteroid-diameter\/Asteroid.csv',nrows = 30000)","4af27878":"df.head()","49c39e01":"#Checking which columns(features) have nan values\nfor column in df.columns:\n    print(column, df[column].isna().sum()\/df.shape[0]) #returns the fraction of NAN values","57dbae20":"#Printing the first ten unique values of each feature\nfor column in df.columns:\n    print(column, df[column].unique()[:10])","a5baf16a":"#Steps 0\ndf['diameter']=pd.to_numeric(df['diameter'],errors='coerce') #transforming to numeric, setting errors to NaN\ndropindexes = df['diameter'][df['diameter'].isnull()].index #rows with nan diameters to drop\ndropped_df = df.loc[dropindexes] #saving dropped rows for the future\ndf = df.drop(dropindexes, axis=0) ","100470cc":"#Steps 1\ntooMuchNa = df.columns[df.isna().sum()\/df.shape[0] > 0.5]\ndf = df.drop(tooMuchNa,axis=1)\ndf = df.drop(['condition_code','full_name'],axis=1)\ndf = df.drop(['neo','pha'],axis=1)","217344f3":"#Step 2\ndf = df.fillna(df.mean())","df3ba54d":"df.head()","b356ff41":"#Last sanity check for nan values\ndf.isna().values.any()","bed81be6":"df = df.drop(['albedo','H'],axis = 1)","8cab0530":"df['diameter']= df['diameter'].apply(np.log)\nfor column in df.columns.drop(['diameter']):\n    df['log('+column+')']=df[column].apply(np.log)\ndf = df.dropna(axis=1)","b67584ec":"df.corr()['diameter'].abs().sort_values(ascending=False)","c1845075":"from sklearn.model_selection import train_test_split\npredictors = df.drop('diameter',axis=1) \ntarget = df['diameter']\nX_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)","4ae9ad17":"X_train.head()","32ec5ea7":"from sklearn import preprocessing\n\n#Input standard normalization:\nstd_scaler = preprocessing.StandardScaler().fit(X_train)\n\ndef scaler(X):\n    x_norm_arr= std_scaler.fit_transform(X)\n    return pd.DataFrame(x_norm_arr, columns=X.columns, index = X.index)\n\nX_train_norm = scaler(X_train)\nX_test_norm = scaler(X_test)\n\ndef inverse_scaler(X):\n    x_norm_arr= std_scaler.inverse_transform(X)\n    return pd.DataFrame(x_norm_arr, columns=X.columns, index = X.index)\n","b2e62117":"from sklearn.metrics import r2_score\nimport seaborn as sns\n\ndef plot(prediction):\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,7)) \n    sns.distplot(Y_test.values,label='test values', ax=ax1)\n    sns.distplot(prediction ,label='prediction', ax=ax1)\n    ax1.set_xlabel('Distribution plot')\n    ax2.scatter(Y_test,prediction, c='orange',label='predictions')\n    ax2.plot(Y_test,Y_test,c='blue',label='y=x')\n    ax2.set_xlabel('test value')\n    ax2.set_ylabel('estimated $\\log(radius)$')\n    ax1.legend()\n    ax2.legend()\n    ax2.axis('scaled') #same x y scale\ndef score(prediction):\n    score = r2_score(prediction,Y_test)\n    return score\ndef announce(score):\n    print('The R^2 score achieved using this regression is:', round(score,3))\nalgorithms = []\nscores = []\n","4fb481bb":"#Defining the model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n###Training\nlr.fit(X_train,Y_train)\n\n###Predicting\nY_pred_lr = lr.predict(X_test)\n\n###Scoring\nscore_lr = score(Y_pred_lr)\nannounce(score_lr)\n\nalgorithms.append('LR')\nscores.append(score_lr)\n","db5adffb":"plot(Y_pred_lr)","54811c71":"### Defining the Model\nfrom sklearn.linear_model import ElasticNetCV\nenet = ElasticNetCV(cv=9,max_iter=10000)\n\n### Training\nenet.fit(X_train_norm,np.ravel(Y_train))\n\n### Predicting\nY_pred_enet = enet.predict(X_test_norm)\n\n###Scoring\nscore_enet = score(Y_pred_enet)\nannounce(score_enet)\n\nalgorithms.append('eNet')\nscores.append(score_enet)","895f72ce":"plot(Y_pred_enet)","714a3269":"### Defining the Model\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n##For weighted metric, more accurate but longer calculation\n#weights = X_train_norm.corrwith(Y_train).abs()\n#neigh = KNeighborsRegressor(n_neighbors=3, metric_params={'w' : weights.values}, metric='wminkowski')\n\nneigh = KNeighborsRegressor(n_neighbors=3)\n\n### Training\nneigh.fit(X_train_norm,Y_train)\n\n### Predicting \nY_pred_neigh = neigh.predict(X_test_norm)\n\n### Scoring\nscore_neigh=score(Y_pred_neigh)\nannounce(score_neigh)\n\nalgorithms.append('k-NN')\nscores.append(score_neigh)","5e86e277":"plot(Y_pred_neigh)","e8800d09":"### Defining the model\nfrom sklearn import tree\ndecTree = tree.DecisionTreeRegressor()\n\n### Training\ndecTree = decTree.fit(X_train_norm,Y_train)\n\n### Predicting\nY_pred_tree = decTree.predict(X_test_norm)\n\n### Scoring\nscore_tree = score(Y_pred_tree)\nannounce(score_tree)\n\nalgorithms.append('DTree')\nscores.append(score_tree)","87ff3f03":"plot(Y_pred_tree)","ac9640a9":"### Defining the model\nfrom sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(max_depth=32, n_estimators=50)\n\n### Training \nforest.fit(X_train_norm,np.ravel(Y_train))\n\n###Predicting\nY_pred_forest = forest.predict(X_test_norm)\n\n### Scoring\nscore_forest = score(Y_pred_forest)\nannounce(score_forest)\n\nalgorithms.append('RForest')\nscores.append(score_forest)","b52755ad":"plot(Y_pred_forest)","911c3471":"### Defining the model\nfrom sklearn import svm\nsvmreg = svm.SVR()\n\n### Training\nsvmreg.fit(X_train_norm,np.ravel(Y_train))\n\n### Predicting\nY_pred_svm = svmreg.predict(X_test_norm)\n\n### Scoring\nscore_svm = score(Y_pred_svm)\nannounce(score_svm)\n\nalgorithms.append('SVM')\nscores.append(score_svm)","cb618c5d":"plot(Y_pred_svm)","066016fd":"### Defining the model\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\n\nAdam(learning_rate=0.005)\nmodel = Sequential()\nmodel.add(Dense(24,activation='tanh',input_dim=X_train_norm.shape[1]))\nmodel.add(Dense(12,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error',optimizer='adam')\n\n### Training\n\nmodel.fit(X_train_norm,Y_train,epochs=100,batch_size=256,verbose=False)\n\n### Predicting\n\nY_pred_nn = model.predict(X_test_norm)\n\n### Scoring\nscore_nn = score(Y_pred_nn)\nannounce(score_nn)\n\nalgorithms.append('NNet')\nscores.append(score_nn)","5cd041d1":"plot(Y_pred_nn)","9cd99332":"### Defining the model\nimport xgboost as xgb \nxgReg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, \n                         learning_rate = 0.08 ,\n                max_depth = 4, n_estimators = 500)\n\n### Training\nxgReg.fit(X_train_norm,Y_train)\n\n### Predicting\nY_pred_xgb = xgReg.predict(X_test_norm)\n\n### Scoring\nscore_xgb = score(Y_pred_xgb)\nannounce(score_xgb)\n\nalgorithms.append('XGB')\nscores.append(score_xgb)","89d03746":"plot(Y_pred_xgb)","8742d500":"# One bonus of using xgboost is being able to \n# simply see how important the different features where when creating the learners.\n\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgReg, height=0.5, ax=ax, importance_type='weight')\nplt.show()","f04c36b8":"sns.barplot(algorithms,scores)","fdb1ea6b":"Thank you for reading !","d9cd6e40":"Linear Regression:","b3f50a93":"k-Nearest Neighbours regression:","c4c0155c":"Nasa's own Asteroid Size Estimator website uses the $H$ value and $\\log(albedo)$: https:\/\/cneos.jpl.nasa.gov\/tools\/ast_size_est.html so I decided to drop those features for curiosity and added difficulty (regression scores went down from 0.98 to 0.8)","9faf54dd":"Splitting:","5da4de45":"XGBoosting wins today with an $R^2$ score of 0.845","1edf6782":"Comparing all regression algorithms","670286b2":"Next thing is to understand what type of data we're dealing with","db96edfe":"*  **Goal:** Getting the diameter of an asteroid from the other data given about that asteroid, in other words supervised regression with the target being $\\log(diameter)$.   \nThe metric used during testing will be the $R^2$ score.\n* **How:**\n    * Step 1: Cleaning and preparing the data\n        * a. I cleared the samples with nan diameter.\n        * b. I dropped the features with more than half nan values.\n        * c. The dataframe had a lot of nan values, I chose to substitute them with the average value for the corresponding feature.\n    * Step 2: Train-test splitting of the data, and then normalizing (standard) using the test dataframe. \n    * Step 3: Trying different regression algorithms (Linear Regression, Elastic Net, Decision Tree, Random Forest, XGBoost, SVM, Neural Network) and picking the best one.\n* **Extra:**  Since NASA's own estimator uses $H$ and $albedo$ to calculate the diameter, I decided to drop those to add difficulty and have less linearity in problem to solve.\n\n\n**Dependencies: numpy, pandas, matplotlib, seaborn, scikit-learn, xgboost, keras.**","db789b0f":"Correlation analysis:","97fa0af1":"## Part I: Importing, exploration & cleaning of the data","bea8e793":"**There are lots of feature columns to check:**   \nFirst I wanted to know if there are NaN values (there are and will be dealt with later)","a796c4b3":"## Part II: Splitting the dataframe into train and test dataframes and normalizing them for our regressions.","21b9dd65":"Random Forest regression:","9c1cd253":"XGBoost regression:","085ace03":"Normalization:","a6dc0fe3":"* **Cleaning and prepping the dataframe:**  \n**Steps:**\n    * 0\/ 'diameter' is string type, I will convert to numeric. This gave errors for some diameters because they were corrupted, so I added the argument \"errors='coerce'\" to set corrupted diameters to nan, and later dropped those.\n    * 1\/ Dropping irrelevent features and choosing my battles:\n        * 1a\/ dropping names because I dont believe asteroids are named according to their diameter.\n        * 1b\/ Dropping all features with more than half nan values\n        * 1c\/ dropping condition_code and neo and pha because most seems to be 0 or nan.\n    * 2\/ Replace nans entries with mean value of column","3ca698a6":"Elastic Net regression:","1fa1d738":"Decision Tree regression:","d1e54d63":"Support Vector Machine regression:\n(too slow when n_samples > 30000)","e80b5604":"Since a lot of values in physics are more relevant when you consider their log, I'll add columns to the dataframe corresponding to the log of the original columns:","6e9bfe7d":"## Part III:  Trying different regressions and ranking them according to their $R^2$ score.\n**Algorithms used:** Linear Regression, Elastic Net, k-Nearest Neighbours, Decision Tree, Random Forest, SVM, Neural Network and XGBoost. ","a4c47674":"Neural Network regression:"}}