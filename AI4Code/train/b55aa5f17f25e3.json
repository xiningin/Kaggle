{"cell_type":{"e190990d":"code","5f1a8da2":"code","50d6e9c7":"code","92a85bea":"code","67950354":"code","5f5000cc":"code","ff3359cc":"code","78435156":"code","72d97db3":"code","aeef8b90":"code","2365483d":"code","48539e72":"code","6e6d08ca":"code","0a4e55c7":"code","887b0c63":"code","da4c6fcc":"code","23770bde":"code","ee4cf8c8":"code","651b73a9":"code","66329bb0":"code","6e4d87c2":"code","720e41c4":"code","9a92c99f":"code","2c75b9c3":"code","40cada93":"code","aa9c53f6":"code","614c30e0":"code","665910d0":"code","6acb779f":"code","108b7a81":"code","6ea7e91c":"code","1c3d55d2":"code","d798786f":"code","1bf4819f":"code","e0dff4d8":"code","5caf5f1c":"code","ef5f6962":"code","e6cd4ca7":"code","068be2fb":"code","cbb62608":"code","a7c9b8b0":"code","7df33d21":"code","fa60add9":"code","afc4f8dc":"code","be3a837d":"code","e18ec5a6":"code","bbeb79b1":"code","c92e5d9e":"code","1544afaa":"code","b02abb3f":"code","44b9d301":"code","6b2ed834":"code","4803eb6c":"code","f60fd879":"code","93db1da9":"code","63d9881e":"code","ddbe1bbb":"code","92df8169":"code","015df4fb":"code","ca13eea0":"code","aa615655":"code","3f354659":"markdown","7080d615":"markdown","74e2a991":"markdown","64fdb0ee":"markdown","e1a69c13":"markdown","ef086021":"markdown","33f8287a":"markdown","8064a73e":"markdown","093330f9":"markdown","28f885d8":"markdown","d4a2d485":"markdown","19662a72":"markdown","9ceb8b29":"markdown","fdbd448e":"markdown","0205794b":"markdown","5236d72b":"markdown","c16de26d":"markdown","41b4aac1":"markdown","25ea5e65":"markdown","a57ae419":"markdown","f24140ff":"markdown","fc2806f4":"markdown","e45f7cf1":"markdown","1161ef1b":"markdown","5a391a3b":"markdown","345efbc1":"markdown","c4cf3c99":"markdown","ad75ee77":"markdown","10ec2342":"markdown","8bb963ae":"markdown","c3f689d3":"markdown","d472fe68":"markdown","428d33be":"markdown","4a0a5b69":"markdown","9244fd8d":"markdown","5a65f9c4":"markdown","597f0865":"markdown","801ce54a":"markdown","7db7c60b":"markdown","cdb042b1":"markdown","0464860a":"markdown"},"source":{"e190990d":"import numpy as np \nimport pandas as pd\nimport pandas_profiling \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, MaxAbsScaler\n\n#Pipelines allow you to create a single object that includes all steps from data preprocessing & classification.\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics \nfrom sklearn.metrics import accuracy_score, recall_score\nfrom IPython.display import display_html\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","5f1a8da2":"#custom function to display dataframes    \n\ndef displayoutput(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline;margin-left:50px !important;margin-right: 40px !important\"'),raw=True)\n    \n\ndef printoutput(string, color=None):\n    colorstr = \"<span style='color:{}'>{}<\/span>\".format(color, string)\n    display(Markdown(colorstr))    ","50d6e9c7":"originalData = pd.read_csv('..\/input\/portuguese-marketing-campaigns-dataset\/bank-full.csv')","92a85bea":"pandas_profiling.ProfileReport(originalData)","67950354":"originalData.shape","5f5000cc":"originalData.describe(include=[\"object\"])","ff3359cc":"CatCloums = originalData.select_dtypes(include=\"object\")\nprint ( \"Categorical Columns:\" )\nprint ( \"_____________________\" )\nsno = 1\n\nfor i in CatCloums.columns:\n    print(sno, \".\" , i)\n    sno += 1","78435156":"for catcol in CatCloums:\n    sns.countplot(data=CatCloums,y=catcol,order=CatCloums[catcol].value_counts().index)\n    plt.figure(figsize=(20,20))\n    plt.show()","72d97db3":"for i in CatCloums:\n    print(\"Cloumn Name :\", i)\n    print ( CatCloums [ i ].value_counts ( ) )","aeef8b90":"for i in CatCloums:\n    f, axes = plt.subplots( figsize = (7,5))\n    print(i)\n    sns.countplot(originalData[i])\n    plt.xticks ( rotation = 50 )\n    plt.show ( )","2365483d":"NumColums = originalData.select_dtypes(exclude= 'object')\nsno = 1\nprint ( \"Numerical columns:\" )\nprint ( \"______________________\" )\nfor i in NumColums.columns:\n    print(sno, \".\", i)\n    sno += 1","48539e72":"_ = originalData.hist(column=NumColums.columns,figsize=(15,15),grid=False,color='#86bf91',zorder=2,rwidth=1.0)","6e6d08ca":"for i in NumColums:\n    print(\"Column :\", i)\n    sns.distplot(originalData[i])\n    plt.show()\n","0a4e55c7":"originalData.describe(include=[\"object\"])","887b0c63":"originalData.describe(exclude =[\"object\"])","da4c6fcc":"originalData.corr()","23770bde":"duplicates = originalData.duplicated()\nsum(duplicates)","ee4cf8c8":"originalData.isna().sum()","651b73a9":"originalData.isnull().sum()","66329bb0":"originalData.drop(['duration'],axis=1,inplace=True)\noriginalData.head(10)","6e4d87c2":"originalData['pdays']=originalData['pdays'].astype('category')\noriginalData['Target']=originalData['Target'].astype('category')\noriginalData.head(5)","720e41c4":"originalData.default.replace(('yes', 'no'), (1, 0), inplace=True)\noriginalData.housing.replace(('yes','no'),(1,0),inplace=True)\noriginalData.loan.replace(('yes','no'),(1,0),inplace=True)\n\n\n","9a92c99f":"from sklearn.model_selection import train_test_split\n\nX = originalData.loc[:,originalData.columns !='Target']\ny = originalData['Target']\n\n#get dummies for catagorical features\nX = pd.get_dummies(X, drop_first=True)\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size= .40,random_state=101)\n","2c75b9c3":"logregmodel = LogisticRegression(solver='liblinear')\n# Fit the model on train\nlogregmodel.fit(X_train,y_train)\n\n#predict on test\ny_predict = logregmodel.predict(X_test)\ny_predict_df = pd.DataFrame(y_predict)\n","40cada93":"# Check is the model an overfit model? \ny_pred = logregmodel.predict(X_test)\nprint(logregmodel.score(X_train, y_train))\nprint(logregmodel.score(X_test , y_test))","aa9c53f6":"cm = metrics.confusion_matrix(y_test,y_predict)\n\n\nplt.clf()\nplt.imshow(cm,interpolation='nearest',cmap=plt.cm.Wistia)\nclsnames = ['Not_Subscribed','Subscribed']\nplt.title('Confusion Matrix for Test Data')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\ntick_marks = np.arange(len(clsnames))\nplt.xticks(tick_marks, clsnames, rotation=45)\nplt.yticks(tick_marks, clsnames)\ns = [['TN','FP'], ['FN', 'TP']]\n\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\nplt.show()","614c30e0":"logisticscore = logregmodel.score(X_test,y_test)\nprint(logisticscore)","665910d0":"logisticaccuracy = metrics.accuracy_score(y_test,y_predict)\nprint(logisticaccuracy)","6acb779f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# instantiate learning model \nknn = KNeighborsClassifier() \n\n# fitting the model\nknn.fit(X_train,y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)","108b7a81":"# evaluate accuracy\naccuracy_score(y_test,y_pred)","6ea7e91c":"# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# evaluate accuracy\naccuracy_score(y_test,y_pred)","1c3d55d2":"# instantiate learning model (k = 9)\nknn = KNeighborsClassifier(n_neighbors=9)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# evaluate accuracy\naccuracy_score(y_test,y_pred)","d798786f":"# creating odd list of K for KNN\nmyList = list(range(1,20))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))\n\n# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 1,3,5....19\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","1bf4819f":"plt.plot(MSE,neighbors)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('MisClassification Error')\nplt.show","e0dff4d8":"lstaccuracy =[]\nfor k in range(20):\n    K_value = k+1\n    neigh = KNeighborsClassifier(n_neighbors=K_value)\n    neigh.fit(X_train,y_train)\n    y_pred = neigh.predict(X_test)\n    lstaccuracy.append(accuracy_score(y_test,y_pred)*100)\n    print(\"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\",K_value)\n","5caf5f1c":"plt.plot(lstaccuracy)\nplt.ylabel('Accuracy')\nplt.xlabel('Number of neighbors')\nplt.title(\"Accuracy vs # Neighbors\")","ef5f6962":"count_misclassified = (y_test != y_pred).sum()\ncount_misclassified","e6cd4ca7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#create a new KNN model\nknn_cv = KNeighborsClassifier(n_neighbors=3)\n#train model with cv of 5 \ncv_scores = cross_val_score(knn_cv, X_test, y_test, cv=5)\n#print each cv score (accuracy) and average them\nprint(cv_scores)\nprint(format(np.mean(cv_scores)))","068be2fb":"from sklearn.tree import DecisionTreeClassifier\n\ndTree = DecisionTreeClassifier(criterion = 'entropy', random_state=10)\ndTree.fit(X_train, y_train)","cbb62608":"print(dTree.score(X_train, y_train))\nprint(dTree.score(X_test, y_test))\n\nprint(recall_score(y_test, y_pred,average=\"binary\", pos_label=\"yes\"))","a7c9b8b0":"predict_dTree = dTree.predict(X_test)\ncm = metrics.confusion_matrix(y_test,predict_dTree)\ncm_df = pd.DataFrame(cm)\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm_df,annot=True,fmt='g')","7df33d21":"#print (pd.DataFrame(dTree.feature_importances_, columns = [\"Importance\"], index = X_train.columns))\n\nfeat_importance = dTree.tree_.compute_feature_importances(normalize=False)\nfeat_imp_dict = dict(zip(X_train.columns, dTree.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict,orient='index')\nfeat_imp.sort_values(by=0, ascending=False)","fa60add9":"dTree_Pruning  = DecisionTreeClassifier(criterion='entropy',random_state=100,max_depth=10,min_samples_leaf=3)\n\ndTree_Pruning.fit(X_train,y_train)","afc4f8dc":"#preds_pruned = dTree_Pruning.predict(X_test)\n#preds_pruned_train = dTree_Pruning.predict(X_train)\n\nprint(dTree_Pruning.score(X_train, y_train))\nprint(dTree_Pruning.score(X_test,y_test))","be3a837d":"predict_dTree_purning = dTree_Pruning.predict(X_test)\ncm_purning = metrics.confusion_matrix(y_test,predict_dTree_purning)\ncm_df_purning = pd.DataFrame(cm_purning)\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm_df_purning,annot=True,fmt='g')","e18ec5a6":"## Calculating feature importance\n\nfeat_importance = dTree_Pruning.tree_.compute_feature_importances(normalize=False)\nfeat_imp_dict = dict(zip(X_train.columns, dTree_Pruning.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict,orient='index')\nfeat_imp.sort_values(by=0, ascending=False)","bbeb79b1":"acc_DT = accuracy_score(y_test, predict_dTree_purning)\nrecall_DT = recall_score(y_test, predict_dTree_purning, average=\"binary\", pos_label=\"yes\")","c92e5d9e":"#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT, 'recall': recall_DT})\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf","1544afaa":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=50)\nrfc = rfc.fit(X_train,y_train)\nrfc","b02abb3f":"predict_rfc = rfc.predict(X_test)\naccuracy_rfc = accuracy_score(y_test,predict_rfc)\nrecall_rfc = recall_score(y_test, predict_rfc, average=\"binary\", pos_label=\"yes\")\n\ntempResultsDf = pd.DataFrame({'Method':['Random Forest'], 'accuracy': [accuracy_rfc]})\ntempResultsDf","44b9d301":"tempResultsDf = pd.DataFrame({'Method':['Random Forest'], 'accuracy': [accuracy_rfc], 'recall': [recall_rfc]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","6b2ed834":"abc1 = AdaBoostClassifier(n_estimators=10,learning_rate=0.1,random_state=25)\nabc1 = abc1.fit(X_train,y_train)\n\naccuracy_AdaBoost = abc1.score(X_test,y_test)\nprint(accuracy_AdaBoost)","4803eb6c":"pred_AB = abc1.predict(X_test)\nacc_AB = accuracy_score(y_test,pred_AB)\nrecall_AB = recall_score(y_test,pred_AB,pos_label='yes')\n","f60fd879":"tempResultsDf = pd.DataFrame({'Method':['Adaboost'], 'accuracy': [acc_AB], 'recall':[recall_AB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","93db1da9":"predict_AdaBoost = abc1.predict(X_test)\ncm = metrics.confusion_matrix(y_test,pred_AB)\ncm_df = pd.DataFrame(cm)\nplt.figure(figsize=(5,5))\nsns.heatmap(cm_df,annot=True ,fmt='g',)","63d9881e":"bgcl = BaggingClassifier(n_estimators=100, max_samples= .7, bootstrap=True, oob_score=True, random_state=22)\nbgcl = bgcl.fit(X_train, y_train)","ddbe1bbb":"pred_BG =bgcl.predict(X_test)\nacc_BG = accuracy_score(y_test, pred_BG)\nrecall_BG = recall_score(y_test, pred_BG, pos_label='yes')","92df8169":"tempResultsDf = pd.DataFrame({'Method':['Bagging'], 'accuracy': [acc_BG], 'recall':[recall_BG]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","015df4fb":"gb_model = GradientBoostingClassifier(n_estimators = 200, learning_rate = 0.1, random_state=22)\ngb_model = gb_model.fit(X_train, y_train)","ca13eea0":"predict_GB =gb_model.predict(X_test)\naccuracy_GB = accuracy_score(y_test, predict_GB)\nrecall_GB = recall_score(y_test, predict_GB, pos_label='yes')","aa615655":"\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost'], 'accuracy': [accuracy_GB], 'recall':[recall_GB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\n","3f354659":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#93DC0B ;\">Descriptive Statistics for Continuous(Numerical) Variables.","7080d615":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#8a3ebe;\">Categorical data by Count plot","74e2a991":" # <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#FF00BF  \">Gradient Boosting for same data","64fdb0ee":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\"> Gini Importance ","e1a69c13":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Accuracy","ef086021":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Data Preration for models","33f8287a":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#2D937C ;\"> Distribution plot-Skewness","8064a73e":"\n![Ensemble Techniques](https:\/\/cdn.pixabay.com\/photo\/2015\/02\/01\/10\/17\/music-619256_960_720.jpg)","093330f9":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\"> Confusion Marix with pirning","28f885d8":"**Drop the Columns based on Corr.**","d4a2d485":" # <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#0e92ea\">Adaboost for the same data","19662a72":"<span style=\"font-family: Arial; font-weight:bold;font-size:2.0em;color:#F36084;\">Five Point Summary","9ceb8b29":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Model Score","fdbd448e":"# <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#86128a\"> Ensemble Techniques in ML","0205794b":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\"> Confusion Matrix","5236d72b":"# <span style=\"font-family: Arial; font-weight:bold;font-size:1em;color:#0e92ea\"> Data Profiling","c16de26d":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Misclassification Error vs K","41b4aac1":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Gini Importance - After Purning","25ea5e65":"# <span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#0e92ea\"> Model Building","a57ae419":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Accuracy with Purning","f24140ff":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.8em;color:#4AAD30;\">Descriptive Statistics for Categorical Variables\n","fc2806f4":"# <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\"> Load Library and Data","e45f7cf1":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#8a3ebe;\">Creating Dummy Variables for Catagorical Features","1161ef1b":"# <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#B3BF21 \"> Logistic Regression","5a391a3b":"![](http:\/\/)<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Regularize\/Prune  ","345efbc1":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Accuracy v\/s Neighbours","c4cf3c99":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Classification accuracy","ad75ee77":" # <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#86B404\">Random Forest Model","10ec2342":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">k-Fold Cross-Validation","8bb963ae":"# <span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#0e92ea\"> EDA","c3f689d3":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Find the optimal number of neighbours \n\n* Small value of K will lead to over-fitting\n* Large value of K will lead to under-fitting. ","d472fe68":" # <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#1D9181 \">Bagging for the same data","428d33be":"***The recall score is relatively low and this has to be improves in the model***","4a0a5b69":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#8a3ebe;\">Correlation of Features","9244fd8d":" # <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#17E345 \"> DecisionTree Classifier","5a65f9c4":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Observation: \n*         Compared to the decision tree, we can see that the accuracy has slightly improved for the Random forest model\n*         Overfitting is reduced after pruning, but recall has slightly reduced","597f0865":"# <span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#0e92ea\"> DATA MINING","801ce54a":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.6em;color:#2D937C ;\">Visualizing Distribution of Continuous Variables","7db7c60b":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#8a3ebe;\">Confusion Matrix[](http:\/\/)","cdb042b1":" # <span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#D17880 \"> KNN Regression","0464860a":"**Bank client data**\n\n1 - age\n\n2 - job : type of job\n\n3 - marital : marital status\n\n4 - education\n\n5 - default: has credit in default?\n\n6 - housing: has housing loan?\n\n7 - loan: has personal loan?\n\n8 - balance in account\n\n**Related to previous contact**\n\n8 - contact: contact communication type\n\n9 - month: last contact month of year\n\n10 - day_of_week: last contact day of the week\n\n11 - duration: last contact duration, in seconds\n\n\n**Other attributes**\n\n12 - campaign: number of contacts performed during this campaign and for this client\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign\n\n14 - previous: number of contacts performed before this campaign and for this client\n\n15 - poutcome: outcome of the previous marketing campaign"}}