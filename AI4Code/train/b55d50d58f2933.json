{"cell_type":{"97237d56":"code","93f76dd4":"code","52cc0875":"code","4abd6b31":"code","e4ff993e":"code","228936af":"code","2f03a0aa":"code","09ef9070":"code","c4cd7cc8":"code","f74c5340":"code","2df61697":"code","43163e78":"code","23160032":"code","80d6b784":"code","f6a5ef35":"code","29943e36":"code","e8c225c3":"code","231c425c":"code","12a20902":"code","90a9895a":"code","01df74be":"code","510c8afe":"code","161fabea":"code","c512a832":"code","edeb69d1":"code","95ead953":"code","deef4feb":"code","1474f3dd":"code","f709bd58":"code","d390c81f":"code","05a105a4":"code","ef6065de":"code","b07171b2":"code","b51f5218":"code","6966e146":"code","15c1ca8f":"code","ba3fbbe3":"code","ff0a8109":"code","230c9a17":"code","41f96fc0":"code","002f449b":"code","5b37bcd3":"code","1be7c742":"code","3400b6e6":"code","29e0b216":"code","472851cb":"code","935dfe9e":"code","4649b3d6":"code","fa6cd779":"code","e5ecc890":"code","bcc5ef47":"code","fc605693":"code","a2adaee6":"code","8833f384":"code","80663418":"code","ac6e7332":"code","32785c8d":"code","617ce3d9":"code","660c2fa3":"code","8cd53f17":"code","ed658ece":"code","392c61a9":"code","5211756c":"code","d17d7231":"code","b7c50383":"code","233ddb02":"code","0aa6f799":"code","dc35d2f4":"code","477c1737":"code","64358ced":"code","bc9edd9c":"code","75af448a":"code","4aefaef0":"code","6b9d59b3":"code","50d6d72d":"code","de30dd0a":"code","d7482c44":"code","b053d449":"code","0292e9f4":"code","34cadcd1":"code","29c52cee":"code","1137148a":"code","097d308c":"code","03018a24":"code","ff3098d1":"code","ff20eb29":"code","c5396745":"code","c42514b6":"code","5be95ea3":"code","cbbae586":"code","5a523e2e":"markdown","72ba903c":"markdown","c5287e24":"markdown","3442461f":"markdown","a87f6653":"markdown","a7ebf303":"markdown","7524ff71":"markdown","ab52ee43":"markdown","ba0be97b":"markdown","d16d6c04":"markdown","edc983e9":"markdown","635e75fa":"markdown","c5e0e5bf":"markdown","81595621":"markdown","0a278027":"markdown","56a6705a":"markdown","4cdc16dc":"markdown","d0658cb3":"markdown","e24bcf52":"markdown","df82cc82":"markdown","24f479f0":"markdown","d1ea4dd7":"markdown"},"source":{"97237d56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport math\nimport matplotlib as p\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy.stats as sps\nimport re\nimport copy\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import Imputer\n# from statsmodels.stats.outliers_influence import variance_inflation_factor","93f76dd4":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","52cc0875":"train = import_data('..\/input\/widsdatathon2020\/training_v2.csv')\ntest = import_data('..\/input\/widsdatathon2020\/unlabeled.csv')\nst = pd.read_csv('..\/input\/widsdatathon2020\/solution_template.csv')\nss = pd.read_csv('..\/input\/widsdatathon2020\/samplesubmission.csv')\ndictionary = pd.read_csv('..\/input\/widsdatathon2020\/WiDS Datathon 2020 Dictionary.csv')","4abd6b31":"# Checking shapes\nprint('train    ', train.shape)\nprint('test     ', test.shape)\n\n# Combining train and test to explore the categorical attributes\ntrain_len = len(train)\ncombined_dataset = pd.concat(objs = [train, test], axis = 0)\nprint('combined', combined_dataset.shape)","e4ff993e":"pd.set_option('display.max_rows', 200)\n# Dictionary\ndictionary.style.set_properties(subset=['Description'], **{'width': '500px'})","228936af":"# Extracing categorical columns\ndf_cat = combined_dataset.select_dtypes(include=['object', 'category'])\ndf_cat.columns","2f03a0aa":"# Checking unique values for each categorical columns\ncol = df_cat.columns\nfor i in col:\n    n = list(df_cat[i].unique())\n    print(\"Unique values for \", i)\n    print(n)","09ef9070":"sns.catplot('hospital_admit_source', data= train, kind='count', alpha=0.7, height=6, aspect= 3.5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['hospital_admit_source'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Hospital Admit Source for train', fontsize = 20, color = 'black')\nplt.show()","c4cd7cc8":"sns.catplot('hospital_admit_source', data= test, kind='count', alpha=0.7, height=6, aspect= 3.5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test['hospital_admit_source'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Hospital Admit Source for test', fontsize = 20, color = 'black')\nplt.show()","f74c5340":"sns.catplot('icu_admit_source', data= train, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['icu_admit_source'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Admit Source for train', fontsize = 20, color = 'black')\nplt.show()","2df61697":"sns.catplot('icu_admit_source', data= test, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test['icu_admit_source'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Admit Source for test', fontsize = 20, color = 'black')\nplt.show()","43163e78":"sns.catplot('icu_stay_type', data= train, kind='count', alpha=0.7, height=6, aspect= 3.5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['icu_stay_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Stay Type for train', fontsize = 20, color = 'black')\nplt.show()","23160032":"sns.catplot('icu_stay_type', data= test, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test['icu_stay_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Stay Type for test', fontsize = 20, color = 'black')\nplt.show()","80d6b784":"sns.catplot('icu_type', data= train, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Type for train', fontsize = 20, color = 'black')\nplt.show()","f6a5ef35":"sns.catplot('icu_type', data= test, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Type for test', fontsize = 20, color = 'black')\nplt.show()","29943e36":"sns.catplot('apache_3j_bodysystem', data= train, kind='count', alpha=0.7, height=4, aspect= 5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of apache_3j_bodysystem for train', fontsize = 20, color = 'black')\nplt.show()","e8c225c3":"sns.catplot('apache_3j_bodysystem', data= test, kind='count', alpha=0.7, height=4, aspect= 5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of apache_3j_bodysystem for test', fontsize = 20, color = 'black')\nplt.show()","231c425c":"sns.catplot('apache_2_bodysystem', data= train, kind='count', alpha=0.7, height=4, aspect= 5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of apache_2_bodysystem for train', fontsize = 20, color = 'black')\nplt.show()","12a20902":"sns.catplot('apache_2_bodysystem', data= test, kind='count', alpha=0.7, height=4, aspect= 5)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of apache_2_bodysystem for test', fontsize = 20, color = 'black')\nplt.show()","90a9895a":"combined_dataset['hospital_admit_source'] = combined_dataset['hospital_admit_source'].replace({'Other ICU': 'ICU','ICU to SDU':'SDU', 'Step-Down Unit (SDU)': 'SDU',\n                                                                                               'Other Hospital':'Other','Observation': 'Recovery Room','Acute Care\/Floor': 'Acute Care'})\n\n# combined_dataset['icu_type'] = combined_dataset['icu_type'].replace({'CCU-CTICU': 'Grpd_CICU', 'CTICU':'Grpd_CICU', 'Cardiac ICU':'Grpd_CICU'}) # Can be explored\n\ncombined_dataset['apache_2_bodysystem'] = combined_dataset['apache_2_bodysystem'].replace({'Undefined diagnoses': 'Undefined Diagnoses'})","01df74be":"sns.catplot('icu_type', data= combined_dataset, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = combined_dataset['icu_type'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of ICU Type for combined_dataset', fontsize = 20, color = 'black')\nplt.show()","510c8afe":"for i in combined_dataset.columns:\n    if combined_dataset[i].nunique() == 1:\n        print('With only 1 unique value: ', i)\n    if combined_dataset[i].nunique() == combined_dataset.shape[0]:\n        print('With all unique value: ', i)","161fabea":"# Dropping 'readmission_status', 'patient_id', along with 'gender'\ncombined_dataset = combined_dataset.drop(['readmission_status', 'patient_id', 'gender'], axis=1)","c512a832":"import copy\ntrain = copy.copy(combined_dataset[:train_len])\ntest = copy.copy(combined_dataset[train_len:])\nprint('combined dataset ', combined_dataset.shape)\nprint('train             ', train.shape)\nprint('test              ', test.shape)","edeb69d1":"# On train data\npd.set_option('display.max_rows', 500)\nNA_col_train = pd.DataFrame(train.isna().sum(), columns = ['NA_Count'])\nNA_col_train['% of NA'] = (NA_col_train.NA_Count\/len(train))*100\nNA_col_train.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","95ead953":"# On test data\npd.set_option('display.max_rows', 500)\nNA_col_test = pd.DataFrame(test.isna().sum(), columns = ['NA_Count'])\nNA_col_test['% of NA'] = (NA_col_test.NA_Count\/len(test))*100\nNA_col_test.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","deef4feb":"# Setting threshold of 80%\nNA_col_train = NA_col_train[NA_col_train['% of NA'] >= 80]\ncols_to_drop = NA_col_train.index.tolist()\n# cols_to_drop.remove('hospital_death')\ncols_to_drop","1474f3dd":"# Dropping columns with >= 80% of NAs\ncombined_dataset = combined_dataset.drop(cols_to_drop, axis=1)","f709bd58":"train = copy.copy(combined_dataset[:train_len])\ntest = copy.copy(combined_dataset[train_len:])\nprint('combined dataset ', combined_dataset.shape)\nprint('train             ', train.shape)\nprint('test              ', test.shape)","d390c81f":"# Suggestion Courtesy: Bruno Taveres - https:\/\/www.kaggle.com\/c\/widsdatathon2020\/discussion\/130532\n# Adding 2 apache columns as well\n\n# Import IterativeImputer from fancyimpute\nfrom fancyimpute import IterativeImputer\n\n# Initialize IterativeImputer\nmice_imputer = IterativeImputer()\n\n# Impute using fit_tranform on diabetes\ntrain[['age', 'height', 'weight', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']] = mice_imputer.fit_transform(train[['age', 'height', 'weight', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']])\ntest[['age', 'height', 'weight', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']] = mice_imputer.fit_transform(test[['age', 'height', 'weight', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']])","05a105a4":"print('Train check')\nprint(train[['age', 'height', 'weight', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']].isna().sum())\nprint('Test check')\nprint(test[['age', 'height', 'weight', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']].isna().sum())","ef6065de":"train['new_bmi'] = (train['weight']*10000)\/(train['height']*train['height'])\ntrain[['bmi', 'new_bmi', 'weight', 'height']].head(10)","b07171b2":"train['bmi'] = train['bmi'].fillna(train['new_bmi'])\ntrain[['bmi', 'new_bmi', 'weight', 'height']].head(10)","b51f5218":"train = train.drop(['new_bmi'], axis = 1)","6966e146":"test['new_bmi'] = (test['weight']*10000)\/(test['height']*test['height'])\ntest[['bmi', 'new_bmi', 'weight', 'height']].head(10)","15c1ca8f":"test['bmi'] = test['bmi'].fillna(test['new_bmi'])\ntest[['bmi', 'new_bmi', 'weight', 'height']].head(10)","ba3fbbe3":"test = test.drop(['new_bmi'], axis = 1)","ff0a8109":"print('For Train')\nd1 = train.nunique()\nprint(sorted(d1))\nprint(\"==============================\")\nprint('For Test')\nd2 = test.nunique()\nprint(sorted(d2))","230c9a17":"col_train = train.columns\ncol_test = test.columns","41f96fc0":"l1 = []\nfor i in col_train:\n    if train[i].nunique() <= 11:\n        l1.append(i)\n               \nl1.remove('hospital_death')","002f449b":"l2 = []\nfor i in col_test:\n    if test[i].nunique() <= 11:\n        l2.append(i)\n        \nl2.remove('hospital_death')","5b37bcd3":"# Checking the columns in train and test are same or not\ndf = pd.DataFrame(l1, columns = ['train'])\ndf['test'] = pd.DataFrame(l2)\ndf","1be7c742":"train[l1] = train[l1].apply(lambda x: x.astype('category'), axis=0)\ntest[l2] = test[l2].apply(lambda x: x.astype('category'), axis=0)\nprint('train dtypes:')\nprint(train[l1].dtypes)\nprint('======================================')\nprint('test dtypes:')\nprint(test[l1].dtypes)","3400b6e6":"cols = train.columns\nnum_cols = train._get_numeric_data().columns\ncat_cols = list(set(cols) - set(num_cols))\ncat_cols","29e0b216":"from sklearn.preprocessing import LabelEncoder\nfor usecol in cat_cols:\n    train[usecol] = train[usecol].astype('str')\n    test[usecol] = test[usecol].astype('str')\n    \n    #Fit LabelEncoder\n    le = LabelEncoder().fit(\n            np.unique(train[usecol].unique().tolist()+ test[usecol].unique().tolist()))\n\n    #At the end 0 will be used for dropped values\n    train[usecol] = le.transform(train[usecol])+1\n    test[usecol]  = le.transform(test[usecol])+1\n    \n    train[usecol] = train[usecol].replace(np.nan, '').astype('int').astype('category')\n    test[usecol]  = test[usecol].replace(np.nan, '').astype('int').astype('category')","472851cb":"train.hospital_death.value_counts()","935dfe9e":"# Separate majority and minority classes\ndf_majority = train[train.hospital_death==0]\ndf_minority = train[train.hospital_death==1]","4649b3d6":"# Resampling the minority levels to match the majority level\n# Upsample minority class\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=83798,    # to match majority class\n                                 random_state= 303) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndf_upsampled.hospital_death.value_counts()","fa6cd779":"from sklearn.model_selection import train_test_split\nTrain, Validation = train_test_split(df_upsampled, test_size = 0.3, random_state = 333)","e5ecc890":"X_train = Train.copy().drop('hospital_death', axis = 1)\ny_train = Train[['encounter_id','hospital_death']]\nX_val = Validation.copy().drop('hospital_death', axis = 1)\ny_val = Validation[['encounter_id','hospital_death']]\nX_test = test.copy().drop('hospital_death', axis = 1)\ny_test = test[['encounter_id', 'hospital_death']]","bcc5ef47":"X_train.set_index('encounter_id', inplace = True)\ny_train.set_index('encounter_id', inplace = True)\nX_val.set_index('encounter_id', inplace = True)\ny_val.set_index('encounter_id', inplace = True)\nX_test.set_index('encounter_id', inplace = True)\ny_test.set_index('encounter_id', inplace = True)","fc605693":"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=4, aspect= 3)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['hospital_death'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of hospital_death', fontsize = 20, color = 'black')\nplt.show()","a2adaee6":"import lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nclf = lgb.LGBMClassifier(silent=True, random_state = 333, metric='roc_auc', n_jobs=4)","8833f384":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparams ={'cat_smooth' : sp_randint(1, 100), 'min_data_per_group': sp_randint(1,1000), 'max_cat_threshold': sp_randint(1,100)}","80663418":"fit_params={\"early_stopping_rounds\":2, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_train, y_train),(X_val,y_val)],\n            'eval_names': ['train','valid'],\n            'verbose': 300,\n            'categorical_feature': 'auto'}","ac6e7332":"gs = RandomizedSearchCV( estimator=clf, param_distributions=params, scoring='roc_auc',cv=3, refit=True,random_state=333,verbose=True)","32785c8d":"gs.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","617ce3d9":"gs.best_params_, gs.best_score_","660c2fa3":"clf2 = lgb.LGBMClassifier(random_state=304, metric = 'roc_auc', cat_smooth = 38, max_cat_threshold = 73, min_data_per_group = 73, n_jobs=4)","8cd53f17":"params_2 = {'learning_rate': [0.08, 0.85, 0.09],   \n            'num_iterations': sp_randint(1000,3000)}","ed658ece":"gs2 = RandomizedSearchCV(estimator=clf2, param_distributions=params_2, scoring='roc_auc',cv=3,refit=True,random_state=333,verbose=True)","392c61a9":"gs2.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs2.best_score_, gs2.best_params_))","5211756c":"gs2.best_params_, gs2.best_score_","d17d7231":"clf3 = lgb.LGBMClassifier(**clf2.get_params())\nclf3.set_params(**gs2.best_params_)","b7c50383":"params_3 = {'scale_pos_weight': sp_randint(1,15)}","233ddb02":"gs3 = RandomizedSearchCV(estimator=clf3, param_distributions=params_3, scoring='roc_auc',cv=3,refit=True,random_state=333,verbose=True)","0aa6f799":"gs3.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs3.best_score_, gs3.best_params_))","dc35d2f4":"gs3.best_params_, gs3.best_score_","477c1737":"clf4 = lgb.LGBMClassifier(**clf3.get_params())\nclf4.set_params(**gs3.best_params_)","64358ced":"params_4 = {'colsample_bytree': sp_uniform(loc=0.4, scale=0.6), 'num_leaves': sp_randint(500, 5000), \n            'min_child_samples': sp_randint(100,500), 'min_child_weight': [1e-2, 1e-1, 1, 1e1]}","bc9edd9c":"gs4 = RandomizedSearchCV(estimator=clf4, param_distributions=params_4, scoring='roc_auc',cv=2,refit=True,random_state=333,verbose=True)","75af448a":"gs4.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs4.best_score_, gs4.best_params_))","4aefaef0":"gs4.best_params_, gs4.best_score_","6b9d59b3":"clf5 = lgb.LGBMClassifier(**clf4.get_params())\nclf5.set_params(**gs4.best_params_)","50d6d72d":"params_5 = {'max_bin': sp_randint(100, 1500), 'max_depth': sp_randint(1, 15), \n            'min_data_in_leaf': sp_randint(500,3500)}","de30dd0a":"gs5 = RandomizedSearchCV(estimator=clf5, param_distributions=params_5, scoring='roc_auc',cv=2,refit=True,random_state=333,verbose=True)","d7482c44":"gs5.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs5.best_score_, gs5.best_params_))","b053d449":"gs5.best_params_, gs5.best_score_","0292e9f4":"clf6 = lgb.LGBMClassifier(**clf5.get_params())\nclf6.set_params(**gs5.best_params_)","34cadcd1":"params_6 = {'reg_lambda': sp_randint(1, 30), 'boosting': ['goss', 'dart']}","29c52cee":"gs6 = RandomizedSearchCV(estimator=clf6, param_distributions=params_6, scoring='roc_auc',cv=2,refit=True,random_state=333,verbose=True)","1137148a":"gs6.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs6.best_score_, gs6.best_params_))","097d308c":"gs6.best_params_","03018a24":"final_params = {**gs.best_params_, **gs2.best_params_, **gs3.best_params_, **gs4.best_params_, **gs5.best_params_, **gs6.best_params_,\n               'bagging_fraction': 0.6, 'feature_fraction': 0.8, 'scoring':'roc_auc', 'metric':'auc', 'objective': 'binary'}\nfinal_params","ff3098d1":"lgbm_train = lgb.Dataset(X_train, y_train, categorical_feature=cat_cols)\nlgbm_val = lgb.Dataset(X_val, y_val, reference = lgbm_train)","ff20eb29":"evals_result = {}  # to record eval results for plotting\nmodel_lgbm = lgb.train(final_params,\n                lgbm_train,\n                num_boost_round=250,\n                valid_sets=[lgbm_train, lgbm_val],\n                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n                categorical_feature= [150],\n                evals_result=evals_result,\n                verbose_eval=100)","c5396745":"ax = lgb.plot_metric(evals_result, metric='auc', figsize=(15, 8))\nplt.show()","c42514b6":"test[\"hospital_death\"] = model_lgbm.predict(X_test, pred_contrib=False)","5be95ea3":"test[[\"encounter_id\",\"hospital_death\"]].to_csv(\"submission_lgbm.csv\",index=False)\ntest[[\"encounter_id\",\"hospital_death\"]].head()","cbbae586":"test[[\"encounter_id\",\"hospital_death\"]].describe()","5a523e2e":"![WiDS.PNG](attachment:WiDS.PNG)","72ba903c":"## 5. Dropping few column\/s with single value and all unique values <a class=\"anchor\" id=\"5\"><\/a>\n[Back to Table of Contents](#0.1)","c5287e24":"## 1. Import libraries, Memory reduction <a class=\"anchor\" id=\"1\"><\/a>\n[Back to Table of Contents](#0.1)","3442461f":"## 12. Model - LGBM <a class=\"anchor\" id=\"12\"><\/a>\n#### Grid Search for 'cat_smooth', 'min_data_per_group', and 'max_cat_threshold'\n[Back to Table of Contents](#0.1)","a87f6653":"## 9. Extracting columns to change to Categorical <a class=\"anchor\" id=\"9\"><\/a>\n[Back to Table of Contents](#0.1)","a7ebf303":"# Forked from the Fork of my own kernel - Double Recurrent?","7524ff71":"## 4. Let's form Teams [EDA] <a class=\"anchor\" id=\"4\"><\/a>\n* **'hospital_admit_source':** \n *     Grouping: ['Other ICU', 'ICU']; ['ICU to SDU', 'Step-Down Unit (SDU)']; ['Other Hospital', 'Other']; ['Recovery Room','Observatoin']\n *     Renaming: Acute Care\/Floor to Acute Care\n* **'icu_type':** \n *     Grouping of the following can be explored: ['CCU-CTICU', 'CTICU', 'Cardiac ICU']\n* **'apache_2_bodysystem':** \n *     Grouping of the following can be explored: ['Undefined Diagnoses', 'Undefined diagnoses']\n \n \n\n[Back to Table of Contents](#0.1)","ab52ee43":"## 8. Using formula to impute BMI <a class=\"anchor\" id=\"8\"><\/a>\n#### [Formula: BMI = Weight(kg)\/(Height(m)* Height(m))]\n[Back to Table of Contents](#0.1)","ba0be97b":"## 18. Final Build <a class=\"anchor\" id=\"18\"><\/a>\n[Back to Table of Contents](#0.1)","d16d6c04":"## 11. Splitting and preparing to Model <a class=\"anchor\" id=\"11\"><\/a>\n[Back to Table of Contents](#0.1)","edc983e9":"## 16. Grid Search for 'max_bin', 'max_depth', 'min_data_in_leaf' <a class=\"anchor\" id=\"16\"><\/a>\n[Back to Table of Contents](#0.1)","635e75fa":"## 7. MICE Imputation <a class=\"anchor\" id=\"7\"><\/a>\n#### ['hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']\n[Back to Table of Contents](#0.1)","c5e0e5bf":"## 17. Grid Search for 'reg_lambda', 'boosting' <a class=\"anchor\" id=\"17\"><\/a>\n[Back to Table of Contents](#0.1)","81595621":"## 14. Grid Search for 'scale_pos_weight' <a class=\"anchor\" id=\"14\"><\/a>\n[Back to Table of Contents](#0.1)","0a278027":"## 3. Visualization of categorical variables <a class=\"anchor\" id=\"3\"><\/a>\n#### ['hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']\n[Back to Table of Contents](#0.1)","56a6705a":"## 13. Grid Search for 'learning_rate' & 'num_iterations' <a class=\"anchor\" id=\"13\"><\/a>\n[Back to Table of Contents](#0.1)","4cdc16dc":"## 6. Checking NAs for initial column clipping <a class=\"anchor\" id=\"6\"><\/a>\n[Back to Table of Contents](#0.1)","d0658cb3":"# Acknowledgements\n\nkernels: \n* https:\/\/www.kaggle.com\/jayjay75\/wids2020-lgb-starter-adversarial-validation\n* https:\/\/www.kaggle.com\/danofer\/wids-2020-starter-catboost-0-9045-lb\n* https:\/\/www.kaggle.com\/mlisovyi\/lightgbm-hyperparameter-optimisation-lb-0-761\n\nDiscussion & Comments:\n* https:\/\/www.kaggle.com\/c\/widsdatathon2020\/discussion\/130532 (Thanks @brunotavares for the suggestions)\n* @arashnic - Thanks for the suggestions on reducing overfitting \n* Thank you everyone for the support. Code on....","e24bcf52":"## 15. Grid Search for 'colsample_bytree', 'num_leaves', 'min_child_samples', 'min_child_weight' <a class=\"anchor\" id=\"15\"><\/a>\n[Back to Table of Contents](#0.1)","df82cc82":"## 2. Reading & Checking categorical variables <a class=\"anchor\" id=\"2\"><\/a>\n[Back to Table of Contents](#0.1)\n","24f479f0":"## 10. Resampling: Minority to Majority <a class=\"anchor\" id=\"10\"><\/a>\n[Back to Table of Contents](#0.1)","d1ea4dd7":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries, Memory reduction](#1)\n1. [Reading & Checking categorical variables](#2)\n1. [Visualization of categorical variables](#3)\n1. [EDA - Let's form Teams](#4)\n1. [Dropping few column\/s with single value and all unique values](#5)\n1. [Checking NAs for initial column clipping](#6)\n1. [MICE Imputation](#7)\n1. [Using formula to impute BMI](#8)\n1. [Extracting columns to change to Categorical](#9)\n1. [Resampling: Minority to Majority](#10)\n1. [Splitting and preparing to Model](#11)\n1. [Model - LGBM](#12)  \n1. [Grid Search for 'learning_rate' & 'num_iterations'](#13)\n1. [Grid Search for 'scale_pos_weight'](#14)\n1. [Grid Search for 'colsample_bytree', 'num_leaves', 'min_child_samples', 'min_child_weight'](#15)\n1. [Grid Search for 'max_bin', 'max_depth', 'min_data_in_leaf'](#16)\n1. [Grid Search for 'reg_lambda', 'boosting'](#17)\n1. [Final Build](#18)"}}