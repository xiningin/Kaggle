{"cell_type":{"abec7d53":"code","b0080f64":"code","86f5eab9":"code","9b089a5c":"code","5acc42af":"code","82139b90":"code","463042c2":"code","e605334d":"code","45ec9c54":"code","ee13ce9e":"code","960dafc7":"code","7c7fe94b":"code","6c89fbc0":"code","eb75bbfd":"code","785ca976":"code","8eb9241a":"code","ee6e38df":"code","ec96e0e3":"code","25a9aba8":"code","6df6f050":"code","448f5c85":"code","57ffb067":"code","b7d9ac7b":"code","48ed6faf":"code","244ba306":"code","fa8c0d92":"code","a1b5ce18":"code","f0ec74c0":"code","c12453b9":"code","eb56e7b1":"code","ea3da03e":"code","a660bde6":"code","8993f8fa":"code","f1a504f1":"code","77286aea":"code","7e35ef36":"code","b0608c58":"code","5c0d845e":"code","5953d617":"code","44ffa7c0":"code","00f205c1":"code","5138258f":"code","5acc973a":"code","a5a38057":"code","50a6616b":"code","c8e86d9a":"code","bd9836a4":"code","9cb31944":"code","656d02d4":"code","6999eabc":"code","042c2d5a":"code","d4a496a8":"code","790911b1":"code","014f2747":"code","f1cedaa1":"code","c41bb95f":"code","4621c20c":"code","d13bb6a4":"markdown","12a11a25":"markdown","490e5551":"markdown","fac8dd91":"markdown","224400d7":"markdown","c7133d9e":"markdown","0f265055":"markdown","78a27d44":"markdown","4b50544a":"markdown","a94c8ce5":"markdown","53eae219":"markdown","c81e1f81":"markdown","d8df005b":"markdown","5ac79799":"markdown","35795491":"markdown","bafc8e1b":"markdown","14689a3d":"markdown","f80703f8":"markdown","1c8628ee":"markdown","fff23603":"markdown","a9183d6b":"markdown","6a4d91fe":"markdown","ea9d4695":"markdown","7a0018fb":"markdown","64ffb4a7":"markdown","17ec4f76":"markdown","3353e2da":"markdown","282296f0":"markdown","2e7033fd":"markdown","d368b29b":"markdown","ab1f0276":"markdown"},"source":{"abec7d53":"# Importing basic libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b0080f64":"# Reading the data\ndf = pd.read_csv('..\/input\/framingham-heart-study-dataset\/framingham.csv')\n\n# To display first 5 rows in the dataframe \ndf.head()","86f5eab9":"# To display last 5 rows in the dataframe \ndf.tail()","9b089a5c":"# To know the data type and null values if any\ndf.info()","5acc42af":"# Percentage of null values in each column\n(df.isnull().sum()\/df.shape[0])*100","82139b90":"df.education.fillna(0,inplace=True)","463042c2":"df.cigsPerDay.fillna(df.cigsPerDay.where(df.currentSmoker==1).median(),inplace=True)","e605334d":"df.BPMeds.fillna(0,inplace=True)","45ec9c54":"df['totChol'].fillna(df.totChol.median(),inplace=True)","ee13ce9e":"df['BMI'].fillna(df.BMI.median(),inplace=True)","960dafc7":"df['heartRate'].fillna(df['heartRate'].where(df['currentSmoker']==1).median(),inplace=True)","7c7fe94b":"df['glucose'].fillna(df['glucose'].where(df['diabetes']==0).median(),inplace=True)","6c89fbc0":"# Checking if there are any misisng values:\n(df.isnull().sum()\/df.shape[0])*100","eb75bbfd":"# Five point summary of clean data\ndf.describe().T","785ca976":"# To know the data type of column are affected\ndf.info()","8eb9241a":"# Names of columns of dataframe\ndf.columns","ee6e38df":"# List of columns names with contineous values\ncol = ['age','totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']","ec96e0e3":"# To find outliers\nfor i in col:\n    sns.boxplot(df[i])\n    plt.show()","25a9aba8":"# Distribution of the contineous values column\nfor i in col:\n    sns.distplot(df[i])\n    plt.show()","6df6f050":"# Making a copy of the clean dataframe\ndf1 = df.copy()\n\n# To remove outliers\nfor i in col:\n    q1 = df1[i].quantile(q=0.25)\n    q2 = df1[i].quantile()\n    q3 = df1[i].quantile(q=0.75)\n    iqr = q3-q1\n    ul = q3+1.5*iqr\n    ll = q1-1.5*iqr\n\n    df1 = df1[(df1[i]<ul ) & (df1[i]>ll)] ","448f5c85":"# Checking distribution of the contineous values column after outliers treatment\n\nfor i in col:\n    sns.distplot(df1[i])\n    plt.show()","57ffb067":"print('There were {} rows before outlier treatment.'.format(df.shape[0]))\nprint('There are {} rows after outlier treatment.'.format(df1.shape[0]))\nprint('After outlier treatment number of rows lost are {}.'.format(df.shape[0] - df1.shape[0]))","b7d9ac7b":"# Ratio of CHD=1 and CHD=0\ndf1['TenYearCHD'].value_counts(normalize=True)","48ed6faf":"# Ploting the ratio\nsns.countplot(df['TenYearCHD'],)\nplt.show()","244ba306":"# Correlation plot using heatmap\ncor = df.corr()\nplt.figure(figsize=(15,9))\nsns.heatmap(cor,annot=True)\nplt.show()","fa8c0d92":"X = df1.drop(['TenYearCHD'], axis=1)\ny = df1['TenYearCHD']","a1b5ce18":"import statsmodels.api as sm\n\nX_const = sm.add_constant(X)","f0ec74c0":"model = sm.Logit(y, X)\nresult = model.fit()\nresult.summary()","c12453b9":"## Backward elimination to drop insignificant variables one by one\n\ncols = list(X.columns)\np = []\nwhile len(cols)>1:\n    X = X[cols]\n    model= sm.Logit(y, X).fit().pvalues\n    p =pd.Series(model.values[1:],index=X.columns[1:])\n    pmax = max(p)\n    pid = p.idxmax()\n    if pmax>0.05:\n        cols.remove(pid)\n        print('Variable removed:', pid, pmax)\n    else:\n        break\ncols   ","eb56e7b1":"# Keeping the significant variables\nX_sig = df1[['male', 'age', 'education', 'cigsPerDay', 'prevalentStroke', 'prevalentHyp', 'sysBP', 'diaBP', 'BMI',\n     'heartRate', 'glucose']]","ea3da03e":"X_sig.describe().T","a660bde6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() \nX_sig = scaler.fit_transform(X_sig)","8993f8fa":"# using significant feature\nmodel = sm.Logit(y, X_sig)\nresult = model.fit()\nresult.summary()","f1a504f1":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_sig, y, test_size=0.30, random_state=1)","77286aea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nlogreg = LogisticRegression(solver='liblinear', fit_intercept=True) \n\nlogreg.fit(X_train, y_train)\n\ny_prob_train = logreg.predict_proba(X_train)[:,1]\ny_pred_train = logreg.predict (X_train)\n\nprint('Confusion Matrix - Train: ', '\\n', confusion_matrix(y_train, y_pred_train))\nprint('\\nOverall accuracy - Train: ', accuracy_score(y_train, y_pred_train))\n\n\ny_prob = logreg.predict_proba(X_test)[:,1]\ny_pred = logreg.predict (X_test)\n\nprint('\\nConfusion Matrix - Test: ','\\n', confusion_matrix(y_test, y_pred))\nprint('\\nOverall accuracy - Test: ','\\n', accuracy_score(y_test, y_pred))\nprint('\\nClassification report for test:\\n',classification_report(y_test,y_pred))","7e35ef36":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=1)","b0608c58":"X_train_sm, y_train_sm = smote.fit_resample(X_train,y_train)","5c0d845e":"logreg_sm = LogisticRegression(solver='liblinear', fit_intercept=True) \n\nlogreg_sm.fit(X_train_sm, y_train_sm)\n\ny_prob_train = logreg_sm.predict_proba(X_train_sm)[:,1]\ny_pred_train = logreg_sm.predict (X_train_sm)\n\nprint('Confusion Matrix - Train: ', '\\n', confusion_matrix(y_train_sm, y_pred_train))\nprint('\\nOverall accuracy - Train: ', accuracy_score(y_train_sm, y_pred_train))\n\n\ny_prob = logreg_sm.predict_proba(X_test)[:,1]\ny_pred = logreg_sm.predict (X_test)\n\nprint('\\nConfusion Matrix - Test: ','\\n', confusion_matrix(y_test, y_pred))\nprint('\\nOverall accuracy - Test: ', accuracy_score(y_test, y_pred))\nprint('\\nClassification report for test:\\n',classification_report(y_test,y_pred))","5953d617":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train_sm, y_train_sm)\n\ny_pred_train = dt.predict(X_train_sm)\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)\n\nprint('Classification report for test:\\n',classification_report(y_test,y_pred))","44ffa7c0":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","00f205c1":"dt = DecisionTreeClassifier()\n\nparams = {'max_depth' : [2,3,4,5,6,7,8],\n        'min_samples_split': [2,3,4,5,6,7,8,9,10],\n        'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10]}\n\ngsearch = GridSearchCV(dt, param_grid=params, cv=3)\n\ngsearch.fit(X,y)\n\ngsearch.best_params_","5138258f":"# DT using best parameters\ndt = DecisionTreeClassifier(**gsearch.best_params_)\n\ndt.fit(X_train_sm, y_train_sm)\n\ny_pred_train = dt.predict(X_train_sm)\ny_prob_train = dt.predict_proba(X_train_sm)[:,1]\n\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)[:,1]\n\nprint('\\nClassification report for test:\\n',classification_report(y_test,y_pred))","5acc973a":"from scipy.stats import randint as sp_randint\n\ndt = DecisionTreeClassifier(random_state=1)\n\nparams = {'max_depth' : sp_randint(2,10),\n        'min_samples_split': sp_randint(2,50),\n        'min_samples_leaf': sp_randint(1,20),\n         'criterion':['gini', 'entropy']}\n\nrand_search = RandomizedSearchCV(dt, param_distributions=params, cv=3, \n                                 random_state=1)\n\nrand_search.fit(X, y)\nprint(rand_search.best_params_)","a5a38057":"# DT using best parameters\ndt = DecisionTreeClassifier(**rand_search.best_params_)\n\ndt.fit(X_train_sm, y_train_sm)\n\ny_pred_train = dt.predict(X_train_sm)\ny_prob_train = dt.predict_proba(X_train_sm)[:,1]\n\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)[:,1]\n\nprint('Classification report for test:\\n',classification_report(y_test,y_pred))","50a6616b":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=10, random_state=1)\n\nrfc.fit(X_train_sm, y_train_sm)\n\ny_pred_train = rfc.predict(X_train_sm)\ny_prob_train = rfc.predict_proba(X_train_sm)[:,1]\n\ny_pred = rfc.predict(X_test)\ny_prob = rfc.predict_proba(X_test)[:,1]\n\nprint('Classification report for test:\\n',classification_report(y_test,y_pred))\n","c8e86d9a":"from scipy.stats import randint as sp_randint\n\nrfc = RandomForestClassifier(random_state=1)\n\nparams = {'n_estimators': sp_randint(5,25),\n    'criterion': ['gini', 'entropy'],\n    'max_depth': sp_randint(2, 10),\n    'min_samples_split': sp_randint(2,20),\n    'min_samples_leaf': sp_randint(1, 20),\n    'max_features': sp_randint(2,15)}\n\nrand_search_rfc = RandomizedSearchCV(rfc, param_distributions=params,\n                                 cv=3, random_state=1)\n\nrand_search_rfc.fit(X, y)\nprint(rand_search_rfc.best_params_)","bd9836a4":"# RFC using best parameters\nrfc = RandomForestClassifier(**rand_search_rfc.best_params_)\n\nrfc.fit(X_train_sm, y_train_sm)\n\ny_pred_train = rfc.predict(X_train_sm)\ny_prob_train = rfc.predict_proba(X_train_sm)[:,1]\n\ny_pred = rfc.predict(X_test)\ny_prob = rfc.predict_proba(X_test)[:,1]\n\nprint('Classification report for test:\\n',classification_report(y_test,y_pred))","9cb31944":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()","656d02d4":"knn.fit(X_train_sm, y_train_sm)\n\ny_pred_train = knn.predict(X_train_sm)\ny_prob_train = knn.predict_proba(X_train_sm)[:,1]\n\ny_pred = knn.predict(X_test)\ny_prob = knn.predict_proba(X_test)[:,1]\n\nprint('Classification report for test:\\n',classification_report(y_test,y_pred))","6999eabc":"knn = KNeighborsClassifier()\n\nparams = {'n_neighbors': sp_randint(1,25),\n        'p': sp_randint(1,5)}\n\nrand_search_knn = RandomizedSearchCV(knn, param_distributions=params,\n                                 cv=3, random_state=1)\nrand_search_knn.fit(X, y)\nprint(rand_search.best_params_)","042c2d5a":"# KNN using best parameters\n\nknn = KNeighborsClassifier(**rand_search_knn.best_params_)\n\nknn.fit(X_train_sm, y_train_sm)\n\ny_pred_train = knn.predict(X_train_sm)\ny_prob_train = knn.predict_proba(X_train_sm)[:,1]\n\ny_pred = knn.predict(X_test)\ny_prob = knn.predict_proba(X_test)[:,1]\n\nprint('Classification report for test:\\n',classification_report(y_test,y_pred))","d4a496a8":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression","790911b1":"lr = LogisticRegression(solver='liblinear')\ndt = DecisionTreeClassifier(**rand_search.best_params_)\nrfc = RandomForestClassifier(**rand_search_rfc.best_params_)\nknn = KNeighborsClassifier(**rand_search_knn.best_params_)","014f2747":"# Without using weights\nclf = VotingClassifier(estimators=[('lr',lr), ('dt',dt),('rfc',rfc), ('knn',knn)], \n                       voting='soft')\nclf.fit(X_train_sm, y_train_sm)\ny_pred_train = clf.predict(X_train_sm)\ny_prob_train = clf.predict_proba(X_train_sm)[:,1]\n\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:,1]","f1cedaa1":"print('Classification report for test:\\n',classification_report(y_test,y_pred))","c41bb95f":"# Using weights\nclf = VotingClassifier(estimators=[('lr',lr),('dt',dt) ,('rfc',rfc), ('knn',knn)], \n                       voting='soft', weights=[4,1,3,2])\nclf.fit(X_train_sm, y_train_sm)\ny_pred_train = clf.predict(X_train_sm)\ny_prob_train = clf.predict_proba(X_train_sm)[:,1]\n\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:,1]","4621c20c":"print('Classification report for test:\\n',classification_report(y_test,y_pred))","d13bb6a4":"#### Tuning:","12a11a25":"The chart shows that 13.3% of the group had Heart disease.","490e5551":"- From df.info() we can see that there are null values. Hence cleaning the data is important.","fac8dd91":"### Using stats model:","224400d7":"#### HyperParameter Tuning(Randomsearch):","c7133d9e":"### Logestic Regression:","0f265055":"## Exploratory Data Analysis:","78a27d44":"- There are outliers in the columns 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose'\n- Hence removing outliers may be needed","4b50544a":"##### Missing values in column cigsPerDay are from smokers, hence imputing it with median of current smoker:","a94c8ce5":"### Splitting the data into train and test with significant features:","53eae219":"### Decision Tree:","c81e1f81":"- After outlier treatment, the distribution look normal.","d8df005b":"### Standardization due to variation in scales:","5ac79799":"##### Missing values in column BPMeds may be because they are not on medication, hence filling missing values with '0':","35795491":"## Modelling:","bafc8e1b":"##### Splitting the data into dependant and independent:","14689a3d":"##### Missing values in column totChol are being imputed with median:","f80703f8":"##### In column heartRate, there is only 1 missing value in the column and its for active smoker, hence filling it with median heartrate for Smoker:","1c8628ee":"#### Hyperparameter Tuning of Random Forest:","fff23603":"## k-NN Classifier:","a9183d6b":"## Data Cleaning:","6a4d91fe":"### Over-sampling using SMOTE:","ea9d4695":"##### Missing values in column BMI are being imputed with median:","7a0018fb":"- Since the F1-score for 1 is 0 even though for 0 is 0.93, the model cannot be accepted. Hence imbalance data should be treated.","64ffb4a7":"### Random Forest Classifier:","17ec4f76":"#### Hyperparameter Tuning(GridSearch):","3353e2da":"##### Missing values for the column eduction may be they arent litrate. Hence filling the missing values with '0':","282296f0":"## Stacking Algorithms","2e7033fd":"## Importing basic libraries and reading the data:","d368b29b":"##### In column glucose, majority of missing values are not diabetic hence the missing values imouted will Be the median of non diabetic:","ab1f0276":"### Creating model after oversamplying:"}}