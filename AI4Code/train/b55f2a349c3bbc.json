{"cell_type":{"566ff785":"code","8cddc1c6":"code","055f1256":"code","1630b196":"code","d080ff29":"code","8934871b":"code","d70245d3":"code","3e76e38f":"code","899ea516":"code","e5540a3b":"code","722d24f4":"code","24c14a7b":"code","509b2f32":"code","8dfe3bb8":"code","6423dce0":"code","1901ea3c":"code","02da5eb7":"code","0c5ba6e8":"code","d51205f5":"code","dc72c9bd":"code","af1a5726":"code","f82ae048":"code","96acf9c3":"code","b3af2f83":"code","8a238bec":"code","9c5d3ffb":"code","f98acfeb":"code","f5fad5b8":"code","e74d5c96":"code","da951272":"code","183ba6e3":"code","0168031f":"code","a4bcb6de":"code","29819a14":"code","d30d6158":"code","c6a5a5ab":"code","89e663b2":"code","1bca3bc9":"code","faa4c453":"code","124422ac":"code","1d2e0fa4":"code","916db4fc":"markdown","f28c5206":"markdown","1417ca52":"markdown","4116c660":"markdown","d81a19a5":"markdown","07e8abdd":"markdown","e55c878a":"markdown","4aa8db17":"markdown","62374a83":"markdown","29fe2a9f":"markdown","d88f33fe":"markdown","9eb8cfa4":"markdown","0fb56a96":"markdown"},"source":{"566ff785":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8cddc1c6":"df= pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","055f1256":"df.shape","1630b196":"df.head()","d080ff29":"df.info()","8934871b":"sns.displot(df['Y house price of unit area'], kde=True, aspect=2, color='purple')\nplt.show()","d70245d3":"fig, axes= plt.subplots(nrows=3, ncols=2, figsize=(15,15))\nfig.subplots_adjust(wspace=0.3, hspace=0.3)\n\nfor i in range(1, df.shape[1]-1):\n    axes[(i-1)\/\/2, (i+1)%2].set_title(f'chart {i}').set_size(20)\n    sns.scatterplot(data=df, x=df.iloc[:, i], y='Y house price of unit area', ax=axes[(i-1)\/\/2, (i+1)%2])","3e76e38f":"fig = plt.figure(figsize=(10,5))\nsns.heatmap(df.iloc[:, 1:].corr(), annot=True, cmap='Greens')\nplt.show()","899ea516":"X = df.drop(['Y house price of unit area', 'No'],axis=1)\ny = df['Y house price of unit area']","e5540a3b":"from sklearn.preprocessing import PolynomialFeatures","722d24f4":"polynomial_converter= PolynomialFeatures(degree=3, include_bias=False)\npoly_features= polynomial_converter.fit_transform(X)","24c14a7b":"poly_features.shape","509b2f32":"from sklearn.model_selection import train_test_split","8dfe3bb8":"X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)","6423dce0":"from sklearn.preprocessing import StandardScaler","1901ea3c":"scaler= StandardScaler()\nscaler.fit(X_train)\n\nX_train= scaler.transform(X_train)\nX_test= scaler.transform(X_test)","02da5eb7":"from sklearn.linear_model import Ridge","0c5ba6e8":"ridge_model= Ridge(alpha=10)","d51205f5":"ridge_model.fit(X_train, y_train)","dc72c9bd":"#predict Test Data\ny_pred= ridge_model.predict(X_test)","af1a5726":"#Evaluating the Model\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nMAE= mean_absolute_error(y_test, y_pred)\nMSE= mean_squared_error(y_test, y_pred)\nRMSE= np.sqrt(MSE)","f82ae048":"pd.DataFrame([MAE, MSE, RMSE], index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","96acf9c3":"from sklearn.linear_model import RidgeCV","b3af2f83":"ridge_cv_model=RidgeCV(alphas=(0.1, 1, 10), scoring='neg_mean_absolute_error')\nridge_cv_model.fit(X_train, y_train)\nridge_cv_model.alpha_","8a238bec":"#Predicting Test Data\ny_pred_ridge= ridge_cv_model.predict(X_test)","9c5d3ffb":"MAE_ridge= mean_absolute_error(y_test, y_pred_ridge)\nMSE_ridge= mean_squared_error(y_test, y_pred_ridge)\nRMSE_ridge= np.sqrt(MSE_ridge)","f98acfeb":"pd.DataFrame([MAE_ridge, MSE_ridge, RMSE_ridge], index=['MAE', 'MSE', 'RMSE'], columns=['Ridge Metrics'])","f5fad5b8":"from sklearn.linear_model import LassoCV","e74d5c96":"lasso_cv_model= LassoCV(eps=0.01, n_alphas=100, cv=5)","da951272":"lasso_cv_model.fit(X_train, y_train)","183ba6e3":"lasso_cv_model.alpha_","0168031f":"y_pred_lasso= lasso_cv_model.predict(X_test)","a4bcb6de":"MAE_Lasso= mean_absolute_error(y_test, y_pred_lasso)\nMSE_Lasso= mean_squared_error(y_test, y_pred_lasso)\nRMSE_Lasso= np.sqrt(MSE_Lasso)","29819a14":"pd.DataFrame([MAE_Lasso, MSE_Lasso, RMSE_Lasso], index=['MAE', 'MSE', 'RMSE'], columns=['Lasso Metrics'])","d30d6158":"from sklearn.linear_model import ElasticNetCV","c6a5a5ab":"elastic_model= ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],cv=5, max_iter=100000)","89e663b2":"elastic_model.fit(X_train, y_train)","1bca3bc9":"elastic_model.l1_ratio_","faa4c453":"y_pred_elastic=elastic_model.predict(X_test)","124422ac":"MAE_Elastic= mean_absolute_error(y_test, y_pred_elastic)\nMSE_Elastic= mean_squared_error(y_test, y_pred_elastic)\nRMSE_Elastic= np.sqrt(MSE_Elastic)","1d2e0fa4":"pd.DataFrame([MAE_Elastic, MSE_Elastic, RMSE_Elastic], index=['MAE', 'MSE', 'RMSE'], columns=['Elastic Metrics'])","916db4fc":"# Ridge Regression:\nRidge regression refers to a type of linear regression where in order to get better predictions in the long term, we introduce a small amount of bias. It is also known as L2 regularization. In ridge regression, we have the same loss function with a slight alteration in the penalty term, as shown below:\n\n![Screenshot%20%28295%29.png](attachment:Screenshot%20%28295%29.png)\n\nAs we can see, the main difference between the above equation and the general equation for the loss function is that <span style=\"font-size:larger;\">$\\sum_{j=1}^{p} \\beta_{j}^2$<\/span> contains the squared value of the regression coefficients. The tuning parameter is \u03bb. Higher values of the coefficients represent a model with greater flexibility. To penalize the flexibility of our model, we use a tuning parameter that decides the extent of the penalty. To minimize the function, these coefficients should be small. L2 regularization ensures the coefficients do not rise too high.","f28c5206":"# Elastic Net:\nA third commonly used model of regression is the Elastic Net which incorporates penalties from both L1 and L2 regularization:\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1024\/1*gAmw-_z6v4bG9HcnPSAK3Q.png\" alt=\"drawing\" width=\"500\"\/>\n\nIn addition to setting and choosing a lambda value elastic net also allows us to tune the alpha parameter where \ud835\udfaa = 0 corresponds to ridge and \ud835\udfaa = 1 to lasso. Simply put, if you plug in 0 for alpha, the penalty function reduces to the L1 (ridge) term and if we set alpha to 1 we get the L2 (lasso) term. Therefore we can choose an alpha value between 0 and 1 to optimize the elastic net. Effectively this will shrink some coefficients and set some to 0 for sparse selection.","1417ca52":"# Regularization:\nOverfitting impacts the accuracy of Machine Learning models. The model attempts to capture the data points that do not represent the accurate properties of data. These data points may be considered as noise. To avoid the occurrence of overfitting, we may use a method called regularization.\n\nRegularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.","4116c660":"# Exploratory Data Analysis:\nas you can see, this below chart shows us the distribution of 'house price of unit area'. based on this chart, mean of 'house price of unit area' is about 40. the maximum of price is about 120.","d81a19a5":"# Split the Data to Train & Test","07e8abdd":" # Preprocessing (Polynomial Conversion)\n \n__Note:__ the best degree for model is 2 and for higher degrees, model will be overfit. but we get degree equals 3 and then with regularization, we will handle the overfitting.","e55c878a":"# Data Overview:\ndataset that i worked on, is about house prices based on these 6 parameters:\n* 1-transaction date\n* 2-house age\n* 3-distance to the nearest MRT station\n* 4-number of convenience stores\n* 5-latitude\n* 6-longitude","4aa8db17":"# Ridge Regression (Coosing an alpha value with Cross-Validation):","62374a83":"# Scaling the Data","29fe2a9f":"#  Lasso Regression:\nLasso regression is a regularization technique used to reduce model complexity. It is also known as L1 regularization. Lasso stands for Least Absolute Shrinkage and Selector Operator.\n\nLet\u2019s look at the equation below:\n\n![Screenshot%20%28297%29.png](attachment:Screenshot%20%28297%29.png)\nWe note that it has a slight variation to the previously discussed loss function, with the introduction of a penalty term. To penalize highly fluctuating coefficients, lasso uses absolute values of the regression coefficients (\u2223\u03b2\u2223).\n\nLasso minimizes the regression coefficients to regularize the model parameters. Sometimes, Lasso can reduce regression coefficients to zero, which is particularly important when it comes to feature selection.","d88f33fe":"## correlation: \nto check the correlation of parameters and house price, i displayed the 6 scatter plots to see is there any correlation or not.\n* chart 1: we cannot see the impressive correlation between transaction date and house price\n* chart 2:there is small negative correlation between house age and house price\n* chart 3: as you can see there is a negative corrolation between distance to the nearest MRT station and house price. this means if the 'distance to the nearest MRT station' become more, the house price become less.\n* chart 4: there is a positive correlation. it means for more number of convenience stores, the house price become more.\n* chart 5 and 6: for these charts, there is a positive correlation.","9eb8cfa4":"to better understand  the correlations you, can see the last row of this chart. as mentioned, 'house age' and 'distance to the nearest MRT station' have negative correlation with house price. but the 'number of convenience store' and 'geographical location' have positive correlation with house price.\n\n__Note__: Green is shown for positive correlation and white for negative correlation.","0fb56a96":"# Determine the Features & Target Variable (Lable)"}}