{"cell_type":{"bec56274":"code","1ad59f8e":"code","0ea9ddd4":"code","4433370e":"code","06988182":"code","da246cb1":"code","0034c36e":"code","ef26df74":"code","c42beeea":"code","b2089903":"code","b412205e":"code","491a4e78":"code","b4c42da2":"code","1efa8d6f":"code","66acaa65":"code","518b63e2":"code","6d7f5f49":"code","3e21262d":"code","4d4006c2":"code","1f55af36":"code","a7ab320b":"code","cc946e36":"code","2b689b6f":"code","19bfbec0":"code","79444e1b":"code","777ed210":"code","a886cbb2":"code","4ff4e6ba":"code","e372f98a":"code","5a9a5a43":"code","32b2ebb6":"code","ff557a30":"code","5628bdd8":"code","48b79521":"code","8149f328":"code","207ffe8c":"code","f175ef0e":"code","57cfc883":"code","f8f1f04d":"code","4ba91562":"code","7af694fe":"code","f075d956":"code","0699f043":"code","437640e8":"code","97074c43":"code","e567b1f0":"code","3fdc5a8b":"markdown","ada7e10e":"markdown","3b1d473e":"markdown","52dc2dc7":"markdown","a0062399":"markdown","4f52773b":"markdown","bf0879a7":"markdown","fe5cfac0":"markdown","468ac293":"markdown","eefb059e":"markdown","5a338d26":"markdown","b4b14006":"markdown","97eddc4c":"markdown","e5681838":"markdown","8e9d76a0":"markdown","4c1e7746":"markdown","4d9dc303":"markdown","926e94d4":"markdown","d3475bc8":"markdown","1f83e85c":"markdown","1b8b4824":"markdown","e86b2320":"markdown","029a19b6":"markdown","29eec313":"markdown","d106571d":"markdown","400cd830":"markdown","915f23c0":"markdown","3e965a3f":"markdown","9c3bb006":"markdown","fb8c5937":"markdown","7e2d63fd":"markdown","0c47c58e":"markdown","208c9304":"markdown","6aed33bc":"markdown","8a5c37e4":"markdown","612d6d03":"markdown","267d3f13":"markdown","d5c3441a":"markdown","5761733f":"markdown","11d889ae":"markdown","9d2abfd1":"markdown","62bb7de3":"markdown","b1be55e8":"markdown","46590c1a":"markdown"},"source":{"bec56274":"# These functions ensure that updates to the packages are reloaded as they occur\n# And that any graphs that you produce are displayed inline in the jupyter notebook\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","1ad59f8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0ea9ddd4":"# PyTorch imports\nimport torch\nprint(\"torch version:\", torch.__version__)\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision   # Note there is an updated torchvision\nprint(\"torchvision version:\", torchvision.__version__)\nimport torchvision.models as pytorch_models\nimport torchvision.datasets as pytorch_datasets","4433370e":"# FastAI imports for vision applications\nfrom fastai import *\nfrom fastai.vision import *","06988182":"a = np.ones([4,4])\nb = torch.from_numpy(a)\nprint(b)","da246cb1":"np.add(a, 1, out=a)\nprint(b)","0034c36e":"c = torch.rand((4,4), dtype=torch.float64)\nprint(c)","ef26df74":"c + b","c42beeea":"print(c.size())\nc_16 = c.view(16)\nprint(c_16.size())\nc_8 = c.view(-1, 8)  # the size -1 is inferred from other dimensions\nprint(c_8.size())\nprint(c_8)","b2089903":"d = torch.rand(4, 4, requires_grad=True)\nprint(d)","b412205e":"e = d * d * 3\nf = e.mean()\nprint(f)","491a4e78":"class Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 3x3 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 3)\n        self.conv2 = nn.Conv2d(6, 16, 3)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\nnet = Net()\nprint(net)","b4c42da2":"params = list(net.parameters())\nprint(params[0].size())","1efa8d6f":"input = torch.randn(1, 1, 32, 32)\nout = net(input)\nprint(out)","66acaa65":"output = net(input)\ntarget = torch.randn(10)  # a dummy target, for example\ntarget = target.view(1, -1)  # make it the same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)","518b63e2":"print(loss.grad_fn)  # MSELoss\nprint(loss.grad_fn.next_functions[0][0])  # Linear\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU","6d7f5f49":"# Zero the gradient buffers of all parameters.\n# This keeps them from accumulating with the older gradients\nnet.zero_grad()\n\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\n\nloss.backward()\n\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)","3e21262d":"resnet34 = models.resnet34()\nresnet34","4d4006c2":"resnet34_pretrained = models.resnet34(pretrained=True)","1f55af36":"# print(torch.hub.help('pytorch\/vision', 'resnet34', force_reload=True))\n# print(torch.hub.list('pytorch\/vision', force_reload=True))","a7ab320b":"mnist_trainset = pytorch_datasets.MNIST(root='.\/data', train=True, download=True, \n                                        transform=torchvision.transforms.Compose([\n                                            torchvision.transforms.ToTensor(),\n                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))])) # global mean and standard deviation of the MNIST dataset\nprint(\"Num MNIST training examples:\", len(mnist_trainset))\nmnist_testset = pytorch_datasets.MNIST(root='.\/data', train=False, download=True, \n                                       transform=torchvision.transforms.Compose([\n                                            torchvision.transforms.ToTensor(),\n                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\nprint(\"Num MNIST test examples:\", len(mnist_testset))\nprint(mnist_trainset)","cc946e36":"mnist_train, mnist_validation = torch.utils.data.random_split(mnist_trainset, (50000, 10000))\nprint(len(mnist_train), len(mnist_validation))","2b689b6f":"train_dataloader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\nprint(train_dataloader)\ntest_dataloader = DataLoader(mnist_testset, batch_size=64, shuffle=True)\nprint(test_dataloader)","19bfbec0":"import matplotlib.pyplot as plt\n\nexamples = enumerate(test_dataloader)\nbatch_idx, (example_data, example_targets) = next(examples)\n\nfig = plt.figure()\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    plt.tight_layout()\n    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n    plt.xticks([])\n    plt.yticks([])\nfig","79444e1b":"path = untar_data(URLs.PETS)\npath","777ed210":"doc(untar_data)","a886cbb2":"help(untar_data)","4ff4e6ba":"path.ls()","e372f98a":"path_anno = path\/'annotations'\npath_img = path\/'images'","5a9a5a43":"batch_size=64","32b2ebb6":"fnames = get_image_files(path_img)\nnp.random.seed(2)\npat = re.compile(r'\/([^\/]+)_\\d+.jpg$')\ndata = ImageDataBunch.from_name_re(path_img, \n                                   fnames, pat, \n                                   ds_tfms=get_transforms(), \n                                   size=224, \n                                   bs=batch_size, \n                                   num_workers=0\n                                  ).normalize(imagenet_stats)\nprint(\"Classes:\", data.classes)\nprint(\"Number of Classes:\", len(data.classes),\"=\", data.c)","ff557a30":"data.show_batch(rows=3, figsize=(7,6))","5628bdd8":"doc(cnn_learner)","48b79521":"# learn = create_cnn(data, models.resnet34, metrics=error_rate)\nlearn = cnn_learner(data, models.resnet34, metrics=error_rate)\nlearn","8149f328":"num_epochs = 4\nlearn.fit_one_cycle(num_epochs)","207ffe8c":"# Save just in case we need to undo some training\nlearn.save('stage-1')","f175ef0e":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)","57cfc883":"interp.plot_top_losses(9, figsize=(15,11))","f8f1f04d":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","4ba91562":"interp.most_confused(min_val=2)","7af694fe":"learn.lr_find()","f075d956":"learn.recorder.plot()","0699f043":"num_epochs_2 = 2\nmax_lr_2 = slice(1e-6,1e-4)","437640e8":"learn.unfreeze()\nlearn.fit_one_cycle(num_epochs_2, max_lr=max_lr_2)","97074c43":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)","e567b1f0":"interp.most_confused(min_val=2)","3fdc5a8b":"# Using Predefined Models in PyTorch\n\nPyTorch supports many different predefined models in its models package","ada7e10e":"You can also create tensors directly","3b1d473e":"# Data in PyTorch","52dc2dc7":"# Neural Network Models in PyTorch","a0062399":"![image.png](attachment:image.png)","4f52773b":"## Basics\nYou can make a PyTorch tensor from a numpy array","bf0879a7":"This means that when you define the model you only need to define the forward functions. PyTorch will automaically keep track of the backward functions.\n\nYou can use the requires_grad to block backpropagation when trying to freeze layers in pretrained models.","fe5cfac0":"### Autograd and Backpropagation\ntorch.Tensor is the class of tensors in PyTorch. \nIf you set the attribute .requires_grad as True for a given tensor, it starts to track all operations on it. \nWhen you finish your computation you can call .backward() and have all the gradients computed automatically. \nThe gradient for this tensor will be accumulated into .grad attribute.\n\nNote that a vast majority of this section is taken directly from \nhttps:\/\/pytorch.org\/tutorials\/beginner\/blitz\/neural_networks_tutorial.html","468ac293":"You can also import a pretrained version from the torch.hub. The pretrained weights are stored in a file with the extension '.pth'. Note also that the name of the file is not just resnet34. The filename follows the naming convention modelname-<sha256>.ext where <sha256> is the first eight or more digits of the SHA256 hash of the contents of the file.\n\nNote that this function copies the model weights into a cache. As you are using the pretrained model, the default location to find the weights is in the \/tmp\/.cache\/torch\/checkpoints\/ directory.","eefb059e":"Resnet-34 Architecture\n\n![image.png](attachment:image.png)\n[https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf]","5a338d26":"A FastAI learner has a function to allow you to examine your learning rates with respect to how quickly you can reduce your loss during training. ","b4b14006":"In this first round, we will just keep the pretrained weights frozen for the most part and just train by adjusting weights on the last layer.\n\nNote that the freezing of the weights in FastAI by default does not freeze the weights in the BatchNormalization layers. Basically this is because batch normalization uses mean and standard deviation of the input data to smooth the optimizaiton space, and these values are data-specific. For a fuller explanation of this, see https:\/\/forums.fast.ai\/t\/why-are-batchnorm-layers-set-to-trainable-in-a-frozen-model\/46560.\n\nThe fit_one_cycle function basically automates the learning rate, momentum and weight adjustments during training. It implements the policies defined in the paper \"A Disciplined Approach to Neural Network\nHyper-Parameters: Part 1 \u2013 LEARNING RATE, Batch Size, Momentum, and Weight Decay\" by Leslie Smith [https:\/\/arxiv.org\/pdf\/1803.09820.pdf](https:\/\/arxiv.org\/pdf\/1803.09820.pdf). See also this [Medium Article](https:\/\/towardsdatascience.com\/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6).","97eddc4c":"### DataSet\n\nData must be wrapped on a Dataset parent class. A dataset is a map from keys to data samples. This mapping is required for the loader to load the data, but does not actually load the data.\n\nRequired methods:\n* \\__getitem__ Returns the data sample and its label for a given key. It may apply transformations to the data along the way. Transformations may include things such as resizing, or converting the input to a tensor.\n* \\__len__ Returns the number of items in the training set X_train.\n    \n","e5681838":"## Training Your model in FastAI","8e9d76a0":"## Data in FastAI (DataBunch)\nA FastAI DataBunch is the object that is used to feed a neural network. It collects together the following funcionality:\n* The Fastai DataSet including the data items and their associated labels, as well as the standard and randomized transformations\n  * Standard transformations apply to all data items uniformly, such as image resizing or conversion of the input to a tensor. They are computed on load\n  * Random transformations are randomly applied to the item as it is presented as an example to the neural network.\n* The DataLoader which allows us to feed the items to the neural network in batches\n\nNote that a FastAI DataSet differs from a PyTorch DataSet in that it also includes information such as:\n  * c -- represents the number of outputs of the final layer of your model. For a regression model, c=1. For a classification model, c is the number of classes\n  * classes -- an enumerated list of the class labels for a classifier\n  * loss_func -- a default loss function if your DataSet requires a specific loss function\n  \nThe DataBunch is split into training set, validation set, and optional test set with consistent processing for all items across all sets","4c1e7746":"### Stage 1 Training","4d9dc303":"Tensors apply so well in neural network models. A layer in a neural network transforms one tensor into a new tensor. So if we look at a typical neural network diagram we can veiw the weights at each layer as a tensor, the input data as a tensor, and the layer as a transforming operation","926e94d4":"# FastAI\nNow that we understand the basics of PyTorch, let us look at a more complex example in FastAI.\n\nWe are going to use the [Oxford-IIIT Pet Dataset](http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/pets\/) by [O. M. Parkhi et al., 2012](http:\/\/www.robots.ox.ac.uk\/~vgg\/publications\/2012\/parkhi12a\/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds. \n\nMuch of this was taken from https:\/\/www.kaggle.com\/mistyn\/fast-ai-v3-lesson-1","d3475bc8":"You should be able to list the models, and describe individual models, though this appears to be broken at the moment","1f83e85c":"## Tensors and Neural Networks","1b8b4824":"## Stage 2","e86b2320":"You can print the learnable parameters","029a19b6":"You can resize tensors","29eec313":"We can use the PyTorch models in FastAI directly.\n\nThere are two parts to keep in mind when you are thinking about a model. When you use a model such as 'models.resnet34' you are actually referring to the **structure** of the model. This includes the layers of neural network nodes (with appropriate functions) as well as the connections between them.\n\nIf you do not use a pretrained model, the weights on the connections are initialized randomly. When you run create_cnn, you see that the default however is to use pretrained weights. The pretrained weights were computed as a result of training that data model over a large and relevant data set (for resnet models this is ImageNet). When you run create_cnn, the defaut also is to load the pretrained weights in the cache, so that any further training you do starts with these pretrained weights.","d106571d":"Plot the confusion matrix.\n\nNote that the model makes the same mistakes over and over again but it rarely confuses other categories.","400cd830":"# Tensors\nA tensor is really a multi-dimensional data container which holds values of a specific type. \nFor example, an MxN matrix of integers can be represented as a tensor.\nHowever, tensors are dynamic, and will transform when interacting with other mathematical entities.\n\nTensors formed the basis of Tensorflow and the languages that have followed from it.\nPyTorch is a second-generation tensor-based system. FastAI is implemented over PyTorch.","915f23c0":"We will first see which were the categories that the model most confused with one another. ","3e965a3f":"So the learn.recorder.plot gives learning rate vs. loss. We want to smooth this curve and look for the steepest point on the smoothed curve.\n\nHere is an example of the plot. \n![image.png](attachment:image.png)","9c3bb006":"### Backpropagation","fb8c5937":"If you like or fork this notebook, please upvote.\n\nThis notebook uses material from\n* https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/neural_networks_tutorial.html\n* https:\/\/www.kaggle.com\/mistyn\/fast-ai-v3-lesson-1\n\nThis notebook is meant to be a basic introduction to tensors, PyTorch, and FastAI. It covers the following topics:\n1. What is a tensor and why is it so useful for neural network training?\n2. What are PyTorch data sets and models?\n3. What are FastAI data sets and learners, and how do you use them in transfer learning?","7e2d63fd":"In FastAI you can do basic path manipulation very easily","0c47c58e":"### Defining MNIST From Scratch\n![image.png](attachment:image.png)","208c9304":"You can add tensors and do other operations on them","6aed33bc":"For the learning rate, we will adjust our lerning rate based on the new weights. If we look at the learning rate finder we see that the loss starts going up readily when it is larger than about 3e-01. We will back off from this a bit, and give a range for the finder to use that is about 3 orders of magnitude.","8a5c37e4":"### Stage 1 Results","612d6d03":"We can follow the gradient functions back for a bit in the model to see what gradients are applied to the loss.","267d3f13":"They do, however, share the same underlying representation","d5c3441a":"## Models and Learners in FastAI\n\nFastAI uses the PyTorch models.\n\nA Learner is the FastAI abstraction that takes a model, the data (as a DataBunch), a loss function (loss_func) and an optimizer such as Adam (opt_func). \nTrainer for model using data to minimize loss_func with optimizer opt_func.\n","5761733f":"### Loss Function\n\nPyTorch nn package defines several different common loss functions. Here we use Mean Squared Error.","11d889ae":"### DataLoader\n\nThe DataLoader effectively uses the DataSet as an iterator function and loads the data into memory. The batch_size defines how many examples are loaded at a time.","9d2abfd1":"In FastAI you can find out the information about a function using doc() or help()","62bb7de3":"PyTorch allows you to split your datasets into training and validation sets","b1be55e8":"PyTorch already has a definition for the MNIST dataset.","46590c1a":"You can run the model forwards like this:"}}