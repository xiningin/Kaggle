{"cell_type":{"5d937be1":"code","48abbeab":"code","318492e1":"code","1aa33949":"code","4faa32a0":"code","c7a56bdb":"code","9ad5ec21":"code","f8804aa8":"code","0317531e":"code","09c72086":"code","e4ea9d6a":"markdown","0f24dad4":"markdown","9b3e11ea":"markdown","9d1e4e44":"markdown","cacb95da":"markdown"},"source":{"5d937be1":"import json\n\nwith open('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json','r') as f:\n\n    data = f.read()\n    data = \"[\" + data.replace(\"}\", \"},\", data.count(\"}\")-1) + \"]\"\n    datastore = json.loads(data)","48abbeab":"headlines = [] \nlabels = []\nurls = []\n\nfor item in datastore:\n    headlines.append(item['headline'])\n    labels.append(item['is_sarcastic'])\n    urls.append(item['article_link'])","318492e1":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","1aa33949":"# configure important hyper-parameters \n\nvocab_size = 3000\nmax_len = 500\nembedding_dim = 16\noov_tok = \"<OOV>\"\npadding_type = \"post\"\ntrunc_type = \"post\"\ntraining_size = 20000\n","4faa32a0":"# split the data into sets for training and testing\n\ntrain_data, test_data = headlines[:training_size], headlines[training_size:]\ntrain_labels, test_labels = labels[:training_size], labels[training_size:]","c7a56bdb":"# tokenize the train_data and test_data\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_data)\n\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_data)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\ntest_padded = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)","9ad5ec21":"train_padded = np.array(train_padded)\ntrain_labels = np.array(train_labels)\n\ntest_padded = np.array(test_padded)\ntest_labels = np.array(test_labels)","f8804aa8":"# define a DNN model with an Embedding layer\n\nmodel = keras.Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n                         layers.GlobalAveragePooling1D(),\n                         layers.Dense(16, activation=\"relu\"),\n                         layers.Dense(1, activation=\"sigmoid\")])\n                         \nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n                         \nmodel.summary()\nkeras.utils.plot_model(model)","0317531e":"# train the model\n\nnum_epochs = 15\n\nhistory = model.fit(train_padded, train_labels,\n                   epochs=num_epochs,\n                   validation_data=(test_padded, test_labels),\n                   verbose=1)","09c72086":"# plot accuracy and loss\n\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\n# accuracy\n\nplt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b--\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.show()\n\n# loss\n\nplt.plot(epochs, loss, \"r\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","e4ea9d6a":"> Using GlobalAveragePooling1D() instead of Flatten() because it is faster than Flatten().","0f24dad4":"> **First thing** is to modify the original json file by separating the json strings from each other by commas, and adding square brackets to make it like a Python list. This helps me iterate through the json file easily.","9b3e11ea":"## Sarcasm Detection from News Headlines\n\n#### Living in the digital world, tons of news headlines pass through our newsfeed every single day and many of them are sarcasm. \n\n#### In this notebook, I am going to predict if the news is sarcastic or not using the \"news headlines dataset for sarcasm detection\". \n\nThis is a binary classification task with 2 labels: 0 for is_sarcastic and 1 for not_sarcastic.\n\n![](https:\/\/www.telegraph.co.uk\/content\/dam\/technology\/2017\/08\/07\/Simpsons-sarcasm-detector-xlarge_trans_NvBQzQNjv4BquYsoiHywuRbpECh2kaughKHxSnsjjYOBMSJiOgQYU2U.jpg)\n\n*Image Source: The Telegraph*","9d1e4e44":"### The Result \n\n**I was able to get 88% of training accuracy and about 84% of validation accuracy after training for 15 epochs. This is the best result after trying many different combinations of hyper-parameters.**\n\nThe validation loss started to increase slightly after the 5th epoch. \n\n> Friends, if you are reading this notebook and know how to reduce the validation loss while getting more than 90% of training accuracy, please give me some advice. Thank you in advance!","cacb95da":"> Using Tokenizer and Embedding layer in Keras to solve this classification task."}}