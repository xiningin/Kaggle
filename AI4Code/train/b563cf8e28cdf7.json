{"cell_type":{"432c2a13":"code","30f010e7":"code","efe29e01":"code","11e1046c":"code","6b9be5b7":"code","e4254f94":"code","cb8f0530":"code","7ec0fce4":"code","3d336f8c":"code","13e05aa2":"code","28b767ff":"code","4de65149":"code","4797a947":"code","2850af98":"code","554ba06c":"code","1f2bc607":"code","79e6621d":"code","e66236f8":"code","4129a89c":"code","66a42fd3":"code","7f0e5449":"code","fcb1e351":"code","17e50a19":"code","0b347427":"code","ab0aff83":"code","b2425908":"code","8ca3cee5":"code","b8bf8a9d":"code","1fef8761":"code","fdee24b7":"code","298934f1":"code","956bd213":"code","3a2ca6ab":"code","70ed98c0":"code","c7765bb6":"code","c696fe76":"code","cbc8a902":"code","601c8025":"code","8ff95ceb":"code","8155c0ee":"code","58f0b237":"code","9c592722":"code","83aca20a":"code","11d9ec99":"code","61bd6d23":"code","525d40c5":"markdown","8a19b639":"markdown","4c681e56":"markdown","7e8db6c8":"markdown","b26267df":"markdown","646ddcbd":"markdown","b0ad0e2a":"markdown","70cef974":"markdown","20c2963c":"markdown","6fa1f128":"markdown","95e53717":"markdown","e6090806":"markdown","880e5a47":"markdown","a1ca09df":"markdown","e4eff125":"markdown","e55836ad":"markdown","014feb8e":"markdown","e35082f9":"markdown","020e87bd":"markdown","79374df7":"markdown","ca630315":"markdown","d15d7364":"markdown","ced3404f":"markdown","92b0b11c":"markdown","c6dabba5":"markdown","7ab3af48":"markdown","09e3f0c6":"markdown","d3078543":"markdown","f6a091bc":"markdown","35d1c042":"markdown","7d199a30":"markdown","5ea778c7":"markdown","ca40f8df":"markdown","14048542":"markdown","73020e21":"markdown"},"source":{"432c2a13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30f010e7":"# Importing Dependencies\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectFromModel\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics","efe29e01":"# Importing data\ndataset = pd.read_csv(\"..\/input\/vehicle-dataset-from-cardekho\/car data.csv\")","11e1046c":"# top 5-rows of dataset\ndataset.head()","6b9be5b7":"# getting more familier with the dataset\ndataset.info()","e4254f94":"# Checking if there is any missing value\ndataset.isnull().sum()","cb8f0530":"cat_features = [feature for feature in dataset.columns if dataset[feature].dtypes in ['object']]\ncat_features","7ec0fce4":"dataset.shape","3d336f8c":"dataset.Car_Name.nunique()\n# There are 98 different features","13e05aa2":"# Mean prices per category of categorical variables\nplt.figure(figsize=(15,8))\nfor feature in cat_features:\n    dataset.groupby(feature)['Selling_Price'].mean().plot.bar()\n    plt.title(feature)\n    plt.show()","28b767ff":"# count of prices per category of categorical variables\nplt.figure(figsize=(15,8))\nfor feature in cat_features:\n    dataset.groupby(feature)['Selling_Price'].count().plot.bar()\n    plt.title(feature)\n    plt.show()","4de65149":"temporal_feature = [feature for feature in dataset.columns if 'year' in feature.lower() or 'yr' in feature.lower()]\ntemporal_feature","4797a947":"for feature in temporal_feature:\n    dataset[feature] = 2021 - dataset[feature]","2850af98":"dataset.head()","554ba06c":"dataset.corr()['Selling_Price'].drop('Selling_Price', axis=0) # pandas.core.frame.DataFrame","1f2bc607":"dataset[['Year', 'Present_Price',  'Kms_Driven', 'Owner']].head()","79e6621d":"# Checking the number of unique values present per numerical variable\ndataset[['Year', 'Present_Price',  'Kms_Driven', 'Owner']].nunique()","e66236f8":"dataset[['Year', 'Present_Price',  'Kms_Driven', 'Owner']].info()","4129a89c":"# Changing the datatype of Kms_driven\ndataset.Kms_Driven = dataset.Kms_Driven.astype('float64')","66a42fd3":"dataset[['Year', 'Present_Price',  'Kms_Driven', 'Owner']].info()","7f0e5449":"discrete_num_features = [feature for feature in dataset.columns if dataset[feature].dtypes in ['int64']]\ndiscrete_num_features","fcb1e351":"# Let's see the relationship of discrete numerical variables with target variable ('SalePrice')\nfor feature in discrete_num_features:\n    dataset.groupby(feature)['Selling_Price'].mean().plot.bar()\n\n    plt.xlabel(feature)\n    plt.ylabel('Selling_Price')\n    plt.title(feature)\n    plt.show()","17e50a19":"# Let's see the relationship of discrete numerical variables with target variable ('SalePrice')\nfor feature in discrete_num_features:\n    dataset.groupby(feature)['Selling_Price'].median().plot.bar()\n\n    plt.xlabel(feature)\n    plt.ylabel('Selling_Price')\n    plt.title(feature)\n    plt.show()","0b347427":"continuous_num_features = [feature for feature in dataset.columns if dataset[feature].dtypes == 'float64']\ncontinuous_num_features","ab0aff83":"# dropping target variable from continuous_num_features list\ncontinuous_num_features.remove('Selling_Price')\ncontinuous_num_features","b2425908":"# Let's analyse the continuous values by creating histograms to understand the distribution\nfor feature in continuous_num_features:\n    dataset[feature].hist(bins=30)\n    plt.xlabel(feature)\n    plt.ylabel('count')\n    plt.title(feature)\n    plt.show()","8ca3cee5":"# Let's analyse the continuous values by creating histograms to understand the distribution\nfor feature in continuous_num_features:\n    sns.histplot(data=dataset, x=feature, kde=True)\n    plt.xlabel(feature)\n    plt.ylabel('count')\n    plt.title(feature)\n    plt.show()","b8bf8a9d":"# boxplot to visualize outliers\n\nfor feature in discrete_num_features + continuous_num_features:\n    dataset.boxplot(column = feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","1fef8761":"X = dataset.drop('Selling_Price', axis=1)\ny = dataset.Selling_Price\nX.head()","fdee24b7":"# label encoding\ncar_dataset = X.copy()\ncar_dataset.replace({'Fuel_Type':{'Petrol':3, 'Diesel':1, 'CNG':2}}, inplace=True)\ncar_dataset.replace({'Seller_Type':{'Dealer':1, 'Individual':2}}, inplace=True)\ncar_dataset.replace({'Transmission':{'Manual':2, 'Automatic':1}}, inplace=True)","298934f1":"x_train, x_test, y_train, y_test = train_test_split(car_dataset, y, random_state=1, test_size=0.1)","956bd213":"x_train.head()","3a2ca6ab":"# 'handle_unknown' helps to discard categories not seen during fit\nencoder_OH = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\n\ntrain_encoded = pd.DataFrame(encoder_OH.fit_transform(x_train[['Car_Name']]), index = x_train.index)\ntest_encoded = pd.DataFrame(encoder_OH.transform(x_test[['Car_Name']]), index = x_test.index)\n\ntrain_OH = pd.concat([x_train.select_dtypes(include = ['int64', 'float64']), train_encoded], axis = 1)\ntest_OH = pd.concat([x_test.select_dtypes(include = ['int64', 'float64']), test_encoded], axis = 1)","70ed98c0":"print(train_OH.shape)\nprint(test_OH.shape)\n# total number of features = 101","c7765bb6":"# Next setps:\n#     1. feature scaling \n#     2. feature selection using lasso and select from model\n#     3. model building using treebase ensemble models randomforest, extra tree, xgboost\n#     4. use of randomized search cv for hyperparameter tuning\n#     5. selecting the best model out of them\n#     6. printing the test score","c696fe76":"train_cols = train_OH.columns\n\n# MinMax scaler object\nscaler = MinMaxScaler()\n\n# transforming train data\nx_train_encoded_scaled = pd.DataFrame(scaler.fit_transform(train_OH), columns = train_cols)\n\n# transforming test data\nx_test_encoded_scaled = pd.DataFrame(scaler.transform(test_OH), columns = train_cols)","cbc8a902":"# The bigger the alpha for Lasso, less features gets selected\n# SelectFromModel selects features whose coefficients are non-zero\n# feature selection using training data\nfeature_sel = SelectFromModel(Lasso(alpha = 0.01, random_state=1, max_iter=10000))\nfeature_sel.fit(x_train_encoded_scaled, y_train)\n\nselected_features = x_train_encoded_scaled.columns[feature_sel.get_support()]\nlen(selected_features)","601c8025":"np.array(selected_features)","8ff95ceb":"# # ExtraTreesRegressor and RandomForestRegressor have exactly same set of parameters,\n\n# Important differences among them exists as below:\n    # 1. Random forest uses bootstrap replicas (bootstrap == True), i.e., it subsamples the input data with replacement. \n    # But Extra Trees uses the whole original samples (bootstrap == False).\n\n    # 2. Random forest chooses optimum split for splitting nodes (computationally costly), but ExtraTrees chooses splits randomly","8155c0ee":"# Considering features selected using lasso regression\nx_train_final = train_OH[selected_features]\nx_test_final = test_OH[selected_features]","58f0b237":"x_train_final.head()","9c592722":"params_ET_RF = {\n'n_jobs' : [-1],\n'n_estimators' : [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200],\n'criterion' : ['mse', 'mae'],\n'max_depth' : [5, 10, 15, 20, 25, 30],\n'max_features' : ['auto', 'sqrt'],\n'min_samples_split' : [2, 5, 10, 15, 100],\n'min_samples_leaf' : [1, 2, 5, 10]\n}\n\nparams_XGBRegressor = {\n'n_jobs' : [-1],\n'learning_rate' : [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n'max_depth' : [3,4,5,6,8,10,12,15],\n'n_estimators' : [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200],\n\"min_child_weight\" : [1,3,5,7],\n\"gamma\" : [0.0, 0.1, 0.2, 0.3, 0.4],\n\"colsample_bytree\" : [0.3, 0.4, 0.5, 0.6]\n}\n","83aca20a":"regressors = [ExtraTreesRegressor(), RandomForestRegressor(), xgb.XGBRegressor()]\nparams = [params_ET_RF, params_ET_RF, params_XGBRegressor]\nnames = ['ExtraTreesRegressor', 'RandomForestRegressor', 'XGBRegressor']\n# looping through each regressor\nfor i in range(3):\n    cv_regressor = RandomizedSearchCV(regressors[i], param_distributions = params[i], n_iter = 5, scoring = 'neg_mean_squared_error', n_jobs=-1, cv = 5)\n    # scorings='roc_auc' for classification problems\n    # scorings='neg_mean_squared_error' for regression problems\n\n    cv_regressor.fit(x_train_final,y_train)\n\n    print(\"************\",names[i],\"************\")\n    print(\"Best estimators: \\n {}\".format(cv_regressor.best_estimator_))\n    print()\n    print(\"Best score: \\n {}\".format(cv_regressor.best_score_))\n    print()\n    print(\"Best parameters: \\n {}\".format(cv_regressor.best_params_))\n    print()\n    print()","11d9ec99":"# defination of final model\nfinal_model = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.3, gamma=0.3, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=3,\n             min_child_weight=5,  monotone_constraints='()',\n             n_estimators=1100, n_jobs=-1, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\n# fitting and prediction\nfinal_model.fit(x_train_final, y_train)\npredictions = final_model.predict(x_test_final)","61bd6d23":"from sklearn import metrics\n\n# performance metrices\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n\n\nimport pickle\n# open a file, where you ant to store the data\nfile = open('final_model.pkl', 'wb')\n\n# dump information to that file\npickle.dump(final_model, file)","525d40c5":"##### Obseravation: Continuous variables are right skewed (mode < median < mean)","8a19b639":"##### XGBRegressor performed best","4c681e56":"#### Now 'year' variable represents the number of years passed since the car was bought","7e8db6c8":"### Refer below articles to know more about Tree based algorithms\n### [Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data](https:\/\/towardsdatascience.com\/do-decision-trees-need-feature-scaling-97809eaa60c6)\n\n### [ExtraTreesRegressor vs RandomForestRegressor](https:\/\/quantdare.com\/what-is-the-difference-between-extra-trees-and-random-forest\/)","b26267df":"# Temporal Feature: Features releted to time, date or year","646ddcbd":"# Train - Test Split","b0ad0e2a":"### Outliers : Checking outliers in numerical variables","70cef974":"##### Updating the temporal feature with number years since the car was bought","20c2963c":"# Numerical Variables","6fa1f128":"### Feature Selection","95e53717":"### One-hot encoding for Car-Name","e6090806":"## Steps I followed:\n1. Importing data\n2. Cleaning data and data exploration\n3. Feature engineering (choosing required features)\n4. Data visualization\n5. Finding if there exists corr among variables\n6. train test split\n7. Model building and Hyper-parameter tuning\n8. using various sklearn.metrics : mean_absolute_error, mean_squared_error, np.sqrt(mean_squared_error)\n9. converting the model to pickle file for future use","880e5a47":"#### Label Encoding for Fuel_Type, Seller_Type, Transmission","a1ca09df":"##### Observations: \n1. With the increase in the number of years, we can see a clear decline in Selling Price\n2. In case of Owner, if there was no previous (0 owner) the selling price is highest, but the 3 owner meadian > 1 onwer median.","e4eff125":"##### finding correlation between all the numerical values with target variable Selling_Price","e55836ad":"# Categorical variable Encoding\n### Label encoding and One-hot encoding","014feb8e":"##### Observations: \n1. With the increase in the number of years, we can see a clear decline in Selling Price\n2. In case of Owner, if there was no previous (0 owner) the selling price is highest, but the 3 owner mean > 1 onwer; we need to check the median value here","e35082f9":"# Missing Values","020e87bd":"### Continuous Numerical Variables","79374df7":"### Final Prediction","ca630315":"#### Hyper-parameter Tuning","d15d7364":"### Selling_Price is the target feature","ced3404f":"## Feature Selection using Lasso regression to select important features\n#### This is the reason why I performed feature scaling","92b0b11c":"# Model building","c6dabba5":"# Splitting Data","7ab3af48":"##### Observations: All the numerical variables have outliers. Had there been any missing values, we would have replaced with median() instead of mean()","09e3f0c6":"# About features\n1. **name** - Name of the cars\n2. **year** - Year of the car when it was bought\n3. **selling_price** - Price at which the car is being sold\n4. **km_driven** - Number of Kilometres the car is driven\n5. **fuel** - Fuel type of car\n6. **seller_type** - tells if a seller is individual or a dealer\n7. **transmission** - Gear transmission fo the car\n8. **owner** - Number of previous owners of the car","d3078543":"### Discrete Numerical Variables","f6a091bc":"##### Observation: The Selling_Price is linearly directly correlated with Present_Price","35d1c042":"## Performance Metrics","7d199a30":"# Categorical Variable Encoding","5ea778c7":"##### Out of 101 features, only 24 got selected","ca40f8df":"##### Although 'Kms_Driven' is of type int64, but it should be continuous numerical variable","14048542":"#### Observation of categorical features\n1. Few car names have higher prices, like 'fortuner', 'innova', 'land cruiser', etc.\n2. In Fuel_Type, 'Diesel' type cars have higher prices as compared to CNG or Petrol driven cars\n3. In seller_type, 'Dealer' are selling cars with higher prices than 'Individual'\n4. And finally, 'Automatic' cars are being sold for higher prices than 'Manual' cars","73020e21":"### Categorical feature distribution with 'Selling_Price'"}}