{"cell_type":{"92c55740":"code","46dfd7f1":"code","131a0b70":"code","c612a746":"code","e3ed6609":"code","4da07d58":"code","4534576f":"code","d03aed8f":"code","9ce8bdfc":"code","a4a77942":"code","2ee982b8":"code","3366971e":"code","9729a208":"markdown"},"source":{"92c55740":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46dfd7f1":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsample_soln_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","131a0b70":"sample_soln_df.head()","c612a746":"train.shape\ntrain.head()","e3ed6609":"test.shape","4da07d58":"train.claim.value_counts()","4534576f":"train.isnull().sum()","d03aed8f":"test.isnull().sum()","9ce8bdfc":"# Getting the features and target column:\nFEATURES = train.columns[:-1]\nTARGET = train.columns[-1]","a4a77942":"# Filling in the missing values with avg value of the column:\nfor col in FEATURES:\n    avg_val = train[col].mean()\n    train[col].fillna(avg_val, inplace=True)\n    test[col].fillna(avg_val, inplace=True)","2ee982b8":"test_preds = 0\ntrain_oof = np.zeros((train.shape[0],1))\ntrain_oof = pd.DataFrame(train_oof, columns=['preds'])\nkf = KFold(n_splits=5)\nfor train_index, test_index in kf.split(train[FEATURES]):\n    clf = HistGradientBoostingClassifier()\n    trn_X, trn_y = train.loc[train_index, FEATURES], train.loc[train_index, TARGET]\n    val_X, val_y = train.loc[test_index, FEATURES], train.loc[test_index, TARGET]\n    clf.fit(trn_X,trn_y)\n    oof_proba = clf.predict_proba(val_X)[:,1]\n    train_oof.loc[test_index,'preds'] = oof_proba\n    tmp = clf.predict_proba(test[FEATURES])\n    test_preds += tmp\/5\n    \nprint(\"The score for OOF is:\")\nroc_auc_score(train['claim'], train_oof['preds'])","3366971e":"sample_soln_df['claim'] = test_preds[:,1]\nsample_soln_df.to_csv('submission.csv', index=False)","9729a208":"this will be approchable but am i making my model slow ? \nas its going to iterate for this big data set."}}