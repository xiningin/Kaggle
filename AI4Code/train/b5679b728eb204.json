{"cell_type":{"41d9842e":"code","01f416ed":"code","b56e17dd":"code","79538af8":"code","2f49ea02":"code","ab5f05db":"code","5d7fd306":"code","98a11a0b":"code","dc5b4909":"code","f81b61da":"code","f28cbd88":"code","2d701842":"code","a19fd88d":"code","088ad0c5":"code","7b7461a2":"code","05cc239e":"code","ab4c3c47":"code","5038a350":"code","2d2e1329":"code","85a4f06d":"code","eb1962e1":"code","d7caae40":"code","a59943eb":"code","19be6ed1":"code","e57c3ded":"code","6c37fe13":"code","4dc80bcf":"code","50cd84e3":"code","c2830bbe":"code","dc462177":"code","9716bf59":"code","3b1965dc":"code","221c37d9":"code","77d2df45":"code","38dba78f":"code","6dd2c65e":"code","fd813824":"code","89c91a21":"code","41552357":"code","0c2582dc":"code","e7c00183":"code","e0f471c7":"code","9f9647aa":"code","d130ad27":"code","d1489744":"code","25b6b3b5":"code","b53d37d9":"code","63b634f9":"code","e947a78c":"code","418db944":"code","30db27b2":"code","f6040319":"code","0c0ec99d":"code","9a574109":"code","d9676049":"code","9ed6ce0d":"code","d8b1fd0b":"code","1cbe8943":"code","efeb3917":"code","4f3a6cef":"code","6c862c20":"code","ccc035cc":"code","8ac49091":"code","64318d68":"code","9657ee51":"code","c106ee67":"code","5ff93df7":"code","69cdfbeb":"code","cb7be734":"code","1123bac4":"code","fe0b0ee7":"code","cfab3c3d":"code","17d44b0b":"code","d7dec111":"code","a99d075a":"code","b190cf95":"code","d81227cd":"code","baa8cc3f":"code","6b6d366d":"code","70c911ba":"code","6c655617":"code","e0f6b9dc":"code","97992225":"code","ef1f29ad":"code","8ba2df41":"code","f6fdeb17":"code","f54c2ea6":"code","3b043fcc":"code","5bb24c90":"code","e6cf8a24":"code","4595cfea":"code","9b4fa225":"code","ce998f94":"code","9122aa25":"code","207728f7":"code","053d3d8f":"code","af811151":"code","0049801a":"code","8fbd30b0":"code","99e99f6e":"code","b7cbd5f2":"code","44bdf3e7":"code","b1210d44":"code","10ceca0c":"code","105c504f":"code","90f61a13":"code","386c113e":"code","bb529d87":"code","3327785a":"code","d2824921":"code","83a6d386":"code","dc013dc3":"code","5ed3279c":"code","0583b7af":"code","85c5a075":"code","e51c75bb":"code","b9716382":"code","35a49c0f":"code","548a0248":"code","5401b6d6":"code","bd246f57":"code","c040dac3":"code","627e2398":"code","fe9f85a6":"code","d646ab6e":"code","1d310607":"code","f2760577":"code","3be0768c":"code","8b9c115f":"code","9f65bc33":"code","5cbb1a36":"code","8b480574":"code","4899c03f":"code","99f308a2":"code","ae1eabe6":"code","c531655e":"code","1399565b":"code","9f5384e0":"code","87580e13":"code","7a5b618b":"code","06cfe92c":"code","a6d9e6e8":"code","36cf2848":"code","3c3c7b7b":"code","d4b0afa3":"code","68295365":"code","6e6e0120":"code","220a5a0a":"code","beb91a79":"code","b10a1205":"markdown","164419ae":"markdown","4e39eaa3":"markdown","948de7ac":"markdown","b1846df8":"markdown","6c8f70ea":"markdown","19665480":"markdown","5b0cf9fc":"markdown","6473e278":"markdown","0043acea":"markdown","e52704ad":"markdown","e5791f20":"markdown","6f82966f":"markdown","f205a6f1":"markdown","78e427d6":"markdown","970612e9":"markdown","65366d75":"markdown","e345a909":"markdown","210b39fb":"markdown","7ebbb43f":"markdown","0f2a3b83":"markdown","752ed154":"markdown","2d17a55e":"markdown","4dfc644c":"markdown","d00f044a":"markdown","e55a3496":"markdown","7fddbb19":"markdown","0b2fb801":"markdown","aed3c39f":"markdown","c2f807a3":"markdown","4c160ed5":"markdown","83a8c78a":"markdown","3f1b0851":"markdown","f74afcfb":"markdown","b1f886e1":"markdown","0938742a":"markdown","93f17340":"markdown","be52bfb7":"markdown","61a46f7a":"markdown","d2969a5f":"markdown","721cd146":"markdown","c765d737":"markdown","e52f14c9":"markdown","913fdc75":"markdown","1d2beca5":"markdown","c8eadf27":"markdown","fb32c102":"markdown","89b6cee9":"markdown","6b1ed006":"markdown","d0587bf6":"markdown","c2137a5e":"markdown","0374c946":"markdown","39e1c72d":"markdown","6aa3dbf9":"markdown","996171fd":"markdown","123bde2a":"markdown","b4983e42":"markdown","56d0d94c":"markdown","2df8088e":"markdown","ccd8f487":"markdown","52efb36a":"markdown","8b573c87":"markdown","5d9115a5":"markdown","69f3c89d":"markdown","25e882c5":"markdown","763a6faa":"markdown","3af3babe":"markdown"},"source":{"41d9842e":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport calendar\nimport datetime  \nfrom datetime import datetime as dt\nfrom pandas import DataFrame, Series\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","01f416ed":"retail = pd.read_csv('mock_kaggle.csv')","b56e17dd":"retail['date'] = pd.to_datetime(retail['date'])","79538af8":"retail.head()","2f49ea02":"retail.tail()","ab5f05db":"retail.info()","5d7fd306":"retail.describe()","98a11a0b":"retail.isnull().sum()","dc5b4909":"retail['dow'] = retail['date'].dt.dayofweek\nretail['day_of_week'] = retail['date'].dt.day_name()\n#retail['weeknum'] = retail['date'].dt.week\nretail['weeknum'] = retail['date'].dt.strftime('%W')","f81b61da":"retail.head(20)","f28cbd88":"retail_dow = retail.groupby(['day_of_week','dow']).mean()","2d701842":"retail['sale']","a19fd88d":"retail_dow","088ad0c5":"retail_dow.sort_values(\"dow\", axis = 0, ascending = True, \n                 inplace = True, na_position ='last') ","7b7461a2":"retail_dow","05cc239e":"df = retail_dow.drop(['stock'], axis=1)","ab4c3c47":"df","5038a350":"df[\"sale\"].plot(label=\"Average sale\",  title = \"Relationship between price(t_avg) and sales(t_avg) - day of week\", legend=True, figsize=(13,7))\ndf[\"price\"].plot(label=\"Average price\", legend=True, secondary_y=True)\n## df.plot(figsize=(12,8),secondary_y=True)\nplt.show()","2d2e1329":"retail","85a4f06d":"retail['year'] = retail['date'].dt.year","eb1962e1":"retail.head()","d7caae40":"retail_dow2 = retail.groupby(['year','weeknum']).mean()","a59943eb":"retail_dow2","19be6ed1":"df2 = retail_dow2.drop(['stock','dow'], axis=1)","e57c3ded":"df2","6c37fe13":"df2[\"sale\"].plot(label=\"Average sale\",  title = \"Relationship between price(avg) and sales(avg) - weekly \", legend=True, figsize=(13,7))\ndf2[\"price\"].plot(label=\"Average price\", legend=True, secondary_y=True)\n## df.plot(figsize=(12,8),secondary_y=True)\nplt.show()","4dc80bcf":"retail['7-day'] = retail['price'].rolling(7).mean()\nretail['14-day'] = retail['price'].rolling(14).mean()\nretail['21-day'] = retail['price'].rolling(21).mean()","50cd84e3":"retail","c2830bbe":"with plt.style.context('ggplot'):\n    plt.figure(figsize = (20,10))\n    plt.plot(retail.price[-120:], label = 'Real Price')\n    plt.plot(retail['7-day'][-120:], label = '7 Day Moving Average')\n    plt.plot(retail['14-day'][-120:], label = '14 Day Moving Average')\n    plt.plot(retail['21-day'][-120:], label = '21 Day Moving Average')\n    plt.legend(loc =1)","dc462177":"retail['7-day'] = retail['sale'].rolling(7).mean()\nretail['14-day'] = retail['sale'].rolling(14).mean()\nretail['21-day'] = retail['sale'].rolling(21).mean()","9716bf59":"with plt.style.context('ggplot'):\n    plt.figure(figsize = (20,10))\n    plt.plot(retail.sale[-120:], label = 'Real Price')\n    plt.plot(retail['7-day'][-120:], label = '7 Day Moving Average')\n    plt.plot(retail['14-day'][-120:], label = '14 Day Moving Average')\n    plt.plot(retail['21-day'][-120:], label = '21 Day Moving Average')\n    plt.legend(loc =1)","3b1965dc":"retail.head(20)","221c37d9":"from statsmodels.tsa.stattools import adfuller\nresult1 = adfuller(retail['price'])\nprint(result1)","77d2df45":"def simple_exp_smooth(d,extra_periods,alpha):  \n\n\n  d = np.array(d)  # Transform the input into a numpy array  \n\n  cols = len(d)  # Historical period length  \n\n  d = np.append(d,[np.nan]*extra_periods)  # Append np.nan into the demand array to cover future periods  \n\n\n\n  f = np.full(cols+extra_periods,np.nan)  # Forecast array  \n\n  f[0] = d[0]  # initialization of first forecast  \n\n    \n\n  # Create all the t+1 forecasts until end of historical period  \n\n  for t in range(1,cols+1):  \n\n    f[t] = alpha*d[t-1]+(1-alpha)*f[t-1]  \n    f[cols+1:] = f[t]  # Forecast for all extra periods  \n\n\n  df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Error\":f-d}) \n  return df","38dba78f":"# Double Exponential Smoothing\ndef double_exp_smooth(d,extra_periods,alpha,beta):  \n\n    d = np.array(d)  # Transform the input into a numpy array  \n    cols = len(d)  # Historical period length  \n    d = np.append(d,[np.nan]*extra_periods)  # Append np.nan into the demand array to cover future periods  \n# Creation of the level, trend, and forecast arrays\n    f= np.full(cols+extra_periods,np.nan)  # Forecast array  \n    a = np.full(cols+extra_periods,np.nan)\n    b = np.full(cols+extra_periods,np.nan)\n# Level and trend initialization \n    a[0] = d[0]\n    b[0] = d[1] -d[0]\n\n    \n  # Create all the t+1 forecasts until end of historical period  \n\n    for t in range(1,cols):  \n        f[t] = a[t-1]+ b[t-1]\n        a[t] = alpha*d[t] + (1-alpha)*(a[t-1] + b[t-1])\n        b[t] = beta*(a[t] - a[t-1]) + (1-beta)* b[t-1]\n\n# Forecast for all extra periods  \n    for t in range(cols, cols+extra_periods):\n        f[t] = a[t-1] + b[t-1]\n        a[t] = f[t]\n        b[t] = b[t-1]\n\n\n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Level\": a, \"Trend\": b, \"Error\":f-d}) \n    return df","6dd2c65e":"df1=simple_exp_smooth(retail['sale'],7,1)\ndf1.head(10)","fd813824":"# For KPI Calculation\nMAE = df1[\"Error\"].abs().mean()  \nprint(\"MAE:\",round(MAE,2)) \nRMSE = np.sqrt((df1[\"Error\"]**2).mean())\nprint(\"RMSE:\",round(RMSE,2))","89c91a21":"#For Plotting\ndf1.index.name = \"Periods\"\ndf1[[\"Demand\",\"Forecast\"]].plot(figsize=(20,8),title=\"Simple exponential smoothing\",style=[\"-\",\"--\"])  \nplt.show()\n","41552357":"df11=double_exp_smooth(retail['sale'],7,1,0.05)\ndf11.tail(10)","0c2582dc":"MAE = df11[\"Error\"].abs().mean()  \nprint(\"MAE:\",round(MAE,2)) \nRMSE = np.sqrt((df11[\"Error\"]**2).mean())\nprint(\"RMSE:\",round(RMSE,2))","e7c00183":"#For Plotting\ndf11.index.name = \"Periods\"\ndf11[[\"Demand\",\"Forecast\"]].plot(figsize=(20,8),title=\"Double exponential smoothing\",style=[\"-\",\"--\"])  \nplt.show()","e0f471c7":"retail","9f9647aa":"retail[\"year_week\"] = retail[\"year\"].map(str) + '-' + retail[\"weeknum\"].map(str)","d130ad27":"retail","d1489744":"retail_ml = retail.groupby([\"year_week\"]).mean()","25b6b3b5":"retail_ml","b53d37d9":"retail_ml = retail_ml.drop(['stock','year','7-day','14-day','21-day','dow','price'], axis=1)","63b634f9":"retail_ml.sort_values(\"year_week\", axis = 0, ascending = True, \n                 inplace = True, na_position ='last') \n","e947a78c":"retail_ml","418db944":"retail_lstm = retail_ml","30db27b2":"df_ml = pd.DataFrame(retail_ml)","f6040319":"df_ml","0c0ec99d":"df_ml = df_ml.sort_values(by = ['year_week'], ascending = [True])\nretail_ml = retail_ml.sort_values(by = ['year_week'], ascending = [True])","9a574109":"# Create 53 weeks of lag values to predict current observation\n# Shift of 53 weeks in this case\nfor i in range(53,0,-1):\n    df_ml[['t-'+str(i)]] = retail_ml.shift(i)\nprint(df_ml)","d9676049":"df_ml","9ed6ce0d":"retail.describe()","d8b1fd0b":"retail.to_csv('output.csv')","1cbe8943":"df_ml = df_ml.drop(['sale'], axis=1)","efeb3917":"df_ml","4f3a6cef":"# Create a new subsetted dataframe, removing Nans from first 12 rows\ndf_ml2 = df_ml[53:]\nprint(df_ml2)","6c862c20":"df_ml2.values","ccc035cc":"# Split Data into dependent(target) and independent(features) variables\n\nretail_ml = df_ml2.values\n# Lagged variables (features) and original time series data (target)\nX2 = retail_ml[:,0:]  # slice all rows and start with column 0 and go up to but not including the last column\ny2 = retail_ml[:,0:]  # slice all rows and last column, essentially separating out 't' column","8ac49091":"# Columns t-1 to t-12, which are the lagged variables\nX2.shape","64318d68":"# Column t, which is the original time series\n# Give first 10 values of target variable, time series\ny2.shape","9657ee51":"X2","c106ee67":"# Target(Y) Train-Test split\n\nY2 = y2\ntraintarget_size = int(len(Y2) * 0.80)   # Set split\nprint(traintarget_size)\ntrain_target, test_target = Y2[:traintarget_size], Y2[traintarget_size:len(Y2)]\n\nprint('Observations for Target: %d' % (len(Y2)))\nprint('Training Observations for Target: %d' % (len(train_target)))\nprint('Testing Observations for Target: %d' % (len(test_target)))","5ff93df7":"Y2[traintarget_size:len(Y2)]","69cdfbeb":"# Features(X) Train-Test split\n\ntrainfeature_size = int(len(X2) * 0.80)\ntrain_feature, test_feature = X2[:trainfeature_size], X2[trainfeature_size:len(X2)]\nprint('Observations for feature: %d' % (len(X2)))\nprint('Training Observations for feature: %d' % (len(train_feature)))\nprint('Testing Observations for feature: %d' % (len(test_feature)))","cb7be734":"train_feature","1123bac4":"from sklearn.linear_model import LinearRegression\n  \nreg = LinearRegression() # Create a linear regression object\n  \nreg = reg.fit(train_feature, train_target) # Fit it to the training data\n  \n# Create two predictions for the training and test sets\ntrain_prediction = reg.predict(train_feature)\ntest_prediction = reg.predict(test_feature)","fe0b0ee7":"train_prediction","cfab3c3d":"test_prediction","17d44b0b":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))\/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,3))\n\nMAE_test=np.mean(abs(test_target-test_prediction))\/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,3))\n\n","d7dec111":"# Decision Tree Regression Model\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a decision tree regression model with default arguments\ndecision_tree_retail = DecisionTreeRegressor()  # max_depth not set\n\n# Fit the model to the training features and targets\ndecision_tree_retail.fit(train_feature, train_target)\n\n# Check the score on train and test\nprint(decision_tree_retail.score(train_feature, train_target))\nprint(decision_tree_retail.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n","a99d075a":"# Find Best Max Depth\n\n# Loop through a few different max depths and check the performance\n# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n\nfor d in [2, 3,4, 5,7,8,10]:\n    # Create the tree and fit it\n    decision_tree_retail = DecisionTreeRegressor(max_depth=d)\n    decision_tree_retail.fit(train_feature, train_target)\n\n    # Print out the scores on train and test\n    print('max_depth=', str(d))\n    print(decision_tree_retail.score(train_feature, train_target))\n    print(decision_tree_retail.score(test_feature, test_target), '\\n')  # You want the test score to be positive\n    \n# R-square for train and test scores are below. \n;","b190cf95":"# Plot predicted against actual values\n\nfrom matplotlib import pyplot as plt\n\n# Use the best max_depth \ndecision_tree_retail = DecisionTreeRegressor(max_depth=5) # Fill in best max depth score here\ndecision_tree_retail.fit(train_feature, train_target)\n\n# Predict values for train and test\ntrain_prediction = decision_tree_retail.predict(train_feature)\n\nMAE_train=np.mean(abs(train_target-train_prediction))\/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\n\ntest_prediction = decision_tree_retail.predict(test_feature)\n\nMAE_test=np.mean(abs(test_target-test_prediction))\/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n# Scatter the predictions vs actual values, orange is predicted\nplt.scatter(train_prediction, train_target, label='train')  # blue \nplt.scatter(test_prediction, test_target, label='test')  # orange\nplt.show()\n","d81227cd":"# Random Forest Model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create the random forest model and fit to the training data\nrfr = RandomForestRegressor(n_estimators=200)\nrfr.fit(train_feature, train_target)\n\n# Look at the R^2 scores on train and test\nprint(rfr.score(train_feature, train_target))\nprint(rfr.score(test_feature, test_target))  # Try to attain a positive value","baa8cc3f":"from sklearn.model_selection import ParameterGrid\nimport numpy as np\n\n# Create a dictionary of hyperparameters to search\n# n_estimators is the number of trees in the forest. The larger the better, but also takes longer it will take to compute. \n# Run grid search\n#grid = {'n_estimators': [200], 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'random_state': [13]}\ngrid = {'n_estimators': [200], 'max_depth': [10], 'max_features': [3], 'random_state': [13]}\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n    rfr.fit(train_feature, train_target)\n    test_scores.append(rfr.score(test_feature, test_target))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])  \n\n# The best test score","6b6d366d":"# Use the best hyperparameters from before to fit a random forest model\nrfr = RandomForestRegressor(n_estimators=200, max_depth=8, max_features = 3, random_state=13)\nrfr.fit(train_feature, train_target)\n\n# Make predictions with our model\ntrain_prediction = rfr.predict(train_feature)\ntest_prediction = rfr.predict(test_feature)\n\n# Create a scatter plot with train and test actual vs predictions\nplt.scatter(train_target, train_prediction, label='train')\nplt.scatter(test_target, test_prediction, label='test')\nplt.legend()\nplt.show()","70c911ba":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))\/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\nMAE_test=np.mean(abs(test_target-test_prediction))\/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n","6c655617":"# Random Forest Feature Importance\n# get column names\ndf_ml.columns","e0f6b9dc":"df_ml","97992225":"# Get feature importances from our random forest model\nimportances = rfr.feature_importances_\n\n# Get the index of importances from greatest importance to least\nsorted_index = np.argsort(importances)[::-1]\nx = range(len(importances))\n\n# Create tick labels \nfeature_names = ['t-53','t-52','t-51','t-50','t-49','t-48','t-47','t-46','t-45','t-44','t-43','t-42','t-41','t-40','t-39','t-38','t-37','t-36','t-35','t-34','t-33','t-32','t-31','t-30','t-29','t-28','t-27','t-26','t-25','t-24','t-23','t-22','t-21','t-20','t-19','t-18','t-17','t-16','t-15','t-14','t-13','t-12','t-11','t-10','t-9','t-8','t-7','t-6','t-5','t-4','t-3','t-2','t-1']\nlabels = np.array(feature_names)[sorted_index]\nplt.figure(figsize=(15, 3))\nplt.bar(x, importances[sorted_index], tick_label=labels, width = 0.4)\n\n# Rotate tick labels to vertical\n\nplt.xticks(rotation=90)\nplt.show()","ef1f29ad":"importances","8ba2df41":"# Target(Y) Train-Test split\n\nY3 = y2\ntraintarget_size = int(len(Y3) * 0.7)   # Set split\nprint(traintarget_size)\ntrain_target, test_target = Y3[:traintarget_size], Y3[traintarget_size:len(Y2)]\n\nprint('Observations for Target: %d' % (len(Y3)))\nprint('Training Observations for Target: %d' % (len(train_target)))\nprint('Testing Observations for Target: %d' % (len(test_target)))","f6fdeb17":"# Features(X) Train-Test split\n\ntrainfeature_size = int(len(X2) * 0.7)\ntrain_feature, test_feature = X2[:trainfeature_size], X2[trainfeature_size:len(X2)]\nprint('Observations for feature: %d' % (len(X2)))\nprint('Training Observations for feature: %d' % (len(train_feature)))\nprint('Testing Observations for feature: %d' % (len(test_feature)))","f54c2ea6":"from sklearn.linear_model import LinearRegression\n  \nreg = LinearRegression() # Create a linear regression object\n  \nreg = reg.fit(train_feature, train_target) # Fit it to the training data\n  \n# Create two predictions for the training and test sets\ntrain_prediction = reg.predict(train_feature)\ntest_prediction = reg.predict(test_feature)","3b043fcc":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))\/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,3))\n\nMAE_test=np.mean(abs(test_target-test_prediction))\/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,3))\n\n","5bb24c90":"# Decision Tree Regression Model\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a decision tree regression model with default arguments\ndecision_tree_avocado = DecisionTreeRegressor()  # max_depth not set\n\n# Fit the model to the training features and targets\ndecision_tree_avocado.fit(train_feature, train_target)\n\n# Check the score on train and test\nprint(decision_tree_avocado.score(train_feature, train_target))\nprint(decision_tree_avocado.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n","e6cf8a24":"# Find Best Max Depth\n\n# Loop through a few different max depths and check the performance\n# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n\nfor d in [2, 3,4, 5,7,8,10]:\n    # Create the tree and fit it\n    decision_tree_avocado = DecisionTreeRegressor(max_depth=d)\n    decision_tree_avocado.fit(train_feature, train_target)\n\n    # Print out the scores on train and test\n    print('max_depth=', str(d))\n    print(decision_tree_avocado.score(train_feature, train_target))\n    print(decision_tree_avocado.score(test_feature, test_target), '\\n')  # You want the test score to be positive\n    \n# R-square for train and test scores are below. ","4595cfea":"# Plot predicted against actual values\n\nfrom matplotlib import pyplot as plt\n\n# Use the best max_depth \ndecision_tree_avocado = DecisionTreeRegressor(max_depth=5) # Fill in best max depth score here\ndecision_tree_avocado.fit(train_feature, train_target)\n\n# Predict values for train and test\ntrain_prediction = decision_tree_avocado.predict(train_feature)\n\nMAE_train=np.mean(abs(train_target-train_prediction))\/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\n\ntest_prediction = decision_tree_avocado.predict(test_feature)\n\nMAE_test=np.mean(abs(test_target-test_prediction))\/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n# Scatter the predictions vs actual values, orange is predicted\nplt.scatter(train_prediction, train_target, label='train')  # blue \nplt.scatter(test_prediction, test_target, label='test')  \nplt.show()","9b4fa225":"from sklearn.model_selection import ParameterGrid\nimport numpy as np\n\n# Create a dictionary of hyperparameters to search\n# n_estimators is the number of trees in the forest. The larger the better, but also takes longer it will take to compute. \n# Run grid search\n#grid = {'n_estimators': [200], 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'random_state': [13]}\ngrid = {'n_estimators': [200], 'max_depth': [8], 'max_features': [3], 'random_state': [13]}\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n    rfr.fit(train_feature, train_target)\n    test_scores.append(rfr.score(test_feature, test_target))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])  \n\n# The best test score","ce998f94":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))\/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\nMAE_test=np.mean(abs(test_target-test_prediction))\/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n","9122aa25":"import math\nimport matplotlib.pyplot as plt\nfrom statsmodels.tools.eval_measures import rmse\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport warnings \nwarnings.filterwarnings(\"ignore\")","207728f7":"retail_lstm","053d3d8f":"retail_lstm['year_week'] = retail_lstm.index","af811151":"retail_lstm","0049801a":"retail_lstm.reset_index(drop=True, inplace=True)","8fbd30b0":"retail_lstm.index = retail_lstm.year_week.str.replace('-', '')","99e99f6e":"retail_lstm","b7cbd5f2":"retail_lstm.rename(index={'year_week': 'year_weeks'})","44bdf3e7":"retail_lstm = retail_lstm.drop(['year_week'], axis=1) \n# (['year_week'], inplace=True)","b1210d44":"retail_lstm","10ceca0c":"train, test = retail_lstm[:-12],retail_lstm[-12:]","105c504f":"train","90f61a13":"test","386c113e":"scalar =MinMaxScaler()\nscalar.fit(train)\ntrain = scalar.transform(train)\ntest = scalar.transform(test)","bb529d87":"test.shape","3327785a":"n_input = 12\nn_features = 1\ngenerator = TimeseriesGenerator(train, train, n_input, batch_size =6)","d2824921":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs = 50)","83a6d386":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","dc013dc3":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 50')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","5ed3279c":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","0583b7af":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","85c5a075":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 250\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","e51c75bb":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","b9716382":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs =100)","35a49c0f":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","548a0248":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 100)')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","5401b6d6":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","bd246f57":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","c040dac3":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 100\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","627e2398":"df_test.iloc[-12:,1].values","fe9f85a6":"df_test.iloc[-12:,0].values","d646ab6e":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","1d310607":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs =150)","f2760577":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","3be0768c":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 150')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","8b9c115f":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","9f65bc33":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","5cbb1a36":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 150\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","8b480574":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","4899c03f":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory=model.fit_generator(generator, epochs =200)","99f308a2":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list\n","ae1eabe6":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 200')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","c531655e":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","1399565b":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","9f5384e0":"df_test.count()","87580e13":"df_predict.count()","7a5b618b":"df_test.info()","06cfe92c":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 200\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","a6d9e6e8":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","36cf2848":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs =250)","3c3c7b7b":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","d4b0afa3":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 250')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","68295365":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","6e6e0120":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","220a5a0a":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 250\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","beb91a79":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","b10a1205":"## Epoch 100","164419ae":"<blank>","4e39eaa3":"<blank>","948de7ac":"### max_depth = 10 ","b1846df8":"# We are using 80-20 and 70-30 split.","6c8f70ea":"## 1. Load ","19665480":"<blank>","5b0cf9fc":"<blank>","6473e278":"## Decision Tree","0043acea":"<blank>","e52704ad":"## Epoch 250","e5791f20":"### - Day of Week Analysis","6f82966f":"### We use max_depth : 10","f205a6f1":"#### Check for stationarity using the Dicky Fuller Test for all three datasets\n","78e427d6":"<blank>\n    <blank>\n        <blank>","970612e9":"<blank>","65366d75":"# 4.2 Double Exponential Smoothing","e345a909":"<blank>","210b39fb":"## Decision Tree Model","7ebbb43f":"## 2. Explorer Dataset","0f2a3b83":"<blank>","752ed154":"## Dicky Fuller Test","2d17a55e":"## Epoch 150","4dfc644c":"<blank>","d00f044a":"<blank>","e55a3496":"<blank>","7fddbb19":"### RMSE (Epoch 200)","0b2fb801":"### RMSE (Epoch 250)","aed3c39f":"# 3. Moving Average","c2f807a3":"<blank>","4c160ed5":"<blank>","83a8c78a":"# 5. Machine Learning","3f1b0851":"<blank>","f74afcfb":"<blank>","b1f886e1":"# Epoch 50","0938742a":"## Random Forrest","93f17340":"<blank>","be52bfb7":"## Linear Regression","61a46f7a":"## Linear Regression","d2969a5f":"## Model Loss","721cd146":"## Epoch 200","c765d737":"### RMSE (Epoch 150)","e52f14c9":"<blank>","913fdc75":"<blank>","1d2beca5":"## Model Loss - Epoch 150","c8eadf27":"<blank>","fb32c102":"## Random Forest Model","89b6cee9":"# Epoch Start","6b1ed006":"<blank>","d0587bf6":"<blank>","c2137a5e":"# Retail Dataset","0374c946":"<blank>","39e1c72d":"<blank>","6aa3dbf9":"## Model Loss - Epoch 200","996171fd":"# 4.1 Simple Exponential Smoothing ","123bde2a":"## According to above dual weekly graph, we could find there is an inverse relationship between sales and price","b4983e42":"This dataset contains lot of historical sales data. It was extracted from a Brazilian top retailer and has many SKUs and many stores. The data was transformed to protect the identity of the retailer.","56d0d94c":"Predict based on weekly average","2df8088e":"# 6. LSTM","ccd8f487":"<blank>\n<blank>\n<blank>","52efb36a":"<blank>","8b573c87":"<blank>","5d9115a5":"## RMSE (Epoch 50)","69f3c89d":"## Benchmark Model","25e882c5":"## 70:30","763a6faa":"### RMSE (Epoch 100)","3af3babe":"<blank>"}}