{"cell_type":{"3d6a691e":"code","6676668d":"code","df4e6c7e":"code","251a18aa":"code","fa6dbda4":"code","76cd3942":"code","52c4bdb3":"code","1fa0ee4b":"code","d5dae11d":"code","ab51de3f":"code","2894b266":"code","b31dc50f":"code","e7947092":"code","c1318dba":"code","e8066dae":"code","dc83cc9e":"markdown","6c8cdb79":"markdown","c9fb81e3":"markdown","41a523c1":"markdown"},"source":{"3d6a691e":"import os\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport json\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import sparse\nimport scipy","6676668d":"PATH_TO_DATA = '..\/input\/'\ndata=pd.read_json(PATH_TO_DATA+'train.json', lines=True)\ntarget=pd.read_csv(PATH_TO_DATA+'train_log1p_recommends.csv')\ndata['target']=target['log_recommends']\ndata.head(2)","df4e6c7e":"from html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\ndef preprocessing(df, columns=['count_by_author','images_count','content_length_type','min_reads','target','image_url',\\\n                               'h1_count','h2_count','h3_count','href_count','count_by_domain','domain']):\n    df.image_url=np.where(df.image_url.isna(), 0, 1)\n    df=df.drop('_spider', axis=1)\n    df.author=df.author.apply(lambda x: x.get('url').split('@')[1])\n    df['count_by_author']=df.groupby('author').transform('count')['content']  #count number of posts per author\n    df['count_by_domain']=df.groupby('domain').transform('count')['content'] #count number of posts per domain\n    df['images_count']=df.content.apply(lambda x: x.count('<img'))  #count number of images\n    df['h1_count']=df.content.apply(lambda x: x.count('<h1')) #count number of titles\n    df['h2_count']=df.content.apply(lambda x: x.count('<h2')) ##count number of titles\n    df['h3_count']=df.content.apply(lambda x: x.count('<h3')) #count number of titles\n    df['href_count']=df.content.apply(lambda x: x.count('href')) #count number of links\n    df['content_length']=df['content'].apply(lambda x: len(strip_tags(x).split()))  #count content lenght and split to categories below\n    df['content_length_type']='medium'\n    df['content_length_type'][df.content_length<800]='short'\n    df['content_length_type'][(df.content_length>2500) & (df.content_length<5000)]='long_read'\n    df['content_length_type'][df.content_length>=5000]='huge'\n    df.domain=df.domain.apply(lambda x: ' '.join(x.split('.')))\n    df['min_reads']=df.meta_tags.apply(lambda x: int(x.get('twitter:data1').split()[0]))  #get read mins \n    df=df[columns]\n    df=pd.get_dummies(df, columns=['content_length_type','domain'])\n    #columns=['count','images_count','content_length','content_length_type','min_reads','text','target','image_url']\n    return df","251a18aa":"data_sample=data.sample(5000)","fa6dbda4":"%%time\ndf=preprocessing(data)","76cd3942":"df.head(2)","52c4bdb3":"fig, axes = plt.subplots(nrows=2, ncols=3,figsize=(15,10) )\n\ndf.h1_count.plot.hist(ax=axes[0,0], title=\"h1\")\ndf.h2_count.plot.hist(ax=axes[0,1], title=\"h2\")\ndf.h3_count.plot.hist(ax=axes[0,2], title=\"h3\")\ndf.images_count.plot.hist(ax=axes[1,0], title=\"images per post\")\ndf.href_count.plot.hist(ax=axes[1,1], title=\"links per post\")\ndf.min_reads.plot.hist(ax=axes[1,2], title=\"min_read\")","1fa0ee4b":"from sklearn.preprocessing import PowerTransformer\ndf2=df.copy()\ncols=['h1_count','h2_count','h3_count','images_count','href_count','count_by_domain','count_by_author']\ndf2[cols] = PowerTransformer().fit_transform(df[cols].values)\n\nfig, axes = plt.subplots(nrows=2, ncols=3,figsize=(15,10) )\n\ndf2.h1_count.plot.hist(ax=axes[0,0], title=\"h1\")\ndf2.h2_count.plot.hist(ax=axes[0,1], title=\"h2\")\ndf2.h3_count.plot.hist(ax=axes[0,2], title=\"h3\")\ndf2.images_count.plot.hist(ax=axes[1,0], title=\"images per post\")\ndf2.href_count.plot.hist(ax=axes[1,1], title=\"links per post\")\ndf2.min_reads.plot.hist(ax=axes[1,2], title=\"mins read\")","d5dae11d":"print(df['target'].mean())\nprint(df['target'].std())","ab51de3f":"from sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nimport xgboost\n\nlgbm = LGBMRegressor(max_depth=-1, learning_rate=0.01, n_estimators=500)\nrf=RandomForestRegressor(max_features=7, n_estimators=100, max_depth=7)\nridge = Ridge(random_state=17,)\nkf=KFold(n_splits=5, random_state=None, shuffle=False)","2894b266":"scores = cross_val_score(lgbm, df.drop(['target'],axis=1), df['target'], scoring=\"neg_mean_absolute_error\", cv=kf)\nprint(scores, scores.mean())","b31dc50f":"scores = cross_val_score(ridge, df.drop(['target'],axis=1), df['target'], scoring=\"neg_mean_absolute_error\", cv=kf)\nprint(scores, scores.mean())","e7947092":"#And ridge again with normalized data\n\nscores = cross_val_score(ridge, df2.drop(['target','h1_count'],axis=1), df['target'], scoring=\"neg_mean_absolute_error\", cv=kf)\nprint(scores, scores.mean())","c1318dba":"lgbm.fit(df.drop(['target'],axis=1), df['target'])\ncols=df.drop(['target'],axis=1).columns\nplt.figure(figsize=(15,5))\nplt.xticks(rotation=70)\nplt.bar(cols[np.argsort(lgbm.feature_importances_)[-15:]][::-1], lgbm.feature_importances_[np.argsort(lgbm.feature_importances_)[-15:]][::-1])\n","e8066dae":"Well, all ","dc83cc9e":"**Let's see what features we can get out of raw HTML and what score we can get by combining them with some others. **\n\nRaw HTML can provide us with the follwoing information:\n* Number of images \n* Number of links\n* Number of titles \n\nIt seems to me that all of these may influence number of claps, so let's check if this assumption is true.","6c8cdb79":"**Preprocessing and feature engineering**","c9fb81e3":"**Well, not the best result ever but at least it is something  :)  <br>\nAnd finally, let's see feature importance.**","41a523c1":"It seems, we have outliers and our data does not look nice,  let's apply some transformations to our data, and see if it helps to improve predictions."}}