{"cell_type":{"33f6c947":"code","e8bb620d":"code","1ef27beb":"code","2b6f425e":"code","54aed553":"code","2df6779a":"code","ad61f59a":"code","db898cd1":"code","e8798f29":"code","64ddf38b":"code","d42f8811":"code","c79c853a":"code","3589a1ad":"code","eae10f4a":"code","148e4938":"code","e93bad99":"code","caeb72c4":"code","01f01a3d":"code","c6896f24":"markdown","969c516a":"markdown","5ebed169":"markdown","1ad33b66":"markdown","f548c02e":"markdown"},"source":{"33f6c947":"!pip install transformers==3.0.2\n!pip install nlp","e8bb620d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nlp import load_dataset\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nnp.random.seed(1234) \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ef27beb":"mnli = load_dataset(path='glue', name='mnli') # loading more data from the Huggin face dataset\n#snli   =  load_dataset(\"snli\") # loading more data from the Huggin face dataset","2b6f425e":"train_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\nprint('Traning Data, the size of the dataset is: {} \\n'.format(train_df.shape))\n\ntest_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')","54aed553":"train_df = pd.concat([train_df]) #appending the original dataset to the additional datasets\ntrain_df = train_df[train_df['label'] != -1] #cleaning values with the wrong label\n\n\nprint('the shape of the whole DF to be used is: ' + str(train_df.shape))","2df6779a":"# searching for duplicates\n\ntrain_df = train_df[train_df.duplicated() == False]\nprint('the shape of the whole DF to be used is: ' + str(train_df.shape))","ad61f59a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize = (15,5))\n\nplt.subplot(1,2,1)\nplt.title('Traning data language distribution')\nsns.countplot(data = train_df, x = 'lang_abv', order = train_df['lang_abv'].value_counts().index)\n\nplt.subplot(1,2,2)\nplt.title('Test data laguage distribution')\nsns.countplot(data = test_df, x = 'lang_abv', order = test_df['lang_abv'].value_counts().index)","db898cd1":"# word count\n\ndef word_count(dataset, column):\n    len_vector = []\n    for text in dataset[column]:\n        len_vector.append(len(text.split()))\n    \n    return len_vector\n\ntrain_premise = word_count(train_df, 'premise')\ntrain_hypothesis = word_count(train_df, 'hypothesis')\n\ntest_premise = word_count(test_df, 'premise')\ntest_hypothesis = word_count(test_df, 'hypothesis')\n\nfig = plt.figure(figsize = (15,10))\n\nplt.subplot(2,2,1)\nplt.title('word count for train dataset premise')\nsns.distplot(train_premise)\n\nplt.subplot(2,2,2)\nplt.title('word count for train dataset hypothesis')\nsns.distplot(train_hypothesis)\n\nplt.subplot(2,2,3)\nplt.title('word count for test dataset premise')\nsns.distplot(test_premise)\n\nplt.subplot(2,2,4)\nplt.title('word count for test dataset hypothesis')\nsns.distplot(test_hypothesis)        ","e8798f29":"# looking at the countplot of the labels of the traning data set\n\nplt.title('Label column countplot')\nsns.countplot(data = train_df, x = 'label')","64ddf38b":"from transformers import BertTokenizer, TFAutoModel, AutoTokenizer\nimport tensorflow as tf\nimport keras\nfrom tensorflow.math import softplus, tanh\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n\n\nnp.random.seed(123)\nmax_len = 50\n\n# this is the model used BERT huggin face\n\nBert_model = \"bert-large-uncased\"\n\n# tokenizer\n\nBert_tokenizer = BertTokenizer.from_pretrained(Bert_model)\n\ndef tokeniZer(dataset,tokenizer):\n    encoded_list = [] # word id array\n    type_id_list = np.zeros((dataset.shape[0], max_len)) #type id array\n    mask_list = np.zeros((dataset.shape[0], max_len)) #masks array\n    \n    for i in range(dataset.shape[0]):\n        datapoint = '[CLS] ' + dataset['premise'][i] + ' [SEP]' + dataset['hypothesis'][i] + ' [SEP]' # putting the two sentences together along with special characters\n        datapoint = tokenizer.tokenize(datapoint)\n        datapoint = tokenizer.convert_tokens_to_ids(datapoint)\n        encoded_list.append(datapoint) \n    \n    encoded_list = pad_sequences(encoded_list, maxlen = max_len, padding = 'post')\n    \n    for i in range(encoded_list.shape[0]):\n        flag = 0\n        a = encoded_list[i]\n        for j in range(len(a)):\n            \n            #building the type_id matrix\n            \n            if flag == 0:\n                type_id_list[i,j] = 0\n            else:\n                type_id_list[i,j] = 1\n                \n            #flag for the type_id matrix\n            \n            if encoded_list[i,j] == 102:\n                flag = 1\n            \n    \n            #building the mask matrix \n            \n            if encoded_list[i,j] == 0:\n                mask_list[i,j] = 0\n            else:\n                mask_list[i,j] = 1\n                \n    return encoded_list,mask_list,type_id_list\n        \n        \n        ","d42f8811":"# softplus - log(exp(x)+1), function that can be used for extra layers in the models\ndef mish(x):\n    return x*tanh(softplus(x))\nget_custom_objects()[\"mish\"] = Activation(mish)","c79c853a":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","3589a1ad":"# model creator\n\ndef create_BERT(random_seed):\n    \n    tf.random.set_seed(random_seed)\n    \n    with tpu_strategy.scope():\n    \n        transformer_encoder = TFAutoModel.from_pretrained(Bert_model)\n\n        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        input_masks = Input(shape = (max_len,), dtype = tf.int32, name = 'input_mask')\n        input_type_id = Input(shape = (max_len,), dtype = tf.int32, name = 'input_type_id')\n\n        sequence_output = transformer_encoder([input_ids, input_masks, input_type_id])[0]\n\n        cls_token = sequence_output[:, 0, :]\n\n        output_layer = Dense(3, activation='softmax')(cls_token)\n\n\n        model = Model(inputs=[input_ids, input_masks, input_type_id], outputs = output_layer)\n\n        model.summary()\n\n        model.compile(Adam(lr=1e-5), \n                loss='sparse_categorical_crossentropy', \n                metrics=['accuracy']\n            )\n    return model","eae10f4a":"#ensemble creation and prediction\n\nfrom sklearn.utils import shuffle # shuffle dataframes\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss', \\\n                                           restore_best_weights = True, mode = 'min')]\n\nshuffled_data = shuffle(train_df).reset_index(drop = True)#shuffle the data to add more variance\n\ntrain_df = None #clearing more memory\n\nbatch_size = 128","148e4938":"XLM_model = \"jplu\/tf-xlm-roberta-large\"\nxlm_tokenizer = AutoTokenizer.from_pretrained(XLM_model) #Xlm tokenizer\n\n\nX_train_ids, X_train_masks, _ = tokeniZer(shuffled_data,xlm_tokenizer) #encoding input","e93bad99":"# creating the XLM model \n\ndef create_xlm(transformer_layer,  random_seed, learning_rate = 1e-5):\n    \n    tf.keras.backend.clear_session()\n\n    tf.random.set_seed(random_seed)\n    \n    with tpu_strategy.scope():\n    \n        input_ids = Input(shape = (max_len,), dtype = tf.int32)\n        input_masks = Input(shape = (max_len,), dtype = tf.int32)\n\n            #insert roberta layer\n        roberta = TFAutoModel.from_pretrained(transformer_layer)\n        roberta = roberta([input_ids, input_masks])[0]\n        \n        out = GlobalAveragePooling1D()(roberta)\n                \n\n                #add our softmax layer\n        out = Dense(3, activation = 'softmax')(out)\n\n        #assemble model and compile\n\n\n        model = Model(inputs = [input_ids, input_masks], outputs = out)\n        model.compile(\n                                optimizer = Adam(lr = learning_rate), \n                                loss = 'sparse_categorical_crossentropy', \n                                metrics = ['accuracy'])\n    model.summary()\n        \n    return model  \n\nXlm = create_xlm(XLM_model ,123443334, 1e-5)","caeb72c4":"#STEPS_PER_EPOCH = int(train_df.shape[0] \/\/ batch_size)\n\nhistory_xlm = Xlm.fit([X_train_ids, X_train_masks], shuffled_data['label'],\n          batch_size = batch_size,\n        validation_split = 0.2,\n         epochs = 39, callbacks = callbacks)","01f01a3d":"# preprocessing test data\n\ninput_ids_test_xml, input_masks_test_xml, _ = tokeniZer(test_df, xlm_tokenizer)\n\n#model predictions\n\npredictions_xlm = Xlm.predict([input_ids_test_xml, input_masks_test_xml])\n\npredictions = predictions_xlm\n\nfinal = np.argmax(predictions, axis = 1)    \n\nsubmission = pd.DataFrame()    \n\nsubmission['id'] = test_df['id']\nsubmission['prediction'] = final.astype(np.int32)\n\nsubmission.to_csv('submission.csv', index = False)","c6896f24":"The language column is clearly unbalanced being English the most frequent language","969c516a":"premises are observed to be longer than the hypothesis","5ebed169":"The following code will preprocess and create a XLM-RoBERTa Model ","1ad33b66":"This is the function to create a single BERT model that takes as single input  random seed which is used in this case to create an esemble model using several BERT models with different seeds and shuffling the data","f548c02e":"The following code is used to tokenize and preprocess the data for the Hugginface model, creating an array of ids, maks and type_id"}}