{"cell_type":{"8999a34f":"code","1fcf868b":"code","07b1da9b":"code","a501afc7":"code","4edba13f":"code","afdef50c":"code","41eb7097":"code","4bdc36e0":"code","ebe447c6":"code","228dc54c":"code","20c5b6fe":"code","8d848ead":"code","78ff6e69":"code","6c9bded7":"code","fe222da9":"code","7c35267a":"code","72b9e2a4":"code","921adb4d":"code","5114765f":"code","946d611c":"code","f30c4485":"code","8b38d96b":"code","ca872d41":"code","e8f16bd5":"code","fdc09459":"code","927a11b5":"code","254383c9":"code","aef4d5ac":"code","57245fea":"code","cf45ac86":"code","0c89ebdf":"code","72fe3dd0":"markdown","b15ff708":"markdown","9017a332":"markdown","60e985e4":"markdown","f1bbeeb3":"markdown","1091bb3a":"markdown","7d5cec8e":"markdown","cdaaba13":"markdown","f23fa739":"markdown","4bf676e8":"markdown","1389e982":"markdown","66c128b4":"markdown","2d319d92":"markdown","922e4593":"markdown","c45fb1a3":"markdown","3d61bf0e":"markdown","f5b5a4ba":"markdown","c497ba18":"markdown","c7808521":"markdown","33ac0cc7":"markdown","d036c346":"markdown","6ebb82e6":"markdown","f2496d6e":"markdown","01ebde99":"markdown","debb19fc":"markdown","e3d88a65":"markdown","78279f05":"markdown","8cb38600":"markdown","d578eff8":"markdown","dc5b98e7":"markdown"},"source":{"8999a34f":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re","1fcf868b":"# USE YOUR FILEPATH TO CSV FILE\nFILE = '\/kaggle\/input\/textclustering\/tickets.csv'\nfile = FILE\ndata = pd.read_csv(file, header=0)\ndata.head()","07b1da9b":"agents = []\n\nfor index, row in data.iterrows():\n    m = re.findall(r'[^: ].+?(?=:)', row[0])\n    if m:\n        agents.append(m[0])\n    else:\n        print(index, row[0])\n\nlen(agents)","a501afc7":"data.head()","4edba13f":"data_cleaned_lst = []\ndata_noagent_lst = []\n\nfor index, row in data.iterrows():\n    m = re.findall(r'[^: ].+?(?=:)', row[0])\n    if m:\n        data_cleaned_lst.append([m[0], row[0].replace(m[0]+\": \", \"\")])\n    else:\n        data_noagent_lst.append(row[0])\n        print(index, row[0])\n\nlen(data_cleaned_lst)","afdef50c":"data_cleaned_df = pd.DataFrame(data_cleaned_lst, columns=['agent', 'event'])\ndata_cleaned_df.head()","41eb7097":"testdata_lst = [\n'NEATLVUAP01.nationalexpresscorp.com: D:\\: File system usage exceeded critical threshold: Limit: 90.0%, Actual: 90.22%',\n'vaqupatlts14.sephoraus.com: Linux File System radzivon.kruhlikauj@10.105.249.39:\/customer\/local\/JMeter32\/scripts : \/customer\/local\/JMeter32\/scripts currently at',\n'iad2-veraction-sw5: \/dev\/md16, mounted on: \/packages\/mnt\/jweb-ex-12.3R5.7: File system usage exceeded critical threshold: Limit: 90.0%, Actual: 100.00%',\n'SENSUWWSQL02.Sente.qts: U:\\: File system usage exceeded critical threshold: Limit: 92.0%, Actual: 95.78%',\n'vaquvatatg86: Linux File System tmpfs : \/tmp currently at 98.0 usage and has exceeded threshold value of 90',\n'ad-aus-2: PowerShell Communication Error: App [1880] Windows device [172.28.1.137]: Incorrect Active Directory server in the PowerShell credential or server off',\n'10.28.155.74: Device Failed Availability Check: ICMP Ping',\n'URCATLVMWDTAP14: Disk G: Logical Physical Percent Disk Time has exceeded threshold: (70%) currently (86%)',\n'URCATLVMWPRSQ01: Disk 1 D: Percent Disk Time has exceeded threshold: (90%) currently (99%)',\n\"VAQUVAPSPL30: Filesystem unavailable: '\/var' on Device '4850'\",\n'vaquvatcom56: CPU usage of 88.0308840981 is above threshold of 80',\n'Kongbucks-x: Device Failed Availability Check: Component device 11901 is not available  (Kongbucks-x); Reason: Availability App ID: 228 running on parent DID 10',\n'DCSQL01F004.hsftdc.local: Host Resource: Storage Utilization (C:\\\\ Label:  Serial Number c42165e0) of type HrStorageFixedDisk has exceeded critical threshold 95',\n'vaqupdpoms11.sephoraus.com: Linux File System 10.105.208.190:\/oms_backup : \/backup currently at 90.0 usage and has exceeded threshold value of 90',\n'db61.prod.nec.com: \/dev\/hd9var: File system usage exceeded critical threshold: Limit: 90.0%, Actual: 90.02%',\n'db23_qa_db24_qa__ASM02: Device Failed Availability Check: Component device 19546 is not available ; Reason: At least one ancestor of db23_qa_db24_qa__ASM02 (ID='    \n]","4bdc36e0":"import pandas as pd\nimport re\n\ntestdata = pd.DataFrame(testdata_lst, columns=['ticket'])\ntestdata.head()","ebe447c6":"class Cleaner:\n    # include docstrings to each method\n    def __init__(self, df):\n        self.data = df\n        \n    def find_names(self):\n        names_lst = []\n        for index, row in self.data.iterrows():\n            m = re.findall(r'[^: ].+?(?=:)', row[0])\n            if m:\n                name = m[0]\n                names_lst.append(name)\n        return names_lst\n    \n    def remove_disk(self):\n        # D:\\: U:\\:\n        nodisk_lst = []\n        for index, row in self.data.iterrows():\n            m = re.findall(r'[A-Z]:\\\\:', row[0])\n            if m:\n                disk_str = m[0]\n                nodisk_lst.append(row[0].replace(disk_str, \"\"))\n        return nodisk_lst\n    \n    def remove_names(self):\n        nonames_lst = []\n        for index, row in self.data.iterrows():                \n            m = re.findall(r'\\S+\\d\\S*', row[0]) # remove very filters almost everything\n\n            if m:\n                # replacement of the detected patterns by empty \n                replace_list = m\n                replaced_row = re.sub(r'|'.join(map(re.escape, replace_list)), '', row[0])\n                nonames_lst.append(replaced_row)\n            else:\n                nonames_lst.append(row[0])\n        return nonames_lst\n    \n    def remove_pathlike(self):\n        nopaths_lst = []\n        for index, row in self.data.iterrows():\n            m = re.findall(r'\\S*[@\/\\\\]+\\S*', row[0]) # remove very useful!\n            if m:\n                replace_list = m\n                replaced_row = re.sub(r'|'.join(map(re.escape, replace_list)), '', row[0])\n                nopaths_lst.append(replaced_row)\n            else:\n                nopaths_lst.append(row[0])\n        return nopaths_lst\n    \n    def remove_inbrackets(self):\n        noinbrackets_lst = []\n        for index, row in self.data.iterrows():\n            m = re.findall(r'[(]\\S+[)]?', row[0]) # remove very useful!\n            \n            if m:\n                replace_list = m\n                replaced_row = re.sub(r'|'.join(map(re.escape, replace_list)), '', row[0])\n                noinbrackets_lst.append(replaced_row)\n            else:\n                noinbrackets_lst.append(row[0])\n        return noinbrackets_lst\n    \n    def remove_diskidentifier(self):\n        nodiskidentifier_lst = []\n        for index, row in self.data.iterrows():\n            m = re.findall(r'(?<=Disk )[A-Z0-9\\s]+(?![a-z])', row[0]) # remove very useful!\n            \n            if m:\n                replace_list = m\n                replaced_row = re.sub(r'|'.join(map(re.escape, replace_list)), '', row[0])\n                nodiskidentifier_lst.append(replaced_row)\n            else:\n                nodiskidentifier_lst.append(row[0])\n        return nodiskidentifier_lst\n    \n    def clean(self):\n        # do remove_names() then do remove_pathlike()\n        # then do remove_inbrackets() then do remove_diskidentifier()\n        self.data = pd.DataFrame(self.remove_names(), columns=['ticket'])\n        self.data = pd.DataFrame(self.remove_pathlike(), columns=['ticket'])\n        self.data = pd.DataFrame(self.remove_inbrackets(), columns=['ticket'])\n        result = pd.DataFrame(self.remove_diskidentifier(), columns=['ticket'])\n        return result\n                ","228dc54c":"import unittest\n\nclass TestCleaner(unittest.TestCase):\n    \n    def setUp(self):\n        self.instance = Cleaner(testdata)\n        \n    def test_find_names(self):\n        print('test_find_names:\\n', self.instance.find_names())\n        self.assertTrue(True)\n        \n    def test_remove_disk(self):\n        print('test_remove_disk:\\n', self.instance.remove_disk())\n        self.assertTrue(True)\n        \n    def test_remove_names(self):\n        print('test_remove_names:\\n', self.instance.remove_names())\n        self.assertTrue(True)\n        \n    def test_remove_pathlike(self):\n        print('remove_pathlike:\\n', self.instance.remove_pathlike())\n        self.assertTrue(True)\n        \n    def test_remove_inbrackets(self):\n        print('test_remove_inbrackets:\\n', self.instance.remove_inbrackets())\n        self.assertTrue(True)\n        \n    def test_remove_diskidentifier(self):\n        print('test_remove_diskidentifier:\\n')\n        for item in self.instance.remove_diskidentifier():\n            print(item)\n        self.assertTrue(True)\n        \n    def test_clean(self):\n        print('\\ntest_clean:\\n')\n        display(self.instance.clean())\n        self.assertTrue(True)\n\nunittest.main(argv=[''], verbosity=2, exit=False)","20c5b6fe":"data_agent = data_cleaned_df.iloc[:, :1]\ndata_agent.head()","8d848ead":"data_event = data_cleaned_df.iloc[:, 1:]\ndata_event.head()","78ff6e69":"data_event_ready = Cleaner(data_event).clean()\ndata_event_ready.rename({'ticket': 'event'}, axis=1, inplace=True)\ndata_event_ready.head()","6c9bded7":"testevents = ['System or agent has recently restarted', \n'Interface state changed to operationally down:   Name:   previous state was up ', \n'Device Failed Availability Check: UDP - SNMP', \n'VMware: VM CPU Usage Has Exceeded Threshold  currently ', \n'Device Failed Availability Check: Component device  is not available   Reason: Availability App', \n'Device Failed Availability Check: Component device  is not available ; Reason: At least one ancestor of IntegrateStaging  ) has', \n'Required service not running: ', \n'Device Failed Availability Check: Component device  is not available ; Reason: At least one ancestor of dataconstructionserver ', \n'System or agent has recently restarted', \n'Host Resource: CPU has exceeded threshold  The current value is ', \n'System or agent has recently restarted', \n'Host Resource: CPU has exceeded threshold  The current value is ', \n'Cisco: BGP Peers  Peer state is not established and Prior state was established', \n' File system usage exceeded critical threshold: Limit:  Actual: ', \n'Device Failed Availability Check: Component device  is not available ; Reason: At least one ancestor of ', \n'Device Failed Availability Check: Component device  is not available ; Reason: At least one ancestor of dswperfdynamic  ) has bee', \n'Filesystem unavailable:  on Device ', \n'UCS: Ethernet Pauses has exceeded threshold  Pauses) currently  Pauses)', \n'Interface state changed to operationally down:  4, Name:  Local Area Connection* 2; previous state was up ', \n'Required service not running: Tax Calculator', \n'Device Failed Availability Check: UDP - SNMP', \n'Device Failed Availability Check: Component device  is not available', \n'Interface state changed to operationally down:   Name:  ; previous state was up ', \n'Interface state changed to operationally down:   Name:  Private vLAN  previous state was up ', \n'Device Failed Availability Check: Component device  is not available   Reason: Availability App ID:  running on parent DID ', \n'IPSLA:   Packet Loss Above minor threshold  currently ', \n'Device Failed Availability Check: UDP - SNMP', \n'Device Failed Availability Check: Component device  is not available   Reason: Availability App ID:  running on parent DID ', \n'System or agent has recently restarted', \n'System or agent has recently restarted', \n'Host Resource: CPU has exceeded threshold  The current value is ', \n'System or agent has recently restarted', \n'Host Resource: Physical Memory has exceeded threshold  The current value is ', \n'Device Failed Availability Check: Component device  is not available ; Reason: At least one ancestor of dswMilano_MD_Cache  )', \n'Device Failed Availability Check: UDP - SNMP', \n'System or agent has recently restarted', \n'Device Failed Availability Check: Component device  is not available   Reason: Availability App ID:  running on parent DID ', \n'System or agent has recently restarted', \n'PowerShell Communication Error: App  Windows device  ', \n'Host Resource: CPU has exceeded threshold  The current value is ', \n'Filesystem unavailable:  on Device ', \n'Required service not running: ', \n'Available Memory is below the threshold  MB. Current value is  MB.', \n'Required service not running: ', \n'Device Failed Availability Check: ICMP Ping', \n'Required service not running: Tomcat', \n'Device Failed Availability Check: Component device  is not available   Reason: Availability App ID:', \n'Required service not running: ', \n'Required service not running: SQL Server Agent ']\n\nlen(testevents)","fe222da9":"import pandas as pd\nimport re\n\ntestevents_df = pd.DataFrame(testevents, columns=['event'])\ntestevents_df.head()","7c35267a":"def split_events(eventdf):\n    event_lst = []\n    for index, row in eventdf.iterrows():\n        m = re.findall(r'[\\d\\w\\s]+(?=:)', row[0]) \n        if m:\n            event_lv1 = m[0].lstrip()\n            remainder = row[0].replace(m[0], \"\") # ': '\n            remainder = remainder.replace(\":\", \"\", 1).lstrip()\n            \n            if remainder:\n                event_lv2 = remainder\n                # limit event_lv2 to three words\n                k = re.findall(r'((?:\\S+\\s+){,2}\\S+)', remainder) # good\n                if k:\n                    event_lv2_limited = \" \".join(k[0].split())\n                    event_lst.append([event_lv1, event_lv2_limited])\n                else:\n                    event_lst.append([event_lv1, event_lv2])\n            else:\n                event_lst.append(event_lv1)\n            \n        else:\n            event_lv1 = row[0].lstrip()\n            event_lst.append(event_lv1)\n    \n    # fit list to create dataframe, as creating a dataframe in a loop\n    # is very time consuming\n    event_lst_squared = []\n    for item in event_lst:\n        if type(item) is not list:\n            event_lst_squared.append([item, \"\"])\n        else:\n            event_lst_squared.append(item)\n            \n    events_df = pd.DataFrame(event_lst_squared, columns=['event_lv1', 'event_lv2'])\n            \n    return events_df","72b9e2a4":"events_df = split_events(testevents_df)\ndisplay(events_df)","921adb4d":"data_event_lv1_lv2 = split_events(data_event_ready)\ndisplay(data_event_lv1_lv2.head(15))","5114765f":"data_splitted = pd.concat([data_agent, data_event_lv1_lv2], axis=1)\ndata_splitted.head(15)","946d611c":"!pip install prince","f30c4485":"class Cluster:\n    def __init__(self, df):\n        self.data = df\n        self.datacopy = df\n        self.itemsets = None\n        \n    def get_agent(self):\n        return self.data[['agent']]\n        \n    def get_event_lv1(self):\n        return self.data[['event_lv1']]\n        \n    def get_event_lv2(self):\n        return self.data[['event_lv2']]\n    \n    def cutout_event_lv2(self):\n        return self.data[['agent', 'event_lv1']]\n    \n    def reset_data(self):\n        self.data = self.datacopy\n        \n    def set_data(self, df):\n        self.data = df\n    \n    def plot_most_popular_event_lv1(self):\n        # describe in details what is the most popular\n        event_lv1_count_df = self.cutout_event_lv2().groupby(by='event_lv1', as_index=False).agg({'agent': pd.Series.nunique})\n        event_lv1_count_sorted = event_lv1_count_df.sort_values(by=['agent'], ascending=False).head(10)\n        display(event_lv1_count_sorted)\n        \n        pop_item = event_lv1_count_sorted.reset_index()\n        pop_item = pop_item.rename(columns = {'event_lv1': 'items','agent': 'count'})\n\n        #Data Visualization\n        %matplotlib inline\n        plt.rcParams['figure.figsize'] = (10, 6)\n        matplotlib.style.use('dark_background')\n        ax = pop_item.plot.barh(x = 'items', y = 'count')\n        plt.title('Most popular items')\n        plt.gca().invert_yaxis()\n\n    def plot_most_frequent_agent(self):\n        df = self.get_agent()\n        counts = df.value_counts(ascending=False).head(20)\n        counts_df = counts.to_frame()\n        display('most_frequent_agent:', counts_df)\n        \n        pop_item = counts_df.reset_index()\n        pop_item = pop_item.rename(columns = {'agent': 'agents', 0: 'count'})\n\n        #Data Visualization\n        %matplotlib inline\n        plt.rcParams['figure.figsize'] = (10, 6)\n        matplotlib.style.use('dark_background')\n        ax = pop_item.plot.barh(x = 'agents', y = 'count')\n        plt.title('Most frequent agents')\n        plt.gca().invert_yaxis()\n        \n    def plot_most_frequent_event_lv1(self):\n        df = self.get_event_lv1()\n        counts = df.value_counts(ascending=False).head(10)\n        counts_df = counts.to_frame().rename(columns = {0: 'count'})\n        display('most_frequent_event_lv1:', counts_df)\n        \n        pop_item = counts_df.reset_index()\n        pop_item = pop_item.rename(columns = {'event_lv1': 'event'})\n\n        #Data Visualization\n        %matplotlib inline\n        plt.rcParams['figure.figsize'] = (10, 6)\n        matplotlib.style.use('dark_background')\n        ax = pop_item.plot.barh(x = 'event', y = 'count')\n        plt.title('Most frequent events')\n        plt.gca().invert_yaxis()\n        \n    def filter_less_frequent_event_lv1(self, countsminimum=10):\n        df = self.get_event_lv1()\n        counts = df.value_counts(ascending=True)\n        counts_df = counts.to_frame().rename(columns = {0: 'count'})\n        countsfiltered_df = counts_df[(counts_df['count'] >= countsminimum)].reset_index()\n        filter_lst = countsfiltered_df['event_lv1'].to_list()\n\n        df1 = self.data\n        data_filtered_df = df1[df1['event_lv1'].isin(filter_lst)]\n        self.set_data(data_filtered_df)\n        return data_filtered_df\n        \n    def get_event_lv1_itemset(self):\n        df = self.cutout_event_lv2()\n        df = df.groupby('agent')['event_lv1'].apply(list).reset_index(name='itemset')\n        df['count'] = 0\n        \n        for index, row in df.iterrows():\n            df.loc[index, ['count']] = len(row[1])\n            \n        df = df.sort_values(by=['count'], ascending=False)\n        df = df.drop(columns=['agent', 'count'])\n        df = df.reset_index().drop(columns=['index'])\n        return df\n    \n    def frequent_itemsets(self, integer_values=False):\n        df = self.get_event_lv1_itemset()\n        dataset = df.iloc[:,0].tolist()\n\n        te = TransactionEncoder()\n        te_ary = te.fit(dataset).transform(dataset)\n        df = pd.DataFrame(te_ary, columns=te.columns_)\n        \n        if integer_values == False:\n            self.itemsets = df\n            return df\n        else:\n            self.itemsets = df.astype(int)\n            return self.itemsets\n\n    def kmodes_elbow(self):\n        data = self.frequent_itemsets()\n        cost = []\n        K = range(1,10)\n        for num_clusters in list(K):\n            kmode = KModes(n_clusters=num_clusters, init = \"Cao\", n_init = 1, verbose=1)\n            kmode.fit_predict(data)\n            cost.append(kmode.cost_)\n\n        plt.plot(K, cost, 'bx-')\n        plt.xlabel('k clusters')\n        plt.ylabel('Cost')\n        plt.title('Elbow Method For Optimal k')\n        plt.show()\n        # optimal k is 5\n        \n    def kmodes_centroids(self, n_clusters=2):\n        data = self.frequent_itemsets()\n        km = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=1)\n        clusters = km.fit_predict(data)\n        # Print the cluster centroids\n        print('kmodes_centroids:', km.cluster_centroids_)\n        \n        print('kmodes_centroids:')\n        for item in km.cluster_centroids_:\n            print(self.sparse_column_as_list(item))\n        \n    def mca_filtered_rare(self):\n        # apply filter on the data\n        self.filter_less_frequent_event_lv1()\n        \n        tips = self.frequent_itemsets()\n        mca = MCA(n_components = 2, n_iter = 3, random_state = 101)\n        mca.fit(tips)\n        tips_mca = mca.transform(tips)\n\n        fig, ax = plt.subplots()\n        fig.set_size_inches(14, 14)\n\n        mca.plot_coordinates(X = tips, ax=ax)\n        ax.set_xlabel('Component 1', fontsize=16)\n        ax.set_ylabel('Component 2', fontsize=16)\n        \n        # reset data to initial condition\n        self.reset_data()\n        \n    def sparse_column_as_list(self, vector, integertype=False):\n        df = self.itemsets\n        if integertype:\n            idxs = [i for i, e in enumerate(vector) if e != 0]\n        else:\n            idxs = [i for i, e in enumerate(vector) if e != False]\n\n        aslist = []\n        for pos in idxs:\n            aslist.append(df.columns[pos])\n            \n        return aslist\n","8b38d96b":"import numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom kmodes.kmodes import KModes\nimport seaborn as sns\nfrom prince import MCA\nimport sys\nimport unittest\n\nclass TestCluster(unittest.TestCase):\n    \n    def setUp(self):\n        self.instance = Cluster(data_splitted)\n        pass\n        \n    def test_get_agent(self):\n#         display('test_get_agent:\\n', self.instance.get_agent())\n        self.assertTrue(True)\n    \n    def test_get_event_lv1(self):\n#         display('test_get_event_lv1:\\n', self.instance.get_event_lv1())\n        self.assertTrue(True)\n    \n    def test_get_event_lv2(self):\n#         display('test_get_event_lv2:\\n', self.instance.get_event_lv2())\n        self.assertTrue(True)\n    \n    def test_cutout_event_lv2(self):\n#         display('test_cutout_event_lv2:\\n', self.instance.cutout_event_lv2())\n        self.assertTrue(True)\n    \n    def test_plot_most_popular_event_lv1(self):\n#         self.instance.plot_most_popular_event_lv1()\n        self.assertTrue(True)\n        \n    def test_plot_most_frequent_agent(self):\n#         self.instance.plot_most_frequent_agent()\n        self.assertTrue(True)\n    \n    def test_plot_most_frequent_event_lv1(self):\n#         self.instance.plot_most_frequent_event_lv1()\n        self.assertTrue(True)\n        \n    def test_get_event_lv1_itemset(self):\n#         self.instance.get_event_lv1_itemset()\n        self.assertTrue(True)\n        \n    def test_frequent_itemsets(self):\n#         display(self.instance.frequent_itemsets().head(10))\n#         print(self.instance.frequent_itemsets().loc[0, :].values.flatten().tolist())\n#         display('test_frequent_itemsets=', self.instance.frequent_itemsets().head(10))\n#         display('test_frequent_itemsets=', self.instance.frequent_itemsets(integer_values=True).head(10))\n        self.assertTrue(True)\n    \n    def test_kmodes_elbow(self):\n#         self.instance.kmodes_elbow()\n        self.assertTrue(True)\n        \n    def test_kmodes_centroids(self):\n#         self.instance.kmodes_centroids(n_clusters=5)\n        self.assertTrue(True)\n        \n    def test_filter_less_frequent_event_lv1(self):\n#         display('filter_less_frequent_event_lv1:', self.instance.filter_less_frequent_event_lv1())\n        self.assertTrue(True)\n    \n    def test_mca_filtered_rare(self):\n#         self.instance.mca_filtered_rare()\n        pass\n        \n\nmain = TestCluster()\nsuite = unittest.TestLoader().loadTestsFromTestCase(TestCluster)\nunittest.TextTestRunner(verbosity=4,stream=sys.stderr).run(suite)  ","ca872d41":"cluster_inst = Cluster(data_splitted)\ncluster_inst.plot_most_popular_event_lv1()","e8f16bd5":"cluster_inst.plot_most_frequent_event_lv1()","fdc09459":"cluster_inst.plot_most_frequent_agent()","927a11b5":"cluster_inst.kmodes_elbow()","254383c9":"cluster_inst.kmodes_centroids(n_clusters=5)","aef4d5ac":"cluster_inst.mca_filtered_rare()","57245fea":"# USE YOUR FILEPATH TO CSV FILE\nfile = FILE \ndata = pd.read_csv(file, header=0)\ndata.head()","cf45ac86":"class Summary:\n    \n    def __init__(self, file):\n        data = pd.read_csv(file, header=0)\n        data_cleaned_lst = []\n        data_noagent_lst = []\n\n        for index, row in data.iterrows():\n            m = re.findall(r'[^: ].+?(?=:)', row[0])\n            if m:\n                data_cleaned_lst.append([m[0], row[0].replace(m[0]+\": \", \"\")])\n            else:\n                data_noagent_lst.append(row[0])\n\n        data_cleaned_df = pd.DataFrame(data_cleaned_lst, columns=['agent', 'event'])\n        \n        data_agent = data_cleaned_df.iloc[:, :1]\n        data_event = data_cleaned_df.iloc[:, 1:]\n\n        data_event_ready = Cleaner(data_event).clean()\n        data_event_ready.rename({'ticket': 'event'}, axis=1, inplace=True)\n        data_event_lv1_lv2 = split_events(data_event_ready)\n        data_splitted = pd.concat([data_agent, data_event_lv1_lv2], axis=1)\n        \n        self.data = data_splitted\n        \n    def most_frequent(self, df):\n        counts = df.value_counts(ascending=False).head(20)\n        counts_df = counts.to_frame()\n        top = counts_df.index[0][0]\n        return top\n        \n    def report(self):\n        cluster_inst = Cluster(self.data)\n        total_entries = len(self.data.index)\n        total_users = len(cluster_inst.get_agent().iloc[:, 0].unique())\n        total_events = len(cluster_inst.get_event_lv1().iloc[:, 0].unique())\n        most_frequent_user = self.most_frequent(cluster_inst.get_agent())\n        most_frequent_event = self.most_frequent(cluster_inst.get_event_lv1())\n        \n        print('\\tR E P O R T',\n              '\\n|------------------------------------------------------------|',\n              '\\nTotal entries:\\t', total_entries, \n              '\\nTotal users:\\t', total_users,\n             '\\nTotal events:\\t', total_events,\n             '\\nMost frequent user:\\t', most_frequent_user,\n             '\\nMost frequent event:\\t', most_frequent_event,\n             '\\n|------------------------------------------------------------|')\n    ","0c89ebdf":"import unittest\n\nclass TestSummary(unittest.TestCase):\n    \n    def setUp(self):\n        file = '\/kaggle\/input\/textclustering\/tickets.csv'\n        self.instance = Summary(file)\n        \n    def test_report(self):\n        self.instance.report()\n        \nmain = TestSummary()\nsuite = unittest.TestLoader().loadTestsFromTestCase(TestSummary)\nunittest.TextTestRunner(verbosity=4,stream=sys.stderr).run(suite)    ","72fe3dd0":"We can see the events that correspond to the **centroids of clusters**. These events happen simultanously with other events, they form a frequency blob. Many of other events are followed by one of these events emitted by the same agent.  \nThey are 'System or agent has recently restarted', 'Interface state changed to operationally down', 'Required service not running', 'Device Failed Availability Check'. The empty centroid is a random event. It is remarkable that random events are quite frequent. The centroid is not associated with any event, because it is a blob of rare events.","b15ff708":"### Function of Splitting Events to Level 1 and Level 2","9017a332":"To write a function to split events I need a test data, much shorter than the original dataset. The test data is taken from the original dataset.","60e985e4":"Now we see that not all of the entries in .csv file has the assumed format 'agent: event'. Some entries don't have the agent part, the entries don't have the colon symbol. Why that happened? Maybe messages like that were sent by the operating system itself, by the computer that runs the script. In this case, agent is not shown. So the first case is that the message is sent by the computer.  \nThe second assumption is that the messages are broken, something happened and the messages are incomplete. So the second assumption is simply that we deal with errors in data.  \nThe percentage of these events is very small. We easily can remove these events. ","f1bbeeb3":"The function ```split_events()``` takes the event dataframe and splits the column to two of level 1 and level 2.","1091bb3a":"The first plot shows the **most popular events**, it means that the most popular event has the maximum of agents that emit it.","7d5cec8e":"Some events are simple, they consist from a single message. Other events are more complex, they have a root message before the colon followed by the submessage. For the deeper analysis I will split messages to 2 levels. ","cdaaba13":"# Text Clustering","f23fa739":"The problem of the dataset is that the variables are categorical and not numerical. That is why most of the clustering techniques don't suit the data. For example, one of the most popular clustering technique, K-means, can't be applied to categorical variables. In this case we use the method for categorical features, K-Modes.  \nFirst, we plot the elbow graph of K-modes to determine the optimal number of clusters. Afterwards, we find the cluster centroids.","4bf676e8":"### Data of Agents, Events Level 1, Events Level 2  ","1389e982":"The idea behind ```Cleaner``` class is simple, it filters out all the information that makes messages unique. Some messages contain IDs, ports, IPs, other numbers. Sometimes they contain paths. This personalized information is irrelevant to our analysis and unimportant. Messages should contain the name of the service and the type of the message. The analysis should be conducted on the types of messages.","66c128b4":"### K-Modes","2d319d92":"For example, the cluster of events 'Disk', 'Pages per Second has exceeded the threshold The value is currently', 'has exceeded threshold', 'Filesystem unavailable' has coordinates (0, 4).  \nAnother cluster of events is 'VMware', 'UCS', 'Trap Received', 'Microsoft', 'IPSLA', 'iDRAC Power Supply Status Problem', located at (0, 0)","922e4593":"**Terminology**  \n**Agent** is the entity that sends a message. It can be a service, for example. Usually, the agent is printed in the beginning of the message.  \n**Event** is the message that is sent by the agent. ","c45fb1a3":"### MCA","3d61bf0e":"The optimal number of clusters is 5.","f5b5a4ba":"### Creating Class for Processing Tickets","c497ba18":"## 1. Data Wrangling","c7808521":"### Summary","33ac0cc7":"The third plot describes the **most frequent agents**, agents that have the maximum entries in the data.","d036c346":"More close look at the entries that don't follow the rule of the format 'agent: event' shows that all of the entries contain the word 'BGP'. It is reasonable to save this entries to another category, we clearly can see a pattern in this entries. ","6ebb82e6":"Now the ticket has the format for the further processing. The data consists of three columns, the first column is the agent that emits the event. The second column is the primary part of the event, it shows the service that generates the event. The third part is the message generated by the service. ","f2496d6e":"Analysis of the data will be made in a ```Cluster``` class. Analysis is divided to two categories: a simple analysis, mostly based on rankings, and more complex analysis based on clustering techniques.  \nFor clustering I used ```TransactionEncoder``` which is similar to ```OneHotEncoder```, a common choice for categorical features.","01ebde99":"This method can't be finished, it contains errors, the problem is in the library for the method. It can't process data like the original. The error can't be removed. Nevertheless, it is useful to show the results. It plols the data as a 2D-graph. We can see some clusters on the plane.  ","debb19fc":"The ```Cleaner``` class is ready and has been tested on the test data. Now we can apply the class to the original data. ","e3d88a65":"The log file contains various data, formats of the log messages are very different. It is important to be confident that all of the data in the file is parsed properly, parsed as expected. The file is long, approximately 5 thousand messages. It is impossible to review the whole file. During approbation of techniques of various kinds, we need a clear test that shows effectiveness of processing the file. That is why I create a short version of the log file, 'testdata'. The short version contains most of the types of messages we can find in the log file. I will run the code against the 'testdata'.","78279f05":"## 2. Data Analysis ","8cb38600":"All assumptions about parsing the data.  \nSuppose, the ticket has the following format: 'agent : event'. The first part before the colon is the agent that sends the event message. The second part after the colon is the event message. This assumption looks reasonable, many of the entries in the .csv file have this format.  I'll try to parse the data in accordance with this hypothesis.","d578eff8":"The second plot shows the **most frequent events**, the most frequent event has the maximum of entries in the data.","dc5b98e7":"```Summary``` class is a class that makes short report on the data."}}