{"cell_type":{"82e4feee":"code","6d8dafd6":"code","ca14f250":"code","13f0325c":"code","aa9a01df":"code","2acfaa71":"code","82e809a7":"code","dce5213b":"code","34760b97":"code","d9724b7e":"code","da514ea7":"code","03ed489f":"code","146f41af":"code","f8ea63a8":"code","2c38b02c":"code","f0c4ffa5":"code","609c69fc":"code","b98fc933":"code","190dcf63":"code","cdc55958":"code","ffa40bca":"code","e6b50f48":"code","5670f23e":"code","8c1537f6":"code","1f43b3ce":"code","737ad572":"code","0daf6450":"code","8fbf4331":"code","d798ddae":"code","af7d89c2":"code","a253a663":"code","5da18d95":"code","87e8a3a4":"code","dcc4b96e":"code","17aec92c":"code","8f4dc4fa":"code","4aabf8c5":"code","aef1e64e":"code","cea37a82":"code","ad8aef04":"code","f894194a":"code","dca8e9f1":"code","97baa34f":"code","8006fec9":"code","cff3d94f":"code","b7744b3b":"code","65d524f4":"code","9c36a854":"code","4c0d7fec":"code","567f1127":"code","24d1659d":"code","549fbca6":"code","f8b9d719":"code","1ff87803":"code","48eeb4f6":"code","ce700ce7":"code","2e8cfece":"code","e716c14e":"code","dd02963f":"code","d1b80294":"code","6868d3e4":"code","10288dec":"code","a9e18d3a":"code","1a0f46ad":"code","7ed53466":"code","3a7c166c":"code","e6e951ea":"code","911bd466":"code","24b43a35":"code","1ec0d77d":"code","a88dbb15":"code","c7cc188c":"code","bbe2f407":"code","06756c5b":"code","dfaf3f1b":"code","65be7249":"code","d48a34c0":"code","8c17d7c4":"code","75cf855b":"code","880930aa":"code","3f6226e6":"code","74220373":"code","0ea1df57":"code","3824db44":"code","2ee58176":"code","7de8280b":"code","b76643e1":"code","07e5e8c5":"code","19f951d8":"code","3d7e3ca5":"code","ef84c226":"code","b7a6dc6d":"code","c054782c":"code","d34f08ae":"code","8189d16d":"markdown","6b0273e1":"markdown","c77afff9":"markdown","29521d97":"markdown","241d457e":"markdown","9c258b4c":"markdown","c80017be":"markdown","f4efab01":"markdown","d8fab993":"markdown","97c71b70":"markdown","f68055ab":"markdown","fd2ee173":"markdown","f4a8b3a9":"markdown","7cfc934f":"markdown","ce76cb36":"markdown","8ef19c89":"markdown","b9255496":"markdown","b84be0fa":"markdown","5d574bcb":"markdown","deb2c71e":"markdown","c84c8940":"markdown","276e8f71":"markdown","75142ca2":"markdown","fdf1580d":"markdown","03580cf6":"markdown","cdc5b55b":"markdown","158431d2":"markdown","e38ce2a9":"markdown","7ff836d5":"markdown","6f5526da":"markdown","667b9e90":"markdown","a4b26d64":"markdown","2fe0a20b":"markdown","66496f98":"markdown","c66ad9cb":"markdown","a70d945c":"markdown","26d171a5":"markdown","40207789":"markdown","8dca9a52":"markdown","c2adba0a":"markdown"},"source":{"82e4feee":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy.stats import norm\nfrom scipy import stats\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('fivethirtyeight')","6d8dafd6":"data = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv') ","ca14f250":"data.head()","13f0325c":"data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)","aa9a01df":"data.shape","2acfaa71":"data.info()","82e809a7":"data.describe()","dce5213b":"data.var()","34760b97":"data.hist(bins=50, figsize=(30,25)) \nplt.show()","d9724b7e":"sns.countplot(x=\"Exited\", data=data)\nplt.show()","da514ea7":"sns.countplot(x=\"Gender\", data=data)\nplt.show()","03ed489f":"sns.displot(data, x=\"Age\", hue=\"Age\")\nplt.show()","146f41af":"sns.countplot(x=\"HasCrCard\", data=data)\nplt.show()","f8ea63a8":"sns.countplot(x=\"IsActiveMember\", data=data)\nplt.show()","2c38b02c":"sns.countplot(x=\"NumOfProducts\", data=data)\nplt.show()","f0c4ffa5":"corr_matrix = data.corr()\nf, ax = plt.subplots(figsize=(25, 15))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr_matrix, cmap=cmap, vmax=.5, annot=True, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","609c69fc":"High_corr = corr_matrix.nlargest(4, 'Exited')['Exited'].index\nHigh_corr","b98fc933":"corr_matrix[\"Exited\"].sort_values(ascending=False)","190dcf63":"new_df = data.copy()","cdc55958":"# This function will calculate the IQR for us and save the values that is higher or lower as follwow\ndef IQR(column_name):\n    Q1 = new_df[column_name].quantile(0.12)\n    Q3 = new_df[column_name].quantile(0.88)\n    IQR = Q3 - Q1\n    upper_limit = Q3 + 1.5 * IQR\n    lower_limit = Q1 - 1.5 * IQR\n    values_upper = new_df[new_df[column_name] > upper_limit]\n    values_lower = new_df[new_df[column_name] < lower_limit]\n    \n    return values_upper, values_lower, upper_limit, lower_limit","ffa40bca":"# this Function will check if the returned shape from IQR is higher than zero \n# why zero! cos the output will be for example like this (2,63) that means there are 2 rows contains outliers \n# and if it more than zero it will show us this rows\ndef upper(column_name):\n    if values_upper.shape[0] > 0:\n        print(\"Outliers upper than the higher limit: \")\n        return new_df[new_df[column_name] > upper_limit]\n    else:\n        print(\"There are no values higher than the upper limit!\")","e6b50f48":"# same as above but for lower values\ndef lower(column_name):\n    if values_lower.shape[0] > 0:\n        print(\"Outliers lower than the higher limit: \")\n        return new_df[new_df[column_name] < lower_limit]\n    else:\n        print(\"There are no values lower than the lower limit!\")","5670f23e":"# this function will delete any outliers upper or lower the limit\ndef outliers_del(column_name):\n    # we will make new_df global to consider the global variable not the local\n    global new_df\n    new_df = new_df[new_df[column_name] < upper_limit]\n    new_df = new_df[new_df[column_name] > lower_limit]\n    print(\"the old data shape is :\", data.shape)\n    print(\"the new data shape is :\", new_df.shape)","8c1537f6":"# this function is for ploting the data \ndef plot(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(16,5))\n    #plt.subplot(1,2,1)\n    # we will use fit norm to draw the normal distibutions that the data sould be it will be in black \n    #sns.distplot(data[column_name], fit=norm)\n    plt.subplot(1,2,1)\n    sns.boxplot(data[column_name],palette=\"rocket\")\n    plt.show()","1f43b3ce":"def outlier_compare(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(25,15))\n    plt.subplot(2,2,1)\n    sns.boxplot(data[column_name], palette=\"rocket\")\n    plt.subplot(2,2,2)\n    sns.boxplot(new_df[column_name], palette=\"rocket\")\n    plt.show()","737ad572":"Upper_Outliers_columns = []\nLower_Outliers_columns = []\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfor column in new_df:\n    if new_df[column].dtype in numeric_dtypes:\n        values_upper, values_lower, upper_limit, lower_limit = IQR(column)\n        if values_upper.shape[0] > 0:\n            Upper_Outliers_columns.append(column)\n        if values_lower.shape[0] > 0:\n            Lower_Outliers_columns.append(column)","0daf6450":"print('Columns upper the limit is: ', Upper_Outliers_columns)\nprint('Columns lower the limit is: ', Lower_Outliers_columns)","8fbf4331":"plot('Age')","d798ddae":"values_upper, values_lower, upper_limit, lower_limit = IQR('Age')","af7d89c2":"upper('Age')","a253a663":"lower('Age')","5da18d95":"outliers_del('Age')","87e8a3a4":"outlier_compare('Age')","dcc4b96e":"from scipy.stats import skew\n\nskewness_list = {}\nfor i in new_df:\n    if new_df[i].dtype != \"object\":\n        skewness_list[i] = skew(new_df[i])\n\nskewness = pd.DataFrame({'Skew' :skewness_list})\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(15,9))\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Skewness', fontsize=15)\nplt.xticks(rotation='90')\nplt.bar(range(len(skewness_list)), list(skewness_list.values()), align='center')\nplt.xticks(range(len(skewness_list)), list(skewness_list.keys()))\n\nplt.show()","17aec92c":"skewness_list","8f4dc4fa":"X = new_df.drop(\"Exited\", axis=1)","4aabf8c5":"y = new_df['Exited'].copy()","aef1e64e":"X.shape, y.shape","cea37a82":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, shuffle=True, random_state=42, stratify=y)","ad8aef04":"# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# Encoders\nfrom sklearn.preprocessing import OneHotEncoder\n# Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n# feature selection \nfrom sklearn.feature_selection import SelectPercentile, chi2\n# Cols transform\nfrom sklearn.compose import make_column_transformer\n# Pipeline\nfrom sklearn.pipeline import make_pipeline\n# cross val\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n# interactive diagrams of Pipelines \nfrom sklearn import set_config\nset_config(display='diagram')","f894194a":"OHE = OneHotEncoder()\nMMS = MinMaxScaler()\ncv = StratifiedKFold(5, random_state=1, shuffle=True)","dca8e9f1":"column_trans = make_column_transformer(\n    (OHE, ['Geography', 'Gender']),\n    (MMS, ['CreditScore', 'Balance', 'EstimatedSalary']),\n    remainder='passthrough')","97baa34f":"column_trans.fit_transform(X_train)","8006fec9":"RF = RandomForestClassifier(random_state=4, criterion='gini', max_depth=10, max_features='auto')","cff3d94f":"RF_pipe = make_pipeline(column_trans, RF)","b7744b3b":"RF_pipe","65d524f4":"cross_val_score(RF_pipe, X_train, y_train, cv=cv, scoring='accuracy').mean()","9c36a854":"RF_pipe.fit(X_train, y_train);","4c0d7fec":"RF_pipe.score(X_train, y_train)","567f1127":"RF_pred = RF_pipe.predict(X_train)","24d1659d":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, RF_pred)","549fbca6":"LR = LogisticRegression(random_state=4, C=1, max_iter=1000, multi_class='auto', penalty='l1',solver='saga')","f8b9d719":"LR_pipe = make_pipeline(column_trans, LR)","1ff87803":"LR_pipe","48eeb4f6":"cross_val_score(LR_pipe, X_train, y_train, cv=cv, scoring='accuracy').mean()","ce700ce7":"LR_pipe.fit(X_train, y_train);","2e8cfece":"LR_pipe.score(X_train, y_train)","e716c14e":"LR_pred = LR_pipe.predict(X_train)","dd02963f":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, LR_pred)","d1b80294":"KNN = KNeighborsClassifier(algorithm='auto', leaf_size=30, n_neighbors=9, weights='uniform')","6868d3e4":"KNN_pipe = make_pipeline(column_trans, KNN)","10288dec":"KNN_pipe","a9e18d3a":"cross_val_score(KNN_pipe, X_train, y_train, cv=cv, scoring='accuracy').mean()","1a0f46ad":"KNN_pipe.fit(X_train, y_train);","7ed53466":"KNN_pipe.score(X_train, y_train)","3a7c166c":"KNN_pred = KNN_pipe.predict(X_train)","e6e951ea":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, KNN_pred)","911bd466":"hyper_params = {}\nhyper_params['randomforestclassifier__n_estimators'] = [150, 200, 250]\nhyper_params['randomforestclassifier__max_depth'] = [7, 8, 9, 10]\nhyper_params['randomforestclassifier__criterion'] = ['gini','entropy']\nhyper_params['randomforestclassifier__max_features'] = ['auto', 'sqrt', 'log2']","24b43a35":"from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(RF_pipe, hyper_params, cv=cv, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train, y_train);","1ec0d77d":"grid.best_score_","a88dbb15":"grid.best_params_","c7cc188c":"results = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]","bbe2f407":"results.sort_values('rank_test_score')","06756c5b":"hyper_params = {}\nhyper_params['logisticregression__penalty'] = ['l1', 'l2']\nhyper_params['logisticregression__C'] = [.001, .01, .1, 1]\nhyper_params['logisticregression__solver'] = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\nhyper_params['logisticregression__max_iter'] = [100, 1000]","dfaf3f1b":"grid = GridSearchCV(LR_pipe, hyper_params, cv=cv, scoring='accuracy')\ngrid.fit(X_train, y_train);","65be7249":"grid.best_score_","d48a34c0":"grid.best_params_","8c17d7c4":"results = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]","75cf855b":"results.sort_values('rank_test_score')","880930aa":"hyper_params = {}\nhyper_params['kneighborsclassifier__n_neighbors'] = [5, 6, 7, 8, 9]\nhyper_params['kneighborsclassifier__weights'] = ['uniform','distance']\nhyper_params['kneighborsclassifier__algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute']\nhyper_params['kneighborsclassifier__leaf_size'] = [30, 40, 50]","3f6226e6":"grid = GridSearchCV(KNN_pipe, hyper_params, cv=cv, scoring='accuracy')\ngrid.fit(X_train, y_train);","74220373":"grid.best_score_","0ea1df57":"grid.best_params_","3824db44":"results = pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'rank_test_score']]","2ee58176":"results.sort_values('rank_test_score')","7de8280b":"# Evaluation metrices\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score","b76643e1":"RF_pred = RF_pipe.predict(X_test)","07e5e8c5":"accuracy_score(y_test, RF_pred)","19f951d8":"plot_confusion_matrix(RF_pipe, X_test, y_test, cmap=plt.cm.Blues)  \nplt.show() ","3d7e3ca5":"roc_auc_score(y_test, RF_pred, multi_class='ovo')","ef84c226":"KNN_pred = KNN_pipe.predict(X_test)","b7a6dc6d":"accuracy_score(y_test, KNN_pred)","c054782c":"plot_confusion_matrix(KNN_pipe, X_test, y_test, cmap=plt.cm.Blues)  \nplt.show() ","d34f08ae":"roc_auc_score(y_test, KNN_pred, multi_class='ovo')","8189d16d":"#### RandomForestClassifier","6b0273e1":"lets start with Age then CreditScore ","c77afff9":"### Skewness","29521d97":"#### Age","241d457e":"What we will do here?\n\nwell i used to to the following steps one by one but when i learned to do it with pipeline eveything has changed!,\n\nits much organized and more simple so, here what we will combine to our pipeline.\n\n##### Encoding\n\nMachine learning algorithms works only with numerical feature and here we have some categrical feature so we need to convert them into numbers and that is what we called encoding there are many ways to encode your data\n\n    Encoding \n    Replacing \n    Get dummies \n    and more\n    \nthe most popular one is \"get_dummies\" but a big note here if you going to use get dummies you have to use it before splitting the data and if you try to apply \"get_dummies\" to the traing set and test set you will get two different results and it not going to work!\n\nwe will use sklearn encoder called one hot encoder \"OneHotEncoder\"\n\n##### Feature Scaling\n\nMachine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled.\n\nwe have three methods in sklearn \n\nMinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).\n\nStandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.\n\nIf there are outliers, use RobustScaler(). Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)\n\nWe delete most outliers earlier so we can use MinMaxScaler or StandardScaler\n\n\n","9c258b4c":"# Our methodology\n\n\n## Data visualization \n\n    Loading the data\n    Take a quick look at our data \n    Understanding our data\n    Finding the correlations\n    \n## Data preperation \n\n    Outliers detection\n    Skweness correction\n    \n## Data spliting\n\n\n## Pipeline\n\n    Encoding \n    Feature scaling\n    \n## Modeling\n\n    Building the model\n    Evaluation with cross-validation\n    \n## Fine-tuning \n\n    Finding the best hyperparameters\n\n    \n## Testing our model\n\n    Evaluate the model with the test set\n    ","c80017be":"## Data preperation","f4efab01":"## Fine-Tune Our Model","d8fab993":"convert results into a DataFrame","97c71b70":"### Loading the data","f68055ab":"## Modeling","fd2ee173":"### Take a quick look at our data","f4a8b3a9":"well the best score we got is around 87% from RandomForestClassifier but while modeling and testing i have got 92% score but when i ran the code a gain the score has changed it you have an idea why this happened just let me know!","7cfc934f":"## Evaluate Our System on the Test Set ","ce76cb36":"Why we will use stratify in spliting?\n\nSome classification problems can exhibit a large imbalance in the distribution of the target \nclasses: for instance there could be several times more negative samples than positive samples. \nIn such cases it is recommended to use stratified sampling as implemented in StratifiedKFold \nand StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in \neach train and validation fold.","8ef19c89":"### Find the correlations","b9255496":"confusion_matrix","b84be0fa":"#### But before getting our hands dirty lets define some functions that we will use a lot like \n    \"IQR\" to calculate the IQR for us \n    \"Upper and Lower\" to fetch upper values and lower values that contain outliers \n    \"outliers_del\" to delete them \n    \"Plot\" function to plot the curves \n    \"outlier_compare\" to compare the data before deleting outliers and correct the skewness and after\n    \nI will write a comment for each function when creating it","5d574bcb":"#### RandomForestClassifier","deb2c71e":"#### RF","c84c8940":"## Data visualization ","276e8f71":"### Outliers Detection\n\nwe have a various methods to detect the outliers i am going to use IQR here this method works fine for me but \n\nyou can try other methods like \n\n            1- Z-score method\n            2. Robust Z-score\n            3. I.Q.R method\n            4. Winterization method(Percentile Capping)\n            5. DBSCAN Clustering\n            6. Isolation Forest\n            7. Visualizing the data\n            \nIQR stands for \"Inter Quartiles Range\"\n\nthis method depends on two values \n    \n    Q1 >> which represents a quarter of the way through the list of all data usually this value is 0.25 but i will use .15 trying not to delete a lot of data \n    \n    Q3 >> which represents three-quarters of the way through the list of all data usually this value is 0.75 but i will use .80 for the same resone\n    \nhow IQR works :\n    well first it sorts the data and finds its median \n    then seperate the numbers before the median and finds its own median \"Q1\"  and also seperates the numbers \n    after the total medain and finds its own median \"Q3\"\n    \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1a\/Boxplot_vs_PDF.svg\/1200px-Boxplot_vs_PDF.svg.png\">\n\nthen we will take the diffrance between Q3 and Q1","75142ca2":"After we trained our model and take an idea about how it performed no time to find the optimal hyperparameters of the model\nOne way to do that would be to fiddle with the hyperparameters manually until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\nInstead, you should get Scikit-Learn\u2019s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation and the amazing thing here that it will search for \nthe hyperparameters for the entire pipeline not only the model!\n\nGridSearchCV may take a long time so you should try RandomizedSearchCV this method chose random hyperparameters \nand mix them and with this way to determine how many times you want your search to iterate cos GridSearchCV maybe \ncos high cost.","fdf1580d":"#### KNeighborsClassifier","03580cf6":"this will fit the data of X_train and it will train from it and when we use predict it will only do transform to the data based on the data that has been learned from fit \"X_train\" means that it learn from Train data and transform the test data based on train data that prevent data leakage! and data leakage accuares when the model learn new staff from test data that was not exiest in traing data!","cdc5b55b":"## Data spliting","158431d2":"this will fit the data of X_train and it will train from it and when we use predict it will only do transform to the data based on the data that has been learned from fit \"X_train\" means that it learn from Train data and transform the test data based on train data that prevent data leakage! and data leakage accuares when the model learn new staff from test data that was not exiest in traing data!","e38ce2a9":"#### KNeighborsClassifier","7ff836d5":"#### KNN","6f5526da":"## Pipeline","667b9e90":"well i will ignore NumOfProducts and Exited cos those are categorical data!","a4b26d64":"#### LogisticRegression","2fe0a20b":"this is so important when using cross validation for the entire pipeline it first split the data into cv \nnumber and then pass it to the pipeline process this is better than preprocess the data first and feed it after\nprocessing to the model and just use cross validation to the model only instead this way will validate the entire \npipeline process!","66496f98":"well i tried to correct age skewness but i got a lower score so we will keep everything as it is","c66ad9cb":"### Understanding our data","a70d945c":"<img src=\"https:\/\/res.cloudinary.com\/dn1j6dpd7\/image\/fetch\/f_auto,q_auto,w_736\/https:\/\/www.livechat.com\/wp-content\/uploads\/2016\/04\/customer-churn@2x.jpg\">","26d171a5":"### Importing needed libraries","40207789":"confusion_matrix","8dca9a52":"#### LogisticRegression","c2adba0a":"this will fit the data of X_train and it will train from it and when we use predict it will only do transform to the data based on the data that has been learned from fit \"X_train\" means that it learn from Train data and transform the test data based on train data that prevent data leakage! and data leakage accuares when the model learn new staff from test data that was not exiest in traing data!"}}