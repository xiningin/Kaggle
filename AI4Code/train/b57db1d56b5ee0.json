{"cell_type":{"59f36698":"code","7962c083":"code","62155dee":"code","dab79f2b":"code","cb533c06":"code","52464894":"code","fb4b6cc8":"code","7aebf562":"code","cea6cc52":"code","32e5b57f":"code","32f876e4":"code","55eef8ca":"code","af54823d":"code","da25b241":"code","e5247983":"code","f31e3c54":"code","06409470":"code","8136110f":"code","9acf314c":"code","27053a01":"code","c9f2a037":"code","731f73c0":"code","1ab18de8":"code","f2e9acdd":"markdown","df5d760d":"markdown","4b53b6a6":"markdown","8cbe5ea9":"markdown","e9bdf5a2":"markdown","7dc9fcf4":"markdown","35314dc8":"markdown"},"source":{"59f36698":"import datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport nltk\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom tqdm import tqdm","7962c083":"df_train = pd.read_csv('..\/input\/20-newsgroups-ciphertext-challenge\/train.csv')\ndf_train.head()","62155dee":"df_test = pd.read_csv('..\/input\/20-newsgroups-ciphertext-challenge\/test.csv')\ndf_test.head()","dab79f2b":"news_list = pd.read_csv('..\/input\/20-newsgroups\/list.csv')\nnews_list.shape","cb533c06":"print(df_train.shape, df_test.shape)","52464894":"len(df_train['target'].value_counts()) # 20 Newsgroups -- checks out","fb4b6cc8":"# Are the classes balanced?\n\ncount_target = df_train['target'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(count_target.index, count_target.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Target', fontsize=12);","7aebf562":"df_train['ciphertext'].iloc[0]","cea6cc52":"df_train.info()","32e5b57f":"## Basic features (a lot of these ideas from https:\/\/www.kaggle.com\/opanichev\/lightgbm-and-simple-features)\n\ndef add_feats(df): # Some of these features might be strongly correlated\n    \n    tqdm.pandas('add_basic')\n    df['len'] = df['ciphertext'].progress_apply(lambda x: len(str(x))) # Characters\n    df['unique'] = df['ciphertext'].progress_apply(lambda x: len(set(str(x)))) # Unique characters\n    df['punctuations'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    df['uniq_punctuations'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c in string.punctuation])))\n    df['letters'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isalpha()]))\n    df['uniq_letters'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isalpha()])))\n    df['numbers'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isdigit()]))\n    df['uniq_numbers'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isdigit()])))\n    df['uppercase'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isupper()]))\n    df['uniq_uppercase'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isupper()])))\n    df['lowercase'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.islower()]))\n    df['uniq_lowercase'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.islower()])))","32f876e4":"add_feats(df_train)","55eef8ca":"add_feats(df_test)","af54823d":"df_train.head()","da25b241":"plt.figure(figsize=(12,12))\nsns.violinplot(x='target', y='unique', data=df_train)\nplt.xlabel('Target', fontsize=12)\nplt.ylabel('Number of unique characters in text', fontsize=12)\nplt.title(\"Number of unique characters by target\", fontsize=15);","e5247983":"plt.figure(figsize=(12,12))\nsns.violinplot(x='target', y='uniq_punctuations', data=df_train)\nplt.xlabel('Target', fontsize=12)\nplt.ylabel('Number of unique punctuations in text', fontsize=12)\nplt.title(\"Number of unique punctuations by target\", fontsize=15);","f31e3c54":"fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,20))\nsns.violinplot(x='difficulty', y='unique', data=df_train, ax=ax1)\nax1.set_xlabel('Difficulty', fontsize=12)\nax1.set_ylabel('Number of unique characters in text', fontsize=12)\nsns.violinplot(x='difficulty', y='uniq_punctuations', data=df_train, ax=ax2)\nax2.set_xlabel('Difficulty', fontsize=12)\nax2.set_ylabel('Number of unique punctuations in text', fontsize=12)\nsns.violinplot(x='difficulty', y='numbers', data=df_train, ax=ax3)\nax3.set_xlabel('Difficulty', fontsize=12)\nax3.set_ylabel('Number of numbers in text', fontsize=12)\nsns.violinplot(x='difficulty', y='uppercase', data=df_train, ax=ax4)\nax4.set_xlabel('Difficulty', fontsize=12)\nax4.set_ylabel('Number of uppercase in text', fontsize=12);","06409470":"df_train.corr()['target'] # Some of these features seem to have strong negative correlations with the target\n## Unique punctuations matter apparently","8136110f":"cols_to_drop = ['Id','ciphertext']\nX = df_train.drop(cols_to_drop, axis=1, errors='ignore')\n\nfeature_names = list(X.columns)\n\ny = df_train['target'].values\nX = X.values\n\nX_test = df_test.drop(cols_to_drop, axis=1, errors='ignore')\nid_test = df_test['Id'].values","9acf314c":"lgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': 5,\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.85,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': -1,\n    'lambda_l1': 1.0,\n    'lambda_l2': 1.0,\n    'min_gain_to_split': 0,\n    'num_class': df_train['target'].nunique()\n}","27053a01":"cnt = 0\np_buf = []\np_valid_buf = []\nn_splits = 5\nkf = KFold(\n    n_splits=n_splits, \n    random_state=0)\nerr_buf = []   \nundersampling = 0","c9f2a037":"print(X.shape, y.shape)\nprint(X_test.shape)","731f73c0":"n_features = X.shape[1]\n\nfor train_index, valid_index in kf.split(X, y):\n    print('Fold {}\/{}'.format(cnt + 1, n_splits))\n    params = lgb_params.copy() \n\n    lgb_train = lgb.Dataset(\n        X[train_index], \n        y[train_index], \n        feature_name=feature_names,\n        )\n    lgb_train.raw_data = None\n\n    lgb_valid = lgb.Dataset(\n        X[valid_index], \n        y[valid_index],\n        )\n    lgb_valid.raw_data = None\n\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=10000,\n        valid_sets=[lgb_train, lgb_valid],\n        early_stopping_rounds=100,\n        verbose_eval=100,\n    )\n\n    if cnt == 0:\n        importance = model.feature_importance()\n        model_fnames = model.feature_name()\n        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n        tuples = [x for x in tuples if x[1] > 0]\n        print('Important features:')\n        for i in range(20):\n            if i < len(tuples):\n                print(tuples[i])\n            else:\n                break\n\n        del importance, model_fnames, tuples\n\n    p = model.predict(X[valid_index], num_iteration=model.best_iteration)\n    err = f1_score(y[valid_index], np.argmax(p, axis=1), average='macro')\n\n    print('{} F1: {}'.format(cnt + 1, err))\n\n    p = model.predict(X_test, num_iteration=model.best_iteration)\n    if len(p_buf) == 0:\n        p_buf = np.array(p, dtype=np.float16)\n    else:\n        p_buf += np.array(p, dtype=np.float16)\n    err_buf.append(err)\n\n    cnt += 1\n\n    del model, lgb_train, lgb_valid, p\n    gc.collect\n\n    # Train on one fold\n#     if cnt > 0:\n#         break\n\n\nerr_mean = np.mean(err_buf)\nerr_std = np.std(err_buf)\nprint('F1 = {:.6f} +\/- {:.6f}'.format(err_mean, err_std))\n\npreds = p_buf\/cnt","1ab18de8":"subm = pd.DataFrame()\nsubm['Id'] = id_test\nsubm['Predicted'] = np.argmax(preds, axis=1)\nsubm.to_csv('submission.csv', index=False)","f2e9acdd":"### Similar trend to the first plot. Perhaps let's look at this grouped by 'difficulty. ","df5d760d":"### Obviously this isn't great, but I'll add text features soon! ","4b53b6a6":"### More unique chars in difficulties 3 and 4, but otherwise not much\n","8cbe5ea9":"### Let's build a baseline with just these generated features for now -- this part draws from Sudalai Rajkumar's https:\/\/www.kaggle.com\/sudalairajkumar\/simple-feature-engg-notebook-spooky-author","e9bdf5a2":"### Examine the properties by target","7dc9fcf4":"### Thanks to https:\/\/www.kaggle.com\/opanichev\/lightgbm-and-simple-features","35314dc8":"### Mostly comparable, perhaps Target 2 has a slightly higher overall number of unique characters"}}