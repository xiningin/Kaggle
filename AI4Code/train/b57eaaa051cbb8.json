{"cell_type":{"2675104e":"code","977db845":"code","279d47cf":"code","ca2af528":"code","5b355d3f":"code","6c6a0013":"code","c1bd7fb5":"code","1bdd6bb4":"code","74003ce9":"code","a2e5fe05":"code","80817d9a":"code","94114996":"code","afe1590b":"code","c262ea33":"code","1b7872f0":"code","d7adca59":"code","72c4d964":"code","f6d9c361":"code","f8a6be0a":"code","46bc3d51":"code","2d561e73":"code","51d09337":"code","e85c26b9":"code","a9b1437b":"code","61b45519":"markdown","1325876d":"markdown","3c1b6b36":"markdown","0cbcd6bf":"markdown","7ecdd54e":"markdown","13534ff0":"markdown"},"source":{"2675104e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","977db845":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt","279d47cf":"df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf.head(10)","ca2af528":"df.describe()","5b355d3f":"df.isnull().sum()\/len(df)","6c6a0013":"fill_mean = lambda col : col.fillna(col.mean())\ndf = df.apply(fill_mean)","c1bd7fb5":"df.isnull().sum()","1bdd6bb4":"df.corr()","74003ce9":"import scipy \nfrom scipy.stats.stats import pearsonr\nfor i in range(1,118):\n    print(pearsonr(df['claim'], df['f'+ str(i)]))","a2e5fe05":"corr = df.corr()\ncriteria = corr[ corr.iloc[:]>= 0.1 ]     \nprint(len(criteria))\n","80817d9a":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(df.drop(labels=['claim', 'id'], axis=1),\n    df['claim'],\n    test_size=0.3,\n    random_state=0)","94114996":"from sklearn.feature_selection import mutual_info_classif\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info","afe1590b":"from sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(df)","c262ea33":"var_thres.get_support()","1b7872f0":"df['claim'].value_counts().sort_values(ascending=False).plot.bar(figsize=(10, 8))","d7adca59":"from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.fit(X_test)","72c4d964":"from sklearn.tree import DecisionTreeClassifier \nfrom sklearn.linear_model import LogisticRegression\nclf = DecisionTreeClassifier()\nclf.fit(scaled_X_train, y_train)","f6d9c361":"Y_pred = clf.predict_proba(X_test)\nprint(Y_pred)","f8a6be0a":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test, Y_pred[:,0]))","46bc3d51":"df1= pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nprint(df1.isnull().sum())","2d561e73":"fill_mean = lambda col : col.fillna(col.mean())\ndf1 = df1.apply(fill_mean)","51d09337":"Test = np.array(df1.drop(['id'], axis = 1))\nscaled_Test = scaler.fit(Test)","e85c26b9":"Test_pred = clf.predict_proba(Test)\nprint(Test_pred)","a9b1437b":"df3 = pd.DataFrame()\ndf3['id'] = df1['id']\ndf3['claim'] = Test_pred[:, 0]\ndf3.to_csv('.\/result.csv', index= False)","61b45519":"* As you can see there are significant number of missing values that is around 2 % of the dataset. So, I replaced all missing values with **Mean**.","1325876d":"As you can see the count of Pearson Correlation Coefficient value greater tha 0.1 is 120, it indicates that no two independent features are significantly related so we cannot remove\/drop any feature based on any **Correlation Coefficient**. \nSo, now I have tried feature selection using **Mutual Information** in classification problem statement. ","3c1b6b36":"# Feature Selection\nAnd the next step is about finding the relation between independent variables. As there are too many independent features plotting heat map could make whole visualization messy so I tried printing the **Correlation coefficient** and checked by running loop.","0cbcd6bf":"##  **We will start with handling missing values in the Dataset**","7ecdd54e":"As nothing worked I proceeded with all independent features into my training model.","13534ff0":"None of the feature have high mutual information with respect to dependent variable. So, I have tried checking for constant features using **Variance Threshold**. This could not be the effective way of removing features because the range of different independent features are very much different. So, I have just checked for constant values with variance threshold 0."}}