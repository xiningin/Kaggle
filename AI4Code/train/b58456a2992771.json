{"cell_type":{"69265441":"code","26d47923":"code","d8ef4922":"code","795ffa63":"code","cd101c25":"code","89bd9ed3":"code","129c77ef":"code","3ec26c97":"code","13e24199":"code","b288fab3":"code","05742d5e":"code","5c4a392e":"code","a05e8c9f":"code","90893dd7":"code","b036131e":"code","70f630f1":"code","8fee8c93":"code","9eb2fc44":"code","0e6b9533":"code","0a081f0a":"code","2ad26c95":"code","eb4ac175":"code","ae2a48f9":"code","3e968215":"code","65b2ac60":"code","58bd26da":"code","d4a11e82":"code","17e00a6c":"code","4da1036d":"markdown","f3c1a649":"markdown","6f099c8f":"markdown","3db1cf89":"markdown","78f2e58f":"markdown","e64c0ec3":"markdown","377bbb13":"markdown","6dbedddf":"markdown","399320ed":"markdown","1536acf9":"markdown","b85a923c":"markdown","8c19d041":"markdown","89a01f7a":"markdown","442de6b2":"markdown","887c552c":"markdown","6000a04b":"markdown","70f13b17":"markdown","9fb33e7b":"markdown","a943d78f":"markdown","2e5842e2":"markdown","fe910e06":"markdown","fffc89ca":"markdown","8d77a761":"markdown","1a6a84fc":"markdown","c19f4bf3":"markdown","d3de5e26":"markdown","62e0b7fc":"markdown","542953bf":"markdown","b4379552":"markdown","297e2f54":"markdown","20a6910d":"markdown","1c655909":"markdown","3e768985":"markdown","92cb63cb":"markdown","6450de79":"markdown"},"source":{"69265441":"# For processing the data\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Visualization tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"white\") # set style for seaborn plots\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Ignore warnings\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Time-related functions\nimport time","26d47923":"data = pd.read_csv(\"..\/input\/new-york-city-taxi-fare-prediction\/train.csv\", nrows = 7000000)","d8ef4922":"print(\"Dimensions of our training set: \", data.shape)\ndata.dtypes","795ffa63":"data.head()","cd101c25":"f, ax = plt.subplots(1, 1, figsize=(8,5))\nsns.distplot(data[\"fare_amount\"], kde=True, color=\"#fdb813\")\nplt.xlim(0, 700)\nplt.ylim(0, 0.08)\nplt.title(\"Distribution of the target: fare_amount\")\nplt.xlabel(\"Frequency\")\nplt.show()","89bd9ed3":"q1  = data['fare_amount'].quantile(0.25)\nq3  = data['fare_amount'].quantile(0.75)\niqr =  q3 - q1\nprint(\"Fare Amount lower bound : \", q1 - (1.5 * iqr), \n      \"Fare Amount upper bound : \", q3 + (1.5 * iqr))","129c77ef":"print(\"Total null values:\\n\", data.isnull().sum())\nprint(\"Percentage of null values:\\n\",\n      data[[\"dropoff_longitude\", \"dropoff_latitude\"]].isnull().sum() \/ data.shape[0])","3ec26c97":"data.dropna(how='any', axis='rows', inplace=True)","13e24199":"f, ax = plt.subplots(1, 2, figsize=(16, 5))\nsns.scatterplot(x=\"pickup_longitude\", y=\"pickup_latitude\", data=data.iloc[:10000], \n                color=\"#fdb813\", ax=ax[0])\nsns.scatterplot(x=\"dropoff_longitude\", y=\"dropoff_latitude\", data=data.iloc[:10000], \n                color=\"#fdb813\", ax=ax[1])\nax[0].set_title(\"Pickup Coordinates\")\nax[1].set_title(\"Dropoff Coordinates\")\nplt.show()","b288fab3":"data.describe()","05742d5e":"def get_cleaned(df):\n    return df[(df.fare_amount > 0) &\n              (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n              (df.pickup_longitude > -80) & (df.pickup_longitude < -68) &\n              (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n              (df.pickup_longitude > -80) & (df.dropoff_longitude < -68) &\n              (df.passenger_count > 0) & (df.passenger_count < 8)]\n\ndata = get_cleaned(data)\nprint(len(data))\nprint(\"Data lost after the cleaning process: \", 7000000 - len(data))","5c4a392e":"def sphere_dist(pick_lat, pick_lon, drop_lat, drop_lon):\n    R_earth = 6371 # Earth radius (in km)\n    # Convert degrees to radians\n    pick_lat, pick_lon, drop_lat, drop_lon = map(np.radians, [pick_lat, pick_lon,\n                                                              drop_lat, drop_lon])\n    # Compute distances along lat, lon dimensions\n    dlat = drop_lat - pick_lat\n    dlon = drop_lon - pick_lon\n    \n    # Compute haversine distance\n    a = np.sin(dlat\/2.0)**2 + np.cos(pick_lat) * np.cos(drop_lat) * np.sin(dlon\/2.0)**2\n    return 2 * R_earth * np.arcsin(np.sqrt(a))","a05e8c9f":"def airport_dist(df):\n    \"\"\"\n    JFK: John F. Kennedy International Airport\n    EWR: Newark Liberty International Airport\n    LGA: LaGuardia Airport\n    \"\"\"\n    jfk_coord = (40.639722, -73.778889)\n    ewr_coord = (40.6925, -74.168611)\n    lga_coord = (40.77725, -73.872611)\n    \n    pick_lat = df['pickup_latitude']\n    pick_lon = df['pickup_longitude']\n    drop_lat = df['dropoff_latitude']\n    drop_lon = df['dropoff_longitude']\n    \n    pickup_jfk = sphere_dist(pick_lat, pick_lon, jfk_coord[0], jfk_coord[1])\n    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], drop_lat, drop_lon) \n    pickup_ewr = sphere_dist(pick_lat, pick_lon, ewr_coord[0], ewr_coord[1])\n    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], drop_lat, drop_lon) \n    pickup_lga = sphere_dist(pick_lat, pick_lon, lga_coord[0], lga_coord[1]) \n    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], drop_lat, drop_lon)\n    \n    df['jfk_dist'] = pd.concat([pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n    df['ewr_dist'] = pd.concat([pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n    df['lga_dist'] = pd.concat([pickup_lga, dropoff_lga], axis=1).min(axis=1)\n    \n    return df","90893dd7":"def datetime_info(df):\n    #Convert to datetime format\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    df['hour'] = df.pickup_datetime.dt.hour\n    df['day'] = df.pickup_datetime.dt.day\n    df['month'] = df.pickup_datetime.dt.month\n    df['weekday'] = df.pickup_datetime.dt.weekday\n    df['year'] = df.pickup_datetime.dt.year\n    \n    return df\n\n\ndata = datetime_info(data)\ndata = airport_dist(data)\ndata['distance'] = sphere_dist(data['pickup_latitude'], data['pickup_longitude'], \n                               data['dropoff_latitude'], data['dropoff_longitude'])\n\ndata.head()","b036131e":"plt.figure(figsize=(10,5))\nsns.lineplot(x=\"year\", y=\"fare_amount\", data=data, color=\"#fdb813\")\nplt.title(\"Fare among years\")\nplt.show()","70f630f1":"f, ax = plt.subplots(1, 2, figsize=(12,5))\nax[0].hist(data[\"passenger_count\"], bins=7, color=(\"#fdb813\"))\nax[0].set_title(\"Number of passengers frequency\")\nax[0].set_xlabel('No. of Passengers')\nax[0].set_ylabel('Frequency')\n\nax[1].scatter(x=data[\"passenger_count\"], y=data[\"fare_amount\"], s=1.5, \n              color=(\"#3D2C05\"))\nax[1].set_title(\"Fare amount by number of passengers\")\nax[1].set_xlabel('No. of Passengers')\nax[1].set_ylabel('Fare');","8fee8c93":"f, ax = plt.subplots(1, 3, figsize=(16,5))\nax[0].hist(data[\"hour\"], bins=24, color=\"#fdb813\")\nax[0].set_title(\"Frequency of rides by Hour of the day\")\nax[0].set_xlabel('Hour of the day')\nax[0].set_ylabel('Frequency')\n\nax[1].scatter(x=data[\"hour\"], y=data[\"fare_amount\"], s=1.5, c=\"#3D2C05\")\nax[1].set_title(\"Fares by Hour of the day\")\nax[1].set_xlabel('Hour of the day')\nax[1].set_ylabel('Fare')\n\nsns.barplot(x=\"hour\", y=\"fare_amount\", data=data, ax=ax[2], color=\"#fdb813\")\nax[2].set_title(\"Mean Fares by Hour of the day\")\nax[2].set_xlabel('Hour of the day')\nax[2].set_ylabel('Mean fare')\nplt.show()","9eb2fc44":"plt.figure(figsize=(15,7))\nsns.barplot(x='weekday', y=\"fare_amount\", data=data, palette=(\"#fdb813\", \"#3D2C05\"))\nplt.ylim(0, 14)\nplt.title(\"Mean Fares among Days of the week\")\nplt.xlabel('Day of Week')\nplt.ylabel('Mean fare')\nplt.show()","0e6b9533":"f, ax = plt.subplots(1, 2, figsize=(16, 5))\nsns.regplot(x=\"distance\", y=\"fare_amount\", data=data, color=\"#fdb813\", ax=ax[0])\nsns.regplot(x=\"distance\", y=\"fare_amount\", data=data, color=\"#fdb813\", ax=ax[1])\nax[1].set_xlim(0, 1000)\nax[1].set_ylim(0, 300)\nplt.title(\"Positive relation between distance and fare\")\nplt.show()","0a081f0a":"f = plt.figure(figsize=(14, 8))\nsns.heatmap(data.corr(), annot=True, linewidths=0.2, cmap=\"viridis\")\nplt.title(\"Correlation Heatmap\")\nplt.show()","2ad26c95":"dropoff_longitude = data['dropoff_longitude'].to_numpy()\ndropoff_latitude = data['dropoff_latitude'].to_numpy()\n\nplt.figure(figsize=(12,8))\nplt.scatter(dropoff_longitude, dropoff_latitude,\n                color=\"#fdb813\", \n                s=.02, alpha=.2)\nplt.title(\"Dropoffs through the city\")\n# Borders of the city\nplt.xlim(-74.03, -73.75)\nplt.ylim(40.63, 40.85)\nplt.show()","eb4ac175":"data.drop(columns=[\"key\", \"pickup_datetime\"], inplace=True)\ndata.head()","ae2a48f9":"y = data[\"fare_amount\"]\ntrain = data.drop(columns=[\"fare_amount\"])\n\nx_train, x_test, y_train, y_test = train_test_split(train, y, random_state=2666, test_size=0.05)","3e968215":"params = {\n    \"max_depth\": 7,\n    \"subsample\": 0.9,\n    \"eta\": 0.03,\n    \"colsample_bytree\": 0.9,\n    \"random_state\": 2666,\n    \"objective\": \"reg:linear\",\n    \"eval_metric\": \"rmse\",\n    \"silent\": 1\n}","65b2ac60":"def XGBmodel(x_train, x_test, y_train, y_test, params):\n    matrix_train = xgb.DMatrix(x_train, label=y_train)\n    matrix_test = xgb.DMatrix(x_test, label=y_test)\n    model = xgb.train(params=params,\n                      dtrain=matrix_train,num_boost_round=5000, \n                      early_stopping_rounds=10,evals=[(matrix_test,'test')])\n    return model\n\nstart_time = time.time()\nmodel = XGBmodel(x_train, x_test, y_train, y_test, params)","58bd26da":"time_taken = time.time() - start_time\ntime_taken","d4a11e82":"test =  pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv')\ntest = datetime_info(test)\ntest = airport_dist(test)\ntest['distance'] = sphere_dist(test['pickup_latitude'], test['pickup_longitude'], \n                               test['dropoff_latitude'] , test['dropoff_longitude'])\ntest_key = test['key']\nx_pred = test.drop(columns=['key', 'pickup_datetime']) \n\n#Predict from test set\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit=model.best_ntree_limit)","17e00a6c":"#Create submission file\nsubmission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission.head()","4da1036d":"### <a id=\"section4\">4. Dataset Cleaning<\/a>","f3c1a649":"# <center> New York Taxi Fares Prediction <\/center>\n## <center> Supervised Learning with XGBoost <\/center>","6f099c8f":"If we focus our attention on the time of the day, it seems that the fares are higher between 22 and 5h., and 14 to 16h.","3db1cf89":"Latitude and longitude coordinates of New York City are around the values 40.730610, and -73.935242 respectively. But the values of our set are much different from the actual NY latitude and longitude. It can be true that some taxis have gone outside the city to dropoff some commuters, but it is deceptive that they could go that far and that many times. What should we believe? That those observations with 0 latitude values went to pickup or dropoff the passenger to the Earth's Equator?\n\nWe will remove points, not near these coordinates in the future. ","78f2e58f":"As suspected, the distance feature is highly correlated with `fare_amount`. Its importance in the prediction will be capital.","e64c0ec3":"Here we have the distribution of our dependent variable:","377bbb13":"With the descriptive statistics of the data, we find some interesting yet problematic insights: \n\n- Some `fare_amount` values are negative. Or the taxi driver is a modern Robin Hood, or they are errors that should be removed. \n\n- On the other side, the `passenger_count` has also unrealistic quantities, ranging from 0 to 200. Maybe I miss something but the maximum number of passengers here in the European Union, which I know better, are 7 for the biggest cars: 2 rows of 3 back seats, and the two pilot and co-driver front seats.\n\nSo we will deal with these problems in the next section.","6dbedddf":"> ","399320ed":"We won't remove anything for now, but it will be positive to know if whether or not we have outliers in our target.","1536acf9":"#### 5.2 Distance to the city airports\n\nTrips from or to the airports of New York have a fixed price, so it would be nice if we state this fact.","b85a923c":"### <a id=\"section5\">5. Feature engineering<\/a>\n\nNow that we have cleaned our data, we will follow by adding some interesting features. These new variables ideas have been drawn from all the other notebooks cited above.","8c19d041":"#### 5.1 Distance measurement from pickup to dropoff\n\nThe haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. Let us calculate it for each observation, in other words, let us calculate the distance along great radius between pickup and dropoff coordinates for each individual ride.","89a01f7a":"Now we need to drop the columns that we will not use to train our model.\n- `key`: Independent variable with no information at all for the fare. Its function was merely for identification purposes.\n- `pickup_datetime`: We divided this variable into multiple ones. Once done this, it is detrimental to keep it in the set, we would be counting date-related information twice.","442de6b2":"#### 5.3 Information from datetime (day of the week, month, hour, day). \n\nTaxi fares change day\/night or on weekdays\/holidays in most of the cities.","887c552c":"## Index\n\n[The data](#section0)\n\n1. [Load the libraries](#section1)\n2. [Load the dataset](#section2)\n3. [Basic exploration](#section3)\n4. [Dataset Cleaning](#section4)\n5. [Feature engineering](#section5)\n6. [Further EDA](#section6)\n7. [Model training](#section7)\n8. [Predictions](#section8)","6000a04b":"This took ages... but at least we have a pretty good model. Let us finally make the predictions of the test set and prepare the submission file.","70f13b17":"Through tunning with CV we know the optimal parameters that we record in the next dictionary for training the model in the future:","9fb33e7b":"The data is from a Kaggle's Playground Prediction Competition, it can be found [here](https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\/data).\n\nThe main signature of this dataset is its massive size, counting with 55 million observations in its train set. Therefore, it is vital to handle this in some way, in order to build a decent model and don't convert your computer into a toaster at the same time. Particularly we will subset the set with 7 million rows, an adequate and more than enough amount of data to get a good performance.\n\nApart from this, the data is extremely cleaned and simple in their dimensions, having only 6 features, an id column, and the dependent variable of the fare amount. These are the attributes:\n\n    pickup_datetime - timestamp value indicating when the taxi ride started. \n    pickup_longitude - float for longitude coordinate of where the taxi ride started.\n    pickup_latitude - float for latitude coordinate of where the taxi ride started.\n    dropoff_longitude - float for longitude coordinate of where the taxi ride ended.\n    dropoff_latitude - float for latitude coordinate of where the taxi ride ended.\n    passenger_count - integer indicating the number of passengers in the taxi ride. ","a943d78f":"![NY taxis](https:\/\/assets.bwbx.io\/images\/users\/iqjWHBFdfxIU\/iaFilM3php7g\/v1\/-1x-1.jpg)","2e5842e2":"Fares have steadily increased over the years. This is important information that our model should take into account.","fe910e06":"### <a id=\"section6\">6. Further EDA<\/a>\n\nNow that we have our brand new features let us continue with further exploration of the set.","fffc89ca":"Here we have the first 5 rows of our data. With them, we can become familiar with all the features,  their values, and have a solid representation of how our set looks like.","8d77a761":"### <a id=\"section2\">2. Load the dataset<\/a>\n\nWe will load our train dataset from the downloaded csv file. Since, as stated, this dataset has 55 million rows, we will set the `nrows` parameter to 7M to prevent memory issues and speed up everything. Considering there is no order among the observations there is no need to randomize this selection of rows.\n\nFeel free to adapt the `nrows` in accordance with your computer capabilities or the performance of the model you want to attain, if you only want to read and learn something from the notebook you can low it down to 1M for example.","1a6a84fc":"The highest fares seem to be on a Sunday. But the differences are minimal, and I would say that they are even non-significative.","c19f4bf3":"## Introduction\n\nHi Kagglers,\n\nHere I come with another notebook. This time trying to predict the New York taxi rides fares, given the pickup, dropoff locations, and some other features. We will stick with the XGBoost algorithm, which gives very good predictions for this specific data after we perform some interesting feature engineering and exploration of the dataset. Check it out!\n\nTaxicabs are, and always will be, an iconic part of New York. The history of their characteristic yellow color goes as follows: owners of cab companies painted their fleets a distinct signature color, resulting in cabs ranging from brown, white, red, and even checker ones. And some were yellow. After a few years, two big cab companies decided that yellow was the way to go, with both ultimately contributing to the tradition of yellow cabs in New York City. These companies were the Yellow Cab Company, started by John Hertz in Chicago in 1910, and the Yellow Taxicab Company, which was incorporated in New York by Albert Rockwell in 1912. More information on this can be found [here](https:\/\/untappedcities.com\/2017\/07\/12\/nyc-fun-facts-why-are-most-nyc-taxi-cabs-yellow\/).\n\nMy notebook has been inspired by others on the same topic, which I enlist hereunder. My recognition to them and recommendation to take them a look:\n\n- Ravi tanwar's Data Cleaning + Eda + Modelling: https:\/\/www.kaggle.com\/ravijoe\/data-cleaning-eda-modelling\n\n- Jes\u00fas Ros' XGBoost'ing Taxi Fares: https:\/\/www.kaggle.com\/gunbl4d3\/xgboost-ing-taxi-fares\n\n- Vinod R's EDA + XGBoost For Predicting Fare Amount: https:\/\/www.kaggle.com\/vinodsunny1\/eda-xgboost-for-predicting-fare-amount\n\n- AlexS2020's Rapdis and XG Boost running on GPU: https:\/\/www.kaggle.com\/alexs2020\/rapdis-and-xg-boost-running-on-gpu\n\n- Nicapotato's Taxi Rides Time Analysis and OOF LGBM: https:\/\/www.kaggle.com\/nicapotato\/taxi-rides-time-analysis-and-oof-lgbm\n\nRemember to upvote if you really enjoyed it. And feel free to comment, suggest or even complain in the comment section. Check my other notebooks, which are also great.\n\nCheers!","d3de5e26":"We have lost 169.718 observations due to incongruous data, a fairly high number but with minimum impact considering our 6.830.282 rows dataset.","62e0b7fc":"Having solved the missing values issue, let us look at how the coordinates are distributed in a scatter plot. Given that the taxi rides are placed in New York, they should be clustered around specific longitude and latitude values. We will only plot the first rows which should be sufficient to show what we want.","542953bf":"### <a id=\"section1\">1. Load the libraries<\/a>","b4379552":"From the above graphs, we can see that single passengers are by far the most frequent, and the highest fare also seems to come from cabs which carry just one commuter.","297e2f54":"## <a id=secion0>The data<\/a>","20a6910d":"We have very few rows with null values, only 47 of the 7M, less than a 0.0007%. With this in mind, it is safe to just remove them. We can be sure that we won't lose any valuable information from them.","1c655909":"Does the distance affect the fare? This is a no-brainer. I am confident enough to bet good money that the distance would affect the fare by a great deal. Let us visualize this phenomenon:","3e768985":"### <a id=\"section3\">3. Basic exploration<\/a>\n\nNow we will explore the loaded data and see whether we find problems that require some sort of fixing, such as missing values, imbalanced data, inconsistent observations, outliers, etc.","92cb63cb":"### <a id=\"section7\">7. Model training<\/a>\n\nNow that we have the dataframe that we wanted we can start to train the XGBoost model. First, we will split the dataset into train (95%) and test (5%). With this amount of data 10% should be enough to test performance.","6450de79":"### <a id=\"section8\">8. Predictions<\/a>"}}