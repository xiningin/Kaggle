{"cell_type":{"e3a5c802":"code","0eeceb88":"code","daec105c":"code","16dba70c":"code","8d82c1e1":"code","4c972ca9":"code","529aae3b":"code","f34cebd4":"code","99ed66c6":"code","ff44b8a9":"code","5939d5ad":"code","fe1854b7":"code","f33e705b":"code","73eb932e":"code","d00debc0":"code","bbc0faf3":"code","d992e7c5":"code","84154ff0":"code","18601996":"code","a91d84f4":"code","e996575e":"code","f8123f49":"code","61efe685":"code","6869fab1":"code","4db394df":"code","8a5fb016":"markdown","54609c3d":"markdown","c867b640":"markdown","178670e3":"markdown","bd14c4ad":"markdown","9bfc9ead":"markdown","cfe6b837":"markdown","08c3ee7f":"markdown","12f93cfc":"markdown","e0cf1595":"markdown","9327a41a":"markdown","8b366626":"markdown"},"source":{"e3a5c802":"import numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n# from sklearn.impute import SimpleImputer     # verified that there're no null values accordingly we don't need SimpleImputer\n\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, balanced_accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom tqdm import tqdm\n\n\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use('fivethirtyeight')\nsns.set(font_scale = 1)  \npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\n\nprint(\"Let's start!\")","0eeceb88":"trn_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv', index_col='Id')\ntst_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv', index_col='Id')\n\ndisplay(trn_df.shape, trn_df.head(1), tst_df.shape, tst_df.head(1))","daec105c":"trn_info = pd.DataFrame(columns=['Name of Col', 'Num of Null', 'Dtype', 'N_unique'])\n\nfor i in range(0, len(trn_df.columns)):\n    trn_info.loc[i] = [trn_df.columns[i],\n                       trn_df[trn_df.columns[i]].isnull().sum(),\n                       trn_df[trn_df.columns[i]].dtypes,\n                       trn_df[trn_df.columns[i]].nunique()]\n    \ntrn_info","16dba70c":"# Target values distribution\n\nplt.figure(figsize=(8, 5))\nax = sns.countplot(trn_df['Cover_Type'])\nax.set_title('Distribution of Cover_Type')\nax.bar_label(ax.containers[0])\nplt.show()","8d82c1e1":"features = trn_df.columns[:-1]\ntarget = trn_df.columns[-1]","4c972ca9":"# Thanks to https:\/\/www.kaggle.com\/maximkazantsev\/tps-11-21-eda-xgboost-optuna#Data-preprocessing\n\ndf = pd.concat([trn_df[features], tst_df[features]], axis=0)\ncolumns = df.columns.values\ncols = 5\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,62), sharex=False)\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(trn_df[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(tst_df[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i\/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            if i == 0:\n                axs[r, c].legend(fontsize=10)\n                                  \n        i+=1\n\nplt.show();","529aae3b":"# trn['sum'] = trn[features].sum(axis=1)\n# tst['sum']=tst[features].sum(axis=1)\n\n# trn['mean']=trn[features].mean(axis=1)\n# tst['mean']=tst[features].mean(axis=1)\n\n# trn['median']=trn[features].median(axis=1)\n# tst['median']=tst[features].median(axis=1)\n\n# trn['std'] = trn[features].std(axis=1)\n# tst['std'] = tst[features].std(axis=1)\n\n# trn['max'] = trn[features].max(axis=1)\n# tst['max'] = tst[features].max(axis=1)\n\n# trn['min'] = trn[features].min(axis=1)\n# tst['min'] = tst[features].min(axis=1)\n\n# trn['kurt'] = trn[features].kurtosis(axis=1)\n# tst['kurt'] = tst[features].kurtosis(axis=1)\n\n# agg_features= ['sum','mean', 'median', 'std','max','min','kurt']","f34cebd4":"# features = list(features)\n# features.extend(agg_features)","99ed66c6":"# dropping the useless features and data containing Cover_Type == 5\n\ntrn_df.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)\ntst_df.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)\ntrn_df = trn_df.loc[trn_df['Cover_Type'] != 5].reset_index(drop=True)","ff44b8a9":"features = trn_df.columns[:-1]\ntarget = trn_df.columns[-1]","5939d5ad":"trn = trn_df.copy()\ntst = tst_df.copy()","fe1854b7":"num_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n                'Horizontal_Distance_To_Fire_Points']\n\n","f33e705b":"# scaling\n\nss = StandardScaler()\n\nfor col in num_features:\n    trn[col] = ss.fit_transform(trn[[col]])\n    tst[col] = ss.fit_transform(tst[[col]])\n    \ndisplay(trn.head(3), tst.head(3))","73eb932e":"# train_test_split\n\nX_trn = trn[features]\ny_trn = trn[target]\nX_tst = tst[features]\n\ndisplay(X_trn.shape, y_trn.shape, X_tst.shape)","d00debc0":"# k-fold cross validation, trial with the default parameter.\n\n\nRANDOM_SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\npreds_lgb = []\nmean_acc = 0\n\nmodel_lgb = LGBMClassifier(objective='multiclass', random_state = RANDOM_SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_lgb.fit(X_train, y_train,\n                  verbose = False,\n                  eval_set = [(X_train, y_train), (X_val, y_val)],\n                  eval_metric = 'multi_logloss',\n                  early_stopping_rounds = 100)\n    \n    y_pred = model_lgb.predict(X_val)\n    score = accuracy_score(y_val, y_pred)\n    mean_acc += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n        \n    preds_lgb.append(model_lgb.predict(X_tst))\n\nprint(\"==========================================\")\nprint(f\"Mean auc of all folds: {mean_acc \/ n_splits}\")","bbc0faf3":"# k-fold cross validation, trial with the default parameter.\n\n\nRANDOM_SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\npreds_xgb = []\nmean_acc = 0\n\nmodel_xgb = XGBClassifier(random_state = RANDOM_SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_xgb.fit(X_train, y_train,\n                  verbose = False,\n                  eval_set = [(X_train, y_train), (X_val, y_val)],\n                  eval_metric = 'mlogloss',\n                  early_stopping_rounds = 100)\n    \n    y_pred = model_xgb.predict(X_val)\n    score = accuracy_score(y_val, y_pred)\n    mean_acc += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n        \n    preds_xgb.append(model_xgb.predict(X_tst))\n\nprint(\"==========================================\")\nprint(f\"Mean auc of all folds: {mean_acc \/ n_splits}\")","d992e7c5":"# k-fold cross validation, trial with the default parameter.\n\n\nRANDOM_SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\npreds_rf = []\nmean_acc = 0\n\nmodel_rf = RandomForestClassifier(random_state = RANDOM_SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_rf.fit(X_train, y_train)\n    \n    y_pred = model_rf.predict(X_val)\n    score = accuracy_score(y_val, y_pred)\n    mean_acc += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n        \n    preds_rf.append(model_rf.predict(X_tst))\n\nprint(\"==========================================\")\nprint(f\"Mean auc of all folds: {mean_acc \/ n_splits}\")","84154ff0":"# k-fold cross validation, trial with the default parameter.\n\n\nRANDOM_SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\npreds_et = []\nmean_acc = 0\n\nmodel_et = ExtraTreesClassifier(random_state = RANDOM_SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_et.fit(X_train, y_train)\n    \n    y_pred = model_et.predict(X_val)\n    score = accuracy_score(y_val, y_pred)\n    mean_acc += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n        \n    preds_et.append(model_et.predict(X_tst))\n\nprint(\"==========================================\")\nprint(f\"Mean auc of all folds: {mean_acc \/ n_splits}\")","18601996":"# # HPO using opuna\n\n# def lgb_objective(trial):\n#     params = {\n#         'boosting_type': 'gbdt',\n#         'objective': 'multiclass',\n#         'n_estimators': trial.suggest_int(\"n_estimators\", 64, 8192, 100),\n#         'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n#         'num_leaves': trial.suggest_int(\"num_leaves\", 20, 3000, 20),\n#         'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n#         'feature_fraction': trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step = 0.1),\n#         'min_gain_to_split' : trial.suggest_int('min_gain_to_split', 0, 15),\n#         'min_data_in_leaf' : trial.suggest_int(\"min_data_in_leaf\", 200, 10000, 100),\n#         'lambda_l1': trial.suggest_int(\"lambda_l1\", 0, 100, 5),\n#         'lambda_l2': trial.suggest_int(\"lambda_l2\", 0, 100, 5),\n#         'bagging_fraction' : trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n#         'bagging_freq' : trial.suggest_categorical(\"bagging_freq\", [1]),\n#         'seed': 42,\n#         'metric' : 'multi_logloss',\n#         'verbose':-1\n#     }\n    \n#     X_train, X_val, y_train, y_val = train_test_split(X_trn, y_trn, stratify = y_trn, test_size = 0.3, random_state = 42)\n    \n#     model_lgb = LGBMClassifier(**params)\n#     model_lgb.fit(X_train, y_train,\n#              eval_set = [(X_train, y_train), (X_val, y_val)],\n#              early_stopping_rounds = 100,\n#              eval_metric = 'multi_logloss',\n#              verbose = False\n#              )\n#     pred_val = model_lgb.predict(X_val)\n    \n#     return accuracy_score(y_val, pred_val)","a91d84f4":"# sampler = TPESampler(seed = 42)\n# study = optuna.create_study(study_name = 'lgbm_hpo',\n#                            direction = 'maximize',\n#                            sampler = sampler)\n# study.optimize(lgb_objective, n_trials = 10, show_progress_bar=True)\n\n# print(\"Best ACC:\", study.best_value)\n# print(\"Best params:\", study.best_params)","e996575e":"# params = study.best_params","f8123f49":"# params_lgb = {'boosting_type': 'gbdt',\n#           'objective': 'multiclass',\n#           'n_estimators': 7276, \n#           'learning_rate': 0.013562603384785458,\n#           'num_leaves': 376,\n#           'max_depth': 10,\n#           'feature_fraction': 0.7847065437552077,\n#           'min_gain_to_split': 8,\n#           'min_data_in_leaf': 794,\n#           'lambda_l1': 0.0008668739724852811,\n#           'lambda_l2': 0.0016878284140548435,\n#           'bagging_fraction': 0.3420328146868397,\n#           'bagging_freq': 3,\n#           'seed': 42,\n#           'metric' : 'multi_logloss',\n#           'verbose':-1\n#          }","61efe685":"# # k-fold cross validation, trial with the tuned parameter.\n\n\n# RANDOM_SEED = 42\n# n_splits = 5\n# skf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\n# preds_lgb = []\n# mean_acc = 0\n\n# model_lgb = LGBMClassifier(objective='multiclass', **params_lgb)\n\n# for fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n#     X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n#     y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n#     model_lgb.fit(X_train, y_train,\n#                   verbose = False,\n#                   eval_set = [(X_train, y_train), (X_val, y_val)],\n#                   eval_metric = 'multi_logloss',\n#                   early_stopping_rounds = 100)\n    \n#     y_pred = model_lgb.predict(X_val)\n#     score = accuracy_score(y_val, y_pred)\n#     mean_acc += score\n    \n#     print(f\"Fold {fold}'s score: {score:.4f}\")\n        \n#     preds_lgb.append(model_lgb.predict(X_tst))\n\n# print(\"==========================================\")\n# print(f\"Mean auc of all folds: {mean_acc \/ n_splits}\")","6869fab1":"final_preds = np.mean(preds_lgb, axis = 0).astype('int64')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsubmission['Cover_Type'] = final_preds\nsubmission","4db394df":"submission.to_csv('submission.csv', index = False)","8a5fb016":"<br>\n\n## Data Load\n\n<br>","54609c3d":"This data is too large to exlore the data information using `.info()` method.\n\nIn this case it will be helpful to create the data frame for the data information.","c867b640":"<br>\n\n### Numeric Variables\n\n* The form of distribution is quite similar between train and test dataset.\n\n\n* As we noticed it does not seem to matter if we drop `Soil_Type7` and `Soil_Type15`. ","178670e3":"<br>\n\n### Target Variable\n\n* `Cover_Type` is consist of 7 unique values. Let's look at its distribution.\n\n\n* If the version of your matplotlib is up to date, you can use `ax.bar_label(ax.containers[0])` to show the counted values within the countplot.\n\n\n* By the way, this data is quite unbalanced.\n    * Most of Cover_Type are distributed on 1, 2\n    * Rarely 4, 5: especially we can drop the data labled 5 because there's only 1 data for it.","bd14c4ad":"<br>\n\n## EDA\n\n<br>","9bfc9ead":"### Data Information","cfe6b837":"## Import Libraries","08c3ee7f":"<br><br>\n\n<img\n     src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25225\/logos\/header.png?t=2021-01-27-17-34-26\">\n     \n\n<br>\n<p style=\"background-color:white;font-family:Tahoma;color:dark gray;font-size:100%;text-align:left;border-radius:10px 0px;\">#Kaggle  #TabularPlayGround  #LightGBM #XGBoost #Ensemble #Multiclass","12f93cfc":"<br>\n\n## Feature Engineering\n\n* Newly created num_features to make numerical features standardized.","e0cf1595":"<br>\n\n## Model\n\n* Generated base LGBM classifier to check the performance of the given features.","9327a41a":"<br>\n\nFor this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\n<br>","8b366626":"<br>\n\n* Number of Null data: None\n\n\n* Data types: All variables have the same data type, int64.\n\n\n* Number of Unique value:  \n    * From `Elevation` to `Horizontal_Distance_To_Fire_Points`: There're so many number of unique value. It means that these columns are the numerical variables.\n    * From `Wilderness_Area1` to `Soil_Type40`: each has only two kinds of values accordingly they will be the boolean variables. But there's only one value in `Soil_Type7` and `Soil_Type15` so that we need to check and decide whether will drop it or not.\n\n<br>"}}