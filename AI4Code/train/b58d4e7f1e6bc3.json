{"cell_type":{"c3d14996":"code","ddb0dd92":"code","728f95a5":"code","e5f0dfba":"code","c6da7c8e":"code","5192bf70":"code","e517b805":"code","65c71980":"code","cec4eef0":"code","baa1ce2c":"code","447539b9":"code","0741ee51":"code","7c7692c3":"code","c9c85b63":"code","be0200fc":"code","657231dc":"code","d222cb2f":"code","5b8e79e3":"code","03b192c5":"code","d51c4504":"code","ed820aba":"code","74794614":"code","cb8cc480":"code","7f6ee81e":"code","1b2508a8":"code","013cf383":"code","18d49975":"code","8a32878e":"code","01b77799":"code","94dc248d":"code","ce2e09e5":"code","cdeaf464":"code","097baad4":"code","4fa021ad":"code","af5f1d71":"code","cd84dd68":"code","36a0700b":"code","97e5a946":"code","57f6fd73":"markdown","5ef45d56":"markdown","ece20792":"markdown","6648fd69":"markdown","b2a669bf":"markdown","caa8e33a":"markdown","dcf2b7a5":"markdown","8ad5927c":"markdown","757f82be":"markdown","c7529da7":"markdown","6c9305ac":"markdown","ed336b2c":"markdown","32ed6dde":"markdown","3946b062":"markdown","a34ec5ab":"markdown","976c8acd":"markdown","347d8557":"markdown","7d43a9cd":"markdown","c3321249":"markdown","3e42a2bd":"markdown","bc8d5bfb":"markdown","b3a38bcb":"markdown","006775f6":"markdown","d230fcbc":"markdown","a673e5eb":"markdown","9f621db6":"markdown","59dfe4eb":"markdown","ecdd38c5":"markdown","75a5580d":"markdown","2536d4a3":"markdown","f77cb63c":"markdown"},"source":{"c3d14996":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ddb0dd92":"dataset = pd.read_csv('..\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\ndataset.head()","728f95a5":"def missing_data(dataset):\n    missing_count = dataset.isnull().sum()\n    missing_percentage = dataset.isnull().sum() \/ dataset.count() * 100\n    missing_table = pd.concat([missing_count, missing_percentage], axis = 1, keys = ['No_missing_data', 'Percentege_missing_data'])\n    return(np.transpose(missing_table))\nmissing_data(dataset)","e5f0dfba":"dataset = dataset.dropna()","c6da7c8e":"dataset.describe()","5192bf70":"# Percentage based on the type of webpage\ndf_administrative = dataset[dataset.Administrative_Duration > 0]\ndf_informational = dataset[dataset.Informational_Duration > 0]\ndf_ProductRelated_Duration = dataset[dataset.ProductRelated_Duration > 0]\ndf_admin_info = dataset[(dataset.Administrative_Duration > 0) & (dataset.Informational_Duration > 0)]\ndf_admin_product = dataset[(dataset.Administrative_Duration > 0) & (dataset.ProductRelated_Duration > 0)]\ndf_info_product = dataset[(dataset.Informational_Duration > 0) & (dataset.ProductRelated_Duration > 0)]\ndf_admin_info_product = dataset[(dataset.Administrative_Duration > 0) & (dataset.Informational_Duration > 0) & (dataset.ProductRelated_Duration > 0)]\n\n\ndf_website_percentage = {}\ndf_website_percentage = pd.DataFrame(df_website_percentage)\n\ndf_website_percentage.loc['Website Usage','Admininstrative'] = df_administrative['Administrative_Duration'].count() \/ dataset['Administrative_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Admininstrative'] = df_administrative['Revenue'].sum()  \/  df_administrative['Administrative_Duration'].count() * 100\n\ndf_website_percentage.loc['Website Usage','Informational'] = df_informational['Informational_Duration'].count() \/ dataset['Informational_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Informational'] = df_informational['Revenue'].sum()  \/  df_informational['Informational_Duration'].count() * 100\n\ndf_website_percentage.loc['Website Usage','Product Related'] = df_ProductRelated_Duration['ProductRelated_Duration'].count() \/ dataset['ProductRelated_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Product Related'] = df_ProductRelated_Duration['Revenue'].sum()  \/  df_ProductRelated_Duration['ProductRelated_Duration'].count() * 100\n\ndf_website_percentage.loc['Website Usage','Admin & Info'] = df_admin_info['Administrative_Duration'].count() \/ dataset['Administrative_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Admin & Info'] = df_admin_info['Revenue'].sum()  \/  df_admin_info['ProductRelated_Duration'].count() * 100\n\ndf_website_percentage.loc['Website Usage','Admin & Product'] = df_admin_product['Administrative_Duration'].count() \/ dataset['Administrative_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Admin & Product'] = df_admin_product['Revenue'].sum()  \/  df_admin_product['Administrative_Duration'].count() * 100\n\ndf_website_percentage.loc['Website Usage','Info & Product'] = df_info_product['ProductRelated_Duration'].count() \/ dataset['Administrative_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Info & Product'] = df_info_product['Revenue'].sum()  \/  df_info_product['ProductRelated_Duration'].count() * 100\n\ndf_website_percentage.loc['Website Usage','Admin & Info & Product'] = df_admin_info_product['ProductRelated_Duration'].count() \/ dataset['ProductRelated_Duration'].count() * 100\ndf_website_percentage.loc['Revenue Rate','Admin & Info & Product'] = df_admin_info_product['Revenue'].sum()  \/  df_admin_info_product['ProductRelated_Duration'].count() * 100\n\n\n# Plot for website percentages baded on usage\nfig, ax1 = plt.subplots(figsize = (12,5))\nsns.barplot(x = df_website_percentage.columns, y = df_website_percentage.loc['Website Usage',:], ax = ax1)\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nax1.set_title('The Percentage of Website Usage Based on Type', fontsize = 16)\nax1.set_ylim(0,100)\nax1.tick_params(axis='x', rotation=30)\nax1.set(xlabel = 'The Website Types', ylabel = 'Percentage')\nplt.show()","e517b805":"no_column = 0\nfig, ax = plt.subplots(2,4, figsize= (12,5))\nplt.suptitle('The Percentage of Revenue Based On Website Type', fontsize = 16)\nfor i in range(2):\n    for k in range(4):\n        ax[i,k].pie([df_website_percentage.iloc[1,no_column], 100-df_website_percentage.iloc[1,no_column]], autopct='%1.1f%%', labels = ['True', 'False'] ,shadow=True, startangle=90 )\n        ax[i,k].axis('equal')\n        ax[i,k].set_title(df_website_percentage.columns[no_column])\n        no_column += 1\n        if no_column == 7:\n           fig.delaxes(ax.flatten()[7])\n           break\n\nplt.show()","65c71980":"df_website_percentage","cec4eef0":"fig, ax = plt.subplots(1,7, figsize = (20,4))\nplt.suptitle('The percentage of revenue based on website')\nfor i, col_title in enumerate(df_website_percentage):\n    ax[i].pie([df_website_percentage.loc['Revenue Rate',col_title], 100- df_website_percentage.loc['Revenue Rate',col_title]],autopct='%1.1f%%', labels = ['True', 'False'] ,shadow=True, startangle=90 )\n    ax[i].set_title(col_title)\n    centre_circle = plt.Circle((0,0),0.70,fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n    ax[i].axis('equal')\n    \nplt.show()","baa1ce2c":"    # The Revenue by Month\nsums_revenue = pd.DataFrame(dataset.groupby(['Month', 'Revenue'])['Revenue'].count().rename('Total')).sort_values(by = ['Total'], ascending = False)\nsums_revenue.reset_index(inplace = True)\nfig, ax = plt.subplots(figsize = (12,10))\nsns.barplot(x = sums_revenue.Month, y = sums_revenue.Total, hue = sums_revenue.Revenue)\nplt.title('The Revenue by Month', fontsize = 16 )\nplt.show()","447539b9":"fig, ax = plt.subplots(2,5, figsize = (18,5))\nno_month = 0\nplt.suptitle('The Percentage of Revenue by Month', fontsize = 16)\nfor i in range(2):\n    for k in range(5):\n        temp_percentage = sums_revenue[(sums_revenue['Month'] == sums_revenue['Month'].unique()[no_month] )&(sums_revenue['Revenue'] == True)]['Total'] \/ sums_revenue[sums_revenue['Month'] == sums_revenue['Month'].unique()[no_month]]['Total'].sum() * 100\n        temp_percentage = pd.Series([temp_percentage, 100- temp_percentage])\n        ax[i,k].pie(temp_percentage, autopct = '%.2f%%', labels = ['True', 'False'])\n        ax[i,k].set_title(sums_revenue['Month'].unique()[no_month])\n        ax[i,k].axis('equal')\n        no_month += 1","0741ee51":"pivot2 = dataset.pivot_table(index = ['Month','VisitorType'], values = ['BounceRates','ExitRates'] , aggfunc = np.mean)\npivot2.reset_index(inplace = True)\n        # Bar plot\nfig, ax = plt.subplots(figsize = (12,10))\nsns.barplot(x = pivot2['Month'], y = pivot2['ExitRates'], hue = pivot2['VisitorType'])\nplt.title('The Exit Rate based on Visitor Type by Month', fontsize = 16)\nplt.show()","7c7692c3":"for k in range(len(pivot2)):\n    if pivot2['Month'][k] == 'June':\n        pivot2['Month'][k] = 6\n    else:\n        pivot2['Month'][k]= datetime.strptime(pivot2['Month'][k], '%b').month\nfig, ax = plt.subplots(figsize = (12,10))\nsns.barplot(x = pivot2.Month, y = pivot2.ExitRates, hue = pivot2.VisitorType)\nplt.title('The Exit Rate based on Visitor Type by Month', fontsize = 16)\nplt.show()","c9c85b63":"VisitorType_groupby = pd.DataFrame(dataset.groupby(['VisitorType', 'Revenue'])['Revenue'].count().rename('Total'))\nVisitorType_groupby.reset_index(inplace = True)\nfig, ax = plt.subplots(figsize = (12,10))\nsns.barplot(x = VisitorType_groupby.VisitorType, y = VisitorType_groupby.Total, hue = VisitorType_groupby.Revenue)\nplt.ylabel('Number of Visitors')\nplt.title('The Number of Revenue by Visitor Type', fontsize = 16)\nplt.show()","be0200fc":"VisitorType_Revenue = {'New Visitor Revenue' : [],'Other Revenue' : [], 'Returning Visitor Revenue':[]}\nVisitorType_Revenue = pd.DataFrame(VisitorType_Revenue)\nVisitorType_Revenue['New Visitor Revenue'] = VisitorType_groupby[(VisitorType_groupby['VisitorType'] == 'New_Visitor') & ( VisitorType_groupby['Revenue'] == True)]['Total'] \/ VisitorType_groupby[VisitorType_groupby['VisitorType'] == 'New_Visitor']['Total'].sum()\nVisitorType_Revenue['Other Revenue'][1] =VisitorType_groupby[(VisitorType_groupby['VisitorType'] == 'Other') & (VisitorType_groupby['Revenue'] == True)]['Total'] \/ VisitorType_groupby[VisitorType_groupby['VisitorType'] == 'Other']['Total'].sum()\nVisitorType_Revenue['Returning Visitor Revenue'][1] = VisitorType_groupby[(VisitorType_groupby['VisitorType'] == 'Returning_Visitor') & (VisitorType_groupby['Revenue'] == True)]['Total'] \/ VisitorType_groupby[VisitorType_groupby['VisitorType'] == 'Returning_Visitor']['Total'].sum()\nVisitorType_Revenue.head()\n\nfig, ax = plt.subplots(1,3, figsize = (12,5))\nplt.suptitle('The Percentage of Revenue by Type of Visitor', fontsize = 18)\nfor i in range(3):\n    ax[i].pie([VisitorType_Revenue.iloc[0,i], 1-VisitorType_Revenue.iloc[0,i]], labels = ['True', 'False'], autopct = '%.2f%%' ,shadow=True, startangle=90)\n    ax[i].set_title(VisitorType_Revenue.columns[i])\n    ax[i].axis('equal')\nplt.show()","657231dc":"# Correlation Matrix\nimport seaborn as sns\ndataset_interval = dataset[['Administrative', 'Administrative_Duration', 'Informational',\n       'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n       'BounceRates', 'ExitRates', 'PageValues', 'Revenue']]\ncorrelation_matrix = dataset_interval.corr()\nfig, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(correlation_matrix, annot =True, annot_kws = {'size': 10})\nplt.xticks(rotation = 30)","d222cb2f":"sns.jointplot(data = dataset, x= 'ExitRates', y = 'BounceRates', kind = 'reg')\nplt.suptitle('Exit and Bounce Rate Correlation', fontsize = 16)\nplt.show()","5b8e79e3":"sns.jointplot(data = dataset, x = 'ProductRelated_Duration', y = 'ProductRelated',kind = 'reg' )\nsns.jointplot(data = dataset, x = 'Administrative_Duration', y = 'Administrative',kind = 'reg' )\nplt.show()","03b192c5":"dataset.drop(['Administrative','Informational','ProductRelated', 'ExitRates'], axis = 1, inplace = True)","d51c4504":"dataset = pd.get_dummies(dataset, columns = ['SpecialDay','Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'Weekend','VisitorType'])","ed820aba":"dataset.columns\nlen(dataset.columns)","74794614":"X = dataset.drop(['Revenue'], axis = 1).values\ny = dataset.loc[:,'Revenue'].values","cb8cc480":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)","7f6ee81e":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","1b2508a8":"from sklearn.linear_model import LogisticRegression\nclassifier_logistic = LogisticRegression(random_state = 0)\nclassifier_logistic.fit(X_train, y_train)\ny_pred_logistic = classifier_logistic.predict(X_test)\ny_pred_logistic_proba = classifier_logistic.predict_proba(X_test)","013cf383":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\naccuracy_logistic = accuracy_score(y_test, y_pred_logistic)\ncm_logistic = confusion_matrix(y_test, y_pred_logistic)\nsns.heatmap(cm_logistic, annot = True, fmt='d')\nprint('Logistic Regression accuracy is {:.4f}'.format(accuracy_logistic))","18d49975":"from sklearn.tree import DecisionTreeClassifier\nclassifier_tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3, min_samples_leaf = 5)\nclassifier_tree.fit(X_train, y_train)\ny_pred_tree = classifier_tree.predict(X_test)","8a32878e":"accuracy_tree = accuracy_score(y_test, y_pred_tree)\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot = True, fmt = 'd')\nprint('Desicion Tree accuracy is {:.4f}'.format(accuracy_tree))","01b77799":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, _ = roc_curve(y_test, y_pred_logistic_proba[:,1])\nplt.plot(fpr,tpr)\nplt.grid()\nplt.show()\nauc_logistic_proba = roc_auc_score(y_test,y_pred_logistic_proba[:,1])\nprint('Logistic regression prob auc score: {:.2f}'.format(auc_logistic_proba))\nauc_logistic = roc_auc_score(y_test,y_pred_logistic)\nprint('Logistic regression auc score: {:.2f}'.format(auc_logistic))","94dc248d":"from xgboost import XGBClassifier\nclassifier_xg = XGBClassifier()\nclassifier_xg.fit(X_train, y_train)\ny_pred_xg = classifier_xg.predict(X_test)","ce2e09e5":"accuracy_xg = accuracy_score(y_test, y_pred_xg)\nprint(\"XGBoost Classifier accuracy is : {:.4f}\".format(accuracy_xg))\ncm_xg = confusion_matrix(y_test, y_pred_xg)\nsns.heatmap(cm_xg, annot = True, annot_kws = {'size' : 10}, fmt = 'd')","cdeaf464":"from sklearn.metrics import classification_report, f1_score, roc_auc_score\nprint(classification_report(y_test, y_pred_logistic))\nprint(classification_report(y_test, y_pred_tree))\nprint(classification_report(y_test, y_pred_xg))","097baad4":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nclassifier_keras = Sequential()\nclassifier_keras.add(Dense(input_dim = 76,output_dim = 25, init = 'uniform', activation = 'relu' ))\nclassifier_keras.add(Dense(output_dim = 25, init = 'uniform', activation = 'relu'))\nclassifier_keras.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\nclassifier_keras.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier_keras.fit(X_train, y_train , epochs = 100)\ny_pred_keras = classifier_keras.predict(X_test)\ny_pred_keras = (y_pred_keras > 0.6)","4fa021ad":"y_pred_keras = classifier_keras.predict(X_test)\ny_pred_keras = (y_pred_keras > 0.5)\naccuracy_keras = accuracy_score(y_test, y_pred_keras)\nprint('Keras accuracy score is : {}'.format(accuracy_keras) )\ncm_keras = confusion_matrix(y_test, y_pred_keras)\nsns.heatmap(cm_keras, annot = True, annot_kws = {'size' : 10}, fmt = 'd')","af5f1d71":"# Lightgbm Classifier \nimport lightgbm as lgb\n\nd_train = lgb.Dataset(X_train, label = y_train)\n\nparams = {}\n\nparams['learning_rate'] = 0.0002\nparams['boosting_type'] = 'gbdt'\nparams['abjective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 20\nparams['min_data'] = 50\nparams['max_depth'] = 10 \n\nclf = lgb.train(params, d_train, 1000)       #  100 is number of iterations\n\ny_pred_lgm = clf.predict(X_test)\n\nfor i in range(len(y_pred_lgm)):\n    if y_pred_lgm[i] >= 0.188:\n        y_pred_lgm[i] = 1\n    else:\n        y_pred_lgm[i] = 0\n        \nfrom sklearn.metrics import accuracy_score\n\naccuracy_lgm = accuracy_score(y_pred_lgm, y_test)\n\nprint(accuracy_lgm)\n\ncm_lgm = confusion_matrix(y_pred_lgm, y_test)\nsns.heatmap(cm_lgm, annot = True, fmt = 'd')","cd84dd68":"# Grid Search for lightgbm\n\nmdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n          objective = 'binary',\n          n_jobs = 3)  \n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {'learning_rate' : [0.5, 0.2 , 0.1 ,0.01 ,0.002 ,0.001 ,0.0001],\n               'boosting_type' : ['gbdt'],\n               'objective' : ['binary'],\n               'metric' : ['binary_logloss'],\n               'sub_feature' : [0.5],\n               'num_leaves' : [10,20,30,40],\n               'min_data' : [20, 40,50],\n               'max_depth' : [10,20,30,40],\n               'reg_alpha' : [1,1.2],\n               'reg_lambda' : [1,1.2,1.4]\n                }\n\ngrid_search = GridSearchCV(estimator = mdl, param_grid = grid_params, scoring = 'accuracy', cv = 4)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_score = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\ny_grid_pred_lgm = grid_search.predict(X_test)\n\n\naccuracy_grid_lgm = accuracy_score(y_grid_pred_lgm, y_test)\n\nprint(accuracy_grid_lgm)\n\ncm_lgm = confusion_matrix(y_grid_pred_lgm, y_test)\nsns.heatmap(cm_lgm, annot = True, fmt = 'd')","36a0700b":"df_comparison = {'Model' : [], 'Accuracy' : [], 'F1_score' : [], 'ROC_AUC_Score': []}\ndf_comparison = pd.DataFrame(df_comparison)\ndf_comparison.loc[0,:] = ['Logistic', accuracy_score(y_test, y_pred_logistic), f1_score(y_test, y_pred_logistic), roc_auc_score(y_test, y_pred_logistic)]\ndf_comparison.loc[1,:] = ['Decision Tree', accuracy_score(y_test, y_pred_tree), f1_score(y_test, y_pred_tree), roc_auc_score(y_test, y_pred_tree)]\ndf_comparison.loc[2,:] = ['XGBoost', accuracy_score(y_test, y_pred_xg), f1_score(y_test, y_pred_xg), roc_auc_score(y_test, y_pred_xg)]\ndf_comparison.loc[3,:] = ['Neural Network', accuracy_score(y_test, y_pred_keras), f1_score(y_test, y_pred_keras), roc_auc_score(y_test, y_pred_keras)]\ndf_comparison.loc[4,:] = ['LightGM', accuracy_score(y_test, y_pred_lgm), f1_score(y_test, y_pred_lgm), roc_auc_score(y_test, y_pred_lgm)]\ndf_comparison.loc[5,:] = ['LightGM with Grid Search', accuracy_score(y_test, y_grid_pred_lgm), f1_score(y_test, y_grid_pred_lgm), roc_auc_score(y_test, y_grid_pred_lgm)]","97e5a946":"df_comparison","57f6fd73":"The new visitors have lower exit rates.","5ef45d56":"This bar chart is same compared to the former one. I just order months by number. (if there is easier way to do, please feel free to share it)","ece20792":"**Dataset PreProcessing**","6648fd69":"**Decision Tree Training**","b2a669bf":"**Reading Dataset**","caa8e33a":"**Neural Network Calssification**","dcf2b7a5":"After converting the variables, we have 77 variables.","8ad5927c":"**Splitting the dataset into Train and Test set**","757f82be":"**Data Visualization**\n#","c7529da7":"As can be seen from the pie charts, September October, and November have higher revenue rate. The users who visit the websites more likely make a purchase.","6c9305ac":"**Dropping the Missing Value**","ed336b2c":"**Dropping the variables**\n#","32ed6dde":"**Converting dataset in same scale**","3946b062":"We dropped the variables that are correlated.","a34ec5ab":"* **Administrative:**\tThe number of different types of pages visited by the visitor for administrative pages\n* **Administrative Duration:**\tSession durations of administrative pages\n* **Informational:**\tThe number of different types of pages visited by the visitor for informational pages\n* **Informational Duration:**\tSession durations of informational pages\n* **Product Related:**\tThe number of different types of pages visited by the visitor for product related pages\n* **Product Related Duration:**\tSession duration that users \n* **Exit Rate:**\t(I have an concern about this figure) This rate is suppose to be per page not per session\n* **Bounce Rate:**  (I have an concern about this figure like Exit Rate)  This rate is suppose to be per page not per session\n* **Page Values:**\tThis feature represents the average value for a web page that a user visited before completing an e-commerce transaction\n* **Special Day:**\tIndicates the closeness of the site visiting time to a specific special day\n* **Month:**\tThe month when a user visits the page in\n* **OperatingSystems:**\tOperating systems that the visitor use\n* **Browser:**\tBrowser type that the visitor use\n* **Region:**\tThe region where visitor visit the website from\n* **TrafficType:**\tThe type of the traffic\n* **VisitorType:**\tThe type of visitor based on whether they are new or not\n* **Weekend:**\tShows whether it is weekend or not \n* **Revenue:**\tDemonstrates the session is resulted successfully with revenue","976c8acd":"**XGBoost Classification**","347d8557":"**Descriptive Statistics**","7d43a9cd":"When the website users visit all page togetger, they more likely make purchase.","c3321249":"The best model is LightGM for both accuracy and roc_auc_score.","3e42a2bd":"**Logistic Regression Training**","bc8d5bfb":"**Dummy Variable**\n#","b3a38bcb":"**Linear Regression Roc AUC Score and Chart**","006775f6":"**Splittig the dataset into independent and dependent**","d230fcbc":"We converted the variables to dummy variables that are categorical.","a673e5eb":"**LightGBM Classification**","9f621db6":"**Missing Value Checking**","59dfe4eb":"This is my first analysis, feel free to share your recommendations in the comments, thanks :D","ecdd38c5":"**LightGM Classification Grid Search for Hyperparameter Tuning**","75a5580d":"This bar graph shows that the percentage of visitor who visited the different type of pages. Around 50% of the users visit the administrative type of page and almost all of them visit the product pages as well.","2536d4a3":"Bounce Rate and Exit Rate is highly correlated.","f77cb63c":"New visitor's revenue rate is higher than other and returning rate."}}