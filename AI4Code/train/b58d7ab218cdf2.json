{"cell_type":{"06dbbe79":"code","a187230d":"code","1ea1ac67":"code","0393f59b":"code","62d50ef8":"code","1937f006":"code","c332e90f":"markdown","31696aff":"markdown","737759f2":"markdown","4acdf36a":"markdown","dab40df4":"markdown","a5d0ab55":"markdown"},"source":{"06dbbe79":"divvy_stations = pd.read_csv(\"..\/input\/Divvy_Bicycle_Stations.csv\")\ndivvy_stations.head()\n","a187230d":"divvy_station_columns = divvy_stations.columns\ncol_with_space_removed = [''.join(j for j in i.title() if not j.isspace()) for i in divvy_station_columns]\ncamel_cols = [col[0].lower()+col[1:] for col in col_with_space_removed ]\ndivvy_stations.columns =camel_cols\n\n#display columns\ndivvy_stations.columns\n\n","1ea1ac67":"#sdf_typed = spark.sql(\"SELECT cast(id as int), stationName, cast(docksInService as int), \n                      #cast(latitude as float), cast(longitude as float)   FROM renamed_divvyStation_df\")\n                      \nsliced_pdf = divvy_stations[[\"id\",\"stationname\",\"docksinservice\",\"latitude\",\"longitude\" ]]                      \nsliced_pdf.astype({'id': 'int32',\n                  'stationname':'str',\n                  'docksinservice':'int32',\n                  'latitude':'float',\n                  'longitude':'float'})\nsliced_pdf.head()\n    \n                      \n                      \n                      ","0393f59b":"sliced_pdf.describe()","62d50ef8":"import pandas as pd\nimport numpy as np\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom folium import IFrame\n\nlocations = sliced_pdf[['latitude', 'longitude']]\nlocationlist = locations.values.tolist()\nlen(locationlist)\nlocationlist[7:9]","1937f006":"\nfolium_map = folium.Map([41.894722,-87.634362],  zoom_start = 13)\n\nfor point in range(0, len(locationlist)):\n    folium.Marker(locationlist[point], popup=divvy_stations['stationname'][point])\\\n    .add_to(folium_map)\n\nfolium_map","c332e90f":"### About:\n\n#### Using Pandas DataFrame\n In this project we will map Divvy bike stations at Chicago using Folium.\n\n-  Rename columns\n-  Convert dtype\n-  Check null values\n-  Convert laittude and longitude into point object and plot to the map.\n\n#### Using PySpark and SparkDataframe\nI have also created this project using pyspark. Here is the [Here is the link of the Databricks notebook](https:\/\/databricks-prod-cloudfront.cloud.databricks.com\/public\/4027ec902e239c93eaaa8714f173bcfc\/8729880089806595\/1158333375392443\/7967251143560102\/latest.html). Please make sure that you install folium package before running this notebook if you are using databricks community edition.\n","31696aff":"### Import Data","737759f2":"It seems that there is no null values. ","4acdf36a":"### Rename Columns\nRemove space between the column lables","dab40df4":"### Plot data using Folium\n","a5d0ab55":"### Data Preparation\nConvert datatype\n\n-id:integer\n-stationName:string\n-docksInService:integer\n-latitude:float\n-longitude:float\n"}}