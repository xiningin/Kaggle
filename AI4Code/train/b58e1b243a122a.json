{"cell_type":{"bed9dd7e":"code","ec5a199e":"code","1cebeb20":"code","0ba2599e":"code","7fa05a57":"code","e1bacbbe":"code","d3767196":"code","24a5a5b4":"code","c838fb9f":"code","7cfd781d":"code","d28d2801":"code","71eb5878":"code","dcc655eb":"code","063d81aa":"code","171fe1bc":"code","435ab704":"code","7b593b83":"code","ceda8e92":"markdown","46a17149":"markdown","1a3f94c0":"markdown","966ad92e":"markdown","c7b19d25":"markdown","6e00bba1":"markdown"},"source":{"bed9dd7e":"import numpy as np\nimport pandas as pd\nimport pickle\nfrom torch.utils.data import Dataset\nimport time\nimport torch\nfrom fastai.text.all import *\nfrom sklearn.preprocessing import MinMaxScaler","ec5a199e":"def load_pickle(filename):\n    infile = open(filename,'rb')\n    obj = pickle.load(infile)\n    infile.close()\n    return obj\n\ndef save_pickle(obj, filename):\n    outfile = open(filename,'wb')\n    pickle.dump(obj, outfile)\n    outfile.close()\n    \n    \n    \ndef comp_metric(xhat, yhat, fhat, x, y, f):\n    intermediate = torch.sqrt(torch.pow(xhat-x, 2) + torch.pow(yhat-y, 2)) + 15 * torch.abs(fhat-f)\n#     intermediate = intermediate * distrib\n    return intermediate.sum()\/xhat.shape[0]\/xhat.shape[1]\n\ndef comp_metric2(xhat, yhat, fhat, x, y, f):\n    intermediate = torch.sqrt(torch.pow(xhat-x, 2) + torch.pow(yhat-y, 2)) + 15 * torch.abs(fhat-f)\n    return intermediate.sum()\/xhat.shape[0]\n\n\ndef loss_fn(outputs, labels):\n    xhat = outputs[:, :, 1]\n    yhat = outputs[:, :, 2]\n    fhat = outputs[:, :, 0]\n    \n    x = labels[:, :, 1]\n    y = labels[:, :, 2]\n    f = labels[:, :, 0]\n\n    return comp_metric(xhat, yhat, fhat, x, y, f)\n\ndef metric_fn(outputs, labels):\n    xhat = outputs[:, -1, 1]\n    yhat = outputs[:, -1, 2]\n    fhat = outputs[:, -1, 0]\n\n    \n    x = labels[:, -1, 1]\n    y = labels[:, -1, 2]\n    f = labels[:, -1, 0]\n\n    return comp_metric2(xhat, yhat, fhat, x, y, f)\n\n\n\nclass StopAt(Callback):\n    \"\"\"stops training after epoch {stop}. when stop is 1, it will train for two cycles (0 and 1)\"\"\"\n    def __init__(self, stop):\n        self.stop = stop\n        super().__init__()\n\n    def before_epoch(self):\n        if self.epoch == self.stop + 1:\n          raise CancelFitException()","1cebeb20":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","0ba2599e":"%%time\ndata = load_pickle('..\/input\/indoor-location-rnn-data-v2\/data.pickle')\ntest_data = load_pickle('..\/input\/indoor-location-rnn-test-data-v2\/test-data.pickle')\npath_data = load_pickle('..\/input\/indoor-location-rnn-data-v2\/path_data.pickle')","7fa05a57":"%%time\n# normalize X\nrssi_list = []\nfor key in data:\n    rssis = data[key][0][:, -1]\n    rssi_list.append(rssis)\nrssi_list = np.hstack(rssi_list).reshape(-1, 1)\n\nscaler = MinMaxScaler()\nscaler.fit(rssi_list)\n\nfor key in data:\n    X = data[key][0].astype('float32')\n    X[:, -1:] = scaler.transform(X[:, -1:])\n    data[key] = (X, data[key][1].astype('float32'))\n    \nfor key in test_data:\n    X = test_data[key][0].astype('float32')\n    X[:, -1:] = scaler.transform(X[:, -1:])\n    test_data[key] = (X, test_data[key][1].astype('float32'))","e1bacbbe":"# seq_lens = []\n# for key in train_data:\n#     seq_lens.append(train_data[key][0].shape[0])\n# pd.Series(seq_lens).describe()\nseq_len = 174","d3767196":"# bssids = set()\n# for key in train_data:\n#     X = train_data[key][0]\n#     for wifi in X:\n#         bssids.add(wifi[1])\n# max(bssids)\nn_bssids = 63114","24a5a5b4":"def train_test_split(state=0, mod=10):\n    assert state < mod\n    train_data = {}\n    val_data = {}\n    for path in path_data:\n        if path % mod == state:\n            for key in path_data[path]:\n                val_data[len(val_data)] = data[key]\n        else:\n            for key in path_data[path]:\n                train_data[len(train_data)] = data[key]\n    return train_data, val_data","c838fb9f":"# sample_data = {} # data of first building\n# for key in data:\n#     if key == 26507:\n#         break\n#     sample_data[key] = data[key]\n    \n# train_data, val_data = train_test_split(sample_data)","7cfd781d":"class MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.padding_len = seq_len\n        self.pad = np.array([[0, n_bssids, -100]] * self.padding_len, dtype='float32')\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        X, Y = self.data[idx]\n        original_len = X.shape[0]\n        if original_len == self.padding_len:\n            pass\n        elif original_len > self.padding_len:\n            X = X[:self.padding_len]\n        else:\n            X = np.vstack([X, self.pad[original_len:]])\n#         X = np.pad(X, ((0, self.padding_len - original_len), (0, 0)), constant_values=((0, self.padding_value), (0, 0)))\n#         X[-1][-1] = original_len\n        X = X[:, 1:]\n        Y = np.repeat(Y.reshape(1,3), self.padding_len, axis=0)\n#         Y = np.array([Y] * self.padding_len)\n#         X = torch.tensor(X, device=device)\n#         Y = torch.tensor(Y, device=device)\n        return (X, Y)\n    \n    \n    \nclass EmptyDataset(Dataset):\n    def __init__(self):\n        pass\n\n    def __len__(self):\n        return 0\n\n    def __getitem__(self, idx):\n        return None","d28d2801":"# dset = MyDataset(data)\n# dl = DataLoader(dset, batch_size=64, shuffle=True, device=device)","71eb5878":"# %%time\n# for i in range(len(dset)):\n#     dset[i]","dcc655eb":"# pure 391ms\n# padding 44.3s\n# padding with values 58.8s\n# padding by stacking 7.9s\n# padding both X and Y 12.8s","063d81aa":"len(test_data)","171fe1bc":"# class Feed_Forward_block(nn.Module):\n#     \"\"\"\n#     out =  Relu( M_out*w1 + b1) *w2 + b2\n#     \"\"\"\n#     def __init__(self, dim_ff):\n#         super().__init__()\n#         self.layer1 = nn.Linear(in_features=dim_ff , out_features=dim_ff)\n#         self.layer2 = nn.Linear(in_features=dim_ff , out_features=dim_ff)\n\n#     def forward(self,ffn_in):\n#         return  self.layer2(   F.relu( self.layer1(ffn_in) )   )\n\n# class Model(Module):\n#     def __init__(self, vocab_sz, embed_dim, n_hidden, n_layers, p):\n        \n#         self.i_h = nn.Embedding(vocab_sz, embed_dim)\n#         self.entity1 = nn.Sequential(\n#             nn.Linear(embed_dim + 3, embed_dim),\n#             nn.Tanh()\n#         )\n#         self.entity2 = nn.Sequential(\n#             nn.Dropout(0.2),\n#             nn.Linear(embed_dim, embed_dim),\n#         )\n#         self.multi_en = nn.MultiheadAttention( embed_dim= embed_dim, num_heads= 4,dropout=0.1  )     # multihead attention    ## todo add dropout, LayerNORM\n#         self.ffn_en = Feed_Forward_block( embed_dim )                                            # feedforward block     ## todo dropout, LayerNorm\n#         self.layer_norm1 = nn.LayerNorm( embed_dim )\n#         self.layer_norm2 = nn.LayerNorm( embed_dim )\n#         gru_dim = embed_dim\n\n#         self.inv_gru2 = nn.GRU(gru_dim, 128, batch_first=True)\n#         self.gru2 = nn.GRU(3*gru_dim, 128, batch_first=True)\n#         self.main = nn.Sequential(\n#             nn.Linear(128, 32),\n#             nn.ReLU(True),\n#             nn.Linear(32, 2),\n#         )\n# #         self.rnn = nn.LSTM(embed_dim + 1, n_hidden, n_layers, batch_first=True, dropout=p)\n#         self.drop = nn.Dropout(p)\n# #         self.h_o = nn.Linear(n_hidden, 3)\n\n# #     def forward(self, x):\n# #         raw, _ = self.rnn(torch.cat((self.i_h(x[:, :, 0].long()), x[:, :, 1].unsqueeze(2)), 2))\n# #         out = self.drop(raw)\n# #         return self.h_o(out)#,raw,out\n    \n#     def forward(self, x):\n#         #x = self.i_h(x)\n#         x = torch.cat((self.drop(self.i_h(x[:, :, 0].long())), x[:, :, 1].unsqueeze(2)), 2)\n#       #  x = self.entity1(x)\n#       #  x = self.entity2(x)\n#         #(batch, memory_length, input_dim)\n#         ################LAST QUERY TRANSFORMER#############\n#         skip_out = x \n\n#         x, attn_wt = self.multi_en( x[-1:,:,:] , x , x )         # Q,K,V\n#         #                        #attn_mask=get_mask(seq_len=n))  # attention mask upper triangular\n#         #print('MLH out shape', out.shape)\n#         x = x + skip_out \n#         x_inv, _ = self.inv_gru2(torch.flip(x, (1,)))\n#         x = torch.cat([x, torch.flip(x_inv, (1,))], -1)\n#         x, _ = self.gru2(x)\n#         output = self.main(x)\n\n#         #raw, _ = self.rnn(torch.cat((self.drop(self.i_h(x[:, :, 0].long())), x[:, :, 1].unsqueeze(2)), 2))\n#         return self.h_o(self.drop(raw))","435ab704":"class Model(Module):\n    def __init__(self, vocab_sz, embed_dim, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, embed_dim)\n        self.rnn = nn.LSTM(embed_dim + 1, n_hidden, n_layers, batch_first=True, dropout=p)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, 3)\n\n#     def forward(self, x):\n#         raw, _ = self.rnn(torch.cat((self.i_h(x[:, :, 0].long()), x[:, :, 1].unsqueeze(2)), 2))\n#         out = self.drop(raw)\n#         return self.h_o(out)#,raw,out\n    \n    def forward(self, x):\n#         max_size = int(x[:, -1, -1].max())\n#         print(max_size)\n#         x = x[:, :max_size]\n        raw, _ = self.rnn(torch.cat((self.drop(self.i_h(x[:, :, 0].long())), x[:, :, 1].unsqueeze(2)), 2))\n        return self.h_o(self.drop(raw))","7b593b83":"%%time\n# hyperparameters\nbatch_size = 256\nembed_dim =  196\nhidden_size = (embed_dim + 1 + 3) \/\/ 2\nn_layers = 3\np = 0\nlr = 0.015625\ncycle = 32\nstop = 27\n\nfor i in range(10):\n    # dataloader\n    train, val = train_test_split(i)\n    dset_train = MyDataset(train)\n    dl_train = DataLoader(dset_train, batch_size=batch_size, shuffle=True)\n    dset_val = MyDataset(val)\n    dl_val = DataLoader(dset_val, batch_size=batch_size)\n    dls = DataLoaders(dl_train, dl_val, device=device)\n\n    # fit\n    print('batch_size, embed_dim, hidden_size, n_layers, p, lr, cycle, stop')\n    print(batch_size, embed_dim, hidden_size, n_layers, p, lr, cycle, stop)\n    learn = Learner(dls, Model(n_bssids + 1, embed_dim, hidden_size, n_layers, p).to(device), loss_func=loss_fn, metrics=metric_fn)\n    learn.fit_one_cycle(cycle, lr, cbs=StopAt(stop))\n    \n    # predict\n    dset = MyDataset(test_data)\n    test_dl = DataLoader(dset, batch_size=batch_size, device=device)\n\n    preds, _ = learn.get_preds(dl=test_dl)\n    pred_df = pd.DataFrame(preds[:, -1])\n    pred_df.to_csv(f'predictions_fold{i}.csv', index=False)","ceda8e92":"# train and predict","46a17149":"# functions","1a3f94c0":"# Model","966ad92e":"# useful infos","c7b19d25":"# ------------------------------------------------","6e00bba1":"# data"}}