{"cell_type":{"31a3d89f":"code","53d4ba3e":"code","2f72138f":"code","939a91cc":"code","05a7638f":"code","cfa8818c":"markdown","df6c9a75":"markdown","4b9fb0bf":"markdown","f70a3dcd":"markdown","ee12ec1c":"markdown"},"source":{"31a3d89f":"# Imports for Deep Learning\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# ensure consistency across runs\nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\n# Imports to view data\nimport cv2\nfrom glob import glob\nfrom matplotlib import pyplot as plt\nfrom numpy import floor\nimport random\n\ndef plot_three_samples(letter):\n    print(\"Samples images for letter \" + letter)\n    base_path = '..\/input\/asl_alphabet_train\/asl_alphabet_train\/'\n    img_path = base_path + letter + '\/**'\n    path_contents = glob(img_path)\n    \n    plt.figure(figsize=(16,16))\n    imgs = random.sample(path_contents, 3)\n    plt.subplot(131)\n    plt.imshow(cv2.imread(imgs[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(imgs[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(imgs[2]))\n    return\n\nplot_three_samples('A')","53d4ba3e":"plot_three_samples('B')","2f72138f":"data_dir = \"..\/input\/asl_alphabet_train\/asl_alphabet_train\"\ntarget_size = (64, 64)\ntarget_dims = (64, 64, 3) # add channel for RGB\nn_classes = 29\nval_frac = 0.1\nbatch_size = 64\n\ndata_augmentor = ImageDataGenerator(samplewise_center=True, \n                                    samplewise_std_normalization=True, \n                                    validation_split=val_frac)\n\ntrain_generator = data_augmentor.flow_from_directory(data_dir, target_size=target_size, batch_size=batch_size, shuffle=True, subset=\"training\")\nval_generator = data_augmentor.flow_from_directory(data_dir, target_size=target_size, batch_size=batch_size, subset=\"validation\")","939a91cc":"my_model = Sequential()\nmy_model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=target_dims))\nmy_model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\nmy_model.add(Dropout(0.5))\nmy_model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\nmy_model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\nmy_model.add(Dropout(0.5))\nmy_model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\nmy_model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\nmy_model.add(Flatten())\nmy_model.add(Dropout(0.5))\nmy_model.add(Dense(512, activation='relu'))\nmy_model.add(Dense(n_classes, activation='softmax'))\n\nmy_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])","05a7638f":"my_model.fit_generator(train_generator, epochs=5, validation_data=val_generator)","cfa8818c":"# The data\n\nThe dataset contains images with 29 different signs in American Sign Language. These are the 26 letters (A through Z) plus the signs for *space*, *delete* and *nothing*. Our model will view these images and learn to classify what sign is made in each image.\n\nSample images below\n","df6c9a75":"# Model Specification","4b9fb0bf":"# Data Processing Set-Up","f70a3dcd":"# Intro\nKaggle provides free access to NVidia K80 GPUs in kernels. This benchmark shows that **enabling a GPU to your Kernel results in a 12.5X speedup during training of a deep learning model.** \n\nThis kernel was run with a GPU. I compare run-times to a kernel training the same model on a CPU [here](https:\/\/www.kaggle.com\/dansbecker\/benchmarking-model-training-with-a-cpu).\n\nThe total run-time with a GPU is 994 seconds. The total run-time for the kernel with only a CPU is 13,419 seconds. This is a 12.5X speedup (total run-time with only a CPU is 13.5X as long).\n\nLimiting the comparison only to model training, we see a reduction from 13,378 seconds on CPU to 950 seconds with a GPU.  So the model training speed-up is a little over 13X.\n\nThe exact speed-up varies based on a number of factors including model architecture, batch-size, input pipeline complexity, etc. That said, the GPU opens up much great possibilities in Kaggle kernels. \n\nIf you want to use these GPU's for deep learning projects, you'll likely find our [Deep Learning Course](kaggle.com\/learn\/deep-learning) the fastest path around to get up to speed so you can run your own projects.  We're also adding new image processing datasets to our [Datasets platform](kaggle.com\/datasets) and we always have many [Competitions](kaggle.com\/competitions) for you to try out new ideas using these free GPU's.  \n\nThe following text shows how to enable a GPU and gives details on the benchmark.\n\n\n# Adding a GPU\nWe set up this kernel to run on a GPU by first opening the kernel controls.\n\n![Imgur](https:\/\/i.imgur.com\/WY2p6bH.png)\n___\nSelect the **Settings** tab. Then select the checkbox for **Enable GPU**. Verify the GPU is attached to your kernel in the console bar, where it should show **GPU ON** next to your resource usage metrics.\n\n![Imgur](https:\/\/i.imgur.com\/F9Hd3DN.png)\n___\n*GPU backed instances have less CPU power and RAM. Moreover, many data science libraries cannot take advantage of a GPU.  So, GPU's will be valuable for some tasks (especially when using deep learning libraries like TensorFlow, Keras and PyTorch).  But you are better off without a GPU for most other tasks.*","ee12ec1c":"# Model Fitting"}}