{"cell_type":{"9397061e":"code","32e44ef9":"code","15050a03":"code","e8a3e962":"code","3e19ec8a":"code","4ebf09ba":"code","2f6b9f30":"code","300b9925":"code","7a0fc17c":"code","4781d337":"code","7f67ed8b":"code","c01ddbe0":"code","d50fd318":"code","c0ebe66a":"code","43d5df4b":"code","473aa9ed":"code","9b5535a4":"code","bc0327d3":"code","1b55cbfd":"code","2561f6eb":"code","578900ab":"code","f5d25a9b":"code","611f6c90":"code","23c438b2":"code","018a6cad":"code","c4ae7ad1":"code","43c69c94":"code","082c31aa":"code","7c7c7ec3":"code","8fee8538":"code","7ea72a90":"code","29e94f72":"code","2cca608f":"code","a7a4dd36":"code","8c9792c3":"code","ee581ec4":"code","7fa74b94":"code","f7b755b8":"code","c77ae00f":"code","3a34ec70":"code","7e4b6191":"code","b93af69e":"code","e72e7ac9":"code","601c517d":"code","f0188f7b":"code","eb53cf72":"code","d0035bce":"code","e7ce9c6f":"markdown","bb9552b5":"markdown","af5564cb":"markdown","0fddef70":"markdown"},"source":{"9397061e":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom torchvision import datasets, transforms, models \nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nfrom torch.utils.data.dataloader import DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline","32e44ef9":"data_dir = '..\/input\/pins-face-recognition\/105_classes_pins_dataset'\nclasses = os.listdir(data_dir)\nprint(len(classes))","15050a03":"train_transform=transforms.Compose([\n        transforms.RandomRotation(10),      # rotate +\/- 10 degrees\n        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n        transforms.Resize(100),             # resize shortest side to 100 pixels\n        transforms.CenterCrop(100),         # crop longest side to 100 pixels at center\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])","e8a3e962":"dataset = ImageFolder(data_dir, transform=train_transform)\nprint('Size of training dataset :', len(dataset))","3e19ec8a":"# view one image shape of the dataset.\nimg, label = dataset[100]\nprint(img.shape)","4ebf09ba":"# function for the showing the image.\ndef show_image(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","2f6b9f30":"show_image(*dataset[200])","300b9925":"show_image(*dataset[5000])","7a0fc17c":"torch.manual_seed(20)\nval_size = len(dataset)\/\/10\ntest_size = len(dataset)\/\/5\ntrain_size = len(dataset) - val_size -test_size","4781d337":"train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds),len(test_ds)   ","7f67ed8b":"batch_size = 64\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)","c01ddbe0":"for images, labels in train_loader:\n    fig, ax = plt.subplots(figsize=(18,10))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n    break","d50fd318":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","c0ebe66a":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","43d5df4b":"torch.cuda.is_available()","473aa9ed":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","9b5535a4":"device = get_default_device()\ndevice","bc0327d3":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","1b55cbfd":"input_size = 3*100*100\noutput_size = len(classes) # Number of classe","2561f6eb":"class Model(ImageClassificationBase):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        # hidden layer\n        self.in_layer = nn.Linear(input_size, 8384)\n        self.hidden1 = nn.Linear(8384, 4192)\n        self.hidden2 = nn.Linear(4192, 2096)\n        self.hidden3 = nn.Linear(2096, 1048)\n        self.out_layer = nn.Linear(1048, output_size)\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1)\n        out = self.in_layer(out)\n        out = self.hidden1(F.relu(out))\n        out = self.hidden2(F.relu(out))\n        out = self.hidden3(F.relu(out))\n        out = self.out_layer(F.relu(out))\n        return out","578900ab":"model = to_device(Model(input_size, output_size), device)","f5d25a9b":"model","611f6c90":"history = [evaluate(model, val_loader)]\nhistory","23c438b2":"history += fit(7, 0.01, model, train_loader, val_loader)","018a6cad":"history += fit(8, 0.001, model, train_loader, val_loader)","c4ae7ad1":"history += fit(3, 0.0001, model, train_loader, val_loader)","43c69c94":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.show()\n    \n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')\n    plt.show()","082c31aa":"plot_accuracies(history)","7c7c7ec3":"plot_losses(history)","8fee8538":"evaluate(model, test_loader)","7ea72a90":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                   # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","29e94f72":"class CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 100, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(100, 150, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 150 x 16 x 16\n\n            nn.Conv2d(150, 200, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 200 x 8 x 8\n\n            nn.Conv2d(200, 250, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(250, 250, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 250 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(36000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(500, 131))\n        \n    def forward(self, xb):\n        return self.network(xb)","2cca608f":"model = CnnModel()\n#model.cuda()","a7a4dd36":"model","8c9792c3":"for images, labels in train_loader:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    #print('out[0]:', out[0])\n    break","ee581ec4":"device = get_default_device()\ndevice","7fa74b94":"train_dl = DeviceDataLoader(train_loader, device)\nval_dl = DeviceDataLoader(val_loader, device)\nto_device(model, device)","f7b755b8":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","c77ae00f":"model = to_device(CnnModel(), device)","3a34ec70":"history=[evaluate(model, val_loader)]\nhistory","7e4b6191":"num_epochs = 3\nopt_func = torch.optim.Adam\nlr = 0.001","b93af69e":"history+= fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","e72e7ac9":"history+= fit(num_epochs, lr\/10, model, train_dl, val_dl, opt_func)","601c517d":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.show()\n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')\n    plt.show()","f0188f7b":"plot_accuracies(history)","eb53cf72":"plot_losses(history)","d0035bce":"evaluate(model, test_loader)","e7ce9c6f":"# Conv2d Model","bb9552b5":"### Hierarchical structure of used pytorch laibrary\n---\n- **torch**\n - nn\n   - functional\n   - Conv2d\n   - Linear\n   - ReLU\n   - MaxPool2d\n   - Flatten\n   - Dropout\n - utils\n   - data\n     - random_split\n     - dataloader\n       - DataLoader\n  - device\n  - cuda\n  - stack\n---\n- **torchvision**\n - datasets\n   - ImageFolder\n - transforms\n   - Compose\n   - Resize\n   - ToTensor\n   - Normalize\n - models\n - utils\n   - make_grid\n---","af5564cb":"# Linear Model","0fddef70":"# Pins Face Classify Torch Linear\/Conv2d"}}