{"cell_type":{"cdccac97":"code","0c3c8e2d":"code","f2ece1f8":"code","d1053912":"code","48d6595e":"code","570287a2":"code","b783d4b3":"code","9f0f7e63":"code","ef45409e":"code","a813d31d":"code","47bd7d3a":"code","53d4a5ed":"code","351e0cc8":"code","847e0ce0":"code","c61e5c72":"code","5907975c":"code","b573f01f":"code","0e114e05":"code","f1de7d3b":"code","0e1839f0":"markdown"},"source":{"cdccac97":"import numpy as np\nimport pandas as pd\nimport transformers\nimport json\nfrom sklearn.model_selection import train_test_split\nimport re\nimport os\nimport torch\nos.environ['WANDB_DISABLED'] = 'true'","0c3c8e2d":"!pip install wandb","f2ece1f8":"\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint('device = ', device)","d1053912":"file_data = \"\/kaggle\/input\/vlsp-2021-nghich-cho-vui\/vlsp_2021_nli_traindata_16200_pairs.json\"","48d6595e":"with open(file_data, 'r') as f:\n    data = json.load(f)","570287a2":"data = pd.DataFrame(data)","b783d4b3":"data[:3]","9f0f7e63":"len(data[data['label'] == 'neutral']), len(data[data['label'] == 'disagree']), len(data[data['label'] == 'agree'])","ef45409e":"print(len(data[(data['label'] == 'neutral') & (data['lang_1'] == 'vi')]))\nprint(len(data[(data['label'] == 'neutral') & (data['lang_1'] == 'en')]))\n\nprint(len(data[(data['label'] == 'agree') & (data['lang_1'] == 'vi')]))\nprint(len(data[(data['label'] == 'agree') & (data['lang_1'] == 'en')]))\n\nprint(len(data[(data['label'] == 'disagree') & (data['lang_1'] == 'vi')]))\nprint(len(data[(data['label'] == 'disagree') & (data['lang_1'] == 'en')]))","a813d31d":"data['label-lang_1'] = data[['label', 'lang_1']].apply(lambda x: x[0] +'-'+ x[1], axis=1)","47bd7d3a":"data_train, data_test = train_test_split(data, test_size=0.3, random_state=42, stratify = data['label-lang_1'])","53d4a5ed":"dict_str2int = {'agree': 0, 'neutral': 1, 'disagree': 2}\ndict_int2str = {0: 'agree', 1: 'neutral', 2: 'disagree'}\ndata_train['label'] = data_train['label'].apply(lambda x: dict_str2int[x])\ndata_test['label'] = data_test['label'].apply(lambda x: dict_str2int[x])","351e0cc8":"from datasets import Dataset\n\nhf_data_train = Dataset.from_pandas(data_train[['sentence_1', 'sentence_2', 'label']])\nhf_data_test = Dataset.from_pandas(data_test[['sentence_1', 'sentence_2', 'label']])\n","847e0ce0":"hf_data_train, hf_data_train[0], hf_data_train[0]['sentence_1']","c61e5c72":"from transformers import TrainingArguments\ntraining_args = TrainingArguments(\".\/output\",\n                                      do_train=True,\n                                      do_eval=True,\n                                      num_train_epochs=5,\n                                      learning_rate=1e-4,\n                                      warmup_ratio=0.05,\n                                      weight_decay=0.01,\n                                      per_device_train_batch_size=1,\n                                      per_device_eval_batch_size=1,\n                                      gradient_accumulation_steps=1,\n                                      logging_dir='.\/log',\n                                      logging_steps=5,\n#                                       label_names=['disagree',\n#                                                    'agree',\n#                                                    'neutral'],\n                                      group_by_length=True,\n                                      save_strategy=\"epoch\",\n                                      metric_for_best_model='f1',\n                                      load_best_model_at_end=True,\n                                      save_total_limit=2,\n                                      #eval_steps=1,\n                                      #evaluation_strategy=\"steps\",\n                                      evaluation_strategy=\"epoch\",\n                                      report_to=None\n                                      )","5907975c":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nxlmr= AutoModelForSequenceClassification.from_pretrained('joeddav\/xlm-roberta-large-xnli', num_labels=3)\ntokenizer = AutoTokenizer.from_pretrained('joeddav\/xlm-roberta-large-xnli')\nxlmr = xlmr.to(device)","b573f01f":"\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence_1'], examples['sentence_2'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n\ntokenized_hf_data_train = hf_data_train.map(tokenize_function, batched=True, batch_size = 64)\ntokenized_hf_data_test = hf_data_test.map(tokenize_function, batched=True, batch_size = 32)","0e114e05":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=xlmr,\n    args=training_args,\n    train_dataset = tokenized_hf_data_train,\n    eval_dataset = tokenized_hf_data_test,\n)","f1de7d3b":"trainer.train()","0e1839f0":"# Statistic"}}