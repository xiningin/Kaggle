{"cell_type":{"1c03027d":"code","e21c7daa":"code","73414799":"code","f4fd19b7":"code","ba4f66df":"code","0bea695a":"code","90221c40":"code","79cfa321":"code","5aa56ac7":"code","ed93446c":"code","8492c631":"code","52dfbf95":"code","4d2c58c6":"code","18b77bca":"code","a244b5b2":"code","29f2bf89":"code","930b5644":"code","aa14b5cd":"code","8cecb54c":"code","09d69767":"code","ab3aae5a":"code","e9497dca":"code","aa026938":"code","ed8dbbb6":"code","3d849ceb":"code","ac77aca2":"code","acc8ee7d":"code","169e1762":"code","ac48af4a":"code","25853437":"code","62d3f99a":"code","4e998e51":"code","e71e74ed":"code","a7b7d8a0":"code","e000dd7a":"code","2226fbb0":"code","365a3276":"code","61d6770f":"code","de6c0ff8":"code","361399ba":"code","ce642e8a":"code","d2da3d6c":"code","cdbcb76c":"code","e705981c":"code","2f8f36c2":"code","9c9da353":"code","9b8e00a8":"code","57659176":"code","718d47e9":"code","56f6d901":"markdown","5b04f9c1":"markdown","da22fa67":"markdown","48548ba2":"markdown","f7c2fae5":"markdown","5a53aae1":"markdown","6269b9a0":"markdown","b45fe9a7":"markdown","bdb80089":"markdown","be0cea3e":"markdown","9b7193df":"markdown","dac1f5c8":"markdown","4008a902":"markdown","5c40ab1a":"markdown","271cd8ae":"markdown","63d7aa7d":"markdown","e0248ee9":"markdown","71cec5a1":"markdown","88a944e1":"markdown","19bab25c":"markdown","5d485c3c":"markdown","1f5bd648":"markdown","04e74dd7":"markdown","0fcf4d91":"markdown","7de3482e":"markdown","f1a75e53":"markdown","89d87675":"markdown","de6976e4":"markdown","3fe869f6":"markdown","8bac82d7":"markdown","f17d0f05":"markdown"},"source":{"1c03027d":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('seaborn')\nfrom plotly import graph_objs as go\nimport plotly.express as px\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\n\nimport keras\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten, Dropout\nfrom keras.optimizers import Adam","e21c7daa":"df1 = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf2 = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmit = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","73414799":"print(df1.shape)\nprint(df2.shape)","f4fd19b7":"df1.info()","ba4f66df":"df1.head()","0bea695a":"temp = df1.groupby('target').count()['text'].reset_index()\ntemp['label'] = temp['target'].apply(lambda x : 'Disaster Tweet' if x==1 else 'Non Disaster Tweet')\ntemp","90221c40":"plt.figure(figsize=(7,5))\nsns.countplot(x='target',data=df1)","79cfa321":"fig = go.Figure(go.Funnelarea(\n    text = temp.label,\n    values = temp.text,\n    title = {\"position\" : \"top center\", \"text\" : \"Funnel Chart for target distribution\"}\n    ))\nfig.show()","5aa56ac7":"df1['target_mean'] = df1.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 78), dpi=100)\n\nsns.countplot(y=df1.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df1.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf1.drop(columns=['target_mean'], inplace=True)","ed93446c":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len=df1[df1['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('Disaster Tweets')\ntweet_len=df1[df1['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Non Disaster Tweets')\nfig.suptitle('No.of words in a tweet')\nplt.show()","8492c631":"def clean_text(text):\n    text = str(text).lower()\n    return text\n\ndf1['text_plot'] = df1['text'].apply(lambda x:clean_text(x))\n\ndf1['temp_list'] = df1['text_plot'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df1['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","52dfbf95":"fig = px.bar(temp, x='count',y='Common_words',title='Common words in tweet',orientation='h',width=700,height=700,color='Common_words')\nfig.show()","4d2c58c6":"def remove_stopwords(x):\n    return [y for y in x if y not in stopwords.words('english')]\ndf1['temp_list'] = df1['temp_list'].apply(lambda x : remove_stopwords(x))","18b77bca":"top = Counter([item for sublist in df1['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","a244b5b2":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","29f2bf89":"text = df1['text'].values\ntwitter_logo = np.array(Image.open('..\/input\/twitter-logo2\/10wmt-articleLarge-v4.jpg'))\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='white',\n                          mask = twitter_logo,\n                          max_words=200\n                         ).generate(\" \".join(text))\n\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","930b5644":"del df1['text_plot']\ndel df1['temp_list']\n\ndf = pd.concat([df1,df2])","aa14b5cd":"df.head()","8cecb54c":"for col in ['keyword', 'location']:\n    df[col] = df[col].fillna(f'no_{col}')","09d69767":"df.head()","ab3aae5a":"df['text']=df['text'].str.replace('https?:\/\/\\S+|www\\.\\S+','').str.replace('<.*?>','')","e9497dca":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","aa026938":"df['text'] = df['text'].apply(lambda x : remove_emoji(x))","ed8dbbb6":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","3d849ceb":"df['text'] = df['text'].apply(lambda x : remove_punct(x))","ac77aca2":"def clean_text(text):\n    text = re.sub('\\s+', ' ', text).strip() \n    return text","acc8ee7d":"df['text'] = df['text'].apply(lambda x : clean_text(x))","169e1762":"dfs = np.split(df, [len(df1)], axis=0)","ac48af4a":"train = dfs[0]\ntrain.shape","25853437":"test = dfs[1]\ntest.shape","62d3f99a":"test.drop('target',axis=1,inplace=True)","4e998e51":"vocab_size = len(test)\ntext = train['text'].values\nlabel = train['target'].values","e71e74ed":"encoded_docs = [one_hot(d,vocab_size) for d in text]\nfor x in range(5):\n    print(encoded_docs[x])","a7b7d8a0":"max_len = len(train['text'].max())\npad_docs = pad_sequences(encoded_docs,maxlen=max_len,padding='post')","e000dd7a":"train.shape","2226fbb0":"model = Sequential()\nmodel.add(Embedding(7613,100,input_length=max_len))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc'])","365a3276":"model.summary()","61d6770f":"model.fit(pad_docs,label,epochs=20,batch_size=32)","de6c0ff8":"prediction = model.predict(pad_docs)","361399ba":"train['prediction'] = prediction","ce642e8a":"train['prediction'] = train['prediction'].apply(lambda x : 0 if x<0.5 else 1)","d2da3d6c":"train.head()","cdbcb76c":"text2 = test['text'].values\nencoded_docs2 = [one_hot(d,vocab_size) for d in text2]\npad_docs2 = pad_sequences(encoded_docs2,maxlen=max_len,padding='post')","e705981c":"prediction2 = model.predict(pad_docs2)","2f8f36c2":"test['prediction'] = prediction2\ntest['prediction'] = test['prediction'].apply(lambda x : 0 if x<0.5 else 1)","9c9da353":"test.head()","9b8e00a8":"submit['target'] = test['prediction']","57659176":"submit.head()","718d47e9":"submit.to_csv('submission.csv',index=False)","56f6d901":"**Let's see how many tweets are disaster and non-disaster tweets**","5b04f9c1":"**Firstly we define vocabulary size as len(test). That means this system here will support len(test) different words.**","da22fa67":"**First we will fill all the null values with no_{column name}.**","48548ba2":"<a id = '2'><\/a>\n# **Reading the data**","f7c2fae5":"**Now we will apply our trained model to the test data. But before that we have to also convert text of test data to padded data like we did earlier.**","5a53aae1":"**Number of words in a tweet**","6269b9a0":"<a id = '5'><\/a>\n# Model\nAfter doing all the data analysis and applying data cleaning process, now its time to create our model. \n\nNatural Language Processing (NLP) is a field in machine learning with the ability of a computer to understand, analyze, manipulate, and potentially generate human language.Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.We will be using Keras for creating our own NLP model.","b45fe9a7":"**Now we will define our model.**\n\nHere we are creating a Sequential model. Then we have one Embedding layer with vocab_size=7613,dimension=100 and input_length=max_len, one Dropout layer, one Flatten layer. Then we add 2 Dense layers with 1024 parameters and activation function is relu and a Dropout layer after each Dense Layer. In the end we add one final Dense layer with output_class=1 and activation function is sigmoid.\n\nThen we will compile our model with Adam optimizer and binary_crossentropy as loss function.","bdb80089":"# **Introduction**\n<div class=\"column\">\n<img src=\"https:\/\/media-exp1.licdn.com\/dms\/image\/C561BAQGEbzpXZ34-gQ\/company-background_10000\/0?e=2159024400&v=beta&t=o3vOn3Ye-qpqlDH64A1of1_aRAQ8TunahPQ4ZWuISRI\" style=\"width:650px;height:350px;\">\n    <\/div><br>\n<b>In this kernel we will go together into Disaster Tweets data and will do some data analysis, data cleaning and then create a simple NLP model to predict whether the tweet is about real disaster or not. I have tried to explain all the steps so that even if this is your first nlp problem, you will not get any confusion in any step.<\/b><br><br>\n\n##  **<font color=\"red\"> Please do an upvote if you find my kernel useful.<\/font>**","be0cea3e":"**So now we will remove all the all the urls and the HTML tags**","9b7193df":"# **Table of Content**\n* [Importing necesseties](#1)\n* [Reading the data](#2)\n* [EDA](#3)\n* [Data Cleaning](#4)\n* [Model](#5)\n","dac1f5c8":"**Removing punctuation marks**","4008a902":"**Submitting our predictions**","5c40ab1a":"**Now here, we are actually padding that means, if the sentence is not long enough, we are just filling it with zeros.**","271cd8ae":"**Let us also visualize the wordcloud**","63d7aa7d":"<a id = '1'><\/a>\n# **Importing necesseties**","e0248ee9":"**As each word is just a sequence of characters and, obviously, we cannot work with sequence of characters. Therefore, we will convert each word into an integer number, and this integer number is unique, as long as we don't exceed the vocabulary size. It's not the one-hot encoding. It's basically just the transformation from a list of the words into a list of integer values.**","71cec5a1":"**Remove leading, trailing, and extra spaces**","88a944e1":"<a id = '3'><\/a>\n# **EDA**","19bab25c":"**This is my first kaggle notebook and I hope I have tried to explain each and every step. I will be back with new ideas and models as I learn more about different machine learning models. Please if you want to give me any suggestion or any doubt in any step comment below.**\n\n#  **<font color=\"red\"> Please do an upvote if you liked my kernel!<\/font>**","5d485c3c":"**Now we will train our model with our padded data pad_docs and label with 20 epochs and batch_size=32.**","1f5bd648":"**As we observed from EDA that we have to remove many things like url, html tags, punctuation marks etc.**","04e74dd7":"**Now let us observe the common words in the tweet. But first we will convert all the text in lowercase so that same words with different case are not counted differently.**","0fcf4d91":"<a id = '4'><\/a>\n# **Data Cleaning**","7de3482e":"**Now as we have applied all the cleaning steps so now its time to separate our data back.**","f1a75e53":"**Removing all the emojis**","89d87675":"**Let's check our predictions**","de6976e4":"**So we have 7613 tweets in the train set and 3263 tweets in the test set**","3fe869f6":"**Target Distribution in Keywords**","8bac82d7":"We will apply data cleaning steps to both our training and test data. Here we are going to concat them so that we don't have to apply each steps separately. Then later on after applying data cleaning process we will separate them.","f17d0f05":"**Now we will remove all the stopwords and then will observe the common words graphically.**"}}