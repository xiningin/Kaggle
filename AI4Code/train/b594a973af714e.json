{"cell_type":{"2bc53d56":"code","a8da1192":"code","11ce86c6":"code","a43e5957":"code","dcae73d4":"code","4f8336e5":"code","2ba60850":"code","a45316b7":"code","1a50cfbc":"code","cc4eb3f2":"code","608a74f4":"code","6a17ee39":"markdown","b60b3136":"markdown","105b3947":"markdown","75119998":"markdown","430971e6":"markdown","77b16e8b":"markdown","09338335":"markdown"},"source":{"2bc53d56":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","a8da1192":"!export XLA_USE_BF16=1","11ce86c6":"!pip install efficientnet_pytorch","a43e5957":"import gc\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom joblib import Parallel, delayed\nimport efficientnet_pytorch\nimport albumentations\nfrom tqdm import tqdm\n\nImageFile.LOAD_TRUNCATED_IMAGES = True","dcae73d4":"df = pd.read_csv(\"..\/input\/jpeg-melanoma-256x256\/train.csv\")\ndf[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target.values\nkf = model_selection.StratifiedKFold(n_splits=8)\nfor fold_, (train_idx, test_idx) in enumerate(kf.split(X=df, y=y)):\n    df.loc[test_idx, \"kfold\"] = fold_\ndf.to_csv(\"train_folds.csv\", index=False)","4f8336e5":"class ClassificationDataset:\n    def __init__(self, image_paths, targets, resize, augmentations=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, item):\n        image = Image.open(self.image_paths[item])\n        targets = self.targets[item]\n        if self.resize is not None:\n            image = image.resize(\n                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n            )\n        image = np.array(image)\n        targets = np.array(targets)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","2ba60850":"class EfficientNet(nn.Module):\n    def __init__(self):\n        super(EfficientNet, self).__init__()\n        self.base_model = efficientnet_pytorch.EfficientNet.from_pretrained(\n            'efficientnet-b7'\n        )\n        self.base_model._fc = nn.Linear(\n            in_features=2560,\n            out_features=1,\n            bias=True\n        )\n\n    def forward(self, image, targets):\n        out = self.base_model(image)\n        loss = nn.BCEWithLogitsLoss()(out, targets.view(-1, 1).type_as(out))\n        return out, loss","a45316b7":"def train(data_loader=None, model=None, optimizer=None, scheduler=None, device=None):\n    model.train()\n    para_loader = pl.ParallelLoader(data_loader, [device])\n    tk0 = para_loader.per_device_loader(device)\n    for b_idx, data in enumerate(tk0):\n        for key, value in data.items():\n            data[key] = value.to(device)\n        optimizer.zero_grad()\n        _, loss = model(**data)\n        loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n        scheduler.step(loss)\n        return loss.item()","1a50cfbc":"def evaluate(\n        data_loader,\n        model,\n        device,\n):\n    model.eval()\n    with torch.no_grad():\n        para_loader = pl.ParallelLoader(data_loader, [device])\n        tk0 = para_loader.per_device_loader(device)\n        for b_idx, data in enumerate(tk0):\n            for key, value in data.items():\n                data[key] = value.to(device)\n            predictions, loss = model(**data)\n            predictions = torch.sigmoid(predictions)\n    return loss.item(), predictions\n","cc4eb3f2":"def run(fold):\n    training_data_path = \"..\/input\/siic-isic-224x224-images\/train\/\"\n    df = pd.read_csv(\".\/train_folds.csv\")\n    device = xm.xla_device()\n    epochs = 5\n    train_bs = 32\n    valid_bs = 16\n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n    \n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    model = EfficientNet()\n    model.to(device)\n    \n    train_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0, \n                always_apply=True\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.0625, \n                scale_limit=0.1, \n                rotate_limit=15\n            ),\n            albumentations.Flip(p=0.5)\n        ]\n    )\n\n    valid_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0,\n                always_apply=True\n            )\n        ]\n    )\n    \n    train_images = df_train.image_name.values.tolist()\n    train_images = [\n        os.path.join(training_data_path, i + \".png\") for i in train_images\n    ]\n    train_targets = df_train.target.values\n\n    valid_images = df_valid.image_name.values.tolist()\n    valid_images = [\n        os.path.join(training_data_path, i + \".png\") for i in valid_images\n    ]\n    valid_targets = df_valid.target.values\n\n    train_dataset = ClassificationDataset(\n        image_paths=train_images,\n        targets=train_targets,\n        resize=None,\n        augmentations=train_aug\n    )\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n      train_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=train_bs,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    \n    valid_dataset = ClassificationDataset(\n        image_paths=valid_images,\n        targets=valid_targets,\n        resize=None,\n        augmentations=valid_aug\n    )\n    \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n      valid_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True\n    )\n    \n    \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=valid_bs,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=0\n    )\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        patience=3,\n        threshold=0.001,\n        mode=\"min\"\n    )\n    \n    for epoch in range(epochs):\n        training_loss = train(\n            data_loader=train_loader,\n            model=model,\n            optimizer=optimizer,\n            device=device,\n            scheduler=scheduler,\n        )\n        \n        valid_loss, predictions = evaluate(\n            valid_loader,\n            model,\n            device,\n        \n        )\n        \n        xm.master_print(f\"Epoch = {epoch}, LOSS = {valid_loss}\")\n        gc.collect()\n    ","608a74f4":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run(0)\n    \n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n","6a17ee39":"## Model","b60b3136":"## Dataloader","105b3947":"## Create Kfolds","75119998":"## Train loop","430971e6":"## Eval function","77b16e8b":"## Train function","09338335":"## Import Packages"}}