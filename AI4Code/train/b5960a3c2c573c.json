{"cell_type":{"f64045bd":"code","85feac2d":"code","5d3b0404":"code","578097c0":"code","093d69d2":"code","a7128d4a":"code","ee47d403":"code","02c8167a":"code","0c8f45eb":"code","188ca8a9":"code","5b2f3dc8":"code","4c271f12":"code","b36f8a20":"code","cd10bc18":"code","1bec3649":"code","917db848":"code","ccf446db":"code","925b526a":"code","1f065944":"code","c73dcc71":"code","795c997d":"code","65dde3a6":"code","22d635b5":"code","635e16f5":"code","06b272e4":"code","e6b7270a":"code","50669a78":"code","2fb9ccf9":"code","5b1344cd":"code","4a6a49f3":"code","63d4f0d9":"code","c59f43db":"code","52b213ab":"code","f55b59e5":"code","6ca8ea47":"code","c646c122":"code","08adccf7":"code","45311198":"code","cc2b3f6e":"code","bfe69226":"code","8793d411":"code","d8fc08be":"code","72f9b8e8":"code","b6d141b2":"code","f0e4fc57":"markdown","7975d318":"markdown","4c28a426":"markdown","5f7ff919":"markdown","21e08def":"markdown","61df7100":"markdown","89fcc3cf":"markdown","8c2018df":"markdown","b60ce345":"markdown","2a3e8dc2":"markdown","39db466e":"markdown","7027b677":"markdown","0dc7b35f":"markdown","a34e1528":"markdown","fd154689":"markdown","9ccb3046":"markdown","bb0e02b7":"markdown","87cc2431":"markdown","1c265964":"markdown","a65a351b":"markdown","2d8840a3":"markdown","dde62658":"markdown","2e205c3c":"markdown","10edf1be":"markdown","2c498b7c":"markdown","4829a4ec":"markdown","9792bc8b":"markdown","c6d92c1e":"markdown","6e29ead2":"markdown","a53b0a0f":"markdown","85fe8b0b":"markdown","b065b9c9":"markdown","3bffa556":"markdown","e9641066":"markdown","5ba3d6d7":"markdown","670b8a58":"markdown","96424798":"markdown"},"source":{"f64045bd":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import Imputer \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#importing all the liabriaries we will need to boost our model \nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","85feac2d":"hp_train = pd.read_csv('..\/input\/traincsv\/train.csv')\nhp_test = pd.read_csv('..\/input\/testcsv\/test.csv')\ntrain_length=1460\nsaleprice=pd.DataFrame(hp_train.iloc[:,-1])","5d3b0404":"# #hp_train = pd.read_csv('https:\/\/raw.githubusercontent.com\/K1TS\/mypackage\/master\/train.csv')\n# #hp_test = pd.read_csv('https:\/\/raw.githubusercontent.com\/K1TS\/House-Prices-Advanced-Regression-Techniques\/master\/test.csv')\n#sumple_supmition =pd.read_csv('https:\/\/raw.githubusercontent.com\/K1TS\/House-Prices-Advanced-Regression-Techniques\/master\/sample_submission.csv')","578097c0":"# We are Concatinating the two datasets so it can be easy to clean data at once for future engineering   \ndf_all=pd.concat([hp_train.drop(columns=['SalePrice']), hp_test])","093d69d2":"# The Shape Of The Two DataSets Combined\ndf_all.shape","a7128d4a":"df_all.head()# viewing the dataset to avoid large output space","ee47d403":"#We Decided To Cut The Correlation Graph In Half Since The Upper Part Is Really Repetition Of the Lower Triangle  \ncorr_matrix = hp_train.corr()\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(25, 15))\nsns.heatmap(corr_matrix, mask=mask, vmax=0.8, vmin=0.05, annot=True);\n\n","02c8167a":"#COLUMNS THAT ARE CORRELATED TO THE SALES PRICE AND INVESLY CORRELATED TO SALES PRICE \ndf=pd.DataFrame(hp_train.corr()['SalePrice'].sort_values(ascending=False))\ndf.plot(kind='bar',figsize=(12,5),color='red')\nplt.title('Brief Correlation Graph of Features and Sales Price')\n\n\n","0c8f45eb":"fig, axes = plt.subplots(nrows= 3,ncols = 3, figsize=(20,12))\n\naxes[0,0].scatter(hp_train['YearBuilt'], hp_train['SalePrice'], color='orange')\naxes[0,1].scatter(hp_train['GarageYrBlt'], hp_train['SalePrice'],  color='green')\naxes[0,2].scatter(hp_train['GrLivArea'], hp_train['SalePrice'], color='blue')\n\n\naxes[1,0].scatter(hp_train['TotRmsAbvGrd'], hp_train['SalePrice'],  color='orange')\naxes[1,1].scatter(hp_train['GarageCars'], hp_train['SalePrice'], color='green')\naxes[1,2].scatter(hp_train['GarageArea'], hp_train['SalePrice'],  color='blue')\n\naxes[2,0].scatter(hp_train['TotalBsmtSF'], hp_train['SalePrice'],  color='orange')\naxes[2,1].scatter(hp_train['1stFlrSF'], hp_train['SalePrice'], color='green')\naxes[2,2].scatter(hp_train['OverallQual'], hp_train['SalePrice'],  color='blue')\n\n#Naming Titles Of The Columns  \naxes[0,0].set_title('YearBuilt')\naxes[0,1].set_title('GarageYrBlt')\naxes[0,2].set_title('GrLivArea')\n\naxes[1,0].set_title('TotRmsAbvGrd')\naxes[1,1].set_title('GarageCars')\naxes[1,2].set_title('GarageArea')\n\n\naxes[2,0].set_title('TotalBsmtSF')\naxes[2,1].set_title('1stFlrSF')\naxes[2,2].set_title('OverallQual')","188ca8a9":"#hp_train.isnull().sum().sort_values(ascending = False).head(20)\n# columns that have NaN on the train dataset.\nis_null=df_all.isnull().sum().sort_values(ascending=False)\nNaN_train=(is_null[is_null>0])\ndict(NaN_train)\nNaN_train","5b2f3dc8":"#THIS ARE THE VISUALS  TO SHOW THE MISSING DATA IN OUR DATASET ..THE COLUMNS WE SORTED IN THE Visualization\nplt.figure(figsize=(15, 8))\nsns.barplot(NaN_train,NaN_train.index)\nplt.title('Missing  Data In The Dataset')\n","4c271f12":"\ndf_all[\"PoolQC\"] = df_all[\"PoolQC\"].fillna(\"None\")\n\ndf_all[\"MiscFeature\"] = df_all[\"MiscFeature\"].fillna(\"None\")\n\ndf_all[\"Alley\"] = df_all[\"Alley\"].fillna(\"None\")\n\ndf_all[\"Fence\"] = df_all[\"Fence\"].fillna(\"None\")\n\ndf_all[\"FireplaceQu\"] = df_all[\"FireplaceQu\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_all[col] =df_all[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_all[col] = df_all[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_all[col] = df_all[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_all[col] = df_all[col].fillna('None')\n    \ndf_all[\"MasVnrType\"] = df_all[\"MasVnrType\"].fillna(\"None\")\ndf_all[\"MasVnrArea\"] = df_all[\"MasVnrArea\"].fillna(0)\n\ndf_all['MSZoning'] = df_all['MSZoning'].fillna(df_all['MSZoning'].mode()[0])\n\ndf_all[\"Functional\"] = df_all[\"Functional\"].fillna(\"Typ\")\n\ndf_all['Electrical'] = df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])\n\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(df_all['KitchenQual'].mode()[0])\n\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\n\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])\n\ndf_all['MSSubClass'] = df_all['MSSubClass'].fillna(\"None\")\n","b36f8a20":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n(mu, sigma) = norm.fit(saleprice['SalePrice'])","cd10bc18":"#We Used This Code To Check The skiweness Of The  Salesprice Column \n(mu, sigma) = norm.fit(saleprice['SalePrice'])\nsns.distplot(saleprice['SalePrice'],fit=norm)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')","1bec3649":"quantile_plot=stats.probplot(saleprice['SalePrice'], plot=plt)","917db848":"saleprice[\"SalePrice\"] = np.log1p(saleprice[\"SalePrice\"])\ny=saleprice\ny.head()","ccf446db":"(mu, sigma) = norm.fit(saleprice['SalePrice'])\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(saleprice['SalePrice'],fit=norm)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1, 2, 2)\nquantile_plot=stats.probplot(saleprice['SalePrice'], plot=plt)","925b526a":"fat = 'OverallQual'\ndata = pd.concat([hp_train['SalePrice'], hp_train[fat]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=fat, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.savefig('lethabo.png')","1f065944":"plt.figure(figsize=(10,10))\nsns.boxenplot(df_all[\"LotFrontage\"],df_all[\"Neighborhood\"])","c73dcc71":"df_all=df_all.drop(['GarageYrBlt','TotRmsAbvGrd','GarageArea','PoolQC', 'MiscFeature', 'Fence','MiscVal','PoolArea','Utilities'], axis=1)","795c997d":"df_all['TotalBath'] = df_all['FullBath'] + df_all['HalfBath']*0.5 + df_all['BsmtFullBath'] + df_all['BsmtHalfBath']*0.5\ndf_all['TotalFlrSF'] = df_all['1stFlrSF'] + df_all['2ndFlrSF']\ndf_all['BsmtFinSF'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2']\n# Deleting singulars since we have combined above:\n\nsingles_to_drop = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'BsmtFinSF2']\nfor col in singles_to_drop:\n    df_all.drop([col], axis =1, inplace = True)","65dde3a6":"\ndf_all.head()","22d635b5":"df_all = pd.get_dummies(df_all)\nX_test=df_all.iloc[train_length:,:]\nX_train=df_all.iloc[:train_length,:]\n","635e16f5":"X=X_train","06b272e4":"df_all.head()","e6b7270a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","50669a78":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","2fb9ccf9":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","5b1344cd":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","4a6a49f3":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n","63d4f0d9":"import random\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =random.randint(0,int(2**16)), nthread = -1)\n","c59f43db":"lgb_model = lgb.LGBMRegressor(lcolsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =random.randint(0,int(2**16)), nthread = -1)","52b213ab":"import random\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =random.randint(0,int(2**16)), nthread = -1)\n","f55b59e5":"score = rmsle_cv(lasso)\nprint(\"\\nLASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6ca8ea47":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c646c122":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","08adccf7":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","45311198":"score = rmsle_cv(model_xgb)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","cc2b3f6e":"score = rmsle_cv(lgb_model)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","bfe69226":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","8793d411":"averaged_models = AveragingModels(models = (GBoost, lasso,KRR,model_xgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d8fc08be":"averaged_models.fit(X,y)","72f9b8e8":"y_average=np.expm1(averaged_models.predict(X_test))","b6d141b2":"sample2 =pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsales2=pd.DataFrame(y_average,columns=['SalePrice'])\nsample2['SalePrice']=sales2['SalePrice']\nsample2.head()\nsample2.to_csv('avg6.csv',index=False)","f0e4fc57":"### The Base Models ","7975d318":"Brief Correlation Graph Showing Positive And Negative Correlation ","4c28a426":"### Filling In The Missing Values ","5f7ff919":"### Droping Columns That Have Less Correlation To Sales Price ","21e08def":"Check Correlation Of Columns To The Target Columns To See Which Columns We Should Drop","61df7100":"#### Light Gradient Boosting","89fcc3cf":"* Colums We Droped Successfully On The Dataset\n* If You Take A Look At The Dataset There Is Catergorial Values And Numerical Values And Machine Learning Only Understand Numbers\n* We Will Have To Convert The Catergorial Values To Numerical By Getting Dummie Variables ","8c2018df":"### Now The Data Is Clean We Convert Catergorial Values To Numerical ","b60ce345":"#### Kernel Ridge Regression","2a3e8dc2":"* We Going To Stack All The Models Together So That We Can Improve Our Predictive Score","39db466e":"####  Elastic Net Regression","7027b677":"### This are all  Columns with Null Values In The Train Csv \n","0dc7b35f":"####  GBoost Regression","a34e1528":"#### The Following Two Graphs Shows The Skiewness Of Our Data And how It Looks Like When It Is Normalised  \n* For The Data To Be Normalised The Blue Dots  Have To  Be Close To The Red Line\n* On The Next Plot We Can See The Datapints Are Much Better Then Before","fd154689":"### Analysing The Sales Price \n \n * We Do This Since SalesPrice Is A Target Variable And It Is  What We Want To Predict","9ccb3046":"* We Need To Look At How The Data\/Obsevations Might Fall Closely To The Line\n* We See That Most Of The Deviations Apear Mostly On The Left And The Right \n* This Is A Right Skewed Data Sets Tend To Be Close To Zero And Less Data Points In The Upper Bound \/ As We Get To Higher Values\n* We Then Need To  Apply The Log Transformation  On The Sales Price Column To Make Sure That Atlist Most Of The Datapoints Fall On The Line Of the Q-Q plot\n* we will use log(1+x) to do this","bb0e02b7":"#### Lasso  Regression ","87cc2431":"### Visualization Of the Null Values In our Dataset","1c265964":"### We will Only Avarage Those Whicha Have Good Mean Scores","a65a351b":"### Predictions For Submittions","2d8840a3":"#### Extreme Gradient Boosting XGB","dde62658":"#### XGB","2e205c3c":"### Import The Important Packages For Data Viewing Data Analysis,Data Manipulation,Mechine-Learning ","10edf1be":"* The idea of stacking models just means we take the models we have tested and simply average them together in hopes of attaining a better score\n* Averaged base models class**\n* Averaged base models score**\n* We just average four models here **ENet, GBoost,XGB,KNN,LGB,and lasso**.  Of course we could easily add more models in the mix. ","2c498b7c":"### Visulization Of The Columns With High Correlation ","4829a4ec":"### Box Plot Overallqual\/Saleprice","9792bc8b":"### The Correlation Graphs","c6d92c1e":"### Base Models Scores","6e29ead2":"### Importing Our Datasets","a53b0a0f":"### To Use The Dataset On Your Pc And Not On Kaggle Use Get The Dataset From Github With The Following Links , 'Dont Forget To Remove #' ","85fe8b0b":"### Model Boosting\n* Importing All The Liabriaries We Will Need To Boost Our Model  And The Nesessary Python Liabriries That We Might Need Later ","b065b9c9":"# Team 14 The Catalyst\n  On This Notebook We Will Solve The House Prices Using Advance Regression Techniques\n","3bffa556":"* We Can See That Sales Price Is Not Normally Distributed\n* We Need To See The Realtionship Using A Normal Probabilty Plot\/Normal Quantile Plot","e9641066":"### Cross Validation\nCross Validation\nWe use the cross_val_score function of Sklearn in order for the model to be stacked together","5ba3d6d7":"###                                            Missing Values","670b8a58":"### Model Stacking","96424798":"### The Columns That Are More Correlated With The Target Are Bellow\n* This Columns Are The Columns That Have Same Relationship To Each Other And Are Correlated To The Sales Price column \n* we will Need To Drop 5 Columns In The Below List That Are Related To Each Other Since They Have The Same Information \n\n  * YearBuilt or GarageYrBlt\n  * GrLivArea or TotRmsAbvGrd\n  * GarageCars or GarageArea\n  * TotalBsmtSF or 1stFlrSF\n  * SalePrice or overalqal"}}