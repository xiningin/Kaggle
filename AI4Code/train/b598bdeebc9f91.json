{"cell_type":{"66c24a27":"code","e2f47167":"code","3c71899e":"code","4c6cd141":"code","2bd3a19f":"code","46571d7b":"code","5a7a36df":"code","3a0ba010":"code","20f54d48":"code","5cc5c25d":"code","0f855309":"code","7e11eb13":"code","7446ffa7":"code","f57ca370":"code","b1ff2298":"code","79dec8e0":"code","dd504713":"code","ef5b500b":"code","8417ebff":"code","86d88531":"code","7a89b2e5":"code","5d629645":"code","1ffa4a44":"code","51530bce":"code","d70b4c38":"code","86e2c2be":"code","2039b811":"markdown","c3d6dede":"markdown","87ec7bda":"markdown","e36f6f66":"markdown","715d766e":"markdown","c8a05117":"markdown","8232e013":"markdown","7b7f183a":"markdown","fde1a21a":"markdown","ec7e07ac":"markdown","640e873f":"markdown","fd62f95c":"markdown","836dd08f":"markdown","af0ce877":"markdown"},"source":{"66c24a27":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport nltk\nimport spacy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nfrom functools import partial\nimport re\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport itertools\nimport collections","e2f47167":"pwd","3c71899e":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","4c6cd141":"train_df.shape","2bd3a19f":"train_df.head()","46571d7b":"train_df.isnull().sum()","5a7a36df":"print('Unique values in the training set:')\nfor col in train_df.columns:\n    print(f'{col} : {train_df[col].nunique()}')","3a0ba010":"### Define Paths for Train and Test Json files\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","20f54d48":"### Function to read JSON files and extract publication Text \n\ndef json_to_text(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n            \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = ' '.join(combined)\n    \n    if output=='text':\n        return all_contents\n    elif output=='head':\n        return all_headings\n    else:\n        return all_data","5cc5c25d":"### Extract Publication Text for Training Data\ntqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(json_to_text)","0f855309":"train_df.head()","7e11eb13":"### Reading the Sample Submission Data\n\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\nsample_sub.head()\n### Extract Publication Text for the sample publications \nsample_sub['text'] = sample_sub['Id'].apply(partial(json_to_text,train_files_path=test_files_path))","7446ffa7":"#### Set size for sns plots\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\n## Plot top 15 popular datasets\ntrain_df.dataset_label.value_counts()[:15].plot(kind='bar',title='Famous Datasets',color = sns.color_palette(\"husl\", 8))","f57ca370":"((train_df.dataset_label.value_counts()\/train_df.dataset_label.shape[0])*100)[:15]","b1ff2298":"def lemmatization(text):\n\n    doc = nlp(text)\n    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n    return ' '.join(lemma_list)\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ntrain_df['text'] = train_df['text'].progress_apply(clean_text)\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\nstop_words = stopwords.words('english')\n\ntry:\n    train_df['text'] = train_df['text'].progress_apply(lemmatization)\nexcept:\n    pass","79dec8e0":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ndocs = train_df['text'].tolist()\n\n#Ignore words that appear in 85% texts, \ncv = CountVectorizer(max_df=0.85, stop_words=stop_words, max_features=60000)\nword_count_vector = cv.fit_transform(docs)\n\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)\nfeature_names = cv.get_feature_names()\n\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","dd504713":"%%time\n\nIds = train_df.Id.tolist()\nkeyword_df = pd.DataFrame()\n\nfor i in range(len(docs)):\n    doc = docs[i]\n    Id = Ids[i]\n    tfidf_vector = tfidf_transformer.transform(cv.transform([doc]))\n\n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tfidf_vector.tocoo())\n\n    #extract only the top n; n here is 10\n    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n\n    temp_df = pd.DataFrame()\n    temp_df['keyword'] = keywords\n    temp_df['weight'] = keywords.values()\n    temp_df['id']=Id\n    keyword_df = keyword_df.append(temp_df)\n\n### We have a DataFrame with Keywords for each article and its keywords with their weights\nkeyword_df[['id','keyword','weight']].head()","ef5b500b":"keyword_df.groupby('keyword')['weight'].sum().sort_values(ascending=False)[:15].plot(\n    kind='bar',title='Keywords with Top Weight',color = sns.color_palette(\"husl\", 8))","8417ebff":"train_df.dataset_label.value_counts()[:15]","86d88531":"keyword_df.groupby('keyword')['weight'].sum().sort_values(ascending=False)[:15]","7a89b2e5":"keyword_df.groupby('keyword')['keyword'].count().sort_values(ascending=False)[:15]","5d629645":"keyword_df.groupby(['id','keyword'])['weight'].sum().sort_values(ascending=False)[:20]","1ffa4a44":"### Looking at the keywords from the first article\n\ndoc = docs[0]\ntfidf_vector = tfidf_transformer.transform(cv.transform([doc]))\n\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tfidf_vector.tocoo())\n\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,20)\n\nprint(train_df.cleaned_label[0])\n# now print the results\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])","51530bce":"train_df.text.str.len().plot(kind='hist')","d70b4c38":"## Function to remove outlier article lenghts\n\ndef get_iqr(df):\n    df['text_len'] = df['text'].str.len()\n    sorted_len = np.sort(df['text_len'])\n    Q1,Q3 = np.percentile(sorted_len , [25,75])\n    IQR = Q3-Q1\n    upper_range = Q3 + (1.5 * IQR)\n    return int(upper_range)\n\ndf_eda = train_df.copy()\nupper_limit = get_iqr(df_eda)\nadjusted_len = train_df.text.str.slice(0,upper_limit)\nsns.histplot(data=adjusted_len.str.len())","86e2c2be":"### Save Cleaned Train file and Keywords to csv for quick reference\ntrain_df.to_csv('.\/train_df_cleaned.csv')\nkeyword_df.to_csv('.\/keywords.csv')\n#keyword_df = pd.read_csv('..\/input\/coleridgetrainkeywords\/keywords.csv')","2039b811":"### Import Libraries","c3d6dede":"- Kids and Education seems to be the most common theme -  (Student, School, Children, Teachers in top Keywords)\n- cov and covid are in top keywords, reflects upon the research on the Coronavirus\n- Keywords \u201cEt al.\u201d is short for the Latin term \u201cet alia,\u201d meaning \u201cand others.\u201d It is used in academic citations when referring to a source with multiple authors","87ec7bda":"## Identify Most popular\/cited Datasets in the Training Data","e36f6f66":"- Some Articles are really huge, need to adjust the outliers to get a sense of the distribution","715d766e":"### Identify Keywords for a specific Article","c8a05117":"#### Import Training Data","8232e013":"#### One of the most Important steps before any keyword identification process is Text Cleaning to avoid GIGO (Garbage In Garbage Out).\n- Lemmatize Text to bring the word to its base form and hence removing redundant words from our vocabulary","7b7f183a":"### Publications length follows a right skewed Normal Distribution, with Median around 25000 words, and exceptions with 80K+ words as well.","fde1a21a":"### How does the Article length Distribution Looks like? Pretty \"Normal\"","ec7e07ac":"## Identify Top Keywords in the entire Training Corpus","640e873f":"- We can look at the percentage distribution of cited Datasets","fd62f95c":"Reference :https:\/\/www.kaggle.com\/manabendrarout\/tabular-data-preparation-basic-eda-and-baseline\n           https:\/\/kavita-ganesan.com\/extracting-keywords-from-text-tfidf\/#.YIQBCpAzaUk\n","836dd08f":"## Coleridge Starter EDA\n- Identify Most popular\/cited Datasets in the Training Data\n- Identify Most important words in a Dataset\n- Identify Top Keywords in the entire Training Corpus\n\n### Addon: Identify a Normal Distribution in the Dataset","af0ce877":"- Alzheimers Accounts for roughly 30% of entire labels. "}}