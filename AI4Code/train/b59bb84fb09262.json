{"cell_type":{"17d764b8":"code","9e4f6745":"code","b0cdabbf":"code","69b87b63":"code","1c536f4c":"code","d6e9de9e":"code","a117e3b5":"code","cfd1a3ab":"code","9b5c0d7f":"code","178ee081":"code","b70a4ca9":"code","45bcad81":"markdown","2bbff092":"markdown","89c3d9ab":"markdown","c7987218":"markdown","6833723f":"markdown","ef383c7e":"markdown","294c63c6":"markdown","2ea7e439":"markdown","9049af02":"markdown"},"source":{"17d764b8":"import numpy as np # linear algebra\n\nimport matplotlib.pyplot as plt # display images\n\nimport pandas as pd # data processing\n\nfrom keras.models import Model # neural network generation, training and fitting \nfrom keras.layers import Dense, Input\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import SGD","9e4f6745":"def organize_data(X):    \n    max_non_null_terms = 0\n    num_rows = X.shape[0]\n\n    # Sort each row in descending order.  \n    for i in range(0,num_rows):\n        temp = np.array(sorted(X[i,:],reverse=True))\n        X[i,:] = temp\n        # We count the number of non null values in each row and keep the largest.\n        non_null_terms = np.sum(X[i,:] > 0)\n        if non_null_terms > max_non_null_terms:\n            max_non_null_terms = non_null_terms\n    \n    # After sorting we \"cut\" the last columns which contains only null terms.\n    X = X[:,0:max_non_null_terms]\n    \n    return X","b0cdabbf":"def normalize_data(X):   \n    num_rows = X.shape[0]\n    num_cols = X.shape[1]\n    X_normalized = np.zeros((num_rows,num_cols), dtype = np.float32)\n    for i in range(0,num_rows):\n        for j in range(0,num_cols):\n            if X[i,j] > 0:\n                X_normalized[i,j] = np.log10(X[i,j])\n    \n    return X_normalized","69b87b63":"# load train dataset\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")        \ntrain_val = train_data.values\nY = train_val[:,1].astype(np.float32)\nX = train_val[:,2:].astype(np.float32)\n# load test dataset\ntest_data = pd.read_csv(\"..\/input\/test.csv\")        \ntest_val = test_data.values\nX_test = test_val[:,1:].astype(np.float32)","1c536f4c":"# stack vertically the train and test datasets\nX_all = np.vstack((X, X_test))\n# organize data\nX_all = organize_data(X_all)\nnum_features = X_all.shape[1]\n# normalize data\nX_all = normalize_data(X_all)\nY = Y.reshape(Y.shape[0],1)\nY_normalized = normalize_data(Y)","d6e9de9e":"# add new features\nnum_new_features = 5\nX_add = np.zeros((X_all.shape[0],num_features + num_new_features), dtype = np.float32)\nfor i in range(0,X_all.shape[0]):\n    X_add[i,0] = np.mean(X_all[i,X_all[i,:]!=0])\n    X_add[i,1] = np.min(X_all[i,X_all[i,:]!=0])\n    X_add[i,2] = np.max(X_all[i,:])\n    X_add[i,3] = np.sum(X_all[i,:] > 0)\n    X_add[i,4] = np.std(X_all[i,:])\nX_add[:,5:] = X_all\n\n# update number of features\nnum_features = num_features + num_new_features","a117e3b5":"# split this new dataset in train and test datasets to be used in training and prediction stages\nX_train_final = X_add[0:X.shape[0],:]\nX_test_final = X_add[X.shape[0]:,:]","cfd1a3ab":"# Check learning plots to see if the model is overfitting or not, if it is learning or not. \n# Now it is the time we choose the best parameters.\nfor cycles in [1,5,10]:\n    for neurons in [10,50,100,150]:\n        # input layer\n        visible = Input(shape=(num_features,))\n        # first feature extractor\n        hid1 = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\n        hid2 = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\n        hidpar2 = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\n        hid3 = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\n        hidpar3 = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\n        # second feature extractor\n        hid1_ = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\n        hid2_ = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\n        hidpar2_ = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\n        hid3_ = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\n        hidpar3_ = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\n        # interpretation layer\n        feedback_hid = Dense(num_features, kernel_initializer='normal', activation = \"relu\")\n    \n        x = visible\n        L = []\n        LP = []\n        for i in range(0,cycles):\n            # first path (L = layer)\n            L.append(hid1(x))\n            L.append(hid2(L[0]))\n            L.append(hidpar2(L[1]))\n            L.append(hid3(L[2]))\n            L.append(hidpar3(L[3]))\n            L.append(concatenate([L[3],L[4]]))\n            # second path (LP = layer in parallel)\n            LP.append(hid1_(x))\n            LP.append(hid2_(LP[0]))\n            LP.append(hidpar2_(LP[1]))\n            LP.append(hid3_(LP[2]))\n            LP.append(hidpar3_(LP[3]))\n            LP.append(concatenate([LP[3],LP[4]]))\n            # merge both paths\n            final_merge = concatenate([L[-1],LP[-1]])        \n            x = feedback_hid(final_merge)\n        \n        # prediction output\n        master = Dense(neurons, kernel_initializer='normal', activation='tanh')(x)\n        output = Dense(1, kernel_initializer='normal', activation='softplus')(master)\n        model = Model(inputs=visible, outputs=output)\n        \n        # compile the network\n        sgd = SGD(lr=0.01, momentum=0.1, decay=0.0, nesterov=False)\n        model.compile(loss='mean_squared_error', optimizer=sgd)\n\n        # fit the model\n        history = model.fit(X_train_final, Y_normalized,validation_split=0.33, epochs=150, batch_size=100, verbose=0)\n        print('Cycles =',cycles)\n        print('Neurons =', neurons)\n        # show some information about the predictions\n        print('\\nLoss of predictions in train dataset:')\n        predictions = model.predict(X_train_final)\n        # Transform predictions to original format\n        predictions = 10**predictions\n        print( np.sqrt(1\/Y.shape[0])*np.linalg.norm((np.log(predictions+1)-np.log(Y+1)),2) )\n\n        # summarize history for loss\n        plt.plot(history.history['loss'][2:])\n        plt.plot(history.history['val_loss'][2:])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()","9b5c0d7f":"# final model\ncycles = 5\nneurons = 150\n\n# input layer\nvisible = Input(shape=(num_features,))\n# first feature extractor\nhid1 = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\nhid2 = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\nhidpar2 = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\nhid3 = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\nhidpar3 = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\n# second feature extractor\nhid1_ = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\nhid2_ = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\nhidpar2_ = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\nhid3_ = Dense(neurons, kernel_initializer='normal', activation = \"tanh\")\nhidpar3_ = Dense(neurons, kernel_initializer='normal', activation = \"relu\")\n# interpretation layer\nfeedback_hid = Dense(num_features, kernel_initializer='normal', activation = \"relu\")\n# master layer\nmast = Dense(neurons, kernel_initializer='normal', activation='tanh')\n# output layer\nout = Dense(1, kernel_initializer='normal', activation='softplus') \n    \nx = visible\nL = []\nLP = []\nfor i in range(0,cycles):\n    # first path (L = layer)\n    L.append(hid1(x))\n    L.append(hid2(L[0]))\n    L.append(hidpar2(L[1]))\n    L.append(hid3(L[2]))\n    L.append(hidpar3(L[3]))\n    L.append(concatenate([L[3],L[4]]))\n    # second path (LP = layer in parallel)\n    LP.append(hid1_(x))\n    LP.append(hid2_(LP[0]))\n    LP.append(hidpar2_(LP[1]))\n    LP.append(hid3_(LP[2]))\n    LP.append(hidpar3_(LP[3]))\n    LP.append(concatenate([LP[3],LP[4]]))\n    # merge both paths\n    final_merge = concatenate([L[-1],LP[-1]])        \n    x = feedback_hid(final_merge)\n        \n# prediction output\nmaster = mast(x)\noutput = out(master)\nmodel = Model(inputs=visible, outputs=output)\n        \n# compile the network\nsgd = SGD(lr=0.01, momentum=0.1, decay=0.0, nesterov=False)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)","178ee081":"# fit the model to make predictions over the test dataset\nhistory = model.fit(X_train_final, Y_normalized, epochs=150, batch_size=100, verbose=2)\n\n# since X has more columns than X_test, we fill X_test with more null columns\npredictions = model.predict(X_test_final)\n\n# We plot some histogram to visualize the distribution of the predictions and make some comparisons. We expect\n#that the histogram of the predictions are similar to the histogram of the outputs in the train dataset.\npredictions = 10**predictions\n\nprint('Train dataset outputs.')\nplt.hist(np.log10(Y),bins=100)\nplt.show()\n\nprint('Predictions of the train dataset outputs.')\nplt.hist(np.log10(predictions),bins=100)\nplt.show()","b70a4ca9":"# Save these predictions.\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission[\"target\"] = predictions\nsubmission.to_csv('submission.csv', index=False)\nprint(submission.head())","45bcad81":"The input data is very sparse, being zero almost everywhere. Also we have much more features than instances, so we will have to deal with the [curse of dimensionality](https:\/\/en.wikipedia.org\/wiki\/Curse_of_dimensionality). These two things indicates we will have to deal with little information in a large dataset. In order to have more useful information about the dataset we discard these features by considering the row elements in descending order, so the features are *larger element, second larger element,* and so on. \n\nBy doing this, we reduce the number of column from 4993 to 1994, which is a big reduction of dimension.","2bbff092":"# Optimizing the parameters\n\nWe can make the loss function equals the metric RMSLE. In this case we don't need the metric parameter, i.e., the loss is the metric too.\n\nNow it is the time to choose the number of neurons and cycles in our network (the number of cycles is the number of times the proccess described above is repeated, the computing and feedbacking proccess). We do this by making a search in grid on the number of neurons and cycles. The activation functions were choosed after a lot of trial and error, but you can try your own in order to improve the model. \n\nWe split the train dataset in training set ($67\\%$) and test set ($33\\%$) in order to validate the model. The learning curves are plotted and from them we can choose the best model.  I will compute just a few models here due to the time limitation. You are welcomed to check [GitHub](https:\/\/github.com\/felipebottega\/Machine-Learning-Codes) version of this code to see more details.","89c3d9ab":"# New features\n\nWe also introduce a few more features to have more information in the input dataset. These are showed below.","c7987218":"# Neural network model description\n\nWe will use deep learning to solve this problem, which seems to be a very difficult nonlinear regression problem. Our model is a neural network with dense layers and concatenation. This model explores a new idea I had of reusing layers, interpreting them as someone who makes a job and right away receives feedback in order to redo the job better. This feedback is given by the **feeedback_hid** layer. This layer analyzes what the previous layers computed and produces a new input for these layers. This proccess of making computations over the input, analyzing the computations and producing new inputs is done several times. The idea is that the **feedback_hid** layer will simplify the input, keeping what is important for the *workers* (the previous hidden layers). After all this, the final input is passed to the **master** layer, which is a layer supposedly smarter. This layer is the last one before the output and the information goes through it only one time.","6833723f":"# Preparing the data","ef383c7e":"For **cycles** $\\geq 10$ we can see the model starting to overfit. In the other cases the test loss curve (orange curve) has a lot of peaks, which means the model can make big errors sometimes. The only exception to this is when we have **cycles** $=5$ and **neurons** $=150$. This seems to be the best option.","294c63c6":"# **Overview**\n\nThis notebook describes an experimental type of neural network, which was used in this competition and resulted in reasonabled scores. It should be noted that I didn't use any special \"magic feature\" or some deep data analysis. In fact, I totally disregard this data comes from a time series and just sort the inputs in descending order. By doing this we lose information but dramatically decreases the number of features. Even doing this it is still possible to make good predictions. All merits goes to this different neural network structure, which was able to make decent predictions all by itself.","2ea7e439":"# Fitting and predicting\n\nThe array **predictions** (below) was obtained using normalized data. The original format should be $10$ to the power of **predictions**. ","9049af02":"# Model illustration\n\n![](https:\/\/i.imgur.com\/EcgCS26.png)"}}