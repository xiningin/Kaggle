{"cell_type":{"af373042":"code","291852b4":"code","0962aea5":"code","1d554df3":"code","cf6c1a48":"code","06627d6e":"code","e0808e6e":"code","fa864a16":"code","8b85118a":"code","9b0866ff":"code","32837e9a":"code","823de642":"code","18126777":"code","10d6449a":"code","3dcb1b3b":"code","9a15c681":"code","43938dd8":"code","1dfa86e4":"code","1043570d":"code","50c018da":"code","186b069c":"code","1fb18e40":"code","31103bea":"code","6eb3844f":"markdown","7d1e1218":"markdown","2f101f2f":"markdown","63411db3":"markdown","b7c3a24c":"markdown","520d1d91":"markdown","bc79d0af":"markdown","acf189a9":"markdown","83d9502b":"markdown"},"source":{"af373042":"import pandas as pd\nimport numpy as np","291852b4":"data = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndata.head()","0962aea5":"data.info()","1d554df3":"data.isnull().sum()","cf6c1a48":"data.Potability.value_counts()\n","06627d6e":"data= data.dropna()","e0808e6e":"data.Potability.value_counts()\n","fa864a16":"notpotable  = data[data['Potability']==0]\npotable = data[data['Potability']==1]  \n\nfrom sklearn.utils import resample\ndf_minority_upsampled = resample(potable, replace = True, n_samples = 1200) \n\nfrom sklearn.utils import shuffle\ndata = pd.concat([notpotable, df_minority_upsampled])\ndata = shuffle(data) \n","8b85118a":"data.shape","9b0866ff":"data.Potability.value_counts()\n","32837e9a":"import matplotlib.pyplot as plt\nimport seaborn as sns","823de642":"corrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n","18126777":"plt.figure(figsize = (15,9))\nsns.heatmap(data.corr(), annot = True)\n","10d6449a":"corr = data.corr()\ncorr[\"Potability\"].sort_values(ascending=False)\n","3dcb1b3b":"g = sns.FacetGrid(data, col='Potability')\ng.map(plt.hist, 'Solids', bins=25)\n","9a15c681":"cols = ['Solids', 'Turbidity', 'Chloramines', 'ph','Trihalomethanes','Hardness','Sulfate'\n        ,'Conductivity','Organic_carbon']\nsns.pairplot(data[cols])\n","43938dd8":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n","1dfa86e4":"x = data.drop(['Potability'], axis = 1)\ny = data['Potability']\n","1043570d":"from sklearn.preprocessing import StandardScaler\nst = StandardScaler()\ncol= x.columns\nx[col] = st.fit_transform(x[col])\nx[col]","50c018da":"X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.1)\n","186b069c":"knn = KNeighborsClassifier()\n\ndt = DecisionTreeClassifier()\n\nrf = RandomForestClassifier()\n\nada = AdaBoostClassifier()\n\nxgb =XGBClassifier(eval_metric = 'logloss', use_label_encoder=False)\n\n\npara_knn = {'n_neighbors':np.arange(1, 50)}  #parameters of knn\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5) #search knn for 5 fold cross validation\n\n#Decision Tree\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5) #grid search decision tree for 5 fold cv\nparams_rf = {'n_estimators':[100,200, 350, 500], 'min_samples_leaf':[2, 10, 30]}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)\n\n#AdaBoost\nparams_ada = {'n_estimators': [50,100,250,400,500,600], 'learning_rate': [0.2,0.5,0.8,1]}\ngrid_ada =  GridSearchCV(ada, param_grid=params_ada, cv=5)\n\n#XGBoost\nparams_xgb = {'n_estimators': [50,100,250,400,600,800,1000], 'learning_rate': [0.2,0.5,0.8,1]}\nrs_xgb =  RandomizedSearchCV(xgb, param_distributions=params_xgb, cv=5)\n","1fb18e40":"grid_knn.fit(X_train, Y_train)\ngrid_dt.fit(X_train, Y_train)\ngrid_rf.fit(X_train, Y_train)\ngrid_ada.fit(X_train, Y_train)\nrs_xgb.fit(X_train, Y_train)\n\nprint(\"Best parameters for KNN:\", grid_knn.best_params_)\nprint(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\nprint(\"Best parameters for Random Forest:\", grid_rf.best_params_)\nprint(\"Best parameters for AdaBoost:\", grid_ada.best_params_)\nprint(\"Best parameters for XGBoost:\", rs_xgb.best_params_)\n","31103bea":"models = [('K Nearest Neighbours', knn),('Decision Tree', dt), ('Random Forest', rf), ('AdaBoost', ada),\n          ('XGBoost', xgb)]\n\n\nfor model_name, model in models:\n \n    model.fit(X_train, Y_train)    \n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(Y_test,y_pred)\n    print('{:s} : {:.2f}'.format(model_name, accuracy))\n","6eb3844f":"as we notice that we have a missing values in 3 features","7d1e1218":"#### Now data is ready to split.","2f101f2f":"Solids have the highest correlation.","63411db3":"#### Applying StandardScaler befor fitting ML model to normalize the features.","b7c3a24c":"### Finally the best Model we can apply is RandomForest with accuracy 88%. ","520d1d91":"### 1. Import libraries","bc79d0af":"## 3. Apply ML algorithms ","acf189a9":"## 2. EDA","83d9502b":"#### Not potable is much more potable(1200 > 811) so we need to balance the data to prevent  bias.\n"}}