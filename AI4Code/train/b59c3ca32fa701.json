{"cell_type":{"ed9d9360":"code","078092ec":"code","b37cf383":"code","ee2dd67f":"code","e9ba0037":"code","c90f21cc":"code","74e56151":"code","ab76b647":"code","1a558e38":"code","d7ae1e04":"code","197943a5":"code","0973afda":"code","716382a8":"code","723490a2":"code","809752dd":"code","fa79f31f":"code","09865162":"code","3077bf8a":"code","a091440c":"code","3b323d2b":"code","9aaa2dbf":"code","7ab8adab":"code","056ba42d":"code","ea2b5d00":"code","75eea063":"code","a6395415":"code","7563e746":"code","c21f495a":"code","af530bd8":"code","99804f6f":"code","2eeb6fb4":"code","cdfb8331":"code","f78a3c34":"code","1e2782f7":"code","f1ad9a4b":"code","26aaa7a7":"code","cf3c5148":"code","f782ffcf":"code","ac633f42":"code","d12cf60e":"code","ff1a98cf":"code","771f701c":"code","f8d3b93b":"code","8de58f76":"code","b90c87fb":"code","49738d44":"code","8c16aa3f":"code","7d454266":"markdown","88f4dc10":"markdown","801c59de":"markdown","bdbbc074":"markdown","640af243":"markdown","97d95f76":"markdown","edd1f4eb":"markdown","1c18f80f":"markdown","5e370267":"markdown","a579cf7f":"markdown","03cb849d":"markdown","0d7fd77d":"markdown","6f27c149":"markdown","a6589c02":"markdown","b6f893e0":"markdown","5f2026fa":"markdown","946eea11":"markdown","424b583f":"markdown","b11913e6":"markdown","9600abc7":"markdown","fde5871b":"markdown","f2ca1a08":"markdown","7715e085":"markdown","53e30438":"markdown","89a313c8":"markdown","e10aa1c5":"markdown","88072f8e":"markdown","682cadc6":"markdown","6805c60b":"markdown","d1c56416":"markdown","37018ea8":"markdown","88f9a2ed":"markdown","261df133":"markdown","4a72b1b2":"markdown","fd05ffef":"markdown","fe485c2f":"markdown","15b0a541":"markdown","23036dfd":"markdown","4fa5185a":"markdown","7a6b3583":"markdown","90632bbc":"markdown","d8f2d737":"markdown","13615307":"markdown","f6050dbd":"markdown","ff171e88":"markdown","a735582b":"markdown","138e8b96":"markdown"},"source":{"ed9d9360":"# Installation of auxiliary libraries\n!pip install missingno","078092ec":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# graphics import\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Natural language tool kits\nimport nltk\nfrom nltk import FreqDist, ngrams\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n# download stopwords\nnltk.download('stopwords')\n\n\n# string operations\nimport string \nimport re\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import f1_score\n\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.classifier import ROCAUC\nfrom yellowbrick.classifier import PrecisionRecallCurve\nfrom yellowbrick.model_selection import FeatureImportances\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.text import TSNEVisualizer\nfrom yellowbrick.contrib.classifier import DecisionViz\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n\nfrom lime.lime_text import LimeTextExplainer\n\n# Changing the number of characters displayed in pandas \npd.options.display.max_colwidth = 150\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b37cf383":"# Import database\ndf = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n# submission file\ndf_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n# Displaying database's sample \ndf.head()","ee2dd67f":"df_sub.head()","e9ba0037":"df.info()","c90f21cc":"msno.matrix(df)","74e56151":"df['text_len'] = df['text'].str.len()\n\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['text_len'])\nax.set(xlabel='Text length', ylabel='Freq.')\nplt.show()","ab76b647":"fig = plt.figure(figsize=(15,5))\nax = sns.distplot(df[df['target'] == 1]['text_len'])\nax = sns.distplot(df[df['target'] == 0]['text_len'])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\nplt.show()","1a558e38":"df['word_count'] = df['text'].str.split().map(lambda x:len(x))\n\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['word_count'])\nax.set(xlabel='Word count', ylabel='Freq.')\nplt.show()","d7ae1e04":"fig = plt.figure(figsize=(15,5))\nax = sns.distplot(df[df['target'] == 1]['word_count'])\nax = sns.distplot(df[df['target'] == 0]['word_count'])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\nplt.show()","197943a5":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df['text'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","0973afda":"vectorizer = CountVectorizer(ngram_range=(2, 2))\ndocs       = vectorizer.fit_transform(df['text'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","716382a8":"fig, axes = plt.subplots(1, 2, figsize=(15, 8))\nsns.barplot(df['target'].value_counts().index,df['target'].value_counts(), ax=axes[0])\n\naxes[1].pie(df['target'].value_counts(),\n            autopct='%1.2f%%',\n            explode=(0.05, 0),\n            startangle=60)\n\nplt.show()","723490a2":"df_real = df[df['target']==1]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","809752dd":"df_fake = df[df['target']==0]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_fake))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","fa79f31f":"df = df[['text', 'target']]\n# applying in submission dataset\ndf_sub = df_sub[['id','text']]\n\n\ndf.head(20)","09865162":"df['text_lw'] = df['text'].str.lower()\n# applying in submission dataset\ndf_sub['text_lw'] = df_sub['text'].str.lower()\n\n\ndf[['text','text_lw']].head(10)","3077bf8a":"def clean_text(text):\n    # remove \n    text = re.sub('\\[.*?\\]', '', text)\n    # remove links\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    # remove tags\n    text = re.sub('<.*?>+', '', text)\n    # remove punctuation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # remove breaklines\n    text = re.sub('\\n', '', text)\n    # remove numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    # transform text into token\n    text_token = nltk.word_tokenize(text)\n    \n    # remove stopwords\n    words = [w for w in text_token if w not in stopwords.words('english')]\n    \n    \n    return ' '.join(words)","a091440c":"df['text_cl'] = df['text_lw'].apply(clean_text)\n\n# applying in submission dataset\ndf_sub['text_cl'] = df_sub['text_lw'].apply(clean_text)\n\n\ndf[['text','text_lw','text_cl']].head(20)","3b323d2b":"lemmatizer = WordNetLemmatizer() \n\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\ndef lemmatize_sentence(sentence):\n    #tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            if len(word) > 2:\n                lemmatized_sentence.append(word)\n        else:        \n            #else use the tag to lemmatize the token\n            lemma = lemmatizer.lemmatize(word, tag)\n            if len(lemma) > 2:\n                lemmatized_sentence.append(lemma)\n    return \" \".join(lemmatized_sentence)","9aaa2dbf":"df['text_lm'] = df['text_cl'].apply(lemmatize_sentence)\n\n# applying in submission dataset\ndf_sub['text_lm'] = df_sub['text_cl'].apply(lemmatize_sentence)\n\n\ndf[['text','text_lw','text_cl', 'text_lm']].head(20)","7ab8adab":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\n\ndf['text_len'] = df['text'].str.len()\nplt.figure(figsize=(15,5))\naxes[0].set_title('Before transformation')\nax=sns.distplot(df['text_len'], ax=axes[0])\naxes[0].set(xlabel='Text length', ylabel='Freq.')\n\ndf['text_len'] = df['text_lm'].str.len()\nplt.figure(figsize=(15,5))\naxes[1].set_title('After transformation')\nsns.distplot(df['text_len'], ax=axes[1])\naxes[1].set(xlabel='Text length', ylabel='Freq.')\nplt.show()","056ba42d":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\ndf['text_len'] = df['text'].str.len()\naxes[0].set_title('Before transformation')\nax = sns.distplot(df[df['target'] == 1]['text_len'], ax=axes[0])\nax = sns.distplot(df[df['target'] == 0]['text_len'], ax=axes[0])\nax.set(xlabel='Text length', ylabel='Freq.')\n\ndf['text_len'] = df['text_lm'].str.len()\naxes[1].set_title('After transformation')\nax = sns.distplot(df[df['target'] == 1]['text_len'], ax=axes[1])\nax = sns.distplot(df[df['target'] == 0]['text_len'], ax=axes[1])\nax.set(xlabel='Text length', ylabel='Freq.')\n\n\nfig.legend(labels=['Real','Fake'])\nplt.show()","ea2b5d00":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\ndf['word_count'] = df['text'].str.split().map(lambda x:len(x))\naxes[0].set_title('Before transformation')\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['word_count'], ax=axes[0])\nax.set(xlabel='Word count', ylabel='Freq.')\n\ndf['word_count'] = df['text_lm'].str.split().map(lambda x:len(x))\naxes[1].set_title('After transformation')\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['word_count'], ax=axes[1])\nax.set(xlabel='Word count', ylabel='Freq.')\n\n\nplt.show()","75eea063":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\ndf['word_count'] = df['text'].str.split().map(lambda x:len(x))\naxes[0].set_title('Before transformation')\nax = sns.distplot(df[df['target'] == 1]['word_count'], ax=axes[0])\nax = sns.distplot(df[df['target'] == 0]['word_count'], ax=axes[0])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\n\ndf['word_count'] = df['text_lm'].str.split().map(lambda x:len(x))\naxes[1].set_title('After transformation')\nax = sns.distplot(df[df['target'] == 1]['word_count'], ax=axes[1])\nax = sns.distplot(df[df['target'] == 0]['word_count'], ax=axes[1])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\n\n\nplt.show()","a6395415":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df['text_lm'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","7563e746":"vectorizer = CountVectorizer(ngram_range=(2, 2))\ndocs       = vectorizer.fit_transform(df['text_lm'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","c21f495a":"fig, axes = plt.subplots(1, 2, figsize=[20, 10])\n\ndf_real = df[df['target']==1]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[0].imshow(wordcloud1)\naxes[0].axis('off')\naxes[0].set_title('Before transformation')\n\ndf_real = df[df['target']==1]['text_lm']\n\nwordcloud2 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[1].imshow(wordcloud2)\naxes[1].axis('off')\naxes[1].set_title('After transformation')\n\nplt.show()","af530bd8":"fig, axes = plt.subplots(1, 2, figsize=[20, 10])\n\ndf_real = df[df['target']==0]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[0].imshow(wordcloud1)\naxes[0].axis('off')\naxes[0].set_title('Before transformation')\n\ndf_real = df[df['target']==0]['text_lm']\n\nwordcloud2 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[1].imshow(wordcloud2)\naxes[1].axis('off')\naxes[1].set_title('After transformation')\n\n\nplt.show()","99804f6f":"X_train, X_test, y_train, y_test = train_test_split(df[['text_lm']], df['target'], test_size=0.20, random_state=42, stratify=df['target'])","2eeb6fb4":"vectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train['text_lm'])\nX_test_vec = vectorizer.transform(X_test['text_lm'])\n\nX_sub = vectorizer.transform(df_sub['text_lm'])\n","cdfb8331":"# Transform into dataframe\nX_train_vec_df = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X_train_vec.toarray())\nX_test_vec_df = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X_test_vec.toarray())\n\nX_sub_vec_df = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X_sub.toarray())\n\n# some sample\nX_train_vec_df[(X_train_vec_df['get']>0) | (X_train_vec_df['like'] > 0) | (X_train_vec_df['fire'] > 0)][['get', 'like', 'fire']].head(10)","f78a3c34":"tsne = TSNEVisualizer()\ntsne.fit(X_train_vec_df, y_train)\ntsne.show()","1e2782f7":"clf = DummyClassifier().fit(X_train_vec_df, y_train)\ny_pred = clf.predict(X_test_vec_df)\nvisualizer = ConfusionMatrix(clf, percent=True)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()\n\nprint(f'Acc = {accuracy_score(y_test, y_pred)}')\nprint(f'F1 = {f1_score(y_test, y_pred)}')","f1ad9a4b":"visualizer = ROCAUC(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()\n","26aaa7a7":"visualizer = PrecisionRecallCurve(clf)\nvisualizer.fit(X_test_vec_df, y_test)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","cf3c5148":"visualizer = ClassificationReport(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","f782ffcf":"clf = MultinomialNB().fit(X_train_vec_df, y_train)\nvisualizer = ConfusionMatrix(clf, percent=True)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()\n\ny_pred = clf.predict(X_test_vec_df)\nprint(f'Acc = {accuracy_score(y_test, y_pred)}')\nprint(f'F1 = {f1_score(y_test, y_pred)}')","ac633f42":"visualizer = ROCAUC(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","d12cf60e":"visualizer = PrecisionRecallCurve(clf)\nvisualizer.fit(X_train_vec_df, y_train)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","ff1a98cf":"visualizer = ClassificationReport(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","771f701c":"c = make_pipeline(vectorizer, clf)\nexplainer = LimeTextExplainer(class_names=['Fake', 'Real'])","f8d3b93b":"idx = 2\n\ns = X_test.iloc[idx]['text_lm']\nprint('Original:', df.loc[7515]['text'])\nprint('Treated:', s)\n\nprint(f'Outcome {y_test.iloc[idx]}')\n\n\nexp = explainer.explain_instance(s, c.predict_proba)\nexp.show_in_notebook()","8de58f76":"idx = 3\n\ns = X_test.iloc[idx]['text_lm']\nprint('Original:', df.loc[1294]['text'])\nprint('Treated:', s)\n\nprint(f'Outcome {y_test.iloc[idx]}')\n\n\nexp = explainer.explain_instance(s, c.predict_proba)\nexp.show_in_notebook()","b90c87fb":"visualizer = DiscriminationThreshold(clf)\nvisualizer.fit(X_test_vec_df, y_test)\nvisualizer.show() ","49738d44":"thr = 0.40\n\nbest_f1 = 0\nbest_thr = 0\n\ny_pred_proba = clf.predict_proba(X_test_vec_df)[:, 1]\ny_pred = [1 if y >= thr else 0 for y in y_pred_proba]\n\n\nfor t in np.linspace(0, 1, 101):\n    new_y_pred = [1 if y >= t else 0 for y in y_pred_proba]    \n    f1 = f1_score(y_test, new_y_pred)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thr = t\n\nnew_y_pred = [1 if y >= best_thr else 0 for y in y_pred_proba]\n        \nprint(f'Acc after= {accuracy_score(y_test, new_y_pred)}')\nprint(f'F1 after = {f1_score(y_test, new_y_pred)}')\nprint(f'Best thr = {best_thr}')","8c16aa3f":"y_pred_proba = clf.predict_proba(X_sub_vec_df)[:, 1]\ny_pred = [1 if y >= best_thr else 0 for y in y_pred_proba]\n\ndf_sub['target'] = y_pred\ndf_sub = df_sub[['id', 'target']]\ndf_sub.to_csv('submission.csv', index=False, header=True)\ndf_sub.head(10)","7d454266":"## Question 3.2 how many words does each Tweet have??","88f4dc10":"## Applying TF-IDF vectoring","801c59de":"## Classification report","bdbbc074":"## Question 3: what is the length of the tweets?","640af243":"## Question 3.2 how many words does each Tweet have per outcome?","97d95f76":"## Explain predictions","edd1f4eb":"## Step 1: I'll use just text and outcome...","1c18f80f":"## Some tf-idf examples...","5e370267":"## Step 3: create a basic predictive model (naive bayes)","a579cf7f":"## Step 4: Text normalization\n\nref: https:\/\/medium.com\/@gaurav5430\/using-nltk-for-lemmatizing-sentences-c1bfff963258","03cb849d":"## Step 3: remove useless items from text (html, links, pontuation, numbers, stopwords, ...)","0d7fd77d":"## 2.1 Data sctructure\n\n### Question 1: How is the data structured?","6f27c149":"## Let's see some charts again","a6589c02":"## ROC-AUC Curve","b6f893e0":"# 5. Submit results","5f2026fa":"# 3. Prepare data","946eea11":"# 1. Problem definition\n\n\nGiven a set of tweets (location + keyword + text) predict whether there is real disaster information or not. \n\n## Special thanks:\n\nPart of the code is based on the following work:\nhttps:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro\n\n**Thanks Parul Pandey!** \n\nAnd some part of the code is based on the following work:\nhttps:\/\/www.kaggle.com\/datafan07\/disaster-tweets-nlp-eda-bert-with-transformers\n\n**Thanks Ertu\u011frul Demir!**\n\n## Special ask: please upvote if you like this solution!","424b583f":"### Real instance","b11913e6":"## Word cloud...\n### Real","9600abc7":"### And bigrams?","fde5871b":"### length per outcome","f2ca1a08":"## ROC-AUC curve","7715e085":"## Most frequent words","53e30438":"### Fake instance","89a313c8":"## Step 2: create a dummy baseline model","e10aa1c5":"# 2. Analyze data","88072f8e":"## Question 4: What are the 10 most frequent words?","682cadc6":"## Question 4.1: and about bigrams?","6805c60b":"## Adjust threshold","d1c56416":"### Step 0: Create train and test dataset (80% = train \/ 20% = test,stratified by the outcome)","37018ea8":"## PR Curve","88f9a2ed":"## how many words","261df133":"### Question 2: How about missing values?","4a72b1b2":"## Special ask again: please upvote if you like this solution!\n\n![That's all folks!!!](https:\/\/i.pinimg.com\/originals\/2c\/2e\/ef\/2c2eef8da1285d958914eef079f9b70c.jpg)","fd05ffef":"## how many words per outcome","fe485c2f":"## How outcome are distributed...","15b0a541":"## Question 5: Outcome distribution","23036dfd":"## Step 2: transform text to lowercase","4fa5185a":"## Question 6: Let's see the word cloud per outcome","7a6b3583":"# 4. Evaluate algorithms","90632bbc":"### text length","d8f2d737":"## PR Curve","13615307":"## Discrimination Threshold","f6050dbd":"### Fake","ff171e88":"![](https:\/\/www.xoriant.com\/blog\/wp-content\/uploads\/2020\/01\/NPL_Blog.png)","a735582b":"## Classification report","138e8b96":"## Question 3.1 what is the length of the reviews per outcome?"}}