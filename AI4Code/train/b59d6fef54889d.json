{"cell_type":{"734b2a44":"code","453c8d88":"code","50987d92":"code","bf50afa6":"code","53ea2d51":"code","90502352":"code","58200131":"code","25b437f4":"code","a2fcd0b8":"code","6e6366ef":"code","6a303a40":"code","0a036a6f":"code","2b31f736":"code","b70b0322":"code","67bcbfce":"code","6ecfbc82":"code","f79e797e":"code","866c85ab":"code","228277a4":"code","2f7ae282":"code","57b97df6":"code","41a7e80f":"code","069ab954":"code","c04c1cf9":"code","af374ebe":"code","8e7948db":"code","b6dadd04":"code","d00e6d9f":"code","db5a63ba":"code","1653b256":"code","527e1a7e":"code","f7edbf41":"code","3e58dae9":"code","1826a3b8":"code","3aef722a":"code","65962732":"code","c42dffb0":"code","a2d8a613":"markdown","e39235d2":"markdown","75fcf064":"markdown","daea6061":"markdown","f33e9381":"markdown","fa8b3aad":"markdown","9e80c86b":"markdown","6a48aafc":"markdown","44fa110a":"markdown","357ea758":"markdown","6193e76d":"markdown","e5f5c64f":"markdown","eea4fdc3":"markdown","0130c94c":"markdown","5312a240":"markdown","4704b49f":"markdown","2e6110de":"markdown","bbc96ede":"markdown","4e4057de":"markdown","77ea3442":"markdown","4d598774":"markdown","9a009a3c":"markdown","f0549e8f":"markdown","6743d995":"markdown","2825ebaf":"markdown","22f1ea17":"markdown","26f655e8":"markdown","12699139":"markdown","4520bda9":"markdown","fdd883f7":"markdown","cedefeae":"markdown","665b48ec":"markdown","805c3229":"markdown","b81d2972":"markdown","3c44c881":"markdown","bd043d57":"markdown","d0aa752d":"markdown","798fabfc":"markdown"},"source":{"734b2a44":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)","453c8d88":"import numpy as np\nimport pandas as pd","50987d92":"df = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ndf.head()","bf50afa6":"df['word_count'] = df['tweet'].apply(lambda x:len(str(x).split(\" \")))\n# df.head()\ndf[['tweet', 'word_count']].head()","53ea2d51":"df['char_count'] = df['tweet'].str.len() # This will include spaces \/ white space.\n# df.head()\ndf[['tweet', 'char_count']].head()","90502352":"def avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words) \/ len(words))\n\ndf['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n\n# df.head()\ndf[['tweet', 'avg_word']].head()","58200131":"from nltk.corpus import stopwords\n\nstop = stopwords.words('english')","25b437f4":"df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n\n# df.head()\ndf[['tweet', 'stopwords']].head()","a2fcd0b8":"df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n\n# df.head()\ndf[['tweet', 'hashtags']].head()","6e6366ef":"df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\n# df.head()\ndf[['tweet', 'numerics']].head()","6a303a40":"df['upper'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n\n# df.head()\ndf[['tweet', 'upper']].head()","0a036a6f":"df['tweet_lower'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n# df.head()\ndf[['tweet', 'tweet_lower']].head()","2b31f736":"df['tweet_lower'] = df['tweet_lower'].str.replace('[^\\w\\s]','')\n# df.head()\ndf['tweet_lower'].head()","b70b0322":"df['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ndf.tweet_lower.head()","67bcbfce":"freq_words = pd.Series(' '.join(df['tweet_lower']).split()).value_counts()[:10]\nfreq_words","6ecfbc82":"freq_words = list(freq_words.index)\n\ndf['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words))\n\ndf['tweet_lower'].head()","f79e797e":"rare_words = pd.Series(' '.join(df['tweet_lower']).split()).value_counts()[-10:]\nrare_words","866c85ab":"rare_words = list(rare_words.index)\n\ndf['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_words))\n\ndf['tweet_lower'].head()","228277a4":"from textblob import TextBlob","2f7ae282":"df['tweet_lower'][:5].apply(lambda x: str(TextBlob(x).correct()))","57b97df6":"TextBlob(df['tweet_lower'][1]).words ","41a7e80f":"from nltk.stem import PorterStemmer","069ab954":"st = PorterStemmer()","c04c1cf9":"df['tweet_lower'][:5].apply(lambda x:\" \".join([st.stem(word) for word in x.split()]))","af374ebe":"from textblob import Word\ndf['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\ndf.tweet_lower.head()","8e7948db":"TextBlob(df['tweet_lower'][0]).ngrams(2)","b6dadd04":"TextBlob(df['tweet'][0]).ngrams(2)","d00e6d9f":"# tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1 = (df['tweet_lower'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n\ntf1.columns = ['words', 'tf']\n\ntf1","db5a63ba":"for i, word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] = np.log(df.shape[0] \/ (len(df[df['tweet_lower'].str.contains(word)])))\n    \ntf1","1653b256":"tf1['tf_idf'] = tf1['tf'] * tf1['idf']\ntf1","527e1a7e":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(max_features = 1000, lowercase = True, analyzer = 'word', stop_words = 'english', ngram_range = (1,1))\n\ndf_vect = tfidf.fit_transform(df['tweet'])\n\ndf_vect\n","f7edbf41":"from sklearn.feature_extraction.text import CountVectorizer\n\n","3e58dae9":"bow = CountVectorizer(max_features=1000, lowercase = True, ngram_range= (1,1), analyzer = 'word')\n\ndf_bow = bow.fit_transform(df['tweet'])\n\ndf_bow","1826a3b8":"df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment )","3aef722a":"df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0])\n\ndf[['tweet','sentiment']].head()","65962732":"df['sentiment2'] = df['tweet_lower'].apply(lambda x: TextBlob(x).sentiment[0])\n\ndf[['tweet_lower','sentiment2']].head()","c42dffb0":"from gensim.scripts.glove2word2vec import glove2word2vec","a2d8a613":"Note that it will actually take a lot of time to make these corrections. Therefore just for the purpose of learning, i have shown this technique by applying it on only the first 5 rows.\n\nMoreover we cannot always expect it to be accurate so some care should be taken before applying it.\n\nWe should also keep in mind that words are often used in their abbreviated form. For instance, `your` is used as `ur`. We should treat this before the spelling correction step, other wise these words might be transformed into any other word like above `model take or \u00f0\u00f0\u00f0\u00f0 \u00f0\u00f0\u00f0` initially it was `model take ur\u00f0 \u00f0\u00f0\u00f0\u00f0 \u00f0\u00f0\u00f0`.","e39235d2":"# 3.6 Sentiment Analysis\n","75fcf064":"# 1.5 Number of Special Char\nOne more interesting feature which we can extract from a tweet is caluclating the number of hastags or methines present in it. This also helps in extracting extra information from our text data.\n\nHere we make use of the `starts with` function because hashtags always appear at the beginning of a word.","daea6061":"# 2.2 Removing Punctuation\nThe next step is to remove punctuation, as it doesn't add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data.","f33e9381":"# 1.2 Number of Characters\nHere we calculate the number of characters in each tweet. This is done by calculating the length of the tweet.","fa8b3aad":"# 3.2 Term frequency\nTerm Frequency is simply the ratio of the count of word present in a sentence, to the length of the sentence.\n\nTherefore we can generalize term frequency as\n\n`TF = (Number of times term T appears in the particular row) \/ (Number of terms in that row`\n\n","9e80c86b":"# 3.3 Inverse Document Frequency\nThe intution behind inverse document frequency (IDF) is that a word is not of much use to us if it's appearing in all the documents.\n\nTherefore the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n\n`IDF = log(N\/n)` where N is the total number of rows and n is thenumber of rows in which the word was present.\n\n","6a48aafc":"Now lets remove these words as their presence will not of any use in classification of our text data.","44fa110a":"# 3.1 N-grams\nN-grams are the combination of multiple words used together.\nN-grams with N = 1 are called unigrams. Similarly, bigrams when N = 2; trigrams when N = 3 and so on.\n\nUnigrams do not usually contain as much information as compared to bigrams and trigrams.\n\nThe basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with.\n\nOptimum length really depends on the application - if yoyr n-grams are too short, you may fail to capture important differences. On the other hand, they are too long, you may fail to capture the \"general knwoeldge\" and only stick to a particular cases.\n\nSo lets quickly extract bigrams from our tweet using the ngrams function of the textblob library.","357ea758":"Alternatively we can use sklearn's `TfidfVectorizer` as below.","6193e76d":"# 1. Basic Feature Extraction","e5f5c64f":"# 2.3 Removal of Stop Words\nAs we discussed earlier, stop words (or commonly occuring words) should be removed from the textdat. \nFor this purpose we can either create a list of stopwords ourselves or we can use predefined libraries.","eea4fdc3":"# 2.7 Tokenization\nTokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words.","0130c94c":"The more the value of IDF, the more unique is the word.","5312a240":"# 3.4 Term Frequency - IDF (TF-IDF)\nTF-ID is the multiplication of the TF and IDF which we calculated above.","4704b49f":"Above we can see that it returns a tuple representing polarity and subjectivity of each tweet.\n\nHere we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive and nearer to -1 means a negative sentiment. This can also work as a feature for building a ML Model.","2e6110de":"All these pre-processing steps are essential and help  us in reducing our vocabulary clutter so that the featues produced in the end are more effective.","bbc96ede":"# 3.7 Word Embeddings\nWord Embedding is the representation of text in the form ofo vectors. Th eunderlying idea here is tha similar words will have a minimum distance between their vectors.\n\nWord2Vec models require a lot of text, so either we can train it on our training data or we can use the pre-trained word vectors developed by Google, Wiki etc.\n","4e4057de":"Remove rare words","77ea3442":"We can use the text data to extract a number of features even if we do not have sufficient knowledge of Natural Language Processing.\n\nBefore starting, lets quickly read the training file from the dataset in order to perform different tasks on it.\n\nI am using the Twitter Sentiment Dataset.","4d598774":"# 2.8 Stemming\nStemming refers to the removal of sufficies, like `ing`, `ly`, `s` etc by a simple rule-based approach. For this purpose we will use PorterStemmer from the NLTK library.","9a009a3c":"Upto this point we have done all the basic pre-processing steps in order to clean our data.\n\nNow we can finally move on to extracting features using NLP technique.","f0549e8f":"Above output, we can see that the word `dysfunctional` has been transformed into `dysfunct`, and `kids` transformed into `kid`, amoong other changes.","6743d995":"# 2.6 Spelling Correction\nWe have all seen tweets with a plethora of spelling mistakes. Out timeline are often filled with hastly sent tweets that are barely legible at times.\n\nIn that regards, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words.\n\nTo achieve this we will use the textblob library.","2825ebaf":"# 1.7 Number of UpperCase words.\nAnger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identiy those words.","22f1ea17":"# 2.9 Lemmatization\nLemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore we usually perfer using lemmatization over stemming.","26f655e8":"# 1.3 Average Word Length\nWe will also extract anothe feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model.\n\nHere we simply take the sum of the length of all the words and divide it by the total length of the tweet.","12699139":"# 3 Advance Text Processing","4520bda9":"Note: As you can see in the above output, all the punctuation including `#` and `@` has been removed from the `df`.","fdd883f7":"# 3.5 Bag of Words\nBag of Words (BoW) referes to the representation of text which describes the presence of words within the text data. The intution behind this is that two similar text fields will contain similar kind of words, and will therefor have s similar bag of words.\n\nFurther that form the text alone we can learn somethins about the meaning of the document.","cedefeae":"# 1.1 Number of words\nOne of the most basic features we can extract is the number of words in each tweet.  \n\nTo do this we simply use the split function in python.","665b48ec":"# 1.6 Number of numerics\nJust like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful that should be run while doing similar exercises.","805c3229":"Note: Here we are only working with Textual data, bt we can also use the same methods to numerical features.","b81d2972":"# 1.4 Number of Stopwords\nGenerally while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n\nHere we have importing stopwords from NLTK which is a basic NLP library in python.","3c44c881":"# 2.5 Rare words removal\nSimilar to the most common words, this time lets remove rarely occuring words from the text. As they are so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts.","bd043d57":"# 2.1 Lower Case\nThe first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words.\n\nFor instance, while calculating the word count \"Analytics' and 'analytics' will be taken as different words.","d0aa752d":"# 2.4 Common word removal\nPrevoously, we just removed commonly occuring words in a general sense. We can also remove commonly occuring words from our text data first, lets check the 10 most frequently occuring words in our text data then take a call to remove or retain.","798fabfc":"# 2. Basic Pre-Processing\nSo far we have learned hwo to extract basic features from the text data. Before diving into text and feature extraction, our first step should b cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data."}}