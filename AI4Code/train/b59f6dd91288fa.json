{"cell_type":{"74032d34":"code","59cb27bc":"code","3f750f96":"code","26d3287e":"code","52326ff5":"code","4a8666bb":"code","00ea4273":"code","385825da":"code","053b5d09":"code","0da41bd4":"code","b015d452":"code","ccc746bc":"code","cc0c7e0a":"code","4bc421e5":"code","4cdb92de":"code","15d09fe7":"code","168c2eef":"code","7a9c6300":"code","b7d35f6b":"code","7f5f102c":"code","338b2c9c":"code","08f42f4d":"code","64c6e0a5":"code","31e1aca5":"code","0a4f5ed5":"code","767bed13":"code","2f00f903":"code","c60e56b4":"code","08b480c7":"code","b319c73c":"code","83111f23":"code","64ba268b":"code","02a27542":"code","e1dfda0c":"code","a8ae8b67":"code","1d99d7c4":"code","3f0f5162":"code","1417f556":"code","ad914fb9":"code","f580cb65":"code","5a606806":"code","16d2fbd1":"code","0761e061":"code","f66d26bd":"code","05799b20":"code","3023a7ea":"code","a6b711f1":"code","19dc8697":"code","24cb6af9":"code","c79735b3":"code","1c258f80":"code","4f285774":"code","cabe9077":"code","f5080832":"code","50b4ba70":"code","f7482b24":"code","97905449":"markdown","9fc56ef8":"markdown","59debc08":"markdown","b3a9052e":"markdown"},"source":{"74032d34":"import seaborn as sns #importing our visualization library\nimport matplotlib.pyplot as plt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n","59cb27bc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)","3f750f96":"df = pd.read_csv('\/kaggle\/input\/holida-package\/Holiday_Package.csv')","26d3287e":"df = df.dropna()\nprint(df.shape)\nprint(list(df.columns))","52326ff5":"df.head()","4a8666bb":"df.shape","00ea4273":"df.info()","385825da":"# Dropping unwanted column\ndf.drop([\"Unnamed: 0\"],axis=1,inplace=True)","053b5d09":"df.head()","0da41bd4":"# Check for duplicates of data\ndf.duplicated().sum()","b015d452":"#Check for any missing values\ndf.isna().sum()","ccc746bc":"# find categorical variables\n\ncategorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)\n","cc0c7e0a":"df['Holliday_Package'].unique()","4bc421e5":"# Data Exploration\ndf['Holliday_Package'].value_counts()","4cdb92de":"# Data Exploration\ndf['foreign'].value_counts()","15d09fe7":"df.groupby('Holliday_Package').mean()","168c2eef":"sns.countplot(x='Holliday_Package',data=df, palette='hls')\nplt.show()\nplt.savefig('count_plot')","7a9c6300":"df.groupby('foreign').mean()","b7d35f6b":"sns.countplot(x='foreign',data=df, palette='hls')\nplt.show()\nplt.savefig('count_plot')","7f5f102c":"df.groupby('no_older_children').mean()","338b2c9c":"df.groupby('no_young_children').mean()","08f42f4d":"%matplotlib inline\npd.crosstab(df.no_older_children,df.Holliday_Package).plot(kind='bar')","64c6e0a5":"%matplotlib inline\npd.crosstab(df.no_young_children,df.Holliday_Package).plot(kind='bar')","31e1aca5":"table=pd.crosstab(df.no_young_children,df.foreign)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)","0a4f5ed5":"table=pd.crosstab(df.no_older_children,df.foreign)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)","767bed13":"df.age.hist()\nplt.title('Histogram of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('hist_age')","2f00f903":"df.describe()","c60e56b4":"# construct box plot for continuous variables\nplt.figure(figsize=(10,10))\ndf.boxplot()","08b480c7":"sns.heatmap(df.corr(), annot=True)","b319c73c":"sns.pairplot(df)","83111f23":"for column in df.columns:\n    if df[column].dtype == 'object':\n        print(column.upper(),': ',df[column].nunique())\n        print(df[column].value_counts().sort_values())\n        print('\\n')","64ba268b":"# Converting Categorical to Numerical Variable\nfor feature in df.columns: \n    if df[feature].dtype == 'object':\n        df[feature] = pd.Categorical(df[feature]).codes ","02a27542":"df.head()","e1dfda0c":"# Train-Test Split\n# Copy all the predictor variables into X dataframe\nX = df.drop(['Holliday_Package'],axis=1)\n\n# Copy target into the y dataframe. \ny = df.Holliday_Package","a8ae8b67":"# Split X and y into training and test set in 70:30 ratio\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)\ntuned_parameters = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}\nclf = GridSearchCV(LogisticRegression(solver='liblinear'), tuned_parameters, cv=3, scoring=\"accuracy\")\nclf.fit(X_train, y_train)","1d99d7c4":"# Fit the Logistic Regression model\nmodel = LogisticRegression(solver='newton-cg',max_iter=10000,penalty='l2',verbose=True,n_jobs=-1)\nmodel.fit(X_train, y_train)","3f0f5162":"ytrain_predict = model.predict(X_train)\nytest_predict = model.predict(X_test)","1417f556":"ytest_predict_prob=model.predict_proba(X_test)\npd.DataFrame(ytest_predict_prob).head()","ad914fb9":"# Accuracy - Train Data\nmodel.score(X_train, y_train)","f580cb65":"# Train Model Roc_AUC SCore\n# predict probabilities\nprobs = model.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","5a606806":"# Accuracy - Test Data\nmodel.score(X_test, y_test)","16d2fbd1":"# Test model roc auc score\n# predict probabilities\nprobs = model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","0761e061":"# Confusion matrix on train data\nconfusion_matrix(y_train, ytrain_predict)","f66d26bd":"print(classification_report(y_train, ytrain_predict))","05799b20":"# Confusion Matrix for Test Data\ncnf_matrix=confusion_matrix(y_test, ytest_predict)\ncnf_matrix","3023a7ea":"#Test Data Accuracy\ntest_acc=model.score(X_test,y_test)\ntest_acc","a6b711f1":"print(classification_report(y_test, ytest_predict))","19dc8697":"# Implementing the model\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y,X)\nresult=logit_model.fit()\nprint(result.summary())","24cb6af9":"# Linear Discriminate Analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n#Build LDA Model\n# Refer details for LDA at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\nclf = LinearDiscriminantAnalysis()\nmodel1=clf.fit(X_train,y_train)\nmodel1","c79735b3":"# Predicting Train Data\n# Predict it\n# Predict it\npred_class = model1.predict(X_train)","1c258f80":"print(classification_report(y_train, pred_class))","4f285774":"# Confusion matrix on train data\n#generate Confusion Matrix\n\nconfusion_matrix(y_train, pred_class)","cabe9077":"#Predicting Test data\nmodel2=clf.fit(X_test,y_test)\nmodel2","f5080832":"pred_class2 = model2.predict(X_test)","50b4ba70":"print(classification_report(y_test, pred_class2))","f7482b24":"# Confusion matrix on test data\nconfusion_matrix(y_test, pred_class2)","97905449":"Compute precision, recall, F-measure and support The precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_test.","9fc56ef8":"The result is being telling us that we have 102+67 correct predictions and 50+43 incorrect predictions","59debc08":"Compute precision, recall, F-measure and support The precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_train.","b3a9052e":"The result is being telling us that there are 252+163 correct predictions and 121+74 wrong predictions"}}