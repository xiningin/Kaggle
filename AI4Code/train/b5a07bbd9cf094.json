{"cell_type":{"df47f5d1":"code","1899049c":"code","7406ecb6":"code","d4869708":"code","2e8dcdb5":"code","2587e447":"code","865408a2":"code","76d4537a":"code","9bc18a2f":"code","9fc76739":"code","0272cb56":"code","9dd1cf3d":"code","35a1c714":"code","f3a3e27d":"code","f875fd9d":"code","26c150a1":"code","aebf7e47":"code","081f8353":"code","595bc65f":"code","8a6f5456":"code","cfc3416d":"code","7d475841":"code","8dc02d18":"code","d6293ac3":"code","bbcf3f5f":"code","c251d1ee":"code","9c6366c5":"code","e500c680":"code","c60405db":"code","8bd9b6e4":"code","88e281be":"code","8736e553":"code","3c3bbabc":"code","95dfc4b0":"code","5cdd2709":"code","66302575":"code","7262f326":"code","e7465484":"markdown","e0ef8ff8":"markdown","93ada07f":"markdown","cecaa03f":"markdown","1fa21eb8":"markdown","95bd0701":"markdown","2f895ce7":"markdown","874d2a77":"markdown","5dd6b344":"markdown","7a722de6":"markdown","7f796c18":"markdown"},"source":{"df47f5d1":"# Importando as bibliotecas necess\u00e1rias:\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import RSLPStemmer\nimport time\nfrom nltk import FreqDist\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport os # accessing directory structure\nfrom wordcloud import WordCloud, STOPWORDS\n","1899049c":"print(os.listdir('..\/input'))","7406ecb6":"# boatos.csv tem 1170 linhas\ndf = pd.read_csv('..\/input\/boatos.csv', delimiter=',')\ndf.dataframeName = 'boatos.csv'\nnRow, nCol = df.shape\nprint(f'H\u00e1 {nRow} linhas e {nCol} colunas')","d4869708":"df.head(5)","2e8dcdb5":"df.info()","2587e447":"def initial_clean(text):\n    \"\"\"\n    Fun\u00e7\u00e3o para limpeza de textos de websites, emails e pontua\u00e7\u00e3o\n    Tamb\u00e9m converte para min\u00fasculas\n    \"\"\"\n    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n    text = text.lower() #\u00a0lower case the text\n    text = nltk.word_tokenize(text)\n    return text\n\nstop_words = stopwords.words('portuguese') #stop words in Portugu\u00eas\ndef remove_stop_words(text):\n    \"\"\"\n    Fun\u00e7\u00e3o apra remover \"stop-words\"\n    \"\"\"\n    return [word for word in text if word not in stop_words]\n\nstemmer = nltk.stem.RSLPStemmer() # Stemmer in Portuguese\ndef stem_words(text):\n    \"\"\"\n    Fun\u00e7\u00e3o para iguralar singular e plural\n    \"\"\"\n    try:\n        text = [stemmer.stem(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except IndexError: #\u00a0the word \"oed\" broke this, so needed try except\n        pass\n    return text\n\ndef apply_all(text):\n    \"\"\"\n    Aglutina todas as fun\u00e7\u00f5es anteriores\n    \"\"\"\n    return stem_words(remove_stop_words(initial_clean(text)))","865408a2":"#\u00a0clean text and title and create new column \"tokenized\"\nt1 = time.time()\ndf['tokenized'] = df['hoax'].apply(apply_all)\nt2 = time.time()\nprint(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)\/60, \"min\")","76d4537a":"df.head()","9bc18a2f":"# first get a list of all words\nall_words = [word for item in list(df['tokenized']) for word in item]\n# use nltk fdist to get a frequency distribution of all words\nfdist = FreqDist(all_words)\nlen(fdist) #\u00a0number of unique words","9fc76739":"top= fdist.most_common()\nprint(top)","0272cb56":"\nk = 50000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","9dd1cf3d":"# choose k and visually inspect the bottom 10 words of the top k\n# Escolhe o valor de k e veririca visualmente as 10 palavras menos usadas das k mais frequentes\nk = 1500\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","35a1c714":"# defini\u00e7\u00e3o de uma fun\u00e7\u00e3o para encontrar as palavras mais frequentes\n# fdist.most_common(k) monta um vetor de tuplas (palavra_1,quantidade_1), (palavra_2,quantidade_2), ...\n# *fdist.most_common(k) separa os itens (tuplas) do vetor em argumentos para a fun\u00e7\u00e3o zip\n# a fun\u00e7\u00e3o zip agrega todas as palavras de cada tupla em uma nova tupla, e todos os numeros (quantidades) em outra tupla\n# Ex: (palavra_1, plavra_2, ...), (4,7,...)\n# top_k_words pega primeira tupla, a tupla das palavras\ntop_k_words,_ = zip(*fdist.most_common(k))\n# a fun\u00e7\u00e3o set torna a tupla em um conjunto de valores que n\u00e3o tem indices, nem pode ter itens repetidos.\n# a ordem \u00e9 aleatoria e n\u00e3o importa\ntop_k_words = set(top_k_words)\ndef keep_top_k_words(text):\n    return [word for word in text if word in top_k_words]","f3a3e27d":"# aplica a fun\u00e7\u00e3o \u00e0 coluna 'tokenized' para manter s\u00f3 as mais frequentes (palavras incomuns s\u00e3o removidas)\ndf['tokenized'] = df['tokenized'].apply(keep_top_k_words)","f875fd9d":"# O tamanho (coluna 'doc_len') \u00e9 calculado\ndf['doc_len'] = df['tokenized'].apply(lambda x: len(x))\n# coloca os tamnahos em um vetor\ndoc_lengths = list(df['doc_len'])\n# remove a coluna 'doc_len' do dataframe\ndf.drop(labels='doc_len', axis=1, inplace=True)\n\nprint(\"length of list:\",len(doc_lengths),\n      \"\\naverage document length\", np.average(doc_lengths),\n      \"\\nminimum document length\", min(doc_lengths),\n      \"\\nmaximum document length\", max(doc_lengths))","26c150a1":"# plot a histogram of document length\nnum_bins = 1000\nfig, ax = plt.subplots(figsize=(20,6));\n# the histogram of the data\nn, bins, patches = ax.hist(doc_lengths, num_bins)\nax.set_xlabel('Document Length (tokens)', fontsize=15)\nax.set_ylabel('Normed Frequency', fontsize=15)\nax.grid()\nax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))\nplt.xlim(0,2000)\nax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-',\n        label='average doc length')\nax.legend()\nax.grid()\nfig.tight_layout()\nplt.show()","aebf7e47":"# MANTEM APENAS OS ARTIGOS COM MAIS DE 40 TOKENS\ndf = df[df['tokenized'].map(len) >= 40]\n\n# make sure all tokenized items are lists\ndf = df[df['tokenized'].map(type) == list]\ndf.reset_index(drop=True,inplace=True)\nprint(\"Ap\u00f3s a limpeza e exclus\u00e3o de artigos curtos, o dataframe tem agora:\", len(df), \"artigos\")","081f8353":"df.head()","595bc65f":"# Cria uma lista aleatoria de itens TRUE or FALSE, onde 98% \u00e9 TRUE, para ser utilizado na divis\u00e3o do dataframe\nmsk = np.random.rand(len(df)) < 0.98","8a6f5456":"#Divis\u00e3o do dataframe usando a lista de valores TRUE e FALSE\n# para treinamento os valores TRUE\ntrain_df = df[msk]\ntrain_df.reset_index(drop=True,inplace=True)\n# para teste s\u00e3o usados os valores FALSE\ntest_df = df[~msk]\ntest_df.reset_index(drop=True,inplace=True)","cfc3416d":"train_df.to_csv('train_df.csv')\ntest_df.to_csv('test_df.csv')","7d475841":"# Tamanho dos conjutnos de ddos de treino e teste\nprint(len(df),len(train_df),len(test_df))","8dc02d18":"def train_lda(data):\n    \"\"\"\n    Esta fun\u00e7\u00e3o treina o modelo LDA\n    Configuramos os parametros como o n\u00famero de t\u00f3picos, o 'chunksize' para usar o m\u00e9todo de Hoffman\n    Fazemos duas passagens de dados j\u00e1 que o dataset \u00e9 pequeno, queremos que o as distribui\u00e7\u00f5es se estabilizem\n    \"\"\"\n    num_topics = 100\n    chunksize = 300\n    dictionary = corpora.Dictionary(data['tokenized'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n    t1 = time.time()\n    # low alpha means each document is only represented by a small number of topics, and vice versa\n    # low eta means each topic is only represented by a small number of words, and vice versa\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    t2 = time.time()\n    print(\"Tempo para treinar o modelo LDA com \", len(df), \"artigos: \", (t2-t1)\/60, \"min\")\n    return dictionary,corpus,lda","d6293ac3":"dictionary,corpus,lda = train_lda(train_df)","bbcf3f5f":"# O m\u00e9todo show_topics mostra as palavras mais frequentes (quantidade definida por 'num_words') na quantidade 'num_topics' de t\u00f3picos aleat\u00f3rios.\nlda.show_topics(num_topics=5, num_words=50)","c251d1ee":"# mostra um t\u00f3pico especifico. argumentos: id do topico e quantidade de palavras (mais significativas)\nlda.show_topic(topicid=98, topn=2)","9c6366c5":"# seleciona um artigo aleatoriamente de train_df\nrandom_article_index = np.random.randint(len(train_df)) # pega um numero aleatorio menor que o tamanho de train_df\nbow = dictionary.doc2bow(train_df.iloc[random_article_index,4]) # Lista de tuplas do artigo com (token_id, token_count)\nprint(random_article_index)","e500c680":"print(train_df.iloc[random_article_index,4])","c60405db":"# Lista os topicos do artigo escolhido aleatoriamente (primeiro item das tuplas)\ndoc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)]) # a fun\u00e7\u00e3o get_document_topics retorna a distribui\u00e7\u00e3o dos topicos","8bd9b6e4":"# Gr\u00e1fico da distribui\u00e7\u00e3o de topicos do artigo\nfig, ax = plt.subplots(figsize=(12,6));\n# Histograma dos dados\npatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\nax.set_xlabel('ID do t\u00f3pico', fontsize=15)\nax.set_ylabel('Contribui\u00e7\u00e3o do t\u00f3pico', fontsize=15)\nax.set_title(\"Distribui\u00e7\u00e3o de t\u00f3picos do artigo \" + str(random_article_index), fontsize=20)\n#ax.set_xticks(np.linspace(10,100,10))\n#fig.tight_layout()\nplt.show()","88e281be":"# Os 5 t\u00f3picos que mais contribuem e suas palavras\nfor i in doc_distribution.argsort()[-5:][::-1]:\n    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")","8736e553":"# Escolha de um artigo aleat\u00f3rio dos dados de teste (test_df)\nrandom_article_index = np.random.randint(len(test_df))\nprint(random_article_index)","3c3bbabc":"new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,4])","95dfc4b0":"print(test_df.iloc[random_article_index,1])","5cdd2709":"new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])","66302575":"# Gr\u00e1fico da distribui\u00e7\u00e3o de topicos do artigo\nfig, ax = plt.subplots(figsize=(12,6));\n# Histograma dos dados\npatches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)\nax.set_xlabel('ID do t\u00f3pico', fontsize=15)\nax.set_ylabel('Contribui\u00e7\u00e3o do t\u00f3pico', fontsize=15)\nax.set_title(\"Distribui\u00e7\u00e3o dos t\u00f3picos para um artigo n\u00e3o visto\", fontsize=20)\nax.set_xticks(np.linspace(10,100,10))\nfig.tight_layout()\nplt.show()","7262f326":"# Os 5 t\u00f3picos que mais contribuem e suas palavras\nfor i in new_doc_distribution.argsort()[-5:][::-1]:\n    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")","e7465484":"Drop short articles\nLDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.\n\nFrom the histogram above, droping all articles less than 40 tokens seems appropriate.","e0ef8ff8":"Repetimos agora as opera\u00e7\u00f5es efetuadas sobre um artigo escolhido de forma aleat\u00f3ria","93ada07f":"## An\u00e1lise Explorat\u00f3ria\n","cecaa03f":"### Let's check 1st file: ..\/input\/boatos.csv","1fa21eb8":"Agora voc\u00ea est\u00e1 pronto para ler os dados e usar as fun\u00e7\u00f5es de plotagem para visualizar os dados.","95bd0701":"\n* Consultas de similaridade e dados n\u00e3o vistos\n* Agora, voltaremos nossa aten\u00e7\u00e3o ao conjunto de dados de teste que o modelo ainda n\u00e3o viu. Embora os artigos em test_df n\u00e3o tenham sido vistos pelo modelo, o gensim tem uma maneira de inferir suas distribui\u00e7\u00f5es de t\u00f3picos, dado o modelo treinado. Obviamente, a abordagem correta para obter resultados precisos seria treinar o modelo novamente com esses novos artigos, mas isso pode ser oportuno e invi\u00e1vel em um cen\u00e1rio de caso real em que os resultados s\u00e3o necess\u00e1rios rapidamente.\n\n* Primeiro, vamos mostrar como podemos inferir t\u00f3picos do documento para um novo artigo n\u00e3o visto.","2f895ce7":"Ser\u00e1 efetuada agora a divis\u00e3o do dataframe em duas partes: treinamento e teste.\n\nO conjunto de dados de treinamento ser\u00e1 utilizado para treinar o modelo LDA, enquanto que o de teste ser\u00e1 usado para encontrar artigos similares em nosso algoritmo de recomenda\u00e7\u00e3o.\n\nO conjunto de dados j\u00e1 est\u00e1 embaralhado desde o come\u00e7o, ent\u00e3o n\u00e3o \u00e9 necess\u00e1rio faz\u00ea-lo novamente.","874d2a77":"There are 2 csv files in the current version of the dataset:\n","5dd6b344":"## Introdu\u00e7\u00e3o\n\nReferencia: Topic Modeling with Gensim (Python)\nhttps:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/\n\nAPI do Gensim: https:\/\/radimrehurek.com\/gensim\/models\/ldamodel.html\n\nUtiliza\u00e7\u00e3o do m\u00e9todo LDA para clusterizar as mensagens de WhatsApp classificadas como boatos pelo site \"boato.org\" dataset de 2018 - Kaggle","7a722de6":"Aqui est\u00e1 a parte importante. Ao obter a representa\u00e7\u00e3o do BOW para este artigo n\u00e3o visto, o gensim apenas considera as palavras no dicion\u00e1rio existente que usamos para treinar o modelo. Portanto, se houver novas palavras neste artigo, elas n\u00e3o ser\u00e3o consideradas ao inferir a distribui\u00e7\u00e3o do t\u00f3pico. Isso \u00e9 bom, pois nenhum erro ocorre para palavras n\u00e3o vistas, mas ruim, pois algumas palavras podem ser cortadas e, portanto, podemos perder uma distribui\u00e7\u00e3o precisa de t\u00f3picos para este artigo.\n\nNo entanto, atenuamos esse risco porque o conjunto de treinamento \u00e9 muito representativo de todo o corpus; 98% das observa\u00e7\u00f5es est\u00e3o no conjunto de treinamento, com apenas 0,02% das observa\u00e7\u00f5es no conjunto de testes. Portanto, a maioria das palavras, se n\u00e3o todas, do conjunto de testes deve estar no dicion\u00e1rio do conjunto de treinamento.","7f796c18":"k = 50,000 \u00e9 um valor elevado j\u00e1 que as palavras menos frequentes n\u00e3o s\u00e3o palavras corretas e ocorrem muit raramente (apenas uma vez na base).\n\nk = 1500 \u00e9 um valor mais razoavel. As palvras s\u00e3o utilizadas pelo menos 11 vezes."}}