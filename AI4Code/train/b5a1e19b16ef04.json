{"cell_type":{"dff2e9ed":"code","1295a6e2":"code","34c1e028":"code","c55930b1":"code","fc02c502":"code","3205b20a":"code","f0d5cafe":"code","ee932c81":"code","d68fd99e":"code","d689cf27":"code","07e69398":"code","d9fe2cb7":"code","55e54d20":"code","4cdbd20e":"code","23d8783b":"code","768bc4ba":"code","301f78f1":"code","4a18ca0b":"code","b501369b":"code","c34ee6b1":"code","d530ba18":"code","46620f59":"code","56a82e5f":"code","8305bdf5":"code","7c2dd2b0":"code","eb5607fa":"code","a87f0d86":"code","8368e053":"code","26db754a":"code","aacbc3dd":"markdown","111afdd8":"markdown","71d91931":"markdown","e6b0b9b8":"markdown","26963764":"markdown","6434e5e0":"markdown","e25efb23":"markdown","fbb62086":"markdown","12a00b44":"markdown","da5ff7fd":"markdown","9b56edc9":"markdown","660281d5":"markdown","66c510f6":"markdown","a8577378":"markdown","e0a1131f":"markdown","c112f0db":"markdown","99be6595":"markdown","9fe304fc":"markdown","f8814b0a":"markdown","3f499619":"markdown","40f56b15":"markdown","7f6067b2":"markdown","9e7f98a1":"markdown"},"source":{"dff2e9ed":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1295a6e2":"!pip install pyspark","34c1e028":"from sklearn.preprocessing import MultiLabelBinarizer\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as f\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\n\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('anime-data').getOrCreate()","c55930b1":"anime_schema = StructType(fields=[StructField('MAL_ID', IntegerType()),\n                           StructField('name', StringType()),\n                            StructField('score', DoubleType()),\n                            StructField('genres', StringType()),\n                            StructField('synopsis', StringType())\n                           ])\n\n\nanimelist_schema = StructType(fields=[StructField('user_id', IntegerType()),\n                           StructField('anime_id', IntegerType()),\n                            StructField('rating', IntegerType()),\n                            StructField('watching_status', IntegerType()),\n                            StructField('watched_episodes', IntegerType())\n                           ])","fc02c502":"anime = spark.read.csv('\/kaggle\/input\/anime-recommendation-database-2020\/anime_with_synopsis.csv', \n              schema=anime_schema,\n              sep=',',\n              header=True)\n\nanimelist = spark.read.csv('\/kaggle\/input\/anime-recommendation-database-2020\/animelist.csv', \n              schema=animelist_schema,\n              sep=',',\n              header=True)","3205b20a":"anime.show(5)","f0d5cafe":"best_anime = anime\\\n            .filter('score is not null')\\\n            .withColumnRenamed('MAL_ID', 'anime_id')\\\n            .orderBy('score', ascending=False)\\\n            .cache()\n\nbest_anime.show(10)","ee932c81":"best_anime_pandas = best_anime.toPandas()\nbest_anime_pandas.head(10)","d68fd99e":"popular_genres = anime.withColumn('genres_list', f.split('genres', ','))\\\n                     .withColumn('genre', f.explode('genres_list'))\\\n                     .groupBy('genre')\\\n                     .agg(f.count(f.col('name')).alias('genre_count'), f.avg(f.col('score')).alias('avg_score'))\\\n                     .orderBy('genre_count', ascending=False)\\\n                    .withColumn('pct_of_total', f.col('genre_count') \/ f.sum('genre_count').over(Window.partitionBy()))\\\n                    .withColumn('pct_of_total', f.col('pct_of_total') * 100)\\\n                    .cache()\n\npopular_genres.show(10)","d689cf27":"popular_genres.select(f.sum(popular_genres['pct_of_total']).alias('pct_sum')).show()","07e69398":"anime_10th_ptile = anime.filter('score is not null')\\\n            .select('name', 'genres', 'score', f.percent_rank().over(Window.partitionBy().orderBy(anime['score'])).alias('pct_rank'))\\\n            .filter('pct_rank >= 0.9')\\\n\npopular_genres_10th_ptile = anime_10th_ptile.withColumn('genres_list', f.split('genres', ','))\\\n             .withColumn('genre', f.explode('genres_list'))\\\n             .groupBy('genre')\\\n             .agg(f.count(f.col('name')).alias('genre_count'), f.avg(f.col('score')).alias('avg_score'))\\\n             .orderBy('genre_count', ascending=False)\\\n            .withColumn('pct_of_total', f.col('genre_count') \/ f.sum('genre_count').over(Window.partitionBy()))\\\n            .withColumn('pct_of_total', f.col('pct_of_total') * 100)\\\n            .cache()\n\npopular_compare = popular_genres[['genre', 'pct_of_total']].withColumnRenamed('pct_of_total', 'pct_of_total_left')\\\n    .join(popular_genres_10th_ptile[['genre', 'pct_of_total']].withColumnRenamed('pct_of_total', 'pct_of_total_right'), on='genre', how='left')\\\n    .withColumn('abs_difference', f.abs(f.col('pct_of_total_right') - f.col('pct_of_total_left')))\\\n    .orderBy('abs_difference', ascending=False)\n\npopular_compare.show(10)","d9fe2cb7":"popular_compare.select('genre').filter(f.col('pct_of_total_right').isNull()).show(10)","55e54d20":"animelist.show(5)","4cdbd20e":"most_watched_anime = animelist.groupBy('anime_id')\\\n    .agg(f.count(f.col('user_id')).alias('user_cnt'),\n         f.mean(f.col('rating')).alias('mean_rating'),\n         f.stddev(f.col('rating')).alias('std_rating'),\n         f.percentile_approx(f.col('rating'), 0.5).alias('median_rating'),\n         f.mean(f.col('watched_episodes')).alias('mean_num_episodes'))\\\n    .orderBy('user_cnt', ascending=False)\\\n    .cache()\n    \n    ","23d8783b":"%%time\nmost_watched_anime.show(10)","768bc4ba":"anime_joined = best_anime\\\n    .join(most_watched_anime, on='anime_id', how='inner')\\\n    .orderBy('user_cnt', ascending=False)\\\n    .cache()\n\nanime_joined.show(3, truncate=False, vertical=True)","301f78f1":"genres_list = [g['genre'].strip() for g in popular_genres.select('genre').collect()]","4a18ca0b":"print('Unique genres: %s' % len(genres_list))","b501369b":"ml_bin = MultiLabelBinarizer()\nml_bin.fit([genres_list])\n\nprint(ml_bin.classes_[:10])\nprint(len(ml_bin.classes_))","c34ee6b1":"def binarize_genres(entry, binarizer):\n    entry_list = [el.strip() for el in entry.split(', ')]\n    entry_list = [el for el in entry_list if el != '']\n    entry_tpl = tuple(entry_list)\n    vector = binarizer.transform([entry_tpl])\n    return [int(i) for i in vector[0]]","d530ba18":"binarize_genres_udf = f.udf(lambda x: binarize_genres(x, ml_bin), returnType=ArrayType(IntegerType()))","46620f59":"data = anime_joined\\\n    .withColumn('genres_binarized', binarize_genres_udf(f.col('genres')))\\\n    .select(['anime_id', 'genres_binarized', 'user_cnt', 'mean_num_episodes', 'score'])\\\n    .cache()\n\nfor idx, genre_name in enumerate(ml_bin.classes_):\n    data = data.withColumn(f'genre_{idx}', f.col('genres_binarized').getItem(idx))","56a82e5f":"print(data.columns)","8305bdf5":"data = data.drop('genres_binarized')","7c2dd2b0":"assembler = VectorAssembler(inputCols=data.drop('score').columns,\n                            outputCol='features')\n\nlinreg = LinearRegression(featuresCol='features', labelCol='score')\n\npipeline = Pipeline(stages=[assembler,\n                           linreg])","eb5607fa":"pipeline_model = pipeline.fit(data)","a87f0d86":"result = pipeline_model.transform(data)","8368e053":"result.select('prediction', 'score').sample(0.1).show()","26db754a":"spark.stop()","aacbc3dd":"### Subsetting data, filtering, grouping, aggregating the results of grouping, ordering, renaming columns etc. - all these typical operations are done by Spark in a 'lazy' manner. The actual computation of transformation will occur only when calling an action.","111afdd8":"### First, let's install PySpark with pip and import all the necessary functions:","71d91931":"### Let's do something more complicated: \n\n**1) Find the top 10% of anime by score;**\n\n**2) Get the distribution of genres among top 10%;**\n\n**3) Compare this distribution with the one of all anime. List anime titles with the greatest absolute difference in distributions**","e6b0b9b8":"When using SparkML (built-in machine-learning models' classes in Spark), you have to input your data in form of a one big Vector-column.","26963764":"We'll use sklearn MultiLabelBinarizer. We fit 83 genres we have into it.","6434e5e0":"### Next, let's read csv-files proving our pre-defined schemas.","e25efb23":"Hmmm... it might seem odd that 2 GB data were read so quickly. Actually, so far Spark hasn't read anything. Spark implements so-called 'lazy evaluation' which basically means that it won't handle any actual data untill we ask it to do so. In another words, we can define multiple 'transformations' which we want to make on our data, but they will be actually implemented only when we call an 'action' function.\n\n\nFor example, showing N rows of the resulting DataFrame is an 'action'. Calling .show() method will trigger execution of the chain of transformations which we defined previously, so that actual 'computation' takes place.","fbb62086":"Next, we define a custom UDF (User-Defined Function) that we'll feed into Spark in a groupby statement.","12a00b44":"### It is preferable to expicitly set the structure of data which we'll be reading with PySpark. It can be done by defining schemas. Basically, schema is a list of columns to be found in our table data as well as types of each column.","da5ff7fd":"### Motivation: data is large\n\nOur data is around 2 GB. It would be cumbersome to analyse it using traditional instruments for flat table data such as pandas. Therefore let's try using Apache Spark (or it's Python wrapper: PySpark) to get around this problem.","9b56edc9":"### Now, let's find out what are the most popular anime genres in our data. Also let's calculate the average score of each genre and it's share among all the data.","660281d5":"### Not forget to stop SparkSession in the end!","66c510f6":"### For example, this way we can get the top-10 scored anime in our data:","a8577378":"### Let's join the resulting DataFrame with another one and sort the result by user count","e0a1131f":"### Let's analyse really big table data, which was in *anime-recommendation-database-2020\/animelist.csv* . This dataframe contains evaluations of anime titles by each user.","c112f0db":"It really takes long time to process...","99be6595":"### Pct_of_total adds up to 100% which is a sign that we've calculated this column correctly:","9fe304fc":"First, we encode each genre an anime has into 1\/0:","f8814b0a":"Spark allows not only for pandas-like data wrangling, but also inference with standard ML-models.\n\nFor example, let's try to fit a simple linear regression model.","3f499619":"### Let's group by anime_id and calculate some useful stats:","40f56b15":"### Genres, which are not present among the top 10% anime (by score):","7f6067b2":"The results seem to be mediocre - but the purpose of this exerise was to familiarize ourselves with PySpark functionality and not to make a flawless ML-model.","9e7f98a1":"### Conveniently, Spark DataFrames can be exported into our favorite pandas DataFrame at anytime:"}}