{"cell_type":{"5822ac6e":"code","4fa59d07":"code","832da75f":"code","daaa42bc":"code","4d284e92":"code","26f91ef5":"code","097f0d10":"code","f9b7918e":"code","949580c5":"code","318acd44":"code","9e99040c":"code","d7e92c63":"code","b219a7b0":"code","190a601e":"code","1c75eed1":"code","2933abc3":"code","c9dfde56":"code","bfafe57e":"code","953ea82f":"code","85dfc454":"code","054721b2":"code","f6814b0a":"code","d5c175c7":"code","eebd0339":"code","e7041fb4":"code","35a8269e":"code","a9ec5f9b":"code","d5add55b":"code","470b5af1":"code","d443ebde":"code","14beb006":"code","43914ec9":"code","d080fe8a":"code","fde0a167":"code","b9de15be":"code","717171ce":"code","10c797c2":"code","ca8dabae":"code","e8ae32db":"code","9ac23276":"code","ebc83680":"code","c21a406e":"code","da35b24c":"code","de976e18":"code","b7eca6a4":"code","a22ef6c6":"code","7cbd6dfe":"code","0839b880":"code","0f6d5a82":"code","d0dadeb9":"code","a7b4d80a":"code","7eca534a":"code","4851e157":"code","ab6acb0e":"code","9fbc5bc9":"code","e94cdd24":"markdown","1d336e93":"markdown","3261fae6":"markdown","7d1f422b":"markdown","e05bd929":"markdown","e904b6af":"markdown","fbbeda95":"markdown","ff24ec5a":"markdown","6c33d399":"markdown","8dc69d67":"markdown","56ebc9cf":"markdown","25f7f68a":"markdown","f82f59a0":"markdown","5708e489":"markdown","2440dd6f":"markdown","6fc5be94":"markdown","8501f27c":"markdown"},"source":{"5822ac6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4fa59d07":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor","832da75f":"df = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")","daaa42bc":"df.head()","4d284e92":"df.tail()","26f91ef5":"df.isnull().sum()","097f0d10":"# convert categorical variable to numerical\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\n# sex\nle.fit(df.sex.drop_duplicates())\ndf.sex = le.transform(df.sex)\n\n# smoke\nle.fit(df.smoker.drop_duplicates())\ndf.smoker = le.transform(df.smoker)","f9b7918e":"df.head()","949580c5":"# Check unique values in 'region' column\ndf['region'].unique()","318acd44":"region_dummies = pd.get_dummies(df['region'], drop_first = True)\nregion_dummies.head()","9e99040c":"df = pd.concat([df, region_dummies], axis = 1)","d7e92c63":"df.drop(['region'], axis = 1, inplace = True)","b219a7b0":"df.head()","190a601e":"# Check NA values\ndf.isnull().sum()","1c75eed1":"# Check the DataFrame Info\ndf.info()","2933abc3":"## group by region","c9dfde56":"# Group the Age to see the relationship between age and charges\ndf_age = df.groupby(by = 'age').mean()\ndf_age","bfafe57e":"df.describe()","953ea82f":"df[['age', 'sex', 'bmi', 'children', 'smoker', 'charges']].hist(bins = 30, figsize = (10,10), color = 'blue')","85dfc454":"#plot pairplot\nsns.pairplot(df)","054721b2":"# Explore the relation between age and charges\nsns.regplot(x = 'age', y = 'charges', data = df)\nplt.show()","f6814b0a":"# Explore the relation between BMI and charges\nsns.regplot(x = 'bmi', y = 'charges', data = df)\nplt.show()","d5c175c7":"# Check the correlation\ncorr = df.corr()\ncorr","eebd0339":"plt.figure(figsize=(8,8))\np = sns.heatmap(df.corr(),cmap=\"coolwarm\",annot=True, square =True)","e7041fb4":"df.columns","35a8269e":"X = df.drop(['charges'], axis = 1)\ny = df.charges","a9ec5f9b":"X = np.array(X).astype('float32')\ny = np.array(y).astype('float32')","d5add55b":"y_reshape = y.reshape(-1,1)","470b5af1":"from sklearn.preprocessing import StandardScaler\n\nscaler_x = StandardScaler()\nX_scale = scaler_x.fit_transform(X)\n\nscaler_y = StandardScaler()\ny_scale = scaler_y.fit_transform(y_reshape)","d443ebde":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scale,y_scale,test_size = 0.2,random_state = 42)","14beb006":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","43914ec9":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,accuracy_score\nfrom math import sqrt\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nlr_R2 = lr.score(X_test,y_test)\nlr_R2","d080fe8a":"# Predict the value\ny_pred = lr.predict(X_test)\n\ny_pred_orig = scaler_y.inverse_transform(y_pred)\ny_test_orig = scaler_y.inverse_transform(y_test)","fde0a167":"k = X_test.shape[1]\nn = len(X_test)\n\nRMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)),'.3f'))\nMSE = mean_squared_error(y_test_orig, y_pred_orig)\nMAE = mean_absolute_error(y_test_orig, y_pred_orig)\nr2 = r2_score(y_test_orig, y_pred_orig)\nadj_r2 = 1 - (1-r2) * (n-1)\/(n-k)\n\nprint('RMSE =',RMSE, \n      '\\nMSE =',MSE, \n      '\\nMAE =',MAE,\n      '\\nR2=',r2, \n      '\\nAdjusted R2 =',adj_r2)","b9de15be":"quad = PolynomialFeatures (degree = 2)\nX_quad = quad.fit_transform(X_scale)\n\nX_train_plr,X_test_plr,y_train_plr,y_test_plr = train_test_split(X_quad,y_scale,test_size = 0.2, random_state = 0)\n\nplr = LinearRegression().fit(X_train_plr,y_train_plr)\n\nplr_R2 = plr.score(X_test_plr,y_test_plr)\nplr_R2","717171ce":"# Predict the value\ny_pred = plr.predict(X_test_plr)\n\ny_pred_orig = scaler_y.inverse_transform(y_pred)\ny_test_orig = scaler_y.inverse_transform(y_test_plr)","10c797c2":"k = X_test.shape[1]\nn = len(X_test)\n\nRMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)),'.3f'))\nMSE = mean_squared_error(y_test_orig, y_pred_orig)\nMAE = mean_absolute_error(y_test_orig, y_pred_orig)\nr2 = r2_score(y_test_orig, y_pred_orig)\nadj_r2 = 1 - (1-r2) * (n-1)\/(n-k)\n\nprint('RMSE =',RMSE, \n      '\\nMSE =',MSE, \n      '\\nMAE =',MAE,\n      '\\nR2=',r2, \n      '\\nAdjusted R2 =',adj_r2)","ca8dabae":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam","e8ae32db":"ANN_model = keras.Sequential()\nANN_model.add(Dense(50, input_dim = 8))\nANN_model.add(Activation('relu'))\n\nANN_model.add(Dense(180))\nANN_model.add(Activation('relu'))\nANN_model.add(Dropout(0.3))\n\nANN_model.add(Dense(180))\nANN_model.add(Activation('relu'))\nANN_model.add(Dropout(0.3))\n\nANN_model.add(Dense(50))\nANN_model.add(Activation('linear'))\n\nANN_model.add(Dense(1))\nANN_model.compile(loss = 'mse', optimizer = 'adam')\nANN_model.summary()","9ac23276":"# Compile the model and trainning\nANN_model.compile(optimizer= 'Adam', loss='mean_squared_error')\nepochs_hist = ANN_model.fit(X_train, y_train, epochs= 50, batch_size = 20, validation_split= 0.2)\n\nresult = ANN_model.evaluate(X_test, y_test)\naccuracy_ANN = 1 - result\nprint('Accuracy : {}'.format(accuracy_ANN) )","ebc83680":"epochs_hist.history.keys()\nplt.plot(epochs_hist.history['loss'])\nplt.plot(epochs_hist.history['val_loss'])\nplt.title('Model Loss Progross During Trainning')\nplt.xlabel('Epochs')\nplt.ylabel('Training and validation loss')\nplt.legend(['Trainnig Loss','Validation Loss'])","c21a406e":"# change the color\ny_pred = ANN_model.predict(X_test)\n\ny_pred_orig = scaler_y.inverse_transform(y_pred)\ny_test_orig = scaler_y.inverse_transform(y_test)\n\nplt.plot(y_test_orig, y_pred_orig, \"^\", color = 'b')\nplt.xlabel('Model Predictions')\nplt.ylabel('True Values')","da35b24c":"k = X_test.shape[1]\nn = len(X_test)\n\nRMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)),'.3f'))\nMSE = mean_squared_error(y_test_orig, y_pred_orig)\nMAE = mean_absolute_error(y_test_orig, y_pred_orig)\nr2 = r2_score(y_test_orig, y_pred_orig)\nadj_r2 = 1 - (1-r2) * (n-1)\/(n-k)\n\nprint('RMSE =',RMSE, \n      '\\nMSE =',MSE, \n      '\\nMAE =',MAE,\n      '\\nR2=',r2, \n      '\\nAdjusted R2 =',adj_r2)","de976e18":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [4, 5, 6, 7],\n    'max_features': [2, 3, 4],\n    'min_samples_leaf': [2, 3, 4],\n    'min_samples_split': [3, 4, 5],\n    'n_estimators': [10, 30, 50, 100]\n}\n\n# Create a based model\nrf = RandomForestRegressor(criterion = 'mse')\n\n# Instantiate the grid search model\ngrid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 4)","b7eca6a4":"grid_search_rf.fit(X_train, y_train)\n\ngrid_search_rf.best_params_","a22ef6c6":"grid_search_rf_R2 = grid_search_rf.score(X_test,y_test)\ngrid_search_rf_R2","7cbd6dfe":"# Predict the value\ny_pred = grid_search_rf.predict(X_test)\n\ny_pred_orig = scaler_y.inverse_transform(y_pred)\ny_test_orig = scaler_y.inverse_transform(y_test)","0839b880":"k = X_test.shape[1]\nn = len(X_test)\n\nRMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)),'.3f'))\nMSE = mean_squared_error(y_test_orig, y_pred_orig)\nMAE = mean_absolute_error(y_test_orig, y_pred_orig)\nr2 = r2_score(y_test_orig, y_pred_orig)\nadj_r2 = 1 - (1-r2) * (n-1)\/(n-k)\n\nprint('RMSE =',RMSE, \n      '\\nMSE =',MSE, \n      '\\nMAE =',MAE,\n      '\\nR2=',r2, \n      '\\nAdjusted R2 =',adj_r2)","0f6d5a82":"#! pip install xgboost\nimport xgboost as xgb","d0dadeb9":"param_grid = {\n        'gamma': [0.1,0.3, 0.5, 1],   # regularization parameter \n        'subsample': [0.6, 0.8, 1.0], # % of rows taken to build each tree\n        'colsample_bytree': [0.6, 0.8, 1.0], # number of columns used by each tree\n        'max_depth': [3, 4, 5], # depth of each tree\n        'n_estimaters': [10, 30, 50, 100],\n        'learning_rate' :[0.01, 0.03, 0.05]\n        }","a7b4d80a":"xgb_model = xgb.XGBRegressor(objective = 'reg:squarederror')\n\ngrid_xgb = GridSearchCV(xgb_model,\n                    param_grid,\n                    cv = 5, \n                    n_jobs = -1, \n                    verbose = 4)","7eca534a":"grid_xgb.fit(X_train, y_train)\n\ngrid_xgb.best_params_","4851e157":"grid_xgb_R2 = grid_xgb.score(X_test,y_test)\ngrid_xgb_R2","ab6acb0e":"# Predict the value\ny_pred = grid_xgb.predict(X_test)\n\ny_pred_orig = scaler_y.inverse_transform(y_pred)\ny_test_orig = scaler_y.inverse_transform(y_test)","9fbc5bc9":"k = X_test.shape[1]\nn = len(X_test)\n\nRMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)),'.3f'))\nMSE = mean_squared_error(y_test_orig, y_pred_orig)\nMAE = mean_absolute_error(y_test_orig, y_pred_orig)\nr2 = r2_score(y_test_orig, y_pred_orig)\nadj_r2 = 1 - (1-r2) * (n-1)\/(n-k)\n\nprint('RMSE =',RMSE, \n      '\\nMSE =',MSE, \n      '\\nMAE =',MAE,\n      '\\nR2=',r2, \n      '\\nAdjusted R2 =',adj_r2)","e94cdd24":"## 7.Multiple linear regression (Degree = 2)","1d336e93":"Smoke have the most positive relationship with the charges","3261fae6":"## 8.Deep Learning Model (ANN)","7d1f422b":"## 2.Load the Dataset and Libraries","e05bd929":"The people in the southeast have the highest insurance charges and obviously, with the highest BMI index.","e904b6af":"We can get a basic idea that the charges will increase when the age increase.\n\nplot analysis","fbbeda95":"## 11.Summary\nCompare the MSE between Linear Regression, Random Forest, Artificial Neural Network and XGBoost tuned by Grid Search, the best model is XGBoost with the lowest MSE and highest R-squared value of 0.881.","ff24ec5a":"## 5.Split the train test dataset","6c33d399":"## 4.Visualzie the dataset","8dc69d67":"Check the realtionship between the age and Charges, we can see the it is more complex than a linear relationship.\n\nSo we will consider some non-linear Algorithim such as ANN or XgBoost.","56ebc9cf":"## 3.Exploratory Data Analysis","25f7f68a":"## 10.XGBoost","f82f59a0":"## 1.Project Introduction","5708e489":"## 9.Random Forest","2440dd6f":"The objective of this case study is to predict the health insurance cost incurred by Individuals based on their age, gender, BMI, number of children, smoking habit and geo-location.\n\n\u672c\u6848\u4f8b\u7814\u7a76\u7684\u76ee\u7684\u662f\u6839\u636e\u5e74\u9f84\uff0c\u6027\u522b\uff0cBMI\uff0c\u513f\u7ae5\u6570\u91cf\uff0c\u5438\u70df\u4e60\u60ef\u548c\u5730\u7406\u4f4d\u7f6e\u6765\u9884\u6d4b\u4e2a\u4eba\u627f\u62c5\u7684\u5065\u5eb7\u4fdd\u9669\u8d39\u7528\u3002","6fc5be94":"- Features available are:\n\n    - sex: insurance contractor gender, female, male\n\n    - bmi: Body mass index (ideally 18.5 to 24.9)\n\n    - children: Number of children covered by health insurance \/ Number of dependents\n\n    - smoker: smoking habits\n\n    - region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n\n    - charges: Individual medical costs billed by health insurance\n\n\nData Source:https:\/\/www.kaggle.com\/mirichoi0218\/insurance","8501f27c":"## 6.Linear Regression Model"}}