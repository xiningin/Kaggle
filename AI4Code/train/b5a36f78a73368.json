{"cell_type":{"21b97163":"code","284c0b0a":"code","acdc3e3a":"code","a82b358b":"code","d5e75d07":"code","389fe5ec":"code","8a3d2e7b":"code","685fc3e5":"code","ff71ad70":"code","daad2b74":"code","90ba74bc":"code","03eac2cc":"code","6ca8a4aa":"code","fbbefe3f":"code","e547f4ec":"markdown","3dd0cf8d":"markdown","494573c8":"markdown","efc9ac1f":"markdown","6ed3a2af":"markdown","eb9195a0":"markdown","b981a0bd":"markdown","9003c3d1":"markdown","38c666fd":"markdown","525efc5f":"markdown"},"source":{"21b97163":"#!pip install fasttext\n#!pip install nltk\nimport numpy as np\nimport pandas as pd\nimport fasttext\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nstopwords = set(stopwords.words('english'))\nstemmer = WordNetLemmatizer()","284c0b0a":"df = pd.read_csv(\"..\/input\/news-headlines-for-sarcasm-detection\/Data.csv\")\ndf.head()","acdc3e3a":"# Separate the data to treat them\nheadlines = df['headlines'].values.tolist()\ntarget = df[\"target\"].values.tolist()","a82b358b":"def apply_re_and_return_lower(headlines):\n  re_list = []\n  for text in headlines:\n    text = re.sub(\"[^0-9A-Za-z ]\", \"\", text)\n    re_list.append(text)\n  return_list = []\n  for title in re_list:\n    tokens = word_tokenize(title) #imported from nltk\n    working_list = []\n    for word in tokens:\n      if word not in stopwords: #check stopwords variable in imports\n        working_list.append(stemmer.lemmatize(word.lower()))\n    return_list.append(' '.join(working_list))\n  return return_list\n\ndef change_labels_for_fasttext(target):\n  target_cleaned = []\n  for label in target:\n    if label == 'Sarcastic':\n      target_cleaned.append(\"__label__positive\")\n    else:\n      target_cleaned.append(\"__label__negative\")\n  return target_cleaned","d5e75d07":"# Apply functions\nheadlines = apply_re_and_return_lower(headlines)\ntarget = change_labels_for_fasttext(target)","389fe5ec":"# Check if there was any error along the way and create the dataset to join and split\nassert len(headlines) == len(target)\ndataset = []\nfor index in range(0, len(headlines)):\n  new_text = target[index] + \" \" + headlines[index]\n  dataset.append(new_text)","8a3d2e7b":"# Split the data\nfrom sklearn.model_selection import train_test_split\ndataset_train, dataset_test = train_test_split(dataset, test_size=0.2, random_state=45)","685fc3e5":"# Fasttext required the data to be separated in txt files\nnp.savetxt(\"x_train_ft.txt\", dataset_train, delimiter=\"\\n\", fmt=\"%s\")\nnp.savetxt(\"x_test_ft.txt\", dataset_test, delimiter=\"\\n\", fmt=\"%s\")","ff71ad70":"# train the model with pre-trained word vectors\nmodel = fasttext.train_supervised(input=\"x_train_ft.txt\", lr=0.075, epoch=10)","daad2b74":"# Test the model\n# Outputs (number of words, precision, recall)\nnum_words, precision, recall = model.test(\"x_test_ft.txt\")","90ba74bc":"F1_score = 2*((precision*recall)\/(precision+recall))\nF1_score","03eac2cc":"# Link: https:\/\/www.theonion.com\/lawyer-explains-that-just-because-you-accidentally-kill-1848251296\nmodel.predict((\"Lawyer Explains That Just Because You Accidentally Kill Santa Doesn't Mean You\u2019re Legally Obligated To Take His Place\").lower())","6ca8a4aa":"# Link: https:\/\/edition.cnn.com\/travel\/article\/qatar-hilton-salwa-resort\/index.html\nmodel.predict((\"Hilton Salwa: The gigantic luxury hotel in the middle of nowhere\").lower())","fbbefe3f":"# Link: https:\/\/www.theonion.com\/want-to-make-these-amazing-glitter-bagels-too-bad-the-1848228492\nmodel.predict((\"Want To Make These Amazing Glitter Bagels? Too Bad, They\u2019re My Secret Recipe And I\u2019ll Never Tell, Even If You Shoot My Kids In Front Of Me\").lower())","e547f4ec":"For the people new to nlp, nltk is a great library with a lot of essential features to aid us in sentiment analysis and other NLP tasks.\n\nHere our interest lies on sentiment analysis and with the super 'clean' dataset we will only focus on removing the stopwords and lemmatizing.\n\nStopwords: words that do not help in defining a sentiment in our case.\n\nLemma: the core meaning of a word.\n\nMost news headlines will already convey the core idea, so lemmatizing will only serve the purpose for most long and fluffy satirical headlines. It also helps on processing time since less tokens are necessary.\n","3dd0cf8d":"# Imports","494573c8":"# Training and Evaluation","efc9ac1f":"Hello everyone and welcome to my first submission on Kaggle.\n\nWhile I a lot of people might think that complex models with a long training time is required (especially in NLP), I want to show that this is not always the case.\n\nIn this notebook, I will train a model that uses pre-trained word vectors already and that is fasttext.\n\nHere is a link of the docs, in case any of you are interested in further reading:\n\nhttps:\/\/fasttext.cc\/\n\nSince the dataset is basically already clean, almost no preprocessing is needed for the following reasons:\n\n- News headlines are already short in nature, so there will not be a problem with text length, loss of meaning etc.\n\n- The words are already pre-trained so 99% of the training time will not be required.\n\n- The task is really simple and perhaps other more complex models might perform worse.","6ed3a2af":"### apply_re_and_return_lower\nFor fasttext, punctuation will not help convey a meaning of the word, the .lower() is to avoid differentiation of the same words (Fast and fast are the same words for humans, but different for Python)\n\n### change_labels_for_fasttext\nFasttext has a particularity that requires the data to be labeled a certain way and \"joined\" with the dataset, which will be done later. For now, we only want the labels to be the way we want them.","eb9195a0":"Given that the dataset is small, I will not prioritize computing power and etc.\n\nFor that, I will keep doing the process with list since it will also help beginners to understand.","b981a0bd":"Very quickly, one was able to train a model to detect sentiment quickly without a lot of computational resources.\n\nI hope you found this model informative and I am happy to answer any comments.\n\nThank you for reading!\n\nLuis Sejas","9003c3d1":"# Conclusion","38c666fd":"# Preprocessing","525efc5f":"# Trying with Real World Examples"}}