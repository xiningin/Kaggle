{"cell_type":{"4d811d70":"code","df5c148c":"code","d35cda42":"code","9096d67f":"code","06736120":"code","7aa5c50f":"code","892888fd":"code","af4bbf13":"code","7feb1980":"code","3dbbc2b0":"code","7b71fe67":"code","379d5340":"code","2c0ec248":"code","228a9138":"code","db996473":"code","44c63c2c":"code","5d699033":"code","cfb842a4":"code","f2637314":"code","3a9ea87d":"code","86085c5b":"code","d53eed5b":"code","bf6af967":"code","8612651d":"code","fc4ced34":"code","f9099025":"code","b4515481":"code","5b7c7dfe":"code","b053a2ea":"code","f7ed8845":"code","ffc8583b":"code","b6c5b2bf":"code","dd45c3e8":"code","4c99c903":"code","f4bf8621":"code","576bf572":"code","39214239":"markdown","e08f3b33":"markdown","b9229a30":"markdown","bb386971":"markdown","ad53b73e":"markdown","cd254343":"markdown","85764c25":"markdown","e767f9cd":"markdown","230a6400":"markdown","843f2ed4":"markdown","2f1f80d0":"markdown","e741978f":"markdown","a3cc7070":"markdown","f145584d":"markdown","cce6d047":"markdown","a58f3e0e":"markdown","50f474cd":"markdown","f3219754":"markdown","c6517957":"markdown","4ef29f3a":"markdown","682e5711":"markdown","eb34c857":"markdown","6fc50635":"markdown","ab9f0b8b":"markdown","5ecd7814":"markdown","a3fe3871":"markdown","dd398bb3":"markdown","116e1882":"markdown","9213eb33":"markdown","c1c9988f":"markdown","3a7045c9":"markdown","488fbe9a":"markdown","fa0d0af6":"markdown","6bebafbc":"markdown"},"source":{"4d811d70":"!pip install plotly","df5c148c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nimport sklearn\nimport janestreet\nimport os, sys\nimport gc\nimport math\nimport random\nimport pathlib\nfrom sklearn.preprocessing import MinMaxScaler\nfrom catboost import CatBoostClassifier\nimport cv2\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","d35cda42":"train_df=pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')","9096d67f":"print('Total number of entries in the train dataset are:', len(train_df))\ntrain_df.head()","06736120":"features_df = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv', index_col = 0)\nprint('Total number of features are:', len(features_df))\nfeatures_df.head()\n","7aa5c50f":"print(\"Properties\",train_df.info())\nprint(\"Shape:\",train_df.shape)","892888fd":"train_df.describe()","af4bbf13":"print(\"Properties\",features_df.info())\nprint(\"Shape:\",features_df.shape)","7feb1980":"features_df.describe()","3dbbc2b0":"train_df.isnull().sum()","7b71fe67":"train_df.isna().head()","379d5340":"features_df.isna().head()","2c0ec248":"fig=px.bar(x = train_df.isnull().sum().index,y = train_df.isnull().sum().values, labels = dict(x = \"Attributes\", y = \"Number of Missing Values\"), title= 'Missing Data')\nfig.show()","228a9138":"tags = features_df.sum(axis = 1)\ntags_dict = {'Features' : tags.index.values, 'Tag Count' : tags.values}\ntags_df = pd.DataFrame(tags_dict)\nplt.figure(figsize = (130, 25))\nplt.xlabel('Features', fontsize = 100)\nplt.ylabel('Tag Count', fontsize = 100)\nplt.title('Tag Counts of Features', fontsize = 120)\nplt.xticks(rotation ='vertical', fontsize = 50)\nplt.yticks(fontsize = 50)\nsns.barplot(x = 'Features', y = 'Tag Count', data = tags_df,palette=\"viridis\")\nplt.show()\ndel tags_dict,tags_df","db996473":"train_df['feature_0'].value_counts()","44c63c2c":"feature_0_is_plus_one  = train_df.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = train_df.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return', color=\"purple\")\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return', color=\"violet\")\nax1.set_title (\"feature_0 = 1\", fontsize=18)\nax2.set_title (\"feature_0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"lower right\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();","5d699033":"\nnull_weights = (train_df['weight'] == 0).sum()\ntotal_weights = len(train_df['weight'])\nnull_weights_per = null_weights \/ total_weights * 100\nplt.figure(figsize = (15, 6))\nplt.pie(((train_df.weight==0).mean(),(1-(train_df.weight==0).mean())), explode = (0, 0.1),shadow=True, labels=(f'Null Weights\\n{round((train_df.weight==0).mean()*100,3)}%',f'Considerable Weights\\n{round((1-(train_df.weight==0).mean())*100,3)}%'.format()), colors = ['pink', 'green'])\nplt.legend(title='Weights')\nplt.show()","cfb842a4":"date_weight_df = pd.DataFrame({'Date' : np.unique(train_df['date'].values), 'NULL_Weights' : train_df[train_df['weight'] == 0.0].groupby(['date']).size().values, 'NOT_NULL_Weights' : train_df[train_df['weight'] != 0.0].groupby(['date']).size().values})\ndate_weight_df.head()","f2637314":"fig = plt.figure(figsize=(505, 100))\n\nplt.xticks(rotation ='vertical', fontsize = 60)\nplt.yticks(fontsize = 200)\n\nax = fig.add_subplot(111) \nax2 = ax.twinx() \n\ndate_weight_df.NOT_NULL_Weights.plot(kind='bar',color='green',ax=ax, position = 0)\ndate_weight_df.NULL_Weights.plot(kind='bar',color='pink', ax=ax2, position = 1)\n\nax.grid(None)\nax2.grid(None)\n\nax.set_ylabel('NOT NULL Weights', fontsize = 300)\nax2.set_ylabel('NULL Weights', fontsize = 300)\nax.set_xlabel('Time (in Days)',fontsize = 300)\nfig.suptitle('NULL Weights Vs NOT NULL Weights per Day', fontsize = 450)\n\nax.set_xlim(-1, 505)\n\nplt.show()","3a9ea87d":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_df['weight'], \n             bins=1000, \n             kde_kws={\"clip\":(0.001,1)}, \n             hist_kws={\"range\":(0.001,1)},\n             color='purple', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\nplt.xlabel(\"Histogram of non-zero weights\", size=10)\nplt.show();\ndel values\ngc.collect();","86085c5b":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_df['resp']).cumsum()\nresp_1= pd.Series(train_df['resp_1']).cumsum()\nresp_2= pd.Series(train_df['resp_2']).cumsum()\nresp_3= pd.Series(train_df['resp_3']).cumsum()\nresp_4= pd.Series(train_df['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\")\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();","d53eed5b":"train_df['weight_resp']   = train_df['weight']* train_df['resp']\ntrain_df['weight_resp_1'] = train_df['weight']* train_df['resp_1']\ntrain_df['weight_resp_2'] = train_df['weight']* train_df['resp_2']\ntrain_df['weight_resp_3'] = train_df['weight']* train_df['resp_3']\ntrain_df['weight_resp_4'] = train_df['weight']* train_df['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+( train_df.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+( train_df.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+( train_df.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+( train_df.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+( train_df.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return(500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\nplt.legend(loc=\"lower left\")\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect()","bf6af967":"corr = train_df.iloc[:, 7 : 137].corr()\npx.imshow(corr, labels = dict(x = \"Features\", y = \"Features\"), width = 1000, height = 1000, title = \"Correlation between Features\",color_continuous_scale='algae')","8612651d":"plt.figure(figsize = (20, 5))\nfig = sns.heatmap(train_df.corr().iloc[2 : 7, 7 : -6], cmap = 'Paired')\nfig.set(xlabel = 'Resps', ylabel = 'Features')\nplt.show()","fc4ced34":"def show_corr_heatmap(df, method=\"pearson\", width=10, calc_corr=False, annot=True):\n    \n    if calc_corr == True:\n        if method == \"MI\":\n            corr = MI_correlations(df)\n        else:\n            corr = df.corr(method)\n    else:\n        corr = df\n        \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(width, width))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=annot, fmt=\".2f\")\n    \n    if calc_corr == True:\n        return corr\n\n\ndef MI_correlations(df):\n    corrs = {}\n    for col_init in df.columns:\n        corrs[col_init] = {}\n        for col_corr in df.columns:\n            if col_init != col_corr:\n                corrs[col_init][col_corr] = calc_MI(df[col_init], df[col_corr])\n\n    return pd.DataFrame(corrs)\n\ndef calc_MI(col_init, col_corr):\n    \n    if col_init.dtype == np.object:\n        col_init = col_init.astype('category').cat.codes\n    elif col_init.dtype.name == \"category\":\n        col_init = col_init.cat.codes\n        \n    if col_corr.dtype == np.object:\n        col_corr = col_corr.astype('category').cat.codes\n    elif col_corr.dtype.name == \"category\":\n        col_corr = col_corr.cat.codes\n\n    mi = mutual_info_score(col_init, col_corr)\n\n    return mi","f9099025":"unnamed_features = [x for x in train_df.columns if \"feature\" in x]\ncorr_matrix = show_corr_heatmap(train_df[unnamed_features].iloc[0:50, 0:50], method=\"spearman\", width=30, calc_corr=True, annot=True)","b4515481":"corr_matrix = show_corr_heatmap(train_df[unnamed_features].iloc[51:100, 51:100], method=\"spearman\", width=30, calc_corr=True, annot=True)\ngc.collect()","5b7c7dfe":"sns.pairplot(corr.iloc[110 : 120, 110 : 120])\nplt.show()","b053a2ea":"sns.pairplot(corr.iloc[120 : 130, 120 : 130])\nplt.show()","f7ed8845":"for i in range(120, 122):\n\n    fig, axes = plt.subplots(2, 2, figsize=(12,12))\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp\", ax = axes[0, 0], color = 'red')\n    axes[0,0].set_title(f\"Feature {str(i)} and Resp\", fontsize = 12)\n    axes[0,0].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_1\", ax = axes[0, 1], color = 'blue')\n    axes[0,1].set_title(f\"Feature {str(i)} and Resp 1\", fontsize = 12)\n    axes[0,1].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_2\", ax = axes[1, 0], color = 'green')\n    axes[1,0].set_title(f\"Feature {str(i)} and Resp 2\", fontsize = 12)\n    axes[1,0].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_3\", ax = axes[1, 1], color = 'yellow')\n    axes[1,1].set_title(f\"Feature {str(i)} and Resp 3\", fontsize = 12)\n    axes[1,1].legend(labels=[f'Feature {str(i)}'])\n    \n    plt.show()\ngc.collect()","ffc8583b":"trades_per_day = train_df.groupby(['date'])['ts_id'].count()\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(trades_per_day, color=\"purple\")\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of ts_id for each day\", fontsize=18)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=200)\nplt.show()\ndel trades_per_day\ngc.collect()","b6c5b2bf":"features = [col for col in list(train_df.columns) if 'feature' in col]\n\ntrain_df = train_df[train_df['weight'] != 0]","dd45c3e8":"train_df['action'] = (train_df['resp'].values > 0).astype(int)","4c99c903":"NAN_VALUE = -999\n\nf_mean = train_df.mean()\ntrain_df.fillna(f_mean)\n\nX_train = train_df.loc[:, features]\ny_train = train_df.loc[:, 'action']\n\ndel train_df\ngc.collect()","f4bf8621":"\nprint('Creating classifier...', end='')\nclf = CatBoostClassifier(loss_function = 'Logloss',\n                         task_type=\"GPU\",\n                         learning_rate = 0.1)\n\n\nclf.fit(X_train, y_train)\n\nprint('Finished.')\n\ndel X_train, y_train\ngc.collect()","576bf572":"\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:    \n    test_weight = test_df.iloc[0].weight\n    if test_weight > 0:\n        sample_prediction_df.action = clf.predict(test_df.loc[:, features].fillna(NAN_VALUE))[0]\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","39214239":"## Importing all the necessary packages","e08f3b33":"**Features 0-50**","b9229a30":"**ANALYSIS:** we can see that there were 'bigger' gains in the beginning and as time approach 500, the gain becomes smaller. In conclusion, the earlier trades are much bigger but we don't know what it's going to be like in our competition test set.","bb386971":"**Correlation matrix is really large and confusing but there are clearly some patterns. I will cut it in parts for easier understanding and compare it to features dataframe.**","ad53b73e":"**Features 51-100**","cd254343":"Imputing NAN values with mean","85764c25":"**Correlation Between features and resps**","e767f9cd":"### Missing Data","230a6400":"**ANALYSIS**\n* feature_0 has no tags\n* feature 79 to 119 all has 4 tags\n* feature 7 to 36 have 3 and 4 tags periodically\n* Similar trend between 2 to 7, 37 to 40, 120 to 129\n","843f2ed4":"### ts_ids","2f1f80d0":"**POINT OF IMPORTANCE:** Weight and resp multiplied together represents a return on the trade.","e741978f":"# Preprocessing","a3cc7070":"**ANALYSIS** We can see that most weights are around 0.2 and we can see two 'peaks' which is around 0.2 and 0.3. Note that maximum weight was 167.29 represented by 1.0 on x-axis. ","f145584d":"**Feature_0 Analysis**","cce6d047":"**ANALYSIS:** when feature_0 is 1, plot shows negative slope while in contrast, when feature_0 is -1, plot shows positive slope. My guess is that feature_0 corresponds to Buy(1) and Sell(-1) or vice versa. So if we set action to 1 with feature_0 = 1 then we are selling and when we set action to 0 with feature_0 = -1, then we are buying. This makes sense since whether we are buying or selling we can still lose or gain profit.","a58f3e0e":"## Training set","50f474cd":"**Features 120-130**","f3219754":"**Correlation Between Features**","c6517957":"## Using Catboost Classifier","4ef29f3a":"**ANALYSIS:** Features 73-95 are highly correlated. Features 85-95 are closely related that they may show linear relationship.","682e5711":"**ANALYSIS:** Features seem to be forming clusters in the above correlation matrix. Features 17 to 26, 27 to 36, and 120 to 129 are some of the many examples shown. These are positively inclined to eachother. In a cluster, the intra cluster distance is lower than the inter cluster distance. Similarly, certain features are clearly negatively related to other features. Amongst the neutral grid, the postive and negative associations stand out!","eb34c857":"### Visualization","6fc50635":"### Tags","ab9f0b8b":"**Features with resps**","5ecd7814":"## Loading the dataset","a3fe3871":"**Features 110-120**","dd398bb3":"## Installing packages","116e1882":"**ANALYSIS:** dataset has too many NULL weights that can be removed for memory efficiency","9213eb33":"**ANALYSIS:** \nWe can see that resp is closely related to resp_4 (blue and purple). Resp_1 and resp_2 also seem to be closely related but much much linear. Resp_3 seem to be in the middle, where the shape is closer to upper group but position is slightly closer to green and orange.\n","c1c9988f":"### Weights","3a7045c9":"### Cleaning the dataset","488fbe9a":"# Exploratory Data Analysis","fa0d0af6":"## Features","6bebafbc":"**ANALYSIS:** Features are either postively, negatively, or neutrally correlated to Resps. A pattern can be observed in the above heatmap which allows us to explore and dig deeper into their distributions. All the greens indicate negative association, blues indicate very negative association, reds indicate no association, oranges indicate positive association, purples, yellow, and brown indicate increasingly postive association in order."}}