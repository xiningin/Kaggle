{"cell_type":{"3360879e":"code","db9b7cdb":"code","4e9d0216":"code","08fcd131":"code","3a2197ec":"code","f5025958":"code","5c43d067":"code","49460ef8":"code","807bdbef":"code","50cc861b":"code","6396db7a":"code","66d09bc8":"code","014574f3":"code","b4842729":"code","6c6df838":"code","f7b0b4c4":"code","91d15d32":"code","a22fc1c7":"code","b6648cdf":"code","d515b458":"code","9350e21f":"code","eb285a74":"code","c95a103f":"code","39705c7d":"code","a21f37ef":"code","a1060f2a":"code","3edcd3e5":"code","840d1359":"code","e4297543":"markdown","9f773062":"markdown","0b3c4091":"markdown","0212accc":"markdown","6d850bcf":"markdown","594e2194":"markdown","b124f381":"markdown","cd5deee3":"markdown","35e2f624":"markdown","885624e5":"markdown","ff058949":"markdown","ec519a73":"markdown","1d5fb0fa":"markdown","c4769a45":"markdown","47c23891":"markdown","f37a6533":"markdown","3891f5dc":"markdown","185b27c5":"markdown","0efb856f":"markdown","c259293c":"markdown","6c990196":"markdown","2acad3c3":"markdown","3229d819":"markdown","68eff134":"markdown","3ebbcbcc":"markdown","2c026ce0":"markdown","e23103c2":"markdown","0a1ab515":"markdown","917889f8":"markdown","be918af5":"markdown","a4ecbb86":"markdown","b73ed0c6":"markdown","36993b42":"markdown","3bfbedb7":"markdown","0d21b756":"markdown","128dc90f":"markdown","2e21c774":"markdown","f816d613":"markdown","ab499c5e":"markdown","22f2bcbf":"markdown","88351823":"markdown","63dfffd0":"markdown","1a406aa0":"markdown","e56b93cd":"markdown","8e3db535":"markdown"},"source":{"3360879e":"# Imports\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('max_colwidth', 200)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nimport re\nimport string\nimport math\nimport random\nfrom random import choice, choices\nimport time\n\nimport gc\n\n\nfrom IPython.display import display\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n# Professionals Import\n\nprofessionals = pd.read_csv('..\/input\/professionals.csv', index_col='professionals_id')\nprofessionals = professionals.rename(columns={'professionals_location': 'location', 'professionals_industry': 'industry', 'professionals_headline': 'headline', 'professionals_date_joined': 'date_joined'})\nprofessionals['headline'] = professionals['headline'].fillna('')\nprofessionals['industry'] = professionals['industry'].fillna('')\n\n# Students Import\n\nstudents = pd.read_csv('..\/input\/students.csv', index_col='students_id')\nstudents = students.rename(columns={'students_location': 'location', 'students_date_joined': 'date_joined'})\n\n# Questions Import\nquestions = pd.read_csv('..\/input\/questions.csv', index_col='questions_id', parse_dates=['questions_date_added'], infer_datetime_format=True)\nquestions = questions.rename(columns={'questions_author_id': 'author_id', 'questions_date_added': 'date_added', 'questions_title': 'title', 'questions_body': 'body', 'questions_processed':'processed'})\n\n# Answers Import\nanswers = pd.read_csv('..\/input\/answers.csv', index_col='answers_id', parse_dates=['answers_date_added'], infer_datetime_format=True)\nanswers = answers.rename(columns={'answers_author_id':'author_id', 'answers_question_id': 'question_id', 'answers_date_added': 'date_added', 'answers_body': 'body'})\n\n# Tags Import\ntags = pd.read_csv('..\/input\/tags.csv',)\ntags = tags.set_index('tags_tag_id')\ntags = tags.rename(columns={'tags_tag_name': 'name'})\n\n# Comments Import\ncomments = pd.read_csv('..\/input\/comments.csv', index_col='comments_id')\ncomments = comments.rename(columns={'comments_author_id': 'author_id', 'comments_parent_content_id': 'parent_content_id', 'comments_date_added': 'date_added', 'comments_body': 'body' })\n\n\n# School Memberships\nschool_memberships = pd.read_csv('..\/input\/school_memberships.csv')\nschool_memberships = school_memberships.rename(columns={'school_memberships_school_id': 'school_id', 'school_memberships_user_id': 'user_id'})\n\n# Groups Memberships\ngroup_memberships = pd.read_csv('..\/input\/group_memberships.csv')\ngroup_memberships = group_memberships.rename(columns={'group_memberships_group_id': 'group_id', 'group_memberships_user_id': 'user_id'})\n\n# Emails\nemails = pd.read_csv('..\/input\/emails.csv')\nemails = emails.set_index('emails_id')\nemails = emails.rename(columns={'emails_recipient_id':'recipient_id', 'emails_date_sent': 'date_sent', 'emails_frequency_level': 'frequency_level'})\n\n#####################################################\nprint('Important numbers:')\nprint('\\nThere are:')\nprint(f'- {len(students)} Students.', end=\"\\t\")\nprint(f'- {len(professionals)} Professionals.')\nprint(f'- {len(questions)} Questions.', end=\"\\t\")\nprint(f'- {len(answers)} Answers.')\nprint(f'- {len(tags)} Tags.', end=\"\\t\\t\")\nprint(f'- {len(comments)} Comments.')\nprint(f'- {school_memberships[\"school_id\"].nunique()} Schools.', end=\"\\t\\t\")\nprint(f'- {len(pd.read_csv(\"..\/input\/groups.csv\"))} Groups.')\nprint(f'- {len(emails)} Emails were sent.')\n#####################################################\n\n# Questions-related stats\ntag_questions = pd.read_csv('..\/input\/tag_questions.csv',)\ntag_questions = tag_questions.rename(columns={'tag_questions_tag_id': 'tag_id', 'tag_questions_question_id': 'question_id'})\ncount_question_tags = tag_questions.groupby('question_id').count().rename(columns={'tag_id': 'count_tags'}).sort_values('count_tags', ascending=False)\nprint('\\nInteresting statistics: ')\nprint(f'- {(answers[\"question_id\"].nunique()\/len(questions))*100:.2f} % of the questions have at least 1 answer.')\nprint(f'\\n- {(len(count_question_tags)\/len(questions))*100:.2f}% of questions are tagged by at least {count_question_tags[\"count_tags\"].tail(1).values[0]} tag.')\nprint(f'- Mean of tags per question: {count_question_tags[\"count_tags\"].mean():.2f} tags per question.')\n\ntag_users = pd.read_csv('..\/input\/tag_users.csv',)\ntag_users = tag_users.rename(columns={'tag_users_tag_id': 'tag_id', 'tag_users_user_id': 'user_id'})\nusers_who_follow_tags = list(tag_users['user_id'].unique())\nnbr_pros_tags = len(professionals[professionals.index.isin(users_who_follow_tags)])\nnbr_students_tags = len(students[students.index.isin(users_who_follow_tags)])\nprint(f'\\n- {(nbr_pros_tags \/ len(professionals))*100:.2f} % of the professionals follow at least 1 Tag ({nbr_pros_tags}).')\nprint(f'- {(nbr_students_tags \/ len(students))*100:.2f} % of the students follow at least 1 Tag ({nbr_students_tags}).')\n\nquestion_scores = pd.read_csv('..\/input\/question_scores.csv')\nnbr_questions_with_hearts = question_scores[question_scores['score'] > 0]['id'].nunique()\nprint(f'\\n- {(nbr_questions_with_hearts\/len(questions))*100:.2f} % of questions were upvoted ({nbr_questions_with_hearts}).')\n\nanswer_scores = pd.read_csv('..\/input\/answer_scores.csv')\nnbr_answers_with_hearts = answer_scores[answer_scores['score'] > 0]['id'].nunique()\nprint(f'- {(nbr_answers_with_hearts\/len(questions))*100:.2f} % of answers were upvoted ({nbr_answers_with_hearts}).')\n\n\n# School\/Group Related Stats\n\ndef is_student(user_id):\n    if user_id in students.index.values:\n        return 1\n    elif user_id in professionals.index.values:\n        return 0\n    else:\n        raise ValueError('User ID not student & not professional')\n\nschool_memberships['is_student'] = school_memberships['user_id'].apply(is_student)\nschool_memberships['is_student'] = school_memberships['is_student'].astype(int)\ncount_students_professionals = school_memberships.groupby('is_student').count()[['school_id']].rename(columns={'school_id':'count'})\nprint(f'\\n- Only {count_students_professionals.loc[1].values[0]\/len(students):.2f} % of the students are members of schools ({count_students_professionals.loc[1].values[0]}).')\nprint(f'- Only {count_students_professionals.loc[0].values[0]\/len(professionals):.2f} % of the professionals are members of schools ({count_students_professionals.loc[0].values[0]}).')\n\ngroup_memberships['is_student'] = group_memberships['user_id'].apply(is_student)\ngroup_memberships['is_student'] = group_memberships['is_student'].astype(int)\ncount_students_professionals = group_memberships.groupby('is_student').count()[['group_id']].rename(columns={'group_id':'count'})\nprint(f'\\n- Only {count_students_professionals.loc[1].values[0]\/len(students):.2f} % of the students are members of groups ({count_students_professionals.loc[1].values[0]}).')\nprint(f'- Only {count_students_professionals.loc[0].values[0]\/len(professionals):.2f} % of the professionals are members of groups ({count_students_professionals.loc[0].values[0]}).')\n\n\nprint('')","db9b7cdb":"# Professionals with zero answers\nnbr_pros_without_answers = len(professionals) - answers['author_id'].nunique()\n#print(f'\\n- {(nbr_pros_without_answers\/len(professionals))*100:.2f} % of the professionals have Zero answers ({nbr_pros_without_answers}).')\nfig = {\n    'data': [{\n        'type': 'pie',\n        'labels': ['Zero answers', '> 0 answers'],\n        'values': [nbr_pros_without_answers , len(professionals) - nbr_pros_without_answers],\n        'textinfo': 'label+percent',\n        'showlegend': False,\n        'marker': {'colors': [ '#00FF66', '#D9BCDB',], 'line': {'width': 3, 'color': 'white'}},\n    }],\n    'layout': {\n        'title': 'Professionals with Zero Answers'\n    }\n}\niplot(fig)\n","4e9d0216":"# Answers Import\nyears = questions['date_added'].dt.year.unique()\nyears = sorted(years)\nprofessionals['date_joined'] = pd.to_datetime(professionals['date_joined'])\nactivity_per_year = {}\n\nfor y in years:\n#y = 2013\n    limit_date = pd.to_datetime(f'{y}-12-31') - np.timedelta64(200, 'D')\n    year_answers = answers[answers['date_added'].dt.year == y]\n    professionals_up_to_year = professionals[professionals['date_joined'].dt.year <= y]\n    \n    nbr_active_pros = year_answers['author_id'].nunique()\n    nbr_inactive_pros = len(professionals_up_to_year) - nbr_active_pros\n    activity_per_year[y] = (nbr_active_pros, nbr_inactive_pros)\n\n\nfig = {\n    'data': [\n        {\n        'type': 'bar',\n        'name': 'Number of Active Professionals',\n        'x': years,\n        'y': [e[0] for e in list(activity_per_year.values())],\n        'marker': {'color': '#db2d43'}\n        },\n        {\n        'type': 'bar',\n        'name': 'Number of Inactive Professionals',\n        'x': years,\n        'y': [e[1] for e in list(activity_per_year.values())],\n        'marker': {'color': '#906FA8'}\n        }\n    ],\n    'layout': {\n        'title': 'Number of Active vs Non-Active Professionals each year',\n        'xaxis': {'title': 'Years'},\n        'yaxis': {'title': 'Number of Professionals',},\n        'barmode': 'stack',\n        'legend': {'orientation': 'h'},\n    }\n}\niplot(fig)\n","08fcd131":"answers = answers.rename(columns={'date_added': 'answers_date_added'})\nquestions = questions.rename(columns={'date_added': 'questions_date_added'})\nfirst_answers = answers[['question_id', 'answers_date_added']].groupby('question_id').min()\nanswers_questions = first_answers.join(questions[['questions_date_added']])\nanswers_questions['diff_days'] = (answers_questions['answers_date_added'] - answers_questions['questions_date_added'])\/np.timedelta64(1,'D')\nvals = [answers_questions[answers_questions['questions_date_added'].dt.year == y]['diff_days'].mean() for y in years]\nLINE_COLOR = '#9250B0'\nfig = {\n    'data': [{\n        'type': 'scatter',\n        'x': years,\n        'y': vals,\n        'line': {'color': LINE_COLOR}\n    }],\n    'layout': {\n        'title': 'Evolution of Time to First Response in days',\n        'xaxis': {'title': 'Years'},\n        'yaxis': {'title': 'Time to First Response'}\n    }\n}\niplot(fig)\nanswers = answers.rename(columns={'answers_date_added': 'date_added'})\nquestions = questions.rename(columns={'questions_date_added': 'date_added'})","3a2197ec":"# Number of accurate recommendations\nemails['date_sent'] = pd.to_datetime(emails['date_sent'], infer_datetime_format=True)\nmatches = pd.read_csv('..\/input\/matches.csv')\nmatches = matches.join(emails[['recipient_id', 'date_sent']], on='matches_email_id')\n\nmatches = matches.rename(columns={'matches_question_id': 'question_id', 'matches_email_id': 'email_id'})\nall_recommendations_per_year = []\naccurate_recommendations_per_year = []\nmatches['author_id'] = matches['recipient_id']\nfor y in years:\n    year_answers = answers[answers['date_added'].dt.year == y]\n    year_recommendations = matches[matches['date_sent'].dt.year == y]\n    all_recommendations_per_year.append(len(year_recommendations))\n    m = year_answers.reset_index().merge(year_recommendations, on=['question_id', 'author_id']).set_index('answers_id')\n    nbr_accurate_recommendations = len(m)\n    accurate_recommendations_per_year.append(nbr_accurate_recommendations)\n    #print(f'- {(nbr_accurate_recommendations\/len(matches))*100:.2f} % of recommended questions in emails were accurate (lead to professional answering the recommended question) ({nbr_accurate_recommendations})')\n\n#print(accurate_recommendations_per_year)\nLINE_COLOR = '#9250B0'\nfig = {\n    'data': [{\n        'type': 'scatter',\n        'x': years,\n        'y': accurate_recommendations_per_year,\n        'line': {'color': LINE_COLOR}\n    }],\n    'layout': {\n        'title': 'Evolution of Number of Accurate recommendations',\n        'xaxis': {'title': 'Years'},\n        'yaxis': {'title': 'Time to First Response'}\n    }\n}\niplot(fig)\n","f5025958":"proportions_of_accurate_recommendations = np.array(accurate_recommendations_per_year)\/np.array(all_recommendations_per_year)\nproportions_of_accurate_recommendations = [0 if np.isnan(e) else e for e in proportions_of_accurate_recommendations]\n#print(proportions_of_accurate_recommendations)\nfig = {\n    'data': [{\n        'type': 'scatter',\n        'x': years,\n        'y': proportions_of_accurate_recommendations,\n        'line': {'color': LINE_COLOR}\n    },\n    ],\n    'layout': {\n        'title': 'Percentage of Accurate recommendations',\n        'xaxis': {'title': 'Years'},\n        'yaxis': {'title': 'Proportion of Accurate Recommendations', 'tickformat': ',.0%'}\n    }\n}\niplot(fig)","5c43d067":"# Garbadge collect stuff we won't be using for building the recommender.\n\ndel m\ndel emails\ndel matches\ndel students\ndel school_memberships\ndel group_memberships\ndel count_question_tags\ndel users_who_follow_tags\ndel nbr_pros_tags\ndel nbr_students_tags\ndel nbr_pros_without_answers\ndel nbr_questions_with_hearts\ndel count_students_professionals\ngc.collect()\nprint('')","49460ef8":"# Drop tags that are not used in any question and not followed by any user (it will clean a lot of useless stuff)\nuseless_tags = tags[~tags.index.isin(tag_questions['tag_id'].unique())]\nuseless_tags = tags[ (tags.index.isin(useless_tags.index.values)) & (~tags.index.isin(tag_users['tag_id'].values)) ]\ntags = tags.drop(useless_tags.index)\n\nprint(f'- {len(useless_tags)} useless tags were found and dropped.')","807bdbef":"# Preprocessing Tags\n\nnbr_tags = len(tags)\n\nstop_words = set(stopwords.words('english'))\n# some common words \/ mistakes to filter out too\nstop_words.update(['want', 'go', 'like', 'aa', 'aaa', 'aaaaaaaaa', \n                   'good', 'best', 'would', 'get', 'as', 'th', 'k',\n                   'become', 'know', 'us'])\nspecial_characters = f'[{string.punctuation}]'\nlm = WordNetLemmatizer()\n\n\ntags['name'] = tags['name'].str.lower()\ntags.fillna('', inplace=True)\ntags['processed'] = tags['name'].str.replace(special_characters, '')\ntags['processed'] = tags['processed'].str.replace('^\\d+$', '') # tags that are just numbers :-\/\ntags['processed'] = tags['processed'].apply(lambda x: lm.lemmatize(x)) # avoid having plurals like 'career' and 'careers'\ntags['processed'] = tags['processed'].str.replace('^\\w$', '') # single letter tags :-\/\ntags['processed'] = tags['processed'].str.replace(r'(\\d+)(yrs?)', r'\\1year') #\ntags['processed'] = tags['processed'].apply(lambda x: x if x not in stop_words else '')\n\n# Drop tags which are prepositions, pronouns, determiners, wh-adverbs (where, ...)\ntags_to_drop = []\nfor i, t in tags['processed'].iteritems():\n    if len(t) > 0 and nltk.pos_tag([t])[0][1] in ['IN', 'PRP', 'WP$', 'PRP$', 'WP', 'DT', 'WRB']:\n        tags_to_drop.append(i)\ntag_questions = tag_questions.drop(tag_questions[tag_questions['tag_id'].isin(tags_to_drop)].index)\ntags = tags.drop(tags_to_drop)\n\n# Drop tags which are just numbers\ntags_to_drop = tags[tags['name'].str.contains('^\\d+$')].index\ntag_questions = tag_questions.drop(tag_questions[tag_questions['tag_id'].isin(tags_to_drop)].index)\ntags = tags.drop(tags_to_drop)\n\n# Drop tags which are just stop words ( after, the , with , ...)\ntags_to_drop = tags[tags['name'].isin(stop_words)].index\ntag_questions = tag_questions.drop(tag_questions[tag_questions['tag_id'].isin(tags_to_drop)].index)\ntags = tags.drop(tags_to_drop)\n\nprint(f'{nbr_tags - len(tags)} Tags were filtered out.')\ntags.sample(2)","50cc861b":"# Questions Cleaning\n\nquestions['processed'] = questions['title'] + ' ' + questions['body']\nquestions['processed'] = questions['processed'].str.lower()\nquestions['processed'] = questions['processed'].str.replace('<.*?>', '') # remove html tags\nquestions['processed'] = questions['processed'].str.replace('[-_]', '') # remove separators\nquestions['processed'] = questions['processed'].str.replace(special_characters, ' ') # remove special characters\n\nquestions['processed'] = questions['processed'].str.replace('\\d+\\s?yrs?', ' years') # single letter tags :-\/\n\ndef lem_question(q):\n    return \" \".join([lm.lemmatize(w) for w in q.split() if w not in stop_words])\nquestions['processed'] = questions['processed'].apply(lem_question)\n\nquestions['processed'] = questions['processed'].str.replace(r'(\\d+)($|\\s+)', r'\\2') # remove numbers which are not part of words\nquestions['processed'] = questions['processed'].str.replace(r'(\\d+)([th]|k)', r'\\2') # remove numbers from before th and k\n\n\n# Function to preprocess new questions\n# TODO: update function to do like above\ndef preprocess_question(q):\n    q = q.lower()\n    q = re.sub(\"<.*?>\", \"\", q)\n    q = re.sub(\"[-_]\", \"\", q)\n    q = re.sub(\"\\d+\", \"\", q)\n    q = q.translate(q.maketrans('', '', string.punctuation))\n    q = \" \".join([lm.lemmatize(t) for t in q.split()])\n    return q\n\ncnt_answers = answers.groupby('question_id').count()[['body']].rename(columns={'body': 'count_answers'})\nquestions = questions.join(cnt_answers)\nquestions['count_answers'] = questions['count_answers'].fillna(0)\nquestions['count_answers'] = questions['count_answers'].astype(int)\n\nprint('Questions preprocessed.')\nquestions.sample(1)[['title', 'body', 'processed', 'count_answers']]","6396db7a":"\n# Count Answers\nprint('Counting Answers ...')\npro_answers_count = answers.groupby('author_id').count()[['question_id']].rename(columns={'question_id': 'count_answers'})\nprofessionals = professionals.join(pro_answers_count)\nprofessionals['count_answers'] = professionals['count_answers'].fillna(0)\nprofessionals['count_answers'] = professionals['count_answers'].astype(int)\n\n\n# Cleaning the headlines\nprint('Cleaning Headlines ...')\nprofessionals['headline'] = professionals['headline'].fillna('')\nprofessionals['headline'] = professionals['headline'].str.lower()\nprofessionals['headline'] = professionals['headline'].str.replace('--|hello|hello!|hellofresh', '')\n\n# Check if follow tags or not\nprint('Creating \"follow_tags\" column ...')\nprofessionals['follow_tags'] = False\nfollowers = list(tag_users['user_id'].unique())\nprofessionals.loc[professionals.index.isin(followers), 'follow_tags'] = True\n\n# Create Last Answer Date Column\nprint('Creating \"last_answer_date\" column ... ')\nprofessionals = professionals.join(answers[['author_id', 'date_added']].groupby('author_id').max().rename(columns={'date_added': 'last_answer_date'}))\n\n\nprint('Professionals preprocessed')\nprofessionals.sample(3)\n","66d09bc8":"start = time.time()\n\ntfidf_vectorizer = TfidfVectorizer(stop_words=stop_words,)\n\nNUM_TOPICS = 1100\ndef build_model(qs , nbr_topics=NUM_TOPICS):\n    print('Building the Model ...')\n    # TF-IDF Transformation\n\n    qs_tfidf = tfidf_vectorizer.fit_transform(qs['processed'])\n    terms = tfidf_vectorizer.get_feature_names()\n    print(' (1\/3) TF-IDF matrix shape: ', qs_tfidf.shape)\n\n    # Dimensionality Reduction with SVD\n    model = TruncatedSVD(n_components=nbr_topics)\n    transformer_model = model.fit(qs_tfidf)\n    qs_transformed = transformer_model.transform(qs_tfidf)\n    print(' (2\/3) Shape after Dimensionality Reduction:', qs_transformed.shape)\n\n    # Construct Similarity Matrix\n    sim_mat = cosine_similarity(qs_transformed, qs_transformed)\n    print(' (3\/3) Similarity Matrix Shape', sim_mat.shape, '\\n')\n    return transformer_model, qs_transformed, sim_mat\n\ntransformer_model, Qs_transformed, Qs_sim_matrix = build_model(questions)\n\nend = time.time()\n\nprint(f'{(end-start)\/60:.2f} minutes')","014574f3":"def calculate_score_question_answered(days_elapsed_after_answer):\n    eps = 370\n    score = np.log10(days_elapsed_after_answer*(1\/eps)) \/ (np.log10(1\/eps))\n    score = 0.001 if score < 0 else score # questions that got a score lower than 0 are still given a very low score\n    return score\n\ndef calculate_exploit_threshold(answered_question_scores, nbr_recommendations, alpha=1.35):\n    nbr_questions_answered = len([s for s in answered_question_scores if s > 0])\n    eps = 0.1 if nbr_questions_answered == 0 else 0\n    return np.log10(np.sqrt(nbr_questions_answered) + 1) * alpha + eps\n","b4842729":"debug = False","6c6df838":"# Set current date as the last day of the data\ndef set_today(d_str):\n    d = pd.to_datetime(d_str)\n    \n    min_for_questions = d - np.timedelta64(600, 'D') # used for the Freezing professional to select the latest questions and for the cold to select the latest questions in followed and suggested tags\n    min_for_answers = d - np.timedelta64(400, 'D')   # used for hot professional to select his last answers. if no answers in this period, Hot professional will be treated as Cold\n    return d, min_for_questions, min_for_answers\n\ntoday, min_date_for_questions, min_date_for_answers = set_today('2019-01-31')","f7b0b4c4":"\ndef choose_random_answered_question(question_score_dic):\n    random_key = choices(list(question_score_dic.keys()), list(question_score_dic.values()))[0]\n    return (random_key, question_score_dic[random_key])\n\n\ndef choose_random_followed_tag(pro_id):\n    followed_tags = tag_users[tag_users['user_id'] == pro_id]\n    return followed_tags.sample(1)['tag_id'].values[0]\n\ndef get_similar_questions(qid, nbr_questions=10, except_questions_ids=[], prioritize=False, similarity_threshold=0.4):\n    recommendations = pd.DataFrame([])\n\n    #print(len(except_questions_ids))\n    #print()\n    q_dists_row = list(Qs_sim_matrix[questions.index.get_loc(qid)])\n    for eq_id in except_questions_ids:\n        #print('removing ', eq_id)\n        #print(len(q_dists_row), questions.index.get_loc(eq_id))\n        q_dists_row[questions.index.get_loc(eq_id)] = -1\n    q_dists_row = pd.Series(q_dists_row).sort_values(ascending=False)[:100]\n    q_dists_row = q_dists_row[1:]\n\n    if not prioritize:\n        q_dists_row = q_dists_row[:nbr_questions]\n        for i, d in q_dists_row.iteritems():\n            qid = questions.index.values[i]\n            recommendations = recommendations.append(questions.loc[qid])\n    else:\n        qid_to_score = {}\n        for i, d in q_dists_row.iteritems():\n            qid = questions.index.values[i]\n            if d > similarity_threshold:\n                #print(qid)\n                q_added = questions.loc[qid, 'date_added']\n                days_elapsed = (today - q_added) \/ np.timedelta64(1, 'D')\n                qid_to_score[qid] = d * days_elapsed\n        qid_scores = sorted(qid_to_score.items(), key=lambda x: x[1])[:nbr_questions]\n        for qid, score in qid_scores:\n            print(q_dists_row[questions.index.get_loc(qid)], qid_to_score[qid]) if debug else None\n            recommendations = recommendations.append(questions.loc[qid])\n    return recommendations\n\n\n\ndef recommend_questions_to_professional(pro_id, nbr_recommendations=10, silent=False, alpha=1.35):\n    print('Professional ID:', pro_id ) if not silent else None\n\n    # tags followed\n    tags_followed = tag_users[tag_users['user_id'] == pro_id]['tag_id']\n    tags_followed = tags[tags.index.isin(tags_followed)]\n    print('Followed Tags: ', tags_followed['name'].values)  if not silent else None\n\n    # Number of answered questions\n    cnt_pro_answers = professionals.loc[pro_id, 'count_answers']\n    if cnt_pro_answers > 0:\n        pros_answers = answers[(answers['author_id'] == pro_id) & (answers['date_added'] < min_date_for_answers)]\n        cnt_pro_answers = len(pros_answers)\n\n    # Type of Start\n    cold_start = (cnt_pro_answers == 0)\n    freezing_start = (cold_start and len(tags_followed) == 0 )\n\n    n = 3 # Nbr of questions per tag\n    recommendations = pd.DataFrame([])\n\n\n    # Freezing Start\n    if freezing_start:\n        print('Freezing ...')  if not silent else None\n        recommendations = recommendations.append(questions[questions['date_added'] > min_date_for_questions].sample(10))\n\n    # Cold Start\n    elif cold_start:\n        print('Cold', cnt_pro_answers)  if not silent else None\n\n        qids_from_followed_tags  = tag_questions[tag_questions['tag_id'].isin(tags_followed.index.values)]['question_id'].values\n        qids_from_followed_tags  = list(questions[(questions.index.isin(qids_from_followed_tags))   & (questions['date_added'] > min_date_for_questions)].sort_values('date_added', ascending=False).index.values)\n\n        tags_suggested = tags[tags['processed'].isin(tags_followed['processed'].values)]\n        tags_suggested = tags_suggested[~tags_suggested.index.isin(tags_followed.index.values)]\n        print('Suggested Tags: ', tags_suggested['name'].values)  if not silent else None\n        suggested_tags_available = len(tags_suggested) > 0\n        # If there are suggested tags, we do explore on them while exploiting on the followed tags\n        if suggested_tags_available:\n            qids_from_suggested_tags = tag_questions[tag_questions['tag_id'].isin(tags_suggested.index.values)]['question_id'].values\n            qids_from_suggested_tags = list(questions[(questions.index.isin(qids_from_followed_tags))  & (questions['date_added'] > min_date_for_questions)].sort_values('date_added', ascending=False).index.values)\n            exploit_threshold = .6\n        # If no suggested tags are available, we just exploit on the followed tags\n        else:\n            exploit_threshold = 1\n\n\n        print('Exploit Threshold: ', exploit_threshold) if debug else None\n        for i in range(1, nbr_recommendations+1):\n            if np.random.rand() < exploit_threshold and len(qids_from_followed_tags) > 0:\n                # Exploit followed tags\n                print(f'{i}- Exploit followed tags') if debug else None\n                random_index = choice(qids_from_followed_tags)\n                q = questions.loc[random_index]\n                recommendations = recommendations.append(q)\n                qids_from_followed_tags.remove(random_index)\n            elif suggested_tags_available and len(qids_from_suggested_tags) > 0:\n                # Suggest from suggested tags\n                print(f'{i}- Explore suggested tags') if debug else None\n                random_index = choice(qids_from_suggested_tags)\n                q = questions.loc[random_index]\n                recommendations = recommendations.append(q)\n                qids_from_suggested_tags.remove(random_index)\n            else:\n                # no more questions from the pool\n                pass\n\n    # Hot Start\n    else:\n        \n        questions_answered_ids = list(pros_answers['question_id'].unique())\n        questions_answered = questions[questions.index.isin(questions_answered_ids)].sort_values('date_added', ascending=False)\n        questions_answered_locs = []\n        for qid in questions_answered_ids:\n            questions_answered_locs.append(questions.index.get_loc(qid))\n\n        print('Hot, Answered Questions: ', cnt_pro_answers)  if not silent else None\n        #print(questions_answered_locs)\n        display(questions_answered[['date_added', 'title', 'body', 'count_answers']])  if not silent else None\n        \n        # calculate answered questions scores\n        q_scores = {}\n        for i, q in questions_answered.iterrows():\n            answer_post_date = pros_answers[pros_answers['question_id'] == i]['date_added'].values[0]\n            days_elapsed_after_answer = (today - answer_post_date)\/np.timedelta64(1, 'D')\n            q_scores[i] = calculate_score_question_answered(days_elapsed_after_answer)\n        print('Question-Scores: ', q_scores) if debug else None\n\n        # calculate exploit_threshold\n        exploit_threshold = calculate_exploit_threshold(list(q_scores.values()), nbr_recommendations, alpha=alpha)\n        print('Exploit Threshold:', exploit_threshold) if debug else None\n        except_qs = []\n        except_qs += questions_answered_ids\n        for i in range(nbr_recommendations):\n\n            if np.random.rand() < exploit_threshold:\n                # Exploit\n                random_q_score = choose_random_answered_question(q_scores)\n                print('\\nExploit Question', random_q_score) if debug else None\n                recommendations = recommendations.append(get_similar_questions(random_q_score[0], nbr_questions=1, except_questions_ids=except_qs, prioritize=True))\n            else:\n                # Explore\n                \n                # Get Latest n questions from all followed tags\n                n = 5\n                latest_questions = pd.DataFrame([])\n                for tid in tags_followed.index.values:\n                    qids = tag_questions[tag_questions['tag_id'] == tid]['question_id'].values\n                    tag_qs = questions[questions.index.isin(qids)]\n                    tag_qs = tag_qs[~tag_qs.index.isin(except_qs)]\n                    if len(tag_qs) > 0:\n                        tag_qs = tag_qs.sort_values('date_added', ascending=False)\n                        latest_questions = latest_questions.append(tag_qs.head(n))\n                #display(latest_questions)\n                \n                # Select the most similar one to the ones answered using the similarity matrix\n                best_question_id = 0\n                best_distance = float('-inf')\n                for qid, r in latest_questions.iterrows():\n                    qloc = questions.index.get_loc(qid)\n                    for aqloc in questions_answered_locs:\n                        d = Qs_sim_matrix[qloc, aqloc]\n                        if best_question_id == 0 or d > best_distance:\n                            best_question_id = qid\n                            best_distance = d\n\n                print('\\nExplore Tags', best_question_id, best_distance) if debug else None\n                if best_question_id != 0:\n                    recommendations = recommendations.append(questions.loc[best_question_id])\n            except_qs = list(recommendations.index.values)\n            except_qs += questions_answered_ids\n\n    return recommendations","91d15d32":"# Random Hot Professional\nrandom_hot_pro_id = professionals[(professionals['count_answers'] > 2) & (professionals['count_answers'] < 5)].sample(1).index.values[0]\n\n# Random Cold Professional ( check if he follows some tag )\nrandom_cold_pro_id = professionals[(professionals['count_answers'] == 0) & (professionals['follow_tags'] == True)].sample(1).index.values[0]\n\n\n#for random_pro_id in [random_hot_pro_id, random_cold_pro_id]:\nfor random_pro_id in [random_hot_pro_id, random_cold_pro_id]:\n    recs = recommend_questions_to_professional(random_pro_id, nbr_recommendations=10)\n    print('Recommendations: ')\n    display(recs[['date_added', 'title', 'body', 'count_answers']]) if len(recs) > 0 else None","a22fc1c7":"random_hot_pro_id = 'fbd6566ddf36402abeb031c088096ae4'\nrecs = recommend_questions_to_professional(random_hot_pro_id, nbr_recommendations=10)\nprint('Recommendations:')\ndisplay(recs[['date_added', 'title', 'body', 'count_answers']])","b6648cdf":"def recommend_professionals_for_question(qid, nbr_recommendations=10, inactivity_period=60):\n    #print(len(questions), len(answers))\n    similar_questions = get_similar_questions(qid, nbr_questions=10, except_questions_ids=[], prioritize=False)\n    #display(similar_questions)\n    answer_author_ids = answers[answers['question_id'].isin(similar_questions.index.values)]['author_id'].values\n    answer_author_ids = pd.Series(answer_author_ids).value_counts()\n    \n    # Step 1: Check how active the the candidates are\n    min_last_answer_date = today - np.timedelta64(inactivity_period, 'D')\n    candidates = professionals[(professionals.index.isin(answer_author_ids.index.values)) & (professionals['last_answer_date'] > min_last_answer_date)].sort_values('last_answer_date', ascending=False)\n    answer_author_ids = answer_author_ids.drop(candidates.index)\n    \n    # Step 2: if number of candidates is still smaller than nbr_recommendations, fill in with other authors based on how many similar questions they answered.\n    if len(candidates) < nbr_recommendations:\n        others = answer_author_ids.head(nbr_recommendations-len(candidates)).index.values\n        candidates = candidates.append(professionals[professionals.index.isin(others)])\n    return candidates[:nbr_recommendations]","d515b458":"random_question_index = choice(questions.index.values)\n\nprint('Random Question: ', random_question_index,  questions.loc[random_question_index]['date_added'])\nprint(questions.loc[random_question_index]['title'])\nprint(questions.loc[random_question_index]['body'])\nrecommend_professionals_for_question(random_question_index, nbr_recommendations=8)[['location', 'industry', 'headline', 'count_answers', 'last_answer_date']]\n","9350e21f":"# Analyze processed question and extracts implicit tags ( eg. 'computer science' => 'computerscience')\ndef get_tag_suggestions(q_p):\n    #q_p = preprocess_question(q)\n    #print(q_p)\n    q_tokens = nltk.word_tokenize(q_p)\n    q_tokens_cpy = q_tokens.copy()\n    \n    qp_tagged = nltk.pos_tag(q_tokens)\n    important = []\n    for t,pos in qp_tagged:\n        if t not in stop_words and pos == 'NN' and len(tags[tags['processed'] == t]) > 0 :\n            i = q_tokens.index(t)\n            #print(len(q_p), t, i)\n            poses_before_after = []\n            if i > 0:\n                poses_before_after.append(nltk.pos_tag([q_tokens[i-1]])[0])\n            if i < (len(q_tokens)-1):\n                poses_before_after.append(nltk.pos_tag([q_tokens[i+1]])[0])\n            for i, bf in enumerate(poses_before_after):\n                #print(t, bf)\n                if bf[1] in ['NN', 'NNS', 'JJ', 'JJR', 'VBG']:\n                    s = f'{t}{bf[0]}' if i == 1 else f'{bf[0]}{t}'\n                    important.append(s)\n            q_tokens.remove(t)\n    important = set(important)\n    for i in set(important):\n        if i not in tags['processed'].values or i in q_tokens_cpy:\n            important.remove(i)\n    #print(len(important),important)\n    \n    return tags[tags['processed'].isin(important)]","eb285a74":"new_question = 'I am a student in computer science and I want to be a data scientist but I dont know how to study machine learning and artificial intelligence. Can anyone give some advice ?' \np_q = preprocess_question(new_question)\nsuggestions = get_tag_suggestions(p_q)\nprint('Question: ', new_question)\nprint('\\nTag Suggestions: ')\nsuggestions[['name']]","c95a103f":"# Generate a random index for adding a question to DB\ndef gen_test_index():\n    length = np.random.randint(10,15)\n    letters_digits = string.ascii_lowercase + string.digits\n    return ''.join(random.sample(letters_digits, length))\n\n\ndef add_question_to_db(title, body):\n    global questions\n    global Qs_transformed\n    global Qs_sim_matrix\n    \n    q = title + ' ' + body\n    q_p = preprocess_question(q)\n    \n    tag_suggestions = get_tag_suggestions(q_p)\n    q_p = q_p + ' ' + ' '.join(tag_suggestions)\n    \n    print(q_p)  if debug else None\n    \n    author_id = 1 # special if for test ( doesn't exist in DB )\n    index = gen_test_index()\n    questions = questions.append(pd.Series({'author_id': author_id,'date_added': pd.to_datetime('now'), \n                                                  'title': title,\n                                                  'body': body, \n                                                  'processed': q_p, \n                                                  'count_answers': 0}, name=index))\n    print('Qs Transformed before', qs_transformed.shape) if debug else None\n    q_transformed = transformer_model.transform(tfidf_vectorizer.transform([q_p]))\n    Qs_transformed = np.append(Qs_transformed, [Qs_transformed[0]], axis=0)\n    print('Qs Transformed after', Qs_transformed.shape)  if debug else None\n    \n    sim_mat_shape = Qs_sim_matrix.shape\n    print('Similarity Matrix shape before', sim_mat_shape)  if debug else None\n    new_sims = cosine_similarity(Qs_transformed[-1].reshape(1,-1),Qs_transformed)[0]\n    print('new_sims', new_sims.shape)  if debug else None\n    Qs_sim_matrix = np.hstack((Qs_sim_matrix, np.zeros((sim_mat_shape[0], 1))))\n    Qs_sim_matrix = np.vstack((Qs_sim_matrix, np.zeros((sim_mat_shape[0]+1))))\n    Qs_sim_matrix[-1] = new_sims\n    Qs_sim_matrix[:, -1] = new_sims\n    print('Similarity Matrix shape before', sim_mat_shape)  if debug else None\n    print('Question Added to DB.')  if debug else None\n    return index\n","39705c7d":"# Backup\nquestions_full = questions.copy()\nanswers_full = answers.copy()\nprofessionals_full = professionals.copy()\ntag_users_full = tag_users.copy()\ntag_questions_full = tag_questions.copy()\n\ndef run_time_machine(today_str):\n    global today\n    global min_date_for_questions\n    global min_date_for_answers\n    global professionals\n    global questions\n    global answers\n    global tag_users\n    global tag_questions\n    global tfidf_vectorizer\n    global transformer_model\n    global Qs_transformed\n    global Qs_sim_matrix\n\n    \n    first_date = pd.to_datetime('2012-01-01') \n\n    today, min_date_for_questions, min_date_for_answers = set_today(today_str)\n    print('Running Time Machine ....', 'Going to', today.strftime('%B %d %Y'), '................\\n')\n\n    professionals = professionals_full[professionals_full['date_joined'] < today].copy()\n    assert (professionals['date_joined'].max() < today), \"Professionals have date_joined > today !\"\n    questions = questions_full[(questions_full['date_added'] > first_date) & (questions_full['date_added'] < today)].copy()\n    answers = answers_full[(answers_full['date_added'] > first_date) & (answers_full['date_added'] < today) & (answers_full['question_id'].isin(questions.index.values))].copy()\n\n    #test_questions = questions_full[(questions_full['date_added'] > today) & (questions_full['count_answers'] > 0)].copy()\n    #test_answers   = answers_full[(answers_full['question_id'].isin(test_questions.index.values)) & (answers_full['author_id'].isin(professionals.index.values))].copy()\n    #print(len(test_questions), 'Test questions (with at least one answer)')\n    #print(len(test_answers), 'Answers were posted for the test questions (from professionals who joined before that date)')\n\n    cnt_answers = answers.groupby('question_id').count()[['body']].rename(columns={'body': 'count_answers'})\n    questions = questions.drop('count_answers', axis=1)\n    questions = questions.join(cnt_answers)\n    questions['count_answers'] = questions['count_answers'].fillna(0)\n    questions['count_answers'] = questions['count_answers'].astype(int)\n\n\n    cnt_answers = answers.groupby('author_id').count()[['question_id']].rename(columns={'question_id': 'count_answers'})\n    professionals = professionals.drop('count_answers', axis=1)\n    professionals = professionals.join(cnt_answers)\n    professionals['count_answers'] = professionals['count_answers'].fillna(0)\n    professionals['count_answers'] = professionals['count_answers'].astype(int)\n\n    # Create Last Answer Date Column\n    professionals = professionals.drop('last_answer_date', axis=1)\n    professionals = professionals.join(answers[['author_id', 'date_added']].groupby('author_id').max().rename(columns={'date_added': 'last_answer_date'}))\n\n\n    tag_users = tag_users_full[tag_users_full['user_id'].isin(professionals.index.values)].copy()\n    tag_questions = tag_questions_full[tag_questions_full['question_id'].isin(questions.index.values)].copy()\n\n    transformer_model, Qs_transformed, Qs_sim_matrix = build_model(questions)\n    gc.collect()\n    print('##################################\\n')","a21f37ef":"# Start Date and End Date of the Simulation\n\ntest_start_date = pd.to_datetime('2018-08-01')\ntest_end_date = pd.to_datetime('2018-08-22')\nnbr_weeks = math.floor((test_end_date - test_start_date)\/ np.timedelta64(1, 'W'))\n","a1060f2a":"\n# Number of recommendations to generate for each professional. ( for the other mode, Question->Professional, it's nbr_recs*2)\nnbr_recs_pro_to_qs = 5\nnbr_recs_q_to_pros = nbr_recs_pro_to_qs * 2\n\n# Exploitation Intensity ( 1.0 ~ 1.7 )\nalpha_arg=1.35\n\n# Number of days to consider professional as inactive\ninactivity_period_arg=60","3edcd3e5":"## %%time\n\nprint('Start First Simulation', test_start_date.strftime('%B %d, %Y'), '--->', test_end_date.strftime('%B %d, %Y'), '(', nbr_weeks, ' weeks )', '\\n')\nstart = time.time()\n#run_time_machine(test_start_date.strftime('%Y-%m-%d'))\n\n\nquestion_to_pros_recs = {}\npro_to_questions_recs = {}\n\nnbr_accurate_q_to_pros = 0\nnbr_accurate_pro_to_qs = 0\nnbr_all_q_to_pros = 0\nnbr_all_pro_to_qs = 0\n\n\ncorrect_question_ids = set([])\nall_question_ids = set([])\n\ntoday = test_start_date\nfor i in range(1, nbr_weeks+1):\n    print('\\n----------- ', 'Week', i, ' -----------')\n    d_old = today\n    run_time_machine((today + np.timedelta64(1, 'W')).strftime('%Y-%m-%d'))\n\n    week_questions= questions[(questions['date_added'] > d_old) & (questions['date_added'] < today)]\n    week_answers = answers[(answers['date_added'] > d_old) & (answers['date_added'] < today) & (answers['author_id'].isin(professionals.index.values))].copy()\n    \n    if len(week_answers) == 0:\n        continue\n    \n    target_questions = week_questions[week_questions.index.isin(week_answers)]\n    qs_answered_this_week = list(week_answers['question_id'].unique())\n    authors_this_week = week_answers['author_id'].unique()\n    all_question_ids.update(qs_answered_this_week)\n\n    print( d_old, ' ~ ', today, ' - Number of answers: ', len(week_answers), ' - Number answered questions: ', len(qs_answered_this_week), ' - Number authors: ', len(authors_this_week))\n    \n    # Hide answers from the system !!!\n    answers = answers.drop(week_answers.index)\n    for auth_id in authors_this_week:\n        professionals.at[auth_id, 'count_answers'] = len(answers[answers['author_id'] == auth_id])\n\n    # some tests to check if everything is ok\n    assert (len(answers[(answers['date_added'] < today) & (answers['date_added'] > d_old) & (answers['author_id'].isin(professionals.index.values)) ]) == 0), \"The answers of this week were not all removed\"\n    random_auth_id = authors_this_week[0]\n    assert (professionals.loc[random_auth_id, 'count_answers'] == len(answers[answers['author_id'] == random_auth_id])), \"Problem with count of answers for professional\"\n\n    print('Making Predictions for the week\\'s answered questions ...')\n    # Predict pros for questions that were answered ( Question->Pros )\n    for qid in qs_answered_this_week:\n        question_to_pros_recs[qid] = recommend_professionals_for_question(qid, nbr_recommendations=nbr_recs_q_to_pros, inactivity_period=inactivity_period_arg)\n        recommended_pro_ids = set(question_to_pros_recs[qid].index.values)\n        nbr_all_q_to_pros += len(recommended_pro_ids)\n        target_pro_ids = set(week_answers[week_answers['question_id'] == qid]['author_id'].unique())\n        union_len = len(target_pro_ids.union(recommended_pro_ids))\n        sum_len = len(recommended_pro_ids) + len(target_pro_ids)\n        if union_len < sum_len:\n            nbr_accurate_q_to_pros += (sum_len - union_len)\n            correct_question_ids.update([qid])\n            \n    # Predict questions for pros who answered ( Pro->Questions )\n    for auth_id in authors_this_week:\n        pro_to_questions_recs[auth_id] = recommend_questions_to_professional(auth_id, nbr_recommendations=nbr_recs_pro_to_qs, silent=True, alpha=alpha_arg)\n        recommended_question_ids = set(pro_to_questions_recs[auth_id].index.values)\n        nbr_all_pro_to_qs += len(recommended_question_ids)\n        target_question_ids = set(week_answers[week_answers['author_id'] == auth_id]['question_id'].unique())\n        union_len = len(target_question_ids.union(recommended_question_ids))\n        sum_len = len(recommended_question_ids) + len(target_question_ids)\n        if union_len < sum_len:\n            nbr_accurate_pro_to_qs += (sum_len - union_len)\n            correct_question_ids.update([e for e in recommended_question_ids if e in target_question_ids])\n    \n    #print('Number of Accurate Recommendations (Question -> Pros): ', nbr_accurate_q_to_pros)\n    #print('Number of Accurate Recommendations (Pro -> Questions): ', nbr_accurate_pro_to_qs)\n\nend = time.time()\nprint(f'\\n-------- End of Simulation ({(end-start)\/60:.2f} minutes) --------')","840d1359":"print('Results of the Test:')\nprint(f\"- Percentage of Answered Questions that got accurate recommendations: {len(correct_question_ids)\/len(all_question_ids)*100:.2f}%\", f'( {len(correct_question_ids)} out of {len(all_question_ids)} questions  )' )\nprint('\\n- Percentage of accurate recommendations ( out of all sent ones ):')\nprint(f'\\t- Question-to-Professionals Mode:  {(nbr_accurate_q_to_pros\/nbr_all_q_to_pros)*100:.2f}% ',  f'( {nbr_accurate_q_to_pros} out of {nbr_all_q_to_pros} recommendations were accurate )')\nprint(f'\\t- Professional-to-Questions Mode: {(nbr_accurate_pro_to_qs\/nbr_all_pro_to_qs)*100:.2f}% ', f'( {nbr_accurate_pro_to_qs} out of {nbr_all_pro_to_qs} recommendations were accurate )')\n","e4297543":"<a href=\"https:\/\/www.kaggle.com\/hamzael1\/an-extensive-eda-for-careervillage\" target=\"_blank\">In a previous notebook<\/a> I made an overall Exploratory Data Analysis on the provided data. Here, I will be brief and focus on the most important statistics and metrics related to the recommendation problem.\n\n*Note: some code snippets that are trivial are collapsed for better readability, feel free to expand them if you want to check the code*","9f773062":"## Parameters of the model:\n\nWe can fine-tune the following parameters:\n* **Number of recommendations to generate each time**: 5 for Professional -> Questions Mode and 10 for Question -> Professionals Mode\n* **Number of days without answers to consider a professional not active ( for selection of professionals to answer a question )**: 60 days\n* **Exploitation Intensity ($\\alpha$)**: 1.35","0b3c4091":"## 3- Number of accurate recommendations:\nNext, we examine how many accurate recommendations are sent each year. How many of them were answered by the recipients of the emails. Again, this number must be maximized by our recommender system.","0212accc":"# Links to useful Ressources: <a class=\"anchor\" id=\"links\"><\/a>\n<h4>About Recommender systems<\/h4>\n* [A whole Coursera Specialization about Recommender Systems](https:\/\/www.coursera.org\/specializations\/recommender-systems)\n* [Recommender Systems in Practice](https:\/\/towardsdatascience.com\/recommender-systems-in-practice-cef9033bb23a)\n* [An amazing Playlist of Stanford University Videos about Mining Datasets](https:\/\/www.youtube.com\/watch?v=1JRrCEgiyHM&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&index=42&t=0s)\n\n<h4>About Epsilon Greedy and Exploration\/Exploitation<\/h4>\n* [The Multi-armed Bandit Problem](https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit)\n* [A nice article About Epsilon-Greedy Algorithm](https:\/\/imaddabbura.github.io\/post\/epsilon_greedy_algorithm\/)\n* [Some great slides about how to solve the Exploration\/Exploitation dilemma](http:\/\/www0.cs.ucl.ac.uk\/staff\/d.silver\/web\/Teaching_files\/XX.pdf)\n\n<h4>About Latent Semantic Analysis and SVD<\/h4>\n* [A nice short video about LSA](https:\/\/www.youtube.com\/watch?v=OvzJiur55vo)\n* [A great explanation of SVD by Professor Jure Leskovec](https:\/\/www.youtube.com\/watch?v=P5mlg91as1c&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&index=47)\n\n<h4>About Reinforcement Learning in Recommender Systems<\/h4>\n* [Reinforcement Learning for Recommender Systems: A Case Study on Youtube](https:\/\/www.youtube.com\/watch?v=HEqQ2_1XRTs)\n* [Contextualized Bandits for Recommendation Systems](https:\/\/towardsdatascience.com\/bandits-for-recommender-system-optimization-1d702662346e)\n* [Netflix using using Contextualized Bandits for personalizing the selection of artwork](https:\/\/medium.com\/netflix-techblog\/artwork-personalization-c589f074ad76)\n* [A Multi-Armed Bandit Framework for Recommendations at Netflix](https:\/\/www.youtube.com\/watch?v=kY-BCNHd_dM)","6d850bcf":"## 3- Professionals\n* **Count Answers:** Create a new column 'count_answers' for professionals\n* **Cleaning the headlines**\n* **Follow Tags ?**: This is just a handy column I added to differentiate after between Pros who do and don't when evaluating the recommender\n* **Last Answer Date**: We will rely also on this new column to know if the professional is active or not.","594e2194":"## Helper functions in production:","b124f381":"## 5- Problem with the current System","cd5deee3":"# Summary and Future Explorations <a class=\"anchor\" id=\"future\"><\/a>\n\nThe proposed system's strengths are **its effectiveness, ease of implementation and ease of maintainance in production.**\nIt uses controlled randomness to encourage new users while keeping engaged professionals in the platform.\n\nThe proposed system is designed to be very **resilient** when it comes to difficult cases like professionals with **various interests**. In addition to recommending questions based on semantic similarity, the proposed system also recommends relevant questions from tags, and from **tags which are textually similar to the followed ones** (eg. 'computer-science' and 'computerscience', 'information-technology' and 'informationtechnology' ) .\n\nFurther, a basic framework for evaluating the system was proposed along with the most important metrics to measure.\n\nSome recommendations for future improvements:\n\n* **Meta-data based Recommendations:** \n\nIt is possible to use valuable information obtained when users open a browsing session ( viewed and visited questions, ... ).\n\n* **Controlling Tags:**\n\nIt is important to preprocess tags and prevent the students from using tags which already exist. A powerful model possibly based on Word2Vec, could be used to model the relationships and similarities between tags. For example, capturing the similarity between 'information-technology' and 'IT' would hugely boost the performance of the recommender.\n\n* **Controlling Typos**:\n\nUsing a spell-checking engine like [Hunspell](https:\/\/pypi.org\/project\/hunspell\/) would increase the quality of the data and help the engine make better recommendations\n\n* **Using the answers and comments in the model**:\n\nIn this model, only the text of the questions was used when encoding the questions. Another model should be explored, where we also make use of the answers and comments of each question to encode the questions.\n\n* **Making use of upvotes**:\n\nUnfortunately, The data provided about upvotes (questions & answers) wasn't specific as to who upvoted what. If this data was available, it could be used to better understand the interests of both professionals and students, and thus making better predictions.\n\n* **Using other Exploration Techniques**:\n\nI decided to go with the $\\epsilon$ Greedy Algorithm for its simpleness and ease of use. Other algorithms can be further explored like \"Optismism in face of uncertainty\" and \"Probability Matching\" ([See Links for more](#links))\n\n* **Making use of serious Reinforcement Learning**\n\nFinally, after exploring the previous suggestions, a very insteresting project would be to to make use of RL techniques, where the recommender uses the feedback of its actions (recommendations) to update and improve its future behaviour (recommendations). [See Links for more](#links)","35e2f624":"# Data Preprocessing is paramount ! <a class=\"anchor\" id=\"preproc\"><\/a>\n\n\n<div style=\"border: solid 1px blue; padding: 5px;\"><h4><center><span style=\"color: red;\">If we let Garbage In, we get Garbage Out ! (GIGO)<\/span><center><\/h4><\/div>\n\n<br\/>\nThe most important data type in this project is Text (questions, tags ...). Unfortunately, if left unpreprocessed, it becomes extremely hard to extract useful information from it.\n\nThis section's goal, is to prepare the data by simplifying it and removing any noise that migh get in the way between us and the True Information that we want to extract.\n\nThis simple preprocessing can be easily done online in production, doesn't require a lot of computation.\n","885624e5":"In the current system, emails containing recommended questions are sent to professionals on a daily basis by default. \n\nThe possible frequencies that a professional can choose from are:\n* Immediate\n* Daily\n* Weekly\n* Turn off all notifications.\n\nThe 'daily' option is problematic. It is extremely difficult to to maintain a good quality of recommendations when the frequency is as high as 'Daily'. **We thus end up with a huge number of emails being sent daily with poor-quality recommendations. This can cause the professional to start ignoring emails and ultimately not returning to the site.**\n\nIn addition, having poor-quality recommendations increases the chance that the professional will choose to turn off all notifications, which is not desirable.\n\n<span style=\"color: blue; font-weight: bold;\">Quick Solution proposal: <\/span> Maintain good-quality recommendations by removing the 'Daily' option, and only keeping the 'Immediate' & 'Weekly' options.\n\nAnother *future* solution would be to leave it up to the system to decide when to email each professional depending on the interaction of the professional with the site.\n","ff058949":"# Why do we need a Recommender ? Let's ask the Data ! <a class=\"anchor\" id=\"why\"><\/a>","ec519a73":"<h4>I hope that this Kernel was useful, and see you in the <a href=\"https:\/\/www.kaggle.com\/hamzael1\/kernels\" target=\"_blank\">next one<\/a> !<\/h4>\n\n*PS: upvotes & feedback are welcome !*\n","1d5fb0fa":"# Introduction:\n<a href=\"https:\/\/www.careervillage.org\/\" target=\"_blank\">CareerVillage.org<\/a> <span style=\"color: purple;\">is a cloud-based solution for career advice<\/span>. It provides a platform where students with career-related questions meet professionals from the industry who help them by answering their questions.\n\nThe goal of <a href=\"https:\/\/www.kaggle.com\/c\/data-science-for-good-careervillage\/overview\" target=\"_blank\">this competition<\/a>, is to develop a method to recommend relevant questions to the professionals who are most likely to answer them.\n\nIn this notebook, I propose a solution that addresses the problem in an efficient manner using a probabilistic approach (Epsilon-Greedy) combined with an *state-of-the-art technique (LSA)*. **This combination aims to balance between Exploration & Exploitation, targeting both the new and already-engaged professionals.**\n\nThe biggest strength that was noticed, is that **this solution behaves particularly well when encountering professionals with diverse interests. For example, when a professional follows a set of tags, and answers questions unrelated to those tags, the system still keeps recommending questions from both ends, adapts continuously to the interests of the professional along time and behaves in a resilient manner.**\n\nControlled randomness is inherent to the proposed approach, this has two advantages:\n* recommendations stay diverse.\n* unanswered new questions have a high chance to get answered because they get propritized. The model doesn't *over-focus* on the answered questions of the professionals.\n\nFurther, a basic framework for evaluating the system was proposed along with the most important metrics to measure. This helps fine-tune the model parameters locally before moving to production and gives and idea about the performance of the system.\n\nThis notebook is structured as follows:\n* First, we ask the question \"[Why do we need a Recommender?](#why)\" and answer it with some focused analytics.\n* Next, [techniques and concepts](#concepts) used in the proposed recommender are explained.\n* Then, we move to the actual [implementation of the proposed recommender system and explain its inner-workings](#implementation) after performing the necessary [proprocessings](#preproc).\n* We discuss the difficulty of evaluating Recommender Systems and [propose a very basic framework to evaluate the proposed system](#eval), along with the metrics that must be took into account.\n* Finally, some [advice and future suggestions for improving the system](#future) are listed, along with links to useful ressources.\n","c4769a45":"Next, we make the following transformations to the tags:\n* make all tags lowercase.\n* create a new 'processed' column to hold the processed version of each tag\n* remove any special characters from the text.\n* correct some short words (yrs -> years)\n* lemmatize the tags ( eg. 'wolves' -> 'wolf' )\n* remove tags without any meaning that are just numbers, just preprositions, pronouns, stop-words ... ('where', 'and', 'the', '10', ...etc)\n","47c23891":"# Building the Recommendations Engine <a class=\"anchor\" id=\"implementation\"><\/a>\n\nIn this section, we will build the recommendations engine from scratch using only the techniques previously talked about. Each sub-section will deal with a specific sub-problem.\n\nThere are two main data structures we will work with:\n* **The Transformed Questions Matrix**: a Matrix where each row represents a single question encoded in a K dimensional vector\n* **The Similarity Matrix**: a NxN Matrix (where N is the number of questions) rating the similarity between all pair of questions on a scale of 0 ~ 1.\n\nNote that the recommender prioritizes **quality over quantity**. So, when asked for N recommendations, it will return the best k recommendations, where $k \\leq N$. The reason for this, is that the cost of bad recommendations can be high. We don't want users to get bad recommendations and ignore our future emails.","f37a6533":"### Testing the recommender","3891f5dc":"* Example: \n\nThe following example illustrates how many tags this question is related to.","185b27c5":"**The next snippet of code builds the recommendation engine using two main functions:**\n* **get_similar_questions**: returns similar questions to the one given using the similarity matrix ( the parameter \"similarity_threshold\" controls what similar means, I set it by default to be 0.4 as I found that value to work well for most cases ) .\n* **recommend_questions_to_professional**: given a professional ID, returns top K recommended questions.\n\nThe debug variable below, if set to True,  makes exploration \/ exploitation decisions visible.","0efb856f":"*Last Update 23th April*: *Code Refactoring, Evaluation Section & more Documentation added*\n\n____","c259293c":"# Evaluating the Recommender System using a Time Machine ! <a class=\"anchor\" id=\"eval\"><\/a>\n\nRecommender Systems are trickier to evaluate than - for example - a machine learning classifier. The reason is that the current recommender system influences the data that we have at hand ( that we are using for training and testing ).\n\nHowever, we can still get a idea about the performance of the model we're testing if we choose the good metrics and methodology for our project.\n\n\nWe can distinguish between two types of evaluation: **Offline & Online Evaluation**.\n\n*The Offline Evaluation* is used before deploying the model, to check its accuracy on the existing data. However, this method is **NOT** enough.\nUsually, only it's only after the model is deployed, and A\/B tests are that we can draw conclusions about the actual performance of our system. This second step is referred to as *Online Evaluation*\n\n<span style=\"color: purple\">Another important thing about having a fixed offline evaluation framework, even though it's not accurate, is that we can use it as basis evaluate multiple recommender systems, or to fine-tune parameters related to our model, and observe how the results change ( before pushing the changes to prod ) .<\/span>\n\n\n\n\n## Methodology\n\nFor offline evaluation, I chose to use a real-world **Split Validation** method. It works as follows:\n\n* First, I extract a small amount of data ( the most recent ) for example the last 6 months (let's say October 2018 ~ November 2018), this will be the **Test Set**\n* The model is built using only the other part of the data ( the oldest ), this is the **Traning Set** .\n* Edit all data so that we can simulate exactly how the system was at that particular point in time : questions, answers and professionals that were added after the date ( July 2018 ) are removed.\n\nNow that the system looks exactly as  it was (let's say at October 2018), the tests occur weekly:\n- Each week, new questions, answers and professionals are added. \n- **Hide all answers that occured that week from the recommender.**\n- The recommender system makes predictions about:\n    * **the professionals who answered the questions: Given that a question \"Q\" was answered this week, use Question-to-Professionals mode and see if the recommendations generated for this question contain the professionals who actually answered the question. ( do that for all questions answered this week and each week of the test period )**\n    * **the questions that were answered by the professionals: Given that a professional \"P\" answered some question(s), use Professional-to-Questions mode and see if the recommendations generated for that professional contain the questions that the professional actually answered. ( do that for all professionals who posted answers this week and each week of the test period )**\n- The goal is to make as many accurate recommendations as possible. Meaning, send recommendations to professionals who actually answered the questions that week, or recommend the answered questions to the right professionals.\n- Note here, that the recommender can perform much better in production, because when evaluating it here, we don't account for potential answers coming as a result of the recommender itself.\n- The test system makes the following assumption: **if the model made accurate offline recommendations about the answered questions, it would do so for the unanswered ones in production.**\n\n\nThe following code snippet builds our *\"Time Machine\"* :","6c990196":"## LSA: Latent Semantic Analysis (in a nutshell)\n\nLatent Semantic Analysis is a **simple**, yet **powerful** technique in Natural Language Processing. It captures the latent (hidden) topics of a corpus of text and represents each document by a vector of k dimensions, each pointing to one latent topic.\n\nTo do this, LSA relies on a robust mathematical technique called SVD (Singular-Value Decomposition), which factorizes a real matrix to a product of 3 matrices. ([More on LSA and SVD](#links))\n\n\n<span style=\"color: red; font-weight: bold;\">Takeaway:<\/span> **Each question will be represented by a vector of length k. comparing the questions will be as easy as performing a cosine similarity between the vectors.**","2acad3c3":"## The only True Evaluation\n\nThe only True Evaluation is done **online** with **good A\/B Tests**, and with **measuring the right metrics**. I list here some metrics that should be considered:\n* **Number of active professionals**: we decide that a professional is active at time t if he answered a question in the last n days ( n = 100 in implementation )\n* **Mean Time-to-First-Answer**: $\\frac{1}{nbr \\thinspace questions}\\sum_{q}^{questions}{nbr \\thinspace days \\thinspace between \\thinspace question \\thinspace q \\thinspace was \\thinspace posted \\thinspace and \\thinspace its \\thinspace first \\thinspace answer}$\n* **Number of accurate recommendations**\n* **Percentage of accurate recommendations**: Ratio $\\frac{Number \\thinspace of \\thinspace accurate \\thinspace recommendations}{Number \\thinspace of \\thinspace all \\thinspace recommendations}$. The higher the ratio, the better. It will also help avoid churning.\n\nRunning A\/B Tests on the parameters of the model will help to find the best combination of values that maximizes the metrics.","3229d819":"## 4- Proportion of accurate recommendations:\n\nThe following graph plots the the Ratio of number of accurate recommendations over all recommendations made. We can see that even though an increase of number of accurate recommendations occured in 2016 (previous graph), it was due to a significant increase in recommendations made. Ideally, a good recommender should maximize the number of accurate recommendations and minimize the number of incorrect ones in order to avoid churning.","68eff134":"## Metrics & Results of the Evaluation\n\n\nTwo principal metrics are used:\n* **Proportion of answered questions got right**: meaning, out of all the questions that were answered, how many of them did we get right when making recommendations\n* **Proportion of accurate recommendations**: out of all recommendations made, how many did we get right\n\nNow, the proposed recommender system is compared to the legacy-one. Let's check the same metrics for recommendations made by the previous legacy-system in the same test period.\n\n*<span style=\"color: red;\">REMINDER:<\/span>* **This evaluation is actually not fair !** Because as mentioned before, the legacy recommender is actually influencing the answers we have at hand. **But ouf of curiosity**, we want to check how many accurate recommendations the proposed system is able to make, even without being deployed ! This must be taken into consideration when looking at the following numbers.\n\nFurthermore, many answers are just the product of users visiting the site \"organically\", and not the product of any recommendations.\n","3ebbcbcc":"* **Testing the recommender Part 2: Difficult Case **\n\nThis is the case of a professional who follows 'cooking' and 'computer-games', but has answered a question about 'journalism' and 'scholarships'. We can see that the system balances nicely between the topics.","2c026ce0":"## 1- Professional-to-Questions Mode:\nIn this mode, three types of professionals can be distinguished:\n* **Hot**: The professional has posted at least one answer. But if no answer is posted for a defined period ( in code I set the variable min_date_for_answers=400 days ) then professional will be treated as Cold.\n* **Cold**: The professional has never posted an answer, but follows some tags.\n* **Freezing**: The professional never posted an answer and doesn't follow any tags.\n\nWhen recommending K questions, each iteration can be either **Exploitation** or **Exploration**\n\n**How to deal with the 'Freezing' professional ?**\n\nThe 'Freezing' professional has most probably registered recently, and doesn't follow any tags. We don't know much about him, the recommendations are more like 'suggestions' with the goal of taking him to the higher categories. \n\nWe can suggest:\n* **Exploit:** session-based recommendations which recommend questions similar to questions already visited \/ upvoted or commented by the professional\n* **Explore:** popular questions on the platform and newly created ones.\n\n( session-based recommendations are only feasible in production, so in this implementation we stick with exploration for this type of users )\n\n**How to deal with the 'Cold' professional ?**\n\nUnlike the 'Freezing' professional, the 'Cold' one follows one more tags. This important hint must be fully exploited as followed:\n* **Exploit:** find relatively recent questions from the tags followed.\n* **Explore:** find tags similar to the followed tags and do the same. This is possible because of the simple *pre-processing of tags*.\n\n**How to deal with the 'Hot' professional ?**\n\nThis type of professional has already expressed interest in one or more questions.\n* **Exploit:** suggest a similar question to one of the previously answered questions. To choose the answered question that will be the basis, we score all the questions and choose one question randomly based on the scores, which act as a probability distribution.\n* **Explore:** suggest most recent & similar questions from the tags followed as follows:\n    * Select the n most recent questions from each followed tag.\n    * from all those questions, select one the question which is most similar to one of the answered ones.\n\n<span style=\"color: red\">The Exploration\/Exploitation approach has the advantage of not letting many questions unanswered by recommending often, and not over-focusing on the answered ones.<\/span>\n\n### Calculating the Exploit Threshold and Scoring Questions (for the 'Hot' professional):\n\nUnlike the the two other types of professionals, the optimal Exploit Threshold for the 'Hot' professional is dynamic and changes from a professional to another. Some professionals have only answered one question, while others have answered many. Some professionals have answers which date to a relatively long time, while others have just recently answered a few. **Taking these parameters into consideration affects positively the quality of the recommendations**.\n\n* We want our recommender to prioritize questions similar to questions recently answered by the professional\n* We also don't want to completely ignore older questions.\n\n### Question-Scoring Formula:\nThe following formula scores the questions answered by the Hot professional while capturing the first note above:\n\n$$ score(x) = \\frac{log (\\frac{x}{\\epsilon})}{log (\\frac{1}{\\epsilon})} $$\nwhere:\n- x is the number of days elapsed between the date question was answered and today\n- $\\epsilon$ is the maximum number of days after which we no longer consider the question to be relevant ( now it is set to 370 but can be changed with the variable \"eps\", see code below )\n\nThe formula gives a score between 0~1 where 1 means that the question is very relevant and should be used as a reference.\n\nBelow is the values-table of the formula (credits to [this online grapher](https:\/\/www.desmos.com\/calculator)). The formula \"rewards\" questions that are recent ( x is small ). And as x gets smaller, the score drops drown in a logarithmic fashion until it interesects with the x axis exactly at the value $\\epsilon$.\n\n![question_scoring_formula_table](https:\/\/i.imgur.com\/HiHiQfA.png)\n\n### Exploit-Threshold Formula:\n\nAfter the (answered) questions get scored, the Exploit Threshold is calculated as follows:\n\n$$ threshold = log (\\sqrt{x} + 1) \\cdot \\alpha +  \\epsilon $$\nwhere:\n- x is the number of recently answered questions ( considering here only number of answers which have scores > 0 ).\n- $\\alpha$ controls the exploitation intensity (1.35 in the implementation). Acceptable values range between 1.0 for low exploitation and 1.7 for high exploitation.\n- $\\epsilon$ is an **optional** small value term (0.1 on the implementation) added if all answered questions are old ( meaning, that if x = 0, we still give a very small probability to $\\epsilon$ for exploitation, see the implementation below ).\n\nBelow is values-table of the formula. The formula give a \"bigger\" exploit-threshold as more questions answered increase. It increases in a logarithmic fashion. ( there is a 1.35 in the left of the function in the value table but I couldn't get it to be visible)\n\n![threshold_formula_table](https:\/\/i.imgur.com\/SqXgM5T.png)\n\nBelow is the implementation of the two formulas:","e23103c2":"## The Epsilon-Greedy Algorithm ( in a nutshell )\n\nTo tackle the Exploration-Exploitation problem, a popular algorithm called **'Epsilon-Greedy'** is used.\n\n> It works by setting an Epsilon threshold, which represents the probability of 'Exploitation' .\n> \n> A random number N between 0.0 and 1.0 is generated,\n> \n> if N < Epsilon\n> \n>     Exploit by searching similar questions based on the past\n> \n> else\n> \n>     Explore new questions\n\n**The Epsilon-Greedy Algorithm is simple, easy to implement and does not need heavy computation, making it a great solution for the problem at hand. **\n\n*( More details on the inner-workings in a later section )*\n\n*Note: normaly Epsilon is used for exploration, in this implementation I used it for exploitation, but the idea is the same*","0a1ab515":"Running this *\"Time Machine\"* is slow. This is because to simulate the exact state of the system at a particular date (each week) I rebuild the model each time ( I tried adding just what changed, but it was not better ). I chose a relatively short period for the test ( 2018-08-01 ~ 2018-08-22 ).\n","917889f8":"### Get Tag Suggestions for a new question:\nIt is very important to control tags, and use existing ones when possible. The following function suggests tags for a new question:","be918af5":"# Start Modeling !\n\nNow that we have pre-processed our data, we are ready for the modeling part.\n\nThe modeling steps are as follows:\n\n* **Apply TF-IDF on the hole question corpus.**\n* **Apply SVD to reduce the dimensionality of the vectors.**\n* **Construct a Questions Similarity Matrix using The Cosine Similarity function.**\n\nAfter some experimentation, I chose the number of topics ( new dimensionality of question vectors ) to be 1100 ( values between 900~1100 are ok ).\n","a4ecbb86":"*( Feel free to check the code below collapsed )*","b73ed0c6":"### Function to add a new question to DB:\n\nWhen a new question enters a DB, it should be converted and added to our model.\n* The following function transforms the new question to a vector and adds it to the matrix\n* adds an entry ( one row and one column ) to the similarity matrix","36993b42":"### Testing the recommender Part 1: standard random cases\n\nHere I'll let the recommender run on two randomly chosen professionals ( cold & Hot ).\n","3bfbedb7":"## 2- Mean Time-To-First-Answer. Can we do better ?\nThe following graph shows the evolution of the means of Time-to-First-Answer for questions of each year. A good recommendation system must minimize this metric.\n\n**Mean Time-to-First-Answer**: $\\frac{1}{nbr \\thinspace questions}\\sum_{q}^{questions}{nbr \\thinspace days \\thinspace between \\thinspace question \\thinspace q \\thinspace was \\thinspace posted \\thinspace and \\thinspace its \\thinspace first \\thinspace answer}$","0d21b756":"## 1- Need to increase the number of active professionals:\nThe following two graphs examine the degree activity of professionals in terms of number of posted answers.\n\n* The first Pie Chart shows that most of the professionals still haven't posted their first answer.\n* The second Bar Graph compares the number of active (posted at least one answer) and inactive (didn't post any answer) professionals each year.\n\nA good recommendation system can surely help professionals find relevant questions to answer and increase their activity on the platform.","128dc90f":"A recommender system's job is not that simple. If a recommender system keeps suggesting the same items to the same users, then in some cases, questions about fairness might be raised, in other cases, users might get bored getting the same type of content. In the case of Question-Answering platforms like CareerVillage, potential interests (other than the ones already expressed by the professional through tags) might be ignored and users might stop coming to the platform.\n\nA recommendation system must not only recommend relevant questions to the professionals, **Occasionally, it should also introduce them to potentially new types of questions that might interest them**. It has to deal with the cold-start problem, where very little information about he professional is known.\n\nIn the ML litterature, finding the right tradeoff between these two components is called the **Exploration-Exploitation problem**.","2e21c774":"## 1- Tags:\nFor some reason, there are many tags which are not used in any question (and they are also not followed by any user).","f816d613":"## 2- Question-to-Professionals mode:\n\nGiven a question, recommend the top K professionals to answer. This mode is used for the 'Immediate' professionals.\n\nThe overall approach taken is straightforward: recommend professionals who answered similar questions to the one given. \n\nThe 'Hot' professionals have the biggest probability to answer and are recommended, since a **key requirement of the recommender is to get a good quality answer as soon as possible.**\n\n\n### Is a professional active ?\nIt is important to know the answer to this question. Maybe the professional has answered numerous questions similar to the query but is no longer active on the platform ! So out of N candidates, we will select our k recommended ones based on their activity.\n\n\nHere, we select the top k candidates using two metrics:\n* How active they are: we determine this by using the 'last_answer_date' column, if the professional has answered a question in the last n days he gets priority (n = 60 by default in implementation) .\n* If the number candidates is still smaller than the required k, we add candidates based on the number of similar questions they answered even if they weren't active in the last n days. ( of course they have all answered similar questions, but in the first step, we prioritized the active ones, in the second step, which is optional, we fill in the missing places with professionals who answered most similar questions ). \n\nYou can check the small piece of code below: ","ab499c5e":"# Basic concepts and techniques used in the Recommender System <a class=\"anchor\" id=\"concepts\"><\/a>\nThe recommender system works in two \"modes\":\n* **Professional-to-Questions**: Recommend top K questions to a particular professional (needed for the professionals who choose a fixed frequency like 'Weekly' option )\n* **Question-to-Professionals**: Recommend top K professionals most likely to answer a particular question. (needed for the professional who choose the 'Immediate' option)\n","22f2bcbf":"<span style=\"color: blue;\">Notes on production:<\/span>\n* *Here, we use the totality of the question corpus when constructing the Similarity Matrix, in practice though, the similarity matrix will only be constructed with relatively recent questions ( last 1 or 2 years ), since old questions will not be of any use. Its construction only takes ~ 40 seconds for a ~ 24k x 24k matrix (pretty quick).*\n* *When in production, the Similarity Matrix & Questions Transformed Matrix must be updated on regular basis, depending on the traffic.*","88351823":"## 2- Questions\n* We create a new column 'processed' containing both 'title' & 'body' text, and do the same transformations we did to tags ( remove special characters, lemmatize words and remove stop words ).\n* Create a new column 'count_answers'.","63dfffd0":"### Takeaways:\n* Tags are heavily used by students in questions.\n* Most professionals follow tags to find questions related to their expertise.\n* **Most of the professionals (~63%) haven't answered any question yet.**\n* **Only a tiny proportion of recommended questions (~0.41%) in emails were accurate enough to probably lead the recipient to answer.**\n* For the moment, we can not rely on school\/group memberships, because only a tiny portion of the users have used them.","1a406aa0":"**<span style=\"color: red\">Result<\/span>**: We are now able to see that the following tags are the same: \"information-technology\", \"#informationtechnology\", \"#information-technology\", \"information-technology-\".\n\nA future task might be to explore how to add to this list the word \"IT\" (using word2vec), but the preprocessing is always necessary.","e56b93cd":"## The Exploration-Exploitation Dilemma in Recommendations:\n\n![slots](https:\/\/i.imgur.com\/pFO04zu.jpg?3)\n\n","8e3db535":"<h1><center>Epsilon-Greedy Latent Recommender<\/center><\/h1>\n\n<center><a href=\"https:\/\/www.kaggle.com\/hamzael1\">Hamza El Bouatmani<\/a> on 14th April, 2019 <\/center>\n"}}