{"cell_type":{"641b1902":"code","80837d85":"code","c7c38c0d":"code","fb1b55df":"code","446563bd":"code","21308b3f":"code","d3187763":"code","fbf06f57":"code","809f3bf5":"code","e2bd8395":"code","f00026fc":"code","d1daaf2a":"code","647f0311":"code","35f4d4a3":"code","0b2b6bc1":"code","b9be6b44":"code","699b540a":"code","fc134eed":"code","9a5dac1e":"code","80f33050":"code","18d543b5":"code","bc182351":"code","a74e0c90":"code","7a722486":"code","d2898540":"code","26043b07":"code","968cfdbd":"code","6925eddb":"code","22339b64":"code","0cc031ce":"code","3d6f6a17":"code","23285b7f":"code","e72857c9":"code","b1403ab5":"code","a5417320":"code","af39c0cb":"code","fe88708a":"code","0cf5cbf1":"code","90035cd5":"code","8d97d1dc":"code","81c07892":"code","d5f8c8d3":"code","9ecc0263":"code","8bcb1997":"code","663af8ee":"code","56d63e49":"code","e42b8589":"markdown","3e7b910d":"markdown","498358f4":"markdown","7c77296b":"markdown","7a3d4750":"markdown","ebd314f3":"markdown","15de13c8":"markdown","735061a9":"markdown","bf0db6b3":"markdown","62ddd4dc":"markdown","6a1fab91":"markdown","e23db2b2":"markdown","398615f8":"markdown","d45b33e6":"markdown","0e88ed55":"markdown","a6d264a7":"markdown","c4456c7a":"markdown","bc5628fc":"markdown","d0eb11e6":"markdown","6de066bc":"markdown","bb61bdb0":"markdown","a55d81eb":"markdown","695ccec1":"markdown","c99d9c61":"markdown"},"source":{"641b1902":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","80837d85":"credit_card = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ncredit_card.head(5)","c7c38c0d":"credit_card.info()","fb1b55df":"credit_card.dtypes","446563bd":"Non_Fraud, Fraud = credit_card['Class'].value_counts()\nprint('Number of Fraud cases     :', Fraud)\nprint('Number of Non Fraud cases :', Non_Fraud)\nvalues = [Non_Fraud, Fraud]\nlabels = ['Non_Fraud', 'Fraud']\nfig, axes = plt.subplots(1,2, figsize = (15,7))\naxes[0].pie(values, labels = labels, autopct = '%.2f%%')\nbarp = sns.barplot(x = labels , y = values, ax = axes[1])\nfor x in barp.patches:\n    barp.annotate(format(x.get_height()),(x.get_x() + x.get_width()\/2 , x.get_height()) , ha = 'center', va = 'center', xytext = (0,6), textcoords = 'offset points')\nplt.show()","21308b3f":"Fraud_amount = credit_card['Amount'][credit_card['Class']==1]\nprint('Maximum Fraud amount:', Fraud_amount.max())\nprint('Minimum Fraud amount:', Fraud_amount.min())","d3187763":"zero_amount = len(credit_card.loc[(credit_card['Amount'] ==0) & (credit_card['Class']==1)])\nprint('There has been', zero_amount , 'cases where the amount of transaction is 0 but has been considered as Fraud transaction')","fbf06f57":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\ncredit_card['scaled_amount'] = rs.fit_transform(credit_card['Amount'].values.reshape(-1,1))\ncredit_card['scaled_time'] = rs.fit_transform(credit_card['Time'].values.reshape(-1,1))","809f3bf5":"fig, axes = plt.subplots(1,2, figsize = (14,6))\nsns.distplot(credit_card['scaled_amount'], ax= axes[0])\nsns.distplot(credit_card['scaled_time'], ax= axes[1])","e2bd8395":"credit_card.drop(['Time', 'Amount'], axis = 1, inplace = True)","f00026fc":"plt.figure(figsize = (14,6))\ncredit_card.corr()['Class'].sort_values().plot(kind = 'bar')\nplt.title('Correlation With Class')\n","d1daaf2a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss","647f0311":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report","35f4d4a3":"X = credit_card.drop('Class', axis = 1)\nY = credit_card['Class']","0b2b6bc1":"x_train,x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\npred = lr.predict(x_test)\nprint(classification_report(y_test, pred))\nconfu = confusion_matrix(y_test, pred)\nsns.heatmap(confu, annot = True, fmt = 'd')","b9be6b44":"nm = NearMiss()\nx_nmtrain, y_nmtrain = nm.fit_sample(x_train,y_train)\nprint('Before undersampling the size of x_train was {}'.format(x_train.shape))\nprint('After undersampling the size is {}'.format(x_nmtrain.shape))\nprint('Before undersamplign the size of y_train was {}'.format(y_train.shape))\nprint('After undersamplign the size is {}'.format(y_nmtrain.shape))\nprint('After undersampling size of class 1 and class 0 are {} & {}'.format(sum(y_nmtrain==1), sum(y_nmtrain ==0)))\n","699b540a":"logre = LogisticRegression()\nlogre.fit(x_nmtrain, y_nmtrain)\nlogpred = logre.predict(x_test)\nprint(classification_report(y_test, logpred))\nsns.heatmap(confusion_matrix(y_test, logpred), annot = True, fmt = 'd')","fc134eed":"sm = SMOTE()\nostrain_x, ostrain_y = sm.fit_sample(x_train,y_train)\nprint('Before oversampling the size of x_train was {}'.format(x_train.shape))\nprint('After oversampling the size is {}'.format(ostrain_x.shape))\nprint('Before oversampling the size of y_train was {}'.format(y_train.shape))\nprint('After oversampling the size is {}'.format(ostrain_y.shape))","9a5dac1e":"logreg = LogisticRegression()\nlogreg.fit(ostrain_x, ostrain_y)\npre = logreg.predict(x_test)\nprint(classification_report(y_test, pre))\nsns.heatmap(confusion_matrix(y_test, pre), annot = True , fmt = 'd')","80f33050":"rf = RandomForestClassifier()\nrf.fit(ostrain_x, ostrain_y)\npredict = rf.predict(x_test)\nprint(classification_report(predict, y_test))\nsns.heatmap(confusion_matrix(y_test, predict), annot = True, fmt = 'd')","18d543b5":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(ostrain_x, ostrain_y)\npdt = knn.predict(x_test)\nprint(classification_report(y_test, pdt))\nsns.heatmap(confusion_matrix(y_test, pdt),annot = True, fmt = 'd')","bc182351":"from sklearn.metrics import precision_recall_curve\nplt.figure(figsize = (10,9))\nprecis_log, recal_log,_ = precision_recall_curve(y_test, pre)\npre_ra, recal_ra, _ = precision_recall_curve(y_test,predict)\npre_knn, recall_knn,_ = precision_recall_curve(y_test, pdt)\nplt.plot(recal_log, precis_log, label = 'logistic regression', marker = '.')\nplt.plot(recal_ra, pre_ra, label= 'Random Forest', marker = '.')\nplt.plot(recall_knn, pre_knn, label = 'KNN', marker = '.')\nplt.legend()\nplt.title('Precision Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')","a74e0c90":"fraud_cases = credit_card[credit_card['Class']==1]\nnon_fraud = credit_card[credit_card['Class']==0].sample(len(fraud_cases))\nnew_dataf = pd.concat([fraud_cases, non_fraud])\nx = new_dataf.drop(['Class'], axis =1)\ny = new_dataf['Class']","7a722486":"train_x,test_x,train_y, test_y = train_test_split(x,y, test_size = 0.3, random_state = 42)\n","d2898540":"logre = LogisticRegression()\nlogre.fit(train_x, train_y)\nprediction = logre.predict(test_x)\nprint(classification_report(test_y, prediction))\nconf = confusion_matrix(test_y, prediction)\nsns.heatmap(conf , annot = True, fmt = 'd')","26043b07":"ranfor = RandomForestClassifier()\nranfor.fit(train_x, train_y)\nranfor_predict = ranfor.predict(test_x)\nprint(classification_report(test_y, ranfor_predict))\nconfumatr = confusion_matrix(test_y, ranfor_predict)\nsns.heatmap(confumatr, annot = True, fmt = \"d\")\nplt.show()","968cfdbd":"knb = KNeighborsClassifier()\nknb.fit(train_x, train_y)\nknn_pred = knn.predict(test_x)\nprint(classification_report(test_y, knn_pred))\nknconfu = confusion_matrix(test_y, knn_pred)\nsns.heatmap(knconfu, annot = True, fmt = 'd')","6925eddb":"log_prec,log_recal,_ = precision_recall_curve(test_y, prediction)\nran_prec, ran_recal,_ = precision_recall_curve(test_y,ranfor_predict )\nknd_prec, knd_recal,_ = precision_recall_curve(test_y,knn_pred)\nplt.figure(figsize = (10,9))\nplt.plot(log_recal, log_prec, label= 'Logistic Regression', marker = '.')\nplt.plot(ran_recal, ran_prec, label = 'Random Forest', marker = '.')\nplt.plot(knd_recal, knd_prec, label = 'KNN', marker = '.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall(50|50 sample)')\nplt.legend()","22339b64":"smo = SMOTE(random_state = 42)\ntrainsmo_x, trainsmo_y = smo.fit_sample(train_x, train_y)\nlogreg = LogisticRegression()\nlogreg.fit(trainsmo_x, trainsmo_y)\nnew_predic = logreg.predict(test_x)\nprint(classification_report(test_y, new_predic))\nsns.heatmap(confusion_matrix(test_y, new_predic),annot = True, fmt = 'd')","0cc031ce":"osprediction = knb.predict(x_test)\nprint(classification_report(y_test, osprediction))\nsns.heatmap(confusion_matrix(y_test, osprediction), annot = True, fmt = 'd')","3d6f6a17":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense","23285b7f":"model = Sequential([\n    # First layter\n    Dense(units = 17, input_dim = 30, activation = 'relu'),\n    #Second layer\n    Dense(units = 20, activation = 'relu'),\n    Dropout(0.5),\n    Dense(units = 20, activation = 'relu'),\n    Dense(units = 1, activation = 'sigmoid')\n])","e72857c9":"model.summary()","b1403ab5":"model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size = 15, epochs = 5)","a5417320":"model.evaluate(x_test,y_test)\n","af39c0cb":"dlpre = model.predict(x_test)\n#Y_test = pd.DataFrame(y_test)\nprint(classification_report(y_test, dlpre.round()))\nsns.heatmap(confusion_matrix(y_test, dlpre.round()), annot = True, fmt = 'd')","fe88708a":"from imblearn.under_sampling import NearMiss\nnm = NearMiss()\nnm_xtrain,nm_ytrain = nm.fit_sample(x_train,y_train)","0cf5cbf1":"y_test = pd.DataFrame(y_test)","90035cd5":"model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = (['accuracy']))\nmodel.fit(nm_xtrain,nm_ytrain, batch_size = 15, epochs = 5)\nkerpdt = model.predict(x_test)\nprint(classification_report(y_test, kerpdt.round()))\nsns.heatmap(confusion_matrix(y_test, kerpdt.round()), annot = True, fmt = 'd')","8d97d1dc":"smot = SMOTE()\nsm_xtrain, sm_ytrain = smot.fit_sample(x_train, y_train)\nmodel.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(sm_xtrain, sm_ytrain, batch_size = 15, epochs =5)\nkpred = model.predict(x_test)\nprint(classification_report(y_test, kpred.round()))\nsns.heatmap(confusion_matrix(y_test, kpred.round()), annot = True, fmt = 'd')","81c07892":"model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics =['accuracy'])\nmodel.fit(train_x, train_y, batch_size = 15, epochs = 5)\nkpredic = model.predict(test_x)\nprint(classification_report(test_y, kpredic.round()))\nsns.heatmap(confusion_matrix(test_y, kpredic.round()), annot = True, fmt='d')","d5f8c8d3":"model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(train_x, train_y, batch_size = 15, epochs = 5)\nkpre = model.predict(x_test)\nprint(classification_report(y_test, kpre.round()))\nsns.heatmap(confusion_matrix(y_test, kpre.round()), annot = True, fmt = 'd')","9ecc0263":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model.fit(x_train, y_train)\nxgbpre = xgb_model.predict(x_test)\nprint(classification_report(y_test, xgbpre))\nsns.heatmap(confusion_matrix(y_test, xgbpre), annot = True, fmt = 'd')","8bcb1997":"log_sm_pre = logreg.predict(test_x)\nprint(classification_report(test_y, log_sm_pre))\nsns.heatmap(confusion_matrix(test_y, log_sm_pre), annot = True, fmt = 'd')","663af8ee":"model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = (['accuracy']))\nmodel.fit(nm_xtrain,nm_ytrain, batch_size = 15, epochs = 5)\nfin_pred = model.predict(test_x)\nprint(classification_report(test_y, fin_pred.round()))\nsns.heatmap(confusion_matrix(test_y, fin_pred.round()), annot = True, fmt = 'd')","56d63e49":"xg_predt = xgb_model.predict(test_x)\nprint(classification_report(test_y, xg_predt))\nsns.heatmap(confusion_matrix(test_y, xg_predt), annot = True, fmt = 'd')","e42b8589":"**Keras Sequential Model after Near Miss Algorithm**","3e7b910d":"**Keras Sequential Model**","498358f4":"***Final Algorithm: Deep Learning Framework Keras gives the best prediction for the given Dataset***","7c77296b":"*Accuracy was too low and false Negative's are too high for Near Miss Algorithm let's try SMOTE(over sampling) alogorithm*","7a3d4750":"Here all the columns are in one range except Time and Amount. So these two columns require Standardization of the data. We have various methods to standardize our data, out of all standardscaler is used more frequently. StandardScaler assumes that your data is normally distributed. So here both Amount, Time columns are not normally distributed so let's go for other method RobustScaler.","ebd314f3":"**Now let's take only few sample cases where length of sample is equal to number of fraud cases and implement different algorithms and check whether it works for original data.**","15de13c8":"**It's a Imbalanced Data, we can solve this by mainly 2 algorithms**\n* SMOTE(over sampling)\n* NearMiss(under sampling)","735061a9":"**Please don't hesitate to comment down for any suggestion or correction's. As a newbie i highly value your suggestion and please do upvote my kernel if you had learned something new from my kernel. **\n","bf0db6b3":"**Precision Recall Curve**","62ddd4dc":"**I think this are the cases where the transaction is identified as fraud before the amount has been deducted. **","6a1fab91":"**Let's try SMOTE on this 50|50 sample**","e23db2b2":"**So let's plot Precision Recall plot.**\n* But remember ROC Curves should be used when the data set has approximately equal class distribution.\n* Precision Recall Plot should be used when there is imbalanced data distribution, so let's plot precision , recall plot.\n","398615f8":"**Apart From the above 3 models Keras which was trained on 50|50 (sample) data gave exceptionally high performace on the Initial data(original data)**","d45b33e6":"**Applying keras model on 50|50 sample data**","0e88ed55":"**Getting the trained model of Logistic Regression(after SMOTE) from above.**","a6d264a7":"**This does'nt work for original data even though it gives good recall but fails to lower the amount of False Negatives**","c4456c7a":"**Keras Sequential Model (after SMOTE algorithm)**","bc5628fc":"**From all the Above algorithms KNN algorithm , looks like it gives good classification for the give sample(50|50). Let's try this on the original split data.**","d0eb11e6":"**Extreme Gradient Boosting(XGB) Algorithm**","6de066bc":"**Near Miss Algorithm (Under Sampling)**","bb61bdb0":"**Wow Accuracy is 100% but if you check recall for Class 1 it's 0.62 which tells clearly that the algorithm does'nt work fine with these imbalanced data so let's use**","a55d81eb":"**SMOTE (Over sampling algorithm)**","695ccec1":"**50|50 (sample method)**","c99d9c61":"**So the algorithms which gave best performance are**\n* Logistic Regression(after SMOTE)\n* Keras (after Near Miss)\n* XGB"}}