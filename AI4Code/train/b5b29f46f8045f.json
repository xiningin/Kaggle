{"cell_type":{"b385af32":"code","e7cecfd4":"code","337e49f2":"code","f12dc56f":"code","487c6fae":"code","083ed822":"code","dd180bf9":"code","fdbe334c":"code","87614914":"code","bbfbeb65":"code","08090ae8":"code","b0d92c63":"code","12aaef69":"markdown","dce071bc":"markdown","aac13980":"markdown","8d5dafc6":"markdown","eaedc2d6":"markdown","f223ffc8":"markdown","eb6a1bd4":"markdown"},"source":{"b385af32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"..\/input\"))\n# print(os.listdir(train_path))\n\n# train_path\n# Any results you write to the current directory are saved as output.\n\nfrom time import time\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nfrom keras.callbacks import TensorBoard,EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,Callback\n# import the necessary packages\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras import backend as K","e7cecfd4":"\n# datapath='..\/input\/'+os.listdir(\"..\/input\")[1]+'\/'\n# if not Path(datapath).is_dir():\ndatapath='..\/input\/'\ndf = pd.read_csv(datapath+'train_labels.csv')\nprint(df.head())","337e49f2":"print('Number of image : ', len(df))\nprint('Ratio labels : ', sum(df['label'].values)\/len(df))\nimg = plt.imread(datapath+\"train\/\"+df.iloc[0]['id']+'.tif')\nprint('Images shape', img.shape)","f12dc56f":"for i in range(5):\n    img = plt.imread(datapath+\"train\/\"+df.iloc[i]['id']+'.tif')\n    print(df.iloc[i]['label'])\n    plt.imshow(img)\n    plt.show()","487c6fae":"model = Sequential()\nmodel.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))\n#96\nmodel.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n#96\nmodel.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n#96\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 3))\n#32\nmodel.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 3))\n#11\nmodel.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 3))\n#4\nmodel.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\nmodel.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\nmodel.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png')\n","083ed822":"df = pd.read_csv(datapath+'train_labels.csv',dtype='str')\ndef append_ext(fn):\n    return fn+\".tif\"\n\ndf[\"id\"]=df[\"id\"].apply(append_ext)\ntrain_datagen = ImageDataGenerator(\n       # horizontal_flip=True,\n       #vertical_flip=True,\n       #brightness_range=[0.5, 1.5],\n       #fill_mode='reflect',                               \n        #rotation_range=15,\n        rescale=1.\/255,\n        #shear_range=0.2,\n        #zoom_range=0.2\n        validation_split=0.15\n    \n)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_path = datapath+'train'\nvalid_path =  datapath+'train'\ntest_path=datapath+'test'\nprint(train_path)\nprint(valid_path)\n\nprint(df.head())\nprint(df.dtypes)\nprint(df.loc[0])\nprint(df.loc[1])","dd180bf9":"\n\ntrain_generator = train_datagen.flow_from_dataframe(\n                dataframe=df,\n                directory=train_path,\n                x_col = 'id',\n                y_col = 'label',\n                has_ext=False,\n                subset='training',\n                target_size=(96, 96),\n                batch_size=256,\n                class_mode='binary'\n                )\n\n\nvalidation_generator = train_datagen.flow_from_dataframe(\n                dataframe=df,\n                directory=valid_path,\n                x_col = 'id',\n                y_col = 'label',\n                has_ext=False,\n                subset='validation', # This is the trick to properly separate train and validation dataset\n                target_size=(96, 96),\n                batch_size=64,\n                shuffle=False,\n                class_mode='binary'\n                )\n# df_test = pd.read_csv(datapath+'train_labels.csv')\n# test_generator = test_datagen.flow_from_directory(test_path,\n#                                         target_size=(96,96),\n#                                         batch_size=64,\n#                                         class_mode='binary',\n#                                         shuffle=False)\n","fdbe334c":"class LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = {'batch': [], 'epoch': []}\n        self.accuracy = {'batch': [], 'epoch': []}\n        self.val_loss = {'batch': [], 'epoch': []}\n        self.val_acc = {'batch': [], 'epoch': []}\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses['batch'].append(logs.get('loss'))\n        self.accuracy['batch'].append(logs.get('acc'))\n        self.val_loss['batch'].append(logs.get('val_loss'))\n        self.val_acc['batch'].append(logs.get('val_acc'))\n\n    def on_epoch_end(self, batch, logs={}):\n        self.losses['epoch'].append(logs.get('loss'))\n        self.accuracy['epoch'].append(logs.get('acc'))\n        self.val_loss['epoch'].append(logs.get('val_loss'))\n        self.val_acc['epoch'].append(logs.get('val_acc'))\n\n    def plot(self, loss_type):\n        iters = range(len(self.losses[loss_type]))\n    \n        plt.figure(figsize=(16,10))\n        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n        plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n        plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n        plt.grid(True)\n        plt.xlabel(loss_type)\n        plt.ylabel('acc-loss')\n        plt.legend(loc=\"upper right\")\n        plt.show()\n        \n    def save(self,name):\n        arr=np.vstack((self.accuracy[\"epoch\"],self.losses[\"epoch\"],self.val_acc[\"epoch\"],self.val_loss[\"epoch\"]))\n        np.save(name,arr)\n        \n","87614914":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n#---\nhistory = LossHistory()\n                              \n                              \n#---\nSTEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID=validation_generator.n\/\/validation_generator.batch_size\n#-----\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr,history]\n\n#----\nhis=model.fit_generator(\n                train_generator,\n                steps_per_epoch=STEP_SIZE_TRAIN,\n                epochs=15,\n                callbacks=callbacks_list,\n                validation_data=validation_generator,\n                validation_steps=STEP_SIZE_VALID)\n","bbfbeb65":"history.plot(\"epoch\")\nhistory.save(\"A.npy\")","08090ae8":"import matplotlib.pyplot as plt\ntrain_acc = his.history['acc']\nval_acc = his.history['val_acc']\n\nepochs = range(len(train_acc))\nplt.plot(epochs,train_acc,'b',label='Training accuracy')\nplt.plot(epochs,val_acc,'r',label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.show()","b0d92c63":"test_df = pd.read_csv(datapath+'sample_submission.csv')\n\nfrom matplotlib.pyplot import imread\n# Kaggle testing\nfrom glob import glob\nTESTING_BATCH_SIZE = 64\n\n# datapath='..\/input\/'+os.listdir(\"..\/input\")[1]+'\/'\n# if not Path(datapath).is_dir():\n#     datapath='..\/intput\/'\ntmp_path=datapath+'test\/'\ntesting_files = glob(os.path.join(tmp_path,'*.tif'))\nsubmission = pd.DataFrame()\nprint(len(testing_files))\nfor index in range(0, len(testing_files), TESTING_BATCH_SIZE):\n    data_frame = pd.DataFrame({'path': testing_files[index:index+TESTING_BATCH_SIZE]})\n    data_frame['id'] = data_frame.path.map(lambda x: x.split('\/')[3].split(\".\")[0])\n    data_frame['image'] = data_frame['path'].map(imread)\n    images = np.stack(data_frame.image, axis=0)\n    predicted_labels = [model.predict(np.expand_dims(image\/255.0, axis=0))[0][0] for image in images]\n    predictions = np.array(predicted_labels)\n    data_frame['label'] = predictions\n    submission = pd.concat([submission, data_frame[[\"id\", \"label\"]]])\n    if index % 1000 == 0 :\n        print(index\/len(testing_files) * 100)\nsubmission.to_csv('submission_new_model.csv', index=False, header=True)\nmodel.save('model.h5')\nprint(submission.head())","12aaef69":"## Some Exploratory Data Analysis (EDA)","dce071bc":"## Submission","aac13980":"## Model definition \uff1a CNN from Scratch","8d5dafc6":"## Draw","eaedc2d6":"## Dataset Generators","f223ffc8":"<h1><center> CNN with Keras <\/center><h1>","eb6a1bd4":"## Training routine "}}