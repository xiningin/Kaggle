{"cell_type":{"3b9bffd1":"code","736012f8":"code","40777ec0":"code","cf29bc92":"code","1190046c":"code","20bff998":"code","32868633":"code","90623e1c":"code","eb27eb3a":"code","b7239161":"code","44185d9b":"code","bafce213":"code","bbff1d51":"code","5f9a7a52":"code","0b4550bc":"code","c60e3b9c":"code","a91e6485":"code","a27fa3cd":"code","98585cbd":"code","350e9c65":"code","b32ede3d":"code","bf883371":"code","ac7ac827":"code","c16b78fa":"code","b59f214c":"code","618f9933":"code","5131f7de":"code","86a773a1":"code","580ad57b":"code","987951aa":"code","cfaa90e5":"code","8fb30e82":"code","ddc5752b":"code","15fa19f4":"code","1fc79168":"code","0da8a576":"code","d33dcf66":"code","becb67f6":"code","5c97d684":"code","f47f42fa":"code","ef5cafe6":"code","5ef1522d":"code","6211b9d6":"code","8e8efcde":"code","9d6616c8":"code","83064980":"code","dfd5b5f4":"code","22f8932e":"code","f6448e91":"code","75a286e9":"markdown","292b1b17":"markdown","1487f367":"markdown","fbbb3ad9":"markdown","b138a99f":"markdown","d2a1bbeb":"markdown","838f184e":"markdown","739e3b3d":"markdown","79350de2":"markdown","b31ab177":"markdown","29207032":"markdown","fdc4e07b":"markdown","e74dbe21":"markdown","2f756f67":"markdown","1fd397ba":"markdown","9a3b4354":"markdown","7b6aaf74":"markdown","8facb2f4":"markdown","47f5b96b":"markdown","c7c74245":"markdown","01d4fdb9":"markdown","14158803":"markdown","f368b31d":"markdown","4a2bb5ff":"markdown","fd5d5931":"markdown","d7ce97ae":"markdown","1083a274":"markdown","869b609d":"markdown","ecfc154d":"markdown","a05e1bf3":"markdown","4fdfd65e":"markdown","2e222cd6":"markdown"},"source":{"3b9bffd1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline","736012f8":" \ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","40777ec0":"# to print first five rows of train data\ntrain_data.head()","cf29bc92":"# to print first five rows of test data\ntest_data.head()","1190046c":"print(\"Total number of rows in training data \", train_data.shape[0])\nprint(\"Total number of columns in training data \", train_data.shape[1])\nprint(\"Total number of rows in test data \", test_data.shape[0])\nprint(\"Total number of columns in test data \", test_data.shape[1])\n","20bff998":"plt.figure(figsize = (13,5))\nplt.bar(train_data.columns, train_data.isna().sum())\nplt.xlabel(\"Columns name\")\nplt.ylabel(\"Number of missing values in training data\")\nplt.show()\n# from the bar plot of missing value we can conclude that Cabin, Embarked and Cabin column has null value so, we \n# can either drop the entire row or can fill the nan value with some values like mean, meadian. ","32868633":"plt.figure(figsize = (13,5))\nplt.bar(test_data.columns, test_data.isnull().sum().values, color = 'red')\nplt.xlabel(\"Columns name\")\nplt.ylabel(\"Number of missing values in test data\")\nplt.show()\n# similarly we can conclude that Age Cabin and Fare column has nan values . ","90623e1c":"#Visualizing the Number of survived passenger\nsns.countplot('Survived', data = train_data)\nplt.show()\n# here we plot only for train_data as we donot have Survived column for test data,\n# This plot show that around 600 people died while around 300 survived","eb27eb3a":"# visualizing the number of passenger from different embarked column in train_data\nsns.countplot('Embarked', data = train_data)\nplt.show()","b7239161":"#visualizing whether gender affect the survival rate or not\nsns.countplot('Survived', hue = 'Sex', data = train_data)\nplt.plot()\n# the graph clearly show that death rate for male passenger is way more than that for female\n","44185d9b":"# visualizing whether pclass affect the survial rate or not\nsns.countplot(\"Survived\", hue = 'Pclass', data = train_data)\nplt.show()\n# this graph clearly show that people in third class are more likely to die ","bafce213":"# visualizing whether embarked place affects the survival rate or not\nsns.countplot('Survived', hue = 'Embarked', data = train_data)\nplt.show()","bbff1d51":"sns.boxplot('Fare', data = train_data)\nplt.show()\n# this shows that there were very few people who payed more than 100","5f9a7a52":"sns.boxplot('Age', data = train_data)\nplt.show()\n# this shows that there were very few people more than 65 years old in training data","0b4550bc":"# ploting histogram\n# choosing value for bin \ninterval = 10\nvalue_for_bin = np.ceil((train_data.Age.max() - train_data.Age.min()) \/ interval).astype(int)\n\nplt.hist(train_data.Age, bins = value_for_bin)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number\")\nplt.show()\n# this shows that lots of passenger we from age between 20 to 40","c60e3b9c":"plt.figure(figsize = (10,4))\nplt.hist(train_data.Fare, bins = 10, color = 'lime')\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Number\")\nplt.show()\n# this shows that around 700 people pay in between 0 and 50","a91e6485":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()\nplt.show()","a27fa3cd":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()\nplt.show()","98585cbd":"corr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()\n# this shows that SibSp and Parch columns are releted , so we can combine this two column to reduce the dimension\n# of our data.. this plot only works for columns with numercal data ","350e9c65":"((train_data.groupby(['Sex','Survived']).Survived.count() * 100) \/ train_data.groupby('Sex').Survived.count())\n# this shows that female have around 74% chance of survival while male have around 81% chance of death","b32ede3d":"(train_data.groupby(['Pclass','Survived']).Survived.count() * 100) \/ train_data.groupby('Pclass').Survived.count()\n# this shows that people belonging to third class are likely to die while people in class one are likely to survive","bf883371":"(train_data.groupby(['Embarked','Survived']).Survived.count() * 100) \/ train_data.groupby('Embarked').Survived.count()\n# this shows that people who embarked from Southampton are likely to die","ac7ac827":"train_data.groupby(by=['Survived']).mean()[\"Age\"]\n# this show that average age of people who survived was around 28 years old","c16b78fa":"# before filling the missing values, let's drop Cabin column from both data.\ntrain_data.drop('Cabin', axis = 1, inplace = True)\ntest_data.drop('Cabin', axis = 1, inplace = True)","b59f214c":"combined_data = [train_data, test_data]\nfor data in combined_data:\n    print(data.isnull().sum())\n    print('*' * 20)\n      ","618f9933":"# filling the nan values fo Age and fare column with the mean while Embarked column with most_frequent value\nfor data in combined_data:\n    data.Age.fillna(data.Age.mean(), inplace = True)\n    data.Fare.fillna(data.Fare.mean(), inplace = True)\n    \n# from visualization we know that Southamptom is most frequent Embarked place so, filling the missing value \n# with 'S'\ntrain_data.Embarked.fillna('S', inplace = True)\n\n# we simply can use SimpleImputer class form the sklearn to deal with the missing value\n# from sklearn.impute import SimpleImputer\n# impute = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n# train_data[['Age']] = impute.fit_transform(train_data[['Age']])\n    ","5131f7de":"\ndef change_gender(x):\n    if x == 'male':\n        return 0\n    elif x == 'female':\n        return 1\ntrain_data.Sex = train_data.Sex.apply(change_gender)\ntest_data.Sex = test_data.Sex.apply(change_gender)\n# we simply can use mapfunction to change the gender\n# train_data.Sex = train_data.Sex.map({'female':1, 'male':0})\n","86a773a1":"\nchange = {'S':1,'C':2,'Q':0}\ntrain_data.Embarked = train_data.Embarked.map(change)\ntest_data.Embarked = test_data.Embarked.map(change)","580ad57b":"\ntrain_data['Alone'] = train_data.SibSp + train_data.Parch\ntest_data['Alone'] = test_data.SibSp + test_data.Parch\n\ntrain_data.Alone = train_data.Alone.apply(lambda x: 1 if x == 0 else 0)\ntest_data.Alone = test_data.Alone.apply(lambda x: 1 if x == 0 else 0)","987951aa":"# now lets drop SibSp and Parch column for both training and testing data\ntrain_data.drop(['SibSp','Parch'], axis = 1, inplace = True)\ntest_data.drop(['SibSp','Parch'], axis = 1, inplace = True )","cfaa90e5":"train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False).unique().size\n# there are total 17 unique title","8fb30e82":"# lets create the Title feature which contain the title of the passenger and drop Name column\nfor data in combined_data:\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n    data.drop('Name', axis = 1, inplace = True)\n       ","ddc5752b":"train_data.Title.value_counts()","15fa19f4":"test_data.Title.unique()","1fc79168":"#lets replace least occuring title in the data with rare\nleast_occuring = [ 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Dona',\n       'Jonkheer']\nfor data in combined_data:\n    data.Title = data.Title.replace(least_occuring, 'Rare')","0da8a576":"# lets perform title mapping in order to change to ordinal\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor data in combined_data:\n    data['Title'] = data['Title'].map(title_mapping)\n\n\n","d33dcf66":"columns_to_drop = ['PassengerId','Ticket']\ntrain_data.drop(columns_to_drop, axis = 1, inplace = True)\ntest_data.drop(columns_to_drop[1], axis = 1, inplace = True)","becb67f6":"for dataset in combined_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n","5c97d684":"for data in combined_data:\n    data.loc[data['Fare'] < 30, 'Fare'] = 1\n    data.loc[(data['Fare'] >= 30) & (data['Fare'] < 50),'Fare'] = 2\n    data.loc[(data['Fare'] >= 50) & (data['Fare'] < 100),'Fare'] = 3\n    data.loc[(data['Fare'] >= 100),'Fare'] = 4","f47f42fa":"corr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()","ef5cafe6":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test = test_data.drop(\"PassengerId\", axis = 1)\nprint(\"shape of X_train\",X_train.shape)\nprint(\"Shape of Y_train\",Y_train.shape)\nprint(\"Shape of x_test\",X_test.shape)\n","5ef1522d":"import tensorflow as tf\nimport keras \nfrom keras.layers import Dense, Dropout, Input\nfrom keras.models import Sequential\n","6211b9d6":"model = Sequential()\nmodel.add(Dense(units = 32, input_shape = (7,), activation = 'relu'))\nmodel.add(Dense(units = 64, activation = 'relu', kernel_initializer = 'he_normal', use_bias = False))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(Dense(units = 128, activation = 'relu',kernel_initializer = 'he_normal', use_bias = False))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(units = 64, activation = 'relu',kernel_initializer = 'he_normal', use_bias = False))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(units = 32, activation = 'relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(units = 16, activation = 'relu'))\nmodel.add(Dense(units = 8, activation = 'relu',kernel_initializer = 'he_normal', use_bias = False))\nmodel.add(Dense(units =1 , activation = 'sigmoid'))","8e8efcde":"model.summary()","9d6616c8":"model.compile(loss = tf.keras.losses.binary_crossentropy, optimizer = tf.keras.optimizers.Adam(), metrics = ['acc'])\nmodel.fit(X_train, Y_train, batch_size = 32, verbose = 2, epochs = 50)","83064980":"predict = model.predict(X_test)\n#since we have use sigmoid activation function in output layer\npredict = (predict > 0.5).astype(int).ravel()\nprint(predict)","dfd5b5f4":"submit = pd.DataFrame({\"PassengerId\":test_data.PassengerId, 'Survived':predict})\nsubmit.to_csv(\"final_submission.csv\",index = False)","22f8932e":"from sklearn import metrics\nY_pred_rand = (model.predict(X_train) > 0.5).astype(int)\nprint('Precision : ', np.round(metrics.precision_score(Y_train, Y_pred_rand)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_pred_rand)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_pred_rand)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_pred_rand)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_pred_rand)*100,2))","f6448e91":"\n# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_pred_rand)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","75a286e9":"##### Let's  start by converting Sex feature to categorical  female=1 and male=0","292b1b17":"**Prediction for test data**","1487f367":"## Importing all necessary libraries\n","fbbb3ad9":"## Brief Introduction about Titanic Ship\nTitanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters.\nThough there were about 2,224 passengers and crew members, we are given data of about 1,300 passengers. Out of these 1,300 passengers details, about 9000 data is used for training purpose and remaining 400 is used for test purpose. In this competition we are given about 400 test data with missing survived column and we have to use different machine learning algorithms to predict whether the passengers in the test data survived or not.\nBoth training and test data arenot clean(contains lots of missing values), so main goal of this competition is to clean the data and build the model with best accuracy.","b138a99f":"#### Any suggestions to improve this model is really appreciated.\n## Feel free to comment \n# Thank you ","d2a1bbeb":"# Data visualization and Analysis","838f184e":"## Binning Age and Fare columns","739e3b3d":"# Dealing with the Missing values","79350de2":"## Converting a categorical feature\u00b6\n\n","b31ab177":"Visualizing the number of null values in both data ","29207032":"## Defining model\nHere, I have used different number of neurons for each layer and different value for dropout. You can play with these hyperparameter for better outut.","fdc4e07b":"# Dropping PassengerId and Ticket column","e74dbe21":"### Box and whisker plot ","2f756f67":"#### Survival rate for male and female","1fd397ba":"**using map funcion to change the Embarked column S = 1, C = 2, Q = 0**","9a3b4354":"# Neural Network\n<img src = 'https:\/\/i.imgur.com\/mIsGh53.jpg'>","7b6aaf74":"## Simple Data Analysis","8facb2f4":"# preparing training and testing data","47f5b96b":"# Creating new feature Title extracting from existing feature Name\n","c7c74245":"<img src = 'https:\/\/media.nationalgeographic.org\/assets\/photos\/000\/273\/27302_c0-41-990-701_r1050x700.jpg?d4ccf3044d9da0d0118103be3a76bd1319370847' >","01d4fdb9":"## Compiling and fitting model","14158803":"**Survival rate on the basis of Embarked place**","f368b31d":"## Understanding the data\n\n\n\nSurvival : 0 = No, 1 = Yes\n\nPclass : A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\nsibsp : The # of siblings \/ spouses aboard the Titanic Sibling = brother, sister, stepbrother, stepsister Spouse = \n\nhusband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch : The # of parents \/ children aboard the Titanic Parent = mother, father Child = daughter, son, \nstepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them.\n\nTicket : Ticket number\n\nFare : Passenger fare\n\nCabin : Cabin number embarked\n\nPort of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\n\nName, Sex , Age are self-explanatory\n\n","4a2bb5ff":"while visualizing the correlation heatmap we came to know that Sibsp and Parch columns were closely related \nso lets created new column called Alone using this two columns\n-------> 1 = Alone , 0 = not Alone","fd5d5931":"## Analyze both training and testing data to get better understanding of the data\n\n","d7ce97ae":"\n#### We can see that total number of columns in test data is one less than that in train data. The missing column in the test data is survived column which we have to predict using suitable machine learning algorithm","1083a274":"# For submission","869b609d":"### Load the data","ecfc154d":"## Import necessary libraries","a05e1bf3":"**Survival rate on the basis of Pclass**","4fdfd65e":"# Feature Extraction\n","2e222cd6":"### Model summary"}}