{"cell_type":{"9ecd960f":"code","f42b6d9c":"code","38f2e407":"code","7c1b3434":"code","3f48c24a":"code","af21ad5d":"code","e179be41":"code","026eaa9a":"code","388a243c":"code","5815a49b":"code","6050f320":"code","c1cef214":"code","672da956":"markdown","3b32e802":"markdown","05a8ddb4":"markdown","63b1d937":"markdown","05316d64":"markdown","c6bfeb30":"markdown"},"source":{"9ecd960f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.utils import plot_model\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import SVG, Image\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","f42b6d9c":"def plot_example_images(plt):\n    img_size = 48\n    plt.figure(0, figsize=(12,20))\n    ctr = 0\n    for expression in os.listdir(\"..\/input\/fer2013\/test\"):\n        for i in range(1,6):\n            ctr += 1\n            plt.subplot(7,5,ctr)\n            img = load_img(\"..\/input\/fer2013\/test\/\" + expression + \"\/\" +os.listdir(\"..\/input\/fer2013\/test\/\" + expression)[i], target_size=(img_size, img_size))\n            plt.imshow(img, cmap=\"gray\")\n\n    plt.tight_layout()\n    return plt","38f2e407":"plot_example_images(plt).show()","7c1b3434":"df = {}\nfor expression in os.listdir(\"..\/input\/fer2013\/train\/\"):\n    directory = \"..\/input\/fer2013\/train\/\" + expression\n    df[expression] = len(os.listdir(directory))\ndf = pd.DataFrame(df, index=[0])","3f48c24a":"pd.options.plotting.backend = \"plotly\"\nfig = df.transpose().plot.bar()\nfig.show()","af21ad5d":"img_size = 48\nbatch_size = 64\n\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255.0, horizontal_flip=True)\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=40,\n      horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.0\/255.0)\n\ntrain_generator = train_datagen.flow_from_directory(\"..\/input\/fer2013\/train\/\", target_size = (img_size, img_size),\n                                             color_mode = 'grayscale',\n                                             batch_size = batch_size,\n                                             class_mode = 'categorical',\n                                             shuffle = True)\n\ntest_generator = test_datagen.flow_from_directory(\"..\/input\/fer2013\/test\/\", target_size = (img_size, img_size),\n                                             color_mode = 'grayscale',\n                                             batch_size = batch_size,\n                                             class_mode = 'categorical',\n                                             shuffle = True)","e179be41":"os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'","026eaa9a":"physical_devices = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)","388a243c":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), padding='same',\n                           input_shape=(48, 48, 1),\n                          kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.PReLU(alpha_initializer='zeros'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    tf.keras.layers.Conv2D(128, (5, 5), padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.PReLU(alpha_initializer='zeros'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    \n    tf.keras.layers.Conv2D(512, (3, 3), padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.PReLU(alpha_initializer='zeros'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    tf.keras.layers.Conv2D(512, (3, 3), padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.PReLU(alpha_initializer='zeros'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.PReLU(alpha_initializer='zeros'),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(1024),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.PReLU(alpha_initializer='zeros'),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(7, activation='softmax')\n])\n# Adadelta(lr=0.1, rho=0.95, epsilon=1e-08)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n              loss=\"categorical_crossentropy\",\n              metrics=['accuracy'])","5815a49b":"epochs = 15\nsteps_per_epoch = train_generator.n \/\/ train_generator.batch_size\nvalidation_steps = test_generator.n \/\/ test_generator.batch_size\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\"os.\/model_weights.h5\", monitor='val_accuracy',\n                            save_weights_only = True,\n                            mode = 'max',\n                            verbose = 1)\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=3, verbose=1)\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(\"model.h5\", monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks = [checkpoint, lr_reducer, checkpointer]\n\nhistory = model.fit(train_generator,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=epochs,\n                    validation_data=test_generator,\n                   validation_steps=validation_steps,\n                   callbacks=callbacks)","6050f320":"epochs = epochs\nmetric = \"loss\"\nmetric2 = \"accuracy\"\ntrain_m1 = history.history[metric]\ntrain_m2 = history.history[metric2]\nval_m1 = history.history[f'val_{metric}']\nval_m2 = history.history[f'val_{metric2}']\n\nfig_model = make_subplots(1,\n                          2,\n                          subplot_titles=(f\"{metric.capitalize()} over Time\",\n                                          f\"{metric2.capitalize()} over Time\"))\nfig_model.add_trace(go.Scatter(x=np.arange(epochs),\n                               y=train_m1,\n                               mode=\"lines+markers\",\n                               name=f\"Training {metric.capitalize()}\"),\n                    row=1,\n                    col=1)\nfig_model.add_trace(go.Scatter(x=np.arange(epochs),\n                               y=val_m1,\n                               mode=\"lines+markers\",\n                               name=f\"Validation {metric.capitalize()}\"),\n                    row=1,\n                    col=1)\n\nfig_model.add_trace(go.Scatter(x=np.arange(epochs),\n                               y=train_m2,\n                               mode=\"lines+markers\",\n                               name=f\"Training {metric2.capitalize()}\"),\n                    row=1,\n                    col=2)\nfig_model.add_trace(go.Scatter(x=np.arange(epochs),\n                               y=val_m2,\n                               mode=\"lines+markers\",\n                               name=f\"Validation {metric2.capitalize()}\"),\n                    row=1,\n                    col=2)\nfig_model.show()","c1cef214":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as f:\n    f.write(model_json)","672da956":"## Generating Training Data","3b32e802":"## Visualizing the Dataset","05a8ddb4":"## Exporting the Model","63b1d937":"Inspired by Goodfellow, I.J., et.al. (2013). Challenged in representation learning: A report of three machine learning contests. Neural Networks, 64, 59-63. doi:10.1016\/j.neunet.2014.09.005\n\n* Human accuracy on the FER-2013 was 65 \u00b1 5%","05316d64":"## Visualising the training over time","c6bfeb30":"## CNN Model"}}