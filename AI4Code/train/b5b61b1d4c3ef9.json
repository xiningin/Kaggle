{"cell_type":{"2649ed61":"code","4513ddec":"code","ebe03c69":"code","9ae05b1f":"code","e8bc2eab":"code","256760f1":"code","43211083":"code","eb814f02":"code","d130a2a0":"code","d73d34f5":"code","d12c50e7":"code","3c9d8db6":"code","b00cf8a2":"code","ed31e976":"code","9b93f846":"code","cc0803b8":"code","9a4a340a":"code","af5c9ae8":"code","1e849c13":"code","cebacb71":"code","91c3ed9a":"code","68b313e0":"code","c6e16ea5":"code","9e77cb4b":"code","abdedaef":"markdown","3e006fc8":"markdown","c0e75732":"markdown","068799d2":"markdown","9c0fb9cf":"markdown","a30ecb0e":"markdown","c1ed35c2":"markdown","96da517a":"markdown","12baf6b6":"markdown","8f3447a4":"markdown","af4aadc1":"markdown","85a1fa28":"markdown","c349e1b3":"markdown","0d323526":"markdown","faca472b":"markdown","697d5b07":"markdown","94ea3bd2":"markdown","6a6c3e9b":"markdown","7920522f":"markdown","63d72011":"markdown","214e5077":"markdown","6ff6bfad":"markdown"},"source":{"2649ed61":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4513ddec":"import numpy as np\nimport os\nimport cv2\nimport pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","ebe03c69":"labels = ['bad', 'good']\nimg_size = 100\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img),cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","9ae05b1f":"train = get_training_data('..\/input\/cubesat\/training_dataset_v3')\ntest = get_training_data('..\/input\/cubesat\/test_dataset_v5')","e8bc2eab":"bad = 0 \ngood = 0 \n\nfor i, j in train:\n    if j == 0:\n        bad+=1\n    else:\n        good+=1\n        \nprint('Bad:', bad)\nprint('good:', good)\nprint('Bad - Good:', bad-good)\n","256760f1":"X = []\ny = []\n\nfor feature, label in train:\n    X.append(feature)\n    y.append(label)\n\nfor feature, label in test:\n    X.append(feature)\n    y.append(label)\n    \n\n\n# resize data for deep learning \nX = np.array(X).reshape(-1, img_size, img_size, 1)\ny = np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=32)","43211083":"X_train = X_train.reshape(X_train.shape[0],-1).T\nX_test = X_test.reshape(X_test.shape[0],-1).T\nX_train = X_train \/ 255\nX_test = X_test \/ 255\nX_val = X_val \/ 255\ny_train = np.array(y_train).reshape(-1,1)\ny_train = y_train.T\ny_test = np.array(y_test).reshape(-1,1)\ny_test = y_test.T","eb814f02":"def sigmoid(Z):\n    \n    A = 1\/(1+np.exp(-Z))\n    cache = Z\n    \n    return A, cache","d130a2a0":"def relu(Z):\n    \n    A = np.maximum(0,Z)\n    \n    assert(A.shape == Z.shape)\n    \n    cache = Z \n    return A, cache","d73d34f5":"def relu_backward(dA, cache):\n\n    \n    Z = cache\n    dZ = np.array(dA, copy=True)\n    \n    dZ[Z <= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ","d12c50e7":"def sigmoid_backward(dA, cache):\n\n    \n    Z = cache\n    \n    s = 1\/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ","3c9d8db6":"### CONSTANTS DEFINING THE MODEL ####\nn_x = 10000     # num_px * num_px * 1\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)","b00cf8a2":"def initialize_parameters(layer_dims):\n\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n\n    for l in range(1, L):\n        \n        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n        \n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters","ed31e976":"def linear_forward(A, W, b):\n\n    Z = np.dot(W,A)+b\n    \n    \n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache","9b93f846":"def linear_activation_forward(A_prev, W, b, activation):\n\n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        \n        Z, linear_cache = linear_forward(A_prev,W,b)\n        A, activation_cache = sigmoid(Z)\n        \n    \n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        \n        Z, linear_cache = linear_forward(A_prev,W,b)\n        A, activation_cache = relu(Z)\n        \n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache","cc0803b8":"def compute_cost(AL, Y):\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    \n    cost = -1\/m * (np.sum(Y*np.log(AL)+((1-Y)*np.log(1-AL))))\n    \n    \n    cost = np.squeeze(cost)     \n    assert(cost.shape == ())\n    \n    return cost","9a4a340a":"def linear_backward(dZ, cache):\n\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    \n    dW = 1\/m *(np.dot(dZ,A_prev.T))\n    db = 1\/m * np.sum(dZ,axis = 1,keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    \n    \n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","af5c9ae8":"def linear_activation_backward(dA, cache, activation):\n\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        \n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = (linear_backward(dZ,linear_cache))\n        \n        \n    elif activation == \"sigmoid\":\n        \n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = (linear_backward(dZ,linear_cache))\n        \n    \n    return dA_prev, dW, db","1e849c13":"def update_parameters(parameters, grads, learning_rate):\n\n    \n    L = len(parameters) \/\/ 2 # number of layers in the neural network\n\n    # Update rule for each parameter. Use a for loop.\n    \n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]- learning_rate*grads['dW'+str(l+1)]\n        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)] - learning_rate*grads['db'+str(l+1)]\n    \n    return parameters","cebacb71":"def L_model_forward(X, parameters):\n\n    caches = []\n    A = X\n    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n        caches.append(cache)\n    \n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n    caches.append(cache)\n    \n    assert(AL.shape == (1,X.shape[1]))\n            \n    return AL, caches\n    ","91c3ed9a":"def predict(X, y, parameters):\n\n    \n    m = X.shape[1]\n    n = len(parameters) \/\/ 2 # number of layers in the neural network\n    p = np.zeros((1, m),dtype=int)\n    \n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n\n\n    # convert probas to 0\/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    #print results\n    #print (\"predictions: \" + str(p))\n    #print (\"true labels: \" + str(y))\n    acc = np.sum(p == y)\/float(m)\n        \n    return acc","68b313e0":"def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost=False):\n\n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    \n    parameters = initialize_parameters(layers_dims)\n    \n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n        \n        #minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n        A2, cache2 = linear_activation_forward(A1,W2,b2,'sigmoid')\n        \n        \n        # Compute cost\n\n        cost = compute_cost(A2, Y)\n        \n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n\n        \n        dA1, dW2, db2 =  linear_activation_backward(dA2, cache2, activation='sigmoid')\n        dA0, dW1, db1 =  linear_activation_backward(dA1, cache1, activation='relu')\n        \n\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n       \n    # plot the cost\n\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","c6e16ea5":"parameters = two_layer_model(X_train, y_train, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\npredictions_train = print(\"train accuracy: {} %\".format(predict(X_train, y_train, parameters) * 100))\npredictions_train = print(\"test accuracy: {} %\".format(predict(X_test, y_test, parameters) * 100))","9e77cb4b":"my_label_y = 0\ntest_image1 = cv2.imread('..\/input\/cubesat\/test_dataset_v5\/good\/0.jpg')\ntest_image = cv2.imread('..\/input\/cubesat\/test_dataset_v5\/good\/14.jpg',cv2.IMREAD_GRAYSCALE)\nresized_arr1 = cv2.resize(test_image, (100, 100))\nresized_arr1 = np.array(resized_arr1).reshape(-1, img_size, img_size, 1)\nresized_arr1 = resized_arr1.reshape(resized_arr1.shape[0],-1).T\nresized_arr1 = resized_arr1\/255\nmy_predicted_image = predict(resized_arr1, my_label_y, parameters)\n\nplt.imshow(test_image1)\nprint (\"ACCURACY IS= \" + str(np.squeeze(my_predicted_image)*100 )+\"%\" )\nif my_predicted_image>0.5:\n    print(\"There is Good Image\")\nelse:\n    print(\"The Image is Bad Image\")\n\n","abdedaef":"**A model For L Layer Deep Neural Network**","3e006fc8":"# **Back Propagation**","c0e75732":"## **Defining the Layer of The Neural Network**\n<img src = \"https:\/\/i.imgur.com\/UX6M1zX.png\" width = 800>","068799d2":"<img src = \"https:\/\/miro.medium.com\/max\/1164\/1*PK0iVgkQepmVCprtTgbsGg.png\">","9c0fb9cf":"# **Import Libraries**","a30ecb0e":"# **Testing Model**","c1ed35c2":"<img src = \"https:\/\/www.researchgate.net\/profile\/Kipp-Johnson\/publication\/331439931\/figure\/fig2\/AS:733179287728129@1551814972455\/mpact-of-deep-learning-design-on-learning-effect-of-learning-rate-A-Efficient.png\" width = \"500\">","96da517a":"# **Activation Functions**","12baf6b6":"# **Data Reshaping and Normalization**","8f3447a4":"# **Predict Function**","af4aadc1":"# **Sigmoid**\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/53\/Sigmoid-function-2.svg\/1280px-Sigmoid-function-2.svg.png\" width=\"400\">","85a1fa28":"# **Updating the values of Weights and Bias**","c349e1b3":"# **Cost Calculation**","0d323526":"# **Fordward Propagation**","faca472b":"# **Relu**\n<img src = \"https:\/\/sebastianraschka.com\/images\/faq\/relu-derivative\/relu_3.png\" width=\"400\">","697d5b07":"# **Implementation of Deep Learning Neural Network Model**\n<img src = \"https:\/\/i.imgur.com\/cizhNhg.jpeg\" width=\"700\">","94ea3bd2":"# **Data PreProcessing**","6a6c3e9b":"# **Finalizing The Model**","7920522f":"# **Backward Activation Function**","63d72011":"# **Initialize Weights and Bias**","214e5077":"# **Data Distribution**","6ff6bfad":"# **Training Model**"}}