{"cell_type":{"fa2c0c4d":"code","5db3f90a":"code","fea3ee62":"code","fe090c57":"code","082324e7":"code","f675244d":"code","8d43becb":"code","7271654e":"code","aaeb9071":"code","bae60564":"code","75cb430f":"code","bb560280":"code","3edf6c1a":"code","a6f80043":"code","ee3e66c9":"code","61c757ba":"code","44d4e4cf":"code","d8afeaa0":"code","5e484ac9":"code","50b92d33":"code","56ed965a":"code","e28005c6":"markdown","2687d293":"markdown","9b8e7a4d":"markdown","8b16b991":"markdown","66e17681":"markdown"},"source":{"fa2c0c4d":"!pip install timm","5db3f90a":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import KFold\n\nimport timm\nimport torchvision.transforms as T\nfrom torchvision.io import read_image\nimport kornia as K\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimport pytorch_lightning as pl\nfrom transformers import get_cosine_schedule_with_warmup","fea3ee62":"train_image_root_path = \"..\/input\/petfinder-pawpularity-score\/train\"\ntest_image_root_path = \"..\/input\/petfinder-pawpularity-score\/test\"","fe090c57":"train_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","082324e7":"train_df.shape, test_df.shape","f675244d":"train_df.head()","8d43becb":"test_df.head()","7271654e":"embedding_features = test_df.columns[1:]","aaeb9071":"for feature in embedding_features:\n    print(f\"The number of {feature} is\", set(train_df[feature]))","bae60564":"labels = train_df[\"Pawpularity\"].values\nmax(labels), min(labels)","75cb430f":"len(list(set(labels)))","bb560280":"skf = KFold(n_splits=5)\ntrain_idx_list = []\nvalid_idx_list = []\nfor train_idx, valid_idx in skf.split(train_df[embedding_features], train_df[\"Pawpularity\"]):\n    train_idx_list.append(train_idx)\n    valid_idx_list.append(valid_idx)","3edf6c1a":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n                T.RandomResizedCrop((224, 224), scale=(0.7, 1.1)),\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.Resize((224, 224)),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform\n\ntransform = get_default_transforms()","a6f80043":"class PetFinderDataset(Dataset):\n    def __init__(self, df, is_train, transform=None):\n        self.df = df\n        self.is_train = is_train\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        features = torch.tensor(row[embedding_features], dtype=torch.long)\n        img_name = row[\"Id\"]\n        img = read_image(os.path.join(train_image_root_path, img_name + \".jpg\"))\n        \n        if self.transform:\n            data = self.transform(img)\n                \n        if self.is_train:\n            label = torch.tensor(row[\"Pawpularity\"], dtype=torch.float)\n            return data, features, label\n        else:\n            return data, features","ee3e66c9":"# train_dataset = PetFinderDataset(train_df, is_train=True, transform = transform[\"train\"])\n# test_dataset = PetFinderDataset(test_df, is_train=False, transform = transform[\"val\"])","61c757ba":"len(embedding_features)","44d4e4cf":"train_df[embedding_features].iloc[0].values","d8afeaa0":"class FeatureEmbedding(nn.Module):\n    def __init__(self, len_embedding_features, embedding_dim = 128):\n        super().__init__()\n        self.len_embedding_features = len_embedding_features\n        self.embedding_models = nn.ModuleList([nn.Embedding(2, embedding_dim) for _ in range(len_embedding_features)])\n        \n    def forward(self, x):\n        y_list = []\n        for i in range(self.len_embedding_features):\n            y_list.append(self.embedding_models[i](x[:, i]))\n        return torch.cat(y_list, dim=-1)\n    \ndef test_FeatureEmbedding():\n    x = torch.tensor(train_df[embedding_features].iloc[0].values, dtype=torch.long)\n    feature_embedding = FeatureEmbedding(x.shape[-1], 16)\n    y = feature_embedding(x.unsqueeze(dim=0))\n    print(y.shape)\n    \n# test_FeatureEmbedding()","5e484ac9":"class ImageEmbedding(nn.Module):\n    def __init__(self, model_name, embedding_dim=1024):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0, in_chans=3)\n        num_features = self.backbone.num_features\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, embedding_dim)\n        )\n        \n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n    \ndef test_ImageEmbedding():\n    img = torch.rand(1, 3, 224, 224)\n    model = ImageEmbedding(\"resnet34\", embedding_dim=32)\n    y = model(img)\n    print(y.shape)\n    \n# test_ImageEmbedding()","50b92d33":"class PetModel(nn.Module):\n    def __init__(self, len_embedding_featutre, model_name, embedding_dim=128):\n        super().__init__()\n        self.feature_embedding = FeatureEmbedding(len_embedding_featutre, embedding_dim)\n        \n        img_embedding_dim = len_embedding_featutre * embedding_dim \/\/ 2\n        self.image_embedding = ImageEmbedding(model_name, img_embedding_dim)\n        \n        embedding_dim = img_embedding_dim + len_embedding_featutre * embedding_dim\n        self.out_layers = nn.Sequential(\n            nn.Linear(embedding_dim, 1024),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.Dropout(0.2),\n            nn.Linear(512, 1)\n        )\n        \n    def forward(self, feature_input, img_input):\n        feature_output = self.feature_embedding(feature_input)\n        img_output = self.image_embedding(img_input)\n        embeddings = torch.cat((feature_output, img_output), dim=-1)\n        out = self.out_layers(embeddings)\n        return 99.1 * torch.sigmoid(out.reshape(-1)) + 0.95 # (0.95, 100.05)\n    \ndef test_PetModel():\n    x = torch.tensor(train_df[embedding_features].iloc[0].values, dtype=torch.long)\n    img = torch.rand(1, 3, 224, 224)\n    model = PetModel(x.shape[-1], \"resnet34\", embedding_dim=32)\n    y = model(x.unsqueeze(dim=0), img)\n    print(y.shape)\n    \n# test_PetModel()","56ed965a":"grad_clip = 1.0\nnum_epochs = 30\nbatch_size = 32\nlr = 1e-2\nwd = 1e-3\nembedding_dim = 16\nmodel_name = \"resnet50\"\nlen_embedding_featutre = len(embedding_features)\n\n\nfor fold, (train_idx, valid_idx) in enumerate(zip(train_idx_list, valid_idx_list)):\n    train_idx, valid_idx = train_idx_list[fold], valid_idx_list[fold]\n    print(len(train_idx), len(valid_idx))\n    split_train_df, split_valid_df = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    train_dataset = PetFinderDataset(split_train_df, is_train=True, transform = transform[\"train\"])\n    valid_dataset = PetFinderDataset(split_valid_df, is_train=True, transform = transform[\"val\"])\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, \n                                  shuffle=True, pin_memory=True, num_workers=2)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=2*batch_size, \n                                  shuffle=False, pin_memory=True, num_workers=2)\n    \n    model = PetModel(len_embedding_featutre, model_name, embedding_dim)\n    device = torch.device(0)\n    model.to(device)\n    \n    loss_fn = nn.MSELoss()\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": wd,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=lr)\n    num_training_steps = num_epochs * len(train_dataloader)\n    num_warmup_steps = int(0.1 * num_training_steps)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, \n                                                num_warmup_steps=num_warmup_steps, \n                                                num_training_steps=num_training_steps)\n    \n    best_valid_score = 1e5\n    for epoch in range(num_epochs):\n        num_train_steps = 0.\n        loss_train = 0.\n        model.train()\n        train_bar = tqdm(train_dataloader)\n        for batch in train_bar:\n            data, features, label = batch\n            data, features, label = data.to(device), features.to(device), label.to(device)\n            y_hat = model(features, data)\n            loss = loss_fn(y_hat, label)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n            optimizer.step()\n            scheduler.step()\n            \n            num_train_steps += data.shape[0]\n            loss_train += loss.detach().cpu().numpy() * data.shape[0]\n            \n            train_bar.set_description_str(\"[Train: EPOCH %d]\" % epoch)\n            train_bar.set_postfix_str(\"RMSE: %.4f, lr: %.6F\" % (np.sqrt(loss_train\/num_train_steps), \n                                                                scheduler.get_lr()[0]))\n        \n        num_valid_steps = 0.\n        loss_valid = 0.\n        model.eval()\n        valid_bar = tqdm(valid_dataloader)\n        for batch in valid_bar:\n            data, features, label = batch\n            data, features, label = data.to(device), features.to(device), label.to(device)\n            with torch.no_grad():\n                y_hat = model(features, data)\n                loss = loss_fn(y_hat, label)\n            \n            num_valid_steps += data.shape[0]\n            loss_valid += loss.detach().cpu().numpy() * data.shape[0]\n            \n            valid_bar.set_description_str(\"[Valid: EPOCH %d]\" % epoch)\n            valid_bar.set_postfix_str(\"RMSE: %.4f\" % np.sqrt(loss_valid\/num_valid_steps))\n            \n        valid_loss = np.sqrt(loss_valid\/num_valid_steps)\n        if best_valid_score > valid_loss:\n            best_valid_score = valid_loss\n            torch.save(model.state_dict(), \"model%d.pth\"%fold)\n            \n        gc.collect()\n        \n#     break","e28005c6":"# 2 Baseline","2687d293":"The label is in the range [1~100].\n\nwe can set the problem to a 100-classes classification problem or a [1~100] regression problem.","9b8e7a4d":"## 2.1 Dataset and DataLoader","8b16b991":"# 1 EDA","66e17681":"## 2.2 Model"}}