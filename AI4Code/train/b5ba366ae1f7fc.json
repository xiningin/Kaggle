{"cell_type":{"7c53fd48":"code","2868d680":"code","0eb365d9":"code","9bf811d4":"code","8072d079":"code","5c3b0499":"code","14257480":"code","5b52bd28":"code","e84ed0e3":"code","b9cc1060":"code","253024e1":"code","ffcbe920":"code","b37e6d5c":"code","435d0b55":"code","b5b3da42":"code","481ba7e3":"code","f8767c83":"code","01806504":"code","1f1468f7":"code","a1fcf440":"code","064cfb3f":"code","ecd25b4d":"code","fa073822":"code","dc54d530":"code","2dda20f8":"code","363e4914":"code","2957621b":"code","f597d7cd":"code","ad803c50":"code","b09c3978":"code","715eb860":"code","aa133353":"code","734b82b5":"code","b868bbf9":"code","4eb18bdb":"code","a69fde45":"code","01d95d4d":"code","abbc2b69":"code","9627db1b":"code","dd55daaf":"code","d180b611":"code","c4ca7d1c":"code","c25065c1":"code","2e66e980":"code","8c0a125c":"code","efeb602f":"code","8d9e85c8":"code","f6bbf67d":"code","aa9aa855":"code","f7bbd6d6":"code","79bbd705":"code","93c3450b":"code","51035d08":"code","b40e4dfe":"code","8804ed2c":"code","1d459ac4":"code","b1bbadf5":"code","da5514a3":"code","288ba227":"code","79bbfb12":"code","aeccaeda":"code","ac58fcc7":"code","9d678277":"code","d475b504":"code","fa2aa8bb":"code","8da0517c":"code","8750a34a":"code","ec35c28b":"code","6295d513":"code","941738e3":"code","52c5d800":"code","1c6fb569":"code","47c77549":"code","df9d6106":"code","4af44145":"code","7e5845c2":"code","7e295d24":"markdown","638b6d11":"markdown","d58cd27e":"markdown","3f897f61":"markdown","4c1fcccf":"markdown","a2f387d2":"markdown","37841933":"markdown","faabeb05":"markdown","b80f4797":"markdown","6f14aa0a":"markdown","60d28bad":"markdown","fddb3551":"markdown","267db7d2":"markdown","1addb615":"markdown","6b6d3a34":"markdown","18fdf5aa":"markdown","a456a78d":"markdown","73de6178":"markdown","5702b0b2":"markdown","3ccfd485":"markdown","3965ac7a":"markdown","c737c27a":"markdown","c68429d8":"markdown","68f181bc":"markdown","333e18b9":"markdown","8730f9b9":"markdown","b408320d":"markdown","7418279f":"markdown","247fe14d":"markdown","994b6700":"markdown","61fe98d7":"markdown"},"source":{"7c53fd48":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV","2868d680":"# Kaggle reading data sets.\n\ntrain_or = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_or = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n","0eb365d9":"train.head()","9bf811d4":"test.head()","8072d079":"train.shape # checking shape \ntest.shape","5c3b0499":"test.isna().sum() # check null values is not enough, not all the columns appeared.\ntrain.isna().sum() # check null values is not enough, not all the columns appeared.","14257480":"# Another way of visualize null values.\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 10))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","5b52bd28":"train.info() \ntest.info()\n# Examination of data description indicates some features which contain categorical values \n# These were displayed as object in the datasets. \n# Therfore, certain features were converted into categorical data type \n","e84ed0e3":"train['LotFrontage'].isna().sum() # check null values in LotFrontage columns \n# 259 is a high number, we need to replace the null with a good value that not affect the model.","b9cc1060":"# Using the following function to replace the null values with ava with its Neighborhood.\nlist_a={}\nfor i in sorted(train['Neighborhood'].unique()): # for each Neighborhood.\n# save it as dictionary type and use it for the following questions.\n    list_a[i]= np.mean(train[train['Neighborhood']==i]['LotFrontage']) # Calculate the LotFrontage average for each Neighborhood.\nlist_a\n\ndef impute_lotFrontage(lotFrontage_Neighborhood): # train or test\n    # for each Neighborhood, filling missing value in lotFrontage with it's ava.\n    for i in train['Neighborhood']: # Neighborhood\n        x_mean= list_a[i] # get ava.\n        # update and change null values with ava.        \n        lotFrontage_Neighborhood.loc[(lotFrontage_Neighborhood['Neighborhood']==i) & (lotFrontage_Neighborhood['LotFrontage'].isna()),'LotFrontage']= list_a[i]\n        ","253024e1":"# call the function to update train and test.\nimpute_lotFrontage(train)\nimpute_lotFrontage(test)\ntrain['LotFrontage'].isna().sum() # check\ntest['LotFrontage'].isna().sum() # check","ffcbe920":"test.GarageYrBlt.unique() \n# we noticed 'None' value which can not be replaced by 0.0 as float value\n# we changed the data type for GarageYrBlt column from float to object to replace it with 'None' as string","b37e6d5c":"test['GarageArea'].isna().sum() \ntest.loc[test['GarageArea'].isna()][['GarageArea', 'GarageYrBlt', 'GarageFinish']] # getting rows with null in GarageArea\n# Checking values for GarageYrBlt and GarageFinish ","435d0b55":"test.loc[test['GarageArea'] == 0.0][['GarageArea', 'GarageYrBlt', 'GarageFinish']]\n# getting rows with 0.0 in GarageArea\n# Checking values for GarageYrBlt and GarageFinish ","b5b3da42":"test.GarageArea.value_counts() # float, 0 , highest\ntest.GarageArea.isna().sum() # there are null values in test.\ntest['GarageArea'] = test['GarageArea'].fillna(0)  #replace it.\ntest.GarageArea.isna().sum() # to check. ","481ba7e3":"# chaning data type from float to object\ntrain['GarageYrBlt'] = train['GarageYrBlt'].astype(object)\ntest['GarageYrBlt'] = test['GarageYrBlt'].astype(object) ","f8767c83":"test['GarageYrBlt'] = test['GarageYrBlt'].fillna('None')  # replace with 'None'\ntest.GarageYrBlt.isna().sum() # checking null","01806504":"train['GarageYrBlt'] =train['GarageYrBlt'].fillna('None') \ntrain.GarageYrBlt.isna().sum() # checking null ","1f1468f7":"test.Exterior1st.value_counts() # get the highest value to replace it with nan. (VinylSd    510).\ntest.Exterior1st.isna().sum()  # get the number of null value.\ntest['Exterior1st'] = test['Exterior1st'].fillna('VinylSd')  #replace it.\ntest.Exterior1st.value_counts() # to check. (VinylSd    511)","a1fcf440":"# performing the same to Exterior2nd.\ntest.Exterior2nd.value_counts() # get the highest value to replace it with nan. (VinylSd    510).\ntest.Exterior2nd.isna().sum()  # get the number of null value.\ntest['Exterior2nd'] = test['Exterior2nd'].fillna('VinylSd')  #replace it.\ntest.Exterior2nd.value_counts() # to check. (VinylSd    511)\n\n","064cfb3f":"train.Electrical.value_counts() # get the highest value to replace it with nan. (SBrkr    1334).\ntrain.Electrical.isna().sum()  # get the number of null value.\ntrain['Electrical'] = train['Electrical'].fillna('SBrkr')  #replace it.\ntrain.Electrical.value_counts() # to check. (SBrkr    1335)","ecd25b4d":"test.MSZoning.value_counts() # get the highest value to replace it with nan. (RL         1114).\ntest.MSZoning.isna().sum()  # get the number of null value.\ntest['MSZoning'] = test['MSZoning'].fillna('RL')  #replace it.\ntest.MSZoning.value_counts() # to check. (RL         1118)\n\n","fa073822":"test.KitchenQual.value_counts() # get the highest value to replace it with nan. (TA    757).\ntest.KitchenQual.isna().sum()  # get the number of null value.\ntest['KitchenQual'] = test['KitchenQual'].fillna('TA')  #replace it.\ntest.KitchenQual.value_counts() # to check. (TA    758)\n","dc54d530":"test.Functional.value_counts() # get the highest value to replace it with nan. (Typ     1357).\ntest.Functional.isna().sum()  # get the number of null value.\ntest['Functional'] = test['Functional'].fillna('Typ')  #replace it.\ntest.Functional.value_counts() # to check. (Typ     1359)","2dda20f8":"test['BsmtUnfSF'].skew() # postive skew, it is not normal distribution","363e4914":"test.BsmtUnfSF.value_counts() # highest is (0.0     124)\ntest.BsmtUnfSF.isna().sum() # there is a null value in test.\ntest['BsmtUnfSF'] = test['BsmtUnfSF'].fillna(0.0)  #replace it.\ntest.BsmtUnfSF.isna().sum() # to check.","2957621b":"test['MasVnrArea'].skew() # positive skew, it is not normal distribution","f597d7cd":"train['MasVnrArea'].skew() # positive skew, it is not normal distribution","ad803c50":"train.MasVnrArea.value_counts() # float, 0 , highest\ntrain.MasVnrArea.unique()\ntrain.MasVnrArea.isna().sum() # there are null values in train, it should be replaced with 0 as a highest value.\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(0)  #replace it.\ntrain.MasVnrArea.isna().sum() # to check.\n\n\ntest.MasVnrArea.value_counts() # float, 0 , highest\ntest.MasVnrArea.isna().sum() # there are null values in test.\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)  #replace it.\ntest.MasVnrArea.isna().sum() # to check.\n","b09c3978":"test['TotalBsmtSF'].skew() # postive skew, it is not normal distribution","715eb860":"test.TotalBsmtSF.value_counts() # highest is (0.0     41)\ntest.TotalBsmtSF.isna().sum() # there is a null value in test.\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].fillna(0.0)  #replace it.\ntest.TotalBsmtSF.isna().sum() # to check.","aa133353":"test['GarageCars'].skew() # negative skew, it is not normal distribution","734b82b5":"test.GarageCars.value_counts() # highest is (2.0    770)\ntest.GarageCars.isna().sum() # there is a null value in test.\ntest['GarageCars'] = test['GarageCars'].fillna(2.0)  #replace it.\ntest.GarageCars.isna().sum() # to check.","b868bbf9":"test['BsmtHalfBath'].skew() # positive skew, it is not normal distribution ","4eb18bdb":"test.BsmtHalfBath.value_counts() # float, 0 , highest\ntest.BsmtHalfBath.isna().sum() # there are null values in test.\ntest['BsmtHalfBath'] = test['BsmtHalfBath'].fillna(0)  #replace it.\ntest.BsmtHalfBath.isna().sum() # to check. ","a69fde45":"train['BsmtFinSF1'].skew() # positive skew, it is not normal distribution ","01d95d4d":"test.BsmtFinSF1.value_counts() # # int, 0 , highest\ntest.BsmtFinSF1.isna().sum() # there is a null value in test.\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].fillna(0)  #replace it.\ntest.BsmtFinSF1.isna().sum() # to check.","abbc2b69":"test['BsmtFullBath'].skew() # positive skew, it is not normal distribution ","9627db1b":"test.BsmtFullBath.value_counts() # float, 0 , highest\ntest.BsmtFullBath.isna().sum() # there are null values in test.\ntest['BsmtFullBath'] = test['BsmtFullBath'].fillna(0)  #replace it.\ntest.BsmtFullBath.isna().sum() # to check.","dd55daaf":"test['BsmtFinSF2'].skew() # positive skew, it is not normal distribution ","d180b611":"test.BsmtFinSF2.value_counts() # # int, 0 , highest\ntest.BsmtFinSF2.isna().sum() # there is a null value in test.\ntest['BsmtFinSF2'] = test['BsmtFinSF2'].fillna(0)  #replace it.\ntest.BsmtFinSF2.isna().sum() # to check.","c4ca7d1c":"test.MasVnrType.value_counts()  # get the highest value to replace it with nan. (None       878).\ntest.MasVnrType.isna().sum()  # get the number of null value.\ntest['MasVnrType'] = test['MasVnrType'].fillna('None')  #replace it.\ntest.MasVnrType.value_counts() # to check. (None       894)","c25065c1":"test.SaleType.value_counts()  # get the highest value to replace it with nan. (WD       1258).\ntest.SaleType.isna().sum()  # get the number of null value.\ntest['SaleType'] = test['SaleType'].fillna('WD')  #replace it.\ntest.SaleType.value_counts() # to check. (WD       1259)","2e66e980":"# change null values with none exist. (Catogrical values)\n\ntrain['GarageType'] = train['GarageType'].fillna('None') \ntest['GarageType'] = test['GarageType'].fillna('None') \n\ntest['GarageFinish'] = test['GarageFinish'].fillna('None') \ntrain['GarageFinish'] = train['GarageFinish'].fillna('None') \n\ntrain['MasVnrType'] = train['MasVnrType'].fillna('None') \ntest['MasVnrType'] = test['MasVnrType'].fillna('None') \n\ntest['GarageQual'] = test['GarageQual'].fillna('None')\ntrain['GarageQual'] = train['GarageQual'].fillna('None') \n\ntrain['GarageCond'] = train['GarageCond'].fillna('None')\ntest['GarageCond'] = test['GarageCond'].fillna('None') \n\n\n#-----------------------\n\ntrain['PoolQC'] = train['PoolQC'].fillna('None') \ntest['PoolQC'] = test['PoolQC'].fillna('None') \n\ntrain['MiscFeature'] = train['MiscFeature'].fillna('None') \ntest['MiscFeature'] = test['MiscFeature'].fillna('None') \n\ntrain['Alley'] = train['Alley'].fillna('None') \ntest['Alley'] = test['Alley'].fillna('None') \n\ntrain['BsmtCond'] = train['BsmtCond'].fillna('None') \ntest['BsmtCond'] = test['BsmtCond'].fillna('None') \n\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna('None') \ntest['BsmtExposure'] = test['BsmtExposure'].fillna('None') \n\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna('None') \ntest['BsmtFinType1'] = test['BsmtFinType1'].fillna('None') \n\n\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna('None') \ntest['BsmtFinType2'] = test['BsmtFinType2'].fillna('None') \n\ntrain['BsmtQual'] = train['BsmtQual'].fillna('None') \ntest['BsmtQual'] = test['BsmtQual'].fillna('None') \n\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna('None') \ntest['FireplaceQu'] = test['FireplaceQu'].fillna('None') \n\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna('None') \ntest['FireplaceQu'] = test['FireplaceQu'].fillna('None') \n\ntrain['Fence'] = train['Fence'].fillna('None') \ntest['Fence'] = test['Fence'].fillna('None') ","8c0a125c":"# most the values are same except null values which are few values compared to non-null values.\ntrain.Street.value_counts()      # (Pave    1454, Grvl       6)\ntrain.Utilities.value_counts()   # (AllPub    1459, NoSeWa       1)\n\ntrain = train.drop([\"Street\", \"Utilities\"], axis=1)\n\n# Removing columns from train requires removing the same columns from test. \ntest = test.drop([\"Street\", \"Utilities\"], axis=1)","efeb602f":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","8d9e85c8":"train.describe()","f6bbf67d":"test.describe()","aa9aa855":"train['SalePrice'].describe() # obtaining statistical measures for the target column\n# we noticed that there are at least one outlier at min and max levels","f7bbd6d6":"sns.boxplot(train['SalePrice']) # there are outliers, however, it will not affect the prediction since it will be used as target","79bbd705":"sns.distplot(train['SalePrice']) # checkin distrubution of target column. It appears that it is not a normal distribution","93c3450b":"# Positive Skeweness:\ntrain['SalePrice'].skew()","51035d08":"# plotting heatmap to identify correlations\nfig, ax = plt.subplots(figsize = (16,16))\nsns.heatmap(train.corr(), cmap = 'coolwarm')\nax.set_title('Correlation Between Features')","b40e4dfe":"# checking degree of correlation between SalePrice, OverallQual and GrLivArea  \ntrain[['SalePrice','OverallQual','GrLivArea']].corr()","8804ed2c":"# Visualizing OverallQual vs SalePrice \nplt.figure(figsize=(8, 5))\nsns.scatterplot(train[\"OverallQual\"], train[\"SalePrice\"])\nplt.title(\"OverallQual vs SalePrice\")","1d459ac4":"# Visualizing dsitrbution of OverallQual\nsns.distplot(train['OverallQual'])","b1bbadf5":"# plotting histogram for OverallQual\ntrain.OverallQual.hist();","da5514a3":"train['OverallQual'].skew() # checking skewness","288ba227":"# Visualization of GrLivArea vs SalePrice\nplt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\nsns.scatterplot(train[\"GrLivArea\"], train[\"SalePrice\"])\nplt.title(\"GrLivArea vs SalePrice\")","79bbfb12":"# We noticed that outliers could be removed using the plot\n# if we could not get a good prediction score without removing outliers,\n#this method could be applied to remove outliers \ntrain_clean= train.drop(train[(train['GrLivArea']>4000)].index) # drop outliers from GrLivArea column.","aeccaeda":"# plotting GrLivArea vs SalePrice without outliers\nplt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\nsns.scatterplot(train_clean[\"GrLivArea\"], train_clean[\"SalePrice\"])\nplt.title(\"GrLivArea vs SalePrice\");","ac58fcc7":"def var_standardized(var):\n    var_stand = (var-var.mean())\/var.std()\n    return var_stand\n\nbox_plots = var_standardized(\n    train.select_dtypes(exclude=['object']).drop(['Id'], axis=1)) # only numeric columns and drop year. \n\nfig,ax=plt.subplots(figsize=(20,20)) # create figure with specific size  \nsns.boxplot(data=box_plots, orient='h', fliersize=5, \n                 linewidth=3, notch=True, saturation=0.5, ax=ax) # plot \nplt.title('This plot is from standardized merged data frame'); # title.","9d678277":"train_dum = pd.get_dummies(train,drop_first=True) # converting categorical values into dummies  \n\ntest_dum = pd.get_dummies(test, drop_first=True) # converting categorical values into dummies  \n\ntest_dum.info() # test columns =!  train columns\n\nprint(train_dum.shape) # since columns are not equal, will have to concatinate then split by id to match kaggle submission file\nprint(test_dum.shape)","d475b504":"# Since above dummies result in different number of columns between train and test, \n# concatinating without Id column can be a solution\n# dropping Id column\ntrain = train.drop(\"Id\", axis=1)\ntest = test.drop(\"Id\", axis=1)\n# concatinating\nconcat_1 = pd.concat((train.loc[:,:], test.loc[:, :])).reset_index(drop=True)\nconcat_1 = concat_1.drop([\"SalePrice\"], axis = 1) # to remove null values in concat_1['SalePrice'] which resulted from concatination\nconcat_1.tail() # check","fa2aa8bb":"concat_1.shape # checking length of concat_1 = len(test) + len(train)","8da0517c":"concat_dum = pd.get_dummies(concat_1) # applying dummies to concat_1","8750a34a":"concat_dum # displaying to check","ec35c28b":"# spliting after getting dumies to match kaggle submission file\nX_train_out = concat_1.iloc[:len(train),:]\nX_test_out = concat_1.iloc[len(X_train_out):,:]\nprint(len(X_test_out) == len(test)) # check\nprint(len(X_train_out) == len(train)) # check","6295d513":"print(X_train_out.shape) # checking shape\nprint(X_test_out.shape)","941738e3":"# initializing scaler for needed model\nscaler = StandardScaler()","52c5d800":"# logistic reg with the all columns.\n# only numeric columns.\n# without removing outliers.\n\nscaler = StandardScaler() #initializing scaler\ny = train['SalePrice'] # get target\nXss = scaler.fit_transform(X_train_out.select_dtypes(exclude=['object'])) # only numeric columns.\ntest_ss = scaler.transform(X_test_out.select_dtypes(exclude=['object'])) # only numeric columns.\n\nlogreg = LogisticRegression() #initializing logreg ()\nlogreg.fit(Xss, y)\nprint('train score: ',logreg.score(Xss,y)) \n\n# Kaggle Submission \n\npred = logreg.predict(test_ss)\ntest_or['SalePrice']= pred\n# # Save the prediction into a file that matches the Kaggle submission requirements\ntest_or[['Id', 'SalePrice']].to_csv('submission_logReg_outliers.csv',index=False)","1c6fb569":"# logistic reg with lasso.\n# logistic reg with the all columns.\n# only numeric columns.\n# without removing outlers.\n\nscaler = StandardScaler() #initializing scaler\ny = train['SalePrice'] \nXss = scaler.fit_transform(X_train_out.select_dtypes(exclude=['object'])) \ntest_ss = scaler.transform(X_test_out.select_dtypes(exclude=['object']))\n\nlogreg = LogisticRegression(penalty='l2') # using lasso results in the same score as above (0.22758) because logreg applies lasso by default \nlogreg.fit(Xss, y)\nprint('train score: ',logreg.score(Xss,y)) \n\n# Kaggle Submission \n\npred = logreg.predict(test_ss)\ntest_or['SalePrice']= pred\n# # Save the prediction into a file that matches the Kaggle submission requirements\ntest_or[['Id', 'SalePrice']].to_csv('submission_logReg_outliers_Lasso.csv',index=False)","47c77549":"# RF.\n# RF with the all columns.\n# only numeric columns.\n# without removing outliers\n\nrf = RandomForestRegressor()\n\nrf.fit(X_train_out.select_dtypes(exclude=['object']), y)\n\nprint('train score: ',rf.score(X_train_out.select_dtypes(exclude=['object']),y)) \n\n# Kaggle Submission \n\npred = rf.predict(X_test_out.select_dtypes(exclude=['object']))\ntest_or['SalePrice']= pred\n\n# # # Save the prediction into a file that matches the Kaggle submission requirements\ntest_or[['Id', 'SalePrice']].to_csv('submission_RF_outliers_allcol.csv',index=False)","df9d6106":"# RF. # cv=10 # n_estimators=100\n# RF with the all columns.\n# only numeric columns.\n# without removing outliers\nrf = RandomForestRegressor(n_estimators=100)\n\nrf.fit(X_train_out.select_dtypes(exclude=['object']), y)\n\ns = cross_val_score(rf, X_train_out.select_dtypes(exclude=['object']), y, cv=10, n_jobs=-1)\nprint(\"{} {} Score:\\t{:0.3} \".format(\"Random Forest\", \"Train\", s.mean().round(3)))\n\n# Kaggle Submission \n\npred = rf.predict(X_test_out.select_dtypes(exclude=['object']))\ntest_or['SalePrice']= pred\n# # # # Save the prediction into a file that matches the Kaggle submission requirements\ntest_or[['Id', 'SalePrice']].to_csv('submission_RF_outliers_allcol_estimators_cv.csv',index=False)\n","4af44145":"# RF. # grid search # cv \n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\nrfkkkkkk = RandomForestRegressor(n_estimators=100)\ngs = GridSearchCV(estimator = rfkkkkkk, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2)\ngs.fit(X_train_out.select_dtypes(exclude=['object']), y)\ngs.score(X_train_out.select_dtypes(exclude=['object']),y)\npred = gs.predict(X_test_out.select_dtypes(exclude=['object']))\ntest_or['SalePrice']= pred\n# # # # Save the prediction into a file that matches the Kaggle submission requirements\ntest_or[['Id', 'SalePrice']].to_csv('submission_RF_outliers__GS_cv.csv',index=False)\n\nprint('train score: ', gs.score(X_train_out.select_dtypes(exclude=['object']),y))\n","7e5845c2":"box_plots = var_standardized(\n    train_dum.select_dtypes(exclude=['object','uint8']).drop(['Id'], axis=1)) # only numeric columns and drop year. \n\nfig,ax=plt.subplots(figsize=(20,20)) # create figure with specific size  \nsns.boxplot(data=box_plots, orient='h', fliersize=5, \n                 linewidth=3, notch=True, saturation=0.5, ax=ax) # plot \nplt.title('This plot is from standardized merged data frame'); # title.","7e295d24":"####  - Dropping unnecessary columns","638b6d11":"###  How complete is the data?\n\nInvestigating missing values etc.","d58cd27e":"### Concluding remarks\n\n\nIn this expermint we predected the house prices using different regression models. This data was challenging since it contained many missing values. Dealing effectively with missing values (ie. replacing them with meaningful values), enabled us to predict house prices to a reasonably good prediction accuracy. We can see that Random Forest Regressor with cross validation, K-Fold=10 results in the best prediction score = 0.14\n\n\n#### Score optimazation recommendations: \n- Accuracy of our score can be further improved by removing extra noise that might be caused by existing outliers\n- we recommend experimenting with advanced regression technique such as gradient boosting and compare results.\n- Creating extra columns via feature engineering might lead a better prediction accuracy. Example: house age can be added as an extra column using year built column.  \n","3f897f61":"####  - Statistical Measures\n- Note: since there are 37 features, not all columns are displayed using .describe()\n- However, from displayed dataframe below, outliers can be noticed in different columns","4c1fcccf":"###  Filling missing data","a2f387d2":"####  - Filling missing data (object - fill with most frequent value)","37841933":"##  Data Cleaning and Data Exploration\n###  Display data","faabeb05":" - submitting above prediction to Kaggle resulted in score = 0.16460 ","b80f4797":" - submitting above prediction to Kaggle resulted in score = 0.22758 because logreg applies lasso by default ","6f14aa0a":"####  - Visualization ","60d28bad":" - submitting above prediction to Kaggle resulted in score = 0.22758 ","fddb3551":"####  - Visualization of missing values (re-check)","267db7d2":"####  - Filling missing data (object - replace with 'None' as a string)  ","1addb615":"|Data type|Action|Condition|\n|:-|:-|:-|\n|int\/float|Fill with mean|If normal distribution|\n|int\/float|Fill with most frequent value|If not normal distribution|\n|object|Replace with 'None' as a string|If indicated by data discription, e.g. 'NA','No garage'|\n|object|Fill with most frequent value|If not indicated by data discription|","6b6d3a34":"###  Data dictionary \n\nKindly refer to this link for full data dictionary:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data","18fdf5aa":"###  Filling missing data - special cases","a456a78d":"#### Random Forest Trees","73de6178":"#### Logistic Regression Model:","5702b0b2":"#####  - Visualizing outliers using boxplot","3ccfd485":"- When GarageArea feet equals to 0.0 that means no Garage as shown in GarageFinish values. \n- Therefore, Garage year built column should has \u2018None\u2019 as string. \n- Null values in GarageArea should be equal to 0.0 and Garage year built column equal to \u2019None\u2019 as string ","3965ac7a":" - spliting concatinated df (concat_1) to get back train and test with dummies","c737c27a":"##  Modeling","c68429d8":"####  - Dummies","68f181bc":" #### Best score:\n - submitting above prediction to Kaggle resulted in score = 0.14835 ","333e18b9":" - submitting above prediction to Kaggle resulted in score = 0.19876 ","8730f9b9":"# Advanced Regression Techniques  - Boston Housing Dataset\n\n## Problem Statement\n\n\nAlthough you might think the height of the basement ceiling might not influence the price for of a particular house, basement ceiling along with other factors highly influece house prices.\n\nPerforming machine learning processes to model the Boton housing dataset featured on Kaggle.com, our challenge was to predict the house price. \n\nGiven the multidimentionality of features in the Boston data, predicting the house price can be quite difficult since unnecessary features generate noise affecting the accuracy of our regression model. Therefore, cleared the noise proir to implementing our regression model.\n\n\n--- \n**Team members (Group 9)**\n - Fahdah Alalyan - fahdah.a15@gmail.com   \n - Amjaad Alsubaie - amjaad_636@hotmail.com  \n - Ahmed Adam - am4ma@hotmail.com\n \n---\n\n\n##  Data Wrangling\n\nLet's read the data.","b408320d":"#### scores worst to best \n - logRq:       0.22\n - RF normal:   0.19\n - GS,RF,CV:    0.16\n - RF CV:       0.14 (best score)","7418279f":"####  - Filling missing data (object - fill with most frequent value)","247fe14d":"####  - Filling missing data (int \/ float - fill with most frequent value)","994b6700":" - Since salePrice is our target, the highest observed correlation exist with 'OverallQual' and 'GrLivArea' ","61fe98d7":"### Investigating data types\nDisplaying the data types of each feature. "}}