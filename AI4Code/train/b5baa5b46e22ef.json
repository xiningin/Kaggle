{"cell_type":{"d34b7294":"code","8b3ed94c":"code","f54cbf83":"code","0ce2d200":"code","e0f14d03":"code","c842d3d2":"code","4d74f638":"code","7e453396":"code","97541dc6":"code","4e18108c":"code","a389299b":"code","cfe1fcf5":"code","faa58657":"code","97fc833a":"code","73ed94fa":"code","7cf9a59c":"code","5cd0151f":"code","5141e317":"code","c4b23804":"code","5f41b036":"code","ac29478a":"code","e1c73dfc":"code","5b637327":"code","3f1685b7":"code","d4ac6205":"code","d1a3ddd6":"code","5802cbc6":"code","10d51315":"code","617205e2":"code","a9f3e8a9":"code","8e00df35":"code","11f14b36":"code","d79d4a6b":"code","e56df5d2":"code","2d008e54":"code","4e3f7d79":"code","15644bb5":"code","c591114c":"code","e65d833e":"code","afeb7d0a":"code","4401761a":"code","547487c2":"code","a23b41a0":"code","7ed4e114":"code","eef53448":"code","841f20fb":"code","71687a9a":"code","98fe0780":"code","b6d0ec38":"code","63adb163":"code","f61ac729":"code","fece920b":"code","62268709":"code","dd4bb33d":"code","19c2bf86":"code","c88fa124":"code","fc2ccbf8":"code","da1b2ff4":"code","360bdc80":"code","d57823bf":"code","85a10bfd":"code","b037c245":"code","b841dbd5":"code","6e548129":"code","3a4f7b51":"code","16cccd7d":"code","05aae79b":"code","1bc98fb0":"code","7039c871":"code","fd901b88":"code","9846324a":"code","af858be4":"code","3332d6f5":"code","0abf61c5":"code","42d331c5":"code","8085a014":"code","b9941f38":"code","b18c3155":"code","853b2a44":"code","6da5cb3e":"code","0fa841ad":"code","e0cae0f1":"code","3416f764":"markdown","659b4988":"markdown","a2a3e5f8":"markdown","e8a99730":"markdown","e0088287":"markdown","65b053d1":"markdown","9adeb921":"markdown","562c5729":"markdown","dac0a0cc":"markdown","8470091b":"markdown","bcbfbf33":"markdown","ea68f086":"markdown","7e233535":"markdown","b59629a0":"markdown","d224faef":"markdown","4c200375":"markdown","be764d0f":"markdown","09a9606b":"markdown","574b397c":"markdown","9b05d7aa":"markdown"},"source":{"d34b7294":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\ntrain_filename = os.path.join(\"..\/input\/titanic\/train.csv\")\ntest_filename = os.path.join(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(train_filename, encoding='latin1')\ntest = pd.read_csv(test_filename, encoding='latin1')\n\ntrain.head()","8b3ed94c":"print(train[['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean())\nplt.figure(figsize=(12, 8))\nsns.boxplot(x='Pclass', y='Age', hue='Survived', data=train)","f54cbf83":"survived = train.loc[(train['Survived']==1)]\ntotal_passengers = len(train)\ntotal_survivors = len(survived)\nnumber_female_survivors = len(survived.loc[(survived['Sex']=='female')])\nnumber_male_survivors = total_survivors - number_female_survivors\ndist_male_female = [number_female_survivors, number_male_survivors]\nprint('Total Passengers: {} \\n'.format(total_passengers))\nprint('Total Number of Survivors: {} \\n'.format(total_survivors))\nprint('Number of Female Survivors: {} \\n'.format(number_female_survivors))\nprint('Number of Male Survivors: {} \\n'.format(number_male_survivors))\nsns.countplot(x='Survived', data=train, hue='Sex')","0ce2d200":"plt.pie(dist_male_female, labels=['female', 'male'])","e0f14d03":"male_survivors = survived.loc[(survived['Sex']=='male')]\nmale_survivors_ages = male_survivors['Age'].dropna()\nsns.distplot(male_survivors_ages)","c842d3d2":"female_survivors = survived.loc[(survived['Sex']=='female')]\nfemale_survivors_ages = female_survivors['Age'].dropna()\nsns.distplot(female_survivors_ages)","4d74f638":"train[[\"Sex\", \"Survived\"]].groupby('Sex', as_index=False).mean()","7e453396":"g = sns.FacetGrid(train, col='Survived', row='Pclass')\ng.map(plt.hist, 'Age', bins=20)","97541dc6":"sns.countplot(x='Embarked', data=train, hue='Survived')","4e18108c":"sns.countplot(x='Parch', data=train, hue='Survived')","a389299b":"sns.countplot(x='SibSp', data=train, hue='Survived')","cfe1fcf5":"sns.catplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\")","faa58657":"train.drop(['PassengerId'], axis=1, inplace=True)\ntest.drop(['PassengerId'], axis=1, inplace=True)","97fc833a":"from collections import Counter\n\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    for col in features:\n        # first quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # third quartile (75%)\n        Q3 = np.percentile(df[col], 75)\n        # interquartile range\n        IQR = Q3 - Q1\n        \n        # Outlier Step\n        outlier_step = 1.5 * IQR\n        \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        \n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)    \n    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n    return multiple_outliers\n\nOutliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])","73ed94fa":"train.loc[Outliers_to_drop]","7cf9a59c":"train = train.drop(Outliers_to_drop, axis=0).reset_index(drop=True)\ntrain_len = len(train)","5cd0151f":"combined = pd.concat([train, test], ignore_index=True)","5141e317":"sns.heatmap(train.isnull())","c4b23804":"sns.heatmap(test.isnull())","5f41b036":"np.sum(train.isnull())","ac29478a":"np.sum(test.isnull())","e1c73dfc":"embarked_mode = train['Embarked'].mode()[0]","5b637327":"combined['Embarked'] = combined['Embarked'].fillna(embarked_mode)","3f1685b7":"combined['Cabin'] = combined['Cabin'].apply(str).apply(lambda x: x[0])","d4ac6205":"combined['Cabin'].unique()","d1a3ddd6":"combined['Cabin'].fillna('U', inplace=True)","5802cbc6":"sns.countplot(combined['Cabin'])","10d51315":"np.where(np.isnan(test['Fare']))","617205e2":"test.iloc[152]","a9f3e8a9":"sns.heatmap(train[['Fare', 'Age', 'Sex', 'SibSp', 'Parch', 'Pclass' ]].corr(), annot=True, cmap='coolwarm')","8e00df35":"train[(train['Pclass'] == 3)]['Fare'].median()\ncombined.loc[(combined['Name'] == 'Storey, Mr. Thomas')]","11f14b36":"combined['Fare'][1033] = train[(train['Pclass']==3)]['Fare'].median()","d79d4a6b":"combined['Fare'] = combined['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\nnp.sum(combined.isnull())","e56df5d2":"train['Name'].head()","2d008e54":"combined['Title'] = combined['Name'].apply(lambda x: (x.split(', ')[1].split('.')[0]))\ncombined.drop('Name', axis=1, inplace=True)\ncombined['Title'].unique()","4e3f7d79":"combined['Title'].value_counts()","15644bb5":"titles_map = {\n 'Capt' : 'Rare',\n 'Col' : 'Rare',\n 'Don': 'Rare',\n 'Dona': 'Rare',\n 'Dr' : 'Rare',\n 'Jonkheer' :'Rare' ,\n 'Lady': 'Rare',\n 'Major': 'Rare',\n 'Master': 'Master',\n 'Miss' : 'Miss',\n 'Mlle' : 'Rare',\n 'Mme': 'Rare',\n 'Mr': 'Mr',\n 'Mrs': 'Mrs',\n 'Ms': 'Rare',\n 'Rev': 'Rare',\n 'Sir': 'Rare',\n 'the Countess': 'Rare'    \n}","c591114c":"combined['Title'] = combined['Title'].map(titles_map)","e65d833e":"combined[['Title', 'Survived']].groupby('Title', as_index=False).mean()","afeb7d0a":"combined.head()","4401761a":"from sklearn.preprocessing import LabelEncoder\n\n\n\n#embarked_encoder = LabelEncoder().fit(combined['Embarked'])\n#combined['Embarked'] = embarked_encoder.transform(combined['Embarked'])\ncombined = pd.get_dummies(combined, columns=['Embarked'], prefix='Em')\n\ncombined = pd.get_dummies(combined, columns=['Cabin'], prefix='Cb')\n\n\n\ncombined = pd.get_dummies(combined, columns=['Title'], prefix='Title')","547487c2":"sex_encoder = LabelEncoder().fit(combined['Sex'])\ncombined['Sex'] = sex_encoder.transform(combined['Sex'])","a23b41a0":"sns.heatmap(train[['Age', 'Sex', 'SibSp', 'Parch', 'Pclass']].corr(), annot=True, cmap='coolwarm')","7ed4e114":"def impute_age(row):\n    pclass = row['Pclass']\n    parch = row['Parch']\n    sibsp = row['SibSp']\n    age = row['Age']\n    if pd.isnull(age):\n        age_median = train['Age'].median()\n        similar_age = train[(train['Pclass'] == pclass) & \n                            (train['Parch'] == parch) & \n                            (train['SibSp'] == sibsp)]['Age'].median()\n        if (similar_age > 0): return similar_age\n        else: return age_median\n    else: return age    \n    ","eef53448":"combined['Age'] = combined.apply(impute_age, axis=1)","841f20fb":"np.sum((combined.drop('Survived', axis=1).isnull()))","71687a9a":"combined['Family_size'] = combined.apply(lambda row: 1 + (row['Parch'] + row['SibSp']), axis=1)\ncombined['Alone'] = combined.apply(lambda row: 1 if (row['Parch'] + row['SibSp']) == 0 else 0, axis=1)","98fe0780":"sns.countplot(x='Family_size', data=combined, hue='Survived')","b6d0ec38":"combined['Small_family'] = combined.apply(lambda row: 1 if 2 <= (row['Family_size']) <= 4 else 0, axis=1)\ncombined['Large_family'] = combined.apply(lambda row: 1 if (row['Family_size']) > 4 else 0, axis=1)","63adb163":"combined.head()","f61ac729":"combined['Age*Pclass'] = combined['Age'] * combined['Pclass']","fece920b":"import re","62268709":"combined['Ticket'] = combined['Ticket'].apply(lambda x: 'X' if x.isdigit() else x)","dd4bb33d":"combined['Ticket'] = combined['Ticket'].apply(lambda x: re.sub(\"[\\d\\.]\", \"\", x).split('\/')[0].strip() if not x.isdigit() else x)","19c2bf86":"ticket_encoder = LabelEncoder().fit(combined['Ticket'])\ncombined['Ticket'] = ticket_encoder.transform(combined['Ticket'])","c88fa124":"train = combined[:train_len]\ntest = combined[train_len:]","fc2ccbf8":"test.drop('Survived', axis=1, inplace=True)","da1b2ff4":"from sklearn.model_selection import train_test_split","360bdc80":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB","d57823bf":"np.sum(test.isnull())","85a10bfd":"train.head()","b037c245":"train['Survived'] = train['Survived'].astype(int)","b841dbd5":"from sklearn.preprocessing import StandardScaler\n\nX = train.drop('Survived', axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","6e548129":"from sklearn.model_selection import cross_val_score","3a4f7b51":"random_state = 42\n\nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier', 'SVC', \n              'RandomForestClassifier', 'XGBClassifier', 'ExtraTreesClassifier'\n              , 'GradientBoostingClassifier','AdaBoostClassifier','GaussianNB']\n\nmodels = [ ('LogisticRegression',LogisticRegression(random_state=random_state)),\n          ('DecisionTreeClassifier', DecisionTreeClassifier(random_state=random_state)),\n          ('SVC', SVC(random_state=random_state)),\n          ('RandomForestClassifier',RandomForestClassifier(random_state=42)),\n          ('XGBClassifier',XGBClassifier(random_state=random_state)),\n          ('ExtraTreesClassifier',ExtraTreesClassifier(random_state=random_state)),\n          ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=random_state)),\n          ('AdaBoostClassifier',AdaBoostClassifier(random_state=random_state)),\n          ('GaussianNB',GaussianNB())\n         ]\n\nmodel_accuracy = []\n\nfor k, model in models:\n    print(k, ':')\n    model.fit(X, y)\n    accuracy = cross_val_score(model, X_train, y_train, cv=5).mean()\n    model_accuracy.append(accuracy)\n    print(accuracy)\n    print('\\n')","16cccd7d":"pd.concat([pd.Series(model_names), pd.Series(model_accuracy)], axis=1).sort_values(by=1, ascending=False)","05aae79b":"from sklearn.model_selection import GridSearchCV","1bc98fb0":"best_models = []\n\nxgboot_param_grid = {\n    'n_estimators': [100, 200, 300, 400],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [.4, .45, .5, .55, .6], \n    'colsample_bytree': [.6, .7, .8, .9, 1]\n}\n\nada_param_grid = {\n 'n_estimators':[100,200,300],\n 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n 'algorithm' : ['SAMME', 'SAMME.R']\n }\n\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,200,300],\n              \"criterion\": [\"gini\"]}\n\nrf_param_grid  = { \n    'n_estimators': [100,200,300],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nlog_param_grid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n\nsvv_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodels = [ \n    ('AdaBoostClassifier',AdaBoostClassifier(), ada_param_grid),\n          ('XGBClassifier',XGBClassifier(), xgboot_param_grid),\n          ('GradientBoostingClassifier',GradientBoostingClassifier(), gb_param_grid),\n        ('RandomForestClassifier',RandomForestClassifier(), rf_param_grid),\n          ('ExtraTreesClassifier',ExtraTreesClassifier(), ex_param_grid),\n    ('SVC',SVC(probability=True), svv_param_grid),\n    ('LogisticRegression',LogisticRegression(), log_param_grid)\n         ]\n\nfor name, model, param in models: \n    print(name, ':')\n    grid_search = GridSearchCV(model, \n                              scoring = 'accuracy',\n                              param_grid=param,\n                              cv=5,\n                              verbose=2,\n                              n_jobs=-1) \n    grid_search.fit(X, y)\n    print(grid_search.best_score_, '\\n')\n    best_models.append(grid_search.best_estimator_)","7039c871":"test","fd901b88":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None: \n        plt.ylim(*ylim)\n    plt.xlabel('Training Examples')\n    plt.ylabel('Score')\n    train_sizes, train_scores, test_scores = learning_curve(estimator, \n                                                            X, y, \n                                                            cv=cv, \n                                                            n_jobs=n_jobs, \n                                                            train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    \n    plt.fill_between(train_sizes, \n                     train_scores_mean - train_scores_std, \n                     train_scores_mean + train_scores_std, \n                     alpha=0.1, \n                     color='r')\n    plt.fill_between(train_sizes,\n                    test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std,\n                    alpha=0.1,\n                    color='g')\n    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Score')\n    plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross Validation Score')\n    \n    plt.legend(loc='best')\n    return plt\n\nfor model in best_models: \n    plot_learning_curve(model, model.__class__.__name__ + 'RF Learning Curves', X, y, cv=10)\n    \n    \n    ","9846324a":"def plot_feature_importances(clf, X_train, \n                            y_train=None, top_n=10, \n                            figsize=(8,8), print_table=False, \n                            title='Feature Importances'):\n    __name__ = 'plot_feature_importances'\n        \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError('{} Does not have feature_importances_ attribute'.format(clf.__class__.__name__))\n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n    \n    feat_imp = pd.DataFrame({'importance': clf.feature_importances_})\n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table: \n        from IPython.display import display\n        print('Top {} features according to importance'.format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp\n\n        \n    \n","af858be4":"for model in best_models:\n    try:\n        _ = plot_feature_importances(model, X_train, y_train, top_n=X.shape[1], title=model.__class__.__name__)\n\n    except AttributeError as e:\n        print(e)\n        \n        ","3332d6f5":"pred = []\nfor model in best_models: \n    pred.append(pd.Series(model.predict(test), name=model.__class__.__name__))\n    \n    ","0abf61c5":"pred = pd.DataFrame(pred).T\npred","42d331c5":"pred.sum()","8085a014":"g = sns.heatmap(pred.corr(), annot=True, cmap='coolwarm')\n","b9941f38":"from sklearn.ensemble import VotingClassifier\n","b18c3155":"best_models","853b2a44":"votingC = VotingClassifier(estimators = [\n    ('ada', best_models[0]),\n    ('rf', best_models[3]),\n    ('ext', best_models[4]), \n    ('scv', best_models[5]),\n    ('log', best_models[6]),\n], voting='soft', n_jobs=5)\n\nvotingC.fit(X, y)","6da5cb3e":"test_Survived = pd.Series(votingC.predict(test), name='Survived')\n\nresults = pd.concat([pd.read_csv('\/kaggle\/input\/titanic\/test.csv')['PassengerId'],test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)\n\nresults","0fa841ad":"ext_best = best_models[4]\next_best.fit(X, y)","e0cae0f1":"test_Survived = pd.Series(ext_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([pd.read_csv('\/kaggle\/input\/titanic\/test.csv')['PassengerId'],test_Survived],axis=1)\n\nresults.to_csv(\"prediction.csv\",index=False)","3416f764":"process parch and sibsp","659b4988":"# Data Exploration","a2a3e5f8":"Cross Validation","e8a99730":"# Model Machine Learning","e0088287":"Checking the accuracy of each model using the random samples generated in the previous step","65b053d1":"Using a grid to search for the best estimator","9adeb921":"process age","562c5729":"## Process embarked (fill 2 NaN values)","dac0a0cc":"# Feature Engineering","8470091b":"Process name","bcbfbf33":"generating random samples of the data to test the models for accuracy","ea68f086":"## Hyperparameter tuning","7e233535":"Get title of each passenger","b59629a0":"# Importing Data","d224faef":"# Data Cleaning","4c200375":"Inspo taken from Ayoub - learning the basics :) ","be764d0f":"combine Pclass & age","09a9606b":"Process ticket","574b397c":"## Process Cabin Column","9b05d7aa":"Putting the model names next to the accuracy of each model for the sample data sets."}}