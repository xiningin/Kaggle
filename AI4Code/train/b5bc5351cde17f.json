{"cell_type":{"157a90f8":"code","d67c0ae0":"code","c86fc389":"code","208ef460":"code","e1d6cd72":"code","7b4e3bdd":"code","3ec6b195":"code","15614d8e":"code","befda924":"code","049bdad9":"code","0d9269e6":"code","dc8f896d":"code","b83822f8":"code","34f669de":"code","673919a4":"code","fd137809":"code","a1fd6f37":"code","087a2249":"code","03d414e3":"code","5920504d":"code","9774f784":"code","85468c73":"code","2700c9e3":"code","83ba8507":"code","fbd80bad":"code","8b62a45d":"code","7483ca95":"code","833b89ca":"code","8234ac89":"code","9ebd4d00":"code","6ec8166f":"code","e3183f6c":"code","62f57e69":"code","52c24fe7":"code","5f25c839":"code","934ad374":"code","1fc2f501":"code","84e4d7f2":"code","b2625b47":"code","54c84269":"code","455d530a":"code","94bdc2b2":"code","b3f87170":"code","c7bef953":"code","07bffbe4":"code","bc97b26d":"code","2b986c3d":"code","074b4b20":"code","dcff5973":"code","2f77ee29":"code","814e2b9e":"code","e6c103c4":"code","4dadb948":"code","95d0fb0c":"code","472aee1b":"code","ed3123b7":"code","75453d18":"code","8b0245c3":"code","3c4e599e":"code","c6e917df":"code","649c8ef9":"code","f3b41a62":"code","4f78cb86":"markdown","b9d79cd9":"markdown","b406c376":"markdown","ed9efeed":"markdown","7330e123":"markdown","a472c25f":"markdown","a0f6dcda":"markdown","757c1432":"markdown","1c96ab22":"markdown","1ffffd4e":"markdown","20e8ab90":"markdown","c513c5fe":"markdown","10fe83e4":"markdown","d71eb39b":"markdown","ac04490e":"markdown","7cea930a":"markdown","9b9e163f":"markdown","975d201d":"markdown","7162b433":"markdown","eee2ef81":"markdown","ffd1fbcb":"markdown","8a16349b":"markdown","395802c3":"markdown","2e084441":"markdown","448c1939":"markdown"},"source":{"157a90f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)  \npd.set_option('display.max_rows', None)  \n# pd.set_option('display.', None)  \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport collections\nimport operator\nimport itertools\nfrom functools import reduce\nimport re\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d67c0ae0":"airports_df = pd.read_csv('\/kaggle\/input\/flight-delays\/airports.csv')\nairports_df.head()","c86fc389":"flights_df = pd.read_csv('\/kaggle\/input\/flight-delays\/flights.csv')","208ef460":"flights_df.shape","e1d6cd72":"flights_df.tail()","7b4e3bdd":"my_flights_df = flights_df[['YEAR','MONTH','DAY','DAY_OF_WEEK','AIRLINE','FLIGHT_NUMBER','ORIGIN_AIRPORT','DESTINATION_AIRPORT',\n                          'DEPARTURE_DELAY','ARRIVAL_DELAY','CANCELLED','DIVERTED']]\nmy_flights_df.tail()","3ec6b195":"my_flights_df = my_flights_df[my_flights_df['CANCELLED']==0]","15614d8e":"my_flights_df.shape","befda924":"my_flights_df = my_flights_df[my_flights_df['DIVERTED']==0]","049bdad9":"# we don't consider negative delay as delay, so we replace 0 for delays < 0.\nmy_flights_df['ARRIVAL_DELAY'][my_flights_df['ARRIVAL_DELAY']<0] = 0\nmy_flights_df['DEPARTURE_DELAY'][my_flights_df['DEPARTURE_DELAY']<0] = 0","0d9269e6":"my_flights_df['DELAY'] = my_flights_df['ARRIVAL_DELAY'] + my_flights_df['DEPARTURE_DELAY']","dc8f896d":"my_flights_df = my_flights_df[my_flights_df['DELAY']!=0]","b83822f8":"# we see that these two columns are always zero in this resulting dataframe, so we drop them\nmy_flights_df = my_flights_df.drop(['CANCELLED','DIVERTED'], axis=1)","34f669de":"my_flights_df = my_flights_df.merge(airports_df, how='left',left_on='ORIGIN_AIRPORT', \n                                    right_on='IATA_CODE').drop(['IATA_CODE','AIRPORT','COUNTRY','LATITUDE','LONGITUDE'], axis=1)\nmy_flights_df = my_flights_df.rename({'CITY':'ORIGIN_CITY','STATE':'ORIGIN_STATE'}, axis=1)\nmy_flights_df = my_flights_df.merge(airports_df, how='left',left_on='DESTINATION_AIRPORT', \n                                    right_on='IATA_CODE').drop(['IATA_CODE','AIRPORT','COUNTRY','LATITUDE','LONGITUDE'], axis=1)\nmy_flights_df = my_flights_df.rename({'CITY':'DESTINATION_CITY','STATE':'DESTINATION_STATE'}, axis=1)\nmy_flights_df.head()","673919a4":"my_flights_df.isnull().sum()","fd137809":"# There are some airports that their city is not provided, we remove these rows too\nmy_flights_df = my_flights_df[~(my_flights_df['ORIGIN_CITY'].isnull() | my_flights_df['DESTINATION_CITY'].isnull())]","a1fd6f37":"# to reduce the size of the dataset, only intrastate flights are condidered\nmy_flights_df = my_flights_df[my_flights_df['ORIGIN_STATE']!=my_flights_df['DESTINATION_STATE']]","087a2249":"my_flights_df['O_D']= my_flights_df['ORIGIN_STATE']+'_'+my_flights_df['DESTINATION_STATE']","03d414e3":"my_flights_df.shape","5920504d":"np.random.seed(10)\nbefore_size = my_flights_df.shape[0]\n# del_rate = 0.99\n# remove_n = int(del_rate * my_flights_df.shape[0])\n# remove_n = 10000\nremainder = 4000\nremove_n = before_size - remainder\ndrop_indices = np.random.choice(my_flights_df.index, remove_n, replace=False)\ntruncated_flights_df = my_flights_df.drop(drop_indices)\nafter_size = truncated_flights_df.shape[0]\nprint('size changed from {} to {}'.format(before_size, after_size))","9774f784":"truncated_flights_df.head()","85468c73":"(my_flights_df['DESTINATION_STATE'].value_counts()+my_flights_df['ORIGIN_STATE'].value_counts()).sort_values(ascending=False).shape","2700c9e3":"(my_flights_df['DESTINATION_CITY'].value_counts()+my_flights_df['ORIGIN_CITY'].value_counts()).sort_values(ascending=False)[:100]","83ba8507":"(my_flights_df['ORIGIN_STATE'].value_counts()+my_flights_df['DESTINATION_STATE'].value_counts()).plot(kind='bar', figsize=(15, 4), title='Origin State of the Flights')","fbd80bad":"my_flights_df['DESTINATION_STATE'].value_counts().plot(kind='bar', figsize=(15, 4), title='Destination State of the Flights')","8b62a45d":"my_flights_df['DESTINATION_CITY'].value_counts()[:50].plot(kind='bar', figsize=(15, 4), title='Destination City of the Flights (top 50)')","7483ca95":"my_flights_df['ORIGIN_CITY'].value_counts()[:50].plot(kind='bar', figsize=(15, 4), title='Origin City of the Flights (top 50)')","833b89ca":"(my_flights_df['DESTINATION_CITY'].value_counts()+my_flights_df['DESTINATION_CITY'].value_counts())[:50].plot(kind='bar', figsize=(15, 4), title='Involved Cities of the Delayed Flights (top 50)')\nplt.ylabel('# of delayed flights')","8234ac89":"my_flights_df.groupby('MONTH').count()['YEAR'].plot(kind='bar')\nplt.ylabel('# delayed flights')\nplt.title('# of delayed flights each month')","9ebd4d00":"my_flights_df.groupby('DAY_OF_WEEK').count()['YEAR'].plot(kind='bar')\nplt.ylabel('# delayed flights')\nplt.title('# of delayed flights in different days of week')","6ec8166f":"my_flights_df.groupby('DAY').count()['YEAR'].plot(kind='bar')\nplt.ylabel('# delayed flights')\nplt.title('# of delayed flights in different days of month')","e3183f6c":"my_flights_df.head()","62f57e69":"print('Here is top ten destination-origin pair flights')\nprint(my_flights_df['O_D'].value_counts()[:5])","52c24fe7":"import os\nos.chdir(r'\/kaggle\/working')\nmy_flights_df.to_csv(r'flights.csv')","5f25c839":"from IPython.display import FileLink\nFileLink(r'flights.csv')","934ad374":"my_flights_df.to_csv(r'flights_4000.csv')\nFileLink(r'flights_4000.csv')","1fc2f501":"# making the dataset\ngp = truncated_flights_df.groupby('MONTH')\ndataset = []\nfor i in list(truncated_flights_df['MONTH'].value_counts().sort_index().index):\n    lst = (gp.get_group(i)['O_D']).tolist()\n    dataset.append(lst)","84e4d7f2":"# !pip install mlxtend","b2625b47":"# from mlxtend.preprocessing import TransactionEncoder\n# from mlxtend.frequent_patterns import fpgrowth\n# from mlxtend.frequent_patterns import association_rules\n\n# te = TransactionEncoder()\n# te_ary = te.fit(dataset).transform(dataset)\n# df = pd.DataFrame(te_ary, columns=te.columns_)\n# res = fpgrowth(df, min_support=1, use_colnames=True)","54c84269":"# association_rules(res, metric=\"confidence\", min_threshold=1)","455d530a":"!pip install efficient-apriori","94bdc2b2":"from efficient_apriori import apriori\nitemsets, rules = apriori(dataset, min_support=1,  min_confidence=1)","b3f87170":"my_itemsets_dicts = []\nfor i in range(1, len(itemsets)):\n  my_itemsets_dicts.append(itemsets[i])","c7bef953":"my_itemsets = []\nfor d in my_itemsets_dicts:\n  my_itemsets.append(list(d.keys()))\n# flattening the list\nmy_itemsets = [item for sublist in my_itemsets for item in sublist]\nprint('There are {} frequent itemsets'.format(len(my_itemsets)))\nprint('There are {} rules'.format(len(rules)))","07bffbe4":"def is_power_of_two(n):\n    \"\"\"Returns True iff n is a power of two.  Assumes n > 0.\"\"\"\n    return (n & (n - 1)) == 0\n\ndef get_maximal_subsets(sequence_of_sets):\n    \"\"\"Return a list of the elements of `sequence_of_sets`, removing all\n    elements that are subsets of other elements.  Assumes that each\n    element is a set or frozenset and that no element is repeated.\"\"\"\n    # The code below does not handle the case of a sequence containing\n    # only the empty set, so let's just handle all easy cases now.\n    if len(sequence_of_sets) <= 1:\n        return list(sequence_of_sets)\n    # We need an indexable sequence so that we can use a bitmap to\n    # represent each set.\n    if not isinstance(sequence_of_sets, collections.Sequence):\n        sequence_of_sets = list(sequence_of_sets)\n    # For each element, construct the list of all sets containing that\n    # element.\n    sets_containing_element = {}\n    for i, s in enumerate(sequence_of_sets):\n        for element in s:\n            try:\n                sets_containing_element[element] |= 1 << i\n            except KeyError:\n                sets_containing_element[element] = 1 << i\n    # For each set, if the intersection of all of the lists in which it is\n    # contained has length != 1, this set can be eliminated.\n    out = [s for s in sequence_of_sets\n           if s and is_power_of_two(reduce(\n               operator.and_, (sets_containing_element[x] for x in s)))]\n    return out","bc97b26d":"maximal_itemsets = get_maximal_subsets(my_itemsets)\nprint('There are {} maximal frequent itemsets'.format(len(maximal_itemsets)))","2b986c3d":"maximal_itemsets[-10:]","074b4b20":"maximal_itemsets[0]","dcff5973":"def get_sample(data_df, size):\n    \"\"\"\n    provides a bootstrap sample of the dataset\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        dataset that the sample is drawn from\n    size : Integer\n        size of the sample\n    \"\"\"\n    chosen_indices = np.random.choice(data_df.index, size, replace=True)\n    sample_flights_df = my_flights_df.loc[chosen_indices]\n    return sample_flights_df","2f77ee29":"def convert_df_to_dataset(data_df):\n    \"\"\"\n    Converts a DataFrame to a list of baskets, the baskets are months and the items are the fligths\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        dataframe to be converted to the dataset\n    \"\"\"\n    gp = data_df.groupby('MONTH')\n    dataset = []\n    for i in list(data_df['MONTH'].value_counts().sort_index().index):\n        lst = (gp.get_group(i)['O_D']).tolist()\n        dataset.append(lst)\n    return dataset","814e2b9e":"def compute_rsupport(sample_dataset, pattern):\n    \"\"\"\n    computes the support of a pattern in the given dataset\n\n    Parameters\n    ----------\n    sample_df : List\n        sample dataset that is a list of baskets\n    pattern : Tuple\n        pattern to be found\n    \"\"\"\n    support = 0\n    for item in sample_dataset:\n        if set(pattern).issubset(item):\n            support = support + 1\n    return support\/len(sample_dataset)","e6c103c4":"def get_lower_bound(cumlative_dist, lower_bound_index):\n    lower_bound = 0\n    for item in cumlative_dist:\n        if item < lower_bound_index:\n            continue\n        lower_bound = cumlative_dist[cumlative_dist == item].index[0]\n        break\n    return lower_bound","4dadb948":"def get_upper_bound(cumlative_dist, upper_bound_index):\n    upper_bound = 0\n    for item in cumlative_dist.sort_values(ascending=False):\n        if item > upper_bound_index:\n            continue\n        upper_bound = cumlative_dist[cumlative_dist == item].index[0]\n        break\n    return upper_bound","95d0fb0c":"def get_rsupport_bound(dataset_df, pattern, k=50, n =2000, alpha=0.99):\n    lower_bound_index = (1-alpha)\/2\n    upper_bound_index = (1+alpha)\/2\n    rsupport = []\n    for i in range(k):\n        sample_df = get_sample(dataset_df, n)\n        sample_dataset = convert_df_to_dataset(sample_df)  \n        rsupport.append(compute_rsupport(sample_dataset, pattern))\n\n    cumlative_dist = (1\/k) * np.cumsum(pd.Series(rsupport).value_counts().sort_index())\n    rsup_bound = get_lower_bound(cumlative_dist, lower_bound_index), get_upper_bound(cumlative_dist, upper_bound_index)\n    return rsup_bound","472aee1b":"get_rsupport_bound(my_flights_df, maximal_itemsets[0], k=500, n=2000)","ed3123b7":"bounds = []\nfor itemset in maximal_itemsets[:20]:\n    bounds.append((itemset,get_rsupport_bound(my_flights_df, itemset, n=5000)))","75453d18":"# taking itemsets with the highest mean bounds\nitem_mean_bound = {}\nfor item in bounds:\n    item_mean_bound.update({item[0]: 0.5*(item[1][0]+ item[1][1])})\n# sorting the dictionary\nitem_mean_bound = {k: v for k, v in sorted(item_mean_bound.items(), key=lambda item: item[1], reverse=True)}\ndict(itertools.islice(item_mean_bound.items(), 10))","8b0245c3":"rhs_lens = []\nfor rule in rules:\n    rhs_lens.append(len(rule.rhs))\nmax_consequent = np.max(rhs_lens)","3c4e599e":"influential_rules = []\nfor rule in rules:\n    if len(rule.lhs) == 1 and len(rule.rhs) == max_consequent:\n        influential_rules.append(rule)\nprint('There are {} influential rules'.format(len(influential_rules)))\ninfluential_rules[:10]","c6e917df":"influential_flights = []\nfor rule in influential_rules:\n    influential_flights.append(rule.rhs)\ninfluential_flights = np.unique(influential_flights)\nprint('There are {} influential flights'.format(len(influential_flights)))\ninfluential_flights","649c8ef9":"influential_states = []\nstates = []\nflatten = lambda l: [item for sublist in l for item in sublist]\nfor flight in influential_flights:\n    states.append(re.split(\"_\", flight))\nstates = flatten(states)\n# finding top 5 \npd.Series(states).value_counts()[:5]","f3b41a62":"pd.Series(states).value_counts()[:5].index","4f78cb86":"## Influential States\nthe most common states in the influential flights are influential states","b9d79cd9":"# Finding Influential Flights\nflights that appear alone at the right side of frequent association rules and the left side has the maximum (7) number of flights","b406c376":"### Step 5) Arrival delays and departure delays are combined","ed9efeed":"### Step 1) Relevant columns are filtered","7330e123":"### Step 13) The size of the dataset is reduced","a472c25f":"### Step 3) Diverted flights are removed","a0f6dcda":"## Preprocessing","757c1432":"## Bootstrap Sampling for Confidence Interval","1c96ab22":"### Step 7) 'Cancelled' and 'Diverted' columns are removed","1ffffd4e":"### Step 2) Cancelled flights are removed","20e8ab90":"### Step 4) Less-than-zero delays are converted to zero","c513c5fe":"## Some Plots","10fe83e4":"### Step 8) Destination and origin city and state of the flight is added","d71eb39b":"### Putting all itemsets in a list","ac04490e":"### Making the dataset","7cea930a":"### Step 9) Null values are checked","9b9e163f":"### Mining Maximal Itemsets","975d201d":"## Frequent itemset mining for the truncated dataset","7162b433":"## Saving the dataset","eee2ef81":"note that itemsets is in the format of:  \n{1: {('a',): 3, ('b',): 2, ('c',): 1},  \n    ...             2: {('a', 'b'): 2, ('a', 'c'): 1}}  ","ffd1fbcb":"### Extracting frequent itemsets and association rules","8a16349b":"### Step 10) Flights with no origin\/departure city are removed","395802c3":"### Step 11) Interstate flights are removed","2e084441":"### Step 6) Flights with no delay are removed","448c1939":"### Step 12) \"O_D\" column is added"}}