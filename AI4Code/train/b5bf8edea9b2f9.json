{"cell_type":{"cf24aaf1":"code","b8bc293a":"code","929d6570":"code","5636a56e":"code","a8f128d9":"code","db0dc405":"code","887047d2":"code","0d9f58ed":"code","8ed9c115":"code","a44e4fae":"code","e91323dd":"code","ce174334":"code","6522efc9":"code","54a72002":"code","31bafb53":"code","cd65e660":"code","ecb4038f":"code","2ef74677":"code","eb2df1f7":"code","bf759864":"code","30bc7170":"code","12b241d2":"code","5e85950b":"code","1b6fc12c":"code","3757731a":"code","2ccf2ff7":"code","35607064":"code","397d02f9":"code","216e0e55":"code","659eaa64":"code","ea053ced":"code","926167d1":"code","e03e9255":"code","6063935e":"code","292fc6c5":"code","f3d38733":"code","428f5510":"code","5dd60244":"code","ceda9feb":"code","5e64f59a":"code","6d9c0820":"code","71d88ca7":"code","8feffce9":"code","53004e55":"code","27f31f28":"code","2d4efaac":"code","4ffc2d5c":"code","e78baefd":"markdown","ad7f792e":"markdown","fcf3fae4":"markdown","d3c60e72":"markdown","0abb8472":"markdown","004a0711":"markdown","b8380f09":"markdown","0004ace6":"markdown","5386e4eb":"markdown","ef5961f8":"markdown","c2625a9e":"markdown","0249cb85":"markdown"},"source":{"cf24aaf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8bc293a":"df = pd.read_csv('..\/input\/product-position\/upper.csv')\ndf","929d6570":"from sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.ensemble import ExtraTreesClassifier #classifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.pipeline import Pipeline\n\n#from imblearn.under_sampling import RandomUnderSampler #imbalanced\n\n#Select feature\n#from sklearn.feature_selection import SelectKBest\n#from sklearn.feature_selection import f_classif\n\n#Imbalanced\n#undersample = RandomUnderSampler(sampling_strategy='majority') \n#X_res, y_res = undersample.fit_resample(X_train, y_train)\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    #('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',ExtraTreesClassifier(random_state=0))#classifier\n])\n\n\n#Tune GridSearchCV\nparams = { \n    #'feature_selection__k':[3,5,7],\n    'classification__n_estimators': [10,20,50,100,200]\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_train, y_train)","5636a56e":"best_clf.best_params_","a8f128d9":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","db0dc405":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","887047d2":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","0d9f58ed":"params = { 'C': [2,4,8,16,32,64,128], #High Cost = High accuracy\n           'gamma': [0.01,0.25,0.50,0.75,1]  #Low Gramma = High Influence  \n         }\nclf = GridSearchCV(SVC(),params,cv=10) #algorithm \nclf.fit(X_train_norm, y_train) # \u0e25\u0e2d\u0e07 train","8ed9c115":"print(\"Best param : \"+str(clf.best_params_))\nprint(\"10CV accuracy : \"+str(clf.best_score_*100))","a44e4fae":"y_predict = clf.predict(X_test_norm)\nprint(\"Test accuracy : \"+str(sum(y_test == y_predict)\/len(y_test)*100))","e91323dd":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative(0)', 'positive(1)']\nC = confusion_matrix(y_test,y_predict) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\"f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","ce174334":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predict, target_names=target_names))","6522efc9":"from imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.svm import SVC\n\nundersample = RandomUnderSampler(sampling_strategy='majority') #resample of imbalanced label\nX_res, y_res = undersample.fit_resample(X_train, y_train)\n\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([\n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    ('feature_selection',SelectKBest(f_classif, k=5)), #select feature\n    ('classification',SVC()) #classifier\n])\nclf.fit(X_res, y_res)","54a72002":"from sklearn.model_selection import cross_val_score\nacc = cross_val_score(clf, X_train, y_train, cv=500) #cross_validation (1023)\nprint(\"10CV Traning accuracy: \"+str(acc.mean()*100))","31bafb53":"clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc*100))","cd65e660":"from imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.svm import SVC\n\nundersample = RandomUnderSampler(sampling_strategy='majority') #resample of imbalanced label\nX_res, y_res = undersample.fit_resample(X_train, y_train)\n\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([\n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    ('feature_selection',SelectKBest(f_classif, k=5)), #select feature\n    ('classification',SVC()) #classifier\n])\nclf.fit(X_res, y_res)","ecb4038f":"best_clf.best_params_","2ef74677":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","eb2df1f7":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","bf759864":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","30bc7170":"params = {\n    'n_estimators' : [10,50,100,200,500],\n    'min_samples_leaf' : [1,2,4,8,16,32],\n    'max_features' : ['sqrt',0.5,0.8],\n    'criterion' : ['gini','entropy']\n}\nclf = GridSearchCV(RandomForestClassifier(random_state=0),params, cv = 10)\nclf.fit(X_train_norm, y_train)","12b241d2":"print(\"Best params : \" + str(clf.best_params_))\nprint(\"10CV accuracy : \"+str(clf.best_score_*100))","5e85950b":"y_predict = clf.predict(X_test_norm)\nprint(\"Test accuracy : \"+str(sum(y_test == y_predict)\/len(y_test)*100))","1b6fc12c":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative', 'positive']\nC = confusion_matrix(y_test,y_predict) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","3757731a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predict, target_names=target_names))","2ccf2ff7":"from sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.ensemble import RandomForestClassifier #classifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.pipeline import Pipeline\n\n#from imblearn.under_sampling import RandomUnderSampler #imbalanced\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n#Imbalanced\n#undersample = RandomUnderSampler(sampling_strategy='majority') \n#X_res, y_res = undersample.fit_resample(X_train, y_train)\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    ('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',RandomForestClassifier(random_state=0))#classifier\n])\n\n\n#Tune GridSearchCV\nparams = { \n    'feature_selection__k':[3,5,7],\n    'classification__n_estimators': [10,20,50,100,200],\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_train, y_train)","35607064":"best_clf.best_params_","397d02f9":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","216e0e55":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","659eaa64":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","ea053ced":"params = {'C': [0.25,0.5,0.75,1,2,3,4,5]} #parameter \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e08\u0e39\u0e19\u0e04\u0e48\u0e32 \nclf = GridSearchCV(LogisticRegression(random_state=0, solver='liblinear'),params, cv=10) #GridSeachcv \u0e40\u0e2d\u0e32\u0e44\u0e27\u0e49\u0e2b\u0e32 parameter \u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e42\u0e14\u0e22\u0e27\u0e31\u0e14\u0e08\u0e32\u0e01\u0e04\u0e48\u0e32 accuracy \nclf.fit(X_train_norm, y_train) #\u0e25\u0e2d\u0e07 train","926167d1":"print(\"Best params : \" +str(clf.best_params_)) #\u0e04\u0e48\u0e32 C \u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e04\u0e37\u0e2d 1\nprint(\"10CV accuracy : \"+str(clf.best_score_*100)) #\u0e04\u0e48\u0e32 accuracy \u0e02\u0e2d\u0e07 train \u0e01\u0e48\u0e2d\u0e19","e03e9255":"100*sum(y_test == y_predict)\/len(y_test) #labal \u0e02\u0e2d\u0e07 test \u0e40\u0e17\u0e35\u0e22\u0e1a\u0e01\u0e31\u0e1a label \u0e08\u0e23\u0e34\u0e07\u0e46\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e19\u0e31\u0e49\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39 accuaracy","6063935e":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative(0)', 'positive(1)']\nC = confusion_matrix(y_test,y_predict) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\"f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","292fc6c5":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predict, target_names=target_names))","f3d38733":"import tensorflow as tf\ndef create_model():\n    tf.random.set_seed(0)\n    tf.compat.v1.reset_default_graph() # Clear Model\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Dense(12, activation='relu', input_shape=(11,)),\n      tf.keras.layers.Dense(6, activation='relu'),\n      tf.keras.layers.Dense(4, activation='relu'),\n      tf.keras.layers.Dense(2, activation='relu'),\n      tf.keras.layers.Dense(1, activation='sigmoid')  \n    ])\n    return model","428f5510":"model = create_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n\n#\u0e43\u0e0a\u0e49 algorithm \u0e0a\u0e37\u0e48\u0e2d adam\n#\u0e43\u0e0a\u0e49 binary_crossentropy \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07 label \u0e21\u0e35\u0e41\u0e04\u0e48 2 \u0e04\u0e48\u0e32","5dd60244":"train_acc = list()\nval_acc = list()\nfor i in range(0,140):\n  history = model.fit(X_train_norm, y_train, epochs= 1, batch_size = 300, validation_data= (X_val_norm, y_val))\n  tmp_avg = np.mean(history.history['accuracy'])\n  tmp_avg_val = np.mean(history.history['val_accuracy'])\n  train_acc.append(tmp_avg)\n  val_acc.append(tmp_avg_val)\n\n#batch_size \u0e04\u0e37\u0e2d \u0e40\u0e2d\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b train \u0e17\u0e35\u0e25\u0e30\u0e40\u0e17\u0e48\u0e32\u0e44\u0e2b\u0e23\u0e48","ceda9feb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nplt.figure(num=None, figsize=(16, 8), dpi=90, facecolor='w', edgecolor='k')\nplt.plot()\nplt.plot(train_acc)\nplt.plot(val_acc)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","5e64f59a":"y_predict = np.round(model.predict(X_test_norm))\ny_predict = [i[0] for i in y_predict.tolist()]\nsum(y_predict == y_test)\/len(y_test)","6d9c0820":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = SVC(random_state=1, probability=True)\nclf4 = ExtraTreesClassifier(random_state=1)\n\neclf = VotingClassifier(\n        estimators=[('lr', clf1), \n                    ('rf', clf2), \n                    ('svm', clf3),\n                    ('ET',clf4)],\n        voting='soft')\n\nparams = {'lr__C': [1.0, 100.0], \n          'rf__n_estimators': [20, 200],\n          'svm__C': [1, 2, 4, 8, 16, 32],\n          'ET__n_estimators': [10,20,50,100,200],\n    \n          }\ngrid = GridSearchCV(estimator=eclf, param_grid=params,  cv=10)\ngrid = grid.fit(X_train,y_train)","71d88ca7":"from sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.svm import SVC  #classifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    #('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',SVC(random_state=0))#classifier\n])\n\n\n#Tune GridSearchCV\nparams = { \n    #'feature_selection__k':[3,5,7],\n    'classification__C': [1,2,4,8,16,32],\n    'classification__gamma' :[0.0625,0.0125,0.025,0.05,0.01,0,1,2,4,8,16,32]\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_train, y_train)","8feffce9":"best_clf.best_params_","53004e55":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","27f31f28":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","2d4efaac":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in (cf\/cf.astype(np.float).sum(axis=0)).flatten()]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","4ffc2d5c":"from sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test, yp)\nmake_confusion_matrix(cf_matrix, cmap='Blues', categories=labelnames)","e78baefd":"# **SVM**","ad7f792e":"# **Voting**","fcf3fae4":"---------------------------------------------------------------------------","d3c60e72":"# **SKLEARN PIPELINE**","0abb8472":"# **Extra Tree**","004a0711":"-----------------------------------------------------------------------------","b8380f09":"# **Random Forest**","0004ace6":"# **Logistic Regression**","5386e4eb":"---------------------------------------------------------------------------------------------------","ef5961f8":"# **Preprocessing**","c2625a9e":"# **Deep learing: CNN**","0249cb85":"---------------------------------------------------------------"}}