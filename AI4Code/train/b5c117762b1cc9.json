{"cell_type":{"85a605d8":"code","a98bbf58":"code","393cafd0":"code","2f6d6aca":"code","47e269b0":"code","da8d74cb":"code","65f6d988":"code","09b9d847":"code","b9cee73b":"code","9b2a2b92":"code","015fe2c2":"code","0f45581c":"markdown"},"source":{"85a605d8":"\"\"\"Linear_Regression_TF_2.0.ipynb\n\n\"\"\"## Import the required Modules\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport tensorflow as tf\nfrom tensorflow import keras as ks\nfrom tensorflow.estimator import LinearRegressor\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(tf.__version__)\n","a98bbf58":"\"\"\"## Load and configure the Boston Housing Dataset\"\"\"\n\nboston_load = datasets.load_boston()\n\nfeature_columns = boston_load.feature_names\ntarget_column = boston_load.target\n\nboston_data = pd.DataFrame(boston_load.data, columns=feature_columns).astype(np.float32)\nboston_data['MEDV'] = target_column.astype(np.float32)\n\nboston_data.head()\n","393cafd0":"\n\"\"\"## Checking the relation between the variables using Pairplot and Correlation Graph\"\"\"\n\nsb.pairplot(boston_data, diag_kind=\"kde\", height=3, aspect=0.6)\n\ncorrelation_data = boston_data.corr()\ncorrelation_data.style.background_gradient(cmap='coolwarm', axis=None)","2f6d6aca":"\n\"\"\"## Descriptive Statistics - Central Tendency and Dispersion\"\"\"\n\nstats = boston_data.describe()\nboston_stats = stats.transpose()\nboston_stats\n","47e269b0":"\"\"\"## Select the required columns\"\"\"\n\nX_data = boston_data[[i for i in boston_data.columns if i not in ['MEDV']]]\nY_data = boston_data[['MEDV']]","da8d74cb":"\"\"\"## Train Test Split\"\"\"\n\ntraining_features , test_features ,training_labels, test_labels = train_test_split(X_data , Y_data , test_size=0.2)\n\nprint('No. of rows in Training Features: ', training_features.shape[0])\nprint('No. of rows in Test Features: ', test_features.shape[0])\nprint('No. of columns in Training Features: ', training_features.shape[1])\nprint('No. of columns in Test Features: ', test_features.shape[1])\n\nprint('No. of rows in Training Label: ', training_labels.shape[0])\nprint('No. of rows in Test Label: ', test_labels.shape[0])\nprint('No. of columns in Training Label: ', training_labels.shape[1])\nprint('No. of columns in Test Label: ', test_labels.shape[1])\n\nstats = training_features.describe()\nstats = stats.transpose()\nstats\n\nstats = test_features.describe()\nstats = stats.transpose()\nstats\n","65f6d988":"\"\"\"## Normalize Data\"\"\"\n\ndef norm(x):\n  stats = x.describe()\n  stats = stats.transpose()\n  return (x - stats['mean']) \/ stats['std']\n\nnormed_train_features = norm(training_features)\nnormed_test_features = norm(test_features)","09b9d847":"\"\"\"## Build the Input Pipeline for TensorFlow model\"\"\"\n\ndef feed_input(features_dataframe, target_dataframe, num_of_epochs=10, shuffle=True, batch_size=32):\n  def input_feed_function():\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features_dataframe), target_dataframe))\n    if shuffle:\n      dataset = dataset.shuffle(2000)\n    dataset = dataset.batch(batch_size).repeat(num_of_epochs)\n    return dataset\n  return input_feed_function\n\ntrain_feed_input = feed_input(normed_train_features, training_labels)\ntrain_feed_input_testing = feed_input(normed_train_features, training_labels, num_of_epochs=1, shuffle=False)\ntest_feed_input = feed_input(normed_test_features, test_labels, num_of_epochs=1, shuffle=False)","b9cee73b":"\"\"\"## Model Training\"\"\"\n\nfeature_columns_numeric = [tf.feature_column.numeric_column(m) for m in training_features.columns]\n\nlinear_model = LinearRegressor(feature_columns=feature_columns_numeric, optimizer='RMSProp')\n\nlinear_model.train(train_feed_input)","9b2a2b92":"\"\"\"## Predictions\"\"\"\n\ntrain_predictions = linear_model.predict(train_feed_input_testing)\ntest_predictions = linear_model.predict(test_feed_input)\n\ntrain_predictions_series = pd.Series([p['predictions'][0] for p in train_predictions])\ntest_predictions_series = pd.Series([p['predictions'][0] for p in test_predictions])\n\ntrain_predictions_df = pd.DataFrame(train_predictions_series, columns=['predictions'])\ntest_predictions_df = pd.DataFrame(test_predictions_series, columns=['predictions'])\n\ntraining_labels.reset_index(drop=True, inplace=True)\ntrain_predictions_df.reset_index(drop=True, inplace=True)\n\ntest_labels.reset_index(drop=True, inplace=True)\ntest_predictions_df.reset_index(drop=True, inplace=True)\n\ntrain_labels_with_predictions_df = pd.concat([training_labels, train_predictions_df], axis=1)\ntest_labels_with_predictions_df = pd.concat([test_labels, test_predictions_df], axis=1)","015fe2c2":"\"\"\"## Validation\"\"\"\n\ndef calculate_errors_and_r2(y_true, y_pred):\n  mean_squared_err = (mean_squared_error(y_true, y_pred))\n  root_mean_squared_err = np.sqrt(mean_squared_err)\n  r2 = round(r2_score(y_true, y_pred)*100,0)\n  return mean_squared_err, root_mean_squared_err, r2\n\ntrain_mean_squared_error, train_root_mean_squared_error, train_r2_score_percentage = calculate_errors_and_r2(training_labels, train_predictions_series)\ntest_mean_squared_error, test_root_mean_squared_error, test_r2_score_percentage = calculate_errors_and_r2(test_labels, test_predictions_series)\n\nprint('Training Data Mean Squared Error = ', train_mean_squared_error)\nprint('Training Data Root Mean Squared Error = ', train_root_mean_squared_error)\nprint('Training Data R2 = ', train_r2_score_percentage)\n\nprint('Test Data Mean Squared Error = ', test_mean_squared_error)\nprint('Test Data Root Mean Squared Error = ', test_root_mean_squared_error)\nprint('Test Data R2 = ', test_r2_score_percentage)","0f45581c":"<h1 style=\"color:brown\">Hello World of Neural Networks - Linear Regression w\/TensorFlow 2.0"}}