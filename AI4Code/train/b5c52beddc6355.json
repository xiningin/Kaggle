{"cell_type":{"ae80fbed":"code","ec7501ca":"code","d8937a41":"code","01da5db1":"code","a3281703":"code","f92e061b":"code","fbcd6fd8":"code","f36e3dbe":"code","9f1ecc96":"code","200f9228":"code","33f6a386":"code","cf91b228":"code","82361e48":"code","8d6b1dba":"code","4620f5ca":"code","d27d85b4":"code","ee71d04b":"code","7a04951a":"code","ed48eeb1":"code","6cfb28bc":"markdown","058d1021":"markdown","e401005d":"markdown","1674a138":"markdown","dd8b615f":"markdown","4cd2dadc":"markdown","1290f74e":"markdown","4f5f44e8":"markdown","8afdae57":"markdown","24aad877":"markdown","2af1a1a0":"markdown"},"source":{"ae80fbed":"import os\nimport sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,KFold\nimport category_encoders as ce\n\n\n#ml algo\nfrom catboost import CatBoostRegressor as catr\n\n#metrics\nfrom sklearn.metrics import mean_absolute_error as mae\n\n#optimization\nimport optuna\nfrom optuna import Trial","ec7501ca":"train=pd.read_csv('..\/input\/analytics-vidhya-jobathon-september-2021\/train.csv')\ntest=pd.read_csv('..\/input\/analytics-vidhya-jobathon-september-2021\/test.csv')\nsample_sub=pd.read_csv('..\/input\/analytics-vidhya-jobathon-september-2021\/sample_submission.csv')","d8937a41":"id_cols=['ID']\ntrain_ids=train[id_cols]\ntest_ids =test[id_cols]\n\ntrain.drop(id_cols,inplace=True,axis=1)\ntest.drop(id_cols,inplace=True,axis=1)","01da5db1":"#target variable\ny=train.pop('Sales')","a3281703":"class feature_processor:\n    \n    def __init__(self,\n                seed,\n                train,\n                test,\n                target):\n        self.seed=seed\n        self.train=train\n        self.test=test\n        self.target=target\n        \n    def get_categorical_var(self): \n        '''returns categorical columns in data'''\n        df=self.train\n        cat_cols=[col for col in df.columns if df[col].dtype=='object']\n        self.categorical=cat_cols\n        \n        return self.categorical\n    \n    def encode_and_bind(self,original_dataframe, feature_to_encode):\n        '''one hot encode a feature'''\n        dummies = pd.get_dummies(original_dataframe[feature_to_encode])\n        original_dataframe.drop(feature_to_encode,axis=1,inplace=True)\n        res = pd.concat([original_dataframe, dummies], axis=1)\n        return res\n\n        ","f92e061b":"\ndef replace_binary(self,df):\n    '''replace the yes\/no with 1\/0'''\n    df['Discount']=df['Discount'].replace({'Yes':1,'No':0})\n    \n    return df\n\n# add function to class\nfeature_processor.replace_binary=replace_binary","fbcd6fd8":"def encode_timeseries(self,\n                      df,\n                      frmt='%Y-%m-%d'):\n    \n    '''convert the datetime into features'''\n    \n    df['Date']=pd.to_datetime(df['Date'],format=frmt)\n    \n    df['year']=df['Date'].dt.year\n    df['month']=df['Date'].dt.month\n    df['day']=df['Date'].dt.day\n    \n    # encoding the months\n    df['cos_'+'month']=np.cos(1\/2.*np.pi* (df['month']\/12))\n    df['sin_'+'month']=np.sin(1\/2.*np.pi*(df['month']\/12))\n    \n    #encoding day\n    df['cos_'+'day']=np.cos(1\/2.*np.pi*(df['day']\/30))\n    df['sin_'+'day']=np.sin(1\/2.*np.pi*(df['day']\/30))\n    \n    #encoding years\n    base_year=df['year'].min()\n    \n    #calculating number of  years from base year\n    df['base_plus_years']=df['year']-base_year\n    \n    #dropping columns:\n    cols_to_drop=['Date','year','month','day']\n    \n    \n    df.drop(cols_to_drop,\n            inplace=True,\n            axis=1)\n    \n    \n     \n    return df\n    \n\n# add function to class\nfeature_processor.encode_timeseries=encode_timeseries","f36e3dbe":"def location_features(self,df,df_test,target):\n    df['location'] = df['Region_Code'] + df['Location_Type']\n    df.drop(['Location_Type','Region_Code'],axis=1,inplace=True)\n    \n    \n    df_test['location'] = df_test['Region_Code'] + df_test['Location_Type']\n    df_test.drop(['Location_Type','Region_Code'],axis=1,inplace=True)\n    \n    \n    te=ce.TargetEncoder(cols=['location'])\n    te.fit_transform(df,target)\n    te.transform(df_test)\n    \n    return df,df_test\n\n# add function to class\nfeature_processor.location_features=location_features","9f1ecc96":"def predict_order(self):\n    '''#Order variable is missing from test set, predicting order variable.'''\n    x=self.train\n    y=x['#Order']\n    \n    x=x.drop('#Order',axis=1)\n    \n    x_test=self.test\n    \n    model=catr(verbose=0,\n               cat_features=[col for col in x.columns if x[col].dtype=='object'])\n    model.fit(x,y)\n    \n    pred=model.predict(self.test)\n    \n    x_test['#Order']=pred\n    \n    return x_test\n       \nfeature_processor.predict_order=predict_order","200f9228":"def transform(self):\n    '''transforms the train\/test df applying the class methods and return train and test datasets'''\n    self.train=self.replace_binary(self.train)\n    self.test=self.replace_binary(self.test)\n    \n    self.train=self.encode_timeseries(self.train)\n    self.test=self.encode_timeseries(self.test)\n   \n    \n    self.test=self.predict_order()    \n     \n    self.train,self.test=self.location_features(self.train,self.test,self.target)\n    \n    #one hot encode\n    self.train=self.encode_and_bind(self.train,'Store_Type')\n    self.test=self.encode_and_bind(self.test,'Store_Type')\n  \n  \n    return self.train,self.test    \n\n# add function to class\nfeature_processor.transform=transform","33f6a386":"preprocessor = feature_processor(7,train,test,y)\nX,X_test=preprocessor.transform()\n\n#categorical features\ncategorical_columns=preprocessor.get_categorical_var()\ncategorical_columns.append('Store_id')\n\nseed=preprocessor.seed","cf91b228":"def objective(trial:Trial):\n    \n    #splitting training data \n    x_train,x_test,y_train,y_test=train_test_split(X,y,\n                                                   random_state=seed,\n                                                   shuffle=True,\n                                                   train_size=0.7)\n    \n    #hyperparam_grid\n    params={   'verbose'        : 0,\n               'loss_function'  :'MAE',\n               'depth'          :trial.suggest_int('depth',4,10),\n               'learning_rate'  :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n               'l2_leaf_reg'    :trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n               'random_strength':trial.suggest_uniform('random_strength',1e-2,0.3),\n               'max_bin'        :trial.suggest_int('max_bin',64,254),\n#                'grow_policy'    :trial.suggest_categorical('grow_policy',\n#                                                            ['SymmetricTree','Depthwise','Lossguide']),\n#                'iterations'     :trial.suggest_int('iterations',1000,2000),\n#                'max_leaves'     :trial.suggest_int('max_leaves',2,64),\n               \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 0.8),\n               \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n               \"bootstrap_type\":trial.suggest_categorical(\"bootstrap_type\",[\"Bayesian\", \"MVS\",'Bernoulli']),\n               'eval_metric': 'MAE'\n                }\n    \n    \n    try:\n        model = catr(**params)\n\n        model.fit(x_train,y_train,\n                 eval_set=[(x_test,y_test)],\n                 verbose=0,\n                 cat_features=categorical_columns,\n                 early_stopping_rounds=300)\n\n        \n        preds=model.predict(x_test)\n        mean_ab_er= mae(y_test,preds)\n        \n        return mean_ab_er\n\n    except Exception as e:\n        print(e)\n        return None\n\n\n","82361e48":"#optimize\n\ndef get_best_params(time_out=5000):\n    '''time_out: time out in seconds'''\n    sampler = optuna.samplers.TPESampler(seed=7)  # Make the sampler behave in a deterministic way.\n    study=optuna.create_study(direction='minimize',sampler=sampler)\n    study.optimize(objective, n_trials=300, timeout=time_out)\n    \n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    \n    return study.best_trial.params\n\n\n# best_params=get_best_params()","8d6b1dba":"best_params={'depth': 8, \n             'learning_rate': 0.07938739115493117, \n             'l2_leaf_reg': 6.860271784664216,\n             'random_strength': 0.13642569580204972,\n             'max_bin': 243, \n             'colsample_bylevel': 0.7361186089688933,\n             'boosting_type': 'Plain',\n             'bootstrap_type': 'Bernoulli'}","4620f5ca":"best_params['verbose']=0\nbest_params['loss_function']='MAE'\nbest_params['eval_metric']='MAE'\nbest_params['iterations']=1000\n","d27d85b4":"def k_fold_predict(k,\n                  params=best_params):\n    \n    \n    skf=KFold(n_splits=k,\n               shuffle=True,\n               random_state=7)\n\n    mean_preds=np.zeros(shape=(X_test.shape[0]))\n    train_check=np.zeros(shape=(X.shape[0]))   \n    \n    i=0\n    for train_idx,val_idx in skf.split(X,y):\n        x_t,x_v=X.iloc[train_idx],X.iloc[val_idx]\n        y_t,y_v=y.iloc[train_idx],y.iloc[val_idx]\n        \n        model=catr(**params)        \n        model.fit(x_t,y_t,\n                 cat_features=categorical_columns\n                 )\n        \n        \n        print('Validation AUC score for fold {} = {}'.format(i,\n                                                         mae(\n                                                            y_v,\n                                                            model.predict(x_v))))\n        i+=1\n        \n        #test predictions\n        test_p=model.predict(X_test)\n        mean_preds+=test_p\n        \n        \n        #training preds\n        train_p=model.predict(X)\n        train_check+=train_p\n        \n        \n    mean_preds=mean_preds\/k\n\n\n    train_check=train_check\/k\n    \n    return mean_preds,train_check\n\n","ee71d04b":"# Training and prediction on folds\n\n# %%time\ntest_pred,train_pred=k_fold_predict(3)\n\npreds=pd.DataFrame(test_pred,columns=['Sales'])\ntrain_preds=pd.DataFrame(train_pred,columns=['Sales'])\n\npreds['ID']=test_ids['ID']\ntrain_preds['ID']=train_ids['ID']\n\n","7a04951a":"preds.to_csv('submission.csv',index=False)","ed48eeb1":"preds.head()","6cfb28bc":"# Applying preprocessing","058d1021":"**Location related features**","e401005d":"# Loading Data ","1674a138":"**Replacing Y\/N with 1\/0**","dd8b615f":"# Feature Creation and Preprocessing","4cd2dadc":"**No missing values, so we can proceed for further preprocessing**","1290f74e":"# Hyperparam optimization","4f5f44e8":"# Prediction and submission","8afdae57":"# Data Description\n\n**Variable Definition**\n\n* ID :  Unique Identifier for a row\n* Store_id:  Unique id for each Store\n* Store_Type:  Type of the Store\n* Location_Type: Type of the location where Store is located\n* Region_Code:  Code of the Region where Store is located\n* Date: Information about the Date\n* Holiday: If there is holiday on the given Date, 1 : Yes, 0 : No\n* Discount: If discount is offered by store on the given Date, Yes\/ No\n* #Orders: Number of Orders received by the Store on the given Day\n* Sales: Total Sale for the Store on the given Day","24aad877":"**Time Series Features**","2af1a1a0":"# Model Fitting and Prediction\n"}}