{"cell_type":{"dc278165":"code","e1679f4a":"code","efbf4b00":"code","deeaa928":"code","1b6f4c78":"code","912fcebb":"code","88525520":"code","a548cab4":"code","9c04fb20":"code","d28cc0d9":"code","602df0d7":"code","60581bc1":"code","05d78e2e":"code","712f9c38":"code","10336d57":"code","f7a57787":"code","73cd6f45":"code","ecd4c51f":"code","32b71f45":"code","924d9a4b":"code","9eec6e18":"code","2bc88cd2":"code","85e52eae":"code","8a906654":"markdown","95e8a0bf":"markdown","bd6eda9f":"markdown","8f5a9368":"markdown","59129789":"markdown","85ef87d6":"markdown","30769a0d":"markdown","1d7ca2a7":"markdown","62d37b55":"markdown","ba70ec98":"markdown","9da51522":"markdown","8e54216c":"markdown","f1ef6ca4":"markdown","d262556f":"markdown","fa1ec469":"markdown","f56edd6c":"markdown","81be2580":"markdown","f807652c":"markdown","a1b1a998":"markdown","2001c261":"markdown"},"source":{"dc278165":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# plt.style.use('ggplot')\nsns.set_theme(style=\"whitegrid\")","e1679f4a":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv', index_col='enrollee_id')\nX_test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv', index_col='enrollee_id')\ntrain.shape, X_test.shape","efbf4b00":"train.head()","deeaa928":"missing_values = train.count().sum()\ntotal_values = np.product(train.shape)\npercentages = train.isna().sum().reset_index().rename(columns={'index': 'Column', 0:'Missing'})\npercentages['Percentage'] = percentages['Missing']\/train.shape[0]*100\nprint(f'Amount of total missing data in train set: {missing_values}\\nRelative amount of missing data: {missing_values\/total_values*100:.3f}%')\nprint('-'*20)\nprint(f'Missing data per column:\\n\\n',percentages)","1b6f4c78":"train.city.nunique()","912fcebb":"train.city_development_index.describe()","88525520":"def plot(data, column):\n    fig, ax = plt.subplots(1,2, figsize=(12,6))\n    perc = percentages.query(f'Column == \"{column}\"')['Percentage'].values\n    sns.countplot(data=data, x=column, ax=ax[0])\n    ax[1].pie(train[column].value_counts().values, labels=train[column].value_counts().index, autopct='%1.1f%%')\n    fig.tight_layout()\n    fig.suptitle(f'Column: \\'{column}\\'   -   Missing data: {perc[0]:.2f}%', fontsize=30)\n    fig.subplots_adjust(top=0.88)\n    fig.show()\nplot(train, 'gender')","a548cab4":"plot(train, 'relevent_experience')","9c04fb20":"plot(train, 'enrolled_university')","d28cc0d9":"plot(train, 'education_level')\n# pd.crosstab(index=train.education_level, columns=train.enrolled_university)","602df0d7":"plot(train, 'major_discipline')","60581bc1":"plot(train, 'experience')","05d78e2e":"plot(train, 'company_size')","712f9c38":"plot(train, 'company_type')","10336d57":"plot(train, 'last_new_job')","f7a57787":"train.training_hours.head(10)","73cd6f45":"class MapTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Encodes the following ordinal variables: \n       \n           relevant_experience\n           education_level\n           experience\n           last_new_job\n       \n       Returns:\n        encoded variables\n       \"\"\"\n    def __init__(self):\n        self.rel_ex_mapping = {'Has relevent experience':1, 'No relevent experience':0}\n        self.ed_level_mapping = {'Primary School': 0, 'High School':1, 'Graduate':2, 'Masters':3, 'Phd':4}\n        self.ex_mapping = {'<1':0, '>20':21}\n        self.last_n_job_mapping = {'never':0, '>4':5}\n    \n    def fit(self, x, y=None):\n        return self\n    \n    def transform(self, x, y=None):\n        df = pd.DataFrame(x, columns=['relevent_experience', 'education_level', 'experience', 'last_new_job'])\n        df['relevent_experience'] = df['relevent_experience'].replace(self.rel_ex_mapping)\n        df['education_level'] = df['education_level'].replace(self.ed_level_mapping)\n        df['experience'] = df['experience'].replace(self.ex_mapping).astype(float)\n        df['last_new_job'] = df['last_new_job'].replace(self.last_n_job_mapping).astype(float)\n        df['experience_per_job'] = df['experience'] \/ [x+1 for x in df['last_new_job']]\n        \n        return df\n    \n    def get_feature_names(self):\n        return ['relevent_experience', 'education_level', 'experience', 'last_new_job', 'experience_per_job']","ecd4c51f":"# DROPPING\nfor x in [train, X_test]:\n    x.drop(columns=['city', 'company_size', 'company_type'], inplace=True)\n    x.dropna(axis=0, subset=['experience'], inplace=True)\n    \nX_train = train.loc[:, train.columns != 'target']\ny_train = train.pop('target')\n    \n# PIPELINE\/COLUMNTRANSFORMER\npipeline_imp_ohe = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npipeline_imp_map = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('mapper', MapTransformer())\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('imp_ohe', pipeline_imp_ohe, ['gender', 'enrolled_university', 'major_discipline']),\n    ('imp_map', pipeline_imp_map, ['relevent_experience', 'education_level', 'experience', 'last_new_job']),\n], remainder='passthrough')\n\n\n# DEMO OF WHAT DATA LOOKS LIKE AFTER PREPROCESSING\ndemo = preprocessor.fit_transform(X_train)\npd.DataFrame(demo)","32b71f45":"# HYPERPARAMETER TUNING\n# params = {'model__n_estimators': [100, 300, 500, 800],\n#           'model__max_depth': [5, 8, 15, 25, None], \n#           'model__min_samples_split':[2, 5, 10, 15, 100],\n#           'model__min_samples_leaf': [1, 2, 5, 10] } \n\n# randomforest_pipeline = Pipeline([\n#     ('preprocessor', preprocessor),\n#     ('model', RandomForestClassifier())\n# ])\n# randomforest_pipeline.fit(X_train, y_train)\n\n# clf = GridSearchCV(randomforest_pipeline, params, cv=3, verbose=1, n_jobs=-1, scoring='accuracy')\n# scores_m1 = clf.fit(X_train, y_train)\n\n# print(f'Best accuracy: {scores_m1.best_score_:.3f}%')\n# print(f'Best Config: {scores_m1.best_params_}')\n\n\n#OUTCOME\nrandomforest_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', RandomForestClassifier(n_estimators= 500, max_depth= 8, min_samples_leaf=1, min_samples_split=2))\n])\nscores_m1 = cross_val_score(randomforest_pipeline, X_train, y_train, cv=3)","924d9a4b":"# # HYPERPARAMETER TUNING\n# params = {'model__C':[0.1,1,10],\n#           'model__kernel':['sigmoid'],\n#           'model__degree':[2,3,5],\n#           'model__gamma': [1, 0.1, 0.01] }\n\n# svmpipeline = Pipeline([\n#     ('preprocessor', preprocessor),\n#     ('model', SVC())\n# ])\n# svmpipeline.fit(X_train, y_train)\n\n# clt = GridSearchCV(svmpipeline, params, cv=3, verbose=1, n_jobs=-1, scoring='accuracy')\n# scores_m2 = clt.fit(X_train, y_train)\n\n# print(f'Best accuracy: {scores_m2.best_score_:.3f}%')\n# print(f'Best Config: {scores_m2.best_params_}')\n\n# #OUTCOME\nsvmpipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', SVC(C= .1, kernel= 'rbf', degree=5, gamma=0.1))\n])\nscores_m2 = cross_val_score(svmpipeline, X_train, y_train, cv=3)\nscores_m2.mean()","9eec6e18":"# HYPERPARAMETER TUNING\n# params = {'model__learning_rate':[0.5,0.1,0.05, 0.01],\n#           'model__n_estimators':[100, 300, 500, 800],\n#           'model__max_depth':[3,5,8] }\n# fit_params = {'early_stopping_rounds': 5}\n\n# xgbpipeline = Pipeline([\n#     ('preprocessor', preprocessor),\n#     ('model', XGBClassifier())\n# ])\n# xgbpipeline.fit(X_train, y_train)\n\n# clt = GridSearchCV(xgbpipeline, params, cv=3) #fit_params=fit_params)\n# scores_m3 = clt.fit(X_train, y_train)\n\n# print(f'Best accuracy: {scores_m3.best_score_:.3f}%')\n# print(f'Best Config: {scores_m3.best_params_}')\n\n#OUTCOME\nxgbpipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', XGBClassifier(learning_rate= 0.1, n_estimators= 100, max_depth=3, use_label_encoder=False))])\nscores_m3 = cross_val_score(xgbpipeline, X_train, y_train, cv=3)","2bc88cd2":"print(f'RESULTS:\\nRandom Forest Classifier: {scores_m1.mean():.3f}% accuracy')\nprint(f'Support Vector Classifier: {scores_m2.mean():.3f}% accuracy')\nprint(f'XGBoost Classifier: {scores_m3.mean():.3f}% accuracy')","85e52eae":"sample_submission = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv')\n\nxgbpipeline.fit(X_train, y_train)\nsolutions = xgbpipeline.predict(X_test)\n\nX_test['target'] = solutions\nsubmission = X_test.reset_index()[['enrollee_id', 'target']]\n\nsubmission.to_csv('submission.csv',index=False)","8a906654":"# Preprocessing\n\nI will first take a look at the distribution of the data, and then create a ColumnTransformer in the end that will handle the transformations","95e8a0bf":"* same as before\/above for missing values\n* Encoding from Primary School (-> 0) to Phd (-> 4)","bd6eda9f":"# Import and first overview","8f5a9368":"* continuous value with no missing data, leave it as is","59129789":"* a lot of data missing, and imputing would introduce a lot of bias. Let's drop the whole column, as this probably doesn't correlate with our target value too much aswell (company size seems to be more a personal preference)","85ef87d6":"* Most Frequent Imputation not the best, but let's give it a try\n* convert 'never' to 0 and '>4' to 5 and treat it as a numerical variable","30769a0d":"* very balanced column counts, and also few data missing plus I have a feeling this column might be important. Dropping the rows with missing data is probably the best, as imputation will be very biased (its also only 65 datapoints)\n* convert <1 to 0 and >20 to 21, and treat it as a numeric variable","1d7ca2a7":"* since this is a numerical variable that is already between 0 and 1, we shouldn't worry about it","62d37b55":"## Create the ColumnTransformer\n\nI will also add another feature that might be of high importance for prediction:<br>'Experience per last new jobs' - This measures how long a employee stays at his job on average<br>(this assumes that Experience=Years of having a job)","ba70ec98":"* a lot of missing values, but the data is also very male dominated, so most frequent imputation should not add too much bias\n* One Hot Encoding should be the way to go","9da51522":"* same as above, although Pvt Ltd is dominant - lets still drop it","8e54216c":"* Target Encoding seems reasonable here, since there can be a ordering.<br> 'Has relevant experience' (-> 1) is better to have in a job than not (-> 0) ","f1ef6ca4":"# Submission","d262556f":"* lets just do most frequent imputation to not delete the data, I don't think the values in this column matter too much\n* One Hot Encoding","fa1ec469":"<h2 style='text-align: center'>That you for reading this notebook to the end!<br>Feel free to upvote and leave a comment.<\/h2><h4 style='text-align: center'>Also please tell me what I could've done better...<h4>","f56edd6c":"# Model 1: Random Forest Classifier","81be2580":"* way too many unique variables for One Hot Encoding, and data is probably rather useless anyways. Let's drop the column","f807652c":"# Model 2: Support Vector Classifier","a1b1a998":"* a lot of data is missing, but the STEM is already by far the most prominent value, so most frequent imputation won't add a lot of bias to the data\n* One Hot Encoding","2001c261":"\n\n\n\n# Model 3: XGBoost Classifier"}}