{"cell_type":{"5967beda":"code","cd63b206":"code","352993df":"code","d1ec4e75":"code","bb3fa5b9":"code","2708a22c":"code","073d3e50":"code","3d7949ae":"code","3af01d5e":"code","2347ad88":"code","6ff3e6a5":"code","73ffd6e6":"code","379b1c05":"code","08b3092d":"code","65ea2edf":"code","393f155c":"code","1cbb8362":"code","314b04f4":"code","cbf68b51":"code","ff5ceaec":"code","ff90051c":"code","f998644c":"code","cfe2e2ea":"code","1d3fd7c7":"code","6326a1cc":"code","2176f141":"code","2ede8ad5":"code","6100cd70":"code","c18d8e77":"code","3d1e8110":"code","826c4fe6":"code","30a965ce":"code","22faa2e9":"code","bafa7d02":"code","f2c3e089":"code","cffca3bc":"code","93207fb5":"code","d872e943":"markdown","43ec0911":"markdown","214770c8":"markdown","fc2063cc":"markdown","6b98684c":"markdown","885b65a1":"markdown","73f297ec":"markdown","77fbca4c":"markdown","e43d7d1c":"markdown","b3f5893f":"markdown","9a5a55cc":"markdown","142c41ef":"markdown","c864fc93":"markdown","6758a6e0":"markdown","f256c941":"markdown","1473de82":"markdown"},"source":{"5967beda":"# import packages and libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import shapiro\n%matplotlib inline","cd63b206":"# Read file\ndf = pd.read_csv(\"..\/input\/obesity-levels\/ObesityDataSet_raw_and_data_sinthetic.csv\")","352993df":"# Top 5 rows show survey data\ndf.head(5)","d1ec4e75":"# Bottom 5 rows show synthetic data\ndf.tail(5)","bb3fa5b9":"# Additional rows showing synthetic data\ndf.iloc[[501,518,516]]","2708a22c":"# Height and weight are highly correlated and they directly correlate to the BMI calc used for the target\n# Remove Height and Weight\ndf = df.drop(columns=['Height', 'Weight'])\nprint(df.shape)","073d3e50":"# no nulls \ndf[df.isnull().any(axis=1)]","3d7949ae":"# Convert object\/text variables to category variables\ncolumns = [\"Gender\", \"family_history_with_overweight\", \"FAVC\", \"CAEC\", \"SMOKE\", \"SCC\", \"CALC\", \"MTRANS\", \"NObeyesdad\"]\n\nfor col in columns:\n    df[col] = df[col].astype('category')","3af01d5e":"# function to interigate data after conversion\n# provides min, max, unique counts\ndef variable_counts(columns, stage):\n\n    if stage == 'pre':\n        print(\"Pre Conversion to Integer\")\n    else:\n        print(\"Post Conversion to Integer\")\n\n    for col in columns:    \n        print(\"Variable:\", col, \"| Count Unique:\",df[col].nunique(),\"| Min: \", df[col].min(), \"| Max: \",df[col].max())","2347ad88":"# Convert float variables to integer to the nearest inter\ncolumns = [\"FCVC\", \"NCP\", \"CH2O\", \"TUE\", \"FAF\"]\n\n# pre conversion countss\nvariable_counts(columns, 'pre')\n\n# convert to int \/ nearest int value\nfor col in columns:\n    #round to nearest whole number\n    df[col] = round(df[col]).astype('int')  \n    \n# post conversion counts\nprint(\"\")\nvariable_counts(columns, 'post')","6ff3e6a5":"# confirm types\ndf.info()","73ffd6e6":"# review non synthetic are still the same\ndf.head()","379b1c05":"# columns of interest\ncolumns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE',\n           'SCC', 'CALC', 'MTRANS', 'NObeyesdad']\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 10))\nfor col, subplot in zip(columns, ax.flatten()):\n    sns.countplot(df[col], ax=subplot)\n    \n    if col==\"MTRANS\":\n        sns.countplot(df[col],ax=subplot)\n        subplot.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.MTRANS)        \n        subplot.yaxis.label.set_text(\"Number of Records\")\n    elif col==\"NObeyesdad\":\n        sns.countplot(df[col],ax=subplot)\n        subplot.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.NObeyesdad)  \n        subplot.yaxis.label.set_text(\"Number of Records\")\n    else:\n        sns.countplot(df[col],ax=subplot)  \n        subplot.yaxis.label.set_text(\"Number of Records\")\n        \n# show figure & plots\nfig.suptitle(\"Categorigal Variables\", fontsize=20)\nplt.tight_layout(pad=5, w_pad=0.0, h_pad=1)\nplt.show()","08b3092d":"# columns of interest\ncolumns = [\"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n\nfig, ax = plt.subplots(1, 5, figsize=(15, 4))\nfor col, subplot in zip(columns, ax.flatten()):\n    sns.countplot(df[col], ax=subplot)\n    subplot.yaxis.label.set_text(\"Number of Records\")\n\n# show figure & plots\nfig.suptitle(\"Ordinal Variables\", fontsize=20)\nplt.tight_layout(pad=5, w_pad=0.7, h_pad=0.5)\nplt.show()","65ea2edf":"# ratio variable distribution \n\nfig = plt.figure(figsize = (16,5))\n\n#distplot\nax1 = fig.add_subplot(121)\nsns.distplot(df[\"Age\"], kde=True)\n\n#boxplot\nax1 = ax1 = fig.add_subplot(122)\nsns.boxplot(df.Age)\n\n# show figure & plots\nfig.suptitle(\"Distribution of Numeric (Ratio) Variable\", fontsize=20)\nplt.tight_layout(pad=5, w_pad=0.5, h_pad=.1)\nplt.show()","393f155c":"# create figure\nfig = plt.figure(figsize=(15, 5))\n\n# add subplot for one row 2 graphs first postion\nax1 = fig.add_subplot(121)\n\n# correlation data matrix\nmatrix = np.triu(df.corr())\n\n# set title \nax1.title.set_text(\"Coorelation Heatmap: Predictor Variables\")\n\n#define plot\nsns.heatmap(df.corr(), \n                 mask=matrix,\n                 annot = False,                 \n                 fmt='.1g', \n                 cmap=\"YlGnBu\", \n                 vmin=-1, vmax=1, center= 0,                 \n                 square=\"True\",\n                 ax=ax1)\n\n# add second subplot\nax2 = fig.add_subplot(122)\n\n# rotate axis label\nax2.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.NObeyesdad)\n\n# Set title text\nax2.title.set_text(\"Weight Category Counts: Target Variable\")\n\n# define second plot\nsns.countplot(x=\"NObeyesdad\",                  \n                 palette=\"Blues_d\", \n                 order=df.NObeyesdad.value_counts().index,\n                 ax = ax2,\n                 data=df)\n\n# labels for x and y\nax2.xaxis.label.set_text(\"Level Category\")\nax2.yaxis.label.set_text(\"Number of Records\")\n\n# turn off top and right frame lines\nax2.spines['right'].set_visible(False)\nax2.spines['top'].set_visible(False)\n\n# show figure & plots\nplt.tight_layout()\nplt.show()","1cbb8362":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n#print highly correlated variables\nprint(\"Number of variables with > 0.95 correlation: \", len(to_drop))","314b04f4":"df_prep = df.copy()","cbf68b51":"# create dummy variables\ndf_prep = pd.get_dummies(df_prep,columns=[\"Gender\",\"family_history_with_overweight\",\n                                          \"FAVC\",\"CAEC\",\"SMOKE\",\"SCC\",\"CALC\",\"MTRANS\"])\ndf_prep.head()","ff5ceaec":"# split dataset in features and target variable\n\n# Features\nX = df_prep.drop(columns=[\"NObeyesdad\"])\n\n# Target variable\ny = df_prep['NObeyesdad'] ","ff90051c":"# import sklearn packages for data treatments\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","f998644c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.preprocessing import StandardScaler # Import for standard scaling of the data\nfrom sklearn.preprocessing import MinMaxScaler # Import for standard scaling of the data\n\n# standard scale data\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)\n\n# tested MinMaxScaler as KNN historically does better with MinMax\nmm = MinMaxScaler()\nX_train_mm_scaled = ss.fit_transform(X_train)\nX_test_mm_scaled = ss.transform(X_test)\n\n# program to run multilple models though sklearn \n# Default settings output accuracy and classification report\n# compares accuracy for scaled and unscaled data\ndef run_models(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame):\n    \n    models = [          \n          ('Random Forest', RandomForestClassifier(random_state=2020)),\n          ('Decision Tree', DecisionTreeClassifier()),                                                 \n          ('KNN', KNeighborsClassifier()),\n          ('SVM', SVC())\n        ]  \n    \n    for name, model in models:        \n        # unscaled data\n        clf = model.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        \n        # scaled data\n        clf_scaled = model.fit(X_train_scaled, y_train)\n        y_pred_scaled = clf_scaled.predict(X_test_scaled)\n        \n        # mm scaled data\n        clf_mm_scaled = model.fit(X_train_mm_scaled, y_train)\n        y_pred_mm_scaled = clf_scaled.predict(X_test_mm_scaled)\n        \n        # accuracy scores\n        accuracy = round(metrics.accuracy_score(y_test, y_pred),5)\n        scaled_accuracy = round(metrics.accuracy_score(y_test, y_pred_scaled),5)\n        scaled_mm_accuracy = round(metrics.accuracy_score(y_test, y_pred_mm_scaled),5)\n        \n        # output\n        print(name + ':')        \n        print(\"---------------------------------------------------------------\")      \n        print(\"Accuracy:\", accuracy)\n        print(\"Accuracy w\/Scaled Data (ss):\", scaled_accuracy)\n        print(\"Accuracy w\/Scaled Data (mm):\", scaled_mm_accuracy)\n        if (accuracy > scaled_accuracy) and (accuracy > scaled_mm_accuracy):\n            print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred))      \n            print(\"                            -----------------------------------               \\n\")      \n        elif (scaled_accuracy > scaled_mm_accuracy):\n            print(\"\\nClassification Report (ss):\\n\", metrics.classification_report(y_test, y_pred_scaled))      \n            print(\"                            -----------------------------------               \\n\")     \n        else:            \n            print(\"\\nClassification Report (mm):\\n\", metrics.classification_report(y_test, y_pred_mm_scaled))      \n            print(\"                            -----------------------------------               \\n\")      ","cfe2e2ea":"#run Decision Trees, Random Forest, KNN and SVM\nrun_models(X_train, y_train, X_test, y_test)","1d3fd7c7":"from sklearn.model_selection import GridSearchCV\n\n#model name, classifier, parameters\n# function used to process models and parameters through gridsearch\ndef hyper_tune(name, clf, parameters, target_names=None): \n    \n    target_names = target_names\n    clf = clf\n    search = GridSearchCV(clf, parameters,verbose=True, n_jobs=15, cv=5)\n    search.fit(X_train_scaled,y_train)\n    y_pred_scaled = search.predict(X_test_scaled)\n    print (\"Accuracy Score = %3.2f\" %(search.score(X_test_scaled,y_test)))\n    print (search.best_params_)\n    print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred_scaled, target_names=target_names))\n    ","6326a1cc":"#the KNN model performs better on the unscaled data this function\n# function for unscaled data\n#model name, classifier, parameters\n# function used to process models and parameters through gridsearch\ndef hyper_tune2(name, clf, parameters, target_names=None): \n    \n    target_names = target_names\n    clf = clf\n    search = GridSearchCV(clf, parameters,verbose=True, n_jobs=15, cv=5)\n    search.fit(X_train,y_train)\n    y_pred = search.predict(X_test)\n    print (\"Accuracy Score = %3.2f\" %(search.score(X_test,y_test)))\n    print (search.best_params_)\n    print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred, target_names=target_names))","2176f141":"# Number of neighbors\nn_neighbors = [int(x) for x in range(4, 15)]\n# weights\nweights = ['uniform','distance']\n# distance metric\nmetric = ['euclidean', 'manhattan', 'chebyshev']\n# computation algorithm\nalgorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n# power paramter\np=[1,2]\n\nparameters = { 'n_neighbors': n_neighbors,\n              'weights':weights,\n              'metric':metric,\n              'p':p,\n              'algorithm': algorithm              \n               }\n\nhyper_tune2('KNN', KNeighborsClassifier(), parameters)","2ede8ad5":"# Number of trees in random forest\nn_estimators = [int(x) for x in range(10, 200,10)]\n# Criterion\ncriterion = ['gini','entropy']\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(10, 100, 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in range(2, 5)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in range(2, 5)]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# random state\nrandom_state = [1010]\n\nparameters = { 'criterion':criterion,\n               'n_estimators': n_estimators,\n              'max_depth':max_depth,\n              #'random_state': random_state,\n              #'max_features':max_features,\n              #'min_samples_split':min_samples_split             \n               }\n\n\nhyper_tune('Random Forest',\n           RandomForestClassifier(), parameters)","6100cd70":"# Create Decision Tree classifer object with optimized parameters\nclf = RandomForestClassifier(criterion='entropy',\n               n_estimators=52,\n              max_depth = 51,              \n              max_features='auto',\n              min_samples_split=2,\n              random_state=1010)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train_scaled,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test_scaled)\n","c18d8e77":"print(X.columns)","3d1e8110":"feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\nfig = plt.figure(figsize=(10, 5))\n\n# Creating a bar plot\nsns.barplot(x=feature_imp.index, y=feature_imp)\n\n# Add labels to your graph\nplt.xticks(rotation=45, horizontalalignment='right')\n\nplt.tight_layout()\nplt.show()\n\n# create features list\nfeatures_list = X.columns\nfeatures_list = features_list.tolist()\n\n# Get numerical feature importances\nimportances = list(clf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \nprint(\"\\nTop 10 Features:\")\ndisplay_top = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]\n\n# Sort the feature importances by least important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = False)\n# Print out the feature and importances \nprint(\"\\nBottom 10 Features:\")\ndisplay_bottom = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]","826c4fe6":"# map values \nweight_map = { 'Normal_Weight':0, 'Overweight_Level_I':0,\n               'Overweight_Level_II':0, 'Obesity_Type_I':1,\n               'Obesity_Type_II':1, 'Obesity_Type_III':1, 'Insufficient_Weight':0}\n\n# map values\ndf_prep['weight_cat'] = df_prep['NObeyesdad'].map(weight_map)","30a965ce":"sns.countplot(x=\"weight_cat\",                  \n                 palette=\"Blues_d\", \n                 order=df_prep[\"weight_cat\"].value_counts().index,                 \n                 data=df_prep)\n\n\n# show figure & plots\nplt.tight_layout()\nplt.show()","22faa2e9":"# split dataset in features and target variable\n\n# Features\nX = df_prep.drop(columns=[\"NObeyesdad\",\"weight_cat\"])\n\n# Target variable\ny = df_prep['weight_cat'] ","bafa7d02":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n\n# Scaled version of X train and X test\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)","f2c3e089":"# Number of trees in random forest\nn_estimators = [int(x) for x in range(10, 200,10)]\n# Criterion\ncriterion = ['gini','entropy']\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(10, 100, 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in range(2, 20,2)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in range(2, 20, 2)]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# random state\nrandom_state = [1010]\n\ntarget_names = ['Not Obese', 'Obese']\n\nparameters = { 'criterion':criterion,\n               'n_estimators': n_estimators,\n              'max_depth':max_depth,\n              'random_state': random_state,\n              'max_features':max_features\n              #'min_samples_split':min_samples_split             \n               }\n\nhyper_tune('Random Forest', RandomForestClassifier(), parameters, target_names=target_names)","cffca3bc":"# Create Random Forest classifer object with optimized parameters\nclf = RandomForestClassifier(criterion='gini',\n               n_estimators=110,\n              max_depth = 20,              \n              max_features='auto',              \n              random_state=1010)\n\n# Train Random Forest classifer\nclf = clf.fit(X_train_scaled,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test_scaled)","93207fb5":"feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\nfig = plt.figure(figsize=(12, 5))\n\n# Creating a bar plot\nsns.barplot(x=feature_imp.index, y=feature_imp)\n\n# Add labels to your graph\nplt.xticks(rotation=45, horizontalalignment='right')\n\nplt.tight_layout()\nplt.show()\n\n# create features list\nfeatures_list = X.columns\nfeatures_list = features_list.tolist()\n\n# Get numerical feature importances\nimportances = list(clf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \nprint(\"\\nTop 10 Features:\")\ndisplay_top = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]\n\n# Sort the feature importances by least important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = False)\n# Print out the feature and importances \nprint(\"\\nBottom 10 Features:\")\ndisplay_bottom = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]","d872e943":"# Exploring Two Category Targert Variable\n\n### Data Prep\n\n![](http:\/\/)Underweight mapped to being not obese. Arguably however it is just as much of a health concern as being overweight. ","43ec0911":"## KNN","214770c8":"## Random Forest","fc2063cc":"#### Data Prep \/ Clean Up\n\n- Drop Height and Weight columns as they are used in the BMI calculation for our target variable\n- Convert categorical variables to category instead of object\/text\n- Convert synthetic floats & floats to whole integers to better reprsent the ordinal data from orignal survey data","6b98684c":"# Machine Learning Models\n\nFor this exercise we will take a look at \n- Decision Trees\n- Random Forest\n- Support Vector Machines (SVM)\n- K Nearest Neighbors \n\nCursory Look at the model results suggest that Random Forest will be our best initial model with an accuracy of 79%. However, many of the other models performance is not too far off. \n\nThe other interesting result is that all of the models classify Obesity_Type_III amazingly well with >= 98% accuracy across all models. ","885b65a1":"# About the Data\n\nThis data comes from the [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+). This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition.\n\n### Notes:\n<ul> \n    <li> This data is created via survery. However the data was imbalanced and records were synthasized using WEKA to simulate additional responses.\n    <li> The synthasized data fits the distribution of the data but generates floats (ratio data) where ordinal and integer data were the original values in the survey.\n    <li> The target variable is comprised of the variables height and weight. Height and weight are used to classisfy a persons BMI. The BMI measure was then used to classify each observation. \n<\/ul>\n\n### Features & Descriptions\n\n<table>\n    <thead>\n        <tr><th>Category<\/th><th>Feature Name<\/th><th>Description<\/th><th>Variable Type<\/th><\/tr>\n    <\/thead>\n    <tbody>\n        <tr><td>Target Variable<\/td><td>NObesity<\/td><td>Based on BMI<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Eating Habits<\/td><td>FAVC<\/td><td>Frequent consumption of high caloric food<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Eating Habits<\/td><td>FCVC<\/td><td>Frequency of consumption of vegetables<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Eating Habits<\/td><td>NCP<\/td><td>Number of main meals<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Eating Habits<\/td><td>CAEC<\/td><td>Consumption of food between meals<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Eating Habits<\/td><td>CH20<\/td><td>Consumption of water daily<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Eating Habits<\/td><td>CALC<\/td><td>Consumption of alcohol<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Physical Conditioning<\/td><td>SCC<\/td><td>Calories consumption monitoring<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Physical Conditioning<\/td><td>FAF<\/td><td>Pysical activity frequency<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Physical Conditioning<\/td><td>TUE<\/td><td>Time using technology devices<\/td><td>Ordinal<\/td><\/tr>\n        <tr><td>Physical Conditioning<\/td><td>MTRANS<\/td><td>Transportation used<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Physical Conditioning<\/td><td>SMOKE<\/td><td>Smokes Yes or No<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Responder Charateristics<\/td><td>Family History with Overweight<\/td><td>Yes or No<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Responder Charateristics<\/td><td>Gender<\/td><td>Gender is Male or Female<\/td><td>Categorical<\/td><\/tr>\n        <tr><td>Responder Charateristics<\/td><td>Age<\/td><td>Age in years<\/td><td>Integer<\/td><\/tr>\n        <tr><td>Responder Charateristics<\/td><td>Height<\/td><td>Height in meters<\/td><td>Float<\/td><\/tr>\n        <tr><td>Responder Charateristics<\/td><td>Weight<\/td><td>Weight in kilograms<\/td><td>Float<\/td><\/tr>        \n     <\/tbody>\n<\/table>","73f297ec":"# Feature Importance w\/ Random Forest","77fbca4c":"\n\n# Exploring the Data & Clean Up\n\n- The data contains 2111 records with 17 columns\n- The data loads as text and float objects for most of the objects. However we know that some are float, categorical and ordinal\n- All of the records are unique and contain no null values\n- Height and Weight are included however they have a direct correlation to each other and our target variable\n- The survey data is distinguishable from the synthazied data based on floats used for ordinal variables\n","e43d7d1c":"## Data Intuition & Further Exploration\n\n##### Categorical Variables\n- The categorical variables are not gaussian\n- Most of the categorical variables are bernoulli in nature\n- The target variables based on synthetic process are fairly balanced\n\n##### Ordinal Variables                                        \n- Will be treated as nominal         \n- Non are Gaussian distributed\n\n##### Ratio Variable\n- Age is the only ratio varible\n- Is not Gaussian\n\n##### Predictor Coorelations \n- The predictor variables not highly correlated\n- Height and Weight have been removed for their correlation to each other and the target\n\n##### Target Variable\n- Is categorical with > 2 classes\n- Is faily balanced in its distribution of weights","b3f5893f":"# Data prep for ML models: General \n- Some models may require additional treatment \n- These steps will prep the data for the most viable models: Decision Trees, Random Forest, SVM, Nerual Network\n- Some models may benifit from scaling so this will be evaluated as well\n\n#### Data Treatment\n- Copy cleaned data to new dataframe\n- Create dummy variables out of categorical variables\n- Split the data into 70\/30 train & test datasets     ","9a5a55cc":"> ## Random Forest with Two Category Target Variable","142c41ef":"# Part 1: Data Preperation and Exploration","c864fc93":"# Part 2: Machine Learning\n\nThe target variable is NObesity. The attempt is is apply ML to find the best model for predicting NObesity. \nNObesity is a categorical variable that is a measure of a person weight ranging from under weight to very obese (Overweight Level II). ","6758a6e0":"## Feature Importance with Random Forest: Two Category Variable Target","f256c941":"# Data Intuition & Model Considerations\n\n##### Data Intuion\n- Models for this data need to cater to > 2 two classes\n- Data is limited so model will need to perform well with limited amount of data\n- Model will need to handle dummy variables well and not be dependent on ratio data\n- The ordinal variables may or may not be helful to the model\n- The data is not Gaussian so the model needs to be non-parametic or at least not strictly parametic \n- The data is pretty wide but not very deep, so reducing the factors may be necessary\n\n##### Potential Models\n- Decision Trees \n- Random Forest \n- Search Vector Machines (SVM)\n\n##### Experiments \nModels that may be used to evaluate data assumptions and model performance:\n- Is the data too limited for a Nerual Network? \n- Does Logistic Regression perform well if target variable reduced to 2 classes?\n- Do the other models perform better if target variable is reduced to 2 classes?","1473de82":"# Hyperparameter Tuning\n\nSearching for better performance out of the models with Gridsearch"}}