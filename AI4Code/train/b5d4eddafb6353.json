{"cell_type":{"877037ce":"code","b618317f":"code","633f396a":"code","956d357d":"code","ce762220":"code","257a270f":"code","7d7257e7":"code","09fb2914":"code","962fab9d":"code","891ef588":"code","d93d4e68":"code","e6725349":"markdown","ab937c6f":"markdown","8eaf5a90":"markdown","174367e6":"markdown","94030b9b":"markdown","5fa985a2":"markdown","d62c0f51":"markdown","0a4dee49":"markdown","a1d792e8":"markdown","5a1b31dc":"markdown","3d5df475":"markdown","3149e8ec":"markdown"},"source":{"877037ce":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.initializers import he_normal\n\nfrom PIL import Image\nfrom io import BytesIO","b618317f":"r = requests.get('https:\/\/gym.openai.com\/videos\/2019-10-21--mqt8Qj1mwo\/CartPole-v1\/poster.jpg')\n\nplt.figure(figsize = (10, 6))\nplt.imshow(Image.open(BytesIO(r.content)))\nplt.axis('off')\nplt.show()","633f396a":"env = gym.make('CartPole-v0') # Create an environment\n\n# To reset the environment we can use reset() function which returns an array with 4 values\n# This 4 values is an observation, which tells us a position of pole. We don't need to know what these values mean\n# this is a job for our ANN.\nobservation = env.reset()\nprint(f'Observation, returned by reset() function: {observation}')\n\n# To see action space we can use action_space attribute\n# Discrete(2) means that actions can be 0 or 1 which can be left or right \nprint('Action space: ', env.action_space)\n\n# To make step we need to use step(action) function, it returns 4 values:\n# Obseravtion - current observation after action\n# Reward - recieved reward after action\n# Done - whether game over or not\n# Debug data which we don't need\nobservation, reward, done, debug = env.step(env.action_space.sample()) # Doing random action\nprint(f'Observation after action: {observation}')\nprint(f'Reward for the action: {reward}')\nprint(f'Game is over: {done}')\nprint(f'Debug data: {debug}')","956d357d":"def generate_data(env = gym.make('CartPole-v0'), n_games = 1000, model = None, percentile = 70):\n    '''\n       env - an environment to solve\n       n_games - number of games to play to collect raw data\n       model - if None, the random actions will be taken to collect data, to predict actions a model must be passed\n       percentile - (100% - percentile%) of the best games will be selected as training data\n    '''\n    observation = env.reset() # Resetting the environment to get our first observation\n\n    train_data = [] # List to store raw data\n    rewards = [] # List to store total rewards of each game\n    \n    print(f'Playing {n_games} games...')\n    \n    # Step 1 of the algorithm - Play N numbers of games using random actions or actions predicted by model to collect raw data.\n    for i in range(n_games):\n        temp_reward = 0 # Counts a current game total reward\n        temp_data = [] # Stores (observation, action) tuples for each step\n        \n        # Playing a current game until done\n        while True:\n            # Use model to predict actions if passed, otherwise take random actions\n            if model:\n                action = model.predict(observation.reshape((-1, 4)))\n                action = int(np.rint(action))\n            else:\n                action = env.action_space.sample()\n            \n            temp_data.append((list(observation), action)) # Appending (observation, action) tuple to temp_data list\n\n            observation, reward, done, _ = env.step(action) # Making action\n\n            temp_reward += reward # Counting reward\n            \n            # If game over - reset environment and break while loop\n            if done:\n                observation = env.reset()\n                break\n        \n        # Append data of last game to train_data list and total reward of last game to rewards list\n        train_data.append(temp_data)\n        \n        # Step 2 of the algorithm - Collect a total reward for each game and calculate threshold - 70 percentile of all total rewards.\n        rewards.append(temp_reward)\n        \n    print('Done playing games\\n')\n    \n    # Calculating threshold value using rewards list an np.percentile function\n    thresh = int(np.percentile(rewards, percentile))\n    print(f'Score threshold value: {thresh}')\n    \n    print(f'Selecting games according to threshold...')\n    # Step 3 of the algorithm - Select games from raw data which have total reward more than threshold.\n    train_data = [episode for (i, episode) in enumerate(train_data) if rewards[i] >= thresh]\n    \n    # Now train_data list contains lists of tuples: [[(observation, action), ...], [(observation, action), ...], ...]\n    # The next string flattens train_data list: [(observation, action), (observation, action), ...]\n    train_data = [observation for episode in train_data for observation in episode]\n    \n    # Creating labels array\n    labels = np.array([observation[1] for observation in train_data])\n    \n    # Storing only observations in train_data array\n    train_data = np.array([observation[0] for observation in train_data])\n    print(f'Total observations: {train_data.shape[0]}' )\n    \n    return train_data, labels","ce762220":"# Generating first training data\ntrain_data, labels = generate_data(n_games = 2000)","257a270f":"# Weights initializer\ninit = he_normal(seed = 666)\n\nmodel = Sequential()\n\n# We are using observations from environment as input data, so input shape of our ANN is (4, )\nmodel.add(Dense(64, input_shape = (4,), activation = 'relu', kernel_initializer = init))\nmodel.add(Dense(128, activation = 'relu', kernel_initializer = init))\n\n# Because our action can be only 0 or 1, I'll use Dense layer with one neuron and sigmoid activation function\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# Compile model using SGD and binary_crossentropy\nmodel.compile(optimizer = 'sgd', loss = 'binary_crossentropy')","7d7257e7":"def plot_loss():    \n    H = model.history.history\n    \n    plt.figure(figsize = (15, 5))\n    plt.plot(H['loss'], label = 'loss')\n    plt.plot(H['val_loss'], label = 'val_loss')\n    plt.grid()\n    plt.legend()\n    plt.show()","09fb2914":"# Model training\nmodel.fit(train_data, labels, epochs = 100, batch_size = 32, validation_split = 0.2, verbose = 0)\nplot_loss()","962fab9d":"# env = gym.make('CartPole-v0')\n# observation = env.reset()\n\n# for i in range(3):\n#     temp_reward = 0\n#     while True:\n#         env.render()\n\n#         action = model.predict(observation.reshape((-1, 4)))\n#         action = int(np.rint(action))       \n\n#         observation, reward, done, _ = env.step(action)\n\n#         temp_reward += reward\n        \n#         if done:\n#             print(temp_reward)\n#             observation = env.reset()\n#             break\n\n# env.close()","891ef588":"# Generating data using actions, predicted by the model\ntrain_data, labels = generate_data(model = model)","d93d4e68":"# Train model on new data\nmodel.fit(train_data, labels, epochs = 30, batch_size = 32, validation_split = 0.2, verbose = 0)\nplot_loss()","e6725349":"The goal of the environment is to hold pole in vertical position moving cart left or right. The game is completed if the pole is balanced for 200 episodes. The environment looks like this:","ab937c6f":"### Round 2 - model predicted actions\n\nNow I want to return to step 1 of our algorithm, but now I want to use predictions of our model as actions, when generating data.","8eaf5a90":"Now our score threshold is much higher and we are taking only those games, which have total reward equal or higher than threshold.\n\nNext - we train our model on new data.","174367e6":"### Model training - sample actions data\n\nFirst - I'll train model using data, generated on random actions. To plot loss I'll also create a plot_loss function:","94030b9b":"Let's create an environment and look at it in details.","5fa985a2":"### Goal\n\nReinforcement learning is a very interesting area which studies creation of self-educating agents, which can solve different tasks in different environments. In this kernel I'm going to solve classic OpenAI Gym CartPole-v0 environment using cross-entropy method. So let's start.","d62c0f51":"### Results\n\nTo show results of training, I made a video which you can watch on YouTube:\n\nhttps:\/\/www.youtube.com\/watch?v=YygkJM13UTM&t=41s\n\nEvery 10 epochs of training, I played 3 games and recorded results, so we can see how our agent improves during training process.","0a4dee49":"Our task here is to minimize a loss. If model is tend to overfit - then additional regularization must be added, but here I'll leave it as is, to keep things simple.\n\nTo see what your model is doing, you can uncomment and run next piece of code which will play 3 games:","a1d792e8":"### Conclusion\n\nAnd this is all here. The cross-entropy method is very simple, I'd rather say it's primitive, but it works very well for simple tasks like CartPole. \n\nOf course the code here is just a baseline and can be easly improved or written as a convenient class, but my main goal was to show you the algorithm and implement it step by step, keeping things as simple as possible, so I hope it will help anybody who doing first steps in deep reinforcement learning area.","5a1b31dc":"### Model creation\nThis is step 4 of our algorithm - **train model on selected data where observations are an input to the model and actions are targets**.\n\nAs a model I'll use simple ANN with two hidden layers (64, and 128 neurons).","3d5df475":"### Algorithm implementation\n\nLet's start coding. The main piece of code - is a function that will generate and preprocess data.","3149e8ec":"### Algorithm\n\nTo solve this environment a cross-entropy algorithm have been used. The algorithm can be written in several steps:\n1. Play N numbers of games using random actions or actions predicted by model to collect raw data.\n2. Collect a total reward for each game and calculate threshold - 70 percentile of all total rewards.\n3. Select games from raw data which have total reward more than threshold.\n4. Train model on selected data where observations are an input to the model and actions are targets.\n5. Repeat from step 1 untill good results.\n\nIn this kernel I want to train model with two steps - first, on data, generated using random actions, second, on data, generated using actions, predicted by the model."}}