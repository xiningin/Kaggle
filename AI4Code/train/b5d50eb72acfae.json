{"cell_type":{"6ae23527":"code","1d1b6c93":"code","9e72526b":"code","cb0fc1ec":"code","ab154e34":"code","24300692":"code","33db60a7":"code","e3b938b8":"code","5be66703":"code","6fd25fcf":"code","68667178":"code","1db941fe":"code","72eaa0f0":"code","b91f2211":"code","5d6b81dc":"code","d41a3087":"code","a60a6fab":"code","16c91704":"code","26890f79":"code","602bb39d":"code","3111b615":"code","0eb11f92":"code","2cf7dbe3":"code","7e89d5ca":"markdown","fec1f477":"markdown","ab2adc26":"markdown","9654a584":"markdown","5969deed":"markdown","02ee7fd6":"markdown","733a8e65":"markdown","900f0782":"markdown","8ba78e6e":"markdown","2abbb747":"markdown","a5e3eb1a":"markdown"},"source":{"6ae23527":"import numpy as np # linear algebra\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport xgboost as xgb\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LassoCV\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nfrom sklearn.ensemble import VotingClassifier,GradientBoostingRegressor\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","1d1b6c93":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nprint(\"Shape of the train data:\",train_data.shape)\nprint(\"Shape of the test data:\",test_data.shape)","9e72526b":"train_data.head()","cb0fc1ec":"#import pandas_profiling\n#report=pandas_profiling.ProfileReport(train_data,check_correlation =True);\n#report.to_file(outputfile=\"eda_report.html\")\n#report","ab154e34":"#Check if the dataset is shuffled\nplt.figure()\nplt.plot(train_data.SalePrice,'.')\nplt.title('SalesPrices within the train dataset')\nplt.xlabel('Row Index')\nplt.ylabel('SalePrice')\nplt.show()","24300692":"plt.figure()\nsns.jointplot(x='OverallQual', y='SalePrice',data= train_data)\nplt.show()","33db60a7":"plt.figure(figsize=(50,25))\nax=sns.boxplot(x='YearBuilt',y='SalePrice',data=train_data)\nplt.show()","e3b938b8":"#concatenate both train and test data\ntest_idx=test_data['Id']\nall_data = pd.concat((train_data, test_data), sort=False).reset_index(drop=True)\n\n#\"SalePrice\" is the target value. We don't include it in data. We don't want \"id\" affecting our model. Hence remove it.\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nall_data = all_data.drop([\"Id\"], axis=1)","5be66703":"all_nans = all_data.isnull().sum()\nall_nans = all_nans[all_nans>0]\nall_nans.sort_values(ascending=False)","6fd25fcf":"#Imput missing values\n\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\nall_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(all_data[\"Electrical\"].mode()[0])\nall_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].fillna(all_data[\"KitchenQual\"].mode()[0])\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(all_data[\"Functional\"].mode()[0])\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nfor col in ('GarageType','GarageFinish','GarageQual','GarageCond'):\n    all_data[col] = all_data[col].fillna(\"None\")\nfor col in ('GarageYrBlt','GarageCars','GarageArea'):\n    all_data[col] = all_data[col].fillna(0)\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].mode()[0])\n\n#Now check again if there are anymore columns with missing values.\nall_nans = all_data.isnull().sum()\nall_nans = all_nans[all_nans>0]\nall_nans.sort_values(ascending=False)\n\nlen(all_nans)","68667178":"plt.figure()\nsns.distplot(train_data['SalePrice'])\nplt.title(\"Sale Price distribution skewed by {}\".format(train_data.SalePrice.skew()))\nplt.show()","1db941fe":"train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\nplt.figure()\nsns.distplot(train_data['SalePrice'])\nplt.title(\"Sale Price distribution skewed by {}\".format(train_data.SalePrice.skew()))\nplt.show()","72eaa0f0":"numerical_features = all_data.select_dtypes(exclude = [\"object\"]).columns\nprint(\"Number of numerical features: \" + str(len(numerical_features)))\n\n#log transform numerical features\nskewness = all_data[numerical_features].apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.7]\nskewed_features = skewness.index\nall_data[skewed_features] = np.log1p(all_data[skewed_features])","b91f2211":"categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\nprint(\"Number of categorical features:\" + str(len(categorical_features)))\n\n#getdummies for categorical features\n#Create a dataFrame with dummy categorical values\ndummy_all_data = pd.get_dummies(all_data[categorical_features])\n#Remove categorical features from original data, which leaves original data with only numerical featues\nall_data.drop(categorical_features, axis=1, inplace=True)\n#Concatenate the numerical features in original data and categorical features with dummies\nall_data = pd.concat([all_data, dummy_all_data], axis=1)\n#print(all_data.shape)","5d6b81dc":"#Separate training and given test data\nX = all_data[:train_data.shape[0]]\ntest_data = all_data[train_data.shape[0]:]\ny = train_data[\"SalePrice\"]\nprint(X.shape)\nprint(y.shape)","d41a3087":"xgb1 = xgb.XGBRegressor(n_estimators=1000, max_depth=7,min_child_weight=1.5,reg_alpha=0.75,reg_lambda=0.45,learning_rate=0.07,subsample=0.95)\nxgb1.fit(X, y)","a60a6fab":"xgb1_predicted_prices = np.expm1(xgb1.predict(test_data))\nprint(xgb1_predicted_prices)","16c91704":"my_submission = pd.DataFrame({'Id': test_idx, 'SalePrice': xgb1_predicted_prices})\nmy_submission.to_csv('submission1.csv', index=False)","26890f79":"xgb2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, min_child_weight=1.5,reg_alpha=0.75,reg_lambda=0.45, learning_rate=0.01, subsample=0.95)\nxgb2.fit(X, y)","602bb39d":"params={'n_estimators': 1000,'learning_rate': 0.01,'max_depth': 4}\ngbm=GradientBoostingRegressor(**params)\ngbm.fit(X, y)","3111b615":"lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X, y)\nlasso_rmse=np.sqrt(-cross_val_score(lasso, X, y, scoring=\"neg_mean_squared_error\", cv = 5)).mean()  \nprint('Lasso error', lasso_rmse)","0eb11f92":"#Make predictions on the test data\nxgb2_predicted_prices = np.expm1(xgb2.predict(test_data))\ngbm_predicted_prices =np.expm1(gbm.predict(test_data))\nlasso_predicted_prices = np.expm1(lasso.predict(test_data))\npredicted_prices = 0.50 * lasso_predicted_prices+0.25 * gbm_predicted_prices + 0.25 * xgb2_predicted_prices\nprint(predicted_prices)","2cf7dbe3":"my_submission = pd.DataFrame({'Id': test_idx, 'SalePrice': predicted_prices})\nmy_submission.to_csv('.\/submission2.csv', index=False)","7e89d5ca":"### Load the data","fec1f477":"# Part 2: Using Model Averaging","ab2adc26":"# Part 1: Using Xgboost","9654a584":"## Load the dependencies","5969deed":"# HOUSE PRICES: ADVANCED REGRESSION TECHNIQUES\n\nThis is an introductory notebook which is intended for users getting started with Kaggle Competitions. Here we will apply two different techniques on the Ames house prices dataset, get acquainted with the submission process and finally see if the performance of our predictions improve. \n\nDeveloping ensembling models is the focus of this notebook and for that reasons, exploratory data analysis has been kept to the minimum.\n\nThe notebook is divided into two sections based on the techniques used for predictions:\n    * Using Xgboost\n    * Model Averaging \n\nEach section involves its own preprocessing and model building modules.","02ee7fd6":"### Preprocessing","733a8e65":"Next, we check for normality of numeric features and target variable","900f0782":"#### Encode Categorical features","8ba78e6e":"#### Imputing missing values\n\nWe first concatenate both train and test datasets, then identify the number of features with missing values and finally fill each feature using appropriate method. ","2abbb747":"#### Applying log transformation on numerical features","a5e3eb1a":"### Basic EDA"}}