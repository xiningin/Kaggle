{"cell_type":{"64d4a3f9":"code","dcac3d5a":"code","b948eaa8":"code","71253280":"code","dd5e624c":"code","6412a498":"code","3db867f7":"code","f836272c":"code","16a5782b":"code","689b1580":"code","7db76a83":"code","01795f89":"code","fdf0c189":"code","dc9f6f23":"code","b2b48578":"code","d569512e":"code","a5a892d9":"code","ccbebfc3":"code","553f30d5":"code","56e8c54f":"markdown","9985a9ec":"markdown","100f7c8a":"markdown","68a6a752":"markdown","6f6b069d":"markdown","bfb28172":"markdown","4ea2637e":"markdown","43c07b2b":"markdown","c6f60f2b":"markdown","289581db":"markdown","536e1e67":"markdown","ed2275ac":"markdown","366cd460":"markdown","764d7f92":"markdown"},"source":{"64d4a3f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dcac3d5a":"import zipfile\nimport os\nimport shutil","b948eaa8":"# Removing files that are allready unzippped\nfor folder in os.listdir(\"\/kaggle\/working\/\"):\n    if folder != \"__notebook_source__.ipynb\":\n        shutil.rmtree(\"\/kaggle\/working\/\"+folder)\n\n# Unzip train data\nwith zipfile.ZipFile(\"\/kaggle\/input\/dogs-vs-cats\/train.zip\") as zip_ref:\n    zip_ref.extractall(\"\/kaggle\/working\/train\")\n\n# Unzip test data\nwith zipfile.ZipFile(\"\/kaggle\/input\/dogs-vs-cats\/test1.zip\") as zip_ref:\n    zip_ref.extractall(\"\/kaggle\/working\/test\")","71253280":"TRAIN_DIR = \"\/kaggle\/working\/train\/\"\nTEST_DIR = \"\/kaggle\/working\/test\/\"\n\n# Creating separate folder for cat and dog images\n# Using try catch so that if the folder is already created, then it won't throw any error\ntry:\n    os.mkdir(TRAIN_DIR + \"cat\")\n    os.mkdir(TRAIN_DIR + \"dog\")\nexcept os.error:\n    pass\n\n# Seperating training data\nfor image in os.listdir(TRAIN_DIR + \"train\"):\n    source = TRAIN_DIR + \"train\/\" + image\n    destination = TRAIN_DIR + image.split(\".\")[0]\n    shutil.move(source, destination)\n\n# Test data can't be sepearated, so putting them in one folder\nfor image in os.listdir(TEST_DIR + \"test1\"):\n    source = TEST_DIR + \"test1\/\" + image\n    destination = TEST_DIR\n    shutil.move(source, destination)\n\n# Removing the empty folders\nos.rmdir(TRAIN_DIR + \"train\")\nos.rmdir(TEST_DIR + \"test1\")","dd5e624c":"DOG_DIR = \"\/kaggle\/working\/train\/dog\/\"\nCAT_DIR = \"\/kaggle\/working\/train\/cat\/\"\n\nTEST_FILES = os.listdir(TEST_DIR)\nCAT_FILES = os.listdir(CAT_DIR)\nDOG_FILES = os.listdir(DOG_DIR)\n\nTEST_DATA_SIZE = len(TEST_FILES)\nDOG_DATA_SIZE = len(DOG_FILES)\nCAT_DATA_SIZE = len(CAT_FILES)\n\nprint(f\"There are {DOG_DATA_SIZE} dog, {CAT_DATA_SIZE} cat and {TEST_DATA_SIZE} test images.\")","6412a498":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random","3db867f7":"# Getting random indexes\ncat1, cat2, dog1, dog2 = [random.randint(0, CAT_DATA_SIZE-1) for i in range(4)]\n\n# Showing the images on a subplot\nfigure, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(8,8))\n\naxes[0, 0].imshow(mpimg.imread(CAT_DIR + CAT_FILES[cat1]))\naxes[0, 1].imshow(mpimg.imread(CAT_DIR + CAT_FILES[cat2]))\naxes[1, 0].imshow(mpimg.imread(DOG_DIR + DOG_FILES[dog1]))\naxes[1, 1].imshow(mpimg.imread(DOG_DIR + DOG_FILES[dog2]))","f836272c":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Input\nfrom tensorflow.keras.models import Sequential","16a5782b":"model = Sequential([\n    Input((150, 150, 3)),\n    Conv2D(16, 3, activation=\"relu\"),\n    MaxPool2D(2, 2),\n    Conv2D(32, 3, activation=\"relu\"),\n    MaxPool2D(2, 2),\n    Conv2D(64, 3, activation=\"relu\"),\n    MaxPool2D(2, 2),\n    Flatten(),\n    Dense(512, activation=\"relu\"),\n    Dense(1, activation=\"sigmoid\")\n])\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","689b1580":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","7db76a83":"BATCH_SIZE = 32\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   validation_split=0.2,\n                                   rotation_range=45,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True)\n\ntrain_gen = train_datagen.flow_from_directory(TRAIN_DIR,\n                                              target_size=(150, 150),\n                                              batch_size=BATCH_SIZE,\n                                              class_mode=\"binary\",\n                                              subset=\"training\")\n\nvalidation_gen = train_datagen.flow_from_directory(TRAIN_DIR,\n                                                   target_size=(150, 150),\n                                                   batch_size=BATCH_SIZE,\n                                                   class_mode=\"binary\",\n                                                   subset=\"validation\")","01795f89":"history = model.fit(train_gen,\n                    epochs=10,\n                    steps_per_epoch=train_gen.samples \/\/ BATCH_SIZE,\n                    validation_data=validation_gen,\n                    validation_steps=validation_gen.samples \/\/ BATCH_SIZE)","fdf0c189":"# Plotting loss and accuracy\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\naxes[0].plot(history.history[\"loss\"], label=\"Loss\")\naxes[0].plot(history.history[\"val_loss\"], label=\"Val Loss\")\naxes[1].plot(history.history[\"accuracy\"], label=\"Accuracy\")\naxes[1].plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\naxes[0].legend()\naxes[1].legend()\nfig.show()","dc9f6f23":"import pandas as pd","b2b48578":"test_df = pd.DataFrame({'filename': TEST_FILES})\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_gen = test_datagen.flow_from_dataframe(test_df,\n                                            TEST_DIR,\n                                            x_col=\"filename\",\n                                            y_col=None,\n                                            class_mode=None,\n                                            target_size=(150, 150),\n                                            shuffle=False,\n                                            batch_size=BATCH_SIZE)","d569512e":"predict = model.predict(test_gen, steps=np.ceil(test_df.shape[0] \/ BATCH_SIZE))","a5a892d9":"output = (predict >= 0.5).astype(int)\n# As output was a 2D array, I had to squeeze it\noutput = np.squeeze(output).tolist()","ccbebfc3":"# Creating the label map for categories\nlabel_map = dict((v, k) for k, v in train_gen.class_indices.items())\nCATEGORY = [label_map[i] for i in output]\n\nTEST_ID = []\nfor file in TEST_FILES:\n    TEST_ID.append(file.split('.')[0])","553f30d5":"# Creating the dataframe for submission\nsubmission_df = pd.DataFrame({\n    \"id\": TEST_ID,\n    \"label\": CATEGORY\n})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)","56e8c54f":"## Loading the dataset\nThe dataset is given in a zip file. So to use this at first we have to **unzip** the zip files.","9985a9ec":"## Getting the test files ready to predict","100f7c8a":"Seperate cats and dogs images in train data","68a6a752":"## Loading and Augmenting images","6f6b069d":"## Train the model on augmented images","bfb28172":"## Time to create the model\nThis time I'm going to create my model from scratch usign only CNN and MaxPool.","4ea2637e":"## Predict","43c07b2b":"As I've used softmax for binary classification, this will predict a float value (0 to 1) for every test image.","c6f60f2b":"## Exploring the dataset","289581db":"My model has only 3 Conv2D layers followed by MaxPool2D layers. And 2 Dense layer on top of that.","536e1e67":"Well, atleast it isn't overfitted. \ud83d\ude1b","ed2275ac":"Let's see how our model performed","366cd460":"Showing some images from the dataset","764d7f92":"I took 1 if the prediction is greater or equal to 0.5, else I took 0.\nHere 1 means it's a dog, and 0 means it's a cat."}}