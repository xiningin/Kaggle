{"cell_type":{"ed1d49ac":"code","b59e531f":"code","bb078bac":"code","fa289d09":"code","b7dea8e1":"code","8c61d889":"code","aac6201f":"code","1beae162":"code","d3ba30bb":"code","abbc003b":"code","dbbaa3e8":"code","60d457c2":"code","c3e13fce":"code","7ca61caa":"code","7bc09290":"code","026a8406":"code","d5197eb0":"code","1d6852a7":"code","d943980e":"code","d5954f26":"code","9be0c37d":"code","e95dbe5a":"code","fd801255":"code","769e95d4":"code","65950117":"code","1f1784bd":"code","0de12e98":"code","8866f084":"code","6b6edf20":"code","10a5f699":"code","33f53e5e":"code","26c93036":"code","41773560":"code","78997e21":"code","fb6ef5d5":"code","cc5acb46":"code","21527186":"code","1488e516":"code","457148a1":"code","3a1b7730":"code","ffc3ebe3":"code","403bd0b7":"code","05ef0285":"code","3a729b3a":"code","9dea9547":"code","4e8a0a17":"code","64e025b2":"code","73d6c37a":"code","a874ba71":"code","bb600294":"code","48c7925a":"code","23f6d127":"code","5cabd458":"code","0b870e9e":"code","abb3441e":"code","4b2b5333":"code","8515bf15":"code","5e20f2dc":"code","4ba7b498":"code","51a2e423":"code","a705fa3a":"code","d4adb050":"code","35376616":"code","00377627":"code","207e966c":"code","fe59b125":"code","0c77f941":"code","2c8cd9a1":"code","758503e0":"code","29be53bf":"code","5b34b442":"code","7185246b":"code","cea8f758":"code","8193a131":"code","df3c9d7c":"code","283095c3":"code","bfdbac63":"code","d6374302":"code","02a04a21":"code","c37e6ae7":"code","b6b0449a":"code","01adfe47":"code","88a66192":"code","ac7b0723":"code","82dea318":"code","55690b24":"code","4a634cd5":"code","59a0b7aa":"code","fe26a04d":"code","00e7d207":"markdown","d5895312":"markdown","09307a0a":"markdown","2b865411":"markdown","94f00f65":"markdown","9c048909":"markdown","8470e8b1":"markdown","b1338dae":"markdown","02c2a344":"markdown","44a840e5":"markdown","c1a460f6":"markdown","1fca25be":"markdown","a3e001e5":"markdown","295ec474":"markdown","7a391a36":"markdown","354ad0ad":"markdown","e8a1e02b":"markdown","5eabe2cc":"markdown","e04f7d30":"markdown","bccd671d":"markdown","87c96e06":"markdown","466fd35b":"markdown","bbf89025":"markdown","6e3fdd5b":"markdown","ff9724a9":"markdown","f3a16935":"markdown","9b639d4d":"markdown","18898807":"markdown","4847a0e8":"markdown","bf1ebefe":"markdown","a854ceeb":"markdown","0750f4c8":"markdown","6c577101":"markdown","833870fe":"markdown","97ab6ac8":"markdown","ea32ed06":"markdown","9f5d92ae":"markdown","d5da26de":"markdown","2ec25e35":"markdown","a05af756":"markdown","173a7e33":"markdown","7ced2944":"markdown","4d133d8e":"markdown","2c816b57":"markdown","a3715757":"markdown","7aa38d36":"markdown","5480086e":"markdown","594fb9a2":"markdown","ab783ea7":"markdown","e3122915":"markdown","caff47ce":"markdown","6e2732a0":"markdown","6f17bb4b":"markdown","9eaeae80":"markdown","5f4cb5fa":"markdown","eaeab34f":"markdown","71607a02":"markdown","cf1ebbe5":"markdown","542ecb6f":"markdown","a85805aa":"markdown","0585d50d":"markdown","f49ffa24":"markdown","cf331f30":"markdown","7b0f82a5":"markdown","91cd7437":"markdown","d3d1c405":"markdown","5e321594":"markdown","0e95a64f":"markdown","0874a8b2":"markdown","489bcd77":"markdown","14f504b1":"markdown","c0b0dd5e":"markdown","b1b97e51":"markdown","f7b15405":"markdown","3797e8c0":"markdown","5f8dd379":"markdown"},"source":{"ed1d49ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b59e531f":"from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\nfrom sklearn import neighbors\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb","bb078bac":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","fa289d09":"# read in the training data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# read in the testing data\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","b7dea8e1":"df_train['SalePrice'] = np.log(df_train['SalePrice'])","8c61d889":"# before I start, I know there this is a typo in the Exterior2nd column\n\n# Cement Board is labeled as CmentBd instead of CemntBd. Therefore we must\n# update it.\ndf_train = df_train.replace({\"Exterior2nd\":{\"CmentBd\":\"CemntBd\"}})\ndf_test = df_test.replace({\"Exterior2nd\":{\"CmentBd\":\"CemntBd\"}})\n\n# Cement Board is labeled as Wd Shng instead of Wd Sdng. Therefore we must\n# update it.\ndf_train = df_train.replace({\"Exterior2nd\":{\"Wd Shng\":\"Wd Sdng\"}})\ndf_test = df_test.replace({\"Exterior2nd\":{\"Wd Shng\":\"Wd Sdng\"}})\n\ncombined = pd.concat([df_train, df_test], sort=False)","aac6201f":"combined = pd.concat([df_train, df_test], sort=False)\n\nexterior1_uniqVals = list(combined[\"Exterior1st\"].dropna().unique())\nexterior2_uniqVals = list(combined[\"Exterior2nd\"].dropna().unique())\nexterior_uniqVals = [x for x in exterior2_uniqVals if x not in exterior1_uniqVals]\nexterior_uniqVals = exterior_uniqVals +exterior1_uniqVals\n\nexterior_cols = []\nfor exterior_type in exterior_uniqVals:\n    new_exType_col = \"Exterior_\"+exterior_type\n    exterior_cols.append(new_exType_col)\n    df_train.loc[((df_train[\"Exterior1st\"]==exterior_type) |\n                  (df_train[\"Exterior2nd\"]==exterior_type)),new_exType_col] = 1\n    df_train.loc[((df_train[\"Exterior1st\"]!=exterior_type) &\n                  (df_train[\"Exterior2nd\"]!=exterior_type)),new_exType_col] = 0\n    df_test.loc[((df_test[\"Exterior1st\"]==exterior_type) |\n                  (df_test[\"Exterior2nd\"]==exterior_type)),new_exType_col] = 1\n    df_test.loc[((df_test[\"Exterior1st\"]!=exterior_type) &\n                  (df_test[\"Exterior2nd\"]!=exterior_type)),new_exType_col] = 0","1beae162":"df_train.loc[df_train[\"Exterior1st\"] != df_train[\"Exterior2nd\"],\"TwoExteriorMaterials\"]=1\ndf_train.loc[df_train[\"Exterior1st\"] == df_train[\"Exterior2nd\"],\"TwoExteriorMaterials\"]=0","d3ba30bb":"df_train = df_train.drop([\"Exterior1st\",\"Exterior2nd\"],axis=1)\ndf_test = df_test.drop([\"Exterior1st\",\"Exterior2nd\"],axis=1)","abbc003b":"combined = pd.concat([df_train, df_test], sort=False)\ncondition1_uniqVals = list(combined[\"Condition1\"].dropna().unique())\ncondition2_uniqVals = list(combined[\"Condition2\"].dropna().unique())\ncondition_uniqVals = [x for x in condition2_uniqVals if x not in condition1_uniqVals]\ncondition_uniqVals = condition_uniqVals + condition1_uniqVals\n\ncondition_cols=[]\nfor condition_type in condition_uniqVals:\n    new_condType_col = \"Condition_\"+condition_type\n    condition_cols.append(new_condType_col)\n    df_train.loc[((df_train[\"Condition1\"]==condition_type) |\n                  (df_train[\"Condition2\"]==condition_type)),new_condType_col] = 1\n    df_train.loc[df_train[new_condType_col].isnull(),new_condType_col] = 0\n    df_test.loc[((df_test[\"Condition1\"]==condition_type) |\n                  (df_test[\"Condition2\"]==condition_type)),new_condType_col] = 1\n    df_test.loc[df_test[new_condType_col].isnull(),new_condType_col] = 0","dbbaa3e8":"df_train = df_train.drop([\"Condition1\",\"Condition2\"],axis=1)\ndf_test = df_test.drop([\"Condition1\",\"Condition2\"],axis=1)","60d457c2":"df_train.loc[((df_train[\"MSSubClass\"]==20) | \n             (df_train[\"MSSubClass\"]==60) |\n             (df_train[\"MSSubClass\"]==120) |\n             (df_train[\"MSSubClass\"]==160)),\"MSSubClass_1946nNewer\"] = 1\n\ndf_train.loc[df_train[\"MSSubClass_1946nNewer\"].isnull()==True,\"MSSubClass_1946nNewer\"]=0\n\ndf_test.loc[((df_test[\"MSSubClass\"]==20) | \n             (df_test[\"MSSubClass\"]==60) |\n             (df_test[\"MSSubClass\"]==120) |\n             (df_test[\"MSSubClass\"]==160)),\"MSSubClass_1946nNewer\"] = 1\n\ndf_test.loc[df_test[\"MSSubClass_1946nNewer\"].isnull()==True,\"MSSubClass_1946nNewer\"]=0","c3e13fce":"df_train.loc[((df_train[\"MSSubClass\"]==90) | \n             (df_train[\"MSSubClass\"]==190)),\"ShareFloor\"] = 1\n\ndf_train.loc[df_train[\"ShareFloor\"].isnull()==True,\"ShareFloor\"]=0","7ca61caa":"df_test.loc[((df_test[\"MSSubClass\"]==90) | \n             (df_test[\"MSSubClass\"]==190)),\"ShareFloor\"] = 1\n\ndf_test.loc[df_test[\"ShareFloor\"].isnull()==True,\"ShareFloor\"]=0","7bc09290":"df_train = df_train.drop(\"MSSubClass\",axis=1)\ndf_test = df_test.drop(\"MSSubClass\",axis=1)","026a8406":"df_train.loc[((df_train[\"HouseStyle\"]=='1Story')  |\n              (df_train[\"HouseStyle\"]=='1.5Unf') |\n              (df_train[\"HouseStyle\"]=='1.5Fin') |\n              (df_train[\"HouseStyle\"]=='SFoyer') |\n              (df_train[\"HouseStyle\"]=='SLvl')),\"2Stories\"] = 0.0\n\ndf_train.loc[((df_train[\"HouseStyle\"]=='2Story')  |\n              (df_train[\"HouseStyle\"]=='2.5Unf') |\n             (df_train[\"HouseStyle\"]=='2.5Fin')),\"2Stories\"] = 1\n\ndf_test.loc[((df_test[\"HouseStyle\"]=='1Story')  |\n               (df_test[\"HouseStyle\"]=='1.5Unf') |\n             (df_test[\"HouseStyle\"]=='1.5Fin') |\n              (df_test[\"HouseStyle\"]=='SFoyer') |\n               (df_test[\"HouseStyle\"]=='SLvl')),\"2Stories\"] = 0.0\n\ndf_test.loc[((df_test[\"HouseStyle\"]=='2Story')  |\n              (df_test[\"HouseStyle\"]=='2.5Unf') |\n             (df_test[\"HouseStyle\"]=='2.5Fin')),\"2Stories\"] = 1","d5197eb0":"df_test.loc[((df_test[\"HouseStyle\"]=='1Story')  |\n               (df_test[\"HouseStyle\"]=='2Story') |\n             (df_test[\"HouseStyle\"]=='1.5Fin') |\n              (df_test[\"HouseStyle\"]=='SFoyer') |\n               (df_test[\"HouseStyle\"]=='SLvl') |\n               (df_test[\"HouseStyle\"]=='2.5Fin')),\"Unfurnished\"] = 0.0\n\ndf_test.loc[((df_test[\"HouseStyle\"]=='1.5Unf')  |\n              (df_test[\"HouseStyle\"]=='2.5Unf')),\"Unfurnished\"] = 1","1d6852a7":"df_train = df_train.drop(\"HouseStyle\",axis=1)\ndf_test = df_test.drop(\"HouseStyle\",axis=1)","d943980e":"combined = pd.concat([df_train, df_test], sort=False)\nfill_masvnrarea = (np.mean(np.sqrt(combined.loc[combined[\"MasVnrArea\"]>0,\n    \"MasVnrArea\"].dropna().values)))**2\n\ndf_train.loc[(df_train[\"MasVnrArea\"]==0) & \\\n             (df_train[\"MasVnrType\"]!=\"None\"),\"MasVnrArea\"] = fill_masvnrarea\n\ndf_test.loc[(df_test[\"MasVnrArea\"]==0) & \\\n             (df_test[\"MasVnrType\"]!=\"None\"),\"MasVnrArea\"] = fill_masvnrarea\n\ndf_train.loc[(df_train[\"MasVnrArea\"]==0) & \\\n             (df_train[\"MasVnrType\"].isnull()),\"MasVnrArea\"] = fill_masvnrarea\n\ndf_test.loc[(df_test[\"MasVnrArea\"]==0) & \\\n             (df_test[\"MasVnrType\"].isnull()),\"MasVnrArea\"] = fill_masvnrarea","d5954f26":"neighboorhoodBySalePrice_df = df_train.groupby('Neighborhood')['SalePrice'].median().sort_values(ascending=False)\n\ntop_tier_n = list(neighboorhoodBySalePrice_df[neighboorhoodBySalePrice_df.values >= df_train['SalePrice'].quantile(.75)].index)\nmid_upper_tier_n = list(neighboorhoodBySalePrice_df[(neighboorhoodBySalePrice_df.values < df_train['SalePrice'].quantile(.75)) & \\\n                                              (neighboorhoodBySalePrice_df.values >= df_train['SalePrice'].quantile(.50))].index)\nmid_low_tier_n = list(neighboorhoodBySalePrice_df[(neighboorhoodBySalePrice_df.values < df_train['SalePrice'].quantile(.50)) & \\\n                                              (neighboorhoodBySalePrice_df.values >= df_train['SalePrice'].quantile(.25))].index)\nlowest_tier_n = list(neighboorhoodBySalePrice_df[neighboorhoodBySalePrice_df.values < df_train['SalePrice'].quantile(.25)].index)\nprint(top_tier_n)\nprint(mid_upper_tier_n)\nprint(mid_low_tier_n)\nprint(lowest_tier_n)\n\ndf_train.loc[df_train['Neighborhood'].isin(top_tier_n), \"neighborhoodTier\"]=3\ndf_train.loc[df_train['Neighborhood'].isin(mid_upper_tier_n), \"neighborhoodTier\"]=2\ndf_train.loc[df_train['Neighborhood'].isin(mid_low_tier_n), \"neighborhoodTier\"]=1\ndf_train.loc[df_train['Neighborhood'].isin(lowest_tier_n), \"neighborhoodTier\"]=0\ndf_test.loc[df_test['Neighborhood'].isin(top_tier_n), \"neighborhoodTier\"]=3\ndf_test.loc[df_test['Neighborhood'].isin(mid_upper_tier_n), \"neighborhoodTier\"]=2\ndf_test.loc[df_test['Neighborhood'].isin(mid_low_tier_n), \"neighborhoodTier\"]=1\ndf_test.loc[df_test['Neighborhood'].isin(lowest_tier_n), \"neighborhoodTier\"]=0","9be0c37d":"cleanup_nums = {\"Alley\":     {\"Grvl\": 1, \"Pave\": 2},\n                \"LotShape\":     {\"Reg\": 0, \"IR1\": 1, \"IR2\" : 2, \"IR3\" : 3},\n               \"LandSlope\":{\"Gtl\":0,\"Mod\":1,\"Sev\":2},\n               \"ExterQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n               \"ExterCond\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n               \"BsmtQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n               \"BsmtCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n               \"BsmtExposure\": {\"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4},\n               \"BsmtFinType1\": {\"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5,\n                               \"GLQ\":6},\n               \"BsmtFinType2\": {\"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5,\n                               \"GLQ\":6},\n               \"HeatingQC\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n               \"CentralAir\": {\"N\": 0, \"Y\": 1},\n               \"KitchenQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n                \"Functional\": {\"Sal\": 0, \"Sev\": 1, \"Maj2\": 2, \"Maj1\": 3, \"Mod\": 4,\n                              \"Min2\": 5, \"Min1\": 6, \"Typ\": 7},\n                \"FireplaceQu\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                 \"GarageFinish\": {\"Unf\": 1, \"RFn\": 2, \"Fin\": 3},\n                \"GarageQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                \"GarageCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n                \"PoolQC\": {\"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n                \"Fence\": {\"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4},\n               }","e95dbe5a":"df_train = df_train.replace(cleanup_nums)\ndf_test = df_test.replace(cleanup_nums)","fd801255":"cat_ord_vars=[ \"MSZoning\", \"Street\", \"LandContour\", \\\n             \"LotConfig\", \"Neighborhood\", \"BldgType\", \\\n             \"RoofStyle\", \"RoofMatl\", \"MasVnrType\", \\\n             \"Foundation\", \"Electrical\", \"Heating\", \"GarageType\", \"MiscFeature\", \\\n             \"SaleType\", \"SaleCondition\", \"MoSold\"]\n","769e95d4":"df_train = df_train.drop(['Utilities'] , axis=1)\ndf_test = df_test.drop(['Utilities'] , axis=1)","65950117":"df_train.loc[df_train[\"Alley\"].isnull()==True,\"Alley\"]=0\ndf_test.loc[df_test[\"Alley\"].isnull()==True,\"Alley\"]=0","1f1784bd":"all_electrical_series = df_train[\"Electrical\"].append(df_test[\"Electrical\"])\nmostCommonElectricalValue=all_electrical_series.mode().values[0]\nprint(mostCommonElectricalValue)\n\ndf_train.loc[df_train[\"Electrical\"].isnull()==True,\"Electrical\"]=mostCommonElectricalValue\ndf_test.loc[df_test[\"Electrical\"].isnull()==True,\"Electrical\"]=mostCommonElectricalValue","0de12e98":"combined = pd.concat([df_train, df_test], sort=False)\nfor nieghborhood in list(combined[\"Neighborhood\"].unique()):\n    df_train.loc[(df_train[\"LotFrontage\"].isnull()==True) & \\\n        (df_train[\"Neighborhood\"]==nieghborhood), \\\n        \"LotFrontage\"] = \\\n        combined[\"LotFrontage\"].groupby(combined[\"Neighborhood\"]).median()[nieghborhood]\n    df_test.loc[(df_test[\"LotFrontage\"].isnull()==True) & \\\n        (df_test[\"Neighborhood\"]==nieghborhood), \\\n        \"LotFrontage\"] = \\\n        combined[\"LotFrontage\"].groupby(combined[\"Neighborhood\"]).median()[nieghborhood]","8866f084":"df_train.loc[df_train[\"Fireplaces\"]==0,\"FireplaceQu\"]=0\ndf_test.loc[df_test[\"Fireplaces\"]==0,\"FireplaceQu\"]=0","6b6edf20":"df_train.loc[df_train[\"PoolArea\"]==0,\"PoolQC\"]=0\ndf_test.loc[df_test[\"PoolArea\"]==0,\"PoolQC\"]=0","10a5f699":"sd_of_pool_sizes = np.mean(df_train.loc[df_train[\"PoolArea\"] > 0 , \"PoolArea\"].append(\n    df_test.loc[df_test[\"PoolArea\"] > 0 , \"PoolArea\"]))\nprint (sd_of_pool_sizes)\nfor idx, row in df_test.loc[df_test[\"PoolQC\"].isnull(),:].iterrows():\n    minPoolArea = row[\"PoolArea\"] - sd_of_pool_sizes\n    if minPoolArea < 0:\n        minPoolArea = 0\n    maxPoolArea = row[\"PoolArea\"] + sd_of_pool_sizes\n    df_train_poolqc_df = df_train.loc[(df_train[\"PoolArea\"] > minPoolArea) & \\\n                                      (df_train[\"PoolArea\"] < maxPoolArea),\n                                      [\"PoolQC\"]]\n    df_test_poolqc_df = df_test.loc[(df_test[\"PoolArea\"] > minPoolArea) & \\\n                                        (df_test[\"PoolArea\"] < maxPoolArea),\n                                        [\"PoolQC\"]]\n    meanPoolQC = np.round(df_train_poolqc_df.append(df_test_poolqc_df).mean(skipna=True),0)\n    df_test.loc[idx,\"PoolQC\"] = meanPoolQC.values[0]\n        #.mean(skipna=True)\n    ","33f53e5e":"df_train.loc[df_train[\"Functional\"].isnull(),\"Functional\"]=7\ndf_test.loc[df_test[\"Functional\"].isnull(),\"Functional\"]=7","26c93036":"df_train.loc[df_train[\"Fence\"].isnull(),\"Fence\"]=0\ndf_test.loc[df_test[\"Fence\"].isnull(),\"Fence\"]=0","41773560":"# Change `HasMasVnr` to 0 if `MasVnrArea` and `MasVnrType` are Null.\ndf_train.loc[ \\\n             df_train.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1), \\\n             \"HasMasVnr\"]=0\ndf_test.loc[ \\\n            df_test.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1), \\\n            \"HasMasVnr\"]=0\n\n# Change `MasVnrArea` to 0 if `MasVnrArea` is Null and `MasVnrType` are Null.\ndf_train.loc[ \\\n             df_train.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1),\n             \"MasVnrArea\"]=0\ndf_test.loc[ \\\n            df_test.loc[:,[\"MasVnrArea\",\"MasVnrType\"]].isnull().all(axis=1),\n            \"MasVnrArea\"]=0\n\n# Change `HasMasVnr` to 0 if `MasVnrArea` == 0 and `MasVnrType` are Null.\ndf_train.loc[ \\\n             (df_train.loc[:,[\"MasVnrType\"]].isnull().all(axis=1)) & \\\n             (df_train[\"MasVnrArea\"]==0), \\\n             \"HasMasVnr\"]=0\ndf_test.loc[ \\\n            (df_test.loc[:,[\"MasVnrType\"]].isnull().all(axis=1)) & \\\n            (df_test[\"MasVnrArea\"]==0), \\\n            \"HasMasVnr\"]=0\n\n# Change `HasMasVnr` to 1 if `MasVnrArea` > 0 or `MasVnrType` is not Null.\ndf_train.loc[(df_train[\"MasVnrArea\"]>0) | (df_train[\"MasVnrType\"].isnull()==False),\"HasMasVnr\"]=1\ndf_test.loc[(df_test[\"MasVnrArea\"]>0) | (df_test[\"MasVnrType\"].isnull()==False),\"HasMasVnr\"]=1","78997e21":"garage_cat_features=[\"GarageType\",\"GarageYrBlt\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"GarageFinish\"]\n#garage_cat_features = [x for x in garage_cat_features_all if x not in lowInfoGainCols]\ngarage_features = garage_cat_features + [\"GarageArea\", \"GarageCars\"]\ndf_train.loc[ \\\n    (df_train.loc[:,garage_cat_features].isnull().all(axis=1)) & \\\n             ((df_train[\"GarageArea\"] == 0) |  (df_train[\"GarageArea\"].isnull() == True)) & \\\n             ((df_train[\"GarageCars\"] == 0) |  (df_train[\"GarageCars\"].isnull() == True)), \\\n    \"HasGarage\"]=0\ndf_train.loc[ \\\n    (df_train.loc[:,garage_cat_features].notnull().any(axis=1)) | \\\n             (df_train[\"GarageArea\"] > 0) | \\\n             (df_train[\"GarageCars\"] > 0), \\\n    \"HasGarage\"]=1\n\ndf_test.loc[ \\\n    (df_test.loc[:,garage_cat_features].isnull().all(axis=1)) & \\\n             ((df_test[\"GarageArea\"] == 0) |  (df_test[\"GarageArea\"].isnull() == True)) & \\\n             ((df_test[\"GarageCars\"] == 0) |  (df_test[\"GarageCars\"].isnull() == True)), \\\n    \"HasGarage\"]=0\ndf_test.loc[ \\\n    (df_test.loc[:,garage_cat_features].notnull().any(axis=1)) | \\\n             (df_test[\"GarageArea\"] > 0) | \\\n             (df_test[\"GarageCars\"] > 0), \\\n    \"HasGarage\"]=1","fb6ef5d5":"for gar_feat in [\"GarageQual\",\"GarageFinish\", \"GarageCond\", \"GarageArea\", \"GarageCars\",\"GarageType\"]:\n    df_train.loc[df_train[\"HasGarage\"]==0,gar_feat]=0\n    df_test.loc[df_test[\"HasGarage\"]==0,gar_feat]=0","cc5acb46":"combined = pd.concat([df_train, df_test], sort=False)\ngarageYrBlt_mode = combined[\"GarageYrBlt\"].mode(dropna=True).values[0]\nprint(\"GarageYrBlt Mode: \"+ str(garageYrBlt_mode))\n\ndf_train.loc[df_train[\"HasGarage\"]==0,\"GarageYrBlt\"]=garageYrBlt_mode\ndf_test.loc[df_test[\"HasGarage\"]==0,\"GarageYrBlt\"]=garageYrBlt_mode\n\ndf_train.loc[df_train[\"GarageYrBlt\"].isnull(),\"GarageYrBlt\"] = garageYrBlt_mode\ndf_test.loc[df_test[\"GarageYrBlt\"].isnull(),\"GarageYrBlt\"] = garageYrBlt_mode","21527186":"low_cols=[]\nfor gcol in garage_features:\n    print(\"Number of %s Nulls in Training + Testing Respectively:\" % gcol)\n    print(df_train[gcol].isnull().sum())\n    print(df_test[gcol].isnull().sum())\n    if df_train[gcol].isnull().sum() + df_test[gcol].isnull().sum() < 3:\n        low_cols.append(gcol)\nprint(\"rows with low amount of null values\")\nprint(low_cols)","1488e516":"df_test.loc[df_test.loc[:,low_cols].isnull().any(axis=1), \\\n            low_cols]","457148a1":"for grow in low_cols:\n    mode_val = combined[grow].mode().values[0]\n    if (df_train[grow].dtypes == \"int64\"):\n        mode_val = int(np.round(mode_val,0))\n    df_test.loc[df_test[grow].isnull(),grow]=mode_val\n    print(mode_val)\n    #print(df_test.loc[df_test[grow].isnull(),grow])\n","3a1b7730":"df_test.iloc[[666,1116],:].loc[:,low_cols]","ffc3ebe3":"basement_cat_features=[\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"]\n#basement_cat_features = [x for x in basement_cat_features_all if x not in lowInfoGainCols]\nbasement_features= basement_cat_features + \\\n    [\"HasBasement\",\"BsmtFinSF1\",\"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]\n\ndf_train.loc[ \\\n    (df_train.loc[:,basement_cat_features].isnull().all(axis=1)) & \\\n             ((df_train[\"BsmtFinSF1\"] == 0) | (df_train[\"BsmtFinSF1\"].isnull() == True)) & \\\n             ((df_train[\"BsmtFinSF2\"] == 0) | (df_train[\"BsmtFinSF2\"].isnull() == True)) & \\\n             ((df_train[\"BsmtUnfSF\"] == 0) | (df_train[\"BsmtUnfSF\"].isnull() == True)) & \\\n             ((df_train[\"TotalBsmtSF\"] == 0) | (df_train[\"TotalBsmtSF\"].isnull() == True)) & \\\n             ((df_train[\"BsmtFullBath\"] == 0) | (df_train[\"BsmtFullBath\"].isnull() == True)) & \\\n             ((df_train[\"BsmtHalfBath\"] == 0) | (df_train[\"BsmtHalfBath\"].isnull() == True)), \\\n    \"HasBasement\"]=0\n#df_train.loc[ \\\n#    (df_train.loc[:,basement_features].isnull().all(axis=1)), \\\n#    \"HasBasement\"]=0\ndf_train.loc[ \\\n    (df_train.loc[:,basement_cat_features].notnull().any(axis=1)) | \\\n             (df_train[\"BsmtFinSF1\"] > 0) | \\\n             (df_train[\"BsmtFinSF2\"] > 0) | \\\n             (df_train[\"BsmtUnfSF\"] > 0) | \\\n             (df_train[\"TotalBsmtSF\"] > 0) | \\\n             (df_train[\"BsmtFullBath\"] > 0) | \\\n             (df_train[\"BsmtHalfBath\"] > 0), \\\n    \"HasBasement\"]=1\n\ndf_test.loc[ \\\n    (df_test.loc[:,basement_cat_features].isnull().all(axis=1)) & \\\n             ((df_test[\"BsmtFinSF1\"] == 0) | (df_test[\"BsmtFinSF1\"].isnull() == True)) & \\\n             ((df_test[\"BsmtFinSF2\"] == 0) | (df_test[\"BsmtFinSF2\"].isnull() == True)) & \\\n             ((df_test[\"BsmtUnfSF\"] == 0) | (df_test[\"BsmtUnfSF\"].isnull() == True)) & \\\n             ((df_test[\"TotalBsmtSF\"] == 0) | (df_test[\"TotalBsmtSF\"].isnull() == True)) & \\\n             ((df_test[\"BsmtFullBath\"] == 0) | (df_test[\"BsmtFullBath\"].isnull() == True)) & \\\n             ((df_test[\"BsmtHalfBath\"] == 0) | (df_test[\"BsmtHalfBath\"].isnull() == True)), \\\n    \"HasBasement\"]=0\n#df_test.loc[ \\\n#    (df_test.loc[:,basement_features].isnull().all(axis=1)), \\\n#    \"HasBasement\"]=0\ndf_test.loc[ \\\n    (df_test.loc[:,basement_cat_features].notnull().any(axis=1)) | \\\n             (df_test[\"BsmtFinSF1\"] > 0) | \\\n             (df_test[\"BsmtFinSF2\"] > 0) | \\\n             (df_test[\"BsmtUnfSF\"] > 0) | \\\n             (df_test[\"TotalBsmtSF\"] > 0) | \\\n             (df_test[\"BsmtFullBath\"] > 0) | \\\n             (df_test[\"BsmtHalfBath\"] > 0), \\\n    \"HasBasement\"]=1","403bd0b7":"for basem_feat in basement_features:\n    if basem_feat not in cat_ord_vars:\n        df_train.loc[df_train[\"HasBasement\"]==0,basem_feat]=0\n        df_test.loc[df_test[\"HasBasement\"]==0,basem_feat]=0","05ef0285":"for bcol in basement_features:\n    print(\"Number of %s Nulls in Training + Testing Respectively:\" % bcol)\n    print(df_train[bcol].isnull().sum())\n    print(df_test[bcol].isnull().sum())","3a729b3a":"df_train.loc[df_train[\"BsmtQual\"].isnull(),\"BsmtQual\"]=df_train.loc[df_train[\"BsmtQual\"].isnull(),\"BsmtCond\"]\ndf_test.loc[df_test[\"BsmtQual\"].isnull(),\"BsmtQual\"]=df_test.loc[df_test[\"BsmtQual\"].isnull(),\"BsmtCond\"]\ndf_train.loc[df_train[\"BsmtCond\"].isnull(),\"BsmtCond\"]=df_train.loc[df_train[\"BsmtCond\"].isnull(),\"BsmtQual\"]\ndf_test.loc[df_test[\"BsmtCond\"].isnull(),\"BsmtCond\"]=df_test.loc[df_test[\"BsmtCond\"].isnull(),\"BsmtQual\"]","9dea9547":"mode_basement_exposure_given_basement = np.round(\n    df_train.loc[df_train[\"HasBasement\"]==1,\"BsmtExposure\"].append(\n        df_test.loc[df_test[\"HasBasement\"]==1,\"BsmtExposure\"]).mode().values[0],0)\nprint(mode_basement_exposure_given_basement)\ndf_train.loc[df_train[\"BsmtExposure\"].isnull(),\"BsmtExposure\"] = mode_basement_exposure_given_basement\ndf_test.loc[df_test[\"BsmtExposure\"].isnull(),\"BsmtExposure\"] = mode_basement_exposure_given_basement","4e8a0a17":"print(df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinSF2\"])","64e025b2":"BsmtFinSF2_train_test_df = \\\n    df_train.loc[df_train[\"BsmtFinSF2\"]>0,[\"BsmtFinSF2\",\"BsmtFinType2\"]].append(\n    df_test.loc[df_test[\"BsmtFinSF2\"]>0,[\"BsmtFinSF2\",\"BsmtFinType2\"]])\nnp.std(BsmtFinSF2_train_test_df[\"BsmtFinSF2\"])","73d6c37a":"min_BsmtFinSF2 = df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinSF2\"] - \\\n    np.std(BsmtFinSF2_train_test_df[\"BsmtFinSF2\"])\nmax_BsmtFinSF2 = df_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinSF2\"] + \\\n    np.std(BsmtFinSF2_train_test_df[\"BsmtFinSF2\"])\nsubset_BsmtFinSF2_train_test_df = BsmtFinSF2_train_test_df.loc[ \\\n    (BsmtFinSF2_train_test_df[\"BsmtFinSF2\"] > min_BsmtFinSF2.values[0]) & \\\n    (BsmtFinSF2_train_test_df[\"BsmtFinSF2\"] < max_BsmtFinSF2.values[0]),\"BsmtFinType2\"]\n\ndf_train.loc[df_train[\"BsmtFinType2\"].isnull(),\"BsmtFinType2\"] = \\\n    np.round(subset_BsmtFinSF2_train_test_df.mean(skipna=True),0)","a874ba71":"df_train.loc[df_train[\"KitchenQual\"].isnull(),\"KitchenQual\"] = np.round(df_train[\"KitchenQual\"].append(df_test[\"KitchenQual\"]).mean(skipna=True),0)\ndf_test.loc[df_test[\"KitchenQual\"].isnull(),\"KitchenQual\"] = np.round(df_train[\"KitchenQual\"].append(df_test[\"KitchenQual\"]).mean(skipna=True),0)","bb600294":"num_sqrt_candidates = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \\\n                       'BsmtFinSF2','BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \\\n                       '2ndFlrSF', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF']\nfor num_feat in num_sqrt_candidates:\n    new_num_col = num_feat +\"_SQRT\"\n    df_train.loc[:,new_num_col]=np.sqrt(df_train.loc[:,num_feat].values)\n    df_test.loc[:,new_num_col]=np.sqrt(df_test.loc[:,num_feat].values)\n    \n    \nnum_log_candidates = ['LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\nfor num_feat in num_log_candidates:\n    new_num_col = num_feat +\"_LOG\"\n    df_train.loc[:,new_num_col]=np.log(df_train.loc[:,num_feat].values+.001)\n    df_test.loc[:,new_num_col]=np.log(df_test.loc[:,num_feat].values+.001)","48c7925a":"# add one-hot encoded columns based on `col` in `df` to `df`\ndef one_hot_encode(df, col):\n    df[col] = pd.Categorical(df[col])\n    dfDummies = pd.get_dummies(df[col], prefix = col)\n    df = pd.concat([df, dfDummies], axis=1)\n    #df = df.drop([col],axis=1)\n    return(df)","23f6d127":"for cat_ord_col in cat_ord_vars:\n        df_train = one_hot_encode(df_train,cat_ord_col)\n        df_test = one_hot_encode(df_test,cat_ord_col)","5cabd458":"train_cols = list(df_train.columns.sort_values().unique())\ntest_cols = list(df_test.columns.sort_values().unique())\nuniq_train_cols = [x for x in train_cols if (x not in test_cols and x != \"SalePrice\")]\nuniq_test_cols = [x for x in test_cols if x not in train_cols]\n\nprint(\"length of unique training columns: \"+ str(len(uniq_train_cols)))\nprint(\"length of unique test columns: \"+ str(len(uniq_test_cols)))\ndf_train = df_train.drop(uniq_train_cols,axis=1)\ndf_test = df_test.drop(uniq_test_cols,axis=1)","0b870e9e":"# Overall quality of the house\ndf_train[\"OverallGrade\"] = df_train[\"OverallQual\"] * df_train[\"OverallCond\"]\ndf_test[\"OverallGrade\"] = df_test[\"OverallQual\"] * df_test[\"OverallCond\"]\n# Overall quality of the garage\ndf_train[\"GarageGrade\"] = df_train[\"GarageQual\"] * df_train[\"GarageCond\"]\ndf_test[\"GarageGrade\"] = df_test[\"GarageQual\"] * df_test[\"GarageCond\"]\n# Overall quality of the exterior\ndf_train[\"ExterGrade\"] = df_train[\"ExterQual\"] * df_train[\"ExterCond\"]\ndf_test[\"ExterGrade\"] = df_train[\"ExterQual\"] * df_train[\"ExterCond\"]\n# Overall kitchen score\ndf_train[\"KitchenScore\"] = df_train[\"KitchenAbvGr\"] * df_train[\"KitchenQual\"]\ndf_test[\"KitchenScore\"] = df_train[\"KitchenAbvGr\"] * df_train[\"KitchenQual\"]\n# Overall fireplace score\ndf_train[\"FireplaceScore\"] = df_train[\"Fireplaces\"] * df_train[\"FireplaceQu\"]\ndf_test[\"FireplaceScore\"] = df_train[\"Fireplaces\"] * df_train[\"FireplaceQu\"]\n# Overall garage score\ndf_train[\"GarageScore\"] = df_train[\"GarageArea\"] * df_train[\"GarageQual\"]\ndf_test[\"GarageScore\"] = df_train[\"GarageArea\"] * df_train[\"GarageQual\"]\n# Overall pool score\ndf_train[\"PoolScore\"] = df_train[\"PoolArea\"] * df_train[\"PoolQC\"]\ndf_test[\"PoolScore\"] = df_train[\"PoolArea\"] * df_train[\"PoolQC\"]\n# Total number of bathrooms\ndf_train[\"TotalBath\"] = df_train[\"BsmtFullBath\"] + (0.5 * df_train[\"BsmtHalfBath\"]) + df_train[\"FullBath\"] + (0.5 * df_train[\"HalfBath\"])\ndf_test[\"TotalBath\"] = df_train[\"BsmtFullBath\"] + (0.5 * df_train[\"BsmtHalfBath\"]) + df_train[\"FullBath\"] + (0.5 * df_train[\"HalfBath\"])\n# Total SF for house (incl. basement)\ndf_train[\"AllSF\"] = df_train[\"GrLivArea\"] + df_train[\"TotalBsmtSF\"]\ndf_test[\"AllSF\"] = df_train[\"GrLivArea\"] + df_train[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\ndf_train[\"AllFlrsSF\"] = df_train[\"1stFlrSF\"] + df_train[\"2ndFlrSF\"]\ndf_test[\"AllFlrsSF\"] = df_train[\"1stFlrSF\"] + df_train[\"2ndFlrSF\"]\n# Total SF for porch\ndf_train[\"AllPorchSF\"] = df_train[\"OpenPorchSF\"] + df_train[\"EnclosedPorch\"] + df_train[\"3SsnPorch\"] + df_train[\"ScreenPorch\"]\ndf_test[\"AllPorchSF\"] = df_train[\"OpenPorchSF\"] + df_train[\"EnclosedPorch\"] + df_train[\"3SsnPorch\"] + df_train[\"ScreenPorch\"]\n# Has masonry veneer or not\ndf_train[\"HasMasVnr\"] = df_train.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \"Stone\" : 1, \"None\" : 0})\ndf_test[\"HasMasVnr\"] = df_train.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \"Stone\" : 1, \"None\" : 0})\n# House completed before sale or not\ndf_train[\"BoughtOffPlan\"] = df_train.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\ndf_test[\"BoughtOffPlan\"] = df_train.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n","abb3441e":"# drop categorical ordinal columns that are not encoded\ndf_train_numerical = df_train.drop(cat_ord_vars, axis=1)\n# drop categorical ordinal columns that are not encoded\ndf_test_numerical = df_test.drop(cat_ord_vars, axis=1)","4b2b5333":"if 1==1:\n    garageCarsReplaceDict = {\"GarageCars\":{4:3}}\n\n    df_train_numerical = df_train_numerical.replace(garageCarsReplaceDict)\n    df_test_numerical = df_test_numerical.replace(garageCarsReplaceDict)","8515bf15":"df_train_numerical.loc[:,'GarageScore_Log']=np.log(df_train_numerical.loc[:,'GarageScore'].values + .01)\ndf_test_numerical.loc[:,'GarageScore_Log']=np.log(df_test_numerical.loc[:,'GarageScore'].values + .01)","5e20f2dc":"if 1==1:\n    FullBathReplaceDict = {\"FullBath\":{0:1}}\n    df_train_numerical = df_train_numerical.replace(FullBathReplaceDict)\n    df_test_numerical = df_test_numerical.replace(FullBathReplaceDict)","4ba7b498":"year_data_released=2010\ndf_train_numerical['YearRemodAdd'] = year_data_released - df_train_numerical['YearRemodAdd']\ndf_train_numerical['YearBuilt'] = year_data_released - df_train_numerical['YearBuilt']\ndf_train_numerical['YearRemodelPlusBuilt'] = df_train_numerical['YearRemodAdd'] + df_train_numerical['YearBuilt']\ndf_test_numerical['YearRemodAdd'] = year_data_released - df_test_numerical['YearRemodAdd']\ndf_test_numerical['YearBuilt'] = year_data_released - df_test_numerical['YearBuilt']\ndf_test_numerical['YearRemodelPlusBuilt'] = df_test_numerical['YearRemodAdd'] + df_test_numerical['YearBuilt']","51a2e423":"df_train_numerical.drop(df_train_numerical[df_train_numerical[\"GrLivArea\"] > 4000].index, inplace=True)","a705fa3a":"combined = pd.concat([df_train_numerical, df_test_numerical], sort=False)\none_hot_cols_lofl = []\nfor car_ord_predix in cat_ord_vars + [\"Exterior\", \"Condition\"]:\n    one_hot_cols_lofl.append(\n        [x for x in list(combined.columns) if car_ord_predix in x])\none_hot_cols = [item for sublist in one_hot_cols_lofl for item in sublist]","d4adb050":"one_hot_cols = [x for x in one_hot_cols if x in list(df_train_numerical.columns)]\n\nbest_one_hot_cols=[]\nfor var in one_hot_cols:\n    val0_mean = df_train_numerical.groupby(var)['SalePrice'].mean()[0]\n    val0_std = df_train_numerical.groupby(var)['SalePrice'].std()[0]\n    val1_mean = df_train_numerical.groupby(var)['SalePrice'].mean()[1]\n    if val1_mean < (val0_mean-val0_std) or val1_mean >(val0_mean+val0_std):\n        if var not in best_one_hot_cols:\n            best_one_hot_cols.append(var)\n        data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nprint(best_one_hot_cols)","35376616":"combined.columns[combined.isna().any()].tolist()","00377627":"combined = pd.concat([df_train_numerical, df_test_numerical], sort=False)\nnonNA_cols=list(combined.dropna(axis=1).drop('Id',axis=1).columns)","207e966c":"fav_variables1=['OverallQual','YearRemodAdd','FireplaceQu', \\\n               'GarageFinish', 'FullBath', 'GarageCars', \\\n               'KitchenQual', 'ExterQual', 'ExterQual',\n              'GrLivArea', 'GarageScore', 'TotalBsmtSF','YearRemodelPlusBuilt'] \\\n                + one_hot_cols","fe59b125":"for fv in fav_variables1:\n    if fv not in list(df_train_numerical.columns):\n        print(\"train no\")\n        print(fv)\n    if fv not in list(df_test_numerical.columns):\n        print(\"test no\")\n        print(fv)","0c77f941":"def linear_reg(df_train,df_test,features):\n    X_train = df_train.loc[:,features].copy()\n    y_train = df_train.loc[:,\"SalePrice\"].copy()\n    X_pred = df_test.loc[:,features].copy()\n    \n    reg = LinearRegression().fit(X_train, y_train)\n    scores = np.sqrt(-cross_val_score(reg,X_train,y_train, cv=5, scoring='neg_mean_squared_error'))\n    print(scores)\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    \n    pred = reg.predict(X_pred)\n    return(pred)","2c8cd9a1":"pred1 = linear_reg(df_train_numerical,\n                  df_test_numerical,\n                  fav_variables1)\ndf_test.loc[:,\"SalePrice\"]=np.exp(pred1)\nsub1_df = df_test.loc[:,[\"Id\",\"SalePrice\"]].copy()\nsub1_df.to_csv(\"submission_linearRegress_favVars1.csv\", index=False)","758503e0":"pred2 = linear_reg(df_train_numerical,\n                  df_test_numerical,\n                  nonNA_cols)\ndf_test.loc[:,\"SalePrice\"]=np.exp(pred2)\nsub2_df = df_test.loc[:,[\"Id\",\"SalePrice\"]].copy()\nsub2_df.to_csv(\"submission_linearRegress_allVars.csv\", index=False)","29be53bf":"def ridge_reg(df_train,df_test,features, alpha, predict=False):\n    from sklearn import linear_model\n    X_train = df_train.loc[:,features].copy()\n    y_train = df_train.loc[:,\"SalePrice\"].copy()\n    X_pred = df_test.loc[:,features].copy()\n    \n    clf = linear_model.Ridge(alpha=alpha).fit(X_train, y_train)\n    scores = np.sqrt(-cross_val_score(clf,X_train,y_train, cv=5, scoring='neg_mean_squared_error'))\n    #print(scores)\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    \n    if predict:\n        pred = clf.predict(X_pred)\n        return(pred)\n    else:\n        return(scores)\n\nalphas=[0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge_mean_scores=[]\n\nfor i in alphas:\n    print(\"alpha=\"+str(i))\n    cv_ridge = np.mean(ridge_reg(df_train_numerical,\n                df_test_numerical,\n                fav_variables1,i))\n    cv_ridge_mean_scores.append(cv_ridge)","5b34b442":"cv_ridge_ser = pd.Series(cv_ridge_mean_scores, index = alphas)\ncv_ridge_ser.plot(title = \"Ridge Regression Validation (favorite variables)\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","7185246b":"# let's summit with alpha=5\npred3 = ridge_reg(df_train_numerical,\n                  df_test_numerical,\n                  fav_variables1,5, True)\ndf_test.loc[:,\"SalePrice\"]=np.exp(pred3)\nsub3_df = df_test.loc[:,[\"Id\",\"SalePrice\"]]\nsub3_df.to_csv(\"submission_ridge_favoriteVars1.csv\", index=False)","cea8f758":"alphas=[0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge_mean_scores=[]\n\nfor i in alphas:\n    print(\"alpha=\"+str(i))\n    cv_ridge = np.mean(ridge_reg(df_train_numerical,\n                df_test_numerical,\n                nonNA_cols,i))\n    cv_ridge_mean_scores.append(cv_ridge)","8193a131":"cv_ridge_ser = pd.Series(cv_ridge_mean_scores, index = alphas)\ncv_ridge_ser.plot(title = \"Ridge Regression Validation (all variables)\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","df3c9d7c":"# let's summit with alpha=10\npred4 = ridge_reg(df_train_numerical,\n                  df_test_numerical,\n                  nonNA_cols,10, True)\ndf_test.loc[:,\"SalePrice\"]=np.exp(pred4)\nsub4_df = df_test.loc[:,[\"Id\",\"SalePrice\"]]\nsub4_df.to_csv(\"submission_ridge_allVars.csv\", index=False)","283095c3":"def lasso_reg(df_train,df_test,features, alpha, predict=False):\n    X_train = df_train.loc[:,features].copy()\n    y_train = df_train.loc[:,\"SalePrice\"].copy()\n    X_pred = df_test.loc[:,features].copy()\n    \n    clf = Lasso(alpha=alpha).fit(X_train, y_train)\n    scores = np.sqrt(-cross_val_score(clf,X_train,y_train, cv=5, scoring='neg_mean_squared_error'))\n    #print(scores)\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    \n    if predict:\n        pred = clf.predict(X_pred)\n        return(pred)\n    else:\n        return(scores)\n\nalphas=[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 1]\ncv_lasso_mean_scores=[]\nfor i in alphas:\n    print(\"alpha=\"+str(i))\n    cv_lasso = np.mean(lasso_reg(df_train_numerical,\n                  df_test_numerical,\n                  fav_variables1,i))\n    cv_lasso_mean_scores.append(cv_lasso)\n","bfdbac63":"cv_lasso_ser = pd.Series(cv_lasso_mean_scores, index = alphas)\ncv_lasso_ser.plot(title = \"Lasso Regression Validation (favorite variables)\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","d6374302":"lassoCVresults = LassoCV(alphas = alphas, cv=5).fit(\n    df_train_numerical.loc[:,fav_variables1].copy(), \n    df_train_numerical.loc[:,\"SalePrice\"].copy())\nprint(lassoCVresults.alpha_)\n\ncoef = pd.Series(lassoCVresults.coef_, index = fav_variables1)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\ncoef.sort_values(ascending=False).head(10)","02a04a21":"# let's summit with alpha=0.0005\npred5 = lasso_reg(df_train_numerical,\n                  df_test_numerical,\n                  fav_variables1,lassoCVresults.alpha_, True)\ndf_test.loc[:,\"SalePrice\"]=np.exp(pred5)\nsub5_df = df_test.loc[:,[\"Id\",\"SalePrice\"]]\nsub5_df.to_csv(\"submission_lasso_favoriteVars1.csv\", index=False)","c37e6ae7":"lassoCVresults = LassoCV(alphas = alphas, cv=5).fit(\n    df_train_numerical.loc[:,nonNA_cols].copy(), \n    df_train_numerical.loc[:,\"SalePrice\"].copy())\nprint(lassoCVresults.alpha_)","b6b0449a":"coef = pd.Series(lassoCVresults.coef_, index = nonNA_cols)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","01adfe47":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\n\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","88a66192":"# let's summit with alpha=0.0005\npred6 = lasso_reg(df_train_numerical,\n                  df_test_numerical,\n                  nonNA_cols,lassoCVresults.alpha_, True)\ndf_test.loc[:,\"SalePrice\"]=np.exp(pred6)\nsub6_df = df_test.loc[:,[\"Id\",\"SalePrice\"]]\nsub6_df.to_csv(\"submission_lasso_allVars.csv\", index=False)","ac7b0723":"def run_knn(df_train,df_test,features):\n    X_train = df_train.loc[:,features].copy()\n    y_train = df_train.loc[:,\"SalePrice\"].copy()\n    X_pred = df_test.loc[:,features].copy()\n    \n    \n    for K in range(20):\n        K=K+1\n        knn = neighbors.KNeighborsRegressor(n_neighbors = K)\n        scores = np.sqrt(-cross_val_score(knn,X_train,y_train, cv=5, scoring='neg_mean_squared_error'))\n        print(K)\n        print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","82dea318":"pred_knn = run_knn(df_train_numerical,\n                  df_test_numerical,\n                  fav_variables1)","55690b24":"# for some reason these columns were not already type float\nfor col in [\"BsmtQual\", \"BsmtCond\", \"GarageQual\", \"GarageGrade\"]:\n    df_train_numerical[col] = df_train_numerical[col].astype(float)\n    df_test_numerical[col] = df_test_numerical[col].astype(float)","4a634cd5":"def xgboost_reg(df_train,df_test,features, regr, predict=False):\n    X_train = df_train.loc[:,features].copy()\n    y_train = df_train.loc[:,\"SalePrice\"].copy()\n    X_pred = df_test.loc[:,features].copy()\n    \n    xgb_fit = regr.fit(X_train, y_train)\n    scores = np.sqrt(-cross_val_score(xgb_fit,X_train,y_train, cv=5, scoring='neg_mean_squared_error'))\n    print(scores)\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    \n    if predict:\n        pred = xgb_fit.predict(X_pred)\n        return(pred)\n    else:\n        return(scores)\n","59a0b7aa":"xgb1 = xgb.XGBRegressor(\n    colsample_bytree=0.2,\n    learning_rate=0.01,\n    max_depth=4,\n    min_child_weight=1.5,\n    n_estimators=7200,\n    gamma=0.0,\n    reg_alpha=0.9,\n    reg_lambda=0.6,\n    subsample=0.2,\n    objective= 'reg:squarederror',\n    seed=14)\n\nxgb_pred_1 = xgboost_reg(df_train_numerical,\n                  df_test_numerical,\n                  nonNA_cols, xgb1, True)","fe26a04d":"df_test.loc[:,\"SalePrice\"]=np.exp(xgb_pred_1)\nsub7_df = df_test.loc[:,[\"Id\",\"SalePrice\"]].copy()\nsub7_df.to_csv(\"submission_xgBoost_allVars.csv\", index=False)","00e7d207":"### Garage Features\nAdd `HasGarage` variable to keep track of whether house has a Garage\n* Set `HasGarage` to 0 if \n    * `GarageArea` is 0\n    * `GarageCars` is 0\n    * the following are Null:\n        * GarageType\n        * GarageYrBlt\n        * GarageFinish\n        * GarageCars\n        * GarageQual\n        * GarageCond\n* Otherwise set `HasGarage` to 1 ","d5895312":"### KitchenQual\nIf there is no Kitchen Quality value then set it to the average kitchen quality value","09307a0a":"## Favorite Variables (Set1)\n","2b865411":"In addition, I create a feature that checks whether the exterior is made of 1 or 2 materials.","94f00f65":"For basement exposure, I just set it to the mode value based on both the training and test set where HasBasement is true.","9c048909":"## Impute Null Values","8470e8b1":"## Transform Categorical Ordinal Variables","b1338dae":"Then I found the standard deviation of the `BsmtFinSF2` in houses with basements","02c2a344":"### Alley\nIf row is missing the `Alley` variable then there is no Alley Access so it should be set to 0.","44a840e5":"The author of the dataset specifically recommends removing 'any houses with more than 4000 square feet' from the dataset. Reference : https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf","c1a460f6":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 5 is about right based on the plot above.","1fca25be":"## HouseStyle (Feature Engineering)","a3e001e5":" I want to one-hot code these 2 columns by treating them as 1 so that we can check if the exterior has a specific type of covering. For example, `Exterior_CBlock` would be 1 if there is a Cinder Block covering on the house and 0 would be if `Exterior1st` and `Exterior2nd` are both not equal to \"CBlock\".","295ec474":"# Cross-Validate\nIn this competition we are evaluated by the root-mean-squared-error (RMSE)","7a391a36":"\n### Lot frontage\nI'm assuming that every property has a lot (since LotFrontage is always >0) so I'll just imput the median based on the neighborhood of the house.","354ad0ad":"## Transform Year Features","e8a1e02b":"# Modify Datasets\nIn this section we will\n* perform feature engineering (create new features)\n* transform features\n* replace values of features\n* drop columns\n\nin no particular order","5eabe2cc":"## NeighborhoodTier (Feature Engineering)\nI want to create a columns that tells what type of \"Tier\" neigborhood the house is located in. \n* The top Tier will have an average Sale price above or equal to the 75% percentile of the SalePrices. \n* The middle upper Tier will have an average Sale price between the 50th and 75th percentile. \n* The middle low Tier will have an average Sale price between the 25th and 50th percentile. \n* The bottom tier will have an average Sale price below the 25th percentile.","e04f7d30":"This model does not do as well as previous ones.","bccd671d":"### PoolQC\nSet Pool Quality to 0 if there is no pool","87c96e06":"## Condition (Feature Engineering)","466fd35b":"convert categorical-ordinal variables to integers","bbf89025":"unfortunately we are not done with the null features in the Garage Columns","6e3fdd5b":"If GarageYrBlt is null, just set GarageYrBlt to the most common year that Garages are built.","ff9724a9":"## KNN\nNow let's try K-nearest neighbors and set K equal to 1-20.","f3a16935":"I took the average of \"BsmtFinType2\" values within 1 SD around the `BsmtFinSF2` value","9b639d4d":"Since there are only 2 rows with null values in the GarageFinish,GarageQual, GarageCond, GarageArea, and GarageCars variables in the testing data I'll just set them to the mode values since there are so few rows with nulls anyway.","18898807":"In some models you can import all of the features","4847a0e8":"Since there is only 1 null row with `BsmtFinType2` and `BsmtFinType2` is highly correlated with `BsmtFinSF2`, I got the \"BsmtFinSF2\" value of the null row:","bf1ebefe":"### Fence\nSet Fence Quality to 0 if there is no fence","a854ceeb":"We get a low RMSE score from this model.","0750f4c8":"## Fix GarageCars\nIn my exploratory notebook I found that I should merge houses with 3 or more GarageCars. However, when I implemented this into the model I got worse predictions.","6c577101":"## Transform Continuous Variables","833870fe":"### Electrical\nOver 90% of the rows that contain `Electrical` values have the same value. Therefore, if row is missing the electrical variable then set it to the mode value of Electrical.","97ab6ac8":"## Fix Weird Values (MasVnrArea)\nWe expect that if the \"MasVnrArea\" is 0 that the \"MasVnrType\" is none. If the \"MasVnrType\" is not none then we should recalculate the \"MasVnrArea\" since it cannot be 0.  I want to reset these values to the average \"MasVnrArea\".","ea32ed06":"## A lot of Feature Engineering\nThe code was adapted came from https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset","9f5d92ae":"Since there are only 2 and 3 missing values in \"BsmtQual\" and \"BsmtCond\"columns respectively and their values have the same range I just set the null values to the value of the other. ","d5da26de":"We get the best alpha value using scikit-learn LassoCV","2ec25e35":"Interesting choices. These are definitely different then the ones I chose as my favorite. It is weird to me that it chose \"1stFlrSF_SQRT\" to be weighted higher than \"GrLivArea\"...","a05af756":"## Transform GarageScore\nget the log of the garageScore\n","173a7e33":"## XGBoost\nTo understand XGBoost I read https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/. Here is my summary:\n\nXGBoost has 3 types of parameters:\n* General Parameters: Guide the overall functioning\n* Booster Parameters: Guide the individual booster (tree\/regression) at each step\n* Learning Task Parameters: Guide the optimization performed\n\nI chose the default general parameters. The only one that really matters is whether we want to run a tree-based(default) or a linear model. Tree-based models usually perform better so we set it to tree-based.\n\nNext, the tree booster parameters are\n* max_depth: depth of tree (default:6)\n  * too high--> overfitting\n  * usually ranges from 3-10\n* learning_rate (default:0.3)\n  * usualy ranges from 0.01-0.2\n* n_estimators: number of trees to fit\n* min_child_weight: minimum sum of weights of all obsercations required in a child (default:1)\n  * too high --> underfitting\n  * too low --> overfitting\n* max_leaf_nodes: terminal nodes\/leaves in a tree\n  *  used instead of max_depth\n  * a depth of `n` would produces `n^2` leaves\n* gamma: minimum loss reduction required to split (default:0)\n* max_delta_step (default:0)\n  * I probably will ignore this parameter\n  * higher values make model more conservative\n  * can help if class is extremely unbalanced\n* subsample: the fraction of observations to be randomly sampled for each tree (default:1)\n  * usually ranges from 0.5-1\n  * low values -> overfitting\n  * high values -> underfitting\n* colsample_bytree: the fraction of columns to be randomly samples for each tree (default:1)\n  * usually ranges from 0.5-1\n* colsample_bylevel: the subsample ratio of columns for each split, in each level (default:1)\n  * we can probably ignore this parameter since the parameters `subsample` and `colsample_bytree` are similar\n* reg_lambda (default:1)\n  * l2 regularization parameter\n  * used to reduce overfitting\n  * not often used\n* reg_alpha (default:0)\n  * l1 regularization parameter\n* scale_pos_weight (default:1)\n  * used if you have class imbalance\n  * always greater than 0\n  \nThe learning tast parameters are the `objective` and the `eval_metric`. We have a regression linear objective and we are measing error with rmse.","7ced2944":"Let's open the training and testing data","4d133d8e":"Unfortunately we still have not dealt with all of the null values for basement features.","2c816b57":"## MSSubClass (Feature Engineering)","a3715757":"# Dropping outliers from training set","7aa38d36":"Create method for XGBoost","5480086e":"To test my code I will use the paramers found in [this kernel](https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso). They are not at all cross-validated so I will need to perform a grid search to get better parameters","594fb9a2":"Machine Learning Libraries","ab783ea7":"### Drop Columns not found in Training or Testing Set","e3122915":"However, in some models you want to perform feature selection to improve the algorithm.","caff47ce":"# Choosing Best Variables","6e2732a0":"### Functionality\nIn the \"data_description.txt\" file it says to \"Assume typical unless deductions are warranted\". Therefore, we set the rows with null functionality to 7 which means its typical","6f17bb4b":"Convert null variables which actually just mean that there is no basement.","9eaeae80":"## Linear model\nLet's try cross-validating a basic linear regression model (cv=5) using only features from my favorite variables","5f4cb5fa":"## Transform SalePrice (Feature Transformation)","eaeab34f":"## Set up Categorical Ordinal Variables","71607a02":"Good job Lasso. One thing to note here however is that the features selected are not necessarily the \"correct\" ones - especially since there are a lot of collinear features in this dataset. One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is. Lets look at some of the weights given to the variables.","cf1ebbe5":"### MasVnrType and MasVnrArea\nAdd `HasMasVnr` variable to keep track of whether house has a Masonry veneer area\n* Change `HasMasVnr` to 0 if `MasVnrArea` and `MasVnrType` are Null.\n* Change `MasVnrArea` to 0 if `MasVnrArea` is Null and `MasVnrType` are Null.\n* Change `HasMasVnr` to 0 if `MasVnrArea` == 0 and `MasVnrType` are Null.\n* Change `HasMasVnr` to 1 if `MasVnrArea` > 0 or `MasVnrType` is not Null.","542ecb6f":"## Drop Utilities Column\nthe value for Utilities is almost always the same 2916\/2917 times. Let's drop this column.","a85805aa":"# Introduction\nThis kernel is a continuation of my EDA notebook: [https:\/\/www.kaggle.com\/sklasfeld\/housingdataeda-v1]\n    \nIn this kernel I look to test different models using the data I analyzed in the previous notebook.\n\nFor reference I also used the follwing kaggle kernels:\n* https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n* https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso","0585d50d":"## Exterior variables (Feature Engineering)","f49ffa24":"## Ridge Regression\nRidge Regression uses L2 regularization. The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.","cf331f30":"Column for houses that share walls.","7b0f82a5":"We take the average pool quality of pools that are around the same Area of the Pool (+\/-1SD) and set the pool quality manually to whatever the average pool quality is of pools that are that size.","91cd7437":"Given our new features, we can drop \"Exterior1st\" and \"Exterior2nd\"","d3d1c405":"### FireplaceQu\nSet Fireplace Quality to 0 if there is no fireplace","5e321594":"If there is no Garage then we can set `GarageQual`,`GarageFinish`, `GarageCond`, `GarageArea`, and `GarageCars` to 0\/","0e95a64f":"Column for houses that have a 1946 and Newer Style","0874a8b2":"Note that LassoCV method can do feature selection for you so we did not necessarily need to narrow down our favorite features. In fact, LassoCV outputs `coef_` which contains a vector of the weights for the final linear model. Let's see what happens when we use all the features.","489bcd77":"### Barement Features: 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'\nAdd `HasBasement` variable to keep track of whether house has a Basement\n* Set `HasBasement` to 0 if\n    * the following are Null:\n        * BsmtQual\n        * BsmtCond\n        * BsmtExposure\n        * BsmtFinType1\n        * BsmtFinType2\n    * BsmtFinSF1 is 0 or Null\n    * BsmtFinSF2 is 0 or Null\n    * BsmtUnfSF is 0 or Null\n    * TotalBsmtSF is 0 or Null\n    * BsmtFullBath is 0 or Null\n    * BsmtHalfBath is 0 or Null\n* Otherwise set `HasBasement` to 1 ","14f504b1":"# Categorical Nominal Variables\n* One-Hot Encode Categorical Nominal Variables","c0b0dd5e":"Column for homes with 2 stories or not.","b1b97e51":"## Lasso Regression\nLasso Regression uses L1 regularization. The main tuning parameter for the Lasso model is also alpha but it works like the inverse of the alpha in the ridge model. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.","f7b15405":"Column for homes with unfurnished floors.","3797e8c0":"## Fix FullBath\nYou need a bath in a house right? I should set the 0's to 1's? \n\nWhen I implemented this into the model I got worse predictions.","5f8dd379":"ignore future warnings..."}}