{"cell_type":{"1cf58bb3":"code","e05559c5":"code","8794db6c":"code","0c79229a":"code","48aee3dc":"code","39a49c53":"code","d932c762":"markdown","265b42d6":"markdown","7b94d1f3":"markdown","a6d1a50c":"markdown","77947527":"markdown"},"source":{"1cf58bb3":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pa\nfrom collections import Counter\nfrom torch.utils.data import DataLoader, Dataset,random_split\n\nprint(os.listdir(\"..\/input\"))\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory","e05559c5":"class SentimentDataset(Dataset):\n    \"\"\"\n    This is a subclass of torch.utils.data Dataset and it implements \n    methods that make the dataset compatible with pytorch data utilities, notably the DataLoader\n    \"\"\"\n    def __init__(self,datalines):\n        self.xydata = datalines\n    \n    def __len__(self):              #API requirement\n        return len(self.xydata)\n    \n    def __getitem__(self,idx):      #API requirement\n        return self.xydata[idx]\n\ndef load_data_set(filename):\n    \"\"\"\n    Loads a dataset as a list of tuples: (text,label)\n    Args:\n       filename (str): the dataset filename \n    Returns:\n       A pytorch compatible Dataset object\n       list of tuples.\n    \"\"\"\n    istream = open(filename)\n    istream.readline()#skips header\n    xydataset = [ ]\n    for line in istream:\n        fields = line.split(',')\n        label  = fields[0]\n        text   = ','.join(fields[1:])\n        xydataset.append( (text,label) )\n    istream.close()\n    return SentimentDataset(xydataset)\n\ntrain_set = load_data_set('..\/input\/sentimentIMDB_train.csv')\nprint('Loaded %d examples as train set. '%(len(train_set)))\n\n#This demonstrates the DataLoader basic usage\nprint('Train data Sample (1st batch only)')\ntrain_loader = DataLoader(train_set, batch_size=4, shuffle=True)\nfor text_batch, label_batch in train_loader:       #iterates over all batches\n    for text,label in zip(text_batch, label_batch):#iterates over example in current batch \n        print('  ',text[:50],'...',label)\n    break #stops displaying once first batch \n\ntest_set = load_data_set('..\/input\/sentimentIMDB_test.csv')\nprint('Loaded %d examples as test set. '%(len(test_set)))","8794db6c":"def  make_w2idx(dataset):\n    \"\"\"\n    Maps words to integers\n    Returns:\n    A dictionary mapping words to integers\n    \"\"\"\n    wordset = set([])\n    for text,label in dataset:\n        words = text.split()\n        wordset.update(words)\n    return dict(zip(wordset,range(len(wordset))))   \n\ndef vectorize_text(text,w2idx):\n    counts = Counter(text.split())\n    xvec = torch.zeros(len(w2idx))\n    for word in counts:\n        if word in w2idx:       #manages unk words (ignored)\n            xvec[w2idx[word]] = counts[word] \n    return xvec.squeeze()\n\ndef vectorize_target(ylabel):\n     return torch.tensor(float(ylabel))\n","0c79229a":"class SentimentAnalyzer(nn.Module): \n    \n    def __init__(self):    \n        super(SentimentAnalyzer, self).__init__()\n        self.reset_structure(1,1)\n        \n    def reset_structure(self,vocab_size, num_labels):\n        self.W = nn.Linear(vocab_size, num_labels)\n            \n    def forward(self, text_vec):    \n        return torch.sigmoid(self.W(text_vec)) #sigmoid is the logistic activation\n        \n    def train(self,train_set,learning_rate,epochs):\n            \n        self.w2idx = make_w2idx(train_set)\n        self.reset_structure(len(self.w2idx),1)\n            \n        #remind that minimizing Binary Cross Entropy <=> minimizing NLL\n        loss_func   = nn.BCELoss() \n        optimizer   = optim.SGD(self.parameters(), lr=learning_rate)\n        \n        train_dataset, dev_dataset = random_split(train_set, [20000, 5000])\n        data_loader = DataLoader(train_dataset, batch_size=len(train_set), shuffle=True)\n        max_acc     = 0\n        for epoch in range(epochs):\n            global_logloss = 0.0\n            for Xbatch,Ybatch in data_loader: #there is a single batch,this loop does a single iteration\n                for X, Y in zip(Xbatch,Ybatch): \n                    self.zero_grad()\n                    xvec            = vectorize_text(X,self.w2idx)\n                    yvec            = vectorize_target(Y)\n                    prob            = self(xvec).squeeze()\n                    loss            = loss_func(prob,yvec)\n                    loss.backward()\n                    optimizer.step()\n                    global_logloss += loss.item()\n            validation_acc = self.eval_test(dev_dataset)\n            print(\"Epoch %d, mean cross entropy = %f, Validation accurracy : %f\"%(epoch,global_logloss\/len(train_set),validation_acc))\n            if validation_acc >= max_acc:\n                torch.save(self.state_dict(), 'sentiment_model.wt')\n                max_acc = validation_acc\n        self.load_state_dict(torch.load('sentiment_model.wt'))\n            \n    def eval_test(self,dev_set):\n        \n        with torch.no_grad():\n            data_loader = DataLoader(dev_set, batch_size=len(dev_set), shuffle=False)\n            ncorrect    = 0\n            N           = 0\n            for Xbatch,Ybatch in data_loader:\n                for X,Y in zip(Xbatch,Ybatch):\n                    xvec = vectorize_text(X,self.w2idx)\n                    prob = self(xvec).squeeze()\n                    if int(prob > 0.5) == int(Y) :\n                        ncorrect += 1\n                    N += 1\n            return float(ncorrect)\/float(N)\n                \n    def run_test(self,test_set,pred_filename):\n        \n        with torch.no_grad():\n            data_loader = DataLoader(test_set, batch_size=len(train_set), shuffle=False)\n            idxList  = []\n            sentList = []\n            for Xbatch,idxbatch in data_loader:\n                for X,idx in zip(Xbatch,idxbatch):\n                    xvec = vectorize_text(X,self.w2idx)\n                    prob = self(xvec).squeeze()\n                    idxList.append(idx)\n                    sentList.append(int(prob > 0.5))\n            df = pa.DataFrame({'idx':idxList,'sentY':sentList})\n            df.to_csv(pred_filename,index=False)\n            print('done.')","48aee3dc":"sent = SentimentAnalyzer()\nsent.train(train_set,0.01,20)","39a49c53":"sent.run_test(test_set,'submission.csv')","d932c762":"Chargement des donn\u00e9es\n======================","265b42d6":"Codage\n======\nFonctionnalit\u00e9s pour associer les mots \u00e0 des entiers et pour vectoriser un texte.","7b94d1f3":"Classifieur\n===========","a6d1a50c":"Inf\u00e9rences\n==========","77947527":"Environnement\n============="}}