{"cell_type":{"c6eb44a6":"code","8a89f17a":"code","1759b40c":"code","3bdf460d":"code","d462fe77":"code","c681e4d0":"code","a9f69dea":"code","e62da299":"code","dc62bf8c":"code","be30f967":"code","3f10cc1c":"code","915fcb13":"code","262f363b":"code","bd4505b7":"code","40c450bf":"code","3acba6cc":"code","b80a3ce3":"code","f32e7272":"code","f6c3d197":"code","d3139822":"code","731842af":"code","6782f1b4":"code","cf64cbbc":"code","b89b6647":"code","26b8593f":"code","ef7b03a1":"code","ad89845c":"code","a052862f":"code","087c262a":"code","8bdb76ea":"code","938c2004":"code","b993fa1d":"code","9193a7e2":"code","336656e1":"code","f3a19f29":"code","d63c099e":"code","68383155":"code","65589aea":"code","25e7942d":"code","f7faf28c":"code","d48ae7d1":"code","92c5ecc0":"code","03107d3a":"code","535fd18e":"code","90cee422":"code","e0c3daa3":"code","317c085b":"code","c6274716":"code","10b8f43c":"code","73ece65e":"code","7e9ebf31":"code","87dd0f06":"code","698132b8":"code","af0daa46":"code","06d5b31f":"code","44f0fd67":"code","33bc9a41":"code","6ab5e07a":"code","4dcf8143":"code","97008643":"code","4b526abb":"code","6ce21f07":"code","62400b49":"code","6eae76cf":"code","6a3a36cb":"code","c1a7df13":"code","ed4c322e":"code","55783a0c":"code","12b07bc0":"code","0e46dade":"code","955b0345":"code","1fc4edcb":"code","2e72c48f":"code","0417761f":"code","25c1acc1":"code","54e5a8a2":"code","0e26ceab":"code","83e4ddc6":"code","fa600230":"code","527ca1cb":"code","1d5234d6":"code","5b1ecbba":"code","d8192fcf":"code","923cf02b":"code","bfbfead9":"code","e9d0fce7":"code","0b46771e":"code","39e9687d":"code","d2feb6f1":"code","3953c037":"code","8c3e2ecb":"code","f6bdc1a8":"code","152ce855":"code","73e1cb2f":"code","6ad4f8c5":"code","b280b6f5":"code","6f74244d":"code","3baa66f7":"code","9b3cf3d4":"code","cfe94f62":"code","9b5ae03a":"code","f09a410d":"code","3a0d5d89":"code","42dd2145":"code","e29a805a":"code","799c52ee":"code","3d2b6cc5":"code","7beef188":"code","9313feca":"code","46daabcc":"code","35a478b3":"code","db06828a":"code","0b08ec4c":"code","b0b116e4":"code","53a7747c":"code","9c133278":"code","2907818d":"code","c0899e5f":"code","82cebc3f":"code","0cfd7f9c":"code","ed02e5c5":"code","94130304":"code","4944af4e":"code","0a58a8ba":"code","bfa8b30f":"code","82bc9bf2":"code","b7999c93":"code","f4a20503":"markdown","21649133":"markdown","fe268e34":"markdown","2ce3961f":"markdown","9d29c2a0":"markdown","3654dfeb":"markdown","c212bc1a":"markdown","9685d9fd":"markdown","6eb28dbd":"markdown","71caef7f":"markdown","79766ea5":"markdown","f01461b8":"markdown","69a212cf":"markdown","f44ea762":"markdown","5e9637d4":"markdown","35502447":"markdown","e9d767ed":"markdown","d4cac64e":"markdown","60fd3749":"markdown","0e57026d":"markdown","50a74f42":"markdown","4d884545":"markdown","0aeeb15e":"markdown","2d914ce6":"markdown","6bf510ff":"markdown","d6cf6965":"markdown","6d29e4df":"markdown","1f576455":"markdown","f38ed5b7":"markdown","53822778":"markdown","54922b1f":"markdown","48b6a701":"markdown","68085a51":"markdown","fdf413ae":"markdown","c772d4c5":"markdown","01d1be0c":"markdown","dd714649":"markdown","0e041104":"markdown","6b4260a6":"markdown","0b77d029":"markdown","a4ce577e":"markdown","02e49e03":"markdown","fb06d8bc":"markdown","63f1033f":"markdown","52da5ba3":"markdown","5796b9f2":"markdown","e89549f2":"markdown","f66c4a9c":"markdown","6b903f9f":"markdown","5ef46040":"markdown","0f423bca":"markdown","d8628b1a":"markdown","e92341d1":"markdown","5eeb45c9":"markdown","fcb1384c":"markdown","6821dd22":"markdown","0d79e290":"markdown","a663a488":"markdown","24d16046":"markdown","93aa725b":"markdown","296ddab7":"markdown","1821dbb2":"markdown","683c2869":"markdown","4772f9a9":"markdown","cd0f5ac5":"markdown","35bac07c":"markdown","e7fe94af":"markdown","dccc3e00":"markdown","9460d1aa":"markdown","4b54aeac":"markdown","ac5c52a7":"markdown","ffe0a683":"markdown","b1155775":"markdown","c0ace9e9":"markdown","4a946d19":"markdown","ed4bf0ef":"markdown"},"source":{"c6eb44a6":"import numpy as np \nimport pandas as pd\nimport os\nimport time\n\nimport string\nimport emoji\nimport re\n\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Dense, Input\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud\n\n\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n#For Stemming, NLTK is needed\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n","8a89f17a":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","1759b40c":"df_train.isnull().sum()","3bdf460d":"df_test.isnull().sum()","d462fe77":"count = df_train['text'].str.split().str.len()\ncount\nprint(max(count))","c681e4d0":"df_train[df_train.keyword.isnull()==False]","a9f69dea":"df_train[df_train.keyword.isnull()==True]","e62da299":"print(\"Train dataset shape : \",df_train.shape)\nprint(\"Test dataset shape : \",df_test.shape)","dc62bf8c":"df_train['target'].value_counts()","be30f967":"sns.barplot(df_train['target'].value_counts().index,df_train['target'].value_counts(),palette='rocket')","3f10cc1c":"df_train['keyword'].value_counts()","915fcb13":"sns.barplot(y=df_train['keyword'].value_counts()[:25].index,x=df_train['keyword'].value_counts()[:25], orient='horizontal', palette='viridis')","262f363b":"# A disaster tweet\ndisaster_tweets = df_train[df_train['target']==1]['text']\ndisaster_tweets.values[:5]","bd4505b7":"non_disaster_tweets = df_train[df_train['target']==0]['text']\nnon_disaster_tweets.values[:5]","40c450bf":"\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","3acba6cc":"def clean(text):\n    text = text.lower() #Lets make it lowercase\n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeEmojis(text)\n    text = removeNumbers(text)\n    text = removeLinks(text)\n    return text","b80a3ce3":"def removeStopwords(text):\n    doc = nlp(text)\n    clean_text = ' '\n    for txt in doc:\n        if (txt.is_stop == False):\n            clean_text = clean_text + \" \" + str(txt)        \n    \n    return clean_text\n\nprint(\"Text before removeStopwords function: \" + df_train['text'][1])\nprint(\"Text after removeStopwords function: \" + removeStopwords(df_train['text'][1]))","f32e7272":"def removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\nprint(\"Text before removePunctuations function: \" + df_train['text'][1])\nprint(\"Text after removePunctuations function: \" + removePunctuations(df_train['text'][1]))","f6c3d197":"def removeEmojis(text):\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    return clean_text\n\ntest_string = \"Hi' \ud83e\udd14 How is your \ud83d\ude48 and \ud83d\ude0c. Have a nice weekend \ud83d\udc95\ud83d\udc6d\ud83d\udc59\".lower()\n(test_string,removeEmojis(test_string))","d3139822":"def removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text\n\ntest_string = \"Hi \ud83d\ude48 99 girls are running\"\n(test_string,removeNumbers(test_string))","731842af":"def removeLinks(text):\n    clean_text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    #https? will match both http and https\n    #A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B.\n    #\\S Matches any character which is not a whitespace character.\n    #+ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match \u2018a\u2019 followed by any non-zero number of \u2018b\u2019s; it will not match just \u2018a\u2019.\n    return clean_text\n\ntest_string = \"http:\/\/www.youtube.com\/ and https:\/\/www.youtube.com\/ should be removed \"\n(test_string,removeLinks(test_string))","6782f1b4":"df_train['text']=df_train.text.apply(clean)\ndf_test['text']=df_test.text.apply(clean)","cf64cbbc":"df_train.head()","b89b6647":"tweets = df_train['text']\nfig, ax1, = plt.subplots(1,  figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(tweets))\nax1.imshow(wordcloud1)\nax1.axis('on')\nax1.set_title('Tweets',fontsize=40);","26b8593f":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ndf_train.at[df_train['id'].isin(ids_with_target_error),'target'] = 0\ndf_train[df_train['id'].isin(ids_with_target_error)]","ef7b03a1":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","ad89845c":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","a052862f":"df_train['text']=df_train.text.apply(convert_abbrev)\ndf_test['text']=df_test.text.apply(convert_abbrev)","087c262a":"text = df_train['text']\nvectorizer = CountVectorizer()\nvectorizer.fit(text)\nprint(vectorizer.vocabulary_['cooool'])\nprint(vectorizer.vocabulary_['cool'])\n","8bdb76ea":"stemmer = SnowballStemmer(language='english')\n\ntokens = df_train['text'][1].split()\nclean_text = ' '\n\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))","938c2004":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + stemmer.stem(token)      \n    \n    return clean_text\n\nprint(\"Text before stemWord function: \" + df_train['text'][1])\nprint(\"Text after stemWord function: \" + stemWord(df_train['text'][1]))","b993fa1d":"df_train['text']=df_train.text.apply(stemWord)\ndf_test['text']=df_test.text.apply(stemWord)","9193a7e2":"df_train.text\n\nfor txt in df_train.text[:40]:\n    print(txt)\n                        ","336656e1":"doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n#for token in doc:\n   # print(token.lemma_)\nfor noun in doc.noun_chunks:\n    print(noun.text)","f3a19f29":"for word in doc:\n    print(word.text,  word.lemma_)","d63c099e":"def lemmatizeWord(text):\n    tokens=nlp(text)\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + token.lemma_      \n    \n    return clean_text\n\nprint(\"Text before lemmatizeWord function: \" + df_train['text'][1])\nprint(\"Text after lemmatizeWord function: \" + lemmatizeWord(df_train['text'][1]))\n\ndoc = \"Apple is looking at buying U.K. startup for $1 billion\"\nlemmatizeWord(doc)","68383155":"df_train['text']=df_train.text.apply(lemmatizeWord)\ndf_test['text']=df_test.text.apply(lemmatizeWord)","65589aea":"df_train['text']","25e7942d":"text = ''\nfor txt in df_train.text:\n    text = text + \"\\n\" + txt\n","f7faf28c":"print(f'Length of text: {len(text)} characters')","d48ae7d1":"# The unique characters in the file\nvocab = sorted(set(text))\nprint(f'{len(vocab)} unique characters')","92c5ecc0":"vocab","03107d3a":"example_texts = ['abcdefg', 'xyz']\n\nchars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\nchars","535fd18e":"ids_from_chars = preprocessing.StringLookup(\n    vocabulary=list(vocab))","90cee422":"ids = ids_from_chars(chars)\nids","e0c3daa3":"chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n    vocabulary=ids_from_chars.get_vocabulary(), invert=True)","317c085b":"chars2 = chars_from_ids(ids)\nchars2","c6274716":"tf.strings.reduce_join(chars, axis=-1).numpy()","10b8f43c":"def text_from_ids(ids):\n  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)","73ece65e":"all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\nall_ids\n","7e9ebf31":"ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)","87dd0f06":"for ids in ids_dataset.take(10):\n    #print(chars_from_ids(ids).numpy().decode('utf-8'))\n    print(chars_from_ids(ids).numpy())","698132b8":"seq_length = 100\nexamples_per_epoch = len(text)\/\/(seq_length+1)","af0daa46":"sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor seq in sequences.take(1):\n  print(chars_from_ids(seq).numpy())","06d5b31f":"for seq in sequences.take(5):\n  print(text_from_ids(seq).numpy())","44f0fd67":"def split_input_target(sequence):\n    input_text = sequence[:-1]\n    target_text = sequence[1:]\n    return input_text, target_text","33bc9a41":"split_input_target(list(\"Earthquake\"))","6ab5e07a":"dataset = sequences.map(split_input_target)","4dcf8143":"for input_example, target_example in dataset.take(1):\n    print(\"Input :\", text_from_ids(input_example).numpy())\n    print(\"Target:\", text_from_ids(target_example).numpy())","97008643":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = (\n    dataset\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .prefetch(tf.data.experimental.AUTOTUNE))\n\ndataset","4b526abb":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","6ce21f07":"class MyModel(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, rnn_units):\n    super().__init__(self)\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(rnn_units,\n                                   return_sequences=True,\n                                   return_state=True)\n    self.dense = tf.keras.layers.Dense(vocab_size)\n\n  def call(self, inputs, states=None, return_state=False, training=False):\n    x = inputs\n    x = self.embedding(x, training=training)\n    if states is None:\n      states = self.gru.get_initial_state(x)\n    x, states = self.gru(x, initial_state=states, training=training)\n    x = self.dense(x, training=training)\n\n    if return_state:\n      return x, states\n    else:\n      return x","62400b49":"model = MyModel(\n    # Be sure the vocabulary size matches the `StringLookup` layers.\n    vocab_size=len(ids_from_chars.get_vocabulary()),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units)","6eae76cf":"for input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","6a3a36cb":"model.summary()","c1a7df13":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()","ed4c322e":"sampled_indices","55783a0c":"print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\nprint()\nprint(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())","12b07bc0":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)","0e46dade":"example_batch_loss = loss(target_example_batch, example_batch_predictions)\nmean_loss = example_batch_loss.numpy().mean()\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"Mean loss:        \", mean_loss)","955b0345":"tf.exp(mean_loss).numpy()","1fc4edcb":"model.compile(optimizer='adam', loss=loss)","2e72c48f":"## Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","0417761f":"EPOCHS = 40","25c1acc1":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","54e5a8a2":"class OneStep(tf.keras.Model):\n  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n    super().__init__()\n    self.temperature = temperature\n    self.model = model\n    self.chars_from_ids = chars_from_ids\n    self.ids_from_chars = ids_from_chars\n\n    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n    sparse_mask = tf.SparseTensor(\n        # Put a -inf at each bad index.\n        values=[-float('inf')]*len(skip_ids),\n        indices=skip_ids,\n        # Match the shape to the vocabulary\n        dense_shape=[len(ids_from_chars.get_vocabulary())])\n    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n\n  @tf.function\n  def generate_one_step(self, inputs, states=None):\n    # Convert strings to token IDs.\n    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n    input_ids = self.ids_from_chars(input_chars).to_tensor()\n\n    # Run the model.\n    # predicted_logits.shape is [batch, char, next_char_logits]\n    predicted_logits, states = self.model(inputs=input_ids, states=states,\n                                          return_state=True)\n    # Only use the last prediction.\n    predicted_logits = predicted_logits[:, -1, :]\n    predicted_logits = predicted_logits\/self.temperature\n    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n    predicted_logits = predicted_logits + self.prediction_mask\n\n    # Sample the output logits to generate token IDs.\n    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n\n    # Convert from token ids to characters\n    predicted_chars = self.chars_from_ids(predicted_ids)\n\n    # Return the characters and model state.\n    return predicted_chars, states","0e26ceab":"one_step_model = OneStep(model, chars_from_ids, ids_from_chars)","83e4ddc6":"start = time.time()\nstates = None\nnext_char = tf.constant(['ablaze'])\nresult = [next_char]\n\nfor n in range(1000):\n  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n  result.append(next_char)\n\nresult = tf.strings.join(result)\nend = time.time()\nprint(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\nprint('\\nRun time:', end - start)","fa600230":"count_vectorizer = CountVectorizer()\ntrain_bag = count_vectorizer.fit_transform(df_train['text'])\ntest_bag = count_vectorizer.transform(df_test[\"text\"])","527ca1cb":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(df_train['text'])\ntest_tfidf = tfidf.transform(df_test[\"text\"])","1d5234d6":"with nlp.disable_pipes():\n    train_vectors = np.array([nlp(text).vector for text in df_train.text])\n    test_vectors = np.array([nlp(text).vector for text in df_test.text])","5b1ecbba":"# Set dual=False to speed up training, and it's not needed\nsvc_wordEmbed = LinearSVC(random_state=1, dual=False, max_iter=10000)\nsvc_wordEmbed.fit(train_vectors, df_train.target)","d8192fcf":"\nscores = model_selection.cross_val_score(svc_wordEmbed, train_vectors, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","923cf02b":"\nxgb_wordEmbed = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n","bfbfead9":"scores = model_selection.cross_val_score(xgb_wordEmbed, train_vectors, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","e9d0fce7":"#clf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n#                     subsample=0.8, nthread=10, learning_rate=0.1)\n#scores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, df_train[\"target\"], cv=3, scoring=\"f1\")\n#scores","0b46771e":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_bag, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","39e9687d":"clf_NB.fit(train_bag, df_train[\"target\"])","d2feb6f1":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","3953c037":"clf_NB_TFIDF.fit(train_tfidf, df_train[\"target\"])","8c3e2ecb":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_wEmbed = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_wEmbed, train_vectors, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","f6bdc1a8":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_bag, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","152ce855":"clf.fit(train_bag, df_train[\"target\"])","73e1cb2f":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, df_train[\"target\"], cv=3, scoring=\"f1\")\nscores","6ad4f8c5":"clf_tfidf.fit(train_bag, df_train[\"target\"])","b280b6f5":"train_vectors.shape","6f74244d":"\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","3baa66f7":"\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_f1_m', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","9b3cf3d4":"#  Neural Network\nnn = keras.Sequential([\n    layers.Dense(256, activation='relu', input_shape=[7613,300]),\n    layers.Dropout(0.4),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.4),\n    layers.Dense(1,activation='sigmoid')\n])\n\nnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=[f1_m])\nhistory=nn.fit(\n    train_vectors,df_train[\"target\"],\n    validation_split=0.1,\n    batch_size=128,\n    epochs=25,\n    callbacks=[early_stopping,learning_rate_reduction])","cfe94f62":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['f1_m','val_f1_m']].plot()\nhistory_frame.loc[:, ['loss','val_loss']].plot();","9b5ae03a":"pred = nn.predict(test_vectors)\n\npred[pred > 0.5] = 1\npred[pred <= 0.5] = 0","f09a410d":"#sample_submission = pd.read_csv(submission_file_path)\n#sample_submission[\"target\"] = Pred.astype('int64')\n#sample_submission.to_csv(\"submission.csv\", index=False)","3a0d5d89":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n # get the official tokenization created by the Google team","42dd2145":"import tokenization","e29a805a":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","799c52ee":"def build_model(bert_layer, max_len = 128, lr = 1e-5):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"segment_ids\")\n        \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    dense_out = Dense(1,activation=\"relu\")(pooled_output)\n    drop_out = tf.keras.layers.Dropout(0.8)(dense_out)\n    out = Dense(1,activation=\"sigmoid\")(pooled_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    adam = tf.keras.optimizers.Adam(lr)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[f1_m])\n        \n    return model","3d2b6cc5":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","7beef188":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","9313feca":"train_input = bert_encode(df_train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(df_test.text.values, tokenizer, max_len=160)\ntrain_labels = df_train.target.values","46daabcc":"train_input","35a478b3":"train_labels","db06828a":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","0b08ec4c":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Train BERT model with my tuning\n#checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\nvalid = 0.2\nepochs_num = 3\nbatch_size_num = 16\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split = valid,\n    epochs = epochs_num, # recomended 3-5 epochs\n    #callbacks=[checkpoint],\n    batch_size = batch_size_num\n)\n#model.save('model.h5')","b0b116e4":"history_frame = pd.DataFrame(train_history.history)\nhistory_frame.loc[:, ['accuracy','val_accuracy']].plot()\nhistory_frame.loc[:, ['loss','val_loss']].plot();","53a7747c":"#model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","9c133278":"test_pred","2907818d":"# submit\n#submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n#submission['target'] = np.round(test_pred).astype('int')\n#submission.to_csv('submission.csv', index=False)\n#submission.groupby('target').count()","c0899e5f":"svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\nclf_NB = MultinomialNB()\nclf = LogisticRegression(C=1.0)\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)","82cebc3f":"def getScore(model, vector):\n    scores = model_selection.cross_val_score(model, vector, df_train[\"target\"], cv=3, scoring=\"f1\")\n    return scores\n\n#print(getScore(clf_xgb,train_vectors).mean())\n    ","0cfd7f9c":"train_vectors","ed02e5c5":"clf_xgb","94130304":"model_name = type(clf_xgb).__name__\nmodel_name","4944af4e":"def appendToModelReport(model_report, model,representation,vector):\n    model_report=model_report.append({\"Model\" : type(model).__name__, \"Representation\":representation, \"F1 Score\": getScore(model,vector).mean() },ignore_index = True)\n    return model_report","0a58a8ba":"model_report = pd.DataFrame(columns=['Model','Representation','F1 Score'])\n#model_report.append({\"Model\" : type(clf_xgb).__name__, \"Representation\":\"Word Embedding\", \"F1 Score\": getScore(clf_xgb,train_vectors).mean() },ignore_index = True)\n#XGB Boost model reports\nmodel_report = appendToModelReport(model_report, clf_xgb, \"Word Embedding\", train_vectors)\nmodel_report = appendToModelReport(model_report, clf_xgb, \"Bag of Words\", train_bag)\nmodel_report = appendToModelReport(model_report, clf_xgb, \"TF IDF\", train_tfidf)\n\n#Support Vector Machines\nmodel_report = appendToModelReport(model_report, svc, \"Word Embedding\", train_vectors)\nmodel_report = appendToModelReport(model_report, svc, \"Bag of Words\", train_bag)\nmodel_report = appendToModelReport(model_report, svc, \"TF IDF\", train_tfidf)\n\n#Naive Bayes Classifier\nmodel_report = appendToModelReport(model_report, clf_NB, \"Word Embedding\", train_vectors)\nmodel_report = appendToModelReport(model_report, clf_NB, \"Bag of Words\", train_bag)\nmodel_report = appendToModelReport(model_report, clf_NB, \"TF IDF\", train_tfidf)\n\n#Logistic Regression Classifier\nmodel_report = appendToModelReport(model_report, clf, \"Word Embedding\", train_vectors)\nmodel_report = appendToModelReport(model_report, clf, \"Bag of Words\", train_bag)\nmodel_report = appendToModelReport(model_report, clf, \"TF IDF\", train_tfidf)\n\nmodel_report","bfa8b30f":"model_report[model_report['F1 Score'] == max(model_report[\"F1 Score\"])]","82bc9bf2":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)\n    ","b7999c93":"submission_file_path = \"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\"\n\nmodel=nn\nsubmission(submission_file_path,model,test_vectors)","f4a20503":"Checking if the transformation is applied as expected, in the text column","21649133":"Though we could not perform stemming with spaCy, we can perform lemmatization using spaCy.\nThis is a time consuming process.\n\nOutput of lemmatization is an actual word in English unlike Stemming.\n(word.lemma_ will print word's lemma in SPacy)","fe268e34":"Find out the best model and representation","2ce3961f":"<a id=\"section-seven\"><\/a>\n# BERT","9d29c2a0":"Will take lot of time to run this.","3654dfeb":"We will create a 'clean' function which comprises of various cleaning function such as removal of emojis, punctuations etc","c212bc1a":"What should be the input_shape to the neural network?\n","9685d9fd":"Loading the train dataset to df_train and test dataset to df_test","6eb28dbd":"Evaluate F1 Score using scikit learns model_selection.cross_val_score","71caef7f":"TIP: Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings","79766ea5":"<a id=\"section-five\"><\/a>\n# Transforming tokens to a vector","f01461b8":"Lets try XGBoost now on word embeddings","69a212cf":"<a id=\"section-six-one\"><\/a>\n# 1. **Support Vector Machines**","f44ea762":"Lets try Naive Bayes Classifier on Bag of Words model","5e9637d4":"BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\nAs opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\u2019s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).","35502447":"A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.","e9d767ed":"<a id=\"subsection-five-one\"><\/a>\n**Bag of Words model**","d4cac64e":"<a id=\"section-zero\"><\/a>\n\n# TABLE OF CONTENTS\n\n\n* [Library Importations](#section-one)\n* [Loading Datasets](#section-two)\n* [Exploratory Data Analysis](#section-three)\n* [Data Preprocessing](#section-four)\n    - [Text Normalization](#subsection-four-one)\n    - [Stemming](#subsection-four-two)\n    - [Lemmatization](#subsection-four-three)\n* [Vector Transformation](#section-five)\n    - [Bag Of Words](#subsection-five-one)\n    - [TD IDF](#subsection-five-two)\n    - [Word Embedding](#subsection-five-three)\n* [Building Model](#section-six)\n    - [Support Vector Machine](#subsection-six-one)\n    - [XGBoost](#subsection-six-two)\n    - [Naive Bayes Classifier](#subsection-six-three)\n    - [Logistic Regression](#subsection-six-four)\n    - [Neural Network](#subsection-six-five)\n* [BERT](#section-seven)\n* [Model Comparison](#section-eight)\n* [Submission](#section-nine)","60fd3749":"The getScore function will return F1 score of various models.","0e57026d":"<a id=\"section-six-five\"><\/a>\n#  **5. Neural Network**","50a74f42":"Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.","4d884545":"# Data Augmentation","0aeeb15e":"Lets see 5 non-disaster tweets","2d914ce6":"<a id=\"section-six\"><\/a>\n# Building a Text Classification Model","6bf510ff":"A bag-of-words (B.o.w) is a representation of text that describes the occurrence of words within a document. It involves two things:\n\nA vocabulary of known words.\nA measure of the presence of known words.\nIt is called a \u201cbag\u201d of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.","d6cf6965":"Lets display first 25 as there are too many to display whole. Putting horizontal orientation and viridis palette. ","6d29e4df":"Let's do Early Stoppping and Learning Rate reduction","1f576455":"Lets generate texts with a RNN","f38ed5b7":"<a id=\"section-six-two\"><\/a>\n#  **2. XGBoost**","53822778":"<a id=\"section-eight\"><\/a>\n# Compare Models","54922b1f":"<a id=\"section-three\"><\/a>\n# Exploratory Data Analysis (EDA) ","48b6a701":"Run this with GPU","68085a51":"Lets see 5 tweets about a disaster.","fdf413ae":"<a id=\"subsection-four-one\"><\/a>\n# Text Normalization\nText normalization is the process of transforming text into a canonical (standard) form. For example, the word \u201cgooood\u201d and \u201cgud\u201d can be transformed to \u201cgood\u201d, its canonical form. Another example is mapping of near identical words such as \u201cstopwords\u201d, \u201cstop-words\u201d and \u201cstop words\u201d to just \u201cstopwords\u201d.","c772d4c5":"Lets see the data when 'keyword' is present","01d1be0c":"Lets define recall, precision and f1 score","dd714649":"Giving metrics as F1 Score instead of Accuracy here as evaluation of this competetion is on F1 score","0e041104":"Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-bert-keras?scriptVersionId=31186559\nSome data is wrong. For example, target of the training dataset at 328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226 are given as 1 whereas they are obviously 0,\nsince they are not related to disaster.\n\nWe change it to 0.","6b4260a6":"Helper functions for BERT","0b77d029":"Lets see the wordcloud for the df_train['text'] now","a4ce577e":"Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset","02e49e03":"Generate a WordCloud for disaster tweets and non-disaster tweets","fb06d8bc":"Let's try out different classifiers on word embeddings representation train_vectors and test_vectors","63f1033f":"Lets write a function to get scores and compare different models ","52da5ba3":"[Back to Top](#section-zero)","5796b9f2":"<a id=\"subsection-four-three\"><\/a>\n# Lemmatization","e89549f2":"<a id=\"section-four\"><\/a>\n# Preprocessing the data","f66c4a9c":"TO DO","6b903f9f":"<a id=\"section-six-four\"><\/a>\n#  **4. Logistic Regression Classifier**","5ef46040":"**Lets explore the target column**","0f423bca":"<a id=\"section-one\"><\/a>\n# Import all the required libraries","d8628b1a":"<a id=\"section-six-three\"><\/a>\n#  **3. Naives Bayes Classifier**","e92341d1":"Lets try Naive Bayes Classifer on TFIDF","5eeb45c9":"<a id=\"section-nine\"><\/a>\n# Making the submission","fcb1384c":"**TF-IDF**","6821dd22":"Check various aspects of the dataset. It may or may not be useful. ","0d79e290":"The appendToModelReport will append Model name, Representation and its corresponding mean F1 score in a dataframe.","a663a488":"Checking which all columns contain NaN values(is missing). 'location' is missing a lot in both the train and test data sets","24d16046":"Count number of words in tweet and maximum count","93aa725b":"<a id=\"subsection-four-two\"><\/a>\n# Stemming\n\nWe will use NLTK for stemming since Spacy doesn't contain any function for stemming as it relies on lemmatization only\nThere are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers.\nSnowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. So we will use that.\n\nStemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.","296ddab7":"<a id=\"subsection-five-two\"><\/a>\n**TFIDF Features**\n\nAnother common representation is TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models.\n\n> Term Frequency: is a scoring of the frequency of the word in the current document.\n\nTF = (Number of times term t appears in a document)\/(Number of terms in the document)\n> Inverse Document Frequency: is a scoring of how rare the word is across documents.\n\nIDF = 1+log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in.","1821dbb2":"Lets see the data when keyword is absent","683c2869":"Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. ","4772f9a9":"Checking shape of train and test datasets. Note that the test dataset does not have 'target' column.","cd0f5ac5":"<a id=\"section-two\"><\/a>\n# Load datasets","35bac07c":"Lets convert all the abbreviations to its full form.\nThanks to https:\/\/www.kaggle.com\/rftexas\/text-only-bert-keras?scriptVersionId=31186559","e7fe94af":"lemmatizeWord converts words into its lemma form. (Will take a while to run)","dccc3e00":"Defining various models below. Just copying and pasting from the above section.","9460d1aa":"Lets check how many tweets are related to disaster","4b54aeac":"We get decent F1 scores","ac5c52a7":"Val loss is increasing even though training loss decreases -> Overfitting","ffe0a683":"Lets explore the keyword column and see if it's useful","b1155775":"We will work with pre-processed data for both training and test data sets. Running this will take a while.","c0ace9e9":"**Bag of words model**","4a946d19":"<a id=\"subsection-five-three\"><\/a>\n**Word Vectors\/Word Embeddings**","ed4bf0ef":"Lets check the number of times gud, goood, cool, coool etc occur."}}