{"cell_type":{"d1e6aee3":"code","cf2fad1a":"code","0192d56c":"code","1b18b185":"code","20d542f9":"code","adeedbe5":"code","af4fbffb":"code","a3b62085":"code","712bb789":"code","ddfc7d16":"code","d595441c":"code","828a629c":"code","78e1ebe5":"code","52af00ba":"code","6e8b8c0e":"code","96f4d551":"code","05860a6c":"code","7ca976c8":"code","8abdb862":"code","c3afd1a4":"code","41bdd5bd":"code","06582933":"code","b8be5e75":"code","be7bdd6d":"code","00332be0":"code","b08baaa0":"code","2cc49ad4":"code","fe8454a9":"code","ad61003d":"code","1357961f":"code","6fcc1d93":"code","d2db4441":"code","227e17b9":"code","938c305a":"code","2782ff14":"code","a94be5f3":"code","02bd336e":"code","91600351":"code","0d75e029":"code","5fec0071":"code","a2cdf87a":"code","66b044a9":"code","53769977":"code","3369b496":"code","be1fd848":"code","57960741":"code","3d10b0f1":"code","13cb8345":"code","fefc36c2":"code","13f014f8":"code","2839ffe1":"code","1eae2a32":"code","cac65a44":"code","48be42e5":"code","a867b1c0":"code","71e8acf4":"code","ffc7721f":"code","21152cee":"markdown","1f602442":"markdown","fb870073":"markdown","a5ea9da0":"markdown","91c412ea":"markdown","52b6df97":"markdown","8676f576":"markdown","56ce4914":"markdown","050b390e":"markdown","bdb2b490":"markdown"},"source":{"d1e6aee3":"# Import the essentials librairies for our use case\n\nimport numpy as np \nimport pandas as pd","cf2fad1a":"# Loading our dataset\n\ndf = pd.read_csv(\"..\/input\/stock-sentiment-analysis-data\/stock_senti_analysis.csv\")","0192d56c":"df.columns # list all the columns of our file","1b18b185":"df.info()","20d542f9":"df.shape # rows & columns","adeedbe5":"df.head()","af4fbffb":"import mitosheet  # Mito is also a good way to visualise our Data and playing with it (its like Excel for Data science)\nmitosheet.sheet(df, view_df=True)","a3b62085":"# Importing essential libraries for visualization\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","712bb789":"# Visualizing the count of 'Label' column from the dataset\n\nplt.figure(figsize=(4,4))\nsns.countplot(x='Label', data=df)\nplt.xlabel('Stock Sentiment (0-goes Down\/Stay same, 1-goes Up)')\nplt.ylabel('Count')\nplt.show()","ddfc7d16":"df.isna().any() # findingb all the NaN values in our Data set","d595441c":"# Dropping NaN values and check the updated number of rows (some days we havent colected all the data required, so we will delete the entire day from the data set)\ndf.dropna(inplace=True)\nprint(df.shape)","828a629c":"df_copy = df.copy() # we create a copy  of our original data set in order to make modification safely","78e1ebe5":"df_copy.reset_index(inplace=True)","52af00ba":"df_copy.head()","6e8b8c0e":"import mitosheet\nmitosheet.sheet(df_copy, view_df=True)","96f4d551":"# Splitting the dataset into train\/test set\n\ntrain = df_copy[df_copy['Date'] < '20150101']\ntest = df_copy[df_copy['Date'] > '20141231']\nprint('Train size: {}, Test size: {}'.format(train.shape, test.shape))","05860a6c":"# Splitting the dataset\n\ny_train = train['Label']\ntrain = train.iloc[:, 3:28] # from collumn 3 to 28 we have the list of our stock headlines per day\ny_test = test['Label']\ntest = test.iloc[:, 3:28]","7ca976c8":"# Importing essential libraries for performing Natural Language Processing on our dataset\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords # stopwors are words without much meaning added to a sentence (example: the, he, have... etc)\nfrom nltk.stem import PorterStemmer # Porter Stemmer is the inventor of the Stemming algorithm","8abdb862":"# Removing punctuation and special character from the text\n\ntrain.replace(to_replace='[^a-zA-Z]', value=' ', regex=True, inplace=True)\ntest.replace(to_replace='[^a-zA-Z]', value=' ', regex=True, inplace=True)","c3afd1a4":"# Renaming columns\n\nnew_columns = [str(i) for i in range(0,25)]\ntrain.columns = new_columns\ntest.columns = new_columns","41bdd5bd":"train.head() #notice that columns are now named from 0 to 24 and no more punctuation on our Headlines","06582933":"import mitosheet\nmitosheet.sheet(train.head() #notice that columns are now named from 0 to 24 and no more punctuation on our Headlines, view_df=True)","b8be5e75":"# Converting the entire text to lower case\n\nfor i in new_columns:\n  train[i] = train[i].str.lower()\n  test[i] = test[i].str.lower()","be7bdd6d":"train.head() # now all the text is in lower case","00332be0":"import mitosheet\nmitosheet.sheet(train.head() # now all the text is in lower case, view_df=True)","b08baaa0":"# Joining all the columns to create a text like day by day (row by row)\n\ntrain_headlines = []\ntest_headlines = []\n\nfor row in range(0, train.shape[0]):\n  train_headlines.append(' '.join(str(x) for x in train.iloc[row, 0:25]))\n\nfor row in range(0, test.shape[0]):\n  test_headlines.append(' '.join(str(x) for x in test.iloc[row, 0:25]))","2cc49ad4":"train_headlines[1]","fe8454a9":"test_headlines[1]","ad61003d":"# Creating corpus of train dataset\n\nps = PorterStemmer() # here we will use the stemming algo instead of the lemmatization\ntrain_corpus = []\n\nfor i in range(0, len(train_headlines)):\n  \n  # Tokenizing the Headlines by words = splitting the text into words\n  words = train_headlines[i].split()\n\n  # Removing the stopwords\n  words = [word for word in words if word not in set(stopwords.words('english'))]\n\n  # Stemming the words\n  words = [ps.stem(word) for word in words]\n\n  # Joining the stemmed words\n  headline = ' '.join(words)\n\n  # Building a corpus of Headlines\n  train_corpus.append(headline)","1357961f":"train_corpus[0:5] # view of the 5 first arrows","6fcc1d93":"# same thing here, we create a corpus of test dataset\ntest_corpus = []\n\nfor i in range(0, len(test_headlines)):\n  \n  # Tokenizing the news-title by words\n  words = test_headlines[i].split()\n\n  # Removing the stopwords\n  words = [word for word in words if word not in set(stopwords.words('english'))]\n\n  # Stemming the words\n  words = [ps.stem(word) for word in words]\n\n  # Joining the stemmed words\n  headline = ' '.join(words)\n\n  # Building a corpus of news-title\n  test_corpus.append(headline)","d2db4441":"test_corpus[0:5]","227e17b9":"down_words = [] #down words are words making stocks going down\nfor i in list(y_train[y_train==0].index):\n  down_words.append(train_corpus[i])\n\nup_words = []\nfor i in list(y_train[y_train==1].index): #up words are words making stocks going up\n  up_words.append(train_corpus[i])","938c305a":"pip install wordcloud","2782ff14":"# Creating wordcloud for down_words and visualise the most negative influential words in row 1 from our dataset (based on frequency and relevance)\nfrom wordcloud import WordCloud\nwordcloud1 = WordCloud(background_color='black', width=3000, height=2500).generate(down_words[1])\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title(\"Words which indicate a fall in Stock Market \")\nplt.show()","a94be5f3":"# we do the same for upwords in row 5 from our dataset as an Example\n\nwordcloud2 = WordCloud(background_color='white', width=3000, height=2500).generate(up_words[5])\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title(\"Words which indicate a rise in Stock Market \")\nplt.show()","02bd336e":"# Creating the Bag of Words model - features extraction for text modeling\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=10000, ngram_range=(2,2))\n\nX_train = cv.fit_transform(train_corpus).toarray()\nX_test = cv.transform(test_corpus).toarray()","91600351":"from sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression()\nlr_classifier.fit(X_train, y_train)","0d75e029":"lr_y_pred = lr_classifier.predict(X_test)","5fec0071":"# measure the Accuracy, Precision and Recall of this model\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nscore1 = accuracy_score(y_test, lr_y_pred)\nscore2 = precision_score(y_test, lr_y_pred)\nscore3 = recall_score(y_test, lr_y_pred)\n\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2))) # here we choose to ahve an output in % format\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))","a2cdf87a":"# Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\nlr_cm = confusion_matrix(y_test, lr_y_pred)\n\nlr_cm","66b044a9":"# Plotting the confusion matrix\n\nplt.figure(figsize=(10,7))\nsns.heatmap(data=lr_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\n\nplt.title('Confusion Matrix for Logistic Regression Algorithm')\n\nplt.show()","53769977":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier(n_estimators=100, criterion='entropy') # with 100 trees in our Forest, and Entropy as Impurity metric\nrf_classifier.fit(X_train, y_train)","3369b496":"rf_y_pred = rf_classifier.predict(X_test)","be1fd848":"# Accuracy, Precision and Recall measurement \n\nscore1 = accuracy_score(y_test, rf_y_pred)\nscore2 = precision_score(y_test, rf_y_pred)\nscore3 = recall_score(y_test, rf_y_pred)\n\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))","57960741":"# Confusion Matrix\n\nrf_cm = confusion_matrix(y_test, rf_y_pred)\n\nrf_cm","3d10b0f1":"# Plotting the confusion matrix\n\nplt.figure(figsize=(10,7))\nsns.heatmap(data=rf_cm, annot=True, cmap=\"binary\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\n\nplt.title('Confusion Matrix for Random Forest Algorithm')\n\nplt.show()","13cb8345":"from sklearn.naive_bayes import MultinomialNB\n\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train, y_train)","fefc36c2":"# Predicting the Test set results\n\nnb_y_pred = nb_classifier.predict(X_test)","13f014f8":"# Accuracy, Precision and Recall\n\nscore1 = accuracy_score(y_test, nb_y_pred)\nscore2 = precision_score(y_test, nb_y_pred)\nscore3 = recall_score(y_test, nb_y_pred)\n\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))","2839ffe1":"# Making the Confusion Matrix\n\nnb_cm = confusion_matrix(y_test, nb_y_pred)\n\nnb_cm","1eae2a32":"# Plotting the confusion matrix\n\nplt.figure(figsize=(10,7))\nsns.heatmap(data=nb_cm, annot=True, cmap=\"Reds\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.title('Confusion Matrix for Multinomial Naive Bayes Algorithm')\nplt.show()","cac65a44":"import re #RegEx Regular Expression Module\n\ndef stock_prediction(sample_news):\n    \n  sample_news = re.sub(pattern='[^a-zA-Z]',repl=' ', string=sample_news)\n  sample_news = sample_news.lower()\n  sample_news_words = sample_news.split()\n  sample_news_words = [word for word in sample_news_words if not word in set(stopwords.words('english'))]\n  ps = PorterStemmer()\n  final_news = [ps.stem(word) for word in sample_news_words]\n  final_news = ' '.join(final_news)\n\n  temp = cv.transform([final_news]).toarray()\n  return lr_classifier.predict(temp) #since the linear regression algorithm performed better, it is more likely to give us more precise predictions","48be42e5":"# For generating random integer\n\nfrom random import randint","a867b1c0":"sample_test = df_copy[df_copy['Date'] > '20141231']","71e8acf4":"sample_test.reset_index(inplace=True)\nsample_test = sample_test['Top1']","ffc7721f":"# Predicting values\n\nrow = randint(0,sample_test.shape[0]-1) # in ordre to choose a rondom row from the Column Top1 as example and make stock behaviours prediction on it\nsample_news = sample_test[row]\n\nprint('News: {}'.format(sample_news))\n\nif stock_prediction(sample_news):\n  print('Prediction: The stock price will remain the same or will go down.')\nelse:\n  print('Prediction: The stock price will go up !') # run this cell many times to test the accuracy of the results","21152cee":"### 1. Logistic Regression","1f602442":"##### note that the column 'Label' is equal to 0 when the stocks goes down or stays the same, and 1 when it goes up","fb870073":"# Predictions","a5ea9da0":"## Model Building \n(We will compare the performance of 3 different models: Linear Regression, Random Forest & Naive Bayes, and the best one will be chosen for our Predictions)","91c412ea":"### Data Cleaning and Preprocessing","52b6df97":"##### Note: Logistic Regression has here also a better performance, with a Precision score of 0.87","8676f576":"##### Note: Logistic Regression has slightly better performance in this case, with a Precision score of 0.87","56ce4914":"### Exploring the dataset","050b390e":"### 2. Random Forest Classifier","bdb2b490":"### 3. Multinomial Naive Bayes (Note: The binomial model could be also applied in this project)"}}