{"cell_type":{"126ed1e5":"code","d47c7e2a":"code","b41cd032":"code","dcddd42f":"code","ffec773b":"code","341b5b09":"code","2908ce1a":"code","4442d1b5":"code","dc591042":"code","3a0d474d":"code","42eb340b":"code","e9841157":"code","97149f98":"code","d23eadcc":"code","2047385e":"code","c108d828":"code","75fc14e8":"code","3e2ab745":"code","90209e27":"code","073c266f":"code","f85a6d8f":"code","cabbe338":"code","17a27b0c":"code","46685a60":"code","a29273c1":"code","92d8e8b3":"code","ef046fe0":"code","8e151b2b":"code","08f075a3":"code","4363a3a3":"code","25e5c98f":"code","af85ea79":"code","e785253c":"code","81d378ec":"code","72d0452b":"code","89f9472e":"code","4acae054":"code","66e0bf7b":"code","070d9a69":"markdown","0ac673e9":"markdown","c590c600":"markdown","3df5eb9f":"markdown","2cd0d110":"markdown","8a1b8b52":"markdown","51bc09e0":"markdown"},"source":{"126ed1e5":"import numpy as np\nimport pandas as pd \nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (16,20)\n\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d47c7e2a":"# Read data\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","b41cd032":"train.shape","dcddd42f":"train.info()","ffec773b":"# From the above info it is clear that out data contains mainly 3 types of data types\n# int, float and string\n# Any in some features there are lots of nul values included\n# In order to understand the null values better we plot them","341b5b09":"cols = train.columns[:81] # I am taking all the 81 features\ncolours = ['#B0E7AE','#F07470'] # Green for non - null values, red for null\nsns.heatmap(train[cols].isnull(),cmap = sns.color_palette(colours))","2908ce1a":"# After plotting , we can see that\n# in more than 16 features are having data missing\n# to understand this better we can use percentage method","4442d1b5":"for col in train.columns:\n    value_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col,round(value_missing*100)))","dc591042":"# FireplaceQu is missing 47% of values\n# PoolQC is missing 100% of values\n# MiscFeature is missing 96% of values\n# Fence is missing 81% of the values\n# So I am going to drop these 4 features\n\n# For other numerical values I am replacing with median\n# For other categorical values, I am going to replace with mode","3a0d474d":"cols_to_drop = ['FireplaceQu','PoolQC','MiscFeature','Fence']\ntrain_new = train.drop(cols_to_drop,axis = 1)","42eb340b":"train_new.shape","e9841157":"# train_new is our new dataframe","97149f98":"# We can apply this function to all the numeric feature together\ndata_numeric = train_new.select_dtypes(include = [np.number])\nnumeric_cols = data_numeric.columns.values\n\nfor col in numeric_cols:\n    missing = train_new[col].isnull()\n    num_missing = np.sum(missing)\n    \n    if num_missing > 0:\n        print('imputing missing values for : {}'.format(col))\n        train_new['{}_ismissing'.format(col)] = missing\n        med = train_new[col].median()\n        train_new[col] = train_new[col].fillna(med)","d23eadcc":"# Imputing non - numeric values\ndata_non_numeric = train_new.select_dtypes(exclude=[np.number])\nnon_numeric_cols = data_non_numeric.columns.values\n\nfor col in non_numeric_cols:\n    missing = train_new[col].isnull()\n    num_missing = np.sum(missing)\n    \n    if num_missing > 0:\n        print('imputing missing values for : {}'.format(col))\n        train_new['{}_ismissing'.format(col)] = missing\n        \n        top = train_new[col].describe()['top']\n        train_new[col] = train_new[col].fillna(top)","2047385e":"# Now verfying all numeric and non- numeric values are handled or not","c108d828":"train_new.isnull().sum()","75fc14e8":"train_new.describe()","3e2ab745":"# look how Sales Price is distributed\nprint(train_new['SalePrice'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(train_new['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});","90209e27":"# From the above figure, it is clear that\n# the graph is RIGHT SKEWED\n# Few values are > 600000 - which can be considerd as OUTLIERS\n# WE NEED TO GET RID OF OUTLIERS","073c266f":"# now plot all NUMERICAL VALUES the features in histogram","f85a6d8f":"train_new_num = train_new.select_dtypes(include = ['float64', 'int64'])\ntrain_new_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); ","cabbe338":"# Features like BsmtUnfSF , TotalBsmtSF , 1stFlrSF , GrLivArea looks similar distribution like SalePrice","17a27b0c":"# Correlation\ntrain_new_num_corr = train_new_num.corr()['SalePrice'][:-1] # -1 because the latest row is SalePrice\nclose_correlated_features_list = train_new_num_corr[abs(train_new_num_corr) > 0.5].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(close_correlated_features_list), close_correlated_features_list))","46685a60":"# But correlation is affected by Outliers\n# So plot these 10 features and try to remove those outliers","a29273c1":"train_correlated = train_new[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd','SalePrice']]\ntrain_correlated.head()","92d8e8b3":"# plot these features\nfor i in range(0, len(train_correlated.columns), 10):\n    sns.pairplot(data=train_correlated,\n                x_vars=train_correlated.columns[i:i+10],\n                y_vars=['SalePrice'])","ef046fe0":"X = train_correlated.drop(columns = 'SalePrice',axis=1)\nY = train_correlated['SalePrice']\nX.head()","8e151b2b":"Y.head()","08f075a3":"model = LogisticRegression(solver=\"liblinear\").fit(X,Y)","4363a3a3":"# Read the test data\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","25e5c98f":"predictor_columns = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']\ntest_X = test[predictor_columns]\ntest_X.head()","af85ea79":"test_X.info()","e785253c":"med = test_X['GarageCars'].median()\ntest_X['GarageCars'] = test_X['GarageCars'].fillna(med)","81d378ec":"med = test_X['GarageArea'].median()\ntest_X['GarageArea'] = test_X['GarageArea'].fillna(med)","72d0452b":"med = test_X['TotalBsmtSF'].median()\ntest_X['TotalBsmtSF'] = test_X['TotalBsmtSF'].fillna(med)","89f9472e":"test_X.info()","4acae054":"# Use the model to make predictions\npredicted_prices = model.predict(test_X)\nprint(predicted_prices)","66e0bf7b":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n\nmy_submission.to_csv('submission.csv', index=False)","070d9a69":"**b) Missing value Imputation**","0ac673e9":"**Exploratory Data Analysis**","c590c600":"**Submission**","3df5eb9f":"**a) Dropping featues**","2cd0d110":"**Handling null values**","8a1b8b52":"**Data Modelling**","51bc09e0":"**Model Training**"}}