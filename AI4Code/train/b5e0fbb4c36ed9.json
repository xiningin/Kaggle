{"cell_type":{"51e41721":"code","12d2908a":"code","7adf844d":"code","42e3cc0b":"code","8b74597f":"code","5a5d4eeb":"code","71d803fe":"code","ad4ef361":"code","9da2750a":"code","e6d60308":"code","61bdcc4a":"code","c2995e6d":"code","8f2648a8":"code","714cedde":"code","746c715d":"code","2848cf66":"code","fbd2fb01":"code","731ccc24":"code","5901b758":"code","2c6854e2":"code","c06c52bc":"code","7a538fdb":"code","f1546518":"code","4e0cee77":"code","337c4c3c":"code","baf43ccf":"code","ef35e118":"code","2e7657c1":"code","22184b15":"code","7c442107":"code","2212bb99":"code","11d63bb8":"code","73413176":"code","cb8dd48e":"code","fb94364c":"code","03124b11":"code","553b5012":"code","f6b4d271":"code","75c5e5e3":"code","7319b4db":"markdown","879abb7e":"markdown","0bf3d8a5":"markdown","9b8062d4":"markdown","af51aa32":"markdown","e0c198ae":"markdown","8882aea1":"markdown","3d976321":"markdown","b18a382f":"markdown","d81cb29e":"markdown","ffa7d6dc":"markdown","c52adbaa":"markdown","f0bdbb1c":"markdown","8537fd1e":"markdown","d6ad39f0":"markdown","9c0da1c2":"markdown","bb5c1f76":"markdown","940315ef":"markdown","0faf7fec":"markdown","b94dd094":"markdown","002beff7":"markdown","c287569f":"markdown","f0e1adec":"markdown","da5d53a4":"markdown","5b27cabc":"markdown","4debcc13":"markdown","a084995f":"markdown","f309e88e":"markdown","22ea65e4":"markdown","3b8cebda":"markdown","35d94e10":"markdown","0dd53d46":"markdown","32e56472":"markdown","f5c74114":"markdown","32b3af36":"markdown","82718f72":"markdown","057b886a":"markdown","a0f6297d":"markdown","73c0f398":"markdown","8b3c981b":"markdown","1d5f2419":"markdown","83efe644":"markdown","ecbb5fa7":"markdown","c4a76859":"markdown","016e434e":"markdown","b5d35b7b":"markdown","0ccc8900":"markdown","27d5088e":"markdown","09c8e46f":"markdown","5ba9211c":"markdown","c5a1f327":"markdown","7e845d1c":"markdown","8f17a4e3":"markdown","a0760c4b":"markdown","19564f52":"markdown","b3199228":"markdown","ee0a5e27":"markdown","99f5db48":"markdown","edd486d4":"markdown","234a1e8b":"markdown","48a6054e":"markdown","804f2817":"markdown","11dfb117":"markdown","82f4d24e":"markdown","5172131d":"markdown","ac9c3966":"markdown","d786d37c":"markdown","16b9b4d9":"markdown","881d9fc4":"markdown"},"source":{"51e41721":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n","12d2908a":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import autocorrelation_plot\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","7adf844d":"#Loading the single csv file to a variable named 'data'\nHR=pd.read_csv(\"..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","42e3cc0b":"#Lets look at a glimpse of table\nHR.head()","8b74597f":"#Lets look at no.of columns and information about its factors\nprint (\"The shape of the  data is (row, column):\"+ str(HR.shape))\nprint (HR.info())","5a5d4eeb":"#Looking at the datatypes of each factor\nHR.dtypes","71d803fe":"import missingno as msno \nmsno.matrix(HR);","ad4ef361":"print('Data columns with null values:',HR.isnull().sum(), sep = '\\n')","9da2750a":"print(\"Gender classification:\",HR.Gender.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Business Travel:\",HR.BusinessTravel.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Departments:\",HR.Department.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Educational Field:\",HR.EducationField.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Job Roles:\",HR.JobRole.value_counts(),sep = '\\n')\nprint(\"-\"*40)","e6d60308":"plt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\nplt.subplot(331)\nlabel = LabelEncoder()\nHR['EducationField'] = label.fit_transform(HR['EducationField'])\nsns.countplot(HR['EducationField'],)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(332)\nsns.countplot(HR['Gender'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(333)\nsns.countplot(HR['JobInvolvement'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(334)\nsns.countplot(HR.JobSatisfaction)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(335)\nsns.countplot(HR.EnvironmentSatisfaction)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(336)\nsns.countplot(HR.RelationshipSatisfaction)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(337)\nsns.countplot(HR.OverTime)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(338)\nsns.countplot(HR.WorkLifeBalance)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(339)\nsns.countplot(HR.StockOptionLevel)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n","61bdcc4a":"plt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\nplt.subplot(331)\nsns.distplot(HR['Age'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(332)\nsns.distplot(HR['DistanceFromHome'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(333)\nsns.distplot(HR['DailyRate'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(334)\nsns.distplot(HR['YearsAtCompany'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(335)\nsns.distplot(HR['TotalWorkingYears'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(336)\nsns.distplot(HR['NumCompaniesWorked'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(337)\nsns.distplot(HR['MonthlyRate'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(338)\nsns.distplot(HR['MonthlyIncome'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(339)\nsns.distplot(HR['PercentSalaryHike'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)","c2995e6d":"plt.figure(figsize=(20,5))\nplt.hist(HR.Age,bins=20)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Counts\")\nplt.title(\"Age Counts\")\nplt.show()","8f2648a8":"# Code forked from- https:\/\/www.kaggle.com\/roshansharma\/fifa-data-visualization\nlabels = ['R&D', 'Sales', 'HR']\nsizes = HR['Department'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0.1, 0.1, 0.2]\n\nplt.rcParams['figure.figsize'] = (9, 9)\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True)\nplt.title('Department Distribution', fontsize = 20)\nplt.legend()\nplt.show()","714cedde":"plt.rcParams['figure.figsize'] = (16, 8)\nax = sns.boxplot(x = HR['JobRole'], y =HR['MonthlyIncome'], data = HR, palette = 'inferno')\nax.set_xlabel(xlabel = 'Names of Job Roles', fontsize = 20)\nax.set_ylabel(ylabel = 'Monthly Income', fontsize = 20)\nax.set_title(label = 'Distribution of Salary across Job Roles', fontsize = 30)\nplt.xticks(rotation = 90)\nplt.show()","746c715d":"plt.figure(figsize=(10,5))\nplt.style.use('ggplot')\nsns.jointplot(x='TotalWorkingYears', y='MonthlyIncome', data=HR)","2848cf66":"sns.lmplot(x = 'TotalWorkingYears', y = 'MonthlyIncome', data = HR, col = 'Attrition')\nplt.show()","fbd2fb01":"plt.figure(figsize = (15, 7))\nplt.style.use('fivethirtyeight')\nplt.subplot(131)\nsns.swarmplot(x=\"Attrition\", y=\"Age\", data=HR)\nplt.subplot(132)\nsns.swarmplot(x=\"Attrition\", y=\"MonthlyIncome\", data=HR)\nplt.subplot(133)\nsns.swarmplot(x=\"Attrition\", y=\"YearsAtCompany\", data=HR)","731ccc24":"fig,ax = plt.subplots(figsize=(10,7))\nsns.violinplot(x='Gender', y='MonthlyIncome',hue='Attrition',split=True,data=HR)","5901b758":"plt.style.use('ggplot')\ng = sns.pairplot(HR, vars=[\"MonthlyIncome\", \"Age\"],hue=\"Attrition\",size=5)","2c6854e2":"f,ax=plt.subplots(figsize=(18,18))\nsns.heatmap(HR.corr(),annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".1f\",ax=ax)\nplt.show()","c06c52bc":"s = (HR.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","7a538fdb":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_data = HR.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_data[col] = label_encoder.fit_transform(HR[col])","f1546518":"s = (label_data.dtypes == 'object')\nprint(list(s[s].index))","4e0cee77":"label_data.head()","337c4c3c":"data_features=['Age','EnvironmentSatisfaction',\n               'Gender','JobInvolvement', 'JobLevel', 'JobRole',\n               'JobSatisfaction','MonthlyIncome','PerformanceRating',\n               'TotalWorkingYears','YearsAtCompany','OverTime']\nX=label_data[data_features]\ny=label_data.Attrition","baf43ccf":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\n\nmy_pipeline = Pipeline(steps=[('model', RandomForestRegressor(n_estimators=50,\n                                                              random_state=0))])","ef35e118":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)\nprint(\"Average MAE score (across experiments):\",scores.mean())","2e7657c1":"from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)\nmy_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\ncv_scores = cross_val_score(my_pipeline, X, y, \n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-validation accuracy: %f\" % cv_scores.mean())","22184b15":"# Drop leaky predictors from dataset\npotential_leaks = ['EnvironmentSatisfaction', 'JobSatisfaction', 'PerformanceRating', 'JobInvolvement']\nX2 = X.drop(potential_leaks, axis=1)\n\n# Evaluate the model with leaky predictors removed\ncv_scores = cross_val_score(my_pipeline, X2, y, \n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-val accuracy: %f\" % cv_scores.mean())","7c442107":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","2212bb99":"val_y.head()","11d63bb8":"#Code forked from -https:\/\/www.kaggle.com\/vanshjatana\/applied-machine-learning\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter = 500000)\nmodel.fit(train_X, train_y)\ny_pred = model.predict(val_X)\naccuracy = model.score(val_X, val_y)\nprint(accuracy)","73413176":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","cb8dd48e":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","fb94364c":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(train_X, train_y)\n    preds = model.predict(val_X)\n    return mean_absolute_error(val_y, preds)","03124b11":"print(\"Mean Absolute error of the Model:\")\nprint(score_dataset(train_X, val_X, train_y, val_y))","553b5012":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(train_X, train_y, \n             early_stopping_rounds=5, \n             eval_set=[(val_X, val_y)], \n             verbose=False)\npredictions = my_model.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","f6b4d271":"from sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(val_y, predictions.round())\nconf_matrix","75c5e5e3":"plt.subplots(figsize=(5,5))\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\ngroup_counts = ['{0:0.0f}'.format(value) for value in conf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten()\/np.sum(conf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()","7319b4db":"The given dataset has majority of employees from Research and Development followed by Sales and HR department ","879abb7e":"# 2.Kernel Goals\n\nThere are three primary goals of this kernel. \n* Do a **statistical analysis** of the employee attrition dataset\n* Do an **exploratory data analysis(EDA)** of the employee attrition dataset\n* **Predict:** Why employees are getting laid off? and which factor influences more towards attrition? \n![employee-attrition.jpg](attachment:employee-attrition.jpg)","0bf3d8a5":"Before jumping onto creating models we have to prepare our dataset for the models. We dont have to perform imputation as we dont have any missing values but we have categorical variables which needs to be encoded.","9b8062d4":"## 5d. Decision Tree Regressor\nDecision Tree is a decision-making tool that uses a flowchart-like tree structure or is a model of decisions and all of their possible results, including outcomes, input costs and utility.Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables. Now we are creating the function directly to get Mean Absolute error to fasten up our coding","af51aa32":"# 3. Importing libraries and exploring Data","e0c198ae":"## 4e. Distribution of Salary across Job Roles-Boxplot","8882aea1":"## 4f.Relationship with working years and income- Joint & lm Plot","3d976321":"## Before we Begin:\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards further datasets and produce more accurate models","b18a382f":"MAE of **0.2** . Great, Now lets boost our model using XGBoost","d81cb29e":"## 5e. Random Forest Regressor\nA **Random Forest** is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.","ffa7d6dc":"# 1.Introduction\n**International Business Machines Corporation (IBM)** is an American multinational technology company headquartered in Armonk, New York, with operations in over 170 countries. The company began in 1911, founded in Endicott, New York, as the Computing-Tabulating-Recording Company (CTR) and was renamed \"International Business Machines\" in 1924. IBM is incorporated in New York.\n\nThere are many factors for an employee working in IBM to get chucked out. The dataset I adopted got a lot of factors involved for an employee to get laid off. Hope the dataset will unveil the answers when we get onto the analysis\n![ibm_dreamstime.jpg](attachment:ibm_dreamstime.jpg)","c52adbaa":"## The Notebooks is still in progres..\n## Hit upvote if you like my work and also comment for suggestions to improve my model accuracy ","f0bdbb1c":"**Inference**\n* Employees within the age band of 25-35 were laid off at a huge rate\n* Employees who were receiving less monthly income(2500-5000) were laid off\n* Employees who had less working years(0-5) in the company were laid off","8537fd1e":"**84% accuracy**, The model performed bad compared to the earlier. Do you have any suggestions on imporving this model accuracy ? Should I have considered some other features? Drop some comments and help me","d6ad39f0":"Woof! We have nearly ***1470 records *** and ***35 factors*** . We need to do a build a lot of models to get an accurate prediction","9c0da1c2":"Great we have an average error of **0.22** . This is spot on !","bb5c1f76":"Woah. This data set seems to have no missing values,Phew. Now we don't need to clean or fill any NaN values, But lets confirm it numerically as well","940315ef":"## 4g.Relationship of predictor factors with Attrition-Swarm Plot","0faf7fec":"From the age distribution histogram, we can conclude that employees aged between **30-40 are working in IBM**","b94dd094":"## Splitting into training and test dataset\nNow let's split our dataset into training and test\/validation data for both feature and label variables","002beff7":"## 4i.Relationship with Age and Monthly income-Pair plot","c287569f":"## 4d. Department Distribution-Pie Chart","f0e1adec":"From the head of data we can clearly identify **Attrition factor as the target\/outcome variable** and rest of the factors are **predictor variables**","da5d53a4":"## 4c.Age Distribution-Histogram","5b27cabc":"Creating a pipline and fitting to the Random Forest Regressor model, then we measure Mean Absolute Error to check the deviation from observed and predicted values ","4debcc13":"## Dropping potential leakage features\nLet's drop few features which has the potential for data leakage feature and validate the model","a084995f":"# 4.Data Visualization\nBefore we dive into finding relations between independent variables and our dependent variable(attrition), let us create some assumptions about how the relations may turn-out among features.","f309e88e":"## 4b. Visualizing Distribution of features","22ea65e4":"**Inference**\n* Woah ! Both the genders weren't discriminated in income and the attrition is at the same rate in both the cases\n","3b8cebda":"**Inferences:**\n* Candidates who has done their bachelor degree are employeed more\n* There are more male employees\n* Majority of the employees show greater involvement\n* Majority of the employees are showing greater satisfaction(Job,Environment,Relationship)\n* Only few employees work overtime\n* Majority of the employees seem to handle their Work Life Balance well\n* Majority of the employees have zero stocks in the company","35d94e10":"**Inference**\n* The Income band widens for elder employees between 40-60 years of age and the income band restricts to 10000 for 20-30 year old employees\n* The employee who were laid off falll under low income(5000-1000) and young age(20-30) category","0dd53d46":"## Calculating Accuracy\nWe use 5 folds cross validation score and check for the model accuracy","32e56472":"After loading the dataset we can see a number of things. These 35 columns provide a very rich amount of information for deep data exploration we can do on this dataset. We have **26 numerical data** and **9 categorical data **","f5c74114":"## Encoding the Categorical Variables\n\nNow lets extract the data into the one which has categorical values into a variable and encode it ","32b3af36":"## 3e.Checking for missing data\nDatasets in the real world are often messy, However, this dataset is almost clean. Lets analyze and see what we have here.","82718f72":"The confusion matrix output is not clear and doesn't satisfy us, So let's make it visually beautiful","057b886a":"Encoding the categorical data with the help of **LabelEncoder()** function gifted by sci-kit learn package !","a0f6297d":"**82% accuracy** for our Logistic Regression model. Not bad for the first try !","73c0f398":"Absolute 0 error on each leaf nodes!","8b3c981b":"**Inference:**\n* Monthly and Daily Rates has proportional normal distribution\n* Most of the features except age parameter are right skewed\n\nShould we standardize this data ? Let's see that before the analysis","1d5f2419":"## Confusion Matrix\nA **confusion matrix** is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.","83efe644":"## 5f. XGBoost\n**XGBoost** is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance","ecbb5fa7":"# 5. Machine learning models","c4a76859":"**Inference**\n* Employees who had less working years were receiving comparitively lower salary\n* There are many fresher employees working in IBM \n* Most of the employees who got laid off had less working years(fresher and junior level) and they were receiving less salary\n* Very few people from senior category and receiving higher income got laid off","016e434e":"## 5a. Pipelines and Random Forest Regressor","b5d35b7b":"MAE of **0.24** ! Great","0ccc8900":"**85% accuracy**. Not great, Not terrible. Lets look at improving the validation accuracy","27d5088e":"## 3b.Importing Visualization and ML Libraries\nIt is important for an analysis to have data visualization and develop machine learning models to get accurate prediction. Here we are going use sklearn and matplotlib for machine learning and plotting respectively ","09c8e46f":"## 4h.Gender vs Monthly Income","5ba9211c":"## 4a.Count of categorical Variable-Bar Chart\nLet's look at the count in each category to assess the records","c5a1f327":"**Assumptions:**\n\n* **Education:** Lower education status employees were laid off more than with higher education status\n* **Age:** Older employees were laid off more than younger employees\n* **Job Satisfaction:** Employees with low satisfaction were laid off than one with higher satisfaction\n* **Environment Satisfaction:** Employees with low satisfaction were laid off than one with higher satisfaction\n* **Relationship Satisfaction:** Employees with low satisfaction were laid off than one with higher satisfaction\n* **Years at company:** Employees who had less work experience in that company were laid off\n* **Stock Option:** Employees with lower stock option were laid off\n* **Work Life balance:** Employees with poor work life balance were laid off\n* **Job Involvement:** Employees with lower Job involvement were laid off\n* **Over Time:** Employees who never worked overtime were laid off\n\nNow, let's see how the features are related to each other by creating some visualizations.","7e845d1c":"## 5c. Logistic Regression model and accuracry\nLogistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes. Mathematically, a logistic regression model predicts P(Y=1) as a function of X.","8f17a4e3":"## 4j.Correlation between Factors- Heatmap","a0760c4b":"Before jumping into the visualizations, Let's look at the **unique values** of few factors","19564f52":"## 3c.Extracting Data\n","b3199228":"## For different leaf nodes\nLet's check MAE at each leaf nodes of our decision tree","ee0a5e27":"**Inference from Confusion matrix**\n* The model has mostly predicted the employees who weren't laid off - **True Negative**\n* But it had a minor flaw, as nearly 13.04% became **False Negative**( which means the model has said employees were laid off when they weren't)\n* The model has 5.43% of correct prediction when the employees where actually laid off- **True Positive**\n* Nearly 3.53% of predictions where **False Positive** when the model predicted the employees weren't laid off when they actually were","99f5db48":"## 3d.Examining Dataset","edd486d4":"**Inference**\n* Highest average monthly income(17500) is given to Managers\n* Lowest average monthly income(2500) is given to Sales Representative, But there are many outliers in Sales Representative income\n* Research Scientist, Laboratory Technician and HR gets almost same average income\n","234a1e8b":"Now we don't have any categorical values in our dataset. Let's look at the top records to see it visually","48a6054e":"##  3a.Importing Libraries\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate\n","804f2817":"Neat, perfect numericals to feed our models. Now, let us split input and output variables","11dfb117":"## Calculating MAE(Mean Absolute Error)\n**Mean Absolute Error** is a  model evaluation metric used with  regression models. The mean absolute error of a model with respect to a  test set is the mean of the absolute values of the individual prediction errors on over all  instances in the  test set. Each prediction error is the difference between the true value and the predicted value for the instance.\n","82f4d24e":"Fine, We got a little idea about what is in dataset, Lets start visualizing the data","5172131d":"## 5b. Cross Validation and Data Leakage\n**Data leakage** refers to a mistake make by the creator of a machine learning model in which they accidentally share information between the test and training data-sets. Typically, when splitting a data-set into testing and training sets, the goal is to ensure that no data is shared between the two","ac9c3966":"As you see we have **0 null records**, lets get this party started !! \n","d786d37c":"Our goal is to check whether an employee got laid off or not. So we assume **Attrition** as our target(y) and rest of the features as predictor variables(X)","16b9b4d9":"**Inference-**\n**The band which has lighter colour has high correlation**\n* Boxes in lower right corner have high correlation,but those are relationship between their working period which obviously correlates and has nothing to aid for our objective, so we neglect that\n* Job level and Monthly income has perfect positive correlation which means higher the job level, better the income\n* Also Job level has high correlation with Working years, when the employyee's year of working increases he\/she is being promoted based on seniority\n* There is also a high correlation between Monthly Income and Total working years which depicts that employees earn higher income owing to their seniority with the firm","881d9fc4":"## Setting feature and target variable"}}