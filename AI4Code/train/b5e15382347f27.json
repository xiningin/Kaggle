{"cell_type":{"9cfb9a90":"code","37315d96":"code","28a8f172":"code","5fa7b010":"code","c46315eb":"code","f5d2e95c":"code","773c47e0":"code","af66f1d1":"code","5d40c023":"code","46529c51":"code","10ae5a9c":"code","9c9f6dbb":"code","27f78e79":"code","fd012341":"code","6729b0dc":"code","cef07952":"code","4d231623":"code","01885633":"code","f5c90425":"code","d282445f":"markdown","3f4985a9":"markdown","fcdfa844":"markdown","b600e701":"markdown","e8697afd":"markdown","32787725":"markdown","df562aab":"markdown","2bf67dd1":"markdown","1eb1859c":"markdown","1b317029":"markdown","07bb6234":"markdown","6d3a8e7c":"markdown","f9f0e643":"markdown"},"source":{"9cfb9a90":"import os\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt","37315d96":"path = '\/kaggle\/input\/happy-whale-and-dolphin\/'\nos.listdir(path)","28a8f172":"train_data = pd.read_csv(path+'train.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","5fa7b010":"print('Number train samples:', len(train_data))\nprint('Number train images:', len(os.listdir(path+'train_images\/')))\nprint('Number test images:', len(os.listdir(path+'test_images\/')))","c46315eb":"train_data.head()","f5d2e95c":"train_data['species'].value_counts()","773c47e0":"row = 0\nfile = train_data.loc[row, 'image']\nspecies = train_data.loc[row, 'species']\n\nimg = cv2.imread(path+'train_images\/'+file)\nprint('Shape:', img.shape)","af66f1d1":"fig, ax = plt.subplots(1, 1, figsize=(7, 7))\nax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_title(species)\nplt.show()","5d40c023":"def plot_examples(category = 'bottlenose_dolphin'):\n    \"\"\" Plot 5 images of a given category \"\"\"\n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 20))\n    fig.subplots_adjust(hspace = .1, wspace=.1)\n    axs = axs.ravel()\n    temp = train_data[train_data['species']==category].copy()\n    temp.index = range(len(temp.index))\n    for i in range(5):\n        file = temp.loc[i, 'image']\n        species = temp.loc[i, 'species']\n        img = cv2.imread(path+'train_images\/'+file)\n        axs[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        axs[i].set_title(species)\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n    plt.show()","46529c51":"plot_examples(category = 'bottlenose_dolphin')","10ae5a9c":"plot_examples(category = 'beluga')","9c9f6dbb":"plot_examples(category = 'humpback_whale')","27f78e79":"\ndef crop_image_from_gray(img, tol=7):\n    \"\"\"\n    Applies masks to the orignal image and \n    returns the a preprocessed image with \n    3 channels\n    \n    :param img: A NumPy Array that will be cropped\n    :param tol: The tolerance used for masking\n    \n    :return: A NumPy array containing the cropped image\n    \"\"\"\n    # If for some reason we only have two channels\n    if img.ndim == 2:\n        mask = img > tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    # If we have a normal RGB images\n    elif img.ndim == 3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img > tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n    \ndef preprocess_image(image, sigmaX=10):\n    \"\"\"\n    The whole preprocessing pipeline:\n    1. Read in image\n    2. Apply masks\n    3. Resize image to desired size\n    4. Add Gaussian noise to increase Robustness\n    \n    :param img: A NumPy Array that will be cropped\n    :param sigmaX: Value used for add GaussianBlur to the image\n    \n    :return: A NumPy array containing the preprocessed image\n    \"\"\"\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (224, 224))\n    image = cv2.addWeighted (image,4, cv2.GaussianBlur(image, (0,0) ,sigmaX), -4, 128)\n    return image\n","fd012341":"image_size = 128","6729b0dc":"y_train = train_data['species']\ny_train = pd.get_dummies(y_train)","cef07952":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nbase_model = EfficientNetB0(include_top=False, weights='imagenet' , input_shape=(224,224,3))\ndef add_new_last_layer(base_model, nb_classes):\n    #x = base_model.output\n    #x = GlobalAveragePooling2D()(x)\n    #x = Dense(512, activation='relu')(x)\n    #predictions = Dense(nb_classes, activation='softmax')(x)\n    #model = Model(input=base_model.input, output=predictions)\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(5, activation='relu')(x)\n    final_output = Dense(nb_classes, activation='softmax', name='final_output')(x)\n    model = tf.keras.Model(inputs=base_model.input, outputs=final_output)\n    return model\nmodel = add_new_last_layer(base_model, 15587)\nmodel.save('B0')\nmodel.compile(\n        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )","4d231623":"train_datagen = ImageDataGenerator(rotation_range=360,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                   validation_split=0.15,\n                                   preprocessing_function=preprocess_image, \n                                   rescale=1. \/ 255)\n\n\ntrain_generator = train_datagen.flow_from_dataframe(train_data, \n                                                    x_col='image', \n                                                    y_col='individual_id',\n                                                    directory = '\/kaggle\/input\/happy-whale-and-dolphin\/train_images\/',\n                                                    target_size=(224, 224),\n                                                    batch_size=32,\n                                                    class_mode='categorical', \n                                                    subset='training')\nval_generator = train_datagen.flow_from_dataframe(train_data, \n                                                  x_col='image', \n                                                y_col='individual_id',\n                                                  directory = '\/kaggle\/input\/happy-whale-and-dolphin\/train_images\/',\n                                                  target_size=(224, 224),\n                                                  batch_size=32,\n                                                  class_mode='categorical',\n                                                  subset='validation')","01885633":"hist = model.fit(train_generator, steps_per_epoch=train_generator.samples \/\/ 32,\n                    epochs=1,\n                    validation_data=val_generator,\n                    validation_steps = val_generator.samples \/\/ 32)\n#training slowly, needs to improve","f5c90425":"#next Step","d282445f":"And the preprocessor step is based on an old competition but I don't have notebook link anymore","3f4985a9":"# Model\n**Coming soon**","fcdfa844":"# Load Single Image\nWe plot the first image of of the train data.","b600e701":"# Image Preprocessing\nAs we can see the images have different format: landscape or portrait. For the neural network we need a standard size. So we have to prepare the data. ","e8697afd":"# Libraries","32787725":"Thanks to: https:\/\/www.kaggle.com\/drcapa\/happywhale-2022-starter\n\n","df562aab":"# Plot Examples\nWe plot example images of the breed top 10.","2bf67dd1":"# Data Generator\nWe define a data generator to load the data on demand.\n\n**Coming soon**","1eb1859c":"# Load Data","1b317029":"# EDA","07bb6234":"# Export","6d3a8e7c":"# Path","f9f0e643":"# Overview"}}