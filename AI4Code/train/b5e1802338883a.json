{"cell_type":{"a98ad07c":"code","f3cf3122":"code","7197723b":"code","89cb5f2a":"code","ee3c59bb":"code","72c4eed2":"code","cf83b705":"code","3104dda5":"code","14d76225":"code","455cdf5c":"code","6c50643e":"code","816621c5":"code","5ebbd429":"code","67762a36":"code","3e45b78e":"code","bb9a4e4c":"code","ba5d78bd":"code","3cb738b5":"code","f8946448":"code","a1dbc751":"code","de0bbc53":"code","04d2c8d0":"code","99802959":"code","cd7d77a9":"code","b197b6e3":"code","0c957d76":"code","aa18502c":"code","6f73929a":"code","282a9367":"code","0bae94c8":"code","d410d417":"code","3b91f2c4":"code","61a9cc1e":"code","37636f20":"code","4f30582a":"code","d2651977":"code","b25e3cf8":"code","0a87fc4c":"markdown","34e5ef87":"markdown","7b7338c3":"markdown","b73a28d8":"markdown","8cff8ab1":"markdown","d894d59d":"markdown","24f8acb2":"markdown","10f4e4d0":"markdown","7c093fbc":"markdown","9be04e6a":"markdown","f753c6df":"markdown","31dd662f":"markdown","f9e60a8e":"markdown"},"source":{"a98ad07c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3cf3122":"#1. Data collection\n\nimport pandas as pd \nimport numpy as np \n# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/nida-competition1\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/nida-competition1\/test.csv\")\n#,na_values='unknown'\n# preview train data","7197723b":"import matplotlib.pyplot as plt\nimport seaborn as sb\ncatlist = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\nfig, ax1 = plt.subplots(figsize=(15, 5))\nfig, ax2 = plt.subplots(figsize=(15, 5))\nfig, ax3 = plt.subplots(figsize=(15, 5))\nfig, ax4 = plt.subplots(figsize=(15, 5))\nfig, ax5 = plt.subplots(figsize=(15, 5))\nfig, ax6 = plt.subplots(figsize=(15, 5))\nfig, ax7 = plt.subplots(figsize=(15, 5))\nfig, ax8 = plt.subplots(figsize=(15, 5))\nfig, ax9 = plt.subplots(figsize=(15, 5))\nfig, ax10 = plt.subplots(figsize=(15, 5))\naxlist = [ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10]\nfor i,j in zip(catlist,axlist):\n    sbdf = train_df.groupby([i,'y']).count().reset_index()[[i,'y','age']].rename(columns={'age':'count'})\n    sb.barplot(x=i, y='count', hue='y', data=sbdf,ax= j)\nplt.show()","89cb5f2a":"catlist = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\nfor i in catlist:\n    percent = train_df.groupby([i,'y']).count().reset_index()[[i,'y','age']]\n    percent2 = train_df.groupby([i,'y']).count().reset_index()[[i,'y','age']].groupby([i]).sum().reset_index()\n    percenttotal = pd.merge(percent,percent2,how='left',on=i)\n    percenttotal['Percent'] = percenttotal['age_x']\/percenttotal['age_y']\n    percenttotal = percenttotal[percenttotal['y']=='yes'][[i,'Percent']]\n    desc = percenttotal.sort_values(['Percent'],ascending=False)\n    asc = percenttotal.sort_values(['Percent'],ascending=True)\n    print(desc.to_string(index=False),'\\n')\n#\u0e2a\u0e31\u0e14\u0e2a\u0e48\u0e27\u0e19\u0e17\u0e35\u0e48 subscribe \u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 (y=1)","ee3c59bb":"#plot heatmap to see correlation\ntraincorr = train_df.corr()\nsb.heatmap(traincorr, cmap = 'Blues')\nplt.show()","72c4eed2":"#features with unknown\nimcol = ['job','marital','education','default','housing','loan']\nfor i in imcol:\n    print(i)\n    print(train_df[i].unique(),'\\n')","cf83b705":"train_df.describe()","3104dda5":"train_df.info()","14d76225":"plt.hist(train_df['age'],bins=100)\nplt.show()","455cdf5c":"plt.hist(train_df['duration'],bins=100)\nplt.show()","6c50643e":"sb.boxplot(x=\"duration\", data=train_df)\nplt.show()","816621c5":"#drop 7 duplicate row\ntrain_df.drop_duplicates(inplace=True)\ntrain_df = train_df[train_df['age']<65]\ntrain_df = train_df[(train_df['duration']<=1000)] #600 obs\ntrain_df = train_df[train_df['duration']>5]","5ebbd429":"#Binning Age (grouping)\ntrain_df.loc[train_df['age']<=30,'AgeRange'] = '<30'\ntrain_df.loc[(train_df['age']>30) & (train_df['age']<=60),'AgeRange'] = '30-60'\ntrain_df.loc[train_df['age']>60,'AgeRange'] = '>60'\ntrain_df.drop('age',axis=1,inplace=True)\ntest_df.loc[test_df['age']<=30,'AgeRange'] = '<30'\ntest_df.loc[(test_df['age']>30) & (test_df['age']<=60),'AgeRange'] = '30-60'\ntest_df.loc[test_df['age']>60,'AgeRange'] = '>60'\ntest_df.drop('age',axis=1,inplace=True)\n","67762a36":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split  #split data into train and test set\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\n#\u0e41\u0e22\u0e01 X \u0e01\u0e31\u0e1a Y \u0e2d\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e01\u0e31\u0e19\nXtrain = train_df.drop('y',axis=1)\n\n#converts string columns into one-hot representation\ny = pd.get_dummies(train_df['y'],drop_first=True)\nXtrain = pd.get_dummies(Xtrain,drop_first=True)","3e45b78e":"Xtrain.head()","bb9a4e4c":"#converts string columns into one-hot representation for Xtest\nXtest = test_df\nXtest = pd.get_dummies(Xtest,drop_first=True)\nXtest = Xtest.reindex(columns=Xtrain.columns).fillna(0)","ba5d78bd":"print(Xtrain.shape)\nprint(y.shape)\nprint(Xtest.shape)","3cb738b5":"Xtrain.head()","f8946448":"multicol = ['emp.var.rate','euribor3m','nr.employed','cons.price.idx']\n[print(list(Xtrain.columns).index(i)) for i in multicol]","a1dbc751":"Allfeatures = list(Xtrain.columns)","de0bbc53":"#train data and Scale features to normalize independent variables (result in comparable scale) \nsc = StandardScaler()\nXtrain.iloc[:,:Xtrain.shape[1]+1]=sc.fit_transform(Xtrain.iloc[:,:Xtrain.shape[1]+1])\nXtest.iloc[:,:Xtrain.shape[1]+1]=sc.transform(Xtest.iloc[:,:Xtrain.shape[1]+1])\nXtrain = Xtrain.values\nXtest = Xtest.values\ny = np.ravel(y)","04d2c8d0":"print(Xtrain.shape,Xtest.shape)","99802959":"#Total subscriber before SMOTE (Class y=1)\nprint(\"%d from %d\\n %.2f\" % (sum(y), y.shape[0], sum(y)\/y.shape[0]*100),'%')","cd7d77a9":"#Synthetic Minority Oversampling Technique \u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a imbalance sample\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0)\nX_res,y_res = sm.fit_resample(Xtrain,y)","b197b6e3":"#Total subscriber after SMOTE (Class y=1)\nprint(\"%d from %d\\n %.2f\" % (sum(y_res), y_res.shape[0], sum(y_res)\/y_res.shape[0]*100),'%')","0c957d76":"#PCA\n#\u0e23\u0e27\u0e21 varible \u0e17\u0e35\u0e48 correlate \u0e01\u0e31\u0e19 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48'emp.var.rate','euribor3m','nr.employed','cons.price.idx'\nimport sklearn.decomposition as skdc\nfrom sklearn.decomposition import FactorAnalysis\nimport sklearn.pipeline as skpl\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\npca = skdc.PCA(n_components=1)\npcafit = pca.fit_transform(X_res[:,[4,7,8,5]],y_res)\nvar_explained = pca.explained_variance_ratio_ \nprint(pd.Series(var_explained))","aa18502c":"#\u0e40\u0e1e\u0e34\u0e48\u0e21 column \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01 dimensionality reduction (PCA)\nX_res = np.append(X_res,pcafit,axis=1)\nXtest = np.append(Xtest,pca.transform(Xtest[:,[4,7,8,5]]),axis=1)\n#Delete 'emp.var.rate','euribor3m','nr.employed','cons.price.idx'\nX_res = np.delete(X_res,[4,7,8,5],1)\nXtest = np.delete(Xtest,[4,7,8,5],1)","6f73929a":"X_res.shape","282a9367":"Allfeatures.remove('emp.var.rate')\nAllfeatures.remove('euribor3m')\nAllfeatures.remove('nr.employed')\nAllfeatures.remove('cons.price.idx')\nAllfeatures.append('PCA_economicsVar')","0bae94c8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nimport xgboost as xgb\nclassifier = LogisticRegression(random_state=1,max_iter=10000)\n#classifier = RandomForestRegressor(n_estimators = 49, random_state = 42)\n#classifier = svm.SVC()\nscores = cross_val_score(classifier, X_res, y_res, cv=10,scoring='roc_auc')\nclassifier.fit(X_res,y_res)\ny_pred = classifier.predict(Xtest)\ny_predprob = classifier.predict_proba(Xtest)\nprint(scores.mean())\nprint(scores)","d410d417":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nimport xgboost as xgb\nclf_xgb=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=2.0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.3, max_delta_step=0, max_depth=8,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=100, n_jobs=-1, num_parallel_tree=1, random_state=42,\n              reg_alpha=2, reg_lambda=2, scale_pos_weight=1, seed=42,\n              subsample=0.9, tree_method='exact', validate_parameters=1,\n              verbosity=None)\nscores = cross_val_score(clf_xgb, X_res, y_res, cv=10,scoring='roc_auc')\nclf_xgb.fit(X_res,y_res)\nprint(scores.mean())\nprint(scores)","3b91f2c4":"feature_importance = clf_xgb.get_booster().get_score()\nobjects = feature_importance.keys()\ny_pos = np.arange(len(objects))\nperformance = feature_importance.values()\nplt.figure(figsize=(8,20))\nplt.barh(y_pos, performance, align='center', alpha=0.5)\nplt.yticks(y_pos, objects)\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance')\nplt.show()","61a9cc1e":"#Most important features\nprint(Allfeatures[0],'\\n',Allfeatures[50],'\\n',Allfeatures[1])","37636f20":"y_predprob=clf_xgb.predict_proba(Xtest)","4f30582a":"\nresult = pd.DataFrame(y_predprob[:,1])","d2651977":"result.index+=1","b25e3cf8":"result.to_csv('result.csv')","0a87fc4c":"# Exploratory Analysis","34e5ef87":"# Final private score 0.94811\n# 6220422005 \u0e01\u0e32\u0e08\u0e19\u0e4c \u0e01\u0e38\u0e25\u0e0a\u0e19\u0e30\u0e23\u0e31\u0e15\u0e19\u0e4c\n# 6220422001 \u0e2a\u0e34\u0e23\u0e34\u0e18\u0e34\u0e14\u0e32 \u0e01\u0e31\u0e07\u0e27\u0e32\u0e2c\u0e27\u0e31\u0e12\u0e19\u0e32\n# 6220422004 \u0e2d\u0e42\u0e22\u0e18\u0e22\u0e32 \u0e28\u0e23\u0e35\u0e2d\u0e34\u0e19\u0e17\u0e23\u0e4c\n# 6220422011 \u0e27\u0e35\u0e23\u0e0a\u0e31\u0e22 \u0e2d\u0e39\u0e4b\u0e2a\u0e21\u0e1a\u0e39\u0e23\u0e13\u0e4c\n","7b7338c3":"# XGBClassifier","b73a28d8":"#Best parameter\nclf_xgb=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=2.0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.3, max_delta_step=0, max_depth=8,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=100, n_jobs=-1, num_parallel_tree=1, random_state=42,\n              reg_alpha=2, reg_lambda=1, scale_pos_weight=1, seed=42,\n              subsample=0.9, tree_method='exact', validate_parameters=1,\n              verbosity=None)","8cff8ab1":"best_clf.best_estimator_\nLogisticRegression(C=1, max_iter=10000, penalty='l1', random_state=0,solver='liblinear')","d894d59d":"from sklearn.model_selection import GridSearchCV\nXXtrain,XXtest,yytrain,yytest = train_test_split(X_res,y_res,random_state=42,stratify=y_res)\nparam_grid = {'max_depth' : [3,4,5],\n             'learning_rate' : [0.3,0.1,0.01,0.05],\n             'gamma' : [0,0.25,1.0],\n             'reg_lambda':[0,1,2,10],\n              'scale_pos_weight':[1,3,5]\n             }\nOptimal_param = GridSearchCV(estimator = xgb.XGBClassifier(objective='binary:logistic',subsample=0.9,colsample_bytree=0.5,seed=42),\n                          param_grid=param_grid,scoring='roc_auc',verbose=0,n_jobs=10,cv=3)\nOptimal_param.fit(XXtrain,yytrain,verbose=False,early_stopping_rounds=50,eval_metric='auc',eval_set = [(XXtest,yytest)])\n#clf_xgb = xgb.XGBClassifier(objective='binary:logistic',missing=None,seed=42)\n#clf_xgb.fit(XXtrain,yytrain,verbose=True,early_stopping_rounds=50,eval_metric='aucpr',eval_set = [(XXtest,yytest)])","24f8acb2":"# Logistic Regression","10f4e4d0":"# Data cleasing & Data engineering","7c093fbc":"# Data collection","9be04e6a":"# % subscribe Campaign for each feature (y=1)","f753c6df":"**Todo**\n1. Data collection\n2. Data cleaning\n3. Feature engineering\n4. Model training\n5. Model evaluation (Cross validation)","31dd662f":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nlogmodel = LogisticRegression(random_state=0,max_iter=10000)\nparam_grid = [{'penalty' : ['l1','elasticnet','none'],'C':[1,10,100,1000],'solver' : ['lbfgs','newton-cg','liblinear','sag','saga']},\n             {'penalty' : ['l2'],'C':[1,10,100,1000],'solver' : ['lbfgs','sag','saga']},\n             {'penalty' : ['elasticnet'],'C':[1,10,100,1000],'solver' : ['saga']}]\nclf = GridSearchCV (logmodel,param_grid,cv=5,verbose=True,n_jobs=-1,scoring='roc_auc')\nbest_clf = clf.fit(X_res,y_res)","f9e60a8e":"# Parameters tuning"}}