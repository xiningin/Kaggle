{"cell_type":{"23528687":"code","33dc1354":"code","d31cfe15":"code","0ae64cf4":"code","445aa92a":"code","3bdc73eb":"code","38572594":"code","ccaae7e3":"code","2c1ab223":"code","7eece372":"code","f9e485b5":"code","fafa4bde":"code","a3214b22":"code","eaed9451":"code","6d85ec49":"code","8c3902e7":"code","5f976dc8":"code","f2c5520d":"code","75a16d2a":"code","7593947d":"code","a1bd3b0e":"code","31ec8729":"code","802d4adf":"code","d653e166":"code","57ed2288":"code","7ad703c7":"code","604ee62d":"code","53e5e6b3":"code","77988163":"code","8a1b6918":"code","8d103fb8":"code","c9b7f408":"code","c41c9820":"code","189e4589":"code","e5dc115b":"code","e865b970":"code","701f7b76":"code","a909a634":"code","2a63a68b":"code","7110b1d4":"code","ec5fc178":"code","718ccdfb":"code","5f50a1e7":"markdown","5df3b08c":"markdown","8a5660ef":"markdown","4d608499":"markdown","dd042364":"markdown","c1244d84":"markdown","1cb5c844":"markdown","16a11d62":"markdown","838b486f":"markdown","749f8ef8":"markdown","c93c12d9":"markdown","a1d08e0b":"markdown","1e58d9c9":"markdown","890296f0":"markdown","a8825ebf":"markdown","4d3e2dea":"markdown","36990270":"markdown","98a8c02e":"markdown","9023d184":"markdown","97db56e0":"markdown","58b86b89":"markdown","0cd074bb":"markdown","e47ef094":"markdown","656b49c3":"markdown","9eb04308":"markdown","675eda1f":"markdown","d4d759b2":"markdown","cd4222e5":"markdown","f867c521":"markdown"},"source":{"23528687":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True) \nfrom plotly import tools\nimport plotly.figure_factory as ff\n\nimport nltk\nimport re\nimport string\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","33dc1354":"data=pd.read_csv('..\/input\/spam.csv',delimiter=',',encoding='latin-1')","d31cfe15":"data.head()","0ae64cf4":"data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)","445aa92a":"py.iplot(ff.create_table(data.head()),filename='show_data')","3bdc73eb":"data.columns=['label','text']\npy.iplot(ff.create_table(data.head()),filename='show_data')","38572594":"data.text[0]","ccaae7e3":"dir(string)","2c1ab223":"string.punctuation","7eece372":"def remove_punctuation(text):\n    new_text=''.join([char for char in text if char not in string.punctuation])\n    return new_text","f9e485b5":"data['new_text']=data['text'].apply(lambda row : remove_punctuation(row))","fafa4bde":"data.head()","a3214b22":"print(data.text[0])\ndata.new_text[0]","eaed9451":"def tokenize(text):\n    tokens=re.split('\\W+',text)\n    return tokens ","6d85ec49":"data['tokenized_text']=data['new_text'].apply(lambda row : tokenize(row.lower()))\ndata.head()","8c3902e7":"stopwords=nltk.corpus.stopwords.words('english')\nstopwords[:5]","5f976dc8":"def remove_stopwords(text):\n    clean_text=[word for word in text if word not in stopwords]\n    return clean_text ","f2c5520d":"data['clean_text']=data['tokenized_text'].apply(lambda row : remove_stopwords(row))\ndata.head()","75a16d2a":"ps = nltk.PorterStemmer()","7593947d":"dir(ps)","a1bd3b0e":"def stemming(tokenized_text):\n    stemmed_text=[ps.stem(word) for word in tokenized_text]\n    return stemmed_text","31ec8729":"data['stemmed_text']=data.clean_text.apply(lambda row : stemming(row))\ndata[['text','stemmed_text']].head()","802d4adf":"def get_final_text(stemmed_text):\n    final_text=\" \".join([word for word in stemmed_text])\n    return final_text","d653e166":"data['final_text']=data.stemmed_text.apply(lambda row : get_final_text(row))\ndata.head()","57ed2288":"from sklearn.feature_extraction.text import TfidfVectorizer","7ad703c7":"tfidf_model=TfidfVectorizer()\ntfidf_vec=tfidf_model.fit_transform(data.final_text)\ntfidf_data=pd.DataFrame(tfidf_vec.toarray())\ntfidf_data.head()","604ee62d":"def count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100","53e5e6b3":"data['punct%'] = data['text'].apply(lambda x: count_punct(x))","77988163":"bins = np.linspace(0, 100, 40)\nplt.hist(data['punct%'], bins)\nplt.title(\"Punctuation Distribution\")\nplt.show()","8a1b6918":"data['text_len'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))","8d103fb8":"bins = np.linspace(0, 250, 40)\nplt.hist(data['text_len'],bins)\nplt.title(\"text Length Distribution\")\nplt.show()","c9b7f408":"final_data=pd.concat([data['punct%'],data['text_len'],tfidf_data],axis=1)\nfinal_data.head()","c41c9820":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split","189e4589":"X_train,X_test,y_train,y_test = train_test_split(final_data,data['label'],test_size=.2)","e5dc115b":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50, max_depth=None, n_jobs=-1)\nrf_model = rf.fit(X_train, y_train)","e865b970":"rf_prediction=rf_model.predict(X_test)","701f7b76":"precision,recall,fscore,support = score(y_test,rf_prediction,pos_label='spam',average='binary')","a909a634":"print('Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((rf_prediction==y_test).sum() \/ len(rf_prediction),3)))","2a63a68b":"sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]","7110b1d4":"from sklearn.model_selection import GridSearchCV","ec5fc178":"rfg = RandomForestClassifier()\nparam = {'n_estimators': [10, 150, 300],\n        'max_depth': [30, 60, 90, None]}\n\ngs = GridSearchCV(rfg, param, cv=5, n_jobs=-1)\ngs_fit = gs.fit(final_data, data['label'])","718ccdfb":"print(gs_fit.best_params_)\nprint(gs_fit.best_score_)","5f50a1e7":"You can see now that our text is out of punctuations","5df3b08c":"To do that we'll import the string labrarie and using the punctuation attribute, you can see all the attributes of this labrarie using the dir() function","8a5660ef":"Let's begin by importing the spam dataset, and showing the first 5 rows.","4d608499":"We'll apply this function to create a new column \n","dd042364":"Let's change the columns name","c1244d84":"You can use the model feature_importances_ attribute to check the features that have been very significant  in our model ","1cb5c844":"Let's apply this function and show our tokenized text","16a11d62":"You can see that our text contains some punctuation characters so we need to deal with it","838b486f":"The lenght of the text may too have an impact on our label? ","749f8ef8":"Let's create a function that will help us to remove the punctuation characters from the original text ","c93c12d9":"let's load the required libraries","a1d08e0b":"Let's show the 5 rows of our data again, using this time the plotly labrarie","1e58d9c9":"In the list above we are interested in the stem method to create a stemmed text ","890296f0":"Our final text preprocessing is the stemming step ,we'll use the PorterStemmer to do that. ","a8825ebf":"To tune the model hyperparameters you can use the Grid Search methode","4d3e2dea":"So we need another function to remove the stop words","36990270":"In the second step we'll split each text into tokens so let's create another function","98a8c02e":"Drop down the unnecessary columns","9023d184":"Our final data have now 8028 columns","97db56e0":"Now we'll doing some feature engineering it's a necessary preprocessing step for machine learning ","58b86b89":"We deleted the punctuation before, but what if they have impact on our label ? so let's create a feature with percentage of punctuations.","0cd074bb":"So finally we'll join the stemmed words to create our final text ","e47ef094":"It's time for machine learning, we'll build a classification model using RandomForestClassifer","656b49c3":"The next step is to remove the stopwords because they does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence.","9eb04308":"Grid-search: Exhaustively search all parameter combinations in a given grid to determine the best model.","675eda1f":"We have now a data with 8026 columns each column represent a word ","d4d759b2":"In this data we have rows with texts so we need to transform each word in this texts into a feature column so let's do that ,we'll using TfidfVectorizer","cd4222e5":"Let's import the stop words and show the first 5 words in the list ","f867c521":"In this notebook we will use nltk librarie to analyse and clean the sms text body and then the random forest classifier algorithme to predict and classify the label of the sms based on the clean text."}}