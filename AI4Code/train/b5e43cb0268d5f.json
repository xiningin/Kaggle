{"cell_type":{"6a17aae4":"code","21006f53":"code","a272093d":"code","63607b39":"code","86871218":"code","220cfcb5":"code","d2639e60":"code","dca746dd":"code","67845c39":"code","623a9869":"code","bba1467c":"markdown","8f13382d":"markdown","4ca3e038":"markdown","e94e586e":"markdown","954d1973":"markdown","8a1bd269":"markdown","e61570f6":"markdown"},"source":{"6a17aae4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","21006f53":"!dpkg -i ..\/input\/libgrapenlp\/libgrapenlp_2.8.0-0ubuntu1_xenial_amd64.deb\n!dpkg -i ..\/input\/libgrapenlp\/libgrapenlp-dev_2.8.0-0ubuntu1_xenial_amd64.deb\n!pip install pygrapenlp","a272093d":"import json\nfrom collections import OrderedDict\nfrom pygrapenlp import u_out_bound_trie_string_to_string\nfrom pygrapenlp.grammar_engine import GrammarEngine","63607b39":"base_dir = os.path.join('..', 'input', 'grammar')\ngrammar_pathname = os.path.join(base_dir, 'test_grammar.fst2')\nbin_delaf_pathname = os.path.join(base_dir, 'test_delaf.bin')","86871218":"grammar_engine = GrammarEngine(grammar_pathname, bin_delaf_pathname)\n","220cfcb5":"def native_results_to_python_dic(sentence, native_results):\n    top_segments = OrderedDict()\n    if not native_results.empty():\n        top_native_result = native_results.get_elem_at(0)\n        top_native_result_segments = top_native_result.ssa\n        for i in range(0, top_native_result_segments.size()):\n            native_segment = top_native_result_segments.get_elem_at(i)\n            native_segment_label = native_segment.name\n            segment_label = u_out_bound_trie_string_to_string(native_segment_label)\n            segment = OrderedDict()\n            segment['value'] = sentence[native_segment.begin:native_segment.end]\n            segment['start'] = native_segment.begin\n            segment['end'] = native_segment.end\n            top_segments[segment_label] = segment\n    return top_segments\n","d2639e60":"sentence = 'this is a test sentence'\ncontext = {}\nnative_results = grammar_engine.tag(sentence, context)\nmatches = native_results_to_python_dic(sentence, native_results)\nprint(json.dumps(matches, indent=4))","dca746dd":"sentence = 'this is another test sentence'\ncontext = {}\nnative_results = grammar_engine.tag(sentence, context)\nmatches = native_results_to_python_dic(sentence, native_results)\nprint(json.dumps(matches, indent=4))","67845c39":"sentence = 'this is a context test sentence'\ncontext = {}\nnative_results = grammar_engine.tag(sentence, context)\nmatches = native_results_to_python_dic(sentence, native_results)\nprint(json.dumps(matches, indent=4))","623a9869":"sentence = 'this is a context test sentence'\ncontext = {'context': 'true'}\nnative_results = grammar_engine.tag(sentence, context)\nmatches = native_results_to_python_dic(sentence, native_results)\nprint(json.dumps(matches, indent=4))","bba1467c":"The grammar engine returns a native object with all the matches found in a text for a given grammar and dictionary. The native object can be consumed from Python using the SWIG corresponding objects returned by the pygrapenlp component, though for easier handling we can use this function in order to convert them first to a Python dictionary.","8f13382d":"In order to create a grammar engine instance we need 3 files:\n\n* a fst2 grammar file\n* a bin DELAF dictionary file\n* an inf DELAF dictionary file\n\nThese can be created with the Unitex grammar editor:\n\nhttps:\/\/unitexgramlab.org\/\n\nThe Unitex manual can be downloaded from that page (the link is at the bottom of the page).\n\nNote the bin and inf dictionary files must have the same name (apart from the file extension). The bin file contains the dictionary automaton (binary format), and the inf file the list of word properties (text format).\n\nUnitex saves the grammars and subgrammars in grf format. Before using a grf grammar with the GrapeNLP engine, it has to be converted to a single fst2 grammar which contains the grammar in a single recursive transition network with output (see https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-04235-5_17). To convert a set of grf files into a single fst2 file, use the following Unitex command in a terminal:\n\nGrf2Fst2 axiom.grf -o grammar.fst2\n\nwhere axiom.grf is the top level grf grammar file and fst2_file is the output grammar file in fst2 format. Note for having that command you need to compile and install some source code files that come with Unitex. After installing Unitex, open a terminal, and follow these steps:\n\n1. Go to the folder where you installed Unitex (usually, in your home folder, some folder starting with \"Unitex\")\n2. Inside the Unitex folder, go to folder Src\/unitex-core\/build\n3. Type \"make install\" to compile and install the Unitex command line tools\n\nThe Unitex command line tools are installed inside folder App of your Unitex installation. Adding the App to you system PATH is recommended.\n\nFor the dictionaries, if the grammar does not use dictionary lexical masks, one can use the small test dictionary in the pygrapenlp unit tests:\n\nhttps:\/\/github.com\/GrapeNLP\/pygrapenlp\/tree\/master\/test\/data\n\nThe folder also contains the test grammar we use here:\n\n![test_grammar.png](attachment:test_grammar.png)\n\nThis grammar recognizes the following sentences, if no context variables are defined:\n\n* this is a test sentence\n* this is another test sentence\n\nIf we define as well a context variable \"context\" with value \"true\", the following sentences are also recognized:\n\n* this is a context test sentence\n* this is another context test sentence\n\nApart from recognizing the sentences, the grammar engine generates a dictionary of tagged segments specifying for each segment the start and end characters of each labeled segment; in the test grammar, words \"a\" and \"another\" are tagged with label \"label\".\n\nNote the grammar engine performs exact matching of text, so either the entire text is recognized by the grammar or no matches are found. In order to match segments of text, one can use the lexical mask <TOKEN>, which matches any word, forming a loop with the <TOKEN> itself to mimic the regular expression \".+\". One can extract certain segments of from a text, and mark them in the grammar output for extraction, with a grammar such as:\n    \n![extract.png](attachment:extract.png)\n\nNote order matters: in this grammar, \"text to extract 1\" must be found before \"text to extract 2\". To allow for the latter text to appear before the former, both combinations must be stated in the grammar.\n\nFinally, the current version of the grammar engine does not allow for using the same tag more than one time in the same text: the grammar engine has been initially used in conversational agents for detecting the service requested by the user in a given sentence (e.g. making a phone call), and to extract potential arguments provided by the user in the sentence (e.g. the person to call):\n\n![phone_call.png](attachment:phone_call.png)\n\nIn this grammar, \"phone_number\" is a subgrammar recognizing phone numbers, which can be reused in any other grammar or part of a grammar. Upon ambiguity (e.g. entity \"name\" could also be a phone number since the lexical mask <TOKEN> matches any token), the grammar engine will select the interpretation matched by the most specific path; in the phone call grammar, as long as the phone_number grammar requires more specific tokens than <TOKEN> (e.g. digits, number words, plus symbol and parenthesis), a phone number will not be mistaken as a person's name.\n    \nSupported lexical masks can be found in chapter 6 of my PhD thesis:\n\nhttp:\/\/igm.univ-mlv.fr\/~sastre\/publications\/sastre11t.zip","4ca3e038":"This notebook illustrates how to use the GrapeNLP grammar engine in a Kaggle notebook. GrapeNLP is an efficient grammar engine that can be used for information extraction by applying handcrafted grammars for exact text matching. Opposite to machine learning approaches, precission with a grammar engine is 100%, as long as the grammar is correct. Recall is achieved by writing more comprehensive grammars. One can combine both grammar and machine learning approaches in order to achieve high precission for critical cases and high recall in general, by first applying the grammar engine then a machine learning recognizer when the grammar does not recognize a text or sentence.\n\nMore info on the grammar engine itself can be found at https:\/\/github.com\/GrapeNLP.\n\nThe examples presented here have been extracted from the unit test code of the pygrapenl Python package, the Python interface of the GrapeNLP grammar engine:\n\nhttps:\/\/github.com\/GrapeNLP\/pygrapenlp\/blob\/master\/test\/unit\/test_pygrapenlp.py\n\nThe test dictionary and test grammar files can be downloaded from here:\n\nhttps:\/\/github.com\/GrapeNLP\/pygrapenlp\/tree\/master\/test\/data","e94e586e":"To use the grammar engine we use method \"tag\", pasing a text as a Python string and a context as a Python dictionary of key\/value pairs. Here are several examples for the test grammar, extracted from the pygrapenlp unit tests.","954d1973":"In order to run the GrapeNLP engine in a Kaggle notebook we need to install the libgrapenlp and libgrapenlp-dev native libraries, as well as the pygrapenlp Python package. Precompiled versions of the native libraries can be downloaded from Launchpad:\n\nhttps:\/\/launchpad.net\/~grapenlp\/+archive\/ubuntu\/ppa\n\nThe Python package is published in Pypi:\n\nhttps:\/\/pypi.org\/project\/pygrapenlp\/","8a1bd269":"Here we create a grammar engine instance with the grammar fst2 file and the bin and inf dictionary files. Note we only specify the dictionary bin file, the inf file is assumed to be in the same path and have the same name than the bin file (apart from the file extension).","e61570f6":"If you use this work please cite the following publication:\n    \nSastre, J. M. (2011).\n  *Efficient finite-state algorithms of application of local grammars*.\n  PhD thesis, Universit\u00e9 Paris-Est & Universidad de Alicante.\n  Download link: http:\/\/monge.univ-mlv.fr\/~sastre\/publications\/sastre11t.zip\n\nOther related publications:\n    \nSastre, J. M., Sastre, J. and Garc\u00eda, J. (2009).\n  Boosting a chatterbot understanding with a weighted filtered-popping network parser.\n  In Vetulani, Z., editor, *Proceeedings of the 4th Language & Technology Conference (LTC'09)*, pages 74\u201478, Poznan, Poland. Wydawnictwo Poznanskie Sp. z o.o.\n  Download link: http:\/\/igm.univ-mlv.fr\/~sastre\/publications\/sastre09ip.pdf\n\nSastre, J. M. (2009).\n  Efficient parsing using filtered-popping recursive transition networks.\n  In Maneth, S., editor, *Implementation and Application of Automata*, volume 5642 of *Lecture Notes in Computer Science*, pages 241\u2014244. Springer-Verlag.\n  URL: https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-02979-0_28\n\nSastre, J. M. and Forcada, M. L. (2009).\n  Efficient parsing using recursive transition networks with output.\n  In Vetulani, Z. and Uszkoreit, H., editors, *Human Language Technology. Challenges of the Information Society*, volume 5603 of *Lecture Notes in Artificial Intelligence*, pages 192\u2014204. Springer-Verlag.\n  Extended version.\n  URL: https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-04235-5_17\n\nSastre, J. M. and Forcada, M. L. (2007).\n  Efficient parsing using recursive transition networks with output.\n  In Vetulani, Z., editor, *Proceedings of the 3rd Language & Technology Conference (LTC'07)*, pages 280\u2014284, Poznan, Poland. Wydawnictwo Poznanskie Sp. z o.o.\n  Download link: http:\/\/igm.univ-mlv.fr\/~sastre\/publications\/sastre07.zip\n"}}