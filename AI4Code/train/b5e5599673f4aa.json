{"cell_type":{"2e76645f":"code","67e71fbe":"code","ffc4ec0c":"code","802f5b25":"code","cb88cbca":"code","a5b8879f":"code","402ea7fe":"code","dcf5ca88":"code","3b3f778c":"code","fa86a522":"code","19d4ff80":"code","7d442223":"markdown","aac9e000":"markdown","40c8a6d4":"markdown","534eac87":"markdown","7e710054":"markdown","1f44c557":"markdown"},"source":{"2e76645f":"from fastai.vision.all import *\nfrom tqdm.notebook import  tqdm\n\nPATH = Path('..\/input\/optiver-realized-volatility-prediction')","67e71fbe":"class ResBlock(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv1d(ch, ch, kernel_size = 5, padding = 2, padding_mode='replicate'),\n            nn.BatchNorm1d(ch),\n            nn.ReLU(),\n            nn.Conv1d(ch, ch, kernel_size = 5, padding = 2, padding_mode='replicate'),\n            nn.BatchNorm1d(ch),\n        )\n        \n    def forward(self, x):\n        res = self.layers(x) + x\n        res = F.relu(res)\n        return res\n\nclass ResnetModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        chan = 32\n        layers = [nn.Conv1d(8, chan, kernel_size=1)]\n        for _ in range(8):\n            layers += [ResBlock(chan), ResBlock(chan), ResBlock(chan), ResBlock(chan)\n                       , nn.AvgPool1d(2, padding=1),\n                      ]\n        layers += [Flatten(), nn.Dropout()]   \n        self.conv_layers = nn.Sequential(*layers)\n        self.classifier = nn.Linear(chan*4, 1)\n        \n    def forward(self, x):\n        feat = self.conv_layers(x)\n        res = self.classifier(feat)\n        return sigmoid_range(res, 0, .1).view(-1)","ffc4ec0c":"data_dir = PATH\/'book_test.parquet'\nmodel_file = '..\/input\/optiver-resnet-model\/resnet_model2.pth'\nmodel = torch.load(model_file)","802f5b25":"means = tensor([  0.9997,   1.0003, 769.9902, 766.7346,   0.9995,   1.0005, 959.3417,\n        928.2203])\nstds = tensor([3.6881e-03, 3.6871e-03, 5.3541e+03, 4.9549e+03, 3.7009e-03, 3.6991e-03,\n        6.6838e+03, 5.7353e+03])","cb88cbca":"def fix_offsets(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df","a5b8879f":"def ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","402ea7fe":"def load_data(fname):\n    data = pd.read_parquet(fname)\n    stock_id = str(fname).split('=')[1]\n    time_ids = data.time_id.unique()\n    row_ids = list(map(lambda x:f'{stock_id}-{x}', time_ids))\n    data = fix_offsets(data)\n    data = ffill(data)\n    data = data[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', 'ask_price2', 'bid_size2', 'ask_size2']].to_numpy()\n    data = torch.tensor(data.astype('float32'))\n    data = (data - means) \/ stds\n    return data, row_ids","dcf5ca88":"def get_preds(data, model):\n    data = data.view(-1,600,8).permute(0, 2, 1)\n    with torch.no_grad():\n        preds = model(data.cuda())\n\n    return preds","3b3f778c":"%%time\nall_preds = []\nfor fname in tqdm(data_dir.ls()):\n    data, row_ids = load_data(fname)\n    preds = get_preds(data, model)\n    df_pred = pd.DataFrame(zip(row_ids, preds.tolist()),columns=['row_id', 'target'])\n    all_preds.append(df_pred)","fa86a522":"df_pred = pd.concat(all_preds)\ndf_pred.to_csv('submission.csv', index=False)","19d4ff80":"pd.read_csv('submission.csv').head()","7d442223":"### See the discussion [here](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251775)","aac9e000":"### Explained [here](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251277)","40c8a6d4":"### Stats from the train data used for normalization:","534eac87":"# Solution overview\n\n### This notebook demonstrates an approach where a neural network is trained on the raw book data. I'm not adding any engineered features, so the network starts with no concept of prices, returns, volatility or logarithms.\n\n### Each input sample is simply a 600x8 tensor representing the 8 numerical columns of the book data at each second of the 10 minute window.","7e710054":"## The model\nI'm using a convolutional neural network with architecture inspired by ResNet. With a total of 65 1D convolutional layers, followed by a single dense layer.\n\nWith a small number of channels and 5x1 convolutions this is still fairly lightweight and doesn't take long to infere. Training took around 25 minutes on a single RTX3090 GPU.","1f44c557":"**UPDATE Aug 11th**: I've cleaned up and simplified the model, now all it uses are 1D convolution instead the 5x1 2D convs. Also I experimented with numbers of layers and channels and managed to improve the score slightly. \nNote that still it only uses a single fold (80% of book data) and no trade data at all, so there's rooom for improvement."}}