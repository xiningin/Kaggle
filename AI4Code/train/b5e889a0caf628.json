{"cell_type":{"d2bc601c":"code","ca9a311f":"code","3f5acfe7":"code","12c2c34e":"code","db02186a":"code","a9b27de0":"code","a1df8340":"code","8d7a81f4":"code","fa866615":"code","39ecda36":"code","e6e7242d":"code","a3c02bc2":"code","d39356d5":"code","94b06d5c":"code","6e529c9e":"code","5a4ac4b0":"code","ed3f210a":"code","6db40ce7":"code","061da17f":"code","d24e346f":"code","d8363dfb":"code","9aa40430":"code","706f2b5f":"code","e07a970b":"code","572c1296":"code","733d37b5":"code","9795bc87":"code","ac63ca20":"code","719c2621":"code","ee51ade2":"code","76b101f7":"code","d79b0d05":"code","c332a809":"code","a6112646":"code","ceecc1cb":"code","e74e6129":"code","71372eaf":"code","de41ee9a":"code","bcf12424":"code","637962ae":"code","108aee46":"markdown","dc358f29":"markdown","121e14d2":"markdown","5ae2689d":"markdown","fd4c09af":"markdown","1e5c9030":"markdown","e7f60792":"markdown","20ee1d38":"markdown","974fe7d1":"markdown","8dadd025":"markdown","c5706352":"markdown","7aec7865":"markdown","d96bdba1":"markdown","f701ab0f":"markdown","7473b408":"markdown","70bc2029":"markdown","07801dc4":"markdown"},"source":{"d2bc601c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport numpy as np\nimport math\nimport pandas as pd\nfrom sympy.solvers import solve\nfrom sympy import Symbol, exp\nimport matplotlib.pyplot as plt\n\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.\n","ca9a311f":"N = 5 # Significant Number  \n\n#True Value of the function \ntv = math.exp(-1*(0.2**2)\/3)\nprint(\"True value: \\n{}\".format(tv))\n\n#Tolerance of the system - Iteration continue until Tolerance error is neglectable for our algorithm.-\neps=0.5*(10)**(2-N)\nprint(\"Tolerance:\\n{}\".format(eps))","3f5acfe7":"# Analytic Defination of function \ndef expp(x,term):\n    expa=0\n    for n in range(term):\n        expa=expa+((-1)**n)*(x**(2*n))\/((3**n)*math.factorial(n))\n    return expa","12c2c34e":"#Print nth order approximation term of Taylar expansion\napt=[] # create empty list for approximation terms\nfor i in range(0,100): # 0 to 100 term approximation of Taylor expansion\n    apt.append(expp(0.2,i)) # Append term approximation to empty list (apt)\n# Print fist to fourth term approximation (This process can be extend if users want to)     \nprint(\"First term approximation: \\n{}\".format(apt[1]))\nprint(\"Second term approximation: \\n{}\".format(apt[2]))\nprint(\"Third term approximation: \\n{}\".format(apt[3]))\nprint(\"Fourth term approximation: \\n{}\".format(apt[4]))","db02186a":"# Unleash the Itreation's Beast  \n\napt1=[] #List of Current Value\nept=[] #True Relative error\nepa=1 #Approximate Relative Error\nitr=[] #Iteration numbers\nepa1=[] # List of Approximate Percent Relative Error terms\nwhile epa > eps:\n    for j in range(0,15):\n        epa=abs((apt[j+1]-apt[j])\/apt[j+1])*100 # Calculate Relative Error with current and previous value\n        itr.append(j)\n        ept.append(abs(((tv-apt[j])\/tv)))\n        print(\"Term\",j+1)\n        print(\"Approximate relative error\",epa)\n        print(\"True relative error\",ept[j])\n        print(\"Previous Value\",apt[j])\n        print(\"Current Value\",apt[j+1])\n        apt1.append(apt[j+1])\n        epa1.append(epa)\n        # if tolerance error bigger than relative error which is calculated in iteration then break the iteration\n        if eps > epa :\n            break","a9b27de0":"# Create table from variables and numbers in previous iteration\ntable = pd.DataFrame({'Iteration':itr,'True Relative Error':ept,'Current Value': apt1, 'Approximate relative error':epa1})\nprint(table.ix[1:]) # The first iteration not count because algorithm has no previous value in inital state\nprint(table.loc[:,\"Current Value\"]) # ","a1df8340":"# Figure of analytic solution and Taylor approximations\n\nx = np.linspace(-1, 2) # range of x-axis\nplt.figure(figsize=(15,7))\n\nplt.plot(x, np.exp(-1*(x**2)\/3), label=\"Analytic Solution\", color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5) # Analytic solution\nplt.plot(x, expp(x, 1), label=\"First Term Approximation\", color=\"orange\", marker=\"o\", markerfacecolor=\"black\", linewidth=2, markersize=5) # First term approximation\nplt.plot(x, expp(x, 2), label=\"Second Term Approximation\", color=\"yellow\", marker=\"o\", markerfacecolor=\"green\", linewidth=2, markersize=5) # Second term approximation\nplt.plot(x, expp(x, 3), label=\"Third Term Approximation\", color=\"red\", marker=\"o\", markerfacecolor=\"blue\", linewidth=2, markersize=5) # Third Term approximation\nplt.plot(x, expp(x, 4), label=\"Fourth Term Approximation\", color=\"green\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5) # Fourth Term approximation\nplt.legend()\nplt.show()\n","8d7a81f4":"# Define the function \ndef f(x):\n    return x**3-4*x**2+3*x+12","fa866615":"# Prediction for x axis. Each prediction done after examine preveous graph. \nx1 = np.linspace(-10, 10)\nx2 = np.linspace(-2.5, 2.5)\nx3 = np.linspace(-2, -1)\nx4 = np.linspace(-1.5, -1)\nx5 = np.linspace(-1.3, -1.1)\nx6 = np.linspace(-1.28, -1.25)","39ecda36":"fig = plt.figure(figsize=(20,10))\n\nax1 = fig.add_subplot(231)\nax1.plot(x1, f(x1), color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5)\nplt.xlabel(\"x values\")\nplt.ylabel(\"f(x)\")\n\nax2 = fig.add_subplot(232)\nax2.plot(x2, f(x2),color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5)\nplt.xlabel(\"x values\")\nplt.ylabel(\"f(x)\")\n\nax3 = fig.add_subplot(233)\nax3.plot(x3, f(x3),color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5)\nplt.xlabel(\"x values\")\nplt.ylabel(\"f(x)\")\n\nax4 = fig.add_subplot(234)\nax4.plot(x4, f(x4), color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5)\nplt.xlabel(\"x values\")\nplt.ylabel(\"f(x)\")\n\nax5 = fig.add_subplot(235)\nax5.plot(x5, f(x5), color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5)\nplt.xlabel(\"x values\")\nplt.ylabel(\"f(x)\")\n\nax6 = fig.add_subplot(236)\nax6.plot(x6, f(x6), color=\"blue\", marker=\"o\", markerfacecolor=\"red\", linewidth=2, markersize=5)\nplt.xlabel(\"x values\")\nplt.ylabel(\"f(x)\")\nplt.show()","e6e7242d":"# result of the function with the estimate value from final graph\nprint(f(-1.253))","a3c02bc2":"# Define the function\n#Same function in graphical method, so methods can be compared\ndef f(x):\n    return x ** 3 - 4 * x ** 2 + 3 * x + 12","d39356d5":"# Number of significant figures and error criterion\nn = 3\neps = 0.5 * (10 ** (2 - n)) # Tolerance Error\n\n# Choose lower and upper value for function\nxu = 4\nxl = -3\n\n# Create Empty List for root values\nxr_l = []  # Empty list of roots\nepa_l = []  # Empty list of Approximation Error\nitr_l = []  # Empty list of Iteration number\n\n# An estimate of root xr is determined by\nxr_l.append((xu + xl) \/ 2)  # First iteration begin this value","94b06d5c":"# Unleash The Beast\nfor i in range(0, 30):  # Iterate 0 to 30\n    if f(xl) * f(xr_l[i]) < 0:  # Step 1: if this condition hold ;\n        xu = xr_l[i]  # Then equal xu(upper value) value to new xr(root value)\n        xr = ((xu + xl) \/ 2)\n        xr_l.append(xr)  # append root values to empty list of root\n        epa = abs((xr_l[i] - xr_l[i - 1]) \/ xr_l[i])  # Calculate Tolerance Error from new root value and old root value\n        epa_l.append(epa)  # append tolerance error to empty list of Approximation error\n        itr_l.append(i)  # append iteration numbers to empty list of iteration list\n\n    if f(xl) * f(xr_l[i]) > 0:  # Step 2: if this condition hold ;\n        xl = xr_l[i]  # Then equal xl(lower value) value to new xr(root value)\n        xr = ((xu + xl) \/ 2)\n        xr_l.append(xr)  # append root values to empty list of root\n        epa = abs(\n            (xr_l[i] - xr_l[i - 1]) \/ xr_l[i]) * 100  # Calculate Tolerance Error from new root value and old root value\n        epa_l.append(epa)  # append tolerance error to empty list of Approximation error\n        itr_l.append(i)  # append iteration numbers to empty list of iteration list\n    # Terminate Criteria for Algorithm\n    elif eps > epa != 0:  # Step 3: else if tolerance error's value bigger than(and also not equal zero) approximation\n        break             # error's value which is calculated in step 1 or step 2 then stop to iteration","6e529c9e":"table = pd.DataFrame({'Iteration': itr_l, 'Root Value': xr_l[1:], 'Approximation Error': epa_l}) # xr_l[1:] used because xr_l has one more\n#term than others list(Remember, xr_l was not begin as empty list.)\nprint(table)\n","5a4ac4b0":"# result of the function with the estimate value from final iteration\nprint(f(-1.252563))","ed3f210a":"# Define Function\ndef f(x):\n    return (np.exp(-1 * 0.5 * x) * (4 - x)) - 2\n\n\n# Number of significant figures and error criterion\nn = 4\neps = 0.5 * 10 ** (2 - n)\nprint(\"Percent Tolerance: \\n{}\".format(eps))","6db40ce7":"# Analytic Solution of the function(this is used for comparing the algorithm's value for function)\nx = Symbol('x')\nAs = solve(exp(-1 * 0.5 * x) * (4 - x) - 2, x)\n\nprint(\"Solution of function: \\n{}\".format(As))","061da17f":"# Graph for guessing to initial value(x lower and x upper)\nx = np.linspace(-1, 3)\n\nplt.figure(figsize=(16,8))\nplt.plot(x, f(x), marker='o', markerfacecolor=\"r\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Function\")\nplt.show()","d24e346f":"# Two initial guesses, they choose from graph\nx_u = 1  # upper point\nx_l = 0  # lower point\n\n# Iteration Algorithm\nitr = []  # list of iteration number\nx_r = []  # list of root\nepa = 1  # approximation error(for start to iteration)\nepa1 = []  # list approximation error\nfor i in range(0, 20):\n    if f(x_l) * f(x_u) < 0: # Step 1\n        x_r.append(x_u - ((f(x_u) * (x_l - x_u)) \/ (f(x_l) - f(x_u)))) # Eq.6\n        epa = (abs((x_r[i] - x_r[i - 1]) \/ x_r[i])) * 100 # Approximation Error Calculation\n        epa1.append(epa) # Append approximation error values to empty list of approximation error\n        itr.append(i) # Append iteration number to empty list of iteration\n        x_u = x_r[i]\n\n    if f(x_l) * f(x_u) > 0: # Step 2\n\n        x_r.append(x_u - ((f(x_u) * (x_l - x_u)) \/ (f(x_l) - f(x_u)))) # Eq.6\n        x_l = x_r[i]\n        epa = (abs((x_r[i] - x_r[i - 1]) \/ x_r[i])) * 100 # Approximation Error Calculation\n\n    elif eps > epa != 0.0: # Step 3 : Stop iteration when tolernace error bigger than approximation error\n        break","d8363dfb":"table = pd.DataFrame({'Iteration': itr, 'Approximation Percent Error': epa1, ' Estimate': x_r})\nprint(table.ix[1:]) # table.ix[1:] is used because first row is not includue iteration process","9aa40430":"# Error of False-position Method\nerr = (abs(np.asarray(As) - np.asarray(x_r[6])) \/ np.asarray(As)) * 100 # x_r[6] represent final(and also optimum) value of the iteration\nprint(\"Error Percent of Newton-Raphon Method \\n{}\".format(err))","706f2b5f":"# Define Function\ndef f(x):\n    return (math.exp(-1 * 0.5 * x) * (4 - x)) - 2\n\n\n# Derivation of The Function\n\ndef fd(x):\n    x = Symbol('x')\n    f = exp(-1 * 0.5 * x) * (4 - x) - 2\n    return f.diff(x)\n\n\nprint(\"Derivation of Function f(x): \\n{}\".format(fd(x)))\n","e07a970b":"def fd(x):  # fd function define for numerical calculation from analytic solution output\n    return -0.5 * (-x + 4) * exp(-0.5 * x) - exp(-0.5 * x)","572c1296":"# Initial Guess\nx_f = 1\n\n# Iteration Algorithm\nitr = []  # list of iteration number\nx_s = []  # list of root\nepa = 1  # approximation error\nepa1 = []  # list approximation error\nwhile epa > eps:\n    for i in range(0, 10): # Step 1\n        x_s.append(x_f - f(x_f) \/ fd(x_f)) # Eq.4\n        epa = abs((x_s[i] - x_s[i - 1]) \/ x_s[i]) * 100 # Calculation of Approximation Error\n        epa1.append(epa) # Append approximation error to empty list \n        itr.append(i) # Append iteration number to empty list\n\n        x_f = x_s[i]\n\n        if eps > epa != 0.0: # Stop if tolerance error is bigger than approximation error\n            break","733d37b5":"table = pd.DataFrame({'Iteration': itr, 'Approximation Percent Error': epa1, ' Estimate': x_s})\nprint(table.ix[1:]) # table.ix[1:] is used because first row is not includue iteration process","9795bc87":"# Error of Newton-Raphson Method\nerr = (abs(np.asarray(As) - np.asarray(x_s[2])) \/ np.asarray(As)) * 100\nprint(\" Percent Error of Newton-Raphson Method \\n{}\".format(err))","ac63ca20":"# Define matrix for coefficient\nA = np.array([[-1, -2, 3],\n               [5, 2, -1],\n               [3, 10, 0.1]])\nprint(\"Coefficient Matrix: \\n{}\".format(A))\n\n\n# Define matrix right side of equation\nB = np.array([[-1],\n               [1],\n               [2]])\nprint(\"Right Side of Equation's Matrix: \\n {}\".format(B))","719c2621":"# Solve Linear System with linalg.solve command for comparison to algorithm's results\nsol = np.linalg.solve(A,B)\nprint(\"Value of x1, x2, x3 \\n{}\".format(sol))\n\n# Tolerance error\nn = 4  # Number of significant figure\neps = 0.5 * (10 ** (2 - n))","ee51ade2":"# Forward Elimination\n\nfor i in range(0, 1): # first column\n    for j in range(1, 3): # second row to third row\n        B[j] = B[j]-(A[j, i]\/A[i, i])*B[i]\n        for k in range(2, -1, -1): # third row to first row\n            #print(j, i, B)\n            A[j, k] = A[j, k]-(A[j, i]\/A[i, i])*A[i, k]\n            print(j, k, i, A)\n\nfor i in range(1, 2): # second column\n    for j in range(2, 3): # third row\n        B[j] = B[j]-(A[j, i]\/A[i, i])*B[i]\n        for k in range(2, -1, -1): # third row to first row\n            #print(j, i, B)\n            A[j, k] = A[j, k]-(A[j, i]\/A[i, i])*A[i, k]\n            print(j, k, i, A)\n\nprint(\"Right Side of Equation's Matrix after Forward Substation: \\n {}\".format(B))\nprint(\"Coefficient Matrix After Forward Substation: \\n{}\".format(A))","76b101f7":"# Back Substation\nx3 = B[2]\/A[2,2]\nx2 = (B[1]-(x3*A[1, 2]))\/A[1, 1]\nx1 = (B[0]-(x2*A[0, 1]+x3*A[0, 2]))\/A[0, 0]\n\nprint(\"Value of x1, x2, x3 \\n {} \\n {} \\n {} \".format(x1, x2, x3))\n\n","d79b0d05":"# Error Calculation\nerx1 = ((sol[0]-x1)\/x1)*100 # Error percent of x1\nerx2 = ((sol[1]-x2)\/x2)*100 # Error percent of x2\nerx3 = ((sol[2]-x3)\/x3)*100 # Error percent of x3\n\nprint(\"Percent Error of x1:\", erx1)\nprint(\"Percent Error of x2:\", erx2)\nprint(\"Percent Error of x3:\", erx3)","c332a809":"# Define matrix for coefficient\ncm = np.array([[-1, -2, 3, 10],\n               [5, 2, -1, 0.5],\n               [3, 10, 0.1, -0.1],\n               [4, 4, -20, 4]])\nprint(\"Coefficient Matrix: \\n{}\".format(cm))\n\n# Define matrix right side of equation\nrm = np.array([[-1],\n               [1],\n               [2],\n               [-3]])\nprint(\"Right Side of Equation's Matrix: \\n {}\".format(rm))","a6112646":"# Summon linalg.solve command for comparison with solution of algorithm\nsol = np.linalg.solve(cm, rm)\nprint(\"Value of x1, x2, x3, x4: \\n{}\".format(sol))","ceecc1cb":"# Tolerance error\nn = 4  # Number of significant figure\neps = 0.5 * (10 ** (2 - n))","e74e6129":"# Create empty list for unknown\nx1 = []\nx2 = []\nx3 = []\nx4 = []\n# Define approximation errors of each unknown\nepa1 = 1\nepa2 = 1\nepa3 = 1\nepa4 = 1\n# Initial guesses for Unknown\nx2.append(0.000000000000000001)\nx3.append(0.000000000000000001)\nx4.append(0.000000000000000001)\n\nitr = [] # List of iteration\nepa1_l = [] # List of approximation percent error for x1\nepa2_l = [] # List of approximation percent error for x2\nepa3_l = [] # List of approximation percent error for x3\nepa4_l = [] # List of approximation percent error for x4\n\n# Unleash the beast\nfor i in range(0, 20): # Iteration 1 to 20\n\n    x1.append((rm[1] - cm[1][1] * x2[i] - cm[1][2] * x3[i] - cm[1][3] * x4[i]) \/ cm[1][0]) # Second row used\n    x2.append((rm[2] - cm[2][0] * x1[i] - cm[2][2] * x3[i] - cm[2][3] * x4[i]) \/ cm[2][1]) # Third row used\n    x3.append((rm[3] - cm[3][0] * x1[i] - cm[3][1] * x2[i] - cm[3][3] * x4[i]) \/ cm[3][2]) # Fourth row used\n    x4.append((rm[0] - cm[0][0] * x1[i] - cm[0][2] * x3[i] - cm[0][1] * x2[i]) \/ cm[0][3]) # First row used\n    epa1 = abs(((x1[i] - x1[i - 1]) \/ x1[i])) * 100 # x1's approximation error\n    epa2 = abs((x2[i] - x2[i - 1]) \/ x2[i]) * 100 # x2's approximation error\n    epa3 = abs((x3[i] - x3[i - 1]) \/ x3[i]) * 100 # x3's approximation error\n    epa4 = abs((x4[i] - x4[i - 1]) \/ x4[i]) * 100 # x4's approximation error\n    itr.append(i)\n\n    epa1_l.append(epa1)\n    epa2_l.append(epa2)\n    epa3_l.append(epa3)\n    epa4_l.append(epa4)\n    \n    if eps > epa1 != 0.0 and eps > epa2 != 0.0 and eps > epa3 != 0.0 and eps > epa4 != 0.0: # Break to Iteration when approximation \n        #errors of unknowns smaller than tolerance error\n        break","71372eaf":"table = pd.DataFrame({'Iteration': itr, 'x1': x1, 'x2': x2[1:], 'x3': x3[1:], 'x4': x4[1:] }) # Crate table with iteration number and unknowns\nprint(table)","de41ee9a":"# Error percent of unknowns (between linalg.solve-sol[]- and algorihm -x_i- result )\nerx1 = ((sol[0]-x1[max(itr)])\/x1[max(itr)])*100 # Error percent of x1 \nerx2 = ((sol[1]-x2[max(itr)])\/x2[max(itr)])*100 # Error percent of x2\nerx3 = ((sol[2]-x3[max(itr)])\/x3[max(itr)])*100 # Error percent of x3\nerx4 = ((sol[3]-x4[max(itr)])\/x4[max(itr)])*100 # Error percent of x4\n\nprint(\"Percent Error of x1:\", erx1)\nprint(\"Percent Error of x2:\", erx2)\nprint(\"Percent Error of x3:\", erx3)\nprint(\"Percent Error of x4:\", erx4)","bcf12424":"#Improvement of convergence by Relaxation\nl = 1.5 # Lambda Value\n\n# Unleash the beast\nfor i in range(0, 20): # Iteration 1 to 20\n\n    x1.append((rm[1] - cm[1][1] * l*x2[i] - cm[1][2] * l*x3[i] - cm[1][3] * l*x4[i]) \/ cm[1][0]) # Second row used\n    x2.append((rm[2] - cm[2][0] * l*x1[i] - cm[2][2] * l*x3[i] - cm[2][3] * l*x4[i]) \/ cm[2][1]) # Third row used\n    x3.append((rm[3] - cm[3][0] * l*x1[i] - cm[3][1] * l*x2[i] - cm[3][3] * l*x4[i]) \/ cm[3][2]) # Fourth row used\n    x4.append((rm[0] - cm[0][0] * l*x1[i] - cm[0][2] * l*x3[i] - cm[0][1] * l*x2[i]) \/ cm[0][3]) # First row used\n    epa1 = abs(((l*x1[i] - l*x1[i - 1]) \/ l*x1[i])) * 100 # x1's approximation error\n    epa2 = abs((l*x2[i] - l*x2[i - 1]) \/ l*x2[i]) * 100 # x2's approximation error\n    epa3 = abs((l*x3[i] - l*x3[i - 1]) \/ l*x3[i]) * 100 # x3's approximation error\n    epa4 = abs((l*x4[i] - l*x4[i - 1]) \/ l*x4[i]) * 100 # x4's approximation error\n    itr.append(i)\n\n    epa1_l.append(epa1)\n    epa2_l.append(epa2)\n    epa3_l.append(epa3)\n    epa4_l.append(epa4)\n    if eps > epa1 != 0.0 and eps > epa2 != 0.0 and eps > epa3 != 0.0 and eps > epa4 != 0.0: # Break to Iteration when approximation \n        #errors of unknowns smaller than tolerance error\n        break","637962ae":"table = pd.DataFrame({'Iteration': itr[9:], 'x1': x1[9:], 'x2': x2[10:], 'x3': x3[10:], 'x4': x4[10:] }) # Crate table with iteration number and unknowns([9:] and [10:] used for writing the second iteration)\nprint(table)","108aee46":"**Conclusion**\n\nThe Newton-Raphson method's error value is acceptable for scientific calculation but same situations, it can performs poorly -specially multiple roots function- \n","dc358f29":"**Conclusion**\nThe error is acceptable  for scientific calculation. There is problem which I can not manage over it : When I use the 4x4 matrix, percent error of unknows give do not acceptable results. Therefore I used Gauss-Siedel method for examination the 4x4 matrix's unknowns. ","121e14d2":"**Conculusion**\n\nThe error of False-position method is very tiny but in scientific calculation, this kind of error can not negligible. Moreover, there is a pitfalls of the false-position method, it can give performs poorly for some functions. \n\n","5ae2689d":"**Conclusion**\n\nThe Graphical method is not precise to decet equation of root but it give to quick and practical guess.  ","fd4c09af":"**The False-Position Method**\n\nIn this method , $f(x_{l})$ and $f(x_{u})$ is joint by straight line. The intersection of this line with $x-axis$ represent an improved estimate of the root. From the trigonometry in graph, \n$\\frac{f(x_{l})}{x_{l}}=\\frac{f(x_{u})}{x_{u}-x_{l}}$. Then rearrange,\n\n$x_{r} = x_{u}-\\frac{f(x_{u})(x_{l}-x_{r})}{f(x_{l})-f(x_{u})}(6)$.\n\nThis is the false-position formula. The value of $x_{r}$ computed with Eq.6 then replaces whichever of two initial guesses, x_{l} or x_{u}, yields a function value with same sign as $f(x_{r})$.\n\n","1e5c9030":"**Conclusion**\n\nBisection Method result has same precision with Graphical method. However, Graphical methond include human factor(choosing the value from graph with using eyes) and this situation can create mistake for choosing a suitable root.Bisection method use only computer algorithm to obtain more reliable result. ","e7f60792":"**Truncation Erros and Taylor Series**\n\nTruncation errors are those that result from using a approximation in place of an exact matematical produne. For example, derivative of veclocity of a object by finite-divided-difference equation of form\n\n$\\frac{dv}{dt}\\cong \\frac{\\Delta v}{\\delta t}= \\frac{\\nu(t_{j+1})-\\nu(t_{j})}{t_{j+1}-t_{j}}(1)$ .\n\nNow turn to mathematical formulation that is used widely in numerical methods to express functions in an approximate fashion- the Taylor series.\n\n\n","20ee1d38":"**Newton-Raphson Method**\n\nIn the previous methods, the root is located within an inverval prescribed by a lower and an upper bound. In constrast, the Newton-Raphson method is based on formulas that require only a single starting value of $x$.\nIf the initial guess at the root is $x_{i}$ a tangent can be extended from the point $[x_{i},f(x_{i})]$. The point where this tangent crosses the $x$ axis usually represents an improved estimate of the root.\n\nConsider a Taylor series expansion around the estimate $x_{i}$ : \n\n$f(x_{i+1}=f(x_{i})+f'(x_{i})(x_{i+1}-x_{i})+R_{1} (1)$\n\nTruncating the series after the first\nderivative term\n\n$f(x_{i+1}=f(x_{i})+f'(x_{i})(x_{i+1}-x_{i}) (2).$\n\nThe new estimate $x_{i+1}$ is located at point $f(x_{i+1})=0$ then,\n\n$0=f(x_{i})+f'(x_{i})(x_{i+1}-x_{i}) (3)$ or,\n\n$x_{i+1}=x_{i}-\\frac{f(x_{i})}{f'(x_{i})} (4)$\n\nEq.4 is called Newton-Raphson Formula.\n","974fe7d1":"**Conclusion**\n\nThe third order term approximation enought to give value of function which is close to true value came from analytic solution. When torrelance error is decreased(from $0.005$), iteration can be continue and it can be more precise value of function but this process is not necessary because computational time cost( True value : $0.9867551618071957$).","8dadd025":"**Gauss-Seidel Method**\n\nIterative or approximate methods provide an alternative to the elimination methods. Consider we will show this 3 by 3 system written in matrix form :\n\n$[A]{X}={B}$ $(1)$\n\n$a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}=b_{1}$\n\n$a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}=b_{2}$  $(2)$\n\n$a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}=b_{3}$\n\nThe unknows can be arrange :\n\n$x_{1}= \\frac{b_{1}-a_{12}x_{2}-a_{13}x_{3}}{a_{11}}$   ;  $x_{2}= \\frac{b_{2}-a_{21}x_{1}-a_{23}x_{3}}{a_{22}}$   ;  $x_{3}= \\frac{b_{3}-a_{31}x_{1}-a_{32}x_{2}}{a_{33}}$  $(3)$\n\nThen start with initial guess : $x_{1}^{0}=x_{2}^{0}=x_{3}^{0}=0$\n\nThe first iteration : $x_{1}^{1}= \\frac{b_{1}}{a_{11}}$   ;   $x_{2}= \\frac{b_{2}-a_{21}x_{1}^{1}}{a_{22}}$   ;   $x_{3}= \\frac{b_{3}-a_{31}x_{1}^{1}-a_{32}x_{2}^{1}}{a_{33}}$  $(4)$\n\nAnd this process will continue until tolerance error's value bigger than approximation error's value.\n\n*  Improvement of Convergence by Relaxation\n\nAfter each new value of $x$ is computed that value is modified by weighted average of the results of the previous and present iterations :\n\n$x_{i}^{new}=\\lambda x_{i}^{new} + (1-\\lambda)x_{i}^{old}$ $(5)$\n\nif $0<\\lambda<1$ under relaxation designed to make a nonconvergent system converge.\n\nif $1<\\lambda<2$ over relaxation designed accelerate the convergence system of an already convergent system.\n","c5706352":"**Abstract**\n\nIn this kernel I will cover numerical method with using Numerical Methods for Engineers by chapra. We will cover each topic first theoretically and after that we will examine algorithm of these topics in python.\n\n<font color='Black'>\n<br>Content:\n* [Truncation Erros and Taylor Series](#1)\n* [Graphical Method](#2)\n* [Bisection Method](#3)\n* [The False Position Method](#4)    \n* [Newton Raphson Method](#5)\n* [Navie Gauss Elemination](#6)\n* [Gauss-Sidel Method](#7)\n    \n<font color='Black'>\n<br>Topic will be cover later\n* Optimization\n* Curve Fitting\n* Numerical Differentiation And Integration\n* Ordinary Differential Equations    ","7aec7865":"**Naive Gauss Elimination**\n\nGauss elimination include foward elimination and back substitution to solve linear equation.\n\n1. The equations were manipulated to eliminate one of the unknowns from the equations.\nThe result of this elimination step was that we had one equation with one unknown.\n2. Consequently, this equation could be solved directly and the result back-substituted\ninto one of the original equations to solve for the remaining unknown.\n\nThis approach is designed to solve a general set of n equation:\n\n$a_{11}x_{1}+a_{12}x_{2}+.......+a_{1n}x_{n}=b_{1}$\n\n$a_{21}x_{1}+a_{22}x_{2}+.......+a_{2n}x_{n}=b_{2}  (1)$\n                        \n$a_{n1}x_{1}+a_{n2}x_{2}+.......+a_{nn}x_{n}=b_{n}$\n\n* Forward Elimination of Unknown\nThe fist phase of designed to reduce the system to upper triangular system. The initial step will be to eliminate the fist unknown, $x_{1}$ for the second through the nth equation. To do this multiply the first equation by $a_{21}\/a_{11}$ then substract on the second equation.\n\n$(a_{22}-\\frac{a_{21}}{a_{11}})x_{2}+(a_{23}-\\frac{a_{21}}{a_{11}}a_{13})x_{3}+.......+(a_{2n}-\\frac{a_{21}}{a_{11}}a_{1n})x_{n}=b_{2} (2)$ \n\nor Eq.2 can be written more regularly\n\n$a'_{22}x_{2}+a'_{23}x_{2}+.......+a'_{2n}x_{n}=b'_{2} (3)$\n\nwhere the prime indicates that the elements have been changed from their orginal values. These process repeat until to reach nth equation in the system. Then the procedure is continued using remaining pivot equation to obtain the upper triangle form:\n\n$a_{11}x_{1}+a_{12}x_{2}+.......+a_{1n}x_{n}=b_{1}$\n\n$             a'_{22}x_{2}+.......+a'_{2n}x_{n}=b'_{2}  (4)$\n                        \n$                                  a_{nn}^{n-1}x_{n}=b_{n}^{n-1}$\n","d96bdba1":"**Algorithm Process for Taylor Series Approximation**","f701ab0f":"**Graphical Method**\n\nA simple method for obtaning an estimate of the root equation $f(x)=0$ to make plot of the function and observe where it crosses the $x$- axis.\n\nIn general same sign lead to no roots or an even numbers of roots oppesite sign lead to odd numbers of roots.\n\nGraphical techniques are of limited practical value because they are not precise. However,\ngraphical methods can be utilized to obtain rough estimates of roots. These estimates\ncan be employed as starting guesses for numerical methods","7473b408":"**The Bisection Method**\n\nIn general, if f(x) is real and continous interval from $x_{l}$ (lower value in x-axis) to $x_{u}$ (upper value in x-axis) and $f(x_{l})$ $f(x_{u})$ have opposite signs, $f(x_{l})f(x_{u})<0$ then, there is a at least one real root between $x_{l}$ and $x_{u}$.","70bc2029":"**The Taylor Series**\n\nTaylor theorem is of great value in the study of numerical methods. It provides a means to predict function at one point in terms of the function value and its derivatives. In matematical formulation, \n\n$f(x)=f(a)+f'(a)(x-a)+\\frac{f''(a)(x-a)^{2}}{2!}+.........+\\frac{f^{n}(a)(x-a)^{n}}{n!}+R_{n} (1)$\n\nWhere $R_{n}$ is called remainder or truncation error and accounts for all terms from $n+1$ to infinity\n\n$R_{n}=\\int_{a}^{x}\\frac{(x-t)^{N}}{n!}f^{n+1}(t)dt (2)$ \nand truncation error can also have second represantation \n\n$R_{n}=\\frac{f^{n+1}(\\zeta)}{(n+1!)}(x-a)^{n+1} (3)$ \nwhere $\\zeta$ is a value that lies intarvel (a,x)\n\nReturn to Eq.1 and applied Taylor expansion. Then, term by term examination of Taylor expansion give to perfect estimination about the function's value at given a point.\n\n$f(x_{i+1})\\cong f(x_{i})   (4)$ \n\nEq.4 is a zero order approximation. For better estiminate, term order can be increase. This condition depend on the function which is examine.","07801dc4":"**Conclusion**\n\nthe Gauss-Seidel technique's error value is acceptable for scientific calculation and also with $\\lambda$ value, iteration number can be decrease. "}}