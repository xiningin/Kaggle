{"cell_type":{"c537f1ce":"code","3712e54c":"code","2f421987":"code","f2f14717":"code","3357119b":"code","d76cfc45":"code","6b514b2d":"code","b1248991":"code","a83cfe32":"code","4b40a90f":"code","6d25b28f":"code","a1c8fb9e":"code","1dce5e1e":"code","9de2cb23":"code","cb2ade95":"code","f2c6d621":"code","a27ed4ce":"code","86b1e236":"code","453f0e93":"code","56c1e3a9":"code","38e50e87":"code","d41555f9":"code","63cb4ade":"code","f7ab1559":"code","652c3fa0":"code","e6c31313":"code","1eb36b87":"code","83543bcb":"code","4574da88":"code","3f432a42":"code","49d91e64":"code","a0f3ee82":"code","9a12b045":"code","b5bd83dc":"code","97793f4a":"code","994b4c73":"code","27f862ed":"code","9c56f3ec":"code","5d561e6b":"code","0ef48fba":"code","ee28c04f":"code","0be75968":"code","6879c934":"code","d8916eb4":"code","ed1edbc3":"code","4ef4fa64":"code","37465bdc":"code","8e74c30c":"code","dd257a64":"code","fa17ab97":"code","a42c712d":"code","599475c8":"code","8d3c4019":"code","488dbcb0":"code","f586026e":"code","a5257fc9":"code","17cb1717":"code","1d4944ed":"code","e54afcd1":"code","b5e90661":"code","6b8866a8":"code","124af12a":"code","d19085c5":"code","5e9d84d3":"code","8dc2c622":"code","aa3a7a94":"code","c8906270":"code","c14ba9de":"code","6b4a47cd":"code","d34bdb5b":"code","2f96e57c":"code","dd973a7f":"code","dad1279d":"code","2f954ffb":"code","94cbac5b":"code","c7a9e205":"code","5d7e7af1":"code","5355acac":"code","9b8f7ccb":"code","b649b0b2":"code","d52ce59c":"code","909f01ef":"code","b62d9284":"code","bc8c3ce7":"code","5fae9d12":"code","bfa1ca6a":"code","158a55fd":"code","55518f5b":"code","bae9ea01":"code","4f09075d":"code","959d57dd":"code","cb7e1406":"code","0498eda9":"code","c8810290":"code","8e312cb8":"code","33066a85":"code","fb95bef0":"code","5ad7c465":"code","6abd65eb":"code","e31309a9":"code","57a907fd":"code","3e639691":"code","19ff99d2":"code","8e74854b":"code","1ceed456":"code","afc08d36":"code","e0f6e481":"code","dbdd57a0":"code","7180c84a":"code","e0d71b19":"code","fdba387b":"code","997a6824":"code","8467e842":"code","741c48b3":"code","55f48d81":"code","1f833f2c":"markdown","7ea11a83":"markdown","62cb8423":"markdown","b535df93":"markdown","56a3a153":"markdown","0631339b":"markdown","dd4d1781":"markdown","93cb3cf2":"markdown","8c361ca8":"markdown","fea4be4e":"markdown","eb3fb2c3":"markdown","9d597458":"markdown","1e4c217a":"markdown","04849001":"markdown","5c2e0a98":"markdown","5065105e":"markdown","97518b96":"markdown","62082939":"markdown","f8154132":"markdown","fc5d4b37":"markdown","3ac1ce27":"markdown","efcf8009":"markdown","ff641831":"markdown","bd7de738":"markdown","f36c34fd":"markdown","4c7120ee":"markdown","2c7d2203":"markdown","69d89bd8":"markdown","efcb21d7":"markdown","867add9c":"markdown","1d6d9519":"markdown"},"source":{"c537f1ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3712e54c":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","2f421987":"df=pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='latin',header=None)","f2f14717":"df.head()","3357119b":"df=df[[5,0]]\ndf.columns=['tweets','sentiments']\ndf.head()","d76cfc45":"df['sentiments'].value_counts()","6b514b2d":"sent_map={0:'negative',4:'positive'}","b1248991":"df['word_counts']=df['tweets'].apply(lambda x:len(str(x).split()))","a83cfe32":"df.head()","4b40a90f":"df['char_counts']=df['tweets'].apply(lambda x:len(x))","6d25b28f":"df.head()","a1c8fb9e":"def get_avg_word_len(x):\n    words=x.split()\n    word_len=0\n    for word in words:\n        word_len+=len(word)\n    return word_len\/len(words)","1dce5e1e":"df['avg_word_len']=df['tweets'].apply(lambda x:get_avg_word_len(x))","9de2cb23":"df.head()","cb2ade95":"def get_stop_word_count(x):\n    words=x.split()\n    count=0\n    for word in words:\n        if word in STOP_WORDS:\n            count+=1\n    return count","f2c6d621":"df['stop_word_count']=df['tweets'].apply(lambda x:get_stop_word_count(x))","a27ed4ce":"df.head()","86b1e236":"df['hashtag_count']=df['tweets'].apply(lambda x:len([t for t in x.split() if t.startswith('#')]))\ndf['mention_count']=df['tweets'].apply(lambda x:len([t for t in x.split() if t.startswith('@')]))","453f0e93":"df.head()","56c1e3a9":"df['digit_count']=df['tweets'].apply(lambda x:len([t for t in x.split() if t.isdigit()]))\ndf['upper_count']=df['tweets'].apply(lambda x:len([t for t in x.split() if t.isupper() and len(t)>3]))\n","38e50e87":"df.head()","d41555f9":"df.loc[45]['tweets']","63cb4ade":"df['tweets']=df['tweets'].apply(lambda x:x.lower())","f7ab1559":"contractions = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"i'd\": \"I had \/ I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall \/ I will\",\n\"i'll've\": \"I shall have \/ I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","652c3fa0":"def cont_to_exp(x):\n    if type(x)==str:\n        for key in contractions:\n            value=contractions[key]\n            x=x.replace(key,value)\n    return x","e6c31313":"df['tweets']=df['tweets'].apply(lambda x:cont_to_exp(x))","1eb36b87":"df.head()","83543bcb":"import re","4574da88":"x='hi mail is abc@gmail.com and second is xyz@gmail.com'","3f432a42":"re.findall(r'([A-Za-z0-9._-]+@[A-Za-z0-9._-]+\\.[A-Za-z0-9_-]+)',x)","49d91e64":"def get_email_count(x):\n    return len(re.findall(r'([A-Za-z0-9._-]+@[A-Za-z0-9._-]+\\.[A-Za-z0-9_-]+)',x))","a0f3ee82":"df['email_count']=df['tweets'].apply(lambda x:get_email_count(x))","9a12b045":"df.head()","b5bd83dc":"df[df['email_count']>0]","97793f4a":"df['tweets']=df['tweets'].apply(lambda x:re.sub(r'([A-Za-z0-9._-]+@[A-Za-z0-9._-]+\\.[A-Za-z0-9_-]+)','',x))","994b4c73":"x='url is https:\/\/abc.com\/xyz thank you'","27f862ed":"re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?',x)","9c56f3ec":"re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?','',x)","5d561e6b":"df['tweets']=df['tweets'].apply(lambda x:re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?','',x))","0ef48fba":"df['tweets']=df['tweets'].apply(lambda x:re.sub('RT',\"\",x))","ee28c04f":"df.head()","0be75968":"df['tweets']=df['tweets'].apply(lambda x:re.sub('[^A-Z a-z 0-9-]+','',x))","6879c934":"df.head()","d8916eb4":"df.loc[35]['tweets']","ed1edbc3":"x='hello   xyz   '","4ef4fa64":"x=\" \".join(x.split())","37465bdc":"df['tweets']=df['tweets'].apply(lambda x:\" \".join(x.split()))","8e74c30c":"from bs4 import BeautifulSoup","dd257a64":"x='<html><h1>hello world<h1>'","fa17ab97":"BeautifulSoup(x,'lxml').get_text()","a42c712d":"#df['tweets']=df['tweets'].apply(lambda x:BeautifulSoup(x,'lxml').get_text())","599475c8":"df.head()","8d3c4019":"import unicodedata","488dbcb0":"x='no\u00e7\u00e3o hello no\u00e7\u00e3o'","f586026e":"def remove_accented_chars(x):\n    x=unicodedata.normalize('NFKD',x).encode('ascii','ignore').decode('ascii','ignore')\n    return x","a5257fc9":"remove_accented_chars(x)","17cb1717":"df['tweets']=df['tweets'].apply(lambda x:remove_accented_chars(x))","1d4944ed":"import spacy","e54afcd1":"x='this is stop word'","b5e90661":"\" \".join([t for t in x.split() if t not in STOP_WORDS])","6b8866a8":"df['tweets']=df['tweets'].apply(lambda x:\" \".join([t for t in x.split() if t not in STOP_WORDS]))","124af12a":"text=\" \".join(df['tweets'])\ntext=text.split()\nfreq=pd.Series(text).value_counts()\nf20=freq[:20]\nr20=freq[-20:]","d19085c5":"f20","5e9d84d3":"r20","8dc2c622":"df['tweets']=df['tweets'].apply(lambda x:\" \".join([t for t in x.split() if t not in f20]))","aa3a7a94":"df['tweets']=df['tweets'].apply(lambda x:\" \".join([t for t in x.split() if t not in r20]))","c8906270":"df.head()","c14ba9de":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline","6b4a47cd":"x=\" \".join(text[:20000])\nwc=WordCloud(width=800,height=400).generate(x)","d34bdb5b":"plt.imshow(wc)\nplt.axis('off')\nplt.show()","2f96e57c":"nlp=spacy.load('en_core_web_lg')","dd973a7f":"x='hello world I am shubham dog cat Ayush'\ndoc=nlp(x)\n","dad1279d":"for token in doc:\n    print(token.text,token.has_vector)","2f954ffb":"token.vector.shape","94cbac5b":"x='one two three dog cat lion'\ndoc=nlp(x)","c7a9e205":"for token1 in doc:\n    for token2 in doc:\n        print(token1.text,token2.text,token1.similarity(token2))\n    print()","5d7e7af1":"df.shape","5355acac":"df.columns","9b8f7ccb":"df0=df[df['sentiments']==0].sample(3000)\ndf4=df[df['sentiments']==4].sample(3000)","b649b0b2":"df_red=df0.append(df4)","d52ce59c":"df_red.shape","909f01ef":"df_red.head()","b62d9284":"df_red_feat=df_red.drop(labels=['tweets','sentiments'],axis=1)\ndf_red_feat.head()","bc8c3ce7":"df_red_y=df_red['sentiments']\ndf_red_y.head()","5fae9d12":"from sklearn.feature_extraction.text import CountVectorizer","bfa1ca6a":"cv=CountVectorizer()\ntext_counts=cv.fit_transform(df_red['tweets'])","158a55fd":"text_counts.toarray().shape","55518f5b":"df_red_bow=pd.DataFrame(text_counts.toarray(),columns=cv.get_feature_names())","bae9ea01":"df_red_bow.head()","4f09075d":"from sklearn.linear_model import SGDClassifier , LogisticRegression, LogisticRegressionCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler","959d57dd":"sgd=SGDClassifier(n_jobs=-1,random_state=2,max_iter=200)\nlr=LogisticRegression(random_state=2,max_iter=200)\nlrcv=LogisticRegressionCV(cv=2,random_state=2,max_iter=1000)\nsvm=LinearSVC(random_state=2,max_iter=200)\nrfc=RandomForestClassifier(n_jobs=-1,random_state=2,n_estimators=200)","cb7e1406":"clf={'SGD':sgd ,'LR':lr , 'LRCV':lrcv,'SVM':svm,'RFC':rfc}\nclf.keys()","0498eda9":"def classify(X,y):\n    scaler=MinMaxScaler(feature_range=(0,1))\n    X=scaler.fit_transform(X)\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)\n    \n    for key in clf.keys():\n        model=clf[key]\n        model.fit(X_train,y_train)\n        y_pred=model.predict(X_test)\n        acc=accuracy_score(y_test,y_pred)\n        print(key,\"---->\",acc)","c8810290":"%%time\nclassify(df_red_bow,df_red_y)","8e312cb8":"%%time\nclassify(df_red_feat,df_red_y)","33066a85":"def get_vector(x):\n    doc=nlp(x)\n    return doc.vector.reshape(1,-1)","fb95bef0":"%%time\ndf_red['vector']=df_red['tweets'].apply(lambda x:get_vector(x))","5ad7c465":"X=np.concatenate(df_red['vector'].to_numpy(),axis=0)","6abd65eb":"X.shape","e31309a9":"classify(pd.DataFrame(X),df_red_y)","57a907fd":"df.head()","3e639691":"df.shape","19ff99d2":"dfr=df.sample(20000)","8e74854b":"%%time\ndfr['vector']=dfr['tweets'].apply(lambda x:get_vector(x))","1ceed456":"dfr.shape","afc08d36":"X=np.concatenate(dfr['vector'].to_numpy(),axis=0)\ny=(dfr['sentiments']>1).astype(int)","e0f6e481":"y.head()","dbdd57a0":"%%time\nclassify(pd.DataFrame(X),y)","7180c84a":"from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,LSTM\nfrom tensorflow.keras.models import Sequential","e0d71b19":"model=Sequential([\n    Dense(128,activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(64,activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(2,activation='sigmoid')\n])","fdba387b":"import tensorflow as tf\ny_oh=tf.keras.utils.to_categorical(y,num_classes=2)","997a6824":"X_train,X_test,y_train,y_test=train_test_split(X,y_oh,random_state=2,test_size=0.2)","8467e842":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory=model.fit(X_train,y_train,epochs=10,batch_size=32,validation_data=[X_test,y_test])","741c48b3":"y_test=np.array(y_test)\ny_test.reshape(10000,1)","55f48d81":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ny_pred=np.argmax(y_pred,axis=1)\ny_pred\ncm=confusion_matrix(y_test,y_pred)\ncm","1f833f2c":"# remove stop words","7ea11a83":"## feature extraction of BoW","62cb8423":"# remove multiple spaces","b535df93":"# remove special characters and punctuations","56a3a153":"# Word Count","0631339b":"# count and remove emails using regular expression","dd4d1781":"# remove RT retweet","93cb3cf2":"#stop word count","8c361ca8":"## contraction to expansion","fea4be4e":"converting word to vector using Word2Vec","eb3fb2c3":"# using whole dataset","9d597458":"# remove HTML tags","1e4c217a":"## using BoW","04849001":"# #hashtag and @mentions","5c2e0a98":"# remove urls","5065105e":"## with manual features","97518b96":"# with word embeddings","62082939":"# using neural network","f8154132":"# remove accented characters","fc5d4b37":"lower case","3ac1ce27":"# character counts","efcf8009":"#average word length","ff641831":"# remove common and rare words","bd7de738":"text counts is our X input","f36c34fd":"# Models","4c7120ee":"# using spacy ","2c7d2203":"# digit count and upper case count","69d89bd8":"**feature extraction**","efcb21d7":"# preprocessing and cleaning","867add9c":"# word embeddings","1d6d9519":"# word cloud visualization"}}