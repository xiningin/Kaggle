{"cell_type":{"028c032d":"code","f554c98e":"code","08e918dc":"code","ac878129":"code","f7fd6884":"code","d1e5bf83":"code","ae2f049a":"code","0b6fecaf":"code","3383b835":"code","7e87f8cf":"code","67895d57":"code","c02f1c26":"code","b2b32354":"code","d622fb42":"code","e88ea5cc":"code","2cc424d3":"code","25328ee6":"code","0726cb07":"code","6be7538a":"code","0c3a0f75":"code","42325205":"code","a1ec480e":"code","67b98529":"code","af9a94bf":"code","5d9c5eaa":"code","0e5832b6":"code","d273fb12":"code","a290f0f1":"code","8f91fe82":"code","4c9b20ae":"code","147ee8cb":"code","c6398c24":"code","a8e058da":"code","9bcf809f":"code","17adc42d":"code","e85f68b5":"code","13087943":"code","d0dd5b28":"code","f237d475":"code","5ee62504":"code","f8c086e3":"code","aaa37b3d":"code","4883139e":"code","a170089b":"code","31ff6008":"code","dc38b27b":"code","98d16d57":"code","34576323":"code","ad47b508":"code","2f4bdedf":"code","97e2dd07":"markdown","c36239c1":"markdown","81abf79d":"markdown","7242c320":"markdown","b8e2de46":"markdown","2af48070":"markdown","32e10c3b":"markdown","a1a8c38d":"markdown","8d843767":"markdown","8fb32abc":"markdown","e0cbe37c":"markdown","73487dc8":"markdown","1d124492":"markdown","f70ec5bc":"markdown","fd24035e":"markdown","ccffd2f0":"markdown","5317e635":"markdown","76a8f3c4":"markdown","c21e0733":"markdown","ba79cf4e":"markdown","0a08a053":"markdown","c8a6d93f":"markdown","ad7dbed1":"markdown","17467772":"markdown","814d5369":"markdown","d6f9baa8":"markdown"},"source":{"028c032d":"import time\nstart = time.time()\n\n# Import modules\nimport pandas as pd\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport re\nimport pickle\n\nend=time.time()\nprint(end-start)","f554c98e":"start = time.time()\n\n# Read in the data\ndf = pd.read_csv('..\/input\/7282_1.csv')\ndf2= pd.read_csv('..\/input\/Datafiniti_Hotel_Reviews.csv')\n\n# Important columns. City, Country, and Province columns are excluded because they are not reliable\nimp_col_list = ['address', 'name', 'reviews.date', 'reviews.text','reviews.title']\ndf = df.loc[:,imp_col_list]\ndf2 = df2.loc[:,imp_col_list]\ndf= df.append(df2)\n\nend=time.time()\nprint(end-start)","08e918dc":"start = time.time()\n\n# Prep text, add some columns and fillna, and rename columns\ndf['reviews.text'] = df['reviews.text'].str.lower()\ndf['reviews.text'] = df['reviews.text'].replace(to_replace='[^A-Za-z0-9]+', regex=True, value=' ')\ndf['reviews.text'] = df['reviews.text'].fillna('')\ndf['review_date'] = pd.to_datetime(df['reviews.date']).dt.date\ndf['review_month'] = pd.to_datetime(df['reviews.date']).dt.month\ndf['words_in_review'] = [len(i.split()) for i in df['reviews.text']]\nseason_dict = {1:'Winter',2:'Winter',3:'Spring',4:'Spring',5:'Spring',6:'Summer',\n               7:'Summer',8:'Summer',9:'Fall',10:'Fall',11:'Fall',12:'Winter'}\ndf['review_season'] = df['review_month'].map(season_dict).fillna('Summer')\ndf.rename(columns={'address': 'hotel_address', 'city': 'hotel_city','country':'hotel_country', \n                   'name':'hotel_name'},inplace=True)\n\nend=time.time()\nprint(end-start)","ac878129":"df.head()","f7fd6884":"df.describe()","d1e5bf83":"# Size of dataframe\ndf.memory_usage().sum()\/1024\/1024","ae2f049a":"df.shape, df.isnull().sum()","0b6fecaf":"# Import and initiate a vectorizer.\n\n\n# The max features is how many words for which I want to create columns. Note I also use the 'stop_words' \n# parameter that removes English language stop words. There are issues with this method, see documentation.\n# I'll leave ngram_range at (1, 1) for now but may increase this in the final model.\nvectorizer = CountVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 1))","3383b835":"# Vectorize the reviews to transform sentences into columns.\nX = vectorizer.fit_transform(df['reviews.text'])\n\n# And then put all of that in a new dataframe.\nbag_of_words = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())","7e87f8cf":"# Join our bag of words back to our initial hotel data.\nfull_df = df.join(bag_of_words)\nfull_df.head()","67895d57":"full_df.memory_usage().sum()\/1024\/1024","c02f1c26":"# X is the list of features. In this case, it's the bag of words. \nX = bag_of_words\n\n# y the target\ny = df['hotel_name']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=30)","b2b32354":"# Import the random forest model classifier.\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(min_samples_leaf=10, random_state=8675309)","d622fb42":"import time\nstart = time.time()\n# Fit the model to the data.\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy score: \",round((accuracy_score(y_test, y_pred)*100),2), \"%\")\n\nend=time.time()\nprint(end-start)","e88ea5cc":"test_review = 'I loved the beach, the nearby bars, the live music, and the walkable neighborhood. The weather was great and it was sunny.'","2cc424d3":"test_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\n\n# Convert your test review into a vector.\nX_test = vectorizer.transform(test_review).toarray()\n\n# Make a prediction of which hotel your review would be a review:\nprediction = clf.predict(X_test)[0]\n\n# Return the essential information about your match. Note the head() option. You can set it to 1 to only\n# get back the hotel information, but this also includes the review date information. This is because\n# it tells you what time of year this place is most often visited. If you search this result, you'll see\n# \"Inn & Suites 2540 S Mccall Rd\" is a hotel in Florida. It's most often visited in Winter, which makes \n# sense because people are likely to visit Florida when the weather is bad elsewhere.\ndf[df['hotel_name'] == prediction][['hotel_name', 'hotel_address', 'review_date','review_month','review_season']].head(15)","25328ee6":"start = time.time()\n\n# Initialize a vectorizer\nvectorizer = TfidfVectorizer(max_features=None, stop_words='english', ngram_range=(1, 3))\n\n# Vectorize the reviews to transform sentences into columns\nX = vectorizer.fit_transform(df['reviews.text'])\n\nend=time.time()\nprint(end-start, X.shape)","0726cb07":"# A memory error happens when I try to expand the sparse matrix into a dense array.\nbag_of_words = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())","6be7538a":"start = time.time()\n\n# Create a vocab and bag of words with the most popular words\nkeep_cols = X.mean(axis=0)*100\nkeep_cols = pd.DataFrame(keep_cols, columns=vectorizer.get_feature_names())\n\nend=time.time()\nprint(end-start)","0c3a0f75":"start = time.time()\n\nkeep_cols = keep_cols.transpose().reset_index().rename(columns={'index':'feature',0:'freq'})\nkeep_cols = keep_cols.reset_index().sort_values('freq')\n\nend=time.time()\nprint(end-start)","42325205":"# Note I'm increasing the threshold because I added the 10,000 updated reviews\nthreshold = 0.00450472037\nkeep_cols = keep_cols.loc[keep_cols['freq'] > threshold]\nX = sparse.csc_matrix(X)\nkeep_list = keep_cols['index'].tolist()\nvocab = keep_cols['feature'].tolist()\nX = X[:,keep_list]\nbag_of_words = pd.DataFrame(X.toarray(), columns=(vocab))\nbag_of_words.shape","a1ec480e":"start = time.time()\ndf_s = df.reset_index(drop=True)\ndf_s = df_s.reset_index(drop=False)\ndf_s['review_season'].value_counts()","67b98529":"df_s1 = df_s.loc[df_s['review_season'] == 'Spring']\ndf_s2 = df_s.loc[df_s['review_season'] == 'Summer']\ndf_s3 = df_s.loc[df_s['review_season'] == 'Fall']\ndf_s4 = df_s.loc[df_s['review_season'] == 'Winter']\ndf_s1.shape, df_s2.shape, df_s3.shape, df_s4.shape","af9a94bf":"l_s1 = df_s1['index'].tolist()\nl_s2 = df_s2['index'].tolist()\nl_s3 = df_s3['index'].tolist()\nl_s4 = df_s4['index'].tolist()\nlen(l_s1),len(l_s2),len(l_s3),len(l_s4), len(l_s1)+len(l_s2)+len(l_s3)+len(l_s4)","5d9c5eaa":"start = time.time()\n\n# Assign X and y\nX_s1 = bag_of_words[bag_of_words.index.isin(l_s1)]\ny_s1 = df_s1['hotel_name']\nX_s2 = bag_of_words[bag_of_words.index.isin(l_s2)]\ny_s2 = df_s2['hotel_name']\nX_s3 = bag_of_words[bag_of_words.index.isin(l_s3)]\ny_s3 = df_s3['hotel_name']\nX_s4 = bag_of_words[bag_of_words.index.isin(l_s4)]\ny_s4 = df_s4['hotel_name']\n\n# Train test split X and y\nX_s1_train, X_s1_test, y_s1_train, y_s1_test = train_test_split(X_s1, y_s1, test_size=0.20, random_state=30)\nX_s2_train, X_s2_test, y_s2_train, y_s2_test = train_test_split(X_s2, y_s2, test_size=0.20, random_state=30)\nX_s3_train, X_s3_test, y_s3_train, y_s3_test = train_test_split(X_s3, y_s3, test_size=0.20, random_state=30)\nX_s4_train, X_s4_test, y_s4_train, y_s4_test = train_test_split(X_s4, y_s4, test_size=0.20, random_state=30)\n\n# Declare the classifiers\nclf_s1 = RandomForestClassifier(min_samples_leaf=3, random_state=8675309)\nclf_s2 = RandomForestClassifier(min_samples_leaf=3, random_state=8675309)\nclf_s3 = RandomForestClassifier(min_samples_leaf=3, random_state=8675309)\nclf_s4 = RandomForestClassifier(min_samples_leaf=3, random_state=8675309)\n\nend=time.time()\n# print(end-start)\nX_s1.shape, X_s2.shape, X_s3.shape, X_s4.shape, y_s1.shape, y_s2.shape, y_s3.shape, y_s4.shape","0e5832b6":"X_s1_train.shape, X_s2_train.shape, X_s3_train.shape, X_s4_train.shape, X_s1_test.shape, X_s2_test.shape, X_s3_test.shape, X_s4_test.shape","d273fb12":"y_s1_train.shape, y_s2_train.shape, y_s3_train.shape, y_s4_train.shape, y_s1_test.shape, y_s2_test.shape, y_s3_test.shape, y_s4_test.shape","a290f0f1":"start = time.time()\n\n# Fit the model to the data\nclf_s1.fit(X_s1_train,y_s1_train)\ny_s1_pred = clf_s1.predict(X_s1_test)\n\nprint(accuracy_score(y_s1_test, y_s1_pred))\n\nend=time.time()\nprint(end-start)","8f91fe82":"start = time.time()\n\nclf_s2.fit(X_s2_train,y_s2_train)\ny_s2_pred = clf_s2.predict(X_s2_test)\n\nprint(accuracy_score(y_s2_test, y_s2_pred))\n\nend=time.time()\nprint(end-start)","4c9b20ae":"start = time.time()\n\nclf_s3.fit(X_s3_train,y_s3_train)\ny_s3_pred = clf_s3.predict(X_s3_test)\n\nprint(accuracy_score(y_s3_test, y_s3_pred))\n\nend=time.time()\nprint(end-start)","147ee8cb":"start = time.time()\n\nclf_s4.fit(X_s4_train,y_s4_train)\ny_s4_pred = clf_s4.predict(X_s4_test)\n\nprint(accuracy_score(y_s4_test, y_s4_pred))\n\nend=time.time()\nprint(end-start)","c6398c24":"start = time.time()\n\n# Reinitialize and refit the vectorizer with the vocabulary\nvectorizer = TfidfVectorizer(max_features=None, vocabulary=vocab, stop_words='english', ngram_range=(1, 3))\nX = vectorizer.fit_transform(df['reviews.text'])\n\nend=time.time()\nprint(end-start)","a8e058da":"start = time.time()\n\n# Create a review to feed the model\ntest_review = 'I loved the beach, the nearby bars, the live music, and the walkable neighborhood#@!$?@#!. The weather was great and it was sunny.'\n\n# Test season has to match case perfectly - use dropdown from website\ntest_season = 'Fall'\n\n# Clean the text and convert your test review into a vector.\ntest_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\nX_test = vectorizer.transform(test_review).toarray()\n\nend=time.time()\nprint(end-start)","9bcf809f":"# Note how we've cleaned the test_review to be easier for the machine to read:\nprint(test_review)","17adc42d":"# Define a prediction function (note that the X_test global thing may cause an error if \n# # I did it wrong, remember this during troubleshooting)\ndef make_prediction(season):\n    global X_test\n    global prediction\n    if test_season == 'Spring':\n        prediction = clf_s1.predict(X_test)[0]\n    elif test_season == 'Summer':\n        prediction = clf_s2.predict(X_test)[0]\n    elif test_season == 'Fall':\n        prediction = clf_s3.predict(X_test)[0]\n    else:\n        prediction = clf_s4.predict(X_test)[0]\n    return df[df['hotel_name'] == prediction][['hotel_name', 'hotel_address']].head(1)","e85f68b5":"start = time.time()\n\nprint(make_prediction(test_season))\n\nend=time.time()\nprint(end-start)","13087943":"test_review = 'This was an amazing spot to go hiking. The crowd was young and the food was delicious.'\ntest_season = 'Fall'\n\n# Clean the text and convert your test review into a vector.\ntest_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\n\nX_test = vectorizer.transform(test_review).toarray()\nprint(make_prediction(test_season))","d0dd5b28":"test_review = 'I loved the fishing. It was a relaxing vacation and this hotel really lived up to its reputation.'\ntest_season = 'Summer'\n\n# Clean the text and convert your test review into a vector.\ntest_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\n\nX_test = vectorizer.transform(test_review).toarray()\nprint(make_prediction(test_season))","f237d475":"test_review = 'Fun for the whole family. The area had a lot of activities for children which adults could enjoy too.'\ntest_season = 'Fall'\n\n# Clean the text and convert your test review into a vector.\ntest_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\n\nX_test = vectorizer.transform(test_review).toarray()\nprint(make_prediction(test_season))","5ee62504":"test_review = 'The snow was incredible. Fresh powder, skiing, snowboarding, jacuzzis at night. This hotel was right by the ski lift which made for quick access to the mountain.'\ntest_season = 'Winter'\n\n# Clean the text and convert your test review into a vector.\ntest_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\n\nX_test = vectorizer.transform(test_review).toarray()\nprint(make_prediction(test_season))","f8c086e3":"test_review = 'I\\'m a big art fan. The number of museums, operas, and collections nearby made this visit a once-in-a-lifetime experience!'\ntest_season = 'Spring'\n\n# Clean the text and convert your test review into a vector.\ntest_review = test_review.lower()\ntest_review = re.sub('[^A-Za-z0-9]+', ' ', test_review)\ntest_review = [test_review]\n\nX_test = vectorizer.transform(test_review).toarray()\nprint(make_prediction(test_season))","aaa37b3d":"test_review = 'The snow was incredible. Fresh powder, skiing, snowboarding, jacuzzis at night. This hotel was right by the ski lift which made for quick access to the mountain.'\ntest_season = 'Winter'","4883139e":"def suggest_destination(review, season):\n    review = review.lower()\n    review = re.sub('[^A-Za-z0-9]+', ' ', review)\n    review = [review]\n    X_test = vectorizer.transform(review).toarray()\n    if season == 'Spring':\n        prediction = clf_s1.predict(X_test)[0]\n    elif season == 'Summer':\n        prediction = clf_s2.predict(X_test)[0]\n    elif season == 'Fall':\n        prediction = clf_s3.predict(X_test)[0]\n    else:\n        prediction = clf_s4.predict(X_test)[0]\n    df_answer = df[df['hotel_name'] == prediction][['hotel_name', 'hotel_address']].head(1)\n    df_answer = df_answer.reset_index(drop=True)\n    answer = df_answer['hotel_name'][0], df_answer['hotel_address'][0]\n    url_str = str(answer[0]).replace(\" \", \"%20\")+\"_\"+str(answer[1]).replace(\" \", \"%20\")\n    url = \"https:\/\/www.google.com\/search?q={}\".format(url_str)\n    return answer, url","a170089b":"answer, url = suggest_destination(test_review, test_season)\nprint(answer, test_season)","31ff6008":"print(url)","dc38b27b":"start = time.time()\n\n# Pickle out the trained models\npickle_out_s1 = open(\"clf_s1.pickle\",\"wb\")\npickle.dump(clf_s1, pickle_out_s1, protocol=0)\npickle_out_s1.close()\n\nend=time.time()\nprint(end-start)","98d16d57":"start = time.time()\n\n# Pickle out the trained models\npickle_out_s2 = open(\"clf_s2.pickle\",\"wb\")\npickle.dump(clf_s2, pickle_out_s2, protocol=0)\npickle_out_s2.close()\n\nend=time.time()\nprint(end-start)","34576323":"start = time.time()\n\n# Pickle out the trained models\npickle_out_s3 = open(\"clf_s3.pickle\",\"wb\")\npickle.dump(clf_s3, pickle_out_s3, protocol=0)\npickle_out_s3.close()\n\nend=time.time()\nprint(end-start)","ad47b508":"start = time.time()\n\n# Pickle out the trained models\npickle_out_s4 = open(\"clf_s4.pickle\",\"wb\")\npickle.dump(clf_s4, pickle_out_s4, protocol=0)\npickle_out_s4.close()\n\nend=time.time()\nprint(end-start)","2f4bdedf":"start = time.time()\n\n# Pickle out the fitted vectorizer\npickle_vec_out = open(\"vectorizer.pickle\",\"wb\")\npickle.dump(vectorizer, pickle_vec_out, protocol=0)\npickle_vec_out.close()\n\nend=time.time()\nprint(end-start)","97e2dd07":"We can see here that some features appear with a frequency greater than 1. No cause to panic - this happens because words can be used more than once in a review. If all of the reviews were the same three words - 'room, room room!' - there would be one feature with a frequency of 3.\n\nAnyway, now I have to drop the features which don't meet my threshold.","c36239c1":"Now let's see if I can have a google search for that","81abf79d":"The low accuracy is not surprising. We are trying to predict the *exact* hotel that a user wants to go to - life isn't that cut and clear. We could get a higher accuracy score if we changed the target from hotel to a group of attributes of that hotel - like if we replaced the hotel name with a code which represented 10 attributes of a hotel - and therefore reduced the prediction categories from ~2,500 (# of unique hotels) to 10 (Close to beach: True\/False, Good nightlife nearby: t\/f, Family friendly: t\/f, etc.)\n\nHowever, the effort required for a single person to do that is beyond my limit. If I worked for a company like Expedia, though, I'd suggest they allow the users to categorize hotels. It would be relatively simple: \n1. Interview subject matter experts to determine a reasonable (10-30 maybe) list of boolean attributes which describe all hotels \n2. Engage web developers to add a feature to review section of Expedia which allows users to select which of those attributes describe the hotel they visited\nAfter some time, you'd have reliable categorizations of hotels which you could predict instead of the hotels themselves. Further features could be added over time that incorporate budget, time of year to visit, place in world to visit, etc, which would allow better filtering of those hotels to a more precise list which would appeal to the user.\n\nSo, we'll leave the model as is for now even though the accuracy is low. Let's make a test review and see where it sends us!","7242c320":"# New predictions with the updated model\nI will do two things here.\n1. Reinitialize the vectorizer with the vocab we declared above (the top 9,997 features that we found)\n2. Take user input of season & review, to create output of prediction\n\n","b8e2de46":"Now I have the top ~10k features, and it didn't run into a memory error. Moving on.","2af48070":"Now let's create a formula which decides which classifier to use based on the season:","32e10c3b":"## Prepare the review text\nNow we want to perform some basic cleaning and character removal, and engineer a more clear date column, and month and season columns. Also create a count of words in each review for exploration.","a1a8c38d":"# Case description\nKnow what kind of vacation you want to experience, but not where to find it? Write a review of your dream vacation and use this code to find it!\n\nThe output of this notebook will take in written descriptions (in a \"review\" format) of dream vacations and return a recommended destination.\n\nTo do that I need three things:\n* A set of vacation reviews\n* A text-based predictive model for hotel matching\n* Dream vacation descriptions\n\nNOTE: Data courtesy of Datafiniti's Hotel Reviews on Kaggle. https:\/\/www.kaggle.com\/datafiniti\/hotel-reviews\n\nIdea courtesy of a free webinar from Thinkful.com. https:\/\/www.thinkful.com\/workshops\/city\/data-science-vacation\/\n\nSee my GitHub: https:\/\/github.com\/gmayock\/vacation_planner\nFor how I use it on my website: http:\/\/gmayock.com\/py\/vacation\/\n\n\n## Data description\nFor data I am using [a set from Datafiniti on Kaggle](https:\/\/www.kaggle.com\/datafiniti\/hotel-reviews\/data). The data has information about the hotel (location, name, etc), information about the reviewer, the review text, and the rating. The review data and description data is unstructured. \n\n## Approach\nI will use a 'bag of words' approach (aka vectorizing) to add structure to the data:\n* Create a 'bag' for each word\n* Count the number of times a word appears in a sentence\/text blog\n* Words are columns, rows are counts.\n\n### Data preprocessing\nIt's important for bag of words to preprocess data if we want words like 'This' to register the same as 'this'. Therefore we can make all words lowercase. \n\nThere are also extra words that don't really add value to the information - they don't tell us anything. Words like 'it', 'is', 'the' - words that are common in any corpus - these words are called 'stop words'. They are usually discarded.\n\nAfter doing this, I have vectors of the essentials for each sentence. Now I have something I can build a model on.\n\n## Predicting your dream destination\nI will use a 'Random Forest' model. A random forest is an ensemble of decision trees used to predict the most likely class of an outcome variable. The input information is the dream vacation description (aka the 'review'). The output is the hotel. \n\nEach decision tree creates a prediction based on the information in a number of features. If (feature value) =\/>\/<\/!=\/isin\/~isin (some value), then (target value) is (prediction). A random forest builts a lot of different decision trees and then lets each one 'vote'. Each decision tree covers a subset of all the features and the random forest weights the group output.\n\n# Building the model\nNow let's start building the model.\n\n## Import tools","8d843767":"# Model review\n\nThe model is strong in that it was easy to code and quick to run. \n\nIt's weak in that we don't have term frequency included (meaning \"beach\" showing up in a review in this model has the same importance if it's one of five words in a review or one of 1,000 words). This can be handled by including term frequency as a feature - (tf-idf).\n\nIt's also weak in that context is lost - \"I love the city and hate the beach\" will have the same results as \"I love the beach and hate the city\". This can be handled by including larger ngrams as features - such as an ngram_range=(1,2), which would include unigrams (n=1) and bigrams (n=2).\n\nFinally, it's weak in that the season the user can travel is not incorporated. If the user can only travel in January, there's no use sending them to a beach that's only good from June through September.\n\nSo that leaves us three things to do. \n1. Add TF-IDF\n2. Add ngram range (1,3)\n3. Build a separate model for each season.\nLet's do those things and see how they look!\n\n## Adding TF-IDF and ngram range (1,3)\nThere are a couple ways to approach this:\n1. Use sklearn.feature_extraction.text.[TfidfVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)\n2. Use sklearn.feature_extraction.text.[CountVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) followed by [TfidfTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html)\n\nIf I were not adding ngrams over 1, I'd just use TfidfTransformer overthe CountVectorizer results we already had (the dataframe called 'bag_of_words'). However, I'd have to revectorize the counts anyway to add the ngrams > 1, so I'll just use the TfidfVectorizer to get it done in fewer lines of code.","8fb32abc":"## Adding structure\nNow we want to create a vectorizer - something which looks at the text and fills out the bag of words data structure I talked about above.","e0cbe37c":"Now we submit the review:","73487dc8":"# Next steps\nThe next steps are to pickle the models in a way that makes it easy for them to be used online. I will probably try to build a single object that incorporates all the pieces to keep the number of pickle objects low. I can probably define a function which takes in the season and raw review text, cleans and vectorizes the review, and then makes the prediction based on the season and vectorized review. That single object should contain all the information.","1d124492":"Now I'll join the bag of words onto the original df.","f70ec5bc":"### How the selection of these 9,997 features differs from the 5,000 chosen before\nAs a side bar, the selection of these features is very different than the original 5,000. The original method just pulled the first 5,000 valid (non-stop-) words it encountered. The updated method looks not only at more features, but selects those features based on highest frequency. It's important to note that setting 'max_features=9997' above would not result in the same features as this method.\n\n## Building a classifier for each season\nNow I'll create classifiers for each season. \n### First I'll split the data by season","fd24035e":"Now run the code below. ","ccffd2f0":"### Making a dataframe to use to fit the model\nThere are 1.3 million features in the bag of words if I cap ngram_range at 3. This causes a memory error if I try to use toarray() on it:","5317e635":"### Now we'll train each classifier","76a8f3c4":"So we have our measurements of frequency\\*100, which makes it a percent (even though it just shows up as decimal notation, it's easier to read this as a percentage). We only want to have columns for words which has a frequency greater than 0.00450472037  - \n\nWe can see there are some non-English reviews, but for simplicity's sake I'll leave them in for now.","c21e0733":"## Creating the model\nNow that I've got structure added, I'll import, fit, and train the model.","ba79cf4e":"### Now I'll split the data into train\/test sets and declare the classifiers","0a08a053":"We'll set the max features to 5000 just to get a first model created. ","c8a6d93f":"## Import and join data","ad7dbed1":"# Using the model\nTo use the model, edit the text of the below review cell, on the second line of the code cell:","17467772":"# Et voil\u00e1*!*\n* There is your recommendation! Here are some other results:","814d5369":"At this point in a deployed version of the model, I'd ask the user for their dream review and the season they want to travel before cleaning the input and vectorizing it.","d6f9baa8":"### Reducing feature count using a frequency threshold\nThere *is* a way around this. I'm going to add a threshold - I will remove a word if the frequency is under the frequency of the top 1,500 words as determined by the 2006 TV test on [Wiktionary](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:Frequency_lists#English). There were 29,213,800 words in the corpus, and the 1,500th most frequent had 1316 occurrences, which is a frequency of 0.00450472037%. I'll start by keeping only the columns that beat that threshold. Please note that this threshold doesn't mean that only 1,316 words will be kept - it's just a way of finding an arbitrary threshold to reduce the feature count to the top X # of features."}}