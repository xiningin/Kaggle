{"cell_type":{"3a0ae439":"code","bf925bc8":"code","de20bd70":"code","11d405e4":"code","8a54b7e5":"code","91d06136":"code","59d0071f":"code","cc2a08a5":"code","857d45cb":"code","68fa3975":"code","779b1bb0":"code","318e055d":"code","e8b2b889":"code","06df19f5":"code","aae720c2":"code","5988a101":"code","8e985962":"code","6873880d":"code","f68926af":"code","8f90165d":"code","c4edc476":"code","d363e4e1":"code","47087be3":"code","196d952d":"code","f200a4f1":"code","dba36c77":"code","71499aae":"code","5f331da9":"markdown"},"source":{"3a0ae439":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport cv2\nimport re","bf925bc8":"wImage,hImage=512,512\nnClasses=20\n\nbatch_size= 4\nepochs=25","de20bd70":"def create_data(trainImage,trainAnnotation):\n  trainImagePath=[]\n  trainAnnotationPath=[]\n  classes=[]\n\n  for folder in os.listdir(trainImage):\n    classes.append(folder.split('-')[1])\n    for image in sorted(os.listdir(trainImage+'\/'+folder)):\n      trainImagePath.append(trainImage+'\/'+folder+'\/'+image)\n\n  for folder in os.listdir(trainAnnotation):\n    for image in sorted(os.listdir(trainAnnotation+'\/'+folder)):\n      trainAnnotationPath.append(trainAnnotation+'\/'+folder+'\/'+image)\n  class_map = {k: index for index, k in enumerate(sorted(classes))}\n\n  #Taking the first 19 classes \n  a=[]\n  for i in trainAnnotationPath[:3363]:\n    with open (i,encoding=\"utf8\",errors='ignore') as f:\n      reader = f.read()\n    predClass = str(re.findall('<name>(.+?)<\/name>', reader)[0])\n    a.append(predClass)\n\n  print(\"Classes\",set(a))\n  print(\"No. of Classes\",len(set(a)))\n\n  class_map = {k: (index) for index, k in enumerate(sorted(set(a)))}\n\n  trainAnnotationPath = trainAnnotationPath[:3363]\n  trainImagePath = trainImagePath[:3363]\n  return trainImagePath,trainAnnotationPath,class_map","11d405e4":"def pre_process_data(img,label):\n  with open (label,encoding=\"utf8\",errors='ignore') as f:\n    reader = f.read()\n  predClass = str(re.findall('<name>(.+?)<\/name>', reader)[0])\n  xmin = int(re.findall('(?<=<xmin>)[0-9]+?(?=<\/xmin>)',\\\n                        reader)[0])\n  xmax = int(re.findall('(?<=<xmax>)[0-9]+?(?=<\/xmax>)',\\\n                        reader)[0])\n  ymin = int(re.findall('(?<=<ymin>)[0-9]+?(?=<\/ymin>)',\\\n                        reader)[0])\n  ymax = int(re.findall('(?<=<ymax>)[0-9]+?(?=<\/ymax>)',\\\n                        reader)[0])\n  orig_width = int(re.findall('(?<=<width>)[0-9]+?(?=<\/width>)',\\\n                            reader)[0])\n  orig_height = int(re.findall('(?<=<height>)[0-9]+?(?=<\/height>)',\\\n                             reader)[0])\n  x = int((xmin+xmax)\/2)\n  y = int((ymin+ymax)\/2)\n  w = int(xmax-xmin)\n  h = int(ymax-ymin)\n\n  #Resizing Bounding box according to resized image\n  x = x*wImage\/orig_width\n  y = y*hImage\/orig_height\n  w = w*wImage\/orig_width\n  h = h*hImage\/orig_height\n\n  class_index = class_map[predClass]\n  label = np.hstack((x,y,w,h,class_index))\n\n  #Converting xywh to x1y1x2y2\n  label = tf.stack([label[...,0]-label[...,2]\/2.0,\n      label[...,1]-label[...,3]\/2.0,\n      label[...,0]+label[...,2]\/2.0,\n      label[...,1]+label[...,3]\/2.0,\n      label[...,4]]\n      ,axis=-1)\n  \n  img_label[img] = label\n  return img_label\n\ntrainImage='\/kaggle\/input\/stanford-dogs-dataset\/images\/Images'\ntrainAnnotation='\/kaggle\/input\/stanford-dogs-dataset\/annotations\/Annotation'\ntrainImagePath,trainAnnotationPath,class_map = create_data(trainImage,\n                                                           trainAnnotation)\n\nimg_label=dict()\nfor i in range(len(trainImagePath)):\n  img_label = pre_process_data(trainImagePath[i],\n                             trainAnnotationPath[i])\n\ndata = pd.DataFrame(img_label.items(), columns=['img', 'label'])","8a54b7e5":"def imshow(image):\n    plt.figure(figsize=(8, 8))\n    plt.imshow(image)","91d06136":"def show_img(img,label):\n  img = cv2.imread(img)\n  color = (255,0,0)\n  img = cv2.resize(img,(hImage,wImage))\n  for i,val in enumerate(label):\n    start = tuple((np.array(label[i][:2])).astype('int'))\n    end = tuple((np.array(label[i][2:4])).astype('int'))\n    cv2.rectangle(img,start,end,color,2)\n  imshow(img)","59d0071f":"def convert_format(out,format):\n  if format == 'x1y1x2y2':\n    return tf.stack([out[...,0]-out[...,2]\/2.0,\n    out[...,1]-out[...,3]\/2.0,\n    out[...,0]+out[...,2]\/2.0,\n    out[...,1]+out[...,3]\/2.0]\n    ,axis=-1)\n\n  elif format == 'xywh':\n    return tf.stack([(out[...,0]+out[...,2])\/2.0,\n    (out[...,1]+out[...,3])\/2.0,\n    out[...,2]-out[...,0],\n    out[...,3]-out[...,1],\n    out[...,4]],axis=-1)  ##sending the class also","cc2a08a5":"def rescale_label(label):\n  return tf.stack([label[:,0]*wImage\/originW,\n  label[:,1]*hImage\/originH,\n  label[:,2]*wImage\/originW,\n  label[:,3]*hImage\/originH,\n  label[:, 4]], axis=-1) ##sending the class also","857d45cb":"def convert_scale(matrix,scale):\n  if scale == 'abs':\n    return tf.stack([matrix[:,0]*wImage,\n    matrix[:,1]*hImage,\n    matrix[:,2]*wImage,\n    matrix[:,3]*hImage],axis=-1)\n\n  elif scale == 'rel':\n    return tf.stack([matrix[:,0]\/wImage,\n    matrix[:,1]\/hImage,\n    matrix[:,2]\/wImage,\n    matrix[:,3]\/hImage],axis=-1)    ","68fa3975":"def normalised_ground_truth(matched_boxes,feature_box,return_format):\n  matched_boxes = tf.cast(matched_boxes,dtype=tf.float32)\n  feature_box = tf.cast(feature_box,dtype=tf.float32)\n  if return_format == \"encode\":\n    return tf.stack([(matched_boxes[:,0] - feature_box[:, 0]) \/ (feature_box[:, 2]),\n                   (matched_boxes[:,1] - feature_box[:, 1]) \/ (feature_box[:, 3]),\n        tf.math.log(matched_boxes[:,2] \/ feature_box[:, 2]),\n        tf.math.log(matched_boxes[:,3] \/ feature_box[:, 3])],\n        axis=-1)\n\n  elif return_format == \"decode\":\n    return tf.stack([matched_boxes[:,0] * feature_box[:, 2] + (feature_box[:, 0]),\n                    matched_boxes[:,1] * feature_box[:, 3] + (feature_box[:, 1]),\n          tf.math.exp(matched_boxes[:,2]) * feature_box[:, 2],\n          tf.math.exp(matched_boxes[:,3]) * feature_box[:, 3]],\n          axis=-1)","779b1bb0":"def create_df_box(feature_layers):\n  #s_min+(s_max-s_min)\/(m-1)*(k-1)\n  s_min = 0.2\n  s_max = 0.9\n  m = 6\n  scale=[]\n\n  for k in range(2,8):\n    sk = s_min+(s_max-s_min)\/(m-1)*(k-1)\n    scale.append(sk)\n  scale.insert(0,s_min)\n  scale.extend([s_max])\n\n  scale =  [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n\n  feature_boxes=[]\n  for feature_layer in feature_layers:\n    if (feature_layer == 64 or feature_layer == 6 or feature_layer == 4):\n      aspect_ratios=[1,2,1\/2]\n    else:\n      aspect_ratios=[1,2,3,1\/2,1\/3]\n    w_ar=[]\n    h_ar=[]\n    for i in aspect_ratios:\n      if i == 1:\n        w=scale[feature_layers.index(feature_layer)]*np.sqrt(i)\n        h=scale[feature_layers.index(feature_layer)]\/np.sqrt(i)\n        w_ar.append(w)\n        h_ar.append(h)\n        sk_1 = np.sqrt(scale[feature_layers.index(feature_layer)]* \n                     scale[feature_layers.index(feature_layer)+1])\n        w = sk_1*np.sqrt(i)\n        h = sk_1\/np.sqrt(i)      \n      else:\n        w = scale[feature_layers.index(feature_layer)]*np.sqrt(i)\n        h = scale[feature_layers.index(feature_layer)]\/np.sqrt(i)\n      w_ar.append(w)\n      h_ar.append(h)\n  \n    x_axis = np.linspace(0,feature_layer,feature_layer+1)\n    y_axis=np.linspace(0,feature_layer,feature_layer+1)\n    xx,yy = np.meshgrid(x_axis,y_axis)\n    x = [(i+0.5)\/(feature_layer) for i in xx[:-1,:-1]]\n    y = [(i+0.5)\/(feature_layer) for i in yy[:-1,:-1]]\n\n    if (feature_layer == 64 or feature_layer == 6 or feature_layer == 4):\n      ndf_box = 4\n    else:\n      ndf_box = 6\n    ndf_boxes = feature_layer*feature_layer*ndf_box\n    nbox_coordinates = 4\n    feature_box = np.zeros((ndf_boxes,nbox_coordinates))\n    x = np.array(x).reshape(feature_layer*feature_layer)\n    x = np.repeat(x,ndf_box)\n    y = np.array(y).reshape(feature_layer*feature_layer)\n    y = np.repeat(y,ndf_box)\n\n    w_ar = np.tile(w_ar,feature_layer*feature_layer)\n    h_ar = np.tile(h_ar,feature_layer*feature_layer)\n    feature_box[:,0] = x\n    feature_box[:,1] = y\n    feature_box[:,2] = w_ar\n    feature_box[:,3] = h_ar\n    feature_boxes.append(feature_box)\n  df_box = np.concatenate(feature_boxes,axis=0)\n  return df_box","318e055d":"def iou(box1,box2):\n  box1 = tf.cast(box1,dtype=tf.float32)\n  box2 = tf.cast(box2,dtype=tf.float32)\n  \n  x1 = tf.math.maximum(box1[:,None,0],box2[:,0])\n  y1 = tf.math.maximum(box1[:,None,1],box2[:,1])\n  x2 = tf.math.minimum(box1[:,None,2],box2[:,2])\n  y2 = tf.math.minimum(box1[:,None,3],box2[:,3])\n  \n  #Intersection area\n  intersectionArea = tf.math.maximum(0.0,x2-x1)*tf.math.maximum(0.0,y2-y1)\n\n  #Union area\n  box1Area = (box1[:,2]-box1[:,0])*(box1[:,3]-box1[:,1])\n  box2Area = (box2[:,2]-box2[:,0])*(box2[:,3]-box2[:,1])\n  \n  unionArea = tf.math.maximum(1e-10,box1Area[:,None]+box2Area-intersectionArea)\n  iou = intersectionArea\/unionArea\n  return tf.clip_by_value(iou,0.0,1.0)\n","e8b2b889":"def df_match(labels,iou_matrix):\n  max_values = tf.reduce_max(iou_matrix,axis=1)\n  max_idx = tf.math.argmax(iou_matrix,axis=1)\n  matched = tf.cast(tf.math.greater_equal(max_values,0.5),\n                  dtype=tf.float32)\n  gt_box = tf.gather(labels,max_idx)\n  return gt_box,matched","06df19f5":"def pre_process_img(img,feature_box_conv,matched):\n  img = cv2.imread(img)\n  img = cv2.resize(img, (hImage,wImage), interpolation = cv2.INTER_AREA)\n  color = (255,0,0)\n  matched_idx = np.where(matched)\n  for i in (matched_idx):\n    for j in i:\n      start = feature_box_conv[j,:2]\n      end = feature_box_conv[j,2:4]\n      start = tuple((start))\n      end = tuple((end))\n      cv2.rectangle(img,start,end,color,2)\n  plt.title('Matched Boxes')\n  imshow(img)  ","aae720c2":"def parse_data(data):\n  parsed_labels = []\n  parsed_image_paths =[]\n  for row in data.itertuples():\n        parsed_image_paths += [row.img]\n        label = [list(map(int, row.label))]\n        parsed_labels += [np.float32(label)]\n  return parsed_image_paths, parsed_labels \n\ndef generator(images, labels):\n    def _g():\n        indices = np.arange(0, len(images))\n        np.random.shuffle(indices)\n        for idx in indices:\n            yield tf.constant(images[idx]), tf.constant(labels[idx])\n    return _g","5988a101":"#Matched Boxes\ndef create_data(data):\n  data = parse_data(data)\n  images,labels = data[0],data[1]\n\n  #GT boxes creation\n  img = images[10]\n  label = labels[10]\n  show_img(img,label)\n  feature_layers = [64,32,16,8,6,4]\n  feature_box = create_df_box(feature_layers)\n  feature_box = convert_scale(feature_box,'abs')\n  feature_box_conv = convert_format(feature_box,'x1y1x2y2')\n  iou_matrix = iou(feature_box_conv,label[:,:4])\n  gt_box,matched = df_match(convert_format(label,'xywh'),iou_matrix)\n  pre_process_img(img,convert_format(feature_box,'x1y1x2y2'),matched)\n  boxes=gt_box[:,:4]\n  classes = gt_box[:,4]\n  classes = tf.cast(classes+1, dtype=tf.int32) #0 for background class\n  matched = tf.cast(matched,dtype=tf.int32)\n  classes = tf.cast(classes*matched,dtype=tf.int32)\n  classes = tf.one_hot(classes,depth=nClasses+1,dtype=tf.float32)\n  normalised_gtbox = normalised_ground_truth(boxes,feature_box,'encode')  \n  normalised_gtbox = normalised_ground_truth(normalised_gtbox,feature_box,'decode')\n  df_box = tf.concat((normalised_gtbox,classes),axis=-1)\n  return df_box\ndf_box=create_data(data)","8e985962":"def read_img(img):\n  image = tf.io.read_file(img)\n  image = tf.io.decode_jpeg(image, channels=3)\n  image = tf.image.resize(image,[hImage,wImage])\n  image = tf.cast(image,tf.float32)\n  # normalize image \n  image = (image + tf.constant([103.939, 116.779, 123.68]))[:, :, ::-1]\n\n  return image","6873880d":"#label\nfeature_layers = [64,32,16,8,6,4]\nfeature_box = create_df_box(feature_layers)\nfeature_box = convert_scale(feature_box,'abs')\nfeature_box_conv = convert_format(feature_box,'x1y1x2y2')\n\ndef main(img,label):\n  image = read_img(img)\n\n  iou_matrix = iou(feature_box_conv,label)\n  gt_box,matched = df_match(convert_format(label,'xywh'),iou_matrix)\n  boxes = gt_box[:,:4]\n  classes = gt_box[:,4]\n\n  classes = tf.cast(classes+1, dtype=tf.int32) #0 for background class\n  matched = tf.cast(matched,dtype=tf.int32)\n  classes = tf.cast(classes*matched,dtype=tf.int32)\n  classes = tf.one_hot(classes,depth=nClasses+1,dtype=tf.float32)\n  normalised_gtbox = normalised_ground_truth(boxes,feature_box,'encode')\n  df_box = tf.concat((normalised_gtbox,classes),axis=-1)\n  df_box.set_shape([feature_box.shape[0], 4+nClasses+1])\n  return image,df_box\n","f68926af":"def data_gen(images, labels):\n  autotune = tf.data.experimental.AUTOTUNE\n  dataset = tf.data.Dataset.from_generator(generator(images, labels),\n                                           (tf.string, tf.float32)) \n  dataset = dataset.map(main, num_parallel_calls=autotune)\n  dataset = dataset.batch(batch_size, drop_remainder=True)\n  dataset = dataset.repeat(epochs)\n  dataset = dataset.prefetch(autotune)\n  return dataset\n\n\nimages, labels = parse_data(data)\ndataset = data_gen(images, labels)\n","8f90165d":"#Test Images\ntest_images = []\ntest_labels = []\nindices = np.arange(0, len(images))\nnp.random.shuffle(indices)\nfor i in range(10):\n  test_images.append(images[indices[i]])\n  test_labels.append(labels[indices[i]])\n","c4edc476":"def total_loss(y_true,y_pred):\n  y_true = tf.cast(y_true,dtype=tf.float32)  \n  y_pred = tf.cast(y_pred,dtype=tf.float32)\n  pos_mask = tf.cast(tf.equal(tf.squeeze(y_true[:,:,4:5],axis=-1),0.0),tf.float32)\n  num_pos = tf.maximum(1.0,tf.cast(tf.math.count_nonzero(pos_mask,axis=-1),tf.float32))\n  loc_loss = tf.compat.v1.losses.huber_loss(labels=y_true[:,:,:4],\n                                            predictions=y_pred[:,:,:4],\n                                            reduction=tf.losses.Reduction.NONE)\n                                            \n  loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n  loc_loss = tf.where(tf.equal(pos_mask, 1.0), loc_loss, 0.0)\n  loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n  loc_loss = loc_loss\/num_pos\n  \n  cce = tf.losses.CategoricalCrossentropy(from_logits=True,\n                                         reduction=tf.losses.Reduction.NONE)\n  cross_entropy = cce(y_true[:,:,4:],y_pred[:,:,4:])\n  \n  #neg:pos 3:1\n  num_neg = 3.0*num_pos\n  \n  #Negative Mining\n  neg_cross_entropy = tf.where(tf.equal(pos_mask,0.0),cross_entropy,0.0)\n  sorted_dfidx = tf.cast(tf.argsort(neg_cross_entropy,\\\n                          direction='DESCENDING',axis=-1),tf.int32)\n  rank = tf.cast(tf.argsort(sorted_dfidx,axis=-1),tf.int32)\n  num_neg = tf.cast(num_neg,dtype=tf.int32)\n  neg_loss = tf.where(rank<tf.expand_dims(num_neg,axis=1),neg_cross_entropy,0.0)\n  \n  pos_loss = tf.where(tf.equal(pos_mask,1.0),cross_entropy,0.0)\n  clas_loss = tf.reduce_sum(pos_loss+neg_loss,axis=-1)\n  clas_loss = clas_loss\/num_pos\n  totalloss = loc_loss+clas_loss\n  return totalloss\n","d363e4e1":"def conv_layer(filter,kernel_size,\n               layer,strides=1,\n               padding='same',\n               activation='linear',pool=False,\n               poolsize=2,poolstride=2,conv=True):\n  if conv == True:\n      layer = tf.keras.layers.Conv2D(filters=filter,\n                                  kernel_size=kernel_size,\n                                  strides=strides,\n                                  activation=activation,\n                                  padding=padding,\n                                   kernel_initializer='he_normal')(layer)\n      layer = tf.keras.layers.BatchNormalization()(layer)\n      layer = tf.keras.layers.ReLU()(layer)\n  elif pool == True:\n    layer=tf.keras.layers.MaxPool2D(pool_size=(poolsize,poolsize),\n                                    strides=poolstride,padding='same')(layer)\n  return layer\n\ndef ssd_model():\n  inputs=tf.keras.layers.Input(shape=(512,512,3))\n  outputs=[]\n  densenet_121 = tf.keras.applications.DenseNet121(input_shape=(512,512,3),\n                                                  include_top = False)\n  \n  #Feature Layer 1\n\n  layer = densenet_121.get_layer('pool3_relu').output\n  output = tf.keras.layers.Conv2D(filters=4*(4+nClasses+1),\n            kernel_size=3,\n            padding='same',\n            kernel_initializer='glorot_normal')(layer)\n  output = tf.keras.layers.Reshape([-1, 4+nClasses+1])(output)\n  outputs.append(output)\n  \n  #Feature Layer 2\n\n  layer = densenet_121.get_layer('pool4_relu').output\n  output = tf.keras.layers.Conv2D(filters=6*(4+nClasses+1),\n            kernel_size=3,\n            padding='same',\n            kernel_initializer='glorot_normal')(layer)\n  output = tf.keras.layers.Reshape([-1, 4+nClasses+1])(output)\n  outputs.append(output)\n\n\n  #Feature Layer 3 \n  \n  layer = densenet_121.get_layer('relu').output\n  output = tf.keras.layers.Conv2D(filters=6*(4+nClasses+1),\n          kernel_size=3,\n          padding='same',\n          kernel_initializer='glorot_normal')(layer)\n  output = tf.keras.layers.Reshape([-1, 4+nClasses+1])(output)\n  outputs.append(output)\n\n  #Feature Layer 4\n\n  layer = conv_layer(128, 1, layer)\n  layer = conv_layer(256, 3, layer, strides=2)\n  output = tf.keras.layers.Conv2D(filters=6*(4+nClasses+1),\n          kernel_size=3,\n          padding='same',\n          kernel_initializer='glorot_normal')(layer)\n  output = tf.keras.layers.Reshape([-1, 4+nClasses+1])(output)\n  outputs.append(output)\n  \n  #Feature Layer 5 \n\n  layer = conv_layer(128, 1, layer,padding= 'valid')\n  layer = conv_layer(256, 3, layer,padding= 'valid')\n  output = tf.keras.layers.Conv2D(filters=4*(4+nClasses+1),\n          kernel_size=3,\n          padding='same',\n          kernel_initializer='glorot_normal')(layer)\n  output = tf.keras.layers.Reshape([-1, 4+nClasses+1])(output)\n  outputs.append(output)\n                 \n  #Feature Layer 6 \n\n  layer = conv_layer(128, 1, layer,padding= 'valid')\n  layer = conv_layer(256, 3, layer,padding= 'valid')\n  output = tf.keras.layers.Conv2D(filters=4*(4+nClasses+1),\n        kernel_size=3,\n        padding='same',\n        kernel_initializer='glorot_normal')(layer)\n  output = tf.keras.layers.Reshape([-1, 4+nClasses+1])(output)\n  outputs.append(output)\n\n  out = tf.keras.layers.Concatenate(axis=1)(outputs)\n  model = tf.keras.models.Model(densenet_121.input,out)\n  model.summary()\n  return model","47087be3":"optimizer = tf.optimizers.Adam(0.001)\n\nmodel = ssd_model()\n\nmodel.compile(optimizer=optimizer,\n            loss=total_loss)\n\ncallback=tf.keras.callbacks.ModelCheckpoint(\n        filepath=\n        'kaggle\/ssd_model.h5',\n        monitor='total_loss',\n        save_best_only=True,\n        save_weights_only=True,\n        mode='min',\n        verbose=1)\n\nstep_per_epoch = len(data)\/\/batch_size","196d952d":"model.fit(dataset,epochs=25,                    \n          steps_per_epoch=step_per_epoch)","f200a4f1":"def decode(y_pred,df_box):\n  y_preds = tf.squeeze(y_pred,axis=0)\n  df_box = tf.cast(df_box,dtype=tf.float32)\n  boxes = y_preds[:,:4]\n  boxes = normalised_ground_truth(boxes,df_box,'decode')\n  boxes_x1y1 = convert_format(boxes,'x1y1x2y2')\n  y_preds = tf.nn.softmax(y_preds[:,4:])\n  cls_idx = tf.argmax(y_preds, axis=-1)\n  cls_scores = tf.reduce_max(y_preds, axis=-1)\n  \n  #Filter out the backgrund class\n  foreground_idx = tf.where(cls_idx != 0)[:, 0]\n\n  filtered_boxes = tf.gather(boxes_x1y1, foreground_idx)\n  filtered_cls_idx = tf.gather(cls_idx, foreground_idx)\n  filtered_cls_scores = tf.gather(cls_scores, foreground_idx)\n  filtered_cls_idx = filtered_cls_idx-1\n\n  filtered_boxes_y1x1 = tf.stack([filtered_boxes[:,1],\n                                filtered_boxes[:,0],\n                                filtered_boxes[:,3],\n                                filtered_boxes[:,2]],axis=-1)  \n  nms_idx = tf.image.non_max_suppression(filtered_boxes_y1x1,\n                                          filtered_cls_scores,\n                                          max_output_size=20,\n                                          iou_threshold=0.5,\n                                          score_threshold=1e-2)\n  final_boxes = tf.gather(filtered_boxes, nms_idx)\n  final_cls_idx = tf.gather(filtered_cls_idx, nms_idx)\n  final_cls_scores = tf.gather(filtered_cls_scores, nms_idx)\n  return final_boxes, final_cls_idx, final_cls_scores","dba36c77":"def decoder_show_img(img,label,class_idx,class_map):\n  img=cv2.imread(img)\n  color=(255,0,0)\n  font = cv2.FONT_HERSHEY_SIMPLEX \n  fontScale = 0.65\n  thickness = 1\n  class_map = {value:key for key, value in class_map.items()}\n  img=cv2.resize(img,(hImage,wImage))\n  #img = (img + tf.constant([103.939, 116.779, 123.68]))[:, :, ::-1]\n  for i,val in enumerate(label):\n    start=tuple((tf.cast((label[i][:2]),dtype=tf.int32).numpy()))\n    end=tuple((tf.cast((label[i][2:4]),dtype=tf.int32).numpy()))\n    cv2.rectangle(img,start,end,color,2)\n    cv2.putText(img,class_map[int(class_idx)],\n                start,font,fontScale,color, thickness, cv2.LINE_AA)    \n  imshow(img)","71499aae":"for i in range(10):\n  img_path = test_images[i]\n  label = test_labels[i]\n  image, labels = main(img_path, label)\n  predictions = model(image[None, ...], training=False)\n  feature_layers=[64, 32, 16, 8, 6, 4]\n  feature_box=create_df_box(feature_layers)\n  feature_box=convert_scale(feature_box,'abs')\n  final_boxes, final_cls_idx, final_cls_scores = decode(labels[None, ...],feature_box)\n  decoder_show_img(img_path , final_boxes,final_cls_idx,class_map)\n","5f331da9":"## Importing Libraries"}}