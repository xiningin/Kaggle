{"cell_type":{"b1fb0107":"code","84accea1":"code","9ed1e8fb":"code","91d7158a":"code","af07fe06":"code","cd3ea668":"code","396613a1":"code","85b15019":"code","96259e85":"code","cdf7d92e":"code","0df971e2":"markdown","050775ad":"markdown","71de3bc7":"markdown","ecd8fda8":"markdown","da590377":"markdown","157db37b":"markdown"},"source":{"b1fb0107":"import numpy as np\nimport pandas as pd\nimport lightgbm\nimport pickle\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nimport dateutil.easter as easter","84accea1":"original_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv', parse_dates=['date'])\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv', parse_dates=['date'])\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv',\n                    index_col='year')\n\noriginal_train_df.head(2)","9ed1e8fb":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200","91d7158a":"# Feature engineering\ngdp_exponent = 1.2121103201489674 # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\ndef get_gdp(row):\n    \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country] ** gdp_exponent\n\nle_dict = {feature: LabelEncoder().fit(original_train_df[feature]) for feature in ['country', 'product', 'store']}\n\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    new_df = pd.DataFrame({'gdp': df.apply(get_gdp, axis=1),\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    new_df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'dayofyear'] += 1 # fix for leap years\n    \n    for feature in ['country', 'product', 'store']:\n        new_df[feature] = le_dict[feature].transform(df[feature])\n        \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-3, 59)\n    new_df.loc[new_df['days_from_easter'].isin(range(12, 39)), 'days_from_easter'] = 12 # reduce overfitting\n    #new_df.loc[new_df['days_from_easter'] == 59, 'days_from_easter'] = -3\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date # used in GroupKFold\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntrain_df['target'] = np.log(train_df['num_sold'] \/ train_df['gdp'])\ntest_df = engineer(original_test_df)\n\nfeatures = test_df.columns.difference(['gdp'])\nprint(list(features))","af07fe06":"DIAGRAMS = True\n\nparams0 = {'objective': 'regression', # Manual optimization\n           'force_row_wise': True,\n           'max_bin': 400, # need more bins than days in a year\n           'verbosity': -1,\n           'seed': 1,\n           'bagging_seed': 3,\n           'feature_fraction_seed': 2,\n           'learning_rate': 0.018,\n           'lambda_l1': 0,\n           'lambda_l2': 1e-2,\n           'num_leaves': 18,\n           'feature_fraction': 0.710344827586207,\n           'bagging_fraction': 0.47931034482758617,\n           'bagging_freq': 3,\n           'min_child_samples': 20}\n\ndef fit_model(X_tr, X_va=None, run=0, fold=0, params=params0):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data\n    X_tr_f = X_tr[features]\n    y_tr = X_tr.target.values\n    data_tr = lightgbm.Dataset(X_tr[features], label=y_tr,\n                           categorical_feature=['country', 'product', 'store'])\n\n    # Train the model\n    model = lightgbm.train(params, data_tr, num_boost_round=2000,\n                           categorical_feature=['country', 'product', 'store'])\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = X_va[features]\n        y_va = X_va.target.values\n        data_va = lightgbm.Dataset(X_va[features], label=y_va, reference=data_tr)\n\n        # Inference for validation\n        y_va_pred = np.exp(model.predict(X_va_f)) * X_va['gdp']\n        oof.update(y_va_pred)\n        \n        # Evaluation: Execution time and SMAPE\n        smape_before_correction = np.mean(smape_loss(X_va.num_sold, y_va_pred))\n        #y_va_pred *= LOSS_CORRECTION\n        smape = np.mean(smape_loss(X_va.num_sold, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}   (before correction: {smape_before_correction:.5f})\")\n        score_list.append(smape)\n        \n        # Plot y_true vs. y_pred\n        if DIAGRAMS and fold == 0:\n            plt.figure(figsize=(10, 10))\n            plt.scatter(X_va.num_sold, y_va_pred, s=1, color='r')\n            #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n            plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n            plt.gca().set_aspect('equal')\n            plt.xlabel('y_true')\n            plt.ylabel('y_pred')\n            plt.title('OOF Predictions')\n            plt.show()\n        \n    else:\n        smape = None\n        \n    return model, smape\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df_e = engineer(demo_df)\n    demo_df['num_sold'] = np.exp(model.predict(demo_df_e[features])) * demo_df.apply(get_gdp, axis=1)\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n    plt.legend()\n    plt.title('Predictions and true num_sold for five years')\n    plt.show()\n\noof = pd.Series(0, index=train_df.index)\nscore_list = []\nkf = GroupKFold(n_splits=4)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df, groups=train_df.date.dt.year)):\n    X_tr = train_df.iloc[train_idx]\n    X_va = train_df.iloc[val_idx]\n    model, smape = fit_model(X_tr, X_va, run=0, fold=fold)\n\nprint(f\"Average SMAPE: {sum(score_list) \/ len(score_list):.5f}\")\nwith open('oof.pickle', 'wb') as handle: pickle.dump(oof, handle)\n    \nif DIAGRAMS: plot_five_years_combination(engineer)","cd3ea668":"# Grid search for the best hyperparameter\ndef optimize_param(params, param_name, pmin, pmax, log=False, int_=False, n_steps=30):\n    \"\"\"Grid search for the best hyperparameter; updates params\n    \"\"\"\n    score_list = []\n    if int_:\n        step_size = max(round((pmax-pmin) \/ n_steps), 1)\n        w_array = np.arange(pmin, pmax+1, step_size) \n    elif log:\n        w_array = np.logspace(np.log10(pmin), np.log10(pmax), n_steps) \n    else:\n        w_array = np.linspace(pmin, pmax, n_steps) \n    for w in w_array:\n        print(f\"{param_name}: {w}\")\n        params[param_name] = w\n        model, smape = fit_model(X_tr, X_va, run=0, fold=fold, params=params)\n        score_list.append(smape)\n    plt.figure(figsize=(12,4))\n    plt.plot(w_array, score_list, label='measured')\n    plt.scatter([w_array[np.argmin(np.array(score_list))]], [min(score_list)], color='b')\n    poly = np.polynomial.polynomial.Polynomial.fit(w_array, score_list, deg=2)\n    plt.plot(w_array, poly(w_array), 'g--', label='fit')\n    plt.scatter([w_array[np.argmin(np.array(poly(w_array)))]], [min(poly(w_array))], color='g')\n    plt.legend(loc='upper left')\n    plt.ylabel('SMAPE')\n    plt.xlabel(param_name)\n    plt.title(f'Optimizing {param_name}')\n    plt.show()\n\n    best_w = w_array[np.argmin(np.array(poly(w_array)))]\n    print(f\"Best {param_name}: {best_w}    | Best SMAPE: {min(poly(w_array)):.5f}\")\n    params[param_name] = best_w\n\nX_tr = train_df[train_df.date.dt.year < 2018]\nX_va = train_df[train_df.date.dt.year == 2018]\nparams = params0\nparams['bagging_seed'] =10\n#optimize_param(params, 'max_depth', 4, 14, int_=True, n_steps=12)\n#optimize_param(params, 'lambda_l1', 0, 1e0, log=False, n_steps=30)\n#optimize_param(params, 'lambda_l2', 1e-4, 0.02, log=True, n_steps=30)\noptimize_param(params, 'bagging_freq', 1, 10, int_=True, n_steps=10)\noptimize_param(params, 'bagging_fraction', 0.1, 0.6, log=False, n_steps=30)\noptimize_param(params, 'feature_fraction', 0.4, 1.0, log=False, n_steps=30)\noptimize_param(params, 'min_child_samples', 1, 40, int_=True, n_steps=30)\noptimize_param(params, 'num_leaves', 6, 30, int_=True, n_steps=10)\noptimize_param(params, 'learning_rate', 0.007, 0.04, n_steps=20)\nparams","396613a1":"# Optuna\nOPTUNA = False\nif OPTUNA:\n    import optuna\n\n    def objective(trial):\n        params = {'objective': 'mae', # 'regression' or 'mae'?\n                  'force_row_wise': True,\n                  'verbosity': -1,\n                  'boosting_type': 'gbdt',\n                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n                  'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n                  'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n                  'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n                  'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n                  'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n                  'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n                  'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        }\n        model, smape = fit_model(X_tr, X_va, run=0, fold=fold, params=params)\n        return smape\n\n    X_tr = train_df[train_df.date.dt.year < 2018]\n    X_va = train_df[train_df.date.dt.year == 2018]\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=100)","85b15019":"# Refit the model on the complete training data several times with different seeds\ntest_pred_list = []\nfor i in range(25):\n    params = params0\n    params['seed'] = i\n    params['bagging_seed'] = i+1\n    params['feature_fraction_seed'] = i+2\n    model, _ = fit_model(train_df, params=params)\n    test_pred_list.append(np.exp(model.predict(test_df[features])) * test_df['gdp'].values)\n\n#plot_five_years_combination(engineer) # Quick check for debugging\ntrain_df['pred'] = np.exp(model.predict(train_df[features])) * train_df['gdp'].values\nwith open('train_pred.pickle', 'wb') as handle: pickle.dump(train_df.pred, handle) # save residuals for further analysis\n\nif len(test_pred_list) > 0:\n    # Create the submission file\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) \/ len(test_pred_list)\n    sub.to_csv('submission_lightgbm_quickstart.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n             density=True, label='Training')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201),\n             density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel('num_sold')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","96259e85":"sub","cdf7d92e":"# Create a rounded submission file\nsub_rounded = sub.copy()\nsub_rounded['num_sold'] = sub_rounded['num_sold'].round()\nsub_rounded.to_csv('submission_lightgbm_quickstart_rounded.csv', index=False)\nsub_rounded","0df971e2":"# Feature engineering","050775ad":"# Training and validation","71de3bc7":"# Hyperparameter optimization with Optuna\nLet's skip this part...","ecd8fda8":"# Re-training, inference and submission","da590377":"# LightGBM Quickstart\n\nIn this notebook, I want to create a good LightGBM model, applying the insight we have gained from [EDA](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense) and [linear model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model).\n\nLightGBM has major advantages compared to the linear model:\n- It finds all seasonal effects and fixed holidays from one single feature and doesn't need a Fourier transformation.\n- It finds all full-moon-dependent holidays from Easter to Pentecost from a single feature.\n\nI still use a **transformed target, but with a new transformation**: I divide `num_sold` by the GDP\\*\\*1.212 and then take the logarithm. At the same time, I hide the year from the booster. With this trick, I avoid that the decision trees have to deal with year or GDP values they haven't seen in training. In other words, I take for granted that `num_sold` is proportional to the GDP\\*\\*1.212, and I want the model to not even try to find any alternative dependence between the GDP and `num_sold`. The derivation of this exponent can be found in the [linear model notebook](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model).\n\nI have **changed the cross-validation scheme** from my earlier notebook: I'm no longer using the years 2015-2017 for training and 2018 for validation, but a full GroupKFold with the years as groups. I use the GroupKFold because after removing year and GDP from the features, the data is no longer a real time series. And we don't need to worry about using information from the future because the GDP is information from the future anyway.\n\nRelease notes:\n- V2: refit several times, tuned learning rate\n- V3: new hyperparameter optimization with visualization\n- V4: GDP exponent\n- V5: hyperparameter tuning, small feature modifications\n","157db37b":"# Forget Optuna\nThere are two reasons not to use Optuna:\n1. It overfits.\n2. You learn more by plotting the effect of varying the parameters"}}