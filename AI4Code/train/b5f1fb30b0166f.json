{"cell_type":{"d4fa636a":"code","2920f7ee":"code","6d63f7bb":"code","f8193197":"code","f8851089":"code","8b32efec":"code","96753d42":"code","512d2e37":"code","f21cfd70":"code","9dd04a0b":"code","6a0c21b9":"code","a71b1bce":"code","f1249348":"code","88860d84":"code","9bc301cc":"code","d9aa3ed8":"code","1753df98":"code","9d09ba0c":"code","0610cd93":"code","3ee288e5":"code","a0a78010":"code","9a745577":"code","93fbbe7f":"code","75a486c0":"code","04238664":"code","a5783f86":"markdown","e81c90a9":"markdown","f4ff9264":"markdown","e705688b":"markdown","4b8ac21d":"markdown","3173e84b":"markdown","551c982d":"markdown","4c60d046":"markdown"},"source":{"d4fa636a":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom nltk import tokenize\nfrom nltk.tokenize import sent_tokenize\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold\n\nimport gc\n##Enable automatic garbage collection\ngc.enable()","2920f7ee":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 258\nMAX_SENTS = 50\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nTOKENIZER_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","6d63f7bb":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n    torch.backends.cudnn.deterministic = True","f8193197":"train_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","f8851089":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","8b32efec":"excerpt = \"how are you? how are you? how am I? not good.\"\nx=tokenizer.batch_encode_plus([excerpt])\n\nconfig = AutoConfig.from_pretrained(ROBERTA_PATH)\nconfig.update({\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\"layer_norm_eps\": 1e-7})                       \nroberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config) \nroberta.eval()\nwith torch.no_grad():\n    roberta_output = roberta(input_ids=torch.tensor(x['input_ids']),attention_mask=torch.tensor(x['attention_mask']))\n\nlast_layer_singleshot = roberta_output.hidden_states[-1].detach().squeeze()\nprint(\"This is the input:\",tokenizer.tokenize(excerpt), \"with length\", len(tokenizer.tokenize(excerpt)), \"excluding CLS and SEP\", \"\\n\")\nprint(\"This is the tokenized version\", x, \"\\n\")\nprint(\"This is the output:\", last_layer_singleshot.shape, \"one 768 embedding for each input token\")","96753d42":"excerpt = \"how are you? how are you? how am I? not good.\"\nsentences_list = tokenize.sent_tokenize(excerpt)  ##NLTK - Aint it ironical\nprint(sentences_list, \"\\n\")\nx=tokenizer.batch_encode_plus(sentences_list)\nprint(\"Tokens before concatenation:\\n\",x, \"\\n\")\ninput_ids = list(np.concatenate(x['input_ids']))\nattention_mask = list(np.concatenate(x['attention_mask']))\nx['input_ids'] = [input_ids]\nx['attention_mask'] = [attention_mask]\nprint(\"Tokens after concatenation::\\n\",x, \"\\n\")\nprint(\"Conventional Single-shot tokenization:\\n\",tokenizer.batch_encode_plus([excerpt]))","512d2e37":"print(excerpt)\nprint(tokenizer.tokenize(excerpt))","f21cfd70":"##x has the tokens after concatenation\n##feed it to Roberta\nroberta_output = roberta(input_ids=torch.tensor(x['input_ids']),attention_mask=torch.tensor(x['attention_mask']))\nroberta.eval()\nwith torch.no_grad():\n    last_layer_conc = roberta_output.hidden_states[-1].detach().squeeze()\n\nprint(\"Singleshot:\", last_layer_singleshot.shape)\nprint(\"Concatenated:\", last_layer_conc.shape, \"\\n\")\nprint(\"Apart from the extra CLS, SEP tokens, are these embeddings nearly the same?\")","9dd04a0b":"print(\"SINGLE SHOT VERSION\", \"\\n\")\nprint(\"Original sentence:\")\nprint(excerpt, \"\\n\")\nprint(\"CLS: First 5 dim\", last_layer_singleshot[0,:5].numpy())\nprint(\"SEP: First 5 dim\", last_layer_singleshot[last_layer_singleshot.size(0)-1,:5].numpy(), \"\\n\")\nprint(\"First 5 dimensions for first 7 words:\\n\")\nfor i in range(7):\n    print(tokenizer.tokenize(excerpt)[i],\"\\n\" ,last_layer_singleshot[i+1,:5].numpy())","6a0c21b9":"print(\"REMASTERED SENTENCE AFTER CONCATENATION OF SENTENCE-WISE TOKENS:\\n\")\nprint(excerpt)\nprint(x['input_ids'], \"\\n\")\nprint(\"FIRST CLS: First 5 dim\", last_layer_conc[0,:5].numpy())\nprint(\"LAST SEP: First 5 dim\", last_layer_conc[last_layer_conc.size(0)-1,:5].numpy(), \"\\n\")\nprint(\"First 5 dimensions for first 7 words including the interim CLS and SEP:\\n\")\nfor i in range(7):\n    print(last_layer_conc[i+1,:5].numpy())","a71b1bce":"x=tokenizer.batch_encode_plus([\"how are you?\"])\nout = roberta(input_ids=torch.tensor(x['input_ids']),attention_mask=torch.tensor(x['attention_mask']))\nyou1 = out.hidden_states[-1].detach().squeeze()[3,:]\n\nx=tokenizer.batch_encode_plus([\"how are you? I am fine.\"])\nout = roberta(input_ids=torch.tensor(x['input_ids']),attention_mask=torch.tensor(x['attention_mask']))\nyou2 = out.hidden_states[-1].detach().squeeze()[3,:]\n\nsentences_list = tokenize.sent_tokenize(\"how are you? I am fine.\") \nx=tokenizer.batch_encode_plus(sentences_list)\nx['input_ids'] = [list(np.concatenate(x['input_ids']))]\nx['attention_mask'] = [list(np.concatenate(x['attention_mask']))]\nout = roberta(input_ids=torch.tensor(x['input_ids']),attention_mask=torch.tensor(x['attention_mask']))\nyou3 = out.hidden_states[-1].detach().squeeze()[3,:]\n\n##Let us take one more reference word say - fine\nfine = out.hidden_states[-1].detach().squeeze()[-3,:]\n\nprint(\"\\n\", you1[:5], \"\\n\", you2[:5], \"\\n\", you3[:5], \"\\n\", fine[:5])\n\ndel roberta\ngc.collect()","f1249348":"def cosine_similarity(arr1, arr2):\n    return sum([i*j for i,j in zip(arr1, arr2)])\/(math.sqrt(sum([i*i for i in arr1]))* math.sqrt(sum([i*i for i in arr2])))\n\nprint(cosine_similarity(you2, you3))\nprint(cosine_similarity(you2, you1))\nprint(cosine_similarity(you3, you1), \"\\n\")\n\nprint(cosine_similarity(you2, fine))\nprint(cosine_similarity(you3, fine))","88860d84":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n        self.df = df        \n        self.inference_only = inference_only\n        excerpts = df.excerpt.tolist()\n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n        ##self.encoded = tokenizer.batch_encode_plus(self.text,padding = 'max_length',max_length = MAX_LEN,truncation = True,return_attention_mask=True)\n        self.encoded, self.start_pos = [], []\n        for excerpt in excerpts:\n            sentences = tokenize.sent_tokenize(excerpt) \n            split_tok=tokenizer.batch_encode_plus(sentences)\n            input_ids = list(np.concatenate(split_tok['input_ids']))\n            attention_mask = list(np.concatenate(split_tok['attention_mask']))\n            if len(input_ids) > MAX_LEN:\n                input_ids,attention_mask = input_ids[:MAX_LEN-1], attention_mask[:MAX_LEN-1]\n                input_ids.extend([2])\n                attention_mask.extend([1])\n                split_tok['input_ids'],split_tok['attention_mask'] = [input_ids],[attention_mask]\n            else:\n                pad_cnt = MAX_LEN - len(input_ids)\n                input_ids.extend([1] * pad_cnt)\n                attention_mask.extend([0] * pad_cnt)\n                split_tok['input_ids'], split_tok['attention_mask'] = [input_ids],[attention_mask]\n            self.encoded.append(split_tok)\n            start_pos = list(np.nonzero(np.array(split_tok['input_ids'][0])==0)[0])\n            start_pos.extend([start_pos[-1]]*(MAX_LEN-len(start_pos)))\n            self.start_pos.append(start_pos)\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded[index]['input_ids'][0])\n        attention_mask = torch.tensor(self.encoded[index]['attention_mask'][0])\n        start_pos = torch.tensor(self.start_pos[index])\n        if self.inference_only:\n            return (input_ids, attention_mask, start_pos)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, start_pos, target)","9bc301cc":"\"\"\"temp = train_df['excerpt'].tolist()[:5]\nfor excerpt in temp:\n    print(excerpt, \"\\n\")\n    sentences = tokenize.sent_tokenize(excerpt) \n    [print(\"-\",sentence) for sentence in sentences]\n    print(\"\\n\")\"\"\"","d9aa3ed8":"\"\"\"train_dataset = LitDataset(train_df.head(2))    \ntrain_loader= DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=False)\nfor batch_num, (input_ids, attention_mask, start_pos, target) in enumerate(train_loader):\n    print(len(start_pos[0]), len(input_ids[0][0]))\"\"\"","1753df98":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\"layer_norm_eps\": 1e-7})                       \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n        ##Neither norm nor adding an LSTM layer nor a self-att layer helped\n        ##self.layer_norm = nn.LayerNorm(768)\n        ##self.lstm = nn.LSTM(768, 512, bidirectional=True, dropout=0.1, batch_first=True)\n        self.attention_layer = nn.Sequential(nn.Linear(768, 512),nn.Tanh(),nn.Linear(512, 1),nn.Softmax(dim=1))   \n        ##Note that above is NOT self-attention, I just retained the naming of the orig author\n        self.regressor = nn.Sequential(nn.Linear(768, 1))\n\n    def forward(self, input_ids, attention_mask, start_pos):\n        ##The commented lines are for the different variants of HAN that I talked off earlier\n        roberta_output = self.roberta(input_ids=input_ids,attention_mask=attention_mask)\n        last_layer = roberta_output[0]\n        ##I apologize for the terseness and will cleanup later\n        \n        for i, excerpt in enumerate(last_layer):\n            ##<num_words, dim>\n            ##Below innocent statement led to HOURS of debugging\n            ##avg_embeddings = torch.empty(768).to(DEVICE)\n            start_pos_excerptwise = start_pos[i,:].squeeze()\n            for cnt, pos in enumerate(start_pos_excerptwise):\n                if (cnt!=len(start_pos_excerptwise)-1) and (pos!=start_pos_excerptwise[cnt+1]):\n                    ##Above, we take adv of the fact that Python will not evaluate\n                    ##(False 'and' anything), so no run-time error in second expression\n                    emb = excerpt[pos:start_pos_excerptwise[cnt+1],:]\n                    ##avg_embeddings = torch.sum(emb,0)\/emb.size(0) if cnt==0 else avg_embeddings+torch.sum(emb,0)\/emb.size(0)\n                    avg_embeddings = (torch.sum(emb,0)\/emb.size(0)).unsqueeze(-1).permute(1,0) if cnt==0 else torch.cat((avg_embeddings,(torch.sum(emb,0)\/emb.size(0)).unsqueeze(-1).permute(1,0)),dim=0)\n                else:\n                    mask = attention_mask[i,:].squeeze()[pos:]\n                    nonmask = torch.count_nonzero(mask)\n                    emb = excerpt[pos:,:]\n                    ##avg_embeddings = torch.sum(emb*mask.unsqueeze(-1),0)\/nonmask if cnt==0 else avg_embeddings+torch.sum(emb*mask.unsqueeze(-1),0)\/nonmask\n                    avg_embeddings = (torch.sum(emb*mask.unsqueeze(-1),0)\/nonmask).unsqueeze(-1).permute(1,0) if cnt==0 else torch.cat((avg_embeddings,(torch.sum(emb*mask.unsqueeze(-1),0)\/nonmask).unsqueeze(-1).permute(1,0)),dim=0)\n                    break\n            ##Let us pad the individual sentence embedding tensor. Let us say there can be max of 50 sentences.\n            ##can explore https:\/\/medium.com\/huggingface\/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983\n            ##for an alternate way to do this\n            pad=MAX_SENTS-avg_embeddings.size(0)\n            if pad>0:\n                pad_emb = torch.zeros(pad, 768).to(DEVICE)\n                avg_embeddings = torch.cat((pad_emb, avg_embeddings), dim=0)\n            else:\n                avg_embeddings = avg_embeddings[:MAX_SENTS,:]\n                \n            if i==0:\n                ##mean_emb_weighted = avg_embeddings.unsqueeze(-1).permute(1,0)\/ (cnt+1)\n                mean_emb_weighted = avg_embeddings.unsqueeze(-1).permute(2,0,1)\n            else:\n                ##mean_emb_weighted = torch.cat((mean_emb_weighted, (avg_embeddings.unsqueeze(-1).permute(1,0)\/(cnt+1))), dim=0)\n                mean_emb_weighted = torch.cat((mean_emb_weighted, (avg_embeddings.unsqueeze(-1).permute(2,0,1))), dim=0)\n        ##<bs, dim> for mean of means and <bs, MAX_SENTS, dim> for LSTM scenario\n        \n        ##norm_mean_emb = self.layer_norm(mean_emb_weighted)\n        ##lstm, (h1, c) = self.lstm(mean_emb_weighted)\n        weights = self.attention_layer(mean_emb_weighted)\n        context_vector = torch.sum(weights * mean_emb_weighted, dim=1)        \n        out = self.regressor(context_vector)\n        ##out = self.regressor(mean_emb_weighted)\n        return out","9d09ba0c":"def eval_mse(model, data_loader):\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask, start_pos, target) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            start_pos = start_pos.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            pred = model(input_ids, attention_mask, start_pos)                       \n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n    return mse_sum \/ len(data_loader.dataset)","0610cd93":"def predict(model, data_loader):\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask, start_pos) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            start_pos = start_pos.to(DEVICE)\n            pred = model(input_ids, attention_mask, start_pos)                        \n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","3ee288e5":"def train(model, model_path, train_loader, val_loader,optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n    start = time.time()\n\n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n        for batch_num, (input_ids, attention_mask, start_pos, target) in enumerate(train_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            start_pos = start_pos.to(DEVICE)            \n            target = target.to(DEVICE)                        \n            optimizer.zero_grad()\n            model.train()\n            pred = model(input_ids, attention_mask, start_pos)\n            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n            mse.backward()\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n                \n            if step >= last_eval_step + eval_period:\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                ##print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break                               \n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", f\"(from epoch {best_epoch})\")                                    \n                    \n                start = time.time()\n            step += 1\n    return best_val_rmse","a0a78010":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n        lr = 2e-5\n        if layer_num >= 69:        \n            lr = 5e-5\n        if layer_num >= 133:\n            lr = 1e-4\n        parameters.append({\"params\": params,\"weight_decay\": weight_decay,\"lr\": lr})\n        \n    return AdamW(parameters)","9a745577":"gc.collect()\nSEED = 1000\nlist_val_rmse = []\nkfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n\nfor fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n    print(f\"\\nFold {fold + 1}\/{NUM_FOLDS}\")\n    model_path = f\"model_{fold + 1}.pth\"\n    set_random_seed(SEED + fold)\n    train_dataset = LitDataset(train_df.loc[train_indices])    \n    val_dataset = LitDataset(train_df.loc[val_indices])    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True, num_workers=2)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False, num_workers=2)    \n    set_random_seed(SEED + fold)    \n    model = LitModel().to(DEVICE)\n    optimizer = create_optimizer(model)                        \n    scheduler = get_cosine_schedule_with_warmup(optimizer,num_training_steps=NUM_EPOCHS * len(train_loader),num_warmup_steps=50)    \n    list_val_rmse.append(train(model, model_path, train_loader,val_loader, optimizer, scheduler=scheduler))\n\n    del model\n    gc.collect()\n    print(\"\\nPerformance estimates:\")\n    print(list_val_rmse)\n    print(\"Mean:\", np.array(list_val_rmse).mean())","93fbbe7f":"test_dataset = LitDataset(test_df, inference_only=True)","75a486c0":"all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(len(list_val_rmse)):            \n    model_path = f\"model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    all_predictions[index] = predict(model, test_loader)\n    del model\n    gc.collect()","04238664":"predictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","a5783f86":"Let us analyze the key differences before proceeding to Roberta output. Firstly we see the start stop markers after every sentence. That difference was expected. We will see if this will harm the internal workings of Roberta. But we also see one more change. The unexpected difference is that certain tokens seem to have gotten corrupted? \"how\" is represented by 9178 consistently in the first tokenization attempt. But when we combine all sentences, we get 141 instead of 9178 for the second \"how\". The first token of the word seems to be corrupted. However this is not really an issue. It is just a matter of representation. It seems that due to its architecture, Roberta encodes the preceeding \"space\" also. If we split every sentence the first word will not have space. When we combine sentences, the first word has a space in front of it and hence the encoding is the \"special character\" + the regular encoding. This should not cause any issue. See below how space is represented by the special character - \u0120","e81c90a9":"So, how do we test whether the embeddings generated in this manner are corrupted or not? A manually look at the first few values seem to indicate that they are more less nearby. Let us take 3 cases\n- Embedding of you from \"how are you?\"\n- Embedding of you from \"how are you? I am fine\" using the single shot approach\n- Embedding of you from \"how are you? I am fine\" using the concatenated token approach\n\nIf each of these embeddings or more or less equidistant to each other, we are good and it is safe to conclude that the concatenated approach does not break Roberta behaviour in any way, considering all the additional CLS, SEP tokens in between","f4ff9264":"This is an account of my tryst with the HAN model.\n\nI experimented with a few variants of this model, but in most cases I could not go below the score of 0.48 on the public LB. Let us take a quick intuitive look at the HAN model before jumping into the code - https:\/\/www.cs.cmu.edu\/~.\/hovy\/papers\/16HLT-hierarchical-attention-networks.pdf. This is pretty old paper, relatively speaking. It came out in 2016... way before BERT, transformers and a full 2 years before the landmark paper - Attention is all you need. \n\nBut this paper in itself was (and is) considered to be a landmark, the reason being the structured way in which a document is broken down into paragraphs, sentences and words for analysis...which is the precise way in which a human brain would analyze a long document. What Yang et al did was to bring in 2 innovations:\n- They made use of Attention (which had by then become famous after Bahdanu's important paper in 2014) and gave it a slight twist by creating an explicit context-object which was to be 'learnt' by the model and then used this context object to determine the attention weights. \n- More importantly they 'applied' this form of attention NOT to the entire document but at various sub-levels. So they broke down a long document into paragraphs and sentences. They would read each sentence in a para and let the model 'learn' a context-object which would have the dimensions of a single word. This context-object would then be multiplied with all the word embeddings in the sentence to determine the attention weights and then the weighted sum of all words would lead to a best representation of the sentence. So now we have each sentence compressed to the dimensions of a 'single special word' - a classic reduction.\n- Next they did the same thing for paragraphs. Each paragraph is a collection of sentences and since they now have one 'special-word' representing each sentence, so a single para boiled down to a collection of 'special-words'. They used the same Attention technique to learn a new context vector, derived attention weights and summed the weighted 'special-words' embeddings to get a single representation for the paragraph which again had the dimensions of a single word.\n- Now a simple linear layer could be applied\n\nThis process is hierarchical and hence the name hierarchical attention networks. It was used for document classification and even after 5 years is still used for tasks dealing with understanding of long texts. The reason why this works do well is that it mimicks the way a human would read the texts - processing words, then sentences and then paragraphs. In fact, any-one would assume that it would be the de-facto standard for Readibility domain.\n\nAnd looks like this indeed was the case. ReadNet - https:\/\/arxiv.org\/pdf\/2103.04083.pdf - was published in 2021 and seemed to break most records in the Readability space. It is based on the original HAN model proposed by Yang but consists of 2 changes:\n- They use self-attention instead of plain-attention and hence the name - Hierarchical self-attention network. Somwehere the \"self-attention\" seems to have gotten replaced by the transformer which is a pity since this model does not have anything to do with the traditional transformers as we know them. They just leveage the 'self-attention' component of the transformer\n- The other major change was an interesting twist by which they combined the self-attention outputs with the \"explicit\" readability features. The \"interesting\" part is in the way they do it by combining sentence-level explicit features like characers per word, num of words, num of long words etc with the sentence 'special word' and paragraph level explicit features like Flesch Reading Ease, Dale Chall Index etc with the paragraph 'special word'\n- Couple of other minor changes w.r.t a transfer learning layer if applicable & a neat loss strategy but I doubt whether these affected the scores much\n\nAs can be seen the changes are pretty minor w.r.t to the original HAN (of course, devil is in the details) but one might safely assume (as I did) that implementing a original HAN and then changing the 'plain-attention' to 'self-attention' would be the key ingredient to a high score. The remaining bits like the choice of loss or addition of the explicit features didnt produce too much of a improvement as per the paper, though it certainly would have mattered at the top of the charts where even these minor differences would count.\n\nThe HAN model implementation is simple but there is one challenge which I faced in implementation. I have outlined it here - https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/252715 and will not go into details. But the 1-line summary is that most  public models have a MAX_LEN of 250-300 which is just about the avg length of the whole excerpt column. So word padding is almost never required and the tokens are rich with data, With HAN we have to bring in sentence padding. Once we do that we cant keeo 250 words per sentence. We have to break it into 40-50 assuming 10 sentence. There is lot of data lost and the resulting tokens are sparse. With good batching strategies or with larger GPUs maybe this problem could be covercome to some extent but nowhere could I fit something like 300 words * 50 sentence = 15000 tokens for one excerpt. The memory starts failing for anything above 500. So I created a small hack to overcome this. I feel that this hack should not impact teh embeddings but please feel free to correct me if I am wrong.\n\nUnfortunately, I just couldnt get this model to work with the ComLit dataset. Here are the key variants I tried:\n- A simple HAN model with the base embeddings being Glove 300 dim\n- A HAN model using fine-tuned RoBERTa-base embeddings\n- A self-attentive HAN model using fine-tuned RoBERTa-base embeddings\n- A HAN model leveraging mean embeddings. Why did I do this - Several excellent kernels published duringthe competition seemed to indicate that mean embeddings seem to perform better than attention heads. So if we can have an hierarchical-attention network, why not a Hierarchical-Mean Embedding Network (why not H-MEN - I like the name :)) I broke down this approach into 2 key variants:\n\n1. Break a para into sentences and take a mean of means. Forget about all architectures, just raw common-sense dictates that this simple technique should ideally give a bump in score instead of an overall mean (or so I thought). The logic is sound but for some reason, the overall paragraph mean gives better scores than a mean-of-means approach\n1. Since I used a Roberta-base, the words already had an element of sequence built into them. So the 'mean' embedding for a sentence probably could not be improved upon, but what about mean-of-mean? i.e. basically when we place all the sentence mean's back-to back in a sequence, instead of just taking their mean(and losing the temporal construct), wouldnt it be better to feed this sequence to a small GRU or LSTM and then take the mean of all hidden states. Again this sounded quite logical and in fact I even tried doing a self-attention instead of LSTM so a to establish relationship between sentences better, but unfortunately each approach seemed to worsen the score further! and so that was the end of my tryst with HAN :)\n\nI am sure many of the gold meadalists would share their versions of HAN which actually get high scores, but I am sharing this version of mine as a baseline and also to welcome comments on where I could have possibly gone wrong.\n\nSource code on top of which I did my changes: https:\/\/www.kaggle.com\/andretugan\/pre-trained-roberta-solution-in-pytorch - an excellent reproducible, simple yet HIGHLY efficient model\n\nI have shared the code for the last variant above, but with couple of line code changes it can be modified to suit all other variants","e705688b":"Let us carry out a simple experiment first!\n\nThis is how we normally get the embeddings","4b8ac21d":"Now let us see what happens if we break a sentence before feeding it to Roberta AND MANUALLY CONCATENATE latter","3173e84b":"A more relevant question is - Are the embeddings generated after the concatenation comparable to the embeddings gotten with a single shot sentence?","551c982d":"Note that our main comparision should be between you2 and you3 which are the single shot and consolidated versions respectively. A quick glance reveal they match decently. you1 is for reference. you2 and you3 should be as close to you1 as each other and all of them should be way different from 'fine'","4c60d046":"I rest my case! One could argue that embeddings are sparse in nature and the relevant info is in few dimensions only and cosine is not the best way to measure differences. Then what is the best way? One way is to use the concatenated-token approach and then use the mean embedding (NOT mean of means) across the entire paragraph. The resulting score is nearly the same as a singleshot mean embedding Common-Lit score. \n\nOk, now for the actual model. Since we now have a definite plan to tackle token-sparseness we can use any of the simple public models available and not worry about space or time. The model takes about an hour to run at most!\n\nWe will do some extra things in the dataset definition like sending a list of BOS (beginning-of-sentence(s) indices within an excerpt) so that we dont waste time in the model code. We need to take care of padding etc manually.\n\nThe only other changes are in the model definition,where we retrieve these BOS indices to calculate individual sentence means and then average them up to get the excerpt mean!"}}