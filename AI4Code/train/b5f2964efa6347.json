{"cell_type":{"468da8b3":"code","6a98dfbd":"code","c5800b5e":"code","79b6098f":"code","6496191c":"code","59b1c475":"code","bd80c60a":"code","3298d278":"code","6ecb84d0":"code","3799351e":"code","adb68985":"code","f6115dfa":"markdown","76b61b0e":"markdown","e468e2c0":"markdown","68dc50a0":"markdown","3e60411c":"markdown","8c3f1085":"markdown","3b208469":"markdown","b29491f8":"markdown","bba8fb68":"markdown","8672601d":"markdown"},"source":{"468da8b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6a98dfbd":"df_1 = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\ndf_1_wo_link = df_1.drop(['article_link'], axis=1)\ndf_1_wo_link.head()","c5800b5e":"df_2 = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf_2.shape\ndf_2_wo_link = df_2.drop(['article_link'], axis=1)\n","79b6098f":"final_df = pd.concat([df_1_wo_link, df_2_wo_link], axis = 0, sort=False)\n\nfinal_df.head","6496191c":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\ntraining_samples = 200\nvalidation_samples = 10000\nmax_words = 10000\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(list(final_df['headline']))\nsequences = tokenizer.texts_to_sequences(list(final_df['headline']))\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' %len(word_index))\n\n\nX = tokenizer.texts_to_sequences(final_df['headline'])\nX = pad_sequences(X, maxlen = maxlen)\ny = final_df['is_sarcastic']\n","59b1c475":"glove_dir = '\/kaggle\/input\/textembedding'\nembeddings_index={}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n\n#f = open(os.path.join(glove_dir, 'crawl-300d-2M.vec'))\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors' % len(embeddings_index))\n","bd80c60a":"embedding_dim = 100\n\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i<max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","3298d278":"from keras.layers import LSTM,Dense,GRU, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras import layers\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim,input_length = maxlen, weights = [embedding_matrix]))\n\nmodel.add(Flatten())\nmodel.add(Dense(40, activation=\"relu\"))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 10\nhistory = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","6ecb84d0":"import matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc, 'b', label = 'Validation acc')\nplt.title('Training and Validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and Validation loss ')\nplt.show()","3799351e":"from keras.layers import LSTM,Dense,GRU, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras import layers\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim,input_length = maxlen, weights = [embedding_matrix]))\nmodel.add(LSTM(64, return_sequences=False, \n               dropout=0.1, recurrent_dropout=0.1))\n#model.add(Flatten())\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 6\nhistory = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","adb68985":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc, 'b', label = 'Validation acc')\nplt.title('Training and Validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and Validation loss ')\nplt.show()","f6115dfa":"Using word embedding files  - First try with 6B(100) - glove file ","76b61b0e":"First try at using Keras( Deep learning ) for text","e468e2c0":"**First try with validation accuracy of 97.68 at 7 epochs with lowest loss of only 0.0785.**","68dc50a0":"Graph","3e60411c":"A slightly better but quite expensive model with an validation accuracy of **96.81**","8c3f1085":"Now a try with RNN","3b208469":"Please note i have concatinated both the files and running the modes using both the files with validation set being 20% of the whole file. Please let me know if i am doing anything wrong by doing so but from what i see the model is doing pretty good considering that both validation and the actual dataset accuracy and loss levels are quite close to each other ","b29491f8":"Let us look at what the data looks like and remove any unwanted values ","bba8fb68":"Tokenizing the data ","8672601d":"Graph to look how the training and validation accuracy looks like "}}