{"cell_type":{"ad2719bb":"code","995d217a":"code","28c8f61d":"code","f15508b1":"code","10a33828":"code","a2ca28f9":"code","3954467b":"code","e50b0f28":"code","930cc43c":"code","14ac0852":"code","42c29d92":"code","7b9893c0":"code","8985fd86":"code","28678a82":"code","203d60cf":"code","38240cf5":"code","bb35f915":"code","edc52111":"code","37c32c25":"code","1358d342":"code","2bcf020e":"code","4a93d247":"code","48cbe1f0":"code","50c5f0bf":"markdown","1baae529":"markdown","3297c4ad":"markdown","0f92c565":"markdown","260e12e7":"markdown","baa2e88c":"markdown","527eb089":"markdown","af7559e7":"markdown","cea7b9bf":"markdown","e1f260db":"markdown","c52624a7":"markdown","ca8274ae":"markdown","97d11b6a":"markdown","0b66e797":"markdown","312c4ca5":"markdown","ddd3168a":"markdown","bc97e604":"markdown","02d4dbfb":"markdown"},"source":{"ad2719bb":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport random\nimport os\nfrom queue import Queue\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import Model\nfrom tensorflow.data import Dataset\nfrom IPython.display import display\nimport PIL","995d217a":"training_dataset_images_path = '\/kaggle\/input\/car-object-detection\/data\/training_images'\ntesting_dataset_images_path = '\/kaggle\/input\/car-object-detection\/data\/testing_images'","28c8f61d":"boxes_df = pd.read_csv('\/kaggle\/input\/car-object-detection\/data\/train_solution_bounding_boxes (1).csv')\nboxes_df.head()","f15508b1":"def single_inputs(img_arr):\n    x,y,z = img_arr.shape\n    return img_arr.reshape((1,x,y,z))","10a33828":"class DetectionBox:\n    def __init__(self,x_min,y_min,x_max,y_max):\n        self.x_min = x_min\n        self.y_min = y_min\n        self.x_max = x_max\n        self.y_max = y_max\n    \n    @staticmethod\n    def from_named_tuple(tup):\n        return DetectionBox(tup.xmin,tup.ymin,tup.xmax,tup.ymax)\n    \n    def scale(self,scale_mul):\n        return DetectionBox(\n            self.x_min * scale_mul,\n            self.y_min * scale_mul,\n            self.x_max * scale_mul,\n            self.y_max * scale_mul\n        )\n    \n    def resize_to_point(self,x,y):\n        if self.x_min > x:\n            self.x_min = x\n        if self.x_max < x:\n            self.x_max = x\n        if self.y_min > y:\n            self.y_min = y\n        if self.y_max < y:\n            self.y_max = y\n\nclass TrainingCase:\n    def __init__(self,img_path):\n        self.img_path = img_path\n        self.boxes = []\n    \n    def add_box(self,tup):\n        self.boxes.append(DetectionBox.from_named_tuple(tup))\n    \n    def get_image(self):\n        img = load_img(self.img_path)\n        img_arr = img_to_array(img)\n        return img_arr\n    \n    def draw_image_with_boxes(self):\n        img = load_img(self.img_path)\n        img_arr = img_to_array(img)\n        h,w = img_arr.shape[:2]\n        \n        def point(y,x,color):\n            x = int(x)\n            y = int(y)\n            if x >= 0 and x < w and y >= 0 and y < h:\n                img_arr[y,x,:] = color\n        \n        for box in self.boxes:\n            if box.x_min-1 >= 0:\n                for y in range(int(box.y_min),int(box.y_max)):\n                    point(y, box.x_min-1,(0,255,0))\n                    point(y, box.x_min  ,(0,255,0))\n                    point(y, box.x_min+1,(0,255,0))\n                    point(y, box.x_max-1,(0,255,0))\n                    point(y, box.x_max  ,(0,255,0))\n                    point(y, box.x_max+1,(0,255,0))\n                for x in range(int(box.x_min),int(box.x_max)):\n                    point(box.y_min-1, x,(0,255,0))\n                    point(box.y_min  , x,(0,255,0))\n                    point(box.y_min+1, x,(0,255,0))\n                    point(box.y_max-1, x,(0,255,0))\n                    point(box.y_max  , x,(0,255,0))\n                    point(box.y_max+1, x,(0,255,0))\n        \n        img = PIL.Image.fromarray(img_arr.astype(np.uint8),'RGB')\n        display(img)\n    \n    def get_answer(self):\n        img = load_img(self.img_path)\n        img_w, img_h = img.size\n        \n        h,w = int(img_h\/32),int(img_w\/32)\n        out_arr = np.concatenate((np.full((1,h,w,1),-1,dtype=np.float),np.ones((1,h,w,1),dtype=np.float)),axis=3)\n        \n        \n        for box in self.boxes:\n            x_min,y_min,x_max,y_max = int(box.x_min \/ 32),int(box.y_min \/ 32),int(box.x_max \/ 32),int(box.y_max \/ 32)\n            for y in range(y_min,y_max):\n                for x in range(x_min,x_max):\n                    if y < 0 or x < 0 or y >= h or x >= w:\n                        continue\n                    \n                    out_arr[0,y,x,0] = 1\n                    out_arr[0,y,x,1] = -1\n        \n        return out_arr\n    \n    def get_answer_as_outputs(self):\n        answer = self.get_answer()\n        return answer[0,:,:,0],answer[0,:,:,1]\n    \n    def get_inputs(self):\n        return single_inputs(self.get_image())","a2ca28f9":"class KerasModelWrapper:\n    def __init__(self,model):\n        self._model = model\n    \n    @staticmethod\n    def _normalize(matrix,min_val,max_val):\n        return (matrix - min_val) \/ (max_val - min_val)\n    \n    def predict(self,case):\n        inputs = case.get_inputs()\n        results = self._model.predict(inputs)\n        \n        outs1 = results[0,:,:,0]\n        outs2 = results[0,:,:,1]\n        \n        min_val = min(outs1.min(),outs2.min())\n        max_val = min(outs1.max(),outs2.max())\n        \n        n_outs1 = KerasModelWrapper._normalize(outs1,min_val,max_val)\n        n_outs2 = KerasModelWrapper._normalize(outs2,min_val,max_val)\n        \n        return outs1,outs2,(n_outs1 > n_outs2)\n    \n    def __call__(self,case):\n        return self.predict(case)","3954467b":"def df_to_training_list(df):\n    trn_dict = dict()\n\n    for i in boxes_df.itertuples():\n        if not i.image in trn_dict:\n            trn_case = TrainingCase(training_dataset_images_path + '\/' + i.image)\n            trn_dict[i.image] = trn_case\n        else:\n            trn_case = trn_dict[i.image]\n        trn_case.add_box(i)\n\n    trn_list = [val for (key,val) in trn_dict.items()]\n\n    return trn_list","e50b0f28":"def draw_outputs(outputs):\n    for out in outputs:\n        out = out * 1 # This operation should case boolean matrix to numeric.\n        min_val = out.min()\n        max_val = out.max()\n        out = (out - min_val) \/ (max_val - min_val)\n        img = PIL.Image.fromarray((out*255).astype(np.uint8),'L')\n        w,h = img.size\n        img = img.resize((w*4,h*4))\n        display(img)","930cc43c":"training_list = df_to_training_list(boxes_df)","14ac0852":"for i in range(5):\n    idx = random.randrange(len(training_list))\n    training_list[idx].draw_image_with_boxes()","42c29d92":"train_X = np.concatenate([x.get_inputs() for x in training_list],axis=0)\ntrain_y = np.concatenate([x.get_answer() for x in training_list],axis=0)","7b9893c0":"def build_model():\n    mobilenet = MobileNetV2(weights='imagenet',include_top=False)\n    x = mobilenet.outputs[0]\n    x = Conv2D(2,1)(x)\n    model = Model(mobilenet.inputs,x)\n    model.compile('adam',loss = 'mse')\n    return model","8985fd86":"model = build_model()\nmodel.summary()","28678a82":"model.fit(train_X,train_y,epochs=100)","203d60cf":"model.save('car_detection')","38240cf5":"!zip -r car_detection.zip car_detection\n!rm -rv car_detection","bb35f915":"model_wrapper = KerasModelWrapper(model)","edc52111":"random_case = training_list[random.randrange(len(training_list))]\ndraw_outputs(random_case.get_answer_as_outputs())","37c32c25":"draw_outputs(model_wrapper.predict(random_case))","1358d342":"def load_test_images(path):\n    cases = []\n    for filename in os.listdir(path):\n        file_path = path + '\/' + filename\n        case = TrainingCase(file_path)\n        cases.append(case)\n    \n    return cases","2bcf020e":"def populate_boxes(model,case):\n    prediction = model.predict(case)\n    mask = prediction[2].copy()\n    h,w = mask.shape\n    boxes = []\n    \n    def walk_on_box(sx,sy):\n        box = DetectionBox(sx,sy,sx,sy)\n        q = Queue()\n        q.put((sx,sy))\n        while not q.empty():\n            x,y = q.get()\n            box.resize_to_point(x+1,y+1)\n            mask[y,x] = False\n            if y+1 < h and mask[y+1,x]:\n                q.put((x,y+1))\n            if y-1 >= 0 and mask[y-1,x]:\n                q.put((x,y-1))\n            if x+1 < w and mask[y,x+1]:\n                q.put((x+1,y))\n            if x-1 >= 0 and mask[y,x-1]:\n                q.put((x-1,y))\n        \n        return box.scale(32)\n    \n    for y in range(h):\n        for x in range(w):\n            if mask[y,x]:\n                boxes.append(walk_on_box(x,y))\n    \n    case.boxes = boxes","4a93d247":"testing_list = load_test_images(testing_dataset_images_path)\nfor case in testing_list:\n    populate_boxes(model_wrapper,case)","48cbe1f0":"for i in range(5):\n    idx = random.randrange(len(testing_list))\n    while len(testing_list[idx].boxes) == 0:\n        idx = random.randrange(len(testing_list))\n    testing_list[idx].draw_image_with_boxes()","50c5f0bf":"### Checking on test data\n\n`load_test_images` - load test cases with empty boxes list.","1baae529":"Fill boxes lists using prediction from neural network","3297c4ad":"`KerasModelWrapper` define utility method for prediction which works with case inputs and get results in necessary format.","0f92c565":"Let's export our model.","260e12e7":"`populate_boxes` have algorythm for walking over detected objects mask for draw predicted boxes. `walk_on_box` do breadth-first search on mask and fill visited points by False values.","baa2e88c":"### Case and neural network inputs and outputs\n\nBelow 2 output channels which should predict our neural network according to random case from training dataset.","527eb089":"`draw_outputs` it is utility function to show neural network outputs.","af7559e7":"### Imports and loading training data\n\nMake some imports first","cea7b9bf":"Below examples of images with boxes in training dataset.","e1f260db":"### Data preparation\n\nLet's prepare our training cases list first.","c52624a7":"We have enough RAM just to put all inputs and outputs to tensors without any data loaders or generators. So, let's do it.","ca8274ae":"Some random examples of testing images and predicted boxes.","97d11b6a":"It is actual predictions and restored mask of detected objects","0b66e797":"### Model definition and training\n\nI will use pretrained MobileNet architecture with additional convolution layer which split data to 2 classes - car and background. Pretrained layers not locked because experiments show me better quality of fitting.","312c4ca5":"`DetectionBox` - it is bounds for X,Y coordinates in training sets and notebook results for specify detected object position.\n\n`TrainingCase` holds data about detected boxes and image paths. Have some utilitary methods to get case inputs, outputs and draw images with boxes.","ddd3168a":"### Definitions\n\n`single_inputs` - convert tensor from image format to acceptable for Keras model - add new dimension for cases.","bc97e604":"`df_to_training_list` allow to convert dataframe loaded from boxes CSV to list with cases and boxes.","02d4dbfb":"There is dataset folders"}}