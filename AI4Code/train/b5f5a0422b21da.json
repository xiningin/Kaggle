{"cell_type":{"ff50934a":"code","d76d3e63":"code","81fea112":"code","148b07da":"code","1e69bda5":"code","30e0aba1":"code","c6cf8222":"code","354f1f66":"markdown","ca78f1a6":"markdown","6db68ad9":"markdown","c31c90cd":"markdown","3725eb78":"markdown","8389897b":"markdown","20c3c396":"markdown","117f983e":"markdown"},"source":{"ff50934a":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pylab as plt\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport numpy as np\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import log_loss\n# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport pickle\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","d76d3e63":"def preprocess(df):\n    df = df.copy()\n    df['cp_type_trt'] = np.where(df['cp_type'].values == 'trt_cp', 1, 0)\n    df['cp_type_ctl'] = np.where(df['cp_type'].values == 'trt_cp', 0, 1)\n    df['cp_dose_D1'] = np.where(df['cp_dose'].values == 'D1', 1, 0)\n    df['cp_dose_D2'] = np.where(df['cp_dose'].values == 'D1', 0, 1)\n    df['cp_time_24'] = np.where(df['cp_time'].values == 24, 1, 0)\n    df['cp_time_48'] = np.where(df['cp_time'].values == 48, 1, 0)\n    df['cp_time_72'] = np.where(df['cp_time'].values == 72, 1, 0)\n    return df\n\ndef make_X(dt, dense_cols, cat_feats):\n    X = {\"dense\": dt[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_feats):\n        X[v] = dt[[v]].to_numpy()\n    return X\n\n\ndef get_data(ROOT = '..\/input\/lish-moa'):\n\n    cat_feat = ['cp_dose', 'cp_time']\n\n    train = pd.read_csv(f\"{ROOT}\/train_features.csv\")\n    test = pd.read_csv(f\"{ROOT}\/test_features.csv\")\n\n    train['where'] = 'train'\n    test['where'] = 'test'\n\n    data = pd.concat([train, test], axis=0)\n\n    # for var in data.iloc[:,4:-1].columns:\n    #     data[var] = (data[var].values-data[var].mean())\/data[var].std()\n\n    label = pd.read_csv(f\"{ROOT}\/train_targets_scored.csv\")\n    label_test = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\n\n    data = preprocess(data)\n\n    uniques = []\n    for i, v in enumerate(cat_feat):\n        data[v] = OrdinalEncoder(dtype=\"int\").fit_transform(data[[v]])\n        uniques.append(len(data[v].unique()))\n\n    FE = list(data)\n    FE.remove('where')\n    FE.remove('sig_id')\n    FE.remove('cp_type_ctl')\n    FE.remove('cp_type_trt')\n    FE.remove('cp_type')\n    for cat in cat_feat:\n        FE.remove(cat)\n\n\n    train = data.loc[data['where']=='train']\n    test = data.loc[data['where'] == 'test']\n\n    del data\n\n    train = train.drop(['where'], axis=1)\n    test = test.drop(['where'], axis=1)\n\n    train = train.set_index('sig_id')\n    test = test.set_index('sig_id')\n    label = label.set_index('sig_id')\n    label_test = label_test.set_index('sig_id')\n\n    label = label.loc[train.index]\n    label_test = label_test.loc[test.index]\n\n    train = pd.concat([train, label], axis=1)\n    test = pd.concat([test, label_test], axis=1)\n\n    train['total'] = np.where(np.sum(train[list(label)].values, axis=1)>0, 0, 1)\n\n    return train, test, FE, cat_feat, list(label), uniques","81fea112":"class block(nn.Module):\n    def __init__(self, input_dim, keep_prob, hidden_dim):\n        super(block, self).__init__()\n        self.batch_norm = nn.BatchNorm1d(input_dim)\n        self.dropout = nn.Dropout(keep_prob)\n        self.dense = nn.Linear(input_dim, hidden_dim)\n\n    def forward(self, x):\n        x = self.batch_norm(x)\n        x = self.dropout(x)\n        x = self.dense(x)\n\n        return x\n\nclass autoencoder(nn.Module):\n    def __init__(self, input_dim):\n        super(autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, 256),\n            nn.ReLU(True),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, 128),\n            nn.ReLU(True), nn.BatchNorm1d(128), nn.Linear(128, 64))\n        self.decoder = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(True),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 256),\n            nn.ReLU(True),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, input_dim))\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x = self.decoder(z)\n        return x, z\n        \nclass MoaModel_encoder(nn.Module):\n    def __init__(self, hidden_dim, emb_dims, n_cont):\n        super(MoaModel_encoder, self).__init__()\n\n        self.auto_encoder = autoencoder(n_cont)\n\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n        num_columns = n_embs + n_cont + 64\n\n        self.block1 = block(num_columns, 0.25, hidden_dim)\n        self.block2 = block(hidden_dim, 0.5, int(hidden_dim\/2))\n        self.block3 = block(int(hidden_dim \/ 2), 0.25, 206)\n\n    def encode_and_combine_data(self, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n        return xcat\n\n    def forward(self, cont_data, cat_data):\n\n        cont_data = cont_data.to(device)\n        cat_data = cat_data.to(device)\n\n        re_cont_data, low_dim = self.auto_encoder(cont_data)\n\n        error = torch.abs(re_cont_data-cont_data)\n        cat_data = self.encode_and_combine_data(cat_data)\n        x = torch.cat([low_dim, cat_data, error], dim=1)\n\n\n        x = F.relu(self.block1(x))\n        x = F.relu(self.block2(x))\n        x = F.sigmoid(self.block3(x))\n\n        return x, re_cont_data","148b07da":"class Loader:\n\n    def __init__(self, X, y, shuffle=True, batch_size=64, cat_cols=[]):\n\n        self.X_cont = X[\"dense\"]\n        self.X_cat = np.concatenate([X[k] for k in cat_cols], axis=1)\n        self.y = y\n\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder  # for debugging\n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cat = self.X_cat[[ridxes]]\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n\n        return self\n\n    def __next__(self):\n        if self.i >= self.len:\n            raise StopIteration\n\n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i + self.batch_size].astype(np.float32))\n\n        else:\n            y = None\n\n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i + self.batch_size])\n        xcat = torch.LongTensor(self.X_cat[self.i:self.i + self.batch_size])\n\n        batch = (xcont, xcat, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches\n","1e69bda5":"## Early stopping algorithm\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\n    def __init__(self, patience=7, verbose=False, delta=0):\n\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model, path):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n        elif score < self.best_score - self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, path):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model, path)\n        self.val_loss_min = val_loss","30e0aba1":"## Model training\ndef model_training(model, train_loader, val_loader, loss_function,\n                   epochs,\n                   lr=0.001, patience=10,\n                   model_path='model.pth'):\n\n\n\n    if os.path.isfile(model_path):\n\n        # load the last checkpoint with the best model\n        model = torch.load(model_path)\n\n        return model\n\n    else:\n\n        # Loss and optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2,\n                                      factor=0.5, verbose=True)\n\n        criteria = loss_function\n\n        train_losses = []\n        val_losses = []\n        early_stopping = EarlyStopping(patience=patience, verbose=True)\n\n        for epoch in tqdm(range(epochs)):\n\n            train_loss, val_loss = 0, 0\n\n            # Training phase\n            model.train()\n            bar = tqdm(train_loader)\n\n            for i, (X_cont, X_cat, y) in enumerate(bar):\n                preds, cont_data_x = model(X_cont, X_cat)\n\n                loss = criteria(preds.flatten().unsqueeze(1), y.to(device).flatten().unsqueeze(1)) + 0.5*F.mse_loss(X_cont.to(device), cont_data_x)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                with torch.no_grad():\n                    train_loss += loss.item() \/ (len(train_loader))\n                    bar.set_description(f\"{loss.item():.3f}\")\n\n            # Validation phase\n            val_preds = []\n            true_y = []\n            model.eval()\n            with torch.no_grad():\n                for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                    preds, cont_data_x = model(X_cont, X_cat)\n\n                    val_preds.append(preds)\n                    true_y.append(y)\n\n                    loss = criteria(preds.flatten().unsqueeze(1),y.to(device).flatten().unsqueeze(1))  # + F.mse_loss(X_cont.to(device), cont_data_x)\n                    val_loss += loss.item() \/ (len(val_loader))\n\n                score = F.binary_cross_entropy(torch.cat(val_preds, dim=0), torch.cat(true_y, dim=0).to(device))\n\n            print(f\"[{'Val'}] Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val score: {score:.4f}\")\n\n            early_stopping(score, model, path=model_path)\n\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n\n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            scheduler.step(score)\n\n        model = torch.load(model_path)\n        return model\n\n\ndef fully_train(model, train_data, cont_features,\n                cat_features, labels, kf,\n                loss_function, hidden_dim, emb_dims,\n                epochs, lr, patience,\n                kfold=5, model_path_temp='model'):\n\n    models = []\n    val_loaders = []\n    val_set = []\n\n    for i, (train_index, test_index) in enumerate(kf.split(train_data, train_data[labels])):\n        print('[Fold %d\/%d]' % (i + 1, kfold))\n\n        model_path = \"%s_%s.pth\" % (model_path_temp, i)\n\n        X_train, valX = train_data.iloc[train_index], train_data.iloc[test_index]\n        X_train = X_train.loc[X_train['cp_type_ctl'] != 1, :]\n        X_val = valX.loc[valX['cp_type_ctl'] != 1, :]\n        y_train, y_valid = X_train[labels].values, X_val[labels].values\n\n        X_train = make_X(X_train.reset_index(), cont_features, cat_features)\n        X_valid = make_X(X_val.reset_index(), cont_features, cat_features)\n\n\n        train_loader = Loader(X_train, y_train, cat_cols=cat_features, batch_size=128, shuffle=True)\n        val_loader = Loader(X_valid, y_valid, cat_cols=cat_features, batch_size=256, shuffle=False)\n\n        model_temp = model(hidden_dim, emb_dims, len(cont_features)).to(device)\n        print(model_temp)\n        exit()\n        model_temp = model_training(model_temp, train_loader, val_loader, loss_function=loss_function,\n                               epochs=epochs,\n                               lr=lr, patience=patience,\n                               model_path=model_path)\n\n        models.append(model_temp)\n        val_loaders.append(val_loader)\n        val_set.append(valX)\n\n    return models, val_loaders, val_set","c6cf8222":"train, test, FE, cat_feat, labels, uniques = get_data()\n\ndims = [2, 8]\nemb_dims = [(x, y) for x, y in zip(uniques, dims)]\nn_cont = len(FE)\n\nNets = [MoaModel_encoder]\nNet_names = ['MoaModel_encoder']\n\n#Hyperparameters\nhidden_dim = 512\nkfold = 5\nskf = KFold(n_splits=kfold, shuffle=True, random_state=45)\n#skf = MultilabelStratifiedKFold(n_splits=kfold, shuffle=True, random_state=128)\n\nall_models = []\nval_loaders = []\nval_sets =[]\n\nfor model, name in zip(Nets, Net_names):\n    models, val_loader, val_set = fully_train(model=model, train_data = train,\n                                      cont_features = FE, cat_features = cat_feat,\n                                      labels=labels, kf=skf,  loss_function = nn.BCELoss(),\n                                      hidden_dim=hidden_dim, emb_dims=emb_dims,\n                                      epochs=1000, lr=0.01, patience=10,\n                                      kfold=kfold, model_path_temp=name)\n\n    all_models.append(models)\n    val_loaders.append(val_loader)\n    val_sets.append(val_set)\n\n\nscores = []\n\ny_val_avg = []\nfor i in range(len(Nets)):\n    for kf in range(kfold):\n        temp_pred = []\n        temp_y = []\n        with torch.no_grad():\n            for X_cont, X_cat, y in val_loaders[i][kf]:\n                preds, _ = all_models[i][kf](X_cont, X_cat)\n                temp_pred.append(preds)\n                temp_y.append(y)\n\n        y_pred = torch.cat(temp_pred, dim=0).detach().cpu().numpy()\n        #y_true = torch.cat(temp_y, dim=0).detach().cpu().numpy()\n\n        val_temp_set = val_sets[i][kf]\n        y_true = val_temp_set[labels].values\n\n        val_temp_set.loc[val_temp_set['cp_type_ctl'] != 1, labels] = y_pred\n        y_pred = val_temp_set[labels].values\n\n        # plt.plot(np.sort(y_pred.flatten()))\n        # plt.show()\n\n\n        score = 0\n        for k in range(y_true.shape[1]):\n            score_ = log_loss(y_true[:, k], y_pred[:, k].astype(float), labels=[0,1])\n            score += score_ \/ y_true.shape[1]\n\n        print('Fold %s:' % kf, score)\n        scores.append(score)\n\nprint('#'*150)\nprint('CV average:', np.mean(scores))\nprint('CV std:', np.std(scores))\nprint('#'*150)\n\nX_test = make_X(test, FE, cat_feat)\ntest_loader = Loader(X_test, None, cat_cols=cat_feat, batch_size=256, shuffle=False)\n\nfull_test = np.zeros([test.shape[0], 206, len(Nets)*kfold])\nfor i in range(len(Nets)):\n    for kf in range(kfold):\n        temp_pred = []\n        temp_y = []\n        with torch.no_grad():\n            for X_cont, X_cat, y in test_loader:\n                preds, _ = all_models[i][kf](X_cont, X_cat)\n                temp_pred.append(preds)\n                temp_y.append(y)\n\n        full_test[:, :, i*kfold+kf] = torch.cat(temp_pred, dim=0).detach().cpu().numpy()\n\n\n#test = test[labels]\nprint(full_test.shape)\nprint(np.mean(full_test, axis=2).shape)\ntest[labels] = np.mean(full_test, axis=2)\ntest.loc[test['cp_type_ctl']==1, labels]=0\ntest[labels].to_csv('submission.csv')","354f1f66":"**Import libraries**","ca78f1a6":"**Datapreprocess**","6db68ad9":"**RUNNING**","c31c90cd":"**Model training**","3725eb78":"**Early stopping algorithm**","8389897b":"**This notebook applied an autoencoder and embedding layers to generate additional features. Here is model summary:**\n\n> MoaModel_encoder(\n>   (auto_encoder): autoencoder(\n>     (encoder): Sequential(\n>       (0): BatchNorm1d(877, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>       (1): Linear(in_features=877, out_features=256, bias=True)\n>       (2): ReLU(inplace=True)\n>       (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>       (4): Linear(in_features=256, out_features=128, bias=True)\n>       (5): ReLU(inplace=True)\n>       (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>       (7): Linear(in_features=128, out_features=64, bias=True)\n>     )\n>     (decoder): Sequential(\n>       (0): Linear(in_features=64, out_features=128, bias=True)\n>       (1): ReLU(inplace=True)\n>       (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>       (3): Linear(in_features=128, out_features=256, bias=True)\n>       (4): ReLU(inplace=True)\n>       (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>       (6): Linear(in_features=256, out_features=877, bias=True)\n>     )\n>   )\n>   (emb_layers): ModuleList(\n>     (0): Embedding(2, 2)\n>     (1): Embedding(3, 8)\n>   )\n>   (block1): block(\n>     (batch_norm): BatchNorm1d(951, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>     (dropout): Dropout(p=0.25, inplace=False)\n>     (dense): Linear(in_features=951, out_features=512, bias=True)\n>   )\n>   (block2): block(\n>     (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>     (dropout): Dropout(p=0.5, inplace=False)\n>     (dense): Linear(in_features=512, out_features=256, bias=True)\n>   )\n>   (block3): block(\n>     (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>     (dropout): Dropout(p=0.25, inplace=False)\n>     (dense): Linear(in_features=256, out_features=206, bias=True)\n>   )\n> )\n\nThis model is trained in end-to-end framework. The loss function has defined as shown below: \n\n**Loss  = Binary_cross_entropy + 0.5*Mean_squared_error **\n\nMean_squared_error loss is used to train an autoencoder. \nBefore I apply this kind of architecture to highly imbalanced datasets for binary classification problem. Here is our paper: https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S095070512030037X\n","20c3c396":"**DATALOADER**","117f983e":"**MODEL WITH AUTOENCODER**"}}