{"cell_type":{"395fe6ff":"code","2b0f3648":"code","e868cdcf":"code","d5c080b9":"code","998a55ad":"code","9c316d77":"code","dad492b7":"code","9aaafecf":"code","1e3376dd":"code","d23f471a":"code","501536f8":"code","ec77cfb9":"code","b153979a":"code","18afeac7":"code","9b845c62":"code","1ebaf88b":"code","b810acc9":"code","68d4965c":"code","1470f28b":"code","f91e39e6":"code","f29332f2":"code","9a1863b8":"code","1b3039c4":"code","c3413442":"code","45e8c458":"code","d617f403":"code","0342e7e8":"code","db0ed901":"code","6e1f163a":"code","f264b9bc":"code","14ca5892":"code","299d6f5d":"code","d77b29ad":"code","30d5640f":"code","1fa6a93a":"code","23443457":"code","96f69e28":"code","c553a3f8":"code","67eca349":"code","b45776f8":"code","c4f9929d":"code","2b309145":"code","19e8b9cf":"code","c1a3e6ff":"code","e0ea4cfb":"markdown","3f2427d0":"markdown","d2acb4f7":"markdown","b0c538b0":"markdown","7b058656":"markdown","3b50d521":"markdown","3dd06942":"markdown","5c84f5a9":"markdown","4261383f":"markdown","e45d5e98":"markdown","5644e943":"markdown","cbd4d139":"markdown","d9964eec":"markdown","f70eff7b":"markdown","89b0c045":"markdown","efc2a348":"markdown","c9f754bd":"markdown","a1f2d651":"markdown","bba1ac99":"markdown"},"source":{"395fe6ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b0f3648":"train=pd.read_csv('\/kaggle\/input\/iba-ml1-mid-project\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/iba-ml1-mid-project\/test.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/iba-ml1-mid-project\/sample_submission.csv')","e868cdcf":"train['credit_line_utilization'] = train.credit_line_utilization.str.replace(',', '').astype(float)","d5c080b9":"test['credit_line_utilization'] = test.credit_line_utilization.str.replace(',', '').astype(float)","998a55ad":"test['age'].fillna(test['age'].median(),inplace=True)\ntrain['age'].fillna(train['age'].median(),inplace=True)\ntest['number_dependent_family_members'].fillna(test['number_dependent_family_members'].median(),inplace=True) \ntrain['number_dependent_family_members'].fillna(train['number_dependent_family_members'].median(),inplace=True)\ntest['monthly_income'].fillna(test['monthly_income'].median(),inplace=True) \ntrain['monthly_income'].fillna(train['monthly_income'].median(),inplace=True)\ntest['number_of_credit_lines'].fillna(test['number_of_credit_lines'].median(),inplace=True) \ntrain['number_of_credit_lines'].fillna(train['number_of_credit_lines'].median(),inplace=True) \ntest['real_estate_loans'].fillna(test['real_estate_loans'].median(),inplace=True) \ntrain['real_estate_loans'].fillna(train['real_estate_loans'].median(),inplace=True) \ntest['ratio_debt_payment_to_income'].fillna(test['ratio_debt_payment_to_income'].median(),inplace=True) \ntrain['ratio_debt_payment_to_income'].fillna(train['ratio_debt_payment_to_income'].median(),inplace=True)\ntest['number_of_previous_late_payments_up_to_59_days'].fillna(test['number_of_previous_late_payments_up_to_59_days'].median(),inplace=True) \ntrain['number_of_previous_late_payments_up_to_59_days'].fillna(train['number_of_previous_late_payments_up_to_59_days'].median(),inplace=True)\ntest['number_of_previous_late_payments_up_to_59_days'].fillna(test['number_of_previous_late_payments_up_to_59_days'].median(),inplace=True) \ntrain['number_of_previous_late_payments_up_to_89_days'].fillna(train['number_of_previous_late_payments_up_to_89_days'].median(),inplace=True) \ntest['number_of_previous_late_payments_up_to_89_days'].fillna(test['number_of_previous_late_payments_up_to_89_days'].median(),inplace=True) \ntrain['number_of_previous_late_payments_90_days_or_more'].fillna(train['number_of_previous_late_payments_90_days_or_more'].median(),inplace=True) \ntest['number_of_previous_late_payments_90_days_or_more'].fillna(test['number_of_previous_late_payments_90_days_or_more'].median(),inplace=True)\ntrain['credit_line_utilization'].fillna(train['credit_line_utilization'].median(),inplace=True) \ntest['credit_line_utilization'].fillna(test['credit_line_utilization'].median(),inplace=True)","9c316d77":"train.isnull().sum()","dad492b7":"from sklearn.ensemble import IsolationForest\niso=IsolationForest()\noutlier_prediction1=iso.fit_predict(train[['age']])\noutlier_prediction2=iso.fit_predict(train[['number_dependent_family_members']])\noutlier_prediction3=iso.fit_predict(train[['monthly_income']])\noutlier_prediction4=iso.fit_predict(train[['number_of_credit_lines']])\noutlier_prediction5=iso.fit_predict(train[['real_estate_loans']])\noutlier_prediction6=iso.fit_predict(train[['ratio_debt_payment_to_income']])\noutlier_prediction7=iso.fit_predict(train[['credit_line_utilization']])\noutlier_prediction8=iso.fit_predict(train[['number_of_previous_late_payments_up_to_59_days']])\noutlier_prediction9=iso.fit_predict(train[['number_of_previous_late_payments_up_to_89_days']])\noutlier_prediction10=iso.fit_predict(train[['number_of_previous_late_payments_90_days_or_more']])\n\nprint((outlier_prediction1==-1).mean(),(outlier_prediction2==-1).mean(),(outlier_prediction3==-1).mean(),(outlier_prediction4==-1).mean(),(outlier_prediction5==-1).mean(),(outlier_prediction6==-1).mean(),(outlier_prediction7==-1).mean(),(outlier_prediction8==-1).mean(),(outlier_prediction9==-1).mean(),(outlier_prediction10==-1).mean())","9aaafecf":"X=train[['age','number_dependent_family_members','monthly_income','number_of_credit_lines','real_estate_loans','ratio_debt_payment_to_income','number_of_previous_late_payments_up_to_59_days','credit_line_utilization','number_of_previous_late_payments_up_to_89_days','number_of_previous_late_payments_90_days_or_more']]\ny=train['defaulted_on_loan']\n","1e3376dd":"test_s=test.drop(columns=['Id'])","d23f471a":"test_s=test_s[['age', 'number_dependent_family_members', 'monthly_income',\n       'number_of_credit_lines', 'real_estate_loans',\n       'ratio_debt_payment_to_income',\n       'number_of_previous_late_payments_up_to_59_days',\n       'credit_line_utilization',\n       'number_of_previous_late_payments_up_to_89_days',\n       'number_of_previous_late_payments_90_days_or_more']]","501536f8":"#Let's import some libraries\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.metrics import roc_auc_score","ec77cfb9":"X_train, X_test, y_train, y_test = train_test_split(X, y)","b153979a":"model1 = LogisticRegression()\nmodel1.fit(X_train,y_train)\nprediction1=model1.predict_proba(X_test)[0:,1]\nroc_auc_score(y_test, prediction1)","18afeac7":"model2=DecisionTreeClassifier()\nmodel2.fit(X_train,y_train)\nprediction2=model2.predict_proba(X_test)[0:,1]\nprint('The accuracy of the Decision Tree is',roc_auc_score(y_test, prediction2))","9b845c62":"model3=KNeighborsClassifier() \nmodel3.fit(X_train,y_train)\nprediction3=model3.predict_proba(X_test)[0:,1]\nprint('The accuracy of the KNeighborsClassifier is',roc_auc_score(y_test, prediction3))","1ebaf88b":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model3=KNeighborsClassifier(n_neighbors=i) \n    model3.fit(X_train,y_train)\n    prediction3=model3.predict_proba(X_test)[0:,1]\n    a=a.append(pd.Series(roc_auc_score(y_test, prediction3)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","b810acc9":"model4=GaussianNB()\nmodel4.fit(X_train,y_train)\nprediction4=model4.predict_proba(X_test)[0:,1]\nprint('The accuracy of the NaiveBayes is',roc_auc_score(y_test, prediction4))","68d4965c":"model5=RandomForestClassifier()\nmodel5.fit(X_train,y_train)\nprediction5=model5.predict_proba(X_test)[0:,1]\nprint('The accuracy of the Random Forests is',roc_auc_score(y_test, prediction5))","1470f28b":"from sklearn.ensemble import RandomForestClassifier \nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX=train[['age','number_dependent_family_members','monthly_income','number_of_credit_lines','real_estate_loans','ratio_debt_payment_to_income','number_of_previous_late_payments_up_to_59_days','credit_line_utilization','number_of_previous_late_payments_up_to_89_days','number_of_previous_late_payments_90_days_or_more']]\ny=train['defaulted_on_loan']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nclf = Pipeline(steps=[('scaler', StandardScaler()),\n                      ('imputer',SimpleImputer(strategy='median')),\n                      ('classifier', RandomForestClassifier ())])\nclf.fit(X_train,y_train)","f91e39e6":"predics=clf.predict_proba(X_test)[0:,1]","f29332f2":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, predics)","9a1863b8":"clf.get_params().keys()","1b3039c4":"parameters = {\n    'classifier__bootstrap'         : [True],\n    'classifier__n_estimators'      : [620],\n    'classifier__max_depth'         : range(1,7),\n     'classifier__max_features': ['sqrt'],\n    'classifier__min_samples_split': [5],\n    'classifier__min_samples_leaf': [4]\n    \n   \n}","c3413442":"gridsearch = GridSearchCV(clf, parameters, cv=5)","45e8c458":"X_train, X_test, y_train, y_test = train_test_split(X, y)","d617f403":"gridsearch.fit(X_train, y_train)","0342e7e8":"gridsearch.best_params_","db0ed901":"predics=gridsearch.predict_proba(X_test)[0:,1]","6e1f163a":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, predics)","f264b9bc":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(gridsearch,X_test,y_test)","14ca5892":"submissions=gridsearch.predict_proba(test_s)[0:,1]\nsubmissions","299d6f5d":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB())\n                                             ], \n                       voting='soft').fit(X_train,y_train)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(X_test,y_test))\n","d77b29ad":"predic_rbf=ensemble_lin_rbf.predict_proba(X_test)[0:,1]","30d5640f":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, predic_rbf)","1fa6a93a":"ids = submission['Id']\n\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'Id' : ids, 'predicted': submissions })\noutput.to_csv('submission.csv', index=False)","23443457":"'''\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX=train[['age','number_dependent_family_members','monthly_income','number_of_credit_lines','real_estate_loans','ratio_debt_payment_to_income','number_of_previous_late_payments_up_to_59_days','credit_line_utilization','number_of_previous_late_payments_up_to_89_days','number_of_previous_late_payments_90_days_or_more']]\ny=train['defaulted_on_loan']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nclf = Pipeline(steps=[('scaler', StandardScaler()),\n                      ('imputer',SimpleImputer(strategy='median')),\n                       ('polynomial_features', PolynomialFeatures(degree=2, include_bias=False)),\n                      ('classifier', RandomForestClassifier ())])\nclf.fit(X_train,y_train)\n'''","96f69e28":"# predics=clf.predict_proba(X_test)[0:,1]","c553a3f8":"'''\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, predics)\n'''","67eca349":"'''\nparameters = {\n    'classifier__bootstrap'         : [True],\n    'classifier__n_estimators'      : [620],\n    'classifier__max_depth'         : range(1,7),\n    #'classifier__random_state'      : [0],\n    'classifier__max_features': ['sqrt'],\n    'classifier__min_samples_split': [5],\n    'classifier__min_samples_leaf': [4]\n    \n   \n}\n'''","b45776f8":"#gridsearch = GridSearchCV(clf, parameters, cv=5)","c4f9929d":"#X_train, X_test, y_train, y_test = train_test_split(X, y)","2b309145":"#gridsearch.fit(X_train, y_train)","19e8b9cf":"# predics=gridsearch.predict_proba(X_test)[0:,1]","c1a3e6ff":"'''\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, predics)\n'''","e0ea4cfb":"<img src=\"https:\/\/media.giphy.com\/media\/l2JefZcpHZEVWOPAc\/giphy.gif\">","3f2427d0":"16708 is the number of correct predictions that an instance is negative,\n58 is the number of incorrect predictions that an instance is positive,\n1162 is the number of incorrect of predictions that an instance negative, and\n113 is the number of correct predictions that an instance is positive.\n","d2acb4f7":"# >                                                  Decision Tree Classifier","b0c538b0":"We have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a customer defaulted on loan or not. So now we will predict the whether the customer will fail to pay loans or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\n\n1)Logistic Regression\n\n2)Support Vector Machines\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n","7b058656":"After some preprocessing steps let's start doing prediction part.","3b50d521":"# >        NaiveBayes","3dd06942":"We should convert credit_line_utilization column from string to float as it is represented as numeric values.","5c84f5a9":"Let's check if we have outliers. If any column will have more than 50% of outlier we should handle with it.","4261383f":"As a result of using polynomial features our score is getting worse. So that we will not use polynomial features.","e45d5e98":"Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. The default value is 5. Lets check the accuracies over various values of n_neighbours","5644e943":"# >                                             Logistic Regression                                                                 ","cbd4d139":"# > RandomForest","d9964eec":"First of all we should handle with null values as wee see in our EDA notebook. We are going to use some imputation techniques","f70eff7b":"# $$ \\color{green}{Preprocessing\\ Steps\\ \\ } $$","89b0c045":"Let's visualize our prediction","efc2a348":"As we see we get better result with RandomForestClassifier. But the accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as model variance.\n\nTo overcome this and get a generalized model,we use Cross Validation.","c9f754bd":"# >                                     KNeighborsClassifier                                                    ","a1f2d651":"# $$ \\color{purple}{Predictive\\ Modeling\\ \\ } $$","bba1ac99":"**Ensembling**\n\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model."}}