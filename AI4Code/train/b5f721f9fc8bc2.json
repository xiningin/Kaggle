{"cell_type":{"f4b7be54":"code","8026a87c":"code","44f5aec7":"code","d2637885":"code","ac8bb745":"code","b3cb4c70":"code","2dbb45a1":"code","93b5c8d7":"code","78f70a8d":"code","25b478dd":"code","be227dc1":"code","c1c25bae":"code","d9f166e5":"code","c3ef4d3a":"code","098468c6":"code","67b0c506":"code","94483572":"code","ba9703ee":"code","996012fc":"code","b9c6d0e9":"code","f8ff3586":"code","862213d0":"code","c6a49d36":"code","49f6ae90":"code","63b32bbb":"code","76b9946c":"code","70d32776":"code","2cfd9414":"code","1a50997d":"code","299adec0":"code","f474b833":"code","a8d0c4be":"code","877c8b08":"code","b4a7f1d0":"code","7b5a8c2b":"code","993a0abd":"code","3c741042":"code","63c2565e":"code","34485a99":"code","d82a5de1":"code","6cc8b42c":"code","defcfdb0":"code","c89edd0a":"code","dfeee90e":"code","94342d23":"code","126cce24":"code","8ecd8ca4":"code","bc15ceff":"code","8c7ceb01":"code","0dcdef51":"code","6f81e50c":"code","f2a01fe4":"markdown","90180496":"markdown","f6d35241":"markdown","33ee157c":"markdown","4687a367":"markdown","ae20964c":"markdown","964d7cfd":"markdown","0d542f5c":"markdown","fe8e6265":"markdown","aeeed9f6":"markdown","52da01f6":"markdown"},"source":{"f4b7be54":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns","8026a87c":"df=pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')","44f5aec7":"print(\"*** First five rows of the dataset ***\")\nprint()\ndf.head()","d2637885":"df=df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n\ndf=df.rename(columns={\"v1\":\"class\",\"v2\":\"text\"})","ac8bb745":"print(\"*** Needed columns of the dataset ***\")\nprint()\ndf.columns","b3cb4c70":"print(\"****Value counts of classes in dataset ****\")\nprint()\ndf['class'].value_counts()","2dbb45a1":"print(\"**** Basic info about dataset ****\")\nprint()\ndf.info()","93b5c8d7":"import re\nimport string\n\ndef preprocess(text):\n    #Converting text into lowercase\n    text = str(text).lower()\n    \n    #Removing square brackets from the text\n    text = re.sub('\\[.*?\\]','',text)\n    \n    \n    #Removing links starting with (https or www)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    #Removing <\"text\"> type of text \n    text = re.sub('<.*?>+','',text)\n    \n    #Removing punctuations\n    text = re.sub(\"[%s]\" % re.escape(string.punctuation),'',text)\n    \n    #Removing new lines\n    text = re.sub(\"\\n\",'',text)\n    \n    #Removing alphanumeric numbers \n    text = re.sub('\\w*\\d\\w*','',text)\n    \n    return(text)\n\n\ndf['text']=df['text'].apply(preprocess)","78f70a8d":"print(\"*** First Five of Rows of Cleaned Text ***\")\n\nfor i in range(0,4):\n    print(\"Cleaned text \"+str(i)+\": \",df['text'][i])","25b478dd":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet=WordNetLemmatizer()\n\ndef stopwords_remove(text):\n    text=text.split()\n    text=[wordnet.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]\n    text=\" \".join(text)\n    return(text)\n\n\ndf['text']=df['text'].apply(stopwords_remove)\n    ","be227dc1":"print(\"*** Text after lemmatizing and removing stopwords ***\")\n\nfor i in range(0,4):\n    print(\"After lemmatizing \"+str(i)+\": \",df['text'][i])","c1c25bae":"df.head()","d9f166e5":"#Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf['class']=le.fit_transform(df['class'])","c3ef4d3a":"#Checking if something null or not\ndf.isnull().sum()","098468c6":"#importing necessary libraries\nimport nltk\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (Embedding , Flatten , Dense ,LSTM , Bidirectional , MaxPooling1D , \nConv1D , GlobalMaxPool1D , BatchNormalization,Dropout , GRU)\nfrom tensorflow.keras.callbacks import ModelCheckpoint , ReduceLROnPlateau","67b0c506":"#Tokenizing each sentence and storing it into the list\nsentence=list()\nfor i in df['text']:\n    sentence.append(nltk.word_tokenize(i))","94483572":"print(\"**** Number of sentences are **** \", len(sentence))","ba9703ee":"print(\"**** First ten tokenized sentences *****\")\nprint()\nprint(sentence[0:10])","996012fc":"#Getting word embeddings using Word2Vec\nmodel_w2c=Word2Vec(sentence,vector_size=100,window=5,workers=4,min_count=2)\nwords=len(model_w2c.wv)\n","b9c6d0e9":"print(\"**** Number of words we get embedding of ****\", words)","f8ff3586":"#Storing text file of our embeddings\nfilename = 'embedding_word2vec.txt'\nmodel_w2c.wv.save_word2vec_format(filename, binary=False)","862213d0":"#Splitting the dataset\nfrom sklearn.model_selection import train_test_split\n\ntexts=df['text']\ncat=df['class']\n\n\nxtrain,xtest,ytrain,ytest=train_test_split(texts,cat,random_state=42,test_size=0.25)","c6a49d36":"print(\"**** Shape of xtrain is ****\",xtrain.shape)\nprint(\"**** Shape of xtest is ****\",xtest.shape)\nprint(\"**** Shape of ytrain is ****\",ytrain.shape)\nprint(\"**** Shape of ytest is ****\",ytest.shape)","49f6ae90":"print(\"**** Value count of training data ****\")\nprint(ytrain.value_counts())\nprint()\nprint(\"**** Value count of test data ****\")\nprint(ytest.value_counts())","63b32bbb":"#Tokenizing training data\n\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(xtrain)\n\nencoded_docs=tokenizer.texts_to_sequences(xtrain)\n\nmax_length=max([len(s) for s in sentence])\nx_train=pad_sequences(encoded_docs,maxlen=max_length,padding='post')\ny_train=np.array(ytrain)\n\nvocab_size= len(tokenizer.word_index)+1\nprint(\"*** Vocab size of texts is ***\", vocab_size)","76b9946c":"#Tokenizing test data\n\n#tokenizer_test=Tokenizer()\ntokenizer.fit_on_texts(xtest)\n\nencoded_docs=tokenizer.texts_to_sequences(xtest)\n\n#max_length_test=len(max(encoded_docs_test , key=len))\n\nx_test=pad_sequences(encoded_docs,maxlen=max_length,padding='post')\ny_test=np.array(ytest)\n\nvocab_size= len(tokenizer.word_index)+1\nprint(\"*** Vocab size of texts is ***\", vocab_size)","70d32776":"def load_embedding(filename):\n    file=open(filename,'r')\n    \n    lines=file.readlines()[1:]\n    #print(lines)\n    embedding=dict()\n    for line in lines:\n        parts=line.split()\n        \n        embedding[parts[0]]=np.asarray(parts[1:],dtype='float32')\n        \n    return(embedding)\n        \n    \ndef get_weight_matrix(vocab,raw_embedding):\n    vocab_size=len(vocab)+1\n    \n    weight_matrix=np.zeros((vocab_size,100))\n    \n    for word , i in vocab.items():\n        vector=raw_embedding.get(word)\n        if(vector is not None):\n            #print(vector)\n            weight_matrix[i]=vector\n        \n    return(weight_matrix)\n","2cfd9414":"#getting embedding from word2vec\nraw_embedding=load_embedding(\"embedding_word2vec.txt\")    \nembedding_vectors=get_weight_matrix(tokenizer.word_index,raw_embedding)\n","1a50997d":"print(\"**** Vocabulary Size **** \",vocab_size)","299adec0":"#Embedding layer\nembedding_layer=Embedding(vocab_size,100,input_length=max_length,trainable=False,weights=[embedding_vectors])","f474b833":"'''model=Sequential()\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(LSTM(units=128,return_sequences=True))) #max_length\nmodel.add(GlobalMaxPool1D())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(max_length, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(max_length, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation = 'sigmoid'))\n          \nmodel.summary()'''\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())\n\n\n'''model = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(embedding_layer)\n#LSTM \nmodel.add(Bidirectional(LSTM(units=128, return_sequences = True)))\nmodel.add(Bidirectional(GRU(units=32)))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()'''","a8d0c4be":"from tensorflow.keras import optimizers","877c8b08":"optimum=optimizers.Adam(clipvalue=0.5)\n#model.compile(loss='binary_crossentropy', optimizer=optimum , metrics=['accuracy'])\nmodel.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\n\nhistory=model.fit(x_train,y_train,epochs=10,verbose=1,validation_data=(x_test,y_test),batch_size=32,callbacks=[checkpoint,reduce_lr])","b4a7f1d0":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","7b5a8c2b":"epochs=[i for i in range(10)]\nfig,ax=plt.subplots(1,2)\n\ntrain_acc= history.history[\"accuracy\"]\ntrain_loss= history.history[\"loss\"]\nval_acc=history.history[\"val_accuracy\"]\nval_loss=history.history[\"val_loss\"]\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","993a0abd":"pred = model.predict_classes(x_test)","3c741042":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred, target_names = ['Not Spam','Spam']))","63c2565e":"def load_embedding_glove(filename):\n    file=open(filename,'r')\n    \n    lines=file.readlines()\n    #print(lines)\n    embedding=dict()\n    for line in lines:\n        parts=line.split()\n        \n        embedding[parts[0]]=np.asarray(parts[1:],dtype='float32')\n        \n    return(embedding)\n\n\ndef get_weight_matrix_glove(vocab,raw_embedding):\n    vocab_size=len(vocab)+1\n    \n    weight_matrix=np.zeros((vocab_size,100))\n    \n    for word , i in vocab.items():\n        vector=raw_embedding.get(word)\n        if(vector is not None):\n            #print(vector)\n            weight_matrix[i]=vector\n        \n    return(weight_matrix)\n\n        ","34485a99":"#Getting pretrained glove embeddings\nraw_embedding_glove=load_embedding_glove(\"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\")    \nembedding_vectors_glove=get_weight_matrix_glove(tokenizer.word_index,raw_embedding_glove)","d82a5de1":"embedding_vectors_glove","6cc8b42c":"embedding_layer1=Embedding(vocab_size,100,input_length=max_length,trainable=False,weights=[embedding_vectors_glove])\n\n'''model1=Sequential()\nmodel1.add(embedding_layer1)\nmodel1.add(Bidirectional(LSTM(max_length,return_sequences=True,recurrent_dropout=0.2)))\nmodel1.add(GlobalMaxPool1D())\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(max_length, activation = \"relu\"))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(max_length, activation = \"relu\"))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(1, activation = 'sigmoid'))\n          \nmodel1.summary()'''\n\nmodel1 = Sequential()\nmodel1.add(embedding_layer1)\nmodel1.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(MaxPooling1D(pool_size=2))\nmodel1.add(Flatten())\nmodel1.add(Dense(1, activation='sigmoid'))\nprint(model1.summary())\n\n\n\n\n'''model1 = Sequential()\n#Non-trainable embeddidng layer\nmodel1.add(embedding_layer)\n#LSTM \nmodel1.add(Bidirectional(LSTM(units=128, return_sequences = True)))\nmodel1.add(Bidirectional(GRU(units=32)))\nmodel1.add(Dense(1, activation='sigmoid'))\n\nmodel1.summary()'''\n","defcfdb0":"model1.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_acc', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\n\nhistory1=model1.fit(x_train,y_train,epochs=10,verbose=1,validation_data=(x_test,y_test),batch_size=32,callbacks=[checkpoint,reduce_lr])","c89edd0a":"print(\"Accuracy of the model on Testing Data is with glove embeddings - \" , model1.evaluate(x_test,y_test)[1]*100 , \"%\")","dfeee90e":"epochs=[i for i in range(10)]\nfig,ax=plt.subplots(1,2)\n\ntrain_acc= history1.history[\"accuracy\"]\ntrain_loss= history1.history[\"loss\"]\nval_acc=history1.history[\"val_accuracy\"]\nval_loss=history1.history[\"val_loss\"]\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()\n","94342d23":"pred1 = model1.predict_classes(x_test)","126cce24":"print(classification_report(y_test, pred1, target_names = ['Not Spam','Spam']))","8ecd8ca4":"embedding_layer2=Embedding(vocab_size,100,input_length=max_length,trainable=False)# weights=[embedding_vectors_glove])\n\n'''model2=Sequential()\nmodel2.add(embedding_layer2)\nmodel2.add(Bidirectional(LSTM(max_length,return_sequences=True,recurrent_dropout=0.2)))\nmodel2.add(GlobalMaxPool1D())\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(max_length, activation = \"relu\"))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(max_length, activation = \"relu\"))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(1, activation = 'sigmoid'))\n          \nmodel2.summary()'''\n\n\nmodel2 = Sequential()\nmodel2.add(embedding_layer2)\nmodel2.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(MaxPooling1D(pool_size=2))\nmodel2.add(Flatten())\nmodel2.add(Dense(1, activation='sigmoid'))\nprint(model2.summary())\n\n","bc15ceff":"model2.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_acc', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\n\nhistory2=model2.fit(x_train,y_train,epochs=10,verbose=1,validation_data=(x_test,y_test),batch_size=32,callbacks=[checkpoint,reduce_lr])","8c7ceb01":"epochs=[i for i in range(10)]\nfig,ax=plt.subplots(1,2)\n\ntrain_acc= history2.history[\"accuracy\"]\ntrain_loss= history2.history[\"loss\"]\nval_acc=history2.history[\"val_accuracy\"]\nval_loss=history2.history[\"val_loss\"]\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()\n","0dcdef51":"pred2 = model2.predict_classes(x_test)","6f81e50c":"print(classification_report(y_test, pred2, target_names = ['Not Spam','Spam']))","f2a01fe4":"<a id=\"3\"><\/a>\n## **Modelling \u270c**","90180496":"<a id=\"3.3\"><\/a>\n### **Without Using Pretrained embeddings**","f6d35241":"# **Spam Classifier**\n\n**Table of Contents \u23e9**\n\n[1) Basic Overview \ud83d\udcfa](#1)\n\n[2) Preprocessing \ud83d\udcca](#2)\n\n   * [2.1) Data Cleaning \ud83e\uddf9](#2.1)\n   \n   * [2.2) Removing Stopwords and Lemmatizing \ud83d\uded1](#2.2)\n   \n   \n[3) Modelling \u270c](#3)\n\n*    [3.1) Pretrained embedding using Word2Vec](#3.1)\n\n*    [3.2) Pretrained embedding using GLove](#3.2)\n\n*    [3.3) Without using pretrained embedding](#3.3) ","33ee157c":"<a id=\"3.2\"><\/a>\n#### **Getting embedding using word2vec model**","4687a367":"# **NOTE**\n\n**Kagglers!!! , If you want to learn about the basic approach of this problem using Countvectorizer and TfidfVectorizer , Please have a look at this notebook of mine**\n\n**Spam CLassifier(Basic approach)** : https:\/\/www.kaggle.com\/aryanml007\/spam-or-ham-sms-classifier-basic-approach","ae20964c":"<a id=\"1\"><\/a>\n## **Basic Overview \ud83d\udcfa**","964d7cfd":"<a id=\"2\"><\/a>\n## **Preprocessing \ud83d\udcca**","0d542f5c":"<a id=\"3.2\"><\/a>\n### **Using pretrained Glove embbedings**","fe8e6265":"<a id=\"2.2\"><\/a>\n### Removing Stopwords \ud83d\uded1","aeeed9f6":"<a id=\"2.1\"><\/a>\n### Data Cleaning \ud83e\uddf9","52da01f6":"**THANK YOU BEING PATIENT AND SCROLL THIS DOWN INTO THIS NOTEBOOK \ud83d\udc9a**\n\nIf you like my work please give it a upvote and any feedbacks\/suggestions\/mistakes is appreciated , I will update this \n\nNew to NLP and doing my hands dirty with deep learning approach \n\nI promised to publish deep learning approach in my previous notebook , I kept my promise \ud83d\ude01\n\nProvided the link of my previous notebook above\n\nMade with LOVE\u2764"}}