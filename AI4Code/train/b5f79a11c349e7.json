{"cell_type":{"46c1666b":"code","ee0701fb":"code","da689290":"code","dc570be0":"code","c87d9596":"code","6aee0f65":"code","559851be":"code","e97a69d5":"code","cbc4cd8e":"code","1e9e386a":"code","cd9e881e":"code","46eddbc3":"markdown","01216df7":"markdown","2116e099":"markdown","594a6b07":"markdown","25ad8dfb":"markdown"},"source":{"46c1666b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","ee0701fb":"# Load Data\ntrain = pd.read_csv('..\/input\/covid19-global-forecasting-week-3\/train.csv')\ntest = pd.read_csv('..\/input\/covid19-global-forecasting-week-3\/test.csv')\n\ntrain.rename(columns={\n        'Id': 'id',\n        'Date': 'date',\n        'Province_State':'state',\n        'Country_Region':'country',\n        'ConfirmedCases': 'confirmed',\n        'Fatalities':'deaths',\n        }, inplace=True)\n\ntest.rename(columns={\n        'ForecastId': 'id',\n        'Date': 'date',\n        'Province_State':'state',\n        'Country_Region':'country',\n        }, inplace=True)\n\n\n\nvalid = train[train['date'] >= test['date'].min()]\ntrain = train[(train['date'] < test['date'].min())]\n\nvalid['date'] = pd.to_datetime(valid['date'])\ntrain['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])\n\ntrain['date'] = (train['date'] - pd.Timestamp('2020-03-01')).dt.days\nvalid['date'] = (valid['date'] - pd.Timestamp('2020-03-01')).dt.days\ntest['date']  = (test['date'] - pd.Timestamp('2020-03-01')).dt.days\n\ntrain['loc'] = train['country'].astype(str) + '-' + train['state'].astype(str)\nvalid['loc'] = valid['country'].astype(str) + '-' + valid['state'].astype(str)\ntest['loc'] = test['country'].astype(str) + '-' + test['state'].astype(str)","da689290":"def get_order(max_order):\n    RMSE_order={} #Dict for the key:value pairs of order and RMSE\n    for order in range(1,max_order,1):    #the maximum order for the optimization is set here\n        z = np.polyfit(X_train_.values, y_train_.values, order)\n        pf = np.poly1d(z)\n        \n        y_preds_ = np.round(X_valid_.apply(pf)).clip(lower=y_linear)\n\n        predictions[coords] = y_preds_\n\n        RMSE_get_order=np.sqrt(np.sum(np.square(y_preds_-y_valid_)))\n        RMSE_order[order]=RMSE_get_order\n        result = min(RMSE_order, key=RMSE_order.get)\n    return result","dc570be0":"all_coords = train['loc'].unique().tolist()\npredictions = dict()\nRMSE = dict()\ntotal_RMSE = 0\nfit_order=1 # If get_order fails the fit_order is set to\nall_orders_used=[]\n#This is for visualising the data. As there are 306 datasets only a fraction of 20 sets can be choosen here\n_, ax = plt.subplots(10,2, figsize=(15, 50))\nax = ax.flatten()\n\nfor k,coords in tqdm(enumerate(all_coords[200:220])):   #Define the part of the dataset you want to look at\n    X_train_ = train[train['loc']==coords]['date']#.values.reshape(-1,1)\n    y_train_ = train[train['loc']==coords]['confirmed']#.values.reshape(-1,1)\n    \n    X_valid_ = valid[valid['loc']==coords]['date']#.values.reshape(-1,1)\n    y_valid_ = valid[valid['loc']==coords]['confirmed']#.values.reshape(-1,1)\n    \n    last_diff = y_train_.iloc[-1] - y_train_.iloc[-2]\n    y_linear = y_train_.iloc[-1] + last_diff*np.arange(1,len(X_valid_)+1,1)\n    \n    fit_order=get_order(11) # Here the order up to the maximum order is optimized to the data\n    all_orders_used.append(fit_order)\n    \n    z = np.polyfit(X_train_.values, y_train_.values, fit_order)\n    pf = np.poly1d(z)\n        \n    y_preds_ = np.round(X_valid_.apply(pf)).clip(lower=y_linear)\n    \n    predictions[coords] = y_preds_\n    RMSE[coords]=np.sqrt(np.sum(np.square(y_preds_-y_valid_)))\n    total_RMSE += np.sqrt(np.sum(np.square(y_preds_-y_valid_)))\n\n    sns.lineplot(x=valid[valid['loc']==coords]['date'], y=valid[valid['loc']==coords]['confirmed'], label='y-valid',ax=ax[k])\n    sns.lineplot(x=train[train['loc']==coords]['date'], y=train[train['loc']==coords]['confirmed'], label='y-train',ax=ax[k])\n    sns.lineplot(x=valid[valid['loc']==coords]['date'], y=y_preds_, label='y-preds',ax=ax[k])\n    ax[k].set_title(f'Confirmed cases: ({coords})')\n\n\nprint(total_RMSE)\nprint(all_orders_used)","c87d9596":"all_coords = train['loc'].unique().tolist()\npredictions = dict()\nRMSE = dict()\ntotal_RMSE = 0\nfit_order=1 # If get_order fails the fit_order is set to\nall_orders_used=[]\n\n#This is for visualising the data. As there are 306 datasets only a fraction of 20 sets can be choosen here\n_, ax = plt.subplots(10,2, figsize=(15, 50))\nax = ax.flatten()\n\nfor k,coords in tqdm(enumerate(all_coords[260:280])): #Define the part of the dataset you want to look at\n    \n    X_train_ = train[train['loc']==coords]['date']#.values.reshape(-1,1)\n    y_train_ = train[train['loc']==coords]['deaths']#.values.reshape(-1,1)\n    \n    X_valid_ = valid[valid['loc']==coords]['date']#.values.reshape(-1,1)\n    y_valid_ = valid[valid['loc']==coords]['deaths']#.values.reshape(-1,1)\n    \n    last_diff = y_train_.iloc[-1] - y_train_.iloc[-2]\n    y_linear = y_train_.iloc[-1] + last_diff*np.arange(1,len(X_valid_)+1,1)\n    \n    fit_order=get_order(11)\n    all_orders_used.append(fit_order)\n\n    z = np.polyfit(X_train_.values, y_train_.values, fit_order)\n    pf = np.poly1d(z)\n        \n    y_preds_ = np.round(X_valid_.apply(pf)).clip(lower=y_linear)\n    \n    predictions[coords] = y_preds_\n    RMSE[coords]=np.sqrt(np.sum(np.square(y_preds_-y_valid_)))\n    total_RMSE += np.sqrt(np.sum(np.square(y_preds_-y_valid_)))\n\n    sns.lineplot(x=valid[valid['loc']==coords]['date'], y=valid[valid['loc']==coords]['deaths'], label='y-valid',ax=ax[k])\n    sns.lineplot(x=train[train['loc']==coords]['date'], y=train[train['loc']==coords]['deaths'], label='y-train',ax=ax[k])\n    sns.lineplot(x=valid[valid['loc']==coords]['date'], y=y_preds_, label='y-preds',ax=ax[k])\n    ax[k].set_title(f'Fatalities: ({coords})')\n    \nprint(total_RMSE)\nprint(all_orders_used)","6aee0f65":"submission = pd.DataFrame()\nsubmission['loc'] = test['loc']\nsubmission.reset_index(inplace=True)\n\nsubmission['ConfirmedCases'] = 0\nsubmission['Fatalities'] = 0","559851be":"all_coords = train['loc'].unique().tolist()\npredictions = dict()\nRMSE = dict()\ntotal_RMSE = 0\nfit_order=1\n\nfor coords in all_coords:\n    \n    X_train_ = train[train['loc']==coords]['date']#.values.reshape(-1,1)\n    y_train_ = train[train['loc']==coords]['confirmed']#.values.reshape(-1,1)\n    \n    X_valid_ = valid[valid['loc']==coords]['date']#.values.reshape(-1,1)\n    y_valid_ = valid[valid['loc']==coords]['confirmed']#.values.reshape(-1,1)\n    \n    X_test_ = test[test['loc']==coords]['date']#.values.reshape(-1,1)\n    \n    last_diff = y_train_.iloc[-1] - y_train_.iloc[-2]\n    y_linear = y_train_.iloc[-1] + last_diff*np.arange(1,len(X_valid_)+1,1)\n    \n    fit_order=get_order(11)\n    \n    z = np.polyfit(X_train_.values, y_train_.values, fit_order)\n    pf = np.poly1d(z)\n    \n    y_linear = y_train_.iloc[-1] + last_diff*np.arange(1,len(X_test_)+1,1)\n        \n    y_preds_ = np.round(X_test_.apply(pf)).clip(lower=y_linear)\n    \n\n    submission.loc[submission['loc']==coords,'ConfirmedCases'] = y_preds_","e97a69d5":"all_coords = train['loc'].unique().tolist()\npredictions = dict()\nRMSE = dict()\ntotal_RMSE = 0\n\nfit_grade=1\n\nfor coords in all_coords:\n    \n    X_train_ = train[train['loc']==coords]['date']#.values.reshape(-1,1)\n    y_train_ = train[train['loc']==coords]['deaths']#.values.reshape(-1,1)\n    \n    X_valid_ = valid[valid['loc']==coords]['date']#.values.reshape(-1,1)\n    y_valid_ = valid[valid['loc']==coords]['deaths']#.values.reshape(-1,1)\n    \n    X_test_ = test[test['loc']==coords]['date']#.values.reshape(-1,1)\n    \n    last_diff = y_train_.iloc[-1] - y_train_.iloc[-2]\n    y_linear = y_train_.iloc[-1] + last_diff*np.arange(1,len(X_valid_)+1,1)\n    \n    fit_order=get_order(13)\n\n    z = np.polyfit(X_train_.values, y_train_.values, fit_order)\n    pf = np.poly1d(z)\n        \n    y_linear = y_train_.iloc[-1] + last_diff*np.arange(1,len(X_test_)+1,1)\n    \n    y_preds_ = np.round(X_test_.apply(pf)).clip(lower=y_linear)\n    \n\n    \n    submission.loc[submission['loc']==coords,'Fatalities'] = y_preds_","cbc4cd8e":"submission.drop('loc', axis=1, inplace=True)\nsubmission['index'] = submission['index'] + 1\nsubmission.rename(columns={\n    'index' : 'ForecastId'}, inplace=True)","1e9e386a":"submission","cd9e881e":"submission.to_csv('submission.csv', index=False)","46eddbc3":"The result shows that many different orders are used for the best polynomial fit.\nBut a higher order than 10 does not decrease the RMSE significantly","01216df7":"As I'm new to this topic I need a point to start somewhere. I found the notebook of Fran L\u00f3pez Guzm\u00e1n very useful, https:\/\/www.kaggle.com\/franlopezguzman\/covid19-3-minimalist-polynomial-regressor.  In principle I follow his code but I asked myself if it might be possible to visualize the polynomial fit and the data. Therefore I used seaborn. I saw that the validation data do not always can be fitted with a polynomial of a fixed order. From this I tried to derive the best fitting order of the polynomial by minimizing the RMSE for every country. I highlight the relevant changes I made to the original notebook .... I hope this is ok ...","2116e099":"Playing with the model and getting an idea of what's happening ...\n\n**Beginning with confirmed cases**","594a6b07":"**... and the fatalities**","25ad8dfb":"To get an optimized order of the polynomial I defined the function get_order. This function can be added in the model"}}