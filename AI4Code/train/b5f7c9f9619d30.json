{"cell_type":{"f42bf22f":"code","414c165d":"code","c5fac7e9":"code","7c504790":"code","3c6dd846":"code","b7c7ef2e":"code","659ce804":"code","ada5daf9":"code","9853f5b3":"code","81358e4b":"code","e8bebe92":"code","378b9ecd":"code","159a0013":"code","4d66ddee":"code","ef60c2d1":"code","f9af5b10":"code","9cd95196":"code","7a84a698":"code","3ce68ad8":"code","4f5e7347":"code","07446010":"code","91821458":"code","bb4f7a46":"code","965b92f3":"code","dda914a5":"code","c17a007b":"code","f2bf43eb":"code","27006a1d":"code","f5d810f4":"code","30ccd642":"code","2f7f9aab":"code","c019d26e":"code","10c383ac":"code","f38ddd58":"code","1bdeb818":"code","90ce7969":"code","9b2fbd76":"code","8596168d":"code","f9d5b931":"code","b6a8cf2b":"code","aaa66186":"code","71500915":"code","b4a597dc":"code","cae334ed":"code","8c06320f":"code","1b07bf29":"code","e4bddf72":"code","35a7c130":"code","5eea2c93":"code","6320d922":"code","c16fffb5":"code","fa2e46a7":"code","8aeb6fb1":"code","faf7e254":"code","6cb371cf":"code","d44e96ad":"code","6a41a52e":"code","fe3248b4":"code","bf3145bd":"code","2a4724ad":"code","23a2295a":"code","1fbd4780":"code","91204da8":"code","27140e78":"code","82f2cf3a":"code","2c2f64d2":"code","07060dd5":"code","4d5ca840":"code","092aca02":"code","6d4f3a2b":"code","01ce52f4":"code","f5a16c62":"code","e900d3c1":"code","47def78e":"code","ee297738":"code","d65d730c":"code","63d2827b":"code","8d50cf6e":"code","a12a1788":"code","e2c718ac":"code","1d7522ec":"code","d82dad01":"code","1e77e8c7":"code","20f17fe9":"code","43b29f51":"code","e8b93937":"code","7fb2f50d":"code","5beb704a":"code","da668558":"code","b3221913":"code","14cb1bda":"code","037dedf3":"code","fae9aa85":"code","f4417a84":"code","c14ee58f":"code","4448ccf2":"code","28f4c22c":"code","2dc29956":"code","3a74838f":"code","2fe8c052":"code","14ca710d":"code","27d7a6e1":"code","d0f66357":"code","712cc8f2":"code","b6897b67":"code","a8aaef7d":"code","c7f95da1":"code","d2a6635a":"code","21c2cd49":"code","5caa966a":"code","3d4ce318":"code","b2ba1f39":"code","4d320018":"code","b35ca35f":"code","62d06aca":"code","1228ddfb":"code","701544a7":"code","ed088e26":"code","b6801b51":"code","3f70aacd":"code","ae2bf22d":"code","47935e4b":"code","97135aea":"code","48232600":"code","79f18e1b":"code","982d36c6":"code","778ec55a":"code","9a93fa6b":"code","b69215c6":"code","b5c5f540":"code","c2999de8":"code","510870b7":"code","e5cdf53f":"code","7bc3b7f1":"code","1727d0e1":"code","c6517403":"code","319bb7d9":"code","54ecd599":"code","ecc12550":"code","d18be3a9":"code","cdc38944":"code","ce7fb19c":"markdown","e3f8f814":"markdown","5ba81d59":"markdown","0829fafb":"markdown","82f3f930":"markdown","cf51590e":"markdown","4c268d6f":"markdown","af159255":"markdown","55d0c222":"markdown","d726b8bb":"markdown","b58f0d53":"markdown","17eb8cd9":"markdown","961a88db":"markdown","dd17122e":"markdown","543db3af":"markdown","4759a436":"markdown","ac92302d":"markdown","1ea620e0":"markdown","a08f2ae1":"markdown","147488c6":"markdown","7f906f0e":"markdown","a7b25b6a":"markdown","04fd8ee7":"markdown","a7100b7c":"markdown","49d1dc07":"markdown","4ca38128":"markdown","8cd3c5e0":"markdown","42a63e3f":"markdown","55bcb442":"markdown","76fd3bcd":"markdown","680eef54":"markdown","abdb40fb":"markdown","99111137":"markdown","f45cf674":"markdown","f3a4e7e0":"markdown","bb526111":"markdown","f425c420":"markdown","fcf54594":"markdown","05d9f873":"markdown"},"source":{"f42bf22f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option('display.max_rows',3000)\npd.set_option('display.max_columns',3000)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","414c165d":"# Data Reading\nchurn_data = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","c5fac7e9":"churn_data.head()","7c504790":"# Data Inspection\nchurn_data.info()","3c6dd846":"# Shape of the dataset\nchurn_data.shape","b7c7ef2e":"# Statistical aspects of the numerical columns of the dataset\nchurn_data.describe()","659ce804":"# Observing the data in the dataset to search for binary variables\nchurn_data.head()","ada5daf9":"# Converting binary variables (Yes\/No) to 0 or 1","9853f5b3":"varlist = ['Partner','Dependents','PhoneService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']","81358e4b":"# function to map binary columns\n\ndef binary_mapping(x):\n    return x.map({'Yes':1,'No':0})","e8bebe92":"churn_data[varlist]=churn_data[varlist].apply(binary_mapping)","378b9ecd":"churn_data.head()","159a0013":"# Creating dummy variables with some of the categorical variables with multiple levels\ndummy1=pd.get_dummies(churn_data[['Contract','PaymentMethod','gender','InternetService']], drop_first=True)","4d66ddee":"churn_data = pd.concat([churn_data,dummy1],axis=1)","ef60c2d1":"churn_data.head()","f9af5b10":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n\nml = pd.get_dummies(churn_data['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n#Adding the results to the master dataframe\nchurn_data = pd.concat([churn_data,ml1], axis=1)\n","9cd95196":"churn_data.head()","7a84a698":"# Dropping the redundant variables\nchurn_data=churn_data.drop(['Contract','PaymentMethod','gender','InternetService','MultipleLines'],1)","3ce68ad8":"churn_data.head()","4f5e7347":"# Observing the data types of the variables\nchurn_data.info()","07446010":"# Converting TotalCharges into float as it is a numerical column\nchurn_data['TotalCharges'] = pd.to_numeric(churn_data.TotalCharges, errors='coerce')\n","91821458":"# Observing the change\nchurn_data.info()","bb4f7a46":"# Checking for outliers in numerical columns\nnum_churn_data=churn_data[['tenure','MonthlyCharges','TotalCharges']]","965b92f3":"# Checking whether the numbers are gradually increasing\nnum_churn_data.describe(percentiles=[0.25,0.50,0.75,0.90,0.95,0.99])","dda914a5":"# Checking for missing values\nchurn_data.isnull().sum()","c17a007b":"# Checking the percentage of missing values\nround(100*(churn_data.isnull().sum()\/len(churn_data.index)),2)","f2bf43eb":"# Checking the rows against the missing values of Online Security\nchurn_data[np.isnan(churn_data['OnlineSecurity'])]","27006a1d":"# Checking the rows against the missing values of OnlineBackup\nchurn_data[np.isnan(churn_data['OnlineBackup'])]","f5d810f4":"# Checking the rows against the missing values of DeviceProtection\nchurn_data[np.isnan(churn_data['DeviceProtection'])]","30ccd642":"# Checking the rows against the missing values of TechSupport\nchurn_data[np.isnan(churn_data['TechSupport'])]","2f7f9aab":"# Checking the rows against the missing values of StreamingTV\nchurn_data[np.isnan(churn_data['StreamingTV'])]","c019d26e":"# Checking the rows against the missing values of StreamingTV\nchurn_data[np.isnan(churn_data['StreamingMovies'])]","10c383ac":"# To check which one has maximum count 0 or 1 \nchurn_data['OnlineSecurity'].value_counts()","f38ddd58":"# ,'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies'\n# To check which one has maximum count 0 or 1 \nchurn_data['OnlineBackup'].value_counts()","1bdeb818":"churn_data['DeviceProtection'].value_counts()","90ce7969":"churn_data['TechSupport'].value_counts()","9b2fbd76":"churn_data['StreamingTV'].value_counts()","8596168d":"churn_data['StreamingTV'].mode()","f9d5b931":"churn_data['StreamingMovies'].value_counts()","b6a8cf2b":"churn_data['StreamingMovies'].mode()","aaa66186":"churn_data[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]=churn_data[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']].fillna(0)","71500915":"# Checking the sum of missing values in the columns now\nchurn_data.isnull().sum()","b4a597dc":"# Checking the percentage of missing values in each of the columns now\nround(100*(churn_data.isnull().sum()\/churn_data.shape[0]),2)","cae334ed":"# Dropping the rows where TotalCharges have missing values\nclean_data=churn_data[~np.isnan(churn_data['TotalCharges'])]","8c06320f":"# Checking the number of missing values in all the columns\nclean_data.isnull().sum()","1b07bf29":"# importing train test split library\nfrom sklearn.model_selection import train_test_split","e4bddf72":"# Putting feature variable to X\nX = clean_data.drop(['Churn','customerID'], axis=1)\n\nX.head()","35a7c130":"# Putting response variable in y\ny = clean_data['Churn']\ny.head()","5eea2c93":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","6320d922":"from sklearn.preprocessing import StandardScaler","c16fffb5":"# scaling numerical variables\nscaler = StandardScaler()\n\nX_train[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\n\nX_train.head()","fa2e46a7":"### Checking the Churn Rate\nchurn = (sum(clean_data['Churn'])\/len(clean_data['Churn'].index))*100\nchurn","8aeb6fb1":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","faf7e254":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(clean_data.corr(),annot = True)\nplt.show()","6cb371cf":"# Dropping highly correlated dummy variable\nX_test = X_test.drop(['MultipleLines_No'], 1)\nX_train = X_train.drop(['MultipleLines_No'], 1)","d44e96ad":"# Checking the correlation matrix\nplt.figure(figsize = (20,10))\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","6a41a52e":"# library required for building the model\nimport statsmodels.api as sm","fe3248b4":"# LOGISTIC REGRESSION MODEL\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","bf3145bd":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","2a4724ad":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","23a2295a":"rfe.support_","1fbd4780":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","91204da8":"col = X_train.columns[rfe.support_]","27140e78":"col","82f2cf3a":"X_train.columns[~rfe.support_]","2c2f64d2":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","07060dd5":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","4d5ca840":"y_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","092aca02":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","6d4f3a2b":"from sklearn import metrics","01ce52f4":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","f5a16c62":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","e900d3c1":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","47def78e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ee297738":"col = col.drop('MonthlyCharges', 1)\n","d65d730c":"col","63d2827b":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","8d50cf6e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a12a1788":"col = col.drop('TotalCharges', 1)","e2c718ac":"X_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","1d7522ec":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d82dad01":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","1e77e8c7":"y_train_pred[:10]","20f17fe9":"y_train_pred_final['Churn_Prob'] = y_train_pred","43b29f51":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","e8b93937":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","7fb2f50d":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nconfusion","5beb704a":"# Actual\/Predicted     not_churn    churn\n        # not_churn        3269      366\n        # churn            595       692 ","da668558":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted)","b3221913":"# Metrics beyond accuracy\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","14cb1bda":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","037dedf3":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FP)","fae9aa85":"# Let us calculate specificity\nTN \/ float(TN+FP)","f4417a84":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","c14ee58f":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","4448ccf2":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )","28f4c22c":"draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","2dc29956":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","3a74838f":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","2fe8c052":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","14ca710d":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","27d7a6e1":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted)","d0f66357":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nconfusion2","712cc8f2":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","b6897b67":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","a8aaef7d":"# Let us calculate specificity\nTN \/ float(TN+FP)","c7f95da1":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","d2a6635a":"# Positive predictive value \nprint (TP \/ float(TP+FP))","21c2cd49":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","5caa966a":"confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nconfusion","3d4ce318":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","b2ba1f39":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","4d320018":"from sklearn.metrics import precision_score, recall_score","b35ca35f":"precision_score(y_train_pred_final.Churn, y_train_pred_final.predicted)","62d06aca":"from sklearn.metrics import precision_recall_curve","1228ddfb":"y_train_pred_final.Churn, y_train_pred_final.predicted","701544a7":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","ed088e26":"# Precision and Recall method of finding the optimal value\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","b6801b51":"X_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])","3f70aacd":"X_test = X_test[col]\nX_test.head()","ae2bf22d":"X_test_sm = sm.add_constant(X_test)","47935e4b":"y_test_pred = res.predict(X_test_sm)","97135aea":"y_test_pred[:10]","48232600":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","79f18e1b":"# Let's see the head\ny_pred_1.head()","982d36c6":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","778ec55a":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","9a93fa6b":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","b69215c6":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","b5c5f540":"y_pred_final.head()","c2999de8":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","510870b7":"# Let's see the head of y_pred_final\ny_pred_final.head()","e5cdf53f":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.42 else 0)","7bc3b7f1":"y_pred_final.head()","1727d0e1":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","c6517403":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","319bb7d9":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","54ecd599":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ecc12550":"# Let us calculate specificity\nTN \/ float(TN+FP)","d18be3a9":"# Precision\nTP \/ float(TP+FP)","cdc38944":"# predicted churn result set\ny_pred_final","ce7fb19c":"## This shows there is 27% of churn rate as per existing data.","e3f8f814":"#### Since in all the above variables 0 appears most of the time we will replace the missing values of all the above columns with 0.","5ba81d59":"# PLOTTING ROC CURVE","0829fafb":"#### The binary variables are as follows:\n- Partner,Dependents,PhoneService,OnlineSecurity, OnlineBackup, DeviceProtection,TechSupport, StreamingTV,StreamingMovies,PaperlessBilling,Churn\t ","82f3f930":"#### The above data shows no missing values in the dataset.","cf51590e":"#### The data is clean now. Thus we can proceed with the next process.","4c268d6f":"#### Therefore there are 7043 rows and 21 columns in the data set.","af159255":"#### The numbers seems to be gradually increasing. Therefore there are no outliers.","55d0c222":"##### Recall\nTP \/ TP + FN","d726b8bb":"### Therefore the columns haveing missing values are as follows:\n- OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies\n### So the above columns missing value needs to be treated\n- TotalCharges column contains very less missing values so they can be dropped","b58f0d53":"#### Therefore the optimal value is 0.42","17eb8cd9":"# 12. MAKING PREDICTIONS ON THE TEST SET","961a88db":"# 7. FEATURE SELECTION USING RFE","dd17122e":"#### From the curve above, 0.3 is the optimum point to take it as a cutoff probability.","543db3af":"# 8. CONFUSION MATRIX","4759a436":"#### Next we will remove Total Charges with high VIF.","ac92302d":"# 9. FINDING THE OPTIMAL CUT OFF POINT","1ea620e0":"### **Objective: To predict the behaviour of the customers to retain them**","a08f2ae1":"# 4. FEATURE SCALING","147488c6":"#### Here Monthly Charges has the highest VIF. So we will drop that column.","7f906f0e":"# 2. DATA PREPARATION","a7b25b6a":"## Checking VIFs","04fd8ee7":"##### Precision\nTP \/ TP + FP","a7100b7c":"# 10. EVALUATION OF THIS MODEL WHICH IS CREATED ACCORDING TO THE OPTIMAL CUT OFF","49d1dc07":"## Following is the future churn behaviour of the customers of the respective customer IDs:","4ca38128":"# BUSINESS ANALYSIS","8cd3c5e0":"#### From the above data we can assume that all missing values are 0 as all these variables are related to internet and these customers have not availed internet as all InternetService_No variables against them appears to be 1.","42a63e3f":"#### Assessing the model using StatsModel","55bcb442":"# PRECISION AND RECALL TRADEOFF","76fd3bcd":"# 5. CHECKING CORRELATIONS","680eef54":"#### From the above data we can observe that the column TotalCharges have negligible number and percentage of missing values.","abdb40fb":"# 1. READING AND UNDERSTANDING THE DATA","99111137":"# MISSING VALUE TREATMENT","f45cf674":"# 3. TRAIN-TEST SPLIT","f3a4e7e0":"#### Now all the variables are highly significant.","bb526111":"# This shows that this is a good model.","f425c420":"# 6. MODEL BUILDING","fcf54594":"# 11. PRECISION AND RECALL","05d9f873":"### The sensitivity that is the probability of yeses correctly converted to yeses is more in test set than training set."}}