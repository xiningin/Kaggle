{"cell_type":{"fc328a0a":"code","05ef693e":"code","84e509e2":"code","f71bf7ef":"code","2e6b48c2":"code","8b088e81":"code","a9a99275":"code","9cac11f2":"code","29038be1":"code","1d621514":"code","11b0ea6f":"code","a10a8dd8":"code","c0772ebe":"code","775302e0":"code","4b735634":"code","d043f1fb":"code","f8e0c1a7":"code","7ec3c8f5":"code","c357e2a7":"code","acb41653":"code","b0ee98f7":"code","d82aca19":"code","ad2cf2fd":"code","347d54d8":"code","5ad49b34":"code","6e521939":"code","8bd94d26":"code","e5c46ece":"code","cb033b6e":"code","3ba9275c":"code","09abe0db":"code","8d4e0dbc":"code","5170ab44":"code","e053e499":"code","f9b541e8":"code","6fd99b48":"code","3b6b8a57":"code","d4d89a51":"code","40f268ce":"code","02e8ecb2":"markdown","7884810c":"markdown"},"source":{"fc328a0a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nrandom_seed = 1\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom hyperopt import fmin, Trials, hp, tpe, STATUS_OK\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","05ef693e":"#Reading training and test dataset\ndf_train = pd.read_csv(\"..\/input\/hdfc-2019\/DataSet\/Train.csv\")\ndf_test = pd.read_csv(\"..\/input\/hdfc-2019\/DataSet\/Test.csv\")","84e509e2":"df_train.shape, df_test.shape","f71bf7ef":"pd.set_option('display.max_columns', 500)\ndf_train.head()","2e6b48c2":"df_test.head()","8b088e81":"#Col1 is unique entries and Col2 is target column\nprint(df_train['Col1'].unique().shape, df_train.shape)\nprint(df_test['Col1'].unique().shape, df_test.shape)","a9a99275":"X_train = df_train.drop(['Col1','Col2'], axis = 1)\ny_train = df_train['Col2']\n\nX_test = df_test.drop(['Col1'], axis = 1)","9cac11f2":"X_train.shape, y_train.shape, X_test.shape","29038be1":"#ratio of 2 classes\ny_train.value_counts(normalize = True)*100","1d621514":"X_train.info()","11b0ea6f":"X_test.info()","a10a8dd8":"# There is different in the count of object datatypes in train and test\n\nfor col in X_train.select_dtypes('object').columns:\n    print(col)\n    print(X_train[col].unique())\n    print(\"_______\")","c0772ebe":"for col in X_test.select_dtypes('object').columns:\n    print(col)\n    print(X_test[col].unique())\n    print(\"_____\")","775302e0":"#converting all string numeric data to float and '-' to nan\n\ndef convert_to_float(row):\n    if row=='-':\n        return np.nan\n    else:\n        return float(row)","4b735634":"columns_need_treatment = list(X_train.select_dtypes('object').columns) + list(X_test.select_dtypes('object').columns)\nprint(len(columns_need_treatment))\nprint(columns_need_treatment)","d043f1fb":"for col in columns_need_treatment:\n    X_train[col] = X_train[col].apply(convert_to_float)\n    X_test[col] = X_test[col].apply(convert_to_float)","f8e0c1a7":"X_train.info(), X_test.info()","7ec3c8f5":"## Duplicate rows\n\nduplicate_columns_in_train = X_train.duplicated()\nduplicate_columns_in_test = X_test.duplicated()","c357e2a7":"sum(duplicate_columns_in_train), sum(duplicate_columns_in_test)","acb41653":"y_train[duplicate_columns_in_train].value_counts(normalize = True)","b0ee98f7":"X_train['duplicate_row'] = False\nX_test['duplicate_row'] = False\nX_train.loc[duplicate_columns_in_train,'duplicate_row'] = True\nX_test.loc[duplicate_columns_in_test,'duplicate_row'] = True\n","d82aca19":"## Duplicate features\n\nfeatures = X_train.columns\nduplicate_columns = set()\nfor i in range(len(features)):\n    for j in range(i+1, len(features)):\n        if np.all(X_train[features[i]] == X_train[features[j]]):\n            print(features[i], features[j])\n            duplicate_columns.add(features[j])","ad2cf2fd":"len(duplicate_columns)","347d54d8":"selected_features = [_ for _ in X_train.columns if _ not in duplicate_columns]\nprint(len(selected_features))","5ad49b34":"X_train, X_valid, y_train, y_valid = train_test_split(X_train[selected_features], y, random_state = random_seed, test_size = 0.2)","6e521939":"def score(params):\n    try:\n        print(\"Training with params: \",params)\n        num_rounds = int(params['n_estimators'])\n        del params['n_estimators']\n        dtrain = xgb.DMatrix(X_train,label = y_train)\n        dvalid = xgb.DMatrix(X_valid, label = y_valid)\n        \n        watchlist = [(dtrain,'train'),(dvalid,'valid')]\n        xgb_model = xgb.train( params, dtrain, num_rounds, evals = watchlist, verbose_eval = False )\n        predictions = xgb_model.predict(dvalid, ntree_limit = xgb_model.best_iteration + 1)\n        \n        predictions = (prediction >= 0.5).astype('int')\n        score = f1_score(y_valid, predictions, average ='weighted')\n        print(\"Score : {0}\\n\".format(score))\n        \n        loss = 1-score\n        print(\"Loss : {0}\".format(loss))\n        return {'loss' :loss, 'status':STATUS_OK}\n    \n    except AssertionError as obj:\n        loss = 1 - 0\n        return {'loss':loss, 'status':STATUS_OK}\n    except Exception as obj:\n        loss = 1 - 0\n        return {'loss':loss, 'status':STATUS_OK}\n    \n    ","8bd94d26":"def optimize(trials, max_evals, random_state = random_seed):\n    space = { \n        'n_estimators' : hp.quniform('n_estimators', 100,300,1),\n        'eta' : hp.quniform('eta',0.025,0.5, 0.025),\n        'max_depth' : hp.choice('max_depth', np.arange(1,7,dtype = int)),\n        'min_child_weight' : hp.quniform('min_child_weight',1,6,1), # \"stop trying to split once your sample size in a node goes below a given threshold\".\n        'subsample' : hp.quniform('subsample', 0.5,1, 0.05),\n        'gamma' : hp.quniform('gamma', 0,1,0.05), #Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n        'colsample_by_tree' : hp.quniform('colsample_by_tree',0.5,1,0.05),\n        'scale_pos_weight' : hp.quniform('scale_pos_weight',1,4,0.05), #Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) \/ sum(positive instances).\n        'reg_alpha' : hp.quniform('reg_alpha', 0,1,0.05), #L1 regularization term on weights. Increasing this value will make model more conservative.\n        'reg_lambda' : hp.quniform('reg_lambda',1,5,0.05), #L2 regularization term on weights. Increasing this value will make model more conservative.\n        'eval_metric': 'logloss',\n        'objective': 'binary:logistic',\n        'nthread': 4,\n        'booster':'gbtree',\n        'tree_method':'exact',\n        'silent':1,\n        'seed':random_seed      \n        \n    }\n    best = fmin(score, space, algo = tpe.suggest, trials = trials, max_evals = max_evals)\n    return best\n","e5c46ece":"trials = Trials()\nmax_evals = 25\n\nbest_hyperparams = optimize(trials, max_evals)\nprint(\"Best hyperparameters are : \\n\", best_hyperparams)","cb033b6e":"#best hyperparameters\nbest_hyperparams","3ba9275c":"param = best_hyperparams\nnum_round = int(param['n_estimators'])\ndel param['n_estimators']","09abe0db":"#oof (out of fold prediction)\nnum_splits = 5\nfolds = StratifiedKFold(n_splits = num_splits, shuffle = True, random_state=random_seed)\n","8d4e0dbc":"dxtest = xgb.DMatrix(X_test[selected_features])","5170ab44":"y_test_pred = np.zeros((X_test[selected_features].shape[0],1))\nprint(y_test_pred.shape)\ny_valid_scores =[]","e053e499":"X_TRAIN = X_train[selected_features].copy()\nY_TRAIN = y_train.copy()\nX_TRAIN = X_TRAIN.reindex()\nY_TRAIN = Y_TRAIN.reindex()\n\nfor fold, (train_index, valid_index) in enumerate(folds.split(X_TRAIN, Y_TRAIN)):\n    print(\"Fold...\", fold)\n    X_train, X_valid = X_TRAIN.iloc[train_index], X_TRAIN.iloc[valid_index]\n    y_train, y_valid= Y_TRAIN.iloc[train_index], Y_TRAIN.iloc[valid_index]\n    \n    dtrain = xgb.DMatrix(X_train, label =y_train)\n    dvalid = xgb.DMatrix(X_valid, label = y_valid)\n    \n    evallist = [(dtrain,'train'),(dvalid,'valid')]\n    \n    xgb_model = xgb.train(param, dtrain, num_round, evallist , verbose_eval = 50)\n    \n    y_pred_valid = xgb_model.predict(dvalid, ntree_limit= xgb_model.best_iteration + 1)\n    \n    y_valid_scores.append(f1_score(y_valid, (y_pred_valid >=0.5).astype(int),average ='weighted'))\n    \n    y_pred = xgb_model.predict(dxtest, ntree_limit = xgb_model.best_iteration + 1)\n    \n    y_test_pred += y_pred.reshape(-1,1)\n\ny_test_pred \/= num_splits\n\n\n                          ","f9b541e8":"y_valid_scores, np.mean(y_valid_scores)","6fd99b48":"output = df_test[['Col1']].copy()\noutput['Col2'] = (y_test_pred >=0.5).astype(int)","3b6b8a57":"output.head()","d4d89a51":"output['Col2'].value_counts()\/output.shape[0] *100\n","40f268ce":"output.to_csv(\"submission.csv\", index = False)","02e8ecb2":"# Lessons learnt\n1. Data cleaning - Removing duplicate rows and columns\n2. XGB - Bayesian optimization\n","7884810c":"Reference: https:\/\/www.kaggle.com\/shobhitupadhyaya\/hdfc-ml-challenge-solution\/data"}}