{"cell_type":{"75383a4e":"code","e8a76a4a":"code","9bdbe532":"code","bd9a066c":"code","ce481bad":"code","a9f5ba08":"code","c22a2114":"code","2faaa261":"code","12e0b7df":"code","15e96096":"code","d02abdf1":"code","05026ff2":"code","35ea06d0":"code","ceec3e27":"code","5c34996a":"code","3c2838f3":"code","2be54b9c":"code","e840dd89":"code","33d8a41e":"code","7abe0ab2":"code","4ab3f5fe":"code","a80f2981":"code","74aa9c80":"code","90a656d5":"code","858dfe3f":"code","b5eb54af":"code","604da485":"code","1907edfd":"code","03e02901":"code","7ffdd9b3":"code","25d8b082":"code","b10483ed":"code","6db132da":"code","85c58eac":"code","1b1eadb0":"code","c9740b91":"code","2f6c54fa":"code","39a1f93f":"code","a4b427b4":"code","bcff7a85":"code","a100be90":"code","2e750588":"code","94a7b882":"code","c352b687":"code","785768bc":"code","7eef4980":"code","7e16d359":"code","6737a1e6":"code","c154b6b7":"code","68d83152":"code","4a70026e":"code","bd03eb0c":"code","8a84a06c":"code","bf8e708d":"code","3a934ab3":"code","4a3820bf":"code","cb57e156":"code","db796217":"code","39782d73":"code","b4e31a9b":"code","2beee441":"code","7bbe9d95":"code","493c7106":"code","010bc7cc":"code","8f518ed3":"code","943f246f":"code","1f027dd1":"code","0c3c492e":"code","237b4d42":"code","977f302e":"code","5fe00f35":"code","77f8e1a3":"code","36223283":"code","b1b8517e":"markdown","4b5789d9":"markdown","f680231d":"markdown","f8281f91":"markdown","737b9dc1":"markdown","9366df08":"markdown","2b5229fd":"markdown","e31df6b7":"markdown","31be09bc":"markdown","9a40f64b":"markdown","833f0752":"markdown","599f7bc8":"markdown","17a58756":"markdown","ea523aa4":"markdown","1ec33f44":"markdown","fff621a3":"markdown","05c0f516":"markdown","4e651b5f":"markdown","9319d7a9":"markdown","aecfccee":"markdown","8be4c8b5":"markdown","ab6bdad7":"markdown"},"source":{"75383a4e":"#Import necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sb\nfrom datetime import datetime\nimport calendar\n\n#Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","e8a76a4a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9bdbe532":"#Read Dataset\ndf_train = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/test.csv')","bd9a066c":"#Review training set data\ndf_train.head()","ce481bad":"#Review test set data\ndf_test.head()","a9f5ba08":"df_train.shape, df_test.shape","c22a2114":"df_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()","2faaa261":"#Reviewing datatypes\ndf_train.dtypes","12e0b7df":"df_test.dtypes","15e96096":"#Saving total number of rows in test and train\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\nntrain, ntest","d02abdf1":"#Let's remove few columns from training set as they are not available in test set.\ndcols = ['count', 'casual', 'registered']\n\n#Let's backkup the data first\ndf_target = df_train[dcols]\n\n#Let's remove them\ndf_train.drop(dcols, inplace=True, axis=1)","05026ff2":"df_train.shape, df_test.shape","35ea06d0":"#Let's concatenate train and test data\ndf_alldata = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n\ndf_alldata.head(3)","ceec3e27":"df_alldata.shape","5c34996a":"#Let start our EDA with datetime\ndf_alldata['datetime'] = pd.to_datetime(df_alldata['datetime'])","3c2838f3":"#New values extracted from datetime -> day, date, hour, month, year\ndf_alldata['day'] = df_alldata['datetime'].apply(lambda x: calendar.day_name[x.weekday()])\ndf_alldata['date'] = df_alldata['datetime'].apply(lambda x: x.day)\ndf_alldata['hour'] = df_alldata['datetime'].apply(lambda x: x.hour)\ndf_alldata['month'] = df_alldata['datetime'].apply(lambda x: calendar.month_name[x.month])\ndf_alldata['year'] = df_alldata['datetime'].apply(lambda x:x.year)\n\ndf_alldata.head(3)","2be54b9c":"#For simplicity of our model\ndf_alldata['year'] = df_alldata['year'].map({2011:0, 2012:1})","e840dd89":"#Let's drop datetime column as we extracted information\ndf_alldata.drop('datetime', inplace=True, axis=1)","33d8a41e":"#Let's change few more cols as per their values\ndf_alldata['season'] = df_alldata['season'].map({1:'Spring', 2:'Summer', 3:'Fall', 4:'Winter'})","7abe0ab2":"df_alldata['weather'] = df_alldata['weather'].map({1:'Clear-Cloudy', 2:'Misty-Cloudy', 3:'LightRain-Storm', 4:'Rain-Ice'})","4ab3f5fe":"#Let's check dataset datatypes\ndf_alldata.dtypes","a80f2981":"#Let's get catg features\ncatg_feats = df_alldata.dtypes[df_alldata.dtypes == 'object'].index\ncatg_feats","74aa9c80":"#Let's onehot encode these categorical feats\nfor col in catg_feats:\n    df_temp = pd.get_dummies(df_alldata[col], prefix=col)\n    dcol = df_temp.columns[0]\n    df_temp.drop(dcol, inplace=True, axis=1) #Dropping dummy variable trap col\n    df_alldata.drop(col, inplace=True, axis=1) #Dropping original column\n    df_alldata = pd.concat([df_alldata, df_temp], axis=1).reset_index(drop=True)","90a656d5":"df_alldata.head(3)","858dfe3f":"df_alldata.dtypes","b5eb54af":"#For submission dataframe saving timestamp\nsrs_timestamp = df_test['datetime']","604da485":"#Let's split train and test set\ndf_train = df_alldata[:ntrain]\ndf_test = df_alldata[ntrain:]\n\ndf_train.shape, df_test.shape","1907edfd":"## Setting up target variable\ntarget = np.log1p(df_target['count'])\n\nlen(target)","03e02901":"from sklearn.model_selection import KFold\n\ncross_val = KFold(n_splits=10, random_state=42, shuffle=True)","7ffdd9b3":"#Calculating RMSLE\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred), 'Error in actual and prediction length.'\n    return np.sqrt(np.mean((np.log1p(y) - np.log1p(y_pred))**2))\n\n#np.sqrt(mean_squared_log_error(y, y_pred))","25d8b082":"#List of models to try\nimport xgboost as xgb\nfrom sklearn.linear_model import RidgeCV, Ridge, LassoCV, Lasso, ElasticNetCV, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.metrics import mean_squared_log_error","b10483ed":"X_train, X_test, y_train, y_test = train_test_split(df_train.values, target, test_size=0.3, random_state=42)","6db132da":"X_train.shape, X_test.shape","85c58eac":"#Function to store model parameters and score\n\ncols = ['Model', 'Parameters', 'Xtrain_RMSLE', 'Xtest_RMSLE', 'dftrain_RMSLE', 'exp_dftrain_RMSLE']\ndf_model_scores = pd.DataFrame(columns=cols)\n\n\ndef model_scores(model, df_model_scores = df_model_scores, X_train = X_train, y_train = y_train, X_test = X_test, \n                 y_test = y_test, df_train = df_train, target = target):\n    #Fit with Xtrain\n    model.fit(X_train, y_train)\n    \n    #Predict X_train\n    pred = model.predict(X_train)\n    \n    #X_train RMSLE\n    xtr_rmsle = rmsle(y_train, pred)\n    \n    #Predict X_test\n    pred = model.predict(X_test)\n    \n    #X_test rmsle\n    xts_rmsle = rmsle(y_test, pred)\n    \n    #Predict df_train\n    model.fit(df_train.values, target)\n    pred = model.predict(df_train.values)\n    dftr_rmsle = rmsle(target, pred)\n    expdftr_rmsle = rmsle(np.expm1(target), np.expm1(pred))\n    \n    #setting up values for data frame\n    mdl = model.__class__.__name__\n    param = str(model.get_params())\n    \n    data = {'Model':[mdl], 'Parameters':[param], 'Xtrain_RMSLE':[xtr_rmsle], 'Xtest_RMSLE':[xts_rmsle], \n                           'dftrain_RMSLE':[dftr_rmsle], 'exp_dftrain_RMSLE':[expdftr_rmsle]}\n    \n    df_temp = pd.DataFrame(data)\n    \n    df_model_scores = pd.concat([df_model_scores, df_temp]).reset_index(drop=True)\n    \n    return df_model_scores","1b1eadb0":"#Let's find optimum parameters for each model\nmodel_xgb = xgb.XGBRegressor(n_estimators=1000, n_jobs=-1, objective='reg:squarederror', random_state=42)","c9740b91":"xgb_param_grid={'max_depth':[5, 6],\n               'learning_rate':[0.1],\n               'booster':['gbtree','dart']}","2f6c54fa":"#Implement Grid Search over XGBoost\ngs_xgb_model = GridSearchCV(param_grid=xgb_param_grid, estimator=model_xgb, cv=cross_val, verbose=1, n_jobs=-1)\n# gs_xgb_model.fit(X_train, y_train)#Training the Model\n# print('Best Score:', gs_xgb_model.best_score_)\n# print('Parameters:', gs_xgb_model.best_params_)","39a1f93f":"xgb_param_grid = {'booster':['gbtree', 'gblinear', 'dart']}","a4b427b4":"#Gathering model scores\nmodel_xgb = xgb.XGBRegressor(n_estimators=1000, n_jobs=-1, objective='reg:squarederror', random_state=42, max_depth=5, \n                             booster='dart')\ndf_model_scores = model_scores(model_xgb, df_model_scores)\ndf_model_scores","bcff7a85":"#Model Training\nmodel_xgb.fit(X_train, y_train)\n\n#Model Prediction\npred=model_xgb.predict(df_test.values)\n\n#Submission\npred = np.expm1(pred)\n\n#Rounding prediction\nsr_pred = pd.Series(data=pred, name='count')\nsr_pred = sr_pred.apply(lambda x: round(x,0))\n\nsubmission = pd.DataFrame({'datetime':srs_timestamp, 'count':sr_pred})\nsubmission.head()","a100be90":"#Saving Submission\nsubmission.to_csv('1119_xgb.csv', index=False)","2e750588":"alphas_ridge = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas_lasso = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nelastic_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","94a7b882":"#RidgeCV\nrcv = RidgeCV(alphas=alphas_ridge, cv=10, scoring='neg_mean_squared_log_error')","c352b687":"rcv.fit(X_train, y_train)","785768bc":"rcv.alpha_","7eef4980":"#Finding best alpha round 2\nalphas_ridge = [0.1, 1, 5, 8, 10, 12, 14, 14.5]\nrcv = RidgeCV(alphas_ridge, cv=10, scoring='neg_mean_squared_log_error')\nrcv.fit(X_train, y_train)\nrcv.alpha_","7e16d359":"#Finding best alpha round 3\nalphas_ridge = [0.1, 0.01, 0.5, 0.001, 0.3]\nrcv = RidgeCV(alphas_ridge, cv=10, scoring='neg_mean_squared_log_error')\nrcv.fit(X_train, y_train)\nrcv.alpha_","6737a1e6":"#Finding best alpha round 4\nalphas_ridge = [0.001, 0.005, 0.015, 0.003, 0.008]\nrcv = RidgeCV(alphas_ridge, cv=10, scoring='neg_mean_squared_log_error')\nrcv.fit(X_train, y_train)\nrcv.alpha_","c154b6b7":"model_ridge = Ridge(alpha=0.001, max_iter=10000, random_state=42)","68d83152":"df_model_scores = model_scores(model_ridge, df_model_scores)\ndf_model_scores","4a70026e":"lcv = LassoCV(alphas=alphas_lasso, max_iter=1000, cv=10, n_jobs=-1, selection='random', random_state=42, verbose=1)","bd03eb0c":"lcv.fit(X_train, y_train)\nlcv.alpha_","8a84a06c":"#Computing best alpha round 2\nalphas = [0.0002, 0.0003, 0.00025, 0.0015]\nlcv = LassoCV(alphas = alphas, max_iter=10000, cv=10, n_jobs=-1, selection='random', random_state=42, verbose=1)\nlcv.fit(X_train, y_train)\nlcv.alpha_","bf8e708d":"model_lasso = Lasso(alpha=0.0002, max_iter=10000, random_state=42, selection='cyclic')\ndf_model_scores = model_scores(model_lasso, df_model_scores)\ndf_model_scores","3a934ab3":"model_rf = RandomForestRegressor(random_state=42)\n\ngrid_rf = {'max_depth':[2, 3, 5],\n           'n_estimators':[200],\n          'criterion':['mse','mae']}\n\nGSearch = GridSearchCV(param_grid=grid_rf, estimator=model_rf, cv=10, n_jobs=-1, verbose=1)\n\nGSearch.fit(X_train, y_train)\n\nprint('Best Score:', GSearch.best_score_)\nprint('Parameters:', GSearch.best_params_)","4a3820bf":"GSearch.best_params_","cb57e156":"model_rf = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=5, criterion='mse')","db796217":"df_model_scores = model_scores(model_rf, df_model_scores)\ndf_model_scores","39782d73":"model_lgbm = lgbm.LGBMRegressor(n_estimators=1000, objective='regression', random_state=42, n_jobs=-1)\n\ngrid_lgbm = {'learning_rate':[0.02, 0.05, 0.08]}\n\nGSearch = GridSearchCV(param_grid=grid_lgbm, estimator=model_lgbm, cv=10, n_jobs=-1, verbose=1)\n\nGSearch.fit(X_train,y_train)\n\nprint('Best Score:', GSearch.best_score_)\nprint('Parameters:', GSearch.best_params_)","b4e31a9b":"model_lgbm = lgbm.LGBMRegressor(n_estimators=1000, objective='regression', random_state=42, n_jobs=-1, learning_rate=0.05)\n\ndf_model_scores = model_scores(model_lgbm, df_model_scores)\n\ndf_model_scores","2beee441":"#Round 2\nelastic_alphas = [0.0003, 0.00035, 0.00028]\nelastic_l1ratio = [0.7, 0.75, 0.8]\necv = ElasticNetCV(alphas=elastic_alphas, l1_ratio=elastic_l1ratio, cv=10, n_jobs=-1, random_state=42, max_iter=10000)\necv.fit(X_train, y_train)\necv.alpha_, ecv.l1_ratio_","7bbe9d95":"#Round 3\nelastic_l1ratio = [0.6, 0.5, 0.7]\nelastic_alphas = [0.0003]\necv = ElasticNetCV(alphas = elastic_alphas, l1_ratio=elastic_l1ratio, cv=10, n_jobs=-1, random_state=42, max_iter=10000)\necv.fit(X_train, y_train)\necv.alpha_, ecv.l1_ratio_","493c7106":"#Round 4\nelastic_l1ratio = [0.3, 0.2, 0.5]\nelastic_alphas = [0.0003]\necv = ElasticNetCV(alphas = elastic_alphas, l1_ratio=elastic_l1ratio, cv=10, n_jobs=-1, random_state=42, max_iter=10000)\necv.fit(X_train, y_train)\necv.alpha_, ecv.l1_ratio_","010bc7cc":"model_elastic = ElasticNet(alpha=0.0003, l1_ratio=0.5, random_state=42, max_iter=10000)\ndf_model_scores = model_scores(model_elastic, df_model_scores)\ndf_model_scores","8f518ed3":"model_GB = GradientBoostingRegressor(n_estimators=300, random_state=42)\n\nGSearch_param = {'max_depth':[3,5],\n             'learning_rate':[0.1, 0.01, 0.3],\n                'alpha':[0.5, 0.1, 0.9]}\n\nGSearch_GB = GridSearchCV(param_grid=GSearch_param, estimator=model_GB, cv=10, n_jobs=-1, verbose=2)\n\nGSearch_GB.fit(X_train, y_train)\n\nprint('Best Score:', GSearch_GB.best_score_)\nprint('Best Param:', GSearch_GB.best_params_)","943f246f":"model_gb = GradientBoostingRegressor(n_estimators=300, random_state=42, max_depth=5, alpha=0.5)\ndf_model_scores = model_scores(model_gb, df_model_scores)\ndf_model_scores","1f027dd1":"from sklearn.ensemble import VotingRegressor\n\nmodel_vote = VotingRegressor([('XGBoost', model_xgb), ('LGBM', model_lgbm), ('GradientBoosting', model_gb)])\nmodel_vote.fit(X_train, y_train)\npred = model_vote.predict(df_test.values)","0c3c492e":"df_model_scores = model_scores(model_vote, df_model_scores)\ndf_model_scores","237b4d42":"#Model Training\nmodel_vote.fit(X_train, y_train)\n\n#Model Prediction\npred=model_vote.predict(df_test.values)\n\n#Submission\npred = np.expm1(pred)\n\n#Rounding prediction\nsr_pred = pd.Series(data=pred, name='count')\nsr_pred = sr_pred.apply(lambda x: round(x,0))\n\nsubmission = pd.DataFrame({'datetime':srs_timestamp, 'count':sr_pred})\nsubmission.head()","977f302e":"submission.to_csv('VotingModelResults.csv', index=False)","5fe00f35":"from mlxtend.regressor import StackingRegressor\n\nstack_reg = StackingRegressor(regressors=[model_gb, model_lgbm], meta_regressor=model_xgb, \n                              use_features_in_secondary=False)\n\ndf_model_scores = model_scores(stack_reg, df_model_scores)\ndf_model_scores","77f8e1a3":"df_model_scores.to_csv('Kaggle_Bike_Sharing_Model.csv')","36223283":"stack_reg.fit(X_train, y_train)\npred = stack_reg.predict(df_test.values)\nx = pred[:5]\nnp.expm1(x)","b1b8517e":"### Ridge","4b5789d9":"### Stacking Regressor","f680231d":"### Gradient Boosting Model","f8281f91":"### XGBoost Model","737b9dc1":"### Features:\n<ol>\n<li> datetime - hourly date + timestamp  \n<li> season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n<li> holiday - whether the day is considered a holiday\n<li> workingday - whether the day is neither a weekend nor holiday\n<li> weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n<li> weather - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n<li> weather - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n<li> weather - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n<li> temp - temperature in Celsius\n<li> atemp - \"feels like\" temperature in Celsius\n<li> humidity - relative humidity\n<li> windspeed - wind speed\n<li> casual - number of non-registered user rentals initiated\n<li> registered - number of registered user rentals initiated\n<li> count - number of total rentals\n    <\/ol>","9366df08":"### Voting Model Secured 0.38715","2b5229fd":"### Lasso","e31df6b7":"### Ask:\nPredict the total count of bikes rented during each hour in test set.","31be09bc":"### Backup data","9a40f64b":"## Defining Cross Validation and Error Function","833f0752":"### Random Forest Regressor","599f7bc8":"## <center> Bike Sharing Demand - Prediction","17a58756":"## Exploratory Data Analysis","ea523aa4":"## Preprocessing for Modelling","1ec33f44":"### Submission Secured 0.40167","fff621a3":"### Voting Regressor","05c0f516":"### Data:\n<br>\n<li> Hourly rental data spanning two years.\n<li> Training set contains first 19 days of each month.\n<li> Test Set contains 20th to end of month.","4e651b5f":"---","9319d7a9":"## Model and Parameter Tuning","aecfccee":"### ElasticNet","8be4c8b5":"### Light GBM","ab6bdad7":"---"}}