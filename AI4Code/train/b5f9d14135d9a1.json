{"cell_type":{"b69e940f":"code","6b12335b":"code","8ea68a6d":"code","39648a7c":"code","313aa590":"code","183ab25a":"code","c0c6eca5":"code","738c6879":"code","4d780cf6":"code","13c5a31e":"code","45fa1dcc":"code","4205372a":"code","17aad4a7":"code","3aebb8ae":"code","ff59a85c":"code","f8a6ab2a":"code","827b8517":"code","6e56429b":"code","b314a56c":"code","315f1288":"code","60e3f7c8":"code","915a8b21":"code","5dfbb5e1":"code","451b8238":"code","62cc6a7f":"code","a4bb0732":"code","1c77cfc8":"code","1550ea00":"code","4a8ca766":"code","1f569624":"code","cdbfe8c6":"code","f7f4f132":"code","2bcfa4a5":"code","7f858a0e":"code","42619672":"code","e069ab72":"code","69c227fe":"code","86ce450f":"code","b6c02747":"code","52a8da81":"code","036502e6":"code","b271a98b":"code","0a349034":"code","83bb96e4":"code","a63a4f65":"code","674b811a":"code","6ddc0c08":"code","03235258":"code","6d83c409":"code","626f368b":"code","8d57a7d4":"code","b95e46c6":"code","cc9d9258":"code","4cad32e6":"code","28c47f21":"code","d7a06b8a":"code","4f78fae8":"code","b2ad39df":"code","b6133cea":"code","4f1ac8f4":"code","fe271853":"code","87747277":"code","41f4096f":"code","0e08f41b":"code","1c551d8a":"code","85f3d9f6":"code","f5327a8e":"markdown","349adf4e":"markdown","8e140e79":"markdown","ffeeb021":"markdown","352940f3":"markdown","7e07243a":"markdown","992bfd96":"markdown","d8aa8f75":"markdown","00d1a8fb":"markdown","2ebc83bf":"markdown","e759b0af":"markdown","69032246":"markdown","348f415d":"markdown","b3abf44c":"markdown","4468b697":"markdown","fc488288":"markdown","f4f2bfb5":"markdown","113eee93":"markdown","1ccf8d62":"markdown","d9f00181":"markdown","1093eaba":"markdown","e974bcc2":"markdown","a15c5f3d":"markdown","8c1ef054":"markdown","578594d5":"markdown","f1514baa":"markdown","2ace050c":"markdown","234c477b":"markdown","ec91143b":"markdown","9c11a87c":"markdown","63273fac":"markdown","c12ff618":"markdown","cf635dd4":"markdown","f3ca8d18":"markdown","56bb9277":"markdown","4f606276":"markdown","c63bd1ac":"markdown","8786e860":"markdown","8b7f5d61":"markdown","824cd43a":"markdown","8182b7ea":"markdown","0e3f5c68":"markdown","fa144654":"markdown","717a4933":"markdown","0144ebde":"markdown"},"source":{"b69e940f":"!pip install git+https:\/\/github.com\/ClaudeCoulombe\/FrenchLefffLemmatizer.git","6b12335b":"# basics\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\n\n# string\nimport string\n!pip install unidecode\nimport unidecode\nimport re\nfrom textwrap import wrap # wrapping long text into lines\n\n# plot\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n# %matplotlib inline\n\n# text mining\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\n\n\n# Because we have some long strings to deal with:\npd.options.display.max_colwidth = 300\n\n\n\nreplacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    def __init__(self, patterns=replacement_patterns): \n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    def replace(self, text):\n        s = text\n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s) \n        return s\n\nreplacer=RegexpReplacer()\n\nfrom nltk.tokenize import WordPunctTokenizer\ntokenizer = WordPunctTokenizer()\n\nfrom french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\nlemmatizer = FrenchLefffLemmatizer()\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstops=set(stopwords.words('french'))\n\nnltk.download('stopwords')","8ea68a6d":"\n# Because we have some long strings to deal with:\npd.options.display.max_colwidth = 300","39648a7c":"os.listdir('..\/input\/granddebat')","313aa590":"themes = {\n    'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv':'La fiscalit\u00e9 et les d\u00e9penses publiques',\n    'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv':\"Organisation de l'\u00e9tat et des services publics\",\n    'DEMOCRATIE_ET_CITOYENNETE.csv':'D\u00e9mocratie et citoyennet\u00e9',\n    'LA_TRANSITION_ECOLOGIQUE.csv':'La transition \u00e9cologique'\n}\n\nfilenames = list(themes.keys())\nthemes = list(themes.values())","183ab25a":"filepaths = [os.path.join(\"..\", \"input\", \"granddebat\", filename) for filename in filenames]\ncol_date = ['createdAt', 'publishedAt', 'updatedAt']\ndf_list = [pd.read_csv(filepath, low_memory=False,\n                       dtype={'authorZipCode':'str'},\n                       parse_dates=col_date) for filepath in filepaths]","c0c6eca5":"col_common = set.intersection(*[set(df.columns) for df in df_list])\ncol_common","738c6879":"pd.concat([df[df.columns.intersection(col_common)] for df in df_list]).isnull().mean() * 100\n","4d780cf6":"df_infos = pd.DataFrame({\n     'theme': themes,\n     'nb_contributions': [df.shape[0] for df in df_list],\n     'nb_questions': [sum(~df.columns.isin(col_common)) for df in df_list]\n    })\ndf_infos","13c5a31e":"day_contrib = pd.concat([df.createdAt for df in df_list]).dt.date.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nday_contrib.plot()\nax.set_title('Daily contributions')\nax.set_xlabel('Date')\nfig.autofmt_xdate()\nax.set_ylim(bottom=0)\nplt.show(fig)","45fa1dcc":"hour_contrib = pd.concat([df.createdAt for df in df_list]).dt.hour.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nhour_contrib.plot()\nax.set_title('Hourly contributions')\nax.set_xlabel('Hour')\nax.set_ylim(bottom=0)\nplt.show(fig)","4205372a":"pd.DataFrame({'theme':themes,\n              'max_contrib_per_author':[df.groupby('authorId').size().max() for df in df_list]})","17aad4a7":"def mode_na(x): \n    m = pd.Series.mode(x)\n    return m.values[0] if not m.empty else np.nan\n\nauthors = pd.concat([df[df.columns.intersection(col_common)] for df in df_list])\n# With pandas>=0.24, we would use: pandas.Series.mode\nauthors = authors.groupby('authorId').agg({'id':'count', # number of contributions\n                                           'authorType':mode_na,\n                                           'authorZipCode':mode_na})","3aebb8ae":"authors.shape[0]\n","ff59a85c":"n_contrib = authors.id.value_counts().reset_index(name='counts')\nn_contrib.loc[n_contrib['index'] > 4, 'index'] = '>4'\nn_contrib = n_contrib.groupby('index').agg(sum)\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.barplot(x='index',\n            y='counts',\n            data=n_contrib.reset_index(),\n            palette=sns.color_palette('Blues'))\nax.set_xlabel('Number of contributions')\nax.set_title('Authors per number of contributions')\nplt.show()","f8a6ab2a":"fig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='authorType',\n                   data=authors,\n                   palette=sns.color_palette('Blues'))\nax.set_yscale('log')\nax.set_title('Author types')\nplt.show()\n","827b8517":"questions = pd.concat([pd.DataFrame({'old_name':df_list[i].columns,\n                                     'df_id':i,\n                                     'theme':themes[i]}) for i in range(len(df_list))])\nquestions = questions[-questions[\"old_name\"].isin(col_common)].reset_index(drop=True)\nquestions = questions.assign(new_name=(pd.Series(\n    ['Q{}'.format(i) for i in range(1, questions.shape[0] + 1)])))\nquestions = questions.assign(question=pd.Series(\n    [name.split(' - ')[1] for name in questions.old_name]))","6e56429b":"# Questions rename\ndict_rename = {old:new for old, new in zip(questions.old_name,questions.new_name)}\nfor df in df_list:\n    df.rename(columns=dict_rename,inplace=True)\n    \nquestions.head()\n","b314a56c":"questions.shape\n","315f1288":"fichier_csv = questions.to_csv(r\"questions.csv\",index=False)\n","60e3f7c8":"questions['nbrow'] = questions.apply(lambda g: df_list[g.df_id].shape[0], axis=1)\nquestions['nbnnull'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                       .notnull().sum(), axis=1)\nquestions['nbunique'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                        .nunique(), axis=1)\n\nquestions['nnull_rate'] = questions.nbnnull\/questions.nbrow * 100\nquestions['unique_rate'] = questions.nbunique\/questions.nbnnull * 100","915a8b21":"questions['closed'] = questions['nbunique'] <= 3\nsum(questions.closed)","5dfbb5e1":"questions.groupby(['theme']).agg({'question':'count', 'closed':'sum',\n                                  'nbrow':'mean', 'nnull_rate':'mean'})","451b8238":"questions.sort_values('nnull_rate').head(10)\n","62cc6a7f":"df_list[1].Q40.value_counts().head(30)\n","a4bb0732":"df_list[3].Q91.value_counts().head(10)\n","1c77cfc8":"def add_frequencies(ax, ncount):\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.1f} %'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom', size='small', color='black', weight='bold')","1550ea00":"# Countplot of questions_df\ndef countplot_qdf(questions_df, suptitle):\n    n = questions_df.shape[0]\n    \n    # If there is nothing to plot, we stop here\n    if n==0:\n        return\n    \n    # Numbers of rows and cols in the subplots\n    ncols = 3\n    nrows = (n+3)\/\/ncols\n    fig,ax = plt.subplots(nrows, ncols, figsize=(25,6*nrows))\n    fig.tight_layout(pad=9, w_pad=10, h_pad=7)\n    fig.suptitle(suptitle, size=30, fontweight='bold')\n    \n    # Hide exceeding subplots\n    for i in range(n, ncols*nrows):\n        ax.flatten()[i].axis('off')\n        \n    # Countplot for each question\n    for index, row in questions_df.iterrows():\n        plt.sca(ax.flatten()[index])\n        # We add the sort_values argument to always have the same order: Oui, Non...\n        xlabels = df_list[row.df_id].loc[:,row.new_name]\n        xlabels = xlabels.value_counts().index.sort_values(ascending=False)\n        axi = sns.countplot(x=row.new_name,\n                           data=df_list[row.df_id],\n                           order = xlabels)\n        # Wrap long questions into lines\n        axi.set_title(\"\\n\".join(wrap(row.new_name + '. ' + row.question, 60)))\n        axi.set_xlabel('')\n        # We also set a wrap here (for one very long answer...)\n        axi.set_xticklabels([\"\\n\".join(wrap(s, 17)) for s in xlabels])\n        axi.set_ylabel('Nombre de r\u00e9ponses')\n        add_frequencies(axi, row.nbnnull)","4a8ca766":"# Plotting questions, grouped by theme\nfor i in range(len(themes)):\n    countplot_qdf(questions[(questions.closed) & (questions.df_id == i)].reset_index(), themes[i])\n","1f569624":"# Count words in a string, a word being here any sequence of characters between white spaces\ndef count_words(s):\n    if s is np.nan:\n        return(0)\n    return(len(s.split()))","cdbfe8c6":"# For each dataframe:\n# filter on questions and title\n# count words for each contribution of each question\n# sum it all\nn_words = [df.filter(regex=r'title|^Q', axis=1).apply(np.vectorize(count_words)).sum().sum()\\\n           for df in df_list]\nsum(n_words)\n","f7f4f132":"stop_words = [unidecode.unidecode(w.lower()) for w in stops]\n# Add punctuation and some missing stopwords using this website : https:\/\/www.ranks.nl\/stopwords\/french\nstop_words = set(stop_words +\n                 list(string.punctuation) +\n                 [\"\u2019\", \"...\", \"'\", \"\", \">>\", \"<<\"] +\n                 [\"oui\", \"non\", \"plus\", \"toute\", \"toutes\", \"faut\",\"\u00e0\",\"tous\",\"tandis\",\"quels\",\n                  \"alors\",\"au\",\"aucuns\",\"aussi\",\"autre\",\"avant\",\"avec\",\"avoir\",\"juste\",\"la\",\"tout\",\"toutes\",\"tr\u00e8s\",\"trop\",\n\"www\",\"http\",\"html\",\"peu\",\"en\",\"etc\",\"chaque\",\"sans\",\"ne\",\"ils\",\"il\",\"que\",\"quand\",\"quoi\",\"qui\",\"plupart\",\n\"doit\",\"donc\",\"dos\",\"elle\",\"elles\",\"comme\",\"comment\",\"ci\",\"ni\",\"m\u00eame\",\"mais\",\"mes\",\"aussi\",\"alors\",\"an\",\"je\",\"\u00e7a\",\"o\u00f9\",\"org\",\"moi\"\n                 \n                 ])","2bcfa4a5":"# Get tokens from list of strings (can probably be optimised)\ndef get_tokens(s):\n    # MosesTokenizer has been moved out of NLTK due to licensing issues\n    # So we define a simple tokenizer based on regex, designed for French language\n    pattern = r\"[cdjlmnstCDJLMNST]['\u00b4`]|\\w+|\\$[\\d\\.]+|\\S+\"\n    tokenizer = RegexpTokenizer(pattern)\n    tokens = tokenizer.tokenize(\" \".join(s.dropna()))\n    # remove punctuation (for words like \"j'\")\n    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n    # lowercase ASCII\n    tokens = [unidecode.unidecode(w.lower()) for w in tokens]\n    # remove stop words from tokens\n    tokens = [w for w in tokens if w not in stop_words]\n    return(tokens)","7f858a0e":"def plot_wordcloud(s, title, mw = 500):\n    wordcloud = WordCloud(width=1200, height=600, max_words=mw,\n                          background_color=\"white\").generate(\" \".join(s))\n    plt.figure(figsize=(20, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title, fontsize=50, pad=50)\n    plt.show()\n\ncol_q = questions.new_name[~questions.closed].append(pd.Series('title'))\nfor i in range(len(themes)):\n    col_q_i = df_list[i].columns.intersection(col_q)\n    tokens = pd.concat([df_list[i][col].dropna() for col in col_q_i])\n    tokens = get_tokens(tokens)\n    plot_wordcloud(tokens, title = themes[i])","42619672":"df_list[0] = df_list[0].astype(str) \n\n#contains all the answers of questions 1 to 8\nreponse_question1 = df_list[0].Q1\nreponse_question2 = df_list[0].Q2\nreponse_question3 = df_list[0].Q3\nreponse_question4 = df_list[0].Q4\nreponse_question5 = df_list[0].Q5\nreponse_question6 = df_list[0].Q6\nreponse_question7 = df_list[0].Q7\nreponse_question8 = df_list[0].Q8","e069ab72":"reponse_question1.value_counts().head(10)\n","69c227fe":"reponse_question2.value_counts().head(10)\n","86ce450f":"reponse_question3.value_counts().head(10)\n","b6c02747":"reponse_question4.value_counts().head(10)\n","52a8da81":"nltk.download('punkt')\n","036502e6":"def preprocess_text(test):\n\n  \n\n    #test = test.lower()\n    #Removing Numbers\n    test=re.sub(r'\\d+','',test)\n\n    \n    #Removing white spaces\n    test=test.strip()\n    \n    #Replacer replace\n    text_replaced = replacer.replace(test)\n\n      #Tokenize\n    tokenizer=nltk.data.load('tokenizers\/punkt\/french.pickle')\n    sentences = tokenizer.tokenize(text_replaced)\n\n     #Tokenize words\n    tokenizer = WordPunctTokenizer()\n    for i in range(len(sentences)):\n        sentences[i] = tokenizer.tokenize(sentences[i])\n        \n     #Remove stop words\n\n\n    for i in range(len(sentences)):\n        sentences[i] = [word for word in sentences[i] if word not in stop_words]\n\n    for i in range(len(sentences)):\n        for j in range(len(sentences[i])):\n            sentences[i][j] = lemmatizer.lemmatize(sentences[i][j])\n\n\n    #Join the words back into a sentence.\n    a=[' '.join(s) for s in sentences]\n    b=['. '.join(a)]\n\n    return b","b271a98b":"reponse_question1_cleaned = [preprocess_text(doc) for doc in reponse_question1]\nreponse_question1 = [' '.join(r) for r in reponse_question1_cleaned]\n\n\nreponse_question2_cleaned = [preprocess_text(doc) for doc in reponse_question2]\nreponse_question2 = [' '.join(r) for r in reponse_question2_cleaned]\n\n\nreponse_question3_cleaned = [preprocess_text(doc) for doc in reponse_question3]\nreponse_question3 = [' '.join(r) for r in reponse_question3_cleaned]\n\n\nreponse_question4_cleaned = [preprocess_text(doc) for doc in reponse_question4]\nreponse_question4 = [' '.join(r) for r in reponse_question4_cleaned]\n\nreponse_question5_cleaned = [preprocess_text(doc) for doc in reponse_question5]\nreponse_question5 = [' '.join(r) for r in reponse_question5_cleaned]\n\n\nreponse_question6_cleaned = [preprocess_text(doc) for doc in reponse_question6]\nreponse_question6 = [' '.join(r) for r in reponse_question6_cleaned]\n\n\nreponse_question7_cleaned = [preprocess_text(doc) for doc in reponse_question7]\nreponse_question7 = [' '.join(r) for r in reponse_question7_cleaned]\n\n\nreponse_question8_cleaned = [preprocess_text(doc) for doc in reponse_question8]\nreponse_question8 = [' '.join(r) for r in reponse_question8_cleaned]","0a349034":"!pip install pyLdavis","83bb96e4":"import pandas as pd\npd.options.mode.chained_assignment = None \nimport numpy as np\nimport re\nimport nltk\nfrom pprint import pprint\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nSTOP_WORDS = nltk.corpus.stopwords.words()\n\nfrom gensim.utils import simple_preprocess\nfrom smart_open import smart_open\nimport pyLDAvis.gensim # To visualise LDA model effectively\n\nimport os\nfrom collections import defaultdict # For accumlating values\nfrom nltk.corpus import stopwords # To remove stopwords\nfrom gensim import corpora # To create corpus and dictionary for the LDA model\nfrom gensim.models import LdaModel # To use the LDA model","a63a4f65":"reponse1 = pd.DataFrame(reponse_question1)\nreponse1.columns = ['Question1_Quelles sont toutes les choses qui pourraient \u00eatre faites pour am\u00e9liorer information des citoyens sur utilisation des imp\u00f4ts ?']\nreponse1 = reponse1[reponse1['Question1_Quelles sont toutes les choses qui pourraient \u00eatre faites pour am\u00e9liorer information des citoyens sur utilisation des imp\u00f4ts ?']!= 'nan']\nreponse1.head()\n","674b811a":"# Create gensim dictionary form a single tet file\ndictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse1['Question1_Quelles sont toutes les choses qui pourraient \u00eatre faites pour am\u00e9liorer information des citoyens sur utilisation des imp\u00f4ts ?'])\n\n# Token to Id map\ndictionary.token2id","6ddc0c08":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse1['Question1_Quelles sont toutes les choses qui pourraient \u00eatre faites pour am\u00e9liorer information des citoyens sur utilisation des imp\u00f4ts ?']]\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]","03235258":"NUM_TOPICS = 10 # This is an assumption. \nldamodel = LdaModel(mycorpus, num_topics = NUM_TOPICS, id2word=mydict, passes=15)#This might take some time.","6d83c409":"topics = ldamodel.show_topics()\nfor topic in topics:\n    print(topic)\n","626f368b":"word_dict = {};\nfor i in range(NUM_TOPICS):\n    words = ldamodel.show_topic(i, topn = 15)\n    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\npd.DataFrame(word_dict)\n","8d57a7d4":"lda_display = pyLDAvis.gensim.prepare(ldamodel, mycorpus, mydict, sort_topics=False)\npyLDAvis.display(lda_display)","b95e46c6":"reponse2 = pd.DataFrame(reponse_question2)\nreponse2.columns = ['Question_2_Que faudrait-il faire pour rendre la fiscalit\u00e9 plus juste et plus efficace ?']\nreponse2 = reponse2[reponse2['Question_2_Que faudrait-il faire pour rendre la fiscalit\u00e9 plus juste et plus efficace ?']!= 'nan']\n\nreponse2.head()\n","cc9d9258":"# Create gensim dictionary form a single tet file\ndictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse2['Question_2_Que faudrait-il faire pour rendre la fiscalit\u00e9 plus juste et plus efficace ?'])\n\n# Token to Id map\ndictionary.token2id","4cad32e6":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse2['Question_2_Que faudrait-il faire pour rendre la fiscalit\u00e9 plus juste et plus efficace ?']]\n\n# Create the Corpus\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]","28c47f21":"NUM_TOPICS = 10 # This is an assumption. \nldamodel = LdaModel(mycorpus, num_topics = NUM_TOPICS, id2word=mydict, passes=15)#This might take some time.","d7a06b8a":"topics = ldamodel.show_topics()\nfor topic in topics:\n    print(topic)","4f78fae8":"word_dict = {};\nfor i in range(NUM_TOPICS):\n    words = ldamodel.show_topic(i, topn = 15)\n    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\npd.DataFrame(word_dict)","b2ad39df":"reponse3 = pd.DataFrame(reponse_question3)\nreponse3.columns = ['Question_3_Quels sont selon vous les imp\u00f4ts qui faut baisser en priorit\u00e9 ?']\nreponse3 = reponse3[reponse3['Question_3_Quels sont selon vous les imp\u00f4ts qui faut baisser en priorit\u00e9 ?']!= 'nan']","b6133cea":"# Create gensim dictionary form a single tet file\ndictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse3['Question_3_Quels sont selon vous les imp\u00f4ts qui faut baisser en priorit\u00e9 ?'])\n\n# Token to Id map\ndictionary.token2id","4f1ac8f4":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse3['Question_3_Quels sont selon vous les imp\u00f4ts qui faut baisser en priorit\u00e9 ?']]\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n","fe271853":"NUM_TOPICS = 10 # This is an assumption. \nldamodel = LdaModel(mycorpus, num_topics = NUM_TOPICS, id2word=mydict, passes=15)#This might take some time.","87747277":"topics = ldamodel.show_topics()\nfor topic in topics:\n    print(topic)\n","41f4096f":"word_dict = {};\nfor i in range(NUM_TOPICS):\n    words = ldamodel.show_topic(i, topn = 15)\n    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\npd.DataFrame(word_dict)","0e08f41b":"reponse6 = pd.DataFrame(reponse_question6)\nreponse6.columns = ['Question6_Quels sont les domaines prioritaires o\u00f9 notre protection sociale doit \u00eatre renforc\u00e9e ?']\nreponse6 = reponse6[reponse6['Question6_Quels sont les domaines prioritaires o\u00f9 notre protection sociale doit \u00eatre renforc\u00e9e ?']!= 'nan']\nreponse6.head()","1c551d8a":"dictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse6['Question6_Quels sont les domaines prioritaires o\u00f9 notre protection sociale doit \u00eatre renforc\u00e9e ?'])\n\n# Token to Id map\ndictionary.token2id","85f3d9f6":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse6['Question6_Quels sont les domaines prioritaires o\u00f9 notre protection sociale doit \u00eatre renforc\u00e9e ?']]\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]","f5327a8e":"## Interpr\u00e9tation\n\nLes imp\u00f4ts qui faut baisser en priorit\u00e9 sont la CSG des retrait\u00e9s, la TVA, ISF mais \u00e9galement l'impot sur le revenu. Les charges sociales pour les PME ainsi que la taxe habitation et la taxe fonciere. Il est vraiment tres interessant de voir ce que pense les fran\u00e7ais \u00e0 travers cette analyse.","349adf4e":"# Pretreatments","8e140e79":"## Le grand debat","ffeeb021":"# Unsupervised analysis","352940f3":"Those 19 questions are closed-ended question: the answer is forced into a few choices, mainly Yes or No.\n\nWe can now aggregate at the theme scale:","7e07243a":"The number of contribution per hour reaches a peak in the late afternoon, between 18h and 19h","992bfd96":"## Interpr\u00e9tation \nLes domaines prioritaires o\u00f9 notre protection sociale doit \u00eatre renforc\u00e9e sont sans aucun doute la sant\u00e9 et l'\u00e9ducation qui ressort beaucoup de ce tableau. Au niveau de tout ce qui est medicale( h\u00f4pital, m\u00e9decin, etc.). Ainsi qu'au niveau des soins et de la prise en charge avec notamment le remboursement. (mutuelle, m\u00e9dicament, etc.)Aider les personnes \u00e2g\u00e9es et handicap\u00e9es. Mais aussi les allocations pour les familles en difficult\u00e9s. Il faudrait aussi mettre en place des formations. Les fran\u00e7ais insiste sur l'assurance chomage.","d8aa8f75":"We can see a first peak at the very beginning of the Grand D\u00e9bat.\n\nLet's look also at the time the contributions were made:","00d1a8fb":"We can notice that some questions have very few distinct answers:","2ebc83bf":"## Import dataset","e759b0af":"In this section we will have a closer look at the authors of the Grand D\u00e9bat. For each contribution we have an authorID that is shared among datasets.\n","69032246":"We notice that the great majority of respondents, are citizens  i.e. they are neither politicals, officials nor part of an organisation.\n\nAfter this very brief dataset analysis, it is time to focus on the variables of interest: the questions. Each dataframe contains several questions, but we will try to treat them all at once.\n\nThe column names for the questions are a bit messy, we will rename them for more clarity. We build a dataframe containing information about each question: old and new name, title, and the theme and dataframe they are linked to.\n","348f415d":"The contributions contain 95 million words!\n\nLet's focus on the 75 open questions. \nWe first remove all stop words, those are the most common words that don't give any insight, and must be filtered out when doing natural language processing.","b3abf44c":"\nFor each of the 19 close questions, we plot the count of each answer in order to identify most popular opinions.\n\nWe use the seaborn library for plotting.","4468b697":"There are more than 150,000 distinct contributors.","fc488288":"The 4 dataframes share some common variables, other columns are questions that are specific to the theme. The common variables are the following:\n","f4f2bfb5":"##  Let's focus on question 6 :  Quels sont les domaines prioritaires o\u00f9 notre protection sociale doit \u00eatre renforc\u00e9e ?","113eee93":"Let's have a look at the missing values\n","1ccf8d62":"##### We can ask ourself when were the contributions submitted?","d9f00181":"As can be seen, around 50% of the authors submitted a single contribution.","1093eaba":"Some other questions have low unique_rate because they are guided question: choices were given but the respondant could decide to answer something else. This is the case for instance for questions Q91, Q79 and Q4:","e974bcc2":"### Let's have a look at the different possible answer","a15c5f3d":"## Interpr\u00e9tation\nIl faudrait diminuer le prix du carburant, faire attention \u00e0 l'\u00e9vasion fiscale.S upprimer la CSG pour tous les retrait\u00e9s ainsi que la taxe d'habitation. Il faudrait \u00e9galement selon eux r\u00e9tablir l'ISF.On peut voir qu'il faut supprimer la CSG. Il faut faire attention \u00e0 l'\u00e9vasion fiscale \u00e9galement. Il faudrait \u00e9galement supprimer les niches fiscales et r\u00e9duire les avantages des hauts fonctionnaires.","8c1ef054":"We can see that all of those questions start with \"Si\". They are conditional: an answer is not necessarily expected.\nThat's eplain why.\n\nIf we pay attention we can notice that the unique_rate is also very low, this is because a lot of contributors answered \"non concern\u00e9\" (\"not applicable\"), for instance with question Q40:","578594d5":"We can see that the dataset concatened contains 94 questions.\n\nWe created for ourself a csv to a better understanding of all the questions.","f1514baa":"## Daily contributions\n","2ace050c":"we see that there are lot of null values, we want to understand that. Let's see which questions have the most null values:","234c477b":"On the themes of State organisation, democracy and citizenship: when asked their opinion, contributors always take side for change.\n\nIt's very interesting to see all these opinions.\n\n### Open questions analysis\n\nMost of the information of the dataset lies in the open questions, but they are the most difficult to analysis!\n\nWe can start with with a basic statistic, the number of words contained in the whole dataset.","ec91143b":"## Import libraries","9c11a87c":"The first statistics we can get out of this new dataframe is the number of distinct contributors","63273fac":"##  Let's focus on question 3 :  Quels sont selon vous les imp\u00f4ts qu'il faut baisser en priorit\u00e9 ?","c12ff618":"Each line of the dataframes corresponds to one contribution: the answers of an author to the questions of the corresponding theme. Let's see how many contributions we have for each dataset, and how many questions:\n","cf635dd4":"For each question, we compute the following statistics:\n\n- nbrow: number of rows (i.e. number of contributions for the corresponding theme)\n- nbnnull: number of answers that are not null (answer is null if the contributor skipped that question)\n- nbunique: number of distinct answers\n- nnull_rate: nbnnull\/nbrow * 100\n- unique_rate: nbunique\/nbnnull * 100","f3ca8d18":"We will have a look at the createdAt variable to spot when the contributions were submitted, and at what time of the day.\n","56bb9277":"### Closed questions analysis","4f606276":"## Discovering the dataset","c63bd1ac":"Since we focus on contributors, we aggregate the table by authorId in order to have one line per author. If an author has several authorType or authorZipCode, we keep the most frequent one: the mode.\n\nWe also add a count statistics: how many contributions that author made over the whole dataset.","8786e860":"### Who are the contributors?","8b7f5d61":"We can now import each file, all in one list of dataframes for easier use.\n\nWe pay special attention to data types: ZipCode must be read as strings and date columns as timestamps.","824cd43a":"The next important step is to run a tokenization, i.e. splitting text into words. This might be tricky because of punctuation, wich is slightly different according to the language. There are some important features we have to take into considreation: punctuation, case, encoding and stop words.","8182b7ea":"## Interpretation \n\nAvant de commenter notre tableau, on peut voir avec un value_counts() qu'une des choses qui pourraient \u00eatre faites pour am\u00e9liorer information des citoyens sur utilisation des imp\u00f4ts est bien la transparence. Ce qui resort sont les m\u00e9dias ou des sites publiques d\u00e9di\u00e9s \u00e0 l\u2019utilisation des imp\u00f4ts bien expliquer avec des informations concr\u00e8tes seraient b\u00e9n\u00e9fiques. Il faut faire des debats, partager l'information via la tv, les journaux et des emissions. Mais surtout etre transparent, il faudrait des forme simples. On peut voir \u00e9galement une sorte de plainte au niveauu des avantages des salaires de haut fonctionnaires travaillant dans la politique.","0e3f5c68":"## Let's analyze question 2 : Que faudrait-il faire pour rendre la fiscalit\u00e9 plus juste et plus efficace ?","fa144654":"# Let's start with La fiscalit\u00e9 et les d\u00e9penses publiques","717a4933":"We will use the tokens to draw a word cloud. This is a visual representation of n-gram counts. The more frequent a term is, the bigger it will appear on the plot.\n\nLet's plot a wordcloud for each of the 4 themes. We will see what are the most raised topics among each of them.","0144ebde":"## Let's start with La fiscalit\u00e9 et les d\u00e9penses publiques"}}