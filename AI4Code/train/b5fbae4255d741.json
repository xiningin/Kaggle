{"cell_type":{"800af837":"code","5cd06ad5":"code","8dab3548":"code","6c078bc6":"code","f6d87c8a":"code","9432e613":"code","44666be3":"code","2255199e":"code","fc36fc0d":"code","9d7fdfd0":"code","fd161e38":"code","d6b5a882":"code","0b08e9d0":"code","803973a6":"code","db32ff50":"code","cb336c47":"code","20f4ee70":"code","41f1a648":"code","490ea9db":"code","b7915d97":"code","9ab14223":"code","98da5d62":"code","6ef6ab4f":"code","0a1f045c":"code","d8b24d23":"code","81b5726c":"code","881eddd8":"code","2d074318":"code","dc73b5bb":"code","08aca98f":"code","c5b886e3":"code","f551801b":"code","f30ab68f":"code","ec9cb849":"code","a14c2ecf":"code","5b52f6d0":"code","b9919599":"code","82ced824":"markdown","3494ee8d":"markdown","115c9a08":"markdown","70fa635c":"markdown","cc66e1d9":"markdown","96a29ce2":"markdown","e8e8bdf3":"markdown","b391f1e8":"markdown","3e2aa5a7":"markdown","fea9fa3c":"markdown","fbaa67c9":"markdown","557b2071":"markdown","820b02b1":"markdown","6504ef13":"markdown","cf24b981":"markdown","bf445477":"markdown","2f3eb12c":"markdown","3b3e2981":"markdown","092d7c18":"markdown","e53287b0":"markdown","9213b92b":"markdown","9d625b82":"markdown","1de34bb6":"markdown","eade1a13":"markdown","9fcf8033":"markdown","dd430d82":"markdown","da5787cd":"markdown","3f3a5061":"markdown","c845c870":"markdown","af480ce2":"markdown","567be45b":"markdown","850d41fe":"markdown","c77282cd":"markdown","6765d0de":"markdown","d0e19b07":"markdown","c03314dd":"markdown","3abc953c":"markdown","d533fcf5":"markdown","fc896945":"markdown","896c682b":"markdown","ed662848":"markdown","78425a98":"markdown","b0cb7127":"markdown","75a35169":"markdown","dfe5a09b":"markdown"},"source":{"800af837":"#Basic \nimport numpy as np \nimport pandas as pd\nimport collections\n\n#Plotting \nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as p3\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.basemap import Basemap as Basemap\n\n#Regular modeling\nimport folium\nimport squarify\nimport matplotlib\nimport statsmodels.api as sm\nfrom itertools import groupby\nfrom operator import itemgetter\nfrom math import sqrt\n\n#Extended modeling\nimport matplotlib\nfrom matplotlib.colors import rgb2hex\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PolyCollection\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn import metrics, mixture, cluster, datasets\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.decomposition import PCA,SparsePCA,KernelPCA,NMF\n\nfrom mlxtend.preprocessing import minmax_scaling\n\nimport xgboost as xgb\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","5cd06ad5":"df_governors = pd.read_csv('..\/input\/us-election-2020\/governors_county_candidate.csv')\ndf_president = pd.read_csv('..\/input\/us-election-2020\/president_county_candidate.csv')\ndf_senate = pd.read_csv('..\/input\/us-election-2020\/senate_county_candidate.csv')\n\nprint(\"The number of rows for senate: \" + format(df_senate.shape[0]) + \". The number of factors: \" + format(df_senate.shape[1]))\nprint(\"The number of rows for president: \" + format(df_president.shape[0]) + \". The number of factors: \" + format(df_president.shape[1]))\nprint(\"The number of rows for governors: \" + format(df_governors.shape[0]) + \". The number of factors: \" + format(df_governors.shape[1]))","8dab3548":"df_president.head(5)","6c078bc6":"df_president_agg_REP  = df_president[(df_president.party == \"REP\")]\ndf_president_agg_DEM  = df_president[(df_president.party == \"DEM\")]\ndf_president_agg_REP = pd.DataFrame(df_president_agg_REP.groupby(['state', 'county'])['total_votes'].sum())\ndf_president_agg_DEM = pd.DataFrame(df_president_agg_DEM.groupby(['state', 'county'])['total_votes'].sum())\ndf_president_agg_REP = df_president_agg_REP.rename(columns={\"total_votes\": \"votes REP president\"})\ndf_president_agg_DEM = df_president_agg_DEM.rename(columns={\"total_votes\": \"votes DEM president\"})\n\ndf_president_agg = pd.merge(df_president_agg_DEM, df_president_agg_REP, on=['state', 'county'],how='left')\ndf_president_agg.head(5)","f6d87c8a":"df_senate_agg_REP  = df_senate[(df_senate.party == \"REP\")]\ndf_senate_agg_DEM  = df_senate[(df_senate.party == \"DEM\")]\ndf_senate_agg_REP = pd.DataFrame(df_senate_agg_REP.groupby(['state', 'county'])['total_votes'].sum())\ndf_senate_agg_DEM = pd.DataFrame(df_senate_agg_DEM.groupby(['state', 'county'])['total_votes'].sum())\ndf_senate_agg_REP = df_senate_agg_REP.rename(columns={\"total_votes\": \"votes REP senate\"})\ndf_senate_agg_DEM = df_senate_agg_DEM.rename(columns={\"total_votes\": \"votes DEM senate\"})\ndf_senate_agg = pd.merge(df_senate_agg_DEM, df_senate_agg_REP, on=['state', 'county'],how='outer')\n\ndf_governors_agg_REP  = df_governors[(df_governors.party == \"REP\")]\ndf_governors_agg_DEM  = df_governors[(df_governors.party == \"DEM\")]\ndf_governors_agg_REP = pd.DataFrame(df_governors_agg_REP.groupby(['state', 'county'])['votes'].sum())\ndf_governors_agg_DEM = pd.DataFrame(df_governors_agg_DEM.groupby(['state', 'county'])['votes'].sum())\ndf_governors_agg_REP = df_governors_agg_REP.rename(columns={\"votes\": \"votes REP governors\"})\ndf_governors_agg_DEM = df_governors_agg_DEM.rename(columns={\"votes\": \"votes DEM governors\"})\ndf_governors_agg = pd.merge(df_governors_agg_DEM, df_governors_agg_REP, on=['state', 'county'],how='outer')\n\ndf = pd.merge(df_president_agg, df_senate_agg, on=['state', 'county'],how='left')\ndf = pd.merge(df, df_governors_agg, on=['state', 'county'],how='left')\ndf = df.fillna(0)\ndf.head(5)","9432e613":"df_census = pd.read_csv('..\/input\/us-census-demographic-data\/acs2017_county_data.csv')\ndf_census = df_census.rename(columns={\"State\": \"state\",\"County\": \"county\"})\ndf_census.head(5)","44666be3":"vars_to_merge = [x for x in df_census.columns if x not in ['CountyId', 'state','county']]\n\ndf_census_agg = pd.DataFrame(df_census.groupby(['state', 'county'])[vars_to_merge].sum())\ndf_census_agg.head(5)","2255199e":"df = pd.merge(df, df_census_agg, on=['state', 'county'],how='left')\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,5)) + \" %\")","fc36fc0d":"df_null = df[df.isnull().any(axis=1)]\nNull_county = np.array(df_null.index.get_level_values('county'))\nNull_county","9d7fdfd0":"df['votes REP president perc'] = df['votes REP president']  \/ (df['votes REP president']+df['votes DEM president'])\ndf['votes DEM president perc'] = df['votes DEM president']  \/ (df['votes REP president']+df['votes DEM president'])\ndf['votes REP senate perc'] = df['votes REP senate']  \/ (df['votes REP senate']+df['votes DEM senate'])\ndf['votes DEM senate perc'] = df['votes DEM senate']  \/ (df['votes REP president']+df['votes DEM senate'])\ndf['votes REP governors perc'] = df['votes REP governors']  \/ (df['votes REP governors']+df['votes DEM governors'])\ndf['votes DEM governors perc'] = df['votes DEM governors']  \/ (df['votes REP governors']+df['votes DEM governors'])\n\nvotes_perc = df.iloc[:,-6:].columns\nvotes = [x for x in df.columns if x not in df_census_agg.columns]\nfactors = [x for x in df.columns if x not in votes]\ndf = df.fillna(0)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,5)) + \" %\")","fd161e38":"print('Number of rows: '+ format(df.shape[0]) +', number of features: '+ format(df.shape[1]))","d6b5a882":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\nprint('Number of numeric variables: '+ format(len(NumericVariables)) +', number of categorical features: '+ format(len(CategoricalVariables)))","0b08e9d0":"df_final = minmax_scaling(df, columns=df.columns)\ndf_final.head(5)","803973a6":"SpearmanCorr = df_final.corr(method=\"spearman\")\nx = SpearmanCorr[votes_perc]\nmatplotlib.pyplot.figure(figsize=(15,20))\nsns.heatmap(x, vmax=.9, square=True, annot=True, linewidths=.3, cmap=\"YlGnBu\", fmt='.1f')","db32ff50":"color_1 = plt.cm.Reds(np.linspace(0.6, 1, 66))\ncolor_2 = plt.cm.Blues(np.linspace(0.6, 1, 66))\n\nstates = df.groupby('state').agg({'votes REP president':'sum','votes DEM president':'sum'}).sort_values(by='state',ascending=False)\nstates['Total votes'] = states['votes REP president'] + states['votes DEM president']\nstates['votes REP president perc'] = states['votes REP president'] \/ states['Total votes']\nstates['votes DEM president perc'] = states['votes DEM president'] \/ states['Total votes']\n\ndata1 = states['votes REP president perc']\ndata2 = states['votes DEM president perc']\n\nplt.figure(figsize=(16,8))\nx = np.arange(51)\nax1 = plt.subplot(1,1,1)\nw = 0.3\n\ncolor = color_1\nplt.title('Number of votes by political party')\nplt.xticks(x + w \/2, data1.index, rotation=-90)\nax1.set_xlabel('States')\nax1.set_ylabel('Percentage of votes')\nax1.bar(x,data1.values,color=color_1,width=w,align='center')\n\ncolor = color_2\nax2 = ax1\nax2.bar(x + w,data2, color=color_2,width=w,align='center')\n\nplt.show()","cb336c47":"fig = plt.figure(figsize=(20, 16), dpi=70, facecolor='w', edgecolor='k')\n\nax = fig.add_subplot(211)\nax.set_title(\"Republicans votes\")\nm = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,\n        projection='lcc',lat_1=33,lat_2=45,lon_0=-95)\n\nshp_info = m.readshapefile('..\/input\/basemaps\/st99_d00','states',drawbounds=True)\n\ncolors={}\nstatenames=[]\ncmap = plt.cm.Reds \nvmin = min(data1); vmax = max(data1)\nfor shapedict in m.states_info:\n    statename = shapedict['NAME']\n    if statename not in ['District of Columbia','Puerto Rico']:\n        pop = data1[statename]\n\n        colors[statename] = cmap(np.sqrt((pop-vmin)\/(vmax-vmin)))[:3]\n    statenames.append(statename)\nax = plt.gca() \nfor nshape,seg in enumerate(m.states):\n    if statenames[nshape] not in ['District of Columbia','Puerto Rico']:\n        color = rgb2hex(colors[statenames[nshape]]) \n        poly = Polygon(seg,facecolor=color,edgecolor=color)\n        ax.add_patch(poly)\n\nax = fig.add_subplot(212)\nax.set_title(\"Democrats votes\")\nm = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,\n        projection='lcc',lat_1=33,lat_2=45,lon_0=-95)\n\nshp_info = m.readshapefile('..\/input\/basemaps\/st99_d00','states',drawbounds=True)\n\ncolors={}\nstatenames=[]\ncmap = plt.cm.Blues \nvmin = min(data2); vmax = max(data2)\nfor shapedict in m.states_info:\n    statename = shapedict['NAME']\n    if statename not in ['District of Columbia','Puerto Rico']:\n        pop = data2[statename]\n\n        colors[statename] = cmap(np.sqrt((pop-vmin)\/(vmax-vmin)))[:3]\n    statenames.append(statename)\nax = plt.gca() \nfor nshape,seg in enumerate(m.states):\n    if statenames[nshape] not in ['District of Columbia','Puerto Rico']:\n        color = rgb2hex(colors[statenames[nshape]]) \n        poly = Polygon(seg,facecolor=color,edgecolor=color)\n        ax.add_patch(poly) \n\nplt.show()","20f4ee70":"X = df_final[factors].values\n\nGM_n_components = np.arange(1, 15)\nGM_models = [mixture.GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in GM_n_components]\n\nplt.figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='r')\nplt.plot(GM_n_components, [m.aic(X) for m in GM_models], label='AIC')\nplt.tight_layout()\nplt.legend(loc='best')\nplt.xlabel('n_components');","41f1a648":"GM_n_classes = 2\n\nGMcluster = mixture.GaussianMixture(n_components=GM_n_classes, covariance_type='full',random_state = 0)\nGMcluster_fit = GMcluster.fit(df_final)\nGMlabels = GMcluster_fit.predict(df_final)\n\nprint('Number of clusters: ' + format(len(np.unique(GMlabels))))","490ea9db":"unique, counts = np.unique(GMlabels, return_counts=True)\ndict(zip(unique, counts))","b7915d97":"fig = plt.figure(figsize=(8, 6),facecolor='w', edgecolor='r')\nax = p3.Axes3D(fig, rect = (1, 1, 1, 1))\nax.set_xlim3d(0.2, 0.8)\nax.set_ylim3d(0.2, 0.45)\nax.set_zlim3d(0.2, 0.8)\nax.view_init(10, 40)\nfor l in np.unique(GMlabels):\n    ax.scatter(X[GMlabels == l, 0], X[GMlabels == l, 1], X[GMlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(GMlabels + 1)),s=20, edgecolor='k')\nplt.title('Expectation-maximization algorithm for business features clustering' )\n\nplt.show()","9ab14223":"df_final['Party_Cluster'] = GMlabels","98da5d62":"pca = PCA().fit(df_final)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), dpi=70, facecolor='w', edgecolor='k')\nax0, ax1 = axes.flatten()\n\nsns.set('talk', palette='colorblind')\n\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\n\nax0.plot(np.cumsum(pca.explained_variance_ratio_), marker='.')\nax0.set_xlabel('Number of components')\nax0.set_ylabel('Cumulative explained variance');\n\nax1.bar(range(df_final.shape[1]),pca.explained_variance_)\nax1.set_xlabel('Number of components')\nax1.set_ylabel('Explained variance');\n\nplt.tight_layout()\nplt.show()","6ef6ab4f":"n_PCA_90 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.9) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.9)\nprint(\"Already: \" + format(n_PCA_90) + \" components cover 90% of variance.\")","0a1f045c":"pca = PCA(5).fit(df_final)\n\nX_pca=pca.transform(df_final) \n\nplt.matshow(pca.components_,cmap='viridis')\nplt.yticks([0,1,2,3,4,5],['1st Comp','2nd Comp','3rd Comp','4th Comp','5th Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks(range(len(df_final.columns)),df_final.columns,fontsize=10,rotation=90)\nplt.tight_layout()\nplt.show()","d8b24d23":"PCA_vars = [0]*len(df_final.columns)\n\ndef ExtractColumn(lst,j): \n    return [item[j] for item in lst] \n\nfor i, feature in zip(range(len(df_final.columns)),df_final.columns):\n    x = ExtractColumn(pca.components_,i)\n    if ((max(x) > 0.2) | (min(x) < -0.2)):\n        if abs(max(x)) > abs(min(x)):\n            PCA_vars[i] = max(x)\n        else:\n            PCA_vars[i] = min(x)                 \n    else:\n        PCA_vars[i] = 0\n\nPCA_vars = pd.DataFrame(list(zip(df_final.columns,PCA_vars)),columns=('Name','Max absolute contribution'),index=range(1,48,1))      \nPCA_vars = (PCA_vars[(PCA_vars['Max absolute contribution']!=0)]).sort_values(by='Max absolute contribution',ascending=False)\nPCA_vars","81b5726c":"df_final_estimation = df_final.drop(Null_county, level='county')\n\nResponse = pd.DataFrame(df_final_estimation['votes REP president perc'] + df_final_estimation['votes REP senate perc'] + df_final_estimation['votes REP governors perc'],columns=[\"Response\"])\nResponse = minmax_scaling(Response, columns=Response.columns)\n#df_final['ResponseDEM'] = df_final['votes DEM president perc'] + df_final['votes DEM senate perc'] + df_final['votes DEM governors perc']\n\ndf_final_estimation = df_final_estimation.drop(votes, axis=1)\n\nx_train,x_test,y_train,y_test = train_test_split(df_final_estimation,Response,test_size=0.2,random_state=0)","881eddd8":"ModelAverage = y_train.mean()\nprint(str(round(ModelAverage,5)))","2d074318":"RMSE = y_test\nRMSE.insert(1, \"Model_Average\", ModelAverage.values[0], True)\ny_test=y_test.drop(['Model_Average'], axis=1)\nRMSE.head(5)","dc73b5bb":"Model_GLM = sm.GLM(y_train, x_train,family=sm.families.Gaussian())\n\nModel_GLM_fit = Model_GLM.fit()\n\nprint(Model_GLM_fit.summary())","08aca98f":"RMSE.insert(2, \"Model_GLM\", Model_GLM_fit.predict(x_test).values, True)","c5b886e3":"space={ 'max_depth': hp.quniform(\"max_depth\", 2,25,1),\n        'alpha': hp.uniform ('alpha', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 25, 1),\n        'learning_rate' : hp.uniform('learning_rate', 0.00001, 0.8),\n        'n_estimators': 500,\n        'seed': 0}\n\ndef f_rmse(predictions, targets):\n    return np.sqrt(np.mean((predictions-targets)**2))\n\ndef objective(space):\n    clf = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), alpha = space['alpha'],\n                        learning_rate = space['learning_rate'],min_child_weight=space['min_child_weight'],colsample_bytree=space['colsample_bytree'])\n\n    evaluation = [( x_train, y_train), ( x_test, y_test)]\n    \n    clf_fit = clf.fit(x_train, y_train,verbose=False,eval_set=evaluation,early_stopping_rounds=20)\n    \n    pred = clf_fit.predict(x_test)\n    score = f_rmse(RMSE['Response'].values, pred)\n    print(format(score))\n    return {'loss': score, 'status': STATUS_OK }\n\ntrials = Trials()\n\nbest_hyperparams = fmin(fn = objective,space = space,algo = tpe.suggest,max_evals = 100,trials = trials)","f551801b":"print(\"The best hyperparameters are : \",\"\\n\")\nprint(best_hyperparams)","f30ab68f":"Model_XGB = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.7046466436203225, learning_rate = 0.02192155309970599,\n                max_depth = 16, alpha = 0.4135415520098815, n_estimators = 300,min_child_weight=14)\n\nModel_XGB_fit = Model_XGB.fit(x_train,y_train)","ec9cb849":"xgb.plot_tree(Model_XGB_fit,num_trees=0)\nplt.rcParams['figure.figsize'] = [80,50]\nplt.show()","a14c2ecf":"xgb.plot_importance(Model_XGB_fit)\nplt.rcParams['figure.figsize'] = [12, 10]\nplt.show()","5b52f6d0":"RMSE.insert(3, 'Model_XGB', Model_XGB_fit.predict(x_test), True)\nRMSE.head(5)","b9919599":"Model_Average_RMSE =     f_rmse(RMSE['Response'].values, RMSE['Model_Average'].values)   \nModel_GLM_RMSE =      f_rmse(RMSE['Response'].values, RMSE['Model_GLM'].values)    \nModel_XGB_RMSE =      f_rmse(RMSE['Response'].values, RMSE['Model_XGB'].values)  \n\nResults = pd.DataFrame({'RMSE': [Model_Average_RMSE,Model_GLM_RMSE,Model_XGB_RMSE],'Name': ['Model_Average','Model_GLM','Model_XGB']})\nResults = Results.set_index('Name')\n\nplt.plot(Results)\nplt.ylabel('RMSE results')\nplt.show()","82ced824":"Two big states (California and Texas) remain the centers of power of opposing parties. Middle part of country is kingdom of Republicans. Coasts prefer Democrats. Nothing surprising, but good check whther data makes sense.","3494ee8d":"And plotting it in 3 dimensions:","115c9a08":"And importance. One can compare it with GLM importance:","70fa635c":"# 1.Data preparation\n\nI will focus on preparing and cleaning the main voting data. But first let's include useful libraries:","cc66e1d9":"Interesting observations:\n* Republican president's vote are really correlated with race. He receives votes mostly from white people. Moreover his votes come from people with low income, non self-employed\n* Democratic president has majority of correlation perfectly opposite\n\nI plot percentages of votes by states:","96a29ce2":"**Please upvote if you like it :)**","e8e8bdf3":"Let's look at Spearman correlation: (why not Pearson? [Here explanation](https:\/\/www.kaggle.com\/jjmewtw\/yt-pearson-spearman-distance-corr-rv-coef)):","b391f1e8":"Indeed, as I am interested in finding wheather cluster captures differences between Republicans and Democrats I impose only two of them:","3e2aa5a7":"Also added to the test data set:","fea9fa3c":"# 4.Multi-way\n\nNow I want to look in a bit more granular way. I would like to capture some interesting multi-dimensional trends. First clustering, precisely GM clustering ([more information here](https:\/\/www.kaggle.com\/jjmewtw\/clustering-k-means-hierarchical-debscan-ema)). First, using Akaike I assess what would be the perfect number of clusters. Using legendary 'elbow rule', I would say something like 2 clusters:","fbaa67c9":"Plotting the tree as an example:","557b2071":"# 5.Estimation\n\nFinal chapter: I want to predict how republican \/ democratic given county is on the basis of demographic data. For this I define my response as sum of percentage votes on republican candidates for presidential, senate, governor elections. To keep it between 0 and 1, I scale it. In this way it is easier explainable. ","820b02b1":"Further, one of the most popular models in Kaggle: extreme boosting. For this I define my own cross validation with the given space and CV function:","6504ef13":"Add the end I look at RMSE (Root Mean Square Error):","cf24b981":"![image.png](attachment:image.png)","bf445477":"Let's add this cluster to the data, maybe it will be useful:","2f3eb12c":"This is the data I would like to continue with. All the information is in the one data set. I have votes for president, senate and governors by the same keys. ","3b3e2981":"Let's look at presidential one:","092d7c18":"# 3.One-way analysis\n\nOnce the data is prepared, cleaned and enriched, I can focus on some visualizations. First, let's scale the data. It will be useful for PCA and clustering in the further part:","e53287b0":"And define the same keys:","9213b92b":"Wyoming is the Republican paradise, while DC Columbia is democratic one. I look at them also on the map:","9d625b82":"![image.png](attachment:image.png)","1de34bb6":"Furthermore, number of votes is nice number but it really makes better sense to rewrite it in percentage form, for example: republican_votes \/ (all votes). For the moment, I replace all NA's with 0's.","eade1a13":"Please notice, that clustering is unsupervised method. It means that I didn't tell it what it tries to predict. It just grouped some behaviours observed in the data.\n\nNext method is PCA ([more information here](https:\/\/www.kaggle.com\/jjmewtw\/total-analysis-of-pca-sparse-pca-nmf-kernel-pca)). I will explore, whether it is possible to create linear transformation of the given data to capture all the infromation. In this case, I use whole data set (votes + demographic), why? I would like to explore whether demographic information is sufficient to capture all trends, or some information is only present in voting data set. The distribution of explained variance:","9fcf8033":"Easy, then I can continue with numeric variables without need of dummy transformation or ordinal one.","dd430d82":"Downloading 3 data sets. For president elections, senate and governors:","da5787cd":"I define final test data set:","3f3a5061":"I will treat state and county as keys. Candidates anyway we know. New data form:","c845c870":"First real model will be Generalized Linear Model. Let's assume Gaussianity for time being:","af480ce2":"# 0.Introduction\n\nThe goal of the notebook is to analyze the results of American elections and plot them with use of one-way and multi-way methods. The work was split in the following chapters:\n1. Data preparation: loading it libraries and data. Mergind data to have consistent structure\n2. Data enrichment: adding the external demographic data to votes information\n3. One-way analysis: plotting by use of one-way methods to show votes on bar graphs and geographical maps\n4. Multi-way analysis: Gaussian Mixture clustering & PCA\n5. Estimation: can we guess what counties vote for given parties","567be45b":"Our final data:","850d41fe":"Quite a lot. Why? Some small counties, like in Alaska or deeply in dessert where out of this data set. I will keep this structure for time being, but I will make a list of counties which have nulls:","c77282cd":"How many PC's we need:","6765d0de":"Presenting it in table, only the most contributing variables:","d0e19b07":"I print best hyperparameters:","c03314dd":"To sum up: extreme boosting performs the best, that's true. But still for example difference between GLM and just average is neglectable. Probably because Gaussianity is wrong assumption, also the link is identical, I could think about some calibration of GLM. All in all, we can see that we can more less predict how counties vote, but the margin of error is really big.","3abc953c":"And the check for data types:","d533fcf5":"Conclusions:\n* a lot of information is present only in voting data set. In other words, it will be hard to predict votes by demographic data\n* 'Party cluster' built only on the basis of demographic data is quite useful\n* race is a big thing in the data set","fc896945":"Race is a big thing again. I add these reults to the main table:","896c682b":"# 2.Data enrichment\n\nThis was already quite interesting data. But what I can easily do is data enrichment. Thanks to defined key (state & county), I can merge some demographic data. Let's include this census 2017 data set:","ed662848":"And include them to calibrated xgb:","78425a98":"Good result for linear transformation. Breaking it down further. I want to check what variables contribute the most to these top 5 components:","b0cb7127":"Okay, merging votes data with demogrpahic data. How much NA's we have now?","75a35169":"My goal is to have all elections in one data file. I will aggregate data sets in the same way and merge together:","dfe5a09b":"As it is type of best rpactice, my first model is 'average'. I just calculate average value for Repsonse on my train data set:"}}