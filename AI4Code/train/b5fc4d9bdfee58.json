{"cell_type":{"41061316":"code","94db00b9":"code","23b18f59":"code","fdcd8e54":"code","b905a42c":"code","5167539e":"code","756860b4":"code","6f771074":"code","35a5fc2a":"code","d31ba82a":"code","4b20c5ab":"code","262e8aa5":"code","94840b4f":"code","b7a7b892":"code","688957a6":"code","11594d0c":"code","e14d0dba":"code","45eddeb9":"code","25451ad3":"code","ee0a0653":"code","27c47f61":"code","6b88e408":"code","68fc50a9":"code","8560f2a5":"code","7ff8c5ee":"code","8457899a":"code","e1b36177":"code","1342f9fa":"code","85269086":"code","78c37eba":"code","187ed742":"code","483534c7":"code","ceed33bf":"markdown","9d610c67":"markdown","ede01ce4":"markdown","b19a6518":"markdown","c8814b00":"markdown","109fe20c":"markdown","cb37b1c0":"markdown","81fc5407":"markdown","74eb618a":"markdown","fb3edb5d":"markdown","6b34b45d":"markdown","7c8d5025":"markdown","927d0d72":"markdown","837eac06":"markdown","ac0b3de7":"markdown","517d8837":"markdown","ff90df67":"markdown","995430d7":"markdown","48bc57bd":"markdown","4e2d4170":"markdown","1bb0ae9b":"markdown","3ba412e3":"markdown","d1f5d2ae":"markdown","00d48807":"markdown","280d4c91":"markdown","8e801187":"markdown","7a9ad275":"markdown","f6553507":"markdown"},"source":{"41061316":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94db00b9":"train_df = pd.read_csv('..\/input\/loan-prediction-analytics-vidhya\/train_ctrUa4K.csv')","23b18f59":"train_df.head()","fdcd8e54":"train_df.isnull().sum()","b905a42c":"import pandas_profiling\n\ntrain_report = pandas_profiling.ProfileReport(train_df)\ntrain_report","5167539e":"plt.figure(figsize=(8,8))\ntrain_df.dtypes.value_counts().plot.pie(explode=[0.1,0.1,0.1],autopct='%1.1f%%',)\n\nplt.title('Data Type %')\nplt.show()","756860b4":"# For All Categorical Data\n\ntrain_df['Gender'].fillna(train_df['Gender'].mode()[0], inplace=True)\n\ntrain_df['Married'].fillna(train_df['Married'].mode()[0], inplace=True)\n\ntrain_df['Dependents'].fillna(train_df['Dependents'].mode()[0], inplace=True)\n\ntrain_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0], inplace=True)\n\ntrain_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].mode()[0], inplace=True)\n\ntrain_df['Credit_History'].fillna(train_df['Credit_History'].mode()[0], inplace=True)","6f771074":"# For Continious data\ntrain_df['LoanAmount'].fillna(train_df['LoanAmount'].mean(), inplace=True)","35a5fc2a":"train_df.isnull().sum()","d31ba82a":"train_df['Gender'] = train_df['Gender'].map({'Male': 0, 'Female': 1})\ntrain_df['Married'] = train_df['Married'].map({'No': 0, 'Yes': 1})\ntrain_df['Dependents'] = train_df['Dependents'].map({'0': 0, '1': 1, '2': 2, '3+': 3})\ntrain_df['Education'] = train_df['Education'].map({'Graduate': 1, 'Not Graduate': 0})\ntrain_df['Self_Employed'] = train_df['Self_Employed'].map({'No': 0, 'Yes': 1})\ntrain_df['Property_Area'] = train_df['Property_Area'].map({'Rural': 0, 'Semiurban': 1, 'Urban': 2})\ntrain_df['Loan_Status'] = train_df['Loan_Status'].map({'N': 0, 'Y': 1})","4b20c5ab":"# Not scaled single column\ntrain_df.Dependents.head()","262e8aa5":"# Scaled single column\ntrain_df['Dependents'] = (train_df['Dependents']-train_df['Dependents'].min())\/(train_df['Dependents'].max()-train_df['Dependents'].min())\ntrain_df.Dependents.head()","94840b4f":"# applying for loop to bring all the variables in range 0 to 1\nfor i in  train_df.columns[1:]:\n    train_df[i] = (train_df[i] - train_df[i].min()) \/ (train_df[i].max() - train_df[i].min())","b7a7b892":"train_df.info()","688957a6":"# removing the loan_ID since these are just the unique values\ntrain_df = train_df.drop('Loan_ID', axis=1)","11594d0c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n%matplotlib inline","e14d0dba":"X = train_df.drop('Loan_Status',axis=1)\n\ny = train_df.Loan_Status\n\nX.shape, y.shape","45eddeb9":"X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=1)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","25451ad3":"import tensorflow\nimport keras\n\nprint(tensorflow.__version__)\nprint(keras.__version__)","ee0a0653":"# importing sequential model for now\n\nfrom keras.models import Sequential","27c47f61":"# importing different layers from keras\nfrom keras.layers import InputLayer, Dense \n\n\n# deciding number of input neurons\nprint(f'Shape of df: {X_train.shape}')\n# number of features in data\nprint(f'Number of Features in data: {X_train.shape[1]}')\n\n###################################\n###################################\n\n# defining input neurons\ninput_neurons = X_train.shape[1]\nprint(f'Number of Input Neurons: {input_neurons}')\n\n# define hidden layers and hidden neuron in each layer (hyperparameters)\nnumber_of_hidden_layers = 2\nneuron_hidden_layer_1 = 10\nneuron_hidden_layer_2 = 5\n\n# define number of output neurons\noutput_neurons = 1","6b88e408":"# defining the architecture of the model  again at one place\n\nmodel = Sequential()\nmodel.add(InputLayer(input_shape = (input_neurons,))) # adding layes\n\nmodel.add(Dense(units = neuron_hidden_layer_1,activation='relu')) # set number of neurons using units\nmodel.add(Dense(units = neuron_hidden_layer_2,activation='relu'))\n\nmodel.add(Dense(units = output_neurons,activation='sigmoid'))","68fc50a9":"# model summary\nmodel.summary()","8560f2a5":"# Number of parameters between input and first hidden layer, adding the bias for each neuron of first hidden layer\n\ninput_neurons * neuron_hidden_layer_1 + 10  # 11 * 10 + 10","7ff8c5ee":"# Number of parameters between first and second hidden layer\n\nneuron_hidden_layer_1 * neuron_hidden_layer_2 + 5   ","8457899a":"# Number of parameters between second hidden and output layer\n\nneuron_hidden_layer_2 * output_neurons + 1","e1b36177":"# loss as 'binary_crossentropy', as binart Classification Problem\n\nmodel.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])","1342f9fa":"model_history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=45)\n","85269086":"# Getting predictions for validation set\nprediction = (model.predict(X_test) > 0.5).astype(\"int32\")","78c37eba":"# calculating accuracy on validation set\naccuracy_score(y_test,prediction)","187ed742":"# summarize history for loss\n\nplt.figure(figsize=(10,4))\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])   # validation loss values\n\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper right')\n\nplt.show()","483534c7":"# summarize history for accuracy\n\nplt.figure(figsize=(10,4))\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\n\nplt.title('Model Accuracy',)\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='lower right')\nplt.show()","ceed33bf":"`Observation:`\ntraining and validation loss is decreasing as the epoch are increasing","9d610c67":"*  `input layer * first hidden layer + bias(output shape)`","ede01ce4":"# Approaching Problem steps:\n1. Will Pre-process the data: impute the missing values, normalize the variables... \n2. Will Define the architecture of your model \n3. Since it is a regression problem, will make sure to use the 'linear' activation function in the output layer \n4. Train model \n5. Will perform the same pre-processing steps on test set as, did for the training set \n6. Generate predictions for the test set using the trained model \n7. Save the predictions in a csv file (to check the format, refer to the sample submission file provided on the problem page) \n8. Submit predictions on the problem page and check your rank on the leaderboard if any\n","b19a6518":"![image.png](attachment:9e630c23-ecf2-47b9-9dbc-7dda9699c40d.png)","c8814b00":"**`Flow`**\n* First `taking input(X)`, multiplying it with `Weight Matrix(W)`, add `Bias (b)` to that\n* then applying `Activation Function(sigmoid above)` to calculate hiddent layer activation\n* next layer (3rd) have its own W and b\n* similarly generate final output\n\nAs all these steps are being done in `Sequential` manner so `I am creating a Sequential Model here` to create a NN <br>\n\nWe also have `Functional Model` in Keras I will work on that in next notebook","109fe20c":"`If this notebook helped you in anyway then`\n![image.png](attachment:f6ec94ad-f852-44f2-a649-a7f2c16fca72.png) `upvote it.`","cb37b1c0":"# 1.3.Scaling","81fc5407":"### Number of output neurons\n* As loan prediction is a Binary Classification Problem\n* We will have `Single Neuron in Output Layer`\n\n### Number of hidden layers and hidden neurons\n\n* It is a hyperparameter\n* We can pick `Hidden Layers` and `Hidden Neurons` on our own\n\n### Activation Function of different layers\n* For now `Relu` is picked as an activation function for hidden layers\n* Using `Sigmoid Activation function` in final layer, as this is binary classification problem","74eb618a":"# 1.Pre-Processing\n* `Missing Value Treatment`:\n    * if Categorical Variables\n        * fill By Mode\n    * if Numerical Variables \n        * fill By Mean\/Median\n        \n        \n* `Convert Categories to Number`:\n    * OneHot Encoding\n    * Label Encoding\n\n\n* `Scaling(bring all var to same scale [generally b\/w 0 to 1])`:\n    * Min-Max Scaler\n        * Helps in reducing time complexity\n        * NN converges fast\n","fb3edb5d":"# 6.Evaluating ModelPerformance on Validation Set\n","6b34b45d":"# 5.Training Model\n* Passing independent and dependent features for training set for training model\n* Validation data will be evaluated at end of each `epoch`\n* Setting `epochs=45`\n* storing trained model in model_history variable which will be used to visualize training process","7c8d5025":"# 1.1.Missing Value Treatment","927d0d72":"![image.png](attachment:2a78000d-d361-426a-a103-559183f0a1e4.png)","837eac06":"# 1.2.Converting Categories to Numbers","ac0b3de7":"# 1.Create Model\n`Computation graph for the Forward Propogation of a Neural Network looks like below graph:`","517d8837":"*  `hidden_layer_1 * hidden_layer_2 + bias(output shape)`","ff90df67":"# Creating Training and Validation set\n* Stratify will make sure that the distribution of classes in train and validation set is similar\n* Test size 0.2 will keep 20% data in validation and remaining 80% in train set","995430d7":"# Trying to show application of Neural Network on this dataset\n* `Applying a Sequential NN using Keras`\n* `Evaluate model performance`","48bc57bd":"# Defining Architecture of the Model","4e2d4170":"* `Keras` is a library based on `Tensorflow`","1bb0ae9b":"# 2.Defining different Layers (Architecture)\nWe have 3 type of different layer in NN\n* `Input Layer` => equall to number of Features\n* Dense Layer => these layer are for defining Hidden and Output Layer\n    * `Hiddent Layer`\n    * `Output Layer` => depends on number of output here Y or N(0or1) so 1output since this is (Binary Classification Problem)","3ba412e3":"# Problem \nLoan Prediction Problem\n\nClassification Problem, here I try to predict whether loan should be approved or not","d1f5d2ae":"# 4. Compiling Model (defining loss function, optimizer)\nThis is a Binary Classification problem\n\n* Choos loss as `binary_crossentropy`\n* Optemizer as `Adam`\n* Evaluation Metric as `accuracy`","00d48807":"# Visualizing Model Performance\nAs I have saved the model performance as epoch increase in model_history so I will plot it from there","280d4c91":"# 3.Model Architecture","8e801187":"`Observation:`\n* Input layer is not shown in above summary\n* 3-Layer are shown (dense,dense_1,dense_2)\n    * last one is output\n    * rest are dense layer\n* `Param #` number of parameters which can be trained => these are the `Weights` learned suring training process\n\n**`In first we have 120 param what!! how come ??`** apart from weight we have biases associated with each neurons ","7a9ad275":"# Steps to build a Neural Network using Keras\n\n1. Loading the dataset. \n2. Creating training and validation set\n3. Defining architecture of the model\n    * How many `input neurons` we want\n    * How many `hidden layers` and `hidden neurons` should be there\n    * What would be the `number of neurons` in the Output layer\n* All above points will be defined while make architecture of the model\n4. Compiling the model (defining loss function, optimizer)\n    * `Define Loss Function`\n    * `Define Optimizers`\n\nKeras Framework have pre-written code for different Loss Functions like:<br>\n* `MSE`\n* `Binary Cross Entropy`\u2026. <br>\n\nIt also supports diff optimizers like: <br>\n* `SGD`\n* `RMSprop`\n* `Adam`\n\n5. Training the model\n    * While training model `define Epochs`\n6. Evaluating model performance on training and validation set","f6553507":"`Results:`\n* We have done pre-processing\n* `Now every data point is in range of 0 to 1 and  no categories along with null values`"}}