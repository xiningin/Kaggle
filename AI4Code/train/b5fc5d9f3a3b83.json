{"cell_type":{"94515bf7":"code","b1f20005":"code","7d5e498c":"code","6dd5627e":"code","bbeea410":"code","0020deee":"code","a5ccf7df":"code","963655c4":"code","254fb69f":"code","7a8e172d":"code","eae36588":"code","674fe7ef":"code","428e97e5":"code","4fdec465":"code","6415fbbf":"code","55daecdb":"code","2e603eff":"code","52169128":"markdown","f7df9a45":"markdown"},"source":{"94515bf7":"!pip install torchviz -q","b1f20005":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\nimport time\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch.nn.utils import weight_norm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torchviz import make_dot, make_dot_from_trace\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.manual_seed(0)","7d5e498c":"RANDOM_STATE = 42\nTARGET = 'target'","6dd5627e":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\n\n\nX = train.drop(['target', 'id'], axis = 1).to_numpy()\n\nlencoder = LabelEncoder()\ny = lencoder.fit_transform(train['target']).astype('int64')","bbeea410":"def seed_everything(seed=43):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(RANDOM_STATE)","0020deee":"BATCH_SIZE = 256\nNUM_FEATURES = X.shape[1]\nNUM_CLASSES = 9\nNUM_EPOCHS = 50\nFEATURE_DICT_SIZE = 360\n\nN_SPLITS = 5\nN_RANDOM_STATES = 3\nN_MODELS = 3\n\nLEARNING_RATE = 0.03\nEPOCH_VERBOSE = False\nMAX_PATIENCE = 5","a5ccf7df":"def residual_block(in_features, out_features, batch_norm, p_drop, non_linear = nn.ReLU(), *args, **kwargs):       \n        net = nn.Sequential(\n            nn.Dropout(p = p_drop),\n            weight_norm(nn.Linear(in_features, out_features)),\n            non_linear) \n        \n        if batch_norm:\n            net = nn.Sequential(nn.BatchNorm1d(in_features),\n                                nn.Dropout(p = p_drop),\n                                nn.Linear(in_features, out_features),\n                                non_linear)\n        return net\n\n\n\nclass TPSResidual(nn.Module):\n    def __init__(self, \n                 num_features = 75, \n                 num_class = 9, \n                 feature_dictionary_size = 360, \n                 batch_norm = False,\n                 dropout = 0.3, \n                 linear_nodes = 32, \n                 linear_out = 16, \n                 emb_output = 4, \n                 num_block = 3):\n        super(TPSResidual, self).__init__()\n        self.num_block = num_block\n        self.final_module_list = nn.ModuleList()\n    \n        \n        self.embedding = nn.Embedding(feature_dictionary_size, emb_output)\n        self.flatten = nn.Flatten()\n\n        self.linear = weight_norm(nn.Linear(emb_output * num_features, linear_nodes))\n        torch.nn.init.xavier_uniform(self.linear.weight)\n        \n        for res_num in range(self.num_block):\n            self.non_linear = nn.ELU() if res_num % 2 else nn.ReLU()\n            self.lin_out = linear_out if res_num == (num_block-1) else linear_nodes\n            self.final_module_list.append(residual_block(emb_output * num_features + (res_num + 1) * linear_nodes, self.lin_out, batch_norm, dropout, self.non_linear))\n        \n        #self.bn = nn.BatchNorm1d(linear_out)\n        self.out = nn.Linear(linear_out, num_class)\n        \n        # nonlinearity - activation function\n        self.selu = nn.SELU()\n        \n        self.dropout = nn.Dropout(p = dropout)\n\n    def forward(self, x):\n        x = torch.tensor(x).to(torch.int64)\n        \n        # Embedding \n        e = self.embedding(x)\n        e = self.flatten(e)\n        \n        h1 = self.dropout(e)\n        h1 = self.linear(h1)\n        h1 = self.selu(h1)\n        \n        ri = torch.cat((e, h1), 1)\n        \n        for res_num in range(self.num_block):          \n            rx = self.final_module_list[res_num](ri)\n            ri = torch.cat((ri, rx), 1)\n        # rx = self.bn(rx)\n        return  self.out(rx)","963655c4":"\n# Below you can find the best params for this approach found in over 270 runs in this notebook: https:\/\/www.kaggle.com\/remekkinas\/pytorch-skorch-residual-hyperparameter\n\nnet_params = [{'net__module__dropout': 0.3, 'net__module__emb_output': 2, 'net__module__linear_nodes': 16, \n               'net__module__linear_out': 16, 'net__module__num_block': 2, 'net__optimizer': optim.Adam},\n             {'net__module__dropout': 0.3, 'net__module__emb_output': 2, 'net__module__linear_nodes': 32,\n              'net__module__linear_out': 16, 'net__module__num_block': 2, 'net__optimizer': optim.Adam},\n             {'net__module__dropout': 0.2, 'net__module__emb_output': 2, 'net__module__linear_nodes': 16, \n              'net__module__linear_out': 16, 'net__module__num_block': 3, 'net__optimizer': optim.Adam},\n             {'net__module__dropout': 0.2, 'net__module__emb_output': 8, 'net__module__linear_nodes': 64, \n              'net__module__linear_out': 32, 'net__module__num_block': 3, 'net__optimizer': optim.Adam},\n             {'net__module__dropout': 0.2, 'net__module__emb_output': 6, 'net__module__linear_nodes': 32, \n              'net__module__linear_out': 16, 'net__module__num_block': 3, 'net__optimizer': optim.Adam}]\n\nnet_params_exp = [{'net__module__dropout': 0.2, 'net__module__emb_output': 2, 'net__module__linear_nodes': 16, \n               'net__module__linear_out': 16, 'net__module__num_block': 2, 'net__optimizer': optim.Adam}]\n\ndef get_estimator(net_params):\n    return TPSResidual(NUM_FEATURES,\n                       NUM_CLASSES,\n                       FEATURE_DICT_SIZE,\n                       batch_norm = False,\n                       dropout = net_params['net__module__dropout'], \n                       emb_output = net_params['net__module__emb_output'], \n                       linear_nodes = net_params['net__module__linear_nodes'], \n                       linear_out = net_params['net__module__linear_out'], \n                       num_block = net_params['net__module__num_block'])","254fb69f":"model_graph = get_estimator(net_params_exp[0])\nmodel_graph.to(device)\nmodel_graph.eval()\n\nprint(model_graph)","7a8e172d":"x_graph = torch.ones(1, NUM_FEATURES).to(device)\ny_graph = model_graph(x_graph)\n\nmake_dot(y_graph.mean(), params=dict(model_graph.named_parameters()))","eae36588":"def acc_calc(y_pred, y_test):\n    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n    \n    correct_pred = (y_pred_tags == y_test).float()\n    acc = correct_pred.sum() \/ len(correct_pred)\n    \n    acc = torch.round(acc * 100)\n    \n    return acc","674fe7ef":"criterion = nn.CrossEntropyLoss()\n\ndef model_train(data_loader, model, optimizer):\n    \n    model.train()\n    \n    train_epoch_loss = 0\n    train_epoch_acc = 0\n    \n    for X_train_batch, y_train_batch in data_loader:\n            \n            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n            \n            optimizer.zero_grad()\n\n            y_train_pred = model(X_train_batch)\n\n            train_loss = criterion(y_train_pred, y_train_batch)\n\n            train_acc = acc_calc(y_train_pred, y_train_batch)\n   \n            train_loss.backward()\n\n            optimizer.step()\n            \n            train_epoch_loss += train_loss.item()\n            train_epoch_acc += train_acc.item()\n            \n    return train_epoch_loss, train_epoch_acc\n","428e97e5":"def model_validate(data_loader, model):\n    \n    val_epoch_loss = 0\n    val_epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n\n            val_epoch_loss = 0\n            val_epoch_acc = 0\n\n            \n            for X_val_batch, y_val_batch in data_loader:\n                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n\n                y_val_pred = model(X_val_batch)\n\n                val_loss = criterion(y_val_pred, y_val_batch)\n                val_acc = acc_calc(y_val_pred, y_val_batch)\n\n                val_epoch_loss += val_loss.item()\n                val_epoch_acc += val_acc.item()\n    \n    return val_epoch_loss, val_epoch_acc","4fdec465":" \n    for r_state in tqdm(range(N_RANDOM_STATES)):\n        \n        for model_idx in range(N_MODELS):\n            \n            fold_random_state = (RANDOM_STATE + r_state)\n            seed_everything(fold_random_state)\n\n            kfold = StratifiedKFold(n_splits = N_SPLITS, shuffle = True, random_state = fold_random_state)\n\n            for fold, (train_idx, valid_idx) in enumerate(kfold.split(X, y)):\n                e_start_time = time.time()\n\n                best_loss = 9999\n                patience = 0\n\n                OOF_PRED = np.zeros((X.shape[0], 9))\n\n                accuracy_stat = {'train': [],\"validation\": []}\n                loss_stat = {'train': [], \"validation\": [] }\n\n                X_train, X_valid = X[train_idx], X[valid_idx]\n                y_train, y_valid = y[train_idx], y[valid_idx]\n\n                train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n                valid_dataset = TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid).long())\n\n                train_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE)\n                valid_loader = DataLoader(dataset = valid_dataset, batch_size = BATCH_SIZE)\n\n                model = get_estimator(net_params[model_idx])\n                model.to(device)\n\n                optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE) \n                scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 3)\n\n                for progress in range(1, NUM_EPOCHS+1):\n\n                    train_epoch_loss, train_epoch_acc = model_train(train_loader, model, optimizer)\n\n                    val_epoch_loss, val_epoch_acc = model_validate(valid_loader, model)\n\n                    # Metrics\n                    e_train_loss = train_epoch_loss \/ len(train_loader)\n                    e_train_acc = train_epoch_acc \/ len(train_loader)\n                    e_valid_loss = val_epoch_loss \/ len(valid_loader)\n                    e_valid_acc = val_epoch_acc \/ len(valid_loader)\n\n                    loss_stat['train'].append(e_train_loss)\n                    loss_stat['validation'].append(e_valid_loss)\n                    accuracy_stat['train'].append(e_train_acc)\n                    accuracy_stat['validation'].append(e_valid_acc) \n\n                    if (e_valid_loss) < best_loss:\n                        torch.save(model.state_dict(),f'model-{RANDOM_STATE + r_state}-m{model_idx}-{fold}.bin')\n                        best_loss = e_valid_loss\n                        patience = 0\n\n                    clr = optimizer.param_groups[0]['lr']        \n                    scheduler.step(e_valid_loss)\n\n\n                    if EPOCH_VERBOSE:\n                        print(f'Epoch { progress + 0:03}: Loss: [Train: {e_train_loss:.7f} | Validation: {e_valid_loss:.7f} ] Accuracy: [Train: {e_train_acc:.3f} | Validation: {e_valid_acc:.3f}] LR: {clr}')\n\n                    if patience == MAX_PATIENCE:\n                        if EPOCH_VERBOSE:\n                            print(f'No vlaidation loss improvement - best: {best_loss}')\n                        break\n                    else:\n                        patience +=1\n\n                print(f'RANDOM: {RANDOM_STATE + r_state} - MODEL_IDX: {model_idx+1} - FOLD: {fold} - BEST MODEL: {best_loss:.5f} - TIME: {(time.time()-e_start_time):.2f}s)')\n            print(\"\\n\")","6415fbbf":"test_df = TensorDataset(torch.Tensor(np.array(test.drop('id', axis = 1))))\ntest_loader = DataLoader(test_df, batch_size = 100000, shuffle = False)\ntest_preds = np.zeros((test.shape[0], 9))\n\nfor r_state in tqdm(range(N_RANDOM_STATES)):\n    for model_idx in range(N_MODELS):\n        \n        for fold in range(N_SPLITS):\n            in_size = test.shape[1]\n            model = get_estimator(net_params[model_idx])\n            model.load_state_dict(torch.load(f'.\/model-{RANDOM_STATE + r_state}-m{model_idx}-{fold}.bin'))\n            model.to(device)\n            preds = list()\n\n            for _, data in enumerate(test_loader, 0):\n                features = data[0]\n                features = features.to(device, dtype=torch.float)\n\n                with torch.set_grad_enabled(False):\n                    y_pred = model(features)\n\n                    sm = nn.Softmax(dim=1)\n                    pred_percentage = sm(y_pred)\n                    preds.extend(pred_percentage.detach().cpu().numpy())\n\n            test_preds += np.array(preds)\n        \ntest_preds = test_preds \/ (N_SPLITS * N_RANDOM_STATES * N_MODELS)\n","55daecdb":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(test_preds, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\" ])\npredictions_df['id'] = sub['id']","2e603eff":"predictions_df.to_csv(\"TPS06-Pytorch_residual_submission.csv\", index = False)","52169128":"### IDEAS BEHIND NOTEBOOK\n1. Implementation of skipped connection in Pytroch - this is still under development (looking for better architecture)\n2. Traing 100 NN networks based on (the best model from each iteration is saved and then used for prediction)\n    - different Random state \n    - different Architecture (I took TOP5 network architectures from Skorch GridSearchCV hyperparameter optimization - https:\/\/www.kaggle.com\/remekkinas\/pytorch-skorch-residual-hyperparameter)\n    - folds variability","f7df9a45":"## TPS-06 Pytorch RESIDUAL connection for tabular data\n\nThis notebook implements concept of residual connection in pure Pytorch. \n\nNotebook is under development (required some investigation regarding network architecture - I am still trying to answer question - why the same (?) architecture developed in Keras (R\/Python) perform better (in terms of LN). If you can point me out where are te differences I will be more then happy.\n\n<div class=\"alert alert-info\">\n  <strong>My other Pytorch notebooks:<\/strong>\n    <ul>\n        <li><a href =\"https:\/\/www.kaggle.com\/remekkinas\/pytorch-skorch-residual-hyperparameter\">Pytorch (skorch) - RESIDUAL + hyperparameter<\/a><\/li>\n        <li><a href = \"https:\/\/www.kaggle.com\/remekkinas\/skorch-tutorial-simple-pytorch-nn-with-scikit\">SKORCH tutorial - simple PyTorch NN with scikit<\/a><\/li>\n        <li><a href =\"https:\/\/www.kaggle.com\/remekkinas\/tps-5-pytorch-nn-for-tabular-step-by-step\">[TPS-5] Pytorch NN for tabular - step by step<\/a><\/li>\n    <\/ul>\n<\/div>"}}