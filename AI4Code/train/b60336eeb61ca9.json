{"cell_type":{"f7251e5a":"code","16d0b8fc":"code","2f9d8550":"code","e993d35b":"code","5cff1512":"code","032e8441":"code","5ea88ed8":"code","68ac7eb0":"code","6b645a19":"code","72e55fe3":"code","29cb6711":"code","3a2e4281":"code","51766a77":"code","3698ad91":"code","98da9a9c":"code","2cc16ef0":"code","9b0e4bfd":"code","96dca453":"code","5fdf388b":"code","98348ba2":"code","5b004a42":"code","bb501483":"code","0943da63":"code","994d01b1":"code","cd449c26":"code","a28e78a6":"code","a7d23291":"code","931bacf8":"code","f99e8cc6":"code","0da3bec5":"code","5b189726":"code","847e7a43":"code","7fe431ee":"code","043d00b9":"code","dc690692":"code","e564f629":"code","ede6b931":"code","b15c4e3e":"code","43b489ae":"code","9d83ed86":"code","4e743604":"code","a5ba02ea":"code","528df3c8":"code","b7ae9579":"code","83f051c0":"code","425a4b32":"code","14481607":"code","2af8379c":"code","62dc136b":"code","3a704730":"code","d9224589":"code","1e8d33c3":"code","d289aac6":"code","5cbd9234":"code","a8c5efcb":"code","35cc25ac":"code","2e2fa63c":"code","d14837d1":"code","0dfb5b6f":"code","a6e43606":"code","fa109946":"code","2d8459de":"code","64e6c022":"code","1a4649d8":"code","90dd8b5b":"code","94e40648":"code","516250b7":"code","79526031":"code","9d1e236b":"code","5bc025b3":"code","14bc91f0":"code","9a5dcb10":"code","b1f320c6":"code","2b4be989":"code","a9b774a6":"code","20ec2e16":"code","8e68cffd":"code","483fbce9":"code","0f129b72":"code","7629db03":"code","ace6957a":"code","17c05080":"code","2ba33715":"code","fa13b71d":"code","69267975":"code","4d055eaa":"code","be393760":"code","936b7dc5":"code","bfec9910":"code","3057e46d":"code","2f0a4baa":"code","ea16d493":"code","3ad85d6b":"code","ad8db3ec":"code","573f610b":"markdown","9a7d8052":"markdown","2f115dcf":"markdown","6411d16f":"markdown","ddcda308":"markdown","af6b3963":"markdown","a15ffb2b":"markdown","90f744a5":"markdown","555d6a3d":"markdown","6a387786":"markdown","89fbf67c":"markdown","311bd4f1":"markdown","11936862":"markdown","9e919f32":"markdown","44adc326":"markdown","7b2bdeba":"markdown","4a7015f9":"markdown","4d6c4034":"markdown","c81085a5":"markdown","a08e3bc3":"markdown","1db921d6":"markdown","6b31e773":"markdown","51241e43":"markdown","bb2ac30d":"markdown","852c0112":"markdown","06c0256a":"markdown","5f95de65":"markdown","b4e5db47":"markdown","9072b962":"markdown","fb9e81ea":"markdown","27bf92b2":"markdown","afe1de9c":"markdown","b85a4dcf":"markdown","b55d26fc":"markdown","8a844584":"markdown","6befd90a":"markdown","694053db":"markdown","523747ba":"markdown","900d1280":"markdown","5ea23733":"markdown","b1405be5":"markdown","943edbb7":"markdown","43b32d54":"markdown"},"source":{"f7251e5a":"# install package to optimize the models \n!pip install scikit-optimize shap","16d0b8fc":"# Turn off warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# libs for data handling\nimport pandas as pd \nimport numpy as np \nimport time\n\n# libs for data visualization\nimport matplotlib.pyplot as plt  \nimport seaborn as sns \n\n%matplotlib inline","2f9d8550":"# load dataset\nURL = 'https:\/\/drive.google.com\/file\/d\/11-vwvXc8ioDuentKHGZ8dM02pYyRUF97\/view?usp=sharing'\npath = 'https:\/\/drive.google.com\/uc?export=download&id='+URL.split('\/')[-2]\ndataset = pd.read_csv(path)\n#\nprint('dataset shape:', dataset.shape)\ndataset.head()","e993d35b":"# check for duplicate rows\nsum(dataset.duplicated())","5cff1512":"# gets number unique values\nfor var in dataset.columns:\n  print('{} has {} unique values, its type is {} and it has {} NaNs'.format(var, dataset[var].nunique(), dataset[var].dtype, dataset[var].isna().sum()))","032e8441":"# descriptive statistics about the dataset\ndataset.describe()","5ea88ed8":"# Drop unnecessary variables\ndrop = ['RowNumber', 'CustomerId', 'Surname']\ndataset_orig = dataset.copy(deep=True)\ndataset.drop(drop, axis=1, inplace=True)\nprint(dataset.shape)","68ac7eb0":"# Target balance check\ndataset['Exited'].value_counts().plot(kind = 'pie', title='Exited')\nprint(dataset['Exited'].value_counts(normalize=True))","6b645a19":"# Define lists of categorical and numerical features \ncat_features = [var for var in dataset.columns if dataset[var].dtype == 'object']\nprint('there are {} categorical variables'.format(len(cat_features)))\nprint(cat_features)\n#\nnum_features = [var for var in dataset.columns if dataset[var].dtype in ('int64', 'float64')]\nprint('there are {} numerical variables'.format(len(num_features)))\nprint(num_features)","72e55fe3":"# distribution plots for categorical features\nfig, axs = plt.subplots(1, 2, figsize=(12,4))\nfor i, var in enumerate(cat_features):\n  sns.countplot(x=var, data=dataset, ax=axs[i])\n  print(dataset[var].value_counts(normalize=True))","29cb6711":"# distribution plots for categorical features grouped by the target\nfig, axs = plt.subplots(1, 2, figsize=(12,4))\nfor i, var in enumerate(cat_features):\n  sns.countplot(x=var, data=dataset, ax=axs[i], hue='Exited')","3a2e4281":"# distribution plots for numerical features\nfig, axs = plt.subplots(2, 4, figsize=(16,5))\nfor i in range(int(len(num_features)\/2)):\n  sns.boxplot(x='Exited', y=num_features[i], data=dataset, ax=axs[0, i])\n  #axs[0, i].set_title(num_features[i])\n  sns.boxplot(x='Exited', y=num_features[i+4], data=dataset, ax=axs[1, i])\n  #axs[1, i].set_title(num_features[i+4])\nfig.tight_layout()\nplt.show()","51766a77":"dataset['GenderInt'] = pd.factorize(dataset['Gender'])[0]\nnum_features.append('GenderInt')\ndataset['GeographyInt'] = pd.factorize(dataset['Geography'])[0]\nnum_features.append('GeographyInt')","3698ad91":"# correlation heatmap function\ndef corr_heatmap(df, method):\n  df_corr = df.iloc[:,3:].corr(method=method)\n  mask = np.triu(np.ones_like(df_corr, dtype=np.bool))\n  fig, ax = plt.subplots(figsize = (10, 10))\n  sns.heatmap(df_corr, annot=True, vmin=-1, vmax=1, cmap='viridis', linewidths=.5, mask=mask, ax=ax)\n  plt.show()","98da9a9c":"# Pearson Correlation analysis\ncorr_heatmap(dataset, 'pearson')","2cc16ef0":"# Pearson Correlation analysis\ncorr_heatmap(dataset, 'spearman')","9b0e4bfd":"# Separtes X for explanatory feaures and y for target\nX = dataset[num_features+['Exited']]\ny = X.pop('Exited')\nprint(X.shape)\nprint(y.shape)","96dca453":"# splits data into train and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=16, stratify=y)\nprint(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)","5fdf388b":"# functions to detect (according to IQR criteria) and replace outliers\n\ndef outlier_detection(feature, df, low, up):\n  q1 = df[feature].quantile(low)\n  q3 = df[feature].quantile(up)\n  iqr = q3-q1\n  uf = q3 + 1.5*iqr\n  lf = q1 - 1.5*iqr\n  return(uf, lf)\n\ndef outlier_replacement(feature, uf, lf):\n  if feature > uf:\n    clean = uf\n  elif feature < lf:\n    clean = lf\n  else:\n    clean = feature\n  return clean","98348ba2":"# 'CreditScore' outlier treatment\n\nuf, lf = outlier_detection('CreditScore', X_train, 0.25, 0.75)\nprint(uf, lf)\nclean = np.vectorize(outlier_replacement)(X_train['CreditScore'], uf, lf)\nprint(len(clean),'=' , X_train.shape[0])\nX_train['CreditScore_clean'] = clean\nclean = np.vectorize(outlier_replacement)(X_valid['CreditScore'], uf, lf)\nprint(len(clean),'=' , X_valid.shape[0])\nX_valid['CreditScore_clean'] = clean\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8))\nsns.violinplot(x='CreditScore', data=X_train, ax=axs[0, 0])\naxs[0, 0].set_title('X_train CreditScore original distribution')\nsns.violinplot(x='CreditScore_clean', data=X_train, ax=axs[0, 1])\naxs[0, 1].set_title('X_train CreditScore clean distribution')\n#\nsns.violinplot(x='CreditScore', data=X_valid, ax=axs[1, 0])\naxs[1, 0].set_title('X_valid CreditScore original distribution')\nsns.violinplot(x='CreditScore_clean', data=X_valid, ax=axs[1, 1])\naxs[1, 1].set_title('X_valid CreditScore clean distribution')\n#\nfig.tight_layout()\nplt.show()","5b004a42":"# 'Age' outlier treatment\n\nuf, lf = outlier_detection('Age', X_train, 0.25, 0.75)\nprint(uf, lf)\nclean = np.vectorize(outlier_replacement)(X_train['Age'], uf, lf)\nprint(len(clean),'=' , X_train.shape[0])\nX_train['Age_clean'] = clean\nclean = np.vectorize(outlier_replacement)(X_valid['Age'], uf, lf)\nprint(len(clean),'=' , X_valid.shape[0])\nX_valid['Age_clean'] = clean\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8))\nsns.violinplot(x='Age', data=X_train, ax=axs[0, 0])\naxs[0, 0].set_title('X_train Age original distribution')\nsns.violinplot(x='Age_clean', data=X_train, ax=axs[0, 1])\naxs[0, 1].set_title('X_train Age clean distribution')\n#\nsns.violinplot(x='Age', data=X_valid, ax=axs[1, 0])\naxs[1, 0].set_title('X_valid Age original distribution')\nsns.violinplot(x='Age_clean', data=X_valid, ax=axs[1, 1])\naxs[1, 1].set_title('X_valid Age clean distribution')\n#\nfig.tight_layout()\nplt.show()","bb501483":"# replace variables by their clean versions (i.e. without outliers) \nnum_features.remove('CreditScore')\nnum_features.append('CreditScore_clean')\n#\nnum_features.remove('Age')\nnum_features.append('Age_clean')","0943da63":"X_train = X_train[num_features]\nprint(X_train.shape)\nX_valid = X_valid[num_features]\nprint(X_valid.shape)","994d01b1":"# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nvars_scale = ['Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'CreditScore_clean', 'Age_clean']\n\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[vars_scale]), columns=X_train[vars_scale].columns)\nX_train_scaled.set_index(X_train.index, inplace=True)\nprint(X_train_scaled.shape)\nX_valid_scaled = pd.DataFrame(scaler.transform(X_valid[vars_scale]), columns=X_valid[vars_scale].columns)\nX_valid_scaled.set_index(X_valid.index, inplace=True)\nprint(X_valid_scaled.shape)","cd449c26":"X_train_final = pd.concat([X_train[['HasCrCard', 'IsActiveMember', 'GenderInt', 'GeographyInt']], X_train_scaled], axis=1)\nprint(X_train_final.shape)\nX_train_final.head(2)","a28e78a6":"X_valid_final = pd.concat([X_valid[['HasCrCard', 'IsActiveMember', 'GenderInt', 'GeographyInt']], X_valid_scaled], axis=1)\nprint(X_valid_final.shape)\nX_valid_final.head(2)","a7d23291":"# package to optimize the models \n\nfrom skopt import BayesSearchCV","931bacf8":"from sklearn.linear_model import LogisticRegression\n\nopt = BayesSearchCV(\n    LogisticRegression(),\n    {\n        'penalty': ['l2'],\n        'C': (1e-1, 1e1, 'log-uniform'),\n        'tol': (1e-6, 1e-3, 'log-uniform'),\n        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_final, y_train)\n\nprint(\"test score: %s\" % opt.best_score_)\nprint(\"valid score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","f99e8cc6":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","0da3bec5":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","5b189726":"reglog = opt.best_estimator_","847e7a43":"from sklearn.svm import SVC\n\nopt = BayesSearchCV(\n    SVC(),\n    {\n        'C': (1e-1, 1e1, 'log-uniform'),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        'degree': (1, 3),\n        'tol': (1e-5, 1e-3, 'log-uniform'),\n        #'gamma': ['scale', 'auto']\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_final, y_train)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","7fe431ee":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","043d00b9":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","dc690692":"svm = opt.best_estimator_","e564f629":"from sklearn.neural_network import MLPClassifier\n\nopt = BayesSearchCV(\n    MLPClassifier(),\n    {\n        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n        'solver': ['lbfgs', 'sgd', 'adam'],\n        'alpha': (1e-4, 1e-2, 'log-uniform'),\n        #'gamma': ['scale', 'auto'],\n        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n        #'tol': (1e-6, 1e-3, 'log-uniform'),\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_final, y_train)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","ede6b931":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","b15c4e3e":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","43b489ae":"mlp = opt.best_estimator_","9d83ed86":"from sklearn.ensemble import RandomForestClassifier\n\nopt = BayesSearchCV(\n    RandomForestClassifier(),\n    {\n        'n_estimators': [200, 400, 800, 1000, 1500, 2000],\n        #'criterion': ['gini', 'entropy'],\n        'min_samples_split': (2, 7),\n        #'min_samples_leaf': (1, 7),\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'bootstrap': [True, False]\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_final, y_train)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))","4e743604":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","a5ba02ea":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","528df3c8":"rf = opt.best_estimator_","b7ae9579":"from xgboost import XGBClassifier\n\nopt = BayesSearchCV(\n    XGBClassifier(),\n    {\n        'learning_rate': (1e-5, 1e-1, 'log-uniform'),\n        'min_split_loss': [0.05, 0.1, 0.3, 0.5, 0.75, 1],\n        'max_depth': (3, 15),\n        #'min_child_weight': (3, 7),\n        'subsample': (1e-2, 0.9999, 'log-uniform'),\n        #'colsample_bytree': (1e-2, 1, 'log-uniform'),\n        #'reg_lambda': (1e-2, 1, 'log-uniform'),\n        #reg_alpha': (1e-2, 1, 'log-uniform'),\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_final, y_train)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))","83f051c0":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","425a4b32":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","14481607":"xgb = opt.best_estimator_","2af8379c":"fig, ax = plt.subplots()\nplot_roc_curve(reglog, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(svm, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(mlp, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(rf, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(xgb, X_valid_final, y_valid, ax=ax)","62dc136b":"print(X_train_final.shape)\nprint(y_train.shape)\nprint(X_valid_final.shape)\nprint(y_valid.shape)","3a704730":"print(y_train.value_counts())\nprint(y_train.value_counts(normalize=True))","d9224589":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train_balanced, y_train_balanced = oversample.fit_resample(X_train_final, y_train)\n\nprint(X_train_balanced.shape)\nprint(y_train_balanced.shape)\n\nprint(y_train_balanced.value_counts())\nprint(y_train_balanced.value_counts(normalize=True))","1e8d33c3":"%%time\nopt = BayesSearchCV(\n    LogisticRegression(),\n    {\n        'penalty': ['l2'],\n        'C': (1e-1, 1e1, 'log-uniform'),\n        'tol': (1e-6, 1e-3, 'log-uniform'),\n        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"test score: %s\" % opt.best_score_)\nprint(\"valid score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","d289aac6":"y_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","5cbd9234":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","a8c5efcb":"reglog = opt.best_estimator_","35cc25ac":"%%time\nopt = BayesSearchCV(\n    SVC(),\n    {\n        'C': (1e-1, 1e1, 'log-uniform'),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        'degree': (1, 3),\n        'tol': (1e-5, 1e-3, 'log-uniform'),\n        #'gamma': ['scale', 'auto']\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","2e2fa63c":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","d14837d1":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","0dfb5b6f":"svm = opt.best_estimator_","a6e43606":"%%time\nopt = BayesSearchCV(\n    MLPClassifier(),\n    {\n        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n        'solver': ['lbfgs', 'sgd', 'adam'],\n        'alpha': (1e-4, 1e-2, 'log-uniform'),\n        #'gamma': ['scale', 'auto'],\n        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n        #'tol': (1e-6, 1e-3, 'log-uniform'),\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","fa109946":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","2d8459de":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","64e6c022":"mlp = opt.best_estimator_","1a4649d8":"%%time\nopt = BayesSearchCV(\n    RandomForestClassifier(),\n    {\n        'n_estimators': [200, 400, 800, 1000, 1500, 2000],\n        #'criterion': ['gini', 'entropy'],\n        'min_samples_split': (2, 7),\n        #'min_samples_leaf': (1, 7),\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'bootstrap': [True, False]\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","90dd8b5b":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","94e40648":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","516250b7":"rf = opt.best_estimator_","79526031":"%%time\nopt = BayesSearchCV(\n    XGBClassifier(),\n    {\n        'learning_rate': (1e-5, 1e-1, 'log-uniform'),\n        'min_split_loss': [0.05, 0.1, 0.3, 0.5, 0.75, 1],\n        'max_depth': (3, 15),\n        #'min_child_weight': (3, 7),\n        'subsample': (1e-2, 0.9999, 'log-uniform'),\n        #'colsample_bytree': (1e-2, 1, 'log-uniform'),\n        #'reg_lambda': (1e-2, 1, 'log-uniform'),\n        #reg_alpha': (1e-2, 1, 'log-uniform'),\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","9d1e236b":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","5bc025b3":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","14bc91f0":"xgb = opt.best_estimator_","9a5dcb10":"fig, ax = plt.subplots()\nplot_roc_curve(reglog, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(svm, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(mlp, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(rf, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(xgb, X_valid_final, y_valid, ax=ax)","b1f320c6":"from imblearn.under_sampling import RandomUnderSampler\nundersample = RandomUnderSampler()\nX_train_balanced, y_train_balanced = undersample.fit_resample(X_train_final, y_train)\n\nprint(X_train_balanced.shape)\nprint(y_train_balanced.shape)\n\nprint(y_train_balanced.value_counts())\nprint(y_train_balanced.value_counts(normalize=True))","2b4be989":"%%time\nopt = BayesSearchCV(\n    LogisticRegression(),\n    {\n        'penalty': ['l2'],\n        'C': (1e-1, 1e1, 'log-uniform'),\n        'tol': (1e-6, 1e-3, 'log-uniform'),\n        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"test score: %s\" % opt.best_score_)\nprint(\"valid score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","a9b774a6":"y_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","20ec2e16":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","8e68cffd":"reglog = opt.best_estimator_","483fbce9":"%%time\nopt = BayesSearchCV(\n    SVC(),\n    {\n        'C': (1e-1, 1e1, 'log-uniform'),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        'degree': (1, 3),\n        'tol': (1e-5, 1e-3, 'log-uniform'),\n        #'gamma': ['scale', 'auto']\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","0f129b72":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","7629db03":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","ace6957a":"svm = opt.best_estimator_","17c05080":"%%time\nopt = BayesSearchCV(\n    MLPClassifier(),\n    {\n        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n        'solver': ['lbfgs', 'sgd', 'adam'],\n        'alpha': (1e-4, 1e-2, 'log-uniform'),\n        #'gamma': ['scale', 'auto'],\n        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n        #'tol': (1e-6, 1e-3, 'log-uniform'),\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","2ba33715":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","fa13b71d":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","69267975":"mlp = opt.best_estimator_","4d055eaa":"%%time\nopt = BayesSearchCV(\n    RandomForestClassifier(),\n    {\n        'n_estimators': [200, 400, 800, 1000, 1500, 2000],\n        #'criterion': ['gini', 'entropy'],\n        'min_samples_split': (2, 7),\n        #'min_samples_leaf': (1, 7),\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'bootstrap': [True, False]\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","be393760":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","936b7dc5":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","bfec9910":"rf = opt.best_estimator_","3057e46d":"%%time\nopt = BayesSearchCV(\n    XGBClassifier(),\n    {\n        'learning_rate': (1e-5, 1e-1, 'log-uniform'),\n        'min_split_loss': [0.05, 0.1, 0.3, 0.5, 0.75, 1],\n        'max_depth': (3, 15),\n        #'min_child_weight': (3, 7),\n        'subsample': (1e-2, 0.9999, 'log-uniform'),\n        #'colsample_bytree': (1e-2, 1, 'log-uniform'),\n        #'reg_lambda': (1e-2, 1, 'log-uniform'),\n        #reg_alpha': (1e-2, 1, 'log-uniform'),\n    },\n    n_iter=32,\n    cv=3\n)\n\nopt.fit(X_train_balanced, y_train_balanced)\n\nprint(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_valid_final, y_valid))\nprint(\"best params: %s\" % str(opt.best_params_))","2f0a4baa":"from sklearn.metrics import classification_report\n\ny_pred = opt.best_estimator_.predict(X_valid_final)\n\nprint(classification_report(y_valid, y_pred))","ea16d493":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(opt.best_estimator_, X_valid_final, y_valid)","3ad85d6b":"xgb = opt.best_estimator_","ad8db3ec":"fig, ax = plt.subplots()\nplot_roc_curve(reglog, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(svm, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(mlp, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(rf, X_valid_final, y_valid, ax=ax)\nplot_roc_curve(xgb, X_valid_final, y_valid, ax=ax)","573f610b":"## Dataset preprocessing","9a7d8052":"#**Customer Churn Prediction**","2f115dcf":"### AUC","6411d16f":"Half of the customers are from France. The other half is equally divided between Spain and Germany. Gender shows a slight predominance of males. When we group by 'exited', it's clear that the proportions of clients who have churned are different: churn rate in germany is the highest among countries. As for gender, female's churn rate are higher than for males.","ddcda308":"### Oversampling\nLet's apply SMOTE for oversampling the minority class","af6b3963":"### Numerical Encoding for Categorical Features","a15ffb2b":"### Random Forest","90f744a5":"Technological advance has deeply changed the way customers and companies interact.\n\nCurrently, it is possible to predict the possible churn rate of a customer and, thus, define better business strategies.\n\nChurn is a metric that indicates the percentage of customers who canceled a given service in a given period of time. It has become very popular in the last few years among all sorts of companies, specilly those with recurrent income, and is considered one of the most important OKR in many of them.\n\nThis notebook takes a dataset previously used on a Kaggle competition (available at https:\/\/www.kaggle.com\/santoshd3\/bank-customers) and conducts a full analysis to understand and predict customer churn for a bank that operates in western europe. Five models will be tested: Logistic Regression, Support Vector Machines, Multi-layer Perceptron, Random Forest and eXtreme Gradient Boosting. All models will have its hyperparameters optimized with scikit-optimize package. Also, a full set of metrics will be evaluated in other to ckeck how good these models are to predict churn. ","555d6a3d":"### Data Split","6a387786":"### Correlation Analysis","89fbf67c":"### Undersampling the majority class","311bd4f1":"### SVM","11936862":"### Logistic Regression","9e919f32":"So far we can see that this dataset is consistent and \"ready to go\". We have 10.000 entries and 14 columns, including the target variable. There are no missing values, nor duplicate rows.\nFrom this point we can already tell that 'RowNumber' (as the variable name suggests) is duplicated with the dataset's index. 'CustomerId' is an identificator for customers and, by the number of unique values, also acts as an index. 'Surname' doesn't hold any meaning for our scope of analysis. So let's drop these two variables.\n\nThe .describe() method also provides us some initital insights about the variables, such as:\n\n*   From 'Tenure' we can tell that the average time a customer stays within the client base is around 5 years. It's reasonable to figure this variable should have an important role in the task at hand.\n*   From 'Exited' (target) we can tell that the churn rate in the dataset is around 20%. Which also tells us that our data is unbalanced.\n\n\n","44adc326":"## Set up","7b2bdeba":"## Churn Prediction - balanced data","4a7015f9":"### SVM","4d6c4034":"### Categorical Features","c81085a5":"### AUC","a08e3bc3":"Undersampling didn't improve results either. At best, they are very similar to the ones we got without adopting any sampling strategy. Again, SVM showed a small improvement.","1db921d6":"### Logistic Regression","6b31e773":"### Numerical Features","51241e43":"### XGBoost","bb2ac30d":"### AUC","852c0112":"Oversampling didn't produce better results, excepted by a slight improvement on SVM. Now let's try undersampling.","06c0256a":"## EDA - Exploratory Data Analysis","5f95de65":"Plots above show us that there are outliers on 'CreditScore' and 'Age' ('NumOfProducts' = 4 doesn't seem to be an outlier). We can also notice that distributions are different for churned cliest for the variables 'Age', 'Tenure' and 'Balance'.","b4e5db47":"### MLP","9072b962":"### SVM","fb9e81ea":"### Outlier treatment","27bf92b2":"## Churn Prediction - unbalanced data","afe1de9c":"### Random Forest","b85a4dcf":"### MLP","b55d26fc":"### Logistic Regression","8a844584":"### XGBoost","6befd90a":"So far XGBoost presented the best results.","694053db":"### Random Forest","523747ba":"### Scaling","900d1280":"### MLP","5ea23733":"Now for modeling purposes, before applying any transformation to the data, it's of **capital importance to isolate some data for validation**, otherwise we would incur in leakage.","b1405be5":"No variables are too highly correlated to be considered a problem. The variables with greater correlation to the target are 'Age', 'IsActiveMember', 'NumOfProducts' and 'Balance'.","943edbb7":"## Final coments\n\nResults show that XGBoost, a tree based method, presented the best results.\nThe final selection of the best model is not a task that can be generalized (or shouldn't). It depends on several factors, like what kind of error your application wants to minimize (impacting on which metrics we should focus on), feature importance, model interpretation etc. This last factor is gaining importance nowadays. In my experience, I've seen big credit companies choosing a simpler model with worse performance because it was simplier, or more intuitive. The final seleciton definetly should be done with a joint effort of data and domains knowledge specialists.\n\nGenerally speaking, AUC of 0.84 is already a satisfactory result, but again, it depends on the application needs and scope. Also, there are a few more steps we could do in order to check if we could improve performance, like feature selection, feature engineering and a mixed strategy of over and under sampling, for example.","43b32d54":"### XGBoost"}}