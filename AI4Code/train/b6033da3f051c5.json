{"cell_type":{"7013edb4":"code","25cd15d9":"code","9a7bd148":"code","87877b76":"code","1cf8ff6e":"code","82ee688b":"code","6bf13423":"code","03fa6156":"code","0c72fee0":"code","68c0dda1":"code","f7794ba3":"code","ab37178d":"code","4463234c":"code","839e7768":"code","680aaf27":"code","62d8d5db":"code","5ae079fd":"code","3625c26c":"code","7225517a":"code","23e4be96":"code","908989e3":"code","9aeeb246":"code","89a63bc5":"code","690f6ca1":"code","b8f30058":"code","1897b304":"code","a3db639b":"code","bc2d9434":"code","2307a845":"code","1886719b":"code","29478799":"code","d8bc2e71":"code","33131950":"code","68a5eea7":"code","cecb918d":"markdown","7dcf97b9":"markdown","09db1294":"markdown","d7e922de":"markdown","7f1d2324":"markdown","e1a77f6d":"markdown","23a31405":"markdown","4a6331c0":"markdown","f5f714f3":"markdown","0cea210f":"markdown","8f4cda3c":"markdown","c9c496a0":"markdown","04b426b1":"markdown","a0fb73e0":"markdown","adf2e5b7":"markdown"},"source":{"7013edb4":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,Activation\nfrom tqdm import tqdm_notebook","25cd15d9":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","9a7bd148":"train_df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","87877b76":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","1cf8ff6e":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","82ee688b":"train_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_ori, 2)","6bf13423":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","03fa6156":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","0c72fee0":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")","68c0dda1":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","f7794ba3":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","ab37178d":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","4463234c":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","839e7768":"def _down_block(x, num_filters=16, kernel_size=(3, 3), padding='same', activation='relu',\npool_size=(2, 2), dropout=0.25):\n    conv = _resnet_block(x, num_filters=num_filters, kernel_size=kernel_size, padding=padding,\n    activation=activation)\n    pool = conv\n    if pool_size != None:\n        #pool = MaxPooling2D(pool_size) (conv)\n        pool = Conv2D(num_filters, kernel_size, padding='same', strides=pool_size, activation='tanh') (conv)\n        #pool = Conv2D(num_filters, kernel_size, padding='same', activation='tanh') (pool)\n\n    if dropout != None:\n        pool = Dropout(dropout) (pool)\n\n    return pool, conv\n\ndef _up_block(x, c, num_filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same',\nactivation='relu', pool_size=(2, 2), dropout=0.25):\n    conv = Conv2DTranspose(num_filters, kernel_size, strides=strides, padding=padding)(x)\n    conv = _activation(activation, conv)\n    conv = concatenate([conv, c])\n    conv = Dropout(dropout)(conv)\n    conv = Conv2D(num_filters, (kernel_size), padding=padding) (conv)\n    conv = _activation(activation, conv)\n    conv = Conv2D(num_filters, (kernel_size), padding=padding) (conv)\n    conv = _activation(activation, conv)\n    #conv = _resnet_block(conv, num_filters=num_filters, kernel_size=kernel_size, padding=padding,\n    #activation=activation)\n    return conv\n\ndef _resnet_block(x, num_filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same',\nactivation='relu'):\n    conv1 = Conv2D(num_filters, (7, 7), padding=padding) (x)\n    conv1 = _activation(activation, conv1)\n\n    conv2 = Conv2D(num_filters, (5, 5), padding=padding) (x)\n    conv2 = _activation(activation, conv2)\n\n    conv3 = Conv2D(num_filters, (3, 3), padding=padding) (x)\n    conv3 = _activation(activation, conv3)\n\n    return concatenate([conv1, conv2, conv3, x])\n\ndef _activation(act, x):\n    if act == 'leakyrelu':\n        return LeakyReLU()(x)\n    else:\n        return Activation(act)(x)\n\n\n\ndef Unet():\n    input_img = Input((img_size_target, img_size_target, 1), name='img')\n\n    p1, c1 = _down_block(input_img, num_filters=16, dropout=0.3, activation='relu')\n    p2, c2 = _down_block(p1, num_filters=32, dropout=0.5, activation='relu')\n    p3, c3 = _down_block(p2, num_filters=64, dropout=0.5, activation='relu')\n    p4, c4 = _down_block(p3, num_filters=128, dropout=0.5, activation='relu')\n\n    _, m1 = _down_block(p4, num_filters=256, dropout=None, pool_size=None)\n    _, m2 = _down_block(m1, num_filters=512, dropout=None, pool_size=None)\n\n    d4 = _up_block(m2, c4, num_filters=128, dropout=0.5, activation='relu')\n    d3 = _up_block(d4, c3, num_filters=64, dropout=0.5, activation='relu')\n    d2 = _up_block(d3, c2, num_filters=32, dropout=0.5, activation='relu')\n    d1 = _up_block(d2, c1, num_filters=16, dropout=0.5, activation='relu')\n\n    dp = Dropout(0.5) (d1)\n    output = Conv2D(1, (1, 1), padding='same', activation='sigmoid') (dp)\n\n    return Model(inputs=[input_img], outputs=[output])\n\n","680aaf27":"model = Unet()","62d8d5db":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","5ae079fd":"model.summary()","3625c26c":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","7225517a":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)\/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)\/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","23e4be96":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\".\/keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","908989e3":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","9aeeb246":"model = load_model(\".\/keras.model\")","89a63bc5":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid = np.array([downsample(x) for x in y_valid])","690f6ca1":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_train[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","b8f30058":"# src: https:\/\/www.kaggle.com\/aglotero\/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection \/ union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp \/ (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","1897b304":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","a3db639b":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","bc2d9434":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","2307a845":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_train[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","1886719b":"# Source https:\/\/www.kaggle.com\/bguberfain\/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","29478799":"x_test = np.array([upsample(np.array(load_img(\"..\/input\/test\/images\/{}.png\".format(idx), grayscale=True))) \/ 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","d8bc2e71":"preds_test = model.predict(x_test)","33131950":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","68a5eea7":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","cecb918d":"# Data augmentation","7dcf97b9":"# Params and helpers","09db1294":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage.","d7e922de":"# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","7f1d2324":"# Scoring\nScore the model and do a threshold optimization by the best IoU.","e1a77f6d":"# Show some example images","23a31405":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","4a6331c0":"# Submission\nLoad, predict and submit the test image predictions.","f5f714f3":"# Training","0cea210f":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","8f4cda3c":"# Build model","c9c496a0":"# Loading of training\/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.","04b426b1":"# About\nSince I am new to learning from image segmentation and kaggle in general I want to share my noteook.\nI saw it is similar to others as it uses the U-net approach. I want to share it anyway because:\n\n- As said, the field is new to me so I am open to suggestions.\n- It visualizes some of the steps, e.g. scaling, to learn if the methods do what I expect which might be useful to others (I call them sanity checks).\n- Added stratification by the amount of salt contained in the image.\n- Added augmentation by flipping the images along the y axes (thanks to the forum for clarification).\n- Added dropout to the model which seems to improve performance.","a0fb73e0":"# Create train\/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.","adf2e5b7":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data."}}