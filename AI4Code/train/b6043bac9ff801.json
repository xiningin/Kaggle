{"cell_type":{"73f19f2c":"code","08c99c0b":"code","eee7bb23":"code","17aad60b":"code","cceddc8e":"code","6f082b6d":"code","08805b30":"code","28188ef4":"code","f51361f1":"code","7047f5df":"code","8ccb7ac0":"code","e509de23":"code","7f424ef8":"code","8794a809":"code","bda9083b":"code","03e2a95f":"markdown","bc297cfb":"markdown","8150cb6a":"markdown","d9d8e0f7":"markdown","436f3830":"markdown","ff8099a0":"markdown","87df415c":"markdown","80c5edc4":"markdown","10f15819":"markdown","19b767f9":"markdown","57617a14":"markdown"},"source":{"73f19f2c":"import warnings\nwarnings.filterwarnings(\"ignore\")","08c99c0b":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")","eee7bb23":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","17aad60b":"#!\/usr\/bin\/env python\n# encoding: utf-8\n\n#PyTextRank Algorithm\n\nfrom collections import defaultdict, OrderedDict\nfrom math import sqrt\nfrom operator import itemgetter\nfrom spacy.tokens import Doc\nimport graphviz\nimport json\nimport logging\nimport networkx as nx\nimport os\nimport os.path\nimport re\nimport spacy\nimport string\nimport sys\nimport time\nimport unicodedata\n\n\n######################################################################\n## utility functions\n######################################################################\n\nPAT_FORWARD = re.compile(\"\\n\\-+ Forwarded message \\-+\\n\")\nPAT_REPLIED = re.compile(\"\\nOn.*\\d+.*\\n?wrote\\:\\n+\\>\")\nPAT_UNSUBSC = re.compile(\"\\n\\-+\\nTo unsubscribe,.*\\nFor additional commands,.*\")\n\n\ndef split_grafs (lines):\n    \"\"\"\n    segment raw text, given as a list of lines, into paragraphs\n    \"\"\"\n    graf = []\n\n    for line in lines:\n        line = line.strip()\n\n        if len(line) < 1:\n            if len(graf) > 0:\n                yield \"\\n\".join(graf)\n                graf = []\n        else:\n            graf.append(line)\n\n    if len(graf) > 0:\n        yield \"\\n\".join(graf)\n\n\ndef filter_quotes (text, is_email=True):\n    \"\"\"\n    filter the quoted text out of a message\n    \"\"\"\n    global PAT_FORWARD, PAT_REPLIED, PAT_UNSUBSC\n\n    if is_email:\n        text = filter(lambda x: x in string.printable, text)\n\n        # strip off quoted text in a forward\n        m = PAT_FORWARD.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off quoted text in a reply\n        m = PAT_REPLIED.split(text, re.M)\n\n        if m and len(m) > 1:\n            text = m[0]\n\n        # strip off any trailing unsubscription notice\n        m = PAT_UNSUBSC.split(text, re.M)\n\n        if m:\n            text = m[0]\n\n    # replace any remaining quoted text with blank lines\n    lines = []\n\n    for line in text.split(\"\\n\"):\n        if line.startswith(\">\"):\n            lines.append(\"\")\n        else:\n            lines.append(line)\n\n    return list(split_grafs(lines))\n\n\ndef maniacal_scrubber (text):\n    \"\"\"\n    it scrubs the garble from its stream...\n    or it gets the debugger again\n    \"\"\"\n    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n\n    x = x.replace('\u201c', '\"').replace('\u201d', '\"')\n    x = x.replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\").replace(\"`\", \"'\")\n    x = x.replace(\"\u2026\", \"...\").replace(\"\u2013\", \"-\")\n\n    x = str(unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\"))\n\n    # some web content returns \"not string\" ?? ostensibly no longer\n    # possibl in Py 3.x but crazy \"mixed modes\" of character encodings\n    # have been found in the wild -- YMMV\n\n    try:\n        assert type(x).__name__ == \"str\"\n    except AssertionError:\n        print(\"not a string?\", type(line), line)\n\n    return x\n\n\ndef default_scrubber (text):\n    \"\"\"\n    remove spurious punctuation (for English)\n    \"\"\"\n    return text.lower().replace(\"'\", \"\")\n\n\n######################################################################\n## class definitions\n######################################################################\n\nclass CollectedPhrase:\n    \"\"\"\n    represents one phrase during the collection process\n    \"\"\"\n\n    def __init__ (self, chunk, scrubber):\n        self.sq_sum_rank = 0.0\n        self.non_lemma = 0\n        \n        self.chunk = chunk\n        self.text = scrubber(chunk.text)\n\n\n    def __repr__ (self):\n        return \"{:.4f} ({},{}) {} {}\".format(\n            self.rank, self.chunk.start, self.chunk.end, self.text, self.key\n        )\n\n\n    def range (self):\n        \"\"\"\n        generate the index range for the span of tokens in this phrase\n        \"\"\"\n        return range(self.chunk.start, self.chunk.end)\n\n\n    def set_key (self, compound_key):\n        \"\"\"\n        create a unique key for the the phrase based on its lemma components\n        \"\"\"\n        self.key = tuple(sorted(list(compound_key)))\n\n\n    def calc_rank (self):\n        \"\"\"\n        since noun chunking is greedy, we normalize the rank values\n        using a point estimate based on the number of non-lemma\n        tokens within the phrase\n        \"\"\"\n        chunk_len = self.chunk.end - self.chunk.start + 1\n        non_lemma_discount = chunk_len \/ (chunk_len + (2.0 * self.non_lemma) + 1.0)\n\n        # normalize the contributions of all the kept lemma tokens\n        # within the phrase using root mean square (RMS)\n\n        self.rank = sqrt(self.sq_sum_rank \/ (chunk_len + self.non_lemma)) * non_lemma_discount\n\n\nclass Phrase:\n    \"\"\"\n    represents one extracted phrase\n    \"\"\"\n\n    def __init__ (self, text, rank, count, phrase_list):\n        self.text = text\n        self.rank = rank\n        self.count = count\n        self.chunks = [p.chunk for p in phrase_list]\n\n\n    def __repr__ (self):\n        return self.text\n\n\nclass TextRank:\n    \"\"\"\n    Python impl of TextRank by Milhacea, et al., as a spaCy extension,\n    used to extract the top-ranked phrases from a text document\n    \"\"\"\n    _EDGE_WEIGHT = 1.0\n    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n    _TOKEN_LOOKBACK = 3\n    \n\n    def __init__ (\n            self,\n            edge_weight=_EDGE_WEIGHT,\n            logger=None,\n            pos_kept=_POS_KEPT,\n            scrubber=default_scrubber,\n            token_lookback=_TOKEN_LOOKBACK\n    ):\n        self.edge_weight = edge_weight\n        self.logger = logger\n        self.pos_kept = pos_kept\n        self.scrubber = scrubber\n        self.stopwords = defaultdict(list)\n        self.token_lookback = token_lookback\n\n        self.doc = None\n        self.reset()\n\n\n    def reset (self):\n        \"\"\"\n        initialize the data structures needed for extracting phrases\n        removing any state\n        \"\"\"\n        self.elapsed_time = 0.0\n        self.lemma_graph = nx.Graph()\n        self.phrases = defaultdict(list)\n        self.ranks = {}\n        self.seen_lemma = OrderedDict()\n\n\n    def load_stopwords (self, path=\"stop.json\"):\n        \"\"\"\n        load a list of \"stop words\" that get ignored when constructing\n        the lemma graph -- NB: be cautious when using this feature\n        \"\"\"\n        stop_path = None\n\n        # check if the path is fully qualified, or if the file is in\n        # the current working directory\n\n        if os.path.isfile(path):\n            stop_path = path\n        else:\n            cwd = os.getcwd()\n            stop_path = os.path.join(cwd, path)\n\n            if not os.path.isfile(stop_path):\n                loc = os.path.realpath(os.path.join(cwd, os.path.dirname(__file__)))\n                stop_path = os.path.join(loc, path)\n\n        try:\n            with open(stop_path, \"r\") as f:\n                data = json.load(f)\n\n                for lemma, pos_list in data.items():\n                    self.stopwords[lemma] = pos_list\n        except FileNotFoundError:\n            pass\n\n\n    def increment_edge (self, node0, node1):\n        \"\"\"\n        increment the weight for an edge between the two given nodes,\n        creating the edge first if needed\n        \"\"\"\n        if self.logger:\n            self.logger.debug(\"link {} {}\".format(node0, node1))\n    \n        if self.lemma_graph.has_edge(node0, node1):\n            self.lemma_graph[node0][node1][\"weight\"] += self.edge_weight\n        else:\n            self.lemma_graph.add_edge(node0, node1, weight=self.edge_weight)\n\n\n    def link_sentence (self, sent):\n        \"\"\"\n        link nodes and edges into the lemma graph for one parsed sentence\n        \"\"\"\n        visited_tokens = []\n        visited_nodes = []\n\n        for i in range(sent.start, sent.end):\n            token = self.doc[i]\n\n            if token.pos_ in self.pos_kept:\n                # skip any stop words...\n                lemma = token.lemma_.lower().strip()\n\n                if lemma in self.stopwords and token.pos_ in self.stopwords[lemma]:\n                    continue\n\n                # ...otherwise proceed\n                key = (token.lemma_, token.pos_)\n\n                if key not in self.seen_lemma:\n                    self.seen_lemma[key] = set([token.i])\n                else:\n                    self.seen_lemma[key].add(token.i)\n\n                node_id = list(self.seen_lemma.keys()).index(key)\n\n                if not node_id in self.lemma_graph:\n                    self.lemma_graph.add_node(node_id)\n\n                if self.logger:\n                    self.logger.debug(\"visit {} {}\".format(\n                        visited_tokens, visited_nodes\n                    ))\n                    self.logger.debug(\"range {}\".format(\n                        list(range(len(visited_tokens) - 1, -1, -1))\n                    ))\n            \n                for prev_token in range(len(visited_tokens) - 1, -1, -1):\n                    if self.logger:\n                        self.logger.debug(\"prev_tok {} {}\".format(\n                            prev_token, (token.i - visited_tokens[prev_token])\n                        ))\n                \n                    if (token.i - visited_tokens[prev_token]) <= self.token_lookback:\n                        self.increment_edge(node_id, visited_nodes[prev_token])\n                    else:\n                        break\n\n                if self.logger:\n                    self.logger.debug(\" -- {} {} {} {} {} {}\".format(\n                        token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes\n                    ))\n\n                visited_tokens.append(token.i)\n                visited_nodes.append(node_id)\n\n\n    def collect_phrases (self, chunk):\n        \"\"\"\n        collect instances of phrases from the lemma graph\n        based on the given chunk\n        \"\"\"\n        phrase = CollectedPhrase(chunk, self.scrubber)\n        compound_key = set([])\n\n        for i in phrase.range():\n            token = self.doc[i]\n            key = (token.lemma_, token.pos_)\n        \n            if key in self.seen_lemma:\n                node_id = list(self.seen_lemma.keys()).index(key)\n                rank = self.ranks[node_id]\n                phrase.sq_sum_rank += rank\n                compound_key.add(key)\n        \n                if self.logger:\n                    self.logger.debug(\" {} {} {} {}\".format(\n                        token.lemma_, token.pos_, node_id, rank\n                    ))\n            else:\n                phrase.non_lemma += 1\n    \n        phrase.set_key(compound_key)\n        phrase.calc_rank()\n\n        self.phrases[phrase.key].append(phrase)\n\n        if self.logger:\n            self.logger.debug(phrase)\n\n\n    def calc_textrank (self):\n        \"\"\"\n        iterate through each sentence in the doc, constructing a lemma graph\n        then returning the top-ranked phrases\n        \"\"\"\n        self.reset()\n        t0 = time.time()\n\n        for sent in self.doc.sents:\n            self.link_sentence(sent)\n\n        if self.logger:\n            self.logger.debug(self.seen_lemma)\n\n        # to run the algorithm, we use PageRank \u2013 i.e., approximating\n        # eigenvalue centrality \u2013 to calculate ranks for each of the\n        # nodes in the lemma graph\n\n        self.ranks = nx.pagerank(self.lemma_graph)\n\n        # collect the top-ranked phrases based on both the noun chunks\n        # and the named entities\n\n        for chunk in self.doc.noun_chunks:\n            self.collect_phrases(chunk)\n\n        for ent in self.doc.ents:\n            self.collect_phrases(ent)\n\n        # since noun chunks can be expressed in different ways (e.g., may\n        # have articles or prepositions), we need to find a minimum span\n        # for each phrase based on combinations of lemmas\n\n        min_phrases = {}\n\n        for phrase_key, phrase_list in self.phrases.items():\n            phrase_list.sort(key=lambda p: p.rank, reverse=True)\n            best_phrase = phrase_list[0]\n            min_phrases[best_phrase.text] = (best_phrase.rank, len(phrase_list), phrase_key)\n\n        # yield results\n\n        results = sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)\n\n        phrase_list = [\n            Phrase(p, r, c, self.phrases[k]) for p, (r, c, k) in results\n        ]\n\n        t1 = time.time()\n        self.elapsed_time = (t1 - t0) * 1000.0\n\n        return phrase_list\n\n\n    def write_dot (self, path=\"graph.dot\"):\n        \"\"\"\n        output the lemma graph in Dot file format\n        \"\"\"\n        keys = list(self.seen_lemma.keys())\n        dot = graphviz.Digraph()\n\n        for node_id in self.lemma_graph.nodes():\n            text = keys[node_id][0].lower()\n            rank = self.ranks[node_id]\n            label = \"{} ({:.4f})\".format(text, rank)\n            dot.node(str(node_id), label)\n\n        for edge in self.lemma_graph.edges():\n            dot.edge(str(edge[0]), str(edge[1]), constraint=\"false\")\n\n        with open(path, \"w\") as f:\n            f.write(dot.source)\n\n\n    def summary (self, limit_phrases=10, limit_sentences=4):\n        \"\"\"\n        run extractive summarization, based on vector distance \n        per sentence from the top-ranked phrases\n        \"\"\"\n        unit_vector = []\n\n        # construct a list of sentence boundaries with a phrase set\n        # for each (initialized to empty)\n\n        sent_bounds = [ [s.start, s.end, set([])] for s in self.doc.sents ]\n\n        # iterate through the top-ranked phrases, added them to the\n        # phrase vector for each sentence\n\n        phrase_id = 0\n\n        for p in self.doc._.phrases:\n            unit_vector.append(p.rank)\n\n            if self.logger:\n                self.logger.debug(\n                    \"{} {} {}\".format(phrase_id, p.text, p.rank)\n                )\n    \n            for chunk in p.chunks:\n                for sent_start, sent_end, sent_vector in sent_bounds:\n                    if chunk.start >= sent_start and chunk.start <= sent_end:\n                        sent_vector.add(phrase_id)\n\n                        if self.logger:\n                            self.logger.debug(\n                                \" {} {} {} {}\".format(sent_start, chunk.start, chunk.end, sent_end)\n                                )\n\n                        break\n\n            phrase_id += 1\n\n            if phrase_id == limit_phrases:\n                break\n\n        # construct a unit_vector for the top-ranked phrases, up to\n        # the requested limit\n\n        sum_ranks = sum(unit_vector)\n        unit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\n        # iterate through each sentence, calculating its euclidean\n        # distance from the unit vector\n\n        sent_rank = {}\n        sent_id = 0\n\n        for sent_start, sent_end, sent_vector in sent_bounds:\n            sum_sq = 0.0\n    \n            for phrase_id in range(len(unit_vector)):\n                if phrase_id not in sent_vector:\n                    sum_sq += unit_vector[phrase_id]**2.0\n\n            sent_rank[sent_id] = sqrt(sum_sq)\n            sent_id += 1\n\n        # extract the sentences with the lowest distance\n\n        sent_text = {}\n        sent_id = 0\n\n        for sent in self.doc.sents:\n            sent_text[sent_id] = sent\n            sent_id += 1\n\n        # yield results, up to the limit requested\n\n        num_sent = 0\n\n        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n            yield sent_text[sent_id]\n            num_sent += 1\n\n            if num_sent == limit_sentences:\n                break\n\n\n    def PipelineComponent (self, doc):\n        \"\"\"\n        define a custom pipeline component for spaCy and extend the\n        Doc class to add TextRank\n        \"\"\"\n        self.doc = doc\n        Doc.set_extension(\"phrases\", force=True, default=[])\n        Doc.set_extension(\"textrank\", force=True, default=self)\n        doc._.phrases = self.calc_textrank()\n\n        return doc","cceddc8e":"length=3\ntext = input()\ntr = TextRank()\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\ndoc = nlp(text)","6f082b6d":"for p in doc._.phrases:\n   print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n   print(p.chunks)","08805b30":"sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\nsent_bounds","28188ef4":"limit_phrases = 4\n\nphrase_id = 0\nunit_vector = []\n\nfor p in doc._.phrases:\n    print(phrase_id, p.text, p.rank)\n    \n    unit_vector.append(p.rank)\n    \n    for chunk in p.chunks:\n        print(\" \", chunk.start, chunk.end)\n        \n        for sent_start, sent_end, sent_vector in sent_bounds:\n            if chunk.start >= sent_start and chunk.start <= sent_end:\n                print(\" \", sent_start, chunk.start, chunk.end, sent_end)\n                sent_vector.add(phrase_id)\n                break\n\n    phrase_id += 1\n\n    if phrase_id == limit_phrases:\n        break","f51361f1":"sent_bounds","7047f5df":"for sent in doc.sents:\n    print(sent)","8ccb7ac0":"unit_vector","e509de23":"sum_ranks = sum(unit_vector)\nunit_vector = [ rank\/sum_ranks for rank in unit_vector ]\n\nunit_vector","7f424ef8":"from math import sqrt\n\nsent_rank = {}\nsent_id = 0\n\nfor sent_start, sent_end, sent_vector in sent_bounds:\n    print(sent_vector)\n    sum_sq = 0.0\n    \n    for phrase_id in range(len(unit_vector)):\n        print(phrase_id, unit_vector[phrase_id])\n        \n        if phrase_id not in sent_vector:\n            sum_sq += unit_vector[phrase_id]**2.0\n\n    sent_rank[sent_id] = sqrt(sum_sq)\n    sent_id += 1\n\nprint(sent_rank)","8794a809":"from operator import itemgetter\n\nsorted(sent_rank.items(), key=itemgetter(1)) ","bda9083b":"limit_sentences = length\n\nsent_text = {}\nsent_id = 0\n\nfor sent in doc.sents:\n    sent_text[sent_id] = sent.text\n    sent_id += 1\n\nnum_sent = 0\n\nfor sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n    print(sent_id, sent_text[sent_id])\n    num_sent += 1\n    \n    if num_sent == limit_sentences:\n        break","03e2a95f":"Importing spacy library","bc297cfb":"Import data or fill text","8150cb6a":"Examine the results: a list of top-ranked phrases in the document","d9d8e0f7":"Extract the sentences with the lowest distance, up to the limit requested...","436f3830":"Construct a list of the sentence boundaries with a phrase vector (initialized to empty set) for each...","ff8099a0":"Let's take a look at the results...","87df415c":"We also construct a `unit_vector` for all of the phrases, up to the limit requested...","80c5edc4":"Sort the sentence indexes in descending order","10f15819":"# Extractive Summarization using Text Rank Algorithm\n","19b767f9":"Iterate through each sentence, calculating its *euclidean distance* from the unit vector...","57617a14":"Iterate through the top-ranked phrases, added them to the phrase vector for each sentence..."}}