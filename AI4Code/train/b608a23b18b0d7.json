{"cell_type":{"40266ab0":"code","62eb1211":"code","d6cfa14a":"code","c077f1c8":"code","bb1ed2db":"code","bf375909":"code","0745f02a":"code","864a7e77":"code","ab8fd554":"code","5529531f":"code","81ffdcf5":"code","7de50789":"code","23a3e4bb":"code","0913a209":"markdown","1fb44317":"markdown","0fe8d7d6":"markdown","2c9b3772":"markdown","245c6454":"markdown","f00b371c":"markdown","a8387b40":"markdown"},"source":{"40266ab0":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\n\nimport gc\ngc.enable()","62eb1211":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/huggingface-roberta\/roberta-base\"\nTOKENIZER_PATH = \"..\/input\/huggingface-roberta\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","d6cfa14a":"test_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","c077f1c8":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","bb1ed2db":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","bf375909":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","0745f02a":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","864a7e77":"test_dataset = LitDataset(test_df, inference_only=True)","ab8fd554":"NUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","5529531f":"model1_predictions = all_predictions.mean(axis=0)","81ffdcf5":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, AutoConfig,\n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\nconfig = {\n    'batch_size':8,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n    \nclass Model(nn.Module): \n    def __init__(self):\n        super().__init__() \n\n\n        config = AutoConfig.from_pretrained('..\/input\/huggingface-roberta\/roberta-large')\n        self.model = AutoModel.from_pretrained('..\/input\/huggingface-roberta\/roberta-large', config=config)\n        \n        \n\n        self.layer_norm1 = nn.LayerNorm(1024)\n        self.l1 = nn.Linear(1024, 512)\n        self.l2 = nn.Linear(512, 1)\n\n        self._init_weights(self.layer_norm1)\n        self._init_weights(self.l1)\n        self._init_weights(self.l2)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    def forward(self, input_ids, attention_mask):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]     \n        out = torch.mean(last_hidden_state, 1)\n        out = self.layer_norm1(out)\n        return out       \n\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/huggingface-roberta\/roberta-large')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\ndef get_preds_svm(X,y,X_test,RidgeReg=0,bins=bins,nfolds=10,C=8,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        if(RidgeReg):\n            print(\"ridge...\")\n            model = Ridge(alpha=80.0)\n        else:\n            model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds\n\ntrain_embeddings0 =  get_embeddings(train_data,'..\/input\/pixx0459\/model0\/model0.bin')\ntest_embeddings0 = get_embeddings(test_data,'..\/input\/pixx0459\/model0\/model0.bin')\nsvm_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0)\nridge_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0,RidgeReg=1)\ndel train_embeddings0,test_embeddings0\ngc.collect()\ntrain_embeddings1 =  get_embeddings(train_data,'..\/input\/pixx0459\/model1\/model1.bin')\ntest_embeddings1= get_embeddings(test_data,'..\/input\/pixx0459\/model1\/model1.bin')\nsvm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\nridge_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1,RidgeReg=1)\ndel train_embeddings1,test_embeddings1\ngc.collect()    \ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/pixx0459\/model2\/model2.bin')\ntest_embeddings2 = get_embeddings(test_data,'..\/input\/pixx0459\/model2\/model2.bin')\nsvm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\nridge_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2,RidgeReg=1)\ndel train_embeddings2,test_embeddings2\ngc.collect()\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/pixx0459\/model3\/model3.bin')\ntest_embeddings3 = get_embeddings(test_data,'..\/input\/pixx0459\/model3\/model3.bin')\nsvm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\nridge_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3,RidgeReg=1)\ndel train_embeddings3,test_embeddings3\ngc.collect()\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/pixx0459\/model4\/model4.bin')\ntest_embeddings4 = get_embeddings(test_data,'..\/input\/pixx0459\/model4\/model4.bin')\nsvm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\nridge_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4,RidgeReg=1)\ndel train_embeddings4,test_embeddings4\ngc.collect()\n\n\n\n\n\n\n\nsvm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds0)\/5","7de50789":"predictions = model1_predictions * 0.4 + svm_preds * 0.6","23a3e4bb":"submission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","0913a209":"# Inference","1fb44317":"# Model 1","0fe8d7d6":"# Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","2c9b3772":"# Model 2\nImported from [https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3)","245c6454":"# Dataset","f00b371c":"**Thanks for the notebooks shared from @Andrey and Tuganov @Maunish dave, I use another model trained by myself to ensemble**","a8387b40":"**\u501f\u9274\u4e86@Andrey Tuganov\u548c @Maunish dave\u7684\u65b9\u6848\uff0c\u53c8\u6362\u4e86\u4e00\u4e2a\u81ea\u5df1\u8bad\u7ec3\u7684\u6a21\u578b\u505a\u4e86\u878d\u5408\u3002**"}}