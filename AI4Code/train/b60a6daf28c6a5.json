{"cell_type":{"53f050d1":"code","d9d806be":"code","44112218":"code","6f54184d":"code","743153c1":"code","c907f444":"code","45c6e1c4":"code","052819f7":"code","7689a670":"code","64ce4ef8":"code","e6ee171c":"code","24c08602":"code","05b8489b":"code","52b08dd6":"code","f73a25cd":"code","5de41335":"code","600e4673":"code","4ebce817":"code","b7a05119":"code","a4dabf78":"code","a13920d6":"code","4d96baf7":"code","23519951":"code","33a36336":"code","66c6bb41":"code","1cfc81f3":"code","59db8521":"code","23b5d28e":"code","5e2744bd":"code","6167ba54":"code","5ef63d95":"code","1835989f":"code","f91b84cb":"code","d78986f2":"code","6d907efd":"code","757c66fa":"markdown","4d03f1a4":"markdown","f56d0bee":"markdown","600abec6":"markdown","6a7030eb":"markdown","c8d7aa14":"markdown"},"source":{"53f050d1":"##### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9d806be":"#import rest of the libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing as pre\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import tree\nfrom six import StringIO\nimport pydot\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm","44112218":"#Get data\n#Input the dataset\ndf_titanic = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","6f54184d":"#Understanding the dataset\ndf_titanic.info()","743153c1":"\"\"\"\nObservation1 -The dataset has numeric, float and categorical columns\n\"\"\"","c907f444":"#Number of rows and columns in the dataset\ndf_titanic.shape","45c6e1c4":"#printing a sample of the train dataset\ndf_titanic.head()","052819f7":"\"\"\"\nObservation 2- \nAll the features in titanic dataset -\n1) PassengerId\n2) Survived\n3) Pclass\n4) Name\n5) Sex\n6) Age\n7) SibSp\n8) Ticket\n9) Fare\n10) Cabin\n11) Embarked\n\"\"\"","7689a670":"#Discribe the dataset to understand the distribution of all the columns in the dataset\ndf_titanic.describe()","64ce4ef8":"\"\"\"\nObeservation 3 -\nTo understand the categorical data better, we need to convert them to numeric data\n\"\"\"","e6ee171c":"#Finding missing data\ndf_titanic.isnull().sum()","24c08602":"\"\"\"\nObservation 4 -\nThe missing data is found in columns Age, Cabin and Embarked - \nTo handle the missing values, we have two options,\n1) Discard these rows - This results in removing all the NaN data and as a result affect the model.\n2) Handle the missing data - Filling the missing data with the mean for example in the case of Age.\n\"\"\"","05b8489b":"df_titanic[\"Embarked\"]\ndf_titanic[\"Embarked\"].unique()","52b08dd6":"#Handling missing data\n#Replace the missing values in Age with the average age\ndf_titanic.Age.fillna(value=df_titanic.Age.mean(), inplace=True)\n#Replace the missing data in the Embarked column with the most frequently used value in the column\ndf_titanic.Embarked.fillna(value=(df_titanic.Embarked.value_counts().idxmax()), inplace=True)\ndf_titanic.isnull().sum()","f73a25cd":"#understanding categorical column 'cabin'\nsns.barplot(df_titanic.Cabin.value_counts().index,df_titanic.Cabin.value_counts())\nplt.show()","5de41335":"#Percentage of NaNs in the 'cabin' column\npercent_of_NaNs = df_titanic.Cabin.isnull().sum()\/df_titanic.Cabin.isnull().count()\npercent_of_NaNs","600e4673":"\"\"\"\nObervation 5 -\nThe % of null values in the Cabin column is 77%, that's more than half of the column values. \nTherefore, we can drop this column.\n\"\"\"\ndf_titanic.drop('Cabin', axis=1, inplace=True)","4ebce817":"#Checking the number of null values again \ndf_titanic.isnull().sum()","b7a05119":"# Describing the columns again\ndf_titanic.info()","a4dabf78":"\"\"\"\nObservation 6 - \nNow that the null values are handled, need to convert the categorical values in the dataset.\nSame goes with float values, they need to be converted to numeric as well.\n\nBefore we change the categorical values to numeric, we need to remove unnecessary columns if any. \nColumns - PassengerId, Name and Ticket are categorical values that don't make sense when converted to numeric.\nSo we can drop these columns too.\n\"\"\"","a13920d6":"#Dropping columns PassengerId,Name and Ticket\ndf_cleaned_titanic = df_titanic.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)","4d96baf7":"#To convert all the categorical values into numerical values using labelencoder()\ndf_cleaned_titanic = df_cleaned_titanic.apply(pre.LabelEncoder().fit_transform)\ndf_cleaned_titanic","23519951":"\"\"\"\nObservation 7 - The dataset we have right now is a cleaned dataset.\nWe now need to make sure our data is understandable by the model. For the same reason,\nwe group Age column and the Fare cloumns by creating buckets.\n\"\"\"","33a36336":"# Understand the distribution of Age\ndf_cleaned_titanic.Age.plot(kind='hist')\nplt.xlabel(\"Age\")","66c6bb41":"# Understand the distribution of Fare\ndf_cleaned_titanic.Fare.plot(kind='hist')\nplt.xlabel(\"Fare\")","1cfc81f3":"# Grouping Age and Fare into buckets\ndf_cleaned_titanic['Age'] = df_cleaned_titanic['Age'].astype(int)\ndf_cleaned_titanic.loc[ df_cleaned_titanic['Age'] <= 10, 'Age'] = 0\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Age'] > 10) & (df_cleaned_titanic['Age'] <= 20), 'Age'] = 1\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Age'] > 20) & (df_cleaned_titanic['Age'] <= 30), 'Age'] = 2\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Age'] > 30) & (df_cleaned_titanic['Age'] <= 40), 'Age'] = 3\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Age'] > 40) & (df_cleaned_titanic['Age'] <= 50), 'Age'] = 4\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Age'] > 50) & (df_cleaned_titanic['Age'] <= 60), 'Age'] = 5\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Age'] > 60) & (df_cleaned_titanic['Age'] <= 70), 'Age'] = 6\ndf_cleaned_titanic.loc[ df_cleaned_titanic['Age'] > 70, 'Age'] = 7\n    \ndf_cleaned_titanic['Fare'] = df_cleaned_titanic['Fare'].astype(int)\ndf_cleaned_titanic.loc[ df_cleaned_titanic['Fare'] <= 10, 'Fare'] = 0\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Fare'] > 10) & (df_cleaned_titanic['Fare'] <= 50), 'Fare'] = 1\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Fare'] > 50) & (df_cleaned_titanic['Fare'] <= 100), 'Fare']   = 2\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Fare'] > 100) & (df_cleaned_titanic['Fare'] <= 150), 'Fare']   = 3\ndf_cleaned_titanic.loc[(df_cleaned_titanic['Fare'] > 150) & (df_cleaned_titanic['Fare'] <= 200), 'Fare']   = 4\ndf_cleaned_titanic.loc[ df_cleaned_titanic['Fare'] > 200 & (df_cleaned_titanic['Fare'] <= 250), 'Fare'] = 5\ndf_cleaned_titanic.loc[ df_cleaned_titanic['Fare'] > 250, 'Fare'] = 6","59db8521":"df_cleaned_titanic.info()","23b5d28e":"#The dataset we have now is the dataset that's going to be used to model\n#Decision Tree model\nX = df_cleaned_titanic.drop(\"Survived\", axis=1)\nY = df_cleaned_titanic[\"Survived\"]\nDesClass = DecisionTreeClassifier()\nDTmodel = DesClass.fit(X, Y)","5e2744bd":"#Q3)Learn and fine-tune a decision tree model with the Titanic training data, plot your decision tree;\n\n#Plotting the tree \ndot_data = StringIO()\ntree.export_graphviz(DesClass, out_file=dot_data) \ngraph = pydot.graph_from_dot_data(dot_data.getvalue()) \nprint(graph[0])","6167ba54":"#plotting tree using figure\nfeature_columns = X.columns\nTarget = ['0','1']\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(DesClass, \n                   feature_names= feature_columns,  \n                   class_names= Target,\n                   filled=True)","5ef63d95":"decision_tree = DecisionTreeClassifier()\nscores = cross_val_score(decision_tree, X, Y, cv=5, scoring = \"accuracy\")\nprint(\"Decison Tree Scores:\", scores)\nprint(\"Desicion Tree Average Scores:\", scores.mean())","1835989f":"random_forest = RandomForestClassifier(n_estimators=100)\nRF_scores = cross_val_score(random_forest, X, Y, cv=5, scoring = \"accuracy\")\nprint(\"Random Forest Scores -\", RF_scores)\nprint(\"Random Forest Average Scores -\", RF_scores.mean())","f91b84cb":"#SVM using linear kernel\nfrom sklearn.model_selection import cross_val_score\nlsvm = svm.SVC(kernel='linear', C=1, random_state=42)\nlinear_scores = cross_val_score(lsvm, X, Y, cv=5)\nprint(\"The accuracy is given by -\",linear_scores.mean())\nprint(\"The standard deviation is given by -\",linear_scores.std())","d78986f2":"#SVM using RBF kernel\nfrom sklearn.model_selection import cross_val_score\nrbf_svm = svm.SVC(kernel='rbf', C=1, random_state=42)\nrbf_scores = cross_val_score(rbf_svm, X, Y, cv=5)\nprint(\"The accuracy is given by -\",rbf_scores.mean())\nprint(\"The standard deviation is given by -\",rbf_scores.std())","6d907efd":"#SVM using degree 2 polynomial kernel\nfrom sklearn.model_selection import cross_val_score\ndeg2_svm = svm.SVC(kernel='poly',degree=2, C=1, random_state=42)\ndeg2_scores = cross_val_score(deg2_svm, X, Y, cv=5)\nprint(\"The accuracy is given by -\",deg2_scores.mean())\nprint(\"The standard deviation is given by -\",deg2_scores.std())","757c66fa":"**Q2) Select a set of important features. Please show your selected features and explain how\nyou perform feature selection.**","4d03f1a4":"**Q4) Apply the five-fold cross validation of your fine-tuned decision tree learning model to\nthe Titanic training data to extract average classification accuracy**","f56d0bee":"**Q3) Learn and fine-tune a decision tree model with the Titanic training data, plot your\ndecision tree**","600abec6":"# TASK 1\n**Q1) Preprocess your Titanic training data**","6a7030eb":"**Data Preprocessing**","c8d7aa14":"**Q5) Apply the five-fold cross validation of your fine-tuned random forest learning model to\nthe Titanic training data to extract average classification accuracy**"}}