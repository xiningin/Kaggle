{"cell_type":{"c5f979f1":"code","00249c35":"code","a3ab5371":"code","6748115e":"code","6f98fee6":"code","894191cd":"code","68fbbd26":"code","8c00b759":"code","e8c7f4ec":"code","0ac61497":"code","e664bd88":"code","9448b251":"code","b38b5e38":"code","b645940a":"code","26a91cfd":"code","15ffce59":"code","5a8030d4":"code","5ae381d5":"code","abc81e90":"code","b742d0e0":"code","c996b152":"code","72612c40":"code","c02dac6e":"code","2921b495":"code","c628eeb1":"code","8e6e23a8":"code","a7a433b7":"code","dc75f64e":"code","6d450043":"code","f2d606a3":"code","912941e2":"code","a634a878":"code","1a8dc175":"code","091cd5bf":"code","31cc58ba":"code","7ed5cae6":"code","1f3e430a":"code","8d0af4f8":"code","85564444":"code","0d9c01f8":"code","b4f06642":"code","d3c430d2":"code","d7790aa8":"code","95c8136f":"code","c723c682":"code","bb8b1ce1":"code","48ea7305":"code","33986779":"code","28e3ad71":"code","496673c9":"code","4f5497c7":"code","6037af9e":"code","7ac3e33c":"code","9925c5eb":"code","9a9e0683":"code","4397be75":"code","458e9a4a":"code","72dc0909":"code","a626e000":"code","0c9125ca":"code","5ff5654e":"code","3c9da7d9":"code","be140344":"code","efc09800":"code","8ca071b6":"code","579bb8c8":"code","a65d1d61":"code","5939e98e":"code","7406bbfb":"code","3513e027":"code","b1556ba0":"code","89ccd5cf":"code","a902b0e0":"code","b61a9c13":"code","828c8844":"code","17702bf0":"markdown","a6b1cab9":"markdown","0dff1aee":"markdown","c4ac15a5":"markdown","d5043267":"markdown","0af17abf":"markdown","3fdecbef":"markdown","144b7882":"markdown","13592f97":"markdown","edbf8c03":"markdown","76afdbf7":"markdown","17a643cd":"markdown","e7197cf7":"markdown","8240e4ba":"markdown","1559706d":"markdown","d2e2a648":"markdown","1f527167":"markdown","3a5b38bf":"markdown","c0a41085":"markdown","397c088f":"markdown","458f8bb1":"markdown","76bc0b0a":"markdown","0403d86e":"markdown","ae1c5356":"markdown","9ad00fef":"markdown","e594bce4":"markdown","3102b4d3":"markdown","c362875e":"markdown","a06037bd":"markdown","f338e05c":"markdown","7c366137":"markdown","875c387d":"markdown","0ae0aeee":"markdown","3fe4f3bf":"markdown","29d2e820":"markdown","f20870e5":"markdown","b88009b1":"markdown","ec97c27f":"markdown","07e095b8":"markdown","857cf46d":"markdown","18e351ba":"markdown","5e58c816":"markdown","fe5b7b1e":"markdown","65f242f8":"markdown","2885eb6a":"markdown","0b5e207b":"markdown","7a47dcce":"markdown","bbb9040c":"markdown","732b855c":"markdown","32fe7f8a":"markdown","f70627d3":"markdown","027fad82":"markdown","3b06ac57":"markdown","05879434":"markdown","be88e41d":"markdown","ac1a2c53":"markdown","3c15fa12":"markdown","e411e54f":"markdown","c34d54df":"markdown","6e26126a":"markdown","8255fd12":"markdown","7a8af594":"markdown","e89590c3":"markdown","d55cf17f":"markdown","f837aa94":"markdown","2700964b":"markdown","2d925829":"markdown","ce467f09":"markdown","51b88471":"markdown","16cdf335":"markdown","8a585cdc":"markdown","3df247c7":"markdown","4e67aaf3":"markdown","71e25cb1":"markdown","c5b77970":"markdown","b3a6c9b8":"markdown","f1114464":"markdown","96f7d31b":"markdown","ad1f6a90":"markdown"},"source":{"c5f979f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00249c35":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')","a3ab5371":"df = pd.read_csv('..\/input\/airline-passenger-satisfaction\/train.csv')","6748115e":"df.drop(['Unnamed: 0', 'id'], axis=1, inplace=True)","6f98fee6":"df['satisfaction'] = df.satisfaction.apply(lambda x: int(1) if x == 'satisfied' else int(0))","894191cd":"print(df.satisfaction.value_counts()\/len(df.satisfaction))","68fbbd26":"NA_columns = [column for column in df.columns if df[column].isnull().sum() > 0]","8c00b759":"print(df[NA_columns].isnull().sum()\/ len(df[NA_columns]))","e8c7f4ec":"df[NA_columns].hist(bins=100)","0ac61497":"for col in NA_columns:\n    df[col + '_na'] = np.where(df[NA_columns].isnull(), 1, 0)\n    sns.countplot(col + '_na', data=df, hue='satisfaction' )\n    plt.tight_layout()","e664bd88":"nan_data = df[(df['Arrival Delay in Minutes_na'] == 1)]\nsns.catplot('Arrival Delay in Minutes_na', data=nan_data, kind='count', hue='satisfaction' )","9448b251":"df.drop('Arrival Delay in Minutes_na', axis=1, inplace=True)","b38b5e38":"num_columns = [column for column in df.columns if df[column].dtypes != 'O']","b645940a":"num_columns_discrete = [column for column in num_columns if len(df[column].unique()) <= 10 and column not in ['satisfaction']]","26a91cfd":"for col in num_columns_discrete:\n    modus_value = str(df[col].mode()[0])\n    df[col] = df[col].astype(str).apply(lambda x: x.replace('0', modus_value))\n    df[col] = df[col].astype(int)","15ffce59":"for col in num_columns_discrete:\n    plt.figure()\n    sns.countplot(col, data=df, alpha=0.5)\n    sns.countplot(col, data=df, hue='satisfaction')\n    plt.tight_layout()","5a8030d4":"num_columns_continue = [column for column in num_columns if column not in num_columns_discrete and column not in ['satisfaction']] ","5ae381d5":"for col in num_columns_continue:\n    fig, axs = plt.subplots(1,2, figsize=(10,5))\n    sns.violinplot(y=col, data=df, x='satisfaction', ax=axs[0])\n    plt.title(col + ' and satisfaction')\n    sns.distplot(x=df[col], ax=axs[1])\n    plt.title(col)\n    plt.tight_layout()","abc81e90":"cat_columns = [column for column in df.columns if column not in num_columns]","b742d0e0":"for col in cat_columns:\n    plt.figure()\n    sns.countplot(col, data=df, alpha=0.5)\n    sns.countplot(col, data=df, hue='satisfaction')\n    plt.tight_layout()","c996b152":"print(num_columns_discrete)","72612c40":"print(num_columns_continue)","c02dac6e":"print(cat_columns)","2921b495":"for col in cat_columns :\n    plt.figure()\n    sns.countplot(data=df, x=col, hue='Gender')","c628eeb1":"for col in num_columns_discrete:\n    plt.figure()\n    sns.countplot(data=df, x=col, hue='Gender')","8e6e23a8":"for col in num_columns_continue:\n    plt.figure()\n    sns.histplot(data=df, x=col, hue='Gender', multiple='dodge')","a7a433b7":"for col in num_columns_continue:\n    plt.figure()\n    sns.scatterplot(data=df, x=col, y='Age', hue='satisfaction')","dc75f64e":"for col in cat_columns:\n    fig, axs = plt.subplots(1,2, figsize=(10,5))\n    sns.violinplot(data=df, x=col, y='Age', hue='satisfaction', ax=axs[0], split=True)\n    sns.violinplot(data=df, x=col, y='Age', ax=axs[1], palette='Greys')","6d450043":"for col in num_columns_discrete:\n    plt.figure()\n    sns.violinplot(data=df, x=col, y='Age')","f2d606a3":"sns.countplot(data=df, x='Class', hue='Type of Travel')","912941e2":"sns.violinplot(data=df, x='Type of Travel', y='Flight Distance')","a634a878":"sns.violinplot(data=df, x='Class', y='Flight Distance')","1a8dc175":"for col in NA_columns:\n    median_value = df[col].median()\n    df[col] = df[col].fillna(median_value)","091cd5bf":"df.isnull().sum()","31cc58ba":"for col in num_columns_discrete:\n    modus_value = str(df[col].mode()[0])\n    df[col] = df[col].astype(str).apply(lambda x: x.replace('0', modus_value))\n    df[col] = df[col].astype(int)","7ed5cae6":"for col in num_columns_continue:\n    plt.figure()\n    sns.histplot(data=df, x=col)","1f3e430a":"FD_log = np.log(df['Flight Distance'])\nplt.figure()\nsns.histplot(FD_log)","8d0af4f8":"import scipy.stats as stats\nFD_boxcox = stats.boxcox(df['Flight Distance'])\nplt.figure()\nsns.histplot(FD_boxcox)","85564444":"df['Flight Distance'] = FD_log","0d9c01f8":"def encode_category(data, column, target):\n    ordinal_data= data.groupby([column])[target].sum().sort_values(by=column, ascending=False).index\n    ordinal_num = {k: i for i, k in enumerate(ordinal_data, start=0)}\n    data[column] = data[column].map(ordinal_num)\n    print(ordinal_data)","b4f06642":"for col in cat_columns:\n    encode_category(df, col, ['satisfaction'])","d3c430d2":"from sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\ncolumns_training = [column for column in df.columns if column not in ['satisfaction']]\ndf[columns_training] = sc_X.fit_transform(df[columns_training])","d7790aa8":"y = df['satisfaction']\nX = df.drop(['satisfaction'], axis=1)","95c8136f":"sns.heatmap(df[num_columns_continue].corr(), annot=True)\nplt.tight_layout()","c723c682":"from sklearn.feature_selection import f_classif, SelectKBest\nselector_cont = SelectKBest(score_func = f_classif, k=3)\nselector_cont.fit(X[num_columns_continue], y)\npd.DataFrame({'Features':X[num_columns_continue].columns, 'F-Score':selector_cont.scores_, 'p-value':selector_cont.pvalues_})","bb8b1ce1":"cont_select = X[num_columns_continue].columns[selector_cont.get_support()].tolist()","48ea7305":"categorical_columns = [column for column in X.columns if column not in num_columns_continue]","33986779":"from sklearn.feature_selection import chi2\nselector_cat = SelectKBest(score_func= chi2, k=15)\nselector_cat.fit(X[categorical_columns], y)\npd.DataFrame({'Features':X[categorical_columns].columns, 'score':selector_cat.scores_, 'p-value':selector_cat.pvalues_})","28e3ad71":"cat_select = X[categorical_columns].columns[selector_cat.get_support()].tolist()","496673c9":"selected_features = cont_select + cat_select","4f5497c7":"y = df['satisfaction']\nX = df[selected_features]","6037af9e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=41, test_size=0.2)","7ac3e33c":"def model_selection(X_train, y_train, X_test, y_test, models):\n    \n    from sklearn.metrics import accuracy_score, precision_score\n    \n    accuracy_result = []\n    precission_result = []\n    str_models = []\n    \n    for model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        precission = precision_score(y_test, y_pred)\n        accuracy = accuracy_score(y_test, y_pred)       \n        accuracy_result.append(accuracy)\n        precission_result.append(precission)  \n        str_models.append(str(model))\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,10))\n\n    ax1.plot(accuracy_result)\n    ax1.set_ylabel('accuracy_score')\n\n    ax2.plot(str_models,precission_result)\n    ax2.set_ylabel('precission_result')\n    ax2.set_xticklabels(str_models, rotation=90)\n    plt.tight_layout()\n    \n    return pd.DataFrame({'models':models, 'accuracy':accuracy_result, 'precission':precission_result}) ","9925c5eb":"from sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier","9a9e0683":"models = [LogisticRegression(), RidgeClassifier(alpha=0.005), LinearSVC(), SVC(), KNeighborsClassifier(),\n          RadiusNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(),\n          AdaBoostClassifier(), MLPClassifier()]\n \nmodel_selection(X_train, y_train, X_test, y_test, models)","4397be75":"rf = RandomForestClassifier()\nsvc = SVC()","458e9a4a":"def confusion(X_train, y_train, X_test, y_test, model):\n    from sklearn.metrics import confusion_matrix\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    confu_score = confusion_matrix(y_pred, y_test, labels=[1,0])\n    return pd.DataFrame(confu_score, columns=['Actual Postive', 'Actual Negative'], \n                        index=['Predicted Positive', 'Predicted Negative'])","72dc0909":"confusion(X_train, y_train, X_test, y_test, rf)","a626e000":"def ROC_AUC_test(X_train, y_train, X_test, y_test, model):\n    from sklearn.metrics import roc_auc_score, roc_curve\n    model.fit(X_train, y_train)\n    y_pred = model.predict_proba(X_test)\n    y_pred = y_pred[:,1]\n    AUC = roc_auc_score(y_test, y_pred)\n    tpr, fpr, _ = roc_curve(y_test, y_pred)\n    print('AUC: ' + str(AUC))\n    plt.plot(tpr, fpr)\n    plt.title('ROC performance for ' + str(model))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')","0c9125ca":"ROC_AUC_test(X_train, y_train, X_test, y_test, rf)","5ff5654e":"def fit_check(model, kfolds):\n    \n    from sklearn.model_selection import KFold\n    from sklearn.metrics import precision_score\n    \n    kf = KFold(n_splits=kfolds)\n    list_training_error = []\n    list_testing_error = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        model.fit(X_train, y_train)\n        y_train_data_pred = model.predict(X_train)\n        y_test_data_pred = model.predict(X_test)\n        fold_training_error = precision_score(y_train, y_train_data_pred)\n        fold_testing_error = precision_score(y_test, y_test_data_pred)\n        list_training_error.append(fold_training_error)\n        list_testing_error.append(fold_testing_error)\n    \n    figsize=(5,5)\n    plt.plot(range(1, kf.get_n_splits() + 1), np.array(list_training_error).ravel(), 'o-', label = 'training')\n    plt.plot(range(1, kf.get_n_splits() + 1), np.array(list_testing_error).ravel(), 'o-', label = 'testing')\n    plt.xlabel('number of fold')\n    plt.ylabel('Precision')\n    plt.title('Precision across folds')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","3c9da7d9":"fit_check(rf, 5)","be140344":"def model_randomCV(X, y, model, parameters):\n    \n    from sklearn.model_selection import RandomizedSearchCV\n    \n    randCV = RandomizedSearchCV(estimator=model, scoring='precision', param_distributions=parameters, n_jobs=-1, cv=3)\n    \n    randCV.fit(X, y)\n   \n    print('best_parameters: ' + str(randCV.best_params_))\n    print('best_score: ' + str(randCV.best_score_))\n    print('best_estimator: ' + str(randCV.best_estimator_))    \n    \n    return pd.DataFrame(randCV.cv_results_).sort_values(by='rank_test_score')","efc09800":"parameters = {'n_estimators': np.arange(290, 310, 5), 'max_features':['auto', 'sqrt', 'log2'], 'max_depth':np.arange(18,23, 1), \n             'min_samples_split':np.arange(3, 8, 1), 'criterion':['gini', 'entropy']}\n\nmodel_randomCV(X_train, y_train, rf, parameters)","8ca071b6":"rf_best = RandomForestClassifier(criterion='entropy', max_depth=19, max_features='sqrt',\n                       min_samples_split=3, n_estimators=295)","579bb8c8":"confusion(X_train, y_train, X_test, y_test, rf_best)","a65d1d61":"ROC_AUC_test(X_train, y_train, X_test, y_test, rf_best)","5939e98e":"fit_check(rf_best, 5)","7406bbfb":"confusion(X_train, y_train, X_test, y_test, svc)","3513e027":"svc_prob_true = SVC(probability=True)\n\nROC_AUC_test(X_train, y_train, X_test, y_test, svc_prob_true)","b1556ba0":"fit_check(svc, 5)","89ccd5cf":"svc_hyp = SVC(C=10)\n\nfit_check(svc_hyp, 5)","a902b0e0":"svc_hyp_1 = SVC(gamma=1)\n\nfit_check(svc_hyp_1, 5)","b61a9c13":"def model_conclusion(X_train, y_train, X_test, y_test, models):\n    \n    from sklearn.metrics import precision_score\n    \n    train_result = []\n    test_result = []\n    str_models = []\n    \n    for model in models:\n        model.fit(X_train, y_train)\n        y_pred_train = model.predict(X_train)\n        y_pred = model.predict(X_test)\n        precision_train = precision_score(y_train, y_pred_train)\n        precision_test = precision_score(y_test, y_pred)       \n        train_result.append(precision_train)\n        test_result.append(precision_test)  \n        str_models.append(str(model))\n    \n    figsize=(10,10)\n\n    sns.lineplot(str_models, train_result, label='train')\n    sns.lineplot(str_models, test_result, label='test')\n    plt.ylabel('Precision')\n    plt.title('Precision for models')\n    plt.xticks(str_models, rotation=90)\n    plt.legend()\n    plt.tight_layout()\n    \n    return pd.DataFrame({'models':models, 'precision_train':train_result, 'precission_test':test_result}) ","828c8844":"rf = RandomForestClassifier()\nsvc = SVC()\nrf_best = RandomForestClassifier(criterion='entropy', max_depth=19, max_features='sqrt', min_samples_split=3, n_estimators=295)\nsvc_hype = SVC(C=10)\nsvc_hype_1 = SVC(gamma=1)\nmodels = [rf, svc, rf_best, svc_hype, svc_hype_1]\n\nmodel_conclusion(X_train, y_train, X_test, y_test, models)","17702bf0":"transform data 0 (Not Applicable) to mode in each columns","a6b1cab9":"## Check the correlation between columns in the data","0dff1aee":"### Categorical columns","c4ac15a5":"### SVC with parameters","d5043267":"#### For the gender and customer type, the data shows that they are less important for the overall satisfaction. The passenger of business class and type shows that they are tend to be satisfied. and the eco class shows the worst percentage of the passenger to not to be satisfied with the airline.","0af17abf":"### Continuous Numeric Columns","3fdecbef":"### Confusion Matrix ","144b7882":"#### Business class are dominant by the business type of travel with long range filght distance","13592f97":"### Discrete Numeric Columns","edbf8c03":"cause the 'Departure Delay in minutes' is the smallest F-score, so I drop it.","76afdbf7":"The Flight Distance need to transform to get the more normally distributed. here Iam using two type of transform, and compare the transform result","17a643cd":"## Drop absolute unnecessary columns","e7197cf7":"## Encode string categorical column into numeric","8240e4ba":"see the effect of the nan with the satisfaction and compare it to non nan with satisfaction. 1 for nan row data and 0 for non nan","1559706d":"### Random Forest after Hyperparameter Tuning","d2e2a648":"## Random Forest","1f527167":"## Transform not normally distributed data to normally distributed","3a5b38bf":"#### so in summary inflight wifi service, online booking and gate location need big improvement for better overall service performance.","c0a41085":"#### yeah its the same result, no correlation","397c088f":"#### its also the same for the numerical discrete columns, which is the gender is the less important factor ","458f8bb1":"# Exploratory Data Analysis","76bc0b0a":"#### The fitting of Random Forest after tuning shows that the overfit is slightly decrease, but it's not enough to dispute the overfitting indication in default Random Forest ","0403d86e":"#### Age < 40 and age > 60 tend to not satisfied, while age range for 40 - 60 tend to be satisifed with the airlines. In short range flight distance the passenger tend to be not satisfied while at the more long range tend to be satisfied. In arrival and departure delay, the shorter delay tend to be satisfied the passenge.","ae1c5356":"## Read the file","9ad00fef":"## Not Available data analysis","e594bce4":"# Feature Selection","3102b4d3":"from the result log transfrom give more normally distributed, so I choose log transform method","c362875e":"#### The best random forest model got slightly better in precision score, which is 0.968","a06037bd":"the num_columns _discrete is appropriate according to the dataset as service satisfaction value in each columns","f338e05c":"### Correlation with the gender","7c366137":"## Support Vector Classifier","875c387d":"### Continuous Columns","0ae0aeee":"#### Food and drink, baggage handling, checkin service and inflight service just need little improvement cause they already have the good trends","3fe4f3bf":"### Hyperparameter using RandomizedsearchCV","29d2e820":"#### from the model conclusion, I choose SVC model with gamma parameter to prevent overfitting with better precision.","f20870e5":"#### the data are balance enough, so the data doesn't need to be resampling","b88009b1":"## Filter Methods","ec97c27f":"### Another Table correlation information","07e095b8":"#### the distribution very right skewed cause the modus of this columns is 0","857cf46d":"## Categorical columns analysis","18e351ba":"from the score, 'Gender', 'Departure\/Arrival time convenient', and 'gate location' are the less important feature and have big score difference with another features","5e58c816":"check the NA after the fillna","fe5b7b1e":"## Fix the NAN and Not Applicable data","65f242f8":"'Departure Delay in Minutes' and 'Arrival Delay in Minutes' are too corelated for the correlation between independent features, so one of the features need to be drop","2885eb6a":"#### and big question mark for the departure\/arrival time convenient, why they have good trends but end up in dissatisfaction.","0b5e207b":"### Overfit and underfit check","7a47dcce":"## Model Comparassion","bbb9040c":"the percentage of the NA data","732b855c":"#### the gender seem balance, therefore gender is less essential for the another categorical columns","32fe7f8a":"All selected features","f70627d3":"#### inflight wifi service most of the data are in 2 and 3 point and end up a lot dissatisfied, it happens also in ease of online booking, gate location . departure\/arrival time are in good trends but not lead to the satisfaction of the airline. Food and drink seems neutral and not impresive . Online boarding, seat comfort, inflight entertaintment, on-board-service, leg room service, cleanliness are in the good shape with the most of the data in point 4 and the trends lead to the satisfaction of the airline. Baggage handling, checkin service and inflight service are slightly like departure\/arrival time but still got a lot satisfaction in point 5","027fad82":"transform the Not Applicable (0) with the modus in columns services (num_columns_discrete)","3b06ac57":"# Feature Engineering","05879434":"## Define the X, Y and split it into Training and Test dataset","be88e41d":"### Model Conclusion","ac1a2c53":"#### there is no correlation here","3c15fa12":"#### online boarding, seat comfort, inflight entertainment, on-board service, leg room service are dominant to provide the good service for older people, age 40 - 60. and Baggage handling tend to be bad service for the older people","e411e54f":"Cause SVC model hyperparameter tuning take a longer time to train the model especially with gamma and kernel poly parameters (I already treid it in my Jupyter Notebook), so here Iam simply test model fitting with the parameter that I already have. ","c34d54df":"https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/","6e26126a":"#### the plot showed that the nan value is sligthly affect the satisfaction, or even not affecting at all cause the satisfaction percentage are the same in non nan and nan.","8255fd12":"#### The overfit and underfit check showed that the model have an overfit indication cause the precision in training have better performance than precision in test set","7a8af594":"## Conclussion for Exploratory Data Analysis","e89590c3":"#### Gender for the age are completely the same, less important. the passenger with age > 40 tend to be loyal and satisfied with the airline, while passenger with the younger age tend to be disloyal but satisfied with the airline. again the passenger with age over 40 dominance the business travel type of travel and tend to be satisfied with the airline. eco plus and eco dominance by the younger passenger, < 20, with tend to be not satisfied, while bussiness class are dominance by the older passenger, with age over 40 years and tend to be satisfied with the airline. ","d55cf17f":"#### The Random Forest have better performance based on the confusion matrix, its showed by the the smaller false positive","f837aa94":"### Correlation with the age","2700964b":"## Scale the dataset","2d925829":"## ROC_AUC","ce467f09":"## Best Classsifier Model","51b88471":"transform the satisfaction string category to numerical category for the imbalance check","16cdf335":"# Machine Learning Modeling","8a585cdc":"#### The SVC's performance(precision and ROC_AUC) is no better than Random Forest, but the fitiing perform better than Random Forest","3df247c7":"## Numeric Columns Analysis","4e67aaf3":"#### The Airlines passenger satisfaction are dominanth by the older people within range of the age between 40 - 60 for the business travel using business class wihtin long flying range distance, which supported and reinforced by good services score for older people like seat comfort, on-board service and leg room.","71e25cb1":"the encode is ordered by the sum of satisfaction within one categoy in each columns","c5b77970":"in the numeric column, there are two types, discrete and continuous. here I Extract the discrete numeric columns with threshold unique data < 10","b3a6c9b8":"for categorical columns I use chi squared function.","f1114464":"cause the nan percentage s very small, so I plot the nan here to see the effect","96f7d31b":"#### Model selection with metrics, the metrics that I use is accuracy and precission in model selection. Precission is choose cause I want to minimize the False Positive (not satisfied data classify as satisfied) to get the worst case scenario, so the Airline can do maximum improve in the service","ad1f6a90":"## Imbalance check"}}