{"cell_type":{"05f173b4":"code","8dc0fb0b":"code","d8251c20":"code","eb3ce0da":"code","2a8d3520":"code","6cae606a":"code","10f6ed12":"code","160b5d96":"code","7f39efac":"code","af184445":"code","6cd1e439":"code","4ea9a5d2":"code","4494d08b":"code","95ec3454":"code","9718d495":"code","6baa154c":"code","8a2ca06e":"code","d4c53125":"code","c52e8e65":"code","83b84af5":"code","734f4651":"code","6830a2c9":"code","2b680037":"code","05b37598":"code","815b0bbd":"code","7300d5cc":"code","0305f11d":"code","812c2b23":"code","dffe2407":"code","81ead161":"code","ce5049d3":"code","29da6910":"code","d29ada57":"code","22a01518":"code","845a2e0e":"code","67ec7b9d":"code","125f9480":"code","7d7d8d8f":"code","63dfd0c1":"code","dd3ccdf1":"code","43d9619c":"code","eaf6ccb8":"code","0c3238cd":"code","d5a4ede8":"code","0d96915e":"code","4ee5d484":"code","cf83b10f":"code","e509abc1":"code","61318eb1":"code","4e0f95af":"code","537f6b52":"code","f28c6fa9":"code","51edd19a":"code","0e6e9e30":"code","4168d5b9":"code","a7d62b06":"code","46ed0c0c":"code","1a04cd5a":"code","32a443ad":"code","e5c5b455":"markdown","2b43aaad":"markdown","5f33b9ae":"markdown","bbe23d46":"markdown","1b09111d":"markdown","f6185711":"markdown","0efb1fb3":"markdown","2e2a0ecc":"markdown","7be74f49":"markdown","d8a7d95b":"markdown","8ef53815":"markdown","b97f6a5c":"markdown","5363f529":"markdown","271230e4":"markdown"},"source":{"05f173b4":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","8dc0fb0b":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score","d8251c20":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","eb3ce0da":"#merge the data for feature engineering and later split it, just before applying Data Pipeline\nTrainFile = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nTestFile = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nDataFile = TrainFile.append(TestFile)","2a8d3520":"TrainFile.shape","6cae606a":"TestFile.shape","10f6ed12":"DataFile.describe()","160b5d96":"DataFile.info()","7f39efac":"# First Split the names to gt Mr. or Miss or Mrs.\n\nFirstName = DataFile[\"Name\"].str.split(\"[,.]\")","af184445":"# now strip the white spaces from the Salutation\ntitles = [str.strip(name[1]) for name in FirstName.values]","6cd1e439":"DataFile[\"Title\"] = titles","4ea9a5d2":"#drop the columns - that may not impact the analysis\nDataFile = DataFile.drop('Name',axis=1)\nDataFile = DataFile.drop('PassengerId',axis=1)\nDataFile = DataFile.drop('Embarked',axis=1)","4494d08b":"# In version 1, we kept Ticket - This time we drop it.\n\nDataFile = DataFile.drop('Ticket',axis=1)","95ec3454":"# In version 1, we kept cabin - This time we drop it.\n\nDataFile = DataFile.drop('Cabin',axis=1)","9718d495":"# Now first we replace the extra titles to Mr and Mrs\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'the Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\nDataFile.replace({'Title': mapping}, inplace=True)","6baa154c":"# get the imputed value for FARE\nDataFile['Fare'].fillna(DataFile['Fare'].median(), inplace=True)\n\n#impute the age based on Titles \ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    imputed_age = DataFile.groupby('Title')['Age'].median()[titles.index(title)]\n    DataFile.loc[(DataFile['Age'].isnull()) & (DataFile['Title'] == title), 'Age'] = imputed_age","8a2ca06e":"# Merge SibSp and Parch into one\nDataFile[\"Family Size\"] = DataFile[\"SibSp\"] + DataFile[\"Parch\"]\n\n#drop SibSp and Parch\nDataFile = DataFile.drop('SibSp',axis=1)\nDataFile = DataFile.drop('Parch',axis=1)","d4c53125":"#Making Fare bins\n\nDataFile['FareBin'] = pd.qcut(DataFile['Fare'], 5)\n\nlabel = LabelEncoder()\nDataFile['FareBin'] = label.fit_transform(DataFile['FareBin'])\nDataFile = DataFile.drop('Fare',axis=1)\n\n#Making Age Bins\nDataFile['AgeBin'] = pd.qcut(DataFile['Age'], 4)\n\nlabel = LabelEncoder()\nDataFile['AgeBin'] = label.fit_transform(DataFile['AgeBin'])\nDataFile = DataFile.drop('Age',axis=1)","c52e8e65":"#create a dummy for male and female\nDataFile['Sex'].replace(['male','female'],[0,1],inplace=True)","83b84af5":"#Now split Back The data to training and test set - before applying the pipeline\n\ntrain_set, test_set = train_test_split(DataFile, test_size=0.3193,shuffle=False)","734f4651":"train_set.shape # This exactly matches the original training set","6830a2c9":"test_set.shape # This exactly matches the original test set","2b680037":"#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n\nobs = train_set.isnull().sum().sort_values(ascending = False)\npercent = round(train_set.isnull().sum().sort_values(ascending = False)\/len(train_set)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","05b37598":"#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n\nobs = test_set.isnull().sum().sort_values(ascending = False)\npercent = round(test_set.isnull().sum().sort_values(ascending = False)\/len(test_set)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","815b0bbd":"# Now define x and y.\n\n#the Y Variable\ntrain_set_y = train_set[\"Survived\"].copy()\ntest_set_y = test_set[\"Survived\"].copy()\n\n#the X variables\ntrain_set_X = train_set.drop(\"Survived\", axis=1)\ntest_set_X = test_set.drop(\"Survived\", axis=1)","7300d5cc":"# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","0305f11d":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","812c2b23":"cat_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector([\"Title\"])),\n        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n    ])\n\nnum_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector([\"Pclass\",\"Family Size\",\"FareBin\", \"AgeBin\"])),\n        ('std_scaler', StandardScaler()),\n      ])\n\nno_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector([\"Sex\"]))\n    ])","dffe2407":"full_pipeline = FeatureUnion(transformer_list=[\n    (\"cat_pipeline\", cat_pipeline),\n    (\"num_pipeline\", num_pipeline),\n    (\"no_pipeline\", no_pipeline),\n    ])\n\nfinal_train_X = full_pipeline.fit_transform(train_set_X)\nfinal_test_X = full_pipeline.transform(test_set_X)","81ead161":"#Introduce KNN Classifier \n\nKNeighbours = KNeighborsClassifier()\nleaf_size = list(range(1,40,5))\nn_neighbors = list(range(4,15,2))\n\nparam_grid_KNeighbours = {'n_neighbors' : n_neighbors,\n'algorithm' : ['auto'],\n'weights' : ['uniform', 'distance'],\n'leaf_size':leaf_size }\n\ngrid_search_KNeighbours = GridSearchCV(KNeighbours, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\ngrid_search_KNeighbours.fit(final_train_X, train_set_y)","ce5049d3":"neighbor_grid = grid_search_KNeighbours.best_estimator_\n\ny_pred_neighbor_grid = neighbor_grid.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_neighbor_grid)","29da6910":"KNeighbours2 = KNeighborsClassifier()\nleaf_size2 = list(range(18,50,1))\nn_neighbors2 = list(range(15,20,1))\n\nparam_grid_KNeighbours = {'n_neighbors' : n_neighbors2,\n'algorithm' : ['auto'],\n'weights' : ['uniform', 'distance'],\n'leaf_size':leaf_size2}\n\ngrid_search_KNeighbours2 = GridSearchCV(KNeighbours2, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\ngrid_search_KNeighbours2.fit(final_train_X, train_set_y)","d29ada57":"neighbor_grid2 = grid_search_KNeighbours2.best_estimator_\n\ny_pred_neighbor_grid2 = neighbor_grid2.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_neighbor_grid2)","22a01518":"forest_class = RandomForestClassifier(random_state = 42)\n\nn_estimators = [10, 50]\nmax_features = [0.1, 0.5]\nmax_depth = [2, 10, 20] \noob_score = [True, False]\nmin_samples_split = [0.1, 0.5]\nmin_samples_leaf = [0.1, 0.5] \nmax_leaf_nodes = [2, 10, 50]\n\nparam_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n                     'max_leaf_nodes' : max_leaf_nodes}\n\n\nrand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n                                 n_jobs = -1, verbose=2)\n\nrand_search_forest.fit(final_train_X, train_set_y)","845a2e0e":"random_estimator = rand_search_forest.best_estimator_\n\ny_pred_random_estimator = random_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_random_estimator)","67ec7b9d":"ada_boost = AdaBoostClassifier(random_state = 42)\n\nn_estimators = [3, 20, 50, 70, 90]\nlearning_rate = [0.1, 0.5, 0.9]\nalgorithm = ['SAMME', 'SAMME.R']\n\nparam_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n\nrand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_ada.fit(final_train_X, train_set_y)","125f9480":"ada_estimator = rand_search_ada.best_estimator_\n\ny_pred_ada_estimator = ada_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_ada_estimator)","7d7d8d8f":"extra_classifier = ExtraTreesClassifier(random_state = 42)\n\nn_estimators = [3, 40, 60, 80]\nmax_features = [0.1, 0.5]\nmax_depth = [2, 50, 100]\nmin_samples_split = [0.1, 0.5]\nmin_samples_leaf = [0.1, 0.5] # Mhm, this one leads to accuracy of test and train sets being the same.\n\nparam_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n                         'min_samples_leaf' : min_samples_leaf}\n\n\nrand_search_extra_trees = RandomizedSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_extra_trees.fit(final_train_X, train_set_y)","63dfd0c1":"extra_estimator = rand_search_extra_trees.best_estimator_\n\ny_pred_extra_estimator = extra_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_extra_estimator)","dd3ccdf1":"SVC_Classifier = SVC(random_state = 42)\n\nparam_distributions = {\"gamma\": reciprocal(0.0001, 1), \"C\": uniform(100000, 1000000)}\n\nrand_search_svc = RandomizedSearchCV(SVC_Classifier, param_distributions, n_iter=10, verbose=2, n_jobs = -1)\n\nrand_search_svc.fit(final_train_X, train_set_y)","43d9619c":"svc_estimator = rand_search_svc.best_estimator_\n\ny_pred_svc_estimator = svc_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_svc_estimator)","eaf6ccb8":"GB_Classifier = GradientBoostingClassifier(random_state = 42)\n\nn_estimators = [3, 100]\nlearning_rate = [0.1, 0.5]\nmax_depth = [3, 50, 70]\nmin_samples_split = [0.1, 0.5]\nmin_samples_leaf = [0.1, 0.5]\nmax_features = [0.1, 0.5]\nmax_leaf_nodes = [2, 50, 70]\n                            \nparam_grid_grad_boost_class = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n                              'min_samples_leaf' : min_samples_leaf, 'max_features' : max_features,\n                              'max_leaf_nodes' : max_leaf_nodes}\n\nrand_search_grad_boost_class = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost_class, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_grad_boost_class.fit(final_train_X, train_set_y)","0c3238cd":"gb_estimator = rand_search_grad_boost_class.best_estimator_\n\ny_pred_gb_estimator = gb_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_gb_estimator)","d5a4ede8":"log_reg = LogisticRegression(random_state = 42)\n\nC = np.array(list(range(1, 100)))\/10\n                            \nparam_grid_log_reg = {'C' : C}\n\nrand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_log_reg.fit(final_train_X, train_set_y)","0d96915e":"log_estimator = rand_search_log_reg.best_estimator_\n\ny_pred_log_estimator = log_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_log_estimator)","4ee5d484":"mlp_clf = MLPClassifier(random_state = 42)\n\nalpha = [.0001,.001,.01,1]\nlearning_rate_init= [.0001,.001,.01,1]\nmax_iter = [50,70,100,200]\ntol = [.0001,.001,.01,1]\n\nparam_grid_mlp_clf = {'alpha':alpha, 'learning_rate_init':learning_rate_init, 'max_iter':max_iter,'tol':tol}\n\nrand_search_mlp_clf = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_mlp_clf.fit(final_train_X, train_set_y)\n","cf83b10f":"mlp_estimator = rand_search_mlp_clf.best_estimator_\n\ny_pred_mlp_estimator = mlp_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_mlp_estimator)","e509abc1":"voting_clf = VotingClassifier(\n    estimators=[('lr', log_estimator), ('ada',ada_estimator), ('gb', gb_estimator), ('knn', neighbor_grid),\n                ('svc', svc_estimator), ('mlp', mlp_estimator)],\n    voting='hard')\nvoting_clf.fit(final_train_X, train_set_y)","61318eb1":"#Predict the y_pred to get accuracy score.\ny_pred = voting_clf.predict(final_train_X)\naccuracy_score(train_set_y, y_pred)","4e0f95af":"total_estimators = [\n    (\"log_reg_clf\", log_estimator),\n    (\"mlp_clf\", mlp_estimator),\n    (\"knn_clf\", neighbor_grid),\n    ('svc_clf', svc_estimator)\n]","537f6b52":"voting_clf = VotingClassifier(total_estimators)","f28c6fa9":"voting_clf.fit(final_train_X, train_set_y)","51edd19a":"#Predict the y_pred to get accuracy score.\ny_pred_voting2 = voting_clf.predict(final_train_X)\naccuracy_score(train_set_y, y_pred)","0e6e9e30":"# now get the predictions\ny_pred_svc_rand = svc_estimator.predict(final_test_X)\n\n#predict using k neighbors 3\ny_pred_knn_grid = neighbor_grid.predict(final_test_X)\n\n#predict using voting\ny_pred_voting = voting_clf.predict(final_test_X)\n\n#predict using voting 2nd version.\ny_pred_voting2 = voting_clf.predict(final_test_X)","4168d5b9":"#Create the datafile for SVC\nresult_test1 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test1[\"PassengerId\"] = passenger_id_test\nresult_test1[\"Survived\"] = y_pred_svc_rand","a7d62b06":"#Create the datafile for voting classifier\nresult_test3 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test3[\"PassengerId\"] = passenger_id_test\nresult_test3[\"Survived\"] = y_pred_voting","46ed0c0c":"#Create the datafile for voting classifier\nresult_test4 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test4[\"PassengerId\"] = passenger_id_test\nresult_test4[\"Survived\"] = y_pred_voting2","1a04cd5a":"#Create the datafile for KNN 3\nresult_test2 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test2[\"PassengerId\"] = passenger_id_test\nresult_test2[\"Survived\"] = y_pred_knn_grid","32a443ad":"result_test1.to_csv(\"Titanic_prediction.csv\")","e5c5b455":"##### MLP Classifier - Nueral Networks","2b43aaad":"SVC looks like a clear winner with 86% accuracy on training set. But Kaggle predicts Voting Classifier as the best classification algorithm","5f33b9ae":"#### Now We Build the Models","bbe23d46":"#### Logistic Classifier","1b09111d":"###### Extra Trees Classifier","f6185711":"##### Here Starts the Data Pipeline","0efb1fb3":"##### Random Forest Classifier","2e2a0ecc":"#### Gradient Boosting Classifier","7be74f49":"#### KNN Classifier","d8a7d95b":"#### Support Vector Classifier","8ef53815":"###### Ada Boost Classifier","b97f6a5c":"#### Enhanced Voting Classifier - Remove ADA and Grad Boost Classifier - This doesn't improve performance on the test data","5363f529":"##### Voting Classifier - Ensemble the models.","271230e4":"#### Another KNN Approach"}}