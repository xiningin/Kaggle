{"cell_type":{"6b34285f":"code","53db3fa9":"code","7c057a5d":"code","17416fcd":"code","122a0fb1":"code","901e4eef":"code","0dd2bd60":"code","3e84f044":"code","a57ba8ff":"code","3cb8d7fd":"code","f3fb4dfc":"code","5941ad0a":"code","0c2fb421":"code","73b64533":"code","a20bfcb5":"code","70b28ccf":"code","0cf1e8a5":"code","10b48c27":"code","c386ee79":"code","4d495feb":"code","62570e99":"code","2ba35b2c":"code","eb2c40e9":"code","071b8765":"code","8797753b":"code","c8443f50":"code","f9917447":"code","275342eb":"code","07e8b7d8":"code","68caab16":"code","5f8500ae":"code","4a92a52e":"code","6a9053c0":"code","4e88f4f7":"code","6ce26f3c":"code","15004791":"code","b4b26dc3":"code","eccc5836":"code","d10a5084":"code","af4fb4da":"markdown","bce54932":"markdown","64c3b5ed":"markdown","c83e3337":"markdown","070133e1":"markdown","ad2d1b37":"markdown","94187977":"markdown","f65aa5fd":"markdown","91f8521d":"markdown","1ff31939":"markdown","b0d8b0f6":"markdown","396377b1":"markdown","19813370":"markdown","a898b7f5":"markdown","f3bc6bd1":"markdown","e4a50fd1":"markdown","76d8fd7a":"markdown","0e528176":"markdown","1326a600":"markdown"},"source":{"6b34285f":"SEED = 42\n# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\n%matplotlib inline\n\n\n# Importing Preprocessing Library\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n# Importing Model Selection Library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#Model\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Accuracy Metrics\nfrom sklearn.metrics import r2_score\n\nplt.rcParams['xtick.labelsize']=11\nplt.rcParams['ytick.labelsize']=11\n\n\nimport missingno as miss\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.preprocessing import StandardScaler","53db3fa9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c057a5d":"#Importing datasets\ntrain_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ndataset_union = [train_df,  test_df]\n\ntest_df_copy = test_df\n\nprint('The size of training dataset is {}'.format(train_df.shape))\nprint('The size of test dataset is {}'.format(test_df.shape))","17416fcd":"# Visualizing the Matrix Plot\n\nfig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(1,1,1)\nmiss.matrix(train_df, labels=True, fontsize=9, ax=ax1, sparkline=False)\nax1.set_title('Matrix plot for missing values in TRAIN dataset', size=20, color='red')\n\nfig = plt.figure(figsize=(30,10))\nax2 = fig.add_subplot(1,1,1)\nmiss.matrix(test_df, labels=True, fontsize=9, ax=ax2)\nax2.set_title('Matrix plot for missing values in TEST dataset', size=20, color='red')\n\n","122a0fb1":"# Dropping ID column from the train dataset\ntrain_df = train_df.drop('Id', axis=1)\ntest_df = test_df.drop('Id', axis=1)\n\nnumerical_var = [var for var in train_df.columns if train_df[var].dtype != 'O']\nprint('Total number of the Numerical features in the train dataset is {}'.format(len(numerical_var)))","901e4eef":"# Histogram plot of the Numerical features\nplt.figure(figsize=(30,30))\nfor key, value in enumerate(numerical_var):\n    plt.subplot(8,5, key+1)\n    x = train_df.loc[:, value]\n    plt.hist(x, color='#50808E',  edgecolor='black')\n    plt.xlabel(value,size=11, color='red')\n    plt.ylabel('frequency', size=11, color='red')\n    plt.suptitle('Histogram plot of the Numerical features in Train Dataset', size=20, color='black')\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\nplt.show()","0dd2bd60":"# Exploring top Numerical features in the dataset\n\ndef top_numeric_corr (dataset, topN=5):\n    numerical_df = dataset.select_dtypes(include=[np.number])\n    numerical_df_corr = numerical_df.corr()['SalePrice'].sort_values(ascending=False)[1:topN]\n    print('Top Numerical features positively corelated with the Sale Price: \\n {}'.format(numerical_df_corr))\n\n\ntop_numeric_corr(train_df)    ","3e84f044":"# Scatter plot of the Numerical Features\n\nplt.figure(figsize=(30,30))\nfor key, value in enumerate(numerical_var):\n  plt.subplot(8,5, key+1)\n  x = train_df.loc[: , value]\n  plt.scatter(x, y=train_df['SalePrice'], color='#50808E', alpha=0.3)\n  plt.xlabel(value, size=11, color='red')\n  plt.suptitle('Scatter Plots of Numerical Features', size=20, color='red')\n  plt.tight_layout()\n  plt.subplots_adjust(top=0.95)\nplt.show()","a57ba8ff":"def missing_val_calculate(dataset, Dtype):\n    numerical_df =dataset.select_dtypes(include=[np.number])\n    for cols in numerical_df:\n        if dataset[cols].isnull().sum()>0:\n            print('The {} column in {} dataset has {} missing values'.format(cols,Dtype, dataset[cols].isnull().sum()))\n            \n#Missing values for training dataset\nmissing_val_calculate(train_df, 'TRAIN')   ","3cb8d7fd":"#Missing values in test dataset\nmissing_val_calculate(test_df, 'TEST') ","f3fb4dfc":"def missing_value_imputer(dataset, Dtype):\n    numerical_df =dataset.select_dtypes(include=[np.number])\n    for cols in numerical_df.columns:  \n         if dataset[cols].isnull().mean():\n                most_frequent_Nval = dataset[cols].mode()[0]\n                dataset[cols].fillna(most_frequent_Nval, inplace=True)\n    for cols in numerical_df.columns:\n        flag=0\n        if dataset[cols].isnull().sum() !=0:\n            flag = flag+1\n    if flag==0:\n        print('All missing values from the {} dataset have been imputed by mode value'.format(Dtype))\n        \nmissing_value_imputer(train_df, 'TRAIN')","5941ad0a":"missing_value_imputer(test_df, 'TEST')","0c2fb421":"def remove_outliers(dataset, factor=1.5):\n    \n    ''' A function to remove outliers of top five positively corelated with the SalePrice from the Dataset'''\n    \n    numerical_features = dataset.select_dtypes(include=[np.number])\n    numerical_features_corr =numerical_features.corr()['SalePrice'].sort_values(ascending=False)[1:6]\n    for feature in numerical_features_corr.index:\n        Q1 = dataset[feature].quantile(0.25)\n        Q3 = dataset[feature].quantile(0.75)\n        IQR = (Q3-Q1)\n        dataset = dataset[(dataset[feature]>= Q1-factor*IQR) & (dataset[feature]<=Q3+factor*IQR)]\n    return dataset\n\n\ntrain_df = remove_outliers(train_df)\n\nprint('New Shape of the Train dataset after outlier removal is {}'.format(train_df.shape))","73b64533":"# Regularization of the Numerical features\n\nfrom sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\n\ntrain_copy = train_df.copy()\nnum_train_df = train_df.select_dtypes(include=[np.number])\nnum_train_df = num_train_df.drop('SalePrice', axis=1)\nfor cols in num_train_df.columns:\n    transformer_train = standard_scaler.fit(train_df[[cols]])\n    train_df[cols] = transformer_train.transform(train_df[[cols]])\n\n\nprint('Train dataset shape after regularization of the Numerical Features {}'.format(train_df.shape))","a20bfcb5":"train_df.head(2)","70b28ccf":"# Regularization for TEST numerical dataset\n\nnum_test_df = train_copy.select_dtypes(include=[np.number])\nnum_test_df = num_test_df.drop('SalePrice', axis=1)\nfor cols in num_test_df.columns:\n    transformer_test = standard_scaler.fit(train_copy[[cols]])\n    test_df[cols] = transformer_test.transform(test_df[[cols]])\n\nprint('Test dataset shape after regularization {}'.format(test_df.shape))\n    ","0cf1e8a5":"test_df.head(2)","10b48c27":"categorical_df = train_df.select_dtypes(exclude=[np.number])\nfor cols in categorical_df.columns:\n    print('Uniques values in {} category are: {}, total {} counts'.format(cols, train_df[cols].unique(), len(train_df[cols].unique())))","c386ee79":"def missing_val_calculate(dataset, Dtype):\n    categorical_df =dataset.select_dtypes(exclude=[np.number])\n    for cols in categorical_df:\n        if dataset[cols].isnull().sum()>0:\n            print('The {} column in {} dataset has {:.2f}% missing values, total values{}'.format(cols, Dtype, (dataset[cols].isnull().sum()\/dataset.shape[0])*100, dataset.shape[0]))\n\nmissing_val_calculate(train_df, 'TRAIN')   #Missing values for training dataset","4d495feb":"missing_val_calculate(test_df, 'TEST')","62570e99":"drop_cols = ['Alley', 'PoolQC', 'Fence', 'MiscFeature']\ntrain_df = train_df.drop(drop_cols, axis=1)\ntest_df = test_df.drop(drop_cols, axis=1)\n\nprint('New Shape of the Training dataset is {}'.format(train_df.shape))\nprint('New Shape of the Testing dataset is {}'.format(test_df.shape))","2ba35b2c":"def missing_value_imputer(dataset, Dtype):\n    categorical_df =dataset.select_dtypes(exclude=[np.number])\n    for cols in categorical_df.columns:  \n         if dataset[cols].isnull().mean():\n                most_frequent_Cval = dataset[cols].mode()[0]\n                dataset[cols].fillna(most_frequent_Cval, inplace=True)\n    for cols in categorical_df.columns:\n        flag=0\n        if dataset[cols].isnull().sum() !=0:\n            flag = flag+1\n    if flag==0:\n        print('All missing values from the {} dataset have been imputed'.format(Dtype))\n        \nmissing_value_imputer(train_df, 'TRAIN')","eb2c40e9":"missing_value_imputer(test_df, 'TEST')","071b8765":"categorical_var= [var for var in  train_df.columns if train_df[var].dtype=='O']\nplt.figure(figsize=(30,50))\nfor key, value in enumerate(categorical_var):\n    plt.subplot(8,5, key+1)\n    sns.boxplot(x=value, y='SalePrice', data=train_df)\n    plt.xticks(rotation=90)\n    plt.xlabel(value, color='red', size=10)\n    plt.ylabel('Sales Price', color='red', size=10)\n    plt.suptitle('Box Plot of all Categorical Features', size=20, color='Black')\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\nplt.show()","8797753b":"from sklearn.preprocessing import LabelEncoder\n\n#making an encoder object\nlabel_encoder = LabelEncoder()\n\n#fit and transform all categorical variables\ncat_cols = [var for var in train_df.columns if train_df[var].dtype == 'O']\nfor cols in cat_cols:\n    label_encoder.fit(pd.concat([train_df[cols], test_df[cols]], axis=0, sort=False))\n    train_df[cols] = label_encoder.transform(train_df[[cols]])\n    test_df [cols] = label_encoder.transform(test_df[[cols]])\n    \nprint('Label Encoding completed......')","c8443f50":"train_df.head()","f9917447":"test_df.head()","275342eb":"fig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(1,1,1)\nmiss.matrix(train_df, labels=True, fontsize=9, ax=ax1)\nax1.set_title('Matrix plot for missing values in TRAIN dataset after PREPORCESSING', size=20, color='red')\n\nfig = plt.figure(figsize=(30,10))\nax2 = fig.add_subplot(1,1,1)\nmiss.matrix(test_df, labels=True, fontsize=9, ax=ax2)\nax2.set_title('Matrix plot for missing values in TEST dataset after PREPROCESSING ', size=20, color='red')\n","07e8b7d8":"print('The Skewness of the Target Variable is: {:.3f}\\n'.format(train_df['SalePrice'].skew()))\n\n\nmatplotlib.rcParams['figure.figsize'] = (20.0, 6.0)\nprices = pd.DataFrame({'price': train_df['SalePrice'], 'log(price+1)':np.log1p(train_df['SalePrice'])})\nprices.hist()","68caab16":"y = np.log1p(train_df['SalePrice'])\nprint('Skewness of the Target feature after log transformation: {:.3f}'.format(y.skew()))","5f8500ae":"y = np.log1p(train_df['SalePrice'])\nX = train_df.drop('SalePrice', axis=1)\n\nprint('Size of independent features: {}'.format(X.shape))\nprint('Size of dependent feature is {}'.format(y.shape))","4a92a52e":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n\nprint('X_train shape: {}'.format(X_train.shape))\n\nprint('X_test shape: {}'.format(X_val.shape))","6a9053c0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Linear Regression object\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n\nlr_scores = cross_val_score(lr, X_val, y_val, scoring='neg_mean_squared_error', cv=10)\n\n\n# scores on the validation dataset\nlr_rmse_val_scores = np.sqrt(-lr_scores)","4e88f4f7":"def display_scores (val):\n    '''\n    A fnction to display scores after cross validation \n    \n    '''\n    print('Scores: {}'.format(val))\n    print('Mean score Value {}'.format(val.mean()))\n    print('Standard Deviation {}'.format(val.std()))\n\ndisplay_scores(lr_rmse_val_scores)","6ce26f3c":"y_train_predict = lr.predict(X_train)\ny_val_predict = lr.predict(X_val)\n\n\n#  Plotting Residuals\n\nplt.scatter(y_train_predict, y_train_predict-y_train, c='blue', marker ='*', label='Residual on Training Data')\nplt.scatter(y_val_predict, y_val_predict-y_val, c='red', marker ='*', label='Residual on Validation Data')\nplt.hlines(y = 0, xmin = 10.5, xmax = 13, color = \"black\")\nplt.title('Linear Regression Residuals', size=15)\nplt.xlabel('Predicted Value')\nplt.ylabel('Rediual Values')\nplt.legend(loc='best')\nplt.show()\n\n\n# Plotting Predictions\n\nplt.scatter(y_train,y_train_predict, c='blue', marker='*', label='Predictions on Training Dataset' )\nplt.scatter(y_val, y_val_predict, c='red', marker='*', label='Prediction on validation Dataset')\nplt.title('Predictions of Linear regression', size=15)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.legend(loc='best')\nplt.show()","15004791":"# finding best parameter alpha for ridge regularization\nfrom sklearn.linear_model import RidgeCV\n\nalphas = [0.003, 0.03, 0.3, 3.0, 30.0, 50.0, 60.0, 80.0]\nridge_clf = RidgeCV(alphas)\nridge_clf.fit(X_train, y_train)\n\nprint('Best alpha for Ridge: {}'.format(ridge_clf.alpha_))\n\nridge_scores = cross_val_score(ridge_clf, X_val, y_val, scoring='neg_mean_squared_error', cv=10 )\nridge_rmse_val_scores = np.sqrt(-ridge_scores)\n\n\ndisplay_scores(ridge_rmse_val_scores)","b4b26dc3":"y_train_ridge_predict = ridge_clf.predict(X_train)\ny_val_ridge_predict = ridge_clf.predict(X_val)\n\n\n\n#  Plotting Residuals\n\nplt.scatter(y_train_ridge_predict, y_train_ridge_predict-y_train, c='blue', marker ='*', label='Residual on Training Data')\nplt.scatter(y_val_ridge_predict, y_val_ridge_predict-y_val, c='red', marker ='*', label='Residual on Validation Data')\nplt.hlines(y = 0, xmin = 10.5, xmax = 13, color = \"black\")\nplt.title('Linear Regression with L2 Penalty: Residuals', size=15)\nplt.xlabel('Predicted Value')\nplt.ylabel('Rediual Values')\nplt.legend(loc='best')\nplt.show()\n\n\n# Plotting Predictions\n\nplt.scatter(y_train,y_train_ridge_predict, c='blue', marker='*', label='Predictions on Training Dataset' )\nplt.scatter(y_val, y_val_ridge_predict, c='red', marker='*', label='Prediction on validation Dataset')\nplt.title('Predictions of Linear regression with L2 penalty', size=15)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.legend(loc='best')\nplt.show()\n","eccc5836":"# Plotting Important coefficient of Ridge Regression \ncoefs = pd.Series(ridge_clf.coef_, index = X_train.columns)\n\nprint('Ridge picked {} features and dropped {} features'.format(sum(coefs !=0), sum(coefs==0)))\nimp_coefs = pd.concat([coefs.sort_values().head(15),\n                     coefs.sort_values().tail(15)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\", size=15)\nplt.show()","d10a5084":"predictions = ridge_clf.predict(test_df)\n\nkaggle_submission_output = pd.DataFrame ({'Id': test_df_copy['Id'], 'SalePrice':predictions })\nkaggle_submission_output.to_csv ('House_Price_Prediction_submission.csv', index=False)\n\nprint('Output submitted Successfully')","af4fb4da":"Imputing Missing values in the categorical features","bce54932":"Most Machine Learning algorithm prefers to work with numbers , so lets convert these categorical from text to numbers. For this we can use Scikit-Learn's Ordinal Encoder Class.","64c3b5ed":"## Simple Linear Regression Model Without Any Penalty","c83e3337":"# **Importing Libraries**","070133e1":"For a linear model, regularization is typically achieved by constraining the weights of the model.","ad2d1b37":"## Regularization of Numerical Features","94187977":"## Encoder for Categorical Features","f65aa5fd":"Scatter plot reveals few important observation, the coorelation indeed very strong for OverallQual, GrLivingArea and GarageArea","91f8521d":"# **Numerical Features Analysis**","1ff31939":"# **Model Selection and Hypertuning**","b0d8b0f6":"## Linear Regression with L2 Penalty- Ridge Regression","396377b1":"# **Categorical feature Analysis**","19813370":"After log transformation SalePrice distribution is normal and skewness also reduced","a898b7f5":"## What we have achieved so far after all previous steps:\n    1. Identified Missing values in both categorical and Numerical features\n    2. Missing values imputed with mode in case of numerical and categorical category\n    3. Oridanl encoding of categorical features \n    4. Outlier removed from the training set\n    ","f3bc6bd1":"It is important to handel extreme values in the dataset for higher efficiency of the model. For good understanding please check: [link](https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/)","e4a50fd1":"## Imputing Missing values in the Numerical Dataset","76d8fd7a":"## Handeling Outliers in the Train Dataset","0e528176":"## **Analysis of Target feature: SalePrice**","1326a600":" Test dataset has additional SalePrice column which we have to predict for the test dataset"}}