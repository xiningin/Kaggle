{"cell_type":{"2ba00bd8":"code","5ee9c3da":"code","7d79e3fb":"code","6c0045fa":"code","2398e229":"code","ec866daf":"code","1863120a":"code","96b2ca8f":"code","917cc595":"code","a83a6228":"code","2576a97a":"code","38bdaaaf":"code","4b9f476c":"code","0343c17d":"code","5f81a3ad":"code","85d6f7d7":"code","b34e0787":"code","3fcb64b2":"code","d2d49395":"code","544a6d08":"code","700967c8":"code","cddb7b2b":"code","d0ef6598":"code","895d5d5b":"code","4dc13400":"code","727f9932":"code","cb699185":"code","b8da2f19":"code","c30aea4c":"code","f635928b":"code","a29af96a":"code","2616a677":"code","2ef9d44a":"code","4d992870":"code","cee4caaa":"code","a00f9793":"code","432c1418":"code","59448f7e":"code","3afd3cb5":"markdown","795e36b1":"markdown","1d72baec":"markdown","7d995d6d":"markdown","1f59b3b4":"markdown","2dd7544d":"markdown","052b05c4":"markdown","5c3f8d72":"markdown"},"source":{"2ba00bd8":"import librosa\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import Input, Model, Sequential, layers\nfrom tensorflow.keras.layers import (LSTM, BatchNormalization, Concatenate,\n                                     Conv2D, Conv1D, Dense, Dropout, Reshape, Flatten,\n                                     MaxPooling2D, MaxPooling1D)\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import LabelBinarizer\n","5ee9c3da":"X = np.load('\/kaggle\/input\/ravdess\/featuresX.npy')[1:]\nY = np.load('\/kaggle\/input\/ravdess\/featuresY.npy')","7d79e3fb":"dic  =  {0:'neutral', 1:'happy', 2:'sad', 3:'angry', 4:'fear', 5:'disgust', 6:'surprise'}\n\nlabels = []\nfor i in range(1440):\n    label = Y[i]\n    if 'neutral' in label:\n        labels.append(0)\n    elif 'happy' in label:\n        labels.append(1)\n    elif 'sad' in label:\n        labels.append(2)\n    elif 'angry' in label:\n        labels.append(3)\n    elif 'fear' in label:\n        labels.append(4)\n    elif 'disgust' in label:\n        labels.append(5)\n    elif 'surprise' in label:\n        labels.append(6)\n\nY = np.asarray(labels).reshape(1440, 1)\nX = X.reshape(1440, 40, 1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","6c0045fa":"def model_1d(number_of_outputs, n_mfcc):\n    '''\n    1d conv + lstm parallel model\n    '''\n    model_input = Input(shape=(n_mfcc, 1))\n\n    # CNN\n    # Block 1\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu', padding='same')(model_input)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = Dropout(rate=0.2)(cnn_output)\n\n    # Block 2\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n    cnn_output = Dropout(rate=0.2)(cnn_output)\n\n    # Block 3\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu', padding='same')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = Dropout(rate=0.2)(cnn_output)\n\n    # Block 4\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n    cnn_output = Dropout(rate=0.2)(cnn_output)\n\n    # Block 5\n    cnn_output = Conv1D(filters=64, kernel_size=(3, ),\n                        activation='relu', padding='same')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = Dropout(rate=0.2)(cnn_output)\n\n    # Block 6\n    cnn_output = Conv1D(filters=64, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n    cnn_output = Dropout(rate=0.2)(cnn_output)\n\n    # LSTM\n    lstm_output = LSTM(16, return_sequences=True)(\n        model_input)\n\n    lstm_output = LSTM(16, return_sequences=True)(lstm_output)\n\n    lstm_output = LSTM(16, return_sequences=True)(lstm_output)\n\n    # Flatten layers\n    lstm_output = Flatten()(lstm_output)\n    cnn_output = Flatten()(cnn_output)\n    concat = Concatenate(axis=1)([cnn_output, lstm_output])\n\n    # Dense\n    output = Dense(256, activation='relu')(concat)\n    output = Dense(128, activation='relu')(output)\n    output = Dense(64, activation='relu')(output)\n\n    output = Dense(number_of_outputs, activation='softmax')(output)\n\n    model = Model(model_input, output)\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model","2398e229":"model = model_1d(number_of_outputs=7, n_mfcc=40)","ec866daf":"model.summary()","1863120a":"history_train = model.fit(X_train, Y_train, validation_split=0.2, batch_size=10,epochs=100, shuffle=True)","96b2ca8f":"print(history_train.history.keys())\n#  \"Accuracy\"\nplt.plot(history_train.history['accuracy'])\nplt.plot(history_train.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history_train.history['loss'])\nplt.plot(history_train.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","917cc595":"model.evaluate(x=X_test, y=Y_test)","a83a6228":"def model_1d_no_dropout(number_of_outputs, n_mfcc):\n    '''\n    1d conv + lstm parallel model\n    '''\n    model_input = Input(shape=(n_mfcc, 1))\n\n    # CNN\n    # Block 1\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu', padding='same')(model_input)\n    cnn_output = BatchNormalization()(cnn_output)\n\n\n    # Block 2\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n\n\n    # Block 3\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu', padding='same')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n\n\n    # Block 4\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n\n\n    # Block 5\n    cnn_output = Conv1D(filters=64, kernel_size=(3, ),\n                        activation='relu', padding='same')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n\n\n    # Block 6\n    cnn_output = Conv1D(filters=64, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n\n\n    # LSTM\n    lstm_output = LSTM(16, return_sequences=True)(\n        model_input)\n\n    lstm_output = LSTM(16, return_sequences=True)(lstm_output)\n\n    lstm_output = LSTM(16, return_sequences=True)(lstm_output)\n\n    # Flatten layers\n    lstm_output = Flatten()(lstm_output)\n    cnn_output = Flatten()(cnn_output)\n    concat = Concatenate(axis=1)([cnn_output, lstm_output])\n\n    # Dense\n    output = Dense(256, activation='relu')(concat)\n    output = Dense(128, activation='relu')(output)\n    output = Dense(64, activation='relu')(output)\n\n    output = Dense(number_of_outputs, activation='softmax')(output)\n\n    model = Model(model_input, output)\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model","2576a97a":"model_rev_2 = model_1d_no_dropout(7, 40)\nhistory_train_rev_2 = model_rev_2.fit(X_train, Y_train, validation_split=0.2, batch_size=10,epochs=100, shuffle=True)","38bdaaaf":"print(history_train_rev_2.history.keys())\n#  \"Accuracy\"\nplt.plot(history_train_rev_2.history['accuracy'])\nplt.plot(history_train_rev_2.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history_train_rev_2.history['loss'])\nplt.plot(history_train_rev_2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","4b9f476c":"model_rev_2.evaluate(x=X_test, y=Y_test)","0343c17d":"X_multiple = np.load('\/kaggle\/input\/multiple-features\/featuresX_multiple.npy')\nY_multiple = np.load('\/kaggle\/input\/multiple-features\/featuresY_multiple.npy')","5f81a3ad":"labels = []\nfor i in range(1440):\n    label = Y_multiple[i]\n    if 'neutral' in label:\n        labels.append(0)\n    elif 'happy' in label:\n        labels.append(1)\n    elif 'sad' in label:\n        labels.append(2)\n    elif 'angry' in label:\n        labels.append(3)\n    elif 'fear' in label:\n        labels.append(4)\n    elif 'disgust' in label:\n        labels.append(5)\n    elif 'surprise' in label:\n        labels.append(6)\n\nY_multiple = np.asarray(labels).reshape(1440, 1)\nX_multiple = X_multiple.reshape(1440, 194, 1)\n\nX_mult_train, X_mult_test, Y_mult_train, Y_mult_test = train_test_split(X_multiple, Y_multiple, test_size=0.2, random_state=42)","85d6f7d7":"model_mult = model_1d_no_dropout(7, 194)\nmodel_mult_history = model_mult.fit(X_mult_train, Y_mult_train, validation_split=0.2, batch_size=10,epochs=100, shuffle=True)","b34e0787":"print(model_mult_history.history.keys())\n#  \"Accuracy\"\nplt.plot(model_mult_history.history['accuracy'])\nplt.plot(model_mult_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(model_mult_history.history['loss'])\nplt.plot(model_mult_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","3fcb64b2":"model_mult.evaluate(x=X_mult_test, y=Y_mult_test)","d2d49395":"def model_18_dec(number_of_outputs, n_mfcc):\n\n    model_input = Input(shape=(n_mfcc, 1))\n\n\n    # Conv Block 1\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu', padding='same')(model_input)\n\n\n    # Conv Block 2\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n\n\n    # Conv Block 3\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu', padding='same')(cnn_output)\n\n\n    # Conv Block 4\n    cnn_output = Conv1D(filters=32, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n\n\n    # Conv Block 5\n    cnn_output = Conv1D(filters=64, kernel_size=(3, ),\n                        activation='relu', padding='same')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n\n\n    # Conv Block 6\n    cnn_output = Conv1D(filters=64, kernel_size=(3, ),\n                        activation='relu')(cnn_output)\n    cnn_output = BatchNormalization()(cnn_output)\n    cnn_output = MaxPooling1D()(cnn_output)\n\n\n    # LSTM Layers\n    lstm_output = LSTM(64, return_sequences=True)(\n        model_input)\n\n    lstm_output = LSTM(32, return_sequences=True)(lstm_output)\n\n    lstm_output = LSTM(16, return_sequences=True)(lstm_output)\n\n    # Flatten layers and concatenation\n    lstm_output = Flatten()(lstm_output)\n    cnn_output = Flatten()(cnn_output)\n    concat = Concatenate(axis=1)([cnn_output, lstm_output])\n\n    # Dense layers\n    output = Dense(256, activation='relu')(concat)\n    output = Dropout(rate=0.3)(output)\n    output = Dense(128, activation='relu')(output)\n    output = Dropout(rate=0.3)(output)\n    output = Dense(64, activation='relu')(output)\n\n    # Output layer\n    output = Dense(number_of_outputs, activation='softmax')(output)\n\n    model = Model(model_input, output)\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model","544a6d08":"new_model = model_18_dec(number_of_outputs=7, n_mfcc=40)","700967c8":"history_train_new = new_model.fit(X_train, Y_train, validation_split=0.2, batch_size=10,epochs=100, shuffle=True)","cddb7b2b":"print(history_train_new.history.keys())\n#  \"Accuracy\"\nplt.plot(history_train_new.history['accuracy'])\nplt.plot(history_train_new.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history_train_new.history['loss'])\nplt.plot(history_train_new.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","d0ef6598":"new_model.evaluate(x=X_test, y=Y_test)","895d5d5b":"X_a = np.load('\/kaggle\/input\/featuresa\/FeaturesX-a.npy')\nY_a = np.load('\/kaggle\/input\/featuresa\/FeaturesY-a.npy')","4dc13400":"dic  =  {0:'neutral', 1:'happy', 2:'sad', 3:'angry', 4:'fear', 5:'disgust', 6:'surprise', 7:'bored'}\n\nlabels = []\nfor i in range(len(Y_a)):\n    label = Y_a[i]\n    if 'neutral' in label:\n        labels.append(0)\n    elif 'happy' in label:\n        labels.append(1)\n    elif 'sad' in label:\n        labels.append(2)\n    elif 'angry' in label:\n        labels.append(3)\n    elif 'fear' in label:\n        labels.append(4)\n    elif 'disgust' in label:\n        labels.append(5)\n    elif 'surprise' in label:\n        labels.append(6)\n    elif 'bored' in label:\n        labels.append(7)\n\nY_a = np.asarray(labels)\n\nX_a = np.expand_dims(X_a, -1)\nY_a = np.expand_dims(Y_a, -1)\n\nX_a_train, X_a_test, Y_a_train, Y_a_test = train_test_split(X_a, Y_a, test_size=0.2, random_state=42)","727f9932":"model_a = model_18_dec(8, 40)","cb699185":"history_train_a = model_a.fit(X_a_train, Y_a_train, validation_split=0.2, batch_size=10,epochs=100, shuffle=True)","b8da2f19":"print(history_train_a.history.keys())\n#  \"Accuracy\"\nplt.plot(history_train_a.history['accuracy'])\nplt.plot(history_train_a.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history_train_a.history['loss'])\nplt.plot(history_train_a.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","c30aea4c":"model_a.evaluate(x=X_a_test, y=Y_a_test)","f635928b":"X_tum = np.load('\/kaggle\/input\/tumdatasetleraugmented\/FeaturesX.npy')\nY_tum = np.load('\/kaggle\/input\/tumdatasetleraugmented\/FeaturesY.npy')","a29af96a":"dic  =  {0:'neutral', 1:'happy', 2:'sad', 3:'angry', 4:'fear', 5:'disgust', 6:'surprise', 7:'bored'}\n\nlabels = []\nfor i in range(len(Y_tum)):\n    label = Y_tum[i]\n    if 'neutral' in label:\n        labels.append(0)\n    elif 'happy' in label:\n        labels.append(1)\n    elif 'sad' in label:\n        labels.append(2)\n    elif 'angry' in label:\n        labels.append(3)\n    elif 'fear' in label:\n        labels.append(4)\n    elif 'disgust' in label:\n        labels.append(5)\n    elif 'surprise' in label:\n        labels.append(6)\n    elif 'bored' in label:\n        labels.append(7)\n\nY_tum = np.asarray(labels)\n\nX_tum = np.expand_dims(X_tum, -1)\nY_tum = np.expand_dims(Y_tum, -1)\n\nX_tum_train, X_tum_test, Y_tum_train, Y_tum_test = train_test_split(X_tum, Y_tum, test_size=0.2, random_state=42)","2616a677":"model_tum = model_18_dec(8, 40)","2ef9d44a":"history_train_tum = model_tum.fit(X_tum_train, Y_tum_train, validation_split=0.2, batch_size=10,epochs=100, shuffle=True)","4d992870":"print(history_train_tum.history.keys())\n#  \"Accuracy\"\nplt.plot(history_train_tum.history['accuracy'])\nplt.plot(history_train_tum.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history_train_tum.history['loss'])\nplt.plot(history_train_tum.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","cee4caaa":"model_tum.evaluate(x=X_tum_test, y=Y_tum_test)","a00f9793":"y_pred = model_tum(X_tum_test)\ny_pred_classes = np.argmax(y_pred, axis=-1)\nreal_classes = np.squeeze(Y_tum_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(real_classes,y_pred_classes)\nprint(cm)","432c1418":"import seaborn as sn\nplt.figure(figsize = (8,8))\nsn.heatmap(cm, annot=True, fmt='g')","59448f7e":"model_tum.save('\/kaggle\/working\/model_tum')","3afd3cb5":"# 20 DEC Model","795e36b1":"# Model: 18 Dec\n### Changelog\n- BatchNormalization lar\u0131n say\u0131s\u0131 azalt\u0131ld\u0131\n- Dense katmanlar aras\u0131na dropoutlar eklendi\n- Lstm katmanlar\u0131 64-32-16 \u015feklinde channel say\u0131s\u0131n\u0131 azaltarak encoding yapacak \u015fekile getirildi\n","1d72baec":"## Multiple features, without dropout","7d995d6d":"### TEST","1f59b3b4":"## conv1d + lstm model with dropout","2dd7544d":"# 1d MFCC Ravdess","052b05c4":"## Tum datasetler augmented","5c3f8d72":"## conv1d + lstm model without dropout"}}