{"cell_type":{"e73ec569":"code","2ab52b49":"code","be04b215":"code","2eda8885":"code","e916cb1c":"code","5ae321fa":"code","691acef4":"code","56b891e0":"code","6126dafb":"code","15379845":"code","0f5aa1c8":"code","718aa82d":"code","a51ed35a":"code","dce2fbfe":"code","84793dc7":"code","ef0de191":"code","79dabe68":"code","dc0cc5c0":"code","1db4d5a4":"code","741df4e7":"code","f58a468d":"code","5e3916af":"markdown","626192a5":"markdown","3cf39387":"markdown","94ab688c":"markdown","60191ea5":"markdown","babf555a":"markdown","76122198":"markdown","3f73edf4":"markdown","982d9b65":"markdown"},"source":{"e73ec569":"!pip install sagemaker","2ab52b49":"import numpy as np\nimport pandas as pd\nimport boto3, io\nfrom scipy.sparse import lil_matrix\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns","be04b215":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\naws_bucket = user_secrets.get_secret(\"AWS-BUCKET\")\naws_access_key_id = user_secrets.get_secret(\"aws_access_key_id\")\naws_secret_access_key = user_secrets.get_secret(\"aws_secret_access_key\")\naws_region = 'eu-central-1'","2eda8885":"df = pd.read_csv('\/kaggle\/input\/ecommerce-dataset\/events.csv')\ndf.head()","e916cb1c":"df.dtypes","5ae321fa":"vals = df.groupby('event').count().timestamp\nplt.pie(vals, labels=vals.index, autopct='%1.2f%%')","691acef4":"df.loc[df['transactionid'].notnull(), ['visitorid', 'itemid']]","56b891e0":"df_views = df.loc[df['event'] == 'view']\ndf_non_views = df.loc[df['event'] != 'view']\n\nview_events_size = df_views.shape[0]\nnon_view_events_size = df_non_views.shape[0]\nprint(f'Count of non views events is {non_view_events_size} and events {view_events_size}')","6126dafb":"# Picking a random sample from the views of equal size to non views\nindex_chosen = np.random.choice(len(df_views), size=non_view_events_size, replace=False)\nrandom_views_choice = df_views.values[index_chosen]\n\n# Concatenating and getting the balanced data\nbalanced_data = np.concatenate([df_non_views.values, random_views_choice])\nassert balanced_data.shape[0] == non_view_events_size * 2\nprint(balanced_data.shape)\nprint(balanced_data.size)\nbalanced_data","15379845":"visitor_enc = LabelEncoder()\nvisitor_enc2 = LabelEncoder()\nvis_enc = visitor_enc.fit_transform(balanced_data[:,1])\nvisitor_enc2.fit(vis_enc)\nassert visitor_enc.classes_.size == np.unique(balanced_data[:,1]).size\nassert visitor_enc.classes_.size == visitor_enc2.classes_.size\nassert visitor_enc2.classes_[-1] == np.unique(balanced_data[:,1]).size - 1","0f5aa1c8":"def encode_visitorid(visitorids):\n    if type(visitorids) == np.ndarray:\n        return visitor_enc2.transform(visitor_enc.transform(visitorids))\n    else:\n        return visitor_enc2.transform(visitor_enc.transform(np.array([visitorids])))\n\n\nprint(balanced_data[:10,1])\nencode_visitorid(balanced_data[:10,1])","718aa82d":"item_enc = LabelEncoder()\nitem_enc2 = LabelEncoder()\nit_enc = item_enc.fit_transform(balanced_data[:,3])\nitem_enc2.fit(it_enc)\nassert item_enc.classes_.size == np.unique(balanced_data[:,3]).size\nassert item_enc.classes_.size == item_enc2.classes_.size\nassert item_enc2.classes_[-1] == np.unique(balanced_data[:,3]).size - 1","a51ed35a":"def encode_itemid(itemids):\n    return item_enc2.transform(item_enc.transform(itemids))\n\n\nprint(balanced_data[:10,3])\nencode_itemid(balanced_data[:10,3])","dce2fbfe":"visitors = visitor_enc2.classes_.size\nitems = item_enc2.classes_.size\nfeatures = visitors + items\nprint(f'Count of visitors is {visitors}, items {items}, and features {features}')","84793dc7":"data = np.copy(balanced_data)\n# Encoding variables\nprint(data)\ndata[:,1] = encode_visitorid(balanced_data[:,1])\ndata[:,3] = encode_itemid(balanced_data[:,3])\nprint(data)","ef0de191":"# Split train and test\ntrain, test = train_test_split(data, train_size=0.7, shuffle=True)\nprint(train.shape)\nprint(test.shape)","79dabe68":"def data_to_X_y(data, features):\n    X = lil_matrix((data.shape[0], features)).astype('float32')\n    y = []\n    for index, row in enumerate(data):\n        X[index, row[1]] = 1.\n        X[index, visitors + row[3]] = 1.\n        if row[2] == 'view':\n            y.append(0.)\n        else:\n            y.append(1.)\n            \n    y = np.array(y).astype('float32')\n    return X, y","dc0cc5c0":"X_train, y_train = data_to_X_y(train, features)\nassert X_train.shape == (train.shape[0], features)\nassert y_train.size == train.shape[0]\nX_test, y_test = data_to_X_y(test, features)\nassert X_test.shape == (test.shape[0], features)\nassert y_test.size == test.shape[0]\nprint(X_train[:10])\nprint(y_train[:10])","1db4d5a4":"prefix = 'retailrocket-reco'\n\ntrain_key      = 'train.protobuf'\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\n\ntest_key       = 'test.protobuf'\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\noutput_prefix  = 's3:\/\/{}\/{}\/output'.format(aws_bucket, prefix)","741df4e7":"#session = boto3.session.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=aws_region)\n\ndef dataset_to_protobuf_to_s3(X, y, prefix, key):\n    buf = io.BytesIO()\n    smac.write_spmatrix_to_sparse_tensor(buf, X, y)\n    buf.seek(0)\n    obj = '{}\/{}'.format(prefix, key)\n    session.resource('s3').Bucket(aws_bucket).Object(obj).upload_fileobj(buf)\n    return 's3:\/\/{}\/{}'.format(aws_bucket,obj)\n    \n    \n#train_path = dataset_to_protobuf_to_s3(X_train, y_train, train_prefix, train_key)\n#test_path = dataset_to_protobuf_to_s3(X_test, y_test, test_prefix, test_key)\n  \n#print(train_path)\n#print(test_path)\n#print('Output: {}'.format(output_prefix))","f58a468d":"def visitor_to_X(visitor_id):\n    \"\"\"\n    Function to get the input data for the model when asking for recommendations for a visitor.\n    The result is a sparse matrix with the visitor_id and all the existing item_id.\n    Input:\n    visitor_id: endoded numpy array with one existing visitor_id\n    \"\"\"\n    items_data = balanced_data[:,3]\n    enc_unique_items_id = encode_itemid(np.unique(items_data))\n    X = lil_matrix((enc_unique_items_id.size, features)).astype('float32')\n    for index, item_id in enumerate(enc_unique_items_id):\n        X[index, visitor_id[0]] = 1.\n        X[index, visitor_id[0] + item_id + 1] = 1. # + 1 as a quick fix when item_id is 0\n    \n    return X\n\n\ndef decode_itemid(itemids):\n    return item_enc.inverse_transform(item_enc2.inverse_transform(itemids))\n\n\ndef prediction_to_items(input_data, prediction):\n    positive_index_y = np.nonzero(prediction)[0][:3] # Select three items here due to excess of memory allocation in the Kaggle server\n    positive_matrix_x = np.take(input_data.toarray(), positive_index_y, axis=0)\n    positive_index_x = np.nonzero(positive_matrix_x)\n    items_reco = [x - 1 for x in positive_index_x[1][1::2]] # x - 1 due to indexing in sparse matrix with the visitor_id\n    return decode_itemid(np.array(items_reco))\n\n\ndef get_three_recommendations(visitor_id):\n    encoded_visitor_id = encode_visitorid(visitor_id)\n    input_data = visitor_to_X(encoded_visitor_id)\n    #prediction = fm_predictor.predict(input_data)\n    prediction = y_test[:np.unique(balanced_data[:,3]).size] # This line is for testing purposes\n    items_predicted = prediction_to_items(input_data, prediction)\n    return items_predicted\n\nvisitor_id = 287857 # Test id\n#visitor_id = 599528\nget_three_recommendations(visitor_id)","5e3916af":"## Splitting train and test","626192a5":"## Build sparse matrix","3cf39387":"## Convert to protobuf and save to S3","94ab688c":"## Encoding visitors and items to built a sparse matrix later\n\n\nThe shape of the sparse matrix is (len(data), unique(visitorsid) + unique(itemsid)). So to implement an algorithm that build a sparse matrix regardless of the ids content, these have to be encoded to the range 0 to len(unique(ids)) - 1.\n\n\nI couldn't figure out why encoding the ids with one encoder don't result in the data with range 0 to len(unique(ids)) - 1. However, by coincidence I realized that using a pipeline of two encoders, it results in the desired operation.","60191ea5":"# Processing the data\n## Balancing","babf555a":"# Modelling with AWS Sagemaker\n\n\nI got some errors with Sagemaker session due to config of credentials, region and role. To solve it quicly, the implementation of the model, train and testing is done in a notebook through the AWS console.\n\n\n# Getting three recommendations for a visitor\n\n\nThe task asked to get the three best recommendations but the model developed is a binary classifier meaning that the prediction is a discrete value: 1 or 0. Therefore the first three recommendations predicted are returned as result.\n\nAssumption: the model is correctly deployed with the adecuate serializer and deserializer, and is accesible through the variable ```fm_predictor```.\n\nContraint: given the encoding of the visitor_id to transform the data to the correct input format, the function can only retrieve three recommendations of an existing visitor (that was fed to the model in the training phase). It doesn't work with new visitors.","76122198":"# Introduction\n\nA binary recommender is selected as approach using Factorization Machines, taking advantage that it is implemented in AWS Sagemaker and it's reliable to deal with sparse data.\n\nTo simplify the development, the data used will be only the events in the commerce. Events labeled as 'transaction' or 'add to cart' will be considered as like (1), and 'views' as don't like (0). Therefore, the classification will be binary.","3f73edf4":"Assuming a 'view' is a negative class, the data is too unbalanced. As a quick solution, the view events will be selected in an equal amount to the non view events so there is a 50\/50 distribution in the y label.","982d9b65":"# Exploring the data"}}