{"cell_type":{"2bc62112":"code","33837db2":"code","910faaeb":"code","0ec860a0":"code","ff6836ea":"code","c1406d2f":"code","098cb2e5":"code","69d8c5d3":"code","9a7b5e4b":"code","09e4e7cd":"code","19eeac0f":"code","e83ee8f7":"code","a1048106":"code","d5c8ee43":"code","046ae02d":"code","097a740e":"code","d8c26a68":"code","6a84e697":"markdown","7e6da334":"markdown","65e70947":"markdown","b09d52b5":"markdown","e02f24a2":"markdown","ae101a88":"markdown","6b09a311":"markdown","50610148":"markdown","ee9a1c6a":"markdown"},"source":{"2bc62112":"!pip install tensorflow==1.14","33837db2":"# computational\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n# functional programming\nfrom functools import partial\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# plot utilities\n%matplotlib inline\nplt.style.use('ggplot')\ndef plt_left_title(title): plt.title(title, loc=\"left\", fontsize=18)\ndef plt_right_title(title): plt.title(title, loc='right', fontsize=13, color='grey')\n\n# use eager execution for better ease-of-use and readability\ntf.enable_eager_execution()\n\n# aliases\ntfk = tf.keras\ntfd = tfp.distributions\n\nprint(f\"            tensorflow version: {tf.__version__}\")\nprint(f\"tensorflow probability version: {tfp.__version__}\")","910faaeb":"# create sample dataset\nw0, b0 = 0.125, 5.0\n\nn_samples = 150\n\nx_range = [-20, 60]\nx_domain = np.linspace(*x_range, n_samples)\n\ndef load_dataset(n=150, n_tst=n_samples):\n    np.random.seed(27)\n    def s(x):\n        g = (x - x_range[0]) \/ (x_range[1] - x_range[0])\n        return 3 * (0.25 + g**2.)\n    x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0]\n    eps = np.random.randn(n) * s(x)\n    y = (w0 * x * (1. + np.sin(x)) + b0) + eps\n    x = x[..., np.newaxis]\n    x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32)\n    x_tst = x_tst[..., np.newaxis]\n    return y, x, x_tst\n\nys, xs, xs_test = load_dataset()\n\ndef plot_training_data(): \n    plt.figure(figsize=(12, 7))\n    plt.scatter(xs, ys, c=\"#619CFF\", label=\"training data\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\nplot_training_data()\nplt_left_title(\"Training Data\");","0ec860a0":"def neg_log_lik(y, rv_y):\n    \"\"\"Evaluate negative log-likelihood of a random variable `rv_y` for data `y`\"\"\"\n    return -rv_y.log_prob(y)","ff6836ea":"# model outputs normal distribution with constant variance\nmodel_case_1 = tfk.Sequential([\n    tfk.layers.Dense(1),\n    tfp.layers.DistributionLambda(\n        lambda t: tfd.Normal(loc=t, scale=1.0)\n    )\n])\n\n# train the model\nmodel_case_1.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), \n                    loss=neg_log_lik)\nmodel_case_1.fit(xs, ys, \n                 epochs=500,\n                 verbose=False)\n\nprint(f\"predicted w : {model_case_1.layers[-2].kernel.numpy()}\")\nprint(f\"predicted b : {model_case_1.layers[-2].bias.numpy()}\")","c1406d2f":"# predict xs\nyhat = model_case_1(xs_test)\n\nplot_training_data()\nplt_left_title(\"No Uncertainty\")\nplt_right_title(\"$Y \\sim N(w_0 x + b_0, 1)$\")\n# plot predicted means for each x\nplt.plot(x_domain, yhat.mean(), \"#F8766D\", linewidth=5, label=\"mean\")\nplt.legend();","098cb2e5":"def normal_scale_uncertainty(t, softplus_scale=0.05):\n    \"\"\"Create distribution with variable mean and variance\"\"\"\n    ts = t[..., :1]\n    return tfd.Normal(loc = ts,\n                      scale = 1e-3 + tf.math.softplus(softplus_scale * ts))","69d8c5d3":"# model outputs normal distribution with mean and variance that \n# depend on the input\nmodel_case_2 = tfk.Sequential([\n    tfk.layers.Dense(2),\n    tfp.layers.DistributionLambda(normal_scale_uncertainty)\n])\n\nmodel_case_2.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.05),\n                    loss=neg_log_lik)\nmodel_case_2.fit(xs, ys,\n                epochs=500,\n                verbose=False)\n\nprint(\"Model 2 weights:\")\n[print(np.squeeze(w.numpy())) for w in model_case_2.weights];","9a7b5e4b":"# predict normal distributions for each x\nyhat = model_case_2(xs_test)\n\n# get mean and variance\nyhat_mean = yhat.mean()\nyhat_std = yhat.stddev()\n\nplot_training_data()\nplt_left_title(\"Aleatoric Uncertainty\")\nplt_right_title(\"$Y \\sim N(\\mu (x), \\sigma (x))$\")\n# plot mean\nplt.plot(x_domain, yhat_mean, \"#F8766D\", linewidth=5, label=\"mean\")\n# plot 2 stddev from mean\nplt.fill_between(x_domain,\n                 (yhat_mean + 2 * yhat_std)[:, 0], \n                 (yhat_mean - 2 * yhat_std)[:, 0],\n                 facecolor=\"#00BA38\", alpha=0.3,\n                 label=\"$\\pm$ 2 stddev\")\nplt.legend();","09e4e7cd":"def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n    n = kernel_size + bias_size\n    c = np.log(np.expm1(1.0))\n    \n    return tfk.Sequential([\n        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n            tfd.Normal(loc=t[..., :n], \n                       scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n            reinterpreted_batch_ndims=1))\n    ])\n\ndef prior_trainable(kernel_size, bias_size=0, dtype=None):\n    n = kernel_size + bias_size\n    \n    return tfk.Sequential([\n        tfp.layers.VariableLayer(n, dtype=dtype),\n        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n            tfd.Normal(loc=t, scale=1.0),\n            reinterpreted_batch_ndims=1))\n    ])","19eeac0f":"model_case_3 = tfk.Sequential([\n    tfp.layers.DenseVariational(1, \n                            posterior_mean_field, \n                            prior_trainable),\n    tfp.layers.DistributionLambda(\n        lambda t: tfd.Normal(loc=t, scale=1.0)\n    )\n])\n\nmodel_case_3.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n                     loss=neg_log_lik)\nmodel_case_3.fit(xs, ys,\n                 epochs=1000,\n                 verbose=False)\n\nprint(\"Model 3 weights:\")\n[print(np.squeeze(w.numpy())) for w in model_case_3.weights];","e83ee8f7":"# sample posterior\nn_posterior_samples = 50\nyhats = [model_case_3(xs_test) for _ in range(n_posterior_samples)]\n\nplot_training_data()\nplt_left_title(\"Epistemic Uncertainty\")\nplt_right_title(\"$Y \\sim N(W x + B, 1)$\")\n\n# plot means for each posterior sample\nfor i, yhat in enumerate(yhats):\n    plt.plot(xs_test, yhat.mean(), \n             '#F8766D', linewidth=0.5, alpha=0.5, \n             label=f\"{n_posterior_samples} sample means\" if i==0 else None)\n\n# plot overall mean\nyhats_mean = sum(yh.mean() for yh in yhats) \/ len(yhats)\nplt.plot(xs_test, yhats_mean, 'darkred', linewidth=3, label=\"aggregate mean\")\nplt.legend();","a1048106":"model_case_4 = tfk.Sequential([\n    tfp.layers.DenseVariational(2, \n                                posterior_mean_field,\n                                prior_trainable),\n    tfp.layers.DistributionLambda(partial(normal_scale_uncertainty, softplus_scale=0.01))\n])\n\nmodel_case_4.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n                     loss=neg_log_lik)\nmodel_case_4.fit(xs, ys,\n                 epochs=1000,\n                 verbose=False)\n\nprint(\"Model 4 weights:\")\n[print(np.squeeze(w.numpy())) for w in model_case_4.weights];","d5c8ee43":"yhats = [model_case_4(xs_test) for _ in range(7)]\n\nplot_training_data()\nplt_left_title(\"Aleatoric & Epistemic Uncertainty\")\nplt_right_title(\"$Y \\sim N ( W x + B, \\sigma(X) )$\")\nplt.ylim(ys.min() - 5, ys.max() + 5)\n\n# for each posterior sample, plot mean plus\/minus 2 std\nfor i, yhat in enumerate(yhats):\n    m = yhat.mean()[:, 0]\n    s = yhat.stddev()[:, 0]\n\n    plt.plot(xs_test, m, \"#F8766D\", linewidth=0.7, \n             label=f\"{len(yhats)} sample means\" if i==0 else None)\n    plt.fill_between(xs_test[:, 0],\n                     m - 2 * s, m + 2 * s,\n                     facecolor=\"#00BA38\", alpha=0.1,\n                     label=f\"2 stddev\" if i==0 else None)\n    \n# plot overall mean\nyhats_mean = sum(yh.mean() for yh in yhats) \/ len(yhats)\nplt.plot(xs_test, yhats_mean, 'darkred', linewidth=3, label=\"aggregate mean\")\n\nplt.legend();","046ae02d":"class RBFKernelFn(tfk.layers.Layer):\n    def __init__(self, **kwargs):\n        super(RBFKernelFn, self).__init__(**kwargs)\n        dtype = kwargs.get('dtype', None)\n        \n        self._amplitude = self.add_variable(\n            initializer=tf.constant_initializer(0),\n            dtype=dtype,\n            name=\"amplitude\"\n        )\n        \n        self._length_scale = self.add_variable(\n            initializer=tf.constant_initializer(0),\n            dtype=dtype,\n            name=\"length_scale\"\n        )\n    \n    def call(self, x):\n        return x\n    \n    @property\n    def kernel(self):\n        return tfp.positive_semidefinite_kernels.ExponentiatedQuadratic(\n            amplitude=tf.nn.softplus(0.1 * self._amplitude),\n            length_scale=tf.nn.softplus(5.0 * self._length_scale))","097a740e":"num_inducing_points = 40\ninducing_index_points_initializer = np.linspace(*x_range, num=num_inducing_points, dtype=xs.dtype)[..., np.newaxis],\n\nmodel_case_5 = tfk.Sequential([\n    tfk.layers.InputLayer(input_shape=[1], dtype=xs.dtype),\n    tfk.layers.Dense(1, \n                     kernel_initializer=\"ones\", \n                     use_bias=False),\n    tfp.layers.VariationalGaussianProcess(\n        num_inducing_points=num_inducing_points,\n        kernel_provider=RBFKernelFn(dtype=xs.dtype),\n        event_shape=[1],\n        inducing_index_points_initializer=tf.constant_initializer(inducing_index_points_initializer),\n        unconstrained_observation_noise_variance_initializer=(tf.constant_initializer(np.array(0.54).astype(xs.dtype)))\n    )\n])\n\nbatch_size = 32\n\ndef variational_loss(y, rv_y):\n    return rv_y.variational_loss(y, \n                                 kl_weight=np.array(batch_size, xs.dtype) \/ xs.shape[0])\n\nmodel_case_5.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n                    loss=variational_loss)\nmodel_case_5.fit(xs, ys,\n                 batch_size=batch_size,\n                 epochs=800,\n                 verbose=False)","d8c26a68":"yhats = model_case_5(xs_test)\n\nplot_training_data()\nplt_left_title(\"Gaussian Process Regression\")\nplt_right_title(\"$ Y \\sim GPR(f(X)) $\")\nplt.ylim(ys.min() - 5, ys.max() + 5)\n\nn_gpr_samples = 50\nfor _ in range(n_gpr_samples):\n    sample_ = yhats.sample().numpy()[..., 0]\n    plt.plot(xs_test, sample_, \"#F8766D\", linewidth=0.9, alpha=0.4,\n             label=f\"{n_gpr_samples} GPR samples\" if _==0 else None)\n    \nplt.legend();","6a84e697":"# Case 4: Aleatoric & Epistemic Uncertainty\n\nWe can include both types of uncertainty in our model. For each of the models we consider due to epistemic uncertainty, we can incorporate aleatoric uncertainty.","7e6da334":"Each time we call `model_case_3(xs_test)`, it will give us a different answer. This is because the model is sampling from the distributions imposed on the weights.\n\nThis means that we need to make predictions multiple times to understand how the distributions on the weights affect the final prediction. In this case we'll take 50 samples, which means there will be 50 guesses as to what the linear relationship between $x$ and $y$ is.\n\nThis is called [**epistemic uncertainty**](https:\/\/en.wikipedia.org\/wiki\/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty), which represents the unknown factors in your model, your sensor, etc. ","65e70947":"# Case 5: Functional Uncertainty","b09d52b5":"Recall that this model outputs a distribution for any input `x`. In this case, the mean of the distribution for any input `x` is $w_0 x + b_0$. That's the equation for a line, so we should expect to see a straight line when we plot the model's mean predictions of each `x` in our test set.","e02f24a2":"# Case 2: Aleatoric Uncertainty\n\nNotice from the training data that:\n\n- There is variability in the $y$ for any particular value of $x$\n- The variance of $y$ seems to be related to $x$ (e.g. it increases as $x$ does)\n\nNow we're constructing a model that has:\n\n- one-dimensional input\n- a hidden layer with two nodes\n- an output layer that yields a Normal distribution with non-constant variance\n\nIn this case, our model is more complex:\n\n$$\nf(x) \\sim N(\\mu (x), \\sigma(x))\n$$\n\n* Where $\\mu$ is a linear function, and $\\sigma$ is a [softplus function](https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks). Once again, we use negative log-likelihood as our loss function.","ae101a88":"# Case 1: Simple Linear Regression\n\nThe model below is a linear regression model written as a Neural Network with:\n\n- one-dimensional input (the `xs` data)\n- a hidden layer with one node, and bias\n- an output layer that is a Normal distribution with constant variance\n\nBy training the Neural Network, we are finding the weights $w_0$ and $b_0$ such that\n\n$$\nf(x) \\sim N(w_0 x + b_0, 1)\n$$\n\nSo the model outputs a distribution, rather than a number. We find these weights in the usual way, by minimizing a loss function. For this model, and all upcoming models, the loss is the negative log-likelihood of the output distribution. \n\nTensorflow Probability Distributions have a `log_prob` method, so that's what we'll use.\n\n![](http:\/\/)We fit the model below.","6b09a311":"# Regression with Probabilistic Layers in TensorFlow Probability\n\nAn up to-date version of [Regression with Probabilistic Layers in TensorFlow Probability](https:\/\/medium.com\/tensorflow\/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf). Working code also exists in this [colab](https:\/\/colab.research.google.com\/github\/tensorflow\/probability\/blob\/master\/tensorflow_probability\/examples\/jupyter_notebooks\/Probabilistic_Layers_Regression.ipynb#scrollTo=Fp4qEWSRzc8m).\n\nThe code below is just the imports and creating the dataset.","50610148":"# Case 3: Epistemic Uncertainty\n\nWe're assuming a linear relationship between $x$ and $y$, and our last two models gave exact estimates of the weights (e.g. $w_0$ and $b_0$). But how certain are we of those _exact_ estimates?\n\nWhat if we could consider many candidates for the weights all at once? Perhaps we could assume that the weights are normally distributed--the mean weight is our best guess, but we're willing to consider things near it as the true weight too.\n\nNow the weights are Normal distributions, instead of numbers. In order to train the neural network, we use something called variational inference. That's outside the scope of this discussion, but just know that TensorFlow Probability's `DenseVariational` layer handles this step.\n\nNow our model has\n\n- a one-dimensional input layer\n- a `DenseVariational` layer, which treats the weights as distributions, rather than numbers\n- an output layer that yields a Normal distribution with constant variance\n\nAnd it has the functional form\n\n$$\nf(x) \\sim N(Wx + B, 1)\n$$\n\nWhere $W$ and $B$ are normally-distributed random variables. ","ee9a1c6a":"The plot below suggests a linear relationship between $x$ and $y$, but it shows that as $x$ gets larger, there is more variance in the prediction $y$. \n\nThis is called [**aleatoric uncertainty**](https:\/\/en.wikipedia.org\/wiki\/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty), which refers to the unknowns that differ each time data is collected."}}