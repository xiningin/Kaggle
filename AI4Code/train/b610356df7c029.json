{"cell_type":{"a23573b5":"code","17caebb5":"code","762c4bea":"code","23c164d9":"code","b3ed9f35":"code","b80c1689":"code","337d1978":"code","c1dc4a9d":"code","b568995e":"code","5d791561":"code","39e7cffb":"code","52d33a9c":"code","96bb230d":"code","10f3b6fb":"code","b3804837":"code","0c01fe9a":"code","5de626d2":"code","a997ad12":"code","fc0780cf":"code","ff0f1eb3":"code","15baa5e3":"code","ce02a4e0":"code","7e1cc696":"code","b094a311":"code","9dc1adbb":"code","85e28e59":"code","48b8957a":"code","ceacd452":"code","d65e5456":"code","a56014ab":"code","e95185a3":"code","cc52cea3":"code","9070a6a0":"code","d022a392":"code","5339d019":"code","a398387c":"code","cc74dc1e":"code","c9e3a4ec":"code","56ee7227":"code","b86bdfc9":"code","0eaf4cb3":"code","9fba297f":"code","c407d49b":"code","0737d5f5":"code","866698e1":"code","28fba0d3":"code","655d856e":"code","b30e5ce3":"code","41ae63bd":"code","e1715e6f":"code","5836abe3":"code","fb2817c8":"code","29458feb":"code","a10b8450":"code","66d2f00a":"markdown","44870d96":"markdown","905f0fb6":"markdown","587a45a4":"markdown","340b03cf":"markdown","1c1bb5b3":"markdown","49751d00":"markdown","53c8a0e6":"markdown","5ec24d83":"markdown","799d3c04":"markdown","0ceabef2":"markdown","984b589d":"markdown","f532a7db":"markdown","71c67133":"markdown","01aeaa15":"markdown","119fd91b":"markdown","db5f2b7a":"markdown","ed0090a3":"markdown","d0c199de":"markdown","38bbe9f4":"markdown","fa025d6a":"markdown","8b435421":"markdown","c17ed5ad":"markdown","41616dc5":"markdown","846813a6":"markdown","0897f726":"markdown","d508d4a6":"markdown","6cbf54e5":"markdown","9693f0f6":"markdown"},"source":{"a23573b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17caebb5":"import matplotlib.pyplot as plt\nimport seaborn as sns\n","762c4bea":"#loading the data set\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","23c164d9":"df.shape","b3ed9f35":"df.head()","b80c1689":"df.columns.values","337d1978":"df.info()","c1dc4a9d":"df.isnull().sum()","b568995e":"#dropping cabin column\ndf.drop(columns = ['Cabin'], inplace = True) \n#inplace = True changes the data inplace doesn't hae to return back ","5d791561":"#filing missing values for age\n# strategy - mean, we can use different techniques for now we are filling it with mean\ndf['Age'].fillna(df['Age'].mean(), inplace = True)\ndf.info()","39e7cffb":"#inputting missing values for embarked\n#we will fill the with most appeared value in embarked column\ndf['Embarked'].value_counts()","52d33a9c":"#people have boarded from S more. so we can assume others with no city can be S\ndf['Embarked'].fillna('S', inplace = True)\n","96bb230d":"df['SibSp'].value_counts()","10f3b6fb":"df['Parch'].value_counts()","b3804837":"df['Survived'] = df['Survived'].astype('category')\ndf['Pclass'] = df['Pclass'].astype('category')\ndf['Sex'] = df['Sex'].astype('category')\ndf['Age'] = df['Age'].astype('int')\ndf['Embarked'] = df['Embarked'].astype('category')\ndf.info()","0c01fe9a":"df.describe()\n#its good practice to check data by using describe","5de626d2":"#univariate analysis\n#we will start analysis with SUrvived column\n\nsns.countplot(df['Survived'])\ndeath_percent = round((df['Survived'].value_counts().values[0]\/891)*100)\nprint('{} percent of people died'.format(death_percent))\n","a997ad12":"#what about Pclass column\nprint((df['Pclass'].value_counts()\/891)*100)\n\nsns.countplot(df['Pclass'])\n","fc0780cf":"print((df['Sex'].value_counts()))\nsns.countplot(df['Sex'])","ff0f1eb3":"print(df['SibSp'].value_counts())\nsns.countplot(df['SibSp'])","15baa5e3":"print((df['Embarked'].value_counts()\/891)*100)\n\nsns.countplot(df['Embarked'])","ce02a4e0":"#Age column\n\nsns.distplot(df['Age']) \nprint(df['Age'].skew())\nprint(df['Age'].kurt())","7e1cc696":"sns.boxplot(df['Age'])","b094a311":"print('people with age in between 60 and 70',df[(df['Age']>60) & (df['Age']<70)].shape[0])\nprint('people with age in between 70 and 75',df[(df['Age']>70) & (df['Age']<75)].shape[0])\nprint('people with age less than 75',df[(df['Age']<75)].shape[0])\n\nprint('-'*50)\n\nprint('People with age between 0 and 1', df[df['Age']<1].shape[0])","9dc1adbb":"#Fare column\nsns.distplot(df['Fare'])","85e28e59":"print(df['Fare'].skew())\nprint(df['Fare'].kurt())","48b8957a":"sns.boxplot(df['Fare'])","ceacd452":"print('people with fare between 200$ to 300$', df[(df['Fare'] > 200) & (df['Fare'] < 300)].shape[0])\nprint('People with fare more than 300$', df[df['Fare'] > 300].shape[0])","d65e5456":"#multivariate analysis\n#Survival with Pclass\n\n\nsns.countplot(df['Survived'], hue = df['Pclass'])\n\npd.crosstab(df['Pclass'], df['Survived']).apply( lambda r: round((r\/r.sum())*100,1), axis =1)","a56014ab":"#survival with sex\n\nsns.countplot(df['Survived'], hue = df['Sex'])\n\npd.crosstab(df['Sex'],df['Survived']).apply(lambda r: round ((r\/r.sum())*100,1),axis = 1)","e95185a3":"#Survival on Embarked \n\n#its really not so good analysis but we ll see if we can see any information on this\n\nsns.countplot(df['Survived'], hue = df['Embarked'])\npd.crosstab(df['Embarked'],df['Survived']).apply(lambda r: round((r\/r.sum())*100, 1),axis = 1)","cc52cea3":"#survival with age \n\nplt.figure(figsize = (15,6))\n\nsns.distplot(df[df['Survived']==0]['Age'])\nsns.distplot(df[df['Survived']==1]['Age'])","9070a6a0":"#Survive with Fare\n\nplt.figure(figsize = (15,6))\nsns.distplot(df[df['Survived']==0][\"Fare\"])\nsns.distplot(df[df['Survived']==1]['Fare'])","d022a392":"sns.pairplot(df)","5339d019":"sns.heatmap(df.corr())","a398387c":"#feature engineering\n\n#we will create a new column by name family which will be the sum of SibSp and Parch cols\n\ndf['family_size'] = df['Parch'] + df['SibSp']","cc74dc1e":"df.sample(5)","c9e3a4ec":"def family_type(number):\n    if number == 0:\n        return 'Alone'\n    elif number > 0 and number <= 4:\n        return 'Medium'\n    else: \n        return 'Large'","56ee7227":"df['family_type']=df['family_size'].apply(family_type)","b86bdfc9":"df.sample(5)","0eaf4cb3":"#dropping SibSp, Parch and family_size\n\ndf.drop(columns = ['SibSp', 'Parch', 'family_size'], inplace = True)","9fba297f":"pd.crosstab(df['family_type'],df['Survived']).apply(lambda r: round((r\/r.sum())*100,1), axis = 1)\n","c407d49b":"#handling outliers in age(Almost normal)\n\ndf = df[df['Age']<(df['Age'].mean() + 3 * df['Age'].std())]\n\n#df.shape\n","0737d5f5":"df.shape","866698e1":"#handling outliers from fare column\n#finding quartiles\n\nQ1 = np.percentile(df['Fare'],25)\nQ3 = np.percentile(df['Fare'],75)\n\noutlier_low = Q1 - 1.5*(Q3 - Q1)\noutlier_high = Q3 + 1.5 * (Q3 - Q1)\n\ndf = df[(df['Fare'] > outlier_low) & (df['Fare'] < outlier_high)]","28fba0d3":"#one hot encoding\n#in categorical data , having values for categories will make our model give errors, because some where in the model the values of the categories will udergo some mathematical formula and give us wrong resutls \n#so one hot encoding will give us the binary values to the categorical columns\n\n\n#columns to be transformed are Pclass, Sex, Embarked, family_type\n#pd.get_dummies(data = df, columns = ['Pclass', 'Sex', 'Embarked', 'family_type'], drop_first = True)\n","655d856e":"df = pd.get_dummies(data = df, columns = ['Pclass', 'Sex', 'Embarked', 'family_type'], drop_first = True)","b30e5ce3":"plt.figure(figsize = (15,6))\nsns.heatmap(df.corr(), cmap = 'summer')","41ae63bd":"df.info()","e1715e6f":"#let us remove Name and PassengerID and Ticket columns which can't be used or not helpful for our model\ndf.drop(columns = ['PassengerId', 'Ticket', 'Name'], inplace = True)\nX = df.loc[:,df.columns !='Survived']\n\nY = df.Survived\nX.head()","5836abe3":"from sklearn.model_selection import train_test_split\n\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.20, random_state = 1)\n","fb2817c8":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)","29458feb":"y_pred_train = logreg.predict(X_train)\ny_pred_test = logreg.predict(X_test)","a10b8450":"from sklearn.metrics import accuracy_score\n\nprint('Accuracy score for test data is :', accuracy_score(Y_test,y_pred_test))","66d2f00a":"**skewness :**\nit gives the type of ditribution in the data , usually **-0.5 to 0.5 skewness** will be a good data , the more the normal distribution the better the data for predictions.\nkurtosis: it gives the flatness of tails , the more the flat tails the more outliers which is not good , and need to investigate the data. \n\nour data has **0.45 skewness** which is a good normal distribution\n\n","44870d96":"we need to analyse more on outliers \nand can consider age as normal distribution","905f0fb6":"* we can see more **female survived than male**, which shows  more of **female and children** were saved during that disatster .\n\n* we ll also analyse survive on basis of age too","587a45a4":"*  people with **high fare **had **more probaility of survivng**....... ","340b03cf":"# **Exploratory Data analysis**\n\n\n\n\n* **Univariate analysis **: analyse each column on each variable , usually target variable\n\n* **Multivariate analysis**: to find out the columns or variales on which our result depends upon and remove columns that are not necessary for our analysis\n    \n**Feature Engineering:**\n\n* creating new columns\n    if we need to add or change our data, by adding new columns \n* modifying existing ones\n\n\n**Handling Outliers**\n* Detect out liers \n* removing outliers:\n    they are not good for our data , they can reduce the our prediction percentage of results\n\n","1c1bb5b3":"* we have a positively skewed distribution  \n* people who bought tickets of lower price are more, so lower kurt value and more positively skewed","49751d00":"* Pclass 3 have more casualities \n* 75% of people who took class 3 tickets died more in the accident","53c8a0e6":"# columns types\n\n**Categorical columns**\n1. survived\n    whether survived or not\n2. PClass\n    Passigenr class\n3. Sex\n4. SibSp : travveling with siblings or not\n5. Parch: travelling with parents or child\n6. Embarked : from cities , titanic boarded passengers from 3 cities\n\n**Numerical columns:**\n\n* Age\n* Fare\n* PassengerID : not really helpful\n\n**Mixed columns**\n\n* Name\n* Tciket\n* Cabin","5ec24d83":"* we can see **children srvived more** \n* young people in their** 25+ died more** , than survived  ","799d3c04":"1. * we can see people **boarded to S **had more **casualities**\n* people **embarked to C survived **more in percent,  may be people who boarded to C might be given seats that are nearer to safe boats or may be they are more aware of the situation at that time.","0ceabef2":"we can find **missing values** in **Age, cabin, and EMbarked** has only **2 missing values**\n\nlet us see the summary of missing values\n\n","984b589d":"# post-processing\n\n* our data after the pre-processing has become 42.1 KB with **769 rows from 891**\n\n","f532a7db":"# Applications of Logisti Regression\n\nLogistic Regression is used mostly in biological sciences as follows:\n\n* The Trauma and Injury Severity Score(TRISS), is used to predict mortality in injured patients, was developed by Boyd et al. using logistic regression.\n* To assess severity of a patient using other medical scales are also developed using logistic regression\n* also used in predicting the risk of developing a given disease (eg. diaetes, heart disease).\n\nLogistic regression is applied in the follwing fields as well:\n\n* Image segementation and categorization\n* Geographic image procesing\n* Handwriting recognition\n* Detection of myocardinal infraction\n* predict if a person is depressed or not by the words from corpus.\n\nLogistic regression is more affordable and efficient and doesn't require too much computational resources to run on a production compared to the state of the art of neural network.","71c67133":"# Logistic Regression in scikit-learn","01aeaa15":"* most of them started to city S ","119fd91b":"# conclusions:\n* **cabin has 77.10 %** missing values , even though Cabin is a good parameter for the prediction of possible dead, it can't be helpful, it has very less values which is not helpful for analysis\n\n* some columns have inapropriate datatypes , as age is float it can't be , ticket doesn't really help our analysis, we can drop cabin and change datatypes","db5f2b7a":"**changing data type for the following cols**\n* Survived(category)\n* Pclass(category)\n* Sex(category)\n* Age(int)\n* Embarked(category)\n\n","ed0090a3":"we can get information on corelation between columns ","d0c199de":"we can analyse all the plots above for further better results","38bbe9f4":"there are lot of out-liers in the right side , 500 on the most right ","fa025d6a":"Detecting outliers\ntwo types data\n* Numerical Data\n1. if the data is following normal distribution, anything beyond (mean - 3SD) (mean + 3SD) can be considered as an outlier\n2. if the data is not following a normal distribution , using boxplot we can eliminate points beyond Q1 - 1.5IQR and Q3 + 1.5IQR\n\n**Q1: quartile 1 **, left middle value from median, \n\n**Q3: quartile 3**, right middle value from median,\n\n**IQR: InterQuartile Range** , number of values between Q1 and Q3 \n\ndata **lying Q1 - 1.5IQR and Q3 + 1.5IQR are considered as outliers** by statisticians\n\n* Categorical Data\n\n* if the col is highly imbalanced , for eg: 300000 survived and 2 died , we can eliminate the dead. \n\n","8b435421":"# Conclusion on Analysis\n\n* female survivours are higher \n* Travelling in Pclass3 is more dangerous\n* people going to C suvived more \n* 20 - 40 aged people mostly have not survived \n* smaller families have survived more than large families and people travelling alone\n\n","c17ed5ad":"So , we **removed the cabin** , and filled the missing datas in age with mean , and Embarked with most commonly boarded city\nso , we have no missing values in our data\n","41616dc5":"we got accuracy score of **0.79 which is almost 79% ** which quite good. ","846813a6":"**conclusion:**\n\n* Fare column is higley skewd \n* many outliers in the data which need to be analysed more","0897f726":"Total **62%** of people died in the accident","d508d4a6":"# Logistic Regression\n\nLogistic regression is used for classification problems. To classify new data with existing data. \nExamples:\nTo predict a email is a Spam(1) or not(0)\nwhether the tumor is malignant(1) or (0)\n","6cbf54e5":"Firstly I would like to thank [Naveen](https:\/\/www.linkedin.com\/in\/naveen-gampala-0105b279\/?originalSubdomain=in) and [CampusX]( http:\/\/www.campusx.in) for their reference in starting this. ","9693f0f6":"# Splitting the Data into training and test datasets"}}