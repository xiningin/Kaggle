{"cell_type":{"2ab24aa6":"code","804ed80a":"code","7cccc9e1":"code","529ba44f":"code","e39bed9e":"code","ca37cc7d":"code","d8dfc1f3":"code","5e7bd60b":"code","fdb6f14c":"code","26f3301e":"code","fee64d4b":"code","0638fb26":"code","99793f5f":"code","432463e9":"code","8244ab10":"code","2257ff9c":"code","35b04992":"code","09c03a18":"code","b9f58931":"code","e53fb134":"code","a422c2af":"code","b392b2a4":"code","09db2416":"code","151b17b1":"code","26080376":"code","7ba902db":"code","2539c85f":"code","de49eb13":"code","dd42d32a":"code","0dd94e70":"code","4c9f35a2":"code","9425e0d3":"code","f19ee368":"code","ec16f7d9":"code","6848e5d7":"code","bdd17e3a":"code","d97c4db3":"code","503ea080":"code","e71a6076":"code","aa940f67":"code","8a733982":"code","e425e572":"code","f781e5a0":"code","02e973e8":"code","7c26bbaf":"code","8c053090":"code","ce352003":"code","df663d74":"code","a9307377":"code","cb1ccfec":"code","2da9f9e1":"code","85d75a24":"code","54f7960f":"code","a7287072":"code","eb7c5b72":"code","3b7a8e60":"code","038188e1":"code","c53ca6e6":"code","9d323d16":"code","663aeba9":"code","0f4d98cb":"code","9c4aba78":"code","e3a60d5d":"code","ae35395c":"code","13e3f2ac":"code","e8e84d31":"code","f64fd74a":"code","b9bae7f6":"code","76ad3c7c":"code","2fa03496":"code","1687a819":"code","a9700813":"code","959769d6":"code","b8f7d283":"code","2ac9b23c":"code","8c024508":"code","743e3e17":"code","63a4710c":"code","0973a271":"code","91713601":"code","27f2f06b":"code","e27140e8":"code","07d0d5f8":"code","15573582":"code","96e51b38":"code","a90a2961":"code","934ce630":"code","65082584":"code","bc153262":"markdown","b9032b3c":"markdown","eefdb59a":"markdown","4b4ea35a":"markdown","97349a69":"markdown","ccac42f5":"markdown","0f193e0b":"markdown","7c1caa50":"markdown","b0c0655a":"markdown","fe0c19aa":"markdown","710839fb":"markdown","b27ace1a":"markdown","726a169b":"markdown","f1fe3371":"markdown","aef1f0e3":"markdown","7e19f8d6":"markdown","b5534c72":"markdown","756f721a":"markdown","85a46aac":"markdown","ffd62d70":"markdown","56d29923":"markdown","106849ac":"markdown","19157c20":"markdown","5e4c3f2e":"markdown","9bbdcc66":"markdown","63272f4c":"markdown","224021e3":"markdown","0168bf5e":"markdown","98386413":"markdown","9c918e78":"markdown","b5049bf6":"markdown","b9318738":"markdown","31fc0e08":"markdown","71f04652":"markdown","43c2abf9":"markdown","770f8b1a":"markdown","4b259102":"markdown","4782de5d":"markdown","48472663":"markdown","52fea5ee":"markdown","c26a26c1":"markdown","a4a40054":"markdown","1afdaaa1":"markdown","df382a54":"markdown","8b8d4a21":"markdown","3b40cf3b":"markdown","c99a5ffa":"markdown","13434e0c":"markdown","e8008f0b":"markdown","003f8854":"markdown","31c9f902":"markdown","a4e4a9ae":"markdown","63824f89":"markdown","9c3d7216":"markdown","6cdc6e62":"markdown"},"source":{"2ab24aa6":"# To enable plotting graphs in Jupyter notebook\n%matplotlib inline\n\n# Importing libraries\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns\n\n#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n#import os,sys\nfrom scipy import stats\n\n# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","804ed80a":"datapath = '..\/input'","7cccc9e1":"my_data = pd.read_csv(datapath+'\/Bank_Personal_Loan_Modelling.csv')\nmy_data.columns = [\"ID\",\"Age\",\"Experience\",\"Income\",\"ZIPCode\",\"Family\",\"CCAvg\",\"Education\",\"Mortgage\",\"Personal_Loan\",\"SecuritiesAccount\",\"CDAccount\",\"Online\",\"CreditCard\"]\n","529ba44f":"my_data.head(10)","e39bed9e":"my_data.shape","ca37cc7d":"my_data.columns","d8dfc1f3":"my_data.dtypes","5e7bd60b":"#null values\nmy_data.isnull().values.any()","fdb6f14c":"val=my_data.isnull().values.any()\n\nif val==True:\n    print(\"Missing values present : \", my_data.isnull().values.sum())\n    my_data=my_data.dropna()\nelse:\n    print(\"No missing values present\")","26f3301e":"my_data.describe().T","fee64d4b":"my_data.info()","0638fb26":"my_data.apply(lambda x: len(x.unique()))","99793f5f":"#Find Shape\nmy_data.shape","432463e9":"#Find Mean\nmy_data.mean()","8244ab10":"#Find Median\nmy_data.median()","2257ff9c":"#Find Standard Deviation\nmy_data.std()","35b04992":"my_data.hist(figsize=(10,10),color=\"blueviolet\",grid=False)\nplt.show()","09c03a18":"sns.pairplot(my_data.iloc[:,1:])","b9f58931":"my_data[my_data['Experience'] < 0]['Experience'].count()","e53fb134":"my_dataExp = my_data.loc[my_data['Experience'] >0]\nnegExp = my_data.Experience < 0\ncolumn_name = 'Experience'\nmy_data_list = my_data.loc[negExp]['ID'].tolist()","a422c2af":"negExp.value_counts()","b392b2a4":"for id in my_data_list:\n    age = my_data.loc[np.where(my_data['ID']==id)][\"Age\"].tolist()[0]\n    education = my_data.loc[np.where(my_data['ID']==id)][\"Education\"].tolist()[0]\n    df_filtered = my_dataExp[(my_dataExp.Age == age) & (my_dataExp.Education == education)]\n    exp = df_filtered['Experience'].median()\n    my_data.loc[my_data.loc[np.where(my_data['ID']==id)].index, 'Experience'] = exp\n    \n#The records with the ID, get the values of Age and Education columns.\n#Then apply filter for the records matching the criteria from the dataframe \n#which has records with positive experience and take the median.\n#Apply the median again to the location(records) which had negative experience.   ","09db2416":"my_data[my_data['Experience'] < 0]['Experience'].count()","151b17b1":"my_data.describe().T","26080376":"my_data.skew(axis = 0, skipna = True) ","7ba902db":"sns.boxplot(x=my_data[\"Age\"])","2539c85f":"sns.boxplot(x=my_data[\"Experience\"])","de49eb13":"sns.boxplot(x=my_data[\"Income\"])","dd42d32a":"import matplotlib.pylab as plt\n\nmy_data.boxplot(by = 'Personal_Loan',  layout=(4,4), figsize=(20, 20))\nprint(my_data.boxplot('Age'))\nprint(my_data.boxplot('Income'))\nprint(my_data.boxplot('Education'))\n","0dd94e70":"my_data['Personal_Loan'].hist(bins=10)","4c9f35a2":"sns.boxplot(x='Education',y='Income',hue='Personal_Loan',data=my_data)","9425e0d3":"sns.boxplot(x=\"Education\", y='Mortgage', hue=\"Personal_Loan\", data=my_data)","f19ee368":"sns.boxplot(x=\"Family\",y=\"Income\",hue=\"Personal_Loan\",data=my_data)","ec16f7d9":"sns.countplot(x='Family',data=my_data,hue='Personal_Loan')","6848e5d7":"sns.countplot(x=\"SecuritiesAccount\", data=my_data,hue=\"Personal_Loan\")","bdd17e3a":"sns.countplot(x='CDAccount',data=my_data,hue='Personal_Loan')","d97c4db3":"sns.countplot(x='Online',data=my_data,hue='Personal_Loan')","503ea080":"sns.countplot(x='CreditCard',data=my_data,hue='Personal_Loan')","e71a6076":"plt.figure(figsize = (10,8))\nsns.scatterplot(x = \"Experience\", y = \"Age\",data =my_data, hue = \"Education\")\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Age\")\nplt.title(\"Distribution of Education by Age and Experience\")","aa940f67":"sns.distplot( my_data[my_data.Personal_Loan == 0]['CCAvg'])","8a733982":"sns.distplot( my_data[my_data.Personal_Loan == 1]['CCAvg'])","e425e572":"#Credit card spending of Non-Loan customers\nmy_data[my_data.Personal_Loan == 0]['CCAvg'].median()*1000","f781e5a0":"#Credit card spending of Loan customers\nmy_data[my_data.Personal_Loan == 1]['CCAvg'].median()*1000","02e973e8":"cor=my_data.corr()\ncor","7c26bbaf":"plt.subplots(figsize=(10,8))\nsns.heatmap(cor,annot=True)","8c053090":"data=my_data.drop(['ID','ZIPCode','Experience'], axis =1 )\ndata.head(10)","ce352003":"data.info()","df663d74":"data1=data[['Age','Income','Family','CCAvg','Education','Mortgage','SecuritiesAccount','CDAccount','Online','CreditCard','Personal_Loan']]","a9307377":"data1.head(10)","cb1ccfec":"data1.shape","2da9f9e1":"data1[\"Personal_Loan\"].value_counts(normalize=True)","85d75a24":"array = data1.values\nX = array[:,0:9] # select all rows and first 10 columns which are the attributes\nY = array[:,10]   # select all rows and the 10th column which is the classification \"0\", \"1\"\ntest_size = 0.30 # taking 70:30 training and test set\nseed = 15  # Random numbmer seeding for reapeatability of the code\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # To set the random state\ntype(X_train)","54f7960f":"# Fit the model on 30%\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\nprint('Accuracy:',model_score)\nprint('confusion_matrix:')\nprint(metrics.confusion_matrix(y_test, y_predict))\nA=model_score  # Accuracy of Logistic regression model","a7287072":"#from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score","eb7c5b72":"X = data1.values[:,0:9]  ## Features\nY = data1.values[:,10]  ## Target.values[:,10]  ## Target","3b7a8e60":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 7)","038188e1":"clf = GaussianNB()\nclf.fit(X_train, Y_train)","c53ca6e6":"Y_pred = clf.predict(X_test)","9d323d16":"B=accuracy_score(Y_test, Y_pred, normalize = True) #Accuracy of Naive Bayes' Model\nprint('Accuracy_score:',B)","663aeba9":"from sklearn.metrics import recall_score\nprint(recall_score(Y_test, Y_pred))","0f4d98cb":"print('Confusion_matrix:')\nprint(metrics.confusion_matrix(Y_test,Y_pred))","9c4aba78":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler","e3a60d5d":"X_std = pd.DataFrame(StandardScaler().fit_transform(data1))\nX_std.columns = data1.columns","ae35395c":"#split the dataset into training and test datasets\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Transform data into features and target\nX = np.array(data1.iloc[:,1:11]) \ny = np.array(data1['Personal_Loan'])\n\n# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)","13e3f2ac":"print(X_train.shape)\nprint(y_train.shape)","e8e84d31":"print(X_train.shape)\nprint(y_train.shape)","f64fd74a":"# loading library\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\n# instantiate learning model (k = 1)\nknn = KNeighborsClassifier(n_neighbors = 1)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))\n\n# instantiate learning model (k = 5)\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))\n\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))\n# instantiate learning model (k = 7)\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nprint(accuracy_score(y_test, y_pred))","b9bae7f6":"myList = list(range(1,20))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))","76ad3c7c":"ac_scores = []\n\n# perform accuracy metrics for values from 1,3,5....19\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","2fa03496":"#Plot misclassification error vs k (with k value on X-axis) using matplotlib.\nimport matplotlib.pyplot as plt\n# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","1687a819":"#Use k=1 as the final model for prediction\nknn = KNeighborsClassifier(n_neighbors = 1)\n\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\n\n# evaluate accuracy\nC=accuracy_score(y_test, y_pred)   #Accuracy of KNN model\nprint('Accuracy_score:',C)    \nprint(recall_score(y_test, y_pred))\n","a9700813":"print('Confusion_matrix:')\nprint(metrics.confusion_matrix(y_test, y_pred))","959769d6":"from sklearn.model_selection import train_test_split\n\n# To calculate the accuracy score of the model\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ntarget = my_data[\"Personal_Loan\"]\nfeatures=my_data.drop(['ID','ZIPCode','Experience'], axis =1 )\nX_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.30, random_state = 10)","b8f7d283":"from sklearn.svm import SVC\n\n# Building a Support Vector Machine on train data\nsvc_model= SVC(kernel='linear')\nsvc_model.fit(X_train, y_train)\n\nprediction = svc_model.predict(X_test)\n","2ac9b23c":"# check the accuracy on the training set\nprint(svc_model.score(X_train, y_train))\nprint(svc_model.score(X_test, y_test))","8c024508":"print(\"Confusion Matrix:\\n\",confusion_matrix(prediction,y_test))","743e3e17":"#Store the accuracy results for each kernel in a dataframe for final comparison\nresultsDf = pd.DataFrame({'Kernel':['Linear'], 'Accuracy': svc_model.score(X_train, y_train)})\nresultsDf = resultsDf[['Kernel', 'Accuracy']]\nresultsDf","63a4710c":"# Building a Support Vector Machine on train data\nsvc_model = SVC(kernel='rbf')\nsvc_model.fit(X_train, y_train)","0973a271":"print(svc_model.score(X_train, y_train))\nprint(svc_model.score(X_test, y_test))","91713601":"#Store the accuracy results for each kernel in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Kernel':['RBF'], 'Accuracy': svc_model.score(X_train, y_train)})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Kernel', 'Accuracy']]\nresultsDf","27f2f06b":"#Building a Support Vector Machine on train data(changing the kernel)\nsvc_model  = SVC(kernel='poly')\nsvc_model.fit(X_train, y_train)\n\nprediction = svc_model.predict(X_test)\n\nprint(svc_model.score(X_train, y_train))\nprint(svc_model.score(X_test, y_test))","e27140e8":"#Store the accuracy results for each kernel in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Kernel':['Poly'], 'Accuracy': svc_model.score(X_train, y_train)})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Kernel', 'Accuracy']]\nresultsDf","07d0d5f8":"svc_model = SVC(kernel='sigmoid')\nsvc_model.fit(X_train, y_train)\n\nprediction = svc_model.predict(X_test)\n\n##print(svc_model.score(X_train, y_train))\nprint(svc_model.score(X_test, y_test))","15573582":"#Store the accuracy results for each kernel in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Kernel':['Sigmoid'], 'Accuracy': svc_model.score(X_train, y_train)})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Kernel', 'Accuracy']]\nresultsDf","96e51b38":"print(A) #Accuracy of Logistic regression model","a90a2961":"print(B) #Accuracy of Naive Bayes' Model","934ce630":"print(C)  #Accuracy of KNN Model","65082584":"resultsDf #Accuracy of SVM Model","bc153262":"## g.Ploting histogram to check that if data columns are normal or almost normal or not ","b9032b3c":"# D.SVM ","eefdb59a":"### Cleaning the negative values ","4b4ea35a":"a.The variable ID does not add any particular information.\n\nb.There are 2 nominal variables:\n\n    1.ID    \n    2.Zip Code\n    \nc.There are 2 Ordinal Categorical Variables:\n\n    1.Family - Family size of the customer    \n    2.Education - education level of the customer\n    \nd.There are 5 independent variables:\n\n    1.Age:Age of the customer\n    2.Experience:Years of experience of the customer\n    3.Income:Annual income in dollars\n    4.CCAvg:Average credit card spending\n    5.Mortage:Value of House Mortgage\n    \ne.There are 5 binary category variables:\n\n    1.Personal Loan:Did this customer accept the personal loan offered in the last campaign?\n    2.Securities Account:Does the customer have a securities account with the bank?\n    3.CD Account:Does the customer have a certificate of deposit (CD) account with the bank?\n    4.Online:Does the customer use internet banking facilities?\n    5.Credit Card:Does the customer use a credit card issued by UniversalBank?\n\nf.And the Target variable is :Personal Loan","97349a69":"### The Majority is the customers  who do not have Personal loan have Securities Account.","ccac42f5":"### Customers with Personal Loan have less count in both the conditions. ","0f193e0b":"## e. 5 point summary of numerical attributes ","7c1caa50":"# 6.CountPlot","b0c0655a":"# 7.ScatterPlot","fe0c19aa":"# B.Naive Bayes","710839fb":"## 2. Read the data ","b27ace1a":"Almost all atributes are numeric. ","726a169b":"## f.Finding unique data ","f1fe3371":"There are 5000 customers.","aef1f0e3":"### Experience and Age gives a positive correlation ,as Experience increases Age also increases.","7e19f8d6":"### The customers having no CDAccount do not have Personal loan. ","b5534c72":"### And the customers with CDAccount almost have Personal Loan.  ","756f721a":"### There are so many outliers in each case. ","85a46aac":"## c.Check for the null values ","ffd62d70":"# 12.Applying classification models (Logistic, K-NN and Na\u00efve Bayes,SVM)","56d29923":"## d. Checking the presence of missing values ","106849ac":"# C.KNN","19157c20":"### b. Data type of each attribute ","5e4c3f2e":"# 8.DistPlot","9bbdcc66":"### Customers with Personal Loan have less count in both the conditions. ","63272f4c":"### 1.Here we can see \"Age\" feature is almost normally distributed where majority of customers are between age 30 to 60 years.Also we can see median is equal to mean.\n### 2.\"Experience\" feature is also almost normally distibuted and mean is also equal to median.But there are some negative values present which should be deleted, as Experience can not be negative.\n### 3.We can see for \"Income\" , \"CCAvg\" , \"Mortgage\" distribution is positively skewed.\n### 4.For \"Income\" mean is greater than median.Also we can confirm from this that majority of the customers have income between 45-55K.\n### 5.For \"CCAvg\" majority of the customers spend less than 2.5K and the average spending is between 0-10K.\n### 6.For \"Mortage\" we can see that almost 70% of the customers have Value of house mortgage less than 40K and the maximum value is 635K.\n### 7.Distributin of \"Family\" and \"Education\" are evenly distributed\n### 8.Income and CCAvg is moderately correlated.\n### 9.Experience and Age gives a positive correlation.\n### 10.Families with income less than 100K are less likely to take loan,than families with high income.\n### 11.The customers whose education level is 1 is having more income than the others.\n### 12.The customers with and without Personal Loan  have high Mortage.\n###  13.Families with income less than 100K are less likely to take loan,than families with high income.\n### 14.Ther is no that much impact on Personal Loan if we consider Family attribute. But the Family with size 3 is taking more Personal loan as compare to other family size. \n### 15.The Majority is the customers  who do not have Personal loan have Securities Account.\n### 16.The customers having no CDAccount do not have Personal loan. \n### 17.Customers with Personal Loan have less count in both the conditions. ","224021e3":"# 11.Conclusion from EDA:","0168bf5e":"# A.Logistic regression ","98386413":"### We can see with the help of colors of education level that more people are in the under graduate level.  ","9c918e78":"### We can see the customers who has taken the Personal Loan have the same Income levels. ","b5049bf6":"### Ther is no that much impact on Personal Loan if we consider Family attribute. ","b9318738":"### But the Family with size 3 is taking more Personal loan as compare to other family size. ","31fc0e08":"### But the customers with and without Personal Loan  have high Mortage.","71f04652":"# Conclusion:","43c2abf9":"### Also the Customers with education levels 2 and 3 have same income level with no Personal Loan. ","770f8b1a":"# 9.Calculate the correlation matrix","4b259102":"### 1.Here we can see \"Age\" feature is almost normally distributed where majority of customers are between age 30 to 60 years.Also we can see median is equal to mean.\n### 2.\"Experience\" feature is also almost normally distibuted and mean is also equal to median.But there are some negative values present which should be deleted, as Experience can not be negative.\n### 3.We can see for \"Income\" , \"CCAvg\" , \"Mortgage\" distribution is positively skewed.\n### 4.For \"Income\" mean is greater than median.Also we can confirm from this that majority of the customers have income between 45-55K.\n### 5.For \"CCAvg\" majority of the customers spend less than 2.5K and the average spending is between 0-10K.\n### 6.For \"Mortage\" we can see that almost 70% of the customers have Value of house mortgage less than 40K and the maximum value is 635K.\n### 7.Distributin of \"Family\" and \"Education\" are evenly distributed","4782de5d":"### a. Shape of the data","48472663":"### Here we can see that the customers with higher CCAvg have Personal Loan. ","52fea5ee":"# DATA SET: Bank_Personal_Loan_Modelling.csv","c26a26c1":"There are 52 records with negative experience.We have to clean it.","a4a40054":"###  Here the customers whose education level is 1 is having more income than the others.","1afdaaa1":"# 13.Comparison of different  Models:","df382a54":"Data Description:\nThe file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.\n\nDomain:Banking\n\nContext:This case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\nObjective:The classification goal is to predict the likelihood of a liability customer buying personal loans.","8b8d4a21":"### The customers who are spending average  credit card  with a median of 3800 dollar gives a higher probability of Personal loan,whereas the customers who are spending Lower credit card with a median of 1400 dollars are less likely to take a loan.","3b40cf3b":"## 3. Basic EDA","c99a5ffa":"### Measure of skewness  ","13434e0c":"52 records with negative experience","e8008f0b":"### Check if there are any records still present with negative Experience ","003f8854":"# 4.PairPlot","31c9f902":"## The classification goal is to predict the likelihood of a liability customer buying personal loans.\n\n## A bank wants a new marketing campaign; so that they need information about the correlation between the variables given in the dataset. \n\n## Here I used 4 classification models to study.\n\n## From the accuracy scores , it seems like \"KNN\" algorithm have the highest accuracy and stability.\n\n## But we can use SVM also as all the Kernels have good accuracy as well.","a4e4a9ae":"## 5.Boxplot ","63824f89":"# 10.Heatmap","9c3d7216":"## 1. Import the necessary libraries","6cdc6e62":"###  Families with income less than 100K are less likely to take loan,than families with high income"}}