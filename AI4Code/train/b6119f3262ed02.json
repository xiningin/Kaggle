{"cell_type":{"9118e87c":"code","99917462":"code","b1a0892f":"code","93a44c04":"code","029abeec":"code","72d4856b":"code","8450835d":"code","e06cb4ea":"code","80a76c42":"code","25d53d52":"code","4b37516a":"code","eb1b1a7b":"code","7bf16a52":"code","382c3c64":"code","0ac8919f":"code","bac0aca2":"code","eb13355a":"code","65bda5f7":"code","f5cf4c31":"markdown","1c99b0d4":"markdown","9dff6c30":"markdown","13ec8ab2":"markdown","1919185e":"markdown","cf1eeac8":"markdown","ce44db97":"markdown","669657aa":"markdown"},"source":{"9118e87c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk \nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import Sequential\nfrom keras.layers import Dense,LSTM,Embedding\nfrom sklearn.metrics import mean_squared_error\nfrom keras.utils.vis_utils import plot_model\nimport tensorflow as tf\nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n\ndef rmse(y,yhat):\n    return np.sqrt((y-yhat)**2)\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\nplt.rc('figure',figsize=(17,13))\n","99917462":"b_data = pd.read_csv('\/kaggle\/input\/highly-rated-children-books-and-stories\/children_stories.Csv',encoding=\"ISO-8859-1\" )\nb_data.head(3)\n","b1a0892f":"def get_min_age(sir):\n    if sir.find('-') != -1:\n        sp =  re.sub(' +', ' ', sir)\n        sp = sp.split(' ')[1]\n        sp = sp.strip()\n        return int(sp.split('-')[0])\n    else:\n        if sir.find('months'):\n            return 0\n        return int(sir.replace('+','').split(' ')[1])\ndef get_max_age(sir):\n    if sir.find('-') != -1:\n        sp =  re.sub(' +', ' ', sir)\n        sp = sp.split(' ')[1]\n        sp = sp.strip()\n        return int(sp.split('-')[1])\n    else:\n        return 99\n    \n#Preprocessing And Feature Engineering\nb_data.names = b_data.names.str.lower()\nb_data.desc = b_data.desc.str.lower()\nb_data['Min_Age'] = b_data.cats.apply(get_min_age)\nb_data['Max_Age'] = b_data.cats.apply(get_max_age)\nb_data['Book Name Nb Words'] = b_data.names.apply(lambda x: len(re.sub(' +', ' ', x).strip().split(' ')))\nb_data['Book Name Avg Word Length'] = b_data.names.apply(lambda x: np.mean(np.array([len(word) for word in (re.sub(' +', ' ', x).strip().split(' '))])))\nb_data['Range Length'] = b_data.Max_Age - b_data.Min_Age\n\n\n#Sentiment Analysis\nsid = SIA()\nb_data['sentiments']           = b_data['desc'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nb_data['Positive Sentiment']   = b_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nb_data['Neutral Sentiment']    = b_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nb_data['Negative Sentiment']   = b_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nb_data.drop(columns=['sentiments'],inplace=True)","93a44c04":"b_data.head(5)","029abeec":"f_data=b_data.copy()\nplt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Book Desriptions',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],bw=0.1)\nsns.kdeplot(f_data['Positive Sentiment'],bw=0.1)\nsns.kdeplot(f_data['Neutral Sentiment'],bw=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Book Desriptions',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(f_data['Positive Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(f_data['Neutral Sentiment'],bw=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","72d4856b":"min_age_g = b_data.groupby(by='Min_Age').mean()\nmin_age_g = min_age_g.reset_index()\nmin_age_g\n#plt.figure(figsize=(20,11))\nfig = ex.box(b_data,x='Min_Age',y='Book Name Avg Word Length')\nfig.update_layout(xaxis=dict(dtick=1),title='Average Book Name Word Length Over Min Target Ages')\nfig.show()","8450835d":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =b_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =b_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","e06cb4ea":"min_age_descs = []\nfor i in range(1,12):\n    min_age_descs.append(' '.join(b_data.query('Min_Age == {}'.format(i)).names))\n\n\nfig,axes = plt.subplots(3,4,figsize=(20,20),facecolor = None)\nfig.set_figwidth=(20)\nfig.set_figheight(18)\naxes[2,3].set_axis_off()\n\nr,c=0,0\nfor i in range(1,12):\n    axes[r,c].imshow(WordCloud(stopwords=STOPWORDS,background_color='white').generate(min_age_descs[i-1]))\n    axes[r,c].axis('off')\n    axes[r,c].set_title('Book Title WC Min Age = {}'.format(i),fontsize=16,fontweight='bold')\n    c+=1\n    if c == 4:\n        r+=1\n        c=0\n\nfig.tight_layout(pad=0.5)\n","80a76c42":"min_age_descs = []\nfor i in range(1,12):\n    min_age_descs.append(' '.join(b_data.query('Min_Age == {}'.format(i)).desc))\n\n\nfig,axes = plt.subplots(3,4,figsize=(20,20),facecolor = None)\nfig.set_figwidth=(20)\nfig.set_figheight(18)\naxes[2,3].set_axis_off()\n\nr,c=0,0\nfor i in range(1,12):\n    axes[r,c].imshow(WordCloud(stopwords=STOPWORDS,background_color='white').generate(min_age_descs[i-1]))\n    axes[r,c].axis('off')\n    axes[r,c].set_title('Description WC Min Age = {}'.format(i),fontsize=16,fontweight='bold')\n    c+=1\n    if c == 4:\n        r+=1\n        c=0\n\nfig.tight_layout(pad=0.5)\n","25d53d52":"desc_number_of_components = 210\ntitle_number_of_components = 225\n\n\ndesc_vecotrizer = CountVectorizer()\ndesc_matrix = desc_vecotrizer.fit_transform(b_data.desc)\ndesc_svd = TruncatedSVD(n_components=desc_number_of_components)\nsvd_desc_matrix = desc_svd.fit_transform(desc_matrix)\n\ntitle_vecotrizer = CountVectorizer()\ntitle_matrix = title_vecotrizer.fit_transform(b_data.names)\ntitle_svd = TruncatedSVD(n_components=title_number_of_components)\nsvd_title_matrix = title_svd.fit_transform(title_matrix)\n","4b37516a":"desc_ex_var = np.cumsum(desc_svd.explained_variance_ratio_)\n\ntr1 = go.Scatter(x=np.arange(0,len(desc_ex_var)),y=desc_ex_var)\ngo.Figure(data=[tr1],layout=dict(title='Description Counts Explained Variance Using {} Components'.format(desc_number_of_components),xaxis_title='# Componenets',yaxis_title='Total Variance Explained'))","eb1b1a7b":"title_ex_var = np.cumsum(title_svd.explained_variance_ratio_)\n\ntr1 = go.Scatter(x=np.arange(0,len(title_ex_var)),y=title_ex_var)\ngo.Figure(data=[tr1],layout=dict(title='Title Counts Explained Variance Using {} Components'.format(title_number_of_components),xaxis_title='# Componenets',yaxis_title='Total Variance Explained'))","7bf16a52":"model = Sequential(\n    [Dense(input_dim =desc_number_of_components,activation='relu',units=100),\n    Dense(activation='relu',units=50),\n    Dense(activation='relu',units=20),\n    Dense(activation='relu',units=2)]\n)\n\nmodel.compile(optimizer='adam',loss='mse',metrics=['mae'])","382c3c64":"plot_model(model,show_shapes=True, show_layer_names=True)","0ac8919f":"history = model.fit(svd_desc_matrix,b_data[['Min_Age','Max_Age']],epochs=550,verbose=False)","bac0aca2":"fig = go.Figure()\nH_diff = pd.DataFrame(history.history)\n\nfig.add_trace(go.Scatter(\n    x= np.arange(len(H_diff)),\n    y=H_diff.loss,\n    name='Loss'\n))\nfig.add_trace(go.Scatter(\n    x= np.arange(len(H_diff)),\n    y=H_diff.mae,\n    name='Mean Abs Error'\n))\n\nfig.update_layout(title='Model Training Evaluation',xaxis_title='Iterration',yaxis_title='Value')\nfig.show()","eb13355a":"predictions = pd.DataFrame({'Min_Age':model.predict(svd_desc_matrix)[:,0],'Max_Age':model.predict(svd_desc_matrix)[:,1]})\npredictions.Min_Age = np.round(predictions.Min_Age).astype(np.int)\npredictions.Max_Age = (np.round(predictions.Max_Age)).astype(np.int)","65bda5f7":"fig = make_subplots(rows=2,cols=1)\n\nfig.add_trace(go.Scatter(\n    x= np.arange(len(H_diff)),\n    y=b_data['Min_Age'].sort_values(),\n    name='Actual Min Age'\n),row=1,col=1)\nfig.add_trace(go.Scatter(\n    x= np.arange(len(H_diff)),\n    y=predictions['Min_Age'].sort_values(),\n    name='Predicted Min Age'\n),row=1,col=1)\n\n\nfig.add_trace(go.Scatter(\n    x= np.arange(len(H_diff)),\n    y=b_data['Max_Age'].sort_values(),\n    name='Actual Max Age'\n),row=2,col=1)\nfig.add_trace(go.Scatter(\n    x= np.arange(len(H_diff)),\n    y=predictions['Max_Age'].sort_values(),\n    name='Predicted Max Age'\n),row=2,col=1)\n\n\nfig.update_layout(title='Model Prediction Evaluation',xaxis_title='Iterration',yaxis_title='Value')\nfig.show()","f5cf4c31":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Exploratory Data Analysis<\/h1>\n","1c99b0d4":"### Loading The Data","9dff6c30":"<a id=\"2\"><\/a>\n\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Preprocessing And Feature Engineering<\/h1>\n","13ec8ab2":"# $\\text{Dense Relu Neural Network Construction}$","1919185e":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Libraires And Utilities<\/h1>\n","cf1eeac8":"<a id=\"5\"><\/a>\n\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Model Selection And Evaluation<\/h1>\n","ce44db97":"# $\\text{Dense Relu Neural Network Evaluation}$","669657aa":"<a id=\"4\"><\/a>\n\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Vectoriztion And Decomposition<\/h1>\n"}}