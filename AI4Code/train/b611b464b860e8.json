{"cell_type":{"c858b1b7":"code","0d3b5a96":"code","4f93be16":"code","8a1a4fd8":"code","88cd222a":"code","de2c968e":"code","d8073734":"code","d4895a78":"markdown"},"source":{"c858b1b7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","0d3b5a96":"#create dataset\nx_train = np.array([1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9])\ny_train = np.array([3, 3.5, 4.5, 6, 6.5, 6.8, 6.9, 7, 7, 7])\n\nx_test = np.array([1, 2.5, 4.5, 5.5, 7, 8])\ny_test = np.array([3.2, 4.5, 5, 7, 8, 7.7])\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x_train,y_train)\nplt.scatter(x_test,y_test)\nplt.show()","4f93be16":"x_train = x_train.reshape(-1,1) #convert into a column vector\nx_test= x_test.reshape(-1,1) #convert into a column vector","8a1a4fd8":"#Bulding a class for Polynomial Linear Regression Using Gradient Descent.\nclass PolyLinearRegression:\n    \n    def __init__(self, l_rate=0.001, iterations=10000, degree=2, lambda_value =0.2):  \n        self.l_rate = l_rate  \n        self.iterations = iterations  \n        self.degree = degree  #degree of polynomial features\n        self.lambda_value= lambda_value\n    \n    def scale(self, x):  #features scaling using z-score\n        x_scaled = x - np.mean(x, axis=0)\n        x_scaled = x_scaled \/ np.std(x_scaled, axis=0)\n        return x_scaled\n      \n    def transformer(self, x ): #Transform data to polynomial features\n        self.n = x.shape[0]  #number of training examples\n        \n        x_transformed = np.empty((self.n, 0))  #empty 2d-array\n        \n        for j in range( self.degree + 1 ) :\n            if j != 0 :\n                x_power = np.power( x, j )  #compute x to the power j\n                x_transformed = np.append( x_transformed, x_power, axis = 1 ) #fill the aary with x_power as column vectors\n        return x_transformed \n\n    def fit(self, x, y):  \n        self.cost = []  \n        self.theta = np.zeros((1 + self.degree)) # 1d array\n        \n        x = self.scale(x)  #preprocessing step 1\n        x_p = self.transformer(x)  ##preprocessing step 2\n        \n        for i in range(self.iterations):\n            y_pred = self.theta[0] + np.dot(x_p, self.theta[1:])\n            l2 = self.lambda_value * np.sum(np.square(self.theta[1:]))\n            mse = (1\/self.n) * np.sum((y_pred - y)**2)+l2  \n            self.cost.append(mse)  \n            \n            #Derivatives\n            d_theta1 = (2\/self.n) * np.dot(x_p.T, (y_pred - y)) - l2\n            d_theta0 = (2\/self.n) * np.sum(y_pred - y)\n            \n            #Values update\n            self.theta[1:] = self.theta[1:] - self.l_rate * d_theta1\n            self.theta[0] = self.theta[0] - self.l_rate * d_theta0                       \n        return self\n    \n    \n    def predict(self, x):  \n        \n        x = self.scale(x)  #preprocessing step 1\n        x_p = self.transformer(x)  #preprocessing step 2\n        \n        return self.theta[0] + np.dot(x_p, self.theta[1:])\n    ","88cd222a":"plr = PolyLinearRegression()  # creating object lr using the class PolyLinearRegression()\nplr.fit(x_train, y_train)  #call the method fitGD()\n\n# show  thetas\nplr.theta","de2c968e":"#predict the training examples\ny_pred_train_poly = plr.predict(x_train)\ny_pred_test_poly = plr.predict(x_test)\n\n#evaluate the learning model using training data\nmse_train = (1\/x_train.shape[0]) * np.sum((y_pred_train_poly - y_train)**2)\nrmse_train = np.sqrt(mse_train)\n\n#evaluate the learning model using testing data\nmse_test = (1\/x_test.shape[0]) * np.sum((y_pred_test_poly - y_test)**2)\nrmse_test = np.sqrt(mse_test)","d8073734":"#Plot the results:\nplt.figure(figsize=(10, 6))\nplt.scatter(x_train, y_train, label='Training points')\nplt.scatter(x_test, y_test, label='Testing points')\nplt.plot(x_train, y_pred_train_poly, label='Fitting Line')\nplt.legend(loc='upper left')\nplt.show()\nprint (\"RMSE Train = \", rmse_train)\nprint (\"RMSE Test = \", rmse_test)","d4895a78":"![ridge.png](attachment:ridge.png)"}}