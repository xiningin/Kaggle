{"cell_type":{"53f62924":"code","999c1c9b":"code","d127ed1c":"code","2c5d2826":"code","1fe31a01":"code","3c9b0ea0":"code","ac1490eb":"code","11dfe470":"code","c56a0014":"code","e0cdeae8":"code","98650a3b":"code","ab8d5076":"markdown","2f501277":"markdown","f774d516":"markdown","efbb9b1b":"markdown","ff34550a":"markdown","b08f02df":"markdown","00cdfdb7":"markdown","7caede6e":"markdown","657ed1e9":"markdown","dda7e8f3":"markdown","6f2a2925":"markdown"},"source":{"53f62924":"M               = 2000       # Number of words from frequency table. \nMODEL           = 'RIDGE_CV' # LR for linear regression, RIDGE for Ridge regression, ANN for neural network\nVALIDATION_SIZE = 0.2        # Controle train\/validation split\nMI_CUTOFF       = 0.01       # Discard featurs if mutual information with target is less than this value\n\n# Neural Network Hyperparameters\n      \nN_EPOCH      = 2**16      # Number of epochs for training\nLR           = 0.001      # Training learning Rate\nNBURN        = 2**6       # Number of epochs that are excluded from plots\nDROPOUT      = 0          # Controls whther dropout will be used\nFREQUENCY    = 2**4       # Plot average loss over FREQUENCY epochs\nWEIGHT_DECAY = 0.0001     # Training weight decay\n\n# Regression Hyperparameters\n\nN_POLY       = 2      # Degree of polynomial for regression\nEPSILON      = 0.001  # Prevents sample weights blowing up when standard_error==0\nALPHA        = 900.0   # Regularization strength for Ridge\n#ALPHAS       = [ 900.0,1000.0,1050.0, 1100.0, 1150.0, 1200.0, 1300.0, 1400.0,1500.0]\nALPHAS       = [ 700.0, 800.0, 850.0, 900.0,950.0, 1000.0]","999c1c9b":"from math                      import log\nfrom matplotlib.pyplot         import figure, title, xlabel, ylabel, scatter, legend, colorbar, plot, barh, yticks\nfrom numpy                     import arange\nfrom pandas                    import read_csv, Series\nfrom os                        import walk\nfrom os.path                   import join\nfrom random                    import gauss\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.linear_model      import LinearRegression, Ridge, RidgeCV\nfrom sklearn.model_selection   import train_test_split\nfrom sklearn.preprocessing     import PolynomialFeatures, StandardScaler\nfrom spacy                     import load\nfrom torch                     import FloatTensor, reshape, no_grad\nfrom torch.nn                  import Module, Linear,  MSELoss, Dropout\nfrom torch.nn.functional       import relu\nfrom torch.optim               import Adam\n","d127ed1c":"train_data    = None\ntest_data     = None\nunigram_freq  = None\nfor dirname, _, filenames in walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_name = join(dirname, filename)\n        if filename.startswith('train'):\n            train_data = read_csv(path_name)\n        if filename.startswith('test'):\n            test_data = read_csv(path_name)\n        if filename.startswith('unigram_freq'):\n            unigram_freq = read_csv(path_name)\n                       \ntrain_data.describe()\n","2c5d2826":"number_to_drop = len(unigram_freq.index) - M\nif number_to_drop>0:\n    drop_these_indices = unigram_freq.tail(number_to_drop).index\n    unigram_freq.drop(drop_these_indices, inplace = True)\n    \nunigram_freq.set_index('word', inplace=True) # so we can index on words\nunigram_freq.describe()","1fe31a01":"# TokenWalker\n#\n# Used to walk throught the tokens in a document, and perform\n# processing determined by subclasses\n\nclass TokenWalker:\n    # Initialization\n    # Parameters:\n    #     Actions         Dict of key\/value pairs for handling tokens\n    #     default_action  Action to be performed if key not found in Actions\n    def __init__(self,doc,\n                Actions        = {},\n                default_action = lambda token: None):\n        self.doc            = doc\n        self.Actions        = Actions\n        self.default_action = default_action\n\n    # get_key\n    #\n    # Calculate key for combination of pos and tag\n    \n    def get_key(self,pos,tag=''):\n          return f'{pos}:{tag}'\n\n    # get_action\n    #\n    # Look up action to be performed for token\n    def get_action(self,token):\n        # First try for exact match on pos and tag\n        key = self.get_key(token.pos_,token.tag_)\n        if key in self.Actions:\n            return self.Actions[key]\n        \n        # Otherwise look for a match on pos only\n        key = self.get_key(token.pos_)\n        if key in self.Actions:\n            return self.Actions[key]\n\n        return self.default_action\n\n    # walk throught the tokens in a document\n    def walk(self):\n        for token in self.doc:\n             self.get_action(token)(token)\n\n\n# SentenceCounter\n#\n# Walk through document and count sentences, i.e. count full stops\n\nclass SentenceCounter(TokenWalker):\n    def __init__(self,doc,tag='.'):\n            super().__init__(doc,\n                           Actions = {self.get_key('PUNCT',tag): lambda token: self.incr()} )\n            self.count = 0\n\n    # incr\n    # Count each full stop\n    def incr(self):\n        self.count += 1\n\n    # get\n    #\n    # Note check that there is at least one sentence.\n    # Program was falling over on hidden test test set with a disvsion by zero before I added check\n    \n    def get(self):\n        self.walk()\n        return max(1,self.count)\n\n# WordCounter\n#\n# Walk through document and count words\n\nclass WordCounter(TokenWalker):\n    def __init__(self,doc):\n        super().__init__(doc,\n                         Actions = {\n                           self.get_key('PUNCT')      : lambda token: None,\n                           self.get_key('PART','POS') : lambda token: None,\n                         },\n                         default_action = lambda token: self.incr())\n        self.count = 0\n\n    def incr(self):\n        self.count += 1\n\n    def get(self):\n        self.walk()\n        return self.count\n\n# SyllableCounter\n#\n# Walk through document and count syllables\n\nclass SyllableCounter(WordCounter):\n    def __init__(self,doc):\n        super().__init__(doc)\n        self.default_action = lambda token: self.incr(str(token).lower())\n\n    def incr(self,word):\n        self.count += max(1,self.countSyllables(word))\n    \n    def countSyllables(self,word): # https:\/\/stackoverflow.com\/questions\/405161\/detecting-syllables-in-a-word\n        vowels       = \"aeiouy\"\n        numVowels    = 0\n        lastWasVowel = False\n        for wc in word:\n            foundVowel = False\n            for v in vowels:\n                if v == wc:\n                    if not lastWasVowel: numVowels+=1   #don't count diphthongs\n                    foundVowel = lastWasVowel = True\n                    break\n            if not foundVowel:  #If full cycle and no vowel found, set lastWasVowel to false\n                lastWasVowel = False\n        if len(word) > 2 and word[-2:] == \"es\": #Remove es - it's \"usually\" silent (?)\n            numVowels-=1\n        elif len(word) > 1 and word[-1:] == \"e\":    #remove silent e\n            numVowels-=1\n        return numVowels\n\n# ClauseCounter\n#\n# Walk through document and count clauses\n# Use verbs as a proxy\n\nclass TagCounter(TokenWalker):\n    def __init__(self,doc,tag='VERB'):\n        super().__init__(doc,\n                         Actions = {self.get_key(tag): lambda token: self.incr()} )\n        self.count = 0\n\n    def incr(self):\n        self.count += 1\n\n    def get(self):\n        self.walk()\n        return self.count \n    \n# WordCounter\n#\n# Walk through document and calculate average rank in frequency table\n\nclass FreqCounter(TokenWalker):\n    def __init__(self,doc):\n        super().__init__(doc,\n                         Actions = {\n                           self.get_key('NOUN')     : lambda token: self.incr(token),\n                           self.get_key('VERB')     : lambda token: self.incr(token), \n                           self.get_key('ADJ')      : lambda token: self.incr(token),\n                           self.get_key('ADV')      : lambda token: self.incr(token),\n                         })\n        self.count = 0\n        self.freq  = 0\n        \n    def incr(self,token):\n        word = str(token).lower()\n        rank = unigram_freq.index.get_loc(word) if word in unigram_freq.index else M\n            \n        self.freq  += log(rank+1) #unigram_freq.loc[word,'freq']\n        self.count += 1\n\n    def get(self):\n        self.walk()\n        return  self.freq\/self.count if self.count>0 else 0 \n    \ndef get_stopwords(doc,lemma=None):\n    return [token.lemma_ for token in doc if token.is_stop and (lemma==None or lemma == token.lemma_)]\n\n# TagCounter\n#\n# Count number of distinct tag types in document,\n# either scaled by length of document or raw.\n\nclass UniqueTagCounter:\n    def __init__(self,doc,scaled=False):\n        self.doc      = doc\n        self.tags     = set()\n        self.n_tokens = 0\n        self.scaled   = scaled\n        \n    def walk(self):\n        for token in self.doc:\n            self.tags.add(token.tag_)\n            self.n_tokens += 1\n            \n    def get(self):\n        return len(self.tags)\/(self.n_tokens if self.scaled else 1)","3c9b0ea0":"FeatureTable = [ \n    ('word_count',           lambda doc: WordCounter(doc).get()),\n    ('sentence_count',       lambda doc: SentenceCounter(doc).get()),\n    ('sentence_length',      lambda doc: WordCounter(doc).get()\/SentenceCounter(doc).get()),\n    ('unique_tags',          lambda doc: UniqueTagCounter(doc).get()),\n    ('clauses',              lambda doc: TagCounter(doc).get()),\n    ('adjs',                 lambda doc: TagCounter(doc,tag='ADJ').get()),\n    ('adps',                 lambda doc: TagCounter(doc,tag='ADP').get()),\n    ('advs',                 lambda doc: TagCounter(doc,tag='ADV').get()),\n    ('auxen',                lambda doc: TagCounter(doc,tag='AUX').get()),\n    ('conjs',                lambda doc: TagCounter(doc,tag='CONJ').get()),\n    ('dets',                 lambda doc: TagCounter(doc,tag='DET').get()),\n    ('intjs',                lambda doc: TagCounter(doc,tag='INTJ').get()),\n    ('nouns',                lambda doc: TagCounter(doc,tag='NOUN').get()),\n    ('nums',                 lambda doc: TagCounter(doc,tag='NUM').get()),\n    ('parts',                lambda doc: TagCounter(doc,tag='PART').get()),\n    ('prons',                lambda doc: TagCounter(doc,tag='PRON').get()),\n    ('proper_nouns',         lambda doc: TagCounter(doc,tag='PROPN').get()),\n    ('puncts',               lambda doc: TagCounter(doc,tag='PUNCT').get()),\n    ('sconjs',               lambda doc: TagCounter(doc,tag='SCONJ').get()),\n    ('syms',                 lambda doc: TagCounter(doc,tag='SYMS').get()),\n    ('xs',                   lambda doc: TagCounter(doc,tag='X').get()),\n    ('syllables',            lambda doc: SyllableCounter(doc).get()),\n    ('frequencies',          lambda doc: FreqCounter(doc).get()),\n    ('commas',               lambda doc: SentenceCounter(doc,tag=',').get()),\n    ('semicolons',           lambda doc: SentenceCounter(doc,tag=';').get()),\n    ('stopword_count',       lambda doc: len(get_stopwords(doc))),\n    ('stopword_count_of',    lambda doc: len(get_stopwords(doc,lemma='of'))),\n    ('stopword_count_with',  lambda doc: len(get_stopwords(doc,lemma='with'))),\n    ('stopword_count_the',   lambda doc: len(get_stopwords(doc,lemma='the'))),\n    ('stopword_count_in',    lambda doc: len(get_stopwords(doc,lemma='in'))),\n    ('stopword_count_as',    lambda doc: len(get_stopwords(doc,lemma='as'))),\n    ('stopword_count_which', lambda doc: len(get_stopwords(doc,lemma='which'))),\n    ('stopword_count_by',    lambda doc: len(get_stopwords(doc,lemma='by'))),\n    ('stopword_count_to',    lambda doc: len(get_stopwords(doc,lemma='to'))),\n    ('stopword_count_and',   lambda doc: len(get_stopwords(doc,lemma='and'))),\n    ('stopword_count_at',    lambda doc: len(get_stopwords(doc,lemma='at'))),\n    ('stopword_count_just',  lambda doc: len(get_stopwords(doc,lemma='just'))),\n    ('stopword_count_one',   lambda doc: len(get_stopwords(doc,lemma='one'))),\n    ('stopword_count_boy',   lambda doc: len(get_stopwords(doc,lemma='boy'))),\n    ('stopword_count_live',  lambda doc: len(get_stopwords(doc,lemma='live'))),\n    ('stopword_count_snow',  lambda doc: len(get_stopwords(doc,lemma='snow'))),\n    ('stopword_count_tree',  lambda doc: len(get_stopwords(doc,lemma='tree'))),\n    ('stopword_count_out',   lambda doc: len(get_stopwords(doc,lemma='out'))),\n    ('stopword_count_think', lambda doc: len(get_stopwords(doc,lemma='think'))),\n    ('stopword_count_away',  lambda doc: len(get_stopwords(doc,lemma='away'))),\n    ('stopword_count_right', lambda doc: len(get_stopwords(doc,lemma='right')))\n    \n]\n\n# https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240064\nnlp      = load(\"en_core_web_sm\")   # Initialize English Language\n\nFeatures = [name for (name,_) in FeatureTable]   # Extract list of feature names\n\n# get_features\n#\n# Calculate values for feature in specified row\n\ndef get_features(row):\n    doc            = nlp(row['excerpt'])\n    return tuple([extract(doc) for (_,extract) in FeatureTable])\n\n\n\n# assign_features\n#\n# Create and populate new columns for features to be used in regression\n\ndef assign_features(dataset):\n    dataset[Features] = dataset.apply(get_features,\n                                      axis        = 1,\n                                      result_type = 'expand')\n\n\nassign_features(train_data)\ntrain_data, validation_data = train_test_split(train_data,\n                                               test_size=VALIDATION_SIZE) \n","ac1490eb":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y, discrete_features=False)\n    mi_scores = Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ny         = train_data.target\nX         = train_data[Features]\nmi_scores = make_mi_scores(X, y)\nmi_scores = mi_scores.sort_values(ascending=True)\n \n\ndef plot_mi_scores(scores,lifters=[],mi_cutoff=0.03):\n    colours = ['r' if len(scores)-i>len(lifters) else 'b' for i in range(len(scores))]\n    width  = arange(len(scores))\n    ticks  = list(scores.index)\n    barh(width, scores,color=colours)\n    yticks(width, ticks)\n    title(f'Mutual Information Scores: cutoff={mi_cutoff}')\n\n\nfigure(dpi=100, figsize=(8, 5))\n\nlifters = list(mi_scores[mi_scores.ge(MI_CUTOFF)].index)\nleaners = list(mi_scores[mi_scores.lt(MI_CUTOFF)].index)\nplot_mi_scores(mi_scores,lifters=lifters,mi_cutoff=MI_CUTOFF)\n\ntrain_data.drop(leaners,axis=1)\nvalidation_data.drop(leaners,axis=1)\n","11dfe470":"class Model:    # Generic parent for all models\n    def __init__(self):\n        pass\n    \n    def train(self,data):\n        pass\n    \n    def predict(self,data,output='submission.csv'):\n        pass\n\n    # save\n    #\n    # Save predictions to submission file\n    # Snarfed from Bishwajit Shil's notebook\n    \n    def save(self,data,prediction,output='submission.csv'):\n        xsub           = data[[\"id\"]].copy()\n        xsub[\"target\"] = prediction\n        xsub.to_csv(output, index = False)\n        \nclass RegressionModel(Model):   # Model to perform regression\n    \n    def __init__(self,\n                 degree = 1,\n                 alpha  = 0,\n                 model  = 'LR'):\n        super().__init__()\n        self.degree = degree\n        self.alpha  = alpha\n        self.model  = None\n        if model  == 'LR':\n            self.model  = LinearRegression() \n        if model  == 'RIDGE':\n            self.model = Ridge(alpha=self.alpha)\n        if model =='RIDGE_CV':\n            self.model = RidgeCV(alphas=ALPHAS) \n        self.scaler = StandardScaler()\n\n    def train(self,data,training=True):\n        y             = data.target\n        X             = data[Features]\n        weights       = 1\/(EPSILON + data.standard_error.pow(2))\n        self.poly_reg = PolynomialFeatures(degree=self.degree)\n        if self.degree>1:\n            X        = self.poly_reg.fit_transform(X)\n        X             = self.scaler.fit_transform(X)\n        if training:           \n            model_cv = self.model.fit(X, y)\n#             print (f'alpha = {model_cv.alpha_}')\n    \n        return self.model.predict(X),y,self.model.score(X, y,weights)\n        \n    def predict(self,data,output='submission.csv'):\n        assign_features(data)\n        X             = data[Features]\n        if self.degree>1:\n            X        = self.poly_reg.fit_transform(X)\n        X             = self.scaler.fit_transform(X)\n        self.save(data, self.model.predict(X), output=output)\n\n\nclass MLP(Model):\n    # Scaler \n    # Used by neural network to scale data into (0,1)\n    class Scaler:\n        def __init__(self,data=train_data.target,tolerance = 0.1):\n            self.min_y = min(data) - tolerance\n            self.max_y = max(data) + tolerance\n            self.range = self.max_y - self.min_y\n            print (f'min={self.min_y}, max={self.max_y}, range={self.range}')\n        \n        # scale \n        # scale data into (0,1)\n        def scale(self,y):\n            return (y-self.min_y)\/self.range\n        \n        #elacs\n        # Restore original value from scaled\n        def elacs(self,y):\n            return self.range*y + self.min_y\n        \n      \n    class ANN(Module):\n        def __init__(self,input_size=6,hidden=[32,16,12]):\n            super().__init__()\n            self.fc1    = Linear(in_features  = input_size,\n                                 out_features = hidden[0])\n            self.fc2    = Linear(in_features  = hidden[0],\n                                 out_features = hidden[1])\n            self.fc3    = Linear(in_features  = hidden[1],\n                                 out_features = hidden[2])\n            self.output = Linear(in_features  = hidden[2],\n                                 out_features = 1)\n            if DROPOUT>0:\n                self.dropout = Dropout(DROPOUT)\n\n        def forward(self, x):\n            x = relu(self.fc1(x))\n            if DROPOUT>0:\n                x = self.dropout(x)\n            x = relu(self.fc2(x))\n            if DROPOUT>0:\n                x = self.dropout(x)\n            x = relu(self.fc3(x))\n            x = self.output(x)\n            return x\n        \n        def predict(self,X,scaler):\n            preds = []\n            with no_grad():\n                for val in X:\n                    y_hat = self.forward(val)\n                    preds.append(scaler.elacs(y_hat[0].item()))\n   \n            return preds\n        \n    def __init__(self):\n        super().__init__()\n        self.model     = self.ANN(len(Features))\n        self.criterion = MSELoss()\n        self.optimizer = Adam(self.model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)\n        self.scaler   =  Scaler(tolerance=max(train_data.standard_error))\n        self.scaler.scale(train_data.target).describe()\n        \n    def train(self,data,nburn=NBURN):\n        \n        X        = data[Features]\n        X_train  = FloatTensor(X.values)\n        loss_arr = []\n        loss_sum = 0\n\n        for epoch in range(N_EPOCH):\n            sample = []\n            for target,sigma in zip(data.target,data.standard_error):\n                sample.append(self.scaler.scale(gauss(target,sigma)))\n            y_train  = reshape(FloatTensor(sample),(len(data.target),1))\n            y_hat    = self.model.forward(X_train)\n            loss     = self.criterion(y_hat, y_train)\n            if epoch>NBURN:\n                loss_sum += loss\n                if (epoch - NBURN) % FREQUENCY==0:\n                    loss_arr.append(loss)\n                    loss_sum=0\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n        figure(figsize=(10,10))\n        plot([FREQUENCY*i for i in range(len(loss_arr))],loss_arr,\n             label = f'Mean Loss over {FREQUENCY}  epochs')\n        title(f'Dropout = {DROPOUT}')\n        legend()\n        xlabel('Epoch')\n        return scaler.elacs(y_hat.detach().numpy()), data.target, loss\n  \n    def predict(self,data,output='submission.csv'):\n        assign_features(data)\n        data.drop(leaners,axis=1)\n        X       = data[Features]\n        X_test  = FloatTensor(X.values)\n        self.save(data,\n                  self.model.predict(X_test,self.scaler),\n                  output=output)\n\ndef createModel():\n    if MODEL == 'ANN':\n        return MLP()\n    \n    if MODEL == 'LR':  \n        return RegressionModel(degree=N_POLY,model=MODEL)\n\n    if MODEL == 'RIDGE' or MODEL == 'RIDGE_CV':  \n        return RegressionModel(degree=N_POLY,alpha=ALPHA,model=MODEL)\n    \nmodel = createModel()","c56a0014":"def plot_predictions(data,predictions,\n                     y          = [],\n                     score      = None,\n                     ax         = None,\n                     plot_title = ''):\n    r2 = r'$R^2$'   # Quick hack to make latex work properly\n    scatter1 =ax.scatter(data['target'],predictions,\n                          c     = data['standard_error'],\n                          cmap  = 'viridis',\n                          label = f'Predictions. {r2}={score:.6f}')\n    ax.scatter(data['target'],data['target'],\n            c     = 'r',\n            s     = 2,\n            label = 'Ideal')\n    \n    ax.set_xlabel('Target')\n    ax.set_ylabel('Predicted')\n    ax.set_title(plot_title)\n    ax.legend()\n    return scatter1\n\nfig  = figure(figsize=(12,12))\naxes = fig.subplots(nrows=1,ncols=2)\ntrain_predictions,y_train,score_train    = model.train(train_data)\nplot_predictions(train_data,train_predictions,\n                 y          = y_train,\n                 score      = score_train,\n                 ax         = axes[0],\n                 plot_title = 'Training')\nvalidation_predictions,y_validation,score_validation    = model.train(validation_data,\n                                                                      training=False)\nscatter1 = plot_predictions(validation_data,validation_predictions,\n                            y          = y_validation,\n                            score      = score_validation, \n                            ax         = axes[1],\n                            plot_title = 'Validation')\ncolorbar(scatter1).set_label('Standard Error', rotation=270)","e0cdeae8":"model.predict(test_data)\n\n\n","98650a3b":"fig = figure(figsize=(15,15))\naxs = fig.subplots(nrows = len(Features)-1,\n                   ncols = len(Features)-1)\nr2 = r'$R^2$'   # Quick hack to make latex work properly\nfor i in range(len(Features)-1):\n    for j in range(1,len(Features)):\n        axs[i][j-1].xaxis.set_ticks([])\n        axs[i][j-1].yaxis.set_ticks([])\n        axs[i][j-1].set_frame_on(False) \n\nfor i in range(len(Features)-1):\n    for j in range(i+1,len(Features)):\n        y           = train_data[Features[j]]\n        X           = train_data[Features[i]].values.reshape(-1,1)\n        interaction = LinearRegression()\n        interaction.fit(X, y)\n        axs[i][j-1].scatter(train_data[Features[i]],interaction.predict(X),\n                           c     = 'r',\n                           s     = 1,\n                           label = f'{r2}={interaction.score(X, y):.6f}')\n        axs[i][j-1].scatter(train_data[Features[i]], train_data[Features[j]],\n                           c = 'b',\n                           s = 1)\n        axs[i][j-1].set_xlabel(Features[i])\n        axs[i][j-1].set_ylabel(Features[j])\n        axs[i][j-1].legend()\n","ab8d5076":"## Predict result of tests and write submission file\n\n","2f501277":"# Hyperparameters","f774d516":"# Create model to be used for predicting targets","efbb9b1b":"# Feature Engineering\n\nCompute [Mutual Information](https:\/\/www.kaggle.com\/ryanholbrook\/mutual-information) between each feature and target, then partition features into two groups, `lifters` and `leaners`, by comparing mutual information with `mi_cutoff`. Drop the leaners out.","ff34550a":"# Load Data\n\n## Data Dictionary\n\n|Train|Public Test|Hidden Test|Description|\n|--------------|--------------|----------|----------------------------------------------------|\n|id|id|id|Unique ID for excerpt|\n|url_legal|url_legal|- |URL of source (Omitted from some records in the test set--see [note](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/238670#1306025))|\n|license|license |-|License of source material (Omitted from some records in the test set--see [note](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/238670#1306025))|\n|excerpt|excerpt|excerpt|Text for predicting readability|\n|target|-|-|Readability|\n|standard_error|-|-|Measure of spread of scores among multiple raters for each excerpt|\n","b08f02df":"# Feature extractors\n\nThis is a collection of classes, each used to walk through a document and extract one feature.","00cdfdb7":"# First attempt at CommonLit Readability\n\n## Purpose\n\n|Description|Status|\n|----------------------------------------------------------------------------|-----------------|\n|Verify that I can write a submission file|See `model.save(...)`, below|\n|Understand the submission errors I have been experiencing|Any exception can stop submission.csv being generated. Be careful of anything that might throw an exception (e.g. division), because the private test data may have different characteristics.|\n|Experiment with regression methods|So far Ridge Regession works best for me. I appear to have hit a wall at a score of 0.777, so I don't plan to take this notebook further.|\n|Experiment with Features|See section headed: Compute Features|\n|See whether a simple neural network can produce a better match than regression|I've used a simple network, which has been disappointing.|\n|Investigate Standard Error|I'm using standard error to generate weights for regression, and samples for the neural net.|\n\n## Acknowledgements\n\nI'd like to thank the following people who made their notebooks available for learning.\n\n| Author | Notebook | Remarks |\n| --------------- | ---------------------- | --- |\n| Bishwajit Shil | [submission score 0.62](https:\/\/www.kaggle.com\/jitshil143\/submission-score-0-62)  | Creation of submission file\n|Manish KC|[Text Pre-processing & Data Wrangling](https:\/\/www.kaggle.com\/manishkc06\/text-pre-processing-data-wrangling)|Introduced me to spaCy|\n\n## Notes from Discussions\n\n1. [The reading criterion was developed using a Bradley-Terry model.](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/236402)\n\n1. [So the mean target value of that excerpt is chosen to coincide with the origin of the scale for each and every rater, so that the excerpt's standard error is also zero. This implies that the standard errors of all the other excerpts are inflated by the \"true\" standard error of the baseline excerpt.](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/236403)\n \n1. [Higher scores are considered easier to read than low ones.](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/236538)","7caede6e":"# Prepare table of frequencies for words\n\nWe discard all but the most frequent words, assuming that all really infrequent words\nare equally infrequent.","657ed1e9":"# Plot Features pairwise\n\n## Purpose\n\nSee whether features are redundant be examing pairwise correlations","dda7e8f3":"# Compare predictions with training data","6f2a2925":"# Compute Features\n\nTeature extraction is driven by `FeatureTable`, a list of feature names, each paired with a function\nthat will generate the value of the feature from a text extract"}}