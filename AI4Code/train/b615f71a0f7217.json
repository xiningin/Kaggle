{"cell_type":{"5fa2fb76":"code","ce31f6ef":"code","20676e4a":"code","d067a58b":"code","132f1439":"code","f90175c0":"code","1211f33e":"code","17fe5d1d":"code","8d045937":"code","7ecee295":"code","09313e9c":"code","c2ad9d32":"code","c86e5f50":"code","41cd7635":"code","b3d4f8c9":"code","ef2bf0b1":"code","fe0a34df":"code","72994534":"code","0e9f1e0e":"code","aaa2081f":"code","a50b9b84":"code","7fd5e2be":"code","00948d87":"code","cbf40784":"code","1238b58e":"code","ca2da865":"code","b7e90a23":"code","ccb3988b":"code","b9269b61":"code","a63e65df":"code","c0d29ebc":"code","40e2646e":"code","3fc0130d":"code","1c120df2":"code","89e701c5":"code","014b8125":"code","ceaf82f5":"code","2a1a778b":"code","d51bfc80":"code","82261b89":"code","77cebf82":"code","3364255b":"code","0f718a7b":"code","098f1daf":"code","21e41a3f":"code","1d9d5f15":"code","15f61970":"code","58803c09":"code","574b6ae0":"code","81dbf0cf":"code","3e8b1c6b":"code","bcac247b":"code","084b1cbc":"code","c64f7c21":"code","bff0977c":"code","4d5da7a0":"code","3f172a7c":"code","c8210f7e":"code","41cd744a":"code","7b0df029":"code","f6f4e67b":"code","34fb756e":"code","13610d2c":"code","f2f63245":"code","7f0e0f6c":"code","d13f92b4":"code","68d44a2d":"code","ba1128ac":"code","a60b02a2":"code","7df2790f":"code","a9a105e6":"code","1ba671eb":"code","8e02d17b":"code","66a16bf2":"code","5b8a8426":"code","b1e0b0aa":"code","0d772492":"code","8152672a":"code","15e1be97":"code","b6cc91ed":"code","abdea84e":"code","1fd07c95":"code","ad94a201":"code","f9b24dbf":"code","16f4f7fe":"code","599066d0":"code","d7d2ae18":"code","31427ac8":"code","0a955fdb":"code","27cc9a8c":"code","d93c08a0":"markdown","a381d823":"markdown","f3f2952e":"markdown","701e3d14":"markdown","01a22ed7":"markdown","829b16e4":"markdown","38789f9b":"markdown","4de14290":"markdown","f4aba03a":"markdown"},"source":{"5fa2fb76":"#2.1 Import python libraries to Load Data Set and downloaded to same directory in which this python file saved.\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ce31f6ef":"pd.set_option('display.max_columns',500)","20676e4a":"#Load Data from csv file\nbank_df = pd.read_csv('..\/input\/bank-additional-full.csv', sep = ';')\n#bank_df = pd.read_csv('bank-additional-full_v1.csv')\nbank_df.info()","d067a58b":"bank_df.head(5)","132f1439":"#3.1 Identify Input feature Type - Categorical or Numerical \n#Input Numerical columns (10) - age duration campaign pdays previous emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed\n#Input Categorical Columns (10) - Job,marital,education,default,housing,loan ,contact,month ,day_of_week,poutcome          \n\n#Check if any missing value in any of the feature\nbank_df.isnull().values.any()","f90175c0":"#3.2 Convert Target feature value to 0 and 1\nbank_df['y'].unique()\n","1211f33e":"bank_df['y'] = np.where(bank_df['y']== 'yes',1,0)","17fe5d1d":"bank_df['y'] = bank_df['y'].astype(np.int64)","8d045937":"# Lets Analyse Categorical input varaible\n# For Better Predictive model lets avoid unknown value for marital , job and education. - To DO\n# Remove records with value - unknown for education, job and marital and assume it is mandatory for predictive model - To DO\n#bank_df = bank_df[(bank_df['marital'] != 'unknown')]\n#bank_df = bank_df[(bank_df['education'] != 'unknown')]\n#bank_df = bank_df[(bank_df['job'] != 'unknown')]","7ecee295":"#Visualize the Categorical feature distribution and relation with target variable. let\u2019s create a User Defined function\ndef Categorical_Grapgh(data,catfeature,distributionName):\n    fig, ax = plt.subplots()\n    fig.set_size_inches(20, 8)\n    sns.countplot(x = catfeature, data = data)\n    ax.set_xlabel(catfeature, fontsize=15)\n    ax.set_ylabel('Count', fontsize=15)\n    ax.set_title(distributionName, fontsize=15)\n    ax.tick_params(labelsize=15)\n    sns.despine()","09313e9c":"Categorical_Grapgh(bank_df,'job','job Distribution')\nCategorical_Grapgh(bank_df,'marital','maritial Distribution')\nCategorical_Grapgh(bank_df,'education','education Distribution')\nCategorical_Grapgh(bank_df,'default','default Distribution')\nCategorical_Grapgh(bank_df,'housing','housing Distribution')\nCategorical_Grapgh(bank_df,'loan','loan Distribution')\nCategorical_Grapgh(bank_df,'contact','contact Distribution')\nCategorical_Grapgh(bank_df,'month','month Distribution')\nCategorical_Grapgh(bank_df,'day_of_week','day_of_week Distribution')\nCategorical_Grapgh(bank_df,'poutcome','poutcome Distribution')","c2ad9d32":"#Check How Categorize variables correlated with Target Variables and How it impacted.\nfrom scipy import stats","c86e5f50":"#Check How Job Type , Education are correlated with Target Variable\nbank_df.groupby(['job','y']).y.count()\n#Admin are more interested in Term Deposit.","41cd7635":"F, p = stats.f_oneway(bank_df[bank_df.job=='admin.'].y,\n                      bank_df[bank_df.job=='blue-collar'].y,\n                      bank_df[bank_df.job=='entrepreneur'].y,\n                      bank_df[bank_df.job=='housemaid'].y,\n                      bank_df[bank_df.job=='management'].y,\n                      bank_df[bank_df.job=='retired'].y,\n                      bank_df[bank_df.job=='self-employed'].y,\n                      bank_df[bank_df.job=='services'].y,\n                      bank_df[bank_df.job=='student'].y,\n                      bank_df[bank_df.job=='technician'].y,\n                      bank_df[bank_df.job=='unemployed'].y\n                      )\nprint(F)","b3d4f8c9":"# Seems JOB has little impact on target variable","ef2bf0b1":"bank_df.groupby(['marital','y']).y.count()\n#married people are more interested in Term Deposit","fe0a34df":"bank_df.groupby(['job','marital','y']).y.count()\n# And Admin - married people are more interested in Term Deposit.","72994534":"bank_df.groupby(['contact','y']).y.count()\n#Contact field has good correlation with Target variable. Since we have two observation for contact lets convert this to binary format. cellular -1 and telephone=0","0e9f1e0e":"F, p = stats.f_oneway(bank_df[bank_df.contact=='telephone'].y,\n                      bank_df[bank_df.contact=='cellular'].y)\nprint(F)","aaa2081f":"F, p = stats.f_oneway(bank_df[bank_df.day_of_week=='mon'].y,\n                      bank_df[bank_df.day_of_week=='tue'].y,\n                      bank_df[bank_df.day_of_week=='wed'].y,\n                      bank_df[bank_df.day_of_week=='thu'].y,\n                      bank_df[bank_df.day_of_week=='fri'].y)\nprint(F)\n","a50b9b84":"bank_df.groupby(['day_of_week','y']).age.count()","7fd5e2be":"# day_of_week - No significant correlation with Target variable","00948d87":"F, p = stats.f_oneway(bank_df[bank_df.loan=='no'].y,\n                      bank_df[bank_df.loan=='yes'].y,\n                      bank_df[bank_df.loan=='unknown'].y)\nprint(F)\n","cbf40784":"# Loan - No correlation with Target variable","1238b58e":"bank_df.groupby(['loan','y']).age.count()","ca2da865":"bank_df.groupby(['default','y']).age.count()","b7e90a23":"F, p = stats.f_oneway(bank_df[bank_df.default=='no'].y,\n                      bank_df[bank_df.default=='yes'].y,\n                      bank_df[bank_df.default=='unknown'].y)\nprint(F)","ccb3988b":"bank_df.groupby(['housing','y']).age.count()","b9269b61":"F, p = stats.f_oneway(bank_df[bank_df.housing=='no'].y,\n                      bank_df[bank_df.housing=='yes'].y,\n                      bank_df[bank_df.housing=='unknown'].y)\nprint(F)","a63e65df":"# housing - No significant Relation with Target Variable","c0d29ebc":"bank_df.groupby(['poutcome','y']).age.count()","40e2646e":"F, p = stats.f_oneway(bank_df[bank_df.poutcome=='success'].y,\n                      bank_df[bank_df.poutcome=='failure'].y,\n                      bank_df[bank_df.poutcome=='nonexistent'].y)\nprint(F)","3fc0130d":"# poutcome - Good Relation with Target Variable","1c120df2":"bank_df.groupby(['month','y']).age.count()","89e701c5":"#Convert Categorical column to Continues Type. use Label conceding for ordinal category and one hot encoding for Nominal\nprint(bank_df.job.unique())\n#job  - Nominal\nprint(bank_df.marital.unique())\n#Maritial Nominal\nprint(bank_df.education.unique())\n#education - Ordinary\nprint(bank_df.default.unique())\n# seems Ordinary if we put -1 for unknown\nprint('housing', bank_df.housing.unique())\n# seems Ordinary if we put -1 for unknown\nprint(bank_df.loan.unique())\n# seems Ordinary if we put -1 for unknown\nprint(bank_df.contact.unique())\n# Nominal\nprint(bank_df.month.unique())\n#ordinal\nprint(bank_df.day_of_week.unique())\n#ordinal\nprint(bank_df.poutcome.unique())\n#ordinal if we put -1 for nonexistent","014b8125":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_X = LabelEncoder()\n\nbank_df = pd.get_dummies(bank_df,columns=['job','marital','education','default','housing','loan'])\nbank_df['month'].replace(['mar','apr','may','jun','jul','aug','sep','oct','nov','dec'], [1,2,3,4,5,6,7,8,9,10], inplace  = True)\n#labelencoder_X.fit(bank_df['day_of_week'])\n#bank_df['day_of_week'] = labelencoder_X.transform(bank_df['day_of_week'])\nbank_df['day_of_week'].replace(['mon','tue','wed','thu','fri'],[1,2,3,4,5],inplace=True)\n#labelencoder_X.fit(bank_df['poutcome'])\n#bank_df['poutcome'] = labelencoder_X.transform(bank_df['poutcome'])\nbank_df['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)\nbank_df['isCellular'] = bank_df['contact']\nbank_df['isCellular'].replace(['telephone', 'cellular'], [0,1], inplace  = True)\n#bank_df['default'].replace(['unknown','no', 'yes'], [1,2,3], inplace  = True)\n#bank_df['housing'].replace(['no' ,'yes'], [0,1], inplace  = True)\n","ceaf82f5":"#bank_df = bank_df.drop(['loan'],axis=1)\n#bank_df = bank_df.drop(['day_of_week'],axis=1)\n#bank_df = bank_df.drop(['housing'],axis=1)\nbank_df = bank_df.drop(['contact'],axis=1)","2a1a778b":"#Lets Analyze Continuous features. - Use Describe and Correlation function.\nbank_df.describe()","d51bfc80":"bank_df.corr()\n# Input feature - nr.employed and  euribor3m (.94) and emp.var.rate and nr.employed (.90) \n#and euribor3m and emp.var.rate (.97) are more correlated and we can remove on column.\n# And lets Remove columns - euribor3m and emp.var.rate ","82261b89":"plt.figure(figsize=(40,40)) \nsns.heatmap(bank_df.corr())","77cebf82":"#Remove Low Correlated input variable \nbank_df = bank_df.drop(['euribor3m'],axis=1)\nbank_df = bank_df.drop(['emp.var.rate'],axis=1)","3364255b":"# Check outlier if any for Numberic column.\nbank_df.age.plot(kind='box')\n# There are outlier and check max age and age greated than 90","0f718a7b":"print(bank_df.age.max())\nbank_df[bank_df['age'] > 80].head(100)","098f1daf":"bank_df.age.plot(kind='hist')\n# it is bit positively skewed but it is ok and seems no high dependency with Output variable","21e41a3f":"bank_df.age.plot(kind='kde')","1d9d5f15":"# Create Binning for all numeric fields base on Box plot quantile\ndef binning(dataframe,featureName):\n    print (featureName)\n    q1 = dataframe[featureName].quantile(0.25)\n    q2 = dataframe[featureName].quantile(0.50)\n    q3 = dataframe[featureName].quantile(0.75)\n    dataframe.loc[(dataframe[featureName] <= q1), featureName] = 1\n    dataframe.loc[(dataframe[featureName] > q1) & (dataframe[featureName] <= q2), featureName] = 2\n    dataframe.loc[(dataframe[featureName] > q2) & (dataframe[featureName] <= q3), featureName] = 3\n    dataframe.loc[(dataframe[featureName] > q3), featureName] = 4 \n    print (q1, q2, q3)\n    ","15f61970":"binning(bank_df,'age')","58803c09":"# let check campaign field now and it is positively skewed..\nbank_df.campaign.plot(kind='hist')","574b6ae0":"bank_df.campaign.plot(kind='box')\n# lot of exreme values.","81dbf0cf":"print(bank_df.campaign.max())\nprint(bank_df.campaign.mean())\nprint(bank_df.campaign.median())\nprint(bank_df.campaign.unique())\nprint('Y=1 for campaign > 10' , bank_df[(bank_df['campaign'] > 10) & (bank_df['y'] ==1)].age.count())\nprint('Y=1 for campaign < 10' , bank_df[(bank_df['campaign'] <= 10) & (bank_df['y'] ==1)].age.count())\nprint('Y=1 for campaign = 1' , bank_df[(bank_df['campaign'] == 1) & (bank_df['y'] ==1)].age.count())","3e8b1c6b":"bank_df.groupby(['campaign','y']).y.count()","bcac247b":"bank_df['campaign'].describe()","084b1cbc":"q1 = bank_df['campaign'].quantile(0.25)\nq2 = bank_df['campaign'].quantile(0.50)\nq3 = bank_df['campaign'].quantile(0.75)\n\nprint(q1)\nprint(q2)\nprint(q3)\n\niqr = q3-q1 #Interquartile range\n\nextreme_low_campaign = q1-1.5*iqr\nextreme_high_capmaign = q3+1.5*iqr\n\nprint (extreme_low_campaign)\nprint (extreme_high_capmaign)","c64f7c21":"binning(bank_df,'campaign')","bff0977c":"# pdays - number of days that passed by after the client was last contacted from a previous \n#campaign (numeric; 999 means client was not previously contacted)\n# exclude Pdays = 999\nbank_df[bank_df['pdays'] != 999].pdays.plot(kind='hist')","4d5da7a0":"bank_df[bank_df['pdays'] != 999].pdays.plot(kind='box')\n#sems Box plot is not applicable here ","3f172a7c":"# lets replace 999 with 0 to avoid extrem upper bound impact in machine learning.\n\nbank_df.loc[(bank_df['pdays'] >= 0) & (bank_df['pdays'] <= 5), 'pdays'] = 2\nbank_df.loc[(bank_df['pdays'] > 5) & (bank_df['pdays'] <= 10), 'pdays'] = 3\nbank_df.loc[(bank_df['pdays'] > 10) & (bank_df['pdays'] <= 20), 'pdays'] = 4\nbank_df.loc[(bank_df['pdays'] > 20) & (bank_df['pdays'] != 999) , 'pdays'] = 5 \n\nbank_df.loc[(bank_df['pdays'] == 999), 'pdays'] = 1","c8210f7e":"bank_df.pdays.unique()","41cd744a":"bank_df.groupby(['previous','y']).age.count()","7b0df029":"bank_df.duration.plot(kind='box')\n\n#sems Box plot is not applicable here ","f6f4e67b":"bank_df.duration.plot(kind='hist')","34fb756e":"bank_df.duration.describe()","13610d2c":"bank_df[bank_df['duration'] > 3000]","f2f63245":"bank_df","7f0e0f6c":"#binning(bank_df,'duration')","d13f92b4":"bank_df[['cons.price.idx','cons.conf.idx','nr.employed','y']].describe()","68d44a2d":"    bank_df[['cons.price.idx','cons.conf.idx','nr.employed','y']].corr()","ba1128ac":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV","a60b02a2":"#UDF for create model\nm_bank_df=bank_df","7df2790f":"m_bank_df.columns","a9a105e6":"def Split_Data(processeddata):\n    #processeddata['y'] = np.where(processeddata['y'] == 'yes',1,0)\n    columns = [column for column in processeddata.columns if column != 'y']\n    columns  = ['y']+columns\n    processeddata= processeddata[columns]\n\n    y=processeddata['y'].ravel()\n    del processeddata['y']\n    X= processeddata.as_matrix().astype('float')\n\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=0)\n    return X_train,X_test,y_train,y_test","1ba671eb":"def Convert_Model(X_train,y_train,X_test,y_test,classifier):\n     from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix\n     classifier.fit(X_train,y_train) \n     print(classifier.score(X_test,y_test)) \n     print(confusion_matrix(y_test,classifier.predict(X_test)))\n     print(accuracy_score(y_test,classifier.predict(X_test)))\n     print(precision_score(y_test,classifier.predict(X_test)))\n     print(recall_score(y_test,classifier.predict(X_test)))\n     f1 = 2 * precision_score(y_test,classifier.predict(X_test)) * recall_score(y_test,classifier.predict(X_test)) \/ (precision_score(y_test,classifier.predict(X_test)) + recall_score(y_test,classifier.predict(X_test)))\n     print(\"f1 score\", f1)\n     return classifier","8e02d17b":"X_train,X_test,y_train,y_test = Split_Data(m_bank_df)","66a16bf2":"# inport Dummy Classifier for creating Base Model\nfrom sklearn.dummy import DummyClassifier\nclassifier = DummyClassifier(strategy='most_frequent',random_state=0)\nfinalModel = Convert_Model(X_train,y_train,X_test,y_test,classifier)","5b8a8426":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","b1e0b0aa":"# inport Dummy Classifier for creating Base Model\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(random_state=0)\nfinalModel_lr = Convert_Model(X_train,y_train,X_test,y_test,classifier_lr)","0d772492":"# roc curve and auc on imbalanced dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nprobs = finalModel_lr.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the precision-recall curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()","8152672a":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\nfinalModel_gb = Convert_Model(X_train,y_train,X_test,y_test,gb)","15e1be97":"# roc curve and auc on imbalanced dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nprobs = finalModel_gb.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the precision-recall curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()","b6cc91ed":"#Ignore Duration field as it is - Duration: last contact duration, in seconds (numeric). \n#Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nm_bank_df = m_bank_df.drop(['duration'],axis=1)","abdea84e":"X_train,X_test,y_train,y_test = Split_Data(m_bank_df)","1fd07c95":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\nfinalModel_gb = Convert_Model(X_train,y_train,X_test,y_test,gb)","ad94a201":"# import pickle library\nimport pickle","f9b24dbf":"# create the file paths\nmodel_file_path = os.path.join('gb_model.pkl')\n#scaler_file_path = os.path.join(os.path.pardir,'models','lr_scaler.pkl')","16f4f7fe":"# open the files to write \nmodel_file_pickle = open(model_file_path, 'wb')\n#scaler_file_pickle = open(scaler_file_path, 'wb')","599066d0":"# open the files to write \nmodel_file_pickle = open(model_file_path, 'wb')\n#scaler_file_pickle = open(scaler_file_path, 'wb')","d7d2ae18":"# persist the model and scaler\npickle.dump(finalModel_lr, model_file_pickle)\n","31427ac8":"# close the file\nmodel_file_pickle.close()","0a955fdb":"import os\nhello_world_script_file = os.path.join('bank_api.py')","27cc9a8c":"%%writefile $hello_world_script_file\nimport pandas as pd\nimport json\nfrom flask import Flask, request\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\napp = Flask(__name__)\n\n# Load Model and Scaler Files\nmodel_path = os.path.join('models')\nmodel_filepath = os.path.join(model_path, 'lr_model.pkl')\n\nmodel = pickle.load(open(model_filepath))\n\n\n@app.route('\/api', methods=['POST'])\ndef say_hello():\n    print('Reached here  1')\n    data = json.dumps(request.get_json(force=True))\n    print(data)\n    print('Reached here  2 ')\n      \n    # create pandas dataframe using json string\n    my_df = pd.read_json(data)\n   \n    print(my_df['age'])\n   \n    my_df.info()\n    #my_df=json.loads(request.get_json(force=True))\n    print(my_df['campaign'])\n    my_df['age'] = my_df['age'].astype(np.int64)\n    my_df['campaign'] = my_df['campaign'].astype(np.int64)\n    my_df['pdays'] = my_df['pdays'].astype(np.int64)\n    my_df['previous'] = my_df['previous'].astype(np.int64)\n    \n    inputData=my_df\n    print(\"Data Preparation and input structure\")\n    inputData = inputData.drop(['euribor3m'],axis=1)\n    inputData = inputData.drop(['loan'],axis=1)\n    inputData = inputData.drop(['day_of_week'],axis=1)\n    inputData = inputData.drop(['emp.var.rate'],axis=1)\n    inputData = inputData.drop(['duration'],axis=1)\n    inputData = inputData.drop(['housing'],axis=1)\n    #inputData = inputData.drop(['cons.conf.idx'],axis=1)\n   \n    inputData = pd.get_dummies(inputData,columns=['job','marital','education','default'])\n    inputData['month'].replace(['mar','apr','may','jun','jul','aug','sep','oct','nov','dec'], [1,2,3,4,5,6,7,8,9,10], inplace  = True)\n    inputData['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)\n    inputData['isCellular'] =inputData['contact']\n    inputData['isCellular'].replace(['telephone', 'cellular'], [0,1], inplace  = True)\n    inputData = inputData.drop(['contact'],axis=1)\n  \n    print(\"Data Preparation and input structure\")\n    \n    inputData.loc[(inputData['age'] <=32),'age'] = 1\n    inputData.loc[(inputData['age'] > 32) &  (inputData['age'] <= 38),  'age'] = 2\n    inputData.loc[(inputData['age'] > 38) & (inputData['age'] <= 47),  'age'] = 3\n    inputData.loc[(inputData['age'] > 47), 'age'] = 4 \n\n    inputData.loc[(inputData['campaign'] <= 1) , 'campaign'] = 1\n    inputData.loc[(inputData['campaign'] > 1) & (inputData['campaign'] <= 2), 'campaign'] = 2\n    inputData.loc[(inputData['campaign'] > 2) & (inputData['campaign'] <= 3), 'campaign'] = 3\n    inputData.loc[(inputData['campaign'] > 3) , 'campaign'] = 4 \n \n    inputData.loc[(inputData['pdays'] >= 0) & (inputData['pdays'] <= 5), 'pdays'] = 2\n    inputData.loc[(inputData['pdays'] > 5) & (inputData['pdays'] <= 10), 'pdays'] = 3\n    inputData.loc[(inputData['pdays'] > 10) & (inputData['pdays'] <= 20), 'pdays'] = 4\n    inputData.loc[(inputData['pdays'] > 20) & (inputData['pdays'] != 999) , 'pdays'] = 5 \n    inputData.loc[(inputData['pdays'] == 999), 'pdays'] = 1\n    \n    column = [u'age', u'contact', u'month', u'campaign', u'pdays', u'previous',\n       u'poutcome', u'cons.price.idx', u'nr.employed', u'y', u'job_admin.',\n       u'job_blue-collar', u'job_entrepreneur', u'job_housemaid',\n       u'job_management', u'job_retired', u'job_self-employed',\n       u'job_services', u'job_student', u'job_technician', u'job_unemployed',\n       u'job_unknown', u'marital_divorced', u'marital_married',\n       u'marital_single', u'marital_unknown', u'education_basic.4y',\n       u'education_basic.6y', u'education_basic.9y', u'education_high.school',\n       u'education_illiterate', u'education_professional.course',\n       u'education_university.degree', u'education_unknown', u'housing_no',\n       u'housing_unknown', u'housing_yes', u'default_no', u'default_unknown',\n       u'default_yes']\n\n    input_columns = inputData.columns\n    dif = list(set(column) - set(input_columns))\n    print (dif)\n\n    for x in dif:\n        if x != 'y':\n            inputData.insert(1, x, 0)\n            inputData[x] = inputData[x].astype(np.uint8)\n        \n    print(list(set(column) - set(input_columns)))\n   \n    inputData.info()\n    prediction = model.predict(inputData)\n    result = pd.DataFrame({'result': prediction})\n    print (result['result'])\n    return result.to_json(orient='records')\n    #return \"{0}\".format(result['result'])\n\nif __name__ == '__main__':\n    app.run(port=10001, debug=True)\n   ","d93c08a0":"# Save model to file","a381d823":"#Tru Negative   |  False Positive\n#False Negative |  True Positive\n\n#Accuarcay - Correct Prediction \/ Total Count\n\n#Precision - TP \/ (TP + FP)\n\n#ReCall - TP \/ (TP + FN)","f3f2952e":"# Creating API using flask ","701e3d14":"# Part 2\nReal Time Integration using API. Here we have to remove duration column since this is not known when Bank agent initiate the call to customer.","01a22ed7":"# Conclusion \nGradientBoosting Classifier is the best classifier to target this proplem.\n","829b16e4":"Input Categorical feature Observation.\n\n* Job - More Job types are Admin , Technician and blue-collor and it means bank targeting high salaried people. \n* Marital - more people of type married - (#To Do - Check the target value distribution for high salaried married people) \n* Education - more count in university.degree people . of course High salaried people should have university degree :) expected. And illiterate count is very less. \n* default - most people have no credit , what it means? \n* housing - comparatively normal distribution loan - Not interested in personal loan :) , interesting. \n* contact - of course cellular communication is more convenient then telephone \n* month - Seems May is busy season in Portuguese \n* Day_of_week - Seems every day is busy but no weekends. \n* p_outcome - Success is small rate. (#To DO - Check how success correlates without put parameter ?)\n","38789f9b":"\n1. What is the Business Requirement.\n    * Predict the client will subscribe term deposit from Bank.\n    * Type of Analysis - Predictive Analysis \n    * Type of Learning - Supervised learning and it is Categorical problem with binary Categorization \n    * Target Variable - y -  (Has the client subscribed a term deposit?)\n    *\n    \n2. Data Understanding and Explore\n    * Data Set - Downloaded from https:\/\/www.kaggle.com\/henriqueyamahata\/bank-marketing#bank-additional-full.csv\n    * Data Science Tool - Python with Popular frameworks like - Panda, numpy ,sklearn,matplotlib ,flask and Pickle \n    * Data Science Environment - Local and Used Web API\n","4de14290":"# Create Model","f4aba03a":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.Data Explore and Preparation \n"}}