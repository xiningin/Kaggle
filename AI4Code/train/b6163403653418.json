{"cell_type":{"bcc5de63":"code","5c470b71":"code","ea8c4b86":"code","bb4ee77c":"code","b5414d18":"code","0ccf1a3a":"code","06a646ba":"code","432514b2":"code","1327656b":"code","61c18dfb":"code","85417105":"code","5c82bd9a":"code","cc698b3d":"markdown","72788709":"markdown","f877a5e5":"markdown","250ff862":"markdown","707c949e":"markdown","d02bc626":"markdown","c2c74ec6":"markdown","f0117f01":"markdown","60343cc3":"markdown","e160a981":"markdown"},"source":{"bcc5de63":"# for numerical things\nimport numpy as np\n\n# opencv & matplotlib to deal with images\nimport cv2\nimport matplotlib.pyplot as plt\n\n# os for file system related tasks\nimport os\n\n# random to fix seeds\nimport random\nimport tensorflow as tf\nimport torch\n\n# import keras to build CNN model\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import plot_model","5c470b71":"CATS_PATH = \"..\/input\/cat-and-dog\/training_set\/training_set\/cats\/\"\n\ncats_images_paths = os.listdir(CATS_PATH)\nlen(cats_images_paths)","ea8c4b86":"# randomly select 5 images.\ncats_5_images = random.sample(cats_images_paths, k=5)\n\nfor img in cats_5_images:\n    cv2_img = cv2.imread(CATS_PATH + img)\n    plt.figure()\n    plt.imshow(cv2_img)","bb4ee77c":"DOGS_PATH = \"..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/\"\n\ndogs_images_paths = os.listdir(DOGS_PATH)\nlen(dogs_images_paths)","b5414d18":"# randomly select 5 images.\ndogs_5_images = random.sample(dogs_images_paths, k=5)\n\nfor img in dogs_5_images:\n    cv2_img = cv2.imread(DOGS_PATH + img)\n    plt.figure()\n    plt.imshow(cv2_img)","0ccf1a3a":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    tf.random.set_seed(seed)\n\n# We fix all the random seed so that, we can reproduce the results.\nseed_everything(2020)","06a646ba":"# images shape\nIMAGE_SHAPE = 128\n\ninput_layer = Input(shape=(IMAGE_SHAPE, IMAGE_SHAPE, 3))\n\nconvolution_layer_1 = Conv2D(16, kernel_size=(3,3), activation = 'relu')(input_layer)\nconvolution_layer_2 = Conv2D(16, kernel_size=(3,3), activation = 'relu')(convolution_layer_1)\nmax_pool_1 = MaxPooling2D(pool_size=(2,2))(convolution_layer_2)\nbatch_norm_1 = BatchNormalization()(max_pool_1)\ndropout_1 = Dropout(0.2)(batch_norm_1)\n\nconvolution_layer_3 = Conv2D(32, kernel_size=(3,3), activation = 'relu')(dropout_1)\nconvolution_layer_4 = Conv2D(32, kernel_size=(3,3), activation = 'relu')(convolution_layer_3)\nmax_pool_2 = MaxPooling2D(pool_size=(2,2))(convolution_layer_4)\nbatch_norm_2 = BatchNormalization()(max_pool_2)\ndropout_2 = Dropout(0.2)(batch_norm_2)\n\nflattened = Flatten()(dropout_2)\ndense_layer_1 = Dense(128, activation='relu')(flattened)\ndense_layer_2 = Dense(64, activation='relu')(dense_layer_1)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\nmodel = Model(input=input_layer, output=output_layer)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","432514b2":"plot_model(model, show_shapes=True)","1327656b":"train_data_generator = ImageDataGenerator(\n    # We divide each pixel value(0, 255) with 255 to make them in range [0, 1]\n    rescale = 1.\/255, \n    \n    # We randomly shear & zoom our image while training to make our training robust\n    shear_range = 0.2, \n    zoom_range = 0.2, \n    \n    # We also flip our images by 180 degree horizontally to make our training robust\n    horizontal_flip = True\n)\n\ntest_data_generator = ImageDataGenerator(\n    rescale = 1.\/255\n)\n\ntraining_set_data = train_data_generator.flow_from_directory(\n    \"..\/input\/cat-and-dog\/training_set\/training_set\/\", \n    target_size = (IMAGE_SHAPE, IMAGE_SHAPE), \n    batch_size = 64,\n    class_mode = 'binary'\n)\n\ntest_set_data = test_data_generator.flow_from_directory(\n    '..\/input\/cat-and-dog\/test_set\/test_set\/',\n    target_size = (IMAGE_SHAPE, IMAGE_SHAPE),\n    batch_size = 64, \n    class_mode = 'binary'\n)","61c18dfb":"model.fit_generator(\n    training_set_data, \n    epochs = 10, \n    validation_data = test_set_data, \n)","85417105":"TEST_PATH = \"..\/input\/cat-and-dog\/test_set\/test_set\/\"\ntest_dogs_images = os.listdir(TEST_PATH + \"dogs\/\")\n\ntest_dog_img = test_dogs_images[5]\ntest_dog_img = cv2.imread(TEST_PATH + \"dogs\/\" + test_dog_img)\n\nplt.figure()\nplt.imshow(test_dog_img)\n\ntest_dog_img = cv2.resize(test_dog_img \/ 255, (IMAGE_SHAPE, IMAGE_SHAPE))\ntest_dog_img = test_dog_img.reshape(1, IMAGE_SHAPE, IMAGE_SHAPE, 3)\n\nprediction = model.predict(test_dog_img)\n\nif prediction[0][0] <= 0.5:\n    print(\"Model : It's a CAT\")\nelse:\n    print(\"Model : It's a DOG\")","5c82bd9a":"TEST_PATH = \"..\/input\/cat-and-dog\/test_set\/test_set\/\"\ntest_cats_images = os.listdir(TEST_PATH + \"cats\/\")\n\ntest_cat_img = test_cats_images[10]\ntest_cat_img = cv2.imread(TEST_PATH + \"cats\/\" + test_cat_img)\n\nplt.figure()\nplt.imshow(test_cat_img)\n\ntest_cat_img = cv2.resize(test_cat_img \/ 255, (IMAGE_SHAPE, IMAGE_SHAPE))\ntest_cat_img = test_cat_img.reshape(1, IMAGE_SHAPE, IMAGE_SHAPE, 3)\n\nprediction = model.predict(test_cat_img)\n\nif prediction[0][0] <= 0.5:\n    print(\"Model : It's a CAT\")\nelse:\n    print(\"Model : It's a DOG\")","cc698b3d":"## Cool.\nOur model have trained.<br>\nYou can see above the accuracy we have achieved.<br>\n\n## Now, let's test our model on test data.\n### Don't forget, we have not used this test data in our training. <br>So, our model have never seen these images ever.","72788709":"Now, we have data ready.<br>\nLet's start model training.","f877a5e5":"So, we have 4001 images of Cats\ud83d\udc31.<br><br>\n\nLet's have a look at some of them.","250ff862":"# Let's open & visualize the data files.","707c949e":"Model have `3.4`M parameters which we will tune while back-propagation.","d02bc626":"So, we have 4006 images of Cats\ud83d\udc36.","c2c74ec6":"# Before training it, let's understand this model first.\n\n* `input_layer` : This layer takes images as input. We specify the shape of images we will give.<br><br>\n* `convolution_layer_1` : This is a convolutional layer with 16 different unique kernels of size 3x3.\n    * All these 3x3 sized 32 kernels will roll over the input images & will capture different patterns.\n    * Whatever pattern they will capture, it will pass to next layer.\n<br><br>\n* `convolution_layer_2` : This is a convolutional layer same as previous one with 16 different unique kernels of size 3x3.\n    * This will get captured patterns from previous layer.\n    * And it'll capture patterns on top it i.e. a bit higher level features & pass to next layer.\n<br><br>\n* `max_pool_1` : Max pool layer is used to minimize the image size by pooling maximum number out of 2x2 grid.<br>\n    * This demografic shows a sample max-pool layer.\n    ![max_pool](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/2x2-max-pool-CNN.png)\n<br><br>\n* `batch_norm_1` : Batch Normalization is just normalizing values of matrixes. It kind of acts as a \"stabilizer\" in neural network training.\n<br><br>\n* `dropout_1` : Dropout is interesting trick. In Dropout, we ***randomly*** turn off some percentage of our neurons so that their's output can't go to next layer. Here we are turning off 20% of our total neurons.\n    * Purpose of doing this is again to make our training robust. \n    * Network should not depend some specific neurons to make predictions. And random turn will allow us to do that.\n    * Picture below help us to understand it.\n    ![dropout](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/dropout-in-deep-learning.png)\n<br><br>\n* `convolution_layer_3` : This is a convolutional layer with 32 different unique kernels of size 3x3.\n    * All these 3x3 sized 64 kernels will roll over the output of max-pool & will capture different patterns.\n    * Whatever pattern they will capture, it will pass to next layer.\n<br><br>\n* `convolution_layer_4` : This is a convolutional layer same as previous one with 32 different unique kernels of size 3x3.\n    * This will get captured patterns from previous layer.\n    * And it'll capture patterns on top it i.e. a bit higher level features & pass to next layer.\n<br><br>\n## Generally, as we go deeper into the network, number of kernels we use in a Conv layer increases as we capture more higher level patterns.\n* `max_pool_2` : This is again same as previous `max_pool_1`<br><br>\n* `batch_norm_2` & `dropout_2` are same type of layer as discussed before<br><br>\n* `flattened` : Flatten means, combining everything into one 1D array.\n    * We do this to combine features of all the kernels into one array & give to Dense layer to classify it.\n<br><br>\n* `dense_layer_1` : This is a simple Dense fully connected layer which we are using to classify what type of patterns have we captured.\n<br><br>\n* `dense_layer_2` : Same as above.<br><br>\n* `output_layer` : From this layer, we get our predictions.\n\n### That's our simple CNN model.\n\nNow, let's train it.<br>\n\n## While training it, we do something called `data augmentation`.\nPurpose of data augmentation is, to leverage same data to create more data.<br>\nLike, \n![image_augmentation](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/image-augmentation.png)\n<center>Image Augmentation ([source](https:\/\/nanonets.com\/blog\/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2\/))<\/center><br>\nIn above image, we created 6 different images out of 1 image by zooming & rotating it !<br>\nLabel for all these images will remain same. Cat.<br>\n### What a trick !","f0117f01":"In [previous lesson](https:\/\/www.kaggle.com\/prashantkikani\/solving-the-titanic-problem-deep-learning-way\/), we try to solve Titanic problem using simple artificial neural network model.<br><br>\n\nIn this lesson we will make a classifier who can predict whether there's a cat in the image or there's a dog.<br>\n\n![cat_vs_dog](https:\/\/images2.minutemediacdn.com\/image\/upload\/c_crop,h_706,w_1256,x_0,y_64\/f_auto,q_auto,w_1100\/v1554995050\/shape\/mentalfloss\/516438-istock-637689912.jpg)\n<center>Cat vs. Dog classifier using Deep Learning ([source](https:\/\/storymaps.arcgis.com\/stories\/b239e2a4d6bf402fa8c6f4b99720f2c1))<\/center>\n<br>\n\n### Input\nImage which contains either a cat or a dog.\n\n### Output\nPrediction of whether cat is there or dog is there in the image.\n\nFor images, we use CNNs.<br>\nSo, let's learn what are they.\n\n## Convolutional Neural Networks (CNN)\nSo, in this lesson, we will learn about convolutional neural nets or CNNs.<br>\n\nCNNs are a type of neural networks which are **used to detect patterns**; mostly in images.<br><br>\nWe use `kernels` to capture patterns in CNNs.<br>\nWe slide `kernels` to all over the image & capture patterns & pass those patterns to next layers.<br><br>\n\n![kernel sliding](https:\/\/miro.medium.com\/max\/700\/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n\n<center>This is how a kernel(3x3 square box above) slide through an image<\/center>\n<br>\n\nWe will see this in details.<br>\n\nThis is how image looks like to computers.<br>\n\n![image to computer](https:\/\/mozanunal.com\/images\/pixel.png)\n<center>Image to a computer<\/center>\n<br>\n\nEvery number in above black & white image is a pixel which indicates how much each color is present in that pixel.<br><br>\nEvery colored image is made of 3 basic colors.<br>\n* <font color=\"red\">Red<\/font>\n* <font color=\"green\">Green<\/font>\n* <font color=\"blue\">Blue<\/font>\n\n<br>\n\n## Kernels\n\nKernels in convolutions is 3x3 or 5x5 matrix which are used to detect different type of features.<br>\nLike, horizontal lines, vertical lines, diagonal lines etc.<br>\nWe can also call kernels as **feature detectors** \/ **pattern detectors**.<br><br>\n\n\n### We do \"convolutional\" operation on whole image while sliding kernel on it.<br>\n\n![kernel](https:\/\/miro.medium.com\/max\/395\/1*1okwhewf5KCtIPaFib4XaA.gif)\n\n\"Convolution\" is nothing but matrix multiplication between pixel values of image & kernel values.<br>\n\n### We are capturing patterns while sliding this kernel over the image.\n\nThese values of 3x3 kernels are random at first.<br>\nBut, while we are training our neural network with images, this values will update in a way, they will capture meaningful patterns like horizontal & vertical lines.<br>\n\n* We use multiple kernels to capture multiple patterns.<br>\n* One kernel capture captures one type of pattern.<br>\n* We combine(concatenate) different patterns captured from every kernel in current layer & pass them to next layer.<br>\n* Then next layer capture patterns on previous layer's captured patterns i.e. one step higher level patterns.<br>\n* This process continues to seveal layer until we reach to very high level patterns like hands, legs, mouth etc body parts.<br>\n\n## That's the CRUX in convolution neural networks.\n\nThat's how a computer can learn to capture patterns in images & take decisions accordingly.<br><br>\n\nWhen one Convolution layer passes it's captured patterns to next Convolution layer, it capture more abstract patterns.<br>\nLike..\n\n![elephant](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/f9\/Loxodonta_africana_-_old_bull_%28Ngorongoro%2C_2009%29.jpg\/220px-Loxodonta_africana_-_old_bull_%28Ngorongoro%2C_2009%29.jpg)\n\n* First it captures the edges of the elephant in image.\n* Then it combines nearby edges & lines to make body parts of the elephant like legs, trunk etc.\n* Then it combines all body parts to make a full elephant.\n\nAfter combining body parts, it remembers from it\u2019s training that these are the body parts which only one animal can have together. **Elephant**.<br>\n\nSo, this is how kernels look like after training on 1st & 2nd CNN layers.\n\n![kernel](https:\/\/i.iter01.com\/images\/6389a7188a0fac8d347f5524dfe05f0daae8db2b67bdaccc115a820031af5e89.png)\n<center>this is how kernels look like on first, second and third layers.<\/center>\n<br>\n\n## * First layer captures vertical, horizontal, diagonal lines.\n## * Second layer captures a bit higher level patterns like textures etc.\n\n![kernel](https:\/\/i.iter01.com\/images\/e97ff7f1bf951a5073cd1a1bdb1c177404db2cf36a4d672792b03ec85177bfde.png)\n<center>this is how kernels look like on 4th & 5th layers.<\/center>\n<br>\n\n## We can see, as we go more towards higher level layers, we see more higher & abstract patterns are captured.\n\n## This is how CNN learns while training from images.\n\n### Enough of verbal knowledge sharing.<br>\nLet's do some coding.<br>\n\nLet's imprort necessary libraries first.\n","60343cc3":"Data looks good.<br>\nNow, let's build our CNN model to train it to detect whether it's cat or a dog.","e160a981":"## We have a moderate level CNN model who can predict whether an image have CAT or DOG.\n### There are tons of things we can improve in this model like transfer learning etc. But this is a great start.\n\n# Summary\n\n* We have understood what Kernels are.\n* We have understood what Convolution is.\n* We saw how kernels roll over on the images.\n* We understood what Max-pool layer does.\n* We saw what dropout is & why we do it.\n* We build a CNN model from scratch to predict whether there is a cat or dog in the image.\n\n## Upvote this kernel if you have learned something from it.<br>\n## Tell me if you have any kind of doubts \/ questions in comment section below.\n\n## In the next lesson we'll see the basics of NLP by solving the Sarcasm Detection problem.\n\n# See you in the [next lesson](https:\/\/www.kaggle.com\/prashantkikani\/are-you-being-sarcastic-sarcasm-detection-nlp) !"}}