{"cell_type":{"ea220729":"code","da1d3790":"code","0fb2b847":"code","b9e4af22":"code","a621ace8":"code","cd6c043e":"code","e5382ef0":"code","7fe2da30":"code","ab0d3c26":"code","8c0b7020":"code","f529e39f":"code","8473d967":"code","c400d801":"code","1d63c1f4":"code","742d78dd":"code","f1d83137":"code","fa70663e":"code","3c765cc2":"code","45c92c4a":"code","f0171d03":"code","5297e0be":"code","6b0d8e2d":"code","5463bb9d":"code","c27b74bd":"code","34ec35fc":"code","9ec28c0d":"code","5ec5438e":"code","e1643c7f":"code","ac62887c":"code","98dd693e":"code","2411cfc1":"code","8517f770":"code","f608abee":"code","20cd246e":"code","28053704":"code","92a69733":"code","e9d3610c":"code","2c68be43":"code","284214c2":"code","a35df814":"code","b428fc7d":"code","e5e0dbdd":"code","6e17964c":"code","9b9ad1df":"code","1180f797":"code","aeb5752f":"code","3eb5abdb":"code","f599ea54":"code","82d9135a":"code","deefad2c":"code","653b7c1f":"code","99a6cd03":"code","87f9b733":"code","21825d66":"code","e780d820":"code","c2b7e4fd":"code","d9093513":"code","a637bba3":"code","6c0aba9b":"code","27726621":"code","10f2af91":"code","d1d1cebe":"code","f9ef2824":"code","7be21604":"code","17e56c3d":"code","d860b8cf":"code","a93a6a7f":"code","ca42a448":"code","483b245f":"code","2a7c47eb":"code","01bbde60":"code","99ff91d2":"code","92889c93":"code","a88b7469":"code","3143188c":"code","08b98217":"code","eb8bdf48":"code","08573de2":"code","1cac95f7":"code","790893b2":"code","e126bdd4":"code","67c6f5b1":"code","da78d89d":"code","4d4375a0":"code","b4941bf5":"markdown","18cc6f01":"markdown","542394c3":"markdown","d2b5e5af":"markdown","fc7e1e51":"markdown","7aefa756":"markdown","b4756895":"markdown","67a39768":"markdown","eb5f6583":"markdown","5d184565":"markdown","7ffe5565":"markdown","75fd1476":"markdown","49979807":"markdown","fe999a5e":"markdown","f5afd17a":"markdown","159be11d":"markdown","15b57a4a":"markdown","3fe8df5a":"markdown","9fbc1b40":"markdown","3f83961d":"markdown","524c2774":"markdown","be7a738c":"markdown","6fd88a9c":"markdown","9a4393ce":"markdown","3f6e6127":"markdown","9560fe1c":"markdown","521f01da":"markdown","b1a6b6a7":"markdown","21626729":"markdown","5a15188b":"markdown","910118f0":"markdown","b3a43972":"markdown","019173a4":"markdown","cf74ae3f":"markdown","77a47dac":"markdown","c611946d":"markdown","9d5a7ac4":"markdown","6804020d":"markdown","bc5cfe9c":"markdown","4e66923f":"markdown","1bc9b8f4":"markdown","aae252c3":"markdown","68e1e294":"markdown","97038ecf":"markdown","1a2bfd01":"markdown","8e01bad8":"markdown","990a8779":"markdown","019099b2":"markdown","1c2b9932":"markdown","9f2eb5f1":"markdown","64eac42d":"markdown","a6e80505":"markdown","256b780b":"markdown","f4889b31":"markdown","fb48759d":"markdown","b03ff740":"markdown","d7d2cbe3":"markdown"},"source":{"ea220729":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da1d3790":"# Standard imports\nimport seaborn as sns\nimport datetime\nimport sys\nimport os\nimport matplotlib.pyplot as plt\nimport warnings\nimport category_encoders as ce\nwarnings.filterwarnings('ignore')\n\n# ML libs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn                 import metrics, svm\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","0fb2b847":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data.head()","b9e4af22":"test_data.head()","a621ace8":"train_data.head()","cd6c043e":"train_data.loc[train_data.Sex == \"male\"][\"Survived\"].count(), train_data.loc[train_data.Sex == \"female\"][\"Survived\"].count()","e5382ef0":"women = train_data.loc[train_data.Sex == \"female\"][\"Survived\"]\nrate_women = round((sum(women)\/len(women))*100,2)\nmen = train_data.loc[train_data.Sex == \"male\"][\"Survived\"]\nrate_men = round((sum(men)\/len(men))*100,2)\n\nprint(\"% of women who survived:\", rate_women)\nprint(\"% of men who survived:\", rate_men)","7fe2da30":"# Age\/Sex (Survived\/ Not survived):\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwoman = train_data[train_data['Sex']==\"female\"]\nman = train_data[train_data['Sex']==\"male\"]\nax = sns.distplot(woman[woman['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(woman[woman['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(man[man['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(man[man['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","ab0d3c26":"train_data[['Pclass', 'Sex', 'Survived']].groupby(['Pclass', 'Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8c0b7020":"sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_data,\n              palette={\"male\": \"blue\", \"female\": \"green\"});","f529e39f":"sns.pointplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train_data,\n              palette={\"male\": \"blue\", \"female\": \"green\"});","8473d967":"grid = sns.FacetGrid(train_data, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","c400d801":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","1d63c1f4":"train_data.isna().sum()","742d78dd":"test_data.isna().sum()","f1d83137":"# Get names of columns with missing values\ncols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\nprint(cols_with_missing)","fa70663e":"train_data.shape, test_data.shape","3c765cc2":"# Get list of categorical variables\ns = (train_data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","45c92c4a":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: train_data[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","f0171d03":"# Removing categorical columns with high cardinality\ntrain_data.drop(['Ticket', 'Name'], axis=1, inplace=True)\ntest_data.drop(['Ticket', 'Name'], axis=1, inplace=True)","5297e0be":"train_data['Embarked'].isna().sum()","6b0d8e2d":"train_data['Embarked'].value_counts()","5463bb9d":"# replacing na values in Embarked with 'S' \ntrain_data['Embarked'].fillna(\"S\", inplace = True) \ntest_data['Embarked'].fillna(\"S\", inplace = True)","c27b74bd":"train_data['Sex'].value_counts()","34ec35fc":"# Low Cardinality columns transformation\nce_ord = ce.OrdinalEncoder(cols = ['Sex', 'Embarked'])\ntrain_data = ce_ord.fit_transform(train_data)\ntest_data  = ce_ord.fit_transform(test_data)\ntrain_data.head()","9ec28c0d":"train_data.isna().sum()","5ec5438e":"#Preprocessing of dataset (train_data)\n#Training and prediction dataset\ncols = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\ndf_Age = train_data[cols]\ndf_test = df_Age[df_Age[\"Age\"].isnull()]\ndf_Age = df_Age.dropna()\n\ny_train = df_Age[\"Age\"]\nX_train = df_Age.drop(\"Age\", axis=1)\nX_test  = df_test.drop(\"Age\", axis=1)\n\n#train model to fit dataset and predict missing values from column \"Age\"\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n#replace the missing values with predicted values\ntrain_data.loc[train_data.Age.isnull(), 'Age'] = y_pred","e1643c7f":"#Preprocessing of dataset (test_data)\n#Training and prediction dataset\ncols = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\ndf_Age = test_data[cols]\ndf_test = df_Age[df_Age[\"Age\"].isnull()]\ndf_Age = df_Age.dropna()\n\n\ny_train = df_Age[\"Age\"]\nX_train = df_Age.drop(\"Age\", axis=1)\nX_test  = df_test.drop(\"Age\", axis=1)\n\n#train model to fit dataset and predict missing values from column \"Age\"\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n#replace the missing values with predicted values\ntest_data.loc[test_data.Age.isnull(), 'Age'] = y_pred","ac62887c":"train_data['Age'].isna().sum(), test_data['Age'].isna().sum()","98dd693e":"train_data.head()","2411cfc1":"train_data[\"Age\"] = train_data[\"Age\"].astype(int)\ntest_data[\"Age\"]  = test_data[\"Age\"].astype(int)","8517f770":"train_data[\"Age\"].dtype, test_data[\"Age\"].dtype","f608abee":"train_data.isna().sum()","20cd246e":"test_data.isna().sum()","28053704":"# replacing na values in Cabin with 'N' \ntrain_data['Cabin'].fillna(\"N\", inplace = True) \ntest_data['Cabin'].fillna(\"N\", inplace = True) \ntrain_data.isna().sum()","92a69733":"train_data['Cabin'].value_counts()","e9d3610c":"train_data['Cabin'] = train_data['Cabin'].astype(str).str[0]\ntest_data['Cabin']  = test_data['Cabin'].astype(str).str[0]","2c68be43":"train_data['Cabin'].value_counts()","284214c2":"plt.figure(figsize=(12,8))\nsns.countplot(x=train_data['Cabin'],data=train_data,hue='Survived',order = train_data['Cabin'].value_counts().index)\nplt.title('Passengers distribution by cabin',fontsize= 16)\nplt.ylabel('Number of passengers')\nplt.legend(( 'Not Survived', 'Survived'), loc=(0.85,0.89))\nplt.xticks(rotation = False)\n\n\nplt.show()","a35df814":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'N':9}\ntrain_data['Cabin'] = train_data['Cabin'].map(cabin_category)\ntest_data['Cabin']  = test_data['Cabin'].map(cabin_category)","b428fc7d":"train_data.head()","e5e0dbdd":"train_data['Cabin'].value_counts()","6e17964c":"test_data.isna().sum()","9b9ad1df":"train_data['Fare'].median(), train_data['Fare'].mean()","1180f797":"train_data['Fare'].fillna(train_data['Fare'].median(),inplace=True)\ntest_data['Fare'].fillna(test_data['Fare'].median(),inplace=True)","aeb5752f":"train_data.isna().sum()","3eb5abdb":"test_data.isna().sum()","f599ea54":"train_data.head()","82d9135a":"train_data[\"Fare\"] = train_data[\"Fare\"].astype(int)\ntest_data[\"Fare\"]  = test_data[\"Fare\"].astype(int)","deefad2c":"train_data[\"Fare\"].dtype, test_data[\"Fare\"].dtype","653b7c1f":"train_data.head()","99a6cd03":"train_data.corr()","87f9b733":"# Visual representation\nplt.figure(figsize=(14,10))\nsns.heatmap(train_data.corr(), linewidths=.01, annot = True, cmap='coolwarm')\nplt.show()","21825d66":"features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]\nX  = train_data[features]\ny  = train_data[\"Survived\"]","e780d820":"X.shape, y.shape","c2b7e4fd":"features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin\", \"Embarked\"]\nX_train  = train_data[features]\ny_train  = train_data[\"Survived\"]\nX_test   = test_data[features]","d9093513":"# Create a dictionary called models which contains all of the classification models we've imported\n# The models dictionary should contain 12 models to be tested.\nmodels = {\"LinearSVC\": LinearSVC(),\n          \"KNN\": KNeighborsClassifier(),\n          \"SVC\": SVC(),\n          \"LogisticRegression\": LogisticRegression(),\n          \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100),\n          \"Ridge\": Ridge(),\n          \"SVR_linear\": SVR(kernel=\"linear\"),\n          \"SVR_rbf\": SVR(kernel=\"rbf\"),          \n          \"DecisionTreeClassifier\": DecisionTreeClassifier(max_leaf_nodes=5), \n          \"DecisionTreeRegressor\": DecisionTreeRegressor(max_leaf_nodes=5), \n          \"RandomForestRegressor\": RandomForestRegressor(), \n          \"XGBClassifier\": XGBClassifier()\n         }","a637bba3":"# Loop through the models dictionary items, fitting the model on the training data\n# and appending the model name and model into dataframe called \"df_results\"\n# Don't forget to set NumPy random seed equal to 42 to don't affect the comparison\n\n# Create an empty pandas dataframe called df_results\ndf_results = pd.DataFrame()","6c0aba9b":"X_train.shape, y_train.shape, X_test.shape","27726621":"np.random.seed(42)\nfor model_name, model in models.items():\n  model.fit(X_train, y_train)\n\n  print(f\"Predicting {model_name}...\")  \n  y_preds = cross_val_predict(model, X_train, y_train)  \n\n  # y_preds = model.predict(X_test)\n\n  print(f\"Scoring {model_name}...\")\n  # results[model_name] = model.score(X_test, y_test)\n\n  new_row = { \"Model\"    : model_name, \n              \"Score\"    : model.score(X_train, y_train),  \n              #\"Accuracy\" : accuracy_score(y_test, y_preds.round(), normalize=False), \n              \"Accuracy\" : accuracy_score(y_train, y_preds.round()), \n              \"Precision\": precision_score(y_train, y_preds.round()), \n              \"Recall\"   : recall_score(y_train, y_preds.round()), \n              \"F1\"       : f1_score(y_train, y_preds.round()), \n              \"MAE\"      : mean_absolute_error(y_train, y_preds), \n              \"MSE\"      : mean_squared_error(y_train, y_preds), \n              \"R2\"       : r2_score(y_train, y_preds),\n              \"ROC_AUC\"  : roc_auc_score(y_train, y_preds) }\n\n  df_results = df_results.append(new_row, ignore_index=True, sort=False);  \n\nprint('\\n')\nprint('===============================')\nprint('Predition finished sucessfully.')\nprint('===============================')","10f2af91":"# Showing results   \ndf_results.sort_values(by=[\"Accuracy\"], ascending=False, inplace=True)\ndf_results","d1d1cebe":"# Create a bar plot of the results dataframe using plot.bar()\ndf_results.plot.bar(x='Model', y=['Accuracy', 'ROC_AUC'],figsize=(10,5));","f9ef2824":"X_test.head()","7be21604":"%%time\nparam_grid = { \"criterion\" : [\"mse\", \"mae\"], \"min_samples_leaf\" : [1, 5, 10, 25], \"min_samples_split\" : [2, 5, 10, 15, 20], \"max_features\" :[\"auto\", \"sqrt\", \"log2\"], \"n_estimators\": [25, 50, 100, 200], \"oob_score\" : [True, False]}\nrfr = RandomForestRegressor(random_state=42, n_jobs=-1)\ngridscv = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=-1, cv=5)\ngridscv.fit(X_train, y_train)\nprint(\"-----------------\")\nprint(gridscv.best_params_)\nprint(\"-----------------\")","17e56c3d":"rfr = RandomForestRegressor(**gridscv.best_params_, random_state=42)\nrfr.fit(X_train, y_train)\npredictions = rfr.predict(X_test)\ny_preds = cross_val_predict(rfr, X_train, y_train)  \nscore_rfr = round(rfr.score(X_train, y_train) * 100, 2)\nacc_rfr   = round(accuracy_score(y_train, y_preds.round()) * 100, 2)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})","d860b8cf":"print(\"Score: \" + str(score_rfr) + \"%\") \nprint(\"Accuracy: \" + str(acc_rfr) + \"%\") ","a93a6a7f":"%%time\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_leaf\" : [1, 5, 10, 25], \"min_samples_split\" : [2, 5, 10, 15, 20], \"max_features\" :[\"auto\", \"sqrt\", \"log2\"]} \ndtc = DecisionTreeClassifier(random_state=42)\ngridscv = GridSearchCV(estimator=dtc, param_grid=param_grid, n_jobs=-1)\ngridscv.fit(X_train, y_train)\nprint(\"-----------------\")\nprint(gridscv.best_params_)\nprint(\"-----------------\")","ca42a448":"dtc = DecisionTreeClassifier(**gridscv.best_params_, random_state=42)\ndtc.fit(X_train, y_train)\npredictions = dtc.predict(X_test)\ny_preds = cross_val_predict(dtc, X_train, y_train)  \nscore_dtc = round(dtc.score(X_train, y_train) * 100, 2)\nacc_dtc   = round(accuracy_score(y_train, y_preds.round()) * 100, 2)","483b245f":"print(\"Score: \" + str(score_dtc) + \"%\") \nprint(\"Accuracy: \" + str(acc_dtc) + \"%\") ","2a7c47eb":"%%time\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25], \"min_samples_split\" : [2, 5, 10, 15, 20], \"max_features\" : [\"auto\", \"sqrt\", \"log2\"], \"n_estimators\": [25, 50, 100, 200], \"oob_score\" : [True, False]}\nrfc = RandomForestClassifier(max_features='auto', random_state=42, n_jobs=-1)\ngridscv = GridSearchCV(estimator=rfc, param_grid=param_grid, n_jobs=-1)\ngridscv.fit(X_train, y_train)\nprint(\"-----------------\")\nprint(gridscv.best_params_)\nprint(\"-----------------\")","01bbde60":"rfc = RandomForestClassifier(**gridscv.best_params_, random_state=42)\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)\ny_preds = cross_val_predict(rfc, X_train, y_train)  \nscore_rfc = round(rfc.score(X_train, y_train) * 100, 2)\nacc_rfc   = round(accuracy_score(y_train, y_preds.round()) * 100, 2)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})","99ff91d2":"print(\"Score: \" + str(score_rfc) + \"%\") \nprint(\"Accuracy: \" + str(acc_rfc) + \"%\") ","92889c93":"%%time\nparam_grid = { \"booster\" : [\"gbtree\", \"gblinear\", \"dart\"], \"max_depth\" : [3, 5, 7, 9, 12], \"n_estimators\": [5, 10, 20, 25, 50, 100, 200], \n               \"learning_rate\": [0.01, 0.05, 0.1, 0.3, 0.5, 1], \"tree_method\" : [\"auto\", \"exact\", \"approx\", \"hist\"]}\nxgbc = XGBClassifier(random_state=42, verbosity=0 )\ngridscv = GridSearchCV(estimator=xgbc, param_grid=param_grid, cv=5, scoring='accuracy')\ngridscv.fit(X_train, y_train)\nprint(\"-----------------\")\nprint(gridscv.best_params_)\nprint(\"-----------------\")","a88b7469":"xgbc = XGBClassifier(**gridscv.best_params_, random_state=42)\nxgbc.fit(X_train, y_train)\npredictions = xgbc.predict(X_test)\ny_preds = cross_val_predict(xgbc, X_train, y_train)  \nscore_xgbc = round(rfc.score(X_train, y_train) * 100, 2)\nacc_xgbc   = round(accuracy_score(y_train, y_preds.round()) * 100, 2)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})","3143188c":"print(\"Score: \" + str(score_xgbc) + \"%\") \nprint(\"Accuracy: \" + str(acc_xgbc) + \"%\") ","08b98217":"feature_importance = pd.DataFrame({'features':X_train.columns,'importance':np.round(xgbc.feature_importances_,3)})\nfeature_importance = feature_importance.sort_values('importance',ascending=False).set_index('features')\nfeature_importance.head(10)","eb8bdf48":"feature_importance.plot.bar();","08573de2":"y_preds = cross_val_predict(xgbc, X_train, y_train)\nconf_matrix = confusion_matrix(y_train, y_preds)\nconf_matrix","1cac95f7":"tp_ = conf_matrix.item(1,1)\ntn_ = conf_matrix.item(0,0)\nfp_ = conf_matrix.item(0,1)\nfn_ = conf_matrix.item(1,0)\n\nprint(\"Where: TP=\" + str(tp_) + \", \" + \"TN=\" + str(tn_) + \", \" + \"FP=\" + str(fp_) + \", \" + \"FN=\" + str(fn_))","790893b2":"Accuracy = (tp_ + tn_)\/(tp_ + tn_ + fp_ + fn_)\nprint (\"Accuracy: \" + str(round(Accuracy*100,2)) + \"%\")","e126bdd4":"# getting the probabilities of our predictions\ny_prob = xgbc.predict_proba(X_train)\ny_prob = y_prob[:,1]\n\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_prob)\n\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=1, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=1)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n\nplt.figure(figsize=(10, 5))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","67c6f5b1":"# Printing the ROC-AUC Score\nprint(\"ROC-AUC-Score:\", round(roc_auc_score(y_train, y_prob)*100,2))","da78d89d":"output","4d4375a0":"output.to_csv('ads_submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","b4941bf5":"<a id=\"submit_results\"><\/a>\n# Submit the results","18cc6f01":"<a id=\"hyperparameter_tuning\"><\/a>\n# Hyperparameter Tuning\nOn this part I will just pick the first 4 models and try to bring better results for each one using the hyperparameter tuning technique.","542394c3":"<a id=\"pclass_survived_age\"><\/a>\n### PClass\/Survived\/Age","d2b5e5af":"Now we can solve the **\"Age\"** column, methods like: mean, median or imputation for this case are not the better approach, so we can use ML to predict the ages value missing.\nWe have to apply the predictions on train and test data.","fc7e1e51":"Let's transform the \"Age\" column from float type into integer.","7aefa756":"It is an excellent score!","b4756895":"## Titanic\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.\n\nTitanic was under the command of Captain Edward Smith, who also went down with the ship. The ocean liner carried some of the wealthiest people in the world, as well as hundreds of emigrants from Great Britain and Ireland, Scandinavia and elsewhere throughout Europe, who were seeking a new life in the United States. The first-class accommodation was designed to be the pinnacle of comfort and luxury, with a gymnasium, swimming pool, libraries, high-class restaurants, and opulent cabins. A high-powered radiotelegraph transmitter was available for sending passenger \"marconigrams\" and for the ship's operational use. Although Titanic had advanced safety features, such as watertight compartments and remotely activated watertight doors, it only carried enough lifeboats for 1,178 people\u2014about half the number on board, and one third of her total capacity\u2014due to the maritime safety regulations of those days. The ship carried 16 lifeboat davits which could lower three lifeboats each, for a total of 48 boats. However, Titanic carried only a total of 20 lifeboats, four of which were collapsible and proved hard to launch during the sinking.\n\nAfter leaving Southampton on 10 April 1912, Titanic called at Cherbourg in France and Queenstown (now Cobh) in Ireland, before heading west to New York. On 14 April, four days into the crossing and about 375 miles (600 km) south of Newfoundland, she hit an iceberg at 11:40 p.m. ship's time. The collision caused the hull plates to buckle inwards along her starboard (right) side and opened five of her sixteen watertight compartments to the sea; she could only survive four flooding. Meanwhile, passengers and some crew members were evacuated in lifeboats, many of which were launched only partially loaded. A disproportionate number of men were left aboard because of a \"women and children first\" protocol for loading lifeboats. At 2:20 a.m., she broke apart and foundered with well over one thousand people still aboard. Just under two hours after Titanic sank, the Cunard liner RMS Carpathia arrived and brought aboard an estimated 705 survivors.\n\nThe disaster was met with worldwide shock and outrage at the huge loss of life, as well as the regulatory and operational failures that led to it. Public inquiries in Britain and the United States led to major improvements in maritime safety. One of their most important legacies was the establishment of the International Convention for the Safety of Life at Sea (SOLAS) in 1914, which still governs maritime safety. Several new wireless regulations were passed around the world in an effort to learn from the many missteps in wireless communications\u2014which could have saved many more passengers.\n\nThe wreck of Titanic was discovered in 1985 (73 years after the disaster) during a Franco-American expedition and United States Military mission. The ship was split in two and is gradually disintegrating at a depth of 12,415 feet (2,069.2 fathoms; 3,784 m). Thousands of artefacts have been recovered and displayed at museums around the world. Titanic has become one of the most famous ships in history, depicted in numerous works of popular culture, including books, folk songs, films, exhibits, and memorials. Titanic is the second largest ocean liner wreck in the world, only being surpassed by her sister ship HMHS Britannic, however, she is the largest sunk while in service as a liner, as Britannic was in use as a hospital ship at the time of her sinking. The final survivor of the sinking, Millvina Dean, aged two months at the time, died in 2009 at the age of 97.\n\n![image.png](attachment:image.png)\n\nhttps:\/\/en.wikipedia.org\/wiki\/Titanic","67a39768":"<a id=\"variable_notes\"><\/a>\n## Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","eb5f6583":"The last item is \"Fare\" on testing data to be mapped. In this specific case that we have just one (1) item missing we can use the median to fill out it.","5d184565":"Drop rows in training data that have the target value missing (Survived column)\n\nExample: train_data.dropna(subset=['Survived'], axis=1, inplace=True)","7ffe5565":"<a id=\"random_forest_classifier\"><\/a>\n## Random Forest Classifier\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\nI've got the definition and parameters from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html","75fd1476":"The result above means that we can easly treat the fields \"Sex\" and \"Embarked\" via mapping and the fields with High cardinality we can remove it (\"Ticket\", \"Name\"), for \"Cabin\" we can use a different method, find out more about \"Cabin\" formatting below.","49979807":"<a id=\"history\"><\/a>\n# History","fe999a5e":"<a id=\"survived_sex\"><\/a>\n### %Survived (Sex)","f5afd17a":"<a id=\"basic_predicting\"><\/a>\n## Basic predicting in different models","159be11d":"<a id=\"data_dictionary\"><\/a>\n## Data Dictionary\n![image.png](attachment:image.png)","15b57a4a":"![image.png](attachment:image.png)\n\nTitanic cutaway diagram\n\nhttps:\/\/en.wikipedia.org\/wiki\/First-class_facilities_of_the_Titanic#\/media\/File:Titanic_cutaway_diagram.png","3fe8df5a":"<a id=\"checking_correlation\"><\/a>\n## Checking the correlation\n The correlation is a statistical measure of the strength of a relationship between two quantitative variables.","9fbc1b40":"<a id=\"converting_types\"><\/a>\n## Converting types","3f83961d":"### Decks\n* **A Deck**, also called the Promenade Deck, extended along the entire 546 feet (166 m) length of the superstructure. It was reserved exclusively for First Class passengers and contained First Class cabins, the First Class lounge, smoke room, reading and writing rooms and Palm Court.\n\n* **B Deck**, the Bridge Deck, was the top weight-bearing deck and the uppermost level of the hull. More First Class passenger accommodations were located here with six palatial staterooms (cabins) featuring their own private promenades. On Titanic, the \u00c0 La Carte Restaurant and the Caf\u00e9 Parisien provided luxury dining facilities to First Class passengers. Both were run by subcontracted chefs and their staff; all were lost in the disaster. The Second Class smoking room and entrance hall were both located on this deck. The raised forecastle of the ship was forward of the Bridge Deck, accommodating Number 1 hatch (the main hatch through to the cargo holds), numerous pieces of machinery and the anchor housings.[b] Aft of the Bridge Deck was the raised Poop Deck, 106 feet (32 m) long, used as a promenade by Third Class passengers. It was where many of Titanic's passengers and crew made their last stand as the ship sank. The forecastle and Poop Deck were separated from the Bridge Deck by well decks.\n \n* **C Deck**, the Shelter Deck, was the highest deck to run uninterrupted from stem to stern. It included both well decks; the aft one served as part of the Third Class promenade. Crew cabins were housed below the forecastle and Third Class public rooms were housed below the Poop Deck. In between were the majority of First Class cabins and the Second Class library.\n \n* **D Deck**, the Saloon Deck, was dominated by three large public rooms\u2014the First Class Reception Room, the First Class Dining Saloon and the Second Class Dining Saloon. An open space was provided for Third Class passengers. First, Second and Third Class passengers had cabins on this deck, with berths for firemen located in the bow. It was the highest level reached by the ship's watertight bulkheads (though only by eight of the fifteen bulkheads).\n \n* **E Deck**, the Upper Deck, was predominantly used for passenger accommodation for all three classes plus berths for cooks, seamen, stewards and trimmers. Along its length ran a long passageway nicknamed Scotland Road, in reference to a famous street in Liverpool. Scotland Road was used by Third Class passengers and crew members.\n \n* **F Deck**, the Middle Deck, was the last complete deck and mainly accommodated Second and Third Class passengers and several departments of the crew. The Third Class dining saloon was located here, as were the swimming pool, Turkish bath and kennels.\n \n* **G Deck**, the Lower Deck, was the lowest complete deck that carried passengers, and had the lowest portholes, just above the waterline. The squash court was located here along with the traveling post office where letters and parcels were sorted ready for delivery when the ship docked. Food was also stored here. The deck was interrupted at several points by orlop (partial) decks over the boiler, engine and turbine rooms.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Titanic\n","524c2774":"Comparing the accuracy from the models above the best one for those conditions is **XGBoost Classifier**","be7a738c":"<a id=\"xgboost\"><\/a>\n## XGBoost\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\nI've got the parameters from https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#general-parameters","6fd88a9c":"Let's transform the \"Fare\" column from float type into integer. ","9a4393ce":"<a id=\"mapping\"><\/a>\n## Mapping","3f6e6127":"### Loading data","9560fe1c":"First we can fill the missing values for \"N\", that can means \"null\" or \"None\".","521f01da":"**Formula:**\n\nAccuracy = (TP + TN)\/(TP + TN + FP + FN) \n\nReplacing the values on the formula we have:","b1a6b6a7":"<a id=\"confusion_matrix\"><\/a>\n### Confusion Matrix\nA confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm.\nMore information on https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix","21626729":"<a id=\"roc-auc_score\"><\/a>\n### ROC-AUC Score\nThere is a complete information about ROC (Receiver Operating Characteristic) and AUC (Area Under Curve) on https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic","5a15188b":"# Table of Contents\n* [History](#history)  \n* [Understand the data](#understand_data)\n    - [Data Dictionary](#data_dictionary)\n    - [Variable Notes](#variable_notes)\n    - [Graphs](#graphs)\n    - [%Survived (Sex)](#survived_sex)\n    - [Age\/Sex (Survived\/Not Survived)](#age_sex)\n    - [PClass\/Sex\/Age](#pclass_sex_age)\n    - [PClass\/Survived\/Age](#pclass_survived_age)     \n    - [Pclass\/Sex\/Survived](#pclass_sex_survived)\n    - [Embarked\/Survived\/Sex](#embarked_survived_sex)\n* [Prepare the data](#prepare_the_data)\n    - [Missing values](#missing_values)\n    - [Checking cardinality for categorical values](#checking_cardinality)\n    - [Predicting missing data using ML](#predicting_missing_data)\n    - [Mapping](#mapping)\n    - [Converting types](#converting_types)\n    - [Checking the correlation](#checking_correlation)\n    - [Heatmap](#heatmap)\n* [Select a model](#select_model) \n    - [Basic predicting in different models](#basic_predicting)  \n    - [Model comparison](#model_comparison)\n* [Hyperparameter Tuning](#hyperparameter_tuning)\n    - [Random Forest Regressor](#random_forest_regressor)\n    - [Decision Tree Classifier](#decision_tree_classifier)\n    - [Random Forest Classifier](#random_forest_classifier)\n    - [XGBoost](#xgboost)\n    - [Checking Accuracy](#checking_accuracy)\n    - [Confusion Matrix](#confusion_matrix)\n    - [ROC-AUC Score](#roc-auc_score)\n* [Submit the results](#submit_results)\n* [References and Credits](#references_credits)","910118f0":"As showed above the accuracy results pointing to Randon Forest Regressor as the most fit for this data, so let's use it to do a \"hyper parameter tuning\" before submit the results.","b3a43972":"<a id=\"decision_tree_classifier\"><\/a>\n## Decision Tree Classifier\nDecisionTreeClassifier is a class capable of performing multi-class classification on a dataset.\nI've got the definition and parameters from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html","019173a4":"### Passengers distribution by deck (Survived\/ Not survived)","cf74ae3f":"<a id=\"pclass_sex_age\"><\/a>\n### Pclass\/Sex\/Age","77a47dac":"<a id=\"references_credits\"><\/a>\n# References and Credits\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n\n\n[Complete Machine Learning and Data Science: Zero to Mastery](https:\/\/www.udemy.com\/course\/complete-machine-learning-and-data-science-zero-to-mastery\/) by [Daniel Bourke](https:\/\/www.mrdbourke.com\/)\n\n[Getting Started with Pandas: Kaggle's Titanic Competition](https:\/\/www.kaggle.com\/c\/titanic)\n\n[Predicting the Survival of Titanic Passengers](https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8)\n\n[Titanic - Hyperparameter tuning with GridSearchCV](https:\/\/www.kaggle.com\/ihelon\/titanic-hyperparameter-tuning-with-gridsearchcv)\n\n[Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n[Titanic Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Titanic)","c611946d":"<a id=\"embarked_survived_sex\"><\/a>\n### Embarked\/Survived\/Sex","9d5a7ac4":"To solve the Cabin problem of missing data and transformation you have to understand better the data","6804020d":"<a id=\"prepare_the_data\"><\/a>\n# Prepare the data\nPrepare the data means transforming raw data into data to be used for the machine learning algorithms. A good example is the column \"Sex\" (Male, Female), so you need to transform these data into numbers (Male=1, Female=2) to be used for the ML.","bc5cfe9c":"<a id=\"select_model\"><\/a>\n# Select a model\nThe training set is a subset of the data set used to train a model.\nWe can name the variables as presented below:\n* **X_train** is the training data set.\n* **y_train** is the set of labels to all the data in X_train.\n\nThe test set is a subset of the data set that you use to test your model.\n* **X_test** is the testing data set.\n* **y_test** is the set of labels to all the data in X_test (not used in this case).","4e66923f":"The information above tell us that there is a strong correlation between the fields \"Sex\", \"PClass\" and \"Survived\", example if you were female in the 1st Class your chances were 96% to survive and for male in the 3rd Class your chances were only 13% to survive.","1bc9b8f4":"<a id=\"checking_accuracy\"><\/a>\n### Checking Accuracy","aae252c3":"<a id=\"graphs\"><\/a>\n# Graphs\n The graphs will help you to visualize data which makes it easy to rapidly scan information in order to understand. ","68e1e294":"* (Not Survived) True Negatives : **501**, False Positives:  **48**\n* (Survived)     False Negatives:  **92**  True Positives : **250**\n\nConsidering this results we can calculate the Accuracy hit ratio.\n\nAccuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. To make the context clear by the semantics, it is often referred to as the \"Rand accuracy\" or \"Rand index\".It is a parameter of the test. The formula for quantifying binary accuracy is:\n\nAccuracy = (TP + TN)\/(TP + TN + FP + FN)\nwhere: TP = True positive; FP = False positive; TN = True negative; FN = False negative\n\nhttps:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision#In_binary_classification","97038ecf":"Now we can transform the columns with low cardinality using an OrdinalEncoder","1a2bfd01":"Let's check if there are any missing value on age column","8e01bad8":"<a id=\"predicting_missing_data\"><\/a>\n## Predicting missing data using ML","990a8779":"<a id=\"random_forest_regressor\"><\/a>\n## Random Forest Regressor\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \nI've got the definition and parameters from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html","019099b2":"### Importing Libs","1c2b9932":"<a id=\"missing_values\"><\/a>\n## Missing values\nLet's check which data is missing and treat them.","9f2eb5f1":"We can note that the cabin code contain in the first character the Deck location. i.e (Cabin B96 = Deck \"B\")\nSo, we can create a category to be mapped and apply an Encoder to transform it in numbers to be used on the Machine Learning algorithm.","64eac42d":"<a id=\"model_comparison\"><\/a>\n## Model comparison","a6e80505":"<a id=\"age_sex\"><\/a>\n### Age\/Sex (Survived\/Not survived)","256b780b":"<a id=\"understand_data\"><\/a>\n# Understand the data\nIt is very important you understand well the data before starting any machine learning process if you want to getting better results.","f4889b31":"<a id=\"checking_cardinality\"><\/a>\n## Checking cardinality for categorical values\nChecking the cardinality is very important before choose what transformation method you must apply for categorical values (objects).","fb48759d":"<a id=\"heatmap\"><\/a>\n## Heatmap","b03ff740":"Replacing missing values for '**Embarked**' column with the most common Embarked, in this case \"S\".","d7d2cbe3":"<a id=\"pclass_sex_survived\"><\/a>\n### Pclass\/Sex\/Survived"}}