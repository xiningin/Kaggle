{"cell_type":{"5033a987":"code","a05d24a6":"code","6b71511e":"code","c82656ac":"code","fe512ef9":"code","fa23e64d":"code","471e3ae2":"code","7ec97e48":"code","b6bd47b1":"code","c1efe5e9":"code","65a16656":"code","7e17e67d":"code","16ca2fb1":"code","14b4d606":"code","08c07e40":"code","e8e0725e":"code","9becbe39":"code","def5ddf5":"code","4807b4be":"code","c71f64cd":"code","f7523fd4":"code","06476e32":"code","d25461ef":"code","6cd2ab90":"code","3ad4c6ae":"code","47f5becb":"code","ce7bc7e8":"code","9723d57b":"code","a2af41ed":"code","d3b2f5c5":"code","a41cb413":"code","17073134":"code","44a06eb8":"code","107e0b33":"code","f21f62f2":"code","99d7812e":"code","be0a74ae":"code","aefa4a15":"code","04fe1311":"code","efa61cf9":"code","66df40a6":"code","c14ad567":"code","7cb91efd":"code","568b238a":"code","7a10079b":"code","d8a4d629":"code","612865d9":"code","ed9f5d2a":"code","6bc2c6e6":"code","d80800c0":"markdown","b1b52f9d":"markdown","74c3258a":"markdown","15520075":"markdown","8504f8be":"markdown","a4a0c50f":"markdown","f787d056":"markdown","d49ea103":"markdown","a4495d43":"markdown","f00a26d7":"markdown"},"source":{"5033a987":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsb.set()\n\nimport folium\n\nfrom sklearn import metrics\nfrom sklearn import metrics\nfrom scipy.stats import zscore\nimport lightgbm as lgbm\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.callbacks import EarlyStopping\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a05d24a6":"# Helping functions from Jeff Heaton's course on Deep Learning\n# https:\/\/github.com\/jeffheaton\/t81_558_deep_learning\/blob\/master\/t81_558_class06_backpropagation.ipynb\n    \n# Convert all missing values in the specified column to the median\ndef missing_median(df, name):\n    med = df[name].median()\n    df[name] = df[name].fillna(med)\n    \ndef remove_outliers(df, name, sd):\n    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n    df.drop(drop_rows, axis=0, inplace=True)\n    \n# Encode a numeric column as zscores\ndef encode_numeric_zscore(df, name, mean=None, sd=None):\n    if mean is None:\n        mean = df[name].mean()\n\n    if sd is None:\n        sd = df[name].std()\n\n    df[name] = (df[name] - mean) \/ sd\n    \ndef encode_text_dummy(df, name):\n    dummies = pd.get_dummies(df[name])\n    for x in dummies.columns:\n        dummy_name = \"{}-{}\".format(name, x)\n        df[dummy_name] = dummies[x]\n    df.drop(name, axis=1, inplace=True)","6b71511e":"# Date parser is used to treat the date and time column data as dates\ndata_train = pd.read_csv('..\/input\/train.csv', nrows = 10_000_000, parse_dates=['pickup_datetime'])\ndata_train.head()","c82656ac":"data_train.describe()\n\n# Note that there are negative fares and very large amount of fares too. \n# Longitude and Latitude are negative and also very large which is incorrrect. ","fe512ef9":"data_train.dtypes","fa23e64d":"data_train['fare_amount'].plot(kind='hist')\nplt.xlabel('Fare Amount')\nplt.title('Variation of Taxi Fare Amount in New York')","471e3ae2":"# Checking number of rides with fare over $100\ndata_train_filtered = data_train[data_train['fare_amount'] >100]\npercent_data_train_filtered = data_train_filtered.shape[0]\/data_train.shape[0]*100\nprint('Number of fares greater than $100 are,',data_train_filtered.shape[0])\nprint('Percentage of fares greater than 100 ', percent_data_train_filtered)","7ec97e48":"Nrows = data_train.shape[0]\nprint ('Number of rows in training data before cleaning are, {}'.format(Nrows))\n\n# Remove the outliers of pickup longitudes different than thse values which are bounding box of New York City\n# North Latitude: 40.917577 South Latitude: 40.477399 East Longitude: -73.700272 West Longitude: -74.259090\n# source https:\/\/www.mapdevelopers.com\/geocode_bounding_box.php\ndata_train = data_train[data_train['dropoff_latitude'] <=40.917577]\ndata_train = data_train[data_train['dropoff_latitude'] >=40.477399]\n           \ndata_train = data_train[data_train['pickup_latitude'] <=40.917577]\ndata_train = data_train[data_train['pickup_latitude'] >=40.477399]\n\ndata_train = data_train[data_train['dropoff_longitude'] <=-73.700272]\ndata_train = data_train[data_train['dropoff_longitude'] >=-74.259090]\n           \ndata_train = data_train[data_train['pickup_longitude'] <=-73.700272]\ndata_train = data_train[data_train['pickup_longitude'] >=-74.259090]\n\n# Keep rows only with different starting and ending location\ndata_train = data_train[data_train['dropoff_latitude'] != data_train['pickup_latitude']]\ndata_train = data_train[data_train['dropoff_longitude'] != data_train['pickup_longitude']]\n\n# Replace missing rows with no fare amount with median value\nmissing_median(data_train,'fare_amount')\n\n# Removing the outliers of fare amount more than standard deviation of 3.\n#remove_outliers(data_train,'fare_amount', sd=3)\n\n# Dropping fares greater than $100\ndata_train = data_train[data_train['fare_amount']<100]\n\n# Keep rows only with non-negative fare and non-negligible erronous amount\ndata_train = data_train[data_train['fare_amount']>=2.5]\n\n# Keep rows only with non-negative fare and non-negligible erronous amount\ndata_train = data_train[data_train['passenger_count']>0]\ndata_train = data_train[data_train['passenger_count']<7]\n\nprint(\"Removing outliers with fare more than $100, negative fares, outside of boundary of NY city,negative passengers and missing na values\")\nNrows_clean = data_train.shape[0]\nPercLost = (Nrows-Nrows_clean)\/Nrows*100\nprint ('Number of rows in data after cleaning are, {}'.format(Nrows_clean))\nprint ('Percentage of data removed after cleaning {}.format',PercLost)","b6bd47b1":"data_train.describe()","c1efe5e9":"# Count if any data is null or zero\nprint(data_train.isnull().sum())","65a16656":"# Drop the data with null longitude\ndata_train = data_train[~data_train.dropoff_longitude.isnull()]\nprint(data_train.isnull().sum())","7e17e67d":"import folium\nNY_CORD = (data_train['pickup_latitude'].mean(),data_train['pickup_longitude'].mean())\nmax_records = 1000\n\nny_map = folium.Map(location=NY_CORD, zoom_start=12.0,tiles= \"OpenStreetMap\")\nfor each in data_train[:max_records].iterrows():\n    folium.CircleMarker(\n        radius = 4,\n        color ='red',\n        location = [each[1]['pickup_latitude'],each[1]['pickup_longitude']],\n        popup='lat='+str(each[1]['pickup_latitude'])+',long='+str(each[1]['pickup_longitude'])\n    ).add_to(ny_map)\nny_map","16ca2fb1":"import folium\nNY_CORD = (data_train['pickup_latitude'].mean(),data_train['pickup_longitude'].mean())\nmax_records = 1000\n\nny_map2 = folium.Map(location=NY_CORD, zoom_start=11.0,tiles= \"OpenStreetMap\")\nfor each in data_train[:max_records].iterrows():\n    folium.CircleMarker(\n        radius = 4,\n        color ='blue',\n        location = [each[1]['dropoff_latitude'],each[1]['dropoff_longitude']],\n        popup='lat='+str(each[1]['dropoff_latitude'])+',long='+str(each[1]['dropoff_longitude'])\n    ).add_to(ny_map2)\nny_map2","14b4d606":"# Calculate the distance between two latitude and longitude data points using Haversine method\n# https:\/\/en.wikipedia.org\/wiki\/Haversine_formula\n# https:\/\/stackoverflow.com\/questions\/27928\/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n\n# Bearing calculated using method - https:\/\/www.movable-type.co.uk\/scripts\/latlong.html \n# http:\/\/mathforum.org\/library\/drmath\/view\/55417.html\n    \nimport math\n    \ndef getDistance(df):\n    R_Earth = 6371 # Radius of earth in km\n    LAT1 = df['pickup_latitude'] \n    LONG1 = df['pickup_longitude']\n    LAT2 = df['dropoff_latitude']\n    LONG2 = df['dropoff_longitude']\n    dLat = deg2rad(LAT2 - LAT1)\n    dLon = deg2rad(LONG2 - LONG1)\n    a = math.sin(dLat\/2) * math.sin(dLat\/2) + math.cos(deg2rad(LAT1)) * math.cos(deg2rad(LAT2)) * math.sin(dLon\/2) * math.sin(dLon\/2)\n    c = 2 * np.arcsin(np.sqrt(a)); \n    return R_Earth*c\n\ndef getBearing(df):\n    R_Earth = 6371 # Radius of earth in km\n    LAT1 = deg2rad( df['pickup_latitude'] )\n    LONG1 = deg2rad(  df['pickup_longitude'] )\n    LAT2 = deg2rad( df['dropoff_latitude']  )\n    LONG2 = deg2rad( df['dropoff_longitude'] )\n    dLat = (LAT2 - LAT1)\n    dLon = (LONG2 - LONG1)\n    a = np.arctan2(np.sin(dLon * np.cos(LAT2)),np.cos(LAT1) * np.sin(LAT2) - np.sin(LAT1) * np.cos(LAT2) * np.cos(dLon))\n    return a\n\ndef deg2rad(deg):\n    return np.radians(deg)","08c07e40":"data_train.drop('key',1, inplace=True)\n\ndata_train['distance_km'] = data_train.apply(getDistance, axis=1)\ndata_train['bearing'] = data_train.apply(getBearing, axis=1)\n\ndef split_time(df):\n    data_train['year'] = data_train.pickup_datetime.dt.year\n    data_train['month'] = data_train.pickup_datetime.dt.month\n    data_train['day_of_year'] = data_train.pickup_datetime.dt.dayofyear\n    data_train['hour'] = data_train.pickup_datetime.dt.hour\n    data_train['day_of_week'] = data_train.pickup_datetime.dt.dayofweek\n    \nsplit_time(data_train)\n\ndata_train.head()","e8e0725e":"def peak_hour(df):\n    if df['hour']>=16 and df['hour']<=20:\n        return 1\n    else:\n        return 0\n\ndef night_hour(df):\n    if df['hour']>=20 or df['hour']<=6:\n        return 1\n    else:\n        return 0\n\ndef JFK_Manhatten_pickup_drop(df):\n    # Range of co-ordinates for JFK airport which have fixed surcharge\n    if (40.6214 <= df['pickup_latitude'] <= 40.6693) and (-73.8250 <= df['pickup_longitude'] <= -73.7459) \\\n        and (40.7002 <= df['dropoff_latitude'] <= 40.7655) and (-74.0217 <= df['dropoff_longitude'] <= -73.9495) and df['fare_amount']>51:\n            return 1\n    elif (40.7002 <= df['pickup_latitude'] <= 40.7655) and (-74.0217 <= df['pickup_longitude'] <= -73.9495) \\\n        and (40.6214 <= df['dropoff_latitude'] <= 40.6693) and (-73.8250 <= df['dropoff_longitude'] <= -73.7459) and df['fare_amount']>51:\n            return 1\n    elif (40.6214 <= df['pickup_latitude'] <= 40.6693) and (-73.8250 <= df['pickup_longitude'] <= -73.7459) \\\n        and (40.7663 <= df['dropoff_latitude'] <= 40.8316) and (-73.9908 <= df['dropoff_longitude'] <= -73.9185) and df['fare_amount']>51: \n            return 1\n    elif (40.7663 <= df['pickup_latitude'] <= 40.8316) and (-73.9908 <= df['pickup_longitude'] <= -73.9185) \\\n        and (40.6214 <= df['dropoff_latitude'] <= 40.6693) and (-73.8250 <= df['dropoff_longitude'] <= -73.7459) and df['fare_amount']>51:\n            return 1\n    else:\n        return 0\n    \n\ndef NEW_airport_dropoff(df):\n    # Range of co-ordinates for Newark airport which have fixed surcharge\n    if ( (40.6694 <= df['dropoff_latitude'] <= 40.7049) and  (-74.1935 <= df['dropoff_longitude'] <= -74.1508)):\n        return 1\n    else:\n        return 0\n\ndata_train['peak'] = data_train.apply(peak_hour,axis=1)\ndata_train['night_hour'] = data_train.apply(night_hour,axis=1)\ndata_train['JFK_airport'] = data_train.apply(JFK_Manhatten_pickup_drop, axis=1)\ndata_train['NEW_airport'] = data_train.apply(NEW_airport_dropoff, axis=1)\n\n\ndata_train.head()","9becbe39":"data_train.describe()","def5ddf5":"data_train['fare_amount'].plot(kind='hist', bins=50)\nplt.xlabel('Fare Amount ($)')\nplt.title('Variation of Taxi Fare Amount in New York')","4807b4be":"data_train['distance_km'].plot(kind='hist', bins=50)\nplt.xlabel('Distance (km)')\nplt.title('Variation of Ride Distance for a taxi in New York')","c71f64cd":"plt.scatter(data_train['distance_km'],data_train['fare_amount'],alpha=0.2)\nplt.xlabel('Distance (km)')\nplt.ylabel('Fare ($)')","f7523fd4":"# Plotting airport rides in a map to confirm location of pickup\ndata_train_airport = data_train[data_train['JFK_airport'] == 1]\n\n#data_train_airport.head(20)\n(data_train_airport.describe())\n# Note that mean fare is around the minimum fare for the ride is 2.5 though minimum is $52.00\n# Mean value is correct, so we have drop the values which are below $52","06476e32":"import folium\nJFK_CORD = (data_train_airport['pickup_latitude'].mean(),data_train_airport['pickup_longitude'].mean())\nmax_records = 1000\n\nairport_map = folium.Map(location=JFK_CORD, zoom_start=12.0,tiles= \"OpenStreetMap\")\n\nfor each in data_train_airport[:max_records].iterrows():\n    folium.CircleMarker(\n        radius = 4,\n        color ='red',\n        location = [each[1]['pickup_latitude'],each[1]['pickup_longitude']],\n        popup='lat='+str(each[1]['pickup_latitude'])+',long='+str(each[1]['pickup_longitude'])\n    ).add_to(airport_map)\nairport_map\n","d25461ef":"data_train_airport['fare_amount'].plot(kind='hist', bins=50)\nplt.xlabel('Fare Amount ($)')\nplt.title('Taxi Fare for rides between Manhatten and JFK')","6cd2ab90":"colors = np.where(data_train[\"JFK_airport\"]==1,'r','b')\ndata_train.plot.scatter('distance_km', 'fare_amount',c=colors,alpha=0.3)\nplt.xlabel('Distance (km)')\nplt.ylabel('Fare ($)')\nplt.title('Distance and Fare for the ride colored in red for airport rides')\nplt.Figure(figsize=(20, 20))","3ad4c6ae":"# Newark Airport Dropoff\n# Plotting airport rides in a map to confirm location of pickup\ndata_train_nw_airport = data_train[data_train['NEW_airport'] == 1]\n\n#data_train_airport.head(20)\n(data_train_nw_airport.describe())\n# Note that mean fare is around the minimum fare for the ride is 2.5 though minimum is $52.00\n# Mean value is correct, so we have drop the values which are below $52","47f5becb":"import folium\nNEW_CORD = (data_train_nw_airport['dropoff_latitude'].mean(),data_train_nw_airport['dropoff_longitude'].mean())\nmax_records = 1000\n\nairport_map = folium.Map(location=NEW_CORD, zoom_start=12.0,tiles= \"OpenStreetMap\")\n\nfor each in data_train_nw_airport[:max_records].iterrows():\n    folium.CircleMarker(\n        radius = 4,\n        color ='red',\n        location = [each[1]['dropoff_latitude'],each[1]['dropoff_longitude']],\n        popup='lat='+str(each[1]['dropoff_latitude'])+',long='+str(each[1]['dropoff_longitude'])\n    ).add_to(airport_map)\nairport_map","ce7bc7e8":"# Reference for this analysis https:\/\/www.kaggle.com\/nicapotato\/taxi-rides-time-analysis-and-oof-lgbm\ndef plot_time_history(df, timeframes, value, color=\"purple\"):\n    \"\"\"\n    Function to count observation occurrence through different lenses of time.\n    \"\"\"\n    f, ax = plt.subplots(len(timeframes), figsize = [12,12])\n    for i,x in enumerate(timeframes):\n        df.loc[:,[x,value]].groupby([x]).mean().plot(ax=ax[i],color=color)\n        ax[i].set_ylabel(value.replace(\"_\", \" \").title())\n        ax[i].set_title(\"{} by {}\".format(value.replace(\"_\", \" \").title(), x.replace(\"_\", \" \").title()))\n        ax[i].set_xlabel(\"\")\n    ax[len(timeframes)-1].set_xlabel(\"Time Frame\")\n    plt.tight_layout(pad=0)\n\nplot_time_history(df=data_train, timeframes=['year',\"month\",\"day_of_week\",\"day_of_year\",\"hour\"], value = \"fare_amount\", color=\"blue\")\n    ","9723d57b":"# Convert LAT\/LONG to radians after plotting is done\ndata_train['pickup_latitude']=deg2rad(data_train['pickup_latitude'])\ndata_train['pickup_longitude']=deg2rad(data_train['pickup_longitude'])\ndata_train['dropoff_longitude']=deg2rad(data_train['dropoff_longitude'])\ndata_train['dropoff_latitude']=deg2rad(data_train['dropoff_latitude'])","a2af41ed":"#data_train.head()","d3b2f5c5":"data_train.drop('pickup_datetime',1, inplace=True)","a41cb413":"import seaborn as sns\n\ncm = data_train.corr()\nplt.figure(figsize=(18, 18))\nsns.heatmap(cm, xticklabels=data_train.columns, yticklabels=data_train.columns, annot=True, cmap=\"YlGnBu\")","17073134":"# Separate features from target variable\nfare = data_train['fare_amount']\nfeatures = data_train.drop('fare_amount', axis = 1)\n\n# Drop columns of LAT, LONG, Month, Day of the week, day of the year as they do not influence the fare but demand\n# Hour is dropped because it is transformed into peak hour or late night\n#features.drop('pickup_latitude',1, inplace=True)\n#features.drop('pickup_longitude',1, inplace=True)\n#features.drop('dropoff_latitude',1, inplace=True)\n#features.drop('dropoff_longitude',1, inplace=True)\n#features.drop('pickup_datetime',1, inplace=True)\nfeatures.drop('day_of_year',1, inplace=True)\nfeatures.drop('day_of_week',1, inplace=True)\nfeatures.drop('month',1, inplace=True)\nfeatures.drop('hour',1, inplace=True)","44a06eb8":"features.head()","107e0b33":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nfeatures_scaled = scaler.fit_transform(features)\n\n#print(features_scaled[0])\n\nscaled_df = pd.DataFrame(features_scaled, columns=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude',\\\n                                             'passenger_count','distance_km','bearing','year','peak','night_hour',\n                                             'JFK_airport','NEW_airport'])\n#scaled_df.head()\nscaled_df.describe()","f21f62f2":"# Data splitting with sklearn train_test_split and 10% of the data is reserved for testing\nfrom sklearn.model_selection import train_test_split\n\nX_train1, X_hold, y_train1, y_hold = train_test_split(features_scaled, \n                                                    fare, \n                                                    test_size = 0.1, \n                                                    random_state = 42)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train1.shape[0]))\nprint(\"Holdout set has {} samples.\".format(X_hold.shape[0]))\n","99d7812e":"# Data splitting with sklearn train_test_split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train1, \n                                                    y_train1, \n                                                    test_size = 0.2, \n                                                    random_state = 42)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Validation set has {} samples.\".format(X_val.shape[0]))","be0a74ae":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom math import sqrt\n\nReg_model = LinearRegression()\nReg_model.fit(X_train, y_train)\ny_pred = Reg_model.predict(X_hold)\n\nerror = math.sqrt(mean_squared_error(y_pred,y_hold))\nprint (f'Root mean squared error is {error}')\n\nerror = mean_absolute_error(y_pred,y_hold)\nprint (f'Mean absolute error is {error}')","aefa4a15":"print ('Coefficients of the linear model are',Reg_model.coef_)\nprint('Intercept of the model is',Reg_model.intercept_)\nprint (f'R2 Score of the model is {Reg_model.score(X_hold, y_hold)}')","04fe1311":"#Reference https:\/\/github.com\/jeffheaton\/t81_558_deep_learning\/blob\/master\/t81_558_class09_regularization.ipynb\n\n# Simple function to evaluate the coefficients of a regression\n%matplotlib inline    \nfrom IPython.display import display, HTML    \n\ndef report_coef(names,coef,intercept):\n    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n    r = r.sort_values(by=['coef'])\n    display(r)\n    print(\"Intercept: {}\".format(intercept))\n    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n    \nnames=['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude',\\\n                                             'passenger_count','distance_km','bearing','year','peak','night_hour',\n                                             'JFK_airport','NEW_airport']   \nreport_coef(\n  names,\n  Reg_model.coef_,\n  Reg_model.intercept_)","efa61cf9":"plt.scatter(y_hold, y_pred)\nx = np.linspace(0, 100, 100)\ny = x\nplt.plot(x, y)\nplt.show()\n\n# reference:https:\/\/www.kaggle.com\/andyxie\/beginner-scikit-learn-linear-regression-tutorial","66df40a6":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': 4,\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,\n        'num_rounds':50000\n    }\n\ntrain_set = lgbm.Dataset(X_train, y_train, silent = False)\nvalid_set = lgbm.Dataset(X_val, y_val, silent = False)\nlgbm_model = lgbm.train(params=params, train_set=train_set, num_boost_round=10000, early_stopping_rounds=500, verbose_eval=500,valid_sets=valid_set)\n\nprediction = lgbm_model.predict(X_hold, num_iteration = lgbm_model.best_iteration)\nerror = math.sqrt(mean_squared_error(prediction,y_hold))\nprint (f'Root mean squared error is {error}')\n\nerror = mean_absolute_error(prediction,y_hold)\nprint (f'Mean absolute error is {error}')","c14ad567":"feature_imp = pd.DataFrame(sorted(zip(lgbm_model.feature_importance(),names)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","7cb91efd":"from keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dropout\nfrom keras import regularizers\n\n\n# Build the neural network\nmodel = Sequential()\n\nmodel.add(Dense(256,input_dim=X_train.shape[1],activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(32,kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01),activation='relu'))\n\nmodel.add(Dense(1))\n\nsgd = optimizers.SGD(lr=0.001, decay=1e-4, momentum=0.9, nesterov=True)\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\nmodel.compile(loss='mean_squared_error', optimizer=adam)\n\nmodel.summary()","568b238a":"from keras.callbacks import LearningRateScheduler\n\nmonitor = EarlyStopping(monitor='val_loss',min_delta=1e-3, patience=5, verbose=0, mode='auto')\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.001,cooldown=5)\n\ndef step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 5.0\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    return lrate\n\nlrate = LearningRateScheduler(step_decay)\n\nhistory = model.fit(X_train,y_train,validation_data=(X_val,y_val), callbacks=[monitor], batch_size = 250, verbose=1, epochs=1000)\n","7a10079b":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('History of Loss')\nplt.xlabel('Number of epochs')\nplt.ylabel('loss')\nplt.legend() ","d8a4d629":"# Making predictions\nprediction = model.predict(X_hold)\n\n# Check how good are predictions with RMS calculation\nRMS = np.sqrt(metrics.mean_squared_error(prediction,y_hold))\n\nprint(f'Model has RMS of : {RMS}')","612865d9":"def CreatDeepNetwork(dropout_rate=0.0):\n    # Build the neural network\n    Gmodel = Sequential()\n    Gmodel.add(Dense(256,input_dim=X_train.shape[1],activation='relu'))\n    Gmodel.add(BatchNormalization())\n    Gmodel.add(Dropout(dropout_rate))\n    \n    Gmodel.add(Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n    Gmodel.add(BatchNormalization())\n    Gmodel.add(Dropout(dropout_rate))\n\n    Gmodel.add(Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n    Gmodel.add(BatchNormalization())\n    Gmodel.add(Dropout(dropout_rate))\n    \n    Gmodel.add(Dense(32,kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01),activation='relu'))\n    Gmodel.add(Dense(1))\n    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\n    Gmodel.compile(loss='mean_squared_error', optimizer=adam)\n    return Gmodel\n#model.summary()","ed9f5d2a":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nneural_network = KerasRegressor(build_fn=CreatDeepNetwork, verbose=0)\nscoring_fnc = make_scorer(mean_squared_error,greater_is_better=False)\nbatch_size = [250, 500, 750]\ndropout_rate =[0.0, 0.1, 0.2]\n\n# Create hyperparameter options\nhyperparameters = dict(batch_size=batch_size,dropout_rate=dropout_rate)\n\ngrid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters, cv=None,scoring= scoring_fnc)\n\n","6bc2c6e6":"#Fit grid search\ngrid_result = grid.fit(X_train,y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","d80800c0":"In the next step, fares greater than \\$100 as well as smaller than \\$2.50 are dropped as being outliers. Please note that $2.50 is minimum fare. ","b1b52f9d":"****Data Cleanup****\n\nDuring this step, the data is cleaned by removing the outliers such as \n* Data with large fares,\n* Data with negative fares and negligible fares below 50 cents\n* Data with same starting to ending locations\n* Any null\/na fields\n* Data with pickup\/drop off beyond NY city limits\n* Data with negative person count","74c3258a":"Optimzation of Hyperparameters\nIn this section, following hyperparameters are tuned to improve the accuracy of the model\n\n* Batch size: Ranges from 250-500\n* Dropout:  0-0.2","15520075":"**Exploring The Data**\n\nIn this section, effect of various variables on fare amount is analyzed. ","8504f8be":"To consider the peak hour and late night fares new features are created. ","a4a0c50f":"**Baseline Model Development with Linear Regression**\n\n1. Separate features and target variables (Fare)\n2. Feature scaling : Scale the data of passenger, year, distance, hour, day\n3. Data will be split into two sets primarily training, validation and testing data.\n4. Baseling model with sklearn","f787d056":"**Feature Engineering**\n\nIn this section, new features are created from locations of pickup\/dropoff and time data. They allow us to look more closely into effect of these new variables on fare. \n\nIt is also necessary to consider the fare methods used by New York Taxi and Limosine Commission. \n\n* As per which, there is a surcharge of \\$1 for taxi pickup between peak hours, Monday - Friday after 4:00 PM & before 8:00 PM. \n* In addition, there is a night surcharge of \\$0.50 after 8:00 PM & before 6:00 AM. This means we need to create a feature for this. \n* There is also a flat fare plus toll charges plus surcharges for pickups to and from between JFK and Manhatten.  \n* There is a also a fixed surcharge of $17.5 for dropoff to Newark airport. \n\nUsing the pickup and dropoff locations, distance can be Haversine distance can be calculated. \nSimilarly date-time time is split into the \n* Year - to capture year over year increase in the taxi fare, \n* Day of the year -  to capture the effect of seasonal weather changes on taxi fares,\n* Day of the week - to capture effect of weekly patterns on taxi fare, such as work week Vs weekend\n* Time of the day - to capture effect of daily work schedules on taxi fares","d49ea103":"**Predicting Fares with LightGBM**\n\n","a4495d43":"**Predicting Fares with Deep Learning Model with Tensorflow and Keras\n**\n\nFollowing will be parameters of the model used for tuning the model\n\n* Plot validation loss curve\n* Learning rate\n* Number of deep layers\n* Number of neurons in layers\n* Add dropout layers\n* Change initial values\n* Batch gradient descent for large data","f00a26d7":"**New York Taxi **\n![](http:\/\/www.nyc.gov\/html\/tlc\/images\/features\/fi_industry_yellow_taxi_photo.jpg)"}}