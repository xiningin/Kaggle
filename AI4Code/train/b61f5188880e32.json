{"cell_type":{"fc3f54de":"code","8a2d05b0":"code","d810d298":"code","9ec44ae0":"code","8b0db177":"code","80194295":"code","11daec79":"code","d05a4d68":"code","a8c219c5":"code","e2f3f97b":"code","8d7ce7fa":"code","162f869b":"code","0f09c73e":"code","ef628308":"code","869de434":"code","8f8cf41b":"code","b1a66f4c":"code","cecda3d3":"code","47d63516":"code","7e0aeba9":"code","80e20756":"code","d828a3c2":"code","8dd63630":"code","c69ad11c":"code","0a0f5572":"code","117effa2":"code","8111a26a":"code","8fe44dc5":"code","b53cd4d3":"code","9d857263":"code","16d19cbb":"code","1ef36571":"code","13c75064":"code","b2204da3":"code","421fcb3e":"code","e558a3c9":"code","a22704de":"code","e5db62bd":"code","ea274b13":"code","0ce474e5":"code","834912b0":"code","4b86f9f0":"code","5e1202da":"code","cd72f8c5":"code","e0b877ef":"code","7ca36276":"code","8dbbdd77":"code","1ae54ddf":"code","7be1456e":"code","27eb84ae":"code","4816527c":"code","b4ca318a":"markdown","64bb8a95":"markdown","a8ad0c36":"markdown","a2c6a913":"markdown","35721be8":"markdown","6165eeed":"markdown","c301754a":"markdown","6124fe27":"markdown","e6fa576b":"markdown","6f8ca04e":"markdown","ece92c40":"markdown","5e1c23bd":"markdown","77b4e991":"markdown","5418f001":"markdown","8d9a38de":"markdown","2f50e3b7":"markdown","3317c534":"markdown","141d3be8":"markdown","4d4ea557":"markdown","591c90b0":"markdown","dddd4266":"markdown"},"source":{"fc3f54de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\n#import janestreet\n#env = janestreet.make_env() # initialize the environment\n\n#!pip install datatable # Internet is not activated in this competition\n!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl\nimport datatable as dt\n\nDF_FILE = '..\/input\/jane-street-first-time-series-dataframe-save\/dataframe.pickle'\nLOAD_DF = True\n\nREMOVE_OUTLIERS = False\n\nimport gc\nimport pickle\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nINPUT_DIR = '\/kaggle\/input\/jane-street-market-prediction\/'\n\npd.set_option('display.max_rows', 2000)","8a2d05b0":"from statsmodels.tsa.stattools import adfuller","d810d298":"%%time\nif (LOAD_DF != True):\n    # Thanks to his notebook for this fast loading : https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\n    train_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\n    df = train_data_datatable.to_pandas()\n\n    # Thanks to this notebook to gain memory usage : https:\/\/www.kaggle.com\/jorijnsmit\/one-liner-to-halve-your-memory-usage\n    float64_cols = df.select_dtypes(include='float64').columns\n    mapper = {col_name: np.float32 for col_name in float64_cols}\n    df = df.astype(mapper)\n    \n    del train_data_datatable\n    \n    df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n    \n    # Temporal features\n    FEATURES_FOR_MACD = ['feature_'+str(i) for i in range(1,130)]\n    \n    for feature in FEATURES_FOR_MACD:\n        df.loc[:, feature + '_macd'] = df[feature].ewm(span=12, adjust=False).mean().astype('float32') # Short term exponential moving average\\\n        - df[feature].ewm(span=26, adjust=False).mean().astype('float32') # Short term exponential moving average\n\n        df.loc[:, feature + '_macd_minus_signal'] = df[feature + '_macd'] - df[feature + '_macd'].ewm(span=9, adjust=False).mean().astype('float32')\n        gc.collect()\nelse:\n    with open(DF_FILE, 'rb') as f:\n        df = pickle.load(f)   ","9ec44ae0":"df['date'].max()","8b0db177":"df.drop(index=df[df['date'] <= 490].index, inplace=True)","80194295":"df.shape","11daec79":"df_train = df","d05a4d68":"#df.shape","a8c219c5":"#train_size = int(df.shape[0] * 0.90)","e2f3f97b":"#train_size","8d7ce7fa":"#df.iloc[0:train_size-1, :]","162f869b":"#df_train = df.iloc[0:train_size-1, :].copy(deep=True)\n#y_train = df.iloc[0:train_size-1]['resp_positive'].copy(deep=True)\n\n#df_test = df.iloc[train_size:df.shape[0]-1].copy(deep=True) \n#df_test_origin = df_test.copy(deep=True)\n#y_test = df.iloc[train_size:df.shape[0]-1]['resp_positive'].copy(deep=True)","0f09c73e":"#del df\n#gc.collect()","ef628308":"cols_with_missing_train = [col for col in df_train.columns if df_train[col].isnull().any()]","869de434":"REMOVE_OUTLIERS = True","8f8cf41b":"if (REMOVE_OUTLIERS == True):\n    for col in cols_with_missing_train:\n        #df_train[col].fillna(-999, inplace=True) \n        pass # No fill NA at -999 : that would impair feature visualization\n    \n    df_train.dropna(axis=0, inplace=True)","b1a66f4c":"#if (REMOVE_OUTLIERS == True):\n#    for col in cols_with_missing_train:\n#        df_test[col].fillna(-999, inplace=True) ","cecda3d3":"gc.collect()","47d63516":"df_train.shape","7e0aeba9":"FEATURES_LIST = ['feature_'+str(i) for i in range(130)] + ['feature_'+str(i)+'_macd' for i in range(1, 130)] +  ['weight']\nFEATURES_LIST_RESP = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nFEATURES_LIST_ORIGIN = ['feature_'+str(i) for i in range(130)] +  ['weight']\n","80e20756":"# Thanks to https:\/\/www.kaggle.com\/mlconsult\/feature-visualization.  I added some smoothing.\ndf_train[FEATURES_LIST_RESP + FEATURES_LIST_ORIGIN].ewm(span = 1000).mean().plot(kind='line', subplots=True, grid=True, title=\"Visualize \", sharex=True, sharey=False, legend=True,figsize=(15,200));","d828a3c2":"# Thanks to https:\/\/www.kaggle.com\/mlconsult\/feature-visualization\ndf_train[FEATURES_LIST_RESP + FEATURES_LIST_ORIGIN].plot(kind='line', subplots=True, grid=True, title=\"Visualize \", sharex=True, sharey=False, legend=True,figsize=(15,200));","8dd63630":"df_train.shape","c69ad11c":"df_train_subsample = df_train.iloc[0:20]\n\nexp1 = df_train_subsample['feature_1'].ewm(span=12, adjust=False).mean() # Short term exponential moving average\nexp2 = df_train_subsample['feature_1'].ewm(span=26, adjust=False).mean() # Long term exponential moving average\n\nmacd = exp1 - exp2\n\nexp3 = macd.ewm(span=9, adjust=False).mean() # Signal line\nresp = df_train_subsample['resp'] * df_train_subsample['feature_1'].mean() \/ np.abs(df_train_subsample['resp'].mean()) # Resp augmented to fit mean of feature 1 for better visualisation\n\n\n\nplt.figure(figsize=(20,10))\n\nplt.plot(df_train_subsample.ts_id, df_train_subsample.feature_1, label='Feature 1')\nplt.plot(df_train_subsample.ts_id, macd, label='Feature 1 MACD', color='orange')\nplt.plot(df_train_subsample.ts_id, exp3, label='Signal Line', color='Magenta')\n#plt.plot(df_train_subsample.ts_id, resp, label='Resp', color='red')\nplt.legend(loc='upper left')\nplt.show()","0a0f5572":"def Augmented_Dickey_Fuller_Test_func(series , column_name):\n    print (f'Results of Dickey-Fuller Test for column: {column_name}')\n    dftest = adfuller(series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n    if dftest[1] <= 0.05:\n        print(\"Conclusion:====>\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data is stationary\")\n        \n        return(True)\n    \n    else:\n        print(\"Conclusion:====>\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data is non-stationary\")\n        \n        return(False)","117effa2":"[Augmented_Dickey_Fuller_Test_func(df_train[feat], feat) for feat in FEATURES_LIST_RESP]","8111a26a":"feats_stationary_bool = [Augmented_Dickey_Fuller_Test_func(df_train[feat], feat) for feat in FEATURES_LIST_ORIGIN]","8fe44dc5":"[i for i, x in enumerate(feats_stationary_bool) if (x == False)]","b53cd4d3":"fig, axs = plt.subplots(figsize=(64, 64))\nplt.title('Corr\u00e9lation des features (coefficient de spearman)')\nsns.heatmap(df_train[FEATURES_LIST + FEATURES_LIST_RESP].corr(method='spearman'), cmap=sns.diverging_palette(2, 255, n=20), square=True, center=0)\nplt.show()","9d857263":"step = 500","16d19cbb":"y_autocorr = [df_train['resp'].autocorr(lag=x) for x in range(step)]","1ef36571":"plt.scatter(range(1,step), y_autocorr[1:])","13c75064":"np.argmax(y_autocorr[1:])","b2204da3":"y_autocorr = [[df_train[feat].autocorr(lag=x) for x in range(step)] for feat in FEATURES_LIST_ORIGIN]","421fcb3e":"argmax_feat_corrs = [np.argmax(autocorr_feat[1:]) for autocorr_feat in y_autocorr]","e558a3c9":"argmax_feat_corrs","a22704de":"feats_todisplay = range(130)\n\nn_step = 0\nfor n_feat in feats_todisplay:\n    #print(f'Feature {n_feat}:')\n    \n    plt.plot(range(len(y_autocorr[n_feat][1:100])), y_autocorr[n_feat][1:100])\n    plt.title(f'Feature {n_step} autocorrelation value accross steps, starting with 1')\n    plt.show()\n    \n    n_step += 1\n    \n    '''\n    for n_step in range(len(y_autocorr[n_feat][1:])):\n        corr_value_for_step = y_autocorr[n_feat][1:][n_step]\n\n        print(f'Correlation coefficient for step {n_step+1} : {corr_value_for_step}')\n        \n        \n        if (n_step > 10):\n            break\n    '''","e5db62bd":"y_autocorr = [[df_train[feat].autocorr(lag=x) for x in range(step)] for feat in FEATURES_LIST_RESP]","ea274b13":"argmax_feat_resp_corrs = [np.argmax(autocorr_feat[1:]) for autocorr_feat in y_autocorr]","0ce474e5":"argmax_feat_resp_corrs","834912b0":"feats_todisplay = range(5)\n\nn_step = 0\nfor n_feat in feats_todisplay:\n    #print(f'Feature {n_feat}:')\n    \n    plt.plot(range(len(y_autocorr[n_feat][1:500])), y_autocorr[n_feat][1:500])\n    plt.title(f'Feature resp {n_step} autocorrelation value accross steps, starting with 1')\n    plt.show()\n    \n    n_step += 1","4b86f9f0":"!pip uninstall typing -y\n!pip install mlfinlab","5e1202da":"import mlfinlab as mlfin","cd72f8c5":"help(mlfin.fracdiff.FractionalDifferentiation)","e0b877ef":"frac = mlfin.fracdiff.FractionalDifferentiation()","7ca36276":"DIFF_AMT = 0.1\n\nfor feat in FEATURES_LIST_ORIGIN:\n    df_train[feat+'_frac'] = frac.frac_diff_ffd(df_train[[feat]], 0.1)","8dbbdd77":"FEATURES_LIST_WITH_FRAC = []\nfor i in range(130):\n    FEATURES_LIST_WITH_FRAC.append('feature_'+str(i))\n    FEATURES_LIST_WITH_FRAC.append('feature_'+str(i)+'_frac')","1ae54ddf":"# Thanks to https:\/\/www.kaggle.com\/mlconsult\/feature-visualization.  I added some smoothing.\ndf_train[FEATURES_LIST_WITH_FRAC].ewm(span = 1000).mean().plot(kind='line', subplots=True, grid=True, title=\"Visualize features with FFD smoothed\", sharex=True, sharey=False, legend=True,figsize=(15,400));","7be1456e":"feat1_diff = frac.frac_diff_ffd(df_train[['feature_1']], 0.1)","27eb84ae":"df_train['feature_1'].ewm(span = 1000).mean().plot(kind='line', title='Feature 1 smoothed', legend=True, figsize=(15, 5));","4816527c":"feat1_diff.ewm(span = 1000).mean().plot(kind='line', title='Feature 1 fractionally differenciated smoothed', legend=True, figsize=(15, 5));","b4ca318a":"# Remove most of the data : keep 9 days","64bb8a95":"This notebook does various explorations to give some insights about some feature engineering techniques regarding time series that might be possible :  \n- Features visualisation against time on a subsample of 9 days (with and without some exponential average smoothing)\n- MACD calculation and visualisation example on 1 feature (see this for example about MACD : https:\/\/towardsdatascience.com\/implementing-macd-in-python-cc9b2280126a)   \n- Stationarity test of features\n- Spearman correlation of features\n- Pearson autocorrelation, to have an idea of the step between values of the time series\n- Visualisation of fractionaly differenciated features with mlfinlab package (implementation of the FFD method : see https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/198994 for more details)","a8ad0c36":"# Split train test","a2c6a913":"# Data clean","35721be8":"# Stationarity test","6165eeed":"## Plot of correlation coefficients accross steps :","c301754a":"# Feature definition","6124fe27":"## MACD Example with feature 1 : check when MACD goes above signal","e6fa576b":"# FFD method test (Fractional differenciating)","6f8ca04e":"### We see a vast majority of max auto correlation indices of 0 which means step 1. Which means features have probably already been pre-engineered so that they have a step of 1.","ece92c40":"# Feature autocorrelations","5e1c23bd":"No split train test in this notebook","77b4e991":"# Conclusion\nSince data seems stationary, it probably means that trends have already been extracted,   \nAnd since step between features is 1, time series are probably consistent with time stamp indexes.   \nWhich confirms some refined feature engineering has already been done.   \n\nMACD and FFD techniques may not be of any use in this competition since they may have already been included in the feature engineering already done.  \nHowever it could still be worth trying to include those features in a model to see what happens : MACD minus signal seems to give some information about local trends in the visualisation example of feature 1.  ","5418f001":"# Compute step indice that has maximal pearson auto correlation","8d9a38de":"## Plot of resp correlation coefficients accross steps :","2f50e3b7":"# Compute step indice among resp features that has maximal pearson auto correlation","3317c534":"# Features smooth plot","141d3be8":"### Result: all features are stationary (empty list of True values)","4d4ea557":"# Load data","591c90b0":"# Jane street time series exploration and statistics","dddd4266":"# Feature correlations"}}