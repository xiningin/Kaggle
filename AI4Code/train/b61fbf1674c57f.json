{"cell_type":{"bbc25585":"code","545dd13b":"code","c887df78":"code","10b7a15a":"code","b5e919e0":"code","a3fbfda9":"code","c78669c4":"code","ea0d7a7e":"code","a07039d5":"code","b2272ecd":"code","9ae41867":"code","5b33e96a":"code","79c8fb32":"code","e2c2ddac":"code","df783938":"code","4be4e00d":"code","7ff4b7d1":"code","33fee442":"markdown","e0afd606":"markdown","a8128f83":"markdown","14343b21":"markdown","7a54a67a":"markdown","21304ad6":"markdown","b56cd263":"markdown","d0efc633":"markdown","5040c35c":"markdown","078b3643":"markdown","20e3b2ad":"markdown"},"source":{"bbc25585":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport string\nfrom random import randint\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, LSTM, Embedding, Dense, Dropout\n\nfrom keras.utils.vis_utils import plot_model","545dd13b":"def load_doc(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text","c887df78":"in_file = '..\/input\/the-republic-by-plato\/pg1497.txt'\ndoc = load_doc(in_file)\nprint(doc[:1000])","10b7a15a":"[m.start() for m in re.finditer('BOOK I\\.', doc)]","b5e919e0":"[m.start() for m in re.finditer('End of', doc)]","a3fbfda9":"books = doc[553615:1195644]\n\nprint(books[:300])","c78669c4":"def remove_punctuation(text):\n    PUNCT_TO_REMOVE = string.punctuation\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))","ea0d7a7e":"cleaned_book = remove_punctuation(books)\n\ntokens = cleaned_book.split()\ntokens = [word for word in tokens if word.isalpha()]\ntokens = [word.lower() for word in tokens]\n\nprint('Tokens :\\n',tokens[:10])\nprint('No of Tokens : ',len(tokens))\nprint('Unique Tokens : ',len(set(tokens)))","a07039d5":"sequences = list()\nfor i in range(0,len(tokens),51):\n    seq = tokens[i:i+51]\n    line = ' '.join(seq)\n    sequences.append(line)\n    \nprint('Number of Sequences : ',len(sequences))","b2272ecd":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sequences)\nencoded = tokenizer.texts_to_sequences(sequences)\nencoded = np.array(encoded[:-1])","9ae41867":"vocab_size = len(tokenizer.word_index) + 1\nvocab_size","5b33e96a":"X , y = encoded[:,:-1], encoded[:,-1]\n\ny = to_categorical(y, num_classes=vocab_size)\n\nseq_length = X.shape[1]\n\nprint('Shape of X : ', X.shape)\nprint('Shape of y : ', y.shape)","79c8fb32":"def define_model(vocab_size, inputlength):\n    model = Sequential()\n    \n    model.add(Embedding(vocab_size, 100, input_length=seq_length))\n    model.add(LSTM(100, return_sequences=True,dropout=0.1))\n    model.add(LSTM(100, return_sequences=True,dropout=0.1))\n    model.add(LSTM(100))\n    model.add(Dense(100))\n    model.add(Dropout(0.2))\n    model.add(Dense(vocab_size, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","e2c2ddac":"model = define_model(vocab_size, seq_length)","df783938":"model.fit(X, y, batch_size=128, epochs=200)","4be4e00d":"def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n    result = list()\n    in_text = seed_text\n    for _ in range(n_words):\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        yhat = model.predict_classes(encoded, verbose=0)\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        in_text += ' ' + out_word\n        result.append(out_word)\n    return ' '.join(result)","7ff4b7d1":"seed_text = sequences[randint(0,len(sequences))]\nprint(seed_text + '\\n')\n# generate new text\ngenerated = generate_seq(model, tokenizer, seq_length, seed_text, 50) \nprint(generated)","33fee442":"<h3><center>Extracting only Books from the doc","e0afd606":"<h3><center> X & y","a8128f83":"<h3><center>Encode Sequences","14343b21":"<h3><center>Loading File","7a54a67a":"<h3><center>Importing Libraries<\/center><\/h3>","21304ad6":"<h3><center>Cleaning Text","b56cd263":"<h3><center>Generating Text","d0efc633":"<h3><center>Modelling<\/center><\/h3>\n<ul>\n    <li>\n        We can now define and fit our language model on the training data.\n<li>\n    The learned embedding needs to know the size of the vocabulary and the length of input sequences. It also has a parameter to specify how many dimensions will be used to represent each word. \nThat is, the size of the embedding vector space.\nCommon values are 50, 100, and 300. We will use 50.\n    <li>\n        We will use a three LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n        <li>\nA dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. \n            <li>\n                The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. \n                <li>A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities.","5040c35c":"<h3><center>Training Model","078b3643":"<h2><center>Introduction<\/center><\/h2>\n\n![image.png](attachment:image.png)\n\n<h3> About the Dataset : <\/h3>\nThe Republic is the classical Greek philosopher Plato\u2019s most famous work. It is structured as a dialog (e.g. conversation) on the topic of order and justice within a city state The entire text is available for free in the public domain.\n<br><br>\n<h3>Language Model Design : <\/h3>\n\nIn this notebook, we will develop a model of the text that we can then use to generate new sequences of text. The language model will be statistical and will predict the probability of each word given an input sequence of text. The predicted word will be fed in as input to in turn generate the next word. A key design decision is how long the input sequences should be. They need to be long enough to allow the model to learn the context for the words to predict. This input length will also define the length of seed text used to generate new sequences when we use the model.\nThere is no correct answer. With enough time and resources, we could explore the ability of the model to learn with differently sized input sequences. Instead, we will pick a length of 50 words for the length of the input sequences, somewhat arbitrarily. We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence.","20e3b2ad":"<h2><center>Neural Language Model for Text Generation<\/center><\/h2>"}}