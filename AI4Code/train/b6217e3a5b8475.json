{"cell_type":{"c44df586":"code","e6aab61a":"code","091cfeb3":"code","dbd65a95":"code","b0378a51":"code","257920b8":"code","1c321a7e":"code","f143f855":"code","a510d63d":"code","52a13616":"code","fdb64bda":"code","f00cf706":"code","d0a71e79":"code","fb619a52":"code","b35ba118":"code","fcac95ca":"code","b9eda339":"code","e95589ae":"code","3631d0a5":"code","16f99b64":"code","d1a07544":"code","763fb839":"code","7237cc57":"code","b652b7d3":"code","787f1507":"code","9841544d":"code","2aa664bf":"code","b7fbd338":"code","721682ad":"code","4f423cf2":"code","e7499ceb":"code","f5849a8b":"code","b7bddeef":"code","c7ad9091":"code","cd46c5ff":"code","0e815a08":"code","9f59c2f4":"code","4999955f":"code","324d71c1":"code","d1595a30":"code","b456216e":"code","3d21bb6d":"code","d50d9473":"code","190e0302":"code","b06fc8d2":"code","7b1da95e":"code","74e63ea8":"code","f773e7f0":"code","cf1bc21b":"code","276fdfeb":"code","7e1789f6":"code","80bb2423":"code","c7f4a320":"code","03b3375e":"code","dd9781ac":"code","3f41145d":"code","b9bfd9f9":"code","6b4dc3f6":"code","6a6817f4":"code","c272aa4d":"code","f2c8e113":"code","79f767e4":"code","0cb99341":"code","f79af663":"code","2c06a781":"code","81f326da":"code","4755e816":"code","1c36f017":"code","ffccd302":"code","89b8d518":"code","abf57ab2":"code","7936ad61":"code","98bc252b":"code","5ca793a5":"code","fbe4aa9d":"code","2613587c":"code","2a23102c":"code","08756696":"code","4cf28aac":"code","622682e4":"code","685f3a3e":"code","030e6be9":"code","2211aafa":"code","05621f73":"code","63b4ee53":"code","8c21783f":"code","cbe5f09a":"code","54de04f2":"code","368663c0":"code","9709f5ec":"code","dc5472a8":"code","97ce6edf":"code","66e9ac98":"code","a8d24335":"code","fc2002ad":"code","d7988aac":"code","bb0a1240":"code","edf50736":"code","2e0aced6":"code","9ecdc5f4":"code","dde50d00":"code","b1ee56e0":"code","214fb1b7":"code","278a9a22":"code","2d50b023":"code","11a48ef0":"code","188c1683":"code","73bda59b":"code","98f9bcc4":"code","e92249b2":"code","03d11498":"code","3ac34de4":"code","6477398c":"markdown","52b08d4a":"markdown","0bce72ec":"markdown","fc73189a":"markdown","f6fdcc29":"markdown","6aa6529e":"markdown","1a4135ee":"markdown","54de38d5":"markdown","180745c2":"markdown","d91fefe1":"markdown","2c0bafa9":"markdown","77612467":"markdown","af3fc127":"markdown","4e5a38ff":"markdown","2112ca59":"markdown","f2f07a50":"markdown","ebd8b7ec":"markdown","4e00be08":"markdown","8e10785b":"markdown","0ca917f2":"markdown","6b8ce5bd":"markdown","244bf891":"markdown","1a7f32bf":"markdown","1fec624a":"markdown","446216ed":"markdown","ad88342d":"markdown","9527534d":"markdown","973c39e9":"markdown","f6c47445":"markdown","8f01c0a0":"markdown","5f923f02":"markdown","ccb4555e":"markdown","d280ba2f":"markdown","94b01e0e":"markdown","b516fe95":"markdown","5c03569f":"markdown","680dc0bf":"markdown","2ff7b4ec":"markdown","081219f1":"markdown"},"source":{"c44df586":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6aab61a":"#import Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\nfrom mpl_toolkits.mplot3d import Axes3D\nimport missingno\nimport pylab \nimport scipy.stats as stats\nimport lightgbm as lgb\nimport feature_engine\n# Pretty display for notebooks\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (20,10)\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom feature_engine import missing_data_imputers as msi\nfrom feature_engine import variable_transformers as vt\nfrom feature_engine import outlier_removers as outr\nfrom catboost import CatBoostClassifier, Pool, cv","091cfeb3":"data=pd.read_csv(\"..\/input\/train.csv\")\nt_data=pd.read_csv(\"..\/input\/test.csv\")\n","dbd65a95":"data.head()","b0378a51":"data.info()","257920b8":"# Describing all the Numerical Features\ndata.describe()","1c321a7e":"data.describe(include=['O'])","f143f855":"\ndef plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=dataset)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(dataset[column])\n            plt.xticks(rotation=25)\n    \nplot_distribution(data.dropna(), cols=3, width=20, height=20, hspace=0.45, wspace=0.5)","a510d63d":"# How many missing values are there in our dataset?\nmissingno.matrix(data, figsize = (10,5))\nmissingno.bar(data, sort='ascending', figsize = (10,5))","52a13616":"vars_with_na=[]\ndef findVariablesWithMissingValues(df):\n    # make a list of the variables that contain missing values\n    global vars_with_na\n    vars_with_na = [var for var in df.columns if df[var].isnull().sum()>1]\n\n    # print the variable name and the percentage of missing values\n    for var in vars_with_na:\n        print(var, np.round(df[var].isnull().mean(), 3),  ' % missing values')\n\nfindVariablesWithMissingValues(data)","fdb64bda":"#  data[[\"Age\",\"Survived\"]\n# mn=data.Age.mean()\n# data.Age.fillna(mn)\n# data['Age']=np.select([data[\"Age\"]<=10,data[\"Age\"]>35],[\"Child\",\"Old\"],default=\"Adult\")\n# data['Age']=np.select([data[\"Age\"]<=10,data[\"Age\"]>35,(data[\"Age\"]>10) & (data[\"Age\"]<=35)],[\"Child\",\"Old\",\"Adult\"],default=\"NaN\")\n# data[[\"Age_\",\"Age\"]].head()\n# data.drop(\"Age\",axis=1,inplace=True)","f00cf706":"l=['Pclass', 'Sex', 'Age', 'SibSp','Parch', 'Embarked']\ndef relBetVarSur(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(len(dataset)) \/ cols)\n    for i, column in enumerate(dataset):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        data.groupby([column,'Survived'])[\"Sex\"].count().plot.bar()\n#         substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n#         g.set(yticklabels=substrings)\n        plt.xticks(rotation=25)\n    \nrelBetVarSur(l, cols=2, width=20, height=20, hspace=0.45, wspace=0.5)","d0a71e79":"def analyse_na_value(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(len(dataset)) \/ cols)\n    for i, column in enumerate(dataset):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        df = data.copy()\n        df[column] = np.where(df[column].isnull(), 1, 0)\n        df.groupby([column,'Survived'])[\"Sex\"].count().plot.bar()\n#         substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n#         g.set(yticklabels=substrings)\n        plt.xticks(rotation=25)\n    \nanalyse_na_value(vars_with_na, cols=2, width=20, height=20, hspace=0.45, wspace=0.5)\n\n","fb619a52":"# list of numerical variables\nnum_vars = [var for var in data.columns if data[var].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(num_vars))\n\n# visualise the numerical variables\ndata[num_vars].head()","b35ba118":"#  list of discrete variables\ndiscrete_vars = [var for var in num_vars if len(data[var].unique())<20 ]\n\nprint('Number of discrete variables: ', len(discrete_vars))","fcac95ca":"data[discrete_vars].head().drop('Survived',axis=1)","b9eda339":"data.groupby([\"Pclass\"]).Survived.count()","e95589ae":"def analyse_discrete(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(len(dataset)) \/ cols)\n    for i, column in enumerate(dataset):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        df = data.copy()\n        df.groupby([column]).Survived.count().plot.bar()\n#         substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n#         g.set(yticklabels=\"Survived\")\n        ax.set_ylabel('No of Passengers')\n        plt.xticks(rotation=25)\n    \nanalyse_discrete(discrete_vars, cols=2, width=20, height=20, hspace=0.45, wspace=0.5)\n","3631d0a5":"# list of continuous variables\ncont_vars = [var for var in num_vars if var not in discrete_vars]\n\nprint('Number of continuous variables: ', len(cont_vars))","16f99b64":"data[cont_vars].head()","d1a07544":"def analyse_continous(df, var):\n    df = df.copy()\n    plt.figure(figsize=(20,6))\n    plt.subplot(1, 2, 1)\n    df[var].hist(bins=20)\n    plt.ylabel('Survived')\n    plt.xlabel(var)\n    plt.title(var)\n    plt.subplot(1, 2, 2)\n    stats.probplot(df[var], dist=\"norm\", plot=pylab)\n    plt.show()\n    \n    \nfor var in cont_vars[1:]:\n    analyse_continous(data, var)","763fb839":"def find_outliers(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(len(dataset)) \/ cols)\n    for i, column in enumerate(dataset):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        df = data.copy()\n        df.boxplot(column=column)\n#         substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n#         ax.set(yticklabels=\"Survived\")\n        ax.set_ylabel(column)\n        plt.xticks(rotation=25)\n    \nfind_outliers(cont_vars[1:], cols=2, width=20, height=20, hspace=0.45, wspace=0.5)\n\n\n","7237cc57":"data.drop(\"Name\",axis=1)\n\n\ncat_vars = [var for var in data.columns if data[var].dtypes=='O']\n\nprint('Number of categorical variables: ', len(cat_vars))","b652b7d3":"data[cat_vars].head()","787f1507":"\nfor var in cat_vars:\n    print(var, len(data[var].unique()), ' categories')","9841544d":"features=data.drop([\"Survived\",\"Name\",\"PassengerId\",\"Ticket\"],axis=1)\nfeatures_test=t_data.drop([\"Name\",\"PassengerId\",\"Ticket\"],axis=1)\nlabels=data.Survived","2aa664bf":"features_test.iloc[152]","b7fbd338":"features_test.info()","721682ad":"\n# X_train, X_test, y_train, y_test = train_test_split(features, labels,\n#                                                     test_size=0.2,\n#                                                     random_state=0) # we are setting the seed here\n# X_train.shape, X_test.shape","4f423cf2":"features.Age.head()","e7499ceb":"median_imputer = msi.MeanMedianImputer(imputation_method='median', variables = ['Age',\"Fare\"])\nmedian_imputer.fit(features)","f5849a8b":"median_imputer.imputer_dict_","b7bddeef":"features = median_imputer.transform(features)","c7ad9091":"features_test = median_imputer.transform(features_test)","cd46c5ff":"plt.figure()\nfeatures.Age.plot(kind=\"kde\")\nplt.show()","0e815a08":"findVariablesWithMissingValues(features)\n# features = median_imputer.transform(features_test)\n","9f59c2f4":"frequentLabel_imputer = msi.FrequentCategoryImputer(variables = ['Embarked'])\nfrequentLabel_imputer.fit(features)\nfeatures=frequentLabel_imputer.transform(features)\n\nfrequentLabel_imputer.imputer_dict_","4999955f":"features_test=frequentLabel_imputer.transform(features_test)","324d71c1":"findVariablesWithMissingValues(features)","d1595a30":"features.Embarked.value_counts().plot(kind=\"bar\")","b456216e":"features.Cabin.isnull().sum()\n","3d21bb6d":"def  findCabin(val):\n    res=str()\n    if val is not np.nan:\n        match=re.findall(\"[A-Z]\",str(val))\n        for x in match:\n            if x not in res:\n                res+=x\n        return res\n    return np.nan\n                \n\nCabin=features.Cabin.apply(findCabin)\nCabin.isnull().sum()","d50d9473":"features.Cabin=Cabin\nfeatures.Cabin[128]","190e0302":"features_test.Cabin=features_test.Cabin.apply(findCabin)\nfeatures_test.head()","b06fc8d2":"\naddLabel_imputer = msi.CategoricalVariableImputer(variables = ['Cabin'])\naddLabel_imputer.fit(features)\nfeatures=addLabel_imputer.transform(features)\n","7b1da95e":"features_test=addLabel_imputer.transform(features_test)","74e63ea8":"findVariablesWithMissingValues(features)","f773e7f0":"\nfeatures.Cabin.value_counts().plot(kind=\"bar\")","cf1bc21b":"features.info()","276fdfeb":"features.Age.min(),features.Age.max()","7e1789f6":"analyse_continous(features,\"Age\")","80bb2423":"# bct = vt.BoxCoxTransformer(variables = [\"Age\"])\n# bct.fit(features)\n# features=bct.transform(features)","c7f4a320":"# windsoriser = outr.Windsorizer(distribution='gaussian', tail='right', fold=3, variables = ['Age'])\n# windsoriser.fit(features)\n# # windsoriser.right_tail_caps_\n# features=windsoriser.transform(features)\n\ndef categAge(df):\n    df['Age']=np.select([df[\"Age\"].astype(int)<=10,df[\"Age\"].astype(int)>35],[\"Child\",\"Old\"],default=\"Adult\")","03b3375e":"categAge(features)\nfeatures.head()","dd9781ac":"# # features_test=windsoriser.transform(features_test)\ncategAge(features_test)\nfeatures_test.head()","3f41145d":"# analyse_continous(features,\"Age\")\n# features.Age.min(),features.Age.max()","b9bfd9f9":"analyse_continous(features,\"Fare\")","6b4dc3f6":"features.Fare.min(),features.Fare.max()","6a6817f4":"# et = vt.ExponentialTransformer(variables = ['Fare'])\n# et.fit(features)\n# features=et.transform(features)","c272aa4d":"# analyse_continous(features,\"Fare\")","f2c8e113":"windsoriser = outr.Windsorizer(distribution='gaussian', tail='right', fold=3, variables = ['Fare'])\nwindsoriser.fit(features)\n# windsoriser.right_tail_caps_\nfeatures=windsoriser.transform(features)","79f767e4":"features_test=windsoriser.transform(features_test)","0cb99341":"features.Fare.min(),features.Fare.max()","f79af663":"# for var in cont_vars[1:]:\n#     find_outliers(features, var)","2c06a781":"from feature_engine import categorical_encoders as ce","81f326da":"# features.columns","4755e816":"features.Pclass.value_counts()","1c36f017":"features.Sex.value_counts()","ffccd302":"# features.Sex=features.Sex.apply(lambda x: 1 if x==\"male\" else 0)#,features.Sex","89b8d518":"# features_test.Sex=features_test.Sex.apply(lambda x: 1 if x==\"male\" else 0)#,features.Sex\n# features_test.head()","abf57ab2":"features.Sex.value_counts()","7936ad61":"features.head()","98bc252b":"ohe_enc = ce.OneHotCategoricalEncoder(top_categories = None, variables = [\"Age\",\"Embarked\",\"Sex\",\"Cabin\"], drop_last = False)\nohe_enc.fit(features)\nfeatures=ohe_enc.transform(features)\nfeatures_test=ohe_enc.transform(features_test)\nohe_enc.encoder_dict_","5ca793a5":"features.head()","fbe4aa9d":"features_test.head()","2613587c":"# count_enc = ce.CountFrequencyCategoricalEncoder(encoding_method = 'frequency',variables = [ 'Cabin'])\n# count_enc.fit(features)\n# features=count_enc.transform(features)\n","2a23102c":"features.head()\n","08756696":"# count_enc.fit(features_test)\n# features_test=count_enc.transform(features_test)\n","4cf28aac":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit_transform(features)\nscaler.transform(features_test)","622682e4":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import plot_tree\nfrom xgboost import plot_importance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\nfrom skopt import BayesSearchCV","685f3a3e":"\nX_train, X_test, y_train, y_test = train_test_split(features, labels,\n                                                    test_size=0.25,\n                                                    random_state=0) # we are setting the seed here\nX_train.shape, X_test.shape","030e6be9":"# def status_print(optim_result):\n#     \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n#     # Get all the models tested so far in DataFrame format\n#     all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n#     # Get current parameters and the best parameters    \n#     best_params = pd.Series(bayes_cv_tuner.best_params_)\n#     print('Model #{}\\nBest Accuracy: {}\\nBest params: {}\\n'.format(\n#         len(all_models),\n#         np.round(bayes_cv_tuner.best_score_, 4),\n#         bayes_cv_tuner.best_params_\n#     ))\n#     # Save all model results\n#     clf_name = bayes_cv_tuner.estimator.__class__.__name__\n#     all_models.to_csv(clf_name + \"_cv_results.csv\")","2211aafa":"\n# bayes_cv_tuner = BayesSearchCV(estimator=xgb.XGBClassifier(\n#     objective='binary:logistic',\n#     eval_metric='logloss',\n#     learning_rate=0.1,\n#     n_estimators= 720,\n#     sub_sample=0.76,\n# #     early_stopping_rounds=106,\n#     max_depth=3,\n#     silent=0),\n#     search_spaces={\n# #     'learning_rate': (0.001, 0.1),\n# #     'n_estimators': (0, 800),\n# #     'sub_sample': (0.5, 1.0),\n#     'early_stopping_rounds': (0, 1000),\n# #     \"max_depth\":  (3, 25),\n# #     \"colsamplebylevel\": (0.6, 1.0),\n# #     'min_child_weight': [1, 7],\n# #     'gamma': (0.05,1.0),\n#     #         'min_child_weight': (15, 20),\n# #         'max_depth': (6, 8),\n# #         'alpha':(0,1.0)\n#     #         'max_delta_step': (0, 20),\n#     #         'subsample': (0.01, 1.0, 'uniform'),\n#     #         'colsample_bytree': (0.01, 1.0, 'uniform'),\n#     #         'colsample_bylevel': (0.01, 1.0, 'uniform'),\n#     #         'reg_lambda': (1e-2, 1000, 'log-uniform'),\n#     #         'reg_alpha': (1e-2, 1.0, 'log-uniform'),\n#     #         'gamma': (1e-2, 0.5, 'log-uniform'),\n#     #         'min_child_weight': (0, 20),\n#     #         'scale_pos_weight': (1e-6, 500, 'log-uniform')\n# },\n# #     scoring='roc',\n#     cv=StratifiedKFold(\n#         n_splits=5,\n#         shuffle=True,\n#         random_state=0),\n#     n_jobs=-1,\n#     n_iter=20,\n#     verbose=500,\n#     refit=True,\n#     random_state=0)\n# result = bayes_cv_tuner.fit(\n#     features.values, labels.values, callback=status_print)\n","05621f73":"bayes_cv_tuner.best_params_","63b4ee53":"# categorical_features_indices = np.where(features.dtypes != np.float)[0]\n# model = CatBoostClassifier(\n#     custom_loss=['Accuracy'],\n#     random_seed=42,\n#     logging_level='Silent'\n# )\n# model.fit(\n#     X_train, y_train,\n#     cat_features=categorical_features_indices,\n#     eval_set=(X_test, y_test),\n# #     logging_level='Verbose',  # you can uncomment this for text output\n#     plot=True\n# )","8c21783f":"# cv_params = model.get_params()\n# cv_params.update({\n#     'loss_function': 'Logloss'\n# })\n# cv_data = cv(\n#     Pool(features, labels, cat_features=categorical_features_indices),\n#     cv_params,\n#     plot=True\n# )\n# print('Best validation accuracy score: {:.2f}\u00b1{:.2f} on step {}'.format(\n#     np.max(cv_data['test-Accuracy-mean']),\n#     cv_data['test-Accuracy-std'][np.argmax(cv_data['test-Accuracy-mean'])],\n#     np.argmax(cv_data['test-Accuracy-mean'])\n# ))\n","cbe5f09a":"# model_without_seed = CatBoostClassifier(iterations=10, logging_level='Silent')\n# model_without_seed.fit(features, labels, cat_features=categorical_features_indices)\n\n# print('Random seed assigned for this model: {}'.format(model_without_seed.random_seed_))","54de04f2":"\n# params = {\n#     'iterations': 500,\n#     'learning_rate': 0.1,\n#     'eval_metric': 'Accuracy',\n#     'random_seed': 42,\n#     'logging_level': 'Silent',\n#     'use_best_model': False\n# }\n# train_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\n# validate_pool = Pool(X_test, y_test, cat_features=categorical_features_indices)","368663c0":"\n# model = CatBoostClassifier(**params)\n# model.fit(train_pool, eval_set=validate_pool)\n\n# best_model_params = params.copy()\n# best_model_params.update({\n#     'use_best_model': True\n# })\n# best_model = CatBoostClassifier(**best_model_params)\n# best_model.fit(train_pool, eval_set=validate_pool);\n\n# print('Simple model validation accuracy: {:.4}'.format(\n#     accuracy_score(y_test, model.predict(X_test))\n# ))\n# print('')\n\n# print('Best model validation accuracy: {:.4}'.format(\n#     accuracy_score(y_test, best_model.predict(X_test))\n# ))","9709f5ec":"# model = CatBoostClassifier(\n#     l2_leaf_reg=int(5.0),\n#     learning_rate=0.1147638000846512,\n#     iterations=500,\n#     eval_metric='Accuracy',\n#     random_seed=42,\n#     logging_level='Silent'\n# )\n# cv_data = cv(Pool(features, labels, cat_features=categorical_features_indices), model.get_params())","dc5472a8":"# print('Precise validation accuracy score: {}'.format(np.max(cv_data['test-Accuracy-mean'])))","97ce6edf":"# xg_cl = xgb.XGBClassifier(objective='binary:logistic',learning_rate=0.01,subsample=0.55,n_estimators=200, seed=123)\nxg_cl = xgb.XGBClassifier(objective='binary:logistic',\n#     eval_metric='auc',\n    learning_rate=0.1,\n#     n_estimators= 720,\n#     sub_sample=0.76,\n#     max_depth=3\n                         )\neval_set = [(X_train, y_train), (X_test, y_test)]\n# eval_set = [(X_test, y_test)]\nxg_cl.fit(X_train, y_train,eval_metric=\"auc\",early_stopping_rounds=20, eval_set=eval_set, verbose=True)\n","66e9ac98":"plot_tree(xg_cl,num_trees=1, rankdir='LR')\nplt.show()","a8d24335":"# plot_importance(xg_cl)\n# plt.show()","fc2002ad":"preds = xg_cl.predict(X_test)","d7988aac":"accuracy = accuracy_score(y_test,preds)\naccuracy","bb0a1240":"results = confusion_matrix(y_test, preds) \nprint(results)","edf50736":"# dmatrix = xgb.DMatrix(data=features,label=labels)\n\n# # params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n# # tuned_params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3, 'max_depth': 10,'subsample': 0.55, 'n_estimators': 200, 'learning_rate': 0.2}\n# # tuned_params = {\"objective\":\"binary:logistic\",'learning_rate': 0.3}\n# # tuned_params = {\"objective\":\"binary:logistic\",\"early_stopping_rounds\":\"6\", \"learning_rate\":\"0.08\", \"max_depth\":\"5\", \"n_estimators\":\"50\"}\n# tuned_params=bayes_cv_tuner.best_params_\n# cv_results = xgb.cv(dtrain=dmatrix, params=tuned_params, nfold=5, num_boost_round=200, metrics=\"error\",as_pandas=True, seed=123)\n\n# # Print the accuracy\n\n# print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(xg_cl, features, labels, cv=kfold)\nresults.mean()","2e0aced6":"# results = xg_cl.evals_result()\n# print(results)","9ecdc5f4":"\nresults = xg_cl.evals_result()\nepochs = len(results['validation_0']['auc'])\nx_axis = range(0, epochs)\n# plot log loss\n\n# fig, ax = plt.subplots(figsize=(8,8))\n# ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n# ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n# ax.legend()\n# plt.ylabel('Log Loss')\n# plt.title('XGBoost Log Loss')\n# plt.show()\n\n# # plot classification error\n# fig, ax = plt.subplots(figsize=(8,8))\n# ax.plot(x_axis, results['validation_0']['error'], label='Train')\n# ax.plot(x_axis, results['validation_1']['error'], label='Test')\n# ax.legend()\n# plt.ylabel('Classification Error')\n# plt.title('XGBoost Classification Error')\n# plt.show()\n\n# plot ROC\nfig, ax = plt.subplots(figsize=(12,12))\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Test')\nax.legend()\nplt.ylabel('Area Under Curve')\nplt.title('XGBoost ROC')\nplt.show()","dde50d00":"# tuned_params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3, 'max_depth': 10,'subsample': 0.55, 'n_estimators': 200, 'learning_rate': 0.2}\nthresholds = sort(xg_cl.feature_importances_)\nmodels = []\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(xg_cl, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = xgb.XGBClassifier(objective='binary:logistic',\n#     eval_metric='logloss',\n    learning_rate=0.1,\n#     n_estimators= 720,\n#     sub_sample=0.76,\n#     max_depth=3\n                                       )\n    selection_model.fit(select_X_train, y_train)\n    # add model to models\n    models.append([selection_model,selection])\n    # eval model\n    select_X_test = selection.transform(X_test)\n    predictions = selection_model.predict(select_X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1],\n    accuracy*100.0))","b1ee56e0":"# Finalize transformations\nfinal_model = models[14][0]\nfinal_selection = models[14][1]\nfinal_X_train = final_selection.transform(X_train)\n\nfinal_X_test = final_selection.transform(X_test)\n\nfinal_y_pred = final_model.predict(final_X_test)\nfinal_predictions = [round(value) for value in final_y_pred]\n\n# Print evaluation metrics\naccuracy = accuracy_score(y_test, final_predictions)\nprint(\"n=%d, Accuracy: %.2f%%\" % (final_X_train.shape[1], accuracy*100.0))\nconfusion_matrix(y_test, final_predictions) ","214fb1b7":"kfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(final_model\n                          , features, labels, cv=kfold)\nresults.mean()","278a9a22":"# selection = SelectFromModel(xg_cl, threshold=0.012, prefit=True)\n# select_X_train = selection.transform(X_train)\n# select_X_test = selection.transform(X_test)\n# # another_model = xgb.XGBClassifier(objective='binary:logistic')\n# another_model = xgb.XGBClassifier(objective='binary:logistic',**bayes_cv_tuner.best_params_)\n\n# another_model.fit(select_X_train, y_train)\n\n# select_y_pred = another_model.predict(select_X_test)\n# select_predictions = [round(value) for value in select_y_pred]\n\n# # Print evaluation metrics\n# accuracy = accuracy_score(y_test, select_predictions)\n# print(\"n=%d, Accuracy: %.2f%%\" % (select_X_train.shape[1], accuracy*100.0))\n# confusion_matrix(y_test, select_predictions) ","2d50b023":"# features_test[features_test.Fare==np.nan]\n# # features_test.head()\n# features_test.isnull().all().all()\n# features.isnull().all().all()","11a48ef0":"features_=final_selection.transform(features)\nfinal_model.fit(features_,labels)\n# final_model_2=xgb.XGBClassifier(objective='binary:logistic',**bayes_cv_tuner.best_params_)\n# final_model_2.fit(features_,labels)\n# another_model_2.fit(features_,labels)\n# model.fit(features,labels, cat_features=categorical_features_indices)\n# another_model_3.fit(features_,labels)","188c1683":"# features_","73bda59b":"\nfeatures_test_=final_selection.transform(features_test)\nprediction = final_model.predict(features_test_)","98f9bcc4":"# final_prediction=prediction+prediction_3\n# final_prediction_=[int(x\/2) for x in final_prediction]\n# prediction_3=prediction_3.astype(int)\n\n# final_prediction_=np.bitwise_and(prediction,prediction_3_b_opt)\n\nfinal_prediction_=prediction","e92249b2":"submission = pd.DataFrame({\n        \"PassengerId\": t_data[\"PassengerId\"],\n        \"Survived\": final_prediction_\n\n    })\n\nsubmission.to_csv('submission.csv', index=False)","03d11498":"submission = pd.read_csv('submission.csv')\nsubmission.head(n=10)","3ac34de4":"len(submission[submission.Survived ==1 ])","6477398c":"# Submission","52b08d4a":"# Analyse the distributions of continuous variables","0bce72ec":"# Feature Fare","fc73189a":"# Build Final Model Using Best Threshold","f6fdcc29":"# Feature Engineering","6aa6529e":"# Plot the distribution of each feature","1a4135ee":"# How many missing values are there in our dataset?","54de38d5":"Separate dataset into train and test","180745c2":"# Realtion Between Variables And Survived","d91fefe1":"# Feature Selection","2c0bafa9":"# Cross Validation","77612467":"# Data Exploration","af3fc127":"# Handling Outliers","4e5a38ff":"# Sex","2112ca59":"# Scaling","f2f07a50":"# Handling Missing Values","ebd8b7ec":"\n**Continuous variables**","4e00be08":"1. # Categorical Variable","8e10785b":"# Feature Age","0ca917f2":"# Outliers","6b8ce5bd":"**Discrete Variables**","244bf891":"# Select The Best Model","1a7f32bf":"**Numerical Variables**","1fec624a":"# Import DataSet","446216ed":"# Hold Out Validation","ad88342d":"# Feature Age","9527534d":"# Describing all the Numerical Features","973c39e9":"# Pclass","f6c47445":"Number of labels: cardinality\n","8f01c0a0":"# CABIN , EMBARKED ,Age","5f923f02":"# Feature Cabin","ccb4555e":"# Describing all the Categorical Features","d280ba2f":"# Trained Model On Whole Data","94b01e0e":"# Plotting Learning Curve","b516fe95":"# Bayesian Optimization","5c03569f":"Pclass Does not need any processing since each class has some weights in context to the problem.","680dc0bf":"# Categorical Variable","2ff7b4ec":"# Modeling","081219f1":"# Feature Embarked"}}