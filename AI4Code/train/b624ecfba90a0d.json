{"cell_type":{"43c50d44":"code","d9901470":"code","25cff84b":"code","5b44e1aa":"code","65346a11":"code","f8a01e68":"code","fb896cae":"code","b2f57e65":"code","41df4e85":"code","e2bef7c2":"code","02505c68":"code","6c737fb0":"code","8127c371":"code","d5c9282b":"code","3a64b6b4":"code","b2775699":"code","5eab8bb1":"code","da9df4b5":"code","e486d4bd":"code","7a2fd217":"code","0b3c5676":"code","ccd3e15c":"code","23166c22":"code","1761fb1b":"code","55dd6de8":"code","4cd33064":"code","6c7ee430":"code","6bd3d2aa":"code","0499d978":"code","fffdf4df":"code","9abf364f":"code","d3caba20":"code","1c46283e":"code","a52049f4":"code","f3e298d2":"code","51aa2bf2":"code","33b3b722":"markdown","1052be3e":"markdown","6c4e123f":"markdown","b261031e":"markdown","4eaab949":"markdown","d3feda3b":"markdown","4c11312b":"markdown","86b5538e":"markdown","01256951":"markdown","0a66dca8":"markdown","5aa616b0":"markdown","3ed5ef87":"markdown","c883c83b":"markdown","e99e1b5f":"markdown","2f38e144":"markdown","a35fdbf0":"markdown","b8ec351e":"markdown","1c7c4eea":"markdown","8860ce00":"markdown","85d32b7b":"markdown","d65a60e7":"markdown","61dc532b":"markdown","b92dd96e":"markdown","87bbf24b":"markdown","96441968":"markdown","37ee6b8a":"markdown","57165419":"markdown"},"source":{"43c50d44":"!pip install git+https:\/\/github.com\/goolig\/dsClass.git","d9901470":"from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport nltk\n#nltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer as Lemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nimport pyLDAvis.gensim\nimport warnings\nwarnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarit\nfrom gensim.models import ldamulticore \nfrom gensim.corpora.dictionary import Dictionary\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pickle\nfrom dsClass.path_helper import *\nprint('great it works')","25cff84b":"#!pip install pyLDAvis\n#!pip install gensim","5b44e1aa":"text = '''The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n\nDuring the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\nMany of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n\nRecent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n\nIn recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'''\n\ncorpus = [line for line in text.splitlines() if line != '']","65346a11":"corpus[0]","f8a01e68":"cv = CountVectorizer(lowercase=False)\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf","fb896cae":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0]","b2f57e65":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words=set(stopwords.words('english')))\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf","41df4e85":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0]","e2bef7c2":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words=set(stopwords.words('english')),ngram_range=(1,2))\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf","02505c68":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0]","6c737fb0":"testText = '''The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural-language understanding by a computer.[6][7][8][9][10] Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.\n\nA year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.[11]\n\nIn 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural-language understanding.[12] This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n\nIn 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input.[13] Instead of phrase structure rules ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.\n\nIn 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field.[14][15] Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process.[16] At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.\n\nIn the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp.[17][18] In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnart.[19]\n\nThe third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, it is debated how much \"understanding\" such systems demonstrate, e.g. according to John Searle, Watson did not even understand the questions.[20]\n\nJohn Ball, cognitive scientist and inventor of Patom Theory supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional natural language processing. \"To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence \u2013 just like a 3-year-old does without guesswork\" Patom Theory'''\n\ntestCorpus = [line for line in testText.splitlines() if line != '']","8127c371":"testTermMatrix = cv.transform(testCorpus)\ntestDf = pd.DataFrame(data=testTermMatrix.toarray(),columns=cv.get_feature_names())","d5c9282b":"print (testCorpus[0])\ntestDf.iloc[0][testDf.iloc[0]>0]","3a64b6b4":"lemmatizer = Lemmatizer()\nprint (lemmatizer.lemmatize('ate',pos='v'))\nprint(lemmatizer.lemmatize('leaves'))","b2775699":"stemmer = PorterStemmer()\nprint (stemmer.stem('ate'))\nprint(stemmer.stem('leaves'))","5eab8bb1":"# Load some categories \ncategories = [\n    'sci.space',\n    'alt.atheism',\n    'comp.graphics',\n    'rec.sport.baseball'\n]\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ntrainDataset = fetch_20newsgroups(categories=categories,subset='train',shuffle=True, remove=('headers', 'footers', 'quotes'))\ntestDataset = fetch_20newsgroups(categories=categories,subset='test',shuffle=True, remove=('headers', 'footers', 'quotes'))\nprint(\"%d documents for training\" % len(trainDataset.data))\nprint(\"%d documents for testing\" % len(testDataset.data))\nprint(\"%d categories\" % len(trainDataset.target_names))","da9df4b5":"def filterSmallDocs(docs,targets):\n    indices = [i for i in range(0,len(docs)) if len(docs[i].split())>20]\n    filteredDocs = [docs[i] for i in indices]\n    filteredTarget = [targets[i] for i in indices]\n    return filteredDocs,filteredTarget","e486d4bd":"trainDocs, trainTarget = filterSmallDocs(trainDataset.data,trainDataset.target)\ntestDocs, testTarget = filterSmallDocs(testDataset.data,testDataset.target)","7a2fd217":"pd.Series(trainTarget).value_counts()","0b3c5676":"pd.Series(testTarget).value_counts()","ccd3e15c":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words='english',min_df=50, max_df=0.8, ngram_range=(1,2))\ndata = cv.fit_transform(trainDocs)\ndata = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\ndata.iloc[0:3]","23166c22":"cols = data.columns\nbt = data.apply(lambda x: x > 0)\nbt = bt.apply(lambda x: list(cols[x.values]), axis=1)\nbt[0:3]","1761fb1b":"dictionary = Dictionary(bt)\n# convert tokenized documents into a document-term matrix\ncorpusTopicModeling = [dictionary.doc2bow(text) for text in bt]\ncorpusTopicModeling[0:3]","55dd6de8":"dictionary[0]","4cd33064":"print(\"The model has %d features\" % (len(dictionary)))","6c7ee430":"#ldaModel = ldamulticore.LdaMulticore(corpus=corpus, id2word=dictionary, passes=150, num_topics=4)","6bd3d2aa":"#with open('ldaModel.pickle', 'wb') as handle:\n#    pickle.dump(ldaModel, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open(get_file_path('ldaModel.pickle'), 'rb') as handle:\n    ldaModel = pickle.load(handle)","0499d978":"pyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(ldaModel, corpusTopicModeling,dictionary,R=20,mds=\"tsne\")","fffdf4df":" ldaModel.show_topics(formatted=False)","9abf364f":"#Q1","d3caba20":"#Q2","1c46283e":"#Q3","a52049f4":"#Q4","f3e298d2":"#Q5","51aa2bf2":"#Q6","33b3b722":"### <br> 5. Explain the difference in the evaluations.\n","1052be3e":"# Data preparation ","6c4e123f":"# Tasks\n\nBetter read the word doc due to Hebrew issues. Read the whole task before starting.\n\n### <br> 1. Annotate the topics. I.e., label them based on the word distribution. Both the true labels and predictions by the model.\n\n\u05dc\u05ea\u05ea \u05e9\u05dd \u05dc\u05db\u05dc Topic \u05dc\u05e4\u05d9 \u05e0\u05d9\u05ea\u05d5\u05d7 \u05d9\u05d3\u05e0\u05d9 \u05e9\u05ea\u05e2\u05e9\u05d5 \u05de\u05e4\u05dc\u05d8 \u05d4\u05ea\u05d0\u05d9\u05dd \u05d4\u05e7\u05d5\u05d3\u05de\u05d9\u05dd. \u05dc\u05e9\u05de\u05d5\u05e8 \u05d0\u05ea \u05d4\u05ea\u05e9\u05d5\u05d1\u05d4 \u05d1\u05de\u05e2\u05e8\u05da: topicAnnotation \u05e9\u05d1\u05d5 \u05db\u05dc \u05ea\u05d0 \u05de\u05e1\u05de\u05dc Topic.\n","b261031e":"\u05d4\u05e9\u05ea\u05de\u05e9\u05d5 \u05d1\u05e7\u05d5\u05d3 \u05e9\u05db\u05ea\u05d1\u05ea\u05dd \u05d1\u05e1\u05e2\u05d9\u05e3 2 \u05db\u05d3\u05d9 \u05dc\u05e7\u05d1\u05dc \u05d0\u05ea \u05d4\u05d3\u05d9\u05d5\u05e7 \u05e9\u05dc \u05d4Train set. \u05d9\u05e9 \u05dc\u05d4\u05e9\u05ea\u05de\u05e9 \u05d1\u05db\u05dc \u05d0\u05d7\u05d3 \u05de\u05d4flags","4eaab949":"### <br> 3. Evalute the model on the train dataset.\n","d3feda3b":"\u05db\u05ea\u05d1\u05d5 \u05e4\u05d5\u05e0\u05e7\u05e6\u05d9\u05d4 \u05dc\u05e0\u05d9\u05e7\u05d5\u05d3 \u05d0\u05e9\u05e8 \u05de\u05e7\u05d1\u05dc\u05ea \u05d0\u05ea \u05d4\u05e4\u05e8\u05de\u05d8\u05e8\u05d9\u05dd:\n\u2022\t\u05d4\u05de\u05d5\u05d3\u05dc\n\u2022\t\u05e1\u05d8 \u05de\u05e1\u05de\u05db\u05d9\u05dd\n\u2022\t\u05e1\u05d8 \u05ea\u05d5\u05d0\u05dd \u05e2\u05dd \u05d4\u05e1\u05d9\u05d5\u05d5\u05d2 \u05d4\u05d0\u05de\u05d9\u05ea\u05d9 \u05e9\u05dc \u05d4\u05de\u05e1\u05de\u05db\u05d9\u05dd\n\u2022\tFlag \u05e9\u05dc \u05e1\u05d5\u05d2 \u05d4\u05d0\u05d1\u05d5\u05dc\u05d5\u05e6\u05d9\u05d4 \u2013 \u05de\u05d7\u05e8\u05d5\u05d6\u05ea \u05d0\u05d9\u05e0\u05d3\u05d9\u05e7\u05d8\u05d9\u05d1\u05d9\u05ea \u05d4\u05e7\u05d5\u05d1\u05e2\u05ea \u05d0\u05d9\u05d6\u05d4 \u05e1\u05d5\u05d2 \u05d3\u05d9\u05d5\u05e7 \u05d9\u05e9 \u05dc\u05d7\u05e9\u05d1 (\u05e4\u05d9\u05e8\u05d5\u05d8 \u05d1\u05d4\u05de\u05e9\u05da)\n\u05e9\u05d9\u05de\u05d5 \u05dc\u05d1 \u05e9\u05e1\u05d8 \u05d4\u05de\u05e1\u05de\u05db\u05d9\u05dd \u05d9\u05db\u05d5\u05dc \u05dc\u05d4\u05d9\u05d5\u05ea \u05d4train \u05d0\u05d5 \u05d4 test \u05d0\u05e9\u05e8 \u05d9\u05e6\u05e8\u05e0\u05d5.\n\u05d4\u05e4\u05d5\u05e0\u05e7\u05e6\u05d9\u05d4 \u05de\u05d7\u05d6\u05d9\u05e8\u05d4 \u05d0\u05ea \u05d4\u05d3\u05d9\u05d5\u05e7 (\u05db\u05de\u05d5\u05ea \u05ea\u05e6\u05e4\u05d9\u05d5\u05ea \u05e9\u05e1\u05d5\u05d5\u05d2\u05d5 \u05e0\u05db\u05d5\u05df \u05dc\u05d7\u05dc\u05e7 \u05d1\u05de\u05e1\u05e4\u05e8 \u05d4\u05ea\u05e6\u05e4\u05d9\u05d5\u05ea \u05e9\u05d4\u05e4\u05d5\u05e0\u05e7\u05e6\u05d9\u05d4 \u05e7\u05d9\u05d1\u05dc\u05d4) \u05e9\u05dc \u05d4\u05de\u05d5\u05d3\u05dc \u05dc\u05e4\u05d9 \u05d0\u05d7\u05ea \u05d4\u05d0\u05e4\u05e9\u05e8\u05d5\u05d9\u05d5\u05ea \u05d4\u05d1\u05d0\u05d5\u05ea:\n1.\t\u05d4\u05d7\u05d9\u05d6\u05d5\u05d9 \u05e9\u05dc \u05d4\u05de\u05d5\u05d3\u05dc \u05e0\u05db\u05d5\u05df \u05d0\u05dd \u05d4\u05e7\u05d8\u05d2\u05d5\u05e8\u05d9\u05d4 \u05e9\u05e7\u05d9\u05d1\u05dc\u05d4 \u05d0\u05ea \u05d4\u05d4\u05e1\u05ea\u05d1\u05e8\u05d5\u05ea \u05d4\u05d2\u05d1\u05d5\u05d4 \u05d1\u05d9\u05d5\u05ea\u05e8 \u05d4\u05d9\u05d0 \u05d0\u05db\u05df \u05d4\u05e7\u05d8\u05d2\u05d5\u05e8\u05d9\u05d4 \u05e9\u05dc \u05d4\u05de\u05e1\u05de\u05da. Flag=high_single\n2.\t\u05d4\u05d7\u05d9\u05d6\u05d5\u05d9 \u05e9\u05dc \u05d4\u05de\u05d5\u05d3\u05dc \u05e0\u05db\u05d5\u05df \u05d0\u05dd \u05d0\u05d7\u05ea \u05de2 \u05d4\u05e7\u05d8\u05d2\u05d5\u05e8\u05d9\u05d5\u05ea \u05d0\u05e9\u05e8 \u05e7\u05d9\u05d1\u05dc\u05d5 \u05d0\u05ea \u05d4\u05d4\u05e1\u05d1\u05e8\u05d5\u05ea \u05d4\u05d2\u05d1\u05d5\u05d4\u05d4 \u05d1\u05d9\u05d5\u05ea\u05e8 \u05d4\u05df \u05d4\u05e7\u05d8\u05d2\u05d5\u05e8\u05d9\u05d5\u05ea \u05d4\u05d0\u05de\u05d9\u05ea\u05d5\u05ea \u05e9\u05dc \u05d4\u05de\u05e1\u05de\u05da. Flag=high_couple\n3.\t\u05db\u05de\u05d5 \u05d1\u05e1\u05e2\u05d9\u05e3 1, \u05d0\u05d1\u05dc \u05d0\u05dd \u05d4\u05d4\u05e1\u05ea\u05d1\u05e8\u05d5\u05ea \u05e9\u05e0\u05d9\u05ea\u05e0\u05d4 \u05e0\u05de\u05d5\u05db\u05d4 \u05de90% \u05d4\u05e1\u05d9\u05d5\u05d5\u05d2 \u05dc\u05d0 \u05e0\u05e1\u05e4\u05e8 \u05db\u05d8\u05e2\u05d5\u05ea \u05d5\u05dc\u05d0 \u05db\u05ea\u05e9\u05d5\u05d1\u05d4 \u05e0\u05db\u05d5\u05e0\u05d4. \u05d4\u05de\u05e9\u05de\u05e2\u05d5\u05ea \u05d4\u05d9\u05d0 \u05e9\u05d4\u05de\u05d5\u05d3\u05dc \u05d0\u05d5\u05de\u05e8 \"\u05d0\u05e0\u05d9 \u05dc\u05d0 \u05d1\u05d8\u05d5\u05d7\" \u05d5\u05d0\u05e0\u05d7\u05e0\u05d5 \u05dc\u05d0 \u05de\u05d7\u05e9\u05d9\u05d1\u05d9\u05dd \u05d0\u05ea \u05d4\u05de\u05e1\u05de\u05da \u05d1\u05e9\u05e7\u05dc\u05d5\u05dc \u05d4\u05e6\u05d9\u05d5\u05df. Flag=unsure. \u05e9\u05d9\u05de\u05d5 \u05dc\u05d1 \u05e9\u05d7\u05dc\u05e7 \u05de\u05d4\u05de\u05e1\u05de\u05db\u05d9\u05dd \u05dc\u05d0 \u05de\u05e1\u05d5\u05d5\u05d2\u05d9\u05dd \u05d5\u05d6\u05d4 \u05de\u05d2\u05d1\u05d9\u05dc \u05d0\u05ea \u05d4\u05e9\u05d9\u05de\u05d5\u05e9 \u05d1\u05de\u05d5\u05d3\u05dc, \u05d4\u05d3\u05e4\u05d9\u05e1\u05d5 \u05dc\u05de\u05e1\u05da \u05db\u05de\u05d4 \u05de\u05e1\u05de\u05db\u05d9\u05dd \u05dc\u05d0 \u05de\u05e1\u05d5\u05d5\u05d2\u05d9\u05dd \u05d5\u05de\u05d4 \u05d4\u05d0\u05d7\u05d5\u05d6 \u05e9\u05dc\u05d4\u05dd (\u05e8\u05e7 \u05db\u05d0\u05e9\u05e8 \u05d4flag  \u05d4\u05d6\u05d4 \u05d1\u05e9\u05d9\u05de\u05d5\u05e9).\n","4c11312b":"## Tokenization","86b5538e":"## Test set","01256951":"## Removing all punctuation, numbers and stop words","0a66dca8":"### <br> 2. Build an accuracy function.","5aa616b0":"## Turn our tokenized documents into an id term, frequancy list","3ed5ef87":"### <br> 4. Evalute the model on the test dataset.\n","c883c83b":"### 6. Set optimal probability\n\u05d1\u05d0\u05de\u05e6\u05e2\u05d5\u05ea \u05e0\u05d9\u05e1\u05d5\u05d9 \u05d5\u05ea\u05d4\u05d9\u05d9\u05d4, \u05d4\u05e9\u05ea\u05de\u05e9\u05d5 \u05d1test set \u05db\u05d3\u05d9 \u05dc\u05e7\u05d1\u05d5\u05e2 \u05d0\u05ea \u05d4\u05d4\u05e1\u05ea\u05d1\u05e8\u05d5\u05ea \u05d4\u05d0\u05d5\u05e4\u05d8\u05d9\u05de\u05dc\u05d9\u05ea \u05d1\u05d9\u05d5\u05ea\u05e8 \u05e2\u05d1\u05d5\u05e8 \u05d4\u05e2\u05e8\u05da \u05d1\u05d5 \u05d4\u05e9\u05ea\u05de\u05e9\u05e0\u05d5 \u05d1\u05e1\u05e2\u05d9\u05e3 2 \u05d1flag=unsure. \u05d1\u05de\u05e7\u05d5\u05e8 \u05d4\u05e9\u05ea\u05de\u05e9\u05e0\u05d5 \u05d190%, \u05e2\u05dc\u05d9\u05db\u05dd \u05dc\u05de\u05e6\u05d5\u05d0 \u05e2\u05e8\u05da \u05e9\u05d9\u05d9\u05ea\u05df \u05ea\u05d5\u05e6\u05d0\u05d5\u05ea \u05d8\u05d5\u05d1\u05d5\u05ea \u05d9\u05d5\u05ea\u05e8. \u05d4\u05e1\u05d1\u05d9\u05e8\u05d5 \u05d0\u05ea \u05d4\u05d1\u05d7\u05d9\u05e8\u05d4 \u05e9\u05dc\u05db\u05dd.","e99e1b5f":"<img src=\"wiki_NLP.png\">","2f38e144":"# Feature selection ","a35fdbf0":"# Modeling","b8ec351e":"# Topic modeling","1c7c4eea":"\u05d4\u05e1\u05d1\u05d9\u05e8\u05d5 \u05d1\u05de\u05d9\u05dc\u05d9\u05dd (\u05d4\u05d3\u05e4\u05e1\u05d4 \u05dc\u05de\u05e1\u05da) \u05d0\u05ea \u05d4\u05d4\u05d1\u05d3\u05dc \u05d1\u05d1\u05d9\u05e6\u05d5\u05e2\u05d9\u05dd \u05d1\u05d9\u05df \u05d4train \u05dcTest \u05d5\u05d1\u05d9\u05df \u05d4flags \u05d4\u05e9\u05d5\u05e0\u05d9\u05dd.","8860ce00":"\u05e9\u05d9\u05de\u05d5 \u05dc\u05d1, \u05d1\u05db\u05dc \u05e1\u05e2\u05d9\u05e3 \u05e2\u05dc\u05d9\u05db\u05dd \u05dc\u05e4\u05e8\u05d8 \u05d1\u05d4\u05e2\u05e8\u05d5\u05ea \u05de\u05d4 \u05d1\u05d9\u05e6\u05e2\u05ea\u05dd. \u05d4\u05df \u05d1\u05e4\u05df \u05d4\u05d8\u05db\u05e0\u05d9 \u05d5\u05d4\u05df \u05d1\u05e4\u05df \u05d4\u05ea\u05d9\u05d0\u05d5\u05e8\u05d8\u05d9 \u05d9\u05e9 \u05dc\u05d4\u05e6\u05d3\u05d9\u05e7 \u05d0\u05ea \u05d4\u05e7\u05d5\u05d3. \u05e0\u05d9\u05ea\u05df \u05dc\u05d4\u05e9\u05ea\u05de\u05e9 \u05d1Markdown.","85d32b7b":"## Lemmatization","d65a60e7":"### As opposed to stemming","61dc532b":"\u05d4\u05e9\u05ea\u05de\u05e9\u05d5 \u05d1\u05e7\u05d5\u05d3 \u05e9\u05db\u05ea\u05d1\u05ea\u05dd \u05d1\u05e1\u05e2\u05d9\u05e3 2 \u05db\u05d3\u05d9 \u05dc\u05e7\u05d1\u05dc \u05d0\u05ea \u05d4\u05d3\u05d9\u05d5\u05e7 \u05e9\u05dc Test set. \u05d9\u05e9 \u05dc\u05d4\u05e9\u05ea\u05de\u05e9 \u05d1\u05db\u05dc \u05d0\u05d7\u05d3 \u05de\u05d4flags.","b92dd96e":"## How does the docs distribute over the topics","87bbf24b":"<img src=\"wiki_NLU.jpg\">","96441968":"## Convert the term matrix into term lists","37ee6b8a":"## Filter out small docs","57165419":"## Bigram"}}