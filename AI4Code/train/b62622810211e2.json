{"cell_type":{"33363c28":"code","58a9a9d7":"code","d6242038":"code","0e56d19a":"code","6e458a87":"code","d94b37b5":"code","a26c8100":"code","91620bc2":"code","260700de":"code","e083d824":"code","c541301b":"code","12c595cd":"code","6e29fccc":"code","f2c941ba":"code","439aab8f":"code","eeab90c1":"code","484a934f":"code","eedee6fd":"code","8b442413":"code","da86c331":"code","ee28964f":"code","89bc0256":"code","5d95ddf3":"code","c880c8fb":"code","bf3492b5":"code","4b25d3aa":"code","11fc1b13":"code","8554eefa":"code","277918d1":"code","6f35c4cc":"code","0bd12788":"code","201ed624":"code","91dd5327":"code","f4316a87":"code","31c31a9f":"code","a3e9b224":"markdown","feff189b":"markdown","5df254a9":"markdown","3eb530ba":"markdown","4c3a398c":"markdown","048e416c":"markdown","663ef829":"markdown","edb800d9":"markdown","2e6323e2":"markdown","e237912f":"markdown","851590c0":"markdown","42174f0f":"markdown","eaf71c6c":"markdown","c007863d":"markdown","9775f993":"markdown","b4077790":"markdown","b9417d61":"markdown","53e90e19":"markdown","710f19d6":"markdown","f2f6d371":"markdown","9b67dfbf":"markdown","5b8f2388":"markdown","b63b0470":"markdown"},"source":{"33363c28":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt #plotting library\nimport seaborn as sns #statistical data visualization\nimport os\n%matplotlib inline","58a9a9d7":"train = pd.read_csv('..\/input\/train.csv') #importing the trainning data set\ntrain.info() #quick look at all the colums and data types. Also good way to check if the data was imported correctly.","d6242038":"train.describe() #Quick statisical overview of the numerical (int64,float64) data. \n#Object types will not show up.ie. Name,Sex Ticket, Cabin, and Embarked\n","0e56d19a":"train.head(15) #Gives you the the first 15 rows of data","6e458a87":"#Looking at the the head of the dataframe we can assume that PassenderID and Ticket \n#are random unique identififier and will have no impact on the predictive outcome \n#so I will drop them from the dataframe\n\n#Dopping Passenger ID and Ticket becasue it will have no vlaue to our machine learning model\ntrain.drop(['PassengerId','Ticket'],axis=1,inplace=True)","d94b37b5":"#Now checking for missing values in dataset\nplt.subplots(figsize=(9,5))\nsns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap=\"YlGnBu_r\")","a26c8100":"#With so many null values in Cabin I will go ahead and drop it\ntrain.drop('Cabin',axis=1,inplace=True)\n#Age has a good amount of null valuse but I will find a way to handle the null values later.\n#Embarked also has a few missing values but I will handle that later too.","91620bc2":"#Plotting Survived against Sex\nplt.subplots(figsize=(9,5))\nsns.countplot(x='Survived',hue='Sex',palette='Set2',data=train)\n#As we can see from the countplot below that Male's have less chance to survive than Female's.\n#So there is a strong possibility that sex may play an important role in the prediciton of who\n#survived\n","260700de":"#Plotting Survived against PClass\nsns.catplot(x=\"Pclass\", col=\"Survived\",palette='Set2',data=train,kind=\"count\")\n\n#Pclass could play an important roll in the prediction of who survied, with more surviving \n#than dying in Pclass 1 and almost 4 times the amount of people dying in Pclass 3.","e083d824":"#Plotting Survived against Embarked\nsns.catplot(x=\"Embarked\", col=\"Survived\",palette='Set2',data=train,kind=\"count\")","c541301b":"# Correlation matrix between numerical values (SibSp Parch Age, and Fare values) and Survived \n#Checking for multicollinearity (also known as collinearity) which are two or more explanatory \n#variables in a multiple regression model that are highly linearly related. \n\nplt.subplots(figsize=(9,5))\nax = sns.heatmap(train[[\"Survived\",\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].corr(),annot=True, fmt = \".2f\",cmap=\"Blues\")\n#From the correlation heatmap below no variable seems to highly correlated with another \n#so I won't have to drop any.","12c595cd":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\n\ntrain['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)\n\n#Checking agian to see if there are any more missing values in dataset\n#sns.heatmap(train.isnull(),yticklabels=False,cbar=False)\nplt.subplots(figsize=(9,5))\nsns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap=\"YlGnBu_r\")\n","6e29fccc":"#We can see from the heatmap above that we have handled all the missing Age values \n#but we can see that we still have a few Embarked rows\/values we need to deal with.\n#To do so we will will complete the Embarked NA rows\/values with the mode values of the column.\ntrain['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)","f2c941ba":"#Quick look at how Embarked, Age, Sex, and Pclass are spread out. One interesting things: Nobody with title rare in pclass 3. \na = sns.catplot(x=\"Age\", y=\"Embarked\",hue=\"Sex\", row=\"Pclass\",\n                 data=train,orient=\"h\", height=4, aspect=3, palette=\"Set2\",\n                 kind=\"violin\", dodge=True, cut=0, bw=.2)","439aab8f":"#Checking the distribution of the fare variable\ntrain['Fare'].hist(bins = 60,color=\"g\")\n#Looking the the distribution we can see that fare is skewed to the right.\n","eeab90c1":"#We will apply a log transformation to the Fare variable to reduce skewness distribution\ntrain[\"Fare\"] = train[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","484a934f":"#We can see that after we apply the log transformation the distribution looks more like a normal distribution\n#train['Fare'].hist(bins = 60,color=\"g\",)\nsns.distplot(train[\"Fare\"],bins = 60,color=\"g\")","eedee6fd":"#Dropping SibSp and Parch but creating a family feature with them.\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] +1\ntrain.drop('SibSp',axis=1,inplace=True)\ntrain.drop('Parch',axis=1,inplace=True)","8b442413":"train.head()","da86c331":"train_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\ntrain[\"Title\"] = pd.Series(train_title)\ntrain[\"Title\"].head()","ee28964f":"train.head()","89bc0256":"#Here we will create four separate categories\n#Converting Title to categorical values\ntrain[\"Title\"] = train[\"Title\"].replace(['Lady','the Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')\ntrain[\"Title\"] = train[\"Title\"].replace('Mlle','Miss')\ntrain[\"Title\"] = train[\"Title\"].replace('Ms','Miss')\ntrain[\"Title\"] = train[\"Title\"].replace('Mme','Mrs')\n#Adding dummy variables to the Title column. More on this later in section 2.4\ntitle = pd.get_dummies(train['Title'],drop_first=True)\ntrain = pd.concat([train,title],axis=1)\ntrain.head()","5d95ddf3":"#Quick look at how Title, Age, Sex, and Pclass are spread out. \n#One interesting things: Nobody with title rare in pclass 3.\nb = sns.catplot(x=\"Age\", y=\"Title\",hue=\"Sex\", row=\"Pclass\",\n                data=train,orient=\"h\", height=4, \n                aspect=3, palette=\"Set2\",kind=\"violin\")","c880c8fb":"#Now that I have what I want from the Name variable I will drop Name\n#I can also go ahead and drop Title because I already have the information in the indivdual titles\ntrain.drop(['Name','Title'],axis=1,inplace=True)\ntrain.head()","bf3492b5":"#Adding dummy variables to Sex and Embarked\nsex =  pd.get_dummies(train['Sex'],drop_first=True)\nembark = pd.get_dummies(train['Embarked'],drop_first=True)\ntrain = pd.concat([train,sex,embark],axis=1)\ntrain.head()","4b25d3aa":"#Now we can go ahead and drop the Sex and Embarked column because we have the needed information at the end with male, Q, S. \ntrain.drop(['Sex','Embarked'],axis=1,inplace=True)\ntrain.head()","11fc1b13":"#First we have to split out dataset into X and Y. X being the all the variable and Y being Survived or not i.e. 0 or 1.\nx = train.drop('Survived',axis=1)\ny = train['Survived']","8554eefa":"#Next we split the dataset into the train and test set\n#Test will be 30% of the data and the train will be 70%. By setting test_size = .3\n#This way we can test our models predictions on the test set to see how we did.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","277918d1":"#from sklearn.model_selection import GridSearchCV\n#from sklearn.ensemble import RandomForestClassifier\n\n#rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\n#param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1,2,3,5], \"min_samples_split\" : [10,11,12,13], \"n_estimators\": [350, 400, 450, 500,550], \"max_depth\":[6,7,8,9]}\n\n#gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n\n#gs = gs.fit(train.iloc[:, 1:], train.iloc[:, 0])\n\n#print(gs.best_score_)\n#print(gs.best_params_)\n#print(gs.scorer_)\n\n#Example of the output\n#0.8451178451178452\n#{'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 1, 'min_samples_split': 11, 'n_estimators': 375}","6f35c4cc":"#Building the Random Forest Classification model\nfrom sklearn.ensemble import RandomForestClassifier\nrfmodel = RandomForestClassifier(random_state=0,n_estimators=450,criterion='gini',n_jobs=-1,max_depth = 8,min_samples_leaf=1,min_samples_split= 11)\n#Fitting the model to x_train and y_train\nrfmodel.fit(x_train,y_train)\n#Predicting the model on the x_test\npredictions = rfmodel.predict(x_test)","0bd12788":"#classification report showing are predictions vs the actually result\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))","201ed624":"#Printing out the confusion matrix. \nfrom sklearn.metrics import  confusion_matrix\nconfusion_matrix(y_test,predictions)","91dd5327":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfmodel, random_state=1).fit(x_test, y_test)\neli5.show_weights(perm, feature_names = x_test.columns.tolist())","f4316a87":"#Applying K-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=rfmodel,X= x_train,y=y_train,cv=10)\naccuracies #Prints out the 10 different Cross Validation scores.\n#As you can see there seems to be a decent amount of variation from as low as .8033 to as hiigh as .9048.","31c31a9f":"accuracies.mean() #Prints out the  average of the 10 scores.","a3e9b224":"**3.2 Hyperparameter Tuning **\n* Hyperparameters Tuning is the process of adjusting the algorithm parameters to optimize its performance. The hyperparameters are set by the data scientist before training. In the case of a random forest model which I have chosen to do here some of the parameters include, number of estimators, maximum depth,  minimum sample leafs, minimum sample splits and more. These parameters are the variables considered by each tree when splitting a node. Picking the most optimal parameters right of the bat is nearly impossible so in some ways it is a trial and error process as you will see below.","feff189b":"**2.1 Exploratory Data Analysis **","5df254a9":"** 2.3 Feature Scaling **\n* Feature scaling is a method used to normalize the range of independent variables or features of data. We will do this below to the Fare variable becasue of its skewed distribution.\n","3eb530ba":"** 2.5 Handling Categorical Data **\n* A categorical variable usally has a fixed outcome (i.e. Male or Female). In this case Sex, Name and Embarked are all categorical and have to be delt with. For now we will deal with Sex and Embarked and as I handle the name variable in the above section. Depedning on the machine learning algorithm, the algorithm may have a hard time exctracting the information needed. One way to fix this problem is to change the inputs to numerical data types. Another way, which I have chosen to do here is to use a technique called one hot encoding (or dummy variabales) in the sklearn python library. Duummy variable takes each category value and turns it into a binary vector of size i (where i is the number of values in category i) and where all columns are equal to zero beside the category column.","4c3a398c":"**4.4 Cross-Validation (CV)**\n* One down side of optimizing the model too much on the training dataset is that the model will score very well on the training set, but will not be able to generalize to new data like the test set, which in this case is what I really care about. When a model performs highly on the training set but poorly on the test set, this is known as overfitting, essentially creating a model that knows the training set very well but cannot be use anywhere else. Cross Validatioin is one way to check if your model is overfitting and if so how much.\n* When we approach a machine learning problem, we make sure to split our data into a training and a testing set like we did above. In K-Fold Cross-Validation CV, we further split our training set into K number of subsets, called folds. In this case we split the training set into 10 folds. Meaning the the training set is split 10 folds\/ways, training on 9 of the folds and validating on one. Giving us 10 different accuracy scores from 10 variations of the training dataset. Taking a look at the different accuracy scores below can give us a better understand of the variation between the differnt cross-validation datasets.","048e416c":"Welcome to my first Kernel on Kaggle. Please leave any comments, concerns, or advise in section below as I would greatly apprectiate the feedback. I have chosen the Titanic dataset as my first kernel for the amount of resourses and content on the topic. I have spent multiple hours working with the dataset testing various machine learning models and evaluating the preformace. My goal for this Kernel is to compress all my work into simple step by step approach you can easy follow along and understand. Now lets get started!","663ef829":"**1.1 Checking Data Import**","edb800d9":"** 3.1 Splitting the Dataset **","2e6323e2":"**3.3 Bulding the Random Forest Model**","e237912f":"** 2.4 Feature Engineering**\n* Feature engineering is the process of getting the most out of the data given. Taking raw data and transforming into features that better represent the underlying problem to the predicitve models. The goal is to imporve the model accuracy on unseen data. ","851590c0":"**1.Importing Libaries and the Titanic Trainning Dataset**","42174f0f":"**4.1 Classification Report**\n* Show us the precision, recall, and f1-score of the predicted vs the actual result.","eaf71c6c":"**5.Conclusion**","c007863d":"**4.3  Permutation Importance**\n* Permutation Importance shows you what features have the biggest impact on the predicted outcome. Permutation importance is calculated after a model has been fitted. So we won't change the model or change what predictions we'd get for a given value of height. If I randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data.","9775f993":"**4. Evaluating the Model**","b4077790":"**3. Model Building **\n","b9417d61":"For the sake of time I have commented out the Hyperparamter Tuning code above, as it took about 5 min to run. Below I will build the Random Forest model with the paramaters outputed from the code above.","53e90e19":"My final leaderboad score once I applied this model on the test dataset was around 82% which at the time was top 3%. As you can see my average(mean) accuracy score for my test dataset with this model was .8507, so I lost about 3% once I applied this model to the test dataset. Still working on how to get a high finaly score! If you found this Kernel helpful please give an up vote to keep me motivated and please leave any quesitons or ways I can imporve in the comments. Thank you for following along.","710f19d6":"**4.2 Confusion Matrix**\n<p>\nExplaining what we see below:\n* True positive (TP): Top left (139) are the one we predicted died and did die.\n* False negative (FN): Bottom left (43) are the ones we predicted they die but they survived\n* False positive (FP): Top right (14) are the ones we predicted they survive but they died\n* True negative (TN) :Bottom right (72) these are the ones we predicted would survive and they did\n* FP is also known as a \"Type I error.\"\n* FN is also known as a \"Type II error.\"","f2f6d371":"Now we will do some feature engineering with the name variable. Stripping the names down to titles only. We do this by taking the title which is after the comma \";\" and before the period \".\"","9b67dfbf":"**2.2 Handling Missing Data**\n* Handling missing age values: There are many different ways I saw on how to handle the missing age values. One of the easier more simple ways I found that still preformed well was to fill the missing age values with the mean age of the Pclass. Here is the python script:","5b8f2388":"**Quick overview of everything I will cover:**\n* **1. Importing Libaries and Dataset**\n    * 1.1 Checking Data Import\n* **2. Data Preperation, Cleaning, and Visualization**\n    * 2.1 Exploratory Data Analysis\n    * 2.2 Handling Missing Data\n    * 2.3 Feature Scaling\n    * 2.4 Feature Engineering\n    * 2.5 Handling Categorical Data\n* **3. Model Building**\n    * 3.1 Splitting the Dataset\n    * 3.2 Hyperparameter Tuning\n    * 3.3 Bulding the Random Forest Model\n* **4. Evaluating the Model**\n    * 4.1 Classification Report\n    * 4.2 Confusion Matrix\n    * 4.3 Permutation Importance\n    * 4.4 Cross-Validation (CV)\n* **5. Conclusion**\n\n","b63b0470":"**2. Data Preperation, Cleaning and Visualization**\n* Once you have the data imported a couple good questions to ask first are: what features are numerical, categorical, and\/or ordinal data? It is also good to look for any null or empty values in the different variables. Lastly checking for various outliers for each variable in the dataset. "}}