{"cell_type":{"b6c54ff5":"code","e5c584be":"code","8429ce2a":"code","4882be85":"code","dec42897":"code","8f228a71":"code","e914763a":"code","94591fea":"code","48c2e190":"code","f8ae81c9":"code","2c131857":"code","f83d0f70":"code","2b91ef7f":"code","53e2bc9f":"code","3bfb876d":"code","f8261784":"code","3ed8c411":"code","eb71cf6d":"code","a407b8b2":"markdown","d65d0806":"markdown","d75c49db":"markdown","c82d3ea6":"markdown","32b57b41":"markdown","3ff59db8":"markdown"},"source":{"b6c54ff5":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport shutil\nimport requests\nimport json\nimport datetime\nimport time\n\nfrom numpy import linalg as LA\n\nimport matplotlib.pyplot as plt\n\nimport json\nimport glob\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import train_test_split\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import GroupKFold\n\nfrom tqdm import tqdm\n\nimport math\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e5c584be":"BUFFER = 1\n\nbase_url = \"https:\/\/www.kaggle.com\/requests\/EpisodeService\/\"\nget_url = base_url + \"GetEpisodeReplay\"\nlist_url = base_url + \"ListEpisodes\"\n# inital team list\n\nr = requests.post(list_url, json = {\"submissionId\":  18923141}) # 1st Rank Team\nrj = r.json()\n\nteams_df = pd.DataFrame(rj['result']['teams'])\nteams_df.sort_values('publicLeaderboardRank', inplace = True)\nteams_df.head(5)","8429ce2a":"os.mkdir(\".\/json-folder\/\")\ndef saveEpisode(sub_id, epid, rj):\n    # request\n    re = requests.post(get_url, json = {\"EpisodeId\": int(epid)})\n        \n    # save replay\n    with open('.\/json-folder\/{}_{}.json'.format(sub_id, epid), 'w') as f:\n        f.write(re.json()['result']['replay'])\n\n    # save episode info\n    with open('.\/json-folder\/{}_{}_info.json'.format(sub_id, epid), 'w') as f:\n        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)","4882be85":"subs = [18923141,19052323,19145375,19120525,19123171,19179822,18913702,18778644,19117574,19191948,19176740,19117330,18983613,19191948,19174241]\n\nfor sub in subs:\n    start_time = datetime.datetime.now()\n    r = BUFFER;\n    result = requests.post(list_url, json = {\"submissionId\":  int(sub)})\n    team_json = result.json()\n    team_df = pd.DataFrame(team_json['result']['episodes'])\n    print('{} games for {}'.format(len(team_df), sub))\n\n    for i in range(len(team_df)):\n        epid = team_df.id.iloc[i]\n\n        saveEpisode(sub, epid, team_json); r+=1;\n        try:\n            size = os.path.getsize('.\/json-folder\/{}_{}.json'.format(sub, epid)) \/ 1e6\n            print('Saved Episode #{} @ {:.1f}MB'.format(epid, size))\n        except:\n            print('file {}_{}.json did not seem to save'.format(sub, epid))    \n        if r > (datetime.datetime.now() - start_time).seconds:\n            time.sleep( r - (datetime.datetime.now() - start_time).seconds)","dec42897":"replay_dirs=[\".\/json-folder\/\"]\njson_paths=[]\njson_info_paths=[]\nfor replay_dir in replay_dirs:\n    for path in glob.glob(replay_dir+\"*\"):\n        if path.count(\"info\")!=0:\n            json_info_paths.append(path)\n        else:\n            json_paths.append(path)\n\nprint(\"replay num: {}\".format(len(json_paths)))","8f228a71":"def log_training(result, n_machines):\n    \"\"\"Records training data from each machine, each agent, each round\n    \n    Generates a training dataset to support prediction of the current\n    payout ratio for a given machine.\n    \n    Args:\n       result ([[dict]]) - output from all rounds provided as output of \n                           env.run([agent1, agent2])\n       n_machines (int) - number of machines\n                           \n    Returns:\n       training_data (pd.DataFrame) - training data, including:\n           \"round_num\"      : round number\n           \"machine_id\"     : machine data applies to\n           \"agent_id\"       : player data applies to (0 or 1)\n           \"n_pulls_self\"   : number of pulls on this machine so far by agent_id\n           \"n_success_self\" : number of rewards from this machine by agent_id\n           \"n_pulls_opp\"    : number of pulls on this machine by the other player\n           \"payout\"         : actual payout ratio for this machine\n    \n    \"\"\"\n    # Initialize machine and agent states\n    machine_state = [{'n_pulls_0': 0, 'n_success_0': 0,\n                      'n_pulls_1': 0, 'n_success_1': 0,\n                      'payout': None}\n                     for ii in range(n_machines)]\n    agent_state = {'reward_0': 0, 'reward_1': 0, 'last_reward_0': 0,\n                   'last_reward_1': 0}\n\n    # Initialize training dataframe\n    # - In the first round, store records for all n_machines\n    # - In subsequent rounds, just store the two machines that updated\n    training_data = pd.DataFrame(\n            index=range(n_machines + 4 * (len(result) - 1)),\n            columns=['round_num', 'machine_id', 'agent_id',\n                     'n_pulls_self', 'n_success_self',\n                     'n_pulls_opp', 'payout'])\n    \n    # Log training data from each round\n    for round_num, res in enumerate(result):\n        # Get current threshold values\n        thresholds = res[0]['observation']['thresholds']\n\n        # Update agent state\n        for agent_ii in range(2):\n            agent_state['last_reward_%i' % agent_ii] = (\n                res[agent_ii]['reward']\n                - agent_state['reward_%i' % agent_ii])\n            agent_state['reward_%i' % agent_ii] = res[agent_ii]['reward']        \n\n        # Update most recent machine state\n        if res[0]['observation']['lastActions']:\n            for agent_ii, r_obs in enumerate(res):\n                action = r_obs['action']\n                machine_state[action]['n_pulls_%i' % agent_ii] += 1\n                machine_state[action]['n_success_%i' % agent_ii] += \\\n                    agent_state['last_reward_%i' % agent_ii]\n                machine_state[action]['payout'] = thresholds[action]\n        else:\n            # Initialize machine states\n            for mach_ii in range(n_machines):\n                machine_state[mach_ii]['payout'] = thresholds[mach_ii]\n            \n        # Record training records\n        # -- Each record includes:\n        #       round_num, n_pulls_self, n_success_self, n_pulls_opp\n        if res[0]['observation']['lastActions']:\n            # Add results for most recent moves\n            for agent_ii, r_obs in enumerate(res):\n                action = r_obs['action']\n\n                # Add row for agent who acted\n                row_ii = n_machines + 4 * (round_num - 1) + 2 * agent_ii \n                training_data.at[row_ii, 'round_num'] = round_num\n                training_data.at[row_ii, 'machine_id'] = action\n                training_data.at[row_ii, 'agent_id'] = agent_ii\n                training_data.at[row_ii, 'n_pulls_self'] = (\n                    machine_state[action]['n_pulls_%i' % agent_ii])\n                training_data.at[row_ii, 'n_success_self'] = (\n                    machine_state[action]['n_success_%i' % agent_ii])\n                training_data.at[row_ii, 'n_pulls_opp'] = (\n                    machine_state[action]['n_pulls_%i' % (\n                        (agent_ii + 1) % 2)])\n                training_data.at[row_ii, 'payout'] = (\n                    machine_state[action]['payout'] \/ 100)\n\n                # Add row for other agent\n                row_ii = n_machines + 4 * (round_num - 1) + 2 * agent_ii + 1\n                other_agent = (agent_ii + 1) % 2\n                training_data.at[row_ii, 'round_num'] = round_num\n                training_data.at[row_ii, 'machine_id'] = action\n                training_data.at[row_ii, 'agent_id'] = other_agent\n                training_data.at[row_ii, 'n_pulls_self'] = (\n                    machine_state[action]['n_pulls_%i' % other_agent])\n                training_data.at[row_ii, 'n_success_self'] = (\n                    machine_state[action]['n_success_%i' % other_agent])\n                training_data.at[row_ii, 'n_pulls_opp'] = (\n                    machine_state[action]['n_pulls_%i' % agent_ii])\n                training_data.at[row_ii, 'payout'] = (\n                    machine_state[action]['payout'] \/ 100)\n                \n        else:\n            # Add initial data for all machines\n            for action in range(n_machines):\n                row_ii = action\n                training_data.at[row_ii, 'round_num'] = round_num\n                training_data.at[row_ii, 'machine_id'] = action\n                training_data.at[row_ii, 'agent_id'] = -1\n                training_data.at[row_ii, 'n_pulls_self'] = 0\n                training_data.at[row_ii, 'n_success_self'] = 0\n                training_data.at[row_ii, 'n_pulls_opp'] = 0\n                training_data.at[row_ii, 'payout'] = (\n                    machine_state[action]['payout'] \/ 100)\n            \n    return training_data","e914763a":"%%time \n\nact_lis =[]\nreplay_group = []\nrows_list = [] # list of dict\n\nreplays = []\n\ntraining_data=[]\nfor replay_ind, path_replay in enumerate(tqdm(json_paths)):\n\n    try:\n        json_open = open(path_replay, 'r')\n        json_load = json.load(json_open)\n        result=json_load[\"steps\"]\n\n#     print(\"Data\")\n#     print(json_load)\n    \n        training_data.append(log_training(\n            result, len(result[0][0]['observation']['thresholds'])))\n    except:\n        continue\n","94591fea":"# Save training data\ntraining_data = pd.concat(training_data, axis=0)\ntraining_data\ntraining_data.to_csv('\/kaggle\/working\/training_data.csv', index=False, header=True)\n\nshutil.rmtree(\".\/json-folder\/\")  # comment it if dont want to delete json directory","48c2e190":"import pickle\nimport base64\nimport random\nfrom catboost import Pool, CatBoostRegressor\n\nFUDGE_FACTOR = 0.99\nVERBOSE = False\nDATA_FILE = \"\/kaggle\/working\/training_data.csv\"\nTRAIN_FEATS = ['round_num', 'n_pulls_self', 'n_success_self', 'n_pulls_opp']\nTARGET_COL = 'payout'\n\ndef make_model():\n   \n    data = pd.read_csv(DATA_FILE)\n#     group_kfold = GroupKFold(n_splits=5)\n#     for train_index, test_index in group_kfold.split(data):\n#         print(len(test_index))\n    x_train=data[:10991129] #  80% in train\n    x_test=data[10991129:] # 20% as test data\n    train_dataset = Pool(data=x_train[TRAIN_FEATS],\n                     label=x_train[TARGET_COL])\n\n    eval_dataset = Pool(data=x_test[TRAIN_FEATS],\n                        label=x_test[TARGET_COL])\n    params = {\n          'depth': 8,\n          'learning_rate': 0.1,\n          'random_seed': 42,\n          'iterations': 1500,\n          \"use_best_model\":True,\n    }\n    model = CatBoostRegressor(**params)\n    model.fit(train_dataset, eval_set=eval_dataset)    \n    return model\n\n# Train model and save as a sav file.\nmodel = make_model()\nfilename = 'model.sav'\npickle.dump(model, open(filename, 'wb'))","f8ae81c9":"model","2c131857":"data=pd.read_csv(DATA_FILE)\ndata\ny_pred = model.predict(data[TRAIN_FEATS])","f83d0f70":"len(y_pred)","2b91ef7f":"len(data[\"payout\"])","53e2bc9f":"from sklearn.metrics import fbeta_score, accuracy_score,roc_auc_score,log_loss\n# accuracy = sum(y_test == y_pred_max) \/ len(y_test)\nprint(\"Accuracy : {}\",roc_auc_score(data[\"payout\"].round(),y_pred))\n\n# from sklearn.metrics import fbeta_score\n# print(\"F1 : {}\".format(fbeta_score(y_test, y_pred_max, average='macro', beta=0.5)))","3bfb876d":"feature_importance = model.get_feature_importance()\n\nfeature_importance","f8261784":"from sklearn import metrics\nres = metrics.classification_report(data[\"payout\"].round(), y_pred.round(), digits=3,output_dict=True)\ndf = pd.DataFrame(res).transpose()\ndf=df.drop(['accuracy', 'macro avg', 'weighted avg'])\ndf.sort_values(\"f1-score\")","3ed8c411":"%%writefile main.py\n\n\nimport pickle\nimport base64\nimport random\nimport numpy as np\nimport pandas as pd\nimport sklearn.tree as skt\nimport sys\nimport os\n\nimport random, os, datetime, math\nfrom collections import defaultdict\n\n# Below is needed to submit tar.gz file to Kaggle.\nsys.path.append(\"\/kaggle_simulations\/agent\")\nworking_dir = \"\/kaggle_simulations\/agent\"\npath_to_model = os.path.join(working_dir,\"model.sav\")\n\n# Parameters\nFUDGE_FACTOR = 0.99\nVERBOSE = False\nDATA_FILE = '\/kaggle\/working\/training_data.csv'\nTRAIN_FEATS = ['round_num', 'n_pulls_self', 'n_success_self', 'n_pulls_opp']\nTARGET_COL = 'payout'\nfilename = 'model.sav'\n\n\nclass GreedyStrategy:\n    \"\"\"Implements strategy to maximize expected value\n\n    - Tracks estimated likelihood of payout ratio for each machine\n    - Tracks number of pulls on each machine\n    - Chooses machine based on maximum expected value\n    \n    \n    \"\"\"\n    def __init__(self, name, agent_num, n_machines):\n        \n        # Record inputs\n        self.name = name\n        self.agent_num = agent_num\n        self.n_machines = n_machines\n        \n        # Initialize distributions for all machines\n        self.n_pulls_self = np.array([0 for _ in range(n_machines)])\n        self.n_success_self = np.array([0. for _ in range(n_machines)])\n        self.n_pulls_opp = np.array([0 for _ in range(n_machines)])\n\n        # Track other players moves\n        self.opp_moves = []\n        \n        # Track winnings\n        self.last_reward_count = 0\n\n        # Load model from other file\n        self.model = pickle.load(open(path_to_model, 'rb'))\n        \n        # Predict expected reward\n        features = np.zeros((self.n_machines, 4))\n        features[:, 0] = len(self.opp_moves)\n        features[:, 1] = self.n_pulls_self\n        features[:, 2] = self.n_success_self\n        features[:, 3] = self.n_pulls_opp\n        self.predicts = self.model.predict(features,prediction_type=\"Probability\")\n        \n\n    def __call__(self):\n        \"\"\"Choose machine based on maximum expected payout\n\n        Returns:\n           <result> (int):  index of machine to pull\n        \n        \"\"\"\n        # Otherwise, use best available\n        est_return = self.predicts\n        max_return = np.max(est_return)\n        result = np.random.choice(np.where(\n            est_return >= FUDGE_FACTOR * max_return)[0])\n        \n        if VERBOSE:\n            print('  - Chose machine %i with expected return of %3.2f' % (\n                int(result), est_return[result]))\n\n        return int(result)\n    \n        \n    def updateDist(self, curr_total_reward, last_m_indices):\n        \"\"\"Updates estimated distribution of payouts\"\"\"\n        # Compute last reward\n        last_reward = curr_total_reward - self.last_reward_count\n        self.last_reward_count = curr_total_reward\n        if VERBOSE:\n            print('Last reward: %i' % last_reward)\n\n        if len(last_m_indices) == 2:\n            # Update number of pulls for both machines\n            m_index = last_m_indices[self.agent_num]\n            opp_index = last_m_indices[(self.agent_num + 1) % 2]\n            self.n_pulls_self[m_index] += 1\n            self.n_pulls_opp[opp_index] += 1\n\n            # Update number of successes\n            self.n_success_self[m_index] += last_reward\n            \n            # Update opponent activity\n            self.opp_moves.append(opp_index)\n\n            # Update predictions for chosen machines\n            self.predicts[[opp_index, m_index]] = self.model.predict([\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[opp_index],\n                    self.n_success_self[opp_index],\n                    self.n_pulls_opp[opp_index]\n                ],\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[m_index],\n                    self.n_success_self[m_index],\n                    self.n_pulls_opp[m_index]\n                ]])\n            \n\ntotal_reward = 0\nbandit_dict = {}\nmy_action_list = []\nop_action_list = []\nop_continue_cnt_dict = defaultdict(int)\n\ndef agent(observation, configuration):\n    global total_reward, bandit_dict, curr_agent\n    \n    if observation.step == 0:\n        # Initialize agent\n        curr_agent = GreedyStrategy('Mr. Agent %i' % observation['agentIndex'],\n            observation['agentIndex'],\n            configuration['banditCount'])\n    \n    # Update payout ratio distribution with:\n    curr_agent.updateDist(observation['reward'], observation['lastActions'])\n    \n    #pull vegas\n    my_pull = random.randrange(configuration['banditCount'])\n    if observation['step'] == 0:\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if last_reward > 0:\n            bandit_dict[my_last_action]['win'] += 1\n        else:\n            bandit_dict[my_last_action]['loss'] += 1\n        bandit_dict[op_last_action]['opp'] += 1\n        \n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = curr_agent()\n                else:\n                    my_pull = curr_agent()\n            else:\n                my_pull = curr_agent()\n    \n    return my_pull","eb71cf6d":"# Make a tar.gz file for submission\n!tar cvfz main.py.tar.gz main.py model.sav","a407b8b2":"Hello... In this notebook I am trying to show whole pipeline from episodes scrapper to applying machine learning models...Honestly I didnt know how to do this.. even i last competitions.. I just saw them as crap.. But in this competition I decided to learn it and learn how we can apply ML models on scrapped episodes data:\/\n> So first giving out credits to those notebooks I was able to learn and apply.. Thanks to them a lot.. Please check them as well:)\n> > [Felipe's notebook](https:\/\/www.kaggle.com\/felipefonte99\/ensemble-pull-vegas-decision-tree-model) <br>\n> > [Robga's notebook](https:\/\/www.kaggle.com\/robga\/google-football-episode-scraper) <br>\n> > [Lebroschar's notebook](https:\/\/www.kaggle.com\/lebroschar\/generate-training-data)","d65d0806":"# Converting episodes JSON to required format","d75c49db":"Thanks for reading out my notebook.. Hope it can help.. Please give suggestions also, Thanks in advance","c82d3ea6":"# Scrapping out Episodes","32b57b41":"# Machine Learning Model","3ff59db8":"# Extracting top-15 Teams episodes"}}