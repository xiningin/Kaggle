{"cell_type":{"a4e20ed9":"code","4d21d033":"code","39d6ddfb":"code","d8855d07":"code","d2e23fbf":"code","959fb117":"code","463009aa":"code","4c0e0f34":"code","3977675d":"code","1f327534":"code","000133a9":"code","07178550":"code","13042e66":"code","fc3ddfa0":"code","578b3349":"code","6807213d":"code","9917655f":"code","aa5fac9f":"code","b2d2613e":"code","72d52e68":"code","a8b7605e":"code","4301c2ae":"code","3cc06f1a":"code","e2d4f7b2":"code","ab208693":"code","d8d00347":"code","88e67eee":"code","10393bf5":"code","3da6e237":"code","de632f2e":"code","7e2d9efe":"code","4de84de3":"code","bd34ac21":"code","bb5e1b67":"markdown","a025f951":"markdown","1b6bb947":"markdown","d9ed1b0c":"markdown","10abac22":"markdown","7e6e0f5a":"markdown","532e9980":"markdown","962f888b":"markdown","aba69aec":"markdown","49ec6c5c":"markdown","44ea6e5d":"markdown","c94608d2":"markdown","1759f5d3":"markdown","95fbb2a7":"markdown","c1cf92b8":"markdown","2ea10c37":"markdown","96d42aa0":"markdown","832b2a82":"markdown","7e497912":"markdown","97c58e1d":"markdown"},"source":{"a4e20ed9":"!pip install feature_engine","4d21d033":"# importing the libraries\nimport numpy as np\nimport pandas as pd\n\n# visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# preprocessing and selection libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom feature_engine.selection import SelectBySingleFeaturePerformance\n\n# machine learning models\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# metric models\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, log_loss","39d6ddfb":"# uploading the data\ntrain_df = pd.read_csv(r'..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv(r'..\/input\/titanic\/test.csv')","d8855d07":"print('The shape of train set: {}'.format(train_df.shape))\nprint('The shape of test set: {}'.format(test_df.shape))","d2e23fbf":"print('The columns of train:\\n')\nprint(train_df.columns)\nprint('The columns of test:\\n')\nprint(test_df.columns)","959fb117":"# deleting the useless columns\ndel train_df['Name']\ndel train_df['Ticket']\ndel test_df['Name']\ndel test_df['Ticket']","463009aa":"print('\\033[1m \\033[94m \\033[4m' + 'The information of data:\\n' + '\\033[0m')\ntrain_df.info()","4c0e0f34":"print('\\033[1m \\033[94m \\033[4m' + 'The information of data:\\n' + '\\033[0m')\ntest_df.info()","3977675d":"# desciribing only meaningful columns\nnum_imp_col = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nprint('\\033[1m \\033[94m \\033[4m' + 'The numerical data can be described as following\\n' + '\\033[0m')\ntrain_df[num_imp_col].describe()","1f327534":"# desciribing only meaningful columns\nnum_imp_col = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nprint('\\033[1m \\033[94m \\033[4m' + 'The numerical data can be described as following\\n' + '\\033[0m')\ntest_df[num_imp_col].describe()","000133a9":"# plotting the histograms of the columns\ncol_for_hist = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\nfor i in train_df[col_for_hist]:\n    print('\\033[1m \\033[94m \\033[4m' +  'The histogram for {}'.format(i) + '\\033[0m')\n    train_df[i].hist()\n    plt.show()","07178550":"lived = 'Survived'\ndied = 'Not survived'\nfig, axes = plt.subplots(nrows = 1, ncols = 2,figsize = (15, 5))\nfemale = train_df[train_df['Sex'] == 'female']\nmale = train_df[train_df['Sex'] == 'male']\nax = sns.kdeplot(female[female['Survived'] == 1].Age.dropna(), label = lived, ax = axes[0], color = \"green\")\nax = sns.kdeplot(female[female['Survived'] == 0].Age.dropna(), label = died, ax = axes[0], color = \"black\")\nax.legend()\nax.set_title('Female')\nax = sns.kdeplot(male[male['Survived'] == 1].Age.dropna(), label = lived, ax = axes[1], color = \"green\")\nax = sns.kdeplot(male[male['Survived'] == 0].Age.dropna(), label = died, ax = axes[1], color = \"black\")\nax.legend()\nax.set_title('Male');","13042e66":"print('\\033[1m \\033[94m \\033[4m' +  'The percentage of missing values:\\n' )\ntrain_df.isnull().mean()*100","fc3ddfa0":"print('\\033[1m \\033[94m \\033[4m' +  'The percentage of missing values:\\n' )\ntest_df.isnull().mean()*100","578b3349":"# deleting column\ndel train_df['Cabin']\ndel test_df['Cabin']","6807213d":"# imputing data\ndef impute_Na(df, variable, value):\n    return df[variable].fillna(value)\n\ncols = ['Age', 'Fare', 'Survived']\nfor i in train_df[cols]:\n    train_df[i] = impute_Na(train_df,i,train_df[i].median())\n    \n# dropping missing values\ntrain_df = train_df.dropna()    ","9917655f":"# imputing data\ndef impute_Na(df, variable, value):\n    return df[variable].fillna(value)\n\ncols = ['Age', 'Fare']\nfor i in test_df[cols]:\n    test_df[i] = impute_Na(test_df,i,test_df[i].median())\n    \n# dropping missing values\ntest_df = test_df.dropna()    ","aa5fac9f":"sns.heatmap(train_df.corr(), annot = True)","b2d2613e":"sns.heatmap(test_df.corr(), annot = True)","72d52e68":"correl_Feat = set() \ncorrel_matrix = train_df.corr()\n    \nfor i in range(len(correl_matrix.columns)):\n    for j in range(i):\n        if abs(correl_matrix.iloc[i, j]) > 0.8:\n            colName = correl_matrix.columns[i]  \n            correl_Feat.add(colname)\ntrain_df.drop(labels=correl_Feat, axis=1, inplace=True)","a8b7605e":"correl_Feat = set() \ncorrel_matrix = test_df.corr()\n    \nfor i in range(len(correl_matrix.columns)):\n    for j in range(i):\n        if abs(correl_matrix.iloc[i, j]) > 0.8:\n            colName = correl_matrix.columns[i]  \n            correl_Feat.add(colname)\ntest_df.drop(labels=correl_Feat, axis=1, inplace=True)","4301c2ae":"X_train = train_df.drop(['Survived'], axis = 1)\ny_train = train_df[\"Survived\"]\ntrain_names = X_train.columns\nX_test = test_df.copy()","3cc06f1a":"LE = LabelEncoder()\nX_train['Sex'] = LE.fit_transform(X_train['Sex'])\nX_train['Embarked'] = LE.fit_transform(X_train['Embarked'])\nX_train","e2d4f7b2":"LE = LabelEncoder()\nX_test['Sex'] = LE.fit_transform(X_test['Sex'])\nX_test['Embarked'] = LE.fit_transform(X_test['Embarked'])\nX_test","ab208693":"# Checking the distribution for X_train set\ncol_for_hist_train = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nfor i in X_train[col_for_hist_train]:\n    print('\\033[1m \\033[94m \\033[4m' +  'The histogram for {}'.format(i) + '\\033[0m')\n    X_train[i].hist()\n    plt.show()","d8d00347":"# Using RandomForestClassifier for univariate analysis\nRF = RandomForestClassifier(n_estimators=10, random_state=1, n_jobs=4)\nsel = SelectBySingleFeaturePerformance(variables=None, estimator=RF, scoring=\"roc_auc\", cv=10, threshold=0.5)\n\nsel.fit(X_train, y_train)\nsel.feature_performance_","88e67eee":"# plotting the imporance of the features\npd.Series(sel.feature_performance_).sort_values(ascending=False).plot.bar(figsize=(10, 5), color = ['coral', 'deepskyblue', 'wheat'])\nplt.title('Univariate performance of ML models', fontsize=14, fontweight='bold')\nplt.xticks(rotation=0, fontsize=10)\nplt.ylabel('roc-auc', fontsize=14)","10393bf5":"log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n \nclasses = [\n    GaussianNB(),\n    MultinomialNB(),\n    BernoulliNB(),\n    ComplementNB()               \n                  ]  \nfor clf in classes:\n    clf.fit(X_train, y_train)\n    mod_name = clf.__class__.__name__\n    print('\\033[1m' + mod_name + '\\033[0m')\n    print('*** Results ***')\n    y_pred = clf. predict(X_test)\n    print('Accuracy: {:.2%}'.format(clf.score(X_train, y_train)))\n    print(\"=\"*60)\n    nb_submission = pd.DataFrame({\n        \"PassengerId\": X_test.PassengerId,\n        \"Survived\": y_pred})\n    nb_submission.to_csv('{}.csv'.format(clf), index=False)\n","3da6e237":"llf = LogisticRegression(random_state=0).fit(X_train, y_train)\nprint('Accuracy: {:.2%}'.format(llf.score(X_train, y_train)))\ny_pred2 = llf.predict(X_test)\nlog_submission = pd.DataFrame({\n    \"PassengerId\": X_test.PassengerId,\n    \"Survived\": y_pred2})\nlog_submission.to_csv('LogisticRegression.csv', index=False)","de632f2e":"print('\\033[1m' + 'RandomForestClassifier()' + '\\033[0m')\nrlf = RandomForestClassifier(max_depth=5, random_state=0)\nrlf.fit(X_train, y_train)\nprint('Accuracy: {:.2%}'.format(rlf.score(X_train, y_train)))\ny_pred1 = rlf.predict(X_test)\nrlf_submission = pd.DataFrame({\n        \"PassengerId\": X_test.PassengerId,\n        \"Survived\": y_pred1})\nrlf_submission.to_csv('RandomForestClassifier().csv', index=False)    \n","7e2d9efe":"print('\\033[1m' + 'KNeigborsClassfier()' + '\\033[0m')\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nprint('Accuracy: {:.2%}'.format(knn.score(X_train, y_train)))\ny_pred4 = knn.predict(X_test)\nknn_submission = pd.DataFrame({\n        \"PassengerId\": X_test.PassengerId,\n        \"Survived\": y_pred4})\nknn_submission.to_csv('KNeigborsClassifier().csv', index=False)\n","4de84de3":"### 5. Decision Tree Classifier","bd34ac21":"print('\\033[1m' + 'DecisionTreeClassifier()' + '\\033[0m')\ndtc =  DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\nprint('Accuracy: {:.2%}'.format(dtc.score(X_train, y_train)))\ny_pred5 = dtc.predict(X_test)\ndtc_submission = pd.DataFrame({\n        \"PassengerId\": X_test.PassengerId,\n        \"Survived\": y_pred5})\ndtc_submission.to_csv('DecisionTreeClassifier().csv', index=False)\n","bb5e1b67":"<div class=\"alert alert-block alert-info\">\n<b>Note:<\/b> The columns are of integer, float and object types. Moreover, some columns contain missing values.\n<\/div>","a025f951":"<div class=\"alert alert-block alert-info\">\n<b>Feature Selection:<\/b> Sometimes some of the columns are useless in ML models by creating redundancy. That is why it is better to select the columns carefully.\n<\/div>","1b6bb947":"# Titanic Dataset - Competition","d9ed1b0c":"<div class=\"alert alert-block alert-info\">\n<b>Correlation Note:<\/b> Correlation is an important term in Statistics and Data Science and should be observed detailly. However, sometimes it creates redundancy which should be removed. \n<\/div>\n","10abac22":"### 3. Random Forest Clasifier","7e6e0f5a":"These models are as following\n* 1. Naive Bayes:\n    *   a. GaussianNB()\n    *   b. MultinomialNB()\n    *   c. BernoulliNB()\n    *   d. ComplementNB()\n* 2. Random Forest Classifier.\n* 3. Logistic Regression.\n* 4. Desicion Tree Classifier.","532e9980":"## Correlation","962f888b":"### 4. KNN Classifier","aba69aec":"Because of containing many unique features, the columns \"Name\" and \"Ticket\" are useless. Thus, they can be deleted","49ec6c5c":"## Application of Machine Learning Models","44ea6e5d":"### 2. Logistic Regression","c94608d2":"## Preprocessing of missing values","1759f5d3":"According to the accuracy results, DecisionTreeClassifier() can define all the cases correctly. However, this can be the outcome of **overfitting**, as well. This should be researched again. \nHowever, RandomForestClassifier is also a good predictor for this problem.\n\n**The files were submitted for competition**","95fbb2a7":"<div class=\"alert alert-block alert-info\">\n<b>Missing value Note:<\/b> The percentages of missing values are provided above. Because the 'Cabin' column has the highest percentage of missing values (77%>30%), this column is useless and we can delete it. While numerical columns will be imputed by the median, the dropping of missing values in 'Embarked' (because of its lowest rate) will have no impact on the data.\n<\/div>","c1cf92b8":"## Encoding categorical data","2ea10c37":"<div class=\"alert alert-block alert-info\">\n<b>Encoding Note:<\/b> Some Machine Learning algorithms are sensitive for categorical data. For this justification, the categorical data is encoded by using LabelEncoder().\n<\/div>","96d42aa0":"## Feature Selection","832b2a82":"## KDE for survived and not survived males and females","7e497912":"<div class=\"alert alert-block alert-info\">\n<b>Machine Learning:<\/b> Different Machine Learning Models have been used for finding the best accuracy.\n<\/div>","97c58e1d":"### 1. Naive Bayes "}}