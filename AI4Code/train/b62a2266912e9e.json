{"cell_type":{"cc71325b":"code","e7809629":"code","c190a971":"code","4a78a85b":"code","83811d85":"code","4486a4ff":"code","522efddf":"code","bdee4fe3":"code","8b2ed652":"code","adf4df8f":"code","40d3de6b":"code","94a69852":"code","b765511f":"code","5337b34e":"code","1873a4c5":"code","f3b3338f":"code","8f785a78":"code","4eda76e3":"code","afa86b35":"code","24a87597":"code","a693db76":"code","565483e3":"code","65b44964":"code","28e74814":"code","b1ec99c9":"code","6f6aa26f":"code","f2031000":"code","2727e4fa":"code","35e4e684":"code","ff0aa277":"code","63c981f0":"code","44270327":"code","d0d9bea8":"code","f4a4d48d":"code","2ac625e7":"code","f4a1c26c":"code","5ef351d7":"code","6e4a668e":"code","d7a773e8":"code","edb369b1":"code","7eb6fc08":"code","85f53949":"code","22778a75":"code","13af2448":"code","85b16479":"markdown","4021c85e":"markdown","4ac9f988":"markdown","e0379024":"markdown","cefc8eb0":"markdown","2bfc22ad":"markdown","380168f5":"markdown","9078279f":"markdown","1d3b2101":"markdown","52014f25":"markdown","a6fae11f":"markdown","7ba36169":"markdown","ae555d74":"markdown","5d729b92":"markdown","01abd354":"markdown","929fbf0d":"markdown","df67fc2d":"markdown","0729e6aa":"markdown","d82f3fa4":"markdown","ab87efec":"markdown","53127826":"markdown","d3ba0a2a":"markdown","d53c0100":"markdown","63c33f50":"markdown","fb689ef9":"markdown","ad6da81f":"markdown","f69504ad":"markdown","ae6a2dc7":"markdown","b7b66d03":"markdown","4707fc29":"markdown","a625306b":"markdown","28a844be":"markdown","317a042d":"markdown","b089951c":"markdown","a24e1fca":"markdown","036b4d2c":"markdown"},"source":{"cc71325b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\nimport plotly as py\nimport plotly.express as px\nimport seaborn as sns\nimport gc\n\npd.options.display.max_columns = 999","e7809629":"dtypes = {\n    'date':'int16',\n    'weight':'float16',\n    'resp_1':'float16',\n    'resp_2':'float16',\n    'resp_3':'float16',\n    'resp_4':'float16',\n    'resp':'float16',\n    'feature_0':'int8',\n    'feature_1':'float16',\n    'feature_2':'float16',\n    'feature_3':'float16',\n    'feature_4':'float16',\n    'feature_5':'float16',\n    'feature_6':'float16',\n    'feature_7':'float16',\n    'feature_8':'float16',\n    'feature_9':'float16',\n    'feature_10':'float16',\n    'feature_11':'float16',\n    'feature_12':'float16',\n    'feature_13':'float16',\n    'feature_14':'float16',\n    'feature_15':'float16',\n    'feature_16':'float16',\n    'feature_17':'float16',\n    'feature_18':'float16',\n    'feature_19':'float16',\n    'feature_20':'float16',\n    'feature_21':'float16',\n    'feature_22':'float16',\n    'feature_23':'float16',\n    'feature_24':'float16',\n    'feature_25':'float16',\n    'feature_26':'float16',\n    'feature_27':'float16',\n    'feature_28':'float16',\n    'feature_29':'float16',\n    'feature_30':'float16',\n    'feature_31':'float16',\n    'feature_32':'float16',\n    'feature_33':'float16',\n    'feature_34':'float16',\n    'feature_35':'float16',\n    'feature_36':'float16',\n    'feature_37':'float16',\n    'feature_38':'float16',\n    'feature_39':'float16',\n    'feature_40':'float16',\n    'feature_41':'float16',\n    'feature_42':'float16',\n    'feature_43':'float16',\n    'feature_44':'float16',\n    'feature_45':'float16',\n    'feature_46':'float16',\n    'feature_47':'float16',\n    'feature_48':'float16',\n    'feature_49':'float16',\n    'feature_50':'float16',\n    'feature_51':'float16',\n    'feature_52':'float16',\n    'feature_53':'float16',\n    'feature_54':'float16',\n    'feature_55':'float16',\n    'feature_56':'float16',\n    'feature_57':'float16',\n    'feature_58':'float16',\n    'feature_59':'float16',\n    'feature_60':'float16',\n    'feature_61':'float16',\n    'feature_62':'float16',\n    'feature_63':'float16',\n    'feature_64':'float16',\n    'feature_65':'float16',\n    'feature_66':'float16',\n    'feature_67':'float16',\n    'feature_68':'float16',\n    'feature_69':'float16',\n    'feature_70':'float16',\n    'feature_71':'float16',\n    'feature_72':'float16',\n    'feature_73':'float16',\n    'feature_74':'float16',\n    'feature_75':'float16',\n    'feature_76':'float16',\n    'feature_77':'float16',\n    'feature_78':'float16',\n    'feature_79':'float16',\n    'feature_80':'float16',\n    'feature_81':'float16',\n    'feature_82':'float16',\n    'feature_83':'float16',\n    'feature_84':'float16',\n    'feature_85':'float16',\n    'feature_86':'float16',\n    'feature_87':'float16',\n    'feature_88':'float16',\n    'feature_89':'float16',\n    'feature_90':'float16',\n    'feature_91':'float16',\n    'feature_92':'float16',\n    'feature_93':'float16',\n    'feature_94':'float16',\n    'feature_95':'float16',\n    'feature_96':'float16',\n    'feature_97':'float16',\n    'feature_98':'float16',\n    'feature_99':'float16',\n    'feature_100':'float16',\n    'feature_101':'float16',\n    'feature_102':'float16',\n    'feature_103':'float16',\n    'feature_104':'float16',\n    'feature_105':'float16',\n    'feature_106':'float16',\n    'feature_107':'float16',\n    'feature_108':'float16',\n    'feature_109':'float16',\n    'feature_110':'float16',\n    'feature_111':'float16',\n    'feature_112':'float16',\n    'feature_113':'float16',\n    'feature_114':'float16',\n    'feature_115':'float16',\n    'feature_116':'float16',\n    'feature_117':'float16',\n    'feature_118':'float16',\n    'feature_119':'float16',\n    'feature_120':'float16',\n    'feature_121':'float16',\n    'feature_122':'float16',\n    'feature_123':'float16',\n    'feature_124':'float16',\n    'feature_125':'float16',\n    'feature_126':'float16',\n    'feature_127':'float16',\n    'feature_128':'float16',\n    'feature_129':'float16',\n    'ts_id':'int32'\n}","c190a971":"train = pd.read_feather('..\/input\/fast-reading-w-pickle-feather-parquet-jay\/jane_street_train.feather')\nfeatures = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\ntrain = train.astype(dtypes)","4a78a85b":"# Old reduce_mem_usage. Directly use dtypes dict from now on to improve speed.\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n# train = reduce_mem_usage(train)","83811d85":"train","4486a4ff":"train.describe()","522efddf":"features","bdee4fe3":"fig, ax = plt.subplots(7, 4, figsize=(15,15))\nlabels= False, True\nfor i in range(28):\n    ax[int(i \/ 4), i%4].pie([len(features[features['tag_' + str(i)] == False]), len(features[features['tag_' + str(i)] == True])], labels = labels, autopct='%1.1f%%')\n    ax[int(i \/ 4), i%4].axis('equal')\n    ax[int(i \/ 4), i%4].set_title('Distribution of tag_' + str(i))\n    \nplt.show()","8b2ed652":"from sklearn.decomposition import PCA\ncols = list(features.columns)\ncols.remove('feature')\nX = features[cols]\nX = X.astype(int)\npca = PCA(n_components=2)\ny = pca.fit_transform(X)\nplt.figure(figsize=(15,15))\nplt.scatter(y[:, 0], y[:, 1])\nfor i in range(130):\n    plt.annotate(\"Feature \" + str(i), y[i])\nplt.title(\"Dimensionality reduction of feature metadata\")\nplt.show()","adf4df8f":"df = pd.DataFrame(y)\ndf = df.round(decimals=2)\ndf = df[df.duplicated(keep=False)]\n\ndf = df.groupby(list(df)).apply(lambda x: tuple(x.index)).tolist()\nprint (df)","40d3de6b":"fig, ax = plt.subplots(5, 4, figsize=(20,20))\ncount = 0\nfor i in df:\n    for col in i:\n        sns.distplot(train['feature_' + str(col)], label='feature_' + str(col), ax=ax[int(count \/ 4), count%4])\n        leg = ax[int(count \/ 4), count%4].legend()\n    count += 1","94a69852":"del cols, df\ngc.collect()","b765511f":"np.corrcoef(train['feature_83'], train['feature_95'])","5337b34e":"train","1873a4c5":"plt.hist(train['weight'], bins=50)\nplt.title(\"Distribution of Weight\")\nplt.show()","f3b3338f":"max(train['weight']), min(train['weight'])","8f785a78":"fig, ax = plt.subplots(3, 2, figsize=(15,15))\nj = 0\nfor i in range(1, 6):\n    if i == 5:\n        ax[int(j \/ 2), j%2].plot(train['ts_id'], train['resp'], c='y')\n        ax[int(j \/ 2), j%2].plot(train['ts_id'], [train['resp'].mean()]*len(train), c='r')\n        ax[int(j \/ 2), j%2].set_title('Distribution of Resp')\n        break\n            \n    ax[int(j \/ 2), j%2].plot(train['ts_id'], train['resp_' + str(i)])\n    ax[int(j \/ 2), j%2].plot(train['ts_id'], [train['resp_' + str(i)].mean()]*len(train), c='r')\n    ax[int(j \/ 2), j%2].set_title('Distribution of Resp_' + str(i))\n    j+= 1\n    \nplt.show()","4eda76e3":"cols = list(train.columns)\nfor removeCol in ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']:\n    cols.remove(removeCol)\n\nX = train[cols]\nX = X.fillna(0)\n\n","afa86b35":"train['feature_0'].unique()","24a87597":"labels = 1, -1\nplt.pie([len(train[train['feature_0'] == 1]), len(train[train['feature_0'] == -1])], labels = labels, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title(\"Feature 0\")\nplt.show()","a693db76":"np.corrcoef(train['feature_0'], train['resp']),np.corrcoef(train['feature_0'], train['resp']>0) ","565483e3":"pca = PCA(n_components=2)\ny = pca.fit_transform(X)\ny","65b44964":"fig = px.scatter_3d(X, x=y[0:200000, 0], y=y[0:200000, 1], z = train['resp'].values[0:200000])\nfig.show()","28e74814":"fig = px.scatter_3d(X, x=y[-200000:, 0], y=y[-200000:, 1], z = train['resp'].values[-200000:])\nfig.show()","b1ec99c9":"plt.figure(figsize=(15,15))\nsns.heatmap(train.corr())","6f6aa26f":"# Taken from this notebook: https:\/\/www.kaggle.com\/blurredmachine\/jane-street-market-eda-viz-prediction\n\ndate = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 600], \n    range_x=[-7, 7]\n)\n\nhist.show()","f2031000":"del X, features, y\ngc.collect()","2727e4fa":"\ny = train['resp']\ntrain.drop(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id'], axis=1, inplace=True)","35e4e684":"import lightgbm as lgb\n\nparams = {'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.3,\n          \"boosting_type\": \"gbdt\",\n          \"random_state\" : 42,\n          'device': 'cpu'\n}\n","ff0aa277":"threshold = 0.00\ntrain = train.values\ny = y.values\ny = y>threshold\ny = y.astype(int)\ndTrain = lgb.Dataset(train[0:2151442], y[0:2151442])","63c981f0":"validationSet = lgb.Dataset(train[2151442:2390492], y[2151442:2390492])","44270327":"model = lgb.train(params, dTrain, 2500, early_stopping_rounds = 100, valid_sets = [validationSet], verbose_eval = 50) #2500","d0d9bea8":"lgb.plot_importance(model, max_num_features = 100, figsize=(25, 25))","f4a4d48d":"confThreshold = 0.5\ndef transformPred(pred):\n    pred = pred>confThreshold\n    pred = pred.astype(int)\n    return pred","2ac625e7":"pred = model.predict(train[2151442:2390492])\nnewpred = transformPred(pred)","f4a1c26c":"del train, y, dTrain, validationSet, pred\ngc.collect()","5ef351d7":"train = pd.read_feather('..\/input\/fast-reading-w-pickle-feather-parquet-jay\/jane_street_train.feather')\ntrain = train.astype(dtypes)\ntrain","6e4a668e":"def calcUtility(p, i):\n    t = (p.sum() \/ np.sqrt((p*p).sum()) ) * np.sqrt((250\/i))\n    u = min(max(t, 0), 6) * p.sum()\n    return u","d7a773e8":"resp = train[\"resp\"]\nw = train['weight']\nresp = resp[2151442:2390492]\ni = len(train[2151442:2390492].date.unique())","edb369b1":"p = newpred * resp\nprint(\"Unweighted utility:  \" + str(calcUtility(p, i)))","7eb6fc08":"# Weighted Score:\np = w[2151442:2390492] * newpred * resp\nprint(\"Weighted utility:  \" + str(calcUtility(p, i)))","85f53949":"del newpred, resp, w, p, i\ndel train\ngc.collect()","22778a75":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","13af2448":"\nfor (test_df, sample_prediction_df) in iter_test:\n#     print(test_df.values.shape)\n    row = test_df.drop(['date', 'weight'], axis=1)\n    \n    pred = model.predict(row.values.reshape(1, -1))\n    sample_prediction_df.action = transformPred(pred)[0] #make your 0\/1 prediction here\n    env.predict(sample_prediction_df)","85b16479":"# Modelling\n\nHere's how we are going to set up the problem statement:\n1. Right now, I will only be using Resp column, not resp_1,2,3,4.\n2. I create a new column called action. If Resp is more than a threshold, action = 1, else action = 0\n\nI am going to set this threshold more than 0 (update: I set it to 0 now, as it seems more appropriate). The logic is that for very very small returns, it is not worth the risk to take the opportunity, as there are error bars in our predictions. Better to miss out on small returns than make negative return.\n\nI am going to create this column before hand, rather than making lightGBM make Resp predictions, and then applying the threshold, as it makes the task simpler.\n\nCross validation strategy (very crude):\n1. Train on First 90% of datapoints.\n2. For crossvalidation, make predictions of last 10% and calculate the p variable (in evaluation section of competition description).\n3. Train on last 10%, and use model for test predictions.\n","4021c85e":"# Introduction\n\nIn this competition, the problem statement is to make a model that can capture all the good investment opportunities, and leave out the bad ones. \n\nIt is a time series competition, so the cross-validation framework won't be straightforward. In this notebook, I want to get a good sense of the data, and the kind of information we are working with, and potentially derive some usefull insights that can be used in modelling later on. Then, we'll establish a baseline using LightGBM, along with a crude cross validation framework.\n\nLet's start by importing some libraries.","4ac9f988":"So it seems that each resp feature has a slightly different distribution, whcih is expected since they are from different time horizons.\nIt looks like Resp_1-3 have a similar distribution. 1&2 are more alike than 3, but even 3 follows more or less the same pattern. Then it looks like Resp_4 and the final Resp have a very similar distribution. Since they are also more volatile, maybe these are from a shorter time horizon? This information won't be present in test data, so we have to be careful with how we use this, especially in a time series competition.\n\nNow let's move onto the features. We already saw a few things before, but let's go through it in more detail.","e0379024":"Wow, almost 50%. Let's also see if this correlates to Resp","cefc8eb0":"There is a wide range in True\/False distribution, but for all tags, an overwhelming majority of them have False. The distribution ranges from only 1% True up to 36.9% True.\n\nNow, I think that many features will have the exact same, or very close metadata (since there are only 29 boolean flags). Let's see if any features are similar, but to do this, I will first use PCA to compress the tags into 2 dimensions.\n","2bfc22ad":"How disappointing...\nFeatures with same metadata don't have same distribution in train, however, there is some similarity. For example, in the plot for Feature_60, 61, 62, 63, 65, etc (3rd row, 3rd column), we can see that all features have a similar distribution, which is different from the rest of the plots.\n\nAlso, we shouldn't think that the metadata can explain the median and mode for the features. If you look closely, all features are centered around 0, so metadata has nothing to do with this. The shift is only due to any outliers present in features, causing the plot horizon to change.\n\nI think this metadata can only explain the variance of the features, but not really the peak or anything else. Let's just verify by calculating the correlation between two features in the same bucket.\n\nSide Notes:\n- The features have a very wide kind of distribution. Most of them seem like a normal distribution, which suports the hypothesis that features were normalized.\n- There is one very interesting set of expections (feature 60..). This has a very interesting distribution. Seems like it is multimodal, but the left mode has a much higher frequency count than the other one. ","380168f5":"# Train data\nPrinting data once again:","9078279f":"So it is boolean. Let's look at the distribution.","1d3b2101":"Now the real weighted utility.","52014f25":"Great. Not too bad for such a crude model. Now let's make the predictions and submit.","a6fae11f":"Seems like Feature 0 isn't really important. It was boolean, with 50-50 distribution, so it is not surprising. In the next iteration, it might make sense only to take the top 50-100 features for training, and leave th rest. \n\nNow for the predictions. Our model outputs the probability that we should take the action. Since we are already a bit conservative in the resp conversion, I will take the opportunity even if we are more than 50% confident. This is another hyperparameter, and is super easy to tune. Just need to change this value, and see the oof output below.","7ba36169":"Thank you for going through my notebook. I hope it helped you get a sense for the data, and a basic way to set up the problem statement. It would be great if you could give me feedback, and I'll try to incroporate it into future versions.\n\nPlease do Upvote if you Liked it!","ae555d74":"Before PCA, I just want to explore the column feature_0. It seems to be boolean and different from the rest","5d729b92":"There are 2390491 rows and 138 columns in train data.\n\nLet's go feature by feature, starting with weight","01abd354":"Plotting only first 200,000 and last 200,000, but hopefully that is representative of the data. They have similar shapes so I think it shouldn't be hiding too much information.","929fbf0d":"Display Train data","df67fc2d":"It seems that mostly features are not correlated to each other, but there are gaps where suddenly there is a very high positive or negative correlation. Also, it seems that these gaps are mostly near the central diagonal line, which means that features with similar numbers generally have a high correlation, so would have a similar distribution.\n\nInteresting thing to note again is that resp is not strongly correlated to any other feature at all. This makes this task extremely difficult. Feature engineering will also be quite interesting, and would be a more numerical \/ brute forced approach than any domain knowledge.\n\nA small animation that I liked to explore distribution of all features:","0729e6aa":"First PCA to reduce the dimension to something more understable. We have already seen a lot of distributions in the feature metadata section, so here it is more about the usefullness of the features","d82f3fa4":"Finally Training the Model. I will do it for 2500 estimators, with an early stopping if there is no improvement with 100 additional estimators. The validation set is the last 10% of rows, and score will be printed after every 50 estimators. Letting it run:","ab87efec":"Really quick small points:\n- There seem to be null values in some of the columns. When exploring the data in depth, we should make sure to check the percentage of null values and drop any high null value features\n- Also, Feature_0 seems to be a binary variable. We'll look into that later.\n\nLet's just find some basic statistics for the train data","53127826":"Yes, the correlation is very poor, so looks like the metadata is not helpful. Even though features may have similar standard deviation, for every individual datapoint in train, features in same bucket are not correlated. Let's move on to see the train data.","d3ba0a2a":"These parameters are just a hunch. I haven't tuned them, but plan to do so in future iterations.","d53c0100":"We will use resp, to calculate the return. Also, i is the number of unique days, so that's is what the last line does.","63c33f50":"Quick points:\n- From the look of it, null values don't seem to be a concern. Most columns seem to have almost all values.\n- It looks like most features have a median at or very close to 0. Similar story with the mean. It seems that the features may have been normalized as well. We'll look into that later.\n\n# Features Metadata\n\nNow, let's look at the faetures metadata file.","fb689ef9":"Looks like there are a large number of 0s. One critical thing is that rows with 0 weight are only present in train, and in test data, there won't be any 0s. I think that the ones with 0 weight should only be used for training, and any non-0 weights could be used for cross validation. Also, the max weight for a single datapoint is 167.293. When modelling, it's important to take special care for these high weight outliers. Making the right decision here would give a huge score boost, compared to getting 10-15 low weight ones right.\n\nMoving on to resp features. Now, these are returns of investment, so let's plot them on a time series. I will use ts_id instead of date, because date repeats, but ts_id is just the chronological id, so it won't mess with our plot.\nThe red line is the average return for each resp","ad6da81f":"This just defines the metric used for evaluation in the competition. We'll use it for crossvalidation","f69504ad":"Great, now we have our predictions. Just need to read in the train data again to find all of the returns.","ae6a2dc7":"Okay, now before making predictions and finding cross-validation score, let's first just plot the feature importances. I will only plot the top 100 features.","b7b66d03":"This is the threshold for the action I mentioned earlier. It is tunable, so is essentially a hyperparameter. Also, the training data is essentially the first 90% of data. The number is just to find the first 90%. The validation is the last 10%","4707fc29":"Finally. Now let's first calculate the unweighted score.","a625306b":"The above is a list of all features very close to each other (within 0.01 of PCA features). Now, for each set of similar features, let's plot their distribution of train data. If the distributions are similar, then it means that this metadata is directly linked to values in train data.","28a844be":"With the PCA, I can't really see any direct clear trend. The market really does seem to be volatile. Maybe there are some complex interactions between features that help a model make a better decision. For this reason, I am going to establish the baseline with Gradient Boosting Decision Trees, as they can take into account feature interaction quite well.\n\nLet's see if any features are correlated with each other.","317a042d":"I was right! Some features have the exact same metadata, so they fall right on top of each other after PCA. Let's isolate the sets of overlapping features. Maybe features that have the same metadata have the same distribution in train as well?","b089951c":"For reading train data, I am using the feather format. All formats are available in this notebook: https:\/\/www.kaggle.com\/pedrocouto39\/fast-reading-w-pickle-feather-parquet-jay\/output. This saves lot of time, and gets code running faster.\n\nUsing this, we can read data in 5 seconds and only use 600 mb!","a24e1fca":"The first correlation is for whether feature 0 is directly correlated to Resp, and the second set is if feature 0 is correlated to whether resp is positive or negative. It seems like there is nothing directly significant.","036b4d2c":"So each feature has 29 boolean flags to describe the nature of the feature. Before we do any detailed analysis, let's see the distribution of True and False for each tag. From what I see, there is an overwhelming majority of False"}}