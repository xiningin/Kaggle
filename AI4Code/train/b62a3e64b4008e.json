{"cell_type":{"282f6762":"code","e45f35eb":"code","350fdb48":"code","422184c5":"code","43473f3c":"code","0e8a0a4b":"code","48058983":"code","ef2dc881":"code","44ff5f58":"code","55599204":"code","d607badd":"code","a94d164c":"code","500684f8":"code","8b4e7948":"code","799d4857":"code","0cb52b1b":"code","c9a71bd1":"code","167159a0":"code","8367c3cc":"code","113a641a":"code","c7192df1":"code","bfaf29eb":"code","6daaf368":"code","4c172368":"code","b6d602a4":"code","684c7536":"code","6b89b9f7":"code","f4362828":"code","c4b3c2f2":"code","118fada2":"code","f2a82361":"code","39463a46":"code","4e796a80":"code","fd0f142e":"code","a7b09fbf":"code","6d4535b2":"code","497c2c7b":"code","a96f5e2b":"code","fbefb8d3":"code","fc33b265":"code","4cecd444":"code","226448d4":"code","f83872f7":"code","8f013769":"code","7a9dbb42":"markdown","149ccafe":"markdown","23638640":"markdown","5df4a042":"markdown","44f9d721":"markdown","e01341f1":"markdown","2151430f":"markdown","cc65073b":"markdown","3a7e7fbe":"markdown","b211e1e0":"markdown","6d013c33":"markdown","f247c503":"markdown","2783b7f4":"markdown","01b013e8":"markdown","c846802d":"markdown","dae80700":"markdown","57552c55":"markdown","3bd4dca4":"markdown","f0a8b3df":"markdown","85f6151b":"markdown","287643eb":"markdown","36857bdf":"markdown","105035c7":"markdown","85073c14":"markdown","07b908e1":"markdown","e9cc6054":"markdown","2091f4db":"markdown","b9136cd7":"markdown","eec2903f":"markdown","d68bc2a0":"markdown"},"source":{"282f6762":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e45f35eb":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom datetime import date\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics\nimport warnings\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\ndata_=pd.read_csv('\/kaggle\/input\/customer-personality-analysis\/marketing_campaign.csv',header=0,sep='\\t')\ndata = data_.copy()\n\npd.set_option(\"max_columns\", None)","350fdb48":"!pip install dataprep","422184c5":"data.head()","43473f3c":"from dataprep.eda import plot, plot_correlation, create_report, plot_missing\nplot(data)","0e8a0a4b":"# Customer Age\ndata[\"Age\"] = 2021 - data[\"Year_Birth\"]","48058983":"# Let's see how much the customer spends in total by summing up the expenditures divided into categories\ndata[\"Total_Spend\"] = data[\"MntWines\"] + data[\"MntFruits\"] + data[\"MntMeatProducts\"] + data[\"MntFishProducts\"] + data[\"MntSweetProducts\"] + data['MntGoldProds']","ef2dc881":"# Since when is the customer our customer? In other words, we subtract 1-2 days after the last shopping date in the data from the customer's first registration date.\ndata[\"Dt_Customer\"] = pd.to_datetime(data[\"Dt_Customer\"], dayfirst=True)","44ff5f58":"last_date = date(2014,7,1)\ndata[\"T\"] = pd.to_numeric(data[\"Dt_Customer\"].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast=\"integer\")","55599204":"data[\"T\"].describe()","d607badd":"data[\"Marital_Status\"] .value_counts()","a94d164c":"data[\"Education\"].value_counts()","500684f8":"data[\"Marital_Status\"] = data[\"Marital_Status\"].replace({'Divorced':'Single', 'Single':'Single', 'Alone':'Single',\n                                                        'Widow':'Single', 'Absurd':'Single', 'YOLO':'Single','Married':'Married','Together':'Married'})\ndata[\"Education\"] = data[\"Education\"].replace({'Basic':'Undergraduate', '2n Cycle':'Undergraduate', 'Graduation':'Postgraduate', 'Master':'Postgraduate', 'PhD':'Postgraduate'})","8b4e7948":"data[\"Children\"] = data[\"Kidhome\"] + data[\"Teenhome\"]\ndata[\"Has_Child\"] = np.where(data.Children >0, 'Has child', 'No child')","799d4857":"data = data.rename(columns={'MntWines': 'Wines', 'MntFruits':'Fruits',\n                           'MntMeatProducts':'Meat', 'MntFishProducts':'Fish',\n                           'MntSweetProducts':'Sweets', 'MntGoldProds':'Gold'})","0cb52b1b":"drop_list = ['ID' ,'Dt_Customer', 'Z_CostContact', 'Z_Revenue', 'Year_Birth']\ndata.drop(drop_list, axis=1 ,inplace=True)","c9a71bd1":"data.describe().T","167159a0":"num_cols = [col for col in data.columns if data[col].dtype != 'O' and data[col].max() > 40]","8367c3cc":"for i in num_cols:\n    plt.figure(figsize=(15,7))\n    sns.boxplot(x=data[i]);\n    plt.show()","113a641a":"def outlier_th(dataframe, col_name, q1=0.10, q3=0.90):\n    q1 = dataframe[col_name].quantile(q1)\n    q3 = dataframe[col_name].quantile(q3)\n    iqr = q3 - q1\n    low_limit = q1 - 1.5 * iqr\n    up_limit = q3 + 1.5 * iqr\n    \n    return low_limit, up_limit\n\nlow_income, up_income = outlier_th(data, 'Income', q1=0.01, q3=0.99)\nlow_age, up_age = outlier_th(data, 'Age', q1=0.25, q3=0.75)","c7192df1":"data = data[~((data['Age'] < low_age) | (data['Age'] > up_age))]\ndata = data[~((data['Income'] < low_income) | (data['Income'] > up_income))]","bfaf29eb":"data[['Age', 'Income']].describe().T","6daaf368":"# Clearing missing values\ndata = data.dropna()\ndata.isnull().sum()","4c172368":"# Let's look at the correlation between the variables\ncorr = data.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, annot=True, cmap='PuRd');","b6d602a4":"le = LabelEncoder()\ncat_cols = [col for col in data.columns if data[col].dtypes == \"O\"]\nfor i in cat_cols:\n    data[i] = le.fit_transform(data[[i]])","684c7536":"# I will do the standardization process, I will copy my data, I am doing it so as not to spoil the original.\ndf = data.copy()\n# I want to leave out variables like accepted campaigns\ncol_del = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']\ndf = df.drop(col_del, axis=1)\n# Scaling\nscaler = StandardScaler()\nscaler.fit(df)\nscaled_df = pd.DataFrame(scaler.transform(df), columns=df.columns)\nprint(\"Scaled OK!\")","6b89b9f7":"scaled_df.head()","f4362828":"# I set the number of dimensions as 3\npca = PCA(n_components=3)\npca.fit(scaled_df)\npca_df = pd.DataFrame(pca.transform(scaled_df), columns=[\"columns1\", \"columns2\", \"columns3\"])","c4b3c2f2":"pca_df.describe().T","118fada2":"x = pca_df[\"columns1\"]\ny = pca_df[\"columns2\"]\nz = pca_df[\"columns3\"]\n\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(x, y, z, c=\"maroon\", marker=\"o\")\nplt.show()","f2a82361":"elbow_method = KElbowVisualizer(KMeans(), k=10)\nelbow_method.fit(pca_df)\nelbow_method.show()","39463a46":"gmm = GaussianMixture(n_components=4, covariance_type='spherical', max_iter=2000, random_state=42).fit(pca_df)\nlabels = gmm.predict(pca_df)","4e796a80":"pca_df['Clusters'] = labels","fd0f142e":"data['Clusters'] = labels","a7b09fbf":"# Plot the clusters\nfig = plt.figure(figsize=(12,7))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=pca_df['Clusters'], marker=\"o\", cmap=\"Paired_r\")\nplt.show()","6d4535b2":"cl = ['#FAD3AE', '#855E46', '#FE800F', '#890000']\nplt.figure(figsize=(14,8))\nsns.countplot(x=data['Clusters'], palette=cl)","497c2c7b":"plt.figure(figsize=(14,8))\nsns.jointplot(x=data[\"Total_Spend\"], y=data[\"Income\"], hue=data[\"Clusters\"], palette=cl);","a96f5e2b":"plt.figure(figsize=(15,6))\nsns.boxenplot(x=\"Clusters\", y=\"Total_Spend\", palette=cl, data=data);","fbefb8d3":"prod = ['Fish', 'Meat', 'Sweets', 'Wines', 'Gold']\nfor i in prod:\n    plt.figure(figsize=(15,6))\n    sns.boxenplot(x=\"Clusters\", y=i, palette=cl ,data=data);","fc33b265":"data['Total_Camp'] = data[\"AcceptedCmp1\"] + data[\"AcceptedCmp2\"] + data[\"AcceptedCmp3\"] + data[\"AcceptedCmp4\"] + data[\"AcceptedCmp5\"]\nplt.figure(figsize=(15,7))\nsns.countplot(x=\"Total_Camp\", hue=\"Clusters\", palette=cl, data=data)","4cecd444":"plt.figure(figsize=(15,7))\nsns.boxenplot(x=\"Clusters\", y=\"NumDealsPurchases\", palette=cl, data=data);","226448d4":"place = [\"NumWebPurchases\", \"NumCatalogPurchases\", \"NumStorePurchases\"]\nfor i in place:\n    sns.boxenplot(x=\"Clusters\", y=i, palette=cl,data=data);\n    plt.show()","f83872f7":"data[\"Age\"] = pd.cut(x=data[\"Age\"], bins=[0,18,35,55,61,95])","8f013769":"prf = [\"Children\", \"Age\", \"Education\", \"Marital_Status\"]\nfor i in prf:\n    plt.figure(figsize=(15,7))\n    sns.swarmplot(x=i, y=\"Total_Spend\", hue=\"Clusters\", palette=cl ,data=data)\n    plt.show()","7a9dbb42":"Elbow Method, the square sum of the distances of the points from the cluster center according to each K value is calculated. According to these values, a graph is drawn for each K value. The elbow point on the graph where the difference between the totals starts to decrease is determined as the most appropriate K value.\n\nFor our example here, the optimal number of clusters seems to be 4.","149ccafe":"For us, the champion group doesn't seem to be chasing discounted shopping too much. For us, the champion group doesn't seem to be chasing discounted shopping too much. The 1st group, which closely follows our champion group, has the largest share in discount shopping. We see that the opportunities offered for discount shopping work well","23638640":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#E8A84B;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\"> EDA \u2692<\/p>\n<center><img\nsrc=\"https:\/\/i.pinimg.com\/originals\/91\/16\/8b\/91168b4873f6659b3e9fdfe4b89cd864.gif\" style=\"width:50%;height:50%;\">\n<\/center>","5df4a042":"The closeness of the 1st and 2nd groups to each other is striking. If we look at the average of the 1st group, we see that it is always higher, but if we look at the expenditures on the Gold side, the 1st and 2nd groups are really close to each other.\n\n\nSo how do the clusters that accept our campaigns disperse?","44f9d721":"When we look at the statistics in the data set, there are outliers in customer age and income value. I will suppress them with the iqr method to reduce their misleading effect.","e01341f1":"# <p style=\"background-color:#EFCD59;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\">Customer Personality Analysis<\/p>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/bugrabuga\/kaggle-picture\/master\/customeranalysis.png\">","2151430f":"**At the end of the analysis, I interpreted the clusters, I did not use tags such as champion, loyal customers for clusters. Here, I only made small comments by showing the clustering and the similarities within the group I clustered. These clusters can be used to implement strategy and send the appropriate campaign.\nThanks for looking, see you soon!**","cc65073b":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#6D50FF;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\"> PREPROCESSING <\/p>","3a7e7fbe":"I have the yield ready and I can perform the clustering process. I will use the **Gaussian Mixture Model** for clustering. GMM assumes that there are a certain number of Gaussian distributions, and each distribution represents a cluster. This model tends to group data points belonging to the same distribution.\n\nAssuming that the sources of the given samples are k-gaussian distributions, it is the optimization of the Gaussian parameters of these sources to maximize the probability density function of the mixture. Thus, GMM is successful even where the modeling algorithms, assuming that the data set is produced from a single distribution, come to a dead end.\n\nI will perform the clustering process with these steps:\n\n* Determining the number of clusters to be created with the Elbow Method\n* Clustering with Gaussian Mixture Model\n* Visualization of created clusters\n\n<center>\n<img\nsrc=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ea\/K-means_convergence.gif\" style=\"width:30%;height:30%;\">\n<center\/>","b211e1e0":"<a id=\"6\"><\/a>\n# <p style=\"background-color:#65DDBA;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\"> CUSTOMER PROFILE <\/p>","6d013c33":"We see that the biggest and most profitable customer group for us is the 2nd group. Cluster 1's proximity to this group cannot be ignored\n\n\nLet's look at the distribution of clusters by products","f247c503":"* In this project, I will perform data clustering on customer records in a grocery store database. I will segment customers to optimize the importance of each customer to the business. The purpose here is for the business to get to know its customers and to launch a special campaign for the customer group. We can start by making the dataset available for analysis. Let's start with the customer personality analysis task with Python.","2783b7f4":"# Dataset Description:\n\n**People**\n\n* ID: Customer's unique identifier\n* Year_Birth: Customer's birth year\n* Education: Customer's education level\n* Marital_Status: Customer's marital status\n* Income: Customer's yearly household income\n* Kidhome: Number of children in customer's household\n* Teenhome: Number of teenagers in customer's household\n* Dt_Customer: Date of customer's enrollment with the company\n* Recency: Number of days since customer's last purchase\n* Complain: 1 if customer complained in the last 2 years, 0 otherwise\n\n**Products**\n\n* MntWines: Amount spent on wine in last 2 years\n* MntFruits: Amount spent on fruits in last 2 years\n* MntMeatProducts: Amount spent on meat in last 2 years\n* MntFishProducts: Amount spent on fish in last 2 years\n* MntSweetProducts: Amount spent on sweets in last 2 years\n* MntGoldProds: Amount spent on gold in last 2 years\n\n**Promotion**\n\n* NumDealsPurchases: Number of purchases made with a discount\n* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n**Place**\n\n* NumWebPurchases: Number of purchases made through the company\u2019s web site\n* NumCatalogPurchases: Number of purchases made using a catalogue\n* NumStorePurchases: Number of purchases made directly in stores\n* NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month","01b013e8":"We see that the 1st group prefers it more in website shopping. In the shopping made using the catalog, our champion group is still ahead.","c846802d":"We see that there is no participation in most of the campaigns. From here, it can be thought that the campaigns are not well planned and do not go to the right target.\n\n\nSo what's the deal with discount shopping?","dae80700":"<a id=\"7\"><\/a>\n# <p style=\"background-color:#E24356;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\"> GROUP ASSESSMENTS <\/p>","57552c55":"<img src=\"https:\/\/miro.medium.com\/max\/828\/1*uSQooRdyLd2aZsadN9Sb0w.gif\">","3bd4dca4":"Here we created an unsupervised learning model. So we don't have a labeled feature to use to evaluate our model. The evaluation in clustering algorithms is to examine the patterns that occur.\n\nNow we can examine the groups that our clusters form","f0a8b3df":"## Summary Statistics","85f6151b":"<a id=\"4\"><\/a>\n# <p style=\"background-color:#F37D16;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\"> CLUSTERING <\/p>","287643eb":"When we look at the correlation table, we can say that there is a relationship between total_spend and wines - meat and income columns. Assuming a strong relationship above 0.70, the strongest relationship between these columns is between total_spend and wines.\n\nWhen we look at the table, our data set looks clean and there is no multicollinearity problem. ","36857bdf":"<a id=\"5\"><\/a>\n# <p style=\"background-color:#A5D204;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\"> MODEL EVALUATION <\/p>","105035c7":"Let's look at the preferences of the places where the groups shop.","85073c14":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#D2A303;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\">Library Importing and Loading Data \ud83d\udcd5<\/p>","07b908e1":"We see the most people in group 1.","e9cc6054":"<img src=\"https:\/\/raw.githubusercontent.com\/bugrabuga\/kaggle-picture\/master\/customer-group.png\">","2091f4db":"Before we move on to clustering, let's apply the standardization process and we need to reduce the size because there are too many features, I will use the PCA method for this.\n\n----\n\nI'm starting with the standardization process, I'll label encoding for categorical features","b9136cd7":"Let's interpret the clusters according to the Income and Total Spend features:\n\n* Group 0: low spend - low income\n* Group 1: high spend - average income\n* Group 2: high spend - high income \n* Group 3: low spend - average income","eec2903f":"# <p style=\"background-color:#EFCD59;font-family:newtimeroman;color:#F4FBFE;font-size:150%;text-align:center;border-radius:10px 10px;\">TABLE OF CONTENTS<\/p>\n\n## [1. IMPORTING LIBRARIES](#1)\n \n## [2. EDA ](#2)\n\n## [3. PREPROCESSING](#3)\n\n## [4. CLUSTERING](#4)\n\n## [5. MODEL EVALUATION](#5)\n\n## [6. CUSTOMER PROFILE](#6)\n\n## [7. GROUP ASSESSMENTS](#7)","d68bc2a0":"The higher the number of features in the clustering problem, the more difficult it will be to work with, processing high-dimensional data will bring high processing power and cost. So we can do size reduction before classification. Dimension reduction is the process of reducing the number of said random variables by obtaining a set of fundamental variables.\n\nHere we will use PCA.\n\n**PCA** : is a technique that reduces the dimensionality of datasets, increases their interpretability, and also minimizes information loss."}}