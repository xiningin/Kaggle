{"cell_type":{"0e70b1bb":"code","faf11765":"code","634241f1":"code","23b74d00":"code","97324477":"code","024a87d3":"code","7ec51276":"markdown","432814ac":"markdown","0968493f":"markdown","1490898c":"markdown","00248d9a":"markdown","d285f032":"markdown","5d26ce44":"markdown","6ba07806":"markdown"},"source":{"0e70b1bb":"!pip install pytorch-tabnet","faf11765":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport datatable as dt\n\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\nfrom sklearn.model_selection import StratifiedKFold\n\nDEVICE = 'gpu' if torch.cuda.is_available() else 'cpu'\n\ninput_path = '\/kaggle\/input\/'\nroot_path = os.path.join(input_path, 'jane-street-market-prediction')\n\n#reading files\ntrain = dt.fread(os.path.join(root_path, \"train.csv\")).to_pandas()\nfloat64_cols = train.select_dtypes('float64').columns\ntrain[float64_cols] = train[float64_cols].astype('float32')\nresp_cols = [i for i in train.columns if 'resp' in i]\nmeta_features = dt.fread(os.path.join(root_path, \"features.csv\")).to_pandas()\n\n\nfeatures_names = list(set(train.columns) - set(resp_cols) - set(['weight', 'ts_id', 'date']))\nfeatures_index = list(map(lambda x: int(re.sub(\"feature_\", \"\", x)), features_names))\nfeatures_tuples = sorted(list(zip(features_names, features_index)), key = lambda x: x[1])\njust_features = [i[0] for i in features_tuples]\n\nimport janestreet\nenv = janestreet.make_env()","634241f1":"train = train.loc[train['weight'] != 0]\n# binarize the target\ntrain['action'] = (train['resp'].values > 0).astype(int)\n#train = train.fillna(-99999)\nf_mean = train.mean()\ntrain = train.fillna(f_mean)\n\n# split data for training and free data space usage to prevent exceeding maximum allowed\nX_features = train.loc[:, just_features]\ny_target = train.loc[:, 'action']\ndel train\n\nprint('Finished.')","23b74d00":"MAX_EPOCHS = 200\nBATCH_SIZE = 1024\nVIRTUAL_BATCH_SIZE = 128\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nprint(\"Using {}\".format(DEVICE))","97324477":"clf = TabNetClassifier(n_d=64, n_a=64, n_steps=5,\n                       gamma=1.5, n_independent=2, n_shared=2,\n                       cat_emb_dim=1, lambda_sparse=1e-4, \n                       momentum=0.3, clip_value=2., optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2), scheduler_params = {\"gamma\": 0.95,\n                         \"step_size\": 20},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n                       device_name = DEVICE\n)\n\nclf.fit(X_train=X_features.values, y_train=y_target.values,\n    max_epochs=MAX_EPOCHS , patience=20,\n    batch_size=BATCH_SIZE, virtual_batch_size=VIRTUAL_BATCH_SIZE,\n    num_workers=0,\n    weights=1,\n    drop_last=False\n)\n","024a87d3":"# perform test and create submissions file\nprint('Creating submissions file...', end='')\nrcount = 0\nI_WANT_TO_SUBMIT = True\nif I_WANT_TO_SUBMIT: \n    for (test_df, prediction_df) in env.iter_test():\n        X_test = test_df.loc[:, just_features].fillna(f_mean)\n        y_preds = clf.predict(X_test.values)\n        prediction_df.action = y_preds.item()\n        env.predict(prediction_df)\n        rcount += len(test_df.index)\n    print(f'Finished processing {rcount} rows.')","7ec51276":"# Submit\n\nBy default I don't, be sure to switch *I_WANT_TO_SUBMIT* to *True*","432814ac":"## Install pytorch-tabnet, read data, downcast Training, make env","0968493f":"Hi all, as you may know, 2020 brought the first (that I know) deep learning architecture for tabular data ([here](https:\/\/arxiv.org\/pdf\/1908.07442.pdf) the paper for those interested). Since it proved to be high-performing for many datasets, including in some of the latest kaggle [competitions](https:\/\/www.kaggle.com\/c\/lish-moa\/notebooks?competitionId=19988&sortBy=scoreAscending&searchQuery=tabnet), why not give a try here? \n\n\n### Props to: \n\n- [pytorch-tabnet](https:\/\/github.com\/dreamquark-ai\/tabnet) I think this library should get a round of applause, fitting a neural network is as easy as a scikit-learn estimator;\n\n- [binary-classification-example](https:\/\/github.com\/dreamquark-ai\/tabnet\/blob\/develop\/census_example.ipynb) a notebook with an example tailored for our use case;\n\n- https:\/\/www.kaggle.com\/wilddave\/xgb-starter I want this to be a complementary to his notebook. \n\n\nI don't have much time (and knowledge) to add a proper torch customization or to use the pretrainer, but I may do it in the next weeks. \n\n**Please let me know what you think about it!**\n\n<img src=\"https:\/\/www.europol.europa.eu\/sites\/default\/files\/images\/finance_budget.jpg\">\n\n\n","1490898c":"### TabNetClassifier","00248d9a":"## Fill Na, define features and target for model","d285f032":"### Hyperparameters and use GPU","5d26ce44":"# Fit and Predict","6ba07806":"# Final Take\n\nI find it as easy to use as other gradient boosting alternatives. Please tell me what you think in the comments. "}}