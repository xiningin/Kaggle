{"cell_type":{"38cff3f7":"code","8e54b932":"code","895a281d":"code","fbf2463f":"code","8cd719cc":"code","05355c61":"code","798a9c23":"code","f9b83fc7":"code","e26c3289":"code","15775631":"code","7d4f8a2f":"code","4e5c596a":"code","8b284b4d":"code","59076c8f":"code","3949131a":"code","8480806e":"markdown","db09dcee":"markdown","df969999":"markdown","b8fb1b36":"markdown","d9b5bb94":"markdown","eb20bfb8":"markdown","689a5070":"markdown","7078100d":"markdown","301d44c9":"markdown","f553a55b":"markdown","68da5e64":"markdown","f7debb49":"markdown","7c393d09":"markdown","500030c2":"markdown"},"source":{"38cff3f7":"import pandas as pd\nimport nltk\nimport seaborn as sns\nimport re\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('..\/input\/bag-of-words-meets-bags-of-popcorn-\/labeledTrainData.tsv', sep='\\t')\ndf.head()","8e54b932":"%%time\n# Here we get transform the documents into sentences for the word2vecmodel\n# we made a function such that later on when we make the submission, we don't need to write duplicate code\ndef preprocess(df):\n    df['review'] = df.review.str.lower()\n    df['document_sentences'] = df.review.str.split('.') \n    df['tokenized_sentences'] = list(map(lambda sentences: list(map(nltk.word_tokenize, sentences)), df.document_sentences))  \n    df['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)), df.tokenized_sentences))\n\npreprocess(df)","895a281d":"from sklearn.model_selection import train_test_split\ntrain, test, y_train, y_test = train_test_split(df.drop(columns='sentiment'), df['sentiment'], test_size=.2)","fbf2463f":"#Collecting a vocabulary\nvoc = []\nfor sentence in train.tokenized_sentences:\n    voc.extend(sentence)\n\nprint(\"Number of sentences: {}.\".format(len(voc)))\nprint(\"Number of rows: {}.\".format(len(train)))","8cd719cc":"%%time\nfrom gensim.models import word2vec, Word2Vec\n\nnum_features = 300    \nmin_word_count = 3    \nnum_workers = 4       \ncontext = 8           \ndownsampling = 1e-3   \n\n# Initialize and train the model\nW2Vmodel = Word2Vec(sentences=voc, sg=1, hs=0, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n                    sample=downsampling, negative=5, iter=6)","05355c61":"%%time\ndef sentence_vectors(model, sentence):\n    #Collecting all words in the text\n    words=np.concatenate(sentence)\n    #Collecting words that are known to the model\n    model_voc = set(model.wv.vocab.keys()) \n    \n    sent_vector = np.zeros(model.vector_size, dtype=\"float32\")\n    \n    # Use a counter variable for number of words in a text\n    nwords = 0\n    # Sum up all words vectors that are know to the model\n    for word in words:\n        if word in model_voc: \n            sent_vector += model[word]\n            nwords += 1.\n\n    # Now get the average\n    if nwords > 0:\n        sent_vector \/= nwords\n    return sent_vector\n\ntrain['sentence_vectors'] = list(map(lambda sen_group:\n                                      sentence_vectors(W2Vmodel, sen_group),\n                                      train.tokenized_sentences))","798a9c23":"def vectors_to_feats(df, ndim):\n    index=[]\n    for i in range(ndim):\n        df[f'w2v_{i}'] = df['sentence_vectors'].apply(lambda x: x[i])\n        index.append(f'w2v_{i}')\n    return df[index]\nX_train = vectors_to_feats(train, 300)\nX_train.head()","f9b83fc7":"%%time\ntest['sentence_vectors'] = list(map(lambda sen_group:sentence_vectors(W2Vmodel, sen_group), test.tokenized_sentences))\nX_test=vectors_to_feats(test, 300)","e26c3289":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","15775631":"from sklearn.metrics import roc_auc_score, confusion_matrix\nroc_auc_score(y_test,lr.predict_proba(X_test)[:,1])\n","7d4f8a2f":"import seaborn as sns\nimport matplotlib.pyplot as plt\ndf_cm = pd.DataFrame(confusion_matrix(y_test,lr.predict(X_test)), index = ['predicted positive', 'predicted negative'],\n                  columns = ['actual positive', 'actual negative'])\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True)\nplt.show()","4e5c596a":"voc_df = []\nfor sentence_group in df.tokenized_sentences:\n    voc_df.extend(sentence_group)\n\nprint(\"Number of sentences: {}.\".format(len(voc_df)))\nprint(\"Number of texts: {}.\".format(len(df)))","8b284b4d":"%%time\nfrom gensim.models import word2vec, Word2Vec\n\nnum_features = 300    \nmin_word_count = 3    \nnum_workers = 4       \ncontext = 8           \ndownsampling = 1e-3   \n\n# Initialize and train the model\nW2Vmodel = Word2Vec(sentences=voc_df, sg=1, hs=0, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n                    sample=downsampling, negative=5, iter=6)","59076c8f":"%%time\ndf['sentence_vectors'] = list(map(lambda sen_group: sentence_vectors(W2Vmodel, sen_group), df.tokenized_sentences))\ndf = vectors_to_feats(df, 300)\ny = pd.read_csv('..\/input\/bag-of-words-meets-bags-of-popcorn-\/labeledTrainData.tsv', sep='\\t')['sentiment'].values","3949131a":"from sklearn.model_selection import ShuffleSplit, cross_val_score\ncv = ShuffleSplit(n_splits=5, random_state=1)\n\ncv_score = cross_val_score(lr, df, y ,cv=cv, scoring='roc_auc')\nprint(cv_score, cv_score.mean())","8480806e":"* Pretty good result. It would be hard to beat one using 'bags' techniques. ","db09dcee":"# USING WORD2VEC EMBEDDINGS AS FEATURES IN SUPERVISED LEARNING","df969999":"And now we're ready to evaluate results:","b8fb1b36":"To transform sentece groups into feature vectors we have to average vectors of particular words in a sentence (we can't use sum cause sentences have different word counts). Of course, our model can only give back vectors of words that are already in the vocabulary.","d9b5bb94":"Now let's split our data to train and test sets. We'll use 20% of the data for evaluation and 80% for training.","eb20bfb8":"Now we'll check it on CV with a vocabulary of all texts. It can be treated as an infprmation leak, but in this case it doesn't mean much.","689a5070":"Now let's implement a function to preprocess textual data in the dataset. In the first step we're lowing the letters in the content. Then we construct already tokenized sentences with the help of ***nltk*** package. Then we remove empty lists to make the model more accurate. Note that you can further delete non-ascii characters, implement stemming or lemmatization or perform some other desirable steps. ","7078100d":"# Data prepocessing\n\nTo get better results you should fisrt clear your data. In case of word2vec we need to split hte text into sentences and futher tokenize senteces. Some prefer to delete punctional marks then. Some prefer not to even split senteces to wors when it comes to big texts. It's up to you.\n\nWe'll work with classified film reviews from IMDb.","301d44c9":"We can observe very stable evaluations on CV which demonstrates low varience and no overfitting is observed.\n\n# Outro\nThat's it. We've briefly covered word2vec implementation in supervised NLP tasks. Remember: word2vec is always the one to try when it comes to NLP!","f553a55b":"# Introduction\n\nIn this tutorial we'll briefly cover the benchmark you should always try when dealing with some NLP task - word2vec - and how to use embeddings as features in supervised learning.\n\nWord2vec generally is an unsupervised learning algorithm, designed by Google developers and released in 2013, to learn vector representations of words The main idea is to encode words with close meaning that can substitute each other in a context as close vectors in an X-dimensional space. In other words, we assume that similar words have similar company like 'tell me who's your friend...'.\n\n\n**Why it's useful**\n\nWhen we have to use words in machine learning models, unless we are using tree based models, we need to convert the words into some kind of numeric representations.  The easiest way of doing this would be using some encoding methods of converting the word into a sparse matrix with only one non-zero element marking a corresponding word. Not even talking about the sparseness,   such aprroaches doens't give any information about local context of the words, it strips away information about words which commonly appear close together in sentences (or between sentences).\n\nSo, as a human-being, you understand that there's nothing more important for analyzing sequential text data than the context each word is used in. Keeping information about the context provides an opportunity to define sentimentally close words as generally similar instances which can be very important when it comes to analyzing text data.\n\n**How is it working**\n\nAn algorithm tries to refrlect the meaning of a word by analyzing its context. The algorithm exists in two flavors: ***CBOW*** and ***Skip-Gram***. In the second approach, looping over a corpus of sentences, model tries to use the current word to predict its neighbors, or in ***CBOW*** it tries to predict the current word with the help of each of the contexts. The limit on the number of words in each context is determined by a parameter called \u201cwindow size\u201d.\n\nBesides allowing for a numerical representation of textual data, the resulting embeddings also learn interesting relationships between words.\n\nPython word2vec implementation can be found as a part ***gensim*** package.","68da5e64":"The next step is constructing a vocabulary with all of the tokens extracted from the training data.","f7debb49":"Now we need to initialize and train word2vec model. We'll set the values of some key parameters.\n\n- *size* - the dimensionality of word vectors (big values take long to compute);\n- *min_count* - minimium frequency count of words;\n- *window* - how many closest words will be used as a context;\n- *workers* - number of threads;","7c393d09":"Now we only have to extract vectors of different dimensions from a list of sentence vectors. ","500030c2":"We'll repeat the same steps for the test part of the set. "}}