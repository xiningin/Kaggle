{"cell_type":{"fd65e1a1":"code","ec914ea3":"code","4cf12039":"code","be3118dd":"code","dc80c76b":"code","42e6d4e8":"code","7b999567":"code","a669d474":"code","54f27e2c":"code","4bb9e968":"code","b230cebb":"code","d806dd97":"code","0ab270e2":"code","2e3c0d21":"markdown","307dfdb4":"markdown","d25f65eb":"markdown","6a93e848":"markdown","8afa9b21":"markdown","7fa7cd0c":"markdown","02010c49":"markdown","b2fc7999":"markdown","350c3f8e":"markdown","4da7b216":"markdown"},"source":{"fd65e1a1":"# Reading the Libraries\nimport pandas as pd\nimport numpy as np\n\n# Reading the Data\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.head(20)","ec914ea3":"data.shape","4cf12039":"data.info()","be3118dd":"# feature names as a list\n# .columns gives columns names in data \ncol = data.columns       \nprint(col)\n\n\ndata.drop(['Unnamed: 32',\"id\"], axis=1, inplace=True)\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny_data = data.diagnosis.values\nx_data = data.drop(['diagnosis'], axis=1)\nx_data","dc80c76b":"y_data","42e6d4e8":"# Using transformer from sklearn library\nfrom sklearn.preprocessing import MinMaxScaler\nscalar = MinMaxScaler()\noutput = scalar.fit_transform(x_data)\n\n# Manual Implementation of the normalization process\nX_data = (x_data -np.min(x_data))\/ (np.max(x_data)-np.min(x_data)).values","7b999567":"X_data","a669d474":"class LogisticRegression(object):\n    \"\"\"\n    Logistic Regression Classifier\n    Parameters\n    ----------\n    learning_rate : int or float, default=0.1\n        The tuning parameter for the optimization algorithm (here, Gradient Descent) \n        that determines the step size at each iteration while moving toward a minimum \n        of the cost function.\n    max_iter : int, default=100\n        Maximum number of iterations taken for the optimization algorithm to converge\n    \n    penalty : None or 'l2', default='l2'.\n        Option to perform L2 regularization.\n    C : float, default=0.1\n        Inverse of regularization strength; must be a positive float. \n        Smaller values specify stronger regularization. \n    tolerance : float, optional, default=1e-4\n        Value indicating the weight change between epochs in which\n        gradient descent should terminated. \n    \"\"\"\n\n    def __init__(self, learning_rate=0.1, max_iter=100, regularization='l2', lambda_ = 10 , tolerance = 1e-4):\n        self.learning_rate  = learning_rate\n        self.max_iter       = max_iter\n        self.regularization = regularization\n        self.lambda_        = lambda_\n        self.tolerance      = tolerance\n        self.loss_log       = []\n    \n    def fit(self, X, y, verbose = False):\n        \"\"\"\n        Fit the model according to the given training data.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Target vector relative to X.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        self.theta = np.random.rand(X.shape[1] + 1)\n        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n        self.loss_log = []\n\n        for iteration in range(self.max_iter):\n            Z = np.matmul(X,  self.theta)\n            y_hat = self.__sigmoid(Z)\n            \n            errors = y_hat - y\n\n            N = X.shape[1] \n            \n            cost = (-1.0\/N) * np.sum( y*np.log(y_hat) + (1.0 - y)*np.log(1.0-y_hat))\n            self.loss_log.append(cost)\n            \n            if verbose:\n                print(f'Iteration {iteration} Loss: {cost}')\n\n            if self.regularization is not None:\n                delta_grad = (1.\/N) *(np.matmul(errors.T, X)+ self.lambda_ * self.theta)\n            else:\n                delta_grad = (1.\/N) *(np.matmul(errors.T, X))\n                \n            self.theta -= self.learning_rate * delta_grad\n\n#             if np.all(abs(delta_grad) >= self.tolerance):\n#                 self.theta -= self.learning_rate * delta_grad\n#             else:\n#                 break\n                \n        return self\n\n    def predict_proba(self, X):\n        \"\"\"\n        Probability estimates for samples in X.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Vector to be scored, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n        Returns\n        -------\n        probs : array-like of shape (n_samples,)\n            Returns the probability of each sample.\n        \"\"\"\n        return self.__sigmoid((X @ self.theta[1:]) + self.theta[0])\n    \n    def predict(self, X):\n        \"\"\"\n        Predict class labels for samples in X.\n        Parameters\n        ----------\n        X : array_like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n        Returns\n        -------\n        labels : array, shape [n_samples]\n            Predicted class label per sample.\n        \"\"\"\n        return np.round(self.predict_proba(X))\n        \n    def __sigmoid(self, z):\n        \"\"\"\n        The sigmoid function.\n        Parameters\n        ------------\n        z : float\n            linear combinations of weights and sample features\n            z = w_0 + w_1*x_1 + ... + w_n*x_n\n        Returns\n        ---------\n        Value of logistic function at z\n        \"\"\"\n        return (1.0 \/ (1.0 + np.exp(-z)))\n\n    def get_params(self):\n        \"\"\"\n        Get method for models coeffients and intercept.\n        Returns\n        -------\n        params : dict\n        \"\"\"\n        try:\n            params = dict()\n            params['intercept'] = self.theta[0]\n            params['coef'] = self.theta[1:]\n            return params\n        except:\n            raise Exception('Fit the model first!')","54f27e2c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=42)\n\n# Train and Test Data Summary\nimport plotly.graph_objects as go\nsplit = ['Train','Test']\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=split, y=[np.sum(y_train), np.sum(y_test)],#                base=[-500,-600],\n                    marker_color='crimson',\n                    name='Malignant'))\nfig.add_trace(go.Bar(x=split, \n                     y=[len(y_train)- np.sum(y_train), len(y_test) - np.sum(y_test)],\n                    base=0,\n                    marker_color='lightgreen',\n                    name='Benign'                ))\nfig.update_layout(width = 800, height = 400)\nfig.update_layout(title = 'Count of Samples in Train and Test Split', title_x = 0.5, xaxis_title = \"Category\", yaxis_title = 'Sample Count')\nfig.show()","4bb9e968":"MAX_ITER = 400\nLR_RATE = 1e-2\n\nclf_no_reg = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE,  lambda_ = 200, regularization= None)\nclf_no_reg.fit(X_train, y_train, verbose = False)\n\nclf_reg_10 = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 10, regularization= 'l2')\nclf_reg_10.fit(X_train, y_train, verbose = False)\n\n\nclf_reg_20 = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 20, regularization= 'l2')\nclf_reg_20.fit(X_train, y_train, verbose = False)\n\nclf_reg_40 = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 40, regularization= 'l2')\nclf_reg_40.fit(X_train, y_train, verbose = False)\n","b230cebb":"import plotly.graph_objects as go\nimport numpy as np\n\ny = clf_no_reg.loss_log \n\n# fig = go.Figure(data=go.Scatter(x= np.arange(start =1, stop = len(y)), \n#                                 y=y,\n#                                 mode = 'lines+markers'))\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_no_reg.loss_log ,\n                    mode='lines+markers',\n                    name='No Regularization'))\n\nfig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_10.loss_log ,\n                    mode='lines+markers',\n                    name='Regulrization W=10'))\n\nfig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_20.loss_log ,\n                    mode='lines+markers',\n                    name='Regulrization W=20'))\n\nfig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_40.loss_log ,\n                    mode='lines+markers',\n                    name='Regulrization W=40'))\n\n\nfig.update_layout(title = \"Error Plot over Iterations\", title_x = 0.5,\n                  xaxis_title = 'Iteration',\n                  yaxis_title = 'Log Loss',\n                  width = 800,\n                  height = 500)\nfig.show()","d806dd97":"from sklearn import metrics\nprint(f'Accuracy {metrics.accuracy_score(y_test, clf_no_reg.predict(X_test))*100}%')\nprint(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_10.predict(X_test))*100}%')\nprint(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_20.predict(X_test))*100}%')\nprint(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_40.predict(X_test))*100}%')","0ab270e2":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, clf_no_reg.predict(X_test)))\nsns.heatmap(confusion_matrix(y_test, clf_no_reg.predict(X_test)), annot=True)","2e3c0d21":"### Plot Training Loss over Time ","307dfdb4":"### Train and Test Data Validation\nLet us split the whole data into two portion. We take 80% data in the train set and then put rest of the data into the test set to check the performance of the trained model. ","d25f65eb":"## Accuracy Calculation","6a93e848":"## Plot Confusion Matrix","8afa9b21":"Let's have a look at the health of the data itself. This function `.info()` in the pandas library is very helpful to understand the basic properties of the data itself. If there is any missing values in the dataset can be known right from here so that they can be taken care of before fitting into a model for training and testing. \n","7fa7cd0c":"## Training Logistic Regression\nNow we train the logistic regression with the training data for a maximum interation of 200. Other parameters are kept default. Feel free to fiddle around the other parameters to understand more of them. ","02010c49":"**There are 4 things that take my attention**\n\n1. There is an **`id`** that cannot be used for classificaiton \n2. **`Diagnosis`** column in the data is our class label\n3. **`Unnamed: 32`** feature includes **`NaN`** so we do not need it.\n4. I do not have any idea about other feature names actually.\n\nTherefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!","b2fc7999":"## Normalization\nNormalization refers to rescaling real-valued numeric attributes into a 0 to 1 range. Data normalization is used in machine learning to make model training less sensitive to the scale of features.\n\nYou can either implement the conversion process with basic python or use `MinMaxScaler()` function from the `sklearn` library. ","350c3f8e":"Okey, now we have features but **`what does they mean`** or actually **`how much do we need to know about these features`**\nThe answer is that we do not need to know meaning of these features however in order to imagine in our mind we should know something like variance, standart deviation, number of sample (count) or max min values.\nThese type of information helps to understand about what is going on data. For example , the question is appeared in my mind the **`area_mean`** feature's max value is 2500 and **`smoothness_mean`** features' max 0.16340. Therefore **do we need standirdization or normalization before visualization, feature selection, feature extraction or classificaiton?** The answer is yes and no not surprising ha :) Anyway lets go step by step and start with visualization. ","4da7b216":"## The Basics: Logistic Regression and Regularization\n\nLogistic Regression is one of the most common machine learning algorithms used for classification. It a statistical model that uses a logistic function to model a binary dependent variable. In essence, it predicts the probability of an observation belonging to a certain class or label. For instance, is this a cat photo or a dog photo?\n\nOrdinary Least Squares linear regression is powerful and versatile right out of the box, but there are certain circumstances where it fails. \n1. it is, expressly, a \u2018regression\u2019 framework, which makes it hard to apply as a classifier.\n2. unlike, say, a decision tree, linear regression models don\u2019t perform their own implicit feature selection, meaning they are prone to overfit if too many features are included. \n\nLuckily, there are some extensions to the linear model that allow us to overcome these issues. Logistic regression turns the linear regression framework into a classifier and various types of **`regularization`** of which the `Ridge` and `Lasso` methods are most common, help avoid overfit in feature rich instances.\n\n### **Hypothesis:** \nWe want our model to predict the probability of an observation belonging to a certain class or label. As such, we want a hypothesis $h$ that satisfies the following condition $0 <= h(x) <= 1$ , where $x$ is an observation.\n\nWe define $h(x) = g(w^T * x)$ , where $g$ is a sigmoid function and $w$ are the trainable parameters or `weights`. As such, we have:\n$$h(x) = \\frac{1}{1+e^{-w^Tx}}$$\n\n### The cost for an observation: \nNow that we can predict the probability for an observation, we want the result to have the minimum error. If the class label is $y$, the cost (error) associated with an observation $x$ is given by:\n\n![](https:\/\/miro.medium.com\/max\/525\/1*vSGnYVz6I7sAObKuxuFAoQ.gif)\n\n### Cost Function: \nThus, the total cost for all the $m$ observations in a dataset is:\n![](https:\/\/miro.medium.com\/max\/368\/0*vZnp94vCoN0vMDAj)\n\nWe can rewrite the cost function J as:\n![](https:\/\/miro.medium.com\/max\/691\/0*o57ug0iMGDJVI1qo)\n\nThe objective of logistic regression is to find params `w` so that `J` is minimum. How can we do that?? We will use the gradient descent algorithm to update each of the weights gradually to minimize the cost `J`. \n\nWe will update each of the params w\u1d62 using the following template:\n![](https:\/\/miro.medium.com\/max\/875\/0*Q6ssvXABrvHUZrfy)\n![](https:\/\/miro.medium.com\/max\/496\/0*7uVvuW-ZGauNWH_V)\n\nThe above step will help us find a set of params w\u1d62, which will then help us to come up with $h(x)$ to solve our binary classification task.\nBut there is also an undesirable outcome associated with the above gradient descent steps. In an attempt to find the best $h(x)$, the following things happen:\n\n**CASE I: For class label = 0**: $h(x)$ will try to produce results as close 0 as possible. As such, $w^T.x$ will be as small as possible\n=> Wi will tend to -infinity\n\n**CASE II: For class label = 1**: $h(x)$ will try to produce results as close 1 as possible. As such, $w^T.x$ will be as large as possible\n=> Wi will tend to +infinity\n\n\n## Regularization:\nRegularization is a technique to solve the problem of overfitting in a machine learning algorithm by penalizing the cost function. It does so by using an additional penalty term in the cost function.\nThere are two types of regularization techniques:\n1. Lasso or L1 Regularization\n2. Ridge or L2 Regularization (We will implement here)\nSo, how can L2 Regularization help to prevent overfitting? Let\u2019s first look at our new cost function:\n\n![](https:\/\/miro.medium.com\/max\/628\/0*Nc_ocecF0dHpUutK)\n\n\nThe regularization term will heavily penalize large $w_i$. The effect will be less on smaller $w_i$\u2019s. As such, the growth of $w$ is controlled. The $h(x)$ we obtain with these controlled params $w$ will be more generalizable.\n\n**NOTE:** $\u03bb$ is a hyper-parameter value. We have to find it using cross-validation. \n* Larger value $\u03bb$ of will make $w_i$ shrink closer to $0$, which might lead to underfitting. \n* $\u03bb = 0$, will have no regulariztion effect. \n\nWhen choosing $\u03bb$, we have to take proper care of bias vs variance trade-off.\n"}}