{"cell_type":{"678e3f80":"code","16726001":"code","172cb256":"code","7e8715e6":"code","eeaa3a9e":"code","9027baf6":"code","a52d9882":"code","865e1a44":"code","5905eb1a":"code","ae38eeea":"code","2c4060ec":"code","654c85eb":"code","734618c8":"code","8becc4ab":"code","80def872":"code","ed405709":"code","7f7edd87":"code","6f113195":"code","b7b1626d":"code","f59bc16a":"code","af7bf8c1":"code","e6dbdd12":"code","49a43819":"code","e3de5eb9":"code","d9899d3b":"code","48c4147b":"code","8415c2cb":"code","59fe2fca":"code","f41967dd":"code","31f20e9d":"code","193262d1":"code","afbb74b4":"code","eab531cd":"code","873b63bd":"code","5018e22a":"markdown","074b07ad":"markdown","075ca51a":"markdown","87cf82f5":"markdown","74f166c4":"markdown","f2f520da":"markdown","3222ba14":"markdown","acf23669":"markdown","cb7c2a04":"markdown","cc439976":"markdown","c7648fce":"markdown","e4d4f71f":"markdown","9298b59a":"markdown","02cc2d41":"markdown","4b48cbac":"markdown","9720c3ff":"markdown","23a36ce0":"markdown","9e3999d0":"markdown","97c269c8":"markdown","83b2402b":"markdown","afa55669":"markdown","3a8a0683":"markdown","e86685ed":"markdown"},"source":{"678e3f80":"# importing libraries\nimport os\nimport torch\nimport torchvision\nimport tarfile\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor,ToPILImage\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision.utils import make_grid\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as tt\nimport numpy as np","16726001":"from numba import cuda\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","172cb256":"# Extract from tar archive file\nwith tarfile.open('..\/input\/cifar10\/cifar10.tgz', 'r:gz') as tar:\n    tar.extractall(path='.\/input')","7e8715e6":"print(os.listdir('.\/input\/cifar10'))\nclasses= os.listdir('.\/input\/cifar10\/train')\nprint(classes)","eeaa3a9e":"cat_files= os.listdir('.\/input\/cifar10\/train\/cat')\nprint('No of training example for cats:',len(cat_files))\nprint(cat_files[:5])\n","9027baf6":"stats= ((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)) #mean and std\ntrain_tfm= tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), # transormation of data together\n                       tt.RandomHorizontalFlip(),\n                       tt.ToTensor(),tt.Normalize(*stats,inplace=True)])\nvalid_tfm = tt.Compose([tt.ToTensor(),tt.Normalize(*stats)])","a52d9882":"# Image transformation\ndata_dir = '.\/input\/cifar10'\ntrain_ds = ImageFolder(data_dir+'\/train',train_tfm)\nvalid_ds= ImageFolder(data_dir+'\/test', valid_tfm)","865e1a44":"train_ds","5905eb1a":"img, label= train_ds[0]\nprint(img.shape,label)\nimg","ae38eeea":"def show_image(img,label):\n    print('Label: ', train_ds.classes[label],\"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1,2,0))","2c4060ec":"show_image(train_ds[222][0],train_ds[222][1])","654c85eb":"show_image(*train_ds[1543])","734618c8":"show_image(*train_ds[453])","8becc4ab":"batch_size=400","80def872":"# Dataloader to load data in batches(mini batch)\ntrain_dl= DataLoader(train_ds,batch_size,shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl= DataLoader(valid_ds, batch_size, num_workers=3,pin_memory=True)","ed405709":"def show_batch(dl):\n    for images, labels in dl:\n        fig,ax= plt.subplots(figsize=(12,12))\n        ax.set_xticks([]) #hide ticks\n        ax.set_yticks([])\n        ax.imshow(make_grid(images[:64],nrow=8).permute(1,2,0))\n        break # printing only first 64 images from first batch","7f7edd87":"show_batch(train_dl)","6f113195":"show_batch(valid_dl)","b7b1626d":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","f59bc16a":"device = get_default_device()\ndevice","af7bf8c1":"train_dl= DeviceDataLoader(train_dl,device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","e6dbdd12":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n","49a43819":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)\n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)\n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n                                        nn.Flatten(), \n                                        nn.Linear(512, num_classes))\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out","e3de5eb9":"model= to_device(ResNet9(3,10), device)\nmodel","d9899d3b":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()  # Realsing cuda memory otherwise might get cuda out of memory error\n    history = []\n    \n    #custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train() #Setting training mode\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","48c4147b":"history = [evaluate(model, valid_dl)]\nhistory","8415c2cb":"epochs = 70\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","59fe2fca":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","f41967dd":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.figure(figsize=(10,6))\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","31f20e9d":"\nplot_accuracies(history)","193262d1":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.figure(figsize=(10,6))\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","afbb74b4":"plot_losses(history)","eab531cd":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.figure(figsize=(10,6))\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","873b63bd":"plot_lrs(history)","5018e22a":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#dfbf9f' role=\"tab\" aria-controls=\"home\"><center>Validation Accuracy ~ 94% <\/center><\/h1>\n","074b07ad":"Let's take a look at some sample images in batch from the training dataloader.","075ca51a":"The dataset is extracted to the directory `input\/cifar10`. It contains 2 folders `train` and `test`, containing the training set (50000 images) and test set (10000 images) respectively. Each of them contains 10 folders, one for each class of images. Let's verify this using `os.listdir`.","87cf82f5":"`As expected, the learning rate starts at a low value, and gradually increases for 30% of the iterations to a \nmaximum value of `0.01`, and then gradually decreases to a very small value as per leslie smith.`","74f166c4":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#dfbf9f' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated \ud83d\ude0a<\/center><\/h2>\n","f2f520da":"<a id=\"1\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#dfbf9f' role=\"tab\" aria-controls=\"home\"><center>About Dataset<\/center><\/h1>\n\n`CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80\n million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, \n with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.`\n\n","3222ba14":"<a id=\"5\"><\/a>\n# ResNet","acf23669":"It's clear from the trend that our model isn't overfitting to the training data just yet. Finally, let's visualize how the learning rate changed over time, batch-by-batch over all the epochs.","cb7c2a04":"<a id=\"6\"><\/a>\n## Training the model\n\nBefore we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n\n* **Learning rate scheduling**: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the **\"One Cycle Learning Rate Policy\"**, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https:\/\/sgugger.github.io\/the-1cycle-policy.html\n\n* **Weight decay**: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https:\/\/towardsdatascience.com\/this-thing-called-weight-decay-a7cd4bcfccab\n\n* **Gradient clipping**: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https:\/\/towardsdatascience.com\/what-is-gradient-clipping-b8e815cdfb48\n\n\nLet's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch.","cc439976":"<a id=\"1\"><\/a>\n# Import Libaries","c7648fce":"<a id=\"2\"><\/a>\n# Exploring the Data","e4d4f71f":"![cifar10](https:\/\/miro.medium.com\/max\/709\/1*LyV7_xga4jUHdx4_jHk1PQ.png)","9298b59a":"We will use the ResNet9 architecture(https:\/\/www.myrtle.ai\/2018\/09\/24\/how_to_train_your_resnet\/) :\n\n![resnet-9](https:\/\/github.com\/lambdal\/cifar10-fast\/raw\/master\/net.svg?sanitize=true)\n\n![resnet-9](https:\/\/miro.medium.com\/max\/1140\/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n","02cc2d41":"We'll use a relatively large batch size of 400 to utlize a larger portion of the GPU RAM.","4b48cbac":"<a id=\"4\"><\/a>\n# GPU Helper Functions","9720c3ff":"<a id=\"3\"><\/a>\n# Data Transformation (Data Normalisation and Data Augmentation)","23a36ce0":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#dfbf9f' role=\"tab\" aria-controls=\"home\"><center>CIFAR-10 - Object Recognition in Images usnig ResNet9<\/center><\/h1>","9e3999d0":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>Thank You    \ud83d\ude4f <\/center><\/h1>\n","97c269c8":"<a id=\"7\"><\/a>\n# Conclusion","83b2402b":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available).","afa55669":"To use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required.","3a8a0683":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#dfbf9f' role=\"tab\" aria-controls=\"home\"><center>Table of Contents<\/center><\/h1>\n\n- [Import Libaries](#1)\n- [Exploring the Data](#2)     \n- [Data Transformation (Data Normalisation and Data Augmentation)](#3)\n- [GPU Helper Functions](#4)\n- [ResNet](#5)\n- [Training the model](#6)\n- [Conclusion](#7)","e86685ed":"`Need to use NO_GRAD to keep the update out of the gradient computationIt boils down to the DYNAMIC GRAPH that PyTorch uses. \nIt allows us to perform regular Python operations on tensors, independent of PyTorch\u2019s computation graph.``"}}