{"cell_type":{"4697f59f":"code","8024b43d":"code","b5668607":"code","193143e3":"code","fbf5aff8":"code","7c7f6bd4":"code","d3af46c7":"code","5414f04b":"code","279b385f":"code","8670347f":"code","2fb21127":"code","7c30aa8c":"code","adf6e169":"code","04cd9484":"code","6bf6f545":"code","758ce516":"code","495f9260":"code","af5cbe56":"code","75ec1e88":"code","7e8c4d24":"markdown","a17b96b7":"markdown","0b800941":"markdown","0534d093":"markdown","4c1d053f":"markdown","2cc25030":"markdown","2b3d00f0":"markdown"},"source":{"4697f59f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8024b43d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Import things that I need","b5668607":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndisplay(df)\n\n# Read train file","193143e3":"null_value = df.isnull().sum()\nprint(null_value)\n\n# Check null values I can see some null values in this data","fbf5aff8":"df_drop = df.dropna(axis=0)\nprint(df_drop.isnull().sum())\ndisplay(df_drop)\n\n# In this case, I just use dropna for drop all rows that have null values. Than I can check there are no null values after I did dropna process","7c7f6bd4":"drop_val = [\"PassengerId\",\"Name\",\"Ticket\"]\n\ndf_1 = df_drop.drop(drop_val,axis=1)\ndisplay(df_1)\n\n# I think those things are not affect to result so I drop those columns","d3af46c7":"def encoder(x):\n    if \"female\" in x:\n        return 0\n    else:\n        return 1\n\n# For doing machine learning, we should encode all char values to num values so I defined function that encode male, female in to 0 and 1.\n# In this case I define \"Female\" as 0 and \"Male\" as 1","5414f04b":"sex_num = df_1.Sex.apply(lambda x : encoder(x)).to_frame()\ndf_2 = df_1.drop(labels=\"Sex\",axis=1)\ndf_2[\"Sex\"] = sex_num\ndisplay(df_2)\n\n# I apply encoder function to \"Sex\" column.","279b385f":"embark_encoded = pd.get_dummies(df_2.Embarked)\ndisplay(embark_encoded)\ndf_3 = pd.concat([df_2,embark_encoded],axis=1)\ndf_3.drop(labels=\"Embarked\",axis=1,inplace=True)\ndisplay(df_3)\n\n# When we see that Embarked columns there are three values. C,Q,S. So I used \"pd.get_dummies\" to encode char to num.\n#  After I made \"embark_encoded\", I use concat to sum up with df_2 and defined new dataframe called df_3","8670347f":"cabin_encoded = pd.get_dummies(df_3.Cabin)\ndisplay(cabin_encoded)\n\n# I also did same thing on \"cabin\" column. ","2fb21127":"df_4 = pd.concat([df_3,cabin_encoded],axis=1)\ndf_4.drop(labels=\"Cabin\",axis=1,inplace=True)\ndisplay(df_4)","7c30aa8c":"scaler = MinMaxScaler()\ndf_5 = scaler.fit_transform(df_4)\ndf_5 = pd.DataFrame(df_5,columns=df_4.columns)\ndisplay(df_5)\n\n# OK we made all columns char to num. After this process we should see whether some values are too high than other values.\n# We can check that \"Age\" and \"Fare\" columns have high values than other columns. So we should do some normalization.\n# In this case I use \"MinMaxScaler\" and made new dataframe df_5.","adf6e169":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n# OK we finished preprocessing. We should import model that we can use. I use \"RandomForestClassifier\", \"AdaBoostClassifier\" and \"SVC\"\n# And for scoring, I import \"accuracy_score\" and \"f1_score\"","04cd9484":"X = df_5.iloc[:,1:]\ny = df_5.iloc[:,0]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=62)\n\nmodel1 = RandomForestClassifier()\nmodel1.fit(X_train,y_train)\nmodel1_preds = model1.predict(X_test)\naccuracy1 = accuracy_score(y_test,model1_preds)\nf1_1 = f1_score(y_test,model1_preds)\nprint(\"Accuracy score of RFC : {} F1 score : {}\".format(accuracy1,f1_1))\n\n# First, I made RandomForestClassifier as model1 and check accuracy and f1 score. ","6bf6f545":"model2 = AdaBoostClassifier()\nmodel2.fit(X_train,y_train)\nmodel2_preds = model2.predict(X_test)\naccuracy2 = accuracy_score(y_test,model2_preds)\nf1_2 = f1_score(y_test,model2_preds)\nprint(\"Accuracy score of AdaBoost : {} F1 score : {}\".format(accuracy2,f1_2))\n\n# Second, I made AdaBoostClassifier as model2. It has higher accuracy score and f1 score than model1","758ce516":"model3 = SVC()\nmodel3.fit(X_train,y_train)\nmodel3_preds = model3.predict(X_test)\naccuracy3 = accuracy_score(y_test,model3_preds)\nf1_3 = f1_score(y_test,model3_preds)\nprint(\"Accuracy score of SVC : {} F1 score : {}\".format(accuracy3,f1_3))\n\n# Third, I made SVC model as model3 and check accuracy and f1 score. It is lower than model2.","495f9260":"#I select Model2(AdaBoost)\n\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import make_scorer\nimport time\n\n# As a result, I choosed model2 as final model and I will do GridSearch to find best hyperparameters.","af5cbe56":"parameters = {\n    \"learning_rate\" : np.arange(0.1,1,0.01)\n}\n\nscorer = make_scorer(f1_score)\n\ngs = GridSearchCV(estimator=model2,param_grid=parameters,scoring=scorer,n_jobs=6)\nstart = time.time()\ngs.fit(X_train,y_train)\nend = time.time()\nprint(\"Search Time : {} seconds\".format(end-start))\n\n# I defined scoring method as f1 score and my cpu has 6 cpu core so n_jobs is 6. \n# I did gridsearch with \"n_estimators\" but with those things, f1 score become lower than default value of AdaBoostClassifier. \n# So I just defined learning_rate in parameters.\n# Check time and it just took 23 seconds.","75ec1e88":"model4 = gs.best_estimator_\nmodel4_preds = model4.predict(X_test)\naccuracy4 = accuracy_score(y_test,model4_preds)\nf1_4 = f1_score(y_test,model4_preds)\nprint(\"Best estimator accuracy score : {} f1 score : {}\".format(accuracy4,f1_4))","7e8c4d24":"<font size=\"5\" font color = \"#08DAF6\"> 2. Train three model <\/font><br>","a17b96b7":"<font size=\"5\" font color = \"#08DAF6\"> 3. Find best model <\/font><br>","0b800941":"<font size=\"4\"> Thank you for seeing my code.[](http:\/\/)","0534d093":" <font size=\"6\" font color = \"#08DAF6\"> Hello this is ML for titanic Data <\/font><br>\n \n <font size=\"4\"> I use sklearn for doing ML. I'm beginner for doing ML and data analysis so feel free to give some advice for me!\n <font size=\"4\"> Thank you very much for seeing my code and have a nice day!\n     \n     \n     \n     \n\n","4c1d053f":"<font size=\"5\" font color = \"#08DAF6\"> 4. Do GridSearch <\/font><br>","2cc25030":"<font size=\"5\" font color = \"#08DAF6\"> 1. Preprocessing <\/font><br>","2b3d00f0":" <font size=\"5\"> ML process \n <font size=\"4\"> \n 1. Preprocessing\n 2. Train three model\n 3. Find best model \n 4. Use GridSearch to find best estimators"}}