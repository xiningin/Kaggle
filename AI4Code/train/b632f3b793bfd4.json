{"cell_type":{"83aecc9d":"code","2f3f9581":"code","4df9e1df":"code","bb8a8412":"code","2f0de820":"code","eb8628bf":"code","450b33d2":"code","dd0bb656":"code","d9df3f2a":"code","7acf3f07":"code","6113fefb":"code","8c18b6a3":"code","1927d41a":"code","01b16e28":"code","667401bd":"code","c7f05021":"code","aac5cdd1":"code","a637f83f":"code","28a5fca0":"code","26d7ce12":"code","e61b48e1":"code","b1ab646d":"code","c7becbed":"code","0d5bbc92":"code","46db56d4":"code","6e9cc62f":"code","230247d8":"code","3869fe46":"code","9c12c6b2":"markdown","182edad9":"markdown","023abe9a":"markdown","77b8cdfd":"markdown","ea5d7033":"markdown","63b395f6":"markdown","1004beb9":"markdown","cf324eb8":"markdown","09bf6e98":"markdown","c44f053d":"markdown","506aeffe":"markdown","3aa7a00e":"markdown","29138279":"markdown","ad890e8d":"markdown","1d26707a":"markdown","20f492d3":"markdown","fad33eae":"markdown","c6038a24":"markdown","e238baac":"markdown","aaf4ab2e":"markdown","e5f73bb1":"markdown","120a725e":"markdown","3dfdef5b":"markdown","9c4aca89":"markdown","d6c6d471":"markdown","3104656c":"markdown","7c1c6e1b":"markdown"},"source":{"83aecc9d":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nimport re\nimport albumentations as A\n%matplotlib inline","2f3f9581":"INPUT_DIR = '..\/input\/global-wheat-detection'\nTRAIN_DIR = f'{INPUT_DIR}\/train'","4df9e1df":"sample = cv2.imread(TRAIN_DIR+'\/01189a3c3.jpg', cv2.IMREAD_UNCHANGED)\n\ndimensions = sample.shape\nheight = sample.shape[0]\nwidth = sample.shape[1]\nn_of_channels = sample.shape[2]\n\nprint('Image characteristics:\\n')\nprint('Dimensions: {}\\nHeight: {}, Width:{}, Number of channels: {}'.format(dimensions, height, width, n_of_channels))","bb8a8412":"train_df = pd.read_csv(INPUT_DIR+'\/train.csv')\ntrain_df.head()","2f0de820":"print(train_df['height'].unique(), train_df['width'].unique())","eb8628bf":"# No of unique images in the csv file\n\nprint('Unique images: ',len(train_df['image_id'].unique()))\n\n# Different regions for which data is collected\n\nprint('Regions: ',train_df['source'].unique())\n\n# No of unique images for each region\nregion_list = []\nunique_images = []\nfor region in train_df['source'].unique():\n    region_list.append(region)\n    unique_images.append(len(train_df[train_df['source']== region]['image_id'].unique()))\n    print('Region: {}, Number of Images: {}'.format(str(region), len(train_df[train_df['source']== region]['image_id'].unique())))","450b33d2":"fig, ax = plt.subplots()\nax.pie(unique_images, labels = region_list, autopct='%1.1f%%')\nax.axis('equal')\nplt.show()","dd0bb656":"train_df['x_min'] = -1\ntrain_df['y_min'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall('([0-9]+[.]?[0-9]*)', x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x_min', 'y_min', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns = ['bbox'], inplace = True)\ntrain_df['x_min'] = train_df['x_min'].astype(np.float)\ntrain_df['y_min'] = train_df['y_min'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)\n\ntrain_df['x_max'] = train_df['x_min'] + train_df['w']\ntrain_df['y_max'] = train_df['y_min'] + train_df['h']\n\ntrain_df.head()","d9df3f2a":"train_df.drop(columns = ['width', 'height'], inplace=True)\ntrain_df.head()","7acf3f07":"def show_sample_images(image_data):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (12, 8))\n    ax = ax.flatten()\n    \n    image = cv2.imread(os.path.join(TRAIN_DIR + '\/{}.jpg').format(image_data.iloc[0]['image_id']), cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n    \n    ax[0].set_title('Original Image')\n    ax[0].imshow(image)\n    \n    for i, row in image_data.iterrows():\n        cv2.rectangle(image,\n                      (int(row['x_min']), int(row['y_min'])),\n                      (int(row['x_max']), int(row['y_max'])),\n                      (220, 0, 0), 3)\n    \n    ax[1].set_title('Image with Bounding Boxes')\n    ax[1].imshow(image)\n    \n    plt.show()\n        ","6113fefb":"show_sample_images(train_df[train_df['image_id'] == 'b6ab77fd7'])","8c18b6a3":"def get_bboxes(bboxes, col, bbox_format = 'pascal_voc', color='white'):\n    for i in range(len(bboxes)):\n        x_min = bboxes[i][0]\n        y_min = bboxes[i][1]\n        x_max = bboxes[i][2]\n        y_max = bboxes[i][3]\n        width = x_max - x_min\n        height = y_max - y_min\n        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor=color, facecolor='none')\n        col.add_patch(rect)","1927d41a":"def show_augmented_images(aug_result, image_data):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (12, 8))\n    ax = ax.flatten()\n    \n    image = cv2.imread(os.path.join(TRAIN_DIR + '\/{}.jpg').format(image_data.iloc[0]['image_id']), cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n    \n    aug_image = aug_result['image']\n\n    get_bboxes(pascal_voc_boxes, ax[0], color='red')\n    orig_bboxes = pascal_voc_boxes\n    ax[0].set_title('Original Image with Bounding Boxes')\n    ax[0].imshow(image)\n\n    get_bboxes(aug_result['bboxes'], ax[1], color='red')\n    aug_bboxes = aug_result['bboxes']\n    ax[1].set_title('Augmented Image with Bounding Boxes')\n    ax[1].imshow(aug_image)\n    \n    plt.show()","01b16e28":"image = cv2.imread(os.path.join(TRAIN_DIR + '\/b6ab77fd7.jpg'), cv2.IMREAD_COLOR)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\nimage_id = 'b6ab77fd7'","667401bd":"pascal_voc_boxes = train_df[train_df['image_id'] == image_id][['x_min', 'y_min', 'x_max', 'y_max']].astype(np.int32).values\nlabels = np.ones((len(pascal_voc_boxes), ))","c7f05021":"aug = A.Compose([\n    A.CLAHE(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","aac5cdd1":"aug = A.Compose([\n    A.Equalize(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","a637f83f":"aug = A.Compose([\n    A.Blur(blur_limit=15, p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","28a5fca0":"aug = A.Compose([\n    A.RandomCrop(512, 512, p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","26d7ce12":"aug = A.Compose([\n    A.Resize(512,512, p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","e61b48e1":"aug = A.Compose([\n    A.RandomGamma(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","b1ab646d":"aug = A.Compose([\n    A.ShiftScaleRotate(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","c7becbed":"aug = A.Compose([\n    A.RandomBrightnessContrast(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","0d5bbc92":"aug = A.Compose([\n    A.RandomSizedBBoxSafeCrop(height=512, width = 512, p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","46db56d4":"aug = A.Compose([\n    A.RandomRain(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","6e9cc62f":"aug = A.Compose([\n    A.RandomFog(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","230247d8":"aug = A.Compose([\n    A.RandomSunFlare(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","3869fe46":"aug = A.Compose([\n    A.ISONoise(p=1)\n], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\naug_result = aug(image=image, bboxes=pascal_voc_boxes, labels=labels)\n\nshow_augmented_images(aug_result, train_df[train_df['image_id'] == 'b6ab77fd7'])","9c12c6b2":"i. RANDOM SIZED BBOX SAFE CROP augmentation","182edad9":"Understanding the csv file.","023abe9a":"Drop the unecessary columns.","77b8cdfd":"Apply a few basic augmentations.","ea5d7033":"Understand the basic image characteristics.","63b395f6":"Check if all the image dimensions are same.","1004beb9":"h. RANDOM BRIGHTNESS & CONTRAST augmentation","cf324eb8":"e. RESIZE augmentation","09bf6e98":"b. EQUALIZE augmentation","c44f053d":"k. RANDOM FOG augmentation","506aeffe":"The 'coco' format for the bounding boxes is (x_min, y_min, width, height) which has been originally provided in the dataframe. However, for my convenience, I have converted this to the 'pascal_voc' format i.e. (x_min, y_min, x_max, y_max). Note: all the functions can be modified using minor changes if you wish to use the 'coco' format.","3aa7a00e":"Also, if you liked the kernel or found it useful, please upvote it so that I will be motivated further to share my work with everyone. Cheers!","29138279":"Extracting bounding boxes data from the csv file. Function taken from Peter's [notebook](http:\/\/www.kaggle.com\/pestipeti\/global-wheat-detection-eda).","ad890e8d":"Load the train csv file.","1d26707a":"d. RANDOM CROP augmentation","20f492d3":"g. RANDOM AFFINE augmentation","fad33eae":"f. RANDOM GAMMA augmentation","c6038a24":"c. BLUR augmentation","e238baac":"Import dependencies.","aaf4ab2e":"a. CLAHE augmentation","e5f73bb1":"m. ISO NOISE augmentation","120a725e":"l. RANDOM SOLAR FLARE augmentation","3dfdef5b":"This is the first competition that I have begun my Kaggle journey with. Also, the first time that I am working on an object-detection problem statement. \nI present here a very basic EDA for the given wheat head dataset hoping that other beginners like me would find it easy to understand the data and get started with this intriguing competition.\nIn addition to the EDA, I have also added a few augmentations that I tried on the images in the second half of the kernel.","9c4aca89":"Display an image with and without the bounding boxes.","d6c6d471":"j. RANDOM RAIN augmentation","3104656c":"Save necessary paths to the data.","7c1c6e1b":"I would highly appreciate if you could let me know how I can improve my kernels with the objective of making them really simple and easy to understand."}}