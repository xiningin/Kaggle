{"cell_type":{"7c98bea2":"code","6ba0cfda":"code","3e5fa0f7":"code","4139ad38":"code","d38a9364":"code","c35f5d0b":"code","9b17b387":"code","8f9004a3":"code","b012ef6c":"code","f6814674":"code","fee4d088":"code","a9cebd15":"code","56bfcc52":"code","7173bc13":"markdown"},"source":{"7c98bea2":"!pip install ..\/input\/kaggle-efficientnet-repo\/efficientnet-1.0.0-py3-none-any.whl\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport argparse\nimport os\nimport skimage.io\nfrom scipy.ndimage import measurements\nimport os\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras import layers as L\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.utils import to_categorical\nimport gc\nimport albumentations\ngc.enable()","6ba0cfda":"sz = 256\nN = 48\n\ndef tile(img):\n    result = []\n    shape = img.shape\n    pad0,pad1 = (sz - shape[0]%sz)%sz, (sz - shape[1]%sz)%sz\n    img = np.pad(img,[[pad0\/\/2,pad0-pad0\/\/2],[pad1\/\/2,pad1-pad1\/\/2],[0,0]],\n                constant_values=255)\n    img = img.reshape(img.shape[0]\/\/sz,sz,img.shape[1]\/\/sz,sz,3)\n    img = img.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n    if len(img) < N:\n        img = np.pad(img,[[0,N-len(img)],[0,0],[0,0],[0,0]],constant_values=255)\n    idxs = np.argsort(img.reshape(img.shape[0],-1).sum(-1))[:42]\n    img = img[idxs]\n    return img","3e5fa0f7":"class ConvNet(tf.keras.Model):\n\n    def __init__(self, engine, input_shape, weights):\n        super(ConvNet, self).__init__()\n        \n        self.engine = engine(\n            include_top=False, input_shape=input_shape, weights=weights)\n        \n        \n        self.avg_pool2d = tf.keras.layers.GlobalAveragePooling2D()\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        self.dense_1 = tf.keras.layers.Dense(1024)\n        self.dense_2 = tf.keras.layers.Dense(1)\n\n    @tf.function\n    def call(self, inputs, **kwargs):\n        x = tf.reshape(inputs, (-1, IMG_SIZE, IMG_SIZE, 3))\n        x = self.engine(x)\n        shape = x.shape\n        x = tf.reshape(x, (-1, N_TILES, shape[1], shape[2], shape[3])) \n        x = tf.transpose(x, perm=[0, 2, 1, 3, 4])\n        x = tf.reshape(x, (-1, shape[1], N_TILES*shape[2], shape[3])) \n        x = self.avg_pool2d(x)\n        x = self.dropout(x, training=False)\n        x = self.dense_1(x)\n        x = tf.nn.relu(x)\n        return self.dense_2(x)","4139ad38":"is_ef = True\nbackbone_name = 'efficientnet-b0'\nN_TILES = 42\nIMG_SIZE = 256\n\n\nif backbone_name.startswith('efficientnet'):\n    model_fn = getattr(efn, f'EfficientNetB{backbone_name[-1]}')\n    \nmodel = ConvNet(engine=model_fn, input_shape=(IMG_SIZE, IMG_SIZE, 3), weights=None)\ndummy_data = tf.zeros((2 * N_TILES, IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32)\n_ = model(dummy_data)","d38a9364":"model.load_weights('..\/input\/tpu-training-tensorflow-iafoos-method-42x256x256x3\/efficientnet-b0.h5')","c35f5d0b":"TRAIN = '..\/input\/prostate-cancer-grade-assessment\/train_images\/'\nMASKS = '..\/input\/prostate-cancer-grade-assessment\/train_label_masks\/'\nBASE_PATH = '..\/input\/prostate-cancer-grade-assessment\/'\ntrain = pd.read_csv(BASE_PATH + \"train.csv\")\ntrain.head()","9b17b387":"sub = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/sample_submission.csv\")\nsub.head()","8f9004a3":"test = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/test.csv\")\ntest.head()","b012ef6c":"TEST = '..\/input\/prostate-cancer-grade-assessment\/test_images\/'","f6814674":"PRED_PATH = TEST\ndf = sub\nt_df = test","fee4d088":"transforms = albumentations.Compose([\n    albumentations.Transpose(p=0.5),\n    albumentations.VerticalFlip(p=0.5),\n    albumentations.HorizontalFlip(p=0.5),\n])","a9cebd15":"if os.path.exists(PRED_PATH):\n    predictions = []\n    for index, row in tqdm(df.iterrows(), total = df.shape[0]):\n        \n        \n        image_id = row['image_id']\n        \n        img_path = PRED_PATH + image_id + '.tiff' #BASE_PATH\n        \n        img = skimage.io.MultiImage(img_path)[1]\n        \n        patches = tile(img)\n        patches1 = patches.copy()\n        patches2 = patches.copy()\n        k = 0\n        while k < 42:\n            patches1[k, ] = transforms(image=patches1[k, ])['image']\n            patches2[k, ] = transforms(image=patches2[k, ])['image']\n            k += 1\n        \n        image = np.stack([patches, patches1, patches2])\n        image = image \/ 255.0\n        \n        pred = model.predict(image)\n        isup = np.round(np.mean(pred))\n        if isup < 0:\n            isup = 0\n        if isup > 5:\n            isup = 5   \n        predictions.append(int(isup))\n        del patches, img\n        gc.collect()\n","56bfcc52":"if os.path.exists(PRED_PATH):\n    sub['isup_grade'] = predictions\n    sub.to_csv(\"submission.csv\", index=False)\nelse:\n    sub.to_csv(\"submission.csv\", index=False)","7173bc13":"Training [kaggle kernal](https:\/\/www.kaggle.com\/micheomaano\/tpu-training-tensorflow-iafoos-method-42x256x256x3)  is available based on Tensorflow TPU to train 42x256x256x3 from intermediate layer in less than 3 hours. It can be improved alot.\nI hope you will like this work.\nHave Fun."}}