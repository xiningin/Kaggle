{"cell_type":{"d2d336b1":"code","98761960":"code","1aac4f58":"code","839faece":"code","656bb6c3":"code","c8e8e021":"code","c49bde92":"code","7c1f193a":"code","c38d7bea":"code","f0ab2635":"code","f5033130":"code","eb292bb8":"code","4245d2d6":"code","a4710ade":"code","d34298ec":"code","9ef0efa1":"code","25a30f54":"code","ea2b0667":"code","bc7e278d":"code","8bf91852":"code","127a1e3f":"code","0e77df07":"code","6680cfd9":"markdown","d9f3017e":"markdown","0970e914":"markdown","cc5c69e9":"markdown","40e2adc0":"markdown","2e1252b7":"markdown","9d897e12":"markdown","927e33e8":"markdown","88c0ff66":"markdown","d5bfdff1":"markdown","4ffb651c":"markdown","19640f52":"markdown","692db8c2":"markdown","1fcf4c9f":"markdown","580c96dc":"markdown","92205e26":"markdown","2da4daef":"markdown","9d6e2ec5":"markdown","77d8949c":"markdown","0a4944ff":"markdown","4f3ff44d":"markdown","14b948c4":"markdown","0b464c62":"markdown"},"source":{"d2d336b1":"# Load all the necessary packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix","98761960":"# Load data\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col = \"Id\")\ntrain = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col = \"Id\")\nsample = pd.read_csv(\"..\/input\/learn-together\/sample_submission.csv\", index_col = \"Id\")","1aac4f58":"# define X,Y for modelling\nY_train = train.Cover_Type\nX_train = train.drop(['Cover_Type'], axis=1)","839faece":"print(\"shape: \", train.shape, \"\\ncolumn names: \\n\", train.columns,\n     \"\\nNaN values: \", train.isnull().sum().sum(),\n\"\\nNA values:\", train.isna().sum().sum())","656bb6c3":"df_train = X_train.copy()\ntrain_cols = df_train.columns.tolist()\ndf_train[train_cols[10:]].sum()#\/df_train.shape[0]*100","c8e8e021":"train_sub = train.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 54]]\ntrain_sub.head()","c49bde92":"train_sub.describe()","7c1f193a":"corr = train.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c38d7bea":"train_no_soil = train.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 54]]\ncorr = train_no_soil.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)","f0ab2635":"train_soil = train.iloc[:, 14:]\ncorr = train_soil.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)","f5033130":"fig, ax =plt.subplots(3,3, figsize=(20,10))\nsns.boxplot(\"Cover_Type\", \"Elevation\", data=train, ax=ax[0][0])\nsns.boxplot(\"Cover_Type\", \"Aspect\", data=train, ax=ax[0][1])\nsns.boxplot(\"Cover_Type\", \"Slope\", data=train, ax=ax[0][2])\nsns.boxplot(\"Cover_Type\", \"Horizontal_Distance_To_Hydrology\", data=train, ax=ax[1][0])\nsns.boxplot(\"Cover_Type\", \"Vertical_Distance_To_Hydrology\", data=train, ax=ax[1][1])\nsns.boxplot(\"Cover_Type\", \"Horizontal_Distance_To_Roadways\", data=train, ax=ax[1][2])\nsns.boxplot(\"Cover_Type\", \"Hillshade_9am\", data=train, ax=ax[2][0])\nsns.boxplot(\"Cover_Type\", \"Hillshade_Noon\", data=train, ax=ax[2][1])\nsns.boxplot(\"Cover_Type\", \"Hillshade_3pm\", data=train, ax=ax[2][2])\nfig.show()","eb292bb8":"# simple random forest with CV\nclf = RandomForestClassifier()\nparam_grid = {\n                 'n_estimators': [300],\n                 'max_depth': [2, 4, 6, 8]\n             }\n\n\ngrid_clf = GridSearchCV(clf, param_grid, cv=10)\ngrid_clf.fit(X_train, Y_train)","4245d2d6":"grid_clf.cv_results_['mean_test_score'] ","a4710ade":"# simple random forest with CV\nclf2 = RandomForestClassifier()\nparam_grid2 = {\n                 'n_estimators': [300],\n                 'max_depth': [10, 12, 14, 16, 18, 20]\n             }\n\n\ngrid_clf2 = GridSearchCV(clf2, param_grid2, cv=10)\ngrid_clf2.fit(X_train, Y_train)","d34298ec":"grid_clf2.cv_results_['mean_test_score'] ","9ef0efa1":"# simple random forest with CV\nbase_model = RandomForestClassifier(n_estimators = 2000, max_depth = 18)\nbase_model.fit(X_train, Y_train)","25a30f54":"# Predict train labels matrix\nY_train_pred = base_model.predict(X_train)","ea2b0667":"cfm = confusion_matrix(Y_train, Y_train_pred)\nfig, ax = plot_confusion_matrix(conf_mat=cfm,\n                                colorbar=True,\n                                show_absolute=True,\n                                show_normed=True, figsize=(20,10))\nplt.show()","bc7e278d":"# Here I use a slightly modified function by Georg Fischer, which can be found in his kernel: \n# https:\/\/www.kaggle.com\/grfiv4\/plotting-feature-importances\ndef plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility\/extendibility\n              2. complicated models\/datasets\n          But for many situations Scikit-plot is the way to go\n          see https:\/\/scikit-plot.readthedocs.io\/en\/latest\/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n   ","8bf91852":"plot_feature_importances(base_model, X_train, Y_train, top_n= X_train.shape[1], \n                         title=base_model.__class__.__name__, figsize = (12, 12))","127a1e3f":"Y_test_pred = base_model.predict(test)","0e77df07":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test.index,\n                       'Cover_Type': Y_test_pred})\noutput.to_csv('submission.csv', index=False)","6680cfd9":"More complexity seems too work better. Lets do another one with higher max_depth.","d9f3017e":"We can clearly see large correlation values between several predictors. Three most obvious correlation structure are most obvious:\n* Correlation between the different hillshade values\n* Correlation between the vertical and horizontal distances to hydrology\n* The wilderness areas and Elevation\n\nAfter we checked this, lets see how the soil types correlate to our labels.","0970e914":"## 1.3 Plots","cc5c69e9":"# Hi there!\nThis code should give insight into the data set and set up a simple baseline model which will be usefull for further feature engineering and application of more complex models. ","40e2adc0":"# 2. Benchmark model\nTo assess how difficult the prediction problem is, lets first start with a super simple model without any kind of feature engineering beforhand.","2e1252b7":"## 1.2 Correlation structure","9d897e12":"The basline model is not only a basis for comparison for further feature engineering or model testing, but can be also used to assess feature importance. Having used a Random Forest classifier the feature importance can be easily estimated. This hopefully leads to a better understanding of the data and shows which features contain the most relevant information.","927e33e8":"The most misclassifications seem to appear in the classes 0, 1, 2.","88c0ff66":"Ok lets go with depth 18 for now. This is still just a benchmark model. I am only interested in having a simple baseline to compare further feature enigneering and more complex models.","d5bfdff1":"We have no missing values.","4ffb651c":"# 1. First Look at the data\nLets see what we are dealing with. Especially if we have missings, small group sizes or in general a problem with unbalanced data.\n## 1.1 sample sizes, missings, distributions","19640f52":"Here we see that especially the soil types 10, 22, 23, 29, 38, 39 and 40 show a high correlation to our labels. Maybe some sort of aggregation of the soil type variables could lead to a more usefull set of variables.","692db8c2":"### Soil_Type and Wilderness_Area\nSoil_Type and Wildernes_Area are already 1-hot encoded! Lets check how many there are in each Soil_Type and Wilderness_Area category. ","1fcf4c9f":"Some soil types are not existant, or show a very small frequency, which is also not very helpfull.","580c96dc":"# 3. Next steps\nThis data insights and the simple baseline model should give a first picture of what we are dealing with and should lead to ideas of how to enhance the model. When trying to work on a task like this I think its extreamly helpful to have something to test your modelling hypothesis on. For example, to assess wheter a newly created feature is really helpful or not. Or if a more complex model with complicated hyperparameter optimization is really producing a significant better prediction. \nTherefore my next steps will be:\n* Generate new features and test if they improve the prediction\n* Apply more complex models and model ensembles to the problem and check for each if it improves the prediction capability","92205e26":"## 2.3 submission of baseline model\nOk Lets do a simple submission with the baseline model!","2da4daef":"The feature importance plot shows nicely some insihts we already saw in the boxplots. Elevation and distances are more important than other features. Furthermore, wilderness_area4, hillshade, aspect and slope contain also usable information. The soil type seems to be hardly relevant for the most cases. Maybe a slimmer model, without any soil types could potentially work as good as the full one. Furthermore, one could think about aggregating the soild types to a more useful set of variables.","9d6e2ec5":"### Variable distribution\nNow lets check the rest of the columns by looking at the head and summary table.","77d8949c":"It seems that single varibles are not explaining much of the differences in cover type. Mainly Elevation & Horizontal_Distance_To_Roadways seem to be most important by themselves. The others might only be interesting in combination with other features.","0a4944ff":"## 2.2 Feature Importance","4f3ff44d":"It is a bit crowded and hard to see the details. One think that is immediatly visible is that the soil type shows hardly any correlation with all the other features. Lets leave them out and look at the rest.","14b948c4":"### Size & Missings","0b464c62":"## 2.1 Choose Parameter and train model"}}