{"cell_type":{"de10220e":"code","1e0c199f":"code","58a45d84":"code","bf02b27b":"code","e5d2d554":"code","87158e85":"code","a6f268e5":"code","69c9b032":"code","71936aca":"code","f66dd952":"code","092b5eb4":"code","8c2f6e44":"code","6b8b8c52":"code","43c25ec3":"markdown","bb47aae8":"markdown","207459c2":"markdown","1501df51":"markdown","bb5a368f":"markdown","8ae55322":"markdown","137b1c26":"markdown"},"source":{"de10220e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e0c199f":"x = np.loadtxt('\/kaggle\/input\/qzfinal\/x.csv', delimiter=\",\")\ny = np.loadtxt('\/kaggle\/input\/qzfinal\/y.csv', delimiter=\",\")\n\nx_eval = np.loadtxt('\/kaggle\/input\/qzfinal\/x_eval.csv', delimiter=\",\")\ny_eval = np.loadtxt('\/kaggle\/input\/qzfinal\/y_eval.csv', delimiter=\",\")","58a45d84":"y.shape","bf02b27b":"model = tf.keras.Sequential()\n\n\n\nmodel.add(tf.keras.layers.Dense(128, input_dim=40, activation=\"relu\",kernel_regularizer ='l2'))\n\n\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\",kernel_regularizer ='l2'))\n\n\n\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\",kernel_regularizer ='l2'))\n\n\n\n\nmodel.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n\nmodel.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=[\"accuracy\"])","e5d2d554":"history = model.fit(x, y, batch_size=1000,epochs=1000, shuffle=True)","87158e85":"model.summary()","a6f268e5":"test_loss, test_acc = model.evaluate(x_eval,y_eval)\nprint(test_loss, test_acc)","69c9b032":"def accuracy(inputs, targets):\n    model_pred = model.predict(inputs)\n    preds = np.array(model_pred)\n    if len(preds.shape) > 1:\n        preds = np.array(model_pred).argmax(-1)\n    return sum(preds==targets)\/len(targets)","71936aca":"accuracy(x_eval,y_eval)","f66dd952":"def create_model():\n    model = tf.keras.Sequential()\n\n    model.add(tf.keras.layers.Dense(16, input_dim=40, activation=\"relu\",kernel_regularizer ='l2'))\n\n    model.add(tf.keras.layers.Dense(16, activation=\"relu\",kernel_regularizer ='l2'))\n    model.add(tf.keras.layers.Dense(16, activation=\"relu\",kernel_regularizer ='l2'))\n    model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n\n    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=[\"acc\",\"mse\"])\n    return model","092b5eb4":"from sklearn.model_selection import GridSearchCV\nfrom keras.wrappers import scikit_learn","8c2f6e44":"model = scikit_learn.KerasClassifier(build_fn=create_model, verbose=0)\n\n# \u8bbe\u7f6e\u53c2\u6570\u5019\u9009\u503c\nbatch_size = [500,1000,3000,5000,10000]\nepochs = [500,1000,5000]\n\n# \u521b\u5efaGridSearchCV\uff0c\u5e76\u8bad\u7ec3\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid,scoring='accuracy', n_jobs=1)\ngrid_result = grid.fit(x, y)","6b8b8c52":"print('Best: {} using {}'.format(grid_result.best_score_, grid_result.best_params_))","43c25ec3":"plt.plot(hist.history['acc'], label = 'acc')\nplt.plot(hist.history['val_acc'], label='val acc')\nplt.title(\"acc vs Val_acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"acc\")\nplt.legend()\nplt.show()","bb47aae8":"# Hyperparameters Tuning","207459c2":"model2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(512, activation='tanh'),\n    tf.keras.layers.Dense(512\/\/2, activation='tanh'),\n    tf.keras.layers.Dense(512\/\/4, activation='tanh'),\n    tf.keras.layers.Dense(512\/\/8, activation='tanh'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\n\nmodel2.compile(optimizer='sgd',loss='mean_squared_error', metrics=['acc', 'mse'])\n\nhist = model2.fit(x, y, epochs=350, batch_size=128, validation_data=(x_eval,y_eval))\n\nprint(model2.summary())","1501df51":"## Batch_Size & Epochs","bb5a368f":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.plot(hist.history['loss'], label = 'loss')\nplt.plot(hist.history['val_loss'], label='val loss')\nplt.title(\"Loss vs Val_Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","8ae55322":"loss1, acc1, mse1 = model2.evaluate(x_eval,y_eval)\nprint(f\"Loss is {loss1},\\nAccuracy is {acc1*100},\\nMSE is {mse1}\")","137b1c26":"# Create a Base Model"}}