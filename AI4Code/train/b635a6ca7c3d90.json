{"cell_type":{"3473fec5":"code","0eaa1a4f":"code","ac91bdcd":"code","fda6c11f":"code","b40a8d30":"code","41503a1a":"code","92265b8b":"code","d3e97a32":"code","f0ca255b":"code","965e1ffe":"code","fe108a08":"code","2c95d5c8":"code","bc9fa50f":"code","29870b28":"code","65bf5797":"code","2dc5804b":"code","bfcec01c":"code","a2c1de40":"code","cdd9a8fe":"code","bf8807b2":"code","1c89aa0e":"code","b3bc8080":"code","447243c7":"code","fcd7f2ce":"code","d50f4d1b":"code","006f0ae5":"code","75f78598":"code","3118234e":"code","1c397b41":"code","5af6f01a":"code","d430fc19":"code","8d033bcd":"code","90abb571":"code","67e443a8":"code","f943fd63":"code","3dc0861c":"code","a8ae15b9":"code","123a0be8":"markdown","d3e24b84":"markdown","0ffd9659":"markdown","f6345dc7":"markdown","cd3cf066":"markdown","2a5bc1f3":"markdown","3e28a130":"markdown","f6d527a2":"markdown","446a0bc5":"markdown","481ec7c5":"markdown","792f3456":"markdown","02a4a497":"markdown","6263b298":"markdown","f1f7473f":"markdown","f7c990e1":"markdown","b9d04542":"markdown","6c78b64b":"markdown","10a5aca0":"markdown","3423727d":"markdown","fcafcc6b":"markdown","a1a76f5d":"markdown","e9f52745":"markdown","05903048":"markdown"},"source":{"3473fec5":"import numpy\nimport pandas as pd\nimport torch\nimport torch.nn as nn","0eaa1a4f":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","ac91bdcd":"train_data.head()","fda6c11f":"train_data.shape","b40a8d30":"train_data.iloc[0:4, [1,2,3,-1,-2]]","41503a1a":"len(test_data.columns), len(train_data.columns)","92265b8b":"all_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:,1:]))\n\n# row wise concat","d3e97a32":"all_features.head()","f0ca255b":"all_features.dtypes","965e1ffe":"all_features.dtypes[all_features.dtypes == 'int64']","fe108a08":"numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\nall_features[numeric_features]","2c95d5c8":"# numeric_features = all_features.dtypes[all_features.dtypes=='int64'].index","bc9fa50f":"def normalise(x):\n    return ((x - x.mean())\/x.std())","29870b28":"# If test data were inaccessible, mean and standard deviation could be\n# calculated from training data\n\nall_features[numeric_features] = all_features[numeric_features].apply(\n    lambda x: (x - x.mean()) \/ (x.std()))\n","65bf5797":"all_features[numeric_features]\n","2dc5804b":"all_features[numeric_features] = all_features[numeric_features].fillna(0)","bfcec01c":"all_features = pd.get_dummies(all_features, dummy_na=True)\n\nall_features\n","a2c1de40":"n_train = len(train_data)\n\ntrain_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n\ntrain_labels = torch.tensor(train_data.iloc[:,-1].values, dtype=torch.float32)\n","cdd9a8fe":"loss = nn.MSELoss()\n\nin_features = train_features.shape[1]\nout_features = 1\n\ndef get_net():\n#     net = nn.Sequential(nn.Linear(in_features,256), nn.ReLU(), nn.Linear(256,out_features))\n    net = nn.Sequential(nn.Linear(in_features, out_features))\n    return net","bf8807b2":"def log_rmse(net, features, labels):\n    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n    return rmse.item()","1c89aa0e":"def load_array(data_array, batch_size):\n    in_dataset = torch.utils.data.TensorDataset(*data_array)\n    in_dataloader = torch.utils.data.DataLoader(in_dataset, shuffle=True, batch_size=batch_size)\n    return in_dataloader","b3bc8080":"def init_weights(m):\n    if type(m)==nn.Linear or type(m)==nn.Conv2d:\n        nn.init.xavier_uniform_(m.weight)","447243c7":"def accuracy(y_hat, y):\n    return (torch.argmax(y_hat, dim=1)==y).sum().float().mean()","fcd7f2ce":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","d50f4d1b":"net = get_net()\nnet","006f0ae5":"test_labels = None\nbatch_size= 64\nlearning_rate = 0.03\nlr = learning_rate\nweight_decay = 0\nnum_epochs = 100","75f78598":"train_dataloader = load_array((train_features, train_labels), batch_size)\nwith torch.no_grad():\n    for X, y in train_dataloader:\n        print(X.shape)\n        print(X, \"\\n\")\n        print(len(y))\n        print(y, \"\\n\")\n        y_hat = net(X)\n        print(y_hat)\n        \n        print(accuracy(y_hat, y))\n        #for x in X:  \n        #    y_hat = net(x)\n        #    print(x)\n        #    print(y_hat)\n        #    print(y_hat.shape)\n        break","3118234e":"def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay,batch_size):\n    train_ls, test_ls = [], []\n    \n    net = net.to(device)\n    net.apply(init_weights)\n    \n    train_acc = []\n    \n    \n    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=weight_decay)\n    \n    train_dataloader = load_array((train_features, train_labels),batch_size)\n    \n    for epoch in range(num_epochs):\n        curr_acc = 0\n        numer = 0\n        for X, y in train_dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            \n            y_hat = net(X)\n            \n            l = loss(y_hat, y.unsqueeze(1))\n            curr_acc += accuracy(y_hat, y)\n            \n#             print(y)\n#             print(y_hat)\n            \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            numer += len(y)\n            \n        train_ls.append(log_rmse(net, train_features, train_labels))\n        train_acc.append(curr_acc\/ numer)\n        if test_labels is not None:\n            test_ls.append(log_rmse(net, test_features, test_labels))\n        print(f'for epoch {epoch} rmse: {train_ls[-1]}, accuracy : {train_acc[-1]}')\n    \n    return train_ls, test_ls\n            ","1c397b41":"net(train_features).detach()","5af6f01a":"#train_ls, test_ls =  train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay,batch_size)","d430fc19":"!pip install -U d2l\nimport d2l\nfrom d2l import torch\nfrom d2l.torch import *","8d033bcd":"def train_and_pred(train_features, test_feature, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    train_ls, _ = train(net, train_features, train_labels, None, None,\n    num_epochs, lr, weight_decay, batch_size)\n    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n    ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n    print(f'train log rmse {float(train_ls[-1]):f}')\n    # Apply the network to the test set\n    preds = net(test_features).detach().numpy()\n    # Reformat it to export to Kaggle\n    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)","90abb571":"train_and_pred(train_features, test_features, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size)\n","67e443a8":"def get_net():\n    net = nn.Sequential(nn.Linear(in_features, 256), nn.ReLU(), nn.Linear(256,out_features))\n    return net\n\nnet = get_net()\nnet","f943fd63":"train_and_pred(train_features, test_features, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size)","3dc0861c":"#would creating a more complex model help?\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features, 256), nn.ReLU(),nn.Linear(256,64), nn.ReLU(), nn.Linear(64,out_features))\n    return net\n\nnet = get_net()\nnet","a8ae15b9":"train_and_pred(train_features, test_features, train_labels, test_data,\nnum_epochs, lr, weight_decay, batch_size)","123a0be8":"notice that now there are 331 columns instead of earlier 25. Data prerpocessing is now over.","d3e24b84":"Now this takes you within earshot of 1k in leaderboard.","0ffd9659":"With house prices, as with stock prices, we care about relative quantities more than absolute quantities. Thus [we tend to care more about the relative error $\\frac{y - \\hat{y}}{y}$] than about the absolute error $y - \\hat{y}$. For instance, if our prediction is off by USD 100,000 when estimating the price of a house in Rural Ohio, where the value of a typical house is 125,000 USD, then we are probably doing a horrible job. On the other hand, if we err by this amount in Los Altos Hills, California, this might represent a stunningly accurate prediction (there, the median house price exceeds 4 million USD).\n\n(One way to address this problem is to measure the discrepancy in the logarithm of the price estimates.) In fact, this is also the official error measure used by the competition to evaluate the quality of submissions. After all, a small value $\\delta$ for $|\\log y - \\log \\hat{y}| \\leq \\delta$ translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. This leads to the following root-mean-squared-error between the logarithm of the predicted price and the logarithm of the label price:\n\n$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}.$$","f6345dc7":"# LETS LOAD THE DATA","cd3cf066":"Looking inside train dataloader. ","2a5bc1f3":"notice the difference in all_features before and after normalisation in `all_features[numeric_features]`","3e28a130":"train data has one column more for labels","f6d527a2":"Since its the beginning we will go for a proof of concept and try and train a simple sequential model.\nLater we will change the definition of `get_net()`. To get started we train a linear model with squared loss. Not surprisingly, our linear model will not lead to a competition-winning submission but it provides a sanity check to see whether there is meaningful information in the data. If we cannot do better than random guessing here, then there might be a good chance that we have a data processing bug. And if things work, the linear model will serve as a baseline giving us some intuition about how close the simple model gets to the best reported models, giving us a sense of how much gain we should expect from fancier models.","446a0bc5":"Adapting from my own notebook here : https:\/\/www.kaggle.com\/fanbyprinciple\/beginner-pytorch-notebook-on-housing-dataset\nI feel I have forgotten stuff. This notebook was originally in part of the D2l book. https:\/\/d2l.ai\/. Much recommended. Explanations of stuff taken from this book.","481ec7c5":"How to look at top 4 rows for columns","792f3456":"# DATA PREPROCESSING\n\nsince we are going to use both of train data and test data through the preprocessing we first concatenate it. We can see that in each example, (the first feature is the ID.) This helps the model identify each training example. While this is convenient, it does not carry any information for prediction purposes. Hence, (we remove it from the dataset) before feeding the data into the model.","02a4a497":"implementation of densenet: mentioned in this model\n\nhttps:\/\/arxiv.org\/pdf\/2108.00864.pdf","6263b298":"### ONE HOT ENCODING\n\ndiscrete features now need to be one hot encoded. The discrete columns are devided based on value, and 0 or 1 put in columns where the value is true, for example if `SaleType` had two discrete values other or WD, there would be two columns made `SaleType_other` and `Saletype_WD`","f1f7473f":"### 1. NORMALISATION\n\nNow lets look for numerical features, since we would want to normalise numerical features first.","f7c990e1":"### HANDLING MISSING ENTRIES\n\none of first jobs we have to do handlind datasets is to handle the missing values","b9d04542":"We use pandas to load the two csv files containing training and test data respectively.","6c78b64b":"Notice the .index, it only returns the column name","10a5aca0":"rmse of 4.08202 takes us to 4k rank in leaderboard. Lets try and use a different get_net function. Notice that accuracy is 0.0 because y_hat predicted is never actually equal to the test value y.","3423727d":"Applying normalisation. \n\nAs stated above, we have a wide variety of data types. We will need to preprocess the data before we can start modeling. Let us start with the numerical features. First, we apply a heuristic, [replacing all missing values by the corresponding feature's mean.] Then, to put all features on a common scale, we (standardize the data by rescaling features to zero mean and unit variance):\n\n$$x \\leftarrow \\frac{x - \\mu}{\\sigma},$$\nwhere $\\mu$ and $\\sigma$ denote mean and standard deviation, respectively. To verify that this indeed transforms our feature (variable) such that it has zero mean and unit variance, note that $E[\\frac{x-\\mu}{\\sigma}] = \\frac{\\mu - \\mu}{\\sigma} = 0$ and that $E[(x-\\mu)^2] = (\\sigma^2 + \\mu^2) - 2\\mu^2+\\mu^2 = \\sigma^2$. Intuitively, we standardize the data for two reasons. First, it proves convenient for optimization. Second, because we do not know a priori which features will be relevant, we do not want to penalize coefficients assigned to one feature more than on any other.","fcafcc6b":"The training dataset includes 1460 examples, 80 features, and 1 label, while the test data contains 1459 examples and 80 features.","a1a76f5d":"### BIFURCATING TEST AND TRAIN DATA\n\nBifurcating data back to train and test data. And converting them into float32 torch tensor. Via the values attribute, we can [extract the NumPy format from the pandas format and convert it into the tensor] representation for training.","e9f52745":"We can use '==' on dataframes for comparison! here is how to find only numerical features.","05903048":"# IMPROVING OUR POSITION ON LEADERBOARD"}}