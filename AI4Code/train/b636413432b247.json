{"cell_type":{"e8fe4813":"code","b7c4079d":"code","c5cfef92":"code","ed6d9662":"code","a1a2eedf":"code","9a902a83":"code","7d575914":"code","37869af7":"code","b05fa69c":"code","1e6cba1f":"code","ecc09644":"code","555ca81f":"code","8a467119":"code","db690fd1":"code","8aeddaa9":"code","d4c70d85":"code","8ec9293b":"code","1d5ad7db":"code","49ff2dee":"code","31c2d19e":"code","62b9f7f9":"markdown","02302401":"markdown","363bed90":"markdown","48210d10":"markdown","dff7b112":"markdown","6cc9dc21":"markdown","ac5a54e8":"markdown","15807822":"markdown","4b18cfcc":"markdown"},"source":{"e8fe4813":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot  as plt\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Sequential, load_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\ntf.__version__\n","b7c4079d":"train = pd.read_csv(r'\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain.shape, test.shape","c5cfef92":"X_train = x_train = train.drop(['label'],1)\nY_train = train['label']\nx_test = test","ed6d9662":"X_train = np.asarray(X_train)\nx_test = np.asarray(x_test)\ng = sns.countplot(Y_train)\nx_train.shape","a1a2eedf":"X_train = X_train.astype('float32')\nx_test = x_test.astype('float32')\n\nX_train = X_train\/255\nx_test - x_test\/255","9a902a83":"Y_train.shape","7d575914":"# one hot encoding y data\nY_train= tf.keras.utils.to_categorical(Y_train, 10)\nY_train.shape","37869af7":"X_train = X_train.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1,28 ,28, 1)\nX_train.shape","b05fa69c":"x_train, val_x, y_train, val_y = train_test_split(X_train, Y_train, test_size=0.20)","1e6cba1f":"val_x.shape","ecc09644":"es = EarlyStopping(monitor='loss', patience=12)\nfilepath=\"\/kaggle\/working\/bestmodel.h5\"\nmd = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')","555ca81f":"# defininig ImageDataGeneratore to increase data\ndatagen = ImageDataGenerator(zoom_range = 0.1,\n                            height_shift_range = 0.1,\n                            width_shift_range = 0.1,\n                            rotation_range = 10)","8a467119":"# Important Variables\nepochs = 30\nnum_classes = 10\nbatch_size = 30\ninput_shape = (28, 28, 1)\nadam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)","db690fd1":"model = Sequential()\n\n# Filter 1\nmodel.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation= tf.nn.relu)) \nmodel.add(Conv2D(32, (3, 3), padding='same', activation= tf.nn.relu))    \n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\n# Filter 2\nmodel.add(Conv2D(64, (3, 3), padding='same', activation= tf.nn.relu))                          \n#model.add(Conv2D(64, (3, 3), padding='same', activation= tf.nn.relu))    \n#model.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\n\n'''# Filter 3 \nmodel.add(Conv2D(64, (3, 3), padding='same', activation= tf.nn.relu))                         \nmodel.add(Conv2D(64, (3, 3), padding='same', activation= tf.nn.relu))                        \nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())'''\n\n\n\n\n\n\n\n\n# Dense Filter 5\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation=tf.nn.relu))                                                \nmodel.add(Dropout(0.25))\n#model.add(BatchNormalization())\n\n\n# Dense Filter 1\n\n'''model.add(Dense(512, activation=tf.nn.relu))                                                \nmodel.add(Dropout(0.25))'''\n#model.add(BatchNormalization())\n\n# Dense Filter 2       \nmodel.add(Dense(512, activation=tf.nn.relu))                                                                 \nmodel.add(Dropout(0.25))\n#model.add(BatchNormalization())\n\n# Dense Filter 3\nmodel.add(Dense(256, activation=tf.nn.relu))                                                   \nmodel.add(Dropout(0.5))\n#model.add(BatchNormalization())\n\n# Dense Filter 4\nmodel.add(Dense(10, activation= tf.nn.softmax))                                                  \n\n# Model Compile\nmodel.compile(optimizer= adam, loss= tf.keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])\n\n# Model Summery\nmodel.summary()\n","8aeddaa9":"History = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n          #steps_per_epoch=840,\n          epochs = epochs,\n          validation_data = (val_x, val_y),\n          callbacks = [es,md],\n          shuffle= True\n        )\n       ","d4c70d85":"fig, ax = plt.subplots(2,1)\nax[0].plot(History.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(History.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=False)\n\nax[1].plot(History.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(History.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=False)","8ec9293b":"model1 = load_model(\"\/kaggle\/working\/bestmodel.h5\")","1d5ad7db":"model1.summary()","49ff2dee":"pred = model1.predict(x_test)\npred_class = model1.predict_classes(x_test)","31c2d19e":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(pred_class)+1)),\n                         \"Label\": pred_class})\nsubmissions.to_csv(\"submissions.csv\", index=False, header=True)\nsubmissions","62b9f7f9":"## **Reshaping Data**","02302401":"\n**Loading data set to Notebook**","363bed90":"## **Prepare the training data**","48210d10":"\n**Normalizing Data**","dff7b112":"**Importing important libraries**","6cc9dc21":"## ** Trainning Model**\n**Training Model to get 99% accuracy**","ac5a54e8":"**Building model**","15807822":"# **Training a MNIST model**","4b18cfcc":"**accuracy Vs val_accuracy & loss vs val_loss Plot **"}}