{"cell_type":{"e70c8f52":"code","494ded47":"code","fe5bcae9":"code","9c5d6575":"code","a9aff20a":"code","9d4d5edc":"code","15243d0f":"code","ec171520":"code","3df6f0e4":"code","2fca1cce":"code","a385cc9b":"code","e60ab3b7":"code","e2f39e3e":"code","da25002e":"code","e83c7330":"code","317832a0":"code","714908d5":"code","763a219d":"code","cc2c6f1a":"code","e5b4c22e":"code","0b5a705f":"code","27ace177":"code","8be763e6":"code","a97065fb":"code","b7dbc791":"code","6ca57881":"code","a207ed01":"code","7abc9378":"code","7feafe04":"code","d88d0804":"code","417821c0":"code","5b0d9722":"code","d95c34ae":"code","5ad4e86e":"code","49879b57":"code","f524a86f":"code","b28acf63":"code","9e5e7558":"code","65ce7340":"code","76fd3dae":"code","0215952f":"code","ca5e28aa":"code","4799894f":"code","39ecb50f":"code","291bc1e7":"code","bae9f3ee":"code","dc06c7d3":"code","c8b626c9":"code","bf3b6d4b":"code","e9f422b3":"code","5016c1d1":"code","8edd3c27":"code","59d98a10":"code","9cb22148":"code","c788f31a":"code","0dec7583":"code","45befca4":"code","100d21f1":"code","97f25038":"code","65eecc42":"code","1b1989c9":"code","21b82302":"code","46f63117":"code","80d802d4":"code","b0f00348":"code","c0f1b6f7":"code","30cdcf3c":"code","55f53dfb":"code","5a420a90":"code","20527e41":"code","c712428d":"code","81ad1b55":"code","d4b5cbb2":"code","34623161":"code","979e25c5":"code","5c7f5d95":"code","eb1e21cd":"code","002c12c8":"code","a58c7923":"code","04503f84":"code","98581202":"code","77c82021":"code","2c3f8e3e":"code","e4fe6cff":"code","08edb999":"code","215fc7fc":"code","a3349ee4":"code","cd323ac0":"code","e8b8c6c8":"code","b1e019d5":"markdown","183c4405":"markdown","5d045c9e":"markdown","245f0afc":"markdown","2aa311ee":"markdown","1f509adb":"markdown","58393308":"markdown","3d012296":"markdown","7960c59c":"markdown","ff299163":"markdown","7e19cc24":"markdown","001dbe8a":"markdown","067f6887":"markdown","5aacac20":"markdown","89479c37":"markdown","fb3ec74c":"markdown","b9a52f19":"markdown","8df984d4":"markdown","74607c6d":"markdown","28506b87":"markdown","bb8f8f17":"markdown","db0db2e4":"markdown","515bf5ee":"markdown","2b68c5bb":"markdown","63673e13":"markdown","ab49c45b":"markdown","c7df7ba0":"markdown","959eabfa":"markdown","45dc24f7":"markdown","f5fb8b41":"markdown","3310c23b":"markdown","9290d659":"markdown","042da062":"markdown","95b054ef":"markdown","7ed01610":"markdown","6c00678c":"markdown","54818d2d":"markdown","ce3f7520":"markdown","5a25c234":"markdown","ef1f0931":"markdown","d4f93b59":"markdown","123088bf":"markdown","0280f640":"markdown","ef54c17b":"markdown","68b1285e":"markdown","680091f6":"markdown","f3b87944":"markdown","486e1741":"markdown","778259da":"markdown","756590f3":"markdown","65370bba":"markdown","a0e7fc20":"markdown","26ec4602":"markdown","b05fcaca":"markdown","a49d9288":"markdown","50c0f86d":"markdown","7fc195d1":"markdown","395b5516":"markdown","db234c3a":"markdown","3c5bf334":"markdown","ea8cb640":"markdown","590f12f1":"markdown","5fd506ad":"markdown","5e848f72":"markdown","7b8e4d20":"markdown","a1807f67":"markdown","1d95ff23":"markdown","0dce60ad":"markdown","4d134567":"markdown","1f383252":"markdown","53820d09":"markdown","7bf056ce":"markdown"},"source":{"e70c8f52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","494ded47":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing\n\n# data visualization for EDA\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\nimport string\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","fe5bcae9":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nprint(df_train.shape)\nprint(df_test.shape)","9c5d6575":"# concatenate data of training and test set\ndef concat_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\n\ndf_all = concat_df(df_train, df_test)","a9aff20a":"# divided data of training and test set\ndef divide_df(all_data):\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis = 1)","9d4d5edc":"df_all.sample(10)  # generate a sample random row and column","15243d0f":"df_train.info()","ec171520":"df_train.describe()","3df6f0e4":"f, ax = plt.subplots(1, 2, figsize=(18,8))  # 1 x 2 subplots\n\nsurvived = df_train['Survived'].value_counts()\nsurvived.plot.pie(explode=[0, 0.1], \n                  autopct='%1.1f%%', \n                  ax=ax[0], \n                  shadow=True)\n\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=df_train, ax=ax[1])\nax[1].set_title('Survived')\n\nplt.show()","2fca1cce":"missing_values_train = df_train.isnull().sum()  # number of missing values\n\nmissing_values_train.sort_values(ascending=False)","a385cc9b":"# finding percentage of missing values\n\nmissing_percent = 100 * missing_values_train \/ len(df_train)\nmissing_percent_train = (round(missing_percent, 1))  # rounding off\n\nmissing_percent_train.sort_values(ascending=False)","e60ab3b7":"# creating dataframe of missing values\n\ndf_missing_train = pd.concat([missing_values_train, \n                              missing_percent_train], axis=1, keys=['Total', '%'])\ndf_missing_train.sort_values(by='%', ascending=False)","e2f39e3e":"missing_values_test = df_test.isnull().sum()  # number of missing values\n\nmissing_values_test.sort_values(ascending=False)","da25002e":"# finding percentage of missing values\n\nmissing_percent = 100 * missing_values_test \/ len(df_test)\nmissing_percent_test = (round(missing_percent, 1))  # rounding off\n\nmissing_percent_test.sort_values(ascending=False)","e83c7330":"# creating dataframe of missing values\n\ndf_missing_test = pd.concat([missing_values_test, \n                             missing_percent_test], axis=1, keys=['Total', '%'])\ndf_missing_test.sort_values(by='%', ascending=False)","317832a0":"f, ax = plt.subplots(figsize=(18,8))\n\nsns.violinplot(\"Pclass\", \"Age\", \n               hue=\"Survived\", \n               data=df_train, \n               split=True, \n               ax=ax)\n\nax.set_title('Pclass and Age vs Survived')\nax.set_yticks(range(0, 110, 10))\n\nplt.show()","714908d5":"df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\",\n                            \"level_1\": \"Feature 2\",\n                            0: \"Correlation Coefficient\"},\n                            inplace = True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Pclass']","763a219d":"f, ax = plt.subplots(figsize=(18,8))\n\nsns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=df_train, split=True, ax=ax)\nax.set_title('Sex and Age vs Survived')\nax.set_yticks(range(0, 110, 10))\n\nplt.show()","cc2c6f1a":"age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1,4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, \n                sex, age_by_pclass_sex[sex][pclass].astype(int)))\n        \ndf_all['Age'] = df_all.groupby(['Sex','Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","e5b4c22e":"sns.factorplot('Embarked', 'Survived', data=df_train)\nfig = plt.gcf()  # used to get current figure\nfig.set_size_inches(5,3)\nplt.show()","0b5a705f":"df_all[df_all['Embarked'].isnull()]","27ace177":"df_all['Embarked'] = df_all['Embarked'].fillna('S')","8be763e6":"FaceGrid = sns.FacetGrid(df_train, row='Embarked', size=3.5, aspect=1.6)\nFaceGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex')\nFaceGrid.add_legend()","a97065fb":"df_all[df_all['Fare'].isnull()]","b7dbc791":"med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp'])['Fare'].median()[3][0][0]\nmed_fare  # median of Fare satisfying condition([3][0][0] -- 3=Pclass, 0=Parch, SibSp)","6ca57881":"df_all['Fare'] = df_all['Fare'].fillna(med_fare)","a207ed01":"sns.barplot(x='Pclass', y='Survived', hue='Sex', data=df_train)","7abc9378":"grid = sns.FacetGrid(df_train, col='Survived', \n                    row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=0.5, bins=20)\ngrid.add_legend();","7feafe04":"data1 = df_train.copy()  # shallow  copy\ndata1['family_size'] = data1['SibSp'] + data1['Parch'] + 1 # 1 if a person is alone\ndata1['family_size'].value_counts().sort_values(ascending=False)","d88d0804":"axes = sns.factorplot('family_size', 'Survived', data=data1, aspect=2.5)","417821c0":"df_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', \n                                                                        'SibSp', 'Parch', 'Fare', 'Embarked', \n                                                                        'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'})\n# doing transpose for accessibility\ndf_all_decks = df_all_decks.transpose()\n\ndf_all_decks","5b0d9722":"def get_pclass_dist(df):\n    \n    # create a dictionary of every deck for 'passenger count' in every class\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, \n                   'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    \n    # extract deck column from df_all_decks\n    decks = df.columns.levels[0]\n    \n    # create a new dataframe with '0' if empty in respective 'Pclass' of df_all_decks\n    for deck in decks:\n        for pclass in range(1,4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count\n            except KeyError:\n                deck_counts[deck][pclass] = 0\n    \n    df_decks = pd.DataFrame(deck_counts)\n    \n    \n    \n    # create a dictionary of every deck for 'percentage count' of passangers in every class \n    deck_percentages = {}\n    \n    for col in df_decks.columns:\n        deck_percentages[col] = [(count\/df_decks[col].sum()) * 100 \n                                 for count in df_decks[col]]\n    \n    return deck_counts, df_decks, deck_percentages\n\n\n\nall_deck_count, df_decks_return, all_deck_percent = get_pclass_dist(df_all_decks)","d95c34ae":"print(df_decks_return)  # returns a dataframe of passenger count\nall_deck_count  # returns a dictionary of passenger count in every class","5ad4e86e":"all_deck_percent # returns a percentage count of passengers","49879b57":"def display_pclass_dist(percentages):\n    \n    # converting dictionary into dataframe and then transpose\n    df_percentages = pd.DataFrame(percentages).transpose()\n    \n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))\n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20,10))\n    \n    plt.bar(bar_count, pclass1, color='brown',\n            edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='teal', \n            edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1+pclass2, color='peru', \n            edgecolor='white', width=bar_width, label='Passenger Class 3')\n    \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    \n    plt.xticks(bar_count, deck_names)\n    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='best', bbox_to_anchor=(1,1), prop={'size':15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)\n    \n    plt.show()\n    \ndisplay_pclass_dist(all_deck_percent)","f524a86f":"idx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'    ","b28acf63":"df_all_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', \n                                                                             'Fare', 'Embarked', 'Pclass', \n                                                                             'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()","9e5e7558":"def get_survived_dist(df):\n    \n    # create a dictionary for 'survival count' in every deck\n    survival_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, \n                       'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    \n    # extract deck column from df_all_decks\n    decks = df.columns.levels[0]\n    \n    \n    for deck in decks:\n        for survive in range(0,2):\n            survival_counts[deck][survive] = df[deck][survive][0]\n            \n    df_survival = pd.DataFrame(survival_counts)\n    \n    # create a dictionary of 'survival count' in every class\n    survival_percentages = {}\n    \n    for col in df_survival.columns:\n        survival_percentages[col] = [(count\/df_survival[col].sum()) * 100 \n                                     for count in df_survival[col]]\n    \n    return survival_counts, survival_percentages\n\nall_survival_count, all_survival_percentage = get_survived_dist(df_all_survived)","65ce7340":"all_survival_count","76fd3dae":"all_survival_percentage","0215952f":"def display_survival_dist(percentages):\n    \n    df_survival_percentages = pd.DataFrame(percentages).transpose()\n    \n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))\n    bar_width = 0.85\n    \n    not_survived = df_survival_percentages[0]\n    survived = df_survival_percentages[1]\n    \n    plt.figure(figsize=(20,10))\n    plt.bar(bar_count, not_survived, color='grey', \n            edgecolor='white', width=bar_width, label='Not Survived')\n    plt.bar(bar_count, survived, color='deepskyblue', bottom=not_survived, \n            edgecolor='white', width=bar_width, label=\"Survived\")\n    \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)\n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1,1), prop={'size':15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n    \ndisplay_survival_dist(all_survival_percentage)","ca5e28aa":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","4799894f":"df_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_all.head()","39ecb50f":"df_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    print(df.isnull().sum())\n    print(\"\\n\")","291bc1e7":"continuous_features = ['Age', 'Fare']\nsurvived = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(continuous_features):\n    \n    # distribution of survival in feature\n    sns.distplot(df_train[~survived][feature], label = 'Not Survived', \n                 hist=True, color='teal', ax=axs[0][i]) \n                        # [~survived] means 'Not Survived'\n    sns.distplot(df_train[survived][feature], label='Survived', \n                 hist=True, color='peru', ax=axs[0][i])\n    \n    # distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', \n                 hist=False, color='teal', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', \n                 hist=False, color='peru', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    # providing the ticks for x and y in respective plots\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    \n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n    \nplt.show()","bae9f3ee":"categorical_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20,20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)\n    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper centre', prop={'size':18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","dc06c7d3":"sns.heatmap(df_all.corr(), annot=True, cmap='RdYlGn', linewidths=0.2)\n\nfig = plt.gcf()\nfig.set_size_inches(10,8)\n\nplt.show()","c8b626c9":"df_all['Fare'] = pd.qcut(df_all['Fare'], 13)","bf3b6d4b":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\n\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size':15})\nplt.title('Count of Survival in {} feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","e9f422b3":"df_all['Age'] = pd.qcut(df_all['Age'], 10)","5016c1d1":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size':15})\nplt.title('Count of Survival in {} feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","8edd3c27":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20,20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, \n            y=df_all['Family_Size'].value_counts().values, \n            ax=axs[0][0])\n\nsns.countplot(x='Family_Size', hue='Survived', \n              data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size', size=20, y=1.05)\n\n\n# mapping family size\nfamily_map = {1:'Alone', \n              2:'Small', 3:'Small', 4:'Small', \n              5:'Medium', 6:'Medium', \n              7:'Large', 8:'Large', 11:'Large'}\n\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, \n            y=df_all['Family_Size_Grouped'].value_counts().values, \n            ax=axs[1][0])\n\nsns.countplot(x='Family_Size_Grouped', hue='Survived', \n              data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n        \nplt.show()","59d98a10":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","9cb22148":"fig, axs = plt.subplots(figsize=(12,9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size = 15, labelpad=20)\nplt.ylabel('Passenger Count', size = 15, labelpad=20)\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size':15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","c788f31a":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1","0dec7583":"fig, axs = plt.subplots(nrows=2, figsize=(20,20))\nsns.barplot(x=df_all['Title'].value_counts().index, \n            y=df_all['Title'].value_counts().values, \n            ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='y', labelsize=15)\n\nfor i in range(2):\n    axs[i].tick_params(axis='y', labelsize=15)\n    \naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs', 'Ms', \n                                           'Mlle', 'Lady', 'Mme', \n                                           'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', \n                                           'Jonkheer', 'Capt', 'Sir', \n                                           'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, \n            y=df_all['Title'].value_counts().values, \n            ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","45befca4":"df_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]","100d21f1":"def extract_surname(data):\n    \n    families = []\n    \n    for i in range(len(data)):\n        name = data.iloc[i]\n        \n        if '(' in name:\n            name_no_bracket = name.split('(')[0]\n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, ' ').strip()\n            \n        families.append(family)\n        \n    return families","97f25038":"df_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","65eecc42":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family', 'Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket', 'Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\n# Checking a family exist in both training and test set and has members more than 1\nfor i in range(len(df_family_survival_rate)):\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i,1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i,0]\n        \n# Checking a ticket exists in both training and test set and has members more than 1\nfor i in range(len(df_ticket_survival_rate)):\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i,1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i,0]","1b1989c9":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\n\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)     \n\nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \n\ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\n\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA  ","21b82302":"train_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\n\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)     \n\nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \n\ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\n\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA  ","46f63117":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2","80d802d4":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder","b0f00348":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:\n        df[feature] = LabelEncoder().fit_transform(df[feature])","c0f1b6f7":"onehot_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in onehot_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1,1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1,n+1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)","30cdcf3c":"# *encoded_features will give all encoded features of each of the Six onehot_features\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","55f53dfb":"df_all = concat_df(df_train, df_test)\n\n# Dropping unwanted features\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', \n             'Survived', 'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', \n             'Ticket', 'Title', 'Ticket_Survival_Rate', 'Family_Survival_Rate', \n             'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\ndf_all.head()","5a420a90":"# model helpers\nfrom sklearn.preprocessing import StandardScaler\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","20527e41":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('Y_train shape: {}'.format(y_train.shape))\n\n\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\nprint('X_test shape: {}'.format(X_test.shape))","c712428d":"# Stochastic Gradient Descent (SGD)\nsgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_test)\nsgd_score = ('{:.2f}'.format(sgd.score(X_train, y_train)*100))\n\n\n# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest_score = ('{:.2f}'.format(random_forest.score(X_train, y_train)*100))\n\n\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nlogreg_score = ('{:.2f}'.format(logreg.score(X_train, y_train)*100))\n\n\n# K Nearest Neighbor\nknn= KNeighborsClassifier(n_neighbors =3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nknn_score = ('{:.2f}'.format(knn.score(X_train, y_train)*100))\n\n\n# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\ngaussian_score = ('{:.2f}'.format(gaussian.score(X_train, y_train)*100))\n\n\n# Perceptron\nperceptron = Perceptron(max_iter = 5)\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_test)\nperceptron_score = ('{:.2f}'.format(perceptron.score(X_train, y_train)*100))\n\n\n# Linear Support Vector Machine\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_test)\nlinear_svc_score = ('{:.2f}'.format(linear_svc.score(X_train, y_train)*100))\n\n\n# Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\ndt_score = ('{:.2f}'.format(dt.score(X_train, y_train)*100))","81ad1b55":"results = pd.DataFrame({'Model': ['Stochastic Gradient Descent', 'Random Forest', \n                                  'Logistic Regression', 'K Nearest Neighbor', 'Naive Bayes', \n                                  'Perceptron', 'Support Vector Machine', 'Decision Tree'], \n                        'Score': [sgd_score, random_forest_score, logreg_score, knn_score, \n                                  gaussian_score, perceptron_score, linear_svc_score, dt_score]})\n\ndf_results = results.sort_values(by='Score', ascending=False)\ndf_results = df_results.set_index('Score')\ndf_results","d4b5cbb2":"from sklearn.model_selection import cross_val_score\n\nrf = RandomForestClassifier(n_estimators=100, oob_score=True)\nscores = cross_val_score(rf, X_train, y_train, cv=10, scoring='accuracy')\n\nprint('Scores:', scores)\nprint('Mean:', scores.mean())\nprint('Standard Deviation:', scores.std())","34623161":"rf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nrf.score(X_train, y_train)\n\nrf_score = ('{:.2f} %'.format(rf.score(X_train, y_train)*100))\nrf_score","979e25c5":"X = df_train.drop(columns=drop_cols)\n\nimportances = pd.DataFrame({'feature': X.columns, \n                            'importance': np.round(rf.feature_importances_, 3)})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\n\n\nimportances.head(26)","5c7f5d95":"importances.plot.bar()","eb1e21cd":"random_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\n\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\n\nrandom_forest_score = ('{:.2f} %'.format(random_forest.score(X_train, y_train)*100))\nrandom_forest_score","002c12c8":"print(\" random forest oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","a58c7923":"print(\"rf oob score:\", round(rf.oob_score_, 4)*100, \"%\")","04503f84":"random_forest = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=1)\nrandom_forest.fit(X_train, y_train)\ny_pred = (random_forest.predict(X_test)).astype(int)\n\nrandom_forest.score(X_train, y_train)\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","98581202":"from sklearn.model_selection import cross_val_score\n\nrf = RandomForestClassifier(n_estimators=100, oob_score=True)\nscores = cross_val_score(random_forest, X_train, y_train, cv=10, scoring = \"accuracy\")","77c82021":"print(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","2c3f8e3e":"from sklearn.model_selection import cross_val_predict\n\npred = cross_val_predict(random_forest, X_train, y_train, cv=3)","e4fe6cff":"from sklearn.metrics import confusion_matrix\nprint('Confusion_Matrix:\\n {}'.format(confusion_matrix(y_train, pred)))","08edb999":"from sklearn.metrics import precision_score, recall_score\nprint('Precision:', precision_score(y_train, pred))\nprint('Recall:', recall_score(y_train, pred))","215fc7fc":"from sklearn.metrics import f1_score\nprint('F1 Score:', f1_score(y_train, pred))","a3349ee4":"from sklearn.metrics import roc_curve\n\n# Getting probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\n# Computing true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n\n# Plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0,1], [0,1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n    \nplt.figure(figsize=(14,7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","cd323ac0":"from sklearn.metrics import roc_auc_score\n\nr_a_score = roc_auc_score(y_train, y_scores) \nprint('ROC-AUC-Score:', r_a_score)","e8b8c6c8":"submission  = pd.DataFrame({'PassengerId': df_test['PassengerId'], \n                            'Survived': y_pred})\n\nsubmission.to_csv('my_submission.csv', index=False)\n","b1e019d5":"Now we will visualize the survival rates in different decks. We will follow the same method as above to group 'Deck' with 'Survived' and then We'll define a function (just like we did earlier to visualize percentage of passengers in every deck.)","183c4405":"* `Fare` feature is *positively skewed* and survival rate is extremely high on the right end.\n* **13** quantile based bins are used for it. Even though the bins are too much, they provide decent amount of information gain.   \n* The groups at the left side of the graph has lowest survival rates and the groups as the right has highest survival rates.\n* This survival rate was not visible in the distribution graph. There is also an unusual group **(15.742, 23.25]** in the middle with high survival rate that is captured in this process.\n\n  \n ### Age:","5d045c9e":"### How many survived?","245f0afc":"### Training Random Forest again:","2aa311ee":"### ROC AUC Curve:","1f509adb":"## Feature Transformation\n\n### Convert Formats:\n* We will convert categoricaldata to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n* In this step, we will also define our x(independent\/features\/exploratory\/predictory etc) and y(dependent\/target\/outcome\/response etc) variables for data modeling.  \n  \n### Label Encoding Non-Numerical Features:\n* `Embarked`, `Sex`, `Deck` `Title` and `Family_Size_Grouped` are object type, and `Age` and `Fare` features are category type. These are converted into numerical type with `LabelEncoder`. \n* `LabelEncoder` basically labels the classes from **0** to **n**.\nThis process is necessary for models to learn from those features.\n","58393308":"We interpret Confusion Matrix as:  \n( [ True Negatives - 500, False Positives - 49 ],  \n[ False Negatives - 88, True Positives - 254 ] )   \n\nIn other words, \n* TN - 500 passengers were correctly classified as not survived\n* FP - 49 passengers were wrongly classified as not survived\n* FN - 88 passengers were wrongly classified as survived\n* TP - 254 passengers were correctly classified as survived  ","3d012296":"#### 2. Test Set","7960c59c":"\nNow, we will group some of the values with eachother based on their similarity.  \nSo, we will label the deck:  \n* **A, B,** & **C** as **ABC** as all have 1st class passengers.\n* **D** & **E** as **DE** and **F** & **G** as **FG** because both have similar passenger class distribution and same survival rates.\n* **M** will remain alone as it is very different from others.","ff299163":"### ROC AUC Score:\nThe ROC AUC Score is corresponding score to the ROC AUC Curve. It is simpply computed by measuring the area under the curve, which is called AUC.  \nA classifier that is 100% correct would have a ROC AUC Score of 1 and a completely random classifier would have a score of 0.5.","7e19cc24":"### One-Hot Encoding the Categorical Features:\n* The categorical features (`Pclass`, `Sex`, `Deck`, `Embarked`, `Title`) are converted to one-hot encoded features with `OneHotEncoder`.\n* `Age` and `Fare` features are not converted because they are ordinal.","001dbe8a":"* Deck **B, C, D** and **E** has highest survival rates, which are mostly occupies by *1st* class passengers.  \n* **M** has lowest survival rate, which is mostly occupied by *2nd and 3rd* class passengers.   \n\nCabin **M** has lowest survival rates as it itself represents the cabin of missing values (which is the data of victims that they couldn't retrive simply because they didn't survive). Also it is a unique group with shared characteristics.  ","067f6887":"When passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median age than males. The median ages below are used for filling the missing values in Age feature.","5aacac20":"### Conclusion:\n* `Age` and `Fare` features are binned. Binning helps dealing wtih outliers and reveals some homogeneous groups in those features.   \n* `Family_Size` is created by adding `Parch` and `SibSp` features and **1**.  \n* `Ticket_Frequency` is created by counting the occurance of `Ticket` values.  \n* `Name` feature is very useful.\n    * `Title` and `IsMarried` features are created from the title prefix in the names. \n* `Family_Survival_Rate` and `Family_Survival_Rate_NA` features are created by target encoding the surname of the passengers.\n* `Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are created by averaging the `Ticket` feature.\n* `Survival_Rate` feature is created by averaging the `Family_Survival_Rate` and `Ticket_Survival_Rate` features.\n* Finally, the non-numeric type of features are label encoded and categorical features are one-hot encoded and created **5** new features (`Family_Size`, `Title`, `IsMarried`, `Survival_Rate` and `Survival_Rate_NA`) are dropped after encoding. ","89479c37":"There is only one passenger in Deck T and he is a 1st Class passanger. So let's group him with Deck A passenger.","fb3ec74c":"Generally, *the more features you have, the more likely your model will suffer from overfitting and vice versa.* Our Random Forest model predicts as good as it did before.  \nThere is also an another approach called **out-of-bag** samples to estimate the generalization accuracy. This approach's estimate is as accurate as using a test set of the same size as training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.","b9a52f19":"### Continuous Features\n\nLet's see the distribution of continuous features `Age` and `Fare` with *Survived \/ Not Survived* on Training and Test sets.\n","8df984d4":"Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.  \n  \n### Ticket:\nThere are too many unique `Ticket` values to analyze, so grouping them up by their frequencies makes things easier. \n  \n**This feature differ from `Family_Size` as** many passengers travelled along with groups which consists of friends, maids, nannies etc. who used same tickets but weren't counted as family.  \n  \nAlso we can not group `Ticket` feature by their prefixes because if it had any meaning in it, it would have already captured by `Pclass` and `Embarked` features.","74607c6d":"### Pclass","28506b87":"## Model Evaluation","bb8f8f17":"Only numeric features are compared as it is obvious that we can not correlate alphabets and numbers.  \n* We can see that the features are not much correlated. In fact the highest correlation is between `SibSp` and `Parch` and it is **0.37**. So we carried on with all the features.","db0db2e4":"### Testing new Parameters","515bf5ee":"## K-Fold Cross Validation\nK-Fold Cross Validation randomly splits the training data into **K subsets** called **Folds**.    \nWe will splits our data into 10 folds (K=10).  \nOur random forest model will be trained and evaluated 10 times, using different folds for evaluation everytime giving an output array with 10 different scores.","2b68c5bb":"The accuracy is really good but still we will try of increase it's performance even further in following section.","63673e13":"## Hyperparameter Tuning\nThe script for Hyperparameter tuning is written below.  \nNote: It is intensionally changed into Markdown as it takes very long time to run.","ab49c45b":"### Cabin:  \n* The large portion of this feature is missing. We also cannot ignore this feature because some cabins might have higher survival rates.  \n* Here, the first letter of the cabin values are the decks in which cabins are located. These decks are mainly seperated for different passanger class.    \n\nLet's create the Deck column by extracting the first letter of Cabin value, and M for the missing value. Also we will drop various columns, group 'Deck' with 'Pclass' and rename 'Name' to 'Count'.","c7df7ba0":"We can see that:  \n* **Pclass1**: survival chances for Age 20-50 is high  \n* **Pclass2**: survival chances for childern is high \n* **Pclass3**: survival chances for Age below 30 is high  \n* The survival chances for childern looks good irrespecitve of Pclass","959eabfa":"## Import Dependencies","45dc24f7":"### Target Encoding:\n`extract_surname` function is used for extracting surnames of passangers from the `Name` feature. Family feature is created with the extracted surname. This is necessary for grouping passengers in the same family.","f5fb8b41":"## Exploratory Data Analysis","3310c23b":"* `Pclass` is contributing to a person's chance of survival, especially if a person is in class 1.  \n* From barplot we can say that survival of Women in Pclass 1 is about 95-96% (as only 3 out of 94 women died from this class).  \n* Survival rate for Men is very low even from Pclass 1, so it is clear that women were given first priority while rescue. Therefore, `Pclass` is also an important feature.","9290d659":"Our random forest model seems to do a good job. Of course we also have a tradeoff, because classifier produces more false positives, the higher the true positive rate is.","042da062":"* High chance of survival for family size of 2 to 4  \n* Lower chance of survival for family size less than 2 or more than 4","95b054ef":"* Our model predicts a passenger's survival **84%** of the time correctly = **Precision**.\n* It predicted the survival of 74% of the people who actually survived = **Recall**.","7ed01610":"* `Family_Survival_Rate` is calculated from families on training set as there is no `Survived` feature in test set. \n* A list of family names that are occuring in both training and test set (`non_unique_families`) is created. \n* The survival rate is calculated for families with more than 1 member in that list, and stored in `Family_Survival_Rate` feature.  \n  \n  \n* An extra binary feature `Family_Survival_Rate_NA` is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families' survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrive their survival rate.  \n* `Ticket_Survival_Rate` and `Ticket_Survival_Rate_NA` features are also created with the same method. \n* `Ticket_Survival_Rate` & `Family_Survival_Rate` are averaged and named `Survival_Rate`, and `Ticket_Survival_Rate_NA` & `Family_Survival_Rate_NA` are also averaged and become `Survival_Rate_NA`.","6c00678c":"Now let's visualize a graph showing a percentage distrubution of passengers of different classes in every Deck.","54818d2d":"### Missing Values  \n#### 1. Training Set","ce3f7520":"### Fare","5a25c234":"We have filled up all the missing values of different features.   \nWe will also drop the `Cabin` feature as we are using `Deck` feature instead.","ef1f0931":"### Precision and Recall:","d4f93b59":"## Feature Importance\nFeature Importance is used to measure the relative importance of each feature using `rf.feature_importances_` function.","123088bf":"Now, we will see the performances of different models. For that we will create a dataframe and arrange the models in order with performances from highest to lowest.","0280f640":"## Feature Engineering\n\n### Fare","ef54c17b":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'criterion ': ['gini', 'entropy'], \n              'min_samples_leaf' : [1, 5, 10,], \n              'min_samples_split' : [2, 4, 10,], \n              'n_estimators': [100,500,11000,1500]}\n\ngd = GridSearchCV(estimator = RandomForestClassifier(random_state=42), \n                  param_grid = param_grid,\n                  verbose = True)\n\ngd.fit(X_train,y_train)\n\nprint(gd.bestscore) \nprint(gd.bestestimator)","68b1285e":"### Confusion Matrix:","680091f6":"* As per google [Titanic Survivor](http:\/\/https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html), **Stone, Mrs.George Nelson(Marth Evelyn)** embarked from **S (Southampton)** with her maid **Amelie Icard**. So, we filled the missing values with **S**.","f3b87944":"**Male**:  \n* High chance of survival for age group 18-30 years old.\n* High chance of survival for children\n* Low chance of survival for age group 5-18 years old.  \n  \n**Female**:  \n* High chance of survival for age group 14-40 years old.\n* High chance of survival for childern.","486e1741":"Here we can see that every categorical features are very helpful in predicting survivor\/victim and every feature has at least one class with high rate of 'Not Survived'.   \nBut the best features are `Pclass` and `Sex` as they have the most homogeneous distributions.","778259da":"We can see that the Random Forest Classifier gives highest performance.  \nNow, let's check how it performs when we use Cross Validation.","756590f3":"###  Categorical Features:\nLet's see the distribution of categorical features `Embarked`, `Parch`, `Pclass`, `Sex`, `SibSp`, and `Deck` with *Survived \/ Not Survived* on Training and Test sets.\n","65370bba":"From this we can say that,  \n* Our model has an average accuracy of **84%** with a standard deviation of **4%**.\n* The Standard Deviation show us how precise the model estimates are. In our case the accuracy of our model can differ **+\/- 4%**","a0e7fc20":"Now, we will get  the percentage count of passangers of different classes in every decks","26ec4602":"### Embarked  \nChance of survival by port of Embarkation","b05fcaca":"We can see that:\n* **A, B** and **C** decks were only for 1st class passengers\n* **D** and **E** decks were for all classes\n* **F** and **G** were for both 2nd and 3rd class passengers","a49d9288":"### F-Score:","50c0f86d":"* `Embarked` is the categorial feature and there are only 2 missing values in the whole data set. Both of those passengers are female, upper class and they have the same ticket number . This means that they know each other and embarked from the same port together.  \n* The mode `Embarked` value for an upper class female passenger is **C (Cherbourg)**, but this doesn't necessarily mean that they embarked from that port.","7fc195d1":"Note : I have written this Notebook by refering a notebook of [kushal Agrawal](https:\/\/www.kaggle.com\/kushal1506)","395b5516":"## Reading Data","db234c3a":"According to the graph, \n* Groups with **2, 3** and **4** members had a higher survival rate. \n* Passengers who travelled alone has the lowest survival rate. \n* After **4** group members, survival rate decreases drastically.  \n  \n### Title and IsMarried:\nTitle is created by extracting the prefix before `Name` feature. \nIn our dataset, there are too many title which doesn't seem correct and needs to be replaced.  \n  \nWe will replace titles like:\n* **Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona** by **Miss\/Mrs\/Ms**\n* **Dr, Col, Major, Jonheer, Capt, Sir, Don, Rev** by **Dr\/Military\/Nobel\/Clergy**\n* **Master** is unique title given to male passengers below 26.\n\nHere **Mlle, Mme, Dona, Jonkheer, Din and Rev** are name of the passesngers which are *miss-classified* as `Name` feature is split by comma\n\n","3c5bf334":"### Correlation between the features:\n\n* **POSITIVE CORRELATION** : If an **increase in feature A** leads to **increase in feature B**, then they are positively correlated. A value 1 means ***Perfect Positive Correlation***.\n\n* **NEGATIVE CORRELATION**: If an **increase in feature A** leads to **decrease in feature B**, then they are negatively correlated. A value -1 means ***Perfect Negative Correlation***.\n\n* If two features are highly or perfectly correlated (i.e., increase in one leads to increase in other) that means they hold highly similar information and there is little or no variance in the information. This is known as **MultiColinearity**. \n* So, while making or training models, we should *try to eliminate redundant features* as it will reduce the training time.","ea8cb640":"* `Age` feature has a normal distribution with some spikes and bumps and **10** quantile based bins are used for it.\n* The 1st bin has the highest survival rate and the 4th bin has the lowest survival rate.\n* There is also an unusual group **(34.0, 40.0]** with high survival rate that is captured in this process.\n\n### Frequency Encoding\n\n`Family Size` is created by adding `SibSp`, `Parch` and **1** (ie., 'Siblings and Spouse, Parents and Children). These columns are added in order to find the total size of families. And adding 1 at the end is the current passenger.\n* Family Size with **1** are labeled as **Alone**\n* Family Size with **2**, **3** and **4** are labeled as **Small**\n* Family Size with **5** and **6** are labeled as **Medium**\n* Family Size with **7**, **8** and **11** are labeled as **Large**","590f12f1":"## Building Machine Learning Models\nWe will train several machine learning models.  \nAlso the dataset does not provide the labels for test set, we will use the predictions on training set to compare the algorithms with each other.\n","5fd506ad":"Here missing values in `Age` are filled with *Median age* of whole data which is not a good idea.   \nIn order to be more accurate, Sex feature is used as the second level of groupby while filling the missing age values.","5e848f72":"### Age and Sex","7b8e4d20":"We can see that the distribution has more spikes and bumps on training set, but is smoother on test set, which means the model might not be able to generalize well on test set.\n* The distribution of `Age` feature shows that children younger than 15 has higher survival rate than any of other age groups.\n* The distribution of `Fare` feature shows the survival rates is higher on distribution tails.","a1807f67":"* There is only one missing value of `Fare`.    \n* We can assume that is related to family size (`Parch`, `SibSp`) and `Pclass` features.  \n* Medain `Fare` value of a male with a third class ticket and no family is logical choice to fill the missing value.","1d95ff23":"Now as we have a proper model, we can start evaluating it's performance in a more better way. Previously we only used accuracy and the oob score which is just another form of accuracy.  \nThe thing is it is more complicated to evaluate a cassification model than a regression model.","0dce60ad":"So we can see that both training set and test set have missing  values in:  \n* Training set : `Age`, `Cabin` & `Embarked` columns  \n* Test set : `Age`, `Cabin` & `Fare` columns\n    \n It will be convenient to work on concatenated training and test set while dealing with the missing values, otherwise filled data may overfit to training or test set samples.","4d134567":"* `Embarked` seems to be correlated with `survival`, depending on `Sex` and `Pclass`. \n* Women on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C.\n* Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.","1f383252":"### SibSp and Parch:  \n`SibSp` and `Parch` would make more sense as a combined feature, that shows a person's family size.","53820d09":"So we can see that:  \n* **Deck A**:\n    * 1st Class: 100%\n* **Deck B**:\n    * 1st Class: 100%\n* **Deck C**:\n    * 1st Class: 100%\n* **Deck D**:\n    * 1st Class: 87%\n    * 2nd Class: 13%\n* **Deck E**:\n    * 1st Class: 83%\n    * 2nd Class: 10%\n    * 3rd Class: 7%\n* **Deck F**:\n    * 2nd Class: 62%\n    * 3rd Class: 38%\n* **Deck G**:\n    * 3rd Class: 100%  \n","7bf056ce":"The above plot confirms that probability of survival is:  \n* High in  Pclass 1  \n* Low in Pclass 3"}}