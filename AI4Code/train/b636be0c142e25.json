{"cell_type":{"0d7075dd":"code","e423254d":"code","4d924773":"code","14b5ca58":"code","39151994":"code","7cceefa7":"code","f0824214":"code","498c4b6c":"code","05920b93":"code","fe01be92":"code","1ee99964":"code","fe47f674":"code","bbf16dee":"code","1a92c052":"code","2fb1871f":"code","53fcab76":"code","7a9756d0":"code","f0ccf7f8":"code","a409e1a6":"code","3983f2ea":"code","25620e3c":"code","8b5c632e":"code","1a9fcad2":"code","bdaa6629":"code","98c8bc6a":"code","20b2fe4d":"code","d86832fe":"code","a3753ca3":"code","4ae83ae3":"code","f3454155":"code","5affb0c0":"code","f9554e9c":"code","74f93349":"code","d17cbf6c":"code","cc239fec":"code","d0ac7e82":"code","a3dbad34":"code","1f70d759":"code","db218f35":"code","9cb9514c":"code","a5592a33":"code","2e25648a":"code","e004a441":"code","2a9d13d5":"code","fe0a6808":"code","ea7c122b":"code","bb5a5f07":"code","1afc9af0":"code","119acba4":"code","038681eb":"code","4b85a8cf":"code","689bbe66":"code","6128bf2d":"code","655909d6":"code","d7efb771":"code","96475e3d":"code","8088d8d1":"code","3f436cb7":"code","61f230fe":"code","2367a441":"code","c438a924":"code","37804f2e":"code","f8a9cce0":"code","b54a5338":"markdown","139e728b":"markdown","427aa5f3":"markdown","f3d5bc4c":"markdown","8e8609f0":"markdown","258ba09c":"markdown","ff74c7a6":"markdown","9464cb34":"markdown","09101c4a":"markdown","5953f582":"markdown","7de435d7":"markdown","adfb861b":"markdown","4eefee12":"markdown","8eef299c":"markdown","b69fbfbe":"markdown","16c28e94":"markdown","d42b6302":"markdown"},"source":{"0d7075dd":"# import the necessary libraries for data preprocessing\nimport numpy as np\nimport pandas as pd\n\n# import libraries for viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set preferred style for visualization and dataframe display\n%matplotlib inline\nsns.set(style='darkgrid')\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', None)\n\n# ignore warnings library\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e423254d":"# Load the datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","4d924773":"train.skew()","14b5ca58":"# Preview of the train dataset\ntrain.head(20)","39151994":"# Statistics summary of the train dataset\ntrain.describe()","7cceefa7":"# Display the brief info of the dataset (showing shape, null values, and types)\ntrain.info()","f0824214":"test.info()","498c4b6c":"# Check for the survival rate of the passengers and total number of survivors\nsurvived_rate = train['Survived'].mean()\nsurvived_count = train['Survived'].value_counts().values[1]\n\nprint('The survival rate of titanic passengers: {:.2f}%'.format(survived_rate*100))\nprint('The number of survivors: {}'.format(survived_count))","05920b93":"# Quick look at  male and female survival rates\nmale_df = train[train['Sex']=='male']\nfemale_df= train[train['Sex']=='female']\nmale_survived = male_df['Survived'].mean()\nfemale_survived = female_df['Survived'].mean()\n# We can classify children as passengers with age <7\nchild_df = train[train['Age'] <7]\nchild_survived = child_df['Survived'].mean()\n# We can classify old people as passengers with age >60\nsenior_df = train[train['Age'] >60]\nsenior_survived = senior_df['Survived'].mean()\nprint('Male survival rate: {:.2f}%'.format(male_survived*100))\nprint('Female survival rate: {:.2f}%'.format(female_survived*100))\nprint('Children (<7) survival rate: {:.2f}%'.format(child_survived*100))\nprint('Senior (>60) survival rate: {:.2f}%'.format(senior_survived*100))","fe01be92":"# A quick look at the survival distribution between males and females\nsns.violinplot('Sex', 'Survived', data=train)\nplt.show()","1ee99964":"class1 = train[train['Pclass']==1]\nclass2 = train[train['Pclass']==2]\nclass3 = train[train['Pclass']==3]\nclass1_survived = class1['Survived'].mean()\nclass2_survived = class2['Survived'].mean()\nclass3_survived = class3['Survived'].mean()\nprint('Class 1 survival rate: {:.2f}%'.format(class1_survived*100))\nprint('Class 2 survival rate: {:.2f}%'.format(class2_survived*100))\nprint('Class 3 survival rate: {:.2f}%'.format(class3_survived*100))","fe47f674":"# A quick look at the survival distribution between males and females\nsns.violinplot('Pclass', 'Survived', data=train)\nplt.show()","bbf16dee":"# A quick look at the distribution of survivors and non-survivors across the 3 ports of embarkation\npd.crosstab(train['Embarked'], train['Survived'])","1a92c052":"port_c = train[train['Embarked']=='C']\nport_q = train[train['Embarked']=='Q']\nport_s = train[train['Embarked']=='S']\nport_c_survived = port_c['Survived'].mean()\nport_q_survived = port_q['Survived'].mean()\nport_s_survived = port_s['Survived'].mean()\nprint('Passengers embarked from Cherbourg survival rate: {:.2f}%'.format(port_c_survived*100))\nprint('Passengers embarked from Queenstown survival rate: {:.2f}%'.format(port_q_survived*100))\nprint('Passengers embarked from Southampton survival rate: {:.2f}%'.format(port_s_survived*100))","2fb1871f":"pd.crosstab(train['Pclass'], train['Embarked'])","53fcab76":"# Plot box and whisker plots to visualize the continuous variables\n\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nsns.boxplot(train['Age'], y=train['Sex'])\n\nplt.subplot(1,2,2)\nsns.boxplot(train['Fare'], y=train['Sex'])\n\n\nplt.show()","7a9756d0":"# Plot histograms to visualize the continuous variables\n\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nsns.distplot(train['Age'])\n\nplt.subplot(1,2,2)\nsns.distplot(train['Fare'])\n\nplt.show()","f0ccf7f8":"# Check the skewness present in the dataset\nskewness = train.skew()\nskewness","a409e1a6":"# Using Isolation Forest to detect\/predict the outliers\n\ntrain_no_age_NA = train[train['Age'].notnull()]\n\nfrom sklearn.ensemble import IsolationForest\nmodel=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.1),max_features=1.0)\nmodel.fit(train_no_age_NA[['Age']])\nmodel.fit(train_no_age_NA[['Fare']])","3983f2ea":"# Adding new columns to show which observation is an outlier (-1)\n\ntrain_no_age_NA['scores_age'] = model.decision_function(train_no_age_NA[['Age']])\ntrain_no_age_NA['scores_fare'] = model.decision_function(train_no_age_NA[['Fare']])\ntrain_no_age_NA['outlier_age'] = model.predict(train_no_age_NA[['Age']])\ntrain_no_age_NA['outlier_fare'] = model.predict(train_no_age_NA[['Fare']])\ntrain_no_age_NA.head(20)","25620e3c":"# The number of outliers detected using machine learning\nfare_out = len(train_no_age_NA[train_no_age_NA['outlier_fare'] == -1])\nage_out = len(train_no_age_NA[train_no_age_NA['outlier_age'] == -1])\n\nprint('The number of outliers found in the age column: {}'.format(age_out))\nprint('The number of outliers found in the fare column: {}'.format(fare_out))","8b5c632e":"train_no_age_NA[train_no_age_NA['outlier_fare'] == -1].head(20)","1a9fcad2":"# We will manually encode the Sex and Embarked column to obtain the correlation\n# as we sort of know there are some relationship that lie within these variables\ntrain['Sex'] = train['Sex'].replace({'male': 1, 'female': 2})\ntest['Sex'] = test['Sex'].replace({'male': 1, 'female': 2})\ntrain['Embarked'] = train['Embarked'].replace({'C': 1, 'Q': 2, 'S': 3})\ntest['Embarked'] = test['Embarked'].replace({'C': 1, 'Q': 2, 'S': 3})","bdaa6629":"# Correlation check\ncorrelation = train.corr()\ncorrelation","98c8bc6a":"# Visualize the correlations\nplt.figure(figsize=(10,8))\nsns.heatmap(correlation, cmap='RdYlGn', annot=True)\nplt.title('Heatmap for the correlations found in training dataset')\nplt.show()","20b2fe4d":"# Check the null rows\nnull_train = train.isnull().sum()\nnull_test = test.isnull().sum()","d86832fe":"null_train.sort_values(ascending=False)","a3753ca3":"null_test.sort_values(ascending=False)","4ae83ae3":"# Combine both datasets\ndf = train.append(test).reset_index(drop=True)","f3454155":"# Fill the missing cells in Embarked column with the mode\nmode_embarked = df['Embarked'].mode().values[0]\ndf['Embarked'] = df['Embarked'].fillna(mode_embarked)","5affb0c0":"# Fill the missing cell in Fare column with the median value by the class the passenger was in\ndf['Fare'] = df.groupby('Pclass')['Fare'].apply(lambda x: x.fillna(x.median()))","f9554e9c":"# Extract the title of the passengers from their names\ndf['Title'] = df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\n# Grouping the titles together to form a smaller group\ntitle_dict = {'Mr': 'Mr',\n              'Miss': 'Miss',\n              'Mrs': 'Mrs',\n              'Master': 'Master',\n              'Dr': 'Other',\n              'Rev': 'Other',\n              'Col': 'Other',\n              'Major': 'Other',\n              'Ms': 'Miss',\n              'Mlle': 'Miss',\n              'Dona': 'Royal',\n              'Sir': 'Royal',\n              'Lady': 'Royal',\n              'Don': 'Royal',\n              'Mme': 'Mrs',\n              'Jonkheer': 'Royal',\n              'Capt': 'Other',\n              'the Countess': 'Royal'    \n}\n\ndf['Title'] = df['Title'].map(title_dict)","74f93349":"# Survival rates by the passenger's title\ndf.groupby('Title')['Survived'].mean()","d17cbf6c":"# Create a new feature call 'Deck' by extracting the first character from the 'Cabin' column\ndf['Deck'] = df['Cabin'].str[0]\n\n# Replace the crew 'T' deck with 'A' as the *passenger* is in Pclass = 1\ndf['Deck'] = df['Deck'].replace('T', 'A')\n\n# Group decks together to form a smaller group\ndf['Deck'] = df['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf['Deck'] = df['Deck'].replace(['D', 'E'], 'DE')\ndf['Deck'] = df['Deck'].replace(['F', 'G'], 'FG')","cc239fec":"# Create a new feature call 'Family Size' from the 'SibSp' and 'Parch' columns\ndf['Family Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Grouping Family size together to form a smaller group\ndf['Family Size'] = df['Family Size'].replace(1, 'No Family')\ndf['Family Size'] = df['Family Size'].replace([2,3,4], 'Small')\ndf['Family Size'] = df['Family Size'].replace([5,6,7,8,11], 'Large')\n#df['Family Size'] = df['Family Size'].replace([7,8,11], 'Large')","d0ac7e82":"df['Family Size'].value_counts()","a3dbad34":"# Obtain family name to analyze the dataset deeper (it will be dropped before fitting any models)\ndf['Surname'] = df['Name'].str.split(',', expand=True)[0]","1f70d759":"# Create new feature to distinguish passengers with no family\n#df['Is Alone'] = df.apply(lambda x: 1 if (x['SibSp'] + x['Parch'] + 1 == 1) else 0, axis=1)","db218f35":"# Create a new feature to distinguish passengers who are in a family with kids\/parents\/guardian\n#df['Travel w\/ Kids'] = df.apply(lambda x: 1 if x['Parch']>0 else 0, axis=1)","9cb9514c":"df.head()","a5592a33":"# For this analysis, we have to split our dataset into the train and test dataset again\ntrain2 = df.iloc[:891].copy()\ntest2 = df.iloc[891:].copy().drop('Survived', axis=1)\nmissing_cabin_df = train2[train2['Deck'].isnull()]","2e25648a":"missing_cabin_df.head()","e004a441":"missing_survived_rate = missing_cabin_df['Survived'].mean()\nmissing_survived = len(missing_cabin_df[missing_cabin_df['Survived'] == 1])\nprint('The survival rate for passengers with missing decks: {:.2f}%'.format(missing_survived_rate*100))\nprint('The number of survivors: {}'.format(missing_survived))","2a9d13d5":"class1m = missing_cabin_df[missing_cabin_df['Pclass']==1]\nclass2m = missing_cabin_df[missing_cabin_df['Pclass']==2]\nclass3m = missing_cabin_df[missing_cabin_df['Pclass']==3]\nclass1m_survived = class1m['Survived'].mean()\nclass2m_survived = class2m['Survived'].mean()\nclass3m_survived = class3m['Survived'].mean()\nprint('For passengers with missing decks,')\nprint('Class 1 survival rate: {:.2f}%'.format(class1m_survived*100))\nprint('Class 2 survival rate: {:.2f}%'.format(class2m_survived*100))\nprint('Class 3 survival rate: {:.2f}%'.format(class3m_survived*100))","fe0a6808":"train2.head()","ea7c122b":"drop_col = ['Name', 'Ticket', 'Cabin', 'Surname', 'Embarked', 'SibSp', 'Parch', 'PassengerId']\ntrain2 = train2.drop(drop_col, axis=1)\ntest2 = test2.drop(drop_col, axis=1)","bb5a5f07":"from fancyimpute import KNN\nfrom sklearn.preprocessing import OrdinalEncoder\n\n#instantiate both packages to use\nencoder = OrdinalEncoder()\nimputer = KNN()\n# create a list of categorical columns to iterate over\ncat_cols = ['Title', 'Deck', 'Family Size']\ndf_all = [train2, test2]\n\ndef encode(data):\n    '''function to encode non-null data and replace it in the original data'''\n    #retains only non-null values\n    nonulls = np.array(data.dropna())\n    #reshapes the data for encoding\n    impute_reshape = nonulls.reshape(-1,1)\n    #encode date\n    impute_ordinal = encoder.fit_transform(impute_reshape)\n    #Assign back encoded values to non-null values\n    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n    return data\n\n#create a for loop to iterate through each column in the data\nfor df in df_all:\n    for columns in cat_cols:\n        encode(df[columns])","1afc9af0":"# impute data and convert\nencode_data_train = pd.DataFrame(imputer.fit_transform(train2),columns = train2.columns)\nencode_data_test = pd.DataFrame(imputer.fit_transform(test2),columns = test2.columns)","119acba4":"# Since KNN produces floats, we will have to round up the deck and age columns\nencode_data_train.loc[:, ['Age', 'Deck']] = round(encode_data_train.loc[:, ['Age', 'Deck']])\nencode_data_test.loc[:, ['Age', 'Deck']] = round(encode_data_test.loc[:, ['Age', 'Deck']])","038681eb":"# Create dummy variables\n#encode_dfs = [encode_data_train, encode_data_test]\n#for df in encode_dfs:\n    #df['Is Infant'] = df['Age'].apply(lambda x: 1 if x<3 else 0)\n    #df['Is Child'] = df['Age'].apply(lambda x: 1 if (x<11 and x>=3) else 0)\n    #df['Is Teen'] = df['Age'].apply(lambda x: 1 if (x<20 and x>=11) else 0)\n    #df['Is Adult'] = df['Age'].apply(lambda x: 1 if (x<60 and x>=20) else 0)\n    #df['Is Senior'] = df['Age'].apply(lambda x: 1 if x>=60 else 0)","4b85a8cf":"encode_data_train.head()","689bbe66":"encode_data_test.head()","6128bf2d":"# Now we can perform binning towards the Age and Fare columns \nencode_dfs = [encode_data_train, encode_data_test]\n\nfor df in encode_dfs:\n    df['Age'] = pd.qcut(df['Age'], 8)\n    df['Fare'] = pd.qcut(df['Fare'], 12)","655909d6":"# Label encode the binned columns\nfrom sklearn.preprocessing import LabelEncoder\n\nencode_cols = ['Age', 'Fare']\nfor df in encode_dfs:\n    for col in encode_cols:\n        df[col] = LabelEncoder().fit_transform(df[col])","d7efb771":"encode_data_train.skew().abs().sort_values(ascending=False)","96475e3d":"encode_data_test.skew().abs().sort_values(ascending=False)","8088d8d1":"# One hot encode the nominal categories\nfrom sklearn.preprocessing import OneHotEncoder\nnominal_col = ['Pclass', 'Family Size', 'Sex', 'Title', 'Deck']\n\ndfs = [encode_data_train, encode_data_test]\n\nencoded_features = []\nfor df in dfs:\n    for col in nominal_col:\n        encoded_feat = OneHotEncoder().fit_transform(df[col].values.reshape(-1, 1)).toarray()\n        n = df[col].nunique()\n        cols = ['{}_{}'.format(col, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([encode_data_train, *encoded_features[:5]], axis=1).drop(nominal_col, axis=1)\ndf_test = pd.concat([encode_data_test, *encoded_features[5:]], axis=1).drop(nominal_col, axis=1)","3f436cb7":"df_train.head()","61f230fe":"df_test.head()","2367a441":"# Now we fit and run a random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\ny_train = train['Survived']\nX_train = df_train.drop('Survived', axis=1)\nX_test = df_test\n\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 4, \n                                       min_samples_split = 10,\n                                       n_estimators = 1300, \n                                       max_features = 'auto', \n                                       oob_score = True, \n                                       random_state = 0, \n                                       n_jobs = -1)\n\nscores1 = cross_val_score(random_forest, X_train, y_train, cv=5)\nrandom_forest.fit(X_train, y_train)\nrf_pred = random_forest.predict(X_test)\n\n\nprint('Mean CV-5 score:', scores1.mean())\n\nrf_score = round(random_forest.score(X_train, y_train) * 100, 2)\nprint('Accuracy:', rf_score, '%')","c438a924":"#area for feature importance visual\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances","37804f2e":"output = pd.DataFrame(columns=['PassengerId', 'Survived'])\noutput['PassengerId'] = test['PassengerId']\noutput['Survived'] = rf_pred.astype(int)\noutput.to_csv('my_submission.csv', header=True, index=False)","f8a9cce0":"output.head(20)","b54a5338":"# DEALING WITH THE NULL VALUES\n\nYou may notice that there are alot of null values found in the datasets. After identifying where the null valued cells are at, depending on the nature of the data, we can use a few different ways to impute the missing cells with meaningful values. \n\nEssentially, it is not advisable to drop features with alot of missing values unless intrinsically we know that the particular feature do not pose significant importance towards the prediction.\n\n## We will deal with the Age and Deck column later on.","139e728b":"## As you can see, if we were do drop these data it means we might be at risk of losing valuable information... \n\nSo we will stick with using the easiest and best approach for this case, which is binning the values.","427aa5f3":"# EDA & FEATURE ENGINEERING\n\nI personally feel like this part of ML is the best part. It is like a *playground*. Feature engineering not only allows you to understand the dataset further, it allows you to stretch your brain and challenge your logical and critical thinking skills to generate new insights from the pre-existing features. \n\n**Some information that we can garner from the dataset:**\n1. Obtaining the title of the passengers from their names.\n2. Simplifying the cabin the passenger was in into decks.\n3. Extracting Family Sizes of the passengers using the 'SibSp' and 'Parch' columns.\n4. Women and children has a much higher survival rate than men. The age of <7 was used because it has the highest surival rate (70.21%).\n\n\n\n**Some questions that we may ask ourselves:**\n1. Is the passenger travelling alone or together with friends? Is this feature useful?\n2. We can distinguish married female passngers (by looking at their titles) but can we distinguish the married males? Is this feature useful?\n3. What is the legal marriagable age back then? There are a few married females below the age of 21. (After doing some googling, it seems like it was legal back then.)\n4. There is a good portion of survivors ~30% categorized under the 'Missing' cabin group. Could we find more information on that?","f3d5bc4c":"# MODEL TESTING, EVALUATION & SELECTION","8e8609f0":"The features created below are known as **\"*Indicator Variables*\"**. Raw data is transformed into a much more straight forward form of input hence allowing the ML model to easily recognize these features, also making the model more robust. In other words, we can also classify this as revealing insights.\n\nIn some cases, the accuracy of the model can be potentially - which makes this an essential practice for ML.","258ba09c":"* Now we can see that most of the class 1 passengers embarked from the Southampton port.\n* Most of the class 2 passengers also embarked from the Southampton port.\n* Majority of the passengers are class 3 passengers.","ff74c7a6":"# DATA LEAKAGE\nIt is still a controversy as to how we should handle data leakage for the titanic survivor predictions. For me personally, I feel that these columns will invoke data leakage if we try to extract any secondary information out of it:\n1. Ticket\n    * I have tried to extract information out of this column in hopes to distinguish passengers who are not exactly alone (with friends) but I quickly realized by doing this it will leak data into the training set. And the next approach was to split the combined dataset back into train and test sets again. But then even if it was split, the ticket frequencies will not be the same anymore and we are back to square one. So I have decided to drop this column and skip the 'With Friends' feature creation.  \n    \n2. Surname\n    * This feature was used for me to analyze whether I could gather any information for married males. Unfortunately, it can't be done. It is the same reason as above. Data will be leaked into the training set as well. The best I could come up with was to identify the passengers who are travelling with their kid(s) - logically also to include the kids into this group *(for the model to recognize these passengers quicker)*","9464cb34":"# I have tried to use one-hot encoding only on the nominal categories but the results were not as great. But I realized something strange, it is when I one-hot encode even the ordinal variables like 'Pclass' and 'Family Size', the accuracy improved quite a fair bit ~0.5%","09101c4a":"It can be quite misleading to just look at the total number of passengers alone as through a quick glance we might think that the passengers who departed fom the Southampton port may have the highest survival rate. With this information, we can also look at the passenger's classes from each port. This information might be of value to us - understanding the wealth levels of the passengers.","5953f582":"# LOADING THE DATASET","7de435d7":"# CORRELATION ANALYSIS\n\nIt seems like Pclass is negatively correlated (-0.34) to the survival of the passengers and Sex is positively correlated (0.54) to the survival of the passengers. There are also some strong correlations between Pclass & Age (-0.37), Pclass & Fare (-0.55), and Age & SipSp (-0.31). We can easily visualize the correlation using a heat map.","adfb861b":"# LOOKING FOR OUTLIERS AND HOW WE SHOULD DEAL WITH IT\n\nOutliers appear to be some sort of extreme value found in datasets. When we are dealing with outliers, we normally conduct univariate analysis or multivariate analysis. Also, as the name suggests, an univariate outlier is an extreme value on a variable and a multivariate outlier is combination of unusual values on at least two variables.\n\n1. The easiest way to visualize outliers is through box and whisker plots. We can also use some math to determine the   outliers by utilizing the **IQR** (Inter-Quartile Range) whereby values that are above the upper limit and below the lower limit will be classified as an outlier:\n    * Any value above Quantile(0.75) + IQR*1.5 (aka the upper limit)\n    * Any value below Quantile(0.25) - IQR*1.5 (aka the lower limit)\n\n\n2. We could also use the **Z-score** method but this method assumes that the distribution of the data follows a normal\/gaussian distribution. The score represents the number of standard deviations of a data point is away from the mean.\n\n3. Another method with an easy approach would be **binning** the values. The outliers will fall into their respective bins (high or low). This is a form of transformation towards the data and we dont have to drop the outliers hence the risk of losing information can be revoked.\n\n4. A more advanced approach would be to use machine learning to look out for the outliers (Isolation Forest). *This is a univariate analysis. \n\n\nNow the question is, when an outlier is detected, do we choose to drop it or not to drop it? The dataset has two continuous variables as features - Age and Fare. We will analyze both these columns to look out for any outliers and see how we can handle them to improve our model's robustness.\n\n**Another additional point to note is that we also have to look at the how we make assumptions when performing statiscal analyses and this can be split into two different kinds of approach - Parametric Distributions vs. Non-Parametric Distributions. Parametric distributions induces an assumption, i.e. Normal distribution; while Non-Parametic distributions means otherwise. ","4eefee12":"# IMPORTING THE INITIAL & NECESSARY LIBRARIES FOR PREPROCESSING","8eef299c":"# CAN WE USE ALGORITHMS TO PREDICT THE AGE AND DECK OF THE MISSING VALUES?\n\nWe can use KNN from fancyimpute for this task.\n\nSome questions that we can ask:\nIs the Age and Deck missing completely at random? Or are there any relationships between the missing cells and the other features?","b69fbfbe":"# ANALYZING THE OBSERVATIONS WITH MISSING DATA\n\nIt seems like there are quite abit of passengers grouped in the 'Missing' deck. Lets quickly analyze these group of people","16c28e94":"# PRELIMINARY EXPLORATION OF THE DATASET BEFORE ANY DATA CLEANING, PREPROCESSING AND EXPLORATORY DATA ANALYSIS (EDA)\n\nThis would allows us to understand the dataset more before we dive into making any decisions on the obstacles that we are and SHOULD expect - outliers, missing values, engaging deeper insights for better model training etc. We can list down some questions like below:\n\n1. What is the statistic summary of the dataset?\n2. Are the any missing values? If yes, how severe is it? What can we do?\n3. Are there any outliers? Do we remove them?\n4. What is the survival rate of the titanic passengers found in the dataset?\n5. Are there any correlations among the independent variables vs the dependent variables? If they are, how are they related?\n6. How are the survival rates differ among males and females?\n7. Are there any children onboard? Are there any seniors onboard? What are their survival rates like?\n8. What information can we garner from the 'SibSp' and 'Parch' columns?","d42b6302":"It is shown that the Cabin column shows the most null valued cells, followed by Age, Embarked and Fare.\nFor simplicity purposes, we can combine both datasets for preprocessing and then split it again to train and test sets before fitting our models.\n\n\n1. Fill the missing cell in Embarked with the mode value.\n\n2. Fill the missing cell in Fare with the median value (Groupby the Class the passenger was in since we do know that Fare is correlated to the Class).\n"}}