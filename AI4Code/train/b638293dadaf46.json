{"cell_type":{"c9e8e099":"code","38cb9ea0":"code","704597d6":"code","6a5303e0":"code","625f8fcb":"code","4e7c1802":"code","05639175":"code","b343f692":"code","3d193b14":"code","29c18410":"code","1cf1f695":"code","1b79baac":"code","5affbbc9":"code","2d33a1de":"code","31484969":"code","db681e8f":"code","8680f8ea":"code","811b79d3":"code","54d6c628":"code","fa523631":"code","b81a0883":"code","d284a509":"markdown","8a06d9a6":"markdown","64fdcf43":"markdown","b6fbbab4":"markdown","eb70612f":"markdown","99760c6b":"markdown","86e907f8":"markdown","12abffb7":"markdown","4c86086a":"markdown","28892253":"markdown","c4d673d2":"markdown","f69e0fed":"markdown"},"source":{"c9e8e099":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport logging\nimport tensorflow as tf\ntf.enable_eager_execution()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport unicodedata\nimport io\nimport time\nimport warnings\nimport sys\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nPATH = \"..\/input\/hindienglish-corpora\/Hindi_English_Truncated_Corpus.csv\"","38cb9ea0":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n    w = w.rstrip().strip()\n    return w\n\ndef hindi_preprocess_sentence(w):\n    w = w.rstrip().strip()\n    return w","704597d6":"def create_dataset(path=PATH):\n    lines=pd.read_csv(path,encoding='utf-8')\n    lines=lines.dropna()\n    lines = lines[lines['source']=='ted']\n    en = []\n    hd = []\n    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n        en_1.append('<end>')\n        en_1.insert(0, '<start>')\n        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n        hd_1.append('<end>')\n        hd_1.insert(0, '<start>')\n        en.append(en_1)\n        hd.append(hd_1)\n    return hd, en","6a5303e0":"def max_length(tensor):\n    return max(len(t) for t in tensor)","625f8fcb":"def tokenize(lang):\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n  return tensor, lang_tokenizer","4e7c1802":"def load_dataset(path=PATH):\n    targ_lang, inp_lang = create_dataset(path)\n    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer","05639175":"input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\nmax_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)","b343f692":"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\nprint(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))","3d193b14":"def convert(lang, tensor):\n  for t in tensor:\n    if t!=0:\n      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n    \nprint (\"Input Language; index to word mapping\")\nconvert(inp_lang, input_tensor_train[0])\nprint ()\nprint (\"Target Language; index to word mapping\")\nconvert(targ_lang, target_tensor_train[0])","29c18410":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)\/\/BATCH_SIZE\nembedding_dim = 128\nunits = 256\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","1cf1f695":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)","1b79baac":"class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    hidden_with_time_axis = tf.expand_dims(query, 1)\n    score = self.V(tf.nn.tanh(\n        self.W1(values) + self.W2(hidden_with_time_axis)))\n    attention_weights = tf.nn.softmax(score, axis=1)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    return context_vector, attention_weights","5affbbc9":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n    x = self.embedding(x)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n    output, state = self.gru(x)\n    output = tf.reshape(output, (-1, output.shape[2]))\n    x = self.fc(output)\n    return x, state, attention_weights\n\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)","2d33a1de":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n  mask = tf.cast(mask, dtype=loss_.dtype)\n#   print(type(mask))\n  loss_ *= mask\n  return tf.reduce_mean(loss_)","31484969":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","db681e8f":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n    # Teacher forcing\n    for t in range(1, targ.shape[1]):\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n      loss += loss_function(targ[:, t], predictions)\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss \/ int(targ.shape[1]))\n  variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  optimizer.apply_gradients(zip(gradients, variables))      \n  return batch_loss","8680f8ea":"EPOCHS = 20\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n    if batch % 100 == 0:\n        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                     batch,\n                                                     batch_loss.numpy()))\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss \/ steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","811b79d3":"def evaluate(sentence):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = preprocess_sentence(sentence)\n    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                           maxlen=max_length_inp,\n                                                           padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result += targ_lang.index_word[predicted_id] + ' '\n        if targ_lang.index_word[predicted_id] == '<end>':\n            return result, sentence\n        dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence","54d6c628":"def translate(sentence):\n    result, sentence = evaluate(sentence)\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))","fa523631":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","b81a0883":"translate(u'politicians do not have permission to do what needs to be done.')","d284a509":"### Decoder","8a06d9a6":"### Create Dataset\n> We are using minimal configuration as the notebbok is not focussed on metrics performance but rather the implementation.","64fdcf43":"## Encoder Decoder with Attention Model\n\n> Encoder Decoder with Attention model is a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. It uses a multilayered Gated Recurrent Unit (GRU) to map the input sequence to a vector of a fixed dimensionality, and then another deep GRU to decode the target sequence from the vector.\n<img src=\"https:\/\/www.researchgate.net\/profile\/Vlad_Zhukov2\/publication\/321210603\/figure\/fig1\/AS:642862530191361@1530281779831\/An-example-of-sequence-to-sequence-model-with-attention-Calculation-of-cross-entropy.png\" width=\"800\" alt=\"attention mechanism\">\n\n> A sequence to sequence model has two parts \u2013 an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. The input is put through an encoder model which gives us the encoder output. Here, each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. We use Bahdanau attention for the encoder.\n\n","b6fbbab4":"### Create Train and Test dataset","eb70612f":"### Encoder","99760c6b":"## Training\n\n>1. Pass *input* through *encoder* to get *encoder output*..\n>2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n>3. Decoder returns *predictions* and *decoder hidden state*.\n>4. Decoder hidden state is then passed back to model.\n>5. Predictions are used to calculate loss.\n>6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n>7. Calculate gradients and apply it to *optimizer* for backpropogation.","86e907f8":"### Optimizer","12abffb7":"### Tokenization of the data","4c86086a":"### Attention Mechanism","28892253":"### Import libraries","c4d673d2":"## The goal of this notebook is to introduce sequence to sequence language translation (seq2seq) and Attention mechanism.\nThe notebook deals with a sequence to sequence model for English to Hindi translation. After training the model one will be able to input a English sentence and get back its Hindi translation.\n\n>RNNs are also capable of doing natural language translation, aka. machine translation. It involves two RNNs, one for the source language and one for the target language. One of them is called an encoder, and the other one decoder. The reason is that, the first one encodes the sentence into a vector and the second one converts the encoded vector into a sentence in target language. The decoder is a separete RNN. Given the encoded sentence, it produces the translated sentence in target language. Attention lets the decoder to focus on specific parts of the input sentence for each output word. This helps the input and output sentences to align with one another.\n\nWe obtained the dataset used from Kaggle: https:\/\/www.kaggle.com\/aiswaryaramachandran\/hindienglish-corpora\n\n<h2> References: <\/h2>\n<li><\/a> Sequence to Sequence Learning with Neural Networks (Research Publication)<\/li>\n<li><\/a> https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention <\/li>\n<li><\/a> Using stochastic computation graphs formalism for optimization of sequence-to-sequence model (Research Publication) <\/li>\n<\/ul>","f69e0fed":"## Preprocess English and Hindi sentences"}}