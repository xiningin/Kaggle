{"cell_type":{"76bf5a6e":"code","4dbbde1d":"code","adfadd10":"code","d076f57d":"code","5d849e80":"code","08cf436f":"code","e90c2d39":"code","c4fb6182":"code","8fc96089":"code","ce482c13":"code","6b8f2e91":"code","c1708ca6":"code","6e78b6a8":"code","5aee442d":"code","8503156d":"code","ec5c786c":"code","5d9f5c00":"code","2adf10fe":"code","c623f473":"code","fe670b0e":"code","a0781be7":"code","a042c5e1":"code","e13424b8":"code","3bfdb74e":"code","111cc868":"code","a442f6fe":"code","0bf0606e":"code","92083537":"code","2dcda3fd":"code","7c6cc8e5":"code","2ec0720b":"code","294c5de9":"code","776194cb":"code","3a654d7a":"code","366cd7c5":"code","f71cb176":"code","3746f183":"code","baedf56a":"code","ef0b5785":"code","d65fca47":"code","6078b0de":"code","a20ae6b8":"code","d9905cee":"code","b4171be7":"code","a467468a":"code","8f0ebfaa":"code","441c9fa5":"code","3005d0a5":"code","4e17a9f3":"code","63b98f23":"code","a2459085":"code","fa8c4be3":"code","5fe70fb2":"code","defec56d":"code","6120708c":"code","036bae61":"code","0a9f21e8":"code","149389f8":"code","f4f8468c":"code","bc811ef8":"markdown","475b25e5":"markdown","cffbd59e":"markdown","bb12ddf8":"markdown","1ed2e303":"markdown","37b80f14":"markdown","868c42c2":"markdown","88f19951":"markdown","c4e67f13":"markdown","daf76bac":"markdown","9f5298a9":"markdown","551aca28":"markdown","189e989d":"markdown","a541e271":"markdown","a3211d84":"markdown"},"source":{"76bf5a6e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport re\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","4dbbde1d":"train_data = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/test.csv')\nsample_date = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv')\n\n#.\ub370\uc774\ud130 \ubcf5\uc81c\ud6c4 \uc870\uc791\ntrain=train_data.copy()\ntest=test_data.copy()","adfadd10":"train_data.shape","d076f57d":"train_data.columns","5d849e80":"train_data.head()","08cf436f":"\n#train=pd.read_csv(train_data, names=columns,header=None)\n#test=pd.read_csv(test_data, names=columns,header=None)\n\n\n\n\n#columns = ['age','workclass','fnlgwt','education','education_num','marital_status',\n#           'occupation','relationship','race','sex','capital_gain','capital_loss',\n#           'hours_week','country','income']\n#train=pd.read_csv(train_data, names=columns)\n#test=pd.read_csv(test_data, names=columns)","e90c2d39":"# defining function for estimating missing values in each columns\ndef missing_value(df):\n    miss=[]\n    col_list=df.columns\n    for i in col_list:\n        missing=df[i].isnull().sum()\n        miss.append(missing)\n        list_of_missing=pd.DataFrame(list(zip(col_list,miss)))\n    return list_of_missing","c4fb6182":"missing_value(test)\nmissing_value(train)","8fc96089":"train.relationship.value_counts()","ce482c13":"test.occupation.value_counts()","6b8f2e91":"print(test.shape)\nprint(train.shape)","c1708ca6":"# \ub370\uc774\ud130 \ud5e4\ub354\uac12\uc744 \uc9c0\uc6b4\ub2e4 \ntest.drop(test_data.index[0]).head()\ntrain.drop(train_data.index[0]).head()","6e78b6a8":"sum_data=[train, test]\nstr_list=[]\n\nfor data in sum_data:\n    for colname, colvalue in data.iteritems(): \n        if type(colvalue[1]) == str:\n            str_list.append(colname) \nnum_list = data.columns.difference(str_list)","5aee442d":"print(test.isnull().sum())","8503156d":"print(train.isnull().sum())","ec5c786c":"for data in sum_data:\n    for i in data.columns:\n        data[i].replace(' ?', np.nan, inplace=True)\n    data.dropna(inplace=True)","5d9f5c00":"train.isnull().sum()","2adf10fe":"for data in sum_data:\n    data['target']=data['income'].apply(lambda x: x.replace('.', ''))\n    data['target']=data['target'].apply(lambda x: x.strip())\n    data['target']=data['target'].apply(lambda x: 1 if x=='>50K' else 0)\n    data.drop(['income'], axis=1, inplace=True)","c623f473":"print(sum_data)","fe670b0e":"train.target.sum()\/len(train)","a0781be7":"#\uc815\ud615\ub370\uc774\ud130\ub4e4\uc758 \ud53c\ucc98\uc911 \uc774\uc9c4, \ubd84\ub958\ub4f1\uc758 \uc131\uaca9\uc744 \uac00\uc9c4 \ub140\uc11d\ub4e4\uc744 \uc815\uaddc\ubd84\ud3ec\ub97c \ub768\uc218 \uc788\uac8c \uc870\uc815\ud55c\ub2e4 \n \ndef bin_changevar(data, var, bins, group_names):\n    bin_value = bins\n    group = group_names\n    data[var+'cat'] = pd.cut(train[var], bin_value, labels=group)","a042c5e1":"bin_changevar(train, 'education_num', [0,6,11,16], ['Low', 'Medium', 'High'])\nbin_changevar(test, 'education_num', [0,6,11,16], ['Low', 'Medium', 'High'])","e13424b8":"pd.crosstab(train['education_numcat'],train['target'] )","3bfdb74e":"bin_changevar(train, 'hours_per_week', [0,35,40,60,100], ['Low', 'Medium', 'High','VeryHigh'])\nbin_changevar(test, 'hours_per_week', [0,35,40,60,100], ['Low', 'Medium', 'High','VeryHigh'])","111cc868":"train.columns","a442f6fe":"pd.crosstab(train['hours_per_weekcat'],train['target'], margins=True)","0bf0606e":"occu=pd.crosstab(train['occupation'],train['target'], margins=True).reset_index()","92083537":"def occup(x):\n    if re.search('managerial', x):\n        return 'Highskill'\n    elif re.search('specialty',x):\n        return 'Highskill'\n    else:\n        return 'Lowskill'","2dcda3fd":"train['occupa_cat']=train['occupation'].apply(lambda x: x.strip()).apply(lambda x: occup(x))\ntest['occupa_cat']=test['occupation'].apply(lambda x: x.strip()).apply(lambda x: occup(x))\n  ","7c6cc8e5":"train['occupa_cat'].value_counts()","2ec0720b":"bin_changevar(test, 'age', [17,30,55,100], ['Young', 'Middle_aged', 'Old'])","294c5de9":"bin_changevar(train, 'age', [17,30,55,100], ['Young', 'Middle_aged', 'Old'])","776194cb":"train['marital_statuscat']=train['marital_status'].apply(lambda x: 'married' if x.startswith('Married',1) else 'Single')\ntest['marital_statuscat']=test['marital_status'].apply(lambda x: 'married' if x.startswith('Married',1) else 'Single')","3a654d7a":"pd.crosstab(train['race'],train['target'], margins=True)","366cd7c5":"train['racecat']=train['race'].apply(lambda x: x.strip())\ntrain['racecat']=train['racecat'].apply(lambda x: 'White' if x=='White' else 'Other')\ntest['racecat']=test['race'].apply(lambda x: x.strip())\ntest['racecat']=test['racecat'].apply(lambda x: 'White' if x=='White' else 'Other')","f71cb176":"train.racecat.value_counts()","3746f183":"train.workclass.value_counts()","baedf56a":"def workclas(x):\n    if re.search('Private', x):\n        return 'Private'\n    elif re.search('Self', x):\n        return 'selfempl'\n    elif re.search('gov', x):\n        return 'gov'\n    else:\n        return 'others'","ef0b5785":"train['workclasscat']=train.workclass.apply(lambda x: x.strip()).apply(lambda x: workclas(x))\ntest['workclasscat']=test.workclass.apply(lambda x: x.strip()).apply(lambda x: workclas(x))","d65fca47":"train['workclasscat'].value_counts()\n","6078b0de":"train.head()","a20ae6b8":"#. \ud574\ub2f9 \uceec\ub7fc\ub9cc \ucd94\ucd9c \ud558\uc5ec \ubcc0\uc218\uc5d0 \ud560\ub2f9\ud55c\ub2e4.\ncov_train = pd.DataFrame(train, columns=['target'])\ncov_tesy = pd.DataFrame(test, columns=['target'])\nY_tr=cov_train\nY_te=cov_tesy","d9905cee":"train.columns","b4171be7":"# since target is already assigned I Will drop the target from the train and test along with other unnecessary variables\ntrain.drop(['education','occupation','race','education_num','age', \n            'hours_per_week', 'marital_status','target','fnlwgt','workclass', \n            'capital_gain','capital_loss', 'native_country'], axis=1, inplace=True)\ntest.drop(['education','occupation','race','education_num','age', \n           'hours_per_week', 'marital_status','workclass',\n           'fnlwgt', 'capital_gain','capital_loss', 'native_country'], axis=1, inplace=True)","a467468a":"train_set=train.copy()","8f0ebfaa":"train_set.columns","441c9fa5":"#str_list=['education', 'occupation', 'race', 'education_num', 'age',\n#       'hours_per_week', 'marital_status', 'target', 'fnlwgt', 'workclass']\n#train_set=pd.get_dummies(train, columns=str_list)","3005d0a5":"#str2_list=['id', 'relationship', 'sex', 'education_numcat', 'hours_per_weekcat',\n#       'occupa_cat', 'agecat', 'marital_statuscat', 'racecat', 'workclasscat']\n#test_set=pd.get_dummies(test, columns=str2_list)","4e17a9f3":"#train.columns ","63b98f23":"display(test_set)\n\n","a2459085":"#from sklearn.feature_selection import VarianceThreshold","fa8c4be3":"#def variance_threshold_select(df, thresh=0.0, na_replacement=-999):\n#    df1 = df.copy(deep=True)  \n#    selector = VarianceThreshold(thresh) \n#    selector.fit(df1.fillna(na_replacement))  \n#    df2 = df.loc[:,selector.get_support(indices=False)] \n#    return df2","5fe70fb2":"#df2=variance_threshold_select(train_set, thresh=.8* (1 - .8))","defec56d":"#print(df2.columns)","6120708c":"#col_tr=df2.columns # creates list of columns\n#col_te=test_set.columns # creates list of columns for test\n#X_tr=df2.values # creates array of values of features\n#X_te=test_set[col_tr].values#subseting the test dataset to get the same variable as train and","036bae61":"#len(col_tr)","0a9f21e8":"import pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n#    else:\n#        print('Confusion matrix, without normalization')\n\n#    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\ndef show_data(cm, print_res = 0):\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    if print_res == 1:\n        print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n        print('Recall (TPR) =  {:.3f}'.format(tp\/(tp+fn)))\n        print('Fallout (FPR) = {:.3e}'.format(fp\/(fp+tn)))\n    return tp\/(tp+fp), tp\/(tp+fn), fp\/(fp+tn)","149389f8":"lrn = LogisticRegression(penalty = 'l1', C = .001, class_weight='balanced')\n\nlrn.fit(X_tr, Y_tr)\ny_pred = lrn.predict(X_te)\ncm = confusion_matrix(Y_te, y_pred)\nif lrn.classes_[0] == 1:\n    cm = np.array([[cm[1,1], cm[1,0]], [cm[0,1], cm[0,0]]])\n\nplot_confusion_matrix(cm, ['0', '1'], )\npr, tpr, fpr = show_data(cm, print_res = 1);","f4f8468c":"from sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n    \nprint ('Accuracy:', accuracy_score(Y_te, y_pred))\nprint ('F1 score:', f1_score(Y_te,y_pred))","bc811ef8":"### \uad50\uc721, \uadfc\ubb34\uc2dc\uac04\uc740 \uc218\uc785\uc758 \uc8fc\uc131\ubd84\uc73c\ub85c \uac04\uc8fc \ud558\uace0 \ucc98\ub9ac\ub97c \uc9c4\ud589\ud55c\ub2e4.\n","475b25e5":"#### \ub098\uc774\uc5d0 \ub300\ud55c \uadf8\ub8f9\ud551\ucc98\ub9ac","cffbd59e":"#### \ucc98\ub9ac\ubd80....\n\n","bb12ddf8":"## 3. \ub370\uc774\ud130 \ud074\ub79c\uc9d5","1ed2e303":"#### \uc778\uc885\uc5d0 \ub300\ud55c \uad6c\ubd84","37b80f14":"#### \uc9c1\uc5c5\uad70\uc5d0\ub300\ud55c \ucc98\ub9ac","868c42c2":"#### dummies for the categorical  \uc6d0\ud56b \uc778\ucf54\ub529\ucc98\ub9ac.. \uae30\ucd08\uac00 \ub108\ubb34 \ubd80\uc871\ud558\ub2e4 \u315c\u3161\u315c","88f19951":"## 2. \ub370\uc774\ud130\uc815\uc81c","c4e67f13":"## 1. Data \uc774\ud574","daf76bac":"#### \uae30\uc220\uc5d0 \ub300\ud55c \uadf8\ub8f9\ud551\ucc98\ub9ac","9f5298a9":"### Modelling Process\n","551aca28":"* . Remind \n* . 1.Data check - feature , Null data, Outlier\n* . 2.Feature engineering - Ratio feature, Rroduct, Addition or Subtraction, New categorical\n* . - Aggregation,Categorical(one-hot,Label,built-in), Values(Numerical,Cat ..)\n* . 3.Feature selection - Use various approaches\n* . 4.Model development - Use LGBM\n* . 5.Training strategy - Ensemble is always answer\n* . 6.Stacking and Averaging\n* . 7.Tuning - Feature generation, Parameter tuning, More stacking, Hyper parameter(Grid Search, Randomized Search, Bayesian Optimization) ","189e989d":"#### \uae30\ud63c \uc5ec\ubd80\uc5d0\ub300\ud55c \ucc98\ub9ac","a541e271":"* \ub370\uc774\ud130 \uc778\uc0ac\uc774\ud2b8 \ucd94\ub860\ubcf4\n* . \ub370\uc774\ud130 \ud0d0\uc0c9 \uc18c\ub4dd\uc608\uce21 ( \uc18c\ub4dd\uc5d0 \ubcc0\ud654\ub97c \uc8fc\ub294 \uc8fc\uc131\ubd84\ubd84\uc11d\uc744 \ud1b5\ud574 \ubaa8\ub378\uc744 \uc218\ub9bd\ud55c\ub2e4)\n* . \uc8fc\uc131\ubd84\uc5d0\ub294 \ubb34\uc5c7\uc774 \uc788\uc744\uae4c?( '\uc218\uc785', \ubd80\ubaa8, \uc778\uc885, \uad50\uc721, \uc9c1\uc5c5, \uc5f0\ub839, \ucd9c\uc2e0, \uacb0\ud63c\uc0c1\ud0dc..) \uad00\uacc4\uc131\ubd84\uc11d\uc744 \ud1b5\ud574 \uceec\ub7fc, \ud589\uc758 \ub370\uc774\ud130\ub97c \uc9c0\uc6cc\uc57c\ud55c\ub2e4.\n* . \uc774\uad00\uc810\uc5d0\uc11c \ub370\uc774\ud130\ub97c \ubcf4\uc790. 50k \uadf8\ub8f9 \uc774\uc0c1\uc758 \ud2b9\uc131 \uc774\ud558\uc758 \ud2b9\uc131\uc758 \uc5f0\uad00\uad00\uacc4\ub97c \ubcf4\uace0 \uacf5\ud1b5\uc758 \uc694\uc18c\ub97c \ucc3e\uc790\n* . \ucd94\ucd9c\ub41c \uadf8\ub8f9\uac04\uc758 \uacf5\ud1b5\uc694\uc18c\ub97c \ubd84\uc11d\ud558\uace0 \uc8fc\uc131\ubd84 \ubaa8\ub378\uc744 \uc7a1\uc73c\uba74 \ubb54\uac00 \ubcf4\uc77c\ub4ef","a3211d84":"## 4. Feature Engineering \n#### Creating the target variable"}}