{"cell_type":{"c017ef17":"code","3a8b30b5":"code","0475f87f":"code","6e32fdc6":"code","84f91edc":"code","b01ceea5":"code","c7afc21c":"code","6c49b0e4":"code","9b29631b":"code","f112b1d8":"code","fb7fc53b":"code","9edd47a4":"code","fbfeb377":"code","a6697364":"code","6e13f939":"code","688f34e8":"code","8a80df4f":"code","7132987e":"code","f6d61616":"code","e88caad6":"code","88b0db3b":"code","b4d1ec8e":"code","c1c00471":"code","db7df8d0":"code","7f9c1658":"code","46e3c63b":"code","3a49098a":"code","3452e8c2":"code","47119e6d":"code","64fc073e":"code","3a5425f9":"code","8aca27ee":"code","69d7a2e9":"code","21bac893":"code","f3bcd0ce":"code","67de6381":"code","7a132e1e":"code","6e3a043d":"code","7171f588":"code","71c81281":"code","00c83a23":"code","cb0de194":"code","4effa1d3":"code","3c2582ee":"code","59d10375":"code","ba80f48b":"code","a5f916cb":"code","967d36a4":"code","352b41e4":"code","9250cbc6":"code","47372b16":"code","fb15cb77":"code","7df93491":"code","4fb031d3":"code","a42314bf":"code","819b2683":"code","8796f49d":"code","6347d909":"code","4a8d38dd":"code","0bf1f058":"code","17aff49c":"code","dbd137cc":"code","9ff7665d":"code","8d2fd0b8":"code","f9a98137":"code","fd12a95b":"code","818c0857":"code","1c91f248":"code","4efb40ea":"code","7a638fd8":"code","fd4b32b0":"code","d1b0f529":"code","94500af9":"code","480899f1":"code","63c26e4f":"code","f79c79d2":"code","8ac37c69":"code","6255f313":"code","705ea7f3":"markdown","f9349bd3":"markdown","4e3a4ce3":"markdown","09d9928b":"markdown","b4a512a8":"markdown","b87415ba":"markdown","e09f2173":"markdown","63e2c5b2":"markdown","3bc1a85e":"markdown","b61b5d87":"markdown","7f5b8f94":"markdown","04858aba":"markdown","12d7ed49":"markdown","d8051f3c":"markdown","9715aceb":"markdown","b1c618d8":"markdown","6949d777":"markdown","bddf4f4a":"markdown","9fe509e1":"markdown","c3b1f35a":"markdown","8ae10d99":"markdown","49aa9f52":"markdown","3e9ca187":"markdown","eff92a3f":"markdown","d80393c1":"markdown","9138a912":"markdown","38466dbe":"markdown","9e7c210e":"markdown","e2b173c0":"markdown","b5c082a3":"markdown","38d12d05":"markdown","2827e8dc":"markdown","0819af79":"markdown","b3f9d0b9":"markdown","e380f414":"markdown","29c34316":"markdown","89af1ab5":"markdown","b105558d":"markdown","2e782495":"markdown","a4ad43d9":"markdown","333dd827":"markdown","8a36772b":"markdown","1af92efd":"markdown","c6912615":"markdown","0c675984":"markdown","a84e8d2a":"markdown","fd9e8ade":"markdown","11874d09":"markdown","58882bd9":"markdown","4956dcb9":"markdown","a67032dc":"markdown","78a48846":"markdown","ea017a7a":"markdown","11c6d1ca":"markdown","7301c3f4":"markdown","e8487f57":"markdown","00dcebb4":"markdown","68b1db93":"markdown","4bad5327":"markdown","93252183":"markdown","a30140b1":"markdown","302a40fd":"markdown","796a7256":"markdown","98af3a59":"markdown","970e5d26":"markdown","8a6843b7":"markdown","4d77f372":"markdown","939839b5":"markdown","c935e851":"markdown","8b8c6eb1":"markdown","8de9a2e0":"markdown"},"source":{"c017ef17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom transformers import AutoTokenizer,BertTokenizer,TFBertModel,TFOpenAIGPTModel,OpenAIGPTTokenizer,DistilBertTokenizer, TFDistilBertModel,XLMTokenizer, TFXLMModel\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import roc_curve,confusion_matrix,auc\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib as mpl\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.initializers import Constant\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3a8b30b5":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n","0475f87f":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n\nvalid_raw = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest_raw = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","6e32fdc6":"# Combine train1 with a subset of train2\ntrain_raw = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n])","84f91edc":"valid_raw['toxic'].value_counts().plot(kind='bar')","b01ceea5":"train_raw['toxic'].value_counts().plot(kind='bar')","c7afc21c":"neg, pos = np.bincount(train_raw['toxic'])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos \/ total))","6c49b0e4":"# First load the real tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","9b29631b":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nEPOCHS=2\nLEARNING_RATE=1e-5\nearly_stopping=early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)\nAUTO = tf.data.experimental.AUTOTUNE\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\nmax_seq_length = 192\n","f112b1d8":"\ndef single_encoding_function(text,tokenizer,name='BERT'):\n    input_ids=[]\n    if name=='BERT':\n        tokenizer.pad_token ='[PAD]'\n    elif name=='OPENAIGPT2':\n        tokenizer.pad_token='<unk>'\n    elif name=='Transformer XL':\n        print(tokenizer.eos_token)\n        tokenizer.pad_token= tokenizer.eos_token\n    elif name=='DistilBert':\n        tokenizer.pad_token='[PAD]'\n    \n    for sentence in tqdm(text):\n        encoded=tokenizer.encode(sentence,max_length=max_seq_length,pad_to_max_length=True)\n        input_ids.append(encoded)\n    return input_ids\n","fb7fc53b":"X_train=np.array(single_encoding_function(train_raw['comment_text'].values.tolist(),tokenizer,name=\"BERT\"))\ny_train=np.array(train_raw['toxic'])\nX_valid=np.array(single_encoding_function(valid_raw['comment_text'].values.tolist(),tokenizer,name=\"BERT\"))\ny_valid=np.array(valid_raw['toxic'])\nX_test=np.array(single_encoding_function(test_raw['content'].values.tolist(),tokenizer,name=\"BERT\"))","9edd47a4":"steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE","fbfeb377":"def make_data():\n    train = (\n        tf.data.Dataset\n        .from_tensor_slices((X_train, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO))\n\n    valid = (\n        tf.data.Dataset\n        .from_tensor_slices((X_valid, y_valid))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n\n    test = (\n        tf.data.Dataset\n        .from_tensor_slices(X_test)\n        .batch(BATCH_SIZE)\n    )\n    return train,valid,test","a6697364":"train,valid,test=make_data()","6e13f939":"def build_model(transformer_layer,max_len=max_seq_length):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer_layer(input_word_ids)[0]\n    \n    cls_token = sequence_output[:, 0, :]\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n    \n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    \n    \n    return model","688f34e8":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\ndef plot_loss(history):\n# Use a log scale to show the wide range of values.\n    plt.semilogy(history.epoch,  history.history['loss'],\n               color='red', label='Train Loss')\n    plt.semilogy(history.epoch,  history.history['val_loss'],\n          color='green', label='Val Loss',\n          linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n  \n    plt.legend()\n    \n    \ndef plot_metrics(history):\n    metrics =  ['loss', 'auc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],\n                 color=colors[0], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n\n        plt.legend()\n\ndef plot_cm(y_true, y_pred, title):\n    ''''\n    input y_true-Ground Truth Labels\n          y_pred-Predicted Value of Model\n          title-What Title to give to the confusion matrix\n    \n    Draws a Confusion Matrix for better understanding of how the model is working\n    \n    return None\n    \n    '''\n    \n    figsize=(10,10)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n\ndef roc_curve_plot(fpr,tpr,roc_auc):\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' %roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n        ","8a80df4f":"%%time\ndef compile_model(name):\n    with strategy.scope():\n        METRICS = [\n          tf.keras.metrics.TruePositives(name='tp'),\n          tf.keras.metrics.FalsePositives(name='fp'),\n          tf.keras.metrics.TrueNegatives(name='tn'),\n          tf.keras.metrics.FalseNegatives(name='fn'), \n          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n          tf.keras.metrics.Precision(name='precision'),\n          tf.keras.metrics.Recall(name='recall'),\n          tf.keras.metrics.AUC(name='auc')]\n        if name=='bert-base-uncased':\n            transformer_layer = (\n                TFBertModel.from_pretrained(name)\n            )\n        elif name=='openai-gpt':\n            transformer_layer = (\n                TFOpenAIGPTModel.from_pretrained(name)\n            )\n        elif name=='distilbert-base-cased':\n            transformer_layer = (\n                TFDistilBertModel.from_pretrained(name)\n            )\n        elif name=='xlm-mlm-en-2048':\n            transformer_layer = (\n                TFBertModel.from_pretrained(name)\n            )\n        elif name=='jplu\/tf-xlm-roberta-large':\n            transformer_layer = (\n                TFAutoModel.from_pretrained(name)\n            )\n        model = build_model(transformer_layer, max_len=max_seq_length)\n        model.compile(optimizer=tf.keras.optimizers.Adam(\n        learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=METRICS)\n    return model\n","7132987e":"steps_per_epoch=X_train.shape[0]\/\/BATCH_SIZE\n\nmodel=compile_model('bert-base-uncased')\nprint(model.summary())\nhistory=model.fit(\n    train,steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid\n)","f6d61616":"plot_loss(history)","e88caad6":"plot_metrics(history)","88b0db3b":"y_predict=model.predict(valid, verbose=1)\ny_predict[ y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'BERT-Confusion Matrix')\n","b4d1ec8e":"y_predict_prob=model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","c1c00471":"# # First load the real tokenizer\ntokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')","db7df8d0":"X_train=np.array(single_encoding_function(train_raw['comment_text'],tokenizer,'OPENAIGPT2'))\ny_train=np.array(train_raw['toxic'])\nX_valid=np.array(single_encoding_function(valid_raw['comment_text'],tokenizer,'OPENAIGPT2'))\ny_valid=np.array(valid_raw['toxic'])\nX_test=np.array(single_encoding_function(test_raw['content'],tokenizer,'OPENAIGPT2'))","7f9c1658":"steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE","46e3c63b":"train,valid,test=make_data()","3a49098a":"\nmodel=compile_model('openai-gpt')\nprint(model.summary())\n\nhistory=model.fit(\n    train,steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid\n)","3452e8c2":"plot_loss(history)","47119e6d":"plot_metrics(history)","64fc073e":"y_predict=model.predict(valid, verbose=1)\ny_predict[ y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'OpenAIGPT-Confusion Matrix')","3a5425f9":"y_predict_prob=model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","8aca27ee":"# # First load the real tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')","69d7a2e9":"X_train=np.array(single_encoding_function(train_raw['comment_text'],tokenizer,'DistilBert'))\ny_train=np.array(train_raw['toxic'])\nX_valid=np.array(single_encoding_function(valid_raw['comment_text'],tokenizer,'DistilBert'))\ny_valid=np.array(valid_raw['toxic'])\nX_test=np.array(single_encoding_function(test_raw['content'],tokenizer,'DistilBert'))","21bac893":"train,valid,test=make_data()","f3bcd0ce":"steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE","67de6381":"\nmodel=compile_model('distilbert-base-cased')\nprint(model.summary())\n\nhistory=model.fit(\n    train,steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid\n)","7a132e1e":"plot_loss(history)","6e3a043d":"plot_metrics(history)","7171f588":"y_predict=model.predict(valid, verbose=1)\ny_predict[ y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'Transformer XL Performance-Confusion Matrix')","71c81281":"y_predict_prob=model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","00c83a23":"# # First load the real tokenizer\ntokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')","cb0de194":"X_train=np.array(single_encoding_function(train_raw['comment_text'],tokenizer,'XLM'))\ny_train=np.array(train_raw['toxic'])\nX_valid=np.array(single_encoding_function(valid_raw['comment_text'],tokenizer,'XLM'))\ny_valid=np.array(valid_raw['toxic'])\nX_test=np.array(single_encoding_function(test_raw['content'],tokenizer,'XLM'))\n","4effa1d3":"train,valid,test=make_data()","3c2582ee":"\nmodel=compile_model('xlm-mlm-en-2048')\nprint(model.summary())\n\nhistory=model.fit(\n    train,steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid\n)","59d10375":"plot_loss(history)","ba80f48b":"plot_metrics(history)","a5f916cb":"y_predict=model.predict(valid, verbose=1)\ny_predict[ y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'XLM-Confusion Matrix')","967d36a4":"y_predict_prob=model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","352b41e4":"from IPython.display import YouTubeVideo\n\nYouTubeVideo(\"Ot6A3UFY72c\", width=800, height=300)\n\n\n","9250cbc6":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","47372b16":"tokenizer = AutoTokenizer.from_pretrained('jplu\/tf-xlm-roberta-large')","fb15cb77":"%%time \n\nX_train = regular_encode(train_raw.comment_text.values, tokenizer, maxlen=max_seq_length)\nX_valid = regular_encode(valid_raw.comment_text.values, tokenizer, maxlen=max_seq_length)\nX_test = regular_encode(test_raw.content.values, tokenizer, maxlen=max_seq_length)\n\ny_train = train_raw.toxic.values\ny_valid = valid_raw.toxic.values","7df93491":"steps_per_epoch = X_train.shape[0] \/\/ BATCH_SIZE","4fb031d3":"train,valid,test=make_data()","a42314bf":"\nfinal_model=compile_model('jplu\/tf-xlm-roberta-large')\nprint(final_model.summary())\n\nhistory=final_model.fit(\n    train,steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid\n)","819b2683":"plot_loss(history)","8796f49d":"plot_metrics(history)","6347d909":"y_predict=final_model.predict(valid, verbose=1)\ny_predict[ y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'XLM-Roberta-Confusion Matrix')","4a8d38dd":"y_predict_prob=final_model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","0bf1f058":"steps_per_epoch = X_valid.shape[0] \/\/ BATCH_SIZE\ntrain_history_2 = final_model.fit(\n    valid.repeat(),\n    steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS\n)","17aff49c":"max_seq_length = 512\nembedding_dim=300\nBATCH_SIZE = 32\nEPOCHS=1\nLEARNING_RATE=1e-5\nearly_stopping=early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    verbose=1,\n    patience=2,\n    mode='max',\n    restore_best_weights=True)\nAUTO = tf.data.experimental.AUTOTUNE","dbd137cc":"\ntokenizer = Tokenizer(split=' ', oov_token='<unw>', filters=' ')\ntokenizer.fit_on_texts(train_raw['comment_text'].values)\n\n# this takes our sentences and replaces each word with an integer\nX_train = tokenizer.texts_to_sequences(train_raw['comment_text'].values)\nX_train=np.array(pad_sequences(X_train, max_seq_length))\ny_train=np.array(train_raw['toxic'])\n\nX_valid = tokenizer.texts_to_sequences(valid_raw['comment_text'].values)\nX_valid = np.array(pad_sequences(X_valid, max_seq_length))\ny_valid=np.array(valid_raw['toxic'])\n\nX_test = tokenizer.texts_to_sequences(test_raw['content'].values)\nX_test = np.array(pad_sequences(X_test, max_seq_length))\n\n","9ff7665d":"print('The X_train,X_valid,X_test shape respectively is {}-{}-{}'.format(X_train.shape,X_valid.shape,X_test.shape))","8d2fd0b8":"train,valid,test=make_data()","f9a98137":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","fd12a95b":"embeddings_index = {}\n\nf = open(os.path.join(os.getcwd(), 'glove.6B.{}d.txt'.format(str(embedding_dim))))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","818c0857":"# first create a matrix of zeros, this is our embedding matrix\nword_index = tokenizer.word_index\nnum_words=len(word_index)+1\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\n# for each word in out tokenizer lets try to find that work in our w2v model\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # we found the word - add that words vector to the matrix\n        embedding_matrix[i] = embedding_vector\n    else:\n        # doesn't exist, assign a random vector\n        embedding_matrix[i] = np.random.randn(embedding_dim)","1c91f248":"steps_per_epoch=X_train.shape[0]\/\/BATCH_SIZE\nwith strategy.scope():\n    model = Sequential()\n\n    model.add(Embedding(num_words,\n                        embedding_dim,\n                        embeddings_initializer=Constant(embedding_matrix),\n                        input_length=max_seq_length,\n                        trainable=True))\n    model.add(SpatialDropout1D(0.2))\n    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n    model.add(Bidirectional(LSTM(32)))\n    model.add(Dropout(0.25))\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    METRICS = [\n              tf.keras.metrics.TruePositives(name='tp'),\n              tf.keras.metrics.FalsePositives(name='fp'),\n              tf.keras.metrics.TrueNegatives(name='tn'),\n              tf.keras.metrics.FalseNegatives(name='fn'), \n              tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n              tf.keras.metrics.Precision(name='precision'),\n              tf.keras.metrics.Recall(name='recall'),\n              tf.keras.metrics.AUC(name='auc')]\n    model.compile(loss ='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(\n            learning_rate=LEARNING_RATE),metrics =METRICS)\n\nhistory=model.fit(\ntrain,steps_per_epoch=steps_per_epoch,epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid)","4efb40ea":"plot_loss(history)","7a638fd8":"plot_metrics(history)","fd4b32b0":"y_predict=model.predict(valid, verbose=1)\ny_predict[y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'LSTM with Glovec-Confusion Matrix')","d1b0f529":"y_predict_prob=model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","94500af9":"with strategy.scope():\n    # Define an input sequence and process it.\n    inputs = Input(shape=(max_seq_length,))\n    embedding=Embedding(num_words,\n                        embedding_dim,\n                        embeddings_initializer=Constant(embedding_matrix),\n                        input_length=max_seq_length,\n                        trainable=True)(inputs)\n    \n\n    # Apply dropout to prevent overfitting\n    embedded_inputs = Dropout(0.2)(embedding)\n    \n    # Apply Bidirectional LSTM over embedded inputs\n    lstm_outs =Bidirectional(\n        LSTM(300, return_sequences=True)\n    )(embedded_inputs)\n    \n    # Apply dropout to LSTM outputs to prevent overfitting\n    lstm_outs = Dropout(0.2)(lstm_outs)\n    \n    # Attention Mechanism - Generate attention vectors\n    attention_vector = TimeDistributed(Dense(1))(lstm_outs)\n    attention_vector = Reshape((X_train.shape[1],))(attention_vector)\n    attention_vector = Activation('softmax', name='attention_vec')(attention_vector)\n    attention_output = Dot(axes=1)([lstm_outs, attention_vector])\n    \n    # Last layer: fully connected with softmax activation\n    fc = Dense(300, activation='relu')(attention_output)\n    output = Dense(1, activation='sigmoid')(fc)\n    \n    model=tf.keras.Model(inputs,output)\n    \n    \n    METRICS = [\n              tf.keras.metrics.TruePositives(name='tp'),\n              tf.keras.metrics.FalsePositives(name='fp'),\n              tf.keras.metrics.TrueNegatives(name='tn'),\n              tf.keras.metrics.FalseNegatives(name='fn'), \n              tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n              tf.keras.metrics.Precision(name='precision'),\n              tf.keras.metrics.Recall(name='recall'),\n              tf.keras.metrics.AUC(name='auc')]\n    model.compile(loss ='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(\n            learning_rate=LEARNING_RATE),metrics =METRICS)\n    print(model.summary())\n\nhistory=model.fit(\ntrain,steps_per_epoch=steps_per_epoch,epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid)","480899f1":"plot_loss(history)","63c26e4f":"plot_metrics(history)","f79c79d2":"y_predict=model.predict(valid, verbose=1)\ny_predict[ y_predict> 0.5] = 1\ny_predict[y_predict <= 0.5] = 0\nplot_cm(y_valid, y_predict, 'LSTM with Attention Mechanism-Confusion Matrix')","8ac37c69":"fy_predict_prob=model.predict(valid, verbose=1)\nfpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\nroc_auc = auc(fpr, tpr)\nroc_curve_plot(fpr,tpr,roc_auc)","6255f313":"sub['toxic'] = final_model.predict(test, verbose=1)\nsub.to_csv('submission.csv', index=False)","705ea7f3":"## OpenAIGPT Performance-Loss","f9349bd3":"### Useful links to understand Choosen Metrics:\n<p1> These are the link which can help you to understand the metrics which I have used to compare the performance of different models<\/p1>\n<ui>\n    <li>[Confusion Matrix](https:\/\/www.coursera.org\/lecture\/python-machine-learning\/confusion-matrices-basic-evaluation-metrics-90kLk)<\/li>\n    <li>[ROC,Precision,Recall,AUC](https:\/\/www.coursera.org\/lecture\/python-machine-learning\/precision-recall-and-roc-curves-8v6DL)<\/li>\n<\/ui>\n    ","4e3a4ce3":"## Usefull Links About XLM-Roberta-Large\n<p1> The following links are quite useful if you to further your knowledge about XLM-Roberta-Large<\/p1>\n<ui>\n    <li>[xlm-roberta-the-multilingual-alternative](https:\/\/towardsdatascience.com\/xlm-roberta-the-multilingual-alternative-for-non-english-nlp-cf0b889ccbbf) <\/li>\n    <li>[Github-XLMR](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/xlmr) <\/li>\n    <li>[bert-roberta-distilbert-xlnet-one-us](https:\/\/www.kdnuggets.com\/2019\/09\/bert-roberta-distilbert-xlnet-one-use.html)<\/li>\n    <li>[Explaining Roberta Technique](https:\/\/arxiv.org\/pdf\/1907.11692.pdf)<\/li>\n    <li>[FaceBook AI post](https:\/\/ai.facebook.com\/blog\/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision\/)\n    <li>[Orginal Paper](https:\/\/arxiv.org\/pdf\/1911.02116.pdf)<\/li>\n<\/ui>","09d9928b":"## Usefull Links About XLM\n<p1> The following links are quite useful if you to further your knowledge about XLM<\/p1>\n<ui>\n    <li>[xlm-enhancing-bert-for-cross-lingual-language-model](https:\/\/towardsdatascience.com\/xlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b#:~:text=Background,(or%20sub%2Dwords).)<\/li>\n    <li>[GitHub XLM](https:\/\/github.com\/facebookresearch\/XLM) <\/li>\n    <li>[a-deep-dive-into-multilingual-nlp-models](https:\/\/peltarion.com\/blog\/data-science\/a-deep-dive-into-multilingual-nlp-models)<\/li>\n    <li>[Orginal Paper](a-deep-dive-into-multilingual-nlp-models)<\/li>\n<\/ui>","b4a512a8":"## XLM-Confusion Matrix","b87415ba":"![](https:\/\/jalammar.github.io\/images\/distilBERT\/bert-distilbert-tutorial-sentence-embedding.png)","e09f2173":"![](https:\/\/user-images.githubusercontent.com\/10358317\/43802664-22c08c8a-9a9f-11e8-83e1-fea4bf334f6e.png)","63e2c5b2":"## LSTM with Glovec Embedding Loss","3bc1a85e":"## LSTM with Attention Mechanism -Loss","b61b5d87":"## Model Compilation Under TPUs","7f5b8f94":"![](https:\/\/www.mdpi.com\/sensors\/sensors-19-00861\/article_deploy\/html\/images\/sensors-19-00861-g004.png)","04858aba":"## LSTM with Attention Mechanism-Confusion Matrix","12d7ed49":"## Glovec Embedding","d8051f3c":"# 3-DistilBert ","9715aceb":"# 5-XLM-Roberta-large -the Choosen one :D","b1c618d8":"## Bert Performance-Metrics","6949d777":"## TPU Distribution Strategy for Efficent Model Training","bddf4f4a":"# 1. BERT","9fe509e1":"## XLM-Roberta-large-ROC Curve","c3b1f35a":"# 7-**LSTM with attention Mechanism**","8ae10d99":"## LSTM with Attention Mechanism-Metrics","49aa9f52":"## Usefull Links About DistilBERT\n<p1> The following links are quite useful if you to further your knowledge about Transformer-XL<\/p1>\n<ui>\n    <li>[distilbert](https:\/\/medium.com\/huggingface\/distilbert-8cf3380435b5) <\/li>\n    <li>[a-visual-guide-to-using-dsitillbert-for-the-first-time](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) <\/li> \n    <li>[Orginal Paper](https:\/\/arxiv.org\/pdf\/1910.01108.pdf)<\/li>\n<\/ui>","3e9ca187":"<img src='https:\/\/www.etcentric.org\/wp-content\/uploads\/2018\/07\/FAIR_Facebook_AI_Research.jpg'>","eff92a3f":"## DataPipeline for LSTMs","d80393c1":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..K5ST4JjFGBMeTtwX1jC9XQ.0KxxSju7AC0c3tRP2w-rTijGPiYJGNZnjThGQx-BG3govElMkJpIJw6gcpAozeKQSH_Q3_Cjh6a38SnweVyY3X6eW1YMl8UvsJNUsQnuKGTTig69_hUGyoPmepANHm4vZnwUN3vljN9RmyRVaXuvHTkgGCs6KASszIFNgE1v309GpSOtQy7Um8_75fzG-szb7mNmMBlkZ_vBs4-4f-Wmf_8JwkQit6kgDM9qXzE2pEq0btxta2CuVyCYUkJ-qV6aCQprqSy68UTOjD6tdwlASMi9hOLyeogQrSoHLzPWgf_Xg3Pm6dwwjrFwDw2MpFoT47szD5MaXTLoT6cxUAdIRZVNMHed7HsC9r-PDwIdDiZKy8Sss5DUrj3jhE02_HCx9cJLYsaLIvPwJVrfKb7aYvaULHhRw4rLaHtxo4r0B7bKLTGkkgkWUmXVv7yDViYdk48ClVic05ZpfjUmpDo9TcD1s7mUCOcoLdqJh7U8UXbBqGBMHpVD95MOFpwAyTC9mnvDuiIiWVxRr8buuC_QnjRITs4CPYhrFG_-5zXC5B4KDUG23AcbCwK7IkSW4PKnufdJHlgKq3g9nNJUcvgE_mB10rGX2YGzJYpIFIegR6HjzR9UuWLIvm7cxOxWTjUSd-8lzFPNFfWyGWgXsyvdtEVa9mkAKJ63A12oFLIou9Gj1L3wMFXBTFsY3n8EB29h.vwq3hDsu0b8kXlx_-yqfkQ\/__results___files\/__results___52_1.png'>","9138a912":"![](https:\/\/i.pinimg.com\/564x\/88\/66\/9b\/88669b6455bde00c747fd9535e7ee98e.jpg)","38466dbe":"## BERT Performance-Confusion Matrix","9e7c210e":"## DistilBert  XL Performance-Metrics","e2b173c0":"## LSTM with Glovec Embedding-ROC","b5c082a3":"## XLM-Roberta-large-Metrics","38d12d05":"![BERT](https:\/\/www.researchgate.net\/profile\/Wei_Shi102\/publication\/336995759\/figure\/fig1\/AS:824051559825408@1573480610178\/The-architecture-from-BERT-Devlin-et-al-2019-for-fine-tuning-of-implicit-discourse.ppm)","2827e8dc":"## BERT Performance-Loss","0819af79":"## Making TF Model for Different Transformers","b3f9d0b9":"## OpenAIGPT-ROC","e380f414":"## Usefull Links About LSTM\n<p1> The following links are quite useful if you to further your knowledge about LSTMs<\/p1>\n<ui>\n    <li>[Understanding-LSTMs](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)<\/li>\n    <li>[illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation](https:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)<\/li>\n    <li>[Andrew Ng Explaining LSTM](https:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/long-short-term-memory-lstm-KXoay)<\/li>\n    <li>[Orginal Paper](https:\/\/www.bioinf.jku.at\/publications\/older\/2604.pdf)<\/li>\n<\/ui>\n<\/p1>\n\n<p2> The following links are quite useful if you want to understand about Glovec <\/p2>\n<ui>\n    <li>[CS224n Notes](http:\/\/web.stanford.edu\/class\/cs224n\/readings\/cs224n-2019-notes02-wordvecs2.pdf)<\/li>\n    <li>[Anrew Ng Explaining Glovec](https:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/glove-word-vectors-IxDTG)<\/li>\n    <li>[CS22n Video](https:\/\/www.youtube.com\/watch?v=ASn7ExxLZws&t=1s)<\/li>\n    <li>[Orginal Paper](http:\/\/nlp.stanford.edu\/pubs\/glove.pdf)<\/li>\n<\/ui>\n<\/p2>","29c34316":"## XLM-Roberta-large-Confusion Matrix","89af1ab5":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..K5ST4JjFGBMeTtwX1jC9XQ.0KxxSju7AC0c3tRP2w-rTijGPiYJGNZnjThGQx-BG3govElMkJpIJw6gcpAozeKQSH_Q3_Cjh6a38SnweVyY3X6eW1YMl8UvsJNUsQnuKGTTig69_hUGyoPmepANHm4vZnwUN3vljN9RmyRVaXuvHTkgGCs6KASszIFNgE1v309GpSOtQy7Um8_75fzG-szb7mNmMBlkZ_vBs4-4f-Wmf_8JwkQit6kgDM9qXzE2pEq0btxta2CuVyCYUkJ-qV6aCQprqSy68UTOjD6tdwlASMi9hOLyeogQrSoHLzPWgf_Xg3Pm6dwwjrFwDw2MpFoT47szD5MaXTLoT6cxUAdIRZVNMHed7HsC9r-PDwIdDiZKy8Sss5DUrj3jhE02_HCx9cJLYsaLIvPwJVrfKb7aYvaULHhRw4rLaHtxo4r0B7bKLTGkkgkWUmXVv7yDViYdk48ClVic05ZpfjUmpDo9TcD1s7mUCOcoLdqJh7U8UXbBqGBMHpVD95MOFpwAyTC9mnvDuiIiWVxRr8buuC_QnjRITs4CPYhrFG_-5zXC5B4KDUG23AcbCwK7IkSW4PKnufdJHlgKq3g9nNJUcvgE_mB10rGX2YGzJYpIFIegR6HjzR9UuWLIvm7cxOxWTjUSd-8lzFPNFfWyGWgXsyvdtEVa9mkAKJ63A12oFLIou9Gj1L3wMFXBTFsY3n8EB29h.vwq3hDsu0b8kXlx_-yqfkQ\/__results___files\/__results___50_0.png'>","b105558d":"## LSTM with Glovec Embedding-Metrics","2e782495":"## LSTM with Attention Mechanism-ROC","a4ad43d9":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..8pFTexbxmoqrx0OFHZ_YFw.qJ_XoPOQxUJaJIyQCzZNDB5Rs-ATsNEA5ZhM9mB5xxiYm-VfPhTiXbJNb8ocdZP9zzurfVgGID4gkAVkdl09D3jx1naobYVbt5me1OmtbWTDUdVKo-hmeMtM3Qf7vKbDl92GRz67Z97b6ghs7pG_9YxQWh5se171PcAuDFsBs4fzwZ_jZ1sw8g4LYOftybCvwmAh3yfLlY2KkV6RkDRgix3PPS6Qxpc2iy91GRDskkwKX2GDcGSycMAccPOuelzVBs-1gQEnuctTXiBLBiLbM1eIcxYbhKtmZn3mqjU4nPv-2qFTh4UKjIOnMSVg_dwtlxZ4I858gd1a0wI4We43kGi0lORZ0CwJDJgemHAcnkcjPYLllJyqwj2pL5zrEwexAf-3oR0DoDZ952DD1xlVR_K5vtt_P2aFybGiTfzSjAfnWpVb-2-QUIF6Esbz25MYXuZo2Trk7HqDt3ApBPMqzdvBjRu5MTcF-cFtkBKIWcBWqYZcTB5IDcmy9PWVDSyKcLksJNHiPPB3Ei3LuE0Z8GfKUGPyBXRe1PYAHZg27QvnmvxoOIi-epUuUoCN2GRm6p6Ekyg-RRb5RXCmJU41UF2h7JxBKPoGmlVvlYMjnrn4vZatLbWVbVS7GBpkWHYb6IIIkqaYFDXICaFgVAIu2C6_sPf2-LQUXNDkOYPYYCKN_FHlw1paRnPDMNRs0J4T.5IAqYeDNXIJujd6C8JCl2Q\/__results___files\/__results___32_0.png'>","333dd827":"### Note:\n<p1> These are four function whic will help me to compare the performance of different models. The first two functions being used will plot loss and other choosen metrics over the number of epochs<\/p1>\n\n<p2>The last two function will plot confusion matrix and roc for the models respectively<\/p2>","8a36772b":"## OpenAIGPT Performance-Metrics","1af92efd":"## XLM Performance-Metrics","c6912615":"## BERT Performance-ROC Curve","0c675984":"## XLM-Roberta-large-Loss","a84e8d2a":"## DistilBert  Performance-Confusion Matrix","fd9e8ade":"# Final Submission","11874d09":"# Hyperparameters-Transformers","58882bd9":"## Making Tensor Data Pipeline","4956dcb9":"## Credits:\n<p1> I have used some help from other notebooks, therefore I would like to give the credit to desire individuals:\n    <ul>\n        <li> 1. [xhlulu](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras) <\/li>\n        <li> 2. [tanulsingh077](https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert\/notebook) <\/li>\n    <\/ul>\n","a67032dc":"# 4-XLM","78a48846":"## DistilBert Performance-Loss","ea017a7a":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..K5ST4JjFGBMeTtwX1jC9XQ.0KxxSju7AC0c3tRP2w-rTijGPiYJGNZnjThGQx-BG3govElMkJpIJw6gcpAozeKQSH_Q3_Cjh6a38SnweVyY3X6eW1YMl8UvsJNUsQnuKGTTig69_hUGyoPmepANHm4vZnwUN3vljN9RmyRVaXuvHTkgGCs6KASszIFNgE1v309GpSOtQy7Um8_75fzG-szb7mNmMBlkZ_vBs4-4f-Wmf_8JwkQit6kgDM9qXzE2pEq0btxta2CuVyCYUkJ-qV6aCQprqSy68UTOjD6tdwlASMi9hOLyeogQrSoHLzPWgf_Xg3Pm6dwwjrFwDw2MpFoT47szD5MaXTLoT6cxUAdIRZVNMHed7HsC9r-PDwIdDiZKy8Sss5DUrj3jhE02_HCx9cJLYsaLIvPwJVrfKb7aYvaULHhRw4rLaHtxo4r0B7bKLTGkkgkWUmXVv7yDViYdk48ClVic05ZpfjUmpDo9TcD1s7mUCOcoLdqJh7U8UXbBqGBMHpVD95MOFpwAyTC9mnvDuiIiWVxRr8buuC_QnjRITs4CPYhrFG_-5zXC5B4KDUG23AcbCwK7IkSW4PKnufdJHlgKq3g9nNJUcvgE_mB10rGX2YGzJYpIFIegR6HjzR9UuWLIvm7cxOxWTjUSd-8lzFPNFfWyGWgXsyvdtEVa9mkAKJ63A12oFLIou9Gj1L3wMFXBTFsY3n8EB29h.vwq3hDsu0b8kXlx_-yqfkQ\/__results___files\/__results___36_1.png'>","11c6d1ca":"## Usefull Links About LSTM with Attention\n<p1> The following links are quite useful if you to further your knowledge about LSTMs<\/p1>\n<ui>\n    <li>[comprehensive-guide-attention-mechanism-deep-learning](https:\/\/www.analyticsvidhya.com\/blog\/2019\/11\/comprehensive-guide-attention-mechanism-deep-learning\/)<\/li>\n    <li>[attention-long-short-term-memory-recurrent-neural-networks](https:\/\/machinelearningmastery.com\/attention-long-short-term-memory-recurrent-neural-networks\/)<\/li>\n    <li>[Andrew Ng Explaining LSTM with Attention](https:\/\/www.youtube.com\/watch?v=SysgYptB198)<\/li>\n    <li>[Orginal Paper](https:\/\/www.aclweb.org\/anthology\/D16-1058.pdf)<\/li>\n<\/ui>","7301c3f4":"# **Main Purpose of the NoteBook**\n<p1> The purpose of this notebook is really simple, I was curious as how different Transformers Architecture will perform over the given data. This notebook is really simple to understand, there is no fancy EDA or any data exploration. **The sole purpose of this notebook is to just compare different Transformers Architecture. I will be judging the Model performance over the accuracy, AUC,recall and precision**<p1>\n\n<p2> NoteBook also can be used as a introduction to as how tensorflow keras can be integrated with hugging face. \nI have tried to make this notebook quite user friendly and easy to understand so that every one can understand the basic of hugging face <\/p2>\n\n![](https:\/\/huggingface.co\/front\/thumbnails\/models.png)\n\n<p3>The following Transformers Architecture have been tested in the notebook<\/p3>\n\n### 1. BERT\n### 2. OpenAIGPT\n### 3. Transformer XL\n### 4. XLM\n### 5. XLM-Roberta-Large\n\n\n![](https:\/\/media-exp1.licdn.com\/dms\/image\/C5112AQHdCkQOA8sjlQ\/article-cover_image-shrink_600_2000\/0?e=1596067200&v=beta&t=aXNCLbGrTOsjxeWZkzx5ksTNThTUFVaEBPl1_vsf6K8)\n    \n<p4> However I also later on added some classical LSTM models with and without attention mechanims in order to  see its performance on the dataset. The same metric system will be used to judge the performance of LSTMs,The following two type of LSTM models have been used <\/p4>\n    \n    \n  \n### 6.LSTM with Glovec Embedding\n### 7.LSTM with Attention Mechanism\n \n<p5>I also have given useful links for each of the model used so that you can further expand your knowledge<\/p5>\n    \n    \n<font color='red'>This is my first notebook and I have tried to put into a lot of effort. Please upvote, it will really encourage me to make better and more helpful notebooks in future<\/font>   \n\n    \n<font color='read'> The notebook takes much time to run therefore I cannot run the entire notebook in one go. Therefore, I have ran notebook sequentailly and attached the output of each model performance in the markdown. If you want to check any model just run the relevant section while also executing the functions which are common to all models. Again, the purpose of notebook is to show how without any substaintial hyperparameter tuning different transformers and attention mechanism LSTMs perform on the task<\/font>","e8487f57":"## Hyperparameter for LSTMs ","00dcebb4":"## Youtube Video-Explaining XLM-Roberta-Large!!!-Really Useful","68b1db93":"## Usefull Links About OPENAI-GPT\n<p1> The following links are quite useful if you to further your knowledge about OPENAI-GPT(Please note most of these resources are related to open-ai GPT-2 but are sufficent to understand to orginal open-ai-gpt as well<\/p1>\n<ui>\n    <li>[illustrated-gpt2](http:\/\/jalammar.github.io\/illustrated-gpt2\/) <\/li>\n    <li>[openai-gpt-language-modeling](https:\/\/towardsdatascience.com\/openai-gpt-language-modeling-on-gutenberg-with-tensorflow-keras-876f9f324b6c) <\/li>\n    <li>[better-language-models](https:\/\/openai.com\/blog\/better-language-models\/) <\/li> \n    <li>[Orginal Paper](https:\/\/s3-us-west-2.amazonaws.com\/openai-assets\/research-covers\/language-unsupervised\/language_understanding_paper.pdf)<\/li>\n<\/ui>","4bad5327":"## XLM Performance-Loss","93252183":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..K5ST4JjFGBMeTtwX1jC9XQ.0KxxSju7AC0c3tRP2w-rTijGPiYJGNZnjThGQx-BG3govElMkJpIJw6gcpAozeKQSH_Q3_Cjh6a38SnweVyY3X6eW1YMl8UvsJNUsQnuKGTTig69_hUGyoPmepANHm4vZnwUN3vljN9RmyRVaXuvHTkgGCs6KASszIFNgE1v309GpSOtQy7Um8_75fzG-szb7mNmMBlkZ_vBs4-4f-Wmf_8JwkQit6kgDM9qXzE2pEq0btxta2CuVyCYUkJ-qV6aCQprqSy68UTOjD6tdwlASMi9hOLyeogQrSoHLzPWgf_Xg3Pm6dwwjrFwDw2MpFoT47szD5MaXTLoT6cxUAdIRZVNMHed7HsC9r-PDwIdDiZKy8Sss5DUrj3jhE02_HCx9cJLYsaLIvPwJVrfKb7aYvaULHhRw4rLaHtxo4r0B7bKLTGkkgkWUmXVv7yDViYdk48ClVic05ZpfjUmpDo9TcD1s7mUCOcoLdqJh7U8UXbBqGBMHpVD95MOFpwAyTC9mnvDuiIiWVxRr8buuC_QnjRITs4CPYhrFG_-5zXC5B4KDUG23AcbCwK7IkSW4PKnufdJHlgKq3g9nNJUcvgE_mB10rGX2YGzJYpIFIegR6HjzR9UuWLIvm7cxOxWTjUSd-8lzFPNFfWyGWgXsyvdtEVa9mkAKJ63A12oFLIou9Gj1L3wMFXBTFsY3n8EB29h.vwq3hDsu0b8kXlx_-yqfkQ\/__results___files\/__results___48_0.png'>","a30140b1":"## Functions to plot Metrics and loss to Compare Different Transformers\n\n![](https:\/\/miro.medium.com\/fit\/c\/1838\/551\/1*aPYAckB1ZtfcqoY8wZ915w.jpeg)","302a40fd":"## OpenAIGPT-Confusion Matrix","796a7256":"# 6-**Glovec Embedding LSTM**","98af3a59":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..8pFTexbxmoqrx0OFHZ_YFw.qJ_XoPOQxUJaJIyQCzZNDB5Rs-ATsNEA5ZhM9mB5xxiYm-VfPhTiXbJNb8ocdZP9zzurfVgGID4gkAVkdl09D3jx1naobYVbt5me1OmtbWTDUdVKo-hmeMtM3Qf7vKbDl92GRz67Z97b6ghs7pG_9YxQWh5se171PcAuDFsBs4fzwZ_jZ1sw8g4LYOftybCvwmAh3yfLlY2KkV6RkDRgix3PPS6Qxpc2iy91GRDskkwKX2GDcGSycMAccPOuelzVBs-1gQEnuctTXiBLBiLbM1eIcxYbhKtmZn3mqjU4nPv-2qFTh4UKjIOnMSVg_dwtlxZ4I858gd1a0wI4We43kGi0lORZ0CwJDJgemHAcnkcjPYLllJyqwj2pL5zrEwexAf-3oR0DoDZ952DD1xlVR_K5vtt_P2aFybGiTfzSjAfnWpVb-2-QUIF6Esbz25MYXuZo2Trk7HqDt3ApBPMqzdvBjRu5MTcF-cFtkBKIWcBWqYZcTB5IDcmy9PWVDSyKcLksJNHiPPB3Ei3LuE0Z8GfKUGPyBXRe1PYAHZg27QvnmvxoOIi-epUuUoCN2GRm6p6Ekyg-RRb5RXCmJU41UF2h7JxBKPoGmlVvlYMjnrn4vZatLbWVbVS7GBpkWHYb6IIIkqaYFDXICaFgVAIu2C6_sPf2-LQUXNDkOYPYYCKN_FHlw1paRnPDMNRs0J4T.5IAqYeDNXIJujd6C8JCl2Q\/__results___files\/__results___34_0.png'>","970e5d26":"## LSTM with Glovec Embedding-Confusion Matrix","8a6843b7":"<img src='https:\/\/www.kaggleusercontent.com\/kf\/36129650\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..K5ST4JjFGBMeTtwX1jC9XQ.0KxxSju7AC0c3tRP2w-rTijGPiYJGNZnjThGQx-BG3govElMkJpIJw6gcpAozeKQSH_Q3_Cjh6a38SnweVyY3X6eW1YMl8UvsJNUsQnuKGTTig69_hUGyoPmepANHm4vZnwUN3vljN9RmyRVaXuvHTkgGCs6KASszIFNgE1v309GpSOtQy7Um8_75fzG-szb7mNmMBlkZ_vBs4-4f-Wmf_8JwkQit6kgDM9qXzE2pEq0btxta2CuVyCYUkJ-qV6aCQprqSy68UTOjD6tdwlASMi9hOLyeogQrSoHLzPWgf_Xg3Pm6dwwjrFwDw2MpFoT47szD5MaXTLoT6cxUAdIRZVNMHed7HsC9r-PDwIdDiZKy8Sss5DUrj3jhE02_HCx9cJLYsaLIvPwJVrfKb7aYvaULHhRw4rLaHtxo4r0B7bKLTGkkgkWUmXVv7yDViYdk48ClVic05ZpfjUmpDo9TcD1s7mUCOcoLdqJh7U8UXbBqGBMHpVD95MOFpwAyTC9mnvDuiIiWVxRr8buuC_QnjRITs4CPYhrFG_-5zXC5B4KDUG23AcbCwK7IkSW4PKnufdJHlgKq3g9nNJUcvgE_mB10rGX2YGzJYpIFIegR6HjzR9UuWLIvm7cxOxWTjUSd-8lzFPNFfWyGWgXsyvdtEVa9mkAKJ63A12oFLIou9Gj1L3wMFXBTFsY3n8EB29h.vwq3hDsu0b8kXlx_-yqfkQ\/__results___files\/__results___38_1.png'>","4d77f372":"## DistilBert  Performace-ROC","939839b5":"# 2. OpenAIGPT","c935e851":"![](http:\/\/www.topbots.com\/wp-content\/uploads\/2019\/04\/cover_GPT_web-1280x640.jpg)","8b8c6eb1":"## Usefull Links About BERT\n<p1> The following links are quite useful if you to further your knowledge about BERT\n<ui>\n    <li>[illustrated-bert](http:\/\/jalammar.github.io\/illustrated-bert\/) <\/li>\n    <li>[a-visual-guide-to-using-bert-for-the-first-time](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) <\/li>\n    <li>[Chris Mccromick AI](https:\/\/www.youtube.com\/watch?v=_eSGWNqKeeY&t=1s) <\/li> \n    <li>[Chris Mccromick AI-2](https:\/\/www.youtube.com\/watch?v=l8ZYCvgGu0o) <\/li>\n    <li>[CS224n Video](https:\/\/www.youtube.com\/watch?v=S-CspeZ8FHc)\n    <li>[Orginal Paper](https:\/\/arxiv.org\/abs\/1810.04805)<\/li>\n<\/ui>\n","8de9a2e0":"## XLM-ROC Curve"}}