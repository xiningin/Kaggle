{"cell_type":{"0d29bebd":"code","019018e1":"code","801a5cb5":"code","42859b60":"code","b36277e2":"code","457a59ca":"code","f1c989ec":"code","ab92363e":"code","16ae60d3":"code","58cb4e45":"code","6742e4cf":"code","1d3a5ef9":"code","44e80f9c":"code","0b4c1e0a":"code","e5c0c63c":"code","54732a5f":"code","f2dc09e5":"code","eefbb9dc":"code","e5c66c5c":"code","15199d04":"code","b08bcea6":"code","165f5498":"code","45c0e43a":"code","ebf09197":"code","4e92980e":"code","da63aa9b":"code","264301c4":"code","70270bfa":"code","77a135ec":"code","d7025137":"code","52675f73":"code","6f2b0ae8":"code","f4fb899a":"code","e90810ab":"code","2b2f1c96":"code","e291b0d3":"code","89fe2bcd":"code","8e955c83":"code","5bd931f9":"code","2b8b634e":"code","89edea04":"markdown","94721c6c":"markdown","d5c2e1b5":"markdown","09a302ab":"markdown","4df624f1":"markdown","1222dbe1":"markdown","8b1e216a":"markdown","8698a36b":"markdown","e5e2bf13":"markdown","9c229401":"markdown","cbfcb9ef":"markdown","69fb94c2":"markdown","672e04d1":"markdown","907a9725":"markdown","bf366d09":"markdown","becfc102":"markdown","7d1ffd16":"markdown","23bea80f":"markdown","fdc4e964":"markdown","5c8b241e":"markdown","ada2cc3d":"markdown","02a911f7":"markdown","17637313":"markdown","d8525b83":"markdown","1cb388ef":"markdown","668762c1":"markdown","86db799b":"markdown","fd7a9b8c":"markdown","87068ff3":"markdown","a24d1941":"markdown"},"source":{"0d29bebd":"import os\nimport PIL\nimport tensorflow as tf\nimport math\n\nfrom matplotlib.image import imread\n\nimport shutil\n\n#Scipy\nimport scipy.misc\n\n#OpenCV\nimport cv2\n\n#Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import cluster, metrics\nfrom sklearn import manifold, decomposition\nfrom sklearn import preprocessing\n\n#Keras\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, SeparableConv2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\n\n#Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n\npath_to_file = '..\/input\/yelp-photos\/photos.json'\npath_to_images = '..\/input\/yelp-photos\/photos\/'","019018e1":"def create_sample(df, size, test=1):\n    df['file_path'] = path_to_images + df['photo_id'] + '.jpg'\n    df = df.drop(['business_id', 'caption'], axis=1)\n\n    #Label Encoding\n    le = preprocessing.LabelEncoder()\n    \n    #Get all labels from our dataset\n    labels = df['label'].value_counts().index.to_list()\n    \n    #Create empty DataFrame\n    sample = []\n    \n    for value in labels:\n        sample.append(df[df['label'] == value].sample(n=size, random_state=42))\n        \n    X = pd.concat(sample)\n    X['photo_id'] = X['photo_id'] + '.jpg'\n    X['label_enc'] = le.fit_transform(X['label'])\n    \n    \n    #If x_test equal 1 then we don't need it.\n    if test == 1:\n        X_train, X_test = train_test_split(X, test_size=0.3)\n        return X_train, X_test, le.classes_\n    \n    return X, labels","801a5cb5":"def cnn_vgg16_prepro(X_train, X_test, labels, BATCH_SIZE):\n    #Images Shape\n    IMG_HEIGHT = 224\n    IMG_WIDTH = 224\n\n    datagen = ImageDataGenerator(validation_split=0.2, rescale=1.\/255)\n\n\n    train_ds = datagen.flow_from_dataframe(dataframe = X_train,\n                                           directory = path_to_images,\n                                           x_col = 'photo_id',\n                                           y_col = 'label',\n                                           subset = \"training\",\n                                           batch_size = BATCH_SIZE,\n                                           seed = 42,\n                                           shuffle = True,\n                                           class_mode = 'categorical',\n                                           target_size = (IMG_HEIGHT, IMG_WIDTH))\n    \n\n    valid_ds = datagen.flow_from_dataframe(dataframe = X_train,\n                                           directory = path_to_images,\n                                           x_col = 'photo_id',\n                                           y_col = 'label',\n                                           subset = 'validation',\n                                           batch_size = BATCH_SIZE,\n                                           seed = 42,\n                                           shuffle = True,\n                                           class_mode = 'categorical',\n                                           target_size = (IMG_HEIGHT, IMG_WIDTH))\n    \n    \n    test_datagen = ImageDataGenerator(rescale=1.\/255)\n    test_ds = test_datagen.flow_from_dataframe(dataframe = X_test,\n                                               directory = path_to_images,\n                                               x_col = 'photo_id',\n                                               y_col = None,\n                                               batch_size = BATCH_SIZE,\n                                               seed = 42,\n                                               shuffle = False,\n                                               class_mode = None,\n                                               target_size = (IMG_HEIGHT, IMG_WIDTH))\n    \n    \n    return train_ds, valid_ds, test_ds","42859b60":"def create_model_vgg16(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n    # VGG16\n    conv_base = VGG16(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape)\n    \n    # Defines how many layers to freeze during training.\n    if fine_tune > 0:\n        for layer in conv_base.layers[:-fine_tune]:\n            layer.trainable = False\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n\n    top_model = conv_base.output\n    top_model = Flatten(name=\"flatten\")(top_model)\n    top_model = Dense(512, activation='relu')(top_model)\n    top_model = Dropout(0.5)(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    # Group the convolutional base and new fully-connected layers into a Model object.\n    model = Model(inputs=conv_base.input, outputs=output_layer)\n\n    # Compiles the model for training.\n    model.compile(optimizer=optimizer, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model","b36277e2":"#Image preprocessing\ndef prepro_images(file_path):\n    img = cv2.imread(file_path, 0) #Open img in gray\n    res = cv2.equalizeHist(img)   # equalize image histogram\n    kp, desc = sift.detectAndCompute(res, None)\n    return (desc)\n\n#Create histograms\ndef build_histogram(kmeans, des):\n    res = kmeans.predict(des)\n    hist = np.zeros(len(kmeans.cluster_centers_))\n    nb_des=len(des)\n    for i in res:\n        hist[i] += 1.0\/nb_des\n\n    return hist\n\n#confusion matrix\ndef conf_mat_transform(y_true, y_pred) :\n    conf_mat = metrics.confusion_matrix(y_true, y_pred)\n    corresp = [4, 3, 1, 2, 0]\n    print (\"Corr. clusters : \", corresp)\n    labels = pd.Series(y_true, name=\"y_true\").to_frame()\n    labels['y_pred'] = y_pred\n    labels['y_pred_transform'] = labels['y_pred'].apply(lambda x : corresp[x]) \n    \n    return labels['y_pred_transform']","457a59ca":"#Configuration\nBATCH_SIZE = 32\nSAMPLE_SIZE = 8000\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\n\n#Generate our samples\nX_train, X_test, labels = create_sample(pd.read_json(path_to_file, lines=True), 8000)\n\nprint('Train dataset shape :', X_train.shape)\nprint('Test dataset shape :', X_test.shape, '\\n')\nprint('All labels :', labels)\nX_train.head(5)","f1c989ec":"for label in labels :\n    print(label)\n    for i in range(3):\n        plt.subplot(130 + 1 + i)\n        image = imread(X_train[X_train['label'] == label].file_path.iloc[i])\n        plt.imshow(image)\n    plt.show()","ab92363e":"#Preprocessing\ntrain_ds, valid_ds, test_ds = cnn_vgg16_prepro(X_train, X_test, labels, BATCH_SIZE)","16ae60d3":"model = Sequential(\n    [\n        keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n        keras.layers.Activation('relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.BatchNormalization(),\n\n        keras.layers.Conv2D(32, (3, 3)),\n        keras.layers.Activation('relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.BatchNormalization(),\n\n        keras.layers.Conv2D(64, (3, 3)),\n        keras.layers.Activation('relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.BatchNormalization(),\n\n        keras.layers.Conv2D(128, (3, 3)),\n        keras.layers.Activation('relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        keras.layers.BatchNormalization(),\n\n        keras.layers.GlobalAveragePooling2D(),\n\n        keras.layers.Flatten(),  # this converts our 3D feature maps to 1D feature vectors\n        keras.layers.Dense(512),\n        keras.layers.Activation('relu'),\n            \n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(5),\n        keras.layers.Activation('softmax'),\n    ])","58cb4e45":"model.summary()","6742e4cf":"%%time\n\nepochs = 20\n\ncompute_steps_per_epoch = lambda x: int(math.ceil(1. * x \/ BATCH_SIZE))\nn_steps = compute_steps_per_epoch(train_ds.samples)\nn_val_steps = compute_steps_per_epoch(valid_ds.samples)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\", save_best_only=True, verbose=1),\n]\n\nmodel.compile(optimizer=keras.optimizers.Adam(1e-3),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\n\n\n\nhistory = model.fit(\n    train_ds,\n    epochs=epochs,\n    callbacks=callbacks,\n    validation_data=valid_ds,\n    steps_per_epoch = n_steps,\n    validation_steps = n_val_steps,\n)","1d3a5ef9":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(12, 12))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","44e80f9c":"model.evaluate(valid_ds, steps=n_val_steps, verbose=1)","0b4c1e0a":"STEP_SIZE_TEST = compute_steps_per_epoch(test_ds.samples)\n\ntest_ds.reset()\n\npred = model.predict(test_ds,\n                     steps=STEP_SIZE_TEST,\n                     verbose=1)\n\npredicted_class_indices = np.argmax(pred,\n                                    axis=1)\n\npredictions = [labels[k] for k in predicted_class_indices]\n\nfilenames = test_ds.filenames\n\nresults = pd.DataFrame({\"photo_id\":filenames,\n                      \"Predictions\":predictions})\n\ndf_res = pd.merge(results,\n                  X_test,\n                  on='photo_id')\n\ndf_res.shape[0]\n\ntotal_raw = df_res.shape[0]\nbad_predictions = df_res[df_res.label != df_res.Predictions].shape[0]\npercentage_cnn =  100 - (100 * bad_predictions \/ total_raw)","e5c0c63c":"print('Result on test set :', percentage_cnn)","54732a5f":"print('Prediction with our CNN : ', df_res.Predictions.iloc[0])\nprint('Current Label : ', df_res.label.iloc[0])\nPIL.Image.open(str(X_test['file_path'].iloc[0]))","f2dc09e5":"print('Prediction with our CNN : ', df_res.Predictions.iloc[1])\nprint('Current Label : ', df_res.label.iloc[1])\nPIL.Image.open(str(X_test['file_path'].iloc[1]))","eefbb9dc":"#Configuration\nSAMPLE_SIZE = 750\n\n#Sample + Preprocessing\nX_train_vgg16, X_test_vgg16, labels = create_sample(pd.read_json(path_to_file, lines=True), SAMPLE_SIZE)\ntrain_ds_vgg16, valid_ds_vgg16, test_ds_vgg16 = cnn_vgg16_prepro(X_train_vgg16, X_test_vgg16, labels, BATCH_SIZE)","e5c66c5c":"n_epochs_vgg16 = 5\n\ninput_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\noptim_1 = Adam(learning_rate = 0.001)\n\nn_steps_vgg16 = compute_steps_per_epoch(train_ds_vgg16.samples)\nn_val_steps_vgg16 = compute_steps_per_epoch(valid_ds_vgg16.samples)\n\n\n# First we'll train the model without Fine-tuning\nvgg_model = create_model_vgg16(input_shape, 5, optim_1, fine_tune = 0)\nvgg_model.summary()","15199d04":"%%time\n\ncheckpoint = [\n    keras.callbacks.ModelCheckpoint(\"vgg16_save_at_{epoch}.h5\",\n                                    save_best_only=True,\n                                    verbose=1),\n]\n\nvgg_history = vgg_model.fit(train_ds_vgg16,\n                            epochs = n_epochs_vgg16,\n                            validation_data = valid_ds_vgg16,\n                            steps_per_epoch = n_steps_vgg16,\n                            validation_steps = n_val_steps_vgg16,\n                            callbacks = checkpoint,\n                            verbose = 1)","b08bcea6":"acc = vgg_history.history['accuracy']\nval_acc = vgg_history.history['val_accuracy']\n\nloss = vgg_history.history['loss']\nval_loss = vgg_history.history['val_loss']\n\nepochs_range = range(n_epochs_vgg16)\n\nplt.figure(figsize=(12, 12))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","165f5498":"vgg_model.evaluate(valid_ds, steps=n_val_steps, verbose=1)","45c0e43a":"test_ds.reset()\n\npred = vgg_model.predict(test_ds,\n                         verbose=1)\n\npredicted_class_indices = np.argmax(pred, axis=1)\n\npredictions = [labels[k] for k in predicted_class_indices]\n\nfilenames = test_ds.filenames\n\nresults = pd.DataFrame({\"photo_id\" : filenames,\n                        \"Predictions\":predictions})\n\ndf_res_vgg16 = pd.merge(results,\n                  X_test,\n                  on='photo_id')\n\ntotal_raw = df_res_vgg16.shape[0]\nbad_predictions_vgg16 = df_res_vgg16[df_res_vgg16.label != df_res_vgg16.Predictions].shape[0]\npercentage_vgg16 =  100 - (100 * bad_predictions_vgg16 \/ total_raw)","ebf09197":"print('Result on test set :', percentage_vgg16)","4e92980e":"print('Prediction with VGG16 : ', df_res_vgg16.Predictions.iloc[0])\nprint('Current Label : ', df_res_vgg16.label.iloc[0])\nPIL.Image.open(str(X_test['file_path'].iloc[0]))","da63aa9b":"print('Prediction with VGG16 : ', df_res_vgg16.Predictions.iloc[1])\nprint('Current Label : ', df_res_vgg16.label.iloc[1])\nPIL.Image.open(str(X_test['file_path'].iloc[1]))","264301c4":"X, labels = create_sample(pd.read_json(path_to_file, lines=True), 100, 0)","70270bfa":"X.groupby('label').count()","77a135ec":"for label in labels :\n    print(label)\n    for i in range(3):\n        plt.subplot(130 + 1 + i)\n        image = imread(X[X['label'] == label].file_path.iloc[i])\n        plt.imshow(image)\n    plt.show()","d7025137":"sift = cv2.SIFT_create()\nimage = cv2.imread(X.file_path.iloc[0],0) # convert in gray\nimage = cv2.equalizeHist(image)   # equalize image histogram\n\nkp, desc = sift.detectAndCompute(image, None)\nimg = cv2.drawKeypoints(image, kp, image)\n\nplt.figure(figsize=(12,8))\nplt.imshow(img)\nplt.show()\nprint(\"Descriptors : \", desc.shape, '\\n')\nprint(desc)","52675f73":"%%time\n\nX['sift_keypoints'] = X.file_path.apply(lambda x : prepro_images(x))\n\nsift_keypoints_all_train = np.concatenate(X['sift_keypoints'].to_numpy(), axis=0)\nprint(\"Number of descriptors : \", sift_keypoints_all_train.shape, '\\n')","6f2b0ae8":"%%time\n\nk = int(round(np.sqrt(len(sift_keypoints_all_train)),0))\nprint(\"estimated cluster number :\", k, '\\n')\n\n# Clustering\nkmeans = cluster.MiniBatchKMeans(n_clusters=k, init_size=3*k, random_state=0)\nkmeans.fit(sift_keypoints_all_train)","f4fb899a":"%%time\n\n# Creation of histograms\nX['histogram'] = X.sift_keypoints.apply(lambda x : build_histogram(kmeans, x))","e90810ab":"pca = decomposition.PCA(n_components=0.99)\nfeat_pca = pca.fit_transform(X['histogram'].to_list())\n\ntsne = manifold.TSNE(n_components=2, perplexity=30, \n                     n_iter=2000, init='random', random_state=6)\n\nX_tsne = tsne.fit_transform(feat_pca)\n\ndf_tsne = pd.DataFrame(X_tsne[:,0:2], columns=['tsne1', 'tsne2'])\ndf_tsne[\"class\"] = X.label.to_list()\nprint(df_tsne.shape)","2b2f1c96":"plt.figure(figsize=(12,8))\nsns.scatterplot(\n    x=\"tsne1\", y=\"tsne2\", hue=\"class\", data=df_tsne, legend=\"brief\",\n    palette=sns.color_palette('tab10', n_colors=5), s=50, alpha=0.6)\n\nplt.title('TSNE with labels', fontsize = 30, pad = 20, fontweight = 'bold')\nplt.xlabel('tsne 1', fontsize = 14, fontweight = 'bold')\nplt.ylabel('tsne 2', fontsize = 14, fontweight = 'bold')\nplt.legend(prop={'size': 14}) \n\nplt.show()","e291b0d3":"cls = cluster.KMeans(n_clusters=5, random_state=42)\ncls.fit(X_tsne)\n\ndf_tsne[\"cluster\"] = cls.labels_\nprint(df_tsne.shape)","89fe2bcd":"plt.figure(figsize=(12,8))\nsns.scatterplot(\n    x=\"tsne1\", y=\"tsne2\",\n    hue=\"cluster\",\n    palette=sns.color_palette('tab10', n_colors=5), s=50, alpha=0.6,\n    data=df_tsne,\n    legend=\"brief\")\n\nplt.title('TSNE clustering', fontsize = 30, pad = 35, fontweight = 'bold')\nplt.xlabel('tsne1', fontsize = 14, fontweight = 'bold')\nplt.ylabel('tsne2', fontsize = 14, fontweight = 'bold')\nplt.legend(prop={'size': 14}) \n\nplt.show()\n\ny = X[\"label_enc\"]\nprint(\"ARI : \", metrics.adjusted_rand_score(y, cls.labels_))","8e955c83":"df_tsne.groupby(\"cluster\").count()[\"class\"]","5bd931f9":"cls_labels_transform = conf_mat_transform(y, cls.labels_)\n\nconf_mat = metrics.confusion_matrix(y, cls_labels_transform)\nprint(conf_mat, '\\n')\nprint(metrics.classification_report(y, cls_labels_transform))","2b8b634e":"df_cm = pd.DataFrame(conf_mat, index = [label for label in labels],\n                  columns = [i for i in \"01234\"])\n\nplt.figure(figsize = (12,8))\nplt.title('Confusion Matrix', fontsize = 30)\nsns.heatmap(df_cm, annot=True, cmap=\"Blues\");","89edea04":"# 3. **Transfer learning VGG16**","94721c6c":"## 4.6 **Clustering**","d5c2e1b5":"## 2.3 **Train\/Test of our CNN**","09a302ab":"# 4. **SIFT**","4df624f1":"# 1. **Utilities**\n## 1.1 **Functions**\n### 1.1.1 **Generate Sample** ","1222dbe1":"# 5. **Conclusion**","8b1e216a":"## 3.2 **Train\/Test VGG16**","8698a36b":"Of the three methods used, the best results were obtained with VGG16.\nTransfer learning is advantageous when you want to obtain good results with few resources. Then comes the CNN and SIFT","e5e2bf13":"*****************************","9c229401":"## 4.2 **SIFT in action**","cbfcb9ef":"## 4.1 **Sampling and Preprocess**","69fb94c2":"## 2.5 **Results obtained with our test set**","672e04d1":"# 2. **Convolutional Neural Networks**\nIn this section we will use a CNN model to perform the classification of our images","907a9725":"### 1.1.4 **SIFT**","bf366d09":"Let's take a look at some of the images contained in our training set","becfc102":"## 4.7 **Results**","7d1ffd16":"### 1.1.3 **VGG16**","23bea80f":"## 2.2 **Summary**","fdc4e964":"## 2.4 **Training session evaluation** ","5c8b241e":"## 2.1 **Build our CNN**","ada2cc3d":"## 4.3 **Histograms**","02a911f7":"## 3.1 **Adaptation of VGG16 model**","17637313":"## 4.4 **PCA**","d8525b83":"## 4.5 **T-SNE Visualisation**","1cb388ef":"## 2.6 **Let's look at some of our predictions**","668762c1":"Before using our training and test sets with the CNN model we need to preprocess our images","86db799b":"## 3.5 **Let's look at some of our predictions**","fd7a9b8c":"## 3.3 **Training session evaluation** ","87068ff3":"### 1.1.2 **Preprocessing**","a24d1941":"## 3.4 **Results obtained with our test set**"}}