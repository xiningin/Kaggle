{"cell_type":{"fe8f5f66":"code","1df84ce1":"code","3e5a3bfe":"code","8aed665b":"code","b82eda60":"code","55d98ebd":"code","a70d1fde":"code","b691651a":"code","9a2b7dc6":"code","87635876":"code","b0648534":"code","7a47f7ea":"code","c897702f":"code","81ee2584":"code","aef2d956":"code","92e094cd":"code","c38848fc":"code","ed129de1":"code","afbd9707":"code","8581414c":"markdown","33834525":"markdown","c53bdede":"markdown","9440bda1":"markdown","d894f665":"markdown","79ca387e":"markdown","b881604d":"markdown","50981752":"markdown","5bb366ed":"markdown","33390873":"markdown","37d26119":"markdown","be773c38":"markdown","755d3a9b":"markdown","bf98c452":"markdown","d0778bfc":"markdown","53287545":"markdown","9d77c8ed":"markdown"},"source":{"fe8f5f66":"#import some necessary librairies\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nfrom sklearn.model_selection import RandomizedSearchCV","1df84ce1":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLars,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","3e5a3bfe":"import pandas as pd\ntrain =pd.read_csv('..\/input\/housingames\/X_train.csv')\ntest = pd.read_csv('..\/input\/housingames\/X_test.csv')\nytrain=pd.read_csv('..\/input\/housingames\/y_train.csv')","8aed665b":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","b82eda60":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9,\n                                                random_state=7))\n#########################################################################\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n#########################################################################\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n","55d98ebd":"# library used for stacking \n!pip install vecstack","a70d1fde":"from vecstack import stacking\n\nestimators = [KRR,GBoost,ENet]\nX_train=train\ny_train=ytrain\nX_test=test\nk=5\n\nL_train_1, L_test_1=stacking(estimators,X_train,\n         y_train, X_test,regression=True, \n         n_folds=k,mode='oof_pred',random_state=7, \n         verbose=2)","b691651a":"ENet2 = make_pipeline(RobustScaler(), ElasticNet(alpha=0.00055, l1_ratio=.45,\n                                                random_state=7))\n#########################################################################\nKRR2 = KernelRidge(alpha=0.4, kernel='polynomial', degree=2, coef0=2.5)\n#########################################################################\nGBoost2 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,\n                                   max_depth=3, max_features='sqrt',\n                                   min_samples_leaf=7, min_samples_split=10, \n                                   loss='huber', random_state =7)","9a2b7dc6":"#layer 2\nestimatorsL2=[ENet2,KRR2,GBoost2]\n\nL_train_2, L_test_2=stacking(estimatorsL2,L_train_1,\n         y_train, L_test_1,regression=True, \n         n_folds=k,mode='oof_pred',random_state=7, \n         verbose=2)\n","87635876":"#our estimator (hyper params have been found by randomized search)\nENet3=make_pipeline(RobustScaler(), ElasticNet(alpha=0.006, l1_ratio=0.0008,\n                                                random_state=7))","b0648534":"#layer 3\nL_train_3, L_test_3=stacking([ENet3],L_train_2,\n         y_train, L_test_2,regression=True, \n         n_folds=k,mode='oof_pred',random_state=7, \n         verbose=1)\n\nprint(rmsle(y_train,L_train_3))","7a47f7ea":"stack_pred=np.expm1(L_test_3).reshape(len(L_test_3),)\n\n#traing predictions are in logged form \n#because the y_train is still in this form too\nstack_train=L_train_3.reshape(len(L_train_3),)","c897702f":"\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n#########################################################################\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n","81ee2584":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","aef2d956":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","92e094cd":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stack_train*0.7 +xgb_train_pred*0.12+ lgb_train_pred*0.18  ))","c38848fc":"stack_pred=stack_pred.reshape(1459,)\nensemble =stack_pred*0.7 +xgb_pred*0.12 + lgb_pred*0.18  ","ed129de1":"ensemble.shape","afbd9707":"sub = pd.DataFrame()\nsub['Id'] = range(1461,1461+1459)\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)\nsub.head()","8581414c":"## Stacking","33834525":"#### layer 1","c53bdede":"### Ensemble prediction","9440bda1":"Tree based models do not need data to be scaled !\nso I haven't use scaling when predicting with boosting models","d894f665":"## Submission","79ca387e":"## House Prices: Advanced Regression Techniques\n\n* Data preprocessing steps is based on this note book :[notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) so I haven't gone through the proprocessing steps in this notebook .\n* You also can find the preprcessed data and the simplified steps for preprocessing of the data in my github repository:[repository](https:\/\/github.com\/Moeinh77\/Kaggle-House-Prices-Advanced-Regression-Techniques)\n*  Hyper parameters have been found by GridSearch and randomizedSearch of scikit learn .\n* The final model is a weitghted  average of : a 3 layer stack ensemble , a LGboost  and a XGBoost model.\n* This kernel guides you through a smaller error than all the other kernels so far, because of multiple layers of stacking (3 models in first stack,3 models in the second stack and one model as an estimator in last stack).\n* For ease of use I have used this library for stacking : [vecstack](https:\/\/github.com\/vecxoz\/vecstack).\n* You can see how the stack used has been implemented in here :[implementation](https:\/\/github.com\/vecxoz\/vecstack\/blob\/master\/examples\/00_stacking_concept_pictures_code.ipynb).\n---\n\n### Possible improvements:\n* Try different ways for preprocessing the data (e.g using only most importanant features of data for some models)\n* Increasing number of layers with more models in each layer \n* Decreasing the corrolation between 3 models in final averaged ensemble","b881604d":"### training error","50981752":"**XGBoost:**","5bb366ed":"Score on the leader board :**0.11433**","33390873":"**LightGBM:**","37d26119":"### Importing libraries","be773c38":"![Image](https:\/\/camo.githubusercontent.com\/fa34150cb31d02f68886584d549f300f8c290ba3\/68747470733a2f2f6769746875622e636f6d2f766563786f7a2f766563737461636b2f7261772f6d61737465722f7069632f616e696d6174696f6e322e676966)","755d3a9b":"#### layer 3\n","bf98c452":"#### Please let me know if you had ideas for improving this notebook,also if have problems understanding the code ask in the comments and I will answer .Thanks for reading this note book hope it helps you !","d0778bfc":"### Metric function","53287545":"## Weighted average ensemble\n\n","9d77c8ed":"#### layer 2"}}