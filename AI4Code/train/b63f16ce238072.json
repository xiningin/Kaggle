{"cell_type":{"474b7679":"code","ed543278":"code","3f589e87":"code","4ddd9074":"code","d4170549":"code","5f4b2ff6":"code","0ea5c2a2":"code","1e0d37d7":"code","70beeb8e":"code","7fcefd8c":"code","20172164":"code","5e1dee9f":"code","c6a198a5":"code","e537a12f":"code","a12d9ccb":"code","6a013a00":"code","54100c2f":"code","242df6e5":"code","a318202d":"code","63a7dbd4":"code","78d7529e":"code","955e011d":"code","9f813f90":"code","186e2625":"code","f3768ec1":"code","98d3255b":"code","528b5e1f":"code","b1667c23":"code","b7f83e2c":"code","ddd996e2":"code","86cb67e3":"code","39345cc3":"code","5adfaeb6":"code","87bfd02a":"code","c81e3faf":"code","7cca0113":"code","3b754975":"code","6c065a25":"code","106d7a4d":"code","13f8bcff":"code","d0d3cf1a":"code","1531ac46":"code","3e4dee2c":"code","12fb2440":"code","697e87da":"code","4e9c1bfb":"code","9c7736e4":"code","23d199dd":"code","ee8af970":"code","39b0c183":"code","5e0e61af":"code","b56338c8":"code","db03aedb":"code","d6d0e793":"code","926758eb":"code","fd931520":"code","c26a2160":"code","e3f02029":"code","eaa9b66f":"code","135238c2":"code","1a688bb3":"code","a5980a6b":"code","c3eb55bf":"code","e51c5306":"code","b3c87319":"code","7e935ccc":"code","999e9483":"code","35d7d07b":"code","8b2fab0e":"code","ab9c2e34":"code","74ceeec3":"code","7d1b4991":"code","c7bbaae9":"code","a042b580":"code","284eba1e":"code","2213c812":"code","34145513":"code","3bcaa917":"code","a5cd660c":"code","b3663741":"code","26de797e":"code","77bc1abf":"code","63b2fa99":"code","a51df98f":"code","75381c87":"code","c04a8af5":"code","ef210e09":"code","eaa98c7e":"code","0dae5962":"code","779aff7e":"code","2025c320":"code","7aa6da8b":"code","dd2d62f7":"code","ad0395e4":"code","689f579a":"code","0c6f38cb":"code","60311a4a":"code","294d1c8f":"code","72925484":"code","b6b01c0f":"code","023cf9e8":"code","3f71db14":"code","faa56aa7":"code","c924908e":"code","ab713a83":"code","f87fb842":"code","53cee6d3":"code","b85bd507":"code","3390d7ae":"code","2f554ae3":"code","e66d5c16":"code","d3e1150e":"code","25335cb0":"code","1bc9a47c":"code","f2dc60e8":"code","a2d536f3":"code","1e6ff351":"code","578a448d":"code","68d4a62b":"code","bc0bfa0f":"code","25330306":"code","e9857cc6":"code","efe4f1b2":"code","296f895c":"code","34e367fe":"code","20995394":"code","3a832fbf":"code","b3882367":"code","570e8471":"code","dc433a60":"code","fd3fc3d0":"code","eee24c32":"code","459e327c":"code","7a90c42b":"code","5c658419":"code","317b2879":"code","13f06ea9":"code","c1d0bc90":"code","c0adef39":"code","1a3947c5":"code","d002aa03":"code","2c9b4380":"code","d8ec2746":"code","5c7a8a79":"code","a360a936":"code","0d3145e9":"code","4c5ac647":"code","eaf58cd2":"code","d354570c":"code","6c3b264a":"code","024f0b44":"code","1c480ba8":"code","6701debf":"code","c4ab5e18":"code","415c60ba":"code","ac359605":"code","6128ac2c":"code","dace0e59":"code","6d82cc82":"code","108cac8f":"code","16b29462":"code","d27a90ef":"code","0a03b496":"code","95d31755":"code","cbcd7edc":"code","1b9b7f87":"code","08a9ec87":"code","9ae2ca32":"code","5dd37bc6":"code","3ce9b1bf":"code","a31da790":"code","608cf4d2":"code","8197aa58":"code","99535302":"code","90ff46d1":"code","630e880d":"code","de33fc66":"code","9c3510fc":"code","0353330a":"code","f62d49ef":"code","f99f94c0":"code","3acdffac":"code","ebd008b0":"code","7f78a645":"code","e849bd5c":"code","6721b1f0":"code","608a2e27":"code","288bb279":"code","6cefdce6":"code","9fc29fb3":"code","7ba8aa15":"code","87bfc361":"code","fb296568":"code","336fe579":"code","5a1205b0":"code","29cda5c8":"code","c363a7a2":"code","36d96af5":"code","daa07ee2":"code","c70d3290":"code","077e6e5d":"code","a7d8bd8f":"code","1f87d7a9":"code","77ccaad6":"code","06a21312":"code","07139b7f":"code","cc59bae8":"code","89e07cc5":"code","e6d2dbf2":"code","d69d398a":"code","a21421f3":"code","1f0e14b0":"code","a887249d":"code","191ea258":"code","51139c65":"code","3be41e6d":"code","f33c57cd":"code","01107879":"markdown","35703bb9":"markdown","9d91a182":"markdown","796a2fff":"markdown","9c18f224":"markdown","fa87c23b":"markdown","63d84f96":"markdown","b00d87c0":"markdown","d05abd37":"markdown","6dfbc0a5":"markdown","55cde1b9":"markdown","9270755f":"markdown","a07a3159":"markdown","a4f6d536":"markdown","aeecefaf":"markdown","0ab3fcfb":"markdown","a3d23a86":"markdown","328a4448":"markdown","1ad1cf52":"markdown","81989ccc":"markdown","bdf836f5":"markdown","d075413a":"markdown","884a5fc3":"markdown","356377c9":"markdown","779293a0":"markdown","0ec86d72":"markdown","999c92c0":"markdown","dfedeeea":"markdown","9f674338":"markdown","075023c7":"markdown","3a86e619":"markdown","581f22e2":"markdown","b4c1cff1":"markdown","2a0d9dad":"markdown","e96d224c":"markdown","018024dd":"markdown","cb0acc03":"markdown","b06fa988":"markdown","efeca1c0":"markdown","bc29656d":"markdown","0af434b4":"markdown","db655c9b":"markdown","b6b25f3d":"markdown","eb2f6539":"markdown","b0dbe2e0":"markdown","bd7cae62":"markdown","bd641c3e":"markdown","5f8c073c":"markdown","0f9be1d4":"markdown","1c3f0e38":"markdown","036c324e":"markdown","7fc2cd86":"markdown","c765d36c":"markdown","13f83472":"markdown","2bafa102":"markdown","534d7b80":"markdown","1a54ba8d":"markdown","1cfa02b4":"markdown","afa5d3e5":"markdown","51a4e828":"markdown","5e5186e7":"markdown","0d39a035":"markdown","3b10a974":"markdown","27d404a8":"markdown","1f07f05a":"markdown","0ea635e2":"markdown","ea2541e7":"markdown","bc093e7e":"markdown","5d806355":"markdown","2ce29dbb":"markdown","5b97a71c":"markdown","4b7d63f3":"markdown","b716c21b":"markdown","5765faf4":"markdown","1a4310bf":"markdown","99524d4f":"markdown","0639e26d":"markdown","04f96ba9":"markdown","88dd4552":"markdown","e7061a23":"markdown","906241e5":"markdown","c371b083":"markdown"},"source":{"474b7679":"# Imports used throughout the notebook\n\n# plotting packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# math packages\nimport numpy as np\n\n# packages needed for introductory ML\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n# packages needed for intermediate ML\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# packages needed for deep learning\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# packages needed for feature engineering\nfrom pathlib import Path\nimport lightgbm as lgb\nfrom scipy.stats import skew\nfrom pandas.api.types import CategoricalDtype\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nprint(\"Finished Imports\")","ed543278":"def get_MAE(X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    Calculates the mean absolute error (MAE) for a ML approach\n    \n    Input\n    -----\n    X_train:\n        the training data used\n    X_valid:\n        the data to be compared to\n    y_train:\n        the y values that are used for training the model\n    y_valid:\n        the y values we want our comparison to be tested against\n    \n    Output:\n    -------\n    mean_absolute_error:\n        sum of total absolute error divided by sample size\n    \"\"\"\n    \n    model = RandomForestRegressor(n_estimators=100)\n    model.fit(X_train, y_train)\n    y_predict = model.predict(X_valid)\n    mae = mean_absolute_error(y_valid, y_predict)\n    return mae\n\ndef get_score(n_estimators, X, y):\n    \"\"\"\n    Return the average MAE over 3 CV folds of random forest model.\n    \n    Input\n    -----\n    n_estimators:\n        the number of trees in the forest\n        \n    Output:\n    -------\n    mean_score:\n        The mean scores when using a pipeline to determine the mean absolute error\n    \"\"\"\n    # Replace this body with your own code\n    pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                               ('model', RandomForestRegressor(n_estimators, random_state=0))])\n    scores = -1 * cross_val_score(pipeline, X, y, cv=3, scoring='neg_mean_absolute_error')\n    return scores.mean()\n\ndef gen_prediction(training_data, target_data, test_data, estimators=100):\n    \"\"\"\n    Calculate the model prediction using an inputted training, target and\n    test data.\n\n    Input\n    -----\n    training_data:\n        training data in a pandas array used to generate a model\n    target_data:\n        target data in a pandas array used to generate a model\n    test_data: \n        test data we will be fitting a model to\n    \n    Output:\n    -------\n    predictions for test data\n    \"\"\"\n    # Define and fit model\n    my_model = RandomForestRegressor(n_estimators=estimators, random_state=0)\n    my_model.fit(training_data, target_data)\n\n    # Get test predictions\n    print (\"Submission data calculated\")\n    return my_model.predict(test_data)\n","3f589e87":"# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path, index_col='Id')\ntrain_data = pd.read_csv(train_data_path, index_col='Id')\nsample_data = pd.read_csv(sample_data_path)\n\n# Create a directory to hold the scores of each part with different techniques\nscores_dict = {}\nsubmission_dict = {}\n\nprint(\"Data loaded and dictionaries initialized\")","4ddd9074":"# Check the head of each file\ntest_data.head(5)","d4170549":"train_data.head(5)","5f4b2ff6":"sample_data.head(5)","0ea5c2a2":"# Output the shape of our test, train, and sample data\nprint(\"Training data shape: {}\".format(train_data.shape))\nprint(\"Testing data shape: {}\".format(test_data.shape))\nprint(\"Sample data shape: {}\".format(sample_data.shape))","1e0d37d7":"# Define the data for testing\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\n# Divide our data into training and validation data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","70beeb8e":"# Define properties that are useful from the intro course\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX_train_features = X_train[features]\nX_valid_features = X_valid[features]","7fcefd8c":"# Check the MAE of this model\nscores_dict['1.a'] = get_MAE(X_train_features, X_valid_features, y_train, y_valid)\n\nprint(\"MAE of simple Random Forest Regressor: \")\nprint(scores_dict['1.a'])","20172164":"# Create a target object\ntrain_y = train_data.SalePrice\n\n# Define properties that are useful from the intro course\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\ntrain_X = train_data[features]\ntest_X = test_data[features]","5e1dee9f":"# Make a prediction on the values using RandomForestRegressor\ntest_prediction_1a = gen_prediction(train_X, train_y, test_X)","c6a198a5":"# Save the prediction to our dictonary\nsubmission_dict['1.a'] = test_prediction_1a\nprint(\"Random Forest Regressor Submission Saved\")","e537a12f":"# Define the data that will be used for all tests\ny = train_data.SalePrice\nX_full = train_data.drop(['SalePrice'], axis=1)\n\n# To keep things simple, we'll use only numerical predictors\nX = X_full.select_dtypes(exclude=['object'])\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\n# Define the data that will be used for all submission generation\nX_test = test_data.select_dtypes(exclude=['object'])","a12d9ccb":"# Check for missing values in each column of training data\nmissing_val_train = [column for column in X_train.columns\n                     if X_train[column].isnull().any()]\nprint(missing_val_train)","6a013a00":"# drop columns in training and validation data\nreduced_X_train = X_train.drop(missing_val_train, axis=1)\nreduced_X_valid = X_valid.drop(missing_val_train, axis=1)","54100c2f":"# Check the MAE of this model\nscores_dict['2.a'] = get_MAE(reduced_X_train, reduced_X_valid, y_train, y_valid)\n\nprint(\"MAE (Drop columns with missing values):\")\nprint(scores_dict['2.a'])","242df6e5":"# Check for missing values in each column of training data and test data using list comprehension\nmissing_val_X = [column for column in X.columns\n                 if X[column].isnull().any()]\n\nmissing_val_X_test = [column for column in X_train.columns\n                      if X_test[column].isnull().any()]\n\nprint(\"Columns with missing training data: \")\nprint(missing_val_X)\n\nprint(\"\\nColumns with missing test data: \")\nprint(missing_val_X_test)","a318202d":"# Combine the two sets of missing columns together\n# If we combine the two sets of missing columns together arbitrarily we'll get duplicates\nprint(missing_val_X + missing_val_X_test)\n\ncombined_missing_val = list(set(missing_val_X + missing_val_X_test))\nprint(combined_missing_val)\n\nprint(\"\\nTotal number of columns to be dropped:\")\nprint(len(combined_missing_val))","63a7dbd4":"# Drop the missing values in both sets\nX_drop = X.drop(combined_missing_val, axis=1)\nX_test_drop = X_test.drop(combined_missing_val, axis=1)\n\nprint (\"Shape of X_train: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\n\nprint (\"Shape of X_train_drop: {}\".format(X_drop.shape))\nprint (\"Shape of X_test_drop: {}\".format(X_test_drop.shape))","78d7529e":"# Generate the predictions\ntest_prediction_2a = gen_prediction(X_drop, y, X_test_drop)","955e011d":"# Save the prediction to our dictonary\nsubmission_dict['2.a'] = test_prediction_2a\nprint(\"Drop Value Submission Saved\")","9f813f90":"# Imputation, setup the simple imputer and apply it to our training data\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","186e2625":"# Check the MAE of this model\nscores_dict['2.b'] = get_MAE(imputed_X_train, imputed_X_valid, y_train, y_valid)\n\nprint(\"MAE (Imputation):\")\nprint(scores_dict['2.b'])","f3768ec1":"# Imputation, setup the simple imputer and apply it to our full data\nmy_imputer = SimpleImputer()\nimputed_X = pd.DataFrame(my_imputer.fit_transform(X))\nimputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n\n# imputation removed column names; put them back\nimputed_X.columns = X.columns\nimputed_X_test.columns = X_test.columns","98d3255b":"# Generate the predictions\ntest_prediction_2b = gen_prediction(imputed_X, y, imputed_X_test)","528b5e1f":"# Save the prediction to our dictonary\nsubmission_dict['2.b'] = test_prediction_2b\nprint(\"Simple Imputation Submission Saved\")","b1667c23":"# hard copy the data to ensure that we don't change the original\nX_train_ext = X_train.copy()\nX_valid_ext = X_valid.copy()\n\n# Use the missing columns found in section 2.a, reminder of how these\n# columns were found\n# missing_column = [col for col in X_train.columns\n#                   if X_train[col].isnull().any()]\n\n# generate new columns we want to impute\nfor column in missing_val_train:\n    X_train_ext[column + '_missing'] = X_train_ext[column].isnull()\n    X_valid_ext[column + '_missing'] = X_valid_ext[column].isnull()\n    \n# impute the extended data\n# Imputation, setup the simple imputer and apply it to our full data\nmy_imputer = SimpleImputer()\nimputed_X_train_ext = pd.DataFrame(my_imputer.fit_transform(X_train_ext))\nimputed_X_valid_ext = pd.DataFrame(my_imputer.transform(X_valid_ext))\n\n# imputation removed column names; put them back\nimputed_X_train_ext.columns = X_train_ext.columns\nimputed_X_valid_ext.columns = X_valid_ext.columns","b7f83e2c":"# Check the MAE of this model\nscores_dict['2.c'] = get_MAE(imputed_X_train_ext, imputed_X_valid_ext, y_train, y_valid)\n\nprint(\"MAE (Extended Imputation):\")\nprint(scores_dict['2.c'])","ddd996e2":"# hard copy the data to ensure that we don't change the original\nX_ext = X.copy()\nX_test_ext = X_test.copy()\n\n# Use the missing columns found in section 2.a, reminder of how these\n# columns were found\n# missing_column = [col for col in X_train.columns\n#                   if X_train[col].isnull().any()]\n\n# generate new columns we want to impute\nfor column in combined_missing_val:\n    X_ext[column + '_missing'] = X_ext[column].isnull()\n    X_test_ext[column + '_missing'] = X_test_ext[column].isnull()\n    \n# impute the extended data\n# Imputation, setup the simple imputer and apply it to our full data\nmy_imputer = SimpleImputer()\nimputed_X_ext = pd.DataFrame(my_imputer.fit_transform(X_ext))\nimputed_X_test_ext = pd.DataFrame(my_imputer.transform(X_test_ext))\n\n# imputation removed column names; put them back\nimputed_X_ext.columns = X_ext.columns\nimputed_X_test_ext.columns = X_test_ext.columns","86cb67e3":"# Generate the predictions\ntest_prediction_2c = gen_prediction(imputed_X_ext, y, imputed_X_test_ext)","39345cc3":"# Save the prediction to our dictonary\nsubmission_dict['2.c'] = test_prediction_2c\nprint(\"Extended Imputation Submission Saved\")","5adfaeb6":"# Define the data that will be used for all tests\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\n# Define the data that will be used for all submission generation\nX_test = test_data.copy()\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","87bfd02a":"# Check for missing values in each column of training data\nmissing_val_train = [column for column in X_train.columns\n                     if X_train[column].isnull().any()]\n\n# drop columns in training and validation data\nreduced_X_train = X_train.drop(missing_val_train, axis=1)\nreduced_X_valid = X_valid.drop(missing_val_train, axis=1)","c81e3faf":"print (\"Shape of X_train: {}\".format(X_train.shape))\nprint (\"Shape of X_valid: {}\".format(X_valid.shape))\n\nprint (\"Shape of reduced_X_train: {}\".format(reduced_X_train.shape))\nprint (\"Shape of reduced_X_valid: {}\".format(reduced_X_valid.shape))","7cca0113":"# Drop the objects from our dataset\ndrop_X_train = reduced_X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = reduced_X_valid.select_dtypes(exclude=['object'])","3b754975":"print (\"Shape of drop_X_train: {}\".format(drop_X_train.shape))\nprint (\"Shape of drop_X_valid: {}\".format(drop_X_valid.shape))","6c065a25":"# Check the MAE of this model\nscores_dict['2.d'] = get_MAE(drop_X_train, drop_X_valid, y_train, y_valid)\n\nprint(\"MAE (Drop categorical variables):\")\nprint(scores_dict['2.d'])","106d7a4d":"# Check for missing values in each column of training data and test data using list comprehension\n# This is taken from section 2.a for finding columns with missing data\nmissing_val_X = [column for column in X.columns\n                 if X[column].isnull().any()]\n\nmissing_val_X_test = [column for column in X_train.columns\n                      if X_test[column].isnull().any()]\n\ncombined_missing_val = list(set(missing_val_X + missing_val_X_test))","13f8bcff":"# drop columns in training and validation data\nreduced_X = X.drop(combined_missing_val, axis=1)\nreduced_X_test = X_test.drop(combined_missing_val, axis=1)","d0d3cf1a":"print (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\n\nprint (\"Shape of reduced_X: {}\".format(reduced_X.shape))\nprint (\"Shape of reduced_X_test: {}\".format(reduced_X_test.shape))","1531ac46":"# Drop the objects from our dataset\ndrop_X = reduced_X.select_dtypes(exclude=['object'])\ndrop_X_test = reduced_X_test.select_dtypes(exclude=['object'])","3e4dee2c":"print (\"Shape of drop_X: {}\".format(drop_X.shape))\nprint (\"Shape of drop_X_test: {}\".format(drop_X_test.shape))","12fb2440":"# Generate the predictions\ntest_prediction_2d = gen_prediction(drop_X, y, drop_X_test)","697e87da":"# Save the prediction to our dictonary\nsubmission_dict['2.d'] = test_prediction_2d\nprint(\"Drop Variable Submission Saved\")","4e9c1bfb":"# All categorical columns\nobject_cols = [column for column in X_train.columns if\n               X_train[column].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [column for column in object_cols if \n                   set(X_train[column]) == set(X_valid[column])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols) - set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","9c7736e4":"# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\nprint (\"Shape of X_train: {}\".format(X_train.shape))\nprint (\"Shape of X_valid: {}\".format(X_valid.shape))\n\nprint (\"\\nShape of label_X_train after dropping bad labels: {}\".format(label_X_train.shape))\nprint (\"Shape of label_X_valid after dropping bad labels: {}\".format(label_X_valid.shape))","23d199dd":"# Apply label encoder \n# Cannot use the code shown in the course, will raise error:\n#     TypeError: Encoders require their input to be uniformly strings or numbers. Got ['float', 'str']\n# For solution:\n#     https:\/\/stackoverflow.com\/questions\/46406720\/labelencoder-typeerror-not-supported-between-instances-of-float-and-str\n\nlabel_encoder = LabelEncoder()\nfor column in set(good_label_cols):\n    label_X_train[column] = label_encoder.fit_transform(X_train[column].astype(str))\n    label_X_valid[column] = label_encoder.transform(X_valid[column].astype(str))","ee8af970":"# We cant directly calculate the MAE from this point,\n# if we do there will be some missing values, need to impute some values\nimputed_label_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\nimputed_label_X_valid = pd.DataFrame(my_imputer.transform(label_X_valid))\n\n# imputation removed column names; put them back\nimputed_label_X_train.columns = label_X_train.columns\nimputed_label_X_valid.columns = label_X_valid.columns","39b0c183":"# Check the MAE of this model\nscores_dict['2.e'] = get_MAE(imputed_label_X_train, imputed_label_X_valid, y_train, y_valid)\n\nprint(\"MAE (Label Encoding):\") \nprint(scores_dict['2.e'])","5e0e61af":"# All categorical columns\nobject_cols_full = X.columns\n\n# Columns that can be safely label encoded\ngood_label_cols_full = [column for column in object_cols_full if \n                        set(X[column]) == set(X_test[column])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols_full = list(set(object_cols_full) - set(good_label_cols_full))\n\nprint('\\nNumber of Categorical columns that will be label encoded:', len(good_label_cols_full))\nprint('Categorical columns that will be label encoded:', good_label_cols_full)\n\nprint('\\nNumber of categorical columns that will be dropped from the dataset:', len(bad_label_cols_full))\nprint('Categorical columns that will be dropped from the dataset:', bad_label_cols_full)","b56338c8":"# Separate the categorical columns and numeric columns\ncat_X = X[object_cols_full].copy()\ncat_X_test = X_test[object_cols_full].copy()\n\nnum_X = X.select_dtypes(exclude=['object']).copy()\n\nnum_X_test = X_test.select_dtypes(exclude=['object']).copy()\n\n# Drop categorical columns that will not be encoded\nlabel_cat_X = cat_X.drop(bad_label_cols_full, axis=1)\nlabel_cat_X_test = cat_X_test.drop(bad_label_cols_full, axis=1)","db03aedb":"print (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\n\nprint (\"\\nShape of cat_X: {}\".format(cat_X.shape))\nprint (\"Shape of cat_X_test: {}\".format(cat_X_test.shape))\n\nprint (\"\\nShape of num_X: {}\".format(num_X.shape))\nprint (\"Shape of num_X_test: {}\".format(num_X_test.shape))\n\nprint (\"\\nShape of label_cat_X after dropping bad labels: {}\".format(label_cat_X.shape))\nprint (\"Shape of label_cat_X_test after dropping bad labels: {}\".format(label_cat_X_test.shape))","d6d0e793":"# Impute the two different sets of data:\n\n# Impute numerical columns\nnum_imputer = SimpleImputer(strategy='mean')\n\nimputed_num_X = pd.DataFrame(num_imputer.fit_transform(num_X))\nimputed_num_X_test = pd.DataFrame(num_imputer.transform(num_X_test))\n\n# imputation removed column names; put them back\nimputed_num_X.columns = num_X.columns\nimputed_num_X_test.columns = num_X_test.columns","926758eb":"# Impute category columns\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\nimputed_label_cat_X = pd.DataFrame(cat_imputer.fit_transform(label_cat_X))\nimputed_label_cat_X_test = pd.DataFrame(cat_imputer.transform(label_cat_X_test))\n\n# imputation removed column names; put them back\nimputed_label_cat_X.columns = label_cat_X.columns\nimputed_label_cat_X_test.columns = label_cat_X_test.columns","fd931520":"# Apply label encoder \n# Cannot use the code shown in the course, will raise error:\n#     TypeError: Encoders require their input to be uniformly strings or numbers. Got ['float', 'str']\n# For solution:\n#     https:\/\/stackoverflow.com\/questions\/46406720\/labelencoder-typeerror-not-supported-between-instances-of-float-and-str\n\nlabel_encoder = LabelEncoder()\nfor column in set(good_label_cols_full):\n    imputed_label_cat_X[column] = label_encoder.fit_transform(X[column].astype(str))\n    imputed_label_cat_X_test[column] = label_encoder.transform(X_test[column].astype(str))","c26a2160":"print (\"\\nShape of imputed_label_cat_X: {}\".format(imputed_label_cat_X.shape))\nprint (\"Shape of imputed_label_cat_X_test: {}\".format(imputed_label_cat_X_test.shape))","e3f02029":"full_label_X = pd.concat([imputed_num_X, imputed_label_cat_X], axis=1)\nfull_label_X_test = pd.concat([imputed_num_X_test, imputed_label_cat_X_test], axis=1)\n\nprint (\"\\nShape of full_label_X after merge: {}\".format(full_label_X.shape))\nprint (\"Shape of full_label_X_test after merge: {}\".format(full_label_X_test.shape))","eaa9b66f":"# Generate the predictions\ntest_prediction_2e = gen_prediction(full_label_X, y, full_label_X_test)","135238c2":"# Save the prediction to our dictonary\nsubmission_dict['2.e'] = test_prediction_2e\nprint(\"Label Variable Submission Saved\")","1a688bb3":"# Investigate cardinality\nobject_cols = [column for column in X_train.columns if X_train[column].dtype == \"object\"]\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [column for column in object_cols if X_train[column].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n\nOH_X_train = X_train[low_cardinality_cols]\nOH_X_valid = X_valid[low_cardinality_cols]","a5980a6b":"# Imputation to categorical columns or we'll encounter problems with NaN\n# Impute category columns\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\nimputed_X_train = pd.DataFrame(cat_imputer.fit_transform(OH_X_train))\nimputed_X_valid = pd.DataFrame(cat_imputer.transform(OH_X_valid))\n\n# imputation removed column names; put them back\nimputed_X_train.columns = OH_X_train.columns\nimputed_X_valid.columns = OH_X_valid.columns\n","c3eb55bf":"# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)","e51c5306":"# Impute numerical columns\nnum_imputer = SimpleImputer(strategy='mean')\n\nimputed_num_X_train = pd.DataFrame(num_imputer.fit_transform(num_X_train))\nimputed_num_X_valid = pd.DataFrame(num_imputer.transform(num_X_valid))\n\n# imputation removed column names; put them back\nimputed_num_X_train.columns = num_X_train.columns\nimputed_num_X_valid.columns = num_X_valid.columns","b3c87319":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = imputed_X_train.index\nOH_cols_valid.index = imputed_X_valid.index\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([OH_cols_train, imputed_num_X_train], axis=1)\nOH_X_valid = pd.concat([OH_cols_valid, imputed_num_X_valid], axis=1)","7e935ccc":"print (\"\\nShape of num_X_train: {}\".format(num_X_train.shape))\nprint (\"Shape of num_X_valid: {}\".format(num_X_valid.shape))\n\nprint (\"\\nShape of OH_cols_train: {}\".format(OH_cols_train.shape))\nprint (\"Shape of OH_cols_valid: {}\".format(OH_cols_valid.shape))\n\nprint (\"\\nShape of OH_X_train after merge: {}\".format(OH_X_train.shape))\nprint (\"Shape of OH_X_valid after merge: {}\".format(OH_X_valid.shape))","999e9483":"scores_dict['2.f'] = get_MAE(OH_X_train, OH_X_valid, y_train, y_valid)\n\nprint(\"MAE (One-Hot Encoding):\") \nprint(scores_dict['2.f'])","35d7d07b":"# Investigate cardinality\nobject_cols_full = [column for column in X.columns if X[column].dtype == \"object\"]\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols_full = [column for column in object_cols_full if X[column].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols_full = list(set(object_cols_full)-set(low_cardinality_cols_full))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols_full)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols_full)\n\nOH_X = X[low_cardinality_cols_full]\nOH_X_test = X_test[low_cardinality_cols_full]","8b2fab0e":"# Imputation to categorical columns or we'll encounter problems with NaN\n# Impute category columns\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\nimputed_X = pd.DataFrame(cat_imputer.fit_transform(OH_X))\nimputed_X_test = pd.DataFrame(cat_imputer.transform(OH_X_test))\n\n# imputation removed column names; put them back\nimputed_X.columns = OH_X.columns\nimputed_X_test.columns = OH_X_test.columns\n","ab9c2e34":"# Remove categorical columns (will replace with one-hot encoding)\nnum_X = X.drop(object_cols_full, axis=1)\nnum_X_test = X_test.drop(object_cols_full, axis=1)","74ceeec3":"# Impute numerical columns\nnum_imputer = SimpleImputer(strategy='mean')\n\nimputed_num_X = pd.DataFrame(num_imputer.fit_transform(num_X))\nimputed_num_X_test = pd.DataFrame(num_imputer.transform(num_X_test))\n\n# imputation removed column names; put them back\nimputed_num_X.columns = num_X.columns\nimputed_num_X_test.columns = num_X_test.columns","7d1b4991":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(imputed_X))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test))\n\n# One-hot encoding removed index; put it back\nOH_cols.index = imputed_X.index\nOH_cols_test.index = imputed_X_test.index\n\n# Add one-hot encoded columns to numerical features\nOH_X = pd.concat([OH_cols, imputed_num_X], axis=1)\nOH_X_test = pd.concat([OH_cols_test, imputed_num_X_test], axis=1)","c7bbaae9":"# Generate the predictions\ntest_prediction_2f = gen_prediction(OH_X, y, OH_X_test)","a042b580":"# Save the prediction to our dictonary\nsubmission_dict['2.f'] = test_prediction_2f\nprint(\"One-Hot Encoding Submission Saved\")","284eba1e":"# Define the data that will be used for all tests\ny_full = train_data.SalePrice\nX_full = train_data.drop(['SalePrice'], axis=1)\n\n# Define the data that will be used for all submission generation\nX_test_full = test_data.copy()\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","2213c812":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","34145513":"# Preprocessing for numerical data\n# depending on choice of strategy we can get very different MAE values later\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","3bcaa917":"# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])","a5cd660c":"# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)","b3663741":"scores_dict['2.g'] = mean_absolute_error(y_valid, preds)\n\nprint(\"MAE (Pipeline):\") \nprint(scores_dict['2.g'])","26de797e":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols_full = [cname for cname in X.columns if\n                         X[cname].nunique() < 10 and \n                         X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols_full = [cname for cname in X.columns if \n                       X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols_full = categorical_cols_full + numerical_cols_full\nX = X_full[my_cols_full].copy()\nX_test = X_test_full[my_cols_full].copy()","77bc1abf":"# Preprocessing for numerical data\n# depending on choice of strategy we can get very different MAE values later\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols_full),\n        ('cat', categorical_transformer, categorical_cols_full)\n    ])","63b2fa99":"# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])","a51df98f":"# Preprocessing of training data, fit model \nclf.fit(X, y)\n\n# Preprocessing of validation data, get predictions\ntest_prediction_2g = clf.predict(X_test)","75381c87":"# Save the prediction to our dictonary\nsubmission_dict['2.g'] = test_prediction_2g\nprint(\"Pipeline Submission Saved\")","c04a8af5":"# Define the data that will be used for all tests\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\n# Define the data that will be used for all submission generation\nX_test = test_data.copy()","ef210e09":"# Take the column filtering from pipelines\ncategorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols","eaa98c7e":"# Filter down to the numerical columns\ncv_X = X[numerical_cols].copy()\ncv_test = X_test[numerical_cols].copy()","0dae5962":"# Generate the results\nresults={}\nfor index in range(1, 9):\n    results[50*index] = get_score(n_estimators=50*index, X=cv_X, y=y)","779aff7e":"# Plot the results to see where the ideal number of n_estimators is\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()","2025c320":"min_n_ests = min(results, key=results.get)\n\nprint(\"n_estimators with lowest score:\")\nprint(min_n_ests)","7aa6da8b":"scores_dict['2.h'] = min(results.values())\n\nprint(\"MAE (Pipeline):\") \nprint(scores_dict['2.h'])","dd2d62f7":"# create the pipeline\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","ad0395e4":"# Define the Model with the n_estimators with lowest score\nmodel = RandomForestRegressor(n_estimators=min_n_ests, random_state=0)","689f579a":"# Preprocessing of training data, fit model \ncv_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\ncv_X = X[my_cols].copy()\ncv_X_test = X_test[my_cols].copy()\ncv_pipeline.fit(cv_X, y)\n\n# Preprocessing of validation data, get predictions\ntest_prediction_2h = cv_pipeline.predict(cv_X_test)","0c6f38cb":"# Save the prediction to our dictonary\nsubmission_dict['2.h'] = test_prediction_2h\nprint(\"Cross Validation Submission Saved\")","60311a4a":"# Define the data that will be used for all tests\ny_full = train_data.SalePrice\nX_full = train_data.drop(['SalePrice'], axis=1)\n\n# Define the data that will be used for all submission generation\nX_test_full = test_data.copy()\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","294d1c8f":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)","72925484":"# Train model\nmy_model = XGBRegressor(random_state=0)\nmy_model.fit(X_train, y_train)\n# Predict\nprediction_1 = my_model.predict(X_valid)","b6b01c0f":"scores_dict['2.i'] = mean_absolute_error(y_valid, prediction_1)\n\nprint(\"MAE (Gradient Boost):\") \nprint(scores_dict['2.i'])","023cf9e8":"# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX = X_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX = pd.get_dummies(X)\nX_test = pd.get_dummies(X_test)\nX, X_test = X.align(X_test, join='left', axis=1)","3f71db14":"my_model = XGBRegressor(random_state=0)\nmy_model.fit(X, y)\ntest_prediction_2i = my_model.predict(X_test)","faa56aa7":"# Save the prediction to our dictonary\nsubmission_dict['2.i'] = test_prediction_2i\nprint(\"XGBoost Submission Saved\")","c924908e":"# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\n\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)","ab713a83":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators=500, learning_rate=0.05)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train,\n               early_stopping_rounds=5,\n               eval_set=[(X_valid, y_valid)],\n               verbose=False)\n               \n\n# Get predictions\nprediction_2 = my_model_2.predict(X_valid)","f87fb842":"scores_dict['2.j'] = mean_absolute_error(y_valid, prediction_2)\n\nprint(\"MAE (Parameter Tuning):\") \nprint(scores_dict['2.j'])","53cee6d3":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","b85bd507":"# Define the model\nmy_model_3 = XGBRegressor(n_estimators=500, learning_rate=0.05)\n\n# Fit the model\nmy_model_3.fit(X_train, y_train,\n               early_stopping_rounds=5,\n               eval_set=[(X_valid, y_valid)],\n               verbose=False)\n\ntest_prediction_2j = my_model_3.predict(X_test)","3390d7ae":"# Save the prediction to our dictonary\nsubmission_dict['2.j'] = test_prediction_2j\nprint(\"Parameter Tuning Submission Saved\")","2f554ae3":"# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path, index_col='Id')\ntrain_data = pd.read_csv(train_data_path, index_col='Id')\nsample_data = pd.read_csv(sample_data_path)\n\n# Define the plot style used\nsns.set_style(\"darkgrid\")","e66d5c16":"# Look at the sale price using the describe function\ntrain_data['SalePrice'].describe()","d3e1150e":"# Look at the sale price visually using a histogram\nplt.figure(figsize=(16, 8))\nsns.distplot(train_data['SalePrice'])","25335cb0":"# The documentation states that there are potential outliers in comparing\n# SalePrice and GrLivArea (Above grade (ground) living area square feet)\n# so lets see if that is true\n\nplt.figure(figsize=(16,8))\nsns.scatterplot(x=train_data['GrLivArea'], y=train_data['SalePrice'])","1bc9a47c":"# There are some pretty extreme outliers out beyond GrLivArea > 4000 with SalePrice < 20000\n# Lets draw trend lines with these points and without to see their effect\n\nplt.figure(figsize=(16,8))\nsns.regplot(x=train_data['GrLivArea'], y=train_data['SalePrice'])","f2dc60e8":"# Removing those two data points\nhigh_GrLivArea = np.where(train_data['GrLivArea'] > 4000)[0]\nlow_SalePrice = np.where(train_data['SalePrice'] < 200000)[0]\n\nprint(\"high GrLivArea points: \", high_GrLivArea)\nprint(\"\\nlow SalePrice points: \", low_SalePrice)\n\noutlier_inds = list(set(high_GrLivArea) & set(low_SalePrice))\noutlier_inds.sort()\n\nprint(\"\\noutliers: \", outlier_inds)\n\nshortened_train_data = train_data.drop(train_data.index[outlier_inds])\n\nplt.figure(figsize=(16,8))\nsns.regplot(x=shortened_train_data['GrLivArea'], y=shortened_train_data['SalePrice'])","a2d536f3":"# need to generate a correlation matrix with our data\ncorrelation_matrix = train_data.corr()\n\nplt.figure(figsize=(16,16))\nsns.heatmap(correlation_matrix, square=True)","1e6ff351":"plt.figure(figsize=(8,8))\n\n# look for the top 10 properties, need to pass number of properties we're\n# interested + 1 because SalePrice has a 1:1 correlation with itself\nnum_variables = 11 \n\ntop_cols = correlation_matrix.nlargest(num_variables, 'SalePrice')['SalePrice'].index\nshort_cm = np.corrcoef(train_data[top_cols].values.T)\n\nsns.heatmap(short_cm, annot=True, yticklabels=top_cols.values, xticklabels=top_cols.values)","578a448d":"# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path, index_col='Id')\ntrain_data = pd.read_csv(train_data_path, index_col='Id')\nsample_data = pd.read_csv(sample_data_path)","68d4a62b":"# Looking for missing data\n# Count the number of null values in our dataset\n\ncount_nulls = train_data.isnull().sum().sort_values(ascending=False)\nprint(count_nulls)","bc0bfa0f":"# Lets drop the properties where the count is 0\nnon_zero_counts = count_nulls != 0\nnz_count_nulls = count_nulls[non_zero_counts]","25330306":"# Calculate the percentage of the data that is missing\n\npercentage_nulls = (train_data.isnull().sum()\/train_data.isnull().count()) * 100\nsorted_precentages = percentage_nulls.sort_values(ascending=False)\nnz_sorted_percentage = sorted_precentages[non_zero_counts]","e9857cc6":"# Put the counts and percentages together\nmissing_data = pd.concat([nz_count_nulls, nz_sorted_percentage], keys=['Count', 'Percentage'], axis=1)\nmissing_data","efe4f1b2":"# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path)\ntrain_data = pd.read_csv(train_data_path)\nsample_data = pd.read_csv(sample_data_path)","296f895c":"# Make copies of the data so we can recall the complete data if necessary\nfull_test_dl = test_data.copy()\nfull_train_dl = train_data.copy()\nfull_combined_dl = pd.concat([full_train_dl, full_test_dl], ignore_index=True)\n\ndtypes = {\n    'MSSubClass': str,\n}\n\nfor col_, type_ in dtypes.items():\n    full_combined_dl[col_] = full_combined_dl[col_].astype(type_)","34e367fe":"print ('Full Test set:', full_test_dl.shape)\nprint ('Full Train set:', full_train_dl.shape)\nprint ('Full Combined set:', full_combined_dl.shape)","20995394":"# summary of the data frame information\nfull_combined_dl.info()","3a832fbf":"# Unlike what was done in section 3.b with the heatmap, we'll use a correlation barplot instead.\ncorrmat = full_combined_dl.corr()\n\n# Plot the barplots\nplt.figure(figsize=(10, 17))\nsns.barplot(y=corrmat['SalePrice'].sort_values().index, x=corrmat['SalePrice'].sort_values().values)\nplt.xlabel(f'correlation between SalePrice')\nplt.show()","b3882367":"for cname in full_combined_dl.columns:\n    if full_combined_dl[cname].dtype == 'object':\n        full_combined_dl[cname].fillna('None', inplace=True)\n    else:\n        full_combined_dl[cname].fillna(full_combined_dl[cname].median(), inplace=True)","570e8471":"# Check to make sure we've replaced all of the NaNs\nfull_combined_dl.isnull().sum().max()","dc433a60":"# Select the categorical columns\nfeatures_cat = [cname for cname in full_combined_dl.columns if\n                full_combined_dl[cname].dtype == \"object\"]","fd3fc3d0":"# Generate the dummies for the categorical columns\ncombined_dl_OH = full_combined_dl.join(pd.get_dummies(full_combined_dl[features_cat]))\n\nprint ('no dummies set:', full_combined_dl.shape)\nprint ('dummies set:', combined_dl_OH.shape)","eee24c32":"# Filter out the categorical columns\nnumerical_features = [cname for cname in combined_dl_OH.columns if\n                      combined_dl_OH[cname].dtype != \"object\"]","459e327c":"# Remove the Id number as well\nnumerical_features.remove('Id')","7a90c42b":"numerical_features","5c658419":"# Copy the entire combined deep learning dataframe\ntraining_dl = combined_dl_OH.copy()\n\n# Filter the entire dataframe and only keep the rows with index matching the original training set\ntraining_dl = training_dl[training_dl.Id.isin(full_train_dl.Id)]","317b2879":"# repeat the above with test data\ntesting_dl = combined_dl_OH.copy()\n\n# Filter the entire dataframe and only keep the rows with index matching the original training set\ntesting_dl = testing_dl[testing_dl.Id.isin(full_test_dl.Id)]","13f06ea9":"print ('combined_dl_OH:', combined_dl_OH.shape)\nprint ('Training_dl:', training_dl.shape)\nprint ('Testing_dl:', testing_dl.shape)","c1d0bc90":"# split the training data into properties (X) and target (y)\ntraining_dl_X = training_dl[numerical_features]\ntraining_dl_y = training_dl['SalePrice']\n\ntesting_dl_X = testing_dl[numerical_features]","c0adef39":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(training_dl_X, training_dl_y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Valid set:', X_valid.shape,  y_valid.shape)","1a3947c5":"# Normalize the data\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)\nX_test = scaler.transform(testing_dl_X)\n","d002aa03":"print ('Train X:', X_train.shape)\nprint ('Valid X:', X_valid.shape)\nprint ('Test X:', X_test.shape)","2c9b4380":"# Clear out the backend to make sure things aren't effected by other models run\ntf.keras.backend.clear_session()","d8ec2746":"# Determine the shape of our input into the model\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","5c7a8a79":"# Create the simple model\nmodel = tf.keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(64, activation='relu'),    \n    layers.Dense(1, activation='linear')\n])","a360a936":"# Compile the model with a simple optimizer and keep track of the mean errors\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mse', 'mae']\n)\n\n# Set an early stopping condition so we dont overfit\nearly_stopping = EarlyStopping(\n    monitor='val_mae',\n    patience=25, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","0d3145e9":"# Fit the model\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=1000,\n    callbacks=[early_stopping],\n    verbose=0  # reduce the output so we dont flood the notebook\n)","4c5ac647":"# Evaluate our simple model\nmodel.evaluate(X_valid,y_valid)","eaf58cd2":"# Plot the mean absolute error\nhistory_df = pd.DataFrame(history.history)\n\nplt.figure(figsize=(8, 6))\nplt.plot(history.history['mae'], label='Training MAE')\nplt.plot(history.history['val_mae'], label='Validation MAE')\nplt.ylabel('MAE')\nplt.xlabel('Epoch Number')\nplt.legend()\nplt.show()\n\n# We can see that initially it decreases very quickly for the first ~100 epochs then slows down\n# realistically the 1000 epochs does help but not to the degree we might want","d354570c":"# Plotting the mean squared error\n\nplt.figure(figsize=(8, 6))\nplt.plot(history.history['mse'], label='Training MSE')\nplt.plot(history.history['val_mse'], label='Validation MSE')\nplt.ylabel('MSE')\nplt.xlabel('Epoch Number')\nplt.legend()\nplt.show()\n\n# Again a very similar figure as the mean absolute error","6c3b264a":"# Calculate the y values to compare the the actual results\n\nyhat_valid=model.predict(X_valid)\nyhat_train=model.predict(X_train)","024f0b44":"# Generate a plot of the fitted values compared to the actual values in our training data\n\nplt.figure(figsize=(8, 6))\n\nax1 = sns.distplot(y_train, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(yhat_train, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\nplt.title('Actual vs Fitted Values for Price')\n\nplt.show()\n\n# The general shape is similar but the peak is a bit off and there are\n# more differences as we move to higher prices ","1c480ba8":"# Generate a plot of the fitted values compared to the actual values in our validation data\n\nplt.figure(figsize=(8, 6))\n\nax2 = sns.distplot(y_valid, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(yhat_valid, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax2)\n\nplt.title('Actual vs Fitted Values for Price')\n\n# Again, the general shape is similar but there are obvious differences","6701debf":"scores_dict['4.f'] = min(history.history['val_mae'])\n\nprint(\"MAE (Simple Deep Learning):\") \nprint(scores_dict['4.f'])","c4ab5e18":"# Get predictions\nprediction_4f = model.predict(X_test)","415c60ba":"# Save the prediction to our dictonary\nsubmission_dict['4.f'] = prediction_4f.flatten()\nprint(\"Simple Deep Learning Model Submission Saved\")","ac359605":"# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path)\ntrain_data = pd.read_csv(train_data_path)\nsample_data = pd.read_csv(sample_data_path)","6128ac2c":"# Combine the data together to deal with any easily apply any transforms\ncombined_data = pd.concat([test_data, train_data])\ncombined_data.head()","dace0e59":"# Use One Hot Encoding to categorical data\ncategorical_columns = [column for column in combined_data.columns\n                       if combined_data[column].dtype == \"object\"]\ncategorical_data = pd.get_dummies(combined_data.loc[:, categorical_columns],\n                                  drop_first=True)\ncategorical_data.head()","6d82cc82":"# Baseline model will not transform numerical data\nnumerical_columns = [column for column in combined_data.columns\n                     if combined_data[column].dtype != \"object\"]\nnumerical_data = combined_data[numerical_columns].drop(\"Id\", axis=1)\nnumerical_data.head()","108cac8f":"# Combine the transformed data back together\nbaseline_data = pd.concat([combined_data['Id'], categorical_data, numerical_data],\n                          axis=1)\nbaseline_data.head()","16b29462":"# Split the data into training and testing sets again\nbase_test_data = baseline_data[:test_data.shape[0]]\nbase_train_data = baseline_data[test_data.shape[0]:]\n\nprint (\"Shape of base_train_data: {}\".format(base_train_data.shape))\nprint (\"Shape of base_test_data: {}\".format(base_test_data.shape))","d27a90ef":"# Define the data that will be used for all tests\ny_train_full_base = base_train_data.SalePrice\nX_train_full_base = base_train_data.drop(['SalePrice', 'Id'], axis=1)\ntest_full_base = base_test_data.drop(['SalePrice', 'Id'], axis=1)","0a03b496":"print (\"Shape of X_train_full: {}\".format(X_train_full_base.shape))\nprint (\"Shape of test_full: {}\".format(test_full_base.shape))","95d31755":"# Break off validation set from training data\nX_train_base, X_valid_base, y_train_base, y_valid_base = train_test_split(X_train_full_base, y_train_full_base,\n                                                                          train_size=0.8, test_size=0.2,\n                                                                          random_state=0)","cbcd7edc":"lgb_train_base = lgb.Dataset(X_train_base, y_train_base)\nparams = {'objective': 'regression',\n          'metric': {'rmse'}}\ngbm_base = lgb.train(params, lgb_train_base)\nprediction_gbm_base = gbm_base.predict(X_valid_base)","1b9b7f87":"scores_dict['5.a'] = mean_absolute_error(y_valid_base, prediction_gbm_base)\n\nprint(\"Feature Engineering - GBM (Baseline):\") \nprint(scores_dict['5.a'])","08a9ec87":"lgb_train_full_base = lgb.Dataset(X_train_full_base, y_train_full_base)\ngbm_full_base = lgb.train(params, lgb_train_full_base)\nprediction_5a = gbm_full_base.predict(test_full_base)","9ae2ca32":"# Save the prediction to our dictonary\nsubmission_dict['5.a'] = prediction_5a","5dd37bc6":"linear_data = numerical_data.copy()\nlinear_data.head()","3ce9b1bf":"linear_skew = linear_data.apply(lambda x: skew(x.dropna())).sort_values()\nlinear_skew.plot.barh(figsize=(12,8), title=\"Skewness of Untransformed Data\")\nplt.show()","a31da790":"print(\"total skewness of unmodified data:\", sum(abs(linear_skew)))","608cf4d2":"# we take ln(1 + value) to ensure we don't run into issues where the value = 0\nln1p_data = np.log1p(numerical_data.copy())\nln1p_data.head()","8197aa58":"ln1p_skew = ln1p_data.apply(lambda x: skew(x.dropna())).sort_values()\nln1p_skew.plot.barh(figsize=(12,8), title=\"Skewness of ln(1 + Data)\")\nplt.show()","99535302":"print(\"total skewness of ln(1 + data):\", sum(abs(ln1p_skew)))","90ff46d1":"# we take sqrt(value)\nsqrt_data = np.sqrt(numerical_data.copy())\nsqrt_data.head()","630e880d":"sqrt_skew = sqrt_data.apply(lambda x: skew(x.dropna())).sort_values()\nsqrt_skew.plot.barh(figsize=(12,8), title=\"Skewness of sqrt(Data)\")\nplt.show()","de33fc66":"print(\"total skewness of sqrt(data):\", sum(abs(sqrt_skew)))","9c3510fc":"combined_skew = abs(pd.concat([linear_skew, ln1p_skew, sqrt_skew], axis=1)).rename(columns={0:'unscaled', 1:'natural log', 2:'sqrt'})\ncombined_skew.plot.barh(figsize=(20,16), title=\"Skewness of Data Using Different Transforms\", width=0.8)\nplt.show()","0353330a":"combined_ln1p_data = pd.concat([combined_data['Id'], categorical_data, ln1p_data],\n                                axis=1)\ncombined_sqrt_data = pd.concat([combined_data['Id'], categorical_data, sqrt_data],\n                                axis=1)","f62d49ef":"# Split the data into training and testing sets again\nln1p_test_data = combined_ln1p_data[:test_data.shape[0]]\nln1p_train_data = combined_ln1p_data[test_data.shape[0]:]","f99f94c0":"# Define the data that will be used for all tests\ny_train_full_ln1p = ln1p_train_data.SalePrice\nX_train_full_ln1p = ln1p_train_data.drop(['SalePrice', 'Id'], axis=1)\ntest_full_ln1p = ln1p_test_data.drop(['SalePrice', 'Id'], axis=1)","3acdffac":"# Break off validation set from training data\nX_train_ln1p, X_valid_ln1p, y_train_ln1p, y_valid_ln1p = train_test_split(X_train_full_ln1p, y_train_full_ln1p,\n                                                                          train_size=0.8, test_size=0.2,\n                                                                          random_state=0)","ebd008b0":"lgb_train_ln1p = lgb.Dataset(X_train_ln1p, y_train_ln1p)\nparams = {'objective': 'regression',\n          'metric': {'rmse'}}\ngbm_ln1p = lgb.train(params, lgb_train_ln1p)\nprediction_gbm_ln1p = gbm_ln1p.predict(X_valid_ln1p)","7f78a645":"scores_dict['5.b'] = mean_absolute_error(np.expm1(y_valid_ln1p),\n                                         np.expm1(prediction_gbm_ln1p))\n\nprint(\"Feature Engineering - GBM (Log Transform):\") \nprint(scores_dict['5.b'])","e849bd5c":"lgb_train_full_ln1p = lgb.Dataset(X_train_full_ln1p, y_train_full_ln1p)\ngbm_full_ln1p = lgb.train(params, lgb_train_full_ln1p)\nprediction_5b = gbm_full_ln1p.predict(test_full_ln1p)","6721b1f0":"# Save the prediction to our dictonary\nsubmission_dict['5.b'] = np.expm1(prediction_5b)","608a2e27":"# Split the data into training and testing sets again\nsqrt_test_data = combined_sqrt_data[:test_data.shape[0]]\nsqrt_train_data = combined_sqrt_data[test_data.shape[0]:]","288bb279":"# Define the data that will be used for all tests\ny_train_full_sqrt = sqrt_train_data.SalePrice\nX_train_full_sqrt = sqrt_train_data.drop(['SalePrice', 'Id'], axis=1)\ntest_full_sqrt = sqrt_test_data.drop(['SalePrice', 'Id'], axis=1)","6cefdce6":"# Break off validation set from training data\nX_train_sqrt, X_valid_sqrt, y_train_sqrt, y_valid_sqrt = train_test_split(X_train_full_sqrt, y_train_full_sqrt,\n                                                                          train_size=0.8, test_size=0.2,\n                                                                          random_state=0)","9fc29fb3":"lgb_train_sqrt = lgb.Dataset(X_train_sqrt, y_train_sqrt)\nparams = {'objective': 'regression',\n          'metric': {'rmse'}}\ngbm_sqrt = lgb.train(params, lgb_train_sqrt)\nprediction_gbm_sqrt = gbm_sqrt.predict(X_valid_sqrt)","7ba8aa15":"scores_dict['5.c'] = mean_absolute_error(np.square(y_valid_sqrt),\n                                         np.square(prediction_gbm_sqrt))\n\nprint(\"Feature Engineering - GBM (Square Root Transform):\") \nprint(scores_dict['5.c'])","87bfc361":"lgb_train_full_sqrt = lgb.Dataset(X_train_full_sqrt, y_train_full_sqrt)\ngbm_full_sqrt = lgb.train(params, lgb_train_full_sqrt)\nprediction_5c = gbm_full_sqrt.predict(test_full_sqrt)","fb296568":"# Save the prediction to our dictonary\nsubmission_dict['5.c'] = np.square(prediction_5c)","336fe579":"# Function to load in data\ndef load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n\n# Function to clean the data\ndef clean(df):\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them\n    # with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"Threeseasonporch\",\n    }, inplace=True,\n    )\n    return df","5a1205b0":"# Function to encode the data\n# The numeric features are already encoded correctly (`float` for\n# continuous, `int` for discrete), but the categoricals we'll need to\n# do ourselves. Note in particular, that the `MSSubClass` feature is\n# read as an `int` type, but is actually a (nominative) categorical.\n\n# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\",\n                \"LandContour\", \"LotConfig\", \"Neighborhood\",\n                \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\",\n                \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\",\n                \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\",\n                \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df\n","29cda5c8":"# Impute the data\ndef impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","c363a7a2":"# Now lets load in the data\ndf_train, df_test = load_data()","36d96af5":"# Lets look at the data\ndisplay(df_train)\ndisplay(df_test)","daa07ee2":"# Define two utility functions that'll be used to get the mutual information scores\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","c70d3290":"# Lets look at these mutual information scores\nX = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)\nmi_scores","077e6e5d":"# Lets remove any features with scores of 0\ndef drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]\n\nX = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores)","a7d8bd8f":"# Label encoding for categorical features\ndef label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X\n\n# Create the features from the previous assignment\n# LivLotRatio and Spaciousness\ndef mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    # This feature ended up not helping performance\n    # X[\"TotalOutsideSF\"] = \\\n    #     df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n    #     df.Threeseasonporch + df.ScreenPorch\n    return X\n\n# Create the feature that shows the interaction between \n# BldgType and GrLivArea\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n# Describe the number of outdoor areas\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n# Roughly group the MSClasses into easier groups\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n# Group by neighbourhood\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X","1f87d7a9":"cluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]\n\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","77ccaad6":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","06a21312":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","07139b7f":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","cc59bae8":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new","89e07cc5":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","e6d2dbf2":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Lesson 2 - Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    # X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering\n    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]","d69d398a":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)","a21421f3":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]","1f0e14b0":"X_train_FE, X_valid_FE, y_train_FE, y_valid_FE = train_test_split(X_train, y_train,\n                                                                  train_size=0.8, test_size=0.2,\n                                                                  random_state=0)","a887249d":"xgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train_FE, np.log(y_train_FE))\nprediction_FE = np.exp(xgb.predict(X_valid_FE))","191ea258":"scores_dict['5.d'] = mean_absolute_error(y_valid_FE, prediction_FE)\n\nprint(\"Feature Engineering - Full Feature Engineering:\") \nprint(scores_dict['5.d'])","51139c65":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\nprediction_5d = np.exp(xgb.predict(X_test))","3be41e6d":"submission_dict['5.d'] = prediction_5d","f33c57cd":"print(\"MAE values generated:\")\nfor i in scores_dict:\n    print(i + \" : \" + str(round(scores_dict[i], 2)))\n    \nmin_key = min(scores_dict, key=scores_dict.get)\n\nprint(\"\\nMethod with lowest MAE:\")\nprint(min_key)\n\nsubmission = submission_dict[min_key]\n# print(submission)\n\noutput = pd.DataFrame({'Id': sample_data.Id,\n                       'SalePrice': submission})\noutput.to_csv('submission.csv', index=False)\nprint(\"\\nOutput generated as submission.csv\")","01107879":"<a id=\"subsec-common\"><\/a>\n## Common Code","35703bb9":"<a id=\"subsec-4b\"><\/a>\n## Part 4.b Simple EDA\nWe'll just output a summary of our data and look at a graphical output of which variables seem most correlated to sales price. This is largely a reminder of what was covered in [section 3](#sec-3).","9d91a182":"### PCA\nLet's find the principle components","796a2fff":"<a id=\"subsec-imports\"><\/a>\n## Imports","9c18f224":"There are quite a few features that have significant skewness, we are going to try taking the natural log and the square root of the features to determine if the skewness decreases.","fa87c23b":"<a id=\"subsec-5b\"><\/a>\n## Part 5.b Simple Numerical Transforms - Logarithm\nAmong the basic feature engineering techniques taught in the course, most are categorical encoding methods which resulted in very small changes in the accuracy of the prediction. In our case we used One hot Encoding in producing our baseline so we wont rerun our model with a large number of different encoding methods. Instead we will look at some other simple techniques to transform our data to improve predictions. One method is to transform numerical features to constrain outliers. These numerical transformations are unlikely to change our predictions very much as we are using a tree-based model but it is worth trying to see if this is true. In this subsection we will see if taking the logarithm of numerical features results in a significant change in predictions.","63d84f96":"### Quick data property checks","b00d87c0":"<a id=\"subsec-2j\"><\/a>\n## Part 2.j XGBoost (Parameter Tuning)\n## Testing Parameter Tuning","d05abd37":"## Testing the Effectiveness of Gradient Boost","6dfbc0a5":"### Data Preprocessing\n\n```\n# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path)\ntrain_data = pd.read_csv(train_data_path)\nsample_data = pd.read_csv(sample_data_path)\n\n```","55cde1b9":"## Testing Numerical Transform - Logarithm","9270755f":"<a id=\"sec-N\"><\/a>\n# Part N. Determining the Best Model\nIn this section we simply check which approach produced the lowest mean absolute error (MAE) and use that model to generate the submission. The name of the given method is the key in the dictionary pointing to the MAE value.","a07a3159":"## Generate the output for label encoding\n\nIf we directly follow the same steps as above for calculating the MAE we encounter an error where there are remaining NaN in the data set. To solve this we need to split the data into categorical and numerical data and work with them separately. Once we have processed the data in two chunks we can concat them back together then generate the predictions.\n","a4f6d536":"## Generate the output for simple imputation","aeecefaf":"This section will use the approach where we just drop categorical variables\n## Test the effectiveness of dropping variables","0ab3fcfb":"<a id=\"subsec-2b\"><\/a>\n## Part 2.b Missing Values (Simple Imputation)\nThis section will use the approach where we use imputation to fill in missing data\n## Test the effectiveness of simple imputation","a3d23a86":"<a id=\"subsec-2a\"><\/a>\n## Part 2.a Missing Values (Dropping values)\nThis section will use the approach where we simply drop any columns missing data\n## Test the effectiveness of dropping values","328a4448":"### Generating the Submission for Full Feature Engineering","1ad1cf52":"<a id=\"sec-preamble\"><\/a>\n# **Preamble**\n\nThis is meant to be a rolling notebook where we analyze the \"House Prices: Advanced Regression Techniques\" dataset using the techniques learned in the coursers offered by Kaggle. Currently the following courses have been completed with italicized courses being relevant to this notebook:\n1. Python\n2. _Intro to Machine Learning_\n3. _Intermediate Machine Learning_\n4. _Data Visualization_\n5. _Pandas_\n6. Intro to SQL\n7. Advanced SQL\n8. _Intro to Deep Learning_\n9. Computer Vision\n10. Data Cleaning\n11. Geospatial Analysis\n12. Machine Learning Explainability\n13. Microchallenges\n14. _Feature Engineering_\n\nCurrently in progress:\n\nWhile I have previous experience in some of these courses, I will try to only update this notebook with concepts\/ideas introduced in the courses.\n\n## **Goals**\n\nThe goal of the analysis is to predict the sale value of each house. For a given ID in the test set we want to predict the SalePrice value generated from the training set.\n\n## **Notes**\n\nThis notebook is updated after each course is finished.\n\n* The imports in part 1 were commented out and moved to a combined part 0 section\n* Shared code such as the reading in of data was consoldated into part 0\n* The most effective ML method was the XGBoost method using parameter tuning. This results in a score of 0.13719 which is top 42% of scores.\n* Due to the order the courses were taken in, the data visualization will come _AFTER_ the intermediate ML which doesn't make much practical sense. In principle one would use data visualization to perform a feature exploration and determine which properties are relevant to their analysis.\n* The pandas course does not explictly add any additional tools to analyze the housing data but we can use it to look at which columns are missing large amounts of rows.\n* Again, similar to how data visualization came after intermediate ML, ML explainability and Feature engineering will also come later in the notebook.\n* Interestingly, the simple neural network doesn't perform better than any of the intermediate ML techniques.\n* Machine Learning Explainability covers material that fits into exploratory data analysis which we do in using the data visualization course.","81989ccc":"## Generating Submission for Square Root Transform","bdf836f5":"## **Categorical Variables Shared Code**\nThis section will contain predictions done using the techniques taught in the intermediate machine learning course in regards to dealing with categorical variables","d075413a":"## Testing Cross-validation","884a5fc3":"### Create features\nNow we're going to start developing the dataset, this is where we should add more feature engineering as we think they could help.","356377c9":"Using the simplified complete feature engineering example from the course resulted in the best result which isn't unexpected. One of the key parts of data analysis is knowing your data and clearing improving your data fed into your model results in better outputs. Our solution could be improved by applying more robust feature engineering to the data but we will be stopping here for now with this notebook. I may revisit it later on when I learn more skills.","779293a0":"Based purely on this simple check of skewness the natural log performs slightly better but we will generate MAE values with both numerical transforms to determine if a given transform generates better predictions.","0ec86d72":"### Create the functions that are used in the Kaggle example.\nThis is done in four steps\n1. Load in the data\n2. Clean the data\n3. Encode the data\n4. Impute the data","999c92c0":"<a id=\"sec-5\"><\/a>\n# Part 5. Feature Engineering\n\n\nFollowing the order of the courses offered on Kaggle, we will use some of the techniques learned in [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering) to hopefully improve the MAE of our models. Similar to what was done in the course, we will use a lightGBM model as the baseline.","dfedeeea":"Again, similar to what was found in [section 3.b](#subect-3b) we find that there are clearly properties that more strongly correlate to SalePrice, for a more in depth explanation see [section 3](#sec-3).","9f674338":"<a id=\"sec-1\"><\/a>\n# **Part 1. Intro to Machine Learning**\nThis section is a brief look at the material covered in [Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)\n<a id=\"subsec-1a\"><\/a>\n## Part 1.a Using a Random Forest Regressor\n* This first submission is unlikely to score very highly as we are only using simple techniques but its a good place to start.\n* This submission scored 0.18806 which in the top 79%, we can definitely improve on this score with more advanced analysis.","075023c7":"## Generate the output for dropping variables","3a86e619":"The bottom row is the SalePrice and we can see which properties seem to correlates most strongly with it.\n1. OverallQual\n2. GrLivArea\nThese two properties seem to be the strongly correlated but we can fiddle with heatmap properties to find the ones that are most relevant.","581f22e2":"### Feature Utility Scores\nLet's compute a utility score for a feature","b4c1cff1":"<a id=\"subsec-4e\"><\/a>\n## Part 4.e Creating and Training a Model\nThis is the section where we will generate and train the deep learning model","2a0d9dad":"## Generate the submission dropping missing values","e96d224c":"## Test using Random Forest Regressor\nThis is the simplest approach, I wont include the attempts using other methods from the intro course","018024dd":"<a id=\"sec-0\"><\/a>\n# **Part 0. Imports, Shared Functions and Common Code**\n\nAfter finishing the intermediate course it became clear that having a combined section for imports and any shared functions would be best","cb0acc03":"# Table of Contents\n\n\n\n* [Preamble](#sec-preamble)\n* [Part 0 - Imports, Shared Functions and Common Code](#sec-0)\n    * [Imports](#subsec-imports)\n    * [Shared Functions](#subsec-shared)\n    * [Common Code](#subsec-common)\n* [Part 1 - Intro To Machine Learning](#sec-1)\n    * [Part 1.a - Random Forest Regressor](#subsec-1a)\n* [Part 2 - Intermediate Machine Learning](#sec-2)\n    * [Part 2.a - Missing Values (Dropping Values)](#subsec-2a)\n    * [Part 2.b - Missing Values (Simple Imputation)](#subsec-2b)\n    * [Part 2.c - Missing Values (Extended Imputation)](#subsec-2c)\n    * [Part 2.d - Categorical Variables (Drop Categorical Variables)](#subsec-2d)\n    * [Part 2.e - Categorical Variables (Label Encoding)](#subsec-2e)\n    * [Part 2.f - Categorical Variables (One-Hot Encoding)](#subsec-2f)\n    * [Part 2.g - Intermediate Machine Learning - Pipelines](#subsec-2g)\n    * [Part 2.h - Intermediate Machine Learning - Cross Validation](#subsec-2h)\n    * [Part 2.i - XGBoost (Gradient Boost)](#subsec-2i)\n    * [Part 2.j - XGBoost (Parameter Tuning)](#subsec-2j)\n* [Part 3 - Exploratry Data Analysis](#sec-3)\n    * [Part 3.a - Visualizing SalePrice](#subsec-3a)\n    * [Part 3.b - Heatmaps Comparing Properties](#subsec-3b)\n    * [Part 3.c - Missing Data](#subsec-3c)\n* [Part 4 - Intro to Deep Learing](#sec-4)\n    * [Part 4.a - Initializing the Data](#subsec-4a)\n    * [Part 4.b - Simple EDA](#subsec-4b)\n    * [Part 4.c - Dealing with Missing Data](#subsec_4c)\n    * [Part 4.d - Setting up the Training and Testing Data](#subsec-4d)\n    * [Part 4.e - Creating and Training a Model](#subsec-4e)\n    * [Part 4.f - Generating the Submission](#subsec-4f)\n* [Part 5 - Feature Engineering](#sec-5)\n    * [Part 5.a - Baseline lightGBM](#subsec-5a)\n    * [Part 5.b - Numerical Transforms - Logarithm](#subsec-5b)\n    * [Part 5.c - Numerical Transforms - Square Root](#subsec-5c)\n    * [Part 5.d - Complete Feature Engineering](#subsec-5d)\n* [Part N - Determining the Best Model](#sec-N)","b06fa988":"<a id=\"subsec-4f\"><\/a>\n## Part 4.f Generate The Submission","efeca1c0":"## Generate the output for One Hot Encoding\nWe can effectively just copy the code for generating the MAE value and change the inputs to take the entire dataset instead.","bc29656d":"<a id=\"subsec-2c\"><\/a>\n## Part 2.c Missing Values (Extended Imputation)\nIn this section we will try to extend the simple imputation by only working on the columns with missing data\n## Test the effectiveness of extended imputation","0af434b4":"<a id=\"subsec-4c\"><\/a>\n## Part 4.c Dealing With Missing Data\nWe can see from the above EDA that there are some columns with significant amounts of data missing. For example, PoolQC is missing data in almost every row, so this needs to be dealt with. One method of dealing with missing data is to simply drop any columns or rows with missing values. If we were to drop any rows with NaN we would quickly reduce our dataset to a handful of rows making this a terrible option. Since we have covered imputation in [section 2](#sec-2), we will fill in the NaN with either 'None' if the column is an object or the median value in numerical categories.","db655c9b":"<a id=\"subsec-5a\"><\/a>\n## Part 5.a Baseline lightGBM\nThis section will make a prediction using only the lightGBM model without any feature engineering to give us a baseline.","b6b25f3d":"## Generate Submission using Log Transform","eb2f6539":"## Generating the Submission for Gradient Boost","b0dbe2e0":"<a id=\"subsec-4d\"><\/a>\n## Part 4.d Setting up the Training and Testing Data\nNow that we've dealt with the NaNs we can split the data into training and testing data","bd7cae62":"Doing this we can see in descending order the most important properties are:\n1. OverallQual: Rates the overall material and finish of the house\n2. GrLivArea: Above grade (ground) living area in square feet\n3. GarageCars: Size of garage in car capacity\n4. GarageArea: Size of garage in square feet\n5. TotalBsmtSF: Total square feet of basement area\n6. 1stFlrSF: First Floor square feet\n7. FullBath: Full bathrooms above grade\n8. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n9. YearBuilt: Original construction date\n10. YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)","bd641c3e":"<a id=\"subsec-3c\"><\/a>\n## Part 3.c Missing Data Analysis\nThe [pandas course](https:\/\/www.kaggle.com\/learn\/pandas) on Kaggle is a good introduction to the package but does not offer many additional tools to analyze the housing prices. One technique taught in the course is filtering and grouping data by properties so in this section we will take a look at columns that contain missing rows. This analysis should also be done prior to modelling similar to our \"visualization\" section but we will do it here instead. Determining where data is missing is important for the analysis, the questions we would like to answer when looking for missing data are the following:\n1. Is the missing data structured or is it random?\n2. How much data is missing and is that data relevant to our analysis?\n\nIf the missing data is structured then there can be additional insight that can be gained in analyzing the missing data and why it is missing.\nIf the missing data doesn't matter to our analysis then removing it from our model can be a perfectly fine approach but if it plays a significant role in analysis then our model must be robust in dealing with the missing rows.","5f8c073c":"<a id=\"subsec-2i\"><\/a>\n## Part 2.i XGBoost (Gradient Boost)\nIn all of our previous sections we have been using random forest, here we will be using a different method called gradient boosting. This method should yield much better results.\n## Shared Code","0f9be1d4":"### Create the Dataset","1c3f0e38":"\nIn regards to this data we can draw some comparisons between some of them\n\n* GarageCars and GarageArea are effectively describing the same thing.\n* TotRmsAbvGrd and GrLivArea are similar and also describe the total space above ground\n* There is strong correlation between 1stFlrSF and TotalBsmtSF likely suggesting that if you have a large basement then you'll also have a large ground floor.\n* There is effectively no correlation between when the house was built and the total square ft of the home.","036c324e":"Now we need to apply One Hot Encoding to convert the categorical features into numerical ones","7fc2cd86":"<a id=\"subsec-2g\"><\/a>\n## Part 2.g Intermediate Machine Learning - Pipelines\nThis section will demonstrate the use of pipelines. Pipelines won't necessarily improve our MAE score but it does bundle the preprocessing and modelling steps together which will streamline our code","c765d36c":"Looking at the output above, four properties have over 50% of its data missing and two other properties have over 10% of its data missing. For what we should be doing with these properties I'm not currently sure but I assume I'll learn in the \"feature engineering\" course. With over 90% of the data missing in PoolQC, MiscFeature and Alley I suspect that outright dropping these columns from the analysis would be reasonable.\n\nOther properties we can see in the missing data analysis is that there is correlation between certain properties.\n* GarageType, GarageCond, GarageFinish, GarageQual and GarageYrBlt all appear to have the same number of values and are likely missing properties of the same garages. \n* BsmtFinType2 and BsmtExposure appear to be correlated similar to the Garage properties.\n* BsmtQual, BsmtCond and BsmtFinType1 are likely correlated as well and are only one count off from the other two basement properties.\n* MasVnrArea and MasVnrType are likely correlated.\n* There is only one missing electrical value.\n\nIn regards to how to handle this data we will revisit it later on when we learn more about feature engineering but naively I would remove any data that is missing over 10% of it's data. Following that if there is a property in the [heatmap](#subsec-7b) which can effectively replace any of the columns here missing data I would also remove that column. One column I would avoid removing would likely be \"Electrical\", there is only one missing value and it seems more reasonable to just ignore that row instead of removing the entire column.","13f83472":"<a id=\"subsec-2f\"><\/a>\n## Part 2.f Categorical Variables (One-Hot Encoding)\nHere we will use one-hot encoding where we create new columns that indicate the presence or absence of values in the original data.\n## Testing One-Hot Encoding","2bafa102":"<a id=\"subsec-3b\"><\/a>\n## Section 3.b Heatmaps of the Data\nThere are too many columns of data to individually compare to the SalePrice. Here we will be using a heatmap to see how each property correlates to SalePrice using a Heatmap.","534d7b80":"### Hyperparameter Tuning","1a54ba8d":"<a id=\"subsec-3a\"><\/a>\n## Part 3.a Analyzing \"SalePrice\"\nFirst thing we'll check out is how the main property of SalePrice changes in relation to other properties. This section and the subsequent section contains techniques and material learned in the [data visualization course](https:\/\/www.kaggle.com\/learn\/data-visualization)","1cfa02b4":"<a id=\"subsec-2h\"><\/a>\n## Part 2.h Intermediate Machine Learning - Cross-Validation\nMachine learning is iterative and is a better way to validate our data, but it shouldn't lead to a strong result without a solid algorithm to start with. For the test portion of this we will still use the full data set and choose the number of estimators that produces the best result for the submission.","afa5d3e5":"## Generate the Submission using Random Forest Regressor","51a4e828":"<a id=\"sec-2\"><\/a>\n# **Part 2.Intermediate Machine Learning**\n* This section will contain predictions done using the techniques taught in [intermediate machine learning course](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)\n* The score using these techniques was 0.14855 which is in the top 58%","5e5186e7":"## Test the effectiveness of pipelines","0d39a035":"<a id=\"subsec-2d\"><\/a>\n## Part 2.d Categorical Variables (Drop Categorical Variables)\n","3b10a974":"<a id=\"sec-3\"><\/a>\n# Part 3. Exploratory Data Analysis\nThese sections will be devoted to exploring the data and seeing what we can learn about the dataset","27d404a8":"## Generate Submission using Parameter Tuning\nUnlike the other methods, parameter tuning requires a training set, a validation set, and a test set","1f07f05a":"It looks like the SalePrice is strongly peaked at ~15000 with a longer tail towards higher prices.","0ea635e2":"<a id=\"subsec-4a\"><\/a>\n## Part 4.a Initializing the Data\nLoading in the data and defining any setup any functions we need","ea2541e7":"## Generating Submission Using Cross-validation","bc093e7e":"### Target encoding","5d806355":"<a id=\"subsec-shared\"><\/a>\n## Shared Functions","2ce29dbb":"<a id=\"subsec-5c\"><\/a>\n## Part 5.c Simple Numerical Transforms - Square Root\nMost of the preliminary exploratory analysis is shown in the above [section](#subsec-5b). This section will simply calculate the MAE using lightGBM with a square root transform applied to the data.\n\n## Testing Numerical Transform - Square Root","5b97a71c":"Superimposing all three skewness values on one plot we can visually see how these different transforms compare.","4b7d63f3":"It looks like removing the two outliers has reduced the spread of our trend and properly handling outliers should be an important step in the analysis. It's important to note that removing outliers is not always safe and should be done with caution. A safer option moving forward should be to make the machine learning model more robust to outliers. Unfortunately, I have not learned this skill yet from the courses so we will not be applying these techniques yet.","b716c21b":"## Testing the Baseline lightGBM model","5765faf4":"## Generate the output for extended imputation","1a4310bf":"<a id=\"subsec-2e\"><\/a>\n## Part 2.e Categorical Variables (Label Encoding)\nThis section will use the approach where we assign a unique value to a different integer\n## Test the effectiveness of converting labels to integer values","99524d4f":"## Generate the output for pipelines","0639e26d":"Numerically, the total skewness of the data has decreased. The type of logarithm should not play an effect on the results as it would scale all values similarly. Next we check if taking the square root of values is more effective.","04f96ba9":"## Generate Submission using Baseline lightGBM model","88dd4552":"<a id=\"sec-4\"><\/a>\n# Part 4. Intro to Deep Learning\n* These sections will focus on the introductory material in the [deep learning course](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning).\n* This section will be more akin to the [Intro to Machine Learning](#sec-1) than [Intermediate Machine Learning](#sec-2) section, so we will produce models that don't quite match up with the topics in the course.","e7061a23":"### K-means Clustering\n","906241e5":"<a id=\"subsec-5d\"><\/a>\n## Part 5.d Complete Feature Engineering\nWe're going to be following the feature engineering example from the course offered on Kaggle.","c371b083":"### Testing the Full Feature Engineering"}}