{"cell_type":{"5b30f3dc":"code","29f7da02":"code","ff5dad6f":"code","8a5a14b5":"code","2f663e9d":"code","d3b5b208":"code","719f4459":"code","2b363eb8":"code","337e2dea":"code","513c3f20":"code","e0627fd7":"code","8b394e2e":"code","b0700606":"code","e99336f8":"markdown","1e47955d":"markdown","b7727ca0":"markdown","0f3d91cd":"markdown","d84affc8":"markdown","12af6377":"markdown","227710a3":"markdown","2b3c59e5":"markdown","3decc33c":"markdown","e1643af4":"markdown","69269fa7":"markdown","cf7d8dca":"markdown","e673c62e":"markdown"},"source":{"5b30f3dc":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","29f7da02":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nfrom spacy.lang.hi import Hindi\nfrom spacy.lang.ta import Tamil\nfrom spacy.lang.hi import STOP_WORDS as hindi_stopwords\nfrom spacy.lang.ta import STOP_WORDS as tamil_stopwords\n\n\nseed=111\nnp.random.seed(seed)\n\n%config IPCompleter.use_jedi = False","ff5dad6f":"# Path to the data diectory\ndata_dir = Path(\"..\/input\/chaii-hindi-and-tamil-question-answering\/\")\n\n# Read the training and test csv files\ntrain_df = pd.read_csv(data_dir \/ \"train.csv\", encoding=\"utf8\")\ntest_df = pd.read_csv(data_dir \/ \"test.csv\", encoding=\"utf8\")\n\n# How many training and test samples have been provided?\nprint(\"Number of training samples: \", len(train_df))\nprint(\"Number of test samples: \", len(test_df))","8a5a14b5":"train_df.head()","2f663e9d":"test_df.head()","d3b5b208":"plt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"language\")\nplt.show()","719f4459":"# Get the actual count values\ntrain_df[\"language\"].value_counts()","2b363eb8":"train_df[\"question\"] = train_df[\"question\"].str.replace(\"?\", \"\", regex=False).str.strip()\ntrain_df.head()","337e2dea":"# Get the text for both the languages\ntamil_text = \" \".join(train_df[train_df[\"language\"]==\"tamil\"][\"question\"])\nhindi_text = \" \".join(train_df[train_df[\"language\"]==\"hindi\"][\"question\"])","513c3f20":"# Download and extract the fonts\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Devanagari.zip\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Tamil.zip\n\n!unzip -qq Devanagari.zip\n!unzip -qq Tamil.zip","e0627fd7":"# Get the tokens and frequencies for Hindi language\n\nhindi_nlp = Hindi()\nhindi_doc = hindi_nlp(hindi_text)\nhindi_tokens = set([token.text for token in hindi_doc])\nhindi_tokens_counter = Counter(hindi_tokens)\n\n\n# Get the tokens and frequencies for Tamil language\ntamil_nlp = Tamil()\ntamil_doc = hindi_nlp(tamil_text)\ntamil_tokens = set([token.text for token in tamil_doc])\ntamil_tokens_counter = Counter(tamil_tokens)","8b394e2e":"def plot_wordcloud(\n    font_path,\n    frequencies,\n    stopwords,\n    width=500,\n    height=500,\n    background_color=\"white\",\n    collocations=True,\n    min_font_size=8,\n):\n    \"\"\"Generates wordcloud from word frequencies.\"\"\"\n    \n    wordcloud = WordCloud(font_path=font_path,\n                      width=width,\n                      height=height,\n                      background_color=background_color,\n                      stopwords=stopwords,\n                      collocations=collocations,\n                      min_font_size=min_font_size).generate_from_frequencies(frequencies)\n\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","b0700606":"# Plot the wordcloud for hindi langauge\nplot_wordcloud(font_path=\"Devanagari\/Lohit-Devanagari.ttf\",\n               frequencies=hindi_tokens_counter,\n               stopwords=hindi_stopwords\n              )","e99336f8":"This suggests that the number of instances for Hindi language is almost double the number of instance of Tamil language in the training dataset. Let's also get the actual count to see the difference","1e47955d":"# WordCloud\nWe will generate two wordclouds, one for each language.","b7727ca0":"# Distribution of the languages in the training dataset\n\nLet's see how many samples we have for each language in the training dataset. For this we can use countplot(..)","0f3d91cd":"# chaii - Hindi and Tamil Question Answering\n\nHello Kagglers! This competition is pretty cool and a bit harder than the Q&A datasets we generally work on. Most of the datasets and the research done focuses on the English language. Although these models show good performance on the English language datasets, they don't work very well on the Indian languages. The Indian languages ecosystem is as diverse as India is. If you consider all the languages and dialects, then almost 19,000 languages or dialects are spoken by Indians daily.\n\n","d84affc8":"# Dataset\n\nWe have been provided with a new question-answering dataset with question-answer pairs, and it goes by the name chaii-1. The task is straightforward.","12af6377":"Let's statrt diving into the data!","227710a3":"# Evaluation Metric\n\nThe predictions would be evaluated using word-level Jaccard score. A sample code has also been provided for the same.","2b3c59e5":"For generating the wordlcoud, we need the right font\n\n1. [Font for Hindi](http:\/\/www.lipikaar.com\/support\/download-unicode-fonts-for-hindi-marathi-sanskrit-nepali)\n2. [Font for Tamil](http:\/\/www.lipikaar.com\/support\/download-unicode-fonts-for-tamil)\n\nNote: I haven't checked how accurate the gven stopwords are, this is something you need to cross-check!","3decc33c":"# Remove punctuation\nAll the questions presented here are represented with a question mark. We will simply remove it and along with it, we will alos strip any whitespace around the text","e1643af4":"Let's take a look at the training data and the test data","69269fa7":"There are only ~1100 training samples, meaning we are in a low data regime, suggesting that transfer-learning and fine-tuning are the best shots if we are going to use DNNs for this task. This doesn't mean you shouldn't build your models!","cf7d8dca":"# The Task\n\nYou are given questions in Tamil\/Hindi about some Wikipedia articles, and you have to generate the answers for those questions using your model.","e673c62e":"A few things to note:\n\n1. There can be English words as well in the given questions\n2. answer_start column isn't in the test dataset, but it gives important information about the training dataset, the starting character for the context\n3. The language column is present in both train and test. One of the things that we can try is to build two separate models, one for Hindi and one for Tamil, and then make the predictions accordingly using the values in this column"}}