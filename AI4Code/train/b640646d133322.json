{"cell_type":{"dda85765":"code","46384751":"code","38865003":"code","623a9b18":"code","a25c85d4":"code","73838bde":"code","775132bb":"code","9ca3b11c":"code","e17651e7":"code","e6a8faef":"code","17504e37":"code","aa85fbee":"code","241e2041":"code","8e5acc46":"code","89d5a31c":"code","a32d2acc":"code","1b79c9ad":"code","b4c53367":"code","9b5fa8f1":"code","4e0b1345":"code","ad24a80d":"code","798c4ae8":"markdown","9f06c474":"markdown","980cb287":"markdown","f40d90ca":"markdown","ab3013a0":"markdown","0a41d2a8":"markdown","da84271e":"markdown","48c339bf":"markdown","0240204b":"markdown","49edcf0b":"markdown","2aff782e":"markdown","e1a26954":"markdown","a824a70f":"markdown","e61a6a8f":"markdown","dc74ad50":"markdown","4bfb39ab":"markdown","06f9b08f":"markdown"},"source":{"dda85765":"#STEP 1: Get right arrows in quiver\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler,normalize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.manifold import TSNE\n\nimport warnings #To hide warnings\n\n#1.1: Set the stage\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom IPython.core.display import display, HTML\n\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings(\"ignore\")","46384751":"cc = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ncc.info()\ncc[cc.columns[cc.isna().any()]].isna().sum().to_frame().T\ncc.sample(5)\ncc.describe()","38865003":"cc.quantile([0.75,0.8,.85,.9,.95,1])","623a9b18":"display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))\n             +' customers in the list who have more balance than the credit limit assigned. '\n             +'It may be due to more payament than usage and\/or continuous pre-payment.<\/h4>'))","a25c85d4":"cc.rename(columns = {col:col.lower() for col in cc.columns.values},inplace=True)\nsns.jointplot(cc.credit_limit,cc.minimum_payments,kind = 'kde', dropna=True)","73838bde":"cc.fillna(cc.median(),inplace=True) #More outliers thus median in both cases\ncust = cc.cust_id\ncc.drop(columns = ['cust_id'],inplace=True)","775132bb":"ss = StandardScaler()\nX= normalize(ss.fit_transform(cc.copy()))\nX = pd.DataFrame(X,columns=cc.columns.values)","9ca3b11c":"fig, axs = plt.subplots(6,3, figsize=(20, 20))\nfor i in range(17):\n        p = sns.distplot(cc[cc.columns[i]], ax=axs[i\/\/3,i%3],kde_kws = {'bw':2})\n        p = sns.despine()\nplt.show()","e17651e7":"X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)","e6a8faef":"plt.figure(figsize=(16,12))\np = sns.heatmap(cc.corr(),annot=True,cmap='jet').set_title(\"Correlation of credit card data\\'s features\",fontsize=20)\nplt.show()","17504e37":"#Selecting correct number of components for GMM\nmodels = [GMM(n,random_state=0).fit(X) for n in range(1,12)]\nd = pd.DataFrame({'BIC Score':[m.bic(X) for m in models],\n                  'AIC Score': [m.aic(X) for m in models]},index=np.arange(1,12))\nd.plot(use_index=True,title='AIC and BIC Scores for GMM wrt n_Compnents',figsize = (10,5),fontsize=12)","aa85fbee":"from sklearn.base import ClusterMixin\nfrom yellowbrick.cluster import KElbow\n\nclass GMClusters(GMM, ClusterMixin):\n\n    def __init__(self, n_clusters=1, **kwargs):\n        kwargs[\"n_components\"] = n_clusters\n        kwargs['covariance_type'] = 'full'\n        super(GMClusters, self).__init__(**kwargs)\n\n    def fit(self, X):\n        super(GMClusters, self).fit(X)\n        self.labels_ = self.predict(X)\n        return self \n\noz = KElbow(GMClusters(), k=(2,12), force_model=True)\noz.fit(X)\noz.show()","241e2041":"model= models[6]\nmodel.n_init = 10\nmodel","8e5acc46":"clusters = model.fit_predict(X)\ndisplay(HTML('<b>The model has converged :<\/b>'+str(model.converged_)))\ndisplay(HTML('<b>The model has taken iterations :<\/b>'+str(model.n_iter_)))","89d5a31c":"sns.countplot(clusters).set_title('Cluster sizes',fontsize=20)","a32d2acc":"cc1 = cc.copy()\ncc1['cluster']=clusters\nfor c in cc1:\n    if c != 'cluster':\n        grid= sns.FacetGrid(cc1, col='cluster',sharex=False,sharey=False)\n        p = grid.map(sns.distplot, c,kde_kws = {'bw':2})\nplt.show()","1b79c9ad":"#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T\nfor i in range(7):\n    display(HTML('<h2>Cluster'+str(i)+'<\/h2>'))\n    cc1[cc1.cluster == i].describe()","b4c53367":"tsne = TSNE(n_components = 2)\ntsne_out = tsne.fit_transform(X.copy())\n\nplt.scatter(tsne_out[:, 0], tsne_out[:, 1],\n            marker=10,\n            s=10,              # marker size\n            linewidths=5,      # linewidth of marker edges\n            c=clusters   # Colour as per gmm\n            )","9b5fa8f1":"density = model.score_samples(X)\ndensity_threshold = np.percentile(density,4)\ncc1['cluster']=clusters\ncc1['Anamoly'] = density<density_threshold\ncc1","4e0b1345":"df = cc1.melt(['Anamoly'], var_name='cols',  value_name='vals')\n\ng = sns.FacetGrid(df, row='cols', hue=\"Anamoly\", palette=\"Set1\",sharey=False,sharex=False,aspect=3)\ng = (g.map(sns.distplot, \"vals\", hist=True, rug=True,kde_kws = {'bw':2}).add_legend())","ad24a80d":"unanomaly = X[density>=density_threshold]\nc = clusters[density>=density_threshold]\ntsne = TSNE(n_components = 2)\ntsne_out = tsne.fit_transform(unanomaly)\nplt.figure(figsize=(15,10))\nplt.scatter(tsne_out[:, 0], tsne_out[:, 1],marker='x',s=10, linewidths=5, c=c)","798c4ae8":"# <a id=\"8\">Anomaly Detection<\/a>\nAnomaly points can be considered as points which do occur in outliers or rather say in areas of low density. We can identify them by setting some density threshold.\nHere we are taking density threshold as 4%.","9f06c474":"# Credit Card Data Clustering\n\nHere we have a sample of around 9000 creditcard holders for a span of 6 months depicting their purchase habits and the same can be used to segment the customers for marketting purpose to maximize sales as well customer satifaction.\n\nThis kernel focuses on **Gaussian Mixture Model** for clustering and will be devided in following sections:-\n* [Introduction to Gaussian Mixture model](#1)\n* [Importing Required Packages](#2)\n* [Preprocessing](#3)\n* [Data Visualization](#4)\n* [Selection of k for GMM](#5)\n* [Clustering using GMM](#6)\n* [Interpretation of Clusters](#7)\n* [Anomaly Detection](#8)","980cb287":"# <a id=\"1\">Gaussian Mixture Model<\/a>\n\nGaussian Mixture Models (GMMs) are based on Gaussian Distributions and are flexible building blocks for other machine learning algorithms. They are great approximations for general probability distributions but also because they remain somewhat interpretable even when the dataset gets very complex. Mixture Models do not require to know about data and the subpopulation to which it belongs but learn about the same later on by finding the distribution(s) for its each feature.\n\nIn our dataset we have 17 features, _viz_ , \n - **CUSTID** : Identification of Credit Card holder (Categorical)\n - **BALANCE** : Balance amount left in their account to make purchases\n - **BALANCEFREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n - **PURCHASES** : Amount of purchases made from account\n - **ONEOFFPURCHASES** : Maximum purchase amount done in one-go\n - **INSTALLMENTSPURCHASES** : Amount of purchase done in installment\n - **CASHADVANCE** : Cash in advance given by the user\n - **PURCHASESFREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n - **ONEOFFPURCHASESFREQUENCY** : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n - **PURCHASESINSTALLMENTSFREQUENCY** : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n - **CASHADVANCEFREQUENCY** : How frequently the cash in advance being paid\n - **CASHADVANCETRX** : Number of Transactions made with \"Cash in Advanced\"\n - **PURCHASESTRX** : Numbe of purchase transactions made\n - **CREDITLIMIT** : Limit of Credit Card for user\n - **PAYMENTS** : Amount of Payment done by user\n - **MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n - **PRCFULLPAYMENT** : Percent of full payment paid by user\n - **TENURE** : Tenure of credit card service for user\n\nLeaving the first feature each may have its own Gaussian distribution that can easily depict the customer behaviour. Now we start with importing the required packages.","f40d90ca":"## Reading and Pre-viewing Data","ab3013a0":"Here we have limited the models to \"full- covariance\".<br>\nThe chart here shows that the value of AIC as well BIC are continuously decreasing. That may be due to so much noise or small size of sample. For covenience, we take model[6] as our model.","0a41d2a8":"# <a id=\"7\">Interpretation of Clusters<\/a>","da84271e":"### OBSERVATIONS\n\n<h4>CLUSTER 0<\/h4>\n\n - 1367 cutomers fall in cluster0.\n - Low to moderate purcahses.\n - Balances are updated frequently.\n - **CASH ADVANCES AVOIDED**\n - All have full tenure(12 years)\n - More than 75% do not pay in full.\n - Oneoff Purchase and purchase installments are again biased on both extremes.\n \n<h4>CLUSTER 1<\/h4>\n\n - Most populous cluster\n - No purchases at all of either kind\n - Low Balance is kept (about Rs. 2000 against max of balances i.e., Rs. 30000)\n - Only Cash Advances are availed.\n - Low credit limit (average Rs. 4000)\n - Full payments **are avoided**\n \n<h4>CLUSTER 2<\/h4>\n\n - **NO CASH ADVANCES**\n - All have tenure of 12 years\n - Rather low level of balance is maintained\n - **NO FULL PAYMENTS**\n - Balance updation is frequent.\n \n<h4>CLUSTER 3<\/h4>\n\n - **NO ONE-OFF PURCHASES**\n - Cash advances are hugely avaoided\n - **Frequent but low  volume INSTALLMENT purchases**\n - Varying but low majorly low tenure facilities availed.\n\n<h4>CLUSTER 4<\/h4>\n\n - Minimum payments are rather present and are multi-fold of credit limit. **This means minimum dues are regularly paid**\n - Tenure of 12 years\n - Balances are updated frequently.\n - Purchases, cash advances and payments all are low volumne and frequent. **It deduces MOSTLY lower-middle income group**\n \n<h4>CLUSTER 5<\/h4>\n\n - **NO CASH ADVANCES--NO ONE-OFF PURCHASES**\n - More utilisation of limit as balances are kept low.\n - Balance updation is frequent.\n - Mean installment purchases are low volume.\n - Low number of purchase transaction (mean 11)\n \n<h4>CLUSTER 6<\/h4>\n\n - Varying credit limits\n - Varying tenures\n - Low balances maintained and updated frequently\n - Over-all mixed up behaviour","48c339bf":"### OBSERVATIONS\n- Balance is updated frequently(**25% percentile is 1**) in most cases.\n- Most of the customers have CC tenure 12 (**1<sup>st<\/sup> quantile is 12**).\n- most of the customers avoid cash advance, but a section (below 5% of them all,above 75% use no cash advances) uses cash advances and these advances are paid even 50% faster than regular advances[**CASHADVANCEFREQUENCY max value is 1.5 instead of general frequency max 1.0**]\n- Only 5% of customers make most of purachases.","0240204b":"### OBSERVATIONS\n\n- In some cases **Balance** is unutilised.\n- There are **two** clear segments for purchases, one avoiding purchases at all and another purchasing much frequenly(just 5% making almost all)\n- Most of the cutomers avail credit limit below 10k\n- Payments are highly skewed and most payments are below 10k","49edcf0b":"# <a id=\"6\">Clustering using GMM (k = 7)<\/a>\nNow we actually cluster the data with k =7. For better results we can change n_init to 10.","2aff782e":"# <a id = \"2\">Importing Required Packages<\/a>\nWe have imported various packages here for purpose as below:-\n - **Numpy** : *for array manipulation*\n - **Pandas** : *for reading dataset and manipulating its data*\n - **matplotlib and Seaborn** : *for Data Visualization*\n - **sklearn packages** : *for clustering*\n - **IPython classes** : *for formatting output in kernel*","e1a26954":"As both the plots are skewed (to right), mean imputation can affect the model thus we go with **median imputation**. To do so we can either use sklearn **SimpleImputer** class or can simply use **fillna method from pandas to replace all NaN values with median of the feature** as shown below.\n\nAfter that,the cust_id feature does not affect our model(being unique) we can drop the same.\nFuthermore, The dataset has to be **Scaled and normalized** to avoid effect of size of value-set premise. ","a824a70f":"# <a id =\"5\">Selection of k for Gaussian Mixture Model<\/a>\nWe can evaluate the likelihood of the data under the model, using cross-validation to avoid over-fitting. Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC).\nHere, we are checking for both AIC as well BIC for estimating value of k","e61a6a8f":"The heatmap shows the features are corellated. Here we can either drop the columns by capping variance or can use all of them for clustering. We gao with the second option.","dc74ad50":"As the chart shows, there is a number of outliers in almost all the features. While Balance frequency and tenure have outliers down almost all other features have outliers up the whisker. We can assume that there we will find quiet a many anomalous observations, that opens a scope of finding frauds also.\n<br>\nNow we can check the correlations of features.","4bfb39ab":"# <a id=\"4\">Data Visualization and Interpretation<\/a>\nAfter scaling and normalization, we can verify the distribution of the features and their mutual- dependence. *First*, we draw a box chart to see the percentiles as below.","06f9b08f":"# <a id=\"3\">Pre Pocessing<\/a>\nThe dataset has 8 features and 8950 observations. The dataset has **NaN** values in 2 features, viz, **CREDIT_LIMIT and\tMINIMUM_PAYMENTS**. To remove these NaN values we can either remove these rows\/columns or can **impute the features with mean\/median\/minimum or maximum** (since data being numerical, forefill or backfill not considered).Again, as the columns are of importance thus min\/max imputation is out of question. Now to find out correct imputation we check their distribution."}}