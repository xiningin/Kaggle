{"cell_type":{"140b5984":"code","55980b04":"code","8ce28b7f":"code","bb8f6ef2":"code","2d57ba60":"code","1b404e4b":"code","cf0cd0c6":"code","177ba894":"code","47da72f5":"code","944fcbb2":"code","313aa7b7":"code","3f5e745f":"code","ce95af5f":"code","a06c231e":"code","102f09d5":"markdown","679390af":"markdown","6b928916":"markdown","ca856cdb":"markdown","e1bd4f6b":"markdown"},"source":{"140b5984":"import gc\nimport os\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AdamW\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.preprocessing import RobustScaler\n\ndevice = torch.device(\"cuda\")\n","55980b04":"class config:\n    EXP_NAME = \"exp080_conti_rc\"\n    \n    INPUT = \"\/kaggle\/input\/ventilator-pressure-prediction\"\n    OUTPUT = \"\/kaggle\/working\"\n    N_FOLD = 5\n    SEED = 0\n    \n    LR = 5e-3\n    N_EPOCHS = 50\n    EMBED_SIZE = 64\n    HIDDEN_SIZE = 256\n    BS = 512\n    WEIGHT_DECAY = 1e-3\n\n    USE_LAG = 4\n    #CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n    CONT_FEATURES = ['u_in', 'u_out', 'time_step'] + ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2'] + ['R_cate', 'C_cate']\n    LAG_FEATURES = ['breath_time']\n    LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n    #LAG_FEATURES += [f'u_in_lag_{i}_back' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n    #LAG_FEATURES += [f'u_in_time{i}_back' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n    #LAG_FEATURES += [f'u_out_lag_{i}_back' for i in range(1, USE_LAG+1)]\n    #ALL_FEATURES = CATE_FEATURES + CONT_FEATURES + LAG_FEATURES\n    ALL_FEATURES = CONT_FEATURES + LAG_FEATURES\n    \n    NOT_WATCH_PARAM = ['INPUT']","8ce28b7f":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df, label_dic=None):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        self.label_dic = label_dic\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        X = df[config.ALL_FEATURES].values\n        y = df['pressure'].values\n        if self.label_dic is None:\n            label = [-1]\n        else:\n            label = [self.label_dic[i] for i in y]\n\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\" : torch.tensor(label).long(),\n        }\n        return d","bb8f6ef2":"class VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        #self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n        #self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n        #self.rc_dot_emb = nn.Embedding(8, 4, padding_idx=0)\n        #self.rc_sum_emb = nn.Embedding(8, 4, padding_idx=0)\n        self.seq_emb = nn.Sequential(\n            #nn.Linear(12+len(config.CONT_FEATURES)+len(config.LAG_FEATURES), config.EMBED_SIZE),\n            nn.Linear(len(config.CONT_FEATURES)+len(config.LAG_FEATURES), config.EMBED_SIZE),\n            nn.LayerNorm(config.EMBED_SIZE),\n        )\n        \n        self.lstm = nn.LSTM(config.EMBED_SIZE, config.HIDDEN_SIZE, batch_first=True, bidirectional=True, dropout=0.0, num_layers=4)\n\n        self.head = nn.Sequential(\n            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(config.HIDDEN_SIZE * 2, 950),\n        )\n        \n        # Encoder\n        #initrange = 0.1\n        #self.r_emb.weight.data.uniform_(-initrange, initrange)\n        #self.c_emb.weight.data.uniform_(-initrange, initrange)\n        #self.rc_dot_emb.weight.data.uniform_(-initrange, initrange)\n        #self.rc_sum_emb.weight.data.uniform_(-initrange, initrange)\n        \n        # LSTM\n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n\n    def forward(self, X, y=None):\n        # embed\n        #bs = X.shape[0]\n        #r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n        #c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n        #rc_dot_emb = self.rc_dot_emb(X[:,:,2].long()).view(bs, 80, -1)\n        #rc_sum_emb = self.rc_sum_emb(X[:,:,3].long()).view(bs, 80, -1)\n        \n        #seq_x = torch.cat((r_emb, c_emb, rc_dot_emb, rc_sum_emb, X[:, :, 4:]), 2)\n        seq_x = X\n        emb_x = self.seq_emb(seq_x)\n        \n        out, _ = self.lstm(emb_x, None) \n        logits = self.head(out)\n\n        if y is None:\n            loss = None\n        else:\n            loss = self.loss_fn(logits, y)\n            \n        return logits, loss\n    \n    def loss_fn(self, y_pred, y_true):\n        loss = nn.CrossEntropyLoss()(y_pred.reshape(-1, 950), y_true.reshape(-1))\n        return loss\n    \n    \nmodel = VentilatorModel()","2d57ba60":"def test_loop(model, loader, target_dic_inv):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n        predicts.append(out.cpu())\n\n    return torch.vstack(predicts).numpy().reshape(-1)","1b404e4b":"def add_feature(df):\n    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta'] = df['time_delta'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/ df['count']\n    \n    df = df.drop(['count','one'], axis=1)\n    return df\n\ndef add_lag_feature(df):\n    # https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\n    for lag in range(1, config.USE_LAG+1):\n        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n\n        # u_in \n        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        #df[f'u_in_lag_{lag}_back'] = df['u_in'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n        #df[f'u_in_time{lag}_back'] = df['u_in'] - df[f'u_in_lag_{lag}_back']\n        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        #df[f'u_out_lag_{lag}_back'] = df['u_out'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n\n    # breath_time\n    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n    df['breath_time'] = df['time_step'] - df['time_step_lag']\n\n    drop_columns = ['time_step_lag']\n    drop_columns += [f'breath_id_lag{i}' for i in range(1, config.USE_LAG+1)]\n    drop_columns += [f'breath_id_lag{i}same' for i in range(1, config.USE_LAG+1)]\n    df = df.drop(drop_columns, axis=1)\n\n    # fill na by zero\n    df = df.fillna(0)\n    return df\n\nc_dic = {10: 0, 20: 1, 50:2}\nr_dic = {5: 0, 20: 1, 50:2}\nrc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\nrc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n\ndef add_category_features(df):\n    df['C_cate'] = df['C'].map(c_dic)\n    df['R_cate'] = df['R'].map(r_dic)\n    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n    return df\n\nnorm_features = config.CONT_FEATURES + config.LAG_FEATURES\ndef norm_scale(train_df, test_df):\n    scaler = RobustScaler()\n    all_u_in = np.vstack([train_df[norm_features].values, test_df[norm_features].values])\n    scaler.fit(all_u_in)\n    train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n    test_df[norm_features] = scaler.transform(test_df[norm_features].values)\n    return train_df, test_df","cf0cd0c6":"train_df = pd.read_csv(f\"{config.INPUT}\/train.csv\")\ntest_df = pd.read_csv(f\"{config.INPUT}\/test.csv\")\nsub_df = pd.read_csv(f\"{config.INPUT}\/sample_submission.csv\")\noof = np.zeros(len(train_df))\ntest_preds_lst = []\n\ntarget_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\ntarget_dic_inv = {v: k for k, v in target_dic.items()}\n\ngkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\nfor fold, (_, valid_idx) in enumerate(gkf):\n    train_df.loc[valid_idx, 'fold'] = fold\n\ntrain_df = add_feature(train_df)\ntest_df = add_feature(test_df)\ntrain_df = add_lag_feature(train_df)\ntest_df = add_lag_feature(test_df)\ntrain_df = add_category_features(train_df)\ntest_df = add_category_features(test_df)\ntrain_df, test_df = norm_scale(train_df, test_df)\n\ntest_df['pressure'] = -1\ntest_dset = VentilatorDataset(test_df)\ntest_loader = DataLoader(test_dset, batch_size=config.BS,\n                         pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n","177ba894":"unique_pressures = train_df[\"pressure\"].unique()\nsorted_pressures = np.sort(unique_pressures)\ntotal_pressures_len = len(sorted_pressures)","47da72f5":"\ndef find_nearest(prediction):\n    insert_idx = np.searchsorted(sorted_pressures, prediction)\n    if insert_idx == total_pressures_len:\n        # If the predicted value is bigger than the highest pressure in the train dataset,\n        # return the max value.\n        return sorted_pressures[-1]\n    elif insert_idx == 0:\n        # Same control but for the lower bound.\n        return sorted_pressures[0]\n    lower_val = sorted_pressures[insert_idx - 1]\n    upper_val = sorted_pressures[insert_idx]\n    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n","944fcbb2":"from glob import glob \n\nmodels = []\nfor model_path in glob('\/kaggle\/input\/ventilator-train-classification\/exp080_conti_rc\/ventilator_f*_best_model.bin'):\n    model = VentilatorModel()\n    model.load_state_dict(torch.load(model_path))\n    model.cuda()\n    models.append(model)","313aa7b7":"classes, class_freq = np.unique(train_df['pressure'].map(target_dic), return_counts=True)","3f5e745f":"class_proba = class_freq \/ np.sum(class_freq)","ce95af5f":"def test_loop_pred(models, loader, target_dic_inv, class_proba=None):\n    predicts = []\n    for model in models:\n        model.eval()\n    if class_proba is not None:\n        class_proba = torch.unsqueeze(torch.unsqueeze(torch.log(class_proba), dim=0), dim=0) # (1, 1, 950)\n    for d in tqdm(loader):\n        outs = []\n        with torch.no_grad():\n            for model in models:\n                out, _ = model(d['X'].to(device))  \n                out = torch.log(torch.nn.functional.softmax(out, dim=2))   # B, S, 950                \n                outs.append(out)\n        if class_proba is not None:\n            outs.append(class_proba.expand(outs[-1].shape))            \n        out = torch.stack(outs, dim=2)  # B, S, n_folds + 1, 950            \n        out = torch.sum(out, dim=2)\n        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n        predicts.append(out.cpu())\n\n    return torch.vstack(predicts).numpy().reshape(-1)\n\n","a06c231e":"df_submission = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/sample_submission.csv')\ndf_submission['pressure'] = test_loop_pred(models, test_loader, target_dic_inv)\ndf_submission['pressure'] = df_submission['pressure'].apply(find_nearest)\ndf_submission.to_csv('submission.csv', index=False)","102f09d5":"# Implementation of probabilistic ensemble ","679390af":"### 2. Calculate probabilistic pressure class & run postprocessing","6b928916":"# VPP classification \n\nFollowing is the inference part of VPP classification solution extracted from takamichitoda's code: https:\/\/www.kaggle.com\/takamichitoda\/ventilator-train-classification\nPlease, upvote it if you find it mind opening!\n","ca856cdb":"### 1. Load models of all 5 folds","e1bd4f6b":"# Probabilistic ensemble\n\nThis notebook builds on top of the [VPP classification solution](https:\/\/www.kaggle.com\/takamichitoda\/ventilator-train-classification). Please, make yourself familiar with it first.\n\nHere I want to present a simple idea of ensembling classification n-folds that appears to perform slightly better than the median ensemble. \nRecall that classificators produce not only the most likely class, but also probabilities for all other classes via softmax operator. So under assumption of independence of probabilities generated by classifiers trained on different training folds, we could predict the true pressure class as\n\n$$pressure\\_class=\\max\\limits_{pressure\\_class\\_i} \\prod_{k}{P_{fold_k}(pressure\\_class\\_i)}$$\n\nWhere $P_{fold_k}(pressure\\_class\\_i)$ is a probability of i-th pressure class by k-th fold via softmax. \n\n## A note on numerical stability\n\nAs you can see the estimated class probability is proportional to the product of multiple probabilities. The value of this product could quickly become too small for reliable calculation. We use 2 trick to fix it:\n\n1. $\\max\\limits_{i} {x_i}=\\max\\limits_{i} {log(x_i)}$ due to mononotic property of log function. So the above expression becomes:\n\n$$\\max\\limits_{pressure\\_class\\_i} \\prod_{k}{P_{fold_k}(pressure\\_class\\_i)}=\\max\\limits_{pressure\\_class\\_i} \\sum_{k}{log(P_{fold_k}(pressure\\_class\\_i))}$$\n\n_This is the final formula we'll use to calculate our ensemble probabilities._\n\n2. Use `float64` instead of `float32` to increase floating point resolution\n\n## Results\n\nPerformance is only slightly better than the median (-0.0002 PL), but you might be able to increase it by manipulating hyperparameters\/number of folds. "}}