{"cell_type":{"ccb90fc4":"code","fd30a511":"code","c971f3a8":"code","112bd455":"code","8af295a9":"code","db87f2fe":"code","a27bf6d7":"code","3150edc0":"code","c74c6b17":"code","000836c6":"code","f1578744":"code","06eedada":"code","716a846b":"code","de197545":"code","69c27697":"code","a7fc534e":"code","351ab824":"code","43605de9":"code","2711c310":"code","c58f88d4":"code","0a03d496":"code","ea3942c3":"code","f7100a0c":"code","38e65d5f":"markdown","c5aa028e":"markdown","7ee1e917":"markdown","d596c027":"markdown","8824165c":"markdown","a3aa0f0c":"markdown","1b35f66d":"markdown","5ea51b03":"markdown","25e72290":"markdown","f07ee8d8":"markdown","eef270fa":"markdown","fe4ea2cf":"markdown","ae94ab8c":"markdown","add31b28":"markdown","5da7de65":"markdown","9eee4fff":"markdown","fb177d49":"markdown","55bd5459":"markdown","d163558f":"markdown","cdb9bacc":"markdown","bdf77804":"markdown","443751ec":"markdown","a22a5e73":"markdown","f4400c6a":"markdown","778e9f9e":"markdown"},"source":{"ccb90fc4":"# Install PyCaret\n!pip install pycaret","fd30a511":"# install watermark\n!pip install watermark","c971f3a8":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as m\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import pycaret \nimport pycaret   \nfrom pycaret.classification import *  #import classification module \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# make pandas show all columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# Formating the plots\nplt.rcParams.update(plt.rcParamsDefault)\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')\nm.rcParams['axes.labelsize'] = 14\nm.rcParams['xtick.labelsize'] = 12\nm.rcParams['ytick.labelsize'] = 12\nm.rcParams['figure.figsize'] = (15, 5)\nm.rcParams['font.size'] = 12\nm.rcParams['legend.fontsize'] = 'large'\nm.rcParams['figure.titlesize'] = 'medium'\nm.rcParams['text.color'] = 'k'\nsns.set(rc={'figure.figsize':(15,5)})","112bd455":"# Vers\u00f5es dos pacotes usados neste jupyter notebook\n%reload_ext watermark\n%watermark -a \"Forest Cover Type Prediction -- Jessica Cabral\" --iversions\n%watermark -n -t -z","8af295a9":"np.random.seed(42)\nrandom.seed(42)\nrandom_seed = 42","db87f2fe":"train = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/forest-cover-type-prediction\/test.csv')\nsample_submission = pd.read_csv('..\/input\/forest-cover-type-prediction\/sampleSubmission.csv')\n\n\nprint('Train: {}'.format(train.shape))\nprint('test: {}'.format(test.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))","a27bf6d7":"display(train.head(), test.head())\n\ntrain.shape, test.shape","3150edc0":"# remove de ID column\n\ntrain = train.drop(columns=['Id'], axis=1)\ntest = test.drop(columns=['Id'], axis=1)\n\ntrain.shape, test.shape","c74c6b17":"# separate training and test dataset\n\ntrain, validation = train_test_split(train, test_size=0.33, random_state=random_seed)\n\ntrain.shape, validation.shape","000836c6":"exp_clf = setup(data = train,           # train data\n              target = 'Cover_Type',   # feature that we are trying to predict\n              train_size = 0.7)     # proportion of training data","f1578744":"%%time\n\n# Train the modelos using default params\nbest_model = compare_models()\nprint(best_model)","06eedada":"tuned_catboost = tune_model(best_model)\nprint(tuned_catboost)","716a846b":"# Increse the number of iterations (n_iter) to 35. \n# Increasing the n_iter parameter will for sure increase the training time but will \n# give a much better performance.\n\ntuned_catboost_v1 = tune_model(best_model, n_iter = 35)\nprint(tuned_catboost_v1)\n\n# you can try differents values for n_iter param","de197545":"tuned_catboost_v1.get_params()","69c27697":"# Let's try a custom grid\n\n# tune hyperparameters with custom_grid\nparams = {#'early_stopping_rounds': 15,\n          'max_depth': list(range(3,10,1)),\n          'learning_rate': [0.001, 0.01, 0.015, 0.02, 0.04, 0.1],\n          #'n_estimators': list(range(100,300,50)),\n          'iterations': [1000, 500, 1500, 800, 1100, 1200],\n          }\n\ntuned_catboost_v2 = tune_model(best_model, n_iter = 35, custom_grid = params)\nprint(tuned_catboost_v2)\n\n# you can try differents values for n_iter param","a7fc534e":"tuned_catboost_v2.get_params()","351ab824":"#evaluate a model\nevaluate_model(tuned_catboost_v1)","43605de9":"# Compare test data predictions and results\nplot_model(tuned_catboost_v1, plot='confusion_matrix')","2711c310":"# predict in train dataframe\ny_train_pred = predict_model(tuned_catboost_v1)\n\n# predict the test dataframe\ny_pred = predict_model(tuned_catboost_v1, data = test)","c58f88d4":"# view the predictions\ndisplay(y_train_pred[['Cover_Type', 'Label']], y_pred['Label'])","0a03d496":"# Finalize model\nfinal_tuned_catboost_v1 = finalize_model(tuned_catboost_v1)","ea3942c3":"# Save model\nsave_model(final_tuned_catboost_v1, 'final_tuned_catboost_v1_30082020'","f7100a0c":"#sample_submission\nsample_submission['Cover_Type'] = y_pred['Label'].tolist()\n\n# Lets see the head of our submission file\ndisplay(sample_submission.head())\n\n# Analyse the % of Cover Types predicted\ndisplay(sample_submission['Cover_Type'].value_counts(normalize=True)*100)\n\n# Save the \nfile_name = '3-sub_catboost_pycaret' \nsample_submission.to_csv('{}.csv'.format(file_name), index=False)","38e65d5f":"# correlation shap plot\ninterpret_model(tuned_catboost_v2, plot = 'correlation')","c5aa028e":"We got 85,72% Accuracy!\n","7ee1e917":"## Pre-Processing","d596c027":"# Forest Cover Type Prediction\nUse cartographic variables to classify forest categories\n\n## PyCaret","8824165c":"not better","a3aa0f0c":"This function train all the models available in the model library and scores them using Stratified Cross Validation. The output prints a score grid with Accuracy, AUC, Recall, Precision, F1, Kappa and MCC (averaged accross folds), determined by fold parameter.\n\n#### Default params\ncompare_models(blacklist = None, whitelist = None, fold = 10,  round = 4,  sort = \u2018Accuracy\u2019,  n_select = 1, turbo = True, verbose = True)\n","1b35f66d":"## Interpret the Model","5ea51b03":"## Model Optimization","25e72290":"with the default parameters \n\n'tune_model(estimator = None,  fold = 10,  round = 4,  n_iter = 10, custom_grid = None,  optimize = \u2018Accuracy\u2019, choose_better = False, verbose = True)'\n\nour accuracy down to 76,76%\n\ninteresting...\n\nLet's check if we can do someting about it","f07ee8d8":"It's possible to plot the metrics indivually\n\nHere is a example:","eef270fa":"# interpret overall model \ninterpret_model(tuned_catboost_v2)","fe4ea2cf":"## Imports","ae94ab8c":"#### Problem\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n\n    1 - Spruce\/Fir\n    2 - Lodgepole Pine\n    3 - Ponderosa Pine\n    4 - Cottonwood\/Willow\n    5 - Aspen\n    6 - Douglas-fir\n    7 - Krummholz\n\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).\n\n#### Evaluation Metric\nMulti-class classification accuracy","add31b28":"## Train and test split","5da7de65":"We split the training data into training and validation, why?\n\nWe are going to setup pycaret to use only the training sample to subdivide again into test and train\nThese new samples will be used to train and validate the model metrics by the pycaret itself\n\nAfter all the training and tuning the model, we will use our validation sample to validate the model against data it haven't seen yet, thereby generating our most important metrics: Accuracy test data\n\nHere is a drawing that tries to illustrate this:\n\n![how_it_works](https:\/\/github.com\/jcabralc\/forest_cover_type_prediction\/blob\/master\/imgs\/how_it_works.png?raw=true)\n\npretty cute my drawing isn't it?\n:D","9eee4fff":"## Import Data","fb177d49":"Warnings:\n    interpret_model doesn\u2019t support multiclass problems.","55bd5459":"## Submission ","d163558f":"## Predictions","cdb9bacc":"Let's see if we can optimize our model accuracy using pycaret tune_model function","bdf77804":"## Setup pycaret","443751ec":"# interactive reason plot\ninterpret_model(tuned_catboost_v2, plot = 'reason')","a22a5e73":"## Model Evaluate","f4400c6a":"## Train and compare models","778e9f9e":"The AUC will be returned as zero (0.0) If target variable is multiclass (more than 2 classes), like in our case\n\nOur winner is the CatBoost Classifier, with 83,15% Accuracy"}}