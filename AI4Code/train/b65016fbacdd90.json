{"cell_type":{"b5926d82":"code","372f804b":"code","41ec717b":"code","4fadae17":"code","60162d28":"code","20730631":"code","0f595777":"code","0330f59c":"code","7c85852f":"code","bf0dfd46":"code","b808f4ec":"code","8c66a565":"code","50671215":"code","f3d8d105":"code","a067f3eb":"code","b37f4fbd":"code","2d152f1a":"code","ac9013cb":"code","3eab0be3":"code","f88583e2":"code","cd061940":"code","b3c8b993":"code","b4395063":"code","1776d2c9":"code","7418cf34":"code","d23a5402":"code","dbfeee5f":"code","d4a82cd9":"code","99ff9473":"code","851f7113":"code","58f6e966":"code","537b9ad7":"code","eafcdee0":"code","f22d6e1d":"code","18d7b9c6":"code","80a743e4":"markdown","21e0778d":"markdown","e5f682c2":"markdown","74fb6b57":"markdown","baf9c534":"markdown","52e1d159":"markdown","78cdbe80":"markdown","43c6a7a8":"markdown","875ca41b":"markdown","12f7979b":"markdown","aa918c10":"markdown","7b500142":"markdown"},"source":{"b5926d82":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\n\n","372f804b":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","41ec717b":"!ls ..\/input","4fadae17":"%%time\nroot = Path('..\/input\/ashrae-feather-format-for-fast-loading')\n\ntrain_df = pd.read_feather(root\/'train.feather')\nweather_train_df = pd.read_feather(root\/'weather_train.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')","60162d28":"train_df['date'] = train_df['timestamp'].dt.date\ntrain_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])","20730631":"def plot_date_usage(train_df, meter=0, building_id=0):\n    train_temp_df = train_df[train_df['meter'] == meter]\n    train_temp_df = train_temp_df[train_temp_df['building_id'] == building_id]    \n    train_temp_df_meter = train_temp_df.groupby('date')['meter_reading_log1p'].sum()\n    train_temp_df_meter = train_temp_df_meter.to_frame().reset_index()\n    fig = px.line(train_temp_df_meter, x='date', y='meter_reading_log1p')\n    fig.show()","0f595777":"plot_date_usage(train_df, meter=0, building_id=0)","0330f59c":"building_meta_df[building_meta_df.site_id == 0]","7c85852f":"train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","bf0dfd46":"debug = False","b808f4ec":"def preprocess(df):\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n#     df[\"day\"] = df[\"timestamp\"].dt.day\n    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n    df[\"month\"] = df[\"timestamp\"].dt.month\n    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n\n#     hour_rad = df[\"hour\"].values \/ 24. * 2 * np.pi\n#     df[\"hour_sin\"] = np.sin(hour_rad)\n#     df[\"hour_cos\"] = np.cos(hour_rad)","8c66a565":"preprocess(train_df)","50671215":"df_group = train_df.groupby('building_id')['meter_reading_log1p']\nbuilding_mean = df_group.mean().astype(np.float16)\nbuilding_median = df_group.median().astype(np.float16)\nbuilding_min = df_group.min().astype(np.float16)\nbuilding_max = df_group.max().astype(np.float16)\nbuilding_std = df_group.std().astype(np.float16)\n\ntrain_df['building_mean'] = train_df['building_id'].map(building_mean)\ntrain_df['building_median'] = train_df['building_id'].map(building_median)\ntrain_df['building_min'] = train_df['building_id'].map(building_min)\ntrain_df['building_max'] = train_df['building_id'].map(building_max)\ntrain_df['building_std'] = train_df['building_id'].map(building_std)","f3d8d105":"building_mean.head()","a067f3eb":"weather_train_df.head()","b37f4fbd":"# weather_train_df.describe()","2d152f1a":"weather_train_df.isna().sum()","ac9013cb":"weather_train_df.shape","3eab0be3":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","f88583e2":"weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))","cd061940":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","b3c8b993":"def add_lag_feature(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_max = rolled.max().reset_index().astype(np.float16)\n    lag_min = rolled.min().reset_index().astype(np.float16)\n    lag_std = rolled.std().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n        weather_df[f'{col}_std_lag{window}'] = lag_std[col]","b4395063":"add_lag_feature(weather_train_df, window=3)\nadd_lag_feature(weather_train_df, window=72)","1776d2c9":"weather_train_df.head()","7418cf34":"weather_train_df.columns","d23a5402":"# categorize primary_use column to reduce memory on merge...\n\nprimary_use_list = building_meta_df['primary_use'].unique()\nprimary_use_dict = {key: value for value, key in enumerate(primary_use_list)} \nprint('primary_use_dict: ', primary_use_dict)\nbuilding_meta_df['primary_use'] = building_meta_df['primary_use'].map(primary_use_dict)\n\ngc.collect()","dbfeee5f":"reduce_mem_usage(train_df, use_float16=True)\nreduce_mem_usage(building_meta_df, use_float16=True)\nreduce_mem_usage(weather_train_df, use_float16=True)","d4a82cd9":"building_meta_df.head()","99ff9473":"category_cols = ['building_id', 'site_id', 'primary_use']  # , 'meter'\nfeature_cols = ['square_feet', 'year_built'] + [\n    'hour', 'weekend', # 'month' , 'dayofweek'\n    'building_median'] + [\n    'air_temperature', 'cloud_coverage',\n    'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n    'wind_direction', 'wind_speed', 'air_temperature_mean_lag72',\n    'air_temperature_max_lag72', 'air_temperature_min_lag72',\n    'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n    'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n    'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72',\n    'wind_speed_mean_lag72', 'air_temperature_mean_lag3',\n    'air_temperature_max_lag3',\n    'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n    'dew_temperature_mean_lag3',\n    'precip_depth_1_hr_mean_lag3', 'sea_level_pressure_mean_lag3',\n    'wind_direction_mean_lag3', 'wind_speed_mean_lag3']","851f7113":"def create_X_y(train_df, target_meter):\n    target_train_df = train_df[train_df['meter'] == target_meter]\n    target_train_df = target_train_df.merge(building_meta_df, on='building_id', how='left')\n    target_train_df = target_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')\n    X_train = target_train_df[feature_cols + category_cols]\n    y_train = target_train_df['meter_reading_log1p'].values\n\n    del target_train_df\n    return X_train, y_train","58f6e966":"def GPMeter0(data):\n    return (4.093981 +\n            1.0*np.tanh(((data[\"building_median\"]) - ((((3.47548198699951172)) + (np.tanh(((((((3.47548198699951172)) + ((-1.0*((data[\"building_median\"])))))) + (((np.where(data[\"cloud_coverage_mean_lag72\"] < -9998, (-1.0*((data[\"sea_level_pressure\"]))), np.tanh(((3.47548198699951172))) )) * 2.0)))))))))) +\n            1.0*np.tanh((((((-1.0*((((np.tanh((((((data[\"building_median\"]) + (((-2.0) * 2.0)))) + (((((data[\"building_median\"]) + (((-2.0) * 2.0)))) \/ 2.0)))))) \/ 2.0))))) + (((data[\"building_median\"]) + (((-2.0) * 2.0)))))) \/ 2.0)) +\n            0.964338*np.tanh(np.where(np.where(data[\"wind_speed_mean_lag72\"] < -9998, ((-1.0) \/ 2.0), ((((data[\"cloud_coverage\"]) * 2.0)) * 2.0) ) < -9998, np.where(np.where(data[\"wind_speed_mean_lag72\"] < -9998, 0.0, ((((data[\"year_built\"]) + (((data[\"hour\"]) - (data[\"weekend\"]))))) * 2.0) ) < -9998, 0.0, ((np.tanh((1.0))) * 2.0) ), 0.0 )) +\n            1.0*np.tanh((-1.0*(((((((2.0) + ((-1.0*(((((np.tanh(((-1.0*((((((np.where(np.tanh((data[\"air_temperature_std_lag72\"])) < -9998, data[\"building_median\"], data[\"site_id\"] )) - (((((data[\"building_median\"]) \/ 2.0)) * 2.0)))) - (2.0)))))))) + (data[\"building_median\"]))\/2.0))))))\/2.0)) \/ 2.0))))) +\n            0.871519*np.tanh(((np.tanh((((0.0) - (((np.tanh((((1.0) - (np.tanh((((((data[\"hour\"]) \/ 2.0)) \/ 2.0)))))))) \/ 2.0)))))) - (((((1.0) - (np.tanh((data[\"building_median\"]))))) * 2.0)))) +\n            1.0*np.tanh(((((((((((1.0) - (np.tanh(((((np.where(data[\"precip_depth_1_hr_mean_lag3\"] > -9998, data[\"air_temperature_mean_lag3\"], (((-1.0*((data[\"cloud_coverage_mean_lag3\"])))) \/ 2.0) )) + ((-1.0*((data[\"cloud_coverage\"])))))\/2.0)))))) \/ 2.0)) \/ 2.0)) \/ 2.0)) \/ 2.0)) +\n            1.0*np.tanh(((((np.where(data[\"air_temperature_mean_lag3\"] > -9998, np.tanh((np.where(((data[\"sea_level_pressure_mean_lag72\"]) + (((data[\"weekend\"]) \/ 2.0))) > -9998, np.where(data[\"sea_level_pressure_mean_lag72\"] > -9998, (0.0), data[\"sea_level_pressure_mean_lag72\"] ), np.tanh((data[\"building_median\"])) ))), np.tanh((data[\"building_median\"])) )) \/ 2.0)) \/ 2.0)) +\n            0.0*np.tanh(0.0) +\n            0.929653*np.tanh((((-1.0) + (np.tanh(((((((((np.where(((data[\"air_temperature_mean_lag3\"]) * (((0.0) + (((((data[\"cloud_coverage_mean_lag72\"]) + (data[\"cloud_coverage_mean_lag72\"]))) * 2.0))))) > -9998, data[\"site_id\"], data[\"dew_temperature_mean_lag72\"] )) + (np.tanh((data[\"cloud_coverage_mean_lag72\"]))))) * 2.0)) + (data[\"site_id\"]))\/2.0)))))\/2.0)) +\n            0.0*np.tanh(0.0))\n\ndef GPMeter1(data):\n    return (4.246485 +\n            1.0*np.tanh((((data[\"building_median\"]) + ((((-3.0) + (((((((((data[\"building_median\"]) - ((6.0)))) + (data[\"air_temperature_mean_lag72\"]))) - ((((6.0)) * (3.0))))) + (((data[\"building_median\"]) - ((((((data[\"wind_speed_mean_lag72\"]) - (data[\"building_median\"]))) + (((3.0) - ((-1.0*((data[\"air_temperature_mean_lag72\"])))))))\/2.0)))))))\/2.0)))\/2.0)) +\n            1.0*np.tanh(((((np.where(data[\"cloud_coverage\"] > -9998, ((np.where(data[\"wind_speed\"] > -9998, ((((np.where(data[\"wind_direction\"] > -9998, data[\"dew_temperature\"], data[\"wind_speed\"] )) \/ 2.0)) - (data[\"cloud_coverage\"])), data[\"precip_depth_1_hr_mean_lag72\"] )) \/ 2.0), ((data[\"air_temperature_min_lag3\"]) - ((((((data[\"wind_speed\"]) + ((8.0)))\/2.0)) + ((8.0))))) )) \/ 2.0)) \/ 2.0)) +\n            1.0*np.tanh((((((data[\"building_median\"]) - ((4.47872495651245117)))) + (np.tanh((((0.0) + (((((((data[\"building_median\"]) * (((np.where(data[\"wind_direction_mean_lag3\"] < -9998, data[\"building_id\"], data[\"building_median\"] )) - (np.where(data[\"building_id\"] < -9998, -1.0, (4.47872495651245117) )))))) - (data[\"site_id\"]))) \/ 2.0)))))))\/2.0)) +\n            1.0*np.tanh(((((np.tanh((np.where(data[\"air_temperature_mean_lag72\"] > -9998, np.where(data[\"cloud_coverage\"] > -9998, (((data[\"hour\"]) + ((-1.0*(((((((2.41199660301208496)) * 2.0)) * 2.0))))))\/2.0), (-1.0*((((((data[\"wind_speed_mean_lag3\"]) * 2.0)) * 2.0)))) ), data[\"weekend\"] )))) \/ 2.0)) \/ 2.0)) +\n            1.0*np.tanh(((np.where((((-1.0) + (-1.0))\/2.0) < -9998, data[\"air_temperature_min_lag72\"], (((np.tanh((np.tanh((-1.0))))) + (np.where(((data[\"air_temperature_min_lag72\"]) * 2.0) < -9998, data[\"primary_use\"], np.tanh((((((((((data[\"primary_use\"]) + (data[\"air_temperature_min_lag72\"]))\/2.0)) + ((0.27198320627212524)))\/2.0)) + ((((-1.0) + (-1.0))\/2.0))))) )))\/2.0) )) \/ 2.0)) +\n            0.918417*np.tanh((((np.tanh(((((7.0)) - ((((((7.0)) - (((data[\"building_median\"]) - (((data[\"cloud_coverage_mean_lag72\"]) * 2.0)))))) - (data[\"cloud_coverage_mean_lag72\"]))))))) + ((-1.0*((np.tanh((np.tanh(((((7.0)) - (data[\"building_median\"])))))))))))\/2.0)) +\n            1.0*np.tanh(((((((np.where(np.where(data[\"year_built\"] < -9998, data[\"year_built\"], data[\"cloud_coverage_mean_lag3\"] ) > -9998, ((((np.where(data[\"dew_temperature_mean_lag72\"] > -9998, data[\"cloud_coverage_mean_lag3\"], data[\"cloud_coverage_mean_lag72\"] )) \/ 2.0)) \/ 2.0), (-1.0*((np.where(data[\"cloud_coverage_mean_lag72\"] > -9998, ((((data[\"cloud_coverage_mean_lag72\"]) \/ 2.0)) \/ 2.0), data[\"building_median\"] )))) )) \/ 2.0)) \/ 2.0)) \/ 2.0)) +\n            1.0*np.tanh(np.tanh(((((((((((((np.tanh((((data[\"weekend\"]) - (1.0))))) - (data[\"weekend\"]))) + (((2.0) + (((np.tanh((data[\"weekend\"]))) - (np.tanh((((data[\"dew_temperature_mean_lag3\"]) + (((data[\"dew_temperature_mean_lag72\"]) * 2.0)))))))))))\/2.0)) \/ 2.0)) \/ 2.0)) \/ 2.0)))) +\n            0.766976*np.tanh(np.tanh((np.tanh((np.where(((((((((((data[\"primary_use\"]) \/ 2.0)) \/ 2.0)) \/ 2.0)) * (data[\"square_feet\"]))) * (np.tanh((((data[\"primary_use\"]) - ((9.45051860809326172))))))) > -9998, ((data[\"primary_use\"]) \/ 2.0), 0.0 )))))) +\n            0.940401*np.tanh(((np.where(data[\"dew_temperature_mean_lag72\"] < -9998, 0.0, np.tanh((np.tanh((np.tanh((((np.where(data[\"cloud_coverage_mean_lag72\"] < -9998, ((((data[\"dew_temperature_mean_lag72\"]) \/ 2.0)) \/ 2.0), ((np.tanh(((-1.0*((((data[\"cloud_coverage_mean_lag72\"]) \/ 2.0))))))) \/ 2.0) )) \/ 2.0))))))) )) \/ 2.0)))\n\ndef GPMeter2(data):\n    return (5.121822 +\n            1.0*np.tanh(((((((((((((data[\"building_median\"]) - (((3.0) - (0.0))))) * 2.0)) * 2.0)) + (np.where(((np.tanh((data[\"air_temperature\"]))) + (data[\"air_temperature_max_lag3\"])) < -9998, (-1.0*((data[\"building_median\"]))), (-1.0*(((((data[\"air_temperature_max_lag3\"]) + (((np.tanh((data[\"air_temperature\"]))) - (0.0))))\/2.0)))) )))) \/ 2.0)) \/ 2.0)) +\n            1.0*np.tanh(((np.where(data[\"cloud_coverage_mean_lag72\"] < -9998, ((-2.0) + ((-1.0*((data[\"air_temperature_min_lag72\"]))))), (((-3.0) + ((((-2.0) + (((((((((-3.0) + (np.where(-3.0 < -9998, data[\"wind_speed_mean_lag72\"], data[\"cloud_coverage_mean_lag72\"] )))\/2.0)) + (((data[\"building_median\"]) * 2.0)))\/2.0)) * 2.0)))\/2.0)))\/2.0) )) - (np.tanh((np.tanh((data[\"air_temperature_min_lag72\"]))))))) +\n            0.929165*np.tanh(((np.where(data[\"cloud_coverage\"] < -9998, data[\"cloud_coverage\"], ((((((((data[\"cloud_coverage_mean_lag72\"]) * (((((((data[\"building_median\"]) * (((((((((data[\"building_median\"]) * (((((((data[\"building_median\"]) \/ 2.0)) \/ 2.0)) \/ 2.0)))) \/ 2.0)) \/ 2.0)) * 2.0)))) \/ 2.0)) \/ 2.0)))) \/ 2.0)) \/ 2.0)) \/ 2.0) )) \/ 2.0)) +\n            1.0*np.tanh((((-1.0*((((np.where(data[\"cloud_coverage_mean_lag72\"] < -9998, data[\"air_temperature_min_lag72\"], (-1.0*((np.tanh((((((((data[\"year_built\"]) - ((((((((-1.0*((((data[\"year_built\"]) \/ 2.0))))) * (data[\"air_temperature_max_lag72\"]))) - (np.where(data[\"year_built\"] > -9998, data[\"air_temperature_std_lag72\"], data[\"air_temperature_max_lag3\"] )))) \/ 2.0)))) - (data[\"cloud_coverage_mean_lag72\"]))) \/ 2.0)))))) )) \/ 2.0))))) \/ 2.0)) +\n            0.899365*np.tanh(((-1.0) + ((((((((data[\"building_median\"]) + (-3.0))) + ((((np.tanh(((((-3.0) + (((((((data[\"wind_direction\"]) + (np.where(2.0 > -9998, 3.0, data[\"wind_direction\"] )))\/2.0)) + (((data[\"cloud_coverage\"]) + (data[\"air_temperature_mean_lag3\"]))))\/2.0)))\/2.0)))) + (3.0))\/2.0)))\/2.0)) \/ 2.0)))) +\n            1.0*np.tanh(np.tanh((np.where(data[\"sea_level_pressure_mean_lag72\"] < -9998, np.where(data[\"dew_temperature_mean_lag3\"] > -9998, data[\"dew_temperature_mean_lag3\"], np.where(data[\"weekend\"] > -9998, data[\"wind_direction\"], -1.0 ) ), (((((-1.0) + (np.tanh((data[\"building_median\"]))))\/2.0)) * ((((((((data[\"dew_temperature_mean_lag3\"]) + (np.tanh((-1.0))))\/2.0)) * ((3.0)))) * ((3.0))))) )))) +\n            1.0*np.tanh(((((np.tanh((np.where(data[\"cloud_coverage_mean_lag3\"] > -9998, ((((((np.tanh((data[\"air_temperature_min_lag72\"]))) + ((((-1.0*((data[\"air_temperature_min_lag72\"])))) \/ 2.0)))\/2.0)) + (((((np.tanh((data[\"air_temperature_min_lag72\"]))) * 2.0)) * 2.0)))\/2.0), (((np.tanh((((data[\"air_temperature_min_lag72\"]) * 2.0)))) + ((((-1.0*((data[\"air_temperature_min_lag72\"])))) \/ 2.0)))\/2.0) )))) \/ 2.0)) \/ 2.0)) +\n            0.0*np.tanh(0.0) +\n            0.863214*np.tanh(np.where(((data[\"air_temperature_mean_lag72\"]) + (data[\"year_built\"])) < -9998, 0.0, (((((((-1.0*((((((((((((((((data[\"weekend\"]) + ((-1.0*((((((data[\"air_temperature_max_lag3\"]) + ((-1.0*((data[\"air_temperature_mean_lag72\"])))))) * 2.0))))))) \/ 2.0)) \/ 2.0)) \/ 2.0)) \/ 2.0)) \/ 2.0)) \/ 2.0))))) * 2.0)) \/ 2.0)) \/ 2.0) )) +\n            0.0*np.tanh(0.0))\n\ndef GPMeter3(data):\n    return (3.333021 +\n            1.0*np.tanh((-1.0*((((((((data[\"air_temperature_mean_lag72\"]) - (((np.where(data[\"cloud_coverage_mean_lag72\"] > -9998, data[\"building_median\"], 2.0 )) * (((data[\"building_median\"]) + (np.tanh((((data[\"precip_depth_1_hr_mean_lag72\"]) - (data[\"air_temperature\"]))))))))))) \/ 2.0)) \/ 2.0))))) +\n            1.0*np.tanh(np.tanh(((((np.where(data[\"cloud_coverage_mean_lag3\"] < -9998, ((data[\"wind_speed_mean_lag72\"]) - (data[\"air_temperature_min_lag3\"])), ((((((((data[\"building_id\"]) + (data[\"building_median\"]))) + (data[\"building_median\"]))) - (((data[\"sea_level_pressure\"]) + (data[\"air_temperature_min_lag3\"]))))) * 2.0) )) + (data[\"site_id\"]))\/2.0)))) +\n            0.999023*np.tanh(((((((((data[\"building_median\"]) + ((-1.0*(((5.0))))))\/2.0)) - (np.where(data[\"building_median\"] < -9998, (4.0), np.tanh((np.tanh((((((((data[\"dew_temperature_mean_lag3\"]) \/ 2.0)) \/ 2.0)) \/ 2.0))))) )))) + ((((data[\"building_median\"]) + ((-1.0*(((4.0))))))\/2.0)))\/2.0)) +\n            1.0*np.tanh(np.tanh((((((1.0) + ((-1.0*((np.tanh(((((data[\"air_temperature_mean_lag3\"]) + (((((data[\"precip_depth_1_hr_mean_lag3\"]) * (np.where(data[\"year_built\"] < -9998, data[\"air_temperature_std_lag72\"], ((data[\"hour\"]) - (((((((data[\"cloud_coverage\"]) * (data[\"air_temperature_mean_lag3\"]))) - ((((-1.0*((data[\"air_temperature_mean_lag3\"])))) * 2.0)))) * 2.0))) )))) * 2.0)))\/2.0))))))))) \/ 2.0)))) +\n            0.899853*np.tanh(((np.tanh((((np.tanh((np.where(data[\"cloud_coverage\"] < -9998, data[\"year_built\"], ((np.where(data[\"precip_depth_1_hr_mean_lag72\"] < -9998, ((((data[\"primary_use\"]) + (np.tanh(((-1.0*((data[\"year_built\"])))))))) \/ 2.0), np.tanh((data[\"year_built\"])) )) + ((-1.0*((((data[\"primary_use\"]) + (-1.0))))))) )))) \/ 2.0)))) \/ 2.0)) +\n            1.0*np.tanh(np.where(data[\"cloud_coverage_mean_lag3\"] > -9998, 0.0, np.where(np.where(data[\"year_built\"] > -9998, data[\"cloud_coverage_mean_lag3\"], data[\"air_temperature_mean_lag72\"] ) > -9998, ((((np.tanh((np.tanh((((np.tanh((((np.tanh((data[\"year_built\"]))) \/ 2.0)))) \/ 2.0)))))) \/ 2.0)) \/ 2.0), np.tanh((np.where(data[\"sea_level_pressure_mean_lag3\"] > -9998, (-1.0*((np.tanh((data[\"air_temperature_mean_lag72\"]))))), data[\"year_built\"] ))) ) )) +\n            0.986810*np.tanh(((data[\"primary_use\"]) * (np.tanh((((((((data[\"primary_use\"]) \/ 2.0)) \/ 2.0)) * (((((data[\"primary_use\"]) \/ 2.0)) * ((((0.05702735483646393)) * (np.where(data[\"air_temperature_mean_lag3\"] < -9998, data[\"wind_direction_mean_lag3\"], np.tanh((np.tanh((np.where(np.where(0.0 < -9998, 0.0, data[\"precip_depth_1_hr\"] ) < -9998, data[\"precip_depth_1_hr\"], 0.0 ))))) )))))))))))) +\n            0.831461*np.tanh(np.where((((((data[\"air_temperature_min_lag3\"]) \/ 2.0)) + (((data[\"precip_depth_1_hr_mean_lag72\"]) * (((np.tanh((((((data[\"air_temperature_max_lag3\"]) * 2.0)) * 2.0)))) * 2.0)))))\/2.0) < -9998, np.tanh((((data[\"air_temperature_max_lag3\"]) \/ 2.0))), ((0.0) \/ 2.0) )) +\n            0.893991*np.tanh(((((((np.tanh((((np.tanh(((-1.0*((data[\"air_temperature_max_lag3\"])))))) + (((((((data[\"sea_level_pressure\"]) * (data[\"primary_use\"]))) + ((-1.0*(((((((((np.tanh((((data[\"primary_use\"]) + (data[\"precip_depth_1_hr_mean_lag3\"]))))) \/ 2.0)) + (data[\"air_temperature_max_lag3\"]))\/2.0)) * 2.0))))))) + ((((6.37950325012207031)) * 2.0)))))))) \/ 2.0)) \/ 2.0)) \/ 2.0)) +\n            1.0*np.tanh((-1.0*((((np.where((-1.0*((data[\"square_feet\"]))) < -9998, np.tanh((np.where(data[\"precip_depth_1_hr_mean_lag72\"] > -9998, 0.0, ((((2.74675083160400391)) + ((((((((((data[\"dew_temperature_mean_lag3\"]) + (data[\"square_feet\"]))\/2.0)) + (((((((((data[\"dew_temperature_mean_lag3\"]) \/ 2.0)) \/ 2.0)) \/ 2.0)) - (3.0))))\/2.0)) + (data[\"precip_depth_1_hr_mean_lag72\"]))\/2.0)))\/2.0) ))), data[\"square_feet\"] )) \/ 2.0))))))","537b9ad7":"target_meter = 0\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\nx0 = pd.DataFrame()\nx0['target'] = GPMeter0(X_train.astype('float32').fillna(-9999)).values\nx0['prediction'] = y_train\nx0['meter'] = target_meter\ndel X_train, y_train\ngc.collect()\ntarget_meter = 1\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\nx1 = pd.DataFrame()\nx1['target'] = GPMeter1(X_train.astype('float32').fillna(-9999)).values\nx1['prediction'] = y_train\nx1['meter'] = target_meter\ndel X_train, y_train\ngc.collect()\ntarget_meter = 2\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\nx2 = pd.DataFrame()\nx2['target'] = GPMeter2(X_train.astype('float32').fillna(-9999)).values\nx2['prediction'] = y_train\nx2['meter'] = target_meter\ndel X_train, y_train\ngc.collect()\ntarget_meter = 3\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\nx3 = pd.DataFrame()\nx3['target'] = GPMeter3(X_train.astype('float32').fillna(-9999)).values\nx3['prediction'] = y_train\nx3['meter'] = target_meter\ndel X_train, y_train\ngc.collect()\nx = pd.concat([x0,x1,x2,x3])\ndel x0, x1, x2, x3\ngc.collect()","eafcdee0":"from sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(x.target,x.prediction))","f22d6e1d":"_ = sns.distplot(x.target)","18d7b9c6":"_ = sns.distplot(x.prediction)","80a743e4":"# ASHRAE - Great Energy Predictor III\n\n\nOur aim in this competition is to predict energy consumption of buildings.\n\nThere are 4 types of energy to predict:\n\n - 0: electricity\n - 1: chilledwater\n - 2: steam\n - 3: hotwater\n\nElectricity and water consumption may have different behavior!\nSo I tried to separately train & predict the model.\n\nI moved previous [ASHRAE: Simple LGBM submission](https:\/\/www.kaggle.com\/corochann\/ashrae-simple-lgbm-submission) kernel.\n\n**[Update] I published \"[Optuna tutorial for hyperparameter optimization](https:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization)\" notebook.\nPlease also check it :)**","21e0778d":"Seems number of nan has reduced by `interpolate` but some property has never appear in specific `site_id`, and nan remains for these features.","e5f682c2":"# Add time feature","74fb6b57":"# References\n\nThese kernels inspired me to write this kernel, thank you for sharing!\n\n - https:\/\/www.kaggle.com\/rishabhiitbhu\/ashrae-simple-eda\n - https:\/\/www.kaggle.com\/isaienkov\/simple-lightgbm\n - https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution\n - https:\/\/www.kaggle.com\/corochann\/ashrae-training-lgbm-by-meter-type","baf9c534":"Some features introduced in https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution by @ryches\n\nFeatures that are likely predictive:\n\n#### Weather\n\n- time of day\n- holiday\n- weekend\n- cloud_coverage + lags\n- dew_temperature + lags\n- precip_depth + lags\n- sea_level_pressure + lags\n- wind_direction + lags\n- wind_speed + lags\n\n#### Train\n\n- max, mean, min, std of the specific building historically\n\n\n\nHowever we should be careful of putting time feature, since we have only 1 year data in training,\nincluding `date` makes overfiting to training data.\n\nHow about `month`? It may be better to check performance by cross validation.\nI go not using this data in this kernel for robust modeling.","52e1d159":"# Train model\n\nTo win in kaggle competition, how to evaluate your model is important.\nWhat kind of cross validation strategy is suitable for this competition? This is time series data, so it is better to consider time-splitting.\n\nHowever this notebook is for simple tutorial, so I will proceed with KFold splitting without shuffling, so that at least near-term data is not included in validation.","78cdbe80":"# Train model by each meter type","43c6a7a8":"# Fill Nan value in weather dataframe by interpolation\n\n\nweather data has a lot of NaNs!!\n\n![](http:\/\/)I tried to fill these values by **interpolating** data.","875ca41b":"## Removing weired data on site_id 0\n\nAs you can see above, this data looks weired until May 20. It is reported in [this discussion](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113054#656588) by @barnwellguy that **All electricity meter is 0 until May 20 for site_id == 0**. I will remove these data from training data.\n\nIt corresponds to `building_id <= 104`.","12f7979b":"# Data preprocessing\n\nNow, Let's try building GBDT (Gradient Boost Decision Tree) model to predict `meter_reading_log1p`. I will try using LightGBM in this notebook.","aa918c10":"# Fast data loading\n\nThis kernel uses the preprocessed data from my previous kernel, [\nASHRAE: feather format for fast loading](https:\/\/www.kaggle.com\/corochann\/ashrae-feather-format-for-fast-loading), to accelerate data loading!","7b500142":"## lags\n\nAdding some lag feature"}}