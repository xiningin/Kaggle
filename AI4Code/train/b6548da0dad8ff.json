{"cell_type":{"ef8a929d":"code","698f8bfd":"code","78d48a3f":"code","4b8f485c":"code","f69f54f0":"code","595c3d4f":"code","32ac19b6":"code","bc3364f5":"code","ae9a16e0":"code","4d6f603d":"code","879cdc54":"code","797b3ec3":"code","08254b72":"code","30a6ac80":"code","fc3e5a9f":"code","da148a17":"code","04c4c1f1":"code","924d7a13":"code","d3cfb72e":"code","22893e60":"code","8819b8ae":"code","3b394647":"code","251e38ee":"code","a0d4713e":"code","3d5a49b0":"code","484ccf05":"code","c747183f":"code","1d361df6":"code","f4bd776b":"code","ebb4c511":"code","2ae1178a":"code","48c02afd":"code","24dbcb24":"code","c584a8b2":"code","f38ca423":"code","2b896760":"code","eeb7884f":"code","ae2b9238":"code","13902665":"code","af5aa0a4":"code","60391b55":"code","9c455948":"code","87e38cf1":"code","2e2b8bf1":"code","b41989e6":"code","fce2724a":"code","d36124c8":"code","b7101cbe":"code","1e51df50":"code","e5b5b9be":"code","b6250da3":"code","c6367e3c":"code","27486dee":"code","3af36862":"code","4d64e557":"code","a1c7d31f":"code","244c138c":"code","d11eeef8":"code","6af77a3e":"code","a5653b37":"code","f0b64376":"code","20db33f4":"code","280944c5":"code","6f17e0a0":"code","b7b6856b":"code","16d86148":"code","f4e04bc4":"code","0b753baa":"code","fb5657a3":"code","b2c17fda":"code","30b0f8c8":"code","7c6b014e":"code","341d0b6a":"code","de2dde71":"code","6d64f508":"code","df963d10":"code","9bbbe2bf":"code","fac6f5df":"code","4486711c":"code","cb6a3123":"code","7598dc44":"code","1be26b52":"code","db328bb0":"code","1772e578":"code","eecf69c4":"code","4b791bfa":"code","226fd2c8":"code","447c346e":"code","3e364abb":"code","ca834573":"code","43a7e8a1":"code","e4f056c9":"code","11805a25":"code","89a11841":"code","e389c23d":"code","71ade409":"code","dbaeee7d":"code","bc7bf0b0":"code","1a6139e3":"code","d8ba980d":"code","f325fbe9":"code","f9e37312":"code","c2ae7d03":"code","02626ad0":"code","c37d1e23":"code","d3244d70":"code","6713aaf0":"code","f85a5646":"code","8e6d7556":"code","567cd50f":"code","a5af5d66":"code","c51e447f":"code","612d248b":"code","b1ab5063":"code","30662901":"code","c00c8a87":"code","126a5c64":"code","3c7002ac":"code","c2b34baa":"code","e741587f":"code","ae6a0f69":"code","35723ce3":"code","3e5f0efa":"code","602df9a6":"code","290979b7":"code","2c1b162c":"code","de9f03d6":"code","554dc1dc":"code","0259f843":"code","9c7d16ee":"code","9621dc55":"code","cbbd1099":"code","fb79ce27":"code","ea311be9":"code","5b246154":"code","bfd6e7f2":"code","0fce74ba":"code","9014243a":"code","f05983f9":"code","c8225421":"markdown","52e1118f":"markdown","e29c3c3b":"markdown","30bab048":"markdown","b3279ec1":"markdown","1f06a59d":"markdown","303a40fc":"markdown","219ded6b":"markdown","0ac12113":"markdown","e835d5f1":"markdown","aad471d1":"markdown","505c1b4b":"markdown","8e4f4c08":"markdown","8de7b2e1":"markdown","e37cd510":"markdown","cd4485d6":"markdown","34d29eb1":"markdown","3a1d54c9":"markdown","21dc13aa":"markdown","ff21d7f6":"markdown","3f11fa37":"markdown","e5a3f9fa":"markdown","71c429ad":"markdown","2d3905d0":"markdown","26291f87":"markdown","08e455ef":"markdown","aee86c83":"markdown","ee3067b4":"markdown","1fead272":"markdown","8bf82656":"markdown","0d2fa74c":"markdown","abea86a6":"markdown","6f1a9336":"markdown","847adbe5":"markdown","68c3a3fb":"markdown","179bf3e7":"markdown","c5c63be8":"markdown","48736f6c":"markdown","656ead26":"markdown","5cf9798b":"markdown","9f559670":"markdown","b136db42":"markdown","6672fa68":"markdown","07b05c36":"markdown","39510a4a":"markdown","3e3fad02":"markdown","4e834475":"markdown","554d6fc8":"markdown","1b8c328f":"markdown","014042b8":"markdown","f0f1b853":"markdown","6db6de36":"markdown","964fccc5":"markdown","6562883b":"markdown","6beed47a":"markdown","3b414016":"markdown","5a38714c":"markdown","49769e12":"markdown","5d89431a":"markdown","c9b7c065":"markdown","85219153":"markdown","87fd9539":"markdown","cd78003b":"markdown","84384637":"markdown","cabe539b":"markdown","68e08e48":"markdown","9ad481fa":"markdown","4aa6a6dd":"markdown","7eb18f23":"markdown","70bafea9":"markdown","6d344447":"markdown","9d22a744":"markdown","826c263c":"markdown","39a4458e":"markdown","6d48e6ad":"markdown","bd5ebc2a":"markdown","6e3d6aec":"markdown","97609c6d":"markdown","f186e33e":"markdown","21871075":"markdown"},"source":{"ef8a929d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint('Done!')","698f8bfd":"# Just incase we need it\nif False:\n    !pip install --quiet --upgrade pip\n    !pip install --quiet pyspark\n\n    import pyspark\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder \\\n            .master(\"local[*]\") \\\n            .appName('pred_nn_model') \\\n            .config('spark.executor.memory','12gb') \\\n            .getOrCreate()\n\n    sc = spark.sparkContext\n    print('Spark Cluster Initialized Successfully!')","78d48a3f":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf\nimport tqdm\nimport time\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler  # for median\/range or Robust Scaling\nfrom sklearn.preprocessing import Normalizer  # for vector unit-length-norm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom copy import deepcopy\n# for the Q-Q plots\nimport scipy.stats as stats\n\n!pip install --upgrade --quiet pip\n!pip install --quiet feature-engine\nfrom feature_engine.outliers import Winsorizer\n\n!pip install --quiet gswrap\nimport gswrap\n\n\n# to display the total number columns present in the dataset\npd.set_option('display.max_columns', None)\n\nprint('imported!')","4b8f485c":"try: # detect TPUs\n    tpu = None\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept Exception as e: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","f69f54f0":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv', index_col=['id'])\nprint('Train data shape is:',train.shape)\ntrain.head(3)","595c3d4f":"test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv', index_col=['id'])\nprint('Test data shape is:',test.shape)\ntest.head(3)","32ac19b6":"sample_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\nprint('Sample data shape is:',sample_sub.shape)\nsample_sub.head(3)","bc3364f5":"train.isna().any().sum()","ae9a16e0":"test.isna().any().sum()","4d6f603d":"train.info()","879cdc54":"cat_vars = train.select_dtypes('object')\ncat_vars.shape","797b3ec3":"cat_vars.nunique()","08254b72":"cat_vars['cat0'].unique()","30a6ac80":"cat_vars.nunique().plot.bar(figsize=(12,6))\nplt.ylabel('Number of unique categories')\nplt.xlabel('Variables')\nplt.title('Cardinality of Categorical Variables', fontweight='bold', fontsize=16)\nplt.show()","fc3e5a9f":"def category_freq(df, x, y, thresh):\n    cols = list(df.columns)\n    col1, col2 = cols[x], cols[y]\n    fontdict_ = {'fontweight':'bold','fontsize':14}\n    \n    label_freq1 = df[col1].value_counts() \/ len(df)\n    label_freq2 = df[col2].value_counts() \/ len(df)\n    \n    fig, ax = plt.subplots(1, 2, figsize=(16,5))\n\n    fig1 = label_freq1.sort_values(ascending=False).plot.bar(ax=ax[0])\n    fig1.axhline(y=thresh, color='red')\n    fig1.set_ylabel('Pct. values per category', fontdict=fontdict_)\n    fig1.set_xlabel(f'Variable: {col1}', fontdict=fontdict_)\n    fig1.set_title('Identifying Rare Categories', fontdict=fontdict_)\n\n    fig2 = label_freq2.sort_values(ascending=False).plot.bar(ax=ax[1], color='y')\n    fig2.axhline(y=thresh, color='red')\n    fig2.set_ylabel('Pct. values per category', fontdict=fontdict_)\n    fig2.set_xlabel(f'Variable: {col2}', fontdict=fontdict_)\n    fig2.set_title('Identifying Rare Categories', fontdict=fontdict_)\n\n    plt.show()","da148a17":"# For the 1st and 2nd cat vars\ncategory_freq(cat_vars, 0, 1, 0.1)","04c4c1f1":"# For the 3rd and 4th cat vars\ncategory_freq(cat_vars, 2, 3, 0.1)","924d7a13":"# For the 5th and 6th cat vars\ncategory_freq(cat_vars, 4, 5, 0.1)","d3cfb72e":"# For the 7th and 8th cat vars\ncategory_freq(cat_vars, 6, 7, 0.1)","22893e60":"# For the 9th and 10th cat vars\ncategory_freq(cat_vars, 8, 9, 0.1)","8819b8ae":"def box_plots(df, col):\n    plt.figure(figsize=(10,8))\n    sns.set_style('ticks')\n\n    plt.title(f'Boxplot showing relations between {col} and Target', fontsize=14, fontweight='bold')\n    sns.boxplot(col, 'target', data=df)\n    plt.xticks(color='red', fontsize=12, fontweight='bold')\n    plt.yticks(color='red', fontsize=12, fontweight='bold')\n    \n    plt.show()","3b394647":"for col in cat_vars.columns:\n    box_plots(train, col)","251e38ee":"# Average target for cat0\n\ntrain[['cat0', 'target']].groupby('cat0').mean()","a0d4713e":"# Average target for cat1\n\ntrain[['cat1', 'target']].groupby('cat1').mean()","3d5a49b0":"# Average target for cat2\n\ntrain[['cat2', 'target']].groupby('cat2').mean()","484ccf05":"# Average target for cat3\n\ntrain[['cat3', 'target']].groupby('cat3').mean()","c747183f":"# Average target for cat4\n\ntrain[['cat4', 'target']].groupby('cat4').mean()","1d361df6":"# Average target for cat5\n\ntrain[['cat5', 'target']].groupby('cat5').mean()","f4bd776b":"# The original column names of Categorical columns\nobj_cols = cat_vars.columns\nprint('Done!')","ebb4c511":"print('Before O-H-E, cat-vars has shape:',cat_vars.shape)","2ae1178a":"def drop_rare_cat_cols(df=cat_vars, thresh=0.1):\n    \"\"\"This method checks each Cat-column and saves the\n        names of values with <= 10% contribution to the columns\n        to be deleted after applying One-Hot-Endoding (O-H-E)\n    \"\"\"\n    drop_cols = []\n    for col in df.columns:\n        below_par = (cat_vars[col].value_counts() \/ len(cat_vars)) <= thresh\n        col_names = [col+'_' + name for name in list(below_par[below_par].index)]\n        drop_cols.extend(col_names)\n\n    return drop_cols","48c02afd":"rare_categories = drop_rare_cat_cols()\nprint(len(rare_categories))\nrare_categories[-5:]","24dbcb24":"cat_vars = pd.get_dummies(cat_vars)\nprint('After O-H-E, cat-vars has shape:',cat_vars.shape)\ncat_vars.head(3)","c584a8b2":"train_copy = deepcopy(train)\nprint(train_copy.shape)","f38ca423":"cat_vars.drop(rare_categories, axis=1, inplace=True)\ntrain_copy.drop(list(obj_cols), axis=1, inplace=True)  # drop former cat column names since O-H-E\n\nprint('Done!')","2b896760":"print('After dropping rare-categories, cat_vars shape is now:', cat_vars.shape)\ncat_vars.head(3)","eeb7884f":"train_copy.head(3)","ae2b9238":"train_copy = train_copy.join(cat_vars, on=cat_vars.index)\n    \nprint('Train copy data shape is:', train_copy.shape)\ntrain_copy.head(3)","13902665":"# Double check for NAN values after join\ntrain_copy.isna().any().sum()","af5aa0a4":"# Let's delete cat_vars from memory\ndel cat_vars\nprint('Deleted!')","60391b55":"num_cols = [col for col in train_copy.columns if not col.startswith('cat')]\nlen(num_cols)","9c455948":"# plot both together to compare\n\ndef reg_plots(df, col1, col2):\n    with strategy.scope():\n        st=time.time()\n        fig, ax = plt.subplots(1,2, figsize=(10,5))\n        sns.regplot(df[col1], df['target'], color='r', line_kws={'color':'navy','linewidth':2.5}, ax=ax[0])\n        ax[0].set_title(f\"RegPlot: Target and {col1}\")\n        sns.regplot(df[col2], df['target'], color='yellow', line_kws={'color':'navy','linewidth':2.5}, ax=ax[1])\n        ax[1].set_title(f\"RegPlot: Target and {col2}\")\n        print(f'Took {time.time()-st} secs!')\n        plt.show()","87e38cf1":"# For cont0 and cont1\n\nreg_plots(train_copy, num_cols[0], num_cols[1])","2e2b8bf1":"# For cont2 and cont3\n\nreg_plots(train_copy, num_cols[2], num_cols[3])","b41989e6":"# For cont4 and cont5\n\nreg_plots(train_copy, num_cols[4], num_cols[5])","fce2724a":"# For cont6 and cont7\n\nreg_plots(train_copy, num_cols[6], num_cols[7])","d36124c8":"# For cont8 and cont9\n\nreg_plots(train_copy, num_cols[8], num_cols[9])","b7101cbe":"# For cont10 and cont11\n\nreg_plots(train_copy, num_cols[10], num_cols[11])","1e51df50":"# For cont12 and cont13\n\nreg_plots(train_copy, num_cols[12], num_cols[13])","e5b5b9be":"corr_df = train_copy[num_cols]\ncorr_data = corr_df.corr()\n\nsns.set_style('ticks')\nplt.figure(figsize=(14,10))\nplt.title('Numerical Variables Correlation Matrix', fontsize=16)\n\nsns.heatmap(corr_data, annot=True)\n\nplt.show()","b6250da3":"corr_data","c6367e3c":"high_corrs = []\ncol_names = list(corr_data.columns)\n\nfor index, row in corr_data.iterrows():\n    count = -1\n    for r in row:\n        count+=1\n        if abs(r) >= 0.5:\n            x = (index, col_names[count], round(r, 2))\n            high_corrs.append(x)\n\n# Let's remove the correlation of same to same columns = 1.0\nhigh_corrs = [i for i in high_corrs if i[2] != 1.0]","27486dee":"high_corrs","3af36862":"corr_cols=set()\nfor i in high_corrs:\n    corr_cols.add(i[0])\n    corr_cols.add(i[1])\ncorr_cols","4d64e557":"data = train_copy[corr_cols]\ndata.head(3)","a1c7d31f":"def standardize(data_features):\n    data_features = (data_features - data_features.mean()) \/ data_features.std()\n    return data_features","244c138c":"data = data.apply(standardize, axis=0)\ndata.head()","d11eeef8":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nvif = pd.DataFrame()\nvif[\"VIF_Factor\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\nvif[\"features\"] = data.columns\n\nvif","6af77a3e":"# Let's remove the target column from numcols\n\nnum_cols.pop(-1)","a5653b37":"target = train_copy.pop('target')\ntrain.drop(['target'], axis=1, inplace=True)\n\ntrain_copy.head(3)","f0b64376":"def plot_hist(df,\n              df_name,\n              color=None,\n              suptitle=None,\n              size=(16,12),\n             linewidth=1.5,\n             edgecolor='black',\n             density=True):\n    \n    global num_cols\n    title = f'Numerical Values Distribution: {df_name}'\n    if suptitle:\n        title = df_name+': '+suptitle\n        \n    df[num_cols].hist(figsize=size,\n                     linewidth=linewidth,\n                     edgecolor=edgecolor,\n                     color=color,\n                     density=density)\n    \n    plt.suptitle(title, fontweight='bold', fontsize=16, y=0.95)\n    plt.show()","20db33f4":"plot_hist(train_copy, 'train-copy-Default')","280944c5":"check = train_copy[num_cols] < 0\ncheck_dict = {col:sum(check[col]) for col in check.columns}\ncheck_dict","6f17e0a0":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nprint('Done!')","b7b6856b":"def apply_boxcox_scaler(dataset, test=None):\n    \n    # Select only Numeric cols:\n    global num_cols\n    dataset_copy = deepcopy(dataset)\n    num_data = dataset_copy[num_cols]\n    \n    # first scale values to range(1,2),\n    scaler = MinMaxScaler(feature_range=(1, 2))\n    \n    # Then apply boxcox transform\n    power = PowerTransformer(method='box-cox')\n    \n    # After boxcox scale back to range(0,1)\n    scaler2 = MinMaxScaler(feature_range=(0, 1))\n    \n    pipeline = Pipeline(steps=[('s1', scaler),('p', power), ('s2', scaler2)])\n    data = pipeline.fit_transform(num_data)\n    \n    # convert the array back to a dataframe\n    data = pd.DataFrame(data)\n    data.columns = num_cols\n    \n    for col in data.columns:\n        dataset_copy[col] = list(data[col])\n        \n    if test is not None:\n        test_copy = deepcopy(test)\n        test_num_data = test_copy[num_cols]\n        test_data = pipeline.transform(test_num_data)\n        # convert the array back to a dataframe\n        test_data = pd.DataFrame(test_data)\n        test_data.columns = num_cols\n        for col in test_data.columns:\n            test_copy[col] = list(test_data[col])\n            return dataset_copy, test_copy\n    \n    return dataset_copy","16d86148":"train_boxcox = apply_boxcox_scaler(train_copy)\nplot_hist(train_boxcox, 'BoxCox', color='green')","f4e04bc4":"train_boxcox.head(3)","0b753baa":"# confirm no missing values from transformation\n\ntrain_boxcox.isna().any().sum() == train_copy.isna().any().sum() == 0","fb5657a3":"def mean_std_distance(df):\n    \"\"\"This method sums the absolute\n        distance between each feature mean\n        and zero and each feature std and one\n        and returns a dictionary with total distance\n        sums for all features for both mean and std\n    \"\"\"\n    mean_serie = df.apply(np.mean, axis=0)\n    std_serie = df.apply(np.std, axis=0)\n    \n    mean_sum = np.sum(np.abs(mean_serie)-0)\n    std_sum = np.sum(1 - np.abs(std_serie))\n    \n    distance_dict = {'MEAN-Distance':np.round(mean_sum, 4),\n                    'STD-Distance':np.round(std_sum, 4)}\n    \n    return distance_dict","b2c17fda":"train_boxcox_norm_dist =  mean_std_distance(train_boxcox[num_cols])\ntrain_boxcox_norm_dist","30b0f8c8":"def plot_range(df, df_name):\n    num_df = df.select_dtypes(['number'])\n    (num_df.max() - num_df.min()).plot.bar(figsize=(14,7))\n    title = f'Range of Numerical Values: {df_name}'\n    plt.title(title, fontweight='bold', fontsize=16)\n    plt.show()","7c6b014e":"plot_range(train_boxcox, 'train_boxcox')","341d0b6a":"# For example, let's see the cont10 variable if it's normally distributed\n\n# The blue dots should adjust to the 45 degree line\n\nfig1 = stats.probplot(train_boxcox['cont10'], dist=\"norm\", plot=plt)\n\nplt.show()","de2dde71":"# For example, let's see the cont10 variable if it's normally distributed\n\n# The blue dots should adjust to the 45 degree line\n\nfig1 = stats.probplot(train_boxcox['cont5'], dist=\"norm\", plot=plt)\n\nplt.show()","6d64f508":"fig, ax = plt.subplots(1, 2, figsize=(12,5))\n\nx, y = np.random.randint(0, len(num_cols), size=2)\ncol1, col2 = list(train_boxcox.columns)[x], list(train_boxcox.columns)[y]\n\nfig1 = sns.boxplot(y=train_boxcox[col1], ax=ax[0], color='brown')\nfig1.set_xlabel(f'{col1}',fontweight='bold', fontsize=12)\nfig1.set_title(f'Box-Plot of {col1}',fontweight='bold', fontsize=14)\n\nfig2 = sns.boxplot(y=train_boxcox[col2], ax=ax[1])\nfig2.set_xlabel(f'{col2}',fontweight='bold', fontsize=12)\nfig2.set_title(f'Box-Plot of {col2}',fontweight='bold', fontsize=14)\n\nplt.show()","df963d10":"def find_boundaries(variable, df, distance):\n    \"\"\"This method computes and returns the upper\n        and lower outlier boundaries for each variable.\n    \"\"\"\n\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n\n    return upper_boundary, lower_boundary","9bbbe2bf":"def outliers_toDF(df, distance):\n    outliers_dict = {}\n    for col in df.columns:\n        upper, lower = find_boundaries(col,df, distance)\n        outliers = np.where(df[col] > upper, True,\n        np.where(df[col] < lower, True, False))\n        count_outliers = np.sum(outliers)\n        outliers_dict[col] = [count_outliers]\n    \n    outliers_df = pd.DataFrame(outliers_dict).T\n    outliers_df.columns = ['count']\n    return outliers_df","fac6f5df":"outliers_trainBoxCox = outliers_toDF(train_boxcox, 3)\nprint(f'Train-BoxCox Extreme Outliers: {np.sum(outliers_trainBoxCox)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_trainBoxCox), train_boxcox.size))*100 \nprint('Extreme Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_trainBoxCox","4486711c":"def standardize_numCols(train_copy, test=None):\n    global num_cols\n    \n    copy_df = deepcopy(train_copy)\n    # Select only numeric cols\n    num_df = copy_df[num_cols]\n    means = num_df.mean()\n    stds = num_df.std()\n    num_df = (num_df - means) \/ stds\n    \n    for col in num_df.columns:\n        copy_df[col] = list(num_df[col])\n    \n    if test is not None:\n        copy_test = deepcopy(test)\n        # Select only numeric cols\n        test_num_df = copy_test[num_cols]\n        test_num_df = (test_num_df - means) \/ stds\n        \n        for col in test_num_df.columns:\n            copy_test[col] = list(test_num_df[col])\n        \n        return copy_df, copy_test\n\n    return copy_df","cb6a3123":"train_stdize = standardize_numCols(train_copy)\nplot_hist(train_stdize, 'Standardize-DF', color='y')","7598dc44":"train_stdize.head(3)","1be26b52":"# confirm no missing values from transformation\n\ntrain_stdize.isna().any().sum() == train_copy.isna().any().sum() == 0","db328bb0":"train_stdize_norm_dist =  mean_std_distance(train_stdize[num_cols])\ntrain_stdize_norm_dist","1772e578":"# For example, let's see the cont10 variable if it's normally distributed\n\n# The blue dots should adjust to the 45 degree line\n\nfig1 = stats.probplot(train_stdize['cont5'], dist=\"norm\", plot=plt)\n\nplt.show()","eecf69c4":"plot_range(train_stdize, 'train_stdize')","4b791bfa":"outliers_train_stdize = outliers_toDF(train_stdize, 3)\nprint(f'Total Extreme Outliers for Train-Stdize: {np.sum(outliers_train_stdize)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_train_stdize), train_stdize.size))*100 \nprint('Extreme Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_train_stdize","226fd2c8":"def mean_norm(train_copy, test=None):\n    \"\"\"This method performs\n        feature-wise mean norm\n        by subtracting the mean and \n        dividing by the range of each\n        feature distribution\n    \"\"\"\n    global num_cols\n    copy_train = deepcopy(train_copy)\n\n    # Select only numeric cols\n    num_df = copy_train[num_cols]\n    \n    # Let's learn the means\n    means = num_df.mean(axis=0)\n    \n    # Let's learn the ranges\n    ranges = num_df.max(axis=0) - num_df.min(axis=0)\n    \n    # Fit the learned means and ranges to the train set\n    train_num_scaled = (num_df - means) \/ ranges\n    \n    for col in train_num_scaled:\n        copy_train[col] = list(train_num_scaled[col])\n    \n    if test is not None:\n        # If test, also fit it on the test set\n        test_copy = deepcopy(test)\n        test_df = test_copy[num_cols]\n        test_num_scaled = (test_df - means) \/ ranges\n        for col in test_num_scaled:\n            test_copy[col] = list(test_num_scaled[col])\n        return copy_train, test_copy\n    \n    return copy_train","447c346e":"mean_norm_df = mean_norm(train_copy)\nplot_hist(mean_norm_df, 'Mean-Norm-DF', color='aqua')","3e364abb":"mean_norm_df.head(3)","ca834573":"# confirm no missing values from transformation\n\nmean_norm_df.isna().any().sum() == train_copy.isna().any().sum() == 0","43a7e8a1":"plot_range(mean_norm_df, 'mean_norm_df')","e4f056c9":"mean_norm_dist =  mean_std_distance(mean_norm_df[num_cols])\nmean_norm_dist","11805a25":"outliers_mean_norm = outliers_toDF(mean_norm_df, 3)\nprint(f'Total Extreme Outliers, Mean-Norm: {np.sum(outliers_mean_norm)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_mean_norm), mean_norm_df.size))*100 \nprint('Extreme Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_mean_norm","89a11841":"def robust_scaler(trainset, test=None):\n    global num_cols\n    trainset_copy = deepcopy(trainset)\n    # set up the scaler\n    scaler = RobustScaler()\n\n    # fit the scaler to the train set, it will learn the parameters\n    # Fit to only numerical columns\n    num_arr = scaler.fit_transform(trainset_copy[num_cols])\n    # convert back to df\n    num_df = pd.DataFrame(num_arr, columns=num_cols)\n    num_df.index.name='id'\n    \n    for col in num_df.columns:\n            trainset_copy[col] = list(num_df[col])\n\n    try:            \n        # transform testset\n        test_copy = deepcopy(test)\n        test_num_arr = scaler.transform(test_copy[num_cols])\n        # convert back to df\n        test_df = pd.DataFrame(test_num_arr, columns=num_cols)\n        test_df.index.name='id'\n        for col in test_df.columns:\n            test_copy[col] = list(test_df[col])\n    except:\n        return trainset_copy\n    \n    return trainset_copy, test_copy","e389c23d":"robust_norm_df = robust_scaler(train_copy)\nplot_hist(robust_norm_df, 'Robust-Scaler-DF', color='pink')","71ade409":"robust_norm_df.head(3)","dbaeee7d":"# confirm no missing values from transformation\n\nrobust_norm_df.isna().any().sum() == train_copy.isna().any().sum() == 0","bc7bf0b0":"plot_range(robust_norm_df, 'robust_norm_df')","1a6139e3":"mean_robust_norm_dist =  mean_std_distance(robust_norm_df[num_cols])\nmean_robust_norm_dist","d8ba980d":"outliers_robust_norm = outliers_toDF(robust_norm_df, 3)\nprint(f'Total Extreme Outliers, Robust-Norm: {np.sum(outliers_robust_norm)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_robust_norm), robust_norm_df.size))*100 \nprint('Extreme Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_robust_norm","f325fbe9":"# norm takes a str value of: l1 or l2\n\ndef vector_unit_scaler(trainset, norm, test=None):\n    # set up the scaler\n    global num_cols\n    train_copy = deepcopy(trainset)\n    scaler = Normalizer(norm=norm)\n\n    # fit\/transform the scaler to the train set, it will learn the parameters\n    # fit on only numeric columns\n    num_arr = scaler.fit_transform(train_copy[num_cols])\n    \n    # convert back to df\n    num_df = pd.DataFrame(num_arr, columns=num_cols)\n    num_df.index.name='id'\n    \n    for col in num_df.columns:\n        train_copy[col] = list(num_df[col])\n\n    try:\n        # transform testset\n        test_copy = deepcopy(test)\n        test_num_arr = scaler.transform(test_copy[num_cols])\n        # convert back to df\n        test_df = pd.DataFrame(test_num_arr, columns=num_cols)\n        test_df.index.name='id'\n        \n        for col in test_df.columns:\n            test_copy[col] = list(test_df[col])\n    except:\n        return train_copy\n    \n    return train_copy, test_copy","f9e37312":"vectorl1_df = vector_unit_scaler(train_copy, 'l1')\nplot_hist(vectorl1_df, 'Vector Unit-Length L1-Norm', color='lime')","c2ae7d03":"vectorl1_df.head(3)","02626ad0":"# confirm no missing values from transformation\n\nvectorl1_df.isna().any().sum() == train_copy.isna().any().sum() == 0","c37d1e23":"plot_range(vectorl1_df, 'vectorl1_df')","d3244d70":"mean_vectorl1_df_dist =  mean_std_distance(vectorl1_df[num_cols])\nmean_vectorl1_df_dist","6713aaf0":"outliers_vectorl1_df_norm = outliers_toDF(vectorl1_df, 3)\nprint(f'Total Extreme Outliers, vectorl1_df_norm: {np.sum(outliers_vectorl1_df_norm)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_vectorl1_df_norm), vectorl1_df.size))*100 \nprint('Extreme Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_vectorl1_df_norm","f85a5646":"vectorl2_df = vector_unit_scaler(train_copy, 'l2')\nplot_hist(vectorl2_df, 'Vector Unit-Length L2-Norm', color='gold')","8e6d7556":"vectorl2_df.head(3)","567cd50f":"# confirm no missing values from transformation\n\nvectorl2_df.isna().any().sum() == train_copy.isna().any().sum() == 0","a5af5d66":"plot_range(vectorl2_df, 'vectorl2_df')","c51e447f":"mean_vectorl2_df_dist =  mean_std_distance(vectorl2_df[num_cols])\nmean_vectorl2_df_dist","612d248b":"outliers_vectorl2_df_norm = outliers_toDF(vectorl2_df, 3)\nprint(f'Total Extreme Outliers, vectorl2_df_norm: {np.sum(outliers_vectorl2_df_norm)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_vectorl2_df_norm), vectorl2_df.size))*100 \nprint('Extreme Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_vectorl2_df_norm","b1ab5063":"df_list = [train_boxcox,\n          train_stdize,\n          mean_norm_df,\n          robust_norm_df,\n          vectorl1_df,\n          vectorl2_df]\n\ndef norm_scale_summary(df_list):\n    global num_cols\n    \n    summary_dict = {'BoxCox':[], \n                    'Standardize':[], \n                    'MeanNorm':[], \n                    'RobustScale':[],\n                   'VectorUnitL1':[],\n                   'VectorUnitL2':[]}\n    \n    for ind, df in enumerate(df_list): \n        for key in list(summary_dict.keys())[ind:]:\n            tot_outliers = np.sum(outliers_toDF(df, 1.5))[0]\n            pct_outliers = np.round(np.divide(tot_outliers, df.size)*100,2)\n            details = [tot_outliers,\n                      pct_outliers,\n                      np.round(list(mean_std_distance(df[num_cols]).values())[0],2),\n                      np.round(list(mean_std_distance(df[num_cols]).values())[1],2),\n                      np.round(np.min(df).min(), 2),\n                      np.round(np.max(df).max(),2),\n                      np.round(np.max(df).max()-np.min(df).min(),2)]\n            \n            summary_dict[key].extend(details)\n            break\n            \n    summary_df = pd.DataFrame(summary_dict, index=['Total_Outliers',\n                                                    'Pct_Outliers',\n                                                    'Sum_MEAN_Dist',\n                                                    'Sum_STD_Dist',\n                                                    'Min_Value',\n                                                    'Max_Value',\n                                                    'Range'])\n    \n    summary_df.loc['Uniform_Range'] = ['Yes', 'No', 'Yes', 'No', 'Almost','Almost']\n    summary_df.index.name='Metrics'\n    \n    return summary_df","30662901":"summary_df = norm_scale_summary(df_list)\nsummary_df","c00c8a87":"winsorizer = Winsorizer(capping_method='iqr', fold=3, tail='both')\nprint('Done!')","126a5c64":"# function to create histogram, Q-Q plot and\n# boxplot of specific variables\n\ndef diagnostic_plots(df, var_name, when):\n    # function takes a dataframe (df) and\n    # the variable of interest as arguments\n    fontdict={'fontweight':'bold', 'fontsize':'14'}\n    # define figure size\n    plt.figure(figsize=(16, 4))\n\n    # histogram\n    plt.subplot(1, 3, 1)\n    sns.distplot(df[var_name], bins=30)\n    plt.title('Histogram',fontdict=fontdict)\n\n    # Q-Q plot\n    plt.subplot(1, 3, 2)\n    stats.probplot(df[var_name], dist=\"norm\", plot=plt)\n    plt.ylabel(f'{var_name} quantiles')\n    plt.title('Probability Plot', fontdict=fontdict)\n\n    # boxplot\n    plt.subplot(1, 3, 3)\n    sns.boxplot(y=df[var_name])\n    plt.title('Boxplot', fontdict=fontdict)\n    \n    plt.suptitle(f'Histogram, Q-Q-Plot and Box-plot of Variable: {var_name} {when} Winsorization',\n                 fontweight='bold',\n                 fontsize=16,\n                 y=1.08)\n    plt.show()","3c7002ac":"# Let's look at variable cont8 before applying winsorization\n\ndiagnostic_plots(vectorl1_df, 'cont8', when='before')","c2b34baa":"chck = winsorizer.fit_transform(vectorl1_df)\nprint('Done!')","e741587f":"# Let's look at variable cont8 before applying winsorization\n\ndiagnostic_plots(chck, 'cont8', when='after')","ae6a0f69":"def calc_kurtosis(df):\n    n = len(df)\n    step1 = ((df - df.mean())**4) \/ (df.std())**4\n    step2 = (np.sum(step1).sum()) \/ n\n    \n    return np.round(step2, 4)","35723ce3":"kurtosis_before_winsorizer = calc_kurtosis(mean_norm_df)\nkurtosis_before_winsorizer","3e5f0efa":"mean_norm_df = winsorizer.fit_transform(mean_norm_df)\nprint('Done!')","602df9a6":"kurtosis_after_winsorizer = calc_kurtosis(mean_norm_df)\nkurtosis_after_winsorizer","290979b7":"# Let's see the data\nmean_norm_df.head(3)","2c1b162c":"# Let's see the normalized data\nsummary = 'Numerical Values Distribution after Winsorizer'\nplot_hist(mean_norm_df, 'Mean-Norm', color='aqua', suptitle=summary)","de9f03d6":"plot_range(mean_norm_df, 'mean_norm_df')","554dc1dc":"queer_vars = ['cat0_A', 'cat2_A', 'cat4_B', 'cat6_A', 'cat7_E', 'cat8_A', 'cat8_G', 'cat9_I', 'cat9_L']\ndf = mean_norm_df\n[(min(df[i]), max(df[i])) for i in queer_vars]","0259f843":"print(f'mean-norm-df shape is {mean_norm_df.shape}')","9c7d16ee":"mean_norm_dist =  mean_std_distance(mean_norm_df[num_cols])\nmean_norm_dist","9621dc55":"outliers_mean_norm = outliers_toDF(mean_norm_df, 1.5)\nprint(f'Total Outliers, Mean-Norm: {np.sum(outliers_mean_norm)}')\nprint()\n\noutliers_to_data_size_percent = (np.divide(np.sum(outliers_mean_norm), mean_norm_df.size))*100 \nprint('Outliers Pct to Data-Size:',outliers_to_data_size_percent)\noutliers_mean_norm","cbbd1099":"print(mean_norm_df.shape)\nmean_norm_df.head(3)","fb79ce27":"class FinalPrep(object):\n\n    def __init__(self, \n                 train=train, \n                 test=test, \n                 train_copy=train_copy, \n                 num_cols=num_cols, \n                 OHE=False, \n                 transform='MEAN', \n                 method='iqr', \n                 fold=3):\n        self.train = train\n        self.test = test\n        self.train_copy = train_copy\n        self.num_cols = num_cols\n        self.OHE = OHE\n        self.transform = transform\n        self.method = method\n        self.fold = fold\n        \n    def _copy_test(self):\n        \"\"\"Make a deep copy of\n            the test set\n        \"\"\"\n        test_copy = deepcopy(self.test)\n\n        try:\n            assert test_copy.shape == self.test.shape\n            assert list(test_copy.columns) == list(self.test.columns)\n        except AssertionError as e:\n            print('ERROR: Copy-Test:')\n            return e\n\n        return test_copy\n\n    ######################################\n\n    def _keep_same_cols(self):\n        \"\"\"Align test set columns \n            to trainset columns\n        \"\"\"\n        test_copy =  self._copy_test()\n        df = None\n        train = None\n        if self.OHE:\n            train = self.train_copy\n            df = pd.get_dummies(test_copy)\n            df = df[train.columns]\n            \n        else:\n            train = self.train\n            df = test_copy[train.columns]\n            \n\n        try:\n            assert df.shape[1] == train.shape[1]\n            assert list(df.columns) == list(train.columns)\n        except AssertionError as e:\n            print('ERROR: Keep-Same-Cols:')\n            return e\n\n        return df\n\n    ######################################\n\n    def _apply_transform(self):\n        \"\"\"Apply specific scaling or\n            normalizing method to the\n            trainset and transform the\n            test set with same params.\n        \"\"\"\n        activation = self.transform\n        test_copy = self._keep_same_cols()\n        train = self.train\n        \n        if self.OHE:\n            train = self.train_copy\n        \n        # ensure activation is all lower-case\n        activation = activation.lower()\n        X, y = None, None\n\n        if 'box' in activation:\n            print('Applying Boxcox transformation...')\n            X, y = apply_boxcox_scaler(train, test_copy)\n        elif 'mean' in activation:\n            print('Applying Mean-norm transformation...')\n            X, y = mean_norm(train, test_copy)\n        elif 'standard' in activation:\n            print('Applying Standard transformation...')\n            X, y = standardize_numCols(train, test_copy)\n        elif 'robust' in activation:\n            print('Applying Robust transformation...')\n            X, y = robust_scaler(train, test_copy)\n        elif 'l1' in activation:\n            print('Applying Vector Unit-Scaler L1 transformation...')\n            X, y = vector_unit_scaler(train, 'l1', test_copy)\n        else:\n            print('Applying Vector Unit-Scaler L2 transformation...')\n            X, y = vector_unit_scaler(train, 'l2', test_copy)\n\n        try:\n            assert X.shape[1] == y.shape[1]\n            assert list(X.columns) == list(y.columns)\n        except AssertionError as e:\n            print('ERROR: Apply-Scale-Norm:')\n            return e\n\n        return X, y\n\n    ######################################\n    \n    @staticmethod\n    def clean_winsorizer(df):\n        \"\"\"Helper function for\n            winsorization function.\n        \"\"\"\n        \n        queer_vars = [col for col in df.columns if round(df[col].min(),1) == round(df[col].max(),1)]\n        return queer_vars\n\n    def _winsorization(self):\n        X, y = self._apply_transform()\n        method = self.method\n        fold = self.fold\n        \n        winsorizer = Winsorizer(capping_method=method, \n                                fold=fold, \n                                tail='both')\n\n        X_winsored = winsorizer.fit_transform(X)\n        y_winsored = winsorizer.transform(y)\n        \n        if self.OHE:\n            queer = FinalPrep.clean_winsorizer(X_winsored)\n            #X_winsored.drop(queer, axis=1, inplace=True)\n            #y_winsored.drop(queer, axis=1, inplace=True)\n\n        try:\n            assert X_winsored.shape[1] == y_winsored.shape[1]\n            assert list(X_winsored.columns) == list(y_winsored.columns)\n        except AssertionError as e:\n            print('ERROR: Winsorization:')\n            return e\n\n        return X_winsored, y_winsored\n\n    ######################################\n\n    def _reduce_datasets(self):\n        train_winsored, test_winsored = self._winsorization()\n        num_cols = self.num_cols\n        \n        train_size = train_winsored.memory_usage().sum()\n        test_size = test_winsored.memory_usage().sum()\n\n        # Reducing Num_cols to Float 32\n        for col in train_winsored[num_cols]:\n            train_winsored[col] = train_winsored[col].astype('float32')\n            test_winsored[col] = test_winsored[col].astype('float32')\n\n        # Reducing Cat_cols to Int 32 if OHE\n        if self.OHE:\n            for col in set(train_winsored.columns) - set(num_cols):\n                train_winsored[col] = train_winsored[col].astype('int32')\n                test_winsored[col] = test_winsored[col].astype('int32')\n\n        print(f'train memory reduced by {100-((train_winsored.memory_usage().sum()\/train_size)*100)} Pct!')\n        print(f'test memory reduced by {100-((test_winsored.memory_usage().sum()\/test_size)*100)} Pct!')\n        print()\n\n        return train_winsored, test_winsored\n    \n    def train_test(self):\n        return self._reduce_datasets()","ea311be9":"transformations = ['boxcox', 'mean-norm', 'robust-scale', 'standardize', 'L1', 'L2']\ndatasets = []","5b246154":"for transform in transformations:\n    final_prep = FinalPrep(OHE=True, transform=transform)\n    (train_, test_) = final_prep.train_test()\n    datasets.append((train_, test_))","bfd6e7f2":"datasets[0][0].to_csv('train_boxcox.csv')\ndatasets[0][1].to_csv('test_boxcox.csv')\ndatasets[1][0].to_csv('train_meannorm.csv')\ndatasets[1][1].to_csv('test_meannorm.csv')\ndatasets[2][0].to_csv('train_robust.csv')\ndatasets[2][1].to_csv('test_robust.csv')\ndatasets[3][0].to_csv('train_standard.csv')\ndatasets[3][1].to_csv('test_standard.csv')\ndatasets[4][0].to_csv('train_l1.csv')\ndatasets[4][1].to_csv('test_l1.csv')\ndatasets[5][0].to_csv('train_l2.csv')\ndatasets[5][1].to_csv('test_l2.csv')\n\nprint('Datasets Saved!')","0fce74ba":"target = target.astype('float32')\ntarget.to_csv('target.csv')","9014243a":"!ls","f05983f9":"pd.read_csv('train_standard.csv', index_col='id').head(3)","c8225421":"**So we shall use standard scaler with the box-cox transformation**","52e1118f":"### Instantiate an instance of the FinalPrep class, passing if we want One-Hot-Encoding or Not and the type of transformation for the final test and train data sets.","e29c3c3b":"## 28. Saving the pre-processed train, target and test sets...","30bab048":"**I'd create a function that takes a dataframe and the factor (default is 1.5 but can be changed) to use in the IQR calculation and returns the IQR proximity rule boundaries:**","b3279ec1":"**Let's create a Winsorizer object to Cap outliers based on the same Inter-Quantile-Range we specified earlier**","1f06a59d":"**Let's investigate the relationship between cat0 and Target**","303a40fc":"## 13. Checking for Multi-collinearity","219ded6b":"## 15. Dropping Target Variable\n\nMost machine learning models work better with a normalized data set. We shall use the box-cox normalization for the numerical columns","0ac12113":"**The MEAN-distance and STD-distance above show the sum-total of how far the numeric features in the DataFrame, in this case `train-boxcox` are far away from a MEAN of 0 and STD of 1**","e835d5f1":"**Yep!! it's gone down from 245 to a mere 41, because the winsorizer has fixed a lot of the outliers in the data!, the only outlies left may warrant us to be more strict with our IQR rate down to 1.5 from 3.0 to capture them all**","aad471d1":"**Let's see how far the numeric values are from the MEAN and STD**","505c1b4b":"**Next, we apply the VIF**","8e4f4c08":"**Let's return to the `mean_norm_df` data and apply kurtosis**","8de7b2e1":"**We can see that there is virtually no distance between the features MEAN and STD of `train_stdize`, as impress as this is, let's see the range of distribution.**","e37cd510":"**Let's see the normalized features**","cd4485d6":"#### The decision is between Box-Cox and Mean-Norm. They both have a perfect range across all features between 0 and 1. But Mean-Norm balances the data better with zero deviation from the mean of 0 and slightly lower deviation from the std of 1 than Box-Cox. Also Box-cox is just slightly better on outliers as compared to Mean-norm. But since I'd treat outliers soon, I so far prefer the centralised distribution of Mean-norm.\n\n#### Standardization is not an option because even though it perfectly balances the distribution of all variables at a MEAN of 0 and STD of 1, it causes a chaotic range of distribution and an outlier ratio worse than Box-cox and similar to Mean-norm... Let's try a couple more.","34d29eb1":"Here, we want to grab the rare cat-column names, just as they would appear after the O-H-E, so that we can drop them off as individual columns from cat_vars dataframe, immediately after applying O-H-E.","3a1d54c9":"## 21. Applying Robust-Scaling to Train-copy\n\n**This is also called scaling with median and quantiles.When scaling variables to the median and quantiles, the median value is deducted from the observations and the result is divided by the inter-quartile range (IQR). Robust scaling produces more robust estimates for the center and value range of the variable, and is recommended if the data contains outliers, just like our present data**\n\n### X_scaled = X - X_median \/ ( X.quantile(0.75) - X.quantile(0.25) )","21dc13aa":"As much as possible, we want to have categorical variables in a column that seem to have distinct relationship with target, not too similar, hence they lose their predictive power.<br>\nLet's see the average target score per categorical variable per column","ff21d7f6":"## 8. Visualize the relationship between Categorical variables and Target","3f11fa37":"**We Can see that the distribution range of pure numerical values for Robust-norm is uneven and chaotic like Z-Score norm, going from 0 up to almost 5. While those of categorical values are within 0 to 1. Plus the MEAN-dist and STD-dist are worse than Mean-norm, but better than Boxcox. This makes Robust-norm not yet an ideal choice over Mean-Norm.**","e5a3f9fa":"**We can see the general bell-curve shape of the distribution from the histogram. This is the result of the mean-norm we did earlier. We can also see the 45-deg line and the blue dots that roughly keep to the red-line of the Q-Q Plot (Probability-Plot), this also indicates presence of a normal distribution. Finally, we can see the thick dotted ouliers above point 4on the y-axis of the Box-plot, indicating the presence of outliers.**","71c429ad":"#### train_stdize data has less than 3% outliers to dataset ratio...Let's try mean-normalization.","2d3905d0":"## 6. Check Cardinality of Categorical Variables..\n\n**Let's check the cardinality of each Cat variable and delete those with high cardinality...**","26291f87":"## 3. Read in The Data Files...","08e455ef":"**Next, we'd fit_transform the training data using the winsorizer and then transform the test data soon, with the learned parameters from the training data using the transform function of the Winsorizer**\n\n**But first let's sample a few variables and see the outliers before and after applying winsorization. Let's look at variables `cont8` and `cont2` of mean_norm_df.**","aee86c83":"**Box-Cox has done a good transformation with the shape of the data, but looking at the numeric variables, they are not centred around zero. This data does not have a general MEAN of ~0 and STD of ~1. Let's consider the Z-score or Standardization method**","ee3067b4":"**Let's find the extreme outliers for train-boxcox, these are outliers 3 times the IQR, rate. While normal outliers are outliers 1.5 the IQR**","1fead272":"Finally reduce the target size and save it..","8bf82656":"## 10. Drop categorical columns with 10% or less value contribution per variable","0d2fa74c":"**Count the cardinality values per categorical column**","abea86a6":"## 1. Import relevant libraries","6f1a9336":"## 18. Exploring Outliers...\n\n**An outlier is a data point that is significantly different from the remaining data. On occasions, outliers are very informative; for example, when looking for credit card transactions, an outlier may be an indication of fraud. In other cases, outliers are rare observations that do not add any additional value.**\n\n**We'd apply the `inter-quartile range (IQR) proximity rule`. According to the IQR proximity rule, a value is an outlier if it falls outside these boundaries:**\n\n```\nUpper boundary = 75th quantile + (IQR * 1.5)\n\nLower boundary = 25th quantile - (IQR * 1.5)\n\nHere, IQR is given by the following equation:\n\nIQR = 75th quantile - 25th quantile\n```","847adbe5":"**Merge both dataframes...**","68c3a3fb":"**First we make the unique high-corr data a dataframe**","179bf3e7":"## 9. One-Hot Encoding Categorical Variables","c5c63be8":"## 25. Winsorization to Address Outliers...\n\n**Winsorization, or winsorizing, is the process of transforming the data by limiting the extreme values, that is, the outliers, to a certain arbitrary value, closer to the mean of the distribution. Winsorizing is different from trimming because the extreme values are not removed, but are instead replaced by other values. A typical strategy involves setting outliers to a specified percentile.**","48736f6c":"**Double-check for possible NAN values after join**","656ead26":"## 14. Applying Variance-Inflation-Factor\n\n**We shall use VIF to determine the overall columns with high multi-collinearity and seive them out...**\n\n**[Link](https:\/\/github.com\/Lawrence-Krukrubo\/Understanding_Multiple_Linear_Regression\/blob\/master\/coefficients_of_multiple_linear_regression.ipynb)**","5cf9798b":"## 16. Applying Box-Cox Transformation to Original Dataset...\n\n**Here we use the Power Transform Function to Normalize the data**","9f559670":"## 27. Performing Data Transformations on Test set\n\n#### We also need to transform the test set on values learnt on the training set.<br>The functions below perform all the transformations we've been doing for specific activation functions in addition to reducing the train, test and target datasets and returning these fit for machine learning....","b136db42":"**We can see that by removing outliers, certain categorical variables basically have no more distribution. It seems these variables are just around 0 all through. Let's print their min and max values to be certain.**","6672fa68":"## 20. Applying Mean-Normalization to Original Dataset\n\n**In mean normalization, we center the variable at zero and rescale the distribution to the value range. This procedure involves subtracting the MEAN from each observation and then dividing the result by the difference between the minimum and maximum values:**","07b05c36":"**Let's see how far away each numeric feature's MEAN and STD is from 0 and 1**","39510a4a":"**Let's define a method that plots each Cat feature and a threshold of percentage importance per Cat feature-value to each Cat feature.**","3e3fad02":"**First let's calculate the kurtosis score and see if it's gone down...**","4e834475":"**The VIF is a measure of colinearity among predictor variables within a multiple regression. If the outcome is 1, it\u2019s okay. If it\u2019s between 1 and 5, it shows low to average colinearity, and above 5 generally means highly redundant and variable should be dropped.<br>In this case all VIF scores are just between 1 and 2.5 and this is not enough to drop the columns , so we continue...**","554d6fc8":"## 19. Applying Standardization to Train-Copy dataset...\n\n**Let's try to reposition the data to have mean ~0 and Std ~1. Standardization is also called Z-Score-Norm**","1b8c328f":"## 23. Applying Scaling to Vector Unit-Length: L2-Norm.\n\n**When scaling to vector unit length, we transform the components of a feature vector so that the transformed vector has a length of 1, or in other words, a norm of 1. Note that this scaling technique scales the feature vector, as opposed to each individual variable. A feature vector contains the values of each variable for a single observation. When scaling to vector unit length, we divide each feature vector by its norm, using either `l1` (manhattan-dist) or `l2` (euclidean-dist) norm.**","014042b8":"## 7. Pinpointing rare categories in categorical variables\n\nCategories that appear in a tiny proportion of the observations are rare. Typically, we consider a label to be rare when it appears in less than 5% or 1% of the population","f0f1b853":"**Each column of values seem too alike and it makes no sense to keep all. We observe this through the boxplot as well as the average comparison to the target variable done above for each cat column. we'd drop Cat O-H-E columns that don't have significance up to a certain threshold we choose.**","6db6de36":"First let's visualize the current shapes","964fccc5":"**We can see that the dotted outliers have disappeared after applying winsorization to the `vectorl1_df` data**","6562883b":"**Clearly there is no linear relationship between each numerical variable and Target<br>The only way to learn any meaningful representation is to use a non-linear style regression**\n\n**One more important thing we can learn from the corr-matrix is that some columns may be highly correlated or multi-collinearity issues. We can identify these by the much lighter colors in the matrix. We need to treat all such columns**","6beed47a":"## 5. Let's see the summary info of the data","3b414016":"**The range of distribution above, from the box-cox transformation is just awesomely perfect! Even though the MEAN and STD are not around 0 or 1**","5a38714c":"## 11. Visualizing The Relationship between Numerical Variables and Target","49769e12":"**Though the data is more central around the mean, the range is uneven and chaotic. Let's see the outliers**","5d89431a":"**Now, let's apply the winsorizer**","c9b7c065":"**Let's re-check the distance of means from 0 and stds from 1 in mean-norm-df**","85219153":"## 4. Checking for missing Values","87fd9539":"**The percentage of outliers to data size has also reduced from 3.01 to 0.33. This balance of 0.33 refers to the few original numerical variables that have moderate outliers around 1.5 times IQR.**","cd78003b":"## 22. Applying Scaling to Vector Unit-Length: L1-Norm.\n\n**When scaling to vector unit length, we transform the components of a feature vector so that the transformed vector has a length of 1, or in other words, a norm of 1. Note that this scaling technique scales the feature vector, as opposed to each individual variable. A feature vector contains the values of each variable for a single observation. When scaling to vector unit length, we divide each feature vector by its norm, using either `l1` (manhattan-dist) or `l2` (euclidean-dist) norm.**","84384637":"**Let's first make a copy of the original train set and use this for the transformations**","cabe539b":"**Next, we standardize the data**","68e08e48":"### Note that each transformation function must only contain one key-word amongst other words.<br>Key words are:- \n\n1. BOX (for boxcox activation)\n2. MEAN (for mean-norm activation)\n3. ROBUST (for robust scaling)\n4. L1 (for vector unit-length L1)\n5. L2 (for vector unit-length L2)\n6. STANDARD (for standardization)","9ad481fa":"**Let's see how far away values are from the MEAN and STD**","4aa6a6dd":"## 2. TPU Check...","7eb18f23":"**Let's randomly plot two variables, we might see some outliers.<br>Running the cell below repeatedly plots different pairs of variables.**","70bafea9":"**Let's see one of the variables**","6d344447":"**Well, it turns out that the percent of outliers to total data size is < 3% for `train-boxcox`.**","9d22a744":"**Let's make a plot with the cardinality of each categorical variable**","826c263c":"## 17. Visualizing Variable Normality using a Q-Q Plot\n\n**Normality can be also assessed by Q-Q plots. In a Q-Q plot we plot the quantiles of the variable in the y-axis and the expected quantiles of the normal distribution in the x-axis. If the variable follows a normal distribution, the dots in the Q-Q plot should fall in a 45 degree diagonal line.**","39a4458e":"**Let's see the distribution range of values after box-cox**","6d48e6ad":"**Just as suspected, all have a min equal min values and max values. These are the values whose relative values were dropped for being smaller than the threshold we set earlier. So let's continue by keeping these variables....**","bd5ebc2a":"## 24. Summarizing Feature Scaling\/Normalization Choices\n\n**Looking at the above charts for choice of Feature-Norm\/Scaling let's create a function that basically summarizes the key similarities or differences of each choice in a table...**","6e3d6aec":"Box_cox can't work with negative values, so let's confirm if we have negative values in the training set","97609c6d":"## 12. Correlation Strength\n\n**Looking at the regplot for each numerical variable and Target, there is a general weak linear relationship.<br>Let's investigate further with a correlation matrix**","f186e33e":"**Since both dataframes have different columns, using pd.concat will return NAN values. So let's use a simple join, since they both have the same index col.**","21871075":"## 26. Kurtosis AKA The 4th Statistical-Moment\n\n**Now just before we fit the Winsorizer on the dataset, let's compute the Kurtosis score of the data, which is a score that computes how much outliers are in the dataset. Kurtosis indicates the outlier content within the data.\nThe higher the Kurtosuis measure is, the more outliers are present and the longer the tails in the distribution of the histogram are**"}}