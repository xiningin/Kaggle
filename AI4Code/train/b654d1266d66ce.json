{"cell_type":{"faac3988":"code","6b1ab03b":"code","db069561":"code","d71247ba":"code","72b5f0ca":"code","c21e1f73":"code","38427ddd":"code","d7bd5be8":"code","f300efb4":"code","b656ad7c":"code","cf758c64":"code","038fb88d":"code","47a7e0f0":"code","58b82eee":"code","bdc48aae":"code","dfe3dc7a":"code","16d40598":"code","dac5c08f":"code","d73d430d":"code","995693f5":"code","60ca7f14":"code","4e19205a":"code","b82a8307":"code","9a22c6be":"code","eda51f01":"code","bd87bdf5":"code","5795b906":"code","dc57941f":"code","5ea5e018":"code","4ff06311":"code","662a39b4":"code","f8e646e4":"code","2a782a89":"code","d4ae583c":"code","5c415188":"code","2c59bf0c":"code","6c0f9967":"code","be5bcdf0":"code","aecf2b2b":"code","5ec0c264":"markdown","c10b1350":"markdown","8be22144":"markdown","a38ccfe5":"markdown","1b0be526":"markdown","bb8b0bd2":"markdown","4d81cba5":"markdown","26348a43":"markdown","a8e29586":"markdown","9dbde61b":"markdown","a7d50af6":"markdown"},"source":{"faac3988":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b1ab03b":"import cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import models \nfrom keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,Dropout,BatchNormalization\nfrom keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator","db069561":"def pic(file_path):\n    img=mpimg.imread(file_path)\n    plt.figure()\n    plt.imshow(img)","d71247ba":"building=['..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/10018.jpg',\n         '..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/1001.jpg',\n         '..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/10144.jpg']\nfor i in building:\n    pic(i)","72b5f0ca":"forest=['..\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/10037.jpg',\n       '..\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/10098.jpg',\n       '..\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/10142.jpg']\nfor i in forest:\n    pic(i)","c21e1f73":"glacier=['..\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/10024.jpg',\n        '..\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/10104.jpg',\n        '..\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/1010.jpg']\nfor i in glacier:\n    pic(i)","38427ddd":"mountain=['..\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/10031.jpg',\n         '..\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/10058.jpg',\n         '..\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/10057.jpg']\nfor i in mountain:\n    pic(i)","d7bd5be8":"sea=['..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10093.jpg',\n    '..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10061.jpg',\n    '..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10114.jpg',\n    '..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10122.jpg']\nfor i in sea:\n    pic(i)","f300efb4":"streat=['..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/10097.jpg',\n       '..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/10070.jpg',\n       '..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/10085.jpg']\nfor i in streat:\n    pic(i)","b656ad7c":"train_directory='..\/input\/intel-image-classification\/seg_train\/seg_train\/'\n\nval_directory='..\/input\/intel-image-classification\/seg_test\/seg_test'\n\ntest_directory='..\/input\/intel-image-classification\/seg_test\/seg_test'","cf758c64":"train_datagen=ImageDataGenerator(rescale=1\/255)\n\nval_datagen=ImageDataGenerator(rescale=1\/255)\n\ntest_datagen=ImageDataGenerator(rescale=1\/255)","038fb88d":"train_generator=train_datagen.flow_from_directory(train_directory,target_size=(150,150),\n                                                 class_mode='sparse',batch_size=128)\n\nval_generator=val_datagen.flow_from_directory(val_directory,target_size=(150,150),\n                                             class_mode='sparse',batch_size=128)\n\ntest_generator=test_datagen.flow_from_directory(test_directory,target_size=(150,150),class_mode='sparse')","47a7e0f0":"model_0=models.Sequential()\n\n\nmodel_0.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(150,150,3)))\nmodel_0.add(MaxPooling2D())\n\n\nmodel_0.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform'))\nmodel_0.add(MaxPooling2D())\n\nmodel_0.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform'))\n#model_0.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform'))\nmodel_0.add(MaxPooling2D())\n\nmodel_0.add(Conv2D(256,(3,3),activation='relu',kernel_initializer='he_uniform'))\n'''model_0.add(Conv2D(256,(3,3),activation='relu',kernel_initializer='he_uniform'))\nmodel_0.add(MaxPooling2D())'''\n\n\nmodel_0.add(Flatten())\n\nmodel_0.add(Dropout(0.25))\nmodel_0.add(Dense(300,activation='relu',kernel_initializer='he_uniform',kernel_regularizer='l2'))\nmodel_0.add(Dense(150,activation='relu',kernel_initializer='he_uniform',kernel_regularizer='l2'))\nmodel_0.add(Dense(40,activation='relu',kernel_initializer='he_uniform'))\n\n\nmodel_0.add(Dense(6,activation='softmax'))\n\nmodel_0.summary()","58b82eee":"model_0.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","bdc48aae":"history_0=model_0.fit_generator(train_generator,\n                   validation_data=val_generator,\n                   epochs=15)","dfe3dc7a":"plot_accuracy_loss(history_0)","16d40598":"history_0=model_0.fit_generator(train_generator,\n                   validation_data=val_generator,\n                   epochs=5)","dac5c08f":"history_0=model_0.fit_generator(train_generator,\n                   validation_data=val_generator,\n                   epochs=5)","d73d430d":"model_0.evaluate_generator(test_generator)","995693f5":"from keras.applications import VGG16","60ca7f14":"conv_base=VGG16(input_shape=(150,150,3),include_top=False)\nconv_base.summary()","4e19205a":"model=models.Sequential()\n\nmodel.add(conv_base)\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=150,kernel_initializer='he_uniform',activation='relu',kernel_regularizer='l2'))\nmodel.add(Dense(units=80,kernel_initializer='he_uniform',activation='relu',kernel_regularizer='l2'))\nmodel.add(Dense(units=30,kernel_initializer='he_uniform',activation='relu',kernel_regularizer='l2'))\nmodel.add(Dense(units=6,activation='softmax'))\n\nmodel.summary()","b82a8307":"model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer='adam')","9a22c6be":"history=model.fit_generator(train_generator,epochs=9,validation_data=val_generator)","eda51f01":"history=model.fit_generator(train_generator,epochs=3,validation_data=val_generator)","bd87bdf5":"model.evaluate_generator(test_generator)","5795b906":"def plot_accuracy_loss(history):\n    \"\"\"\n        Plot the accuracy and the loss during the training of the nn.\n    \"\"\"\n    fig = plt.figure(figsize=(20,10))\n\n    # Plot accuracy\n    plt.subplot(221)\n    plt.plot(history.history['accuracy'],'bo--', label = \"acc\")\n    plt.plot(history.history['val_accuracy'], 'ro--', label = \"val_acc\")\n    plt.title(\"train_acc vs val_acc\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    # Plot loss function\n    plt.subplot(222)\n    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n    plt.title(\"train_loss vs val_loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epochs\")\n\n    plt.legend()\n    plt.show()","dc57941f":"plot_accuracy_loss(history)","5ea5e018":"model.save('model.h5')","4ff06311":"labels=['buildings','forest','glacier','mountain','sea','street']","662a39b4":"def pred(img):\n    img=img_to_array(img)\n    img.shape\n    img=np.expand_dims(img,[0])\n    img=img\/255\n    img.shape\n    print(labels[np.argmax(model.predict(img))])","f8e646e4":"img=load_img('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10012.jpg',target_size=(150,150))\nimg","2a782a89":"pred(img)","d4ae583c":"img=load_img('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10175.jpg',target_size=(150,150))\nimg","5c415188":"pred(img)","2c59bf0c":"img=load_img('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10272.jpg',target_size=(150,150))\nimg","6c0f9967":"pred(img)","be5bcdf0":"img=load_img('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10286.jpg',target_size=(150,150))\nimg","aecf2b2b":"pred(img)","5ec0c264":"# VGG16","c10b1350":"# Setting Directory","8be22144":"# Glacier","a38ccfe5":"# Let's have a look at images\n\n## We have total of 6 categories to predict\n- Buildings\n- Forest\n- Glacier\n- Mountain\n- Sea\n- Street","1b0be526":"# Sea","bb8b0bd2":"# Model","4d81cba5":"## Building","26348a43":"# Predicting on pred set","a8e29586":"# Street","9dbde61b":"# Forest","a7d50af6":"# Mountain"}}