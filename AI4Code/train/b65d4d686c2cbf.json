{"cell_type":{"c614217c":"code","ecb477ef":"code","d8dda53d":"code","ab2f69c4":"code","45f96219":"code","5c8fee56":"code","cfe00972":"code","69207b55":"code","f0332bec":"code","411aa891":"code","81c17f63":"code","23509d02":"code","f2be7bfb":"code","0b118171":"code","7005ec44":"code","1cba77b7":"code","d6458893":"code","63d9fecf":"code","00873359":"markdown","9d9d84d1":"markdown","182d352f":"markdown","a6f13d9f":"markdown","b3263e07":"markdown","d0686b75":"markdown","48253f18":"markdown","306cc52b":"markdown","16098f6a":"markdown","72fcb05c":"markdown","74cde298":"markdown","28789434":"markdown","142baed8":"markdown","96691609":"markdown","8d9eb5f3":"markdown","59fcac76":"markdown","1b4289f7":"markdown","d435927d":"markdown","289bde08":"markdown"},"source":{"c614217c":"#Import numerical libraries\nimport pandas as pd\nimport numpy as np\n\n#Import graphical plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Import Linear Regression Machine Learning Libraries\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score","ecb477ef":"data = pd.read_csv('..\/input\/carmpg\/car-mpg (1).csv')\ndata.head()","d8dda53d":"#Drop car name\n#Replace origin into 1,2,3.. dont forget get_dummies\n#Replace ? with nan\n#Replace all nan with median\n\ndata = data.drop(['car_name'], axis = 1)\ndata['origin'] = data['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})\ndata = pd.get_dummies(data,columns = ['origin'])\ndata = data.replace('?', np.nan)\ndata = data.apply(lambda x: x.fillna(x.median()), axis = 0)","ab2f69c4":"data.head()","45f96219":"X = data.drop(['mpg'], axis = 1) # independent variable\ny = data[['mpg']] #dependent variable","5c8fee56":"#Scaling the data\n\nX_s = preprocessing.scale(X)\nX_s = pd.DataFrame(X_s, columns = X.columns) #converting scaled data into dataframe\n\ny_s = preprocessing.scale(y)\ny_s = pd.DataFrame(y_s, columns = y.columns) #ideally train, test data should be in columns","cfe00972":"#Split into train, test set\n\nX_train, X_test, y_train,y_test = train_test_split(X_s, y_s, test_size = 0.30, random_state = 1)\nX_train.shape","69207b55":"#Fit simple linear model and find coefficients\nregression_model = LinearRegression()\nregression_model.fit(X_train, y_train)\n\nfor idx, col_name in enumerate(X_train.columns):\n    print('The coefficient for {} is {}'.format(col_name, regression_model.coef_[0][idx]))\n    \nintercept = regression_model.intercept_[0]\nprint('The intercept is {}'.format(intercept))","f0332bec":"#alpha factor here is lambda (penalty term) which helps to reduce the magnitude of coeff\n\nridge_model = Ridge(alpha = 0.3)\nridge_model.fit(X_train, y_train)\n\nprint('Ridge model coef: {}'.format(ridge_model.coef_))\n#As the data has 10 columns hence 10 coefficients appear here    ","411aa891":"#alpha factor here is lambda (penalty term) which helps to reduce the magnitude of coeff\n\nlasso_model = Lasso(alpha = 0.1)\nlasso_model.fit(X_train, y_train)\n\nprint('Lasso model coef: {}'.format(lasso_model.coef_))\n#As the data has 10 columns hence 10 coefficients appear here   ","81c17f63":"#Model score - r^2 or coeff of determinant\n#r^2 = 1-(RSS\/TSS) = Regression error\/TSS \n\n\n#Simple Linear Model\nprint(regression_model.score(X_train, y_train))\nprint(regression_model.score(X_test, y_test))\n\nprint('*************************')\n#Ridge\nprint(ridge_model.score(X_train, y_train))\nprint(ridge_model.score(X_test, y_test))\n\nprint('*************************')\n#Lasso\nprint(lasso_model.score(X_train, y_train))\nprint(lasso_model.score(X_test, y_test))","23509d02":"#poly = PolynomialFeatures(degree = 2, interaction_only = True)\n\n#Fit calculates u and std dev while transform applies the transformation to a particular set of examples\n#Here fit_transform helps to fit and transform the X_s\n#Hence type(X_poly) is numpy.array while type(X_s) is pandas.DataFrame \n#X_poly = poly.fit_transform(X_s)\n#Similarly capture the coefficients and intercepts of this polynomial feature model","f2be7bfb":"data_train_test = pd.concat([X_train, y_train], axis =1)\ndata_train_test.head()","0b118171":"import statsmodels.formula.api as smf\nols1 = smf.ols(formula = 'mpg ~ cyl+disp+hp+wt+acc+yr+car_type+origin_america+origin_europe+origin_asia', data = data_train_test).fit()\nols1.params","7005ec44":"print(ols1.summary())","1cba77b7":"#Lets check Sum of Squared Errors (SSE) by predicting value of y for test cases and subtracting from the actual y for the test cases\nmse  = np.mean((regression_model.predict(X_test)-y_test)**2)\n\n# root of mean_sq_error is standard deviation i.e. avg variance between predicted and actual\nimport math\nrmse = math.sqrt(mse)\nprint('Root Mean Squared Error: {}'.format(rmse))","d6458893":"# Is OLS a good model ? Lets check the residuals for some of these predictor.\n\nfig = plt.figure(figsize=(10,8))\nsns.residplot(x= X_test['hp'], y= y_test['mpg'], color='green', lowess=True )\n\n\nfig = plt.figure(figsize=(10,8))\nsns.residplot(x= X_test['acc'], y= y_test['mpg'], color='green', lowess=True )","63d9fecf":"# predict mileage (mpg) for a set of attributes not in the training or test set\ny_pred = regression_model.predict(X_test)\n\n# Since this is regression, plot the predicted y value vs actual y values for the test data\n# A good model's prediction will be close to actual leading to high R and R2 values\n#plt.rcParams['figure.dpi'] = 500\nplt.scatter(y_test['mpg'], y_pred)","00873359":"## 2.c Regularized Lasso Regression","9d9d84d1":"We have to predict the mpg column given the features.","182d352f":"# 4. Model Parameter Tuning","a6f13d9f":"## 2.b Regularized Ridge Regression","b3263e07":"# Featurization, Model Selection & Tuning - Linear Regression","d0686b75":"## 2.a Simple Linear Model","48253f18":"# 2. Model building","306cc52b":"**Why is regularization required ?**\n\nWe are well aware of the issue of 'Curse of dimensionality', where the no. of columns are so huge that the no. of rows does not cover all the permutation and combinations that is applicable for this dataset.\nFor eg: Data having 10 columns should have 10! rows but it has only 1000 rows\n\nTherefore,when we depict this graphically there would be lot of white spaces as the datapoints for those regions may not be covered in the dataset.\n\nIf a  linear regression model is tested over such a data, the model will tend to overfit this data by having sharp peaks & slopes. Such a model would have 100% training accuracy but would definitely fail in the test environment.\n\nThus arose the need of introducing slight errors in the form of giving smooth bends instead of sharp peaks (thereby reducing overfit).This is achieved by tweaking the model parameters (coefficients) and the hyperparameters (penalty factor). ","16098f6a":"If you wish to further compute polynomial features, you can use the below code.","72fcb05c":"# 3. Score Comparison","74cde298":"## Polynomial Features","28789434":"# 5. Inference","142baed8":"Here we notice many coefficients are turned to 0 indicating drop of those dimensions from the model","96691609":"**Both Ridge & Lasso regularization performs very well on this data, though Ridge gives a better score. The above scatter plot depicts the correlation between the actual and predicted mpg values.**\n\n***This kernel is a work in progress.***","8d9eb5f3":"**So there is an avg. mpg difference of 0.37 from real mpg**","59fcac76":"Here we would like to scale the data as the columns are varied which would result in 1 column dominating the others.\n\nFirst we divide the data into independent (X) and dependent data (y) then we scale it. \n\n#### Tip!: ####\n\n*The reason we don't scale the entire data before and then divide it into train(X) & test(y) is because once you scale the data, the type(data_s) would be numpy.ndarray. It's impossible to divide this data when it's an array. \n*\n\nHence we divide type(data) pandas.DataFrame, then proceed to scaling it.","1b4289f7":"# 1. Import packages and observe dataset","d435927d":"* r^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no influence on the predicted variable. Instead we use adjusted r^2 which removes the statistical chance that improves r^2 \n\n(adjusted r^2 = r^2 - fluke)\n* Scikit does not provide a facility for adjusted r^2... so we use statsmodel, a library that gives results similar to what you obtain in R language\n* This library expects the X and Y to be given in one single dataframe","289bde08":"## Agenda\n\n* Perform basic EDA\n* Scale data and apply Linear, Ridge & Lasso Regression with Regularization \n* Compare the r^2 score to determine which of the above regression methods gives the highest score\n* Compute Root mean squared error (RMSE) which inturn gives a better score than r^2\n* Finally use a scatter plot to graphically depict the correlation between actual and predicted mpg values"}}