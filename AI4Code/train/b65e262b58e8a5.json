{"cell_type":{"0bf7dde4":"code","6e265c5e":"code","b4e358b2":"code","105e7763":"code","8a3d6bbe":"code","3136caa7":"code","b4d6d436":"code","16323f4c":"code","7a8b1fc9":"code","5d3e3750":"code","0f9177d3":"code","1984d5c8":"code","ed1b223b":"code","bd16f023":"code","2bf0e7f9":"code","6cc2913c":"code","9ea3d24e":"code","4d315f12":"code","79836f9d":"code","9566e59a":"code","842e3abe":"code","c1914615":"code","5977c56e":"code","1c8b2d55":"code","bcd18602":"code","43200279":"code","b40e7fa6":"code","85151749":"markdown","5a6816cd":"markdown","74f38dec":"markdown","95656553":"markdown","0b872609":"markdown","4eace1c1":"markdown","55eadc3a":"markdown","60f11840":"markdown","62ff3dcc":"markdown","7a30bd30":"markdown","33527a12":"markdown","f4fea7e5":"markdown","f840a911":"markdown","29ef2553":"markdown","b27b48fe":"markdown","cebfd0c3":"markdown"},"source":{"0bf7dde4":"# Global Variables \nK = 2 # number of components\nquery = 'nice good price'","6e265c5e":"import pandas as pd\nimport numpy as np\n\n# Data filename\ndataset_filename = \"..\/input\/Womens Clothing E-Commerce Reviews.csv\"\n\n# Loading dataset\ndata = pd.read_csv(dataset_filename, index_col=0)\n\n\n# We are reducing the size of our dataset to decrease the running time of code\nlist_of_clothing_id = data['Clothing ID'].value_counts()[:10].index\ny1 = [x == 0 for x in data['Recommended IND']]\ny2 = [x in list_of_clothing_id for x in data['Clothing ID']]\ny3 = [a or b for a,b in zip(y1,y2)]\n\ndatax = data.loc[y3, :]\n\n\n# Delete missing observations for variables that we will be working with\nfor x in [\"Recommended IND\",\"Review Text\"]:\n    datax = datax[datax[x].notnull()]\n\n# Keeping only those features that we will explore\ndatax = datax[[\"Recommended IND\",\"Review Text\"]]\n\n# Resetting the index\ndatax.index = pd.Series(list(range(datax.shape[0])))\n    \nprint('Shape : ',datax.shape)\ndatax.head()","b4e358b2":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nwordnet_lemmatizer = WordNetLemmatizer()\ntokenizer = RegexpTokenizer(r'[a-z]+')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(document):\n    document = document.lower() # Convert to lowercase\n    words = tokenizer.tokenize(document) # Tokenize\n    words = [w for w in words if not w in stop_words] # Removing stopwords\n    # Lemmatizing\n    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n    return \" \".join(words)","105e7763":"datax['Processed Review'] = datax['Review Text'].apply(preprocess)\n\ndatax.head()","8a3d6bbe":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nTF_IDF_matrix = vectorizer.fit_transform(datax['Processed Review'])\nTF_IDF_matrix = TF_IDF_matrix.T\n\nprint('Vocabulary Size : ', len(vectorizer.get_feature_names()))\nprint('Shape of Matrix : ', TF_IDF_matrix.shape)","3136caa7":"# import numpy as np\n\n# # Applying SVD\n# U, s, VT = np.linalg.svd(TF_IDF_matrix.toarray()) # .T is used to take transpose and .toarray() is used to convert sparse matrix to normal matrix\n\n# TF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# # Getting document and term representation\n# terms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\n# docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix ","b4d6d436":"import numpy as np\nfrom scipy.sparse.linalg import svds\n\n# Applying SVD\nU, s, VT = svds(TF_IDF_matrix) # .T is used to take transpose and .toarray() is used to convert sparse matrix to normal matrix\n\nTF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# Getting document and term representation\nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\ndocs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix ","16323f4c":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=datax['Recommended IND'])\nplt.title(\"Document Representation\")\nplt.show()","7a8b1fc9":"plt.scatter(terms_rep[:,0], terms_rep[:,1])\nplt.title(\"Term Representation\")\nplt.show()","5d3e3750":"# This is a function to generate query_rep\n\ndef lsa_query_rep(query):\n    query_rep = [vectorizer.vocabulary_[x] for x in preprocess(query).split()]\n    query_rep = np.mean(terms_rep[query_rep],axis=0)\n    return query_rep","0f9177d3":"from scipy.spatial.distance import cosine\n\nquery_rep = lsa_query_rep(query)\n\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nprint_count = 0\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index],' Review : ', datax['Review Text'][sort_index])\n    if print_count == 4 :\n        break\n    else:\n        print_count += 1","1984d5c8":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\n\ndef create_logistic_model(X,y):\n    \n    # Splitting data for training and validation\n    x_train, x_test, y_train, y_test = train_test_split(pd.DataFrame(X),y,test_size=0.1, random_state=1)\n    \n    # Getting the input dimension\n    input_dim = X.shape[1]\n    \n    # this is our input placeholder\n    input_doc = Input(shape=(input_dim,))\n    # This is dense layer\n    dense_layer = Dense(1, activation='sigmoid')(input_doc)\n    # Our final model\n    model = Model(input_doc, dense_layer)\n    \n    # Compiling model\n    model.compile(optimizer='sgd', loss='binary_crossentropy',metrics=['accuracy'])\n    \n    \n    # Training model\n    history = model.fit(x_train, y_train,\n                epochs=5,\n                batch_size=100,\n                shuffle=True,\n                validation_data=(x_test, y_test),\n                verbose=0)\n    # Printing Accuracy\n    print('Accuracy on Training Data : ', history.history['acc'][-1])\n    print('Accuracy on Validation Data : ', history.history['val_acc'][-1])\n    \n    # Returning model\n    return model, history","ed1b223b":"model_using_lsa, history = create_logistic_model(docs_rep, datax['Recommended IND'])","bd16f023":"datax['Recommended IND'].value_counts() \/ datax['Recommended IND'].shape[0]","2bf0e7f9":"print(np.sum(model_using_lsa.predict(docs_rep) > .5))\nprint(docs_rep.shape[0])","6cc2913c":"from sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\nencoding_dim = 2 # Size of encoding\ninput_dim = TF_IDF_matrix.shape[0] # Size of docs\n\n# Splitting Data for training and validation\ndf = pd.DataFrame(TF_IDF_matrix.T.toarray())\nx_train, x_val, y_train, y_val = train_test_split(df, df[0], test_size=0.1, random_state=1)\n\n# Encoder and Decoder\n# this is our input placeholder\ninput_docs = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='tanh')(input_docs)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='relu')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_docs, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_docs, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\nhistory = autoencoder.fit(x_train, x_train,\n                epochs=100,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=(x_val, x_val))\n\n# encode and decode some data points\nprint('Original Data : ', x_val[:5])\nencoded_datapoints = encoder.predict(x_val[:5])\nprint('Encodings : ', encoded_datapoints)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\nprint('Reconstructed Data : ', decoded_datapoints)","9ea3d24e":"docs_rep_autoencoder = encoder.predict(TF_IDF_matrix.T)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep_autoencoder[:,0], docs_rep_autoencoder[:,1], c=datax['Recommended IND'])\nplt.title(\"Document Representation\")\nplt.show()","4d315f12":"# This is a function to generate query_rep\ndef autoencoder_query_rep(query):\n    query_rep = vectorizer.transform([query])\n    query_rep = encoder.predict(query_rep)\n    return query_rep","79836f9d":"from scipy.spatial.distance import cosine\n\nquery_rep = autoencoder_query_rep(query)\n\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nprint_count = 0\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index],' Review : ', datax['Review Text'][sort_index])\n    if print_count == 4 :\n        break\n    else:\n        print_count += 1","9566e59a":"model_using_autoencoder, history = create_logistic_model(docs_rep_autoencoder, datax['Recommended IND'])","842e3abe":"datax['Recommended IND'].value_counts() \/ datax['Recommended IND'].shape[0]","c1914615":"print(np.sum(model_using_lsa.predict(docs_rep) > .5))\nprint(docs_rep.shape[0])","5977c56e":"from sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\ninput_dim = TF_IDF_matrix.shape[0] # Size of docs\n\n# Splitting Data for training and validation\ndf = pd.DataFrame(TF_IDF_matrix.T.toarray())\nx_train, x_val, y_train, y_val = train_test_split(df, datax['Recommended IND'], test_size=0.1, random_state=1)\n\n# this is our input placeholder\ninput_docs = Input(shape=(input_dim,))\nlayer1 = Dense(100, activation='relu')(input_docs)\nlayer2 = Dense(10, activation='relu')(layer1)\nlayer3 = Dense(2, activation='relu')(layer2)\nlayer4 = Dense(1, activation='sigmoid')(layer3)\n\n# Get encoding\nencoder = Model(input_docs, layer3)\n\n# Final Model\nmodel = Model(input_docs, layer4)\n\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                epochs=125,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=(x_val, y_val))\n\n# Printing Accuracy\nprint('Accuracy on Training Data : ', history.history['acc'][-1])\nprint('Accuracy on Validation Data : ', history.history['val_acc'][-1])","1c8b2d55":"import matplotlib.pyplot as plt\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","bcd18602":"docs_rep_nn = encoder.predict(TF_IDF_matrix.T)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep_nn[:,0], docs_rep_nn[:,1], c=datax['Recommended IND'])\nplt.show()\n\nplt.savefig('doc_rep_plot_nn.png')","43200279":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=datax['Recommended IND'])\nplt.show()\n\nplt.savefig('doc_rep_plot_lsa.png')","b40e7fa6":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep_autoencoder[:,0], docs_rep_autoencoder[:,1], c=datax['Recommended IND'])\nplt.show()\n\nplt.savefig('doc_rep_plot_ae.png')","85151749":"## 6 Information Retreival Using Autoencoder <a id=\"6\"><\/a>","5a6816cd":"## 4. Create Model to Predict Recommendation <a id=\"4\"><\/a>","74f38dec":"### 1.3 Creating TF-IDF Matrix <a id=\"1.3\"><\/a>","95656553":"### 1.1 Importing Data and Separating Data of Our Interest <a id=\"1.1\"><\/a>","0b872609":"## 2. Apply SVD to TF-IDF Matrix <a id=\"apply_svd\"><\/a>","4eace1c1":"### 1.2 Creating Preprocessing Function and Applying it on Our Data <a id=\"1.2\"><\/a>","55eadc3a":"## 5. IR using Autoencoder with TF-IDF matrix <a id=\"5\"><\/a>","60f11840":"# Displaying all plot of encodings and saving these for report","62ff3dcc":"### 2.1 Create Term and Document Representation  <a id=\"2.1\"><\/a>","7a30bd30":"##  1. Data Preprocessing <a id=\"data_preprocessing\"><\/a>","33527a12":"## 3 Information Retreival Using LSA <a id=\"ir_lsa\"><\/a>","f4fea7e5":"### 2.2 Visulize Those Representation <a id=\"2.2\"><\/a>","f840a911":"## Table of Content <a id=\"toc\"><\/a>\n* [Global Variables](#gv)\n* [1. Data Preprocessing](#data_preprocessing)\n    * [1.1 Importing Data and Separating Data of Our Interest](#1.1)\n    * [1.2 Creating Preprocessing Function and Applying it on Our Data](#1.2)\n    * [1.3 Creating TF-IDF Matrix](#1.3)\n* [2. Apply SVD to TF-IDF Matrix](#apply_svd)\n    * [2.1 Create Term and Document Representation](#2.1)\n    * [2.2 Visulize Those Representation](#2.2)\n* [3 Information Retreival Using LSA](#ir_lsa)\n* [4. Create Model to Predict Recommendation](#4)\n* [5. Train Autoencoder on TF-IDF Matrix](#5)\n* [6. Information Retrieval Using Autoencoder](#6)\n* [7. Predict Recommendation using Encoding of Autoencoder](#7)\n* [8. Use simple NN to predict Recommendation](#8)","29ef2553":"# Predicting Recommendation from Review Text : LSA vs Autoencoder vs NN","b27b48fe":"## 8. Use simple NN to predict Recommendation <a id=\"8\"><\/a>","cebfd0c3":"## 7. Predict Recommendation using Encoding of Autoencoder <a id=\"7\"><\/a>"}}