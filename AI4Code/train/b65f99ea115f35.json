{"cell_type":{"c5940cce":"code","c0fa2b5f":"code","4a6bb0ad":"code","9374461f":"code","c9d95afd":"code","51875b6d":"code","9878ca6b":"code","4f327365":"code","2e66b37c":"code","cb6439cf":"code","db29e6b5":"code","be63bb52":"markdown","4fd56f55":"markdown","342f611b":"markdown","e594fc89":"markdown","32b6498c":"markdown","8f385c60":"markdown"},"source":{"c5940cce":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nfrom IPython.display import clear_output\npd.set_option('display.max_colwidth', None)","c0fa2b5f":"init_urls_df = pd.DataFrame() # Initializing DataFrame to store the scraped URLs\n\n#last_page = 91 # Original number of pages\nlast_page = 1 # Limiting the number of pages to 1. Change it to 91 to scrape the whole website\n\nfor i in range(1,last_page+1): # Loop to iterating through the pages\n    \n    #'requests' module has a method 'get' which fetches the data from a webpage\n    r = requests.get(\"https:\/\/www.cars-data.com\/en\/all-cars\/page\" + str(i) + \".html\")\n    \n    # BeautifulSoup enables to find the elements\/tags in a webpage\n    bs = BeautifulSoup(r.text) # passing the source code of the webpage to 'BeautifulSoup'\n    \n    # Selecting all the 'a' tags (URLs) present in the webpage and extracting their 'href' attribute\n    init_urls = pd.Series([a.get(\"href\") for a in bs.find_all(\"a\")])\n    \n    ''' Among all the URLs we could find a pattern in the URLs of the cars.\n        All the URLs of cars have 5 forward slashes (\/) and end with a digit. Hence we select URLs containing 5 '\/'\n        and ending with a digit\n    '''\n    init_urls = init_urls[(init_urls.str.count(\"\/\")==5) & (init_urls.str.contains(\"\\d$\")==True)].unique()\n    \n    # Adding the URL we need to a DataFrame 'df'\n    df = pd.DataFrame({\"initial_urls\":init_urls})\n    \n    # Appending 'df' to a main DataFrame 'init_urls_df'\n    init_urls_df = init_urls_df.append(df).copy()\n    \n    # Printing the status\n    print(\"Processed \" + str(i) + \"\/\" + str(last_page) + \" URLs\")\n    \n    '''\n    Pausing the process by 5 to 10 seconds to ensure the website isn't overloaded with requests.\n    '''\n    time.sleep(np.random.randint(5,10,1))\n    \n    # clearing the output printed\n    clear_output(wait=True)\n    \n    # Writing the scraped URLs to a csv file with a tab separator\n    init_urls_df = init_urls_df.reset_index(drop=True)\n    init_urls_df.to_csv(\"cars-data-init-urls.csv\",sep=\"\\t\",index=False)","4a6bb0ad":"init_urls_df","9374461f":"# Reading the URLs scraped in Phase 1\ninit_urls = pd.read_csv(\"cars-data-init-urls.csv\",sep=\"\\t\")[\"initial_urls\"].unique()\n\n# Limiting to only first 10 URLs. Comment\/remove the statement below to scrape all the URLs\ninit_urls = init_urls[:10]\n\n# Initializing DataFrame to store URLs scraped in this phase\nstage2_url_df = pd.DataFrame()\n\n\nfor i in range(len(init_urls)): # Iterating through each URL scraped in Phase 1\n    r = requests.get(init_urls[i])\n    bs = BeautifulSoup(r.text)\n    \n    # Selecting all the URLs in the page\n    stage2_url = pd.Series([a.get(\"href\") for a in bs.find_all(\"a\")])\n    \n    '''\n    Among all the URLs, the URLs having the car details have a '-specs\/' string in them which \n    distinguishes them from others.\n    '''\n    stage2_url = stage2_url[stage2_url.str.contains(\"-specs\/\",regex=False)]\n    \n    # Adding the scraped URLs to a DataFrame as we did in Phase 1\n    df = pd.DataFrame({\"stage2_urls\":stage2_url})\n    \n    stage2_url_df = stage2_url_df.append(df).copy()\n    print(\"Processed \"+str(i+1) + \"\/\"+str(len(init_urls))+\" URLs\")\n    clear_output(wait=True)\n    time.sleep(np.random.randint(5,10,1))\n    \n    # Writing the scraped URLs to a csv file with a tab separator\n    stage2_url_df = stage2_url_df.reset_index(drop=True)\n    stage2_url_df.to_csv(\"cars-data-stage-2-urls.csv\",sep=\"\\t\",index=False)","c9d95afd":"stage2_url_df","51875b6d":"# Function to scrape the cars data\ndef final_scrape(url,key):\n    r = requests.get(url)\n    bs = BeautifulSoup(r.text)\n    \n    '''Getting the full name of a car from the breadcrumb on the top of the page.\n    example: https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/tech\n    The last part of the breadcrumb is the full name of the car, which contains \"-specs\/\" on it's \"href\"\n    '''\n    namu = pd.Series([a.get(\"href\") for a in bs.find_all(\"a\")])\n    namu = namu.str.contains(\"-specs\/\",regex=False)\n    nam = pd.Series([a.text for a in bs.find_all(\"a\")])\n    nam = nam[namu]\n    nam = nam.str.replace(\"\\n\",\"\") # removing \\n (if any) in the car' name\n    nam = nam.reset_index(drop=True)[0]\n\n    # 'dt' tags represent the property of the car\n    dt = pd.Series([a.text for a in bs.find_all(\"dt\")])\n    \n    # removing the : at the end and also \\n (if any)\n    dt = dt.str.replace(\"\\n|(:$)\",\"\")\n    \n    # 'dd' tags represent the values of the car\n    dd = pd.Series([a.text for a in bs.find_all(\"dd\")])\n\n    dt = dt.reset_index(drop=True)\n    dd = dd.reset_index(drop=True)\n\n    # Creating DataFrame with full car name, scraped 'dt' and 'dd' tags, key, and source URL\n    # 'key' odentifies the page (tech \/ sizes \/ options) from which data is scraped\n    df = pd.DataFrame({\"Car Name_Full\":nam,\"Spec\":dt,\"Info\":dd,\"Page\":key,\"Source URL\":url})\n    \n    # Replacing N.A. and '-' with NaN\n    df[\"Info\"] = df[\"Info\"].replace(\"N.A.\",np.nan,regex=False)\n    df[\"Info\"] = df[\"Info\"].replace(\"-\",np.nan,regex=False)\n    time.sleep(np.random.randint(5,10,1))\n    return df","9878ca6b":"# Reading URLs scraped in Phase 2\nstage2_urls = pd.read_csv(\"cars-data-stage-2-urls.csv\",sep=\"\\t\")[\"stage2_urls\"].unique()\n\n\n# Initializing DataFrame to store final data\nfinal_df = pd.DataFrame()\n\nfcnt=0 # File counter which is explained later\nfor i in range(len(stage2_urls)):\n    \n    \n    # Scraping the \/tech page (example: https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/tech)\n    final_df = final_df.append(final_scrape(stage2_urls[i] + \"\/tech\",\"tech\"))\n    print(\"Step \"+str(i+1)+str(\".1\"))\n    \n    # Scraping the \/sizes page (example: https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/sizes)\n    final_df = final_df.append(final_scrape(stage2_urls[i] + \"\/sizes\",\"sizes\"))\n    print(\"Step \"+str(i+1)+str(\".2\"))\n    \n    # Scraping the \/options page (example: https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/options)\n    final_df = final_df.append(final_scrape(stage2_urls[i] + \"\/options\",\"options\")).copy()\n    print(\"Step \"+str(i+1)+str(\".3\"))\n    \n    # Printing the progress\n    print(\"Processed \"+str(i+1)+\"\/\"+str(len(stage2_urls))+\" URL\")\n    \n    clear_output(wait=True)\n    \n    # The scraped data is written to an Excel file after every 100 iterations to prevent data loss\n    if i%100==0:\n        final_df.to_excel(\"cars-data-final-data\"+str(fcnt)+\".xlsx\",index=False)\n        \n    ''' Since the data is being written to a file after every 100 iterations, \n    it is better to reinitialize the 'final_df' after every 10,000 iterations which removes the data already written to Excel\n    and reduces the time taken to write the dataframe to Excel. \n    '''\n    if (i%10000==0) & (i>0):\n        final_df=pd.DataFrame()\n        fcnt = fcnt+1\n        \nfinal_df.to_excel(\"cars-data-final-data\"+str(fcnt+1)+\".xlsx\",index=False)","4f327365":"# Combining all the data written to different files\nfiles = pd.Series(os.listdir(os.getcwd()))\n\n# Selecting files which contain the phrase 'cars-data-final-data'\nfiles = files[files.str.contains(\"cars-data-final-data\")].reset_index(drop=True)\nfiles","2e66b37c":"# Combining\/appending all the files and storing them in FINAL_DF dataframe\nFINAL_DF = pd.DataFrame()\nfor i,j in enumerate(files):\n    FINAL_DF = FINAL_DF.append(pd.read_excel(j)).copy()\n    print(\"Added \"+str(i+1)+\"\/\"+str(len(files))+\" Files\")","cb6439cf":"FINAL_DF.drop_duplicates(inplace=True)\nFINAL_DF = FINAL_DF.reset_index(drop=True)\nFINAL_DF.to_excel(\"Cars_data_Final.xlsx\",index=False)","db29e6b5":"FINAL_DF","be63bb52":"<h2>Phase 2<\/h2>\nEach URL scraped in the phase 1 has multiple variants of the car listed as separate URLS,  like \n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-2019\/4138'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-2019\/4138<\/a> scraped in the phase 1 has 6 car variants. All of these are scraped and stored in another file for further processing.","4fd56f55":"It is always better to store the data in the below melted fprmat, as it makes it easy to reshape the data ","342f611b":"<h2>Phase 1<\/h2>\n<h3>Originally the website has 91 pages to scrape in phase 1. Scraping data of ~100,000 cars from all these URLs may take some days to complete (as random pauses are used to prevent from being blacklisted by the website).\n<br>Hence, this process limits to scraping the first page and few cars only.<\/h3>\n\n\nScraping the URLs of all cars from all the pages like<br>\n<a href='https:\/\/www.cars-data.com\/en\/all-cars\/page1.html'>https:\/\/www.cars-data.com\/en\/all-cars\/page1.html<\/a>\n<br>\n<a href='https:\/\/www.cars-data.com\/en\/all-cars\/page2.html'>https:\/\/www.cars-data.com\/en\/all-cars\/page2.html<\/a>","e594fc89":"<h1>Web scraping using BeautifulSoup<\/h1>\n<p>The following code scrapes ~200 data points of ~100,000 cars from <a href=\"https:\/\/www.cars-data.com\/en\/all-cars.html\">cars-data.com<\/a>","32b6498c":"<h2>Phase 3<\/h2>\nThe car URLs scraped in phase 2 have the complete details of a car, hence the URL scrape process ends in phase 2.<br>\nPhase 3 scrapes the data of each car from the URLs scraped in phase 2. The URL of a car (example: <a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385<\/a>) doesn't display the entire data on opening\/visiting it, instead it shows a \"view all specs\" URL.<br>On thorough inspection of the page, we find that \"view all specs\" uses 3 unique URLs:<br>\n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/tech'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/tech<\/a><br>\n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/sizes'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/sizes<\/a><br>\n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/options'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/options<\/a><br>Hence, scraping data from the 3 URLs fetches the complete data of a car.","8f385c60":"The process is divided into 3 phases\n<ul>\n<li><u>Phase 1:<\/u> Scraping the URLs of all cars from all the pages like<br>\n<a href='https:\/\/www.cars-data.com\/en\/all-cars\/page1.html'>https:\/\/www.cars-data.com\/en\/all-cars\/page1.html<\/a>\n<br>\n<a href='https:\/\/www.cars-data.com\/en\/all-cars\/page2.html'>https:\/\/www.cars-data.com\/en\/all-cars\/page2.html<\/a>\n<\/li><br>\n<li><u>Phase 2:<\/u> Each URL scraped in the phase 1 has multiple variants of the car listed as separate URLS,  like \n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-2019\/4138'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-2019\/4138<\/a> scraped in the phase 1 has 6 car variants. All of these are scraped and stored in another file for further processing.<\/li><br>\n<li><u>Phase 3:<\/u> The car URLs scraped in phase 2 have the complete details of a car, hence the URL scrape process ends in phase 2.<br>\nPhase 3 scrapes the data of each car from the URLs scraped in phase 2. The URL of a car (example: <a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385<\/a>) doesn't display the entire data on opening\/visiting it, instead it shows a \"view all specs\" URL.<br>On thorough inspection of the page, we find that \"view all specs\" uses 3 unique URLs:<br>\n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/tech'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/tech<\/a><br>\n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/sizes'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/sizes<\/a><br>\n<a href='https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/options'>https:\/\/www.cars-data.com\/en\/audi-tt-coupe-40-tfsi-specs\/80385\/options<\/a><br>Hence, scraping data from the 3 URLs fetches the complete data of a car.<\/li><\/ul>\n    \n<h2>Let's get started...<\/h2>"}}