{"cell_type":{"bf4b1525":"code","6f45e506":"code","cd900800":"code","324fd8ea":"code","356c35da":"code","d004ca99":"code","7cf5f2ce":"code","57c93e2c":"code","ee2a1724":"code","0dffa82a":"code","d32b94d4":"code","4f61eec1":"code","29db02e8":"code","5da139ba":"code","802a5fd9":"code","9a85529b":"markdown","3a710483":"markdown","a0da0fa1":"markdown","aa89de21":"markdown","a5318e70":"markdown","ca416d97":"markdown","d024af42":"markdown","afbcf167":"markdown"},"source":{"bf4b1525":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f45e506":"# imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport os\nimport numpy as np\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n","cd900800":"# some helper functions\n\n\n# custom weights initialization called on netG \ndef custom_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    if classname.find('BlockGen') != -1:\n        if classname.find('Conv') != -1:\n            m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\n\n# compute the current classification accuracy\ndef compute_acc(preds, labels):\n    correct = 0\n    preds_ = preds.data.max(1)[1]\n    correct = preds_.eq(labels.data).cpu().sum()\n    acc = float(correct) \/ float(len(labels.data)) * 100.0\n    return acc\n\n\ndef show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n\n\ndef show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break\n\n\ndef denorm(img_tensors, mean, std):\n    \"Denormalize image\"\n    return img_tensors * std[0] + mean[0]\n\n\n\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    \"\"\" 3 things:\n    1. Connected to Nvidia GPU\n    2. Cuda drivers\n    3. Pytorch suitable to GPU version\n    then torch.cuda.is_available is True\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\n\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n","324fd8ea":"# block for code readable\n\n\nclass BlockGen(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, kernel_size):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                               padding=0 if stride == 1 else 1,  # stride=1, padding=0 or stride=2, padding=1\n                               bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n       \n\n    def forward(self, x):\n        x = self.conv(x)\n        # print(f\"COnvBlockGen: {x.shape}\")\n        return x\n\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=110, features=[384, 192, 96, 48, 3]):\n        super(Generator, self).__init__()\n        self.in_channels = in_channels\n        # first linear layer\n        self.linear = nn.Linear(self.in_channels, 384)  # 110 is a length latent input vector\n\n        layers = []\n        in_feature = features[0]\n        for feature in features[1:-1]:\n            # print(f\"in_feature:{in_feature}, output_feature:{feature}\")\n            layers.append(BlockGen(\n                in_feature, feature, kernel_size=4, stride=1 if in_feature == features[0] else 2,\n            ))\n            in_feature = feature\n        self.model = nn.Sequential(*layers)\n\n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(in_feature, features[-1], kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n        )\n\n\n    def forward(self, x):\n        x = x.view(-1, self.in_channels)\n        x = self.linear(x)\n        # print(x.shape)\n        # here error\n        x = x.view(-1, 384, 1, 1)  # reshape to required shape\n        # print(x.shape)\n        x = self.model(x)\n        output = self.last(x)\n        return output\n","356c35da":"\n\nimage_size = (32, 32)  # cifar 10\nnum_classes = 10\n\n\nclass BlockDis(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.5)\n        )\n\n        \n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[16, 32, 64, 128, 256, 512]):\n        super(Discriminator, self).__init__()\n\n        self.initial = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                features[0],\n                kernel_size=(3, 3),\n                stride=2,\n                padding=1,\n                bias=False\n            ),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.5)\n        )\n\n        layers = []\n        in_channels = features[0]\n        for i, feature in enumerate(features[1:]):\n            layers.append(\n                BlockDis(in_channels, feature, stride=1 if i % 2 == 0 else 2))  # even layer, stride=1, else 2\n            in_channels = feature\n        self.model = nn.Sequential(*layers)\n\n        # The height and width of downsampled image\n        # downsampled in conv_layer 1, 3, 5 with stride=2\n        ds_size = image_size[0] \/\/ 2 ** 3\n        # Output layer: discriminator fc\n        self.fc_dis = nn.Sequential(nn.Linear(512 * ds_size**2, 1), nn.Sigmoid())\n        # Ouput Layer: aux-classifier fc\n        self.fc_aux = nn.Sequential(nn.Linear(512 * ds_size**2, num_classes), nn.Softmax())\n\n    def forward(self, x):\n        x = self.initial(x)\n        x = self.model(x)\n        x = x.view(x.shape[0], -1)  # flatten\n        validity = self.fc_dis(x)\n        label = self.fc_aux(x)\n\n        return validity, label\n","d004ca99":"imgSize = 32\nlr = 0.0002\nepochs = 110\nbatch_size=100\nnum_classes = 10 # cifar10\n# narmalization constraints\nmean, std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\ndata_dir = \"data\"\nsave_dir = \"generated_images\"\n\n#  some GAN configuration\nnoise_dim = 110\n","7cf5f2ce":"# load cifar10 dataset\ndataset = CIFAR10(\n    root=data_dir, download=True,\n    transform=transforms.Compose([\n        transforms.Scale((32, 32)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std) # (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n    ])\n)\n","57c93e2c":"\ndataloader = DataLoader(dataset, batch_size=batch_size)\ndevice = get_default_device()  # check gpu if available else cpu","ee2a1724":"print(device)","0dffa82a":"# instantiate generator\nnetG = Generator(noise_dim).to(device)  # hidden latent vector length\nnetG.apply(custom_init)  # apply custom intitialization to generator\nnetG = to_device(netG, device)\nprint(netG)","d32b94d4":"# instantiate discriminator\nnetD = Discriminator(in_channels=3)\nnetD = to_device(netD, device)\nprint(netD)","4f61eec1":"\n# defining Optimizer\noptimD = optim.Adam(netD.parameters(), lr)\noptimG = optim.Adam(netG.parameters(), lr)\n\n# defining Loss\ndisc_criterion = nn.BCELoss()\naux_criterion = nn.NLLLoss()","29db02e8":"# noise for evaluation\neval_noise = torch.FloatTensor(batch_size, noise_dim, 1, 1).normal_(0, 1)\neval_noise_ = np.random.normal(0, 1, (batch_size, noise_dim))\neval_label = np.random.randint(0, num_classes, batch_size)\neval_onehot = np.zeros((batch_size, num_classes))\neval_onehot[np.arange(batch_size), eval_label] = 1\neval_noise_[np.arange(batch_size), :num_classes] = eval_onehot[np.arange(batch_size)]\neval_noise_ = (torch.from_numpy(eval_noise_))\neval_noise.data.copy_(eval_noise_.view(batch_size, noise_dim, 1, 1))\neval_noise = eval_noise.to(device)\n\n\n\ndef save_samples(index, latent_tensors):\n  fake_images = netG(latent_tensors)\n  fake_fname = 'generated=images-{0:0=4d}.png'.format(index)\n  vutils.save_image(denorm(fake_images, mean, std), os.path.join(save_dir, fake_fname), nrow=8)\n  print(\"Saving\", fake_fname)\n\n# create directory to save images\nos.makedirs(save_dir, exist_ok=True)","5da139ba":"# Training\nG_losses = []\nD_losses = []\nfor epoch in range(epochs):\n    with tqdm(dataloader, unit=\"batch\") as tepoch:\n        for i, data in enumerate(tepoch):\n            tepoch.set_description(f\"Epoch--[ {epoch}\/{epochs}]\")\n            image, label = to_device(data[0], device), to_device(data[1], device)\n\n            # First train discriminator\n            ############################\n            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n            ###########################\n            # zero gradient of optimizer in every epoch\n            optimD.zero_grad()\n            # feed the batch of real image into the discriminator\n            disc_output, aux_output = netD(image)\n            disc_error_real = disc_criterion(disc_output, torch.ones_like(disc_output))\n            aux_error_real = aux_criterion(aux_output, label)\n\n            total_error_real = disc_error_real + aux_error_real\n            total_error_real.backward()\n            optimD.step()\n            D_x = disc_output.data.mean()\n\n            # get the current classification accuracy\n            accuracy = compute_acc(aux_output, label)\n\n            # generating noise by random sampling\n            noise = torch.normal(0, 1, (batch_size, noise_dim), dtype=torch.float).to(device)\n            # generating label for entire batch\n            fake_label = torch.randint(0, 10, (batch_size,), dtype=torch.long).to(\n                device)  # num of classes in CIFAR10 is 10\n\n            fake_image = netG(noise)  # generator generate fake image\n\n            # passing fake image to the discriminator\n            disc_output_fake, aux_output_fake = netD(fake_image.detach())  # we will be using this tensor later on\n            disc_error_fake = disc_criterion(disc_output_fake, torch.zeros_like(disc_output_fake))  # Train discriminator that it is fake image\n            aux_error_fake = aux_criterion(aux_output_fake, fake_label)\n            total_error_fake = disc_error_fake + aux_error_fake\n            total_error_fake.backward()\n            optimD.step()\n\n            # Now we train the generator as we have finished updating weights of the discriminator\n            netG.zero_grad()\n            disc_output_fake, aux_output_fake = netD(fake_image)\n            disc_error_fake = disc_criterion(disc_output_fake, torch.ones_like(disc_output_fake))  # Fool the discriminator that it is real\n            aux_error_fake = aux_criterion(aux_output_fake, fake_label)\n            total_error_gen = disc_error_fake + aux_error_fake\n            total_error_gen.backward()\n            optimG.step()\n            \n            G_losses.append(total_error_gen.item())\n            D_losses.append(total_error_fake.item())\n            #tepoch.set_postfix(Loss_Discriminator =total_error_fake.item(), Loss_Generator=total_error_gen.item(), Accuracy=accuracy)\n            if i % 50 == 0:\n                print(\n                    \"Epoch--[{} \/ {}], Loss_Discriminator--[{}], Loss_Generator--[{}],Accuracy--[{}]\".format(epoch,\n                                                                                                             epochs,\n                                                                                                             total_error_fake,\n                                                                                                             total_error_gen,\n                                                                                                             accuracy))\n                   \n    # save generated samples at each epoch\n    save_samples(epoch, eval_noise)","802a5fd9":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","9a85529b":"## GAN VS AC-GAN\nThe main difference is in discriminator, where in AC-GAN discriminator outputs **$P_{real}$** and **$P_{class}$** with auxiliary Classifier. In contrast, GAN discriminator only outputs **$P_{real}$**.\n\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/148333723-7404d99f-6612-4920-999a-b4e3461daafb.png)\n\n\n","3a710483":"#### Let's Get Started!","a0da0fa1":"### Architecture of AC-GAN for CIFAR10\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/148333825-5c3f95ff-b508-42e2-85a8-960bb73af075.png)\n","aa89de21":"#### GENERATOR\n* First take noise vector of length, which is reshape as (batch_size, 110, 1, 1) for input to convolution layers\n* Use of Transpose Convolution layer for upsampling the noise. And after 3 upsampling layers, the noise 110 vector length converted into image.","a5318e70":"Now, Our generator part is done. It's just implementation of the architecture of AC-GAN shown in figure above. Hope you understand it.","ca416d97":"## Discriminator\n* Take 32x32x3 CIFAR 10 image. Note that in PyTorch, RGB channels is in first as: `3x32x32`\n* Pass through six convolution layers","d024af42":"#### Config","afbcf167":"# AC-GAN on CIFAR10 with PyTorch\n#### Part of [30 Days GAN Paper Reading](https:\/\/github.com\/sushant097\/30-Days-GANs-Paper-Reading)\n\nIn this notebook, we read about what is AC-GAN and implement from scratch with PyTorch.\nIf you are not familiar with GAN, then I recommend look on mine Medium post [Intuition behind GANs for begineers](https:\/\/susant.medium.com\/intuition-behind-gan-for-beginners-bb4da5c54480)"}}