{"cell_type":{"f3021813":"code","647ca8cb":"code","603d1a28":"code","2135526f":"code","442b6338":"code","65cfbf7b":"code","86681952":"code","3b79e937":"code","9472132e":"code","900f2595":"code","fe64ba81":"code","35031b2b":"code","3289aa52":"code","4a90a7e8":"code","582fa8b9":"code","ffcfc314":"code","bcd51858":"code","a946682e":"code","44317d0e":"code","740483cf":"code","c624c1d6":"code","03798436":"code","ba3cfde7":"code","0bd5b2b9":"code","817d57ab":"code","0a915291":"code","853a7f72":"code","3587f6e9":"code","d7951ab8":"code","618915b2":"code","6bf10398":"code","a5624d31":"code","6bd10390":"code","02cd8909":"code","7935c8e3":"code","21bca279":"code","674e4395":"code","4c52a93e":"code","79e0c4a3":"code","70835798":"code","3085a7cc":"code","a2c63577":"code","5b49ad08":"code","262718e5":"code","7d1b8574":"code","ed5efecf":"code","1cef4e2f":"code","eee86aaa":"code","b94e645b":"code","b3d8deee":"code","83b01128":"code","e3d25027":"code","d579a25e":"code","41792c64":"code","ecae584d":"code","de7b0d71":"code","5bc6c848":"code","6ac75476":"code","b516d0d9":"code","423e3311":"code","3298d870":"code","3c4179da":"code","04425c90":"code","2df0f61e":"code","af460d54":"code","e2355dfe":"code","bbf4eba6":"code","34f146d7":"code","1eb4b8e9":"code","29352d34":"code","1db71350":"code","f38436c1":"code","895e3596":"code","6d14b673":"code","d7422d30":"code","e218db47":"code","8a2bf1a8":"code","1f38cc5d":"code","dcbda5a3":"code","63bec48e":"code","425f9085":"code","532b1557":"code","38a0cd4e":"code","c864380e":"markdown","674296bd":"markdown","6c0f4a32":"markdown","1865f151":"markdown","4889db34":"markdown","2a08ed32":"markdown","a4881406":"markdown","18781862":"markdown","68540bfb":"markdown","32c76c40":"markdown","dfd756c3":"markdown","230de9f0":"markdown","86158027":"markdown","15b6aef8":"markdown","c03be512":"markdown","2a943ee4":"markdown","bcd21485":"markdown","58e2408d":"markdown","dc590d6f":"markdown","890f1592":"markdown","53f5f6da":"markdown","4c988a10":"markdown","d20d9f0d":"markdown","bf39b90a":"markdown","bc4a2e07":"markdown","12c0d6e7":"markdown","1dc4c719":"markdown","7b5efeed":"markdown","512a2ba6":"markdown","75536fb4":"markdown","70a9301b":"markdown","46c59e8f":"markdown","b8e205bc":"markdown","8d60146a":"markdown","391d380d":"markdown","bb5432f5":"markdown","c69274cd":"markdown","50b22742":"markdown","f4fb122d":"markdown","3abbd3fe":"markdown","db4442c5":"markdown","03e37de2":"markdown","2522e939":"markdown","baf97ab9":"markdown","cc64f4d3":"markdown","1173e7db":"markdown","4bd8f38f":"markdown","40005e63":"markdown","81bfc753":"markdown","a8150298":"markdown","38215634":"markdown","8cfc78f8":"markdown","24071da3":"markdown","de54742b":"markdown","4ff29526":"markdown","1efed29c":"markdown","e6b73728":"markdown","c3e25cea":"markdown","563b5ba4":"markdown","0b662291":"markdown","e420c5a2":"markdown","42987b22":"markdown","ff1c4f25":"markdown","a54492b7":"markdown","3b41eb36":"markdown","c9a923a4":"markdown","b6944aab":"markdown","27e478eb":"markdown","bf220bad":"markdown","ad4f0e5f":"markdown","afea8a25":"markdown","39d8ff79":"markdown","9357584f":"markdown"},"source":{"f3021813":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, BayesianRidge\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport time\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nimport pickle\nfrom catboost import CatBoostRegressor\nimport catboost as cb\nimport lightgbm\nimport os ","647ca8cb":"df = pd.read_csv(r\"..\/input\/kickstarter-projects\/ks-projects-201801.csv\")\ndf.head()\ndf.info()\ndescribe = df.describe()\n# %%\ndf.isna().any()","603d1a28":"def missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n\n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n\n    # Renaming the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns={0: 'Missing Values', 1: '% of Total Values'})\n\n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n\n    # Print some summary information\n    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n\n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns","2135526f":"missing_values_table(df)","442b6338":"df.drop(columns=[\"goal\", \"usd pledged\", \"pledged\", \"ID\"], inplace=True)","65cfbf7b":"df[df[\"name\"].isna()]\n# We can also drop these rows as 4 rows won't make any statistical change in our analysis.\n","86681952":"df.dropna(inplace=True)","3b79e937":"plt.figure(figsize=(10, 10))","9472132e":"fig = plt.figure(figsize = (14,8))\nplt.style.use(\"seaborn\")\nplt.hist(df[\"usd_pledged_real\"], bins=100, edgecolor=\"r\")\nplt.xlabel(\"USD Pledged\")\nplt.ylabel(\"Number of Campaigns\")\nplt.title(\"Kickstarter Campaigns Pledged Amount Distribution\")\nplt.show()","900f2595":"fig = plt.figure(figsize = (14,8))\nplt.style.use(\"seaborn\")\nplt.hist(df[\"usd_pledged_real\"], bins=1000, edgecolor=\"r\", range=(10000, 200000))\nplt.xlabel(\"USD Pledged\")\nplt.ylabel(\"Number of Campaigns\")\nplt.title(\"Kickstarter Campaigns Pledged Amount Distribution\")","fe64ba81":"plt.show()","35031b2b":"categories = df[\"category\"].value_counts()\ncategories = list(categories[categories.values > 10000].index)","3289aa52":"fig = plt.figure(figsize = (14,8))\nfor cat_type in categories:\n    # Select the category type\n    subset = df[df[\"category\"] == cat_type]\n\n    # Density plot of amount pledged\n    sns.kdeplot(subset[\"usd_pledged_real\"], label=cat_type, shade=False)\n    plt.title(\"Kickstarter Campaigns Pledged Amount Distribution\")\nplt.xlim(0, 200000)\nplt.ylim(0, 0.000008)\nplt.show()","4a90a7e8":"# Creating subplots for each category\nsns.set_palette(sns.cubehelix_palette(8))\nfig = plt.figure(figsize = (15,15))\nfig.add_subplot(3, 3, 1)\nplt.title(\"Product Design\")\nsns.countplot(df[df[\"category\"] == \"Product Design\"][\"state\"])\nfig.add_subplot(3, 3, 2)\nplt.title(\"Tabletop Games\")\nsns.countplot(df[df[\"category\"] == \"Tabletop Games\"][\"state\"])\nfig.add_subplot(3, 3, 3)\nplt.title(\"Shorts\")\nsns.countplot(df[df[\"category\"] == \"Shorts\"][\"state\"])\nfig.add_subplot(3, 3, 4)\nplt.title(\"Video Games\")\nsns.countplot(df[df[\"category\"] == \"Video Games\"][\"state\"])\nfig.add_subplot(3, 3, 5)\nplt.title(\"Food\")\nsns.countplot(df[df[\"category\"] == \"Food\"][\"state\"])\nfig.add_subplot(3, 3, 6)\nplt.title(\"Film & Video\")\nsns.countplot(df[df[\"category\"] == \"Film & Video\"][\"state\"])\nfig.add_subplot(3, 3, 7)\nplt.title(\"Documentary\")\nsns.countplot(df[df[\"category\"] == \"Documentary\"][\"state\"])\nfig.subplots_adjust(wspace = 0.4, hspace= 0.8, right= 0.9, left = 0.125)\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=50)\n    \nplt.show()\n","582fa8b9":"correlations_data = df.corr()[\"usd_pledged_real\"].sort_values()","ffcfc314":"print(correlations_data.head(15))","bcd51858":"df[\"deadline\"] = pd.to_datetime(df[\"deadline\"], format=\"%Y\/%m\/%d\")\ndf[\"launched\"] = pd.to_datetime(df[\"launched\"], format=\"%Y\/%m\/%d\")\ndf[\"deadline\"] = df[\"deadline\"].dt.date\ndf[\"launched\"] = df[\"launched\"].dt.date","a946682e":"df[\"campaign duration\"] = (df[\"deadline\"] - df[\"launched\"]).dt.days\ndf.drop(columns=[\"deadline\", \"launched\"], inplace=True)","44317d0e":"df[\"name\"] = df[\"name\"].apply(lambda x: len(x))","740483cf":"categorical_subset = df.drop(columns=[\"usd_pledged_real\", \"backers\", \"usd_goal_real\", \"name\", \"campaign duration\"])","c624c1d6":"categorical_subset = pd.get_dummies(categorical_subset)","03798436":"numeric_subset = df[[\"name\", \"backers\", \"campaign duration\", \"usd_goal_real\", \"usd_pledged_real\"]]","ba3cfde7":"features = pd.concat([numeric_subset, categorical_subset], axis=1)","0bd5b2b9":"correlations = features.corr()[\"usd_pledged_real\"].dropna().sort_values(ascending=False)","817d57ab":"correlations.head(15)","0a915291":"correlations.tail(15)","853a7f72":"plot_features = features.copy()","3587f6e9":"plot_features[\"category\"] = df[\"category\"]","d7951ab8":"sns.set()\nplot_features = plot_features[plot_features[\"category\"].isin(categories)]\nsns.lmplot(\"backers\", \"usd_pledged_real\", data=plot_features, hue=\"category\",\n               scatter_kws={'s': 60}, fit_reg=False,\n               height=8, aspect=1)\nplt.xlim((-500, 50000))\nplt.ylim(0, 1000000)\nplt.title(\"Backers vs Pledged USD\", size=12)\nplt.xlabel(\"Backers\", size=10)\nplt.ylabel(\"Pledged USD\", size=10)\nplt.tick_params(labelsize=10)\nplt.show()","618915b2":"pairplot_data = plot_features[[\"usd_pledged_real\", \"name\", \"backers\",\n                          \"campaign duration\", \"usd_goal_real\"]]","6bf10398":"pairplot_data[\"category\"] = df[\"category\"]\npairplot_data.reset_index(drop= True, inplace= True)","a5624d31":"pairplot_data = pairplot_data[pairplot_data[\"category\"].isin(categories)]","6bd10390":"sns.set(font_scale = 1.1)\ngrid = sns.PairGrid(data=pairplot_data, hue=\"category\")\ngrid = grid.map_offdiag(plt.scatter, linewidths=1, s=40)\ngrid = grid.map_diag(plt.hist, color= \"darkred\", edgecolor= \"black\", bins= 10)\ngrid = grid.add_legend(fontsize=14)\nplt.show()","02cd8909":"def corr_df(x, corr_val):\n    \"\"\"\n    Obj: Drops features that are strongly correlated to other features.\n          This lowers model complexity, and aids in generalizing the model.\n    Inputs:\n          df: features df (x)\n          corr_val: Columns are dropped relative to the corr_val input (e.g. 0.8)\n    Output: df that only includes uncorrelated features\n    \"\"\"\n\n    # Creates Correlation Matrix and Instantiates\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterates through Correlation Matrix Table to find correlated columns\n    for i in iters:\n        for j in range(i):\n            item = corr_matrix.iloc[j:(j + 1), (i + 1):(i + 2)]\n            col = item.columns\n            row = item.index\n            val = item.values\n            if val >= corr_val:\n                # Prints the correlated feature set and the corr val\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(i)\n    drops = sorted(set(drop_cols))[::-1]\n\n    # Drops the correlated columns\n    for i in drops:\n        col = x.iloc[:, (i + 1):(i + 2)].columns.values\n        x.drop(col, axis=1, inplace=True)\n    return x","7935c8e3":"y_features = features[\"usd_pledged_real\"]","21bca279":"features.drop(columns=[\"usd_pledged_real\"], inplace=True)","674e4395":"features = corr_df(features, 0.6)","4c52a93e":"features = pd.concat([features, y_features], axis=1)\n","79e0c4a3":"X = features.drop(columns=[\"usd_pledged_real\"])\ny = pd.DataFrame(features[\"usd_pledged_real\"])","70835798":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","3085a7cc":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a2c63577":"scaler = MinMaxScaler(feature_range=(0, 1))","5b49ad08":"scaler.fit(X_train)","262718e5":"X_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","7d1b8574":"y_train = np.array(y_train).reshape((-1,))\ny_test = np.array(y_test).reshape((-1,))","ed5efecf":"def mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))","1cef4e2f":"def fit_and_evaluate(model):\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions and evalute\n    model_pred = model.predict(X_test)\n    model_mae = mae(y_test, model_pred)\n\n    # Return the performance metric\n    return model_mae","eee86aaa":"lr = LinearRegression()\nlr_mae = fit_and_evaluate(lr)\nprint(\"Linear regression mean of error is {}.\".format(lr_mae))","b94e645b":"gradient_boosted = XGBRegressor(random_state=1, silent = True)\ngradient_boosted_mae = fit_and_evaluate(gradient_boosted)\nprint(\"Gradient boosted regression mean of error is {}.\".format(gradient_boosted_mae))","b3d8deee":"knn = KNeighborsRegressor(n_neighbors=10)\nknn_mae = fit_and_evaluate(knn)\nprint(\"KNN regression mean of error is {}.\".format(knn_mae))","83b01128":"sklearn_gradient_boosted = GradientBoostingRegressor()\nsklearn_gradient_boosted_mae = fit_and_evaluate(sklearn_gradient_boosted)\nprint(\"sklearn Gradient boosted regression mean of error is {}.\".format(sklearn_gradient_boosted_mae))","e3d25027":"sgd = SGDRegressor(random_state=1)\nsgd_mae = fit_and_evaluate(sgd)\nprint(\"Stochastic gradiant descent regression mean of error is {}.\".format(sgd_mae))","d579a25e":"br = BayesianRidge()\nbr_mae = fit_and_evaluate(br)\nprint(\"Bayesian ridge regression mean of error is {}.\".format(br_mae))","41792c64":"lgb = LGBMRegressor(objective =\"regression\", num_leaves = 35, random_state= 1)\nlgb_mae = fit_and_evaluate(lgb)\nprint(\"LightGBM regression mean of error is {}.\".format(lgb_mae))","ecae584d":"\ncatboost = CatBoostRegressor(silent=True, n_estimators=300)\ncatboost_mae = fit_and_evaluate(catboost)\nprint(\"CatBoost regression mean of error is {}.\".format(catboost_mae))","de7b0d71":"models_df = pd.DataFrame(\n    {\"model\": [\"Linear Regression\",\"CatBoost Regression\", \"XGBoost Gradient Boosted Regression\",\n               \"K-Nearest Neighbours\", \"SGD Regression\", \"Bayesian Ridge Regression\",\n               \"LightGBM Regression\"], \"mae\": [lr_mae,catboost_mae, gradient_boosted_mae,\n                                            knn_mae, sgd_mae, br_mae, lgb_mae]})","5bc6c848":"plt.figure(figsize=(13, 13))\nsns.catplot(x=\"mae\", y=\"model\", kind=\"bar\", data=models_df, height=5, aspect=3)\nplt.show()","6ac75476":"# List of algorithms we will use\n\nmodels = [XGBRegressor(silent=True), LGBMRegressor(), CatBoostRegressor(silent=True, n_estimators=300)]\nsets = []\n\n\n# Function to select features for each model.\ndef FeatureSelector(models,X_train, y_train,X_Test, y_test):\n\n\n    \n    for model in models:\n        model_iter = model.fit(X_train, y_train)\n        selection = SelectFromModel(model_iter, threshold=0.01, prefit=True)\n        train = selection.transform(X_train)\n        \n        model_iter = model.fit(X_test, y_test)\n        selection = SelectFromModel(model_iter, threshold=0.01, prefit=True)\n        test = selection.transform(X_test)\n        \n        #Appending each selected feature to sets list for every model.\n        sets.append(train)\n        sets.append(test)\n        \n\n        \nFeatureSelector(models, X_train, y_train, X_test, y_test)","b516d0d9":"# Assigning the models to variables in the sets list.\nXGB_y_train = y_train.copy()\nXGB_X_train = sets[0]\nXGB_X_test = sets[1]\n\n# Creating a validation set from our X_train and y_train sets. \n# We will use the validation set to train our level 1 model.\nXGB_X_train, XGB_X_valid, XGB_y_train, XGB_y_valid = train_test_split(XGB_X_train, XGB_y_train, test_size = 0.2, random_state=42)\n\n# Assigning the models to variables in the sets list.\nLGBM_y_train = y_train.copy()\nLGBM_X_train = sets[2]\nLGBM_X_test = sets[3]\n\n# Creating a validation set from our X_train and y_train sets. \n# We will use the validation set to train our level 1 model.\nLGBM_X_train, LGBM_X_valid, LGBM_y_train, LGBM_y_valid = train_test_split(LGBM_X_train, LGBM_y_train, test_size = 0.2, random_state=42)\n\n# Assigning the models to variables in the sets list.\ncatboost_y_train = y_train.copy()\ncatboost_X_train = sets[4]\ncatboost_X_test = sets[5]\n\n# Creating a validation set from our X_train and y_train sets. \n# We will use the validation set to train our level 1 model.\ncatboost_X_train, catboost_X_valid, catboost_y_train, catboost_y_valid = train_test_split(catboost_X_train, catboost_y_train, test_size = 0.2, random_state=42)","423e3311":"XGB_model = XGBRegressor(random_state=1, silent= True)\nmax_depth_xgb = [6, 7, 8, 9, 10]\nn_estimators_xgb = [100]\nmin_child_weight_xgb = [1, 2, 4, 6]\ngamma_xgb = [0, 0.1, 0.2, 0.3, 0.4]\n\n\nhyperparameter_grid =  {\"estimator__max_depth\": max_depth_xgb, \"estimator__n_estimators\": n_estimators_xgb,\n                        \"estimator__min_child_weight\": min_child_weight_xgb, \"estimator__gamma\": gamma_xgb}\n\ngridsearch_cv = GridSearchCV(XGB_model, hyperparameter_grid, cv= 4, scoring=\"neg_mean_absolute_error\",\n                             n_jobs=-1, verbose=1, return_train_score=True)\ngridsearch_cv.fit(XGB_X_train, XGB_y_train)\ngridsearch_results = pd.DataFrame(gridsearch_cv.cv_results_).sort_values(\"mean_test_score\", ascending=False)\ngridsearch_results.head(10)","3298d870":"print(\"Best score of the grid search: {}\".format(gridsearch_cv.best_score_))\nprint(\"Best score parameters: {}\".format(gridsearch_cv.best_params_))","3c4179da":"# Tuning parameters for XGB algorithm through gridsearch.\n\n\nXGB_model = XGBRegressor(random_state=1)\nmax_depth_xgb = [6]\nn_estimators_xgb = [100]\nmin_child_weight_xgb = [1]\ngamma_xgb = [0]\n\nsubsample_xgb = [0.6, 0.7, 0.8, 0.9, 1]\ncolsample_bytree_xgb = [0.6, 0.7, 0.8, 0.9, 1]\n\n\nmodel = XGBRegressor(random_state=1, silent= True)\n\nhyperparameter_grid =  {\"estimator__max_depth\": max_depth_xgb, \"estimator__n_estimators\": n_estimators_xgb,\n                        \"estimator__min_child_weight\": min_child_weight_xgb, \"estimator__gamma\": gamma_xgb,\n                        \"estimator__subsample\": subsample_xgb, \"estimator__colsample_bytree\": colsample_bytree_xgb}\n\n\ngridsearch_cv = GridSearchCV(XGB_model, hyperparameter_grid, cv= 4, scoring=\"neg_mean_absolute_error\",\n                             n_jobs=-1, verbose=1, return_train_score=True)\ngridsearch_cv.fit(XGB_X_train, XGB_y_train)\ngridsearch_results = pd.DataFrame(gridsearch_cv.cv_results_).sort_values(\"mean_test_score\", ascending=False)\ngridsearch_results.head(10)","04425c90":"print(\"Best score of the grid search: {}\".format(gridsearch_cv.best_score_))\nprint(\"Best score parameters: {}\".format(gridsearch_cv.best_params_))","2df0f61e":"\ngamma_xgb = [0]\nmin_child_weight_xgb = [1]\nsubsample_xgb = [0.6]\ncolsample_bytree_xgb = [0.6]\nmax_depth_xgb = [6]\nn_estimators_xgb = [100]\n\n# Assigning selected parameters to our XGB algorithm.\nXGB_model = XGBRegressor(max_depth= max_depth_xgb[0], n_estimators= n_estimators_xgb[0],gamma = gamma_xgb[0],\n                    min_child_weight= min_child_weight_xgb[0], subsample = subsample_xgb[0], \n                     colsample_by_tree= colsample_bytree_xgb[0], silent= True)","af460d54":"#Function to fit and cross validate the algorithm with 5 folds.\n\ndef XGBmodelfit(alg, X_train, y_train, useTrainCV=True, cv_folds= 5, early_stopping_rounds=500):\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        \n        # Creating XGB matrix for the cross validation method.\n        xgtrain = xgb.DMatrix(X_train, label=y_train)\n        global cvresult\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n                          metrics='mae', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n\n    # Fit the algorithm on the data\n    alg.fit(X_train, y_train, eval_metric='mae')\n    global XGB_pred\n    # Predict training set:\n    XGB_pred = alg.predict(X_train)","e2355dfe":"XGBmodelfit(XGB_model, XGB_X_train, XGB_y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=500)\n\n# Print model report:\nprint(\"\\nModel Report\")\nprint(\"MAE (Train): {}\".format(mean_absolute_error(XGB_y_train, XGB_pred)))","bbf4eba6":"LGBM_model = LGBMRegressor(metric = \"l1\", verbose =0)\nnum_leaves_LGBM = [ 75, 100, 125, 150]\nlearning_rate_LGBM = [0.01,0.02, 0.03, 0.04, 0.05]\nn_estimators_LGBM = [100,200,300]\nhyperparameter_grid = {\"num_leaves\": num_leaves_LGBM,\n                       \"learning_rate\": learning_rate_LGBM, \"n_estimators\":n_estimators_LGBM}\n\ngridsearch_cv = GridSearchCV(LGBM_model, hyperparameter_grid, cv= 4, scoring=\"neg_mean_absolute_error\",\n                             n_jobs=-1, verbose=1, return_train_score=True)\n\ngridsearch_cv.fit(LGBM_X_train, LGBM_y_train)\n\ngridsearch_results = pd.DataFrame(gridsearch_cv.cv_results_).sort_values(\"mean_test_score\", ascending=False)\ngridsearch_results.head(10)","34f146d7":"print(\"Best score of the grid search: {}\".format(gridsearch_cv.best_score_))\nprint(\"Reinitializing grid search with the following parameters: {}\".format(gridsearch_cv.best_params_))","1eb4b8e9":"num_leaves_LGBM = [150]\nlearning_rate_LGBM = [0.05]\nn_estimators_LGBM= [100]\nmin_data_leaves_LGBM = [1]","29352d34":"# Assigning the paramaters to our LightGBM algorithm.\n\nLGBM_model = LGBMRegressor(metric = \"l1\", num_leaves= num_leaves_LGBM[0],\n                           learning_rate= learning_rate_LGBM[0], n_estimators= n_estimators_LGBM[0],\n                           min_data_leaves= min_data_leaves_LGBM[0], verbose =0)","1db71350":"#Function to fit and cross validate the algorithm with 5 folds.\n\n\ndef modelfitLGBM(alg, X_train, y_train, useTrainCV=True, nfolds= 5, early_stopping_rounds=500):\n    if useTrainCV:\n        LGBM_param = alg.get_params()\n        \n        # Creating LightGBM matrix to use in our CV method.\n        lgtrain = lightgbm.Dataset(X_train, label=y_train)\n        cvresult = lightgbm.cv(LGBM_param, lgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=nfolds,\n                          metrics='mae', early_stopping_rounds=early_stopping_rounds, stratified=False)\n        \n    # Fit the algorithm on the data\n    alg.fit(X_train, y_train, eval_metric='mae')\n\n    # Predict training set:\n    global lgbm_pred\n    lgbm_pred = alg.predict(X_train)\n\n","f38436c1":"modelfitLGBM(LGBM_model, LGBM_X_train, LGBM_y_train, useTrainCV=True, nfolds= 5, early_stopping_rounds=500 )\n\n# Print model report:\nprint(\"\\nModel Report\")\nprint(\"MAE (Train): {}\".format(mean_absolute_error(LGBM_y_train, lgbm_pred)))\n","895e3596":"# Selecting second best parameters as the mean fit_time is drastically higher in the best parameters.\n# The MAE isn't very different between first and second best score, which means our decision is justified.\n\nmax_depth_catboost = [10]\nlearning_rate_catboost = [0.05]\nn_estimators_catboost =  [500]\ncatboost_model = CatBoostRegressor(silent=True, max_depth = max_depth_catboost[0],\n                                   learning_rate= learning_rate_catboost[0], \n                                   n_estimators= n_estimators_catboost[0], \n                                   random_state = 42, loss_function = \"MAE\")","6d14b673":"#Function to fit and cross validate the algorithm with 5 folds.\n\n\ndef modelfit_catboost(alg, X_train, y_train, useTrainCV=True, nfolds= 5, early_stopping_rounds=500):\n    if useTrainCV:\n        cat_param = alg.get_params()\n        \n        # Creating CatBoost matrix to use in our CV method.\n\n        cattrain = cb.Pool(X_train, label=y_train)\n        cvresult = cb.cv(cattrain, cat_param, \n                               num_boost_round=alg.get_params()['n_estimators'], \n                       fold_count=5, early_stopping_rounds=500, stratified=False,\n                      logging_level = \"Verbose\")\n   \n    # Fit the algorithm on the data\n    alg.fit(X_train, y_train)\n\n    # Predict training set:\n    global catboost_pred\n    catboost_pred = alg.predict(X_train)\n\n","d7422d30":"modelfit_catboost(catboost_model, catboost_X_train, catboost_y_train, useTrainCV=True, \n                  nfolds= 5, early_stopping_rounds=500)\n\n# Print model report:\nprint(\"\\nModel Report\")\nprint(\"MAE (Train):{}\".format(mean_absolute_error(catboost_y_train, catboost_pred)))\n    \n","e218db47":"# Reshaping our predictions so that we can merge it with the validation dataset and use it to train \n# our algorithm again.\n\nlevel_0_catboost_pred = catboost_model.predict(catboost_X_valid)\nlevel_0_catboost_pred = level_0_catboost_pred.reshape(60585,1)\n\n\nlevel_0_XGB_pred = XGB_model.predict(XGB_X_valid)\nlevel_0_XGB_pred = level_0_XGB_pred.reshape(60585,1)\n\n\nlevel_0_LGBM_pred = LGBM_model.predict(LGBM_X_valid)\nlevel_0_LGBM_pred = level_0_LGBM_pred.reshape(60585,1)","8a2bf1a8":"# Converting our Validation features dataset into a dataframe.\nXGB_X_valid = pd.DataFrame(XGB_X_valid)\n\n# Converting our predictions into dataframes.\nlevel_0_XGB_pred = pd.DataFrame(level_0_XGB_pred)\nlevel_0_LGBM_pred = pd.DataFrame(level_0_LGBM_pred)\nlevel_0_catboost_pred = pd.DataFrame(level_0_catboost_pred)","1f38cc5d":"\n# Merging the prediction dataframes to the Validation features dataframe as new columns.\n\nXGB_X_valid[\"XGB_pred\"] = level_0_XGB_pred\nXGB_X_valid[\"LGBM_pred\"] = level_0_LGBM_pred \nXGB_X_valid[\"catboost_pred\"] = level_0_catboost_pred","dcbda5a3":"# Converting the validation dataframe back to np.array so that we can use it to train another algorithm.\n\nXGB_X_valid = XGB_X_valid.values","63bec48e":"# Reassigning our level 1 algorithm\n\nXGB_level_1_model = XGBRegressor(max_depth= max_depth_xgb[0], n_estimators= n_estimators_xgb[0],gamma = gamma_xgb[0],\n                    min_child_weight= min_child_weight_xgb[0], subsample = subsample_xgb[0], \n                     colsample_by_tree= colsample_bytree_xgb[0], silent= True)","425f9085":"# Fitting new XGB model and doing 5 folds cross validation with the merged DataFrame\n\nXGBmodelfit(XGB_level_1_model, XGB_X_valid, XGB_y_valid, useTrainCV=True, cv_folds=5, early_stopping_rounds=500)\nprint(\"\\nModel Report\")\nprint(\"MAE (Train): {}\".format(mean_absolute_error(XGB_y_valid, XGB_pred)))","532b1557":"# Testing our fitted model on the original test set we have split at the start.\n\nXGBmodelfit(XGB_model, XGB_X_test, y_test, useTrainCV=True, cv_folds=5, early_stopping_rounds=500)\nprint(\"\\nModel Report\")\nprint(\"MAE (Train): {}\".format(mean_absolute_error(y_test, XGB_pred)))","38a0cd4e":"residuals = XGB_pred - y_test\n\nplt.hist(residuals, color=\"darkred\", bins=50, range=(-10000, 10000))\nplt.xlabel(\"MAE\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Residuals\")\nplt.show()","c864380e":"## Calculating Residuals","674296bd":"Choosing the categories with >10000 observations from the mask we created earlier","6c0f4a32":"We create a features dataframe which has our encoded categorical and numerical features","1865f151":"We are formatting the date columns so that we can engineer a campaign duration column","4889db34":"## Flowchart for Modeling","2a08ed32":"## Feature Reduction","a4881406":"## SGDRegressor","18781862":"## Most Negative Correlations","68540bfb":"## Tuning parameters for XGB algorithm through gridsearch.\n","32c76c40":"Now lets create our numeric subset","dfd756c3":"It looks like top 3 performers are in order XGBoost, sklearnGBM, LightGBM","230de9f0":"We can see that only backers column is strongly correlated with the amount pledged","86158027":"## Tuning parameters for LightGBM Algorithm Through Gridsearch.","15b6aef8":"## CatBoostRegressor","c03be512":"\nNow lets do some feature engineering to make some columns useful<br>\nWe will make the name column to be the length of titles for the campaigns","2a943ee4":"We drop usd_pledged since usd_pledged_real shows the same data but the conversion is done from the Fixer.io API instead of kickstarter. This column has no nans. We can also drop goal and pledged column since we are using Fixer.io API conversions","bcd21485":"## Campaign Status Comparison Between Top Categories","58e2408d":"## Fitting the Level 1 Model","dc590d6f":"## Adding in CatBoost Algorithm\n","890f1592":"Creating the category column again for the pairplot data","53f5f6da":"Create the scaler object with a range of 0-1","4c988a10":"## XGBRegressor","d20d9f0d":"Adding back our dependent variable","bf39b90a":"Split the data into 80% 20% training and testing sets","bc4a2e07":"## Feature Engineering","12c0d6e7":"We will remove features that are collinear with eachother to reduce unneccesary features in our model.<br>\nBelow is a function taken from stackoverflow to do this.<br>\nhttps:\/\/stackoverflow.com\/a\/43104383****","1dc4c719":"## Histogram of the USD Pledged Above 10,000$","7b5efeed":"Creating a list with categories that has more than 10,000 campaigns.","512a2ba6":"\n## Scaling the features","75536fb4":"## Correlations","70a9301b":"This shows we have a problem: Outliers<br>\nWe come to the conclusion that alot of the campaigns has raised none or very little money","46c59e8f":"Select the categorical columns to one hot encode them so that we can add them to the correlations","b8e205bc":"## Analyzing Cross Relationships Between Different Variables For Top Categories","8d60146a":"Seperating dependent and independent variables","391d380d":"Function to calculate mean absolute error of the training algorithm","bb5432f5":"## Plotting Relationship of USD Amount Pledged vs Backers","c69274cd":"From our plot, we can see that the relationship between backers and pledged amount isn't<br>\ncompletely linear. We can also see that some categories are doing significantly better<br>\nand higher pledged to backers ratio.","50b22742":"## Histogram of the USD Pledged","f4fb122d":"## Bayesian Ridge","3abbd3fe":"Finally, we will analyze relationships between many variables for the top categories with a pair plot.","db4442c5":"Now lets plot these categories to check if the campaigns failed or succeeded.","03e37de2":"## Most positive correlations","2522e939":"Print the correlations","baf97ab9":"Transform both the training and testing data","cc64f4d3":"We check correlations of the features with pledged amount","1173e7db":"From this graph we can see that categories have some effect on the amount pledged.<br>\nEspecially short category has a big number of very low amounts pledged.","4bd8f38f":"## Linear Regression","40005e63":"## LightGBMRegressor","81bfc753":"## Splitting Data Into Train and Test Sets","a8150298":"Function will remove collinear features that are above 60%","38215634":"We can see that there is a huge difference in pledge states between different categories","8cfc78f8":"## KNNRegressor","24071da3":"Dropping the dependant variable so that the function does not test correlation between features and dependent variable","de54742b":"Fit on the training data","4ff29526":"Creating the category column again since our categories are encoded","1efed29c":"## Predicting Values for the Validation Dataset","e6b73728":"## Feature selection for XGBoost, LightGBM and CatBoost Regression","c3e25cea":"The first plot will show the distribution of amounts pledged for categories<br>\nwith more than 10,000 campaigns.","563b5ba4":"Choosing the categories with >10000 observations from the mask we created earlier","0b662291":"\nFunction to train a model, and evaluates the model on the test set","e420c5a2":"<img src=\"https:\/\/go.gliffy.com\/go\/share\/image\/sue90o585kisdtlbmkvv.png?utm_medium=live-embed&utm_source=wordpress\" width=\"1000px\" align=\"middle\">","42987b22":"## Calculating Missing values by column","ff1c4f25":"> ## Testing Final Level 1 Model with Test Sample","a54492b7":"## Gradient Boosting Regressor","3b41eb36":"Creating campaign duration column","c9a923a4":"Moving our dependent into another variable","b6944aab":"## OneHotEncode","27e478eb":"\nChecking the training and testing data from the EDA","bf220bad":"Evaluate Model","ad4f0e5f":"## Plotting Amount Pledged For Categories With > 10.0000 Observations","afea8a25":"[](http:\/\/)Find all correlations and sort","39d8ff79":"## Plotting Model Performances","9357584f":"Use seaborn to plot a scatterplot of USD Amount pledged vs Backers"}}