{"cell_type":{"9334c7bf":"code","8fce2108":"code","ad304058":"code","f31c8575":"code","7a400bfb":"code","35868765":"code","40226b4d":"code","29314ecd":"code","42f7a4dd":"code","fa656c1a":"code","6d8500da":"code","cb2340f0":"code","493d2f80":"code","b7c6fc3f":"code","be557bc9":"code","fa04cef3":"code","5d4551da":"code","dd735d17":"code","d5063b0f":"code","3d7280d9":"code","da689878":"code","e450bc9f":"code","2d608ed8":"code","423750cf":"code","ef05ccb9":"markdown","47810bfc":"markdown","872f3eed":"markdown","3831dc5f":"markdown","409a1615":"markdown","8a436925":"markdown","acc9f822":"markdown"},"source":{"9334c7bf":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport html\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec, Phrases\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences","8fce2108":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom xgboost import XGBClassifier\n\nr = RandomForestClassifier()\ngbc = GradientBoostingClassifier()\nd = DecisionTreeClassifier()\nlog = LogisticRegression()\nk = KNeighborsClassifier()\ng = GaussianNB()\nb = BernoulliNB()\nxgbc = XGBClassifier()\nnb = MultinomialNB()","ad304058":"train=pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\",'\\t')\ntest=pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\",\"\\t\")\natt=pd.read_csv(\"..\/input\/imdb-review-dataset\/imdb_master.csv\")","f31c8575":"att=att[[\"review\",\"label\"]]\natt.columns=[\"review\",\"sentiment\"]\natt=att[att.sentiment != \"unsup\"]\natt=att.replace({\"neg\":0,\"pos\":1})\natt.head()","7a400bfb":"train.shape, test.shape, att.shape","35868765":"df=train.append(att, ignore_index=True)\ndf=df.append(test, ignore_index=True)\ndf.info()","40226b4d":"df.head()","29314ecd":"df.review=html.unescape(df.review)\n# removing html symbols\n\ndf.review=df.review.str.replace('http\\S+|www.\\S+', '', case=False).str.replace(r\"\\&\\#[0-9]+\\;\",\"\", regex=True) \\\n.str.replace(r\"[^\\w\\s]\",\"\").str.replace(\"\\d+\",\"\").str.replace(r\"[\\s]+\",\" \",regex=True) \\\n.str.replace(\"\\n\",\" \").replace(\"\\r\",\"\").str.replace(\"_\",\" \").str.lower()","42f7a4dd":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['review']=df['review'].apply(lambda x: remove_emoji(x))","fa656c1a":"from unicodedata import normalize\n\nremove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n\ndf[\"review\"] = df[\"review\"].apply(remove_accent)","6d8500da":"df.head()","cb2340f0":"df.sentiment.value_counts()","493d2f80":"corpora = df[\"review\"].values\ntokenized = [word_tokenize(corpus) for corpus in corpora]\n\nprint(tokenized[2222])","b7c6fc3f":"tokenized = [list(filter(lambda x: len(x) > 1, document)) \\\n             for document in tokenized]\n\nprint(tokenized[2222])","be557bc9":"%%time\nfrom gensim.models import word2vec\nnp.set_printoptions(suppress=True)\n\nfeature_size = 256\ncontext_size = 5\nmin_word = 1\n\nword_vec= word2vec.Word2Vec(tokenized, vector_size=feature_size, \\\n                            window=context_size, min_count=min_word, \\\n                            epochs=50, seed=42)","fa04cef3":"word_vec_unpack = [(word, idx) for word, idx in \\\n                   word_vec.wv.key_to_index.items()]\n\ntokens, indexes = zip(*word_vec_unpack)\n\nword_vec_df = pd.DataFrame(word_vec.wv.vectors[indexes, :], index=tokens)\n\nword_vec_df.head()","5d4551da":"tokenized_array = np.array(tokenized)\n\nmodel_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])","dd735d17":"model_df = pd.DataFrame(model_array)\nmodel_df[\"sentiment\"] = df[\"sentiment\"]\n\nmodel_df.head()","d5063b0f":"df_train=model_df[:75000]\ndf_test=model_df[75000:]","3d7280d9":"y=df_train[\"sentiment\"]\ndel df_train[\"sentiment\"]\ntext=df_train","da689878":"del df_test[\"sentiment\"]\ntest=df_test","e450bc9f":"text.shape, y.shape, test.shape","2d608ed8":"r.fit(text,y)\npred=r.predict(test)\npred = pred.astype(int)\npred","423750cf":"sub=pd.DataFrame()\ntest_df=df[75000:]\nsub[\"id\"]=test_df[\"id\"]\nsub[\"sentiment\"]=pred\nsub\nsub.to_csv(\"submission.csv\", index = False)","ef05ccb9":"# Tokenization","47810bfc":"# Final Dataframe","872f3eed":"# Import","3831dc5f":"# Model & Predict","409a1615":"# Clean Data","8a436925":"# EDA","acc9f822":"# Word Embedding for Feature Engineering"}}