{"cell_type":{"e666f2ca":"code","76167540":"code","8d490321":"code","943c63a0":"code","92173204":"code","41e0d3d8":"code","703f5b63":"code","c9339b26":"code","59920df1":"code","77aa280e":"code","4db432b5":"markdown","19e8afc0":"markdown","72d5a2ca":"markdown","939bd2cf":"markdown","1d016bf8":"markdown"},"source":{"e666f2ca":"!pip install google-play-scraper >> z_pip_install.log","76167540":"import numpy as np\nimport pandas as pd\n\n_SW_URL = \"https:\/\/raw.githubusercontent.com\/masdevid\/ID-Stopwords\/master\/id.stopwords.02.01.2016.txt\"\n_POS_URL = \"https:\/\/raw.githubusercontent.com\/fajri91\/InSet\/master\/positive.tsv\"\n_NEG_URL = \"https:\/\/raw.githubusercontent.com\/fajri91\/InSet\/master\/negative.tsv\"\n\n_SW = set(pd.read_csv(_SW_URL, header = None, names = ['word'])['word'])\n_POS = pd.read_csv(_POS_URL, sep = '\\t')\n_NEG = pd.read_csv(_NEG_URL, sep = '\\t')","8d490321":"# Define various auxiliary helpers and\n# design a primary function to scrape\n\nfrom google_play_scraper import reviews, app\nimport time\nimport logging\nimport sys\n\ndef setup_logger(silent = False):\n    \"\"\"Setup a logger that streams to stdout (not red background on Jupyter)\n    \n    When used in Kaggle, the output log can be obtained via Log page.\n    \"\"\"\n    logger = logging.getLogger(\"mylog\")\n    \n    if silent:\n        logger.setLevel(logging.ERROR)\n    else:\n        logger.setLevel(logging.DEBUG)\n    \n    stream_handler = logging.StreamHandler(stream = sys.stdout)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    \n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    \n    return logger\n\ndef scrape(APP_ID, sleep_second = 0.05, max_time = 8.5*3600, silent = False):\n    \"\"\"Scrape reviews on APP_ID\n    \n    Scrape Google Play Store reviews of application with id APP_ID,\n    using language and country tag 'id' (Indonesia).\n    \n    Every 1000 reviews scraped, add delay time of sleep_second seconds\n    to avoid dead responses\/bottleneck\/blocking.\n    \n    Halt scraping process when reaches max_time seconds.\n    \"\"\"\n    \n    logger = setup_logger(silent = silent)\n    \n    logger.info(f\"BEGINNING OF FUNCTION CALL\")\n    logger.info(f\"==================================\")\n    logger.info(f\"Scraping started for: {APP_ID}\")\n    logger.info(f\"Language and country tag is 'id'\")\n    \n    app_info = app(APP_ID, lang = 'id', country = 'id')\n    \n    logger.info(f'----------------------------------')\n    logger.info(f\"Information\")\n    logger.info(f\"Name: {app_info['title']}\")\n    logger.info(f\"Enlisted reviews count : {app_info['reviews']}\")\n    logger.info(f'----------------------------------')\n    \n    timeout = time.time() + max_time\n    \n    result, continuation_token = reviews(\n        APP_ID, lang = 'id', country = 'id'\n    )\n    \n    _n_prev, _n_current = len(result), len(result)\n    logger.info(f\"Number of acquired reviews so far: {_n_current}\")\n    \n    # If the .token attribute is not None, it means there's more to scrape.\n    while continuation_token.token:\n        _result, continuation_token = reviews(\n            APP_ID, lang = 'id', country = 'id',\n            continuation_token = continuation_token\n        )\n        \n        result += _result\n        _n_current = len(result)\n        # Update counts every 1000 reviews fetched\n        # Deliberately delay requests by sleep_in_seconds seconds on every count update\n        if _n_current - _n_prev >= 1000:\n            _n_prev = _n_current\n            logger.info(f\"Number of acquired reviews so far: {_n_current}\")\n            \n            time.sleep(sleep_second)\n        \n        if time.time() >= timeout:\n            logger.warning(f\"Timeout reached, escaping loop.\")\n            break\n    \n    logger.info(f\"Scraping finished\")\n    \n    final_count = len(result)\n    fraction = np.round(final_count\/app_info['reviews'], 4)*100\n    \n    logger.info(f\"Final count of reviews scraped: {final_count}\")\n    logger.info(f\"{fraction}% of enlisted review count.\")\n    logger.info(f\"==================================\")\n    logger.info(f\"END OF FUNCTION CALL\")\n    \n    return result","943c63a0":"# Perform scraping\nPEDULILINDUNGI = 'com.telkom.tracencare'\nresult = scrape(PEDULILINDUNGI)","92173204":"# Turn the resulting list of dictionary (json format)\n# into pandas dataframe.\nraw_df = pd.json_normalize(result)\n\n# Drop the user image column, it will never be used.\n# Other columns may be useful. But who knows.\nraw_df = raw_df.drop(['userImage'], axis = 1)\n\n# Get indexes for latest version\nidx_latest_ver = raw_df[raw_df['reviewCreatedVersion'] == '4.0.2'].index\n\n# Take latest version\nresult_df = raw_df.loc[idx_latest_ver, :]\n\n# Show dataframe\nresult_df","41e0d3d8":"# Save to csv\nraw_df.to_csv('pedulilindungi_raw.csv', index = False)\nresult_df.to_csv('pedulilindungi_latest.csv', index = False)","703f5b63":"# Define various auxiliary helpers for preprocessing and labeling\n\n# Prepare a translation table to remove punctuations and numbers\nimport string\nunwanted_chars = string.punctuation + string.digits\ndel_trans_table = str.maketrans({k: None for k in unwanted_chars})\n\n# Prepare function to remove repeated letters i.e. aduuuuh into aduuh\ndef shorten(word):\n    result = ''\n    cur_char = word[0]\n    cur_count = 0\n    for char in word:\n        if char == cur_char:\n            cur_count += 1\n        else:\n            cur_char = char\n            cur_count = 1\n        \n        if cur_count < 3:\n            result += char\n    \n    return result\n\ndef preprocess(text):\n    \"\"\"Preprocess a single string of text\"\"\"\n    \n    result = text\n    \n    # Normalize (case-folding)\n    result = result.lower()\n    \n    # Clean up nonstandard characters\n    result = result.encode('ascii', 'ignore').decode()\n    \n    # Clean up punctuations\n    result = result.translate(del_trans_table)\n    \n    # Tokenize by whitespace, drop stopwords, and merge back to a sentence\n    result = ' '.join([shorten(word) for word in result.split() if word not in _SW])\n    \n    return result\n\n# Prepare a sentiment weight lexicon dictionary for labeling\nlexicon_dict = pd.concat([_POS, _NEG]) \\\n    .reset_index(drop = True) \\\n    .set_index('word') \\\n    .to_dict()['weight']\n\ndef weight_sentiment(text):\n    \"\"\"Label sentiment of a single string of text\"\"\"\n    words = text.split()\n    \n    sentiment = sum([lexicon_dict.get(word, 0) for word in words])\n    \n    return sentiment","c9339b26":"from tqdm.auto import tqdm\ntqdm.pandas()\n\n# Establish initial dataframe from raw\nready_df = raw_df[['content']]\nready_df = ready_df.rename(columns = {'content': 'raw'})\n\n# Perform preprocessing\nready_df['preprocessed'] = ready_df['raw'].progress_apply(preprocess)\n\n# Perform labeling\nready_df['weight'] = ready_df['preprocessed'].progress_apply(weight_sentiment)\n\n# Classify into pos\/neg\nready_df['label'] = [\"positive\" if weight > 0 else\n                     \"negative\" if weight < 0 else \"neutral\"\n                     for weight in ready_df['weight']]\n\n# Remove neutral sentiment\nready_df = ready_df[ready_df['label'] != 'neutral']\n\n# Show dataframe\nready_df","59920df1":"ready_df.to_csv('pedulilindungi_preprocessed_labeled.csv', index = False)\n\nready_df[ready_df.index.isin(idx_latest_ver)] \\\n    .to_csv('pedulilindungi_preprocessed_labeled_latest.csv', index = False)","77aa280e":"print(ready_df['label'].value_counts())\nprint(ready_df['label'][ready_df.index.isin(idx_latest_ver)].value_counts())","4db432b5":"# Preprocess and label data","19e8afc0":"# Get extra stuffs required","72d5a2ca":"# Save raw scrapes to csv output","939bd2cf":"# Save preprocessed\/labeled data to csv output","1d016bf8":"# Scrape data from Google Play"}}