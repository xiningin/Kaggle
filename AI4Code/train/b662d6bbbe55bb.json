{"cell_type":{"58a5a4b8":"code","eb3bd16b":"code","afdf0ecc":"code","1066f56e":"code","b222a0ee":"code","46df02d1":"code","2f92fd4b":"code","6bad418d":"code","a6cb0cb8":"code","8c223e56":"markdown","21322392":"markdown","86f3dbb7":"markdown","3501616b":"markdown","6a9620a8":"markdown","726fd93b":"markdown","4d265ead":"markdown"},"source":{"58a5a4b8":"import pandas as pd\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nimport subprocess\nfrom sklearn import tree\nfrom os import system\nfrom IPython.display import SVG\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import LabelEncoder \nimport seaborn as sns","eb3bd16b":"def read_dataset():\n    df = pd.read_csv(\"..\/input\/mines-vs-rocks\/sonar.all-data.csv\")# Reading input sheet with read_csv method\n    X=df.iloc[:,0:60]\n    y=df.iloc[:,-1]\n    #Encode the independent variable \n    encoder=LabelEncoder()\n    encoder.fit(y)\n    Y=encoder.transform(y)\n    return(X,Y)","afdf0ecc":"X,Y=read_dataset()","1066f56e":"X,Y=shuffle(X,Y,random_state=1)","b222a0ee":"\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1) # 70% training and 30% test","46df02d1":"\nfrom sklearn.ensemble import RandomForestClassifier\n\nmax_depth_array = [2,3,4,5]\nmin_samples_leaf_array = [2,3,4,5,6,7,8]\nn_estimators_array = [10,20,30,40,50,60,70,80,90,100]\n\n# Finding ROC_AUC_SCORE & Confusion Matrix for train and test data with multiple max depths,min_samples_leaf,n_estimators\nfor x in max_depth_array:\n    for y in min_samples_leaf_array:\n        for z in n_estimators_array:\n            randomforest_classifier = RandomForestClassifier(criterion='gini',\n                max_depth=x, max_features='auto',min_samples_leaf=y,\n                n_estimators=z,random_state=0)\n            \n            rf=randomforest_classifier.fit(X_train, y_train)\n            y_pred = rf.predict(X_train)\n            print(\"max_depth,min_samples_leaf,n_estimator -: \",x,'-',y,'-',z)\n            MSE = np.square(np.subtract(y_train,y_pred)).mean() \n            print(MSE)\n            print(confusion_matrix(y_train, y_pred))","2f92fd4b":"randomforest_classifier = RandomForestClassifier(criterion='gini',\n                max_depth=3, max_features='auto',min_samples_leaf=2,\n                n_estimators=100,random_state=0)\n            \nrf=randomforest_classifier.fit(X_train, y_train)\ny_pred = rf.predict(X_train)\nMSE = np.square(np.subtract(y_train,y_pred)).mean() \nprint('Min Mean squared error - ',MSE)\nrf_cf=confusion_matrix(y_train, y_pred)\n\ncolors = [\"#0101DF\", \"#DF0101\"]\nsns.heatmap(rf_cf,annot=True, cmap=plt.cm.copper)\nplt.title('Random Forest Classifier \\n Confusion Matrix', fontsize=14)","6bad418d":"from sklearn.ensemble import GradientBoostingClassifier\n\n# Finding min mean squared error & Confusion Matrix for train and test data with multiple max depths,min_samples_leaf,n_estimators\nmax_depth_array = [2,3,4,5]\nmax_features_array = [2,3,4,5,6,7,8]\nn_estimators_array = [10,20,30,40,50,60,70,80,90,100]\nsample_weight_array= [1,2,3,4,5,6,7,8]\n\nfor x in max_depth_array:\n    for y in max_features_array:\n        for z in n_estimators_array:\n            for w in sample_weight_array:\n                gradientBoosting_classifier = GradientBoostingClassifier(\n                    learning_rate=0.05,\n                    n_estimators=z,\n                    max_depth=x,\n                    random_state=1,\n                    max_features=y)\n            \n                gb=gradientBoosting_classifier.fit(X_train, y_train)\n                y_pred = gb.predict(X_train)\n\n                print(\"max_depth,min_samples_leaf,n_estimator -: \",x,'-',y,'-',z,'-',w)\n                MSE = np.square(np.subtract(y_train,y_pred)).mean() \n                print(MSE)\n                print(confusion_matrix(y_train, y_pred))","a6cb0cb8":"from sklearn.metrics import accuracy_score\ngradientBoosting_classifier = GradientBoostingClassifier(\n                    learning_rate=0.05,\n                    n_estimators=90,\n                    max_depth=2,\n                    random_state=1,\n                    max_features=2)\n            \ngb=gradientBoosting_classifier.fit(X_train, y_train)\ny_pred = gb.predict(X_train)\n\nprint(\"max_depth,min_samples_leaf,n_estimator -: \",x,'-',y,'-',z,'-',w)\nMSE = np.square(np.subtract(y_train,y_pred)).mean() \nprint('Min Mean squared error - ',MSE)\nrf_cf=confusion_matrix(y_train, y_pred)\nprint(\"Accuracy score: \", accuracy_score(y_train,y_pred))\nprint(y_train.shape)\nsns.heatmap(rf_cf,annot=True, cmap=plt.cm.copper)\nplt.title('Random Forest Classifier \\n Confusion Matrix', fontsize=14)","8c223e56":"**Min mean Square error and better prediction & confusion matrix**\n\nmax_depth,min_samples_leaf,n_estimator -: 2 - 2 - 90 - 1\n\nMSE - 0.0\n\nConfusion Matrix\n\n[[80 0]\n\n[ 0 64]","21322392":"**Reading the data set**","86f3dbb7":"**Shuffle the data to mix up rows**","3501616b":"**Min mean Square error and better prediction & confusion matrix**\n\nmax_depth,min_samples_leaf,n_estimator -: 3 - 2 - 100\n\nMSE - 0.041666666666666664\n\nConfusion Matrix\n\n[[80 0]\n\n[ 6 58]]","6a9620a8":"**Split dataset into training set and test set**","726fd93b":"**GradientBoostingClassifier**\n\n**Automated to find out best hyper parameters with prediction and min mse**","4d265ead":"**Applying RandomForestClassifier**\n\n   *  To find the best hyper parameters for sonar dataset\n        * max_depth : max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data\n        * min_samples_leaf is The minimum number of samples required to be at a leaf node\n        * n_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data\n"}}