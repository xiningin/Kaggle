{"cell_type":{"fa661fcb":"code","cc95ec2b":"code","e02b448d":"code","ae96ee75":"code","2097eeb8":"code","170eef7c":"code","318f8425":"code","ac58f15b":"code","2aa6fcb0":"code","7799cab7":"code","3994493a":"code","916a1e7b":"code","f7f88da0":"code","a0a265f1":"code","318833d3":"code","3e3383b2":"code","83fe9792":"code","2e01ae12":"code","1c9eaa41":"code","9a2cfa5f":"code","1cb67388":"code","a02eeb8e":"code","42d05be8":"code","0d334cd4":"code","0dc74c9a":"code","64314371":"code","8f3b6a2e":"code","b8dcdf67":"code","79ab0c4a":"code","37f22b59":"code","af812cca":"markdown","cc095d86":"markdown","30e96795":"markdown","885d04c5":"markdown","34b57718":"markdown","a72ee74b":"markdown","0391623b":"markdown","4398fc90":"markdown","2e46866a":"markdown","33c31d97":"markdown","7b61d507":"markdown","99fc9e4c":"markdown","88e59a6d":"markdown","a1aefd00":"markdown"},"source":{"fa661fcb":"from __future__ import absolute_import, division, print_function, unicode_literals\n\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport io\nfrom PIL import Image\nfrom IPython.display import Image as IPyImage\nimport imageio\n\nfrom tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler, ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n#%load_ext tensorboard\n\nimport os\nimport matplotlib.pylab as plt\nimport numpy as np\nimport math\nimport datetime\nimport pandas as pd\n\nprint(\"Version: \", tf.__version__)\ntf.get_logger().setLevel('INFO')","cc95ec2b":"# Download and prepare the horses or humans dataset from tensorflow datasets\n\nsplits, info = tfds.load('horses_or_humans', as_supervised=True, with_info=True, split=['train[:80%]', 'train[80%:]', 'test'])\n\n(train_examples, validation_examples, test_examples) = splits\n\nnum_examples = info.splits['train'].num_examples\nnum_classes = info.features['label'].num_classes","e02b448d":"BATCH_SIZE = 32\nIMAGE_SIZE = (150, 150)","ae96ee75":"# Image format function\ndef format_image(image, label):\n  image = tf.image.resize(image, IMAGE_SIZE) \/ 255.0\n  return  image, label","2097eeb8":"# Let's define train,validation,test batches\ntrain_batches = train_examples.shuffle(num_examples \/\/ 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\nvalidation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)\ntest_batches = test_examples.map(format_image).batch(1)","170eef7c":"# Let's see the shape of one batch\nfor image_batch, label_batch in train_batches.take(1):\n  pass\nimage_batch.shape","318f8425":"# Let's create a function to create the simple models\ndef build_model(dense_units, input_shape=IMAGE_SIZE + (3,)):\n  model = tf.keras.models.Sequential([\n          tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),\n          tf.keras.layers.MaxPooling2D(2, 2),\n          tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n          tf.keras.layers.MaxPooling2D(2, 2),\n          tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n          tf.keras.layers.MaxPooling2D(2, 2),\n          tf.keras.layers.Flatten(),\n          tf.keras.layers.Dense(dense_units, activation='relu'),\n          tf.keras.layers.Dense(2, activation='softmax')\n  ])\n  return model","ac58f15b":"# tf.keras.callbacks.ModelCheckpoint(\n            # filepath,\n            # monitor='val_loss', \n            # verbose=0, \n            # save_best_only=False,\n            # save_weights_only=False,\n            # mode='auto', \n            # save_freq='epoch',     \n            # options=None, **kwargs)\n","2aa6fcb0":"# Let's save the model's weights\nmodel_check_w = build_model(dense_units=256)\n\nmodel_check_w.compile(optimizer='sgd',\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n\nmodel_check_w.fit(train_batches,\n                 epochs=5,\n                 validation_data=validation_batches,\n                 verbose=2,\n                 callbacks=[ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.h5', \n                                             verbose=1,\n                                             ),])","7799cab7":"os.listdir('\/kaggle\/working')","3994493a":"# Save the models\nmodel_check_m = build_model(dense_units=256)\nmodel_check_m.compile(\n    optimizer='sgd',\n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'])\n  \nmodel_check_m.fit(train_batches, \n          epochs=3, \n          validation_data=validation_batches, \n          verbose=2,\n          callbacks=[ModelCheckpoint('model.h5', \n                                      verbose=1,\n                                      save_best_only=True) # just save the best model\n          ])","916a1e7b":"os.listdir('\/kaggle\/working')","f7f88da0":"# tf.keras.callbacks.EarlyStopping(\n                             #     monitor='val_loss', \n                             #     min_delta=0, \n                             #     patience=0, \n                             #     verbose=0,\n                             #     mode='auto', \n                             #     baseline=None, \n                             #     restore_best_weights=False)","a0a265f1":"model_early = build_model(dense_units=256)\n\nmodel_early.compile(\n    optimizer='sgd',\n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'])\n\nmodel_early.fit(train_batches,\n                epochs=100,\n                validation_data= validation_batches,\n                verbose=2,\n                callbacks=[EarlyStopping(patience=3, # model will check 3 epoch more after best score\n                                         mode ='min',\n                                         monitor='val_loss',\n                                         # even if training will stop after 3 epoch, it get best weights\n                                         restore_best_weights=True,\n                                         verbose=1)])\n\n# in this example we will get the weights from epoch 24 ","318833d3":"os.listdir('\/kaggle\/working')","3e3383b2":"model = build_model(dense_units=256)\n\nmodel.compile(\n    optimizer='sgd',\n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'])\n  \ncsv_file = 'training.csv'\n\nmodel.fit(train_batches, \n          epochs=3, \n          validation_data=validation_batches, \n          callbacks=[CSVLogger(csv_file)\n          ])","83fe9792":"pd.read_csv(csv_file).head()","2e01ae12":"model = build_model(dense_units=256)\n\nmodel.compile(\n    optimizer='sgd',\n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'])\n\n#example 1\ndef scheduler(epoch):\n    initial_lr = 0.01\n    drop = 0.5\n    epochs_drop = 1\n    lr = initial_lr * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    return lr \n\n# example 2\ndef scheduler_1(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\n\nmodel.fit(train_batches, \n          epochs=5, \n          validation_data=validation_batches, \n          callbacks=[LearningRateScheduler(scheduler, verbose=1)])","1c9eaa41":"model = build_model(dense_units=256)\n\nmodel.compile(\n    optimizer='sgd',\n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'])\n  \nmodel.fit(train_batches, \n          epochs=50, \n          validation_data=validation_batches, \n          callbacks=[ReduceLROnPlateau(monitor='val_loss', \n                                       factor=0.2, verbose=1,\n                                       patience=3, min_lr=0.001)])","9a2cfa5f":"# Define the Keras model to add callbacks to\ndef get_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(1, activation = 'linear', input_dim = 784))\n    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.1), \n                  loss='mean_squared_error', \n                  metrics=['mae'])\n    return model","1cb67388":"# load the MNIST data from Keras datasets API:\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(60000, 784).astype('float32') \/ 255\nx_test = x_test.reshape(10000, 784).astype('float32') \/ 255","a02eeb8e":"# define a simple custom callback to track the start and end of every batch of data. \n# During those calls, it prints the index of the current batch\n\nclass myCallback(tf.keras.callbacks.Callback):\n\n    def on_train_batch_begin(self, batch, logs=None):\n        print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n\n    def on_train_batch_end(self, batch, logs=None):\n        print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))","42d05be8":"model = get_model()\n_ = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=1,\n          steps_per_epoch=5,\n          verbose=0,\n          callbacks=[myCallback()])","0d334cd4":"class DetectOverfittingCallback(tf.keras.callbacks.Callback):\n    def __init__(self, threshold):\n        super(DetectOverfittingCallback, self).__init__()\n        self.threshold = threshold\n\n    def on_epoch_end(self, epoch, logs=None):\n        ratio = logs[\"val_loss\"] \/ logs[\"loss\"]\n        print(\"Epoch: {}, Val\/Train loss ratio: {:.2f}\".format(epoch, ratio))\n\n        if ratio > self.threshold:\n            print(\"Stopping training...\")\n            self.model.stop_training = True\n\nmodel = get_model()\n_ = model.fit(x_train, y_train,\n              validation_data=(x_test, y_test),\n              batch_size=64,\n              epochs=100,\n              verbose=1,\n              callbacks=[DetectOverfittingCallback(threshold=1.1)])","0dc74c9a":"# Visualization utilities\nplt.rc('font', size=20)\nplt.rc('figure', figsize=(15, 3))\n\ndef display_digits(inputs, outputs, ground_truth, epoch, n=10):\n    plt.clf()\n\n    plt.yticks([])\n    plt.grid(None)\n    inputs = np.reshape(inputs, [n, 28, 28])\n    inputs = np.swapaxes(inputs, 0, 1)\n    inputs = np.reshape(inputs, [28, 28*n])\n    plt.imshow(inputs)\n    plt.xticks([28*x+14 for x in range(n)], outputs)\n    for i,t in enumerate(plt.gca().xaxis.get_ticklabels()):\n        if outputs[i] == ground_truth[i]: \n            t.set_color('green') \n        else: \n            t.set_color('red')\n    plt.grid(None)","64314371":"GIF_PATH = '.\/animation.gif'","8f3b6a2e":"class VisCallback(tf.keras.callbacks.Callback):\n    def __init__(self, inputs, ground_truth, display_freq=10, n_samples=10):\n        self.inputs = inputs\n        self.ground_truth = ground_truth\n        self.images = []\n        self.display_freq = display_freq\n        self.n_samples = n_samples\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Randomly sample data\n        indexes = np.random.choice(len(self.inputs), size=self.n_samples)\n        X_test, y_test = self.inputs[indexes], self.ground_truth[indexes]\n        predictions = np.argmax(self.model.predict(X_test), axis=1)\n\n        # Plot the digits\n        display_digits(X_test, predictions, y_test, epoch, n=self.display_freq)\n\n        # Save the figure\n        buf = io.BytesIO()\n        plt.savefig(buf, format='png')\n        buf.seek(0)\n        image = Image.open(buf)\n        self.images.append(np.array(image))\n\n        # Display the digits every 'display_freq' number of epochs\n        if epoch % self.display_freq == 0:\n            plt.show()\n\n    def on_train_end(self, logs=None):\n        imageio.mimsave(GIF_PATH, self.images, fps=1)","b8dcdf67":"def get_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation='linear', input_dim=784))\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","79ab0c4a":"model = get_model()\nmodel.fit(x_train, y_train,\n          batch_size=64,\n          epochs=20,\n          verbose=0,\n          callbacks=[VisCallback(x_test, y_test)])","37f22b59":"SCALE = 60\n\n# FYI, the format is set to PNG here to bypass checks for acceptable embeddings\nIPyImage(GIF_PATH, format='png', width=15 * SCALE, height=3 * SCALE) ","af812cca":"# Keras Callbacks\n\n- Callback is a Python class meant to be subclassed to provide specific functionality, with a set of methods called at various stages of training (including batch\/epoch start and ends), testing, and predicting\n- Callbacks are useful to get a view on internal states and statistics of the model during training. \n- The methods of the callbacks can be called at different stages of training\/evaluating\/inference.\n- Keras's available Callbacks is [here](https:\/\/keras.io\/api\/callbacks\/). We will see some of them in action","cc095d86":"### Callback 3","30e96795":"## [Early stopping](https:\/\/keras.io\/api\/callbacks\/early_stopping\/)\n- It can stop training when a monitored metric has stopped improving\n- It can also be used the other way, if there's not enough improvement, it could end training,","885d04c5":"# Keras Some Custom Callbacks\n\n- The custom callbacks can still make use of all of the features of the built-in Keras call-backs","34b57718":"### Callback 2","a72ee74b":"- We measure in this example, the ratio between our validation loss and our training loss to detect overfitting. \n- when the ratio gets too high we should stop training\n\nSo, here we'll compute the ratio at the end of every epoch and if that ratio was higher than our threshold value, we can stop training.","0391623b":"## Define Some Variables","4398fc90":"## [Learning Rate Scheduler](https:\/\/keras.io\/api\/callbacks\/learning_rate_scheduler\/)\n- Updates the learning rate during training.\n- When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error.\n- When the learning rate is too small, training is not only slower, but may become permanently stuck with a high training error.\n\n![lr](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/02\/Screen-Shot-2018-02-24-at-11.47.09-AM.png)","2e46866a":"## [ReduceLROnPlateau](https:\/\/keras.io\/api\/callbacks\/reduce_lr_on_plateau\/)\n\n- This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.","33c31d97":"## Usage of `logs` dict\n- The `logs` dict contains the loss value, and all the metrics at the end of a batch or epoch. \n- Example includes the loss and mean absolute error.","7b61d507":"# Import Libraries","99fc9e4c":"### Callback 1","88e59a6d":"## [Model Checkpoint](https:\/\/keras.io\/api\/callbacks\/model_checkpoint\/)\n\n- Models details can be saved out epoch by epoch for later inspection, or we can monitor progress through them","a1aefd00":"## [CSV Logger](https:\/\/keras.io\/api\/callbacks\/csv_logger\/)\n\n- It streams epoch results to a CSV File"}}