{"cell_type":{"a01f51d5":"code","788f6dd0":"code","80090016":"code","cf2a6a2d":"code","4c5b7fa0":"code","99493f33":"code","b23be585":"code","d52aa659":"code","99177a7b":"code","943418ff":"code","881f12bc":"code","85057d90":"code","25f09a09":"code","34db38dc":"code","75e602dc":"code","ba31c188":"code","50a81949":"code","de9442ab":"code","a780b4ee":"code","5eb68693":"code","816263af":"code","b331cf43":"code","9838235c":"code","1554d2c2":"code","f2fec9f7":"code","52f8614b":"code","fde9b581":"code","7c47898e":"code","404b725c":"code","756b07c0":"code","c0d5b976":"markdown","69e3e6d2":"markdown","e1da37c6":"markdown","3663ad9c":"markdown","96d8c3d7":"markdown","b22d8039":"markdown","02246c0e":"markdown"},"source":{"a01f51d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","788f6dd0":"DIR=\"..\/input\/mushroom-classification\/mushrooms.csv\"","80090016":"df=pd.read_csv(DIR)","cf2a6a2d":"# Print first 20 rows.\ndf.head(20)","4c5b7fa0":"#Finding the missing values in our dataset.\nmissing_value=df.isnull()","99493f33":"# Printing the columns with missing values. [False- Number of non-missing values; True- Number of missing values]\nfor columns in missing_value.columns.values.tolist():\n    print (columns)\n    final=missing_value[columns].value_counts()\n    print (final)","b23be585":"df.describe()","d52aa659":"# Dataset shape is 8124 rows and 23 columns.\ndf.shape","99177a7b":"# Applying label encoder to transform labels into numeric form so as to convert it into the machine-readable form\ndf1=df.apply(LabelEncoder().fit_transform)","943418ff":"df1.head(10)","881f12bc":"# Describe function will calculate the mean, standard deviation, min value, max value, values under 25 percentile, 50 percentile and 75 percentile respectively.\ndf1.describe()","85057d90":"# Correlation function will calcuate the correlation of each feature with each other feature.\ndf1.corr()","25f09a09":"# Plotting heat map to visualize the correlation of each feature.\nsns.heatmap(df1.corr())\nplt.show()","34db38dc":"# Since correlation of feature 'veil-type' is very low with other features, we can drop this feature as it will not help in the classification.\ndf2=df1.drop(['veil-type'],axis=1)","75e602dc":"# Shape of our dataset after dropping the 'veil-type' feature.\ndf2.shape","ba31c188":"sns.set_style('darkgrid')\nsns.regplot(df2['cap-shape'],df2['class'])\nplt.show()","50a81949":"# Calculating the pearson coefficient and p-value. In this, pearson coefficient is 0.0529 which means the feature 'cap-shape' is not much positively linearly dependent on target variable which is 'class'.\n#p-value indicates the probability for strong coorelation. In this case, p-value is <0.001 which is the prediction of high correlation. \nscipy.stats.pearsonr(df2['cap-shape'],df2['class'])","de9442ab":"sns.regplot(df2['cap-surface'],df2['class'])","a780b4ee":"#Here pearson coefficient is 0.178 which is positive but not close to 1 hence it is not much linealy dependent on 'class'.\n# p-value is <0.001 which is the prediction for high correlation. \nscipy.stats.pearsonr(df2['cap-surface'],df2['class'])","5eb68693":"sns.regplot(df2['gill-spacing'],df2['class'])","816263af":"#Pearson Coefficient is -0.348 which is negative and little bit closer to 1. Hence, it will be negatively linear dependent on 'class'.\n#p-value is <0.0001 which is the indication for high correlation.\nscipy.stats.pearsonr(df2['gill-spacing'],df2['class'])","b331cf43":"# Dividing the dataset into X (features) and y (target) variables.\nX=df2.drop(df1[['class']],axis=1)\ny=df2[['class']]","9838235c":"# Using train_test_split function to divide data into training and test dataset.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","1554d2c2":"print ('size of X train data and y train data is {} and {} respectively.'.format(X_train.shape, y_train.shape))\nprint ('size of X test data and y test data is {} and {} respectively'.format(X_test.shape, y_test.shape))","f2fec9f7":"#Converting the train and test dataset into numpy array.\nX_Train=np.array(X_train)\nX_Test=np.array(X_test)\ny_Train=np.array(y_train).reshape(-1,)\ny_Test=np.array(y_test).reshape(-1,)","52f8614b":"def prior_probability(y_t,labels):\n    for i in range(y_t.shape[0]):\n        numerator= (y_t== labels).sum()\n    denominator= y_t.shape[0]\n    return numerator\/denominator\n","fde9b581":"prior_probability(y_Train,0)","7c47898e":"def prior_probability(y_Train,label):\n    for i in range(y_Train.shape[0]):\n        numerator= (y_Train== label).sum()\n    denominator= y_Train.shape[0]\n    return numerator\/denominator\n\n\n\n#Function to calculate the conditional probability.\ndef cond_prob(X_Train,y_Train,feature_col,feature_val,label):\n    x_fil=X_Train[y_Train==label]\n    num=np.sum(x_fil[:,feature_col]==feature_val)\n    den=np.sum(y_Train==label)\n    return num\/float(den)\n\ndef prediction(X_Train,y_Train,X_Test):\n    L= np.unique(y_Train)\n    n=X_Train.shape[1]\n    pp=[]\n    for label in L:\n        Likelihood=1\n        for k in range(n):\n            cond= cond_prob(X_Train,y_Train,k,X_Test[k],label)\n            Likelihood*=cond\n        prior_prob=prior_probability(y_Train,label)\n        posterior_p= Likelihood*prior_prob\n        pp.append(posterior_p)\n    return np.argmax(pp)\n\n#Function to calculate the accuracy of our prediction.\ndef score(X_Train,y_Train,X_Test,y_Test):\n    pred=[]\n    for i in range(X_Test.shape[0]):\n        pred_list=prediction(X_Train,y_Train,X_Test[i])\n        pred.append(pred_list)\n    pred=np.array(pred)\n    accuracy=(np.sum(pred==y_Test)\/y_Test.shape[0])\n    return accuracy\n    \n    ","404b725c":"print (prediction(X_Train,y_Train,X_Test[4]))\nprint (y_Test[4])","756b07c0":"# Accuracy is 99 percent.\nprint (score(X_Train,y_Train,X_Test,y_Test))","c0d5b976":"# Data Wrangling","69e3e6d2":"# Posterior Probability ","e1da37c6":"# Data Preprocessing","3663ad9c":"# PRIOR PROBABILITY\n","96d8c3d7":"# Loading data","b22d8039":"## CUSTOM NAIVE BAYES IMPLEMENTATION","02246c0e":"# Importing important libraries"}}