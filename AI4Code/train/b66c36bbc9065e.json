{"cell_type":{"b1a6afeb":"code","59f9338d":"code","a9d57e87":"code","d74eb803":"code","a199e1b2":"code","6724a459":"code","00f5ef4f":"code","691bf8b1":"code","70b8763e":"code","d06d13f5":"code","18c19a8e":"code","689ce5d9":"code","042ad22d":"code","5a3700a9":"code","9a8db168":"code","a57ea3cc":"markdown"},"source":{"b1a6afeb":"%%bash\npip install transformers\n","59f9338d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#!pip install transformers     \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig , DistilBertModel\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\ngc.collect()        \n\n# Any results you write to the current directory are saved as output.","a9d57e87":"df = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep=\"\\t\",usecols = ['Phrase','Sentiment'])\ndf = df.rename(columns={'Phrase': 0 , 'Sentiment' : 1})\ndf.head()\n","d74eb803":"## Subset \nbatch_1 = df[:2000]\nbatch_1[1].value_counts()\n","a199e1b2":"### Loading the Pre-trained BERT model\u00b6\n","6724a459":"# For DistilBERT:\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n## Want BERT instead of distilBERT? Uncomment the following line:\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n\n# Load pretrained model\/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","00f5ef4f":"### Tokenization\u00b6\ntokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n","691bf8b1":"## Padding\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n","70b8763e":"## Masking\nattention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape\n","d06d13f5":"## Model - The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.\n","18c19a8e":"input_ids = torch.tensor(padded)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n","689ce5d9":"#We'll save those in the features variable, as they'll serve as the features to our logitics regression model.\nfeatures = last_hidden_states[0][:,0,:].numpy()\n\n#lables - Target \nlabels = batch_1[1]\n","042ad22d":"# Input to the Logistic Regression\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n","5a3700a9":"lr_clf = LogisticRegression(multi_class = 'ovr', C=1, solver='sag')\nlr_clf.fit(train_features, train_labels)\n","9a8db168":"# Model Evaluation \nlr_clf.score(test_features, test_labels)\n","a57ea3cc":"Referance - https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/"}}