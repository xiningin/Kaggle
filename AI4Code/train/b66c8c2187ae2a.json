{"cell_type":{"8ed3fdb7":"code","52551a7c":"code","40b10da9":"code","07668361":"code","2fbfa0f9":"code","3fb8c139":"code","d3487393":"code","46b19aab":"code","7e59a1be":"code","f8df3b80":"code","a88b8af0":"code","36ffa69a":"code","379ebe76":"code","91cd9753":"code","c23e5748":"code","f284e4b9":"code","950a9d43":"code","681c8228":"code","df575598":"code","d6fb53cb":"code","8bb93b6d":"code","aea0b187":"code","3e1ff0c9":"code","a570d406":"code","da07fdcd":"code","b5a765aa":"code","cb505737":"code","f5881f8b":"code","00cb81a7":"code","3784f206":"code","5b748b92":"code","3f155453":"code","97a1b26c":"code","70141034":"code","74f0e419":"markdown","80567265":"markdown","85bd7ec3":"markdown","14bd30d1":"markdown","74ec3123":"markdown","5ae35dba":"markdown","d67da91a":"markdown","84647829":"markdown","9d44f1d5":"markdown","37e0fad9":"markdown","edbe7316":"markdown","749e859a":"markdown","be274e69":"markdown","0acf97e1":"markdown","0eb8c922":"markdown","ae76cd39":"markdown","b83249ab":"markdown","46c41a01":"markdown","1fd14a67":"markdown","3020918a":"markdown"},"source":{"8ed3fdb7":"%reset -sf","52551a7c":"import glob\nimport pickle\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nfrom copy import deepcopy\nfrom collections import Counter\nfrom matplotlib import collections as mc\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom pandas.plotting import register_matplotlib_converters\nfrom sklearn import linear_model\nregister_matplotlib_converters()  # to convert pandas dates\n\n# so that we can print multiple dataframes in the same cell\nfrom IPython.display import display, HTML\ndef displayer(df): display(HTML(df.head(2).to_html()))\npd.set_option('display.max_columns', 500)\ntimes = pd.date_range('2018-04-23', periods=28*24*6, freq='10min')","40b10da9":"!ls ..\/input\nnrr = np.load(\"..\/input\/dbaproject\/full.npy\")","07668361":"def calc_square_error(preds, actual):\n    return (preds-actual)**2","2fbfa0f9":"def calc_square_log_error(preds, actual):\n    preds = deepcopy(preds)\n    actual = deepcopy(actual)\n    MIN_VAL = 1\n    preds[preds<=MIN_VAL] = MIN_VAL\n    actual[actual<=MIN_VAL] = MIN_VAL\n    return (np.log(preds)-np.log(actual))**2","3fb8c139":"def calc_direction_error(preds, actual):\n    actual, previous, preds = actual[1:], actual[:-1], preds[1:], \n    rrr = np.random.uniform(low=-1, high=1, size=actual.shape)\n    diff_actual = actual - previous + rrr\n    diff_preds = preds - previous + rrr\n    diff_mul = diff_actual*diff_preds\n    return diff_mul<0","d3487393":"STARTING_EVAL_INDEX = 21*24*6  # will no longer be changed\n\ndef evaluate(function_evaluated, initial_state, \n             starting_eval_index=STARTING_EVAL_INDEX):\n    k = starting_eval_index\n    square_error_loss = np.empty((0,38,26))\n    square_log_error_loss = np.empty((0,38,26))\n    direction_error_sum = np.empty((0,38,26))\n    state = initial_state\n    \n    preds = np.empty((0,38,26))\n    actual = np.empty((0,38,26))\n    for i,_ in tqdm_notebook(enumerate(nrr[k:])): # for each slice from k onwards\n        \n        # use everything before i+k to predict\n        pred, state = function_evaluated(nrr[:i+k], state) \n        preds = np.append(preds, [pred], axis=0)\n        actual = np.append(actual, [nrr[i+k]], axis=0)\n    else:\n        total_evaluated = (i+1.)*nrr.shape[1]*nrr.shape[2]\n        \n    assert preds.shape == actual.shape\n    return preds, actual","46b19aab":"def plot_predictions(preds, actual, pixel_plotted=(10,10), plot_range=np.arange(100,130)):\n    fig,ax = plt.subplots(figsize=(15,4))\n    fig.autofmt_xdate()\n    xfmt = mdates.DateFormatter('%d-%m-%y %H:%M')\n    ax.set(ylabel=\"Population Count\")\n    ax.xaxis.set_major_formatter(xfmt)\n    ax.set_title(\"Population Count at {} over Time\".format(pixel_plotted))\n    plt.plot(times[plot_range], preds[plot_range,pixel_plotted[0],pixel_plotted[1]], color='blue', label=\"Predicted\")\n    plt.plot(times[plot_range], actual[plot_range,pixel_plotted[0],pixel_plotted[1]], color='green', label=\"Actual\")\n    plt.legend()\n    plt.show()\n    return None","7e59a1be":"def calc_errors(preds, actual):\n    square_error = calc_square_error(preds, actual)\n    square_log_error = calc_square_log_error(preds, actual)\n    direction_error = calc_direction_error(preds, actual)\n    \n    square_error_grid     = np.sqrt(np.mean(square_error, axis=0))\n    square_log_error_grid = np.sqrt(np.mean(square_log_error, axis=0))\n    direction_error_grid  = np.sqrt(np.mean(direction_error, axis=0))\n    error_grids = square_error_grid, square_log_error_grid, direction_error_grid\n    \n    results = {\n        \"root_mean_square_error_loss\": np.sqrt(np.mean(square_error)),\n        \"root_mean_square_log_error_loss\": np.sqrt(np.mean(square_log_error)),\n        \"average_direction_error\": np.mean(direction_error)\n    }\n    return error_grids, results","f8df3b80":"def plot_error_grids(error_grids, save=False):\n    square_error_grid, square_log_error_grid, direction_error_grid = error_grids\n\n    f, ax = plt.subplots(nrows=2, ncols=3, figsize=(15,7),\n                         gridspec_kw={\"height_ratios\":[1, 0.05]})\n    im1 = ax[0][0].imshow(square_error_grid, cmap=\"YlGnBu\",\n                          norm=colors.LogNorm(vmin=1, vmax=np.nanmax(square_error_grid)))\n    im2 = ax[0][1].imshow(square_log_error_grid, cmap=\"YlGnBu\",\n                          norm=colors.LogNorm(vmin=0.001, vmax=1))\n    im3 = ax[0][2].imshow(direction_error_grid, cmap=\"YlGnBu\",\n                          vmin=0, vmax=1)\n\n    ax[0][0].set_title(\"Root Mean Square Error\")\n    ax[0][1].set_title(\"Root Mean Square Log Error\")\n    ax[0][2].set_title(\"Percentage Direction Error\")\n    ax[0][0].set(ylabel=\"Latitude\", xlabel=\"Longitude\")\n    ax[0][1].set(ylabel=\"Latitude\", xlabel=\"Longitude\")\n    ax[0][2].set(ylabel=\"Latitude\", xlabel=\"Longitude\")\n\n    f.colorbar(im1, cax=ax[1][0], orientation=\"horizontal\", pad=0.2)\n    f.colorbar(im2, cax=ax[1][1], orientation=\"horizontal\", pad=0.2)\n    f.colorbar(im3, cax=ax[1][2], orientation=\"horizontal\", pad=0.2)\n    \n    plt.subplots_adjust()\n    if save: plt.savefig(\"error.svg\")\n    plt.show()","a88b8af0":"def plot_error_grid_difference(error_grids, reference_error_grids, save=False):\n    square_error_grid, square_log_error_grid, direction_error_grid = error_grids\n    square_error_grid_ref, square_log_error_grid_ref, direction_error_grid_ref = reference_error_grids\n    \n    square_error_grid_diff     = square_error_grid     - square_error_grid_ref\n    square_log_error_grid_diff = square_log_error_grid - square_log_error_grid_ref\n    direction_error_grid_diff  = direction_error_grid  - direction_error_grid_ref\n    \n    f, ax = plt.subplots(nrows=2, ncols=3, figsize=(15,7),\n                         gridspec_kw={\"height_ratios\":[1, 0.05]})\n    im1 = ax[0][0].imshow(square_error_grid_diff, cmap=\"seismic\",\n                          norm=colors.SymLogNorm(10), vmin=-1000, vmax=1000)\n    im2 = ax[0][1].imshow(square_log_error_grid_diff, cmap=\"seismic\",\n                          norm=colors.SymLogNorm(0.001), vmin=-0.01, vmax=0.01)\n    im3 = ax[0][2].imshow(direction_error_grid_diff, cmap=\"seismic\",\n                          vmin=-0.25, vmax=0.25)\n\n    ax[0][0].set_title(\"Difference from Baseline\\nRoot Mean Square Error\")\n    ax[0][1].set_title(\"Difference from Baseline\\nRoot Mean Square Log Error\")\n    ax[0][2].set_title(\"Difference from Baseline\\nPercentage Direction Error\")\n    ax[0][0].set(ylabel=\"Latitude\", xlabel=\"Longitude\")\n    ax[0][1].set(ylabel=\"Latitude\", xlabel=\"Longitude\")\n    ax[0][2].set(ylabel=\"Latitude\", xlabel=\"Longitude\")\n\n    f.colorbar(im1, cax=ax[1][0], orientation=\"horizontal\", pad=0.2)\n    f.colorbar(im2, cax=ax[1][1], orientation=\"horizontal\", pad=0.2)\n    f.colorbar(im3, cax=ax[1][2], orientation=\"horizontal\", pad=0.2)\n    \n    plt.subplots_adjust()\n    if save: plt.savefig(\"error_ref.svg\")\n    plt.show()","36ffa69a":"results_dict = {}","379ebe76":"def baseline_copylast(past_slices, state):\n    \n    # example of reading and modifying the state\n    state[\"i\"] = state[\"i\"]+1\n    # if state[\"i\"]%1000 == 0: print(state[\"i\"])\n\n    # prediction, which is to take the previous slice\n    pred = past_slices[-1]\n    \n    # return the prediction and the next state\n    return pred, state\n\npreds, actual = evaluate(baseline_copylast, {\"i\": 1})\nplot_predictions(preds, actual)\nerror_grids_baseline, results = calc_errors(preds, actual)\nplot_error_grids(error_grids_baseline)","91cd9753":"results_dict[\"copy_last\"] = results\nresults","c23e5748":"def baseline_copylast_diff(past_slices, state):\n    pred = past_slices[-1] + (past_slices[-1] - past_slices[-2])\n    return pred, state\n\npreds, actual = evaluate(baseline_copylast_diff, None)\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids)\nplot_error_grid_difference(error_grids, error_grids_baseline)","f284e4b9":"results_dict[\"extrapolate_last\"] = results\nresults","950a9d43":"def baseline_copylastweek(past_slices, state):\n    pred = past_slices[-1] + (past_slices[-7*24*6] - past_slices[-7*24*6-1])\n    return pred, state\n\npreds, actual = evaluate(baseline_copylastweek, None)\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids)\nplot_error_grid_difference(error_grids, error_grids_baseline)","681c8228":"results_dict[\"diff_last_week\"] = results\nresults","df575598":"def moving_average(past_slices, state):\n    n = 4\n    pred = past_slices[-1] + sum([1\/n * (past_slices[-7*24*6-i] - past_slices[-7*24*6-i-1]) for i in range(n)])\n    return pred, state\n\npreds, actual = evaluate(moving_average, {\"i\": 1})\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids)\nplot_error_grid_difference(error_grids, error_grids_baseline)","d6fb53cb":"results_dict[\"moving_average_4\"] = results\nresults","8bb93b6d":"def exponential_smoothing_1(past_slices, previous_state):\n    \n    alpha, previous_forecast_diff = deepcopy(previous_state)\n    current_forecast_diff = (1-alpha)*previous_forecast_diff + (alpha)*(past_slices[-1]-past_slices[-2])\n    pred = past_slices[-1] + current_forecast_diff\n        \n    return pred, [alpha, current_forecast_diff]\n\nalpha = 0.35  # optimised\ninitial_forecast_diff = nrr[STARTING_EVAL_INDEX - 1] - nrr[STARTING_EVAL_INDEX - 2]\n\npreds, actual = evaluate(exponential_smoothing_1, [alpha, initial_forecast_diff])\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids)\nplot_error_grid_difference(error_grids, error_grids_baseline)","aea0b187":"results_dict[\"exponential_smoothing_a35\"] = results\nresults","3e1ff0c9":"def exponential_smoothing_2(data, previous_state):   \n    \n    alpha, beta, previous_A, previous_B, previous_forecast_diff = deepcopy(previous_state)\n    current_A = (1-alpha)*previous_forecast_diff + (alpha)*(data[-1]-data[-2])\n    current_B = (1-beta)*previous_B + (beta)*(previous_A-current_A)\n    current_forecast_diff = current_A + current_B\n    pred = data[-1] + current_forecast_diff\n    \n    return pred, [alpha, beta, current_A, current_B, current_forecast_diff]\n\nalpha = 0.35\nbeta = 0.01\ninitial_A = nrr[STARTING_EVAL_INDEX - 1]\ninitial_B = np.zeros(nrr.shape[1:])\ninitial_forecast_diff = nrr[STARTING_EVAL_INDEX - 1] - nrr[STARTING_EVAL_INDEX - 2]\n\npreds, actual = evaluate(exponential_smoothing_2, [alpha, beta, initial_A, initial_B, initial_forecast_diff])\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids)\nplot_error_grid_difference(error_grids, error_grids_baseline)","a570d406":"# not recorded because it is not better than alpha-only\n# results_dict[\"exponential_smoothing_a35\"] = results  \nresults","da07fdcd":"# load LightGBM boosters\nbst_lst = []\n# for file in glob.glob(\"..\/input\/dba-lightgbm-independent-cells\/*.h5\"):\n#     bst_lst.append(lgb.Booster(model_file=file))  # in case folder name is updated\nfor file in glob.glob(\"..\/input\/dba-lightgbm-model\/*.h5\"):\n    bst_lst.append(lgb.Booster(model_file=file))\nlightgbm_range = np.load(\"..\/input\/dba-lightgbm-model\/lightgbm_range.npy\")","b5a765aa":"def lightGBM(data, previous_state):\n    train = []\n    for i in lightgbm_range:\n        arr = data[-2-i:-1-i] - data[-3-i:-2-i]\n        train.append(arr.flatten())\n    train = np.array(train).transpose()\n    \n    test_data = pd.DataFrame(train[-38*26:])\n    \n    preds = []\n    for bst in bst_lst:\n        pred = bst.predict(test_data) # not test_dataset\n        preds.append(pred)\n    preds = np.array(preds)\n\n    preds_mean = np.mean(preds, axis=0)\n    current_forecast = np.reshape(preds_mean, (38,26), order='C') + data[-1]\n    \n    return current_forecast, None\n\npreds, actual = evaluate(lightGBM, None)\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids)\nplot_error_grid_difference(error_grids, error_grids_baseline)","cb505737":"results_dict[\"lightGBM_independent\"] = results\nresults","f5881f8b":"# load linear regression models\nlinear_regression_range = np.load(\"..\/input\/dba-linear-regression\/linear_regression_range.npy\")\nclf = pickle.load(open(\"..\/input\/dba-linear-regression\/linear_regression.pkl\", 'rb'))","00cb81a7":"def linear_regression(data, previous_state):\n    test = []\n    for i in linear_regression_range:\n        arr = data[-2-i:-1-i] - data[-3-i:-2-i]\n        test.append(arr.flatten())\n    test = np.array(test).transpose()\n    \n    preds = clf.predict(test)\n    preds = np.reshape(preds, (38,26), order='C')\n    current_forecast = preds + data[-1]\n    \n    return current_forecast, None\n\npreds, actual = evaluate(linear_regression, None)\nplot_predictions(preds, actual)\nerror_grids, results = calc_errors(preds, actual)\nplot_error_grids(error_grids, save=True)\nplot_error_grid_difference(error_grids, error_grids_baseline, save=True)","3784f206":"results_dict[\"linear_regression\"] = results\nresults","5b748b92":"def plot_predictions_compare(preds_arr, pred_names, actual, pixel_plotted=(10,10), plot_range=np.arange(120,135)):\n    fig,ax = plt.subplots(figsize=(15,7))\n    fig.autofmt_xdate()\n    xfmt = mdates.DateFormatter('%d-%m-%y %H:%M')\n    ax.set(ylabel=\"Population Count\")\n    ax.xaxis.set_major_formatter(xfmt)\n    ax.set_title(\"Population Count at {} over time, and its predictions\".format(pixel_plotted))\n    plt.plot(times[plot_range], actual[plot_range,pixel_plotted[0],pixel_plotted[1]], \n             color='blue', label=\"Actual\", lw=5)\n    plot_colors = [\"red\", \"purple\", \"green\"]\n    for preds, pred_name, color in zip(preds_arr, pred_names, plot_colors):\n        plt.plot(times[plot_range], preds[plot_range,pixel_plotted[0],pixel_plotted[1]], \n                 label=pred_name, color=color)\n    plt.legend()\n    plt.savefig(\"compare.svg\")\n    plt.show()\n    return None","3f155453":"preds_baseline, actual = evaluate(baseline_copylast, {\"i\": 1})\npreds_linear, actual = evaluate(linear_regression, None)\npreds_light, actual = evaluate(moving_average, None)\npreds_arr = preds_baseline, preds_linear, preds_light\npred_names = [\"Baseline\", \"Linear Regression\", \"Moving Average\"]","97a1b26c":"plot_predictions_compare(preds_arr, pred_names, actual)","70141034":"pd.DataFrame.from_dict(results_dict, orient='index')","74f0e419":"# Refined Methods","80567265":"# Summary of results","85bd7ec3":"**Square error**\n\nThe client suggested the use of square error to first evaluate the accuracy of our predictions.","14bd30d1":"# Loss metrics\nThese metrics evaluates the quality of our predictions.","74ec3123":"# Baseline Methods\nThese are methods that use a very little amount of information and code.","5ae35dba":"# Evaluation Module\nTo standardise evaluation, we will use an evaluation module to measure the quality of your predictions. We can add more measures as requested.\n\nYou are tasked to write a function, with a single input and output of the following specification: \n\n| Item   | Var name    | Expected type             |\n|:------:|-------------|--------------------------:|\n| Input  | past_slices | `(k, 38, 26)` numpy array |\n| Input  | state       | specified by you          |\n| Output | pred        | `(38, 26)` numpy array    |\n| Output | state       | specified by you          |\n\n<br>\n$$k > 7 \\times 24 \\times 6 = 1008$$\n<br>\n<br>\nThe evaluation function allow state to be passed, to reduce the required computation.","d67da91a":"## Linear regression","84647829":"## Moving average\nTake the average of last `n` elements. It does not perform better than baseline.","9d44f1d5":"## Exponential smoothing (alpha only)\nGiven the previous prediction $F$ and previous result $X$, the next prediction is \n$$\n(1-\\alpha) F + \\alpha X\n$$","37e0fad9":"## Taking the last frame\nAs suggested by the client, we predict the next frame to be equal to the last frame.","edbe7316":"# Assessing the methods","749e859a":"**Percentage direction correct**\n\nTo evaluate if your method correctly predict the change in an increase or decrease.","be274e69":"## Exponential smoothing (alpha and beta)\nGiven the previous prediction $F = A + B$ and previous result $X$, $A$ and $B$ is updated and the next prediction is $F' = A' + B'$\n$$\n\\begin{align}\nA' &= (1-\\alpha)F + \\alpha X\\\\\nB' &= (1-\\beta)B' + \\beta(A-A')\n\\end{align}\n$$","0acf97e1":"Objective - predict the heatmap of the next time frame.\n\n$28 \\times 24 \\times 6$ frames of $38 \\times 26$ cells","0eb8c922":"# Plotting code\nTo compare different methods","ae76cd39":"## Using the difference from the previous week\nWe add the difference obtained two frames in last week to the previous slice.","b83249ab":"**Square log error**\n\nSome may argue that predicting 100,000 when the actual number is 20,000 (you are off at five times) is worse than predicting 100,000 when the actual number is 200,000 (you are off at two times). However, a square error evaluation judges for latter prediction to be worse than the former because the squared difference is larger (80000^2 < 100000^2)\n\nThe use of logarithms normalises the scale. Predicting off by 2 times at 100 penalised similar if predicting off by 2 times at 100,000.\n\nFor prediction values and actual values that are zero, we convert it to one, to avoid mathematical errors.","46c41a01":"## LightGBM\nLightGBM is a random forest.","1fd14a67":"# Course Methods\nWe experiment with some methods taught in our course.","3020918a":"## Using the previous difference\nAdd the difference between the last and second-last frame to the last frame for the prediction."}}