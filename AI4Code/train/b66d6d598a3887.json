{"cell_type":{"df78f786":"code","2b98b12d":"code","9fea99b3":"code","40a32af9":"code","c98af037":"code","493da3de":"code","2045bcc3":"markdown","e3abaf9f":"markdown","036bbf8d":"markdown","7db21d29":"markdown","ed987f44":"markdown"},"source":{"df78f786":"%%writefile model.py\n\nimport torch\nimport numpy as np\n\n\nbce_logits = torch.nn.functional.binary_cross_entropy_with_logits\nmse = torch.nn.functional.mse_loss\n\n\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n        super().__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n    \n    def forward(self, x_in):\n        attn_out, _ = self.attn(x_in, x_in, x_in)\n        x = self.layernorm_1(x_in + attn_out)\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n        x = self.layernorm_2(x + ff_out)\n        return x\n\n\nclass TransformerAutoEncoder(torch.nn.Module):\n    def __init__(\n            self, \n            num_inputs, \n            n_cats, \n            n_nums, \n            hidden_size=1024, \n            num_subspaces=8,\n            embed_dim=128, \n            num_heads=8, \n            dropout=0, \n            feedforward_dim=512, \n            emphasis=.75, \n            task_weights=[10, 14],\n            mask_loss_weight=2,\n        ):\n        super().__init__()\n        assert hidden_size == embed_dim * num_subspaces\n        self.n_cats = n_cats\n        self.n_nums = n_nums\n        self.num_subspaces = num_subspaces\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.emphasis = emphasis\n        self.task_weights = np.array(task_weights) \/ sum(task_weights)\n        self.mask_loss_weight = mask_loss_weight\n\n        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        \n        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n\n    def divide(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n        return x\n\n    def combine(self, x):\n        batch_size = x.shape[1]\n        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n        return x\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        \n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        x = self.combine(x3)\n        \n        predicted_mask = self.mask_predictor(x)\n        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n        return (x1, x2, x3), (reconstruction, predicted_mask)\n\n    def split(self, t):\n        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n\n    def feature(self, x):\n        attn_outs, _ = self.forward(x)\n        return torch.cat([self.combine(x) for x in attn_outs], dim=1)\n\n    def loss(self, x, y, mask, reduction='mean'):\n        _, (reconstruction, predicted_mask) = self.forward(x)\n        x_cats, x_nums = self.split(reconstruction)\n        y_cats, y_nums = self.split(y)\n        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n\n        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none'))\n        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n\n        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n\n        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]\n\n\nclass SwapNoiseMasker(object):\n    def __init__(self, probas):\n        self.probas = torch.from_numpy(np.array(probas))\n\n    def apply(self, X):\n        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n        mask = (corrupted_X != X).float()\n        return corrupted_X, mask\n\n\ndef test_tf_encoder():\n    m = TransformerEncoder(4, 2, .1, 16)\n    x = torch.rand((32, 8))\n    x = x.reshape((32, 2, 4)).permute((1, 0, 2))\n    o = m(x)\n    assert o.shape == torch.Size([2, 32, 4])\n\n\ndef test_dae_model():\n    m = TransformerAutoEncoder(5, 2, 3, 16, 4, 4, 2, .1, 4, .75)\n    x = torch.cat([torch.randint(0, 2, (5, 2)), torch.rand((5, 3))], dim=1)\n    f = m.feature(x)\n    assert f.shape == torch.Size([5, 16 * 3])\n    loss = m.loss(x, x, (x > .2).float())\n\n\ndef test_swap_noise():\n    probas = [.2, .5, .8]\n    m = SwapNoiseMasker(probas)\n    diffs = []\n    for i in range(1000):\n        x = torch.rand((32, 3))\n        noisy_x, _ = m.apply(x)\n        diffs.append((x != noisy_x).float().mean(0).unsqueeze(0)) \n\n    print('specified : ', probas, ' - actual : ', torch.cat(diffs, 0).mean(0))\n\n\nif __name__ == '__main__':\n    test_tf_encoder()\n    test_dae_model()\n    test_swap_noise()","2b98b12d":"%%writefile data.py\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data import Dataset\n\n\ndef get_data():\n    train_data = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\n    test_data = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n    print(train_data.iloc[:, 11:-1])\n\n    X_nums = np.vstack([\n        train_data.iloc[:, 11:-1].to_numpy(),\n        test_data.iloc[:, 11:].to_numpy()\n    ])\n    X_nums = (X_nums - X_nums.mean(0)) \/ X_nums.std(0)\n\n    X_cat = np.vstack([\n        train_data.iloc[:, 1:11].to_numpy(),\n        test_data.iloc[:, 1:11].to_numpy()\n    ])\n    encoder = OneHotEncoder(sparse=False)\n    X_cat = encoder.fit_transform(X_cat)\n\n    X = np.hstack([X_cat, X_nums])\n    y = train_data['target'].to_numpy().reshape(-1, 1)\n\n    ##\n    X_test_nums = np.vstack([\n        train_data.iloc[:, 11:-1].to_numpy(),\n        test_data.iloc[:, 11:].to_numpy()\n    ])\n    X_test_nums = (X_test_nums - X_test_nums.mean(0)) \/ X_test_nums.std(0)\n\n    X_test_cat = np.vstack([\n        train_data.iloc[:, 1:11].to_numpy(),\n        test_data.iloc[:, 1:11].to_numpy()\n    ])\n    encoder = OneHotEncoder(sparse=False)\n    X_test_cat = encoder.fit_transform(X_test_cat)\n\n    X_test = np.hstack([X_test_cat, X_test_nums])\n\n    return X, y, X_cat.shape[1], X_nums.shape[1], X_test\n\nclass SingleDataset(Dataset):\n    def __init__(self, x, is_sparse=False):\n        self.x = x.astype('float32')\n        self.is_sparse = is_sparse\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, index):\n        x = self.x[index]\n        if self.is_sparse: x = x.toarray().squeeze()\n        return x    ","9fea99b3":"%%writefile requirements.txt\n\ntorch==1.7.0\nscikit-learn==0.23.2\nscipy==1.5.2\nnumpy==1.19.2\npandas==1.1.3","40a32af9":"%%writefile util.py\n# THIS FILE IS OK\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","c98af037":"%%writefile train.py\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom util import AverageMeter\nfrom model import SwapNoiseMasker, TransformerAutoEncoder\nfrom data import get_data, SingleDataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport dask.array as da\ngdcls = [\"f2891\", \"f881\", \"f942\", \"f1540\", \"f202\", \"f684\", \"f104\", \"f1753\", \"f2057\", \"f2532\", \"f304\", \"f2871\", \"f1223\", \"f1364\", \"f399\", \"f1018\", \"f505\", \"f1123\", \"f1195\", \"f1708\", \"f1020\", \"f840\", \"f2705\", \"f2014\", \"f2396\", \"f3046\", \"f1799\", \"f1907\", \"f2043\", \"f2486\", \"f2900\", \"f1959\", \"f782\", \"f1026\", \"f332\", \"f86\", \"f1456\", \"f2288\", \"f2384\", \"f2921\", \"f731\", \"f454\", \"f1014\", \"f1771\", \"f1802\", \"f1400\", \"f102\", \"f1861\", \"f2453\", \"f1500\", \"f2167\", \"f2252\", \"f1221\", \"f1782\", \"f854\", \"f2668\", \"f370\", \"f2823\", \"f283\", \"f2401\", \"f1867\", \"f2436\", \"f3037\", \"f1852\", \"f1112\", \"f2280\", \"f706\", \"f1367\", \"f1752\", \"f1169\", \"f2061\", \"f1854\", \"f2901\", \"f3031\", \"f2392\", \"f1403\", \"f2979\", \"f987\", \"f1246\", \"f71\", \"f2588\", \"f341\", \"f2295\", \"f1985\", \"f2646\", \"f427\", \"f933\", \"f1059\", \"f1300\", \"f1031\", \"f152\", \"f1906\", \"f328\", \"f168\", \"f1165\", \"f738\", \"f1674\", \"f3048\", \"f2808\", \"f2945\", \"f405\", \"f338\", \"f1418\", \"f1455\", \"f2153\", \"f1532\", \"f1419\", \"f1828\", \"f2015\", \"f1412\", \"f2771\", \"f1067\", \"f2080\", \"f1946\", \"f709\", \"f2047\", \"f2113\", \"f223\", \"f2117\", \"f1490\", \"f753\", \"f2718\", \"f1808\", \"f2505\", \"f83\", \"f1881\", \"f2224\", \"f888\", \"f546\", \"f2559\", \"f1530\", \"f3068\", \"f295\", \"f1096\", \"f1895\", \"f2027\", \"f711\", \"f3024\", \"f1567\", \"f350\", \"f2580\", \"f1679\", \"f786\", \"f1506\", \"f1577\", \"f2375\", \"f1101\", \"f2037\", \"f2406\", \"f974\", \"f124\", \"f543\", \"f2907\", \"f180\", \"f1348\", \"f1395\", \"f689\", \"f2731\", \"f1770\", \"f991\", \"f814\", \"f1225\", \"f2065\", \"f2521\", \"f2112\", \"f722\", \"f2666\", \"f2924\", \"f3030\", \"f1816\", \"f739\", \"f1541\", \"f843\", \"f958\", \"f1898\", \"f173\", \"f1072\", \"f2920\", \"f1551\", \"f2724\", \"f1899\", \"f2286\", \"f1288\", \"f2100\", \"f1341\", \"f1468\", \"f599\", \"f2033\", \"f2385\", \"f2506\", \"f868\", \"f280\", \"f245\", \"f2897\", \"f988\", \"f2873\", \"f1543\", \"f1610\", \"f114\", \"f741\", \"f103\", \"f609\", \"f281\", \"f860\", \"f2674\", \"f2931\", \"f1660\", \"f1172\", \"f2960\", \"f1260\", \"f3063\", \"f1083\", \"f2904\", \"f273\", \"f2393\", \"f798\", \"f1627\", \"f2589\", \"f1308\", \"f2877\", \"f986\", \"f1996\", \"f1774\", \"f2659\", \"f1038\", \"f1977\", \"f669\", \"f979\", \"f1920\", \"f2618\", \"f1681\", \"f646\", \"f1036\", \"f2478\", \"f1312\", \"f1046\", \"f214\", \"f749\", \"f377\", \"f242\", \"f2000\", \"f2557\", \"f1287\", \"f585\", \"f2843\", \"f1588\", \"f362\", \"f271\", \"f1890\", \"f2648\", \"f2319\", \"f2265\", \"f2158\", \"f2227\", \"f453\", \"f2176\", \"f1127\", \"f2081\", \"f2475\", \"f3028\", \"f2328\", \"f1508\", \"f642\", \"f1109\", \"f258\", \"f611\", \"f1751\", \"f1237\", \"f1756\", \"f2539\", \"f2990\", \"f826\", \"f1537\", \"f2410\", \"f2941\", \"f1857\", \"f1943\", \"f2507\", \"f619\", \"f1016\", \"f2363\", \"f162\", \"f329\", \"f1835\", \"f2946\", \"f2604\", \"f2497\", \"f203\", \"f1763\", \"f777\", \"f2625\", \"f869\", \"f1238\", \"f183\", \"f762\", \"f1547\", \"f2359\", \"f142\", \"f1844\", \"f1071\", \"f751\", \"f1838\", \"f366\", \"f513\", \"f1353\", \"f1580\", \"f1326\", \"f171\", \"f1824\", \"f795\", \"f2677\", \"f2511\", \"f2956\", \"f2021\", \"f1562\", \"f2445\", \"f1428\", \"f2603\", \"f542\", \"f1118\", \"f1107\", \"f962\", \"f2004\", \"f2519\", \"f1323\", \"f626\", \"f2672\", \"f1602\", \"f2082\", \"f2560\", \"f2769\", \"f2204\", \"f2772\", \"f2531\", \"f653\", \"f2572\", \"f2387\", \"f1608\", \"f2903\", \"f3049\", \"f1701\", \"f1286\", \"f2925\", \"f288\", \"f1796\", \"f1663\", \"f952\", \"f710\", \"f785\", \"f624\", \"f1849\", \"f2309\", \"f2608\", \"f1965\", \"f522\", \"f1930\", \"f2036\", \"f1571\", \"f528\", \"f159\", \"f1766\", \"f1232\", \"f2641\", \"f2767\", \"f1325\", \"f1471\", \"f2621\", \"f672\", \"f1132\", \"f1629\", \"f197\", \"f982\", \"f2256\", \"f2466\", \"f2801\", \"f331\", \"f3035\", \"f2682\", \"f1855\", \"f1887\", \"f2926\", \"f558\", \"f249\", \"f917\", \"f1167\", \"f1191\", \"f629\", \"f886\", \"f2315\", \"f145\", \"f1536\", \"f1454\", \"f1053\", \"f2514\", \"f2968\", \"f2454\", \"f1697\", \"f1662\", \"f1858\", \"f2379\", \"f578\", \"f643\", \"f1483\", \"f2683\", \"f191\", \"f1482\", \"f545\", \"f727\", \"f1710\", \"f1966\", \"f1671\", \"f2852\", \"f647\", \"f339\", \"f1889\", \"f337\", \"f155\", \"f492\", \"f2095\", \"f466\", \"f1643\", \"f2099\", \"f2196\", \"f1767\", \"f667\", \"f1659\", \"f1892\", \"f779\", \"f1631\", \"f1980\", \"f2263\", \"f2483\", \"f2967\", \"f662\", \"f1178\", \"f234\", \"f2361\", \"f276\", \"f850\", \"f3015\", \"f534\", \"f1934\", \"f2046\", \"f1655\", \"f153\", \"f817\", \"f1141\", \"f1700\", \"f531\", \"f2152\", \"f2389\", \"f3069\", \"f615\", \"f1557\", \"f1423\", \"f550\", \"f554\", \"f1726\", \"f2245\", \"f1642\", \"f1374\", \"f2407\", \"f160\", \"f837\", \"f2008\", \"f1493\", \"f2544\", \"f119\", \"f2438\", \"f553\", \"f548\", \"f2582\", \"f2257\", \"f1783\", \"f2128\", \"f2551\", \"f334\", \"f1208\", \"f1122\", \"f2400\", \"f1616\", \"f883\", \"f475\", \"f2552\", \"f568\", \"f2491\", \"f613\", \"f394\", \"f1998\", \"f413\", \"f1676\", \"f174\", \"f2349\", \"f2958\", \"f809\", \"f964\", \"f1661\", \"f2656\", \"f973\", \"f1699\", \"f207\", \"f289\", \"f1986\", \"f887\", \"f2168\", \"f725\", \"f2031\", \"f1984\", \"f2913\", \"f2728\", \"f1116\", \"f812\", \"f2254\", \"f2517\", \"f1583\", \"f2753\", \"f2009\", \"f601\", \"f1453\", \"f2273\", \"f2936\", \"f1429\", \"f185\", \"f2890\", \"f125\", \"f255\", \"f1624\", \"f1211\", \"f712\", \"f1495\", \"f2179\", \"f2755\", \"f2329\", \"f2297\", \"f1727\", \"f2807\", \"f1304\", \"f1314\", \"f383\", \"f166\", \"f2121\", \"f1680\", \"f1594\", \"f1203\", \"f3062\", \"f1486\", \"f1656\", \"f1276\", \"f2355\", \"f1872\", \"f1261\", \"f1869\", \"f594\", \"f688\", \"f1233\", \"f1075\", \"f1086\", \"f1510\", \"f3021\", \"f660\", \"f622\", \"f2750\", \"f2934\", \"f327\", \"f204\", \"f1049\", \"f2671\", \"f2729\", \"f714\", \"f569\", \"f2197\", \"f1297\", \"f1779\", \"f2575\", \"f507\", \"f2429\", \"f1950\", \"f743\", \"f510\", \"f2277\", \"f1915\", \"f617\", \"f2236\", \"f571\", \"f2866\", \"f1004\", \"f1695\", \"f769\", \"f1050\", \"f1603\", \"f2723\", \"f132\", \"f947\", \"f2944\", \"f1108\", \"f2605\", \"f147\", \"f1356\", \"f70\", \"f2571\", \"f2629\", \"f876\", \"f81\", \"f1896\", \"f2623\", \"f2837\", \"f1383\", \"f1291\", \"f1894\", \"f2127\", \"f2796\", \"f2654\", \"f1999\", \"f2460\", \"f2595\", \"f1469\", \"f2142\", \"f2714\", \"f519\", \"f1646\", \"f409\", \"f192\", \"f213\", \"f908\", \"f458\", \"f1968\", \"f2863\", \"f792\", \"f1988\", \"f906\", \"f1693\", \"f2327\", \"f2875\", \"f1401\", \"f1161\", \"f2882\", \"f535\", \"f116\", \"f221\", \"f368\", \"f844\", \"f1065\", \"f1289\", \"f2651\", \"f3056\", \"f2686\", \"f1121\", \"f1568\", \"f320\", \"f440\", \"f216\", \"f156\", \"f1997\", \"f902\", \"f1190\", \"f2403\", \"f1769\", \"f266\", \"f1750\", \"f2716\", \"f450\", \"f2402\", \"f2826\", \"f2984\", \"f471\", \"f323\", \"f1722\", \"f2547\", \"f2628\", \"f367\", \"f2378\", \"f1267\", \"f655\", \"f2115\", \"f1820\", \"f190\", \"f2247\", \"f2999\", \"f980\", \"f2326\", \"f2420\", \"f911\", \"f681\", \"f592\", \"f2181\", \"f2017\", \"f2479\", \"f400\", \"f1794\", \"f972\", \"f1931\", \"f2490\", \"f658\", \"f2928\", \"f685\", \"f965\", \"f312\", \"f2881\", \"f1704\", \"f1318\", \"f2793\", \"f1389\", \"f563\", \"f1119\", \"f1846\", \"f1487\", \"f195\", \"f2203\", \"f3003\", \"f316\", \"f2660\", \"f2702\", \"f778\", \"f2513\", \"f529\", \"f2452\", \"f1653\", \"f3026\", \"f759\", \"f2607\", \"f2098\", \"f1498\", \"f373\", \"f996\", \"f141\", \"f1975\", \"f345\", \"f608\", \"f2138\", \"f483\", \"f2492\", \"f644\", \"f121\", \"f2650\", \"f2616\", \"f925\", \"f379\", \"f2071\", \"f1386\", \"f2397\", \"f3065\", \"f3007\", \"f944\", \"f1060\", \"f439\", \"f1925\", \"f303\", \"f1196\", \"f2846\", \"f1644\", \"f1723\", \"f2905\", \"f2930\", \"f1668\", \"f1013\", \"f130\", \"f134\", \"f1306\", \"f633\", \"f1664\", \"f588\", \"f1041\", \"f742\", \"f2964\", \"f3012\", \"f730\", \"f1074\", \"f675\", \"f2543\", \"f360\", \"f2777\", \"f1733\", \"f215\", \"f3008\", \"f2562\", \"f1227\", \"f2481\", \"f2212\", \"f206\", \"f425\", \"f1305\", \"f803\", \"f2563\", \"f1130\", \"f2161\", \"f773\", \"f954\", \"f2089\", \"f2060\", \"f1806\", \"f455\", \"f1099\", \"f3055\", \"f931\", \"f1424\", \"f128\", \"f1559\", \"f747\", \"f1153\", \"f1877\", \"f2258\", \"f2524\", \"f157\", \"f855\", \"f1599\", \"f2208\", \"f1436\", \"f2370\", \"f2938\", \"f1103\", \"f2390\", \"f2016\", \"f1875\", \"f580\", \"f2710\", \"f1626\", \"f847\", \"f1623\", \"f2294\", \"f2842\", \"f1801\", \"f2024\", \"f1332\", \"f1859\", \"f935\", \"f2298\", \"f1836\", \"f2503\", \"f232\", \"f654\", \"f2272\", \"f1552\", \"f2872\", \"f178\", \"f1729\", \"f1990\", \"f2752\", \"f1928\", \"f1845\", \"f677\", \"f2186\", \"f1900\", \"f1815\", \"f2244\", \"f2358\", \"f485\", \"f961\", \"f2528\", \"f2548\", \"f2105\", \"f1544\", \"f846\", \"f77\", \"f1826\", \"f2300\", \"f2334\", \"f781\", \"f2456\", \"f2951\", \"f605\", \"f1871\", \"f2020\", \"f1703\", \"f1324\", \"f299\", \"f2833\", \"f1459\", \"f2815\", \"f1176\", \"f2816\", \"f875\", \"f2836\", \"f2079\", \"f506\", \"f1560\", \"f290\", \"f80\", \"f2764\", \"f349\", \"f810\", \"f2504\", \"f1021\", \"f957\", \"f2360\", \"f733\", \"f2029\", \"f874\", \"f2948\", \"f1204\", \"f1340\", \"f955\", \"f1039\", \"f967\", \"f1940\", \"f1728\", \"f606\", \"f2129\", \"f2289\", \"f79\", \"f344\", \"f824\", \"f1145\", \"f1230\", \"f1682\", \"f2049\", \"f1408\", \"f2058\", \"f1061\", \"f652\", \"f2320\", \"f1390\", \"f105\", \"f2912\", \"f2344\", \"f238\", \"f1215\", \"f286\", \"f897\", \"f1941\", \"f2701\", \"f2611\", \"f1303\", \"f3025\", \"f1823\", \"f1264\", \"f2546\", \"f2966\", \"f2662\", \"f2377\", \"f1515\", \"f2130\", \"f2310\", \"f538\", \"f537\", \"f851\", \"f1102\", \"f1445\", \"f2788\", \"f2888\", \"f101\", \"f1420\", \"f1747\", \"f3042\", \"f92\", \"f1526\", \"f2488\", \"f1017\", \"f1862\", \"f2190\", \"f839\", \"f149\", \"f1480\", \"f293\", \"f161\", \"f1245\", \"f2578\", \"f2917\", \"f429\", \"f1384\", \"f1539\", \"f729\", \"f1520\", \"f229\", \"f129\", \"f393\", \"f1969\", \"f924\", \"f2685\", \"f1217\", \"f2409\", \"f1535\", \"f2042\", \"f983\", \"f903\", \"f1466\", \"f721\", \"f1156\", \"f1672\", \"f861\", \"f88\", \"f718\", \"f2053\", \"f1574\", \"f2414\", \"f1410\", \"f976\", \"f1152\", \"f1028\", \"f1640\", \"f388\", \"f2215\", \"f2991\", \"f1860\", \"f536\", \"f82\", \"f915\", \"f3044\", \"f758\", \"f1008\", \"f1821\", \"f251\", \"f823\", \"f695\", \"f1666\", \"f1625\", \"f2131\", \"f877\", \"f1259\", \"f1776\", \"f2450\", \"f470\", \"f595\", \"f2045\", \"f768\", \"f1922\", \"f1281\", \"f1784\", \"f2766\", \"f2798\", \"f2910\", \"f713\", \"f2195\", \"f472\", \"f734\", \"f1897\", \"f2139\", \"f589\", \"f1772\", \"f1339\", \"f2792\", \"f1658\", \"f1375\", \"f1840\", \"f1618\", \"f2251\", \"f374\", \"f2933\", \"f2073\", \"f1139\", \"f1847\", \"f2689\", \"f859\", \"f636\", \"f900\", \"f2748\", \"f1702\", \"f2787\", \"f1893\", \"f896\", \"f2006\", \"f2136\", \"f2189\", \"f2647\", \"f73\", \"f1077\", \"f2635\", \"f1641\", \"f2231\", \"f2335\", \"f614\", \"f2717\", \"f2909\", \"f620\", \"f623\", \"f1692\", \"f1194\", \"f1904\", \"f2408\", \"f340\", \"f1622\", \"f1732\", \"f2044\", \"f2221\", \"f638\", \"f2859\", \"f1956\", \"f1527\", \"f2861\", \"f1371\", \"f2434\", \"f520\", \"f1266\", \"f1707\", \"f2839\", \"f1479\", \"f1868\", \"f1596\", \"f1667\", \"f2976\", \"f865\", \"f744\", \"f1027\", \"f2929\", \"f1236\", \"f1923\", \"f1358\", \"f737\", \"f919\", \"f1452\", \"f1908\", \"f300\", \"f1743\", \"f1043\", \"f2762\", \"f2665\", \"f2183\", \"f2484\", \"f2293\", \"f426\", \"f2323\", \"f1775\", \"f2446\", \"f1839\", \"f1698\", \"f2592\", \"f3006\", \"f995\", \"f1994\", \"f2743\", \"f1617\", \"f2461\", \"f326\", \"f2643\", \"f2732\", \"f1113\", \"f1320\", \"f1293\", \"f1995\", \"f562\", \"f533\", \"f1967\", \"f2613\", \"f1158\", \"f1313\", \"f3043\", \"f2118\", \"f2125\", \"f1886\", \"f2840\", \"f1842\", \"f2422\", \"f1402\", \"f656\", \"f259\", \"f745\", \"f2916\", \"f1407\", \"f410\", \"f682\", \"f2489\", \"f226\", \"f969\", \"f2499\", \"f1494\", \"f2166\", \"f1360\", \"f2011\", \"f279\", \"f2986\", \"f799\", \"f87\", \"f2096\", \"f2316\", \"f2374\", \"f2069\", \"f2274\", \"f2170\", \"f1462\", \"f1359\", \"f3058\", \"f514\", \"f2585\", \"f1442\", \"f2939\", \"f787\", \"f2581\", \"f2817\", \"f1426\", \"f1993\", \"f1333\", \"f2779\", \"f1734\", \"f1069\", \"f2093\", \"f2867\", \"f946\", \"f1564\", \"f1433\", \"f966\", \"f2365\", \"f2915\", \"f511\", \"f1632\", \"f2678\", \"f2783\", \"f2039\", \"f306\", \"f384\", \"f943\", \"f1147\", \"f2346\", \"f1441\", \"f1621\", \"f2818\", \"f746\", \"f2601\", \"f1691\", \"f2860\", \"f1033\", \"f1362\", \"f666\", \"f2206\", \"f1342\", \"f728\", \"f2141\", \"f2540\", \"f484\", \"f100\", \"f776\", \"f1285\", \"f2614\", \"f2155\", \"f690\", \"f1511\", \"f1528\", \"f1949\", \"f2791\", \"f1427\", \"f904\", \"f1720\", \"f1715\", \"f2947\", \"f419\", \"f2465\", \"f1673\", \"f716\", \"f1724\", \"f2568\", \"f1124\", \"f1392\", \"f1685\", \"f2526\", \"f2803\", \"f1807\", \"f2356\", \"f2518\", \"f793\", \"f2848\", \"f540\", \"f381\", \"f1611\", \"f871\", \"f2447\", \"f1214\", \"f1063\", \"f858\", \"f2442\", \"f1529\", \"f1328\", \"f498\", \"f2084\", \"f2239\", \"f610\", \"f311\", \"f247\", \"f1182\", \"f1711\", \"f84\", \"f2202\", \"f1686\", \"f1231\", \"f541\", \"f120\", \"f827\", \"f945\", \"f1755\", \"f2493\", \"f1058\", \"f443\", \"f1352\", \"f1630\", \"f2742\", \"f2763\", \"f1913\", \"f659\", \"f990\", \"f3050\", \"f126\", \"f579\", \"f1385\", \"f391\", \"f1201\", \"f700\", \"f131\", \"f1514\", \"f2970\", \"f2440\", \"f2950\", \"f2496\", \"f2332\", \"f1989\", \"f3023\", \"f2134\", \"f432\", \"f1185\", \"f1805\", \"f1414\", \"f1705\", \"f2220\", \"f2720\", \"f1357\", \"f1818\", \"f2178\", \"f1437\", \"f482\", \"f112\", \"f2829\", \"f1834\", \"f2856\", \"f467\", \"f1143\", \"f2457\", \"f1523\", \"f1942\", \"f1960\", \"f866\", \"f2064\", \"f2895\", \"f770\", \"f2405\", \"f1354\", \"f2148\", \"f1006\", \"f205\", \"f91\", \"f775\", \"f1128\", \"f1187\", \"f315\", \"f430\", \"f1078\", \"f1615\", \"f1005\", \"f354\", \"f616\", \"f2030\", \"f411\", \"f1830\", \"f1883\", \"f2171\", \"f296\", \"f318\", \"f2610\", \"f1874\", \"f369\", \"f2314\", \"f263\", \"f820\", \"f182\", \"f2278\", \"f1068\", \"f1970\", \"f1470\", \"f1888\", \"f2688\", \"f2324\", \"f1334\", \"f117\", \"f138\", \"f1274\", \"f2052\", \"f211\", \"f1131\", \"f2713\", \"f789\", \"f1458\", \"f1243\", \"f1553\", \"f1612\", \"f1235\", \"f1186\", \"f497\", \"f2844\", \"f2977\", \"f314\", \"f754\", \"f2982\", \"f2937\", \"f2802\", \"f227\", \"f1912\", \"f209\", \"f2267\", \"f3000\", \"f235\", \"f2173\", \"f756\", \"f691\", \"f856\", \"f1917\", \"f2191\", \"f1754\", \"f2962\", \"f3011\", \"f85\", \"f1788\", \"f1639\", \"f984\", \"f2425\", \"f1369\", \"f1443\", \"f2087\", \"f2609\", \"f1361\", \"f1768\", \"f1381\", \"f2811\", \"f1948\", \"f651\", \"f135\", \"f436\", \"f2622\", \"f2120\", \"f1193\", \"f1262\", \"f442\", \"f2074\", \"f2737\", \"f2602\", \"f441\", \"f2477\", \"f2751\", \"f977\", \"f864\", \"f3070\", \"f2085\", \"f841\", \"f999\", \"f2899\", \"f1337\", \"f438\", \"f1030\", \"f1413\", \"f1525\", \"f1851\", \"f2776\", \"f496\", \"f791\", \"f895\", \"f1138\", \"f1635\", \"f2804\", \"f2981\", \"f1637\", \"f2160\", \"f1825\", \"f627\", \"f1309\", \"f1885\", \"f1411\", \"f1349\", \"f1421\", \"f2062\", \"f2330\", \"f285\", \"f343\", \"f625\", \"f1678\", \"f1645\", \"f342\", \"f1935\", \"f559\", \"f1773\", \"f2973\", \"f934\", \"f1157\", \"f2645\", \"f2922\", \"f2997\", \"f852\", \"f1738\", \"f1450\", \"f468\", \"f1322\", \"f774\", \"f1955\", \"f241\", \"f1213\", \"f1278\", \"f1827\", \"f1609\", \"f1258\", \"f1464\", \"f291\", \"f2150\", \"f2624\", \"f1598\", \"f2733\", \"f2591\", \"f2985\", \"f724\", \"f1730\", \"f1417\", \"f2367\", \"f76\", \"f1085\", \"f1255\", \"f1398\", \"f1180\", \"f736\", \"f1811\", \"f1463\", \"f2806\", \"f2094\", \"f2270\", \"f1587\", \"f1310\", \"f2086\", \"f2707\", \"f2923\", \"f2066\", \"f905\", \"f2959\", \"f457\", \"f2070\", \"f2586\", \"f2739\", \"f2942\", \"f587\", \"f1742\", \"f3017\", \"f212\", \"f1749\", \"f2351\", \"f236\", \"f1757\", \"f1793\", \"f2472\", \"f2534\", \"f2156\", \"f267\", \"f240\", \"f1976\", \"f3014\", \"f2023\", \"f1590\", \"f2975\", \"f3027\", \"f1740\", \"f639\", \"f2719\", \"f2075\", \"f2109\", \"f920\", \"f2699\", \"f469\", \"f2542\", \"f2906\", \"f1226\", \"f1891\", \"f224\", \"f1709\", \"f1448\", \"f325\", \"f2911\", \"f2432\", \"f1302\", \"f802\", \"f2711\", \"f1555\", \"f2908\", \"f1558\", \"f1606\", \"f2114\", \"f1460\", \"f873\", \"f683\", \"f687\", \"f260\", \"f2841\", \"f1270\", \"f2207\", \"f1601\", \"f801\", \"f912\", \"f2219\", \"f1971\", \"f489\", \"f1938\", \"f604\", \"f1222\", \"f2698\", \"f555\", \"f1277\", \"f890\", \"f1146\", \"f1044\", \"f1179\", \"f1372\", \"f1275\", \"f1809\", \"f2754\", \"f1405\", \"f1758\", \"f2214\", \"f313\", \"f418\", \"f1856\", \"f835\", \"f2282\", \"f1550\", \"f1446\", \"f2545\", \"f2301\", \"f175\", \"f760\", \"f2169\", \"f2032\", \"f1718\", \"f797\", \"f2415\", \"f90\", \"f99\", \"f2235\", \"f2932\", \"f1251\", \"f2509\", \"f1978\", \"f2969\", \"f2268\", \"f499\", \"f3016\", \"f2849\", \"f1168\", \"f936\", \"f196\", \"f287\", \"f1549\", \"f829\", \"f1952\", \"f1136\", \"f641\", \"f2063\", \"f2154\", \"f449\", \"f2276\", \"f1268\", \"f1870\", \"f2620\", \"f2814\", \"f1475\", \"f717\", \"f849\", \"f1524\", \"f256\", \"f1748\", \"f2308\", \"f2525\", \"f424\", \"f2259\", \"f583\", \"f154\", \"f1717\", \"f1533\", \"f650\", \"f916\", \"f1380\", \"f1467\", \"f910\", \"f2884\", \"f2111\", \"f1366\", \"f2533\", \"f2902\", \"f2669\", \"f2261\", \"f1503\", \"f2172\", \"f2287\", \"f576\", \"f75\", \"f1229\", \"f1737\", \"f2162\", \"f1566\", \"f1425\", \"f2226\", \"f324\", \"f2250\", \"f1142\", \"f1149\", \"f2868\", \"f2243\", \"f1929\", \"f3039\", \"f502\", \"f1345\", \"f2005\", \"f1714\", \"f163\", \"f825\", \"f2696\", \"f2795\", \"f2971\", \"f593\", \"f1247\", \"f2199\", \"f867\", \"f3059\", \"f2198\", \"f1979\", \"f1346\", \"f1759\", \"f1378\", \"f2858\", \"f1344\", \"f358\", \"f523\", \"f2467\", \"f2892\", \"f412\", \"f2369\", \"f72\", \"f1115\", \"f2028\", \"f2123\", \"f2998\", \"f1764\", \"f1944\", \"f404\", \"f248\", \"f1592\", \"f1987\", \"f2537\", \"f800\", \"f2758\", \"f723\", \"f2940\", \"f526\", \"f1335\", \"f1879\", \"f389\", \"f2538\", \"f428\", \"f1250\", \"f1416\", \"f2302\", \"f278\", \"f1945\", \"f1791\", \"f1301\", \"f821\", \"f1517\", \"f1954\", \"f1447\", \"f992\", \"f2285\", \"f2893\", \"f1181\", \"f422\", \"f1134\", \"f1926\", \"f402\", \"f115\", \"f2734\", \"f1690\", \"f270\", \"f833\", \"f1832\", \"f3009\", \"f220\", \"f365\", \"f1200\", \"f2248\", \"f2357\", \"f2192\", \"f336\", \"f1581\", \"f1476\", \"f2821\", \"f1593\", \"f2296\", \"f198\", \"f1918\", \"f2090\", \"f2164\", \"f2494\", \"f2653\", \"f193\", \"f556\", \"f1126\", \"f1192\", \"f2462\", \"f1011\", \"f2373\", \"f884\", \"f2577\", \"f1166\", \"f302\", \"f1296\", \"f2040\", \"f2661\", \"f794\", \"f1431\", \"f2116\", \"f1485\", \"f143\", \"f2266\", \"f294\", \"f2413\", \"f2331\", \"f1154\", \"f2500\", \"f376\", \"f1841\", \"f2744\", \"f3036\", \"f2174\", \"f3019\", \"f815\", \"f1406\", \"f2774\", \"f1097\", \"f2229\", \"f1110\", \"f2421\", \"f2784\", \"f551\", \"f1982\", \"f150\", \"f735\", \"f937\", \"f2570\", \"f355\", \"f382\", \"f2564\", \"f1778\", \"f407\", \"f1512\", \"f2864\", \"f265\", \"f2756\", \"f2639\", \"f189\", \"f748\", \"f2927\", \"f2954\", \"f640\", \"f1924\", \"f1317\", \"f3067\", \"f2184\", \"f2110\", \"f1902\", \"f490\", \"f2573\", \"f2119\", \"f552\", \"f2919\", \"f2249\", \"f3071\", \"f390\", \"f298\", \"f2048\", \"f699\", \"f1045\", \"f2339\", \"f938\", \"f1765\", \"f2670\", \"f1387\", \"f971\", \"f2989\", \"f1397\", \"f461\", \"f1461\", \"f284\", \"f591\", \"f94\", \"f1159\", \"f1927\", \"f1054\", \"f1003\", \"f612\", \"f1336\", \"f2449\", \"f2318\", \"f2759\", \"f2352\", \"f676\", \"f237\", \"f1556\", \"f2216\", \"f2978\", \"f1170\", \"f1572\", \"f2735\", \"f451\", \"f1909\", \"f1253\", \"f2002\", \"f2831\", \"f1582\", \"f2026\", \"f1184\", \"f1283\", \"f137\", \"f1396\", \"f465\", \"f488\", \"f848\", \"f670\", \"f1481\", \"f1472\", \"f1546\", \"f2700\", \"f1205\", \"f1117\", \"f257\", \"f1829\", \"f2146\", \"f564\", \"f2038\", \"f2354\", \"f1683\", \"f1848\", \"f914\", \"f1284\", \"f2606\", \"f272\", \"f252\", \"f2343\", \"f780\", \"f804\", \"f1104\", \"f2101\", \"f508\", \"f970\", \"f2473\", \"f1814\", \"f486\", \"f406\", \"f1279\", \"f517\", \"f1239\", \"f201\", \"f95\", \"f930\", \"f254\", \"f1505\", \"f2418\", \"f2727\", \"f2584\", \"f2303\", \"f495\", \"f1804\", \"f2736\", \"f607\", \"f2078\", \"f2388\", \"f621\", \"f1290\", \"f544\", \"f1292\", \"f1209\", \"f2145\", \"f2593\", \"f2886\", \"f423\", \"f1575\", \"f1933\", \"f2399\", \"f1719\", \"f527\", \"f1789\", \"f1024\", \"f2305\", \"f1831\", \"f1365\", \"f1129\", \"f2317\", \"f408\", \"f1633\", \"f2612\", \"f169\", \"f228\", \"f123\", \"f1388\", \"f1843\", \"f1048\", \"f686\", \"f547\", \"f309\", \"f2336\", \"f2687\", \"f139\", \"f1150\", \"f1992\", \"f2523\", \"f1932\", \"f396\", \"f2424\", \"f2255\", \"f2485\", \"f3053\", \"f1833\", \"f3052\", \"f1220\", \"f2376\", \"f963\", \"f566\", \"f1605\", \"f106\", \"f1439\", \"f2108\", \"f2159\", \"f1790\", \"f2444\", \"f661\", \"f2371\", \"f1991\", \"f2834\", \"f993\", \"f187\", \"f118\", \"f2832\", \"f757\", \"f2007\", \"f704\", \"f1648\", \"f1650\", \"f1507\", \"f1019\", \"f1465\", \"f2785\", \"f2993\", \"f1351\", \"f1570\", \"f210\", \"f766\", \"f262\", \"f2498\", \"f2193\", \"f2987\", \"f1391\", \"f487\", \"f907\", \"f2182\", \"f474\", \"f1963\", \"f2874\", \"f462\", \"f1619\", \"f2738\", \"f882\", \"f2828\", \"f950\", \"f1578\", \"f2480\", \"f1087\", \"f2512\", \"f1298\", \"f1135\", \"f2437\", \"f1212\", \"f1786\", \"f1269\", \"f2520\", \"f1694\", \"f1761\", \"f941\", \"f2072\", \"f1502\", \"f1242\", \"f401\", \"f282\", \"f2550\", \"f2034\", \"f1597\", \"f515\", \"f1257\", \"f403\", \"f1509\", \"f1670\", \"f574\", \"f1545\", \"f1947\", \"f521\", \"f2820\", \"f3045\", \"f808\", \"f596\", \"f1817\", \"f1876\", \"f2246\", \"f2679\", \"f250\", \"f939\", \"f1880\", \"f1379\", \"f1972\", \"f1055\", \"f602\", \"f1037\", \"f133\", \"f1254\", \"f335\", \"f2269\", \"f2632\", \"f2233\", \"f1706\", \"f1911\", \"f1800\", \"f1981\", \"f2435\", \"f194\", \"f2617\", \"f186\", \"f1739\", \"f2333\", \"f2362\", \"f1022\", \"f1937\", \"f790\", \"f2137\", \"f386\", \"f1070\", \"f784\", \"f308\", \"f1614\", \"f2366\", \"f2419\", \"f2655\", \"f1422\", \"f2722\", \"f2018\", \"f2644\", \"f1091\", \"f2790\", \"f1677\", \"f2693\", \"f3001\", \"f375\", \"f1438\", \"f1338\", \"f657\", \"f2010\", \"f2107\", \"f975\", \"f693\", \"f2800\", \"f333\", \"f1079\", \"f1561\", \"f1496\", \"f1866\", \"f697\", \"f949\", \"f2059\", \"f2487\", \"f2013\", \"f371\", \"f1056\", \"f1781\", \"f1921\", \"f694\", \"f1155\", \"f1430\", \"f177\", \"f940\", \"f2157\", \"f918\", \"f415\", \"f1042\", \"f755\", \"f2640\", \"f1125\", \"f901\", \"f1591\", \"f3032\", \"f2845\", \"f929\", \"f2627\", \"f740\", \"f2885\", \"f2495\", \"f1504\", \"f892\", \"f2569\", \"f764\", \"f878\", \"f2835\", \"f261\", \"f584\", \"f444\", \"f1001\", \"f870\", \"f1760\", \"f2642\", \"f1522\", \"f1140\", \"f2225\", \"f2325\", \"f395\", \"f503\", \"f1105\", \"f2025\", \"f448\", \"f420\", \"f1409\", \"f1919\", \"f2876\", \"f2416\", \"f2530\", \"f2786\", \"f477\", \"f2451\", \"f1249\", \"f1256\", \"f1939\", \"f1183\", \"f573\", \"f2341\", \"f1319\", \"f351\", \"f719\", \"f1347\", \"f2041\", \"f2313\", \"f1330\", \"f2464\", \"f1665\", \"f1798\", \"f1905\", \"f2684\", \"f219\", \"f3020\", \"f2730\", \"f2697\", \"f981\", \"f1563\", \"f822\", \"f1492\", \"f2553\", \"f880\", \"f2555\", \"f2851\", \"f834\", \"f1745\", \"f1244\", \"f2143\", \"f516\", \"f2810\", \"f504\", \"f2961\", \"f1853\", \"f2088\", \"f1884\", \"f348\", \"f702\", \"f2587\", \"f1228\", \"f158\", \"f2827\", \"f1321\", \"f1822\", \"f2102\", \"f978\", \"f2430\", \"f572\", \"f1569\", \"f1735\", \"f2935\", \"f923\", \"f2345\", \"f763\", \"f310\", \"f2541\", \"f2757\", \"f319\", \"f597\", \"f1721\", \"f1810\", \"f1916\", \"f493\", \"f2163\", \"f2658\", \"f2476\", \"f2262\", \"f2391\", \"f1649\", \"f1015\", \"f2819\", \"f456\", \"f673\", \"f1025\", \"f1521\", \"f1151\", \"f1604\", \"f1451\", \"f140\", \"f2292\", \"f2765\", \"f387\", \"f3041\", \"f631\", \"f2799\", \"f1780\", \"f1082\", \"f2974\", \"f2527\", \"f2104\", \"f2217\", \"f1084\", \"f437\", \"f752\", \"f225\", \"f1311\", \"f2091\", \"f218\", \"f765\", \"f2223\", \"f2638\", \"f2474\", \"f2894\", \"f2680\", \"f1953\", \"f2536\", \"f2880\", \"f2879\", \"f1865\", \"f2439\", \"f3064\", \"f2782\", \"f452\", \"f3057\", \"f2242\", \"f1174\", \"f2337\", \"f1035\", \"f1114\", \"f107\", \"f1812\", \"f1964\", \"f2745\", \"f3005\", \"f500\", \"f2260\", \"f2133\", \"f771\", \"f1376\", \"f2097\", \"f3038\", \"f2077\", \"f2554\", \"f2855\", \"f1689\", \"f3022\", \"f1548\", \"f3029\", \"f648\", \"f1595\", \"f2281\", \"f1047\", \"f509\", \"f2649\", \"f2185\", \"f2781\", \"f222\", \"f1177\", \"f1497\", \"f179\", \"f1878\", \"f2463\", \"f2522\", \"f2694\", \"f2675\", \"f1240\", \"f1795\", \"f494\", \"f1350\", \"f1474\", \"f570\", \"f433\", \"f2180\", \"f715\", \"f1252\", \"f2404\", \"f2637\", \"f307\", \"f2382\", \"f989\", \"f1534\", \"f1434\", \"f2980\", \"f1363\", \"f2381\", \"f2165\", \"f2122\", \"f2549\", \"f832\", \"f1331\", \"f628\", \"f2574\", \"f3002\", \"f2234\", \"f932\", \"f2149\", \"f1863\", \"f813\", \"f1903\", \"f1199\", \"f2433\", \"f1973\", \"f1901\", \"f1457\", \"f549\", \"f828\", \"f2870\", \"f2850\", \"f3054\", \"f1961\", \"f788\", \"f97\", \"f1271\", \"f512\", \"f2482\", \"f2535\", \"f680\", \"f1435\", \"f2996\", \"f347\", \"f2283\", \"f2515\", \"f1206\", \"f985\", \"f968\", \"f2768\", \"f268\", \"f2780\", \"f2054\", \"f899\", \"f463\", \"f2898\", \"f231\", \"f1579\", \"f2348\", \"f2501\", \"f2706\", \"f1299\", \"f1316\", \"f172\", \"f398\", \"f2051\", \"f1736\", \"f1651\", \"f836\", \"f378\", \"f3060\", \"f2056\", \"f1327\", \"f913\", \"f1607\", \"f2290\", \"f1531\", \"f2237\", \"f321\", \"f122\", \"f2561\", \"f2955\", \"f1837\", \"f1440\", \"f2132\", \"f275\", \"f2558\", \"f2103\", \"f1432\", \"f127\", \"f1491\", \"f1373\", \"f2003\", \"f2918\", \"f2957\", \"f927\", \"f678\", \"f1265\", \"f2368\", \"f1910\", \"f2240\", \"f705\", \"f2383\", \"f3018\", \"f2516\", \"f2983\", \"f2271\", \"f148\", \"f1516\", \"f2307\", \"f2443\", \"f692\", \"f1914\", \"f146\", \"f2380\", \"f1565\", \"f1813\", \"f481\", \"f525\", \"f1173\", \"f518\", \"f960\", \"f2529\", \"f1144\", \"f1377\", \"f1797\", \"f2857\", \"f2636\", \"f1080\", \"f2304\", \"f2338\", \"f1415\", \"f2797\", \"f2566\", \"f301\", \"f1197\", \"f1120\", \"f1688\", \"f2749\", \"f2284\", \"f2264\", \"f2241\", \"f1449\", \"f1777\", \"f1636\", \"f701\", \"f1634\", \"f2988\", \"f586\", \"f479\", \"f2994\", \"f1489\", \"f1064\", \"f891\", \"f491\", \"f1234\", \"f2350\", \"f478\", \"f1100\", \"f831\", \"f2789\", \"f1248\", \"f885\", \"f1382\", \"f2340\", \"f1164\", \"f679\", \"f170\", \"f560\", \"f2022\", \"f1034\", \"f2576\", \"f1076\", \"f3061\", \"f2953\", \"f2253\", \"f2151\", \"f951\", \"f2725\", \"f879\", \"f2615\", \"f838\", \"f1073\", \"f1669\", \"f2398\", \"f953\", \"f1936\", \"f2140\", \"f926\", \"f2594\", \"f1864\", \"f1307\", \"f2854\", \"f113\", \"f618\", \"f796\", \"f2949\", \"f352\", \"f524\", \"f1792\", \"f330\", \"f2761\", \"f2556\", \"f3013\", \"f2426\", \"f889\", \"f2634\", \"f2663\", \"f3040\", \"f2865\", \"f1394\", \"f1095\", \"f2106\", \"f2364\", \"f1499\", \"f1040\", \"f2342\", \"f2321\", \"f664\", \"f1052\", \"f246\", \"f1370\", \"f1098\", \"f1762\", \"f909\", \"f2394\", \"f151\", \"f2395\", \"f575\", \"f446\", \"f819\", \"f199\", \"f674\", \"f1092\", \"f2822\", \"f732\", \"f98\", \"f385\", \"f184\", \"f1962\", \"f1111\", \"f1002\", \"f665\", \"f726\", \"f2279\", \"f2427\", \"f417\", \"f720\", \"f1477\", \"f208\", \"f1198\", \"f200\", \"f1958\", \"f3051\", \"f2508\", \"f994\", \"f2963\", \"f2805\", \"f959\", \"f2809\", \"f2896\", \"f577\", \"f2441\", \"f2001\", \"f2232\", \"f2083\", \"f353\", \"f1162\", \"f2952\", \"f872\", \"f3047\", \"f1803\", \"f830\", \"f2431\", \"f2222\", \"f532\", \"f144\", \"f2631\", \"f707\", \"f1404\", \"f997\", \"f1573\", \"f2238\", \"f1542\", \"f108\", \"f431\", \"f708\", \"f2889\", \"f1295\", \"f2458\", \"f2299\", \"f111\", \"f1731\", \"f2175\", \"f2695\", \"f269\", \"f1090\", \"f414\", \"f1787\", \"f2712\", \"f2726\", \"f634\", \"f501\", \"f1282\", \"f217\", \"f2211\", \"f2740\", \"f2067\", \"f1399\", \"f645\", \"f1654\", \"f863\", \"f2126\", \"f816\", \"f2187\", \"f421\", \"f1819\", \"f2076\", \"f1066\", \"f1224\", \"f363\", \"f1093\", \"f93\", \"f435\", \"f1094\", \"f1148\", \"f1133\", \"f1538\", \"f842\", \"f2565\", \"f2664\", \"f894\", \"f1355\", \"f632\", \"f253\", \"f1029\", \"f317\", \"f346\", \"f233\", \"f1263\", \"f2455\", \"f167\", \"f530\", \"f2830\", \"f459\", \"f110\", \"f1850\", \"f1657\", \"f2068\", \"f2657\", \"f2673\", \"f1081\", \"f2135\", \"f445\", \"f2596\", \"f1207\", \"f2194\", \"f1106\", \"f2708\", \"f460\", \"f2579\", \"f2681\", \"f2055\", \"f2470\", \"f1218\", \"f176\", \"f1280\", \"f1219\", \"f1202\", \"f1272\", \"f845\", \"f2633\", \"f2417\", \"f561\", \"f2746\", \"f2012\", \"f853\", \"f1785\", \"f244\", \"f581\", \"f1628\", \"f480\", \"f243\", \"f1647\", \"f2459\", \"f2448\", \"f1713\", \"f1188\", \"f557\", \"f1012\", \"f2794\", \"f2825\", \"f806\", \"f1613\", \"f590\", \"f2972\", \"f277\", \"f767\", \"f380\", \"f2322\", \"f2218\", \"f1189\", \"f2386\", \"f2428\", \"f1744\", \"f1062\", \"f2704\", \"f2667\", \"f1684\", \"f2869\", \"f818\", \"f2995\", \"f2312\", \"f2747\", \"f96\", \"f750\", \"f2770\", \"f1137\", \"f2773\", \"f671\", \"f372\", \"f2147\", \"f2760\", \"f1484\", \"f2144\", \"f1696\", \"f2812\", \"f464\", \"f3004\", \"f2411\", \"f1586\", \"f2709\", \"f2943\", \"f2838\", \"f416\", \"f2692\", \"f2311\", \"f3033\", \"f565\", \"f637\", \"f1554\", \"f359\", \"f274\", \"f1171\", \"f3066\", \"f1478\", \"f898\", \"f1057\", \"f1444\", \"f893\", \"f2469\", \"f1620\", \"f1873\", \"f2914\", \"f630\", \"f1638\", \"f2019\", \"f1584\", \"f1000\", \"f696\", \"f2210\", \"f357\", \"f1746\", \"f2347\", \"f2703\", \"f1089\", \"f2201\", \"f2652\", \"f78\", \"f1241\", \"f3010\", \"f447\", \"f2423\", \"f2813\", \"f2887\", \"f928\", \"f2471\", \"f1393\", \"f1368\", \"f1009\", \"f2630\", \"f239\", \"f805\", \"f598\", \"f761\", \"f1010\", \"f1163\", \"f649\", \"f1160\", \"f1675\", \"f2124\", \"f772\", \"f434\", \"f1576\", \"f1983\", \"f297\", \"f2510\", \"f1513\", \"f2583\", \"f1519\", \"f1951\", \"f1088\", \"f1741\", \"f181\", \"f2626\", \"f2778\", \"f292\", \"f1725\", \"f2590\", \"f807\", \"f1585\", \"f109\", \"f74\", \"f1007\", \"f922\", \"f2824\", \"f582\", \"f600\", \"f2050\", \"f2209\", \"f998\", \"f2412\", \"f1687\", \"f1175\", \"f2213\", \"f2878\", \"f2676\", \"f2599\", \"f2230\", \"f2291\", \"f635\", \"f1216\", \"f1473\", \"f2177\", \"f2598\", \"f2691\", \"f230\", \"f2715\", \"f1882\", \"f2690\", \"f2597\", \"f1023\", \"f1652\", \"f2092\", \"f2502\", \"f1032\", \"f2847\", \"f1957\", \"f2862\", \"f397\", \"f2035\", \"f1974\", \"f668\", \"f2228\", \"f2619\", \"f356\", \"f2353\", \"f2721\", \"f164\", \"f322\", \"f364\", \"f361\", \"f698\", \"f956\", \"f567\", \"f663\", \"f703\", \"f2965\", \"f2600\", \"f811\", \"f165\", \"f1294\", \"f2883\", \"f3034\", \"f2306\", \"f476\", \"f188\", \"f1600\", \"f136\", \"f2200\", \"f2567\", \"f2853\", \"f2775\", \"f1589\", \"f2275\", \"f2741\", \"f603\", \"f2205\", \"f1329\", \"f1488\", \"f2188\", \"f2992\", \"f1343\", \"f1518\", \"f1273\", \"f89\", \"f857\", \"f862\", \"f783\", \"f392\", \"f948\", \"f1315\", \"f1712\", \"f1210\", \"f1501\", \"f1716\", \"f539\", \"f1051\", \"f2372\", \"f921\", \"f305\", \"f473\", \"f264\", \"f2468\"]\n\n#####################\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n#####################\n# Hyper-params\nmodel_params = dict(\n    hidden_size=1024,\n    num_subspaces=8,\n    embed_dim=128,\n    num_heads=8,\n    dropout=0,\n    feedforward_dim=512,\n    emphasis=.75,\n    mask_loss_weight=2\n)\nbatch_size = 384\ninit_lr = 3e-4\nlr_decay = .998\nmax_epochs = 1 #2001\n\nrepeats = [  2,  2,  2,  4,  4,  4,  8,  8,  7, 15,  14]\nprobas =  [.95, .4, .7, .9, .9, .9, .9, .9, .9, .9, .25]\nswap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n\n#  get data\nX, Y, n_cats, n_nums, X_test = get_data()\n\ntrain_dl = DataLoader(\n    dataset=SingleDataset(X),\n    batch_size=batch_size,\n    shuffle=True,\n    pin_memory=True,\n    drop_last=True\n)\n\n# setup model\nmodel = TransformerAutoEncoder(\n    num_inputs=X.shape[1],\n    n_cats=n_cats,\n    n_nums=n_nums,\n    **model_params\n).cuda()\nmodel_checkpoint = 'model_checkpoint.pth'\n\n# print(model)\n\nnoise_maker = SwapNoiseMasker(swap_probas)\noptimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n\n# train model\nfor epoch in range(max_epochs):\n    t0 = datetime.now()\n    model.train()\n    meter = AverageMeter()\n    for i, x in enumerate(train_dl):\n        x = x.cuda()\n        x_corrputed, mask = noise_maker.apply(x)\n        optimizer.zero_grad()\n        loss = model.loss(x_corrputed, x, mask)\n        loss.backward()\n        optimizer.step()\n\n        meter.update(loss.detach().cpu().numpy())\n\n    delta = (datetime.now() - t0).seconds\n    scheduler.step()\n    print('\\r epoch {:5d} - loss {:.6f} - {:4.6f} sec per epoch'.format(epoch, meter.avg, delta), end='')\n\ntorch.save({\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"model\": model.state_dict()\n    }, model_checkpoint\n)\nmodel_state = torch.load(model_checkpoint)\nmodel.load_state_dict(model_state['model'])\n\n# extract features\ndl = DataLoader(dataset=SingleDataset(X), batch_size=1024, shuffle=False, pin_memory=True, drop_last=False)\nfeatures = []\nmodel.eval()\nwith torch.no_grad():\n    for x in dl:\n        features.append(model.feature(x.cuda()).detach().cpu().numpy())\nfeatures = da.vstack(features)\n\n# downstream supervised regressor\nalpha = 1250 # 1000\n# X = features[:300_000, :]\nlenN=3000\nX = features[:lenN, :]\nY = Y[:lenN, :]\nfinal_test_predictions = []\n\nscores = []\nfinal_valid_predictions = {}\nfor train_idx, valid_idx in KFold().split(X, Y):\n    print(\"\\nSOS\\n\")\n    model = XGBRegressor(\n        # random_state=fold,\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor=\"gpu_predictor\"\n    )\n    xtest = df_test.copy()\n\n    model.fit(X[train_idx], Y[train_idx])\n    preds_valid = model.predict(X[valid_idx])\n    # test_preds = model.predict(X_test)\n    print(X_test)\n    final_test_predictions.append(test_preds[gdcls])\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(Y[valid_idx], preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\nprint(np.mean(scores))\n\n# np.save('dae_features.npy', features)","493da3de":"!python train.py","2045bcc3":"## Train.py","e3abaf9f":"## Util.py","036bbf8d":"## Data.py","7db21d29":"Link : https:\/\/github.com\/ryancheunggit\/Denoise-Transformer-AutoEncoder\n","ed987f44":"## Requirements.txt"}}