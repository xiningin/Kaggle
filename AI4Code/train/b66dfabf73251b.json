{"cell_type":{"e4aefc86":"code","4f0e2d9a":"code","551e99f7":"code","ac133ea3":"code","174afc32":"code","ed218b94":"code","e90c0a70":"code","974f19a4":"code","3aee4647":"code","b468d45f":"code","00534a4a":"code","1aa48d4a":"code","843fd767":"code","39b0c440":"code","a92e14d6":"code","179627b8":"code","9404c073":"code","2702d2c4":"code","c7ac06e9":"code","7fe4fa45":"markdown","c2bb9853":"markdown","75a9320d":"markdown","808d55fa":"markdown","3a761778":"markdown","467a7c1e":"markdown","4a3c9aea":"markdown","d4892f14":"markdown","7ffe27d9":"markdown","da4e09f2":"markdown","3c257a4b":"markdown","04903a3c":"markdown","668bff3f":"markdown","4bd266cf":"markdown","7acf3b6e":"markdown","5e304ec7":"markdown"},"source":{"e4aefc86":"import os\nimport sys\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss, confusion_matrix\nimport matplotlib.pyplot as plt\nimport random\nimport warnings\n\nwarnings.filterwarnings('ignore') # Supress Warnings.\nnp.random.seed(100)\nLEVEL = 'level_1'","4f0e2d9a":"def read_all(folder_path, key_prefix=\"\"):\n    '''\n    It returns a dictionary with 'file names' as keys and 'flattened image arrays' as values.\n    '''\n    print(\"Reading:\")\n    images = {}\n    files = os.listdir(folder_path)\n    for i, file_name in tqdm_notebook(enumerate(files), total=len(files)):\n        file_path = os.path.join(folder_path, file_name)\n        image_index = key_prefix + file_name[:-4]\n        image = Image.open(file_path)\n        image = image.convert(\"L\")\n        images[image_index] = np.array(image.copy()).flatten()\n        image.close()\n    return images","551e99f7":"languages = ['ta', 'hi', 'en']\n\nimages_train = read_all(\"..\/input\/level_1_train\/\"+LEVEL+\"\/\"+\"background\", key_prefix='bgr_') \nfor language in languages:\n images_train.update(read_all(\"..\/input\/level_1_train\/\"+LEVEL+\"\/\"+language, key_prefix=language+\"_\" ))\nprint(len(images_train))\n\nimages_test = read_all(\"..\/input\/level_1_test\/kaggle_\"+LEVEL, key_prefix='') \nprint(len(images_test))","ac133ea3":"list(images_test.keys())[:5] # View first five keys for images in the test set","174afc32":"X_train = []\nY_train = []\nfor key, value in images_train.items():\n    X_train.append(value)\n    if key[:4] == \"bgr_\":\n        Y_train.append(0)\n    else:\n        Y_train.append(1)\n\nID_test = []\nX_test = []\nfor key, value in images_test.items():\n  ID_test.append(int(key))\n  X_test.append(value)\n  \nX_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_test = np.array(X_test)\n\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape)","ed218b94":"scaler = StandardScaler()\nX_scaled_train = scaler.fit_transform(X_train)\nX_scaled_test = scaler.transform(X_test)","e90c0a70":"class SigmoidNeuron:\n  \n  def __init__(self):\n    self.w = None\n    self.b = None\n    \n  def perceptron(self, x):\n    return np.dot(x, self.w.T) + self.b\n  \n  def sigmoid(self, x):\n    return 1.0\/(1.0 + np.exp(-x))\n  \n  def grad_w_mse(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    return (y_pred - y) * y_pred * (1 - y_pred) * x\n  \n  def grad_b_mse(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    return (y_pred - y) * y_pred * (1 - y_pred)\n  \n  def grad_w_ce(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    if y == 0:\n      return y_pred * x\n    elif y == 1:\n      return -1 * (1 - y_pred) * x\n    else:\n      raise ValueError(\"y should be 0 or 1\")\n    \n  def grad_b_ce(self, x, y):\n    y_pred = self.sigmoid(self.perceptron(x))\n    if y == 0:\n      return y_pred \n    elif y == 1:\n      return -1 * (1 - y_pred)\n    else:\n      raise ValueError(\"y should be 0 or 1\")\n  \n  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn=\"mse\", display_loss=False):\n    \n    # initialise w, b\n    if initialise:\n      self.w = np.random.randn(1, X.shape[1])\n      self.b = 0\n      \n    if display_loss:\n      loss = {}\n    \n    for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      dw = 0\n      db = 0\n      for x, y in zip(X, Y):\n        if loss_fn == \"mse\":\n          dw += self.grad_w_mse(x, y)\n          db += self.grad_b_mse(x, y) \n        elif loss_fn == \"ce\":\n          dw += self.grad_w_ce(x, y)\n          db += self.grad_b_ce(x, y)\n      self.w -= learning_rate * dw\n      self.b -= learning_rate * db\n      \n      if display_loss:\n        Y_pred = self.sigmoid(self.perceptron(X))\n        if loss_fn == \"mse\":\n          loss[i] = mean_squared_error(Y, Y_pred)\n        elif loss_fn == \"ce\":\n          loss[i] = log_loss(Y, Y_pred)\n    \n    if display_loss:\n      plt.plot(loss.values())\n      plt.xlabel('Epochs')\n      if loss_fn == \"mse\":\n        plt.ylabel('Mean Squared Error')\n      elif loss_fn == \"ce\":\n        plt.ylabel('Log Loss')\n      plt.show()\n      \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.sigmoid(self.perceptron(x))\n      Y_pred.append(y_pred)\n    return np.array(Y_pred)","974f19a4":"sn_mse = SigmoidNeuron()\nsn_mse.fit(X_scaled_train, Y_train, epochs=100, learning_rate=0.1, loss_fn=\"mse\", display_loss=True)","3aee4647":"sn_ce = SigmoidNeuron()\nsn_ce.fit(X_scaled_train, Y_train, epochs=100, learning_rate=0.1, loss_fn=\"ce\", display_loss=True)","b468d45f":"# Calculate accuracy\ndef cal_accuracy(sn, X, y, prob_thres=0.5):\n  y_pred = sn.predict(X)\n  y_pred_binarised = (y_pred >= prob_thres).astype(\"int\").ravel()\n  accuracy = accuracy_score(y_pred_binarised, y)\n  return accuracy","00534a4a":"print('Train Accuracy when loss function is mean squared error: ', cal_accuracy(sn_mse, X_scaled_train, Y_train))\nprint('Train Accuracy when loss function is cross-entropy: ', cal_accuracy(sn_ce, X_scaled_train, Y_train))","1aa48d4a":"def cross_val(estimator, X, y, k=5, epochs=100, lr=0.1, loss_fn='ce', prob_thres=0.5, display_loss=True):\n\n    # Split the data in k-folds\n    population = range(X.shape[0]) # Number of rows in X_train\n    rand_indices = random.sample(population, X.shape[0])\n    k = k # k-splits.\n    X_split = np.array(np.split(X[rand_indices], k)) # k random splits of X\n    y_split = np.array(np.split(y[rand_indices], k)) # k random splits of y\n    train_acc = np.empty((0)); test_acc = np.empty((0)) # Here we store the accuracy for different folds\n    \n    # Fit and predict on k different train and test folds\n    for i in range(k):\n        \n        # Create train and test sets for different folds\n        k_X_test = X_split[i]; k_y_test = y_split[i] # cross-validation test set\n        train_indices = list(range(k)); train_indices.remove(i)\n        k_X_train = np.empty((0, k_X_test.shape[1])); k_y_train = np.empty((0)) \n        for j in train_indices:\n            k_X_train = np.vstack((k_X_train, X_split[j])); k_y_train = np.hstack((k_y_train, y_split[j])) # train set for k-fold\n\n        # Fit data to model.\n        estimator.fit(k_X_train, k_y_train, epochs=epochs, learning_rate=lr, loss_fn=loss_fn, display_loss=display_loss)\n        # Calculate the train and test accuracy on validation set\n        train_acc = np.hstack((train_acc, cal_accuracy(estimator, k_X_train, k_y_train, prob_thres=prob_thres)))\n        test_acc = np.hstack((test_acc, cal_accuracy(estimator, k_X_test, k_y_test, prob_thres=prob_thres)))\n        \n    print('Mean Train Accuracy: ', np.mean(train_acc))\n    print('Mean Test Accuracy: ', np.mean(test_acc))\n    return([np.mean(train_acc), np.mean(test_acc)])","843fd767":"cross_val(SigmoidNeuron(), X_scaled_train, Y_train, epochs=200, lr=0.5, display_loss=False)","39b0c440":"cross_val(SigmoidNeuron(), X_scaled_train, Y_train, epochs=200, lr=0.5, loss_fn='mse', display_loss=False)","a92e14d6":"cutoff_df = pd.DataFrame(columns=['prob', 'train_acc', 'test_acc'])\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i, prob in enumerate(num):\n    acc = cross_val(SigmoidNeuron(), X_scaled_train, Y_train, epochs=200, lr=0.5, prob_thres=prob, display_loss=False) # get train and test accuracies\n    cutoff_df = cutoff_df.append({'prob': prob, 'train_acc': acc[0], 'test_acc': acc[1]}, ignore_index=True)","179627b8":"# Different probabilites and their train and test accuracies\nprint(cutoff_df)\nprint('Row with maximum test accuracy:\\n', cutoff_df.iloc[cutoff_df[['test_acc']].idxmax()])\ncutoff_df.plot.line(x='prob', y=['train_acc', 'test_acc'])\nplt.show()","9404c073":"# Do train-test split\nX_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_scaled_train, Y_train, train_size=0.7, test_size=0.3, random_state=123, stratify=Y_train)","2702d2c4":"sn = SigmoidNeuron()\nsn.fit(X_train_f, y_train_f, epochs=200, learning_rate=0.5, loss_fn=\"ce\", display_loss=True)\nprint('Train Accuracy: ', cal_accuracy(sn, X_train_f, y_train_f, prob_thres=0.4))\nprint('Test Accuracy: ', cal_accuracy(sn, X_test_f, y_test_f, prob_thres=0.4))","c7ac06e9":"y_pred_kgl = sn.predict(X_scaled_test) # kgl stands for kaggle here\ny_pred_binarised_kgl = (y_pred_kgl >= 0.4).astype(\"int\").ravel()\n\nsubmission = {}\nsubmission['ImageId'] = ID_test\nsubmission['Class'] = y_pred_binarised_kgl\n\nsubmission = pd.DataFrame(submission)\nsubmission = submission[['ImageId', 'Class']]\nsubmission = submission.sort_values(['ImageId'])\nsubmission.to_csv(\"submisision.csv\", index=False)","7fe4fa45":"# Instantiate and Fit Model.\n1. Loss function: Mean-squared error.[](http:\/\/)","c2bb9853":"# Import required Libraries.","75a9320d":"<hr>","808d55fa":"2. Loss function: Cross-entropy.","3a761778":"# k-Fold Cross-Validation.","467a7c1e":"# Create train and test set.","4a3c9aea":"<hr>\n# Make predictions on Kaggle test set.","d4892f14":"<hr>","7ffe27d9":"<hr>\n# Prepare Final Model.","da4e09f2":"# Create Model.","3c257a4b":"# Read image files to arrays.","04903a3c":"# Scale the data.","668bff3f":"<hr>","4bd266cf":"<hr>\n# Selecting Probability Threshold.","7acf3b6e":"**5-fold cross validation.**\n1.  When running 200 epochs at a learning rate of 0.5, loss funcion set to 'cross-entropy' and probability threshold set to 0.5, I am getting a mean accuracy of 100% on train set and 97 to 98% accuracy on test set when doing cross-validation.<br>\n2. Keeping all the other parameters constant and changing only loss function to 'mean squared error' leads to a train accuracy of around 96 to 97% and test accuracy of around 95%.<br><br>\n*When observing the above metrics, it seems that for same number of epochs and learning rate cross-entropy error shows better performance than means squared error.*<br>\n*Also not that, these accuracy scores will be different every time you run `cross_val` function, because k-splits are done randomly.*","5e304ec7":"Maximum test accuracy is obtained at a probability threshold of 0.4. Now, we need to prepare a final model with this threshold."}}