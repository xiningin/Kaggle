{"cell_type":{"22b5042c":"code","e78490b8":"code","7ccd503a":"code","bbdec692":"code","6529d5f1":"code","fd497190":"code","114782ae":"code","bc7805ef":"code","d45b0723":"code","1bb2a25a":"code","cc177280":"code","8f0461a9":"code","0a0e602f":"code","1d686324":"code","331bffd1":"code","e1c004c8":"code","10ee008b":"code","91d04305":"code","c3a0f8f2":"code","5fa06b44":"code","18733861":"code","a7f20272":"code","e16e0d34":"code","55cd96d6":"code","04d61bb5":"code","ebd99525":"code","65044094":"code","e4dee6e7":"code","e5b5fecb":"code","77696d8a":"code","5b8144f7":"code","48ebef7d":"code","82d45f66":"code","47065e86":"code","8e2660f0":"code","a62ceaa7":"code","b7adf73d":"code","54f1c146":"code","7539db4e":"code","3c10b83a":"code","3ec7e8b6":"code","61827414":"code","247adde7":"code","e76db22f":"code","59a9d372":"code","5f21c698":"code","b0cc413b":"code","ac5bd3a2":"code","275a225a":"code","043f25c4":"code","29198ebc":"code","18f81113":"code","fbd27e9f":"code","199fcf19":"code","4cc252d2":"code","0732029d":"code","c6855fb6":"code","b48dc256":"code","94201c9c":"code","6e3d3b70":"code","73a2da6d":"code","c46e3940":"code","9baad9ef":"code","561ed1e4":"code","1f6ab1df":"code","09334817":"code","a1574c4d":"code","733ab21a":"code","6801b3ef":"code","dfbf7b34":"code","c1c44b24":"code","7881e135":"code","d750fc85":"code","82f9273d":"code","3c25705e":"code","3dd50f96":"code","6df90baf":"code","12454c3e":"code","b8523734":"code","2b063f6b":"code","9aceecef":"code","86edcd09":"code","fad616b1":"code","edb99ed1":"code","2e552e0f":"code","8823632d":"code","4edc412c":"code","148a268a":"code","509a1fae":"code","19f14625":"code","0e233ee9":"code","23e1e84f":"code","f58cb75d":"code","a9f8fa6a":"code","d5e02d6e":"code","9e626e03":"code","ea1380cf":"code","b1b4df1f":"code","34c090e2":"code","4b8f7e57":"code","6cbef7e0":"code","612c4e4e":"code","1b70663f":"code","e9240fa2":"code","cdf3e400":"code","1bc09b17":"code","7108cfd3":"code","7bb9dd55":"code","604b9dc0":"code","d0f30710":"code","86c676da":"code","e27cc3fe":"code","b0f17591":"code","f04e79e2":"code","dc59b93d":"code","3e05af9b":"code","f35465eb":"code","bb76633a":"code","d57e545c":"code","901824a6":"code","a62a1661":"code","6bfd581b":"code","3fb41ef6":"code","7ab977a9":"code","11b077ca":"code","bace4ee7":"code","21c5256d":"code","a564621f":"code","39c5eef7":"code","c44cba51":"code","e8c14eca":"code","95e3d9f0":"code","d99b99cc":"code","94239eba":"code","87d1cbb4":"markdown","d577d48c":"markdown","4275ee75":"markdown","daeb97af":"markdown","c28de4a8":"markdown","d5309b31":"markdown","681e690e":"markdown","cc72b8ef":"markdown","67187aaf":"markdown","2f9c8ae3":"markdown","ad1c1239":"markdown","5ec111c4":"markdown","eb2611f6":"markdown","55fc171c":"markdown","2028b0ef":"markdown","c8544268":"markdown","897a8e11":"markdown","17fd5745":"markdown","ece8f55a":"markdown","df029e3a":"markdown","c21aeb4e":"markdown","40d5808a":"markdown","eb6c693e":"markdown","8c99359d":"markdown","53e88d52":"markdown","5a856326":"markdown","d99c7eb7":"markdown","80ca4300":"markdown","3daee3a3":"markdown","83cb883f":"markdown","b3135ab8":"markdown","50b293b2":"markdown","adc5fe72":"markdown","e6b92fae":"markdown","aeed5e93":"markdown","d443a39b":"markdown","ae581ee9":"markdown","28a5c242":"markdown","ef590748":"markdown","a14c2b11":"markdown","2b356372":"markdown","1d98b297":"markdown","4ec4bf22":"markdown","29dfe298":"markdown","90aa6589":"markdown","a0cdca94":"markdown","5e5c8bd4":"markdown","b5c584ab":"markdown","2dce2fb2":"markdown","48d7a9bf":"markdown","3a1dc0a8":"markdown","e223148a":"markdown","4b59a8a8":"markdown","621ec3c0":"markdown","394e63f6":"markdown","9700715e":"markdown","e4ffb8f4":"markdown","56f3065c":"markdown","480f74f5":"markdown","e58daa28":"markdown","0e48c9ef":"markdown","a9993d3b":"markdown","902f9334":"markdown","e27bb5cc":"markdown","4ed41403":"markdown","f81484fa":"markdown","a62d7699":"markdown","90cdf540":"markdown","36dc08b1":"markdown","b0ebacfa":"markdown"},"source":{"22b5042c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","e78490b8":"print(os.listdir('..\/input\/ashrae-energy-prediction'))","7ccd503a":"# Function to read and do the initial data interpretation\ndef read_and_interpret_data(filename):\n    path = \"..\/input\/ashrae-energy-prediction\"\n    df = pd.read_csv('{0}\/{1}'.format(path,filename))\n    print(\"~~~~~~Shape of the data~~~~~~ : \",df.shape)\n    print(\"~~~~~~Columns and their datatype~~~~~~ : \")\n    print(df.info())\n    print(\"~~~~~~Quick Look at the data~~~~~~ : \")\n    print(df.head())\n    print(\"~~~~~~Description of the data~~~~~~ : \")\n    print(df.describe())\n    print(\"~~~~~~NAs present in the data~~~~~~ : \")\n    print(df.isna().sum())\n    if 'timestamp' in df.columns: \n        df['timestamp'] = pd.to_datetime(df['timestamp'],format = \"%Y-%m-%d %H:%M:%S\")\n        print(\"~~~~~~Year of the data~~~~~~ :\")\n        print(df.timestamp.dt.year.unique())\n    return df","bbdec692":"df_train = read_and_interpret_data('train.csv')","6529d5f1":"df_train.groupby('building_id')['meter_reading'].agg(['count','min','max','mean','median','std'])\n# We can see that the values for building number 1099 are exceptionally high. These can be safely considered as outliers and can be dropped.","fd497190":"df_train.head()","114782ae":"# Remove outliers\ndf_train = df_train [df_train['building_id'] != 1099 ]\ndf_train = df_train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","bc7805ef":"start = df_train['timestamp'].min()\nend = df_train['timestamp'].max()","d45b0723":"type(start)","1bb2a25a":"def plot_for_date_range(num_buildings,start_building = 0,start_date=start, end_date=end):\n    plt.figure(figsize=(18,15), facecolor='white')\n    plot_num = 1\n    for i in range(start_building,start_building+num_buildings):\n        ax = plt.subplot(num_buildings, 1, plot_num)\n        data=df_train[df_train.building_id == i].set_index('timestamp').loc[start_date:end_date]\n        data.plot(y='meter_reading', ax=ax, label=i, legend=False)\n        ax.set_title(f'Building id {i}')\n        plot_num +=1\n    \n    plt.tight_layout()","cc177280":"plot_for_date_range(num_buildings = 10,start_building = 100)","8f0461a9":"plot_for_date_range(7,0,'2016-07-01', '2016-08-01')","0a0e602f":"plot_for_date_range(7,80,'2016-07-01', '2016-08-01')","1d686324":"df_train['month'] = df_train['timestamp'].dt.month\ndf_train['dayofweek'] = df_train['timestamp'].dt.dayofweek\ndf_train['hourofday'] = df_train['timestamp'].dt.hour","331bffd1":"# Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e1c004c8":"df_train = reduce_mem_usage(df_train)","10ee008b":"df_building_metadata = read_and_interpret_data('building_metadata.csv')","91d04305":"def plot_hist(df,var_name):\n    plt.figure(figsize=(17,8))\n    plt.hist(df[var_name],bins = 50)\n    plt.title(f\"Histogram - {var_name}\")\n    plt.show()","c3a0f8f2":"plot_hist(df_building_metadata,'year_built')","5fa06b44":"plot_hist(df_building_metadata,'floor_count')","18733861":"# df_building_metadata = df_building_metadata.drop(['year_built','floor_count'],axis = 1)","a7f20272":"import random\ndef fill_building_data(df, col):\n    df_notna = df[df[col].notnull()]\n    df_na = df[~df[col].notnull()]\n    filler_list = df[col].value_counts().index.tolist()[0:5]\n    df_na[col] = random.choices(filler_list,k = len(df_na))\n    print(df_na.head())\n    print(df_na.isna().sum())\n    return pd.concat([df_na,df_notna],axis=0).sort_values(\"building_id\")","e16e0d34":"# df_building_metadata = fill_building_data(df_building_metadata,'year_built')","55cd96d6":"# df_building_metadata = fill_building_data(df_building_metadata,'floor_count')","04d61bb5":"df_building_metadata.describe()","ebd99525":"df_building_metadata.isna().sum()","65044094":"df_building_metadata['year_built'].value_counts()","e4dee6e7":"df_building_metadata['year_built'].fillna(1976, inplace = True)","e5b5fecb":"df_building_metadata['floor_count'].fillna(1, inplace = True)","77696d8a":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_building_metadata[\"primary_use\"] = le.fit_transform(df_building_metadata[\"primary_use\"])","5b8144f7":"df_building_metadata = reduce_mem_usage(df_building_metadata)","48ebef7d":"df_weather_train = read_and_interpret_data('weather_train.csv')","82d45f66":"for var in ['dew_temperature','air_temperature','wind_speed']:\n    plot_hist(df_weather_train,var)","47065e86":"import datetime\ndef fill_weather_dataset(weather_df):\n    \n    # Find Missing Dates\n    time_format = \"%Y-%m-%d %H:%M:%S\"\n#     start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n#     end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n    start_date = weather_df['timestamp'].min().to_pydatetime()\n    end_date = weather_df['timestamp'].max().to_pydatetime()\n#     total_hours = int(((end_date - start_date).total_seconds() + 3600) \/ 3600)\n#     hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n    total_hours = int(((end_date - start_date).total_seconds() + 3600) \/ 3600)\n    hours_list = np.array([(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)])\n\n    missing_hours = []\n    for site_id in range(16):\n        \n        site_tot_hrs = df_weather_train[df_weather_train['site_id'] == 1]['timestamp']\n        site_hours = np.array([x.strftime(time_format) for x in site_tot_hrs])\n#         site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n        new_rows['site_id'] = site_id\n        weather_df = pd.concat([weather_df,new_rows])\n\n        weather_df = weather_df.reset_index(drop=True)           \n\n    # Add new Features\n    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n    \n    # Reset Index for Fast Update\n    weather_df = weather_df.set_index(['site_id','day','month'])\n\n    # AIR TEMPERATURE\n    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n    weather_df.update(air_temperature_filler,overwrite=False)\n\n    # CLOUD COVERAGE\n    # Step 1\n    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n    # Step 2\n    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n    weather_df.update(cloud_coverage_filler,overwrite=False)\n\n    # DEW TEMPERATURE\n    dew_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n    weather_df.update(dew_temperature_filler,overwrite=False)\n\n    # SEA LEVEL PRESSURE\n    # Step 1\n    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n    # Step 2\n    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n\n    weather_df.update(sea_level_filler,overwrite=False)\n\n    # WIND DIRECTION\n    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n    weather_df.update(wind_direction_filler,overwrite=False)\n\n    # WIND SPEED\n    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n    weather_df.update(wind_speed_filler,overwrite=False)\n\n    # PRECIPITATION DEPTH\n    # Step 1\n    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n    # Step 2\n    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n\n    weather_df.update(precip_depth_filler,overwrite=False)\n\n    weather_df = weather_df.reset_index()\n    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n        \n    return weather_df\n\ndef limit_dew_temp(air_temp, dew_temp):\n    if dew_temp > air_temp:\n        return air_temp\n    else:\n        return dew_temp","8e2660f0":"df_weather_train = fill_weather_dataset(df_weather_train)\ndf_weather_train['dew_temperature'] = df_weather_train.apply(lambda x: limit_dew_temp(x.air_temperature, x.dew_temperature), axis=1)","a62ceaa7":"# drop_cols = ['cloud_coverage','precip_depth_1_hr','sea_level_pressure','wind_direction']\n# df_weather_train = df_weather_train.drop(drop_cols,axis =1)","b7adf73d":"df_weather_train.head()","54f1c146":"df_weather_train.head()","7539db4e":"df_weather_train.isna().sum()","3c10b83a":"# Visualizing distributions after median imputations\nfor var in ['dew_temperature','air_temperature','wind_speed']:\n    plot_hist(df_weather_train,var)","3ec7e8b6":"df_weather_train = reduce_mem_usage(df_weather_train)","61827414":"train_df = pd.merge(df_train,df_building_metadata,on = 'building_id')\n# train_df.head()","247adde7":"df_weather_train['timestamp'] = pd.to_datetime(df_weather_train['timestamp'])\ntrain_df = pd.merge(train_df,df_weather_train,on = ['site_id','timestamp'])","e76db22f":"train_df['square_feet'] =  np.log1p(train_df['square_feet'])","59a9d372":"import gc\ntarget = np.log1p(train_df[\"meter_reading\"])\nfeatures = train_df.drop('meter_reading', axis = 1)\ndel df_train, df_weather_train, train_df\ngc.collect()","5f21c698":"features=features.drop(\"timestamp\",axis = 1)","b0cc413b":"features.isna().sum()","ac5bd3a2":"features.info()","275a225a":"[var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]","043f25c4":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold","29198ebc":"categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"dayofweek\",\"month\",\"hourofday\"]\nparams = {\n    \"objective\": \"regression\",\n    \"boosting\": \"gbdt\",\n    \"num_leaves\": 1280,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.85,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\",\n}\n\nkf = KFold(n_splits=3)\nmodels = []\nfor train_index,test_index in kf.split(features):\n    train_features = features.loc[train_index]\n    train_target = target.loc[train_index]\n    \n    test_features = features.loc[test_index]\n    test_target = target.loc[test_index]\n    \n    d_training = lgb.Dataset(train_features, label=train_target,categorical_feature=categorical_features, free_raw_data=False)\n    d_test = lgb.Dataset(test_features, label=test_target,categorical_feature=categorical_features, free_raw_data=False)\n    \n    model = lgb.train(params, train_set=d_training, num_boost_round=1000, valid_sets=[d_training,d_test], verbose_eval=25, early_stopping_rounds=50)\n    models.append(model)\n    del train_features, train_target, test_features, test_target, d_training, d_test\n    gc.collect()","18f81113":"del features, target\ngc.collect()","fbd27e9f":"for model in models:\n    lgb.plot_importance(model)\n    plt.show()","199fcf19":"#Import the regression tree model\n# from sklearn.tree import DecisionTreeRegressor\n# regression_model = DecisionTreeRegressor(criterion=\"mse\",min_samples_leaf=5)","4cc252d2":"#Fit the model\n# x_train = train_df.drop(['meter_reading','timestamp'],axis = 1)\n# y_train = train_df['meter_reading']\n# regression_model.fit(x_train,y_train)","0732029d":"# x_train.isna().sum()","c6855fb6":"#Predict on Training Data\n# predicted_train = regression_model.predict(x_train)","b48dc256":"#Compute the RMSLE\n# def RMSLE(pred,act): \n#     return np.sqrt(np.sum((np.log(pred+1)-np.log(act+1))**2)\/len(act))","94201c9c":"# Training Error\n# RMSLE(predicted_train,y_train)","6e3d3b70":"# Checking Actual and Predicted values side by side\n# pd.DataFrame(zip(y_train,predicted_train),columns = ['Actual','Predicted']).iloc[2000000:10000000,].head(10)","73a2da6d":"df_test = read_and_interpret_data('test.csv')","c46e3940":"df_test = reduce_mem_usage(df_test)","9baad9ef":"df_test['month'] = df_test['timestamp'].dt.month\ndf_test['dayofweek'] = df_test['timestamp'].dt.dayofweek\ndf_test['hourofday'] = df_test['timestamp'].dt.hour","561ed1e4":"# df_test.head()","1f6ab1df":"# df_test.info()","09334817":"df_test = reduce_mem_usage(df_test)","a1574c4d":"df_weather_test = read_and_interpret_data('weather_test.csv')","733ab21a":"for var in ['dew_temperature','air_temperature','wind_speed']:\n    plot_hist(df_weather_test,var)","6801b3ef":"# drop_cols = ['cloud_coverage','precip_depth_1_hr','sea_level_pressure','wind_direction']\n# df_weather_test = df_weather_test.drop(drop_cols,axis =1)","dfbf7b34":"df_weather_test.head()","c1c44b24":"# df_weather_test.isna().sum()","7881e135":"# df_weather_test.fillna(df_weather_test.median(),inplace=True)","d750fc85":"# df_weather_test.head()","82f9273d":"df_weather_test.isna().sum()","3c25705e":"# Visualizing distributions after median imputations\n# for var in ['dew_temperature','air_temperature','wind_speed']:\n#     plot_hist(df_weather_test,var)","3dd50f96":"test_df = pd.merge(df_test,df_building_metadata,on = 'building_id')\n\ntest_df = pd.merge(test_df,df_weather_test,on = ['site_id','timestamp'],how='left')","6df90baf":"test_df = reduce_mem_usage(test_df)","12454c3e":"[var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]","b8523734":"del df_test, df_weather_test, df_building_metadata\ngc.collect()","2b063f6b":"row_ids = test_df['row_id']\ntest_df = test_df.drop(['timestamp','row_id'],axis = 1)","9aceecef":"test_df[\"square_feet\"] = np.log1p(test_df[\"square_feet\"])","86edcd09":"test_df.fillna(test_df.median(),inplace=True)","fad616b1":"test_df.head()","edb99ed1":"test_df.info()","2e552e0f":"test_df.isna().sum()","8823632d":"gc.collect()","4edc412c":"results = []\nfor model in models:\n    if  results == []:\n        results = np.expm1(model.predict(test_df, num_iteration=model.best_iteration)) \/ len(models)\n    else:\n        results += np.expm1(model.predict(test_df, num_iteration=model.best_iteration)) \/ len(models)\n    del model\n    gc.collect()","148a268a":"del test_df, models\ngc.collect()","509a1fae":"#Predict on Testing Data\n# predicted_test = regression_model.predict(test_df)","19f14625":"# submission_df = pd.DataFrame(zip(row_id,predicted_test),columns = ['row_id','meter_reading'])\nresults_df = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(results, 0, a_max=None)})\ndel row_ids,results\ngc.collect()\nresults_df.to_csv(\"submission_lgbm1.csv\", index=False)","0e233ee9":"# submission_df = submission_df.sort_values(by = 'row_id')","23e1e84f":"# submission_df.head()","f58cb75d":"# submission_df.shape","a9f8fa6a":"# submission_df.to_csv(\"ashrae_submit.csv\", index=False)","d5e02d6e":"# from IPython.display import FileLink\n# FileLink(r'ashrae_submit.csv')","9e626e03":"# df_building_metadata['year_built'].isna().sum()","ea1380cf":"# plt.figure(figsize=(17,8))\n# plt.hist(df_building_metadata['year_built'],bins = 20)\n# plt.title(\"Histogram showing the Distribution of the Year in which Builidings were built\")\n# plt.show()","b1b4df1f":"# Making a list of mode years\n# mode_yr1,mode_yr2 = list(range(1960,1975)),  list(range(2000,2010))\n# mode_years = mode_yr1 + mode_yr2","34c090e2":"# import random\n# random.seed(123)","4b8f7e57":"# Replacing NAs by chooosing randomly from the mode years\n# nans = df_building_metadata['year_built'].isna()\n# length = sum(nans)\n# replacement = random.choices(mode_years, k=length)\n# df_building_metadata.loc[nans,'year_built'] = replacement","6cbef7e0":"# plt.figure(figsize=(17,8))\n# plt.hist(df_building_metadata['year_built'],bins = 20)\n# plt.title(\"Histogram showing the Distribution of the Year in which Builidings were built\")\n# plt.show()","612c4e4e":"# plt.figure(figsize=(17,8))\n# plt.hist(df_building_metadata['year_built'],bins = 20)\n# plt.title(\"Histogram showing the Distribution of the Year in which Builidings were built\")\n# plt.show()","1b70663f":"# df_building_metadata[df_building_metadata.year_built.isnull()]","e9240fa2":"# df_building_metadata['floor_count'].isna().sum()","cdf3e400":"# plt.figure(figsize=(17,8))\n# plt.hist(df_building_metadata['floor_count'],bins = 20)\n# plt.title(\"Histogram showing the distribution of number of Floors in Buildings\")\n# plt.show()","1bc09b17":"# Making a list of mode years\n# mode_floors = range(10)","7108cfd3":"# Replacing NAs by chooosing randomly from the mode years\n# nans = df_building_metadata['floor_count'].isna()\n# length = sum(nans)\n# replacement = random.choices(mode_floors, k=length)\n# df_building_metadata.loc[nans,'floor_count'] = replacement","7bb9dd55":"# plt.figure(figsize=(17,8))\n# plt.hist(df_building_metadata['floor_count'],bins = 20)\n# plt.title(\"Histogram showing the distribution of number of Floors in Buildings\")\n# plt.show()","604b9dc0":"# [1,0,np.nan,2].isnull().replace(0)","d0f30710":"# nan_yrs = sum(df_building_metadata.year_built.isnull())\n# rand_mode_year = random.choices(mode_years, k =nan_yrs)","86c676da":"# df_building_metadata['year_built'].replace(np.nan,rand_mode_year)","e27cc3fe":"# list(range(1960,1975))","b0f17591":"# mode_years[0]","f04e79e2":"# df_building_metadata['year_built'] = df_building_metadata['year_built'].astype('int16')","dc59b93d":"#df_train['timestamp'].dt.weekday_name.unique()","3e05af9b":"#df_train['day_of_weak'] = df_train['timestamp'].dt.day","f35465eb":"#df_train.building_id.nunique() * 24 * 365 ","bb76633a":"#df_test = read_and_interpret_data('test.csv')","d57e545c":"# df_weather_train = read_and_interpret_data('weather_train.csv')","901824a6":"#df_weather_test = read_and_interpret_data('weather_test.csv')","a62a1661":"#df_meta = read_and_interpret_data('building_metadata.csv')","6bfd581b":"# Reducing dataframe size\n# df_train = reduce_mem_usage(df_train)\n# df_test = reduce_mem_usage(df_test)\n# df_weather_train = reduce_mem_usage(df_weather_train)\n# df_weather_test = reduce_mem_usage(df_weather_test)\n# df_meta = reduce_mem_usage(df_meta)","3fb41ef6":"# train_df = df_train.join(df_meta.set_index('building_id'), on = 'building_id')","7ab977a9":"# train_df.info()","11b077ca":"# train_df = train_df.join(df_weather_train.set_index('site_id'), on = 'site_id')","bace4ee7":"# pr = [0 for x in df_test['row_id'] == df_test.index.tolist()]","21c5256d":"# len(pr)","a564621f":"# df_test = df_test.drop('row_id',axis=1)","39c5eef7":"# df_test['meter_reading'] = 'NA'","c44cba51":"# df_test.head()","e8c14eca":"# df_test_train = pd.concat([df_train,df_test])","95e3d9f0":"# del df_train, df_test","d99b99cc":"# df_test_train[df_test_train.meter_reading == 'NA'].head()","94239eba":"# df_test_train_meta = df_test_train.join(df_meta.set_index('building_id'), on = 'building_id')\n# df_test_train_meta.head()","87d1cbb4":"### 2. Imputing Floor Count -","d577d48c":"### Step 1) Reading and Interpreting Data","4275ee75":"### Step 3.1) Imputation","daeb97af":"Number of NAs - ","c28de4a8":"Next, we list the input files to see that there are 6 files available.","d5309b31":"# Predictions","681e690e":"# b) weather_test.csv","cc72b8ef":"Problem : If we impute all NA values with a single value(either mean, median or mode), this is going to significantly distort the distribution.\n\nSolution : From the histograms above, we see that most buildings were built around 1960 - 1975 and 2000 - 2010, so we will impute from this year range.","67187aaf":"** b) Imputing with the median value **","2f9c8ae3":"** 2) Target variable with time - Specified Range**","ad1c1239":"### Step 1) Reading and Interpreting Data","5ec111c4":"### Step 4) Reducing memory usage","eb2611f6":"### Step 3) Handling NA values","55fc171c":"# a) test.csv","2028b0ef":"Because there are 6 files, reading them individually and then interpreting them one by one will make the code longer. So, I have written a small function to help me for this task.","c8544268":"We can again check if there are any null values left out","897a8e11":"# Decision Tree Regression Model","17fd5745":"### Step 3) Handling NAs","ece8f55a":"**1) Variation of Target Variable `meter_reading` with `timestamp` - For the whole year 2016**","df029e3a":"# Analysis:\n\nHere, we will start working with the datasets and try to build some understanding about the same first using certain visualizations. Then we will see if we can do some feature engineering and try to build some model.\n\nFirst, we import necessary packages","c21aeb4e":"** b) Imputing with the median value **","40d5808a":"We can see that this is an asymmetric distribution with most of the values taking 1 to 10 values","eb6c693e":"Specific Notes\n - For Buildings 2 and 5, we can clearly see that there is a daily seasonality pattern in the data i.e. the meter readings are very low during early morning and night time whereas it peaks during the daytime. I think this pattern denotes a household or a small company which switches off its energy requirements during these wee hours.\n \n - For Building Id 1, we can see that the meter reading is very much constant throughout the day and it keeps on for most of the days. In between there are sudden up-spikes and downspikes. This kind of pattern suggests this building must be a powerplant or a computer data center which needs continuous supply of energy. The spikes might suggest sudden energy surge or server downtime. \n \n - For other buildings, there is no identifiable pattern.","8c99359d":"# Training Data","53e88d52":"Training Error -","5a856326":"# b) building_metadata.csv","d99c7eb7":"### Step 2) Visualizing Data","80ca4300":"From the above graph, we note the following -\n\nGeneral Trend\n - meter reading is very low from Jan to April in general but for some building it is not true\n - There is a sudden spike in May after which the meter reading goes again to very low.\n - From mid-June onwards, the meter_reading follows a noisy time series","3daee3a3":"### Step 3) Feature Engineering","83cb883f":"# a) train.csv","b3135ab8":"Observation - \n\nWe can confirm from the histograms above that the imputation performed did not change the variable distribution very much as there were very less rows having NA values. So we are good.","50b293b2":"Step 2) - Note that the data is huge, so in order to be in the kernel limit, we append these two datasets into one.","adc5fe72":"** a) Removing columns which have lot of NAs **","e6b92fae":"# LGBM Model","aeed5e93":"### Step 1) Reading and Interpreting Data","d443a39b":"**!!! PLEASE UPVOTE THE KERNEL IF YOU LIKE IT !!!**","ae581ee9":"We can see that there are 774 NA values out of 1449 rows in the `year_built` column. (50% NA)","28a5c242":"### 1. Imputing Year built -","ef590748":"### Step 3) Removing NAs\n\nWe can see that there are 774 NA values out of 1449 rows in the `year_built` column. (50% NA)\n\nProblem : If we impute all NA values with a single value(either mean, median or mode), this is going to significantly distort the distribution.\n\nSolution : For now, let's remove these two columns altogether to simplify things. Later we will try some other strategies for imputation.","a14c2b11":"STILL IN PROGRESS ....","2b356372":"Variable Distribution - ","1d98b297":"# Prepare Training Data","4ec4bf22":"** a) Removing columns which have lot of NAs **","29dfe298":"# Testing Data","90aa6589":"### Step 4) Encode Categorical Data","a0cdca94":"### Step 2) - Visualizing Data","5e5c8bd4":"Note that the distribution has still increased around the mode years as there were lot of rows which were NA. But we can see it is kind of levelled out","b5c584ab":"# Prepare Testing Data","2dce2fb2":"### Step 2) - Visualizing Data","48d7a9bf":"### Step 2) - Reducing Memory Usage\n1) From the information above, we can see that `df_train` and `df_test` consume a huge amount of memory (~1GB). But there is a scope to reduce the memory.\n\n2) We notice that the int and float datatypes are 64 bits, which is the underlying cause for this huge size of the datasets. We have a function available to reduce this size.","3a1dc0a8":"### Step 1) Reading and Interpreting Data","e223148a":"### Step 3) Feature Engineering","4b59a8a8":"We can see that there are 1094 NA values out of 1449 rows in the `floor_count` column. (75% NA)","621ec3c0":"# c) weather_train.csv","394e63f6":"Here, we notice that `df_train` dataset does not have many features available. But other datasets `df_weather_train` and `df_meta` have features which we can use to build model. So, we prepare training data by joining these 3 datasets. This way, we will have features available in the same place in one dataset.","9700715e":"### Step 1) Reading and Interpreting Data","e4ffb8f4":"Although, the above graph lets us visualize how `meter_reading` varies over the year 2016 as a whole, we are unable to visualize the seasonal changes in the graph.\n\nFor this, we can use the same function by passing the start and end dates between which we are interested to see the graph","56f3065c":"Variable Distribution -","480f74f5":"### Step 4) Reducing memory usage","e58daa28":"# Introduction:\nThe American Society of Heating, Refrigerating and Air-Conditioning Engineers ([ASHRAE](https:\/\/en.wikipedia.org\/wiki\/ASHRAE)) is an American professional association seeking to advance heating, ventilation, air conditioning and refrigeration systems design and construction. ASHRAE has more than 57,000 members in more than 132 countries worldwide. ","0e48c9ef":"# Problem Statement : \n\nSignificant investments are being made to improve building efficiencies to reduce costs and emissions. But, are the improvements working? Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don\u2019t work with different building types.\n\nIn this competition, you\u2019ll develop accurate predictions of metered building energy usage in the following areas: chilled water, electric, natural gas, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe.","a9993d3b":"**!!! PLEASE UPVOTE THE KERNEL IF YOU LIKE IT !!!**","902f9334":"We are interested to see how the target variable varies with time. \n\nFor this, I have implemented a function that lets us look at the data. \n\nThe beauty of this function is that -\n- By default, this function plots the whole data starting from building_id = 0 to `num_buildings` specified\n- If provided the start and end data, it will plot the data between this specified period.\n- If provided with parameter `start_building`, it will plot from building_id = `start_building` onwards","e27bb5cc":"### Step 2) Reducing memory usage","4ed41403":"### Step 3) Handling NAs","f81484fa":"### Step 2) Visualizing Data","a62d7699":"### Step 4) Reducing memory usage","90cdf540":"Number of NAs -","36dc08b1":"Same as before, we join `df_test` , `df_weather_test` and `df_meta` to have features available in the same place in one dataset.","b0ebacfa":"Observation - \n\nWe can confirm from the histograms above that the imputation performed did not change the variable distribution very much as there were very less rows having NA values. So we are good."}}