{"cell_type":{"1773a55f":"code","5ac8dffa":"markdown","1c569369":"markdown"},"source":{"1773a55f":"#!\/usr\/bin\/python3\n# coding=utf-8\n#===========================================================================\n# This is a simple script to perform recursive feature elimination on \n# the kaggle 'House Prices' data set using the scikit-learn RFE\n# Carl McBride Ellis (2.V.2020)\n#===========================================================================\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\nimport numpy as np\n\n#===========================================================================\n# read in the data and specify the target feature\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col=0)\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col=0)\ntarget     = 'SalePrice'\n\n#===========================================================================\n#===========================================================================\nX_train = train_data.select_dtypes(include=['number']).copy()\nX_train = X_train.drop([target], axis=1)\ny_train = train_data[target]\nX_test  = test_data.select_dtypes(include=['number']).copy()\n\n#===========================================================================\n# simple preprocessing: imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train = X_train.fillna(X_train.mean())\nX_test  = X_test.fillna(X_test.mean())\n\n#===========================================================================\n# set up our regressor. Today we shall be using the random forest\n#===========================================================================\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators=100, max_depth=10)\n\n#===========================================================================\n# perform a scikit-learn Recursive Feature Elimination (RFE)\n#===========================================================================\nfrom sklearn.feature_selection import RFE\n# here we want only one final feature, we do this to produce a ranking\nn_features_to_select = 1\nrfe = RFE(regressor, n_features_to_select=n_features_to_select)\nrfe.fit(X_train, y_train)\n\n#===========================================================================\n# now print out the features in order of ranking\n#===========================================================================\nfrom operator import itemgetter\nfeatures = X_train.columns.to_list()\nfor x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):\n    print(x, y)\n\n#===========================================================================\n# ok, this time let's choose the top 10 featues and use them for the model\n#===========================================================================\nn_features_to_select = 10\nrfe = RFE(regressor, n_features_to_select=n_features_to_select)\nrfe.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict the prices for the test data\n#===========================================================================\npredictions = rfe.predict(X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"Id\":test_data.index, target:predictions})\noutput.to_csv('submission.csv', index=False)","5ac8dffa":"## Recursive Feature Elimination (RFE) example\nRecursive feature elimination [1] is an example of *backward feature elimination* [2] in which we essentially first fit our model using *all* the features in a given set, then progressively one by one we remove the *least* significant features, each time re-fitting, until we are left with the desired number of features, which is set by the parameter `n_features_to_select`.\n\nThis simple script uses the scikit-learn *Recursive Feature Elimination* routine [sklearn.feature_selection.RFE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html). In this example we shall use the [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) data.\nFor the regressor we shall use the [Random Forest Regressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) routine, also from scikit-learn.\n\nScikit-learn also has a variant of this routine that incorporates [cross-validation](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics&#41;), see: [sklearn.feature_selection.RFECV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html).\n\n### Forward feature selection\nRFE has its counterpart in *forward feature selection*, which does the opposite: accrete features rather than eliminate them, usually via some form of [greedy algorithm](https:\/\/en.wikipedia.org\/wiki\/Greedy_algorithm) [3]. Scikit-learn has the routine [sklearn.feature_selection.f_regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.f_regression.html) to facilitate this. \n\nNote that both of these *wrapper* methods can be beaten if one has access to \"domain knowledge\", i.e. understanding the problem and having a good idea as to which features will be important in the model one is constructing.\n\n### The python code:","1c569369":"### References\n\n1. [Isabelle Guyon, Jason Weston, Stephen Barnhill and Vladimir Vapnik \"Gene Selection for Cancer Classification using Support Vector Machines\", Machine Learning volume 46, pages 389\u2013422 (2002) (doi: 10.1023\/A:1012487302797)](https:\/\/doi.org\/10.1023\/A:1012487302797)\n2. [Ron Kohavi and George H. John \"Wrappers for feature subset selection\", Artificial Intelligence Volume 97 pages 273-324 (1997) (doi: 10.1016\/S0004-3702(97)00043-X)](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S000437029700043X)\n3. [Haleh Vafaie and Ibrahim F. Imam \"Feature Selection Methods: Genetic Algorithms vs. Greedy-like Search\", Proceedings of the 3rd International Conference on Fuzzy and Intelligent Control Systems (1994)](https:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.48.8452)"}}