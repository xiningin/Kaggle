{"cell_type":{"4c29506e":"code","dce13453":"code","a363e5c5":"code","fe0c7abf":"code","3a03c8aa":"code","4725cabe":"code","c9d97bef":"code","1da7934c":"code","b82740ff":"code","3d38d1f9":"code","efa9a97f":"code","792b4f29":"code","4b4173a2":"code","fe329521":"code","eda9b70a":"code","897270a8":"code","03912d77":"code","baba32fc":"code","e7ce1304":"code","b5b9bea5":"code","86491bc8":"code","08a6eb6d":"code","fc8e10e8":"code","1133e168":"code","a391dd47":"code","a961a228":"code","172bfa47":"code","9b0d8624":"code","8371c55e":"code","270847af":"code","7caa50e8":"code","e9b92c04":"code","d6638405":"code","408137fe":"code","3618b7a8":"code","c5f60546":"code","b0233522":"code","4503c55e":"code","37e5b5cf":"code","9f28c1d7":"code","951c5231":"code","7ab5abbd":"code","0692e2ce":"code","7f6aefe8":"code","3879c90d":"code","bfdfd045":"code","d3b44119":"code","0d0d8739":"code","c5e69e71":"code","bcd52eb3":"code","c1e0def7":"code","db53fbf3":"code","182fb238":"code","2868f785":"code","d8034ff6":"code","698deb83":"code","b7e9c219":"code","cc54aa21":"code","78a57f5d":"code","80d5f67d":"code","f009f108":"markdown","bfffed38":"markdown","b4ef1b8d":"markdown","e7da012e":"markdown","13d8edd0":"markdown","18c275f1":"markdown","33bff3c8":"markdown","89feb0ca":"markdown","bdd8beb1":"markdown","2c6844b6":"markdown","911c3f87":"markdown","1b8471d0":"markdown","a9aceeca":"markdown","d690b75d":"markdown","2dce4b0b":"markdown","4837ae79":"markdown","0fcad2f7":"markdown","a916720f":"markdown","6b214f82":"markdown","1e2fcb0d":"markdown"},"source":{"4c29506e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dce13453":"import matplotlib.pyplot as plt\n%matplotlib inline","a363e5c5":"import warnings\nwarnings.filterwarnings(\"ignore\")","fe0c7abf":"fish= pd.read_csv(\"\/kaggle\/input\/soai21-fish-regression\/dati.csv\")","3a03c8aa":"fish.shape","4725cabe":"fish.head()","c9d97bef":"fish.describe()","1da7934c":"# controllo che le features siano scritte correttamente (spazi, ...)\nfish.columns","b82740ff":"fish = fish.loc[fish.Weight!=0,:].copy()","3d38d1f9":"# controllo missing\nfish.isna().sum()","efa9a97f":"fish.drop([\"Length1\", \"Length2\"], axis=1, inplace=True)","792b4f29":"plt.figure(figsize=(18,10))\nfor s in fish.Species.unique():\n    l = fish.loc[fish.Species==s, \"Length3\"]\n    h = fish.loc[fish.Species==s, \"Height\"]\n    plt.scatter(x=l, y=h, marker=\"o\")\nplt.legend(fish.Species.unique())\nplt.show()","4b4173a2":"plt.figure(figsize=(18,10))\nfor s in fish.Species.unique():\n    l = fish.loc[fish.Species==s, \"Length3\"]\n    h = fish.loc[fish.Species==s, \"Width\"]\n    plt.scatter(x=l, y=h, marker=\"o\")\nplt.legend(fish.Species.unique())\nplt.show()","fe329521":"plt.figure(figsize=(18,10))\nfor s in fish.Species.unique():\n    l = fish.loc[fish.Species==s, \"Width\"]\n    h = fish.loc[fish.Species==s, \"Height\"]\n    plt.scatter(x=l, y=h, marker=\"o\")\nplt.legend(fish.Species.unique())\nplt.show()","eda9b70a":"fish.Species.value_counts()","897270a8":"plt.figure(figsize=(18,10))\nfish.Species.hist()\nplt.show()","03912d77":"X = fish.drop([\"Weight\"], axis=1).copy()\ny= fish[\"Weight\"]","baba32fc":"from sklearn.model_selection import train_test_split","e7ce1304":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=X.Species)","b5b9bea5":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder","86491bc8":"scaler = RobustScaler()\nnumeric_df = scaler.fit_transform(X_train.iloc[:,:3])\nnumeric_val = scaler.transform(X_val.iloc[:,:3])","08a6eb6d":"encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\ncat_df = encoder.fit_transform(X_train[[\"Species\"]])\ncat_val = encoder.transform(X_val[[\"Species\"]])","fc8e10e8":"from sklearn.decomposition import PCA\npca = PCA(n_components=1)\nx_pca = pca.fit_transform(numeric_df)","1133e168":"plt.figure(figsize=(18,10))\nplt.scatter(x_pca, y_train)\nplt.show()","a391dd47":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","a961a228":"poly = PolynomialFeatures(2)","172bfa47":"poly_train=poly.fit_transform(numeric_df)\npoly_val = poly.fit_transform(numeric_val)","9b0d8624":"X_train_new = np.hstack((poly_train, cat_df))\nX_val_new = np.hstack((poly_val, cat_val))","8371c55e":"PolyR = LinearRegression()","270847af":"PolyR.fit(X_train_new, y_train)\nprint(\"R2 on train: \", PolyR.score(X_train_new, y_train))\nprint(\"R2 on validation: \", PolyR.score(X_val_new, y_val))","7caa50e8":"yhat=PolyR.predict(X_val_new)","e9b92c04":"print(\"MSE on Validation: \", mean_squared_error(y_val, yhat))\nprint(\"RMSE on Validation: \", np.sqrt(mean_squared_error(y_val, yhat)))","d6638405":"yhat_train = PolyR.predict(X_train_new)\nplt.figure(figsize=(18,10))\nplt.scatter(x_pca, y_train)\nplt.scatter(x_pca, yhat_train)\nplt.legend([\"yreal\", \"yhat\"])\nplt.show()","408137fe":"from sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import RidgeCV","3618b7a8":"PolyRCV = RidgeCV(cv=10)\ncv_score = cross_validate(PolyRCV, X_train_new, y_train, cv = 10, n_jobs=-1, return_estimator=True, return_train_score=True)","c5f60546":"print(cv_score['train_score'].mean(), \"+-\", cv_score['train_score'].std())\nprint(cv_score['test_score'].mean(), \"+-\", cv_score['test_score'].std())","b0233522":"PolyRCV.fit(X_train_new, y_train)\nprint(\"Parametro alpha per la regolarizzazione: \", PolyRCV.alpha_)\nprint(\"R2 on validation: \", PolyRCV.score(X_train_new, y_train))\nprint(\"R2 on validation: \", PolyRCV.score(X_val_new, y_val))","4503c55e":"yhatCV=PolyRCV.predict(X_val_new)\nprint(\"MSE on Validation: \", mean_squared_error(y_val, yhatCV))\nprint(\"RMSE on Validation: \", np.sqrt(mean_squared_error(y_val, yhatCV)))","37e5b5cf":"yhat_trainCV = PolyRCV.predict(X_train_new)\nplt.figure(figsize=(18,10))\nplt.scatter(x_pca, y_train)\nplt.scatter(x_pca, yhat_trainCV)\nplt.legend([\"yreal\", \"yhat\"])\nplt.show()","9f28c1d7":"columns = poly.get_feature_names() + encoder.categories_[0].tolist()","951c5231":"plt.figure(figsize=(18,10))\nplt.bar(columns, PolyR.coef_)\nplt.bar(columns, PolyRCV.coef_, align='edge')\nplt.legend([\"PolyRegression\", \"Ridge\"])\nplt.title(\"Polynomial Regression vs. Ridge.\\n Differenze tra i coefficienti\")\nplt.show()","7ab5abbd":"!pip install shap\n!pip install eli5","0692e2ce":"import eli5\nfrom eli5.sklearn import PermutationImportance","7f6aefe8":"permPolyR = PermutationImportance(PolyR, random_state=42, cv=\"prefit\").fit(X_train_new, y_train)\neli5.show_weights(permPolyR, feature_names= columns)","3879c90d":"permPolyR = PermutationImportance(PolyR, random_state=42, cv=\"prefit\").fit(X_val_new, y_val)\neli5.show_weights(permPolyR, feature_names= columns)","bfdfd045":"permPolyRCV = PermutationImportance(PolyRCV, random_state=42, cv=\"prefit\").fit(X_train_new, y_train)\neli5.show_weights(permPolyRCV, feature_names= columns)","d3b44119":"permPolyRCV = PermutationImportance(PolyRCV, random_state=42, cv=\"prefit\").fit(X_val_new, y_val)\neli5.show_weights(permPolyRCV, feature_names= columns)","0d0d8739":"import shap\nshap.initjs()","c5e69e71":"df_train = pd.DataFrame(X_train_new, columns=columns)\ndf_val = pd.DataFrame(X_val_new, columns=columns)","bcd52eb3":"# Polynomial regression\nex0 = shap.KernelExplainer(PolyR.predict, df_train)\nshap_values0 = ex0.shap_values(df_val.iloc[0,:])\nshap.force_plot(ex0.expected_value, shap_values0, df_val.iloc[0,:])","c1e0def7":"# Ridge Polynomial Regressiom\nex1 = shap.KernelExplainer(PolyRCV.predict, df_train)\nshap_values1 = ex1.shap_values(df_val.iloc[0,:])\nshap.force_plot(ex1.expected_value, shap_values1, df_val.iloc[0,:])","db53fbf3":"ex2 = shap.KernelExplainer(PolyR.predict, df_train)\nshap_values2 = ex2.shap_values(df_val)\nshap.summary_plot(shap_values2, df_val)","182fb238":"ex3 = shap.KernelExplainer(PolyRCV.predict, df_train)\nshap_values3 = ex3.shap_values(df_val)\nshap.summary_plot(shap_values3, df_val)","2868f785":"test = pd.read_csv(\"\/kaggle\/input\/soai21-fish-regression\/test.csv\")\ntest = test.drop(columns=['Id'])","d8034ff6":"test.describe()","698deb83":"test.drop([\"Length1\", \"Length2\"], axis=1, inplace=True)","b7e9c219":"numeric_test = scaler.transform(test.iloc[:,:3])\ncat_test = encoder.transform(test[[\"Species\"]])\npoly_test = poly.fit_transform(numeric_test)\nX_test_new = np.hstack((poly_test, cat_test))","cc54aa21":"output=pd.DataFrame()","78a57f5d":"output['Weight']=PolyRCV.predict(X_test_new)","80d5f67d":"output['Id'] = np.arange(len(test))\noutput.to_csv(\"coffee_outputCV20.csv\",index=False)","f009f108":"Ci sono campioni con peso 0, eliminiamo","bfffed38":"### SHAP Plot\nSHAP da Shapely Values, un approccio molto usato nella teoria dei giochi, permette di spiegare come impattano le features sul modello. Ha anche dei plot molto belli a mio avviso.","b4ef1b8d":"### Valutazione su una previsione\n* Regressione Polinomiale Semplice\n* Ridge Regression Polinomiale","e7da012e":"### Permutation Importance\nPermutare i valori delle features ci permette di valutarne l'impatto sul modello, in maniera semplicistica:\n* se le performance si abbassano --> Feature importante\n* se le performance restano invariate --> Feature senza impatto\n* se una permutazione casuale dei valori migliora il modello --> impatto negativo\n\neli5 produce un plot a semaforo, sumando dal verde al rosso, buona feature - cattiva feature.\n\nVerranno valutati:\n* Regressione Polinomiale\n* Ridge Regression Polinomiale\n\nApplico prima il modello al train per vedere cosa ha imparato e come, e successivamente alla validation per vedere se le features che reputa importanti generalizzano bene.","13d8edd0":"## Explanable Machine Learning Models","18c275f1":"**AGGIORNAMENTO:** Le variabili Length sono collineari, tengo solo la L3, la coda pu\u00f2 avere un peso nella regressione di casi particolari (non sono esperto di pesci). Lascio nella nota la prima versione del commento.\n\n> Le variabili Length sono collineari, tengo solo la L2 perch\u00e8 dovrebbe essere la mediana. Se tenessi tutte e tre avrei ottimi risultati in train - validation ma pessimi in test. **Overfitting**.","33bff3c8":"Il plot mi aiuta a scegliere il grado del polinomio della mia regressione, come si vede la relazione non \u00e8 lineare ma di grado 2. Potrei usara un regressione ad alberi ma preferisco utilizzare un modello parametrico visto che i modelli ad albero restituiscono la media dei valori nel nodo avrei una funzione *a gradini* mentre qui la relazione \u00e8 abbastanza visibile.","89feb0ca":"## Prima analisi esplorativa","bdd8beb1":"## Baseline Model: Polynomial Regression","2c6844b6":"# Tokyo Fish Market\n\n<br>\n\n![tfm](https:\/\/www.driveontheleft.com\/wp-content\/uploads\/2017\/10\/Fish-market-22-min.jpg)\n\n<br>\n\nStimare con la maggiore precisione possibile il peso dei pesci. La misura utilizzata per l'errore \u00e8 **RMSE**.\n\n## Dataset\n\n* 294 samples\n* 7 features compreso il target\n\n* Length1: standard length\n* Length2: fork length\n* Lenght3: total length\n* Height\n* Width\n* Species: 7 categorie\n* ***Weight*** (target)\n\n<br>\n\n![fish](https:\/\/www.fishbase.de\/Images\/Glospic\/G_Fig13a6181_SL.jpg)\n\n<br>\n","911c3f87":"## Analisi visuale","1b8471d0":"Creo dei dataframe per vedere il nome delle colonne","a9aceeca":"Visto che sicuramente le tre componenti numeriche sono correlate voglio vedere in che relazione sono con il peso. Applico una PCA e tengo solo la prima componente principale per fare lo scatterplot. Potrebbe essere rinominata *Stazza* perch\u00e8 riassume le tre dimensioni.","d690b75d":"Chiaramente le tre variabili numeriche sono correlate, la specie \u00e8 una discriminante.\n\nConto le occorrenze per specie, sono distribuite abbastanza uniformemente nel dataset.","2dce4b0b":"## Train - Validation split\n\nUso il campionamento stratificato sulla specie.","4837ae79":"La regressione senza regolarizzazione sembra sovrastimare leggermente, sovrastima dovuta alle variabili dummies.\n#### SHAP plot dell'impatto sul modello","0fcad2f7":"## Ridge Regression\n\n***Ridge Regression***, chiamato anche *Tikhonov regularization*, \u00e8 una versione regolarizzata della Regressione Lineare. Per saperne di pi\u00f9 potete andare [qui](https:\/\/andreaprovino.it\/ridge-regression\/).\n\nUso la versione *Cross-Validation* disponibile su scikit-learn per valutare i diversi valori di *alpha*.","a916720f":"## Preprocessing\nPer i valori numerici uso *RobustScaler*, basato sulla distanza interquartile perch\u00e8 pi\u00f9 robusto ad eventuali valori anomali.","6b214f82":"## Test set per la submission","1e2fcb0d":"### Confronto tra i modelli\n#### Analisi Visuale"}}