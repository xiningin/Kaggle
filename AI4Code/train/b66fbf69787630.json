{"cell_type":{"e55da14a":"code","79c87daf":"code","d0be2adb":"code","9ff632fe":"code","d57349bb":"code","544a3732":"code","ebdffceb":"code","b25bccf5":"code","4685ec64":"code","fb8d4044":"code","b8e484bd":"code","fc5fd671":"code","5a454051":"code","cbc5e4f7":"code","a1ddce6b":"code","50f0fd71":"code","f27e1791":"code","bf5e0b1c":"code","f76703d9":"code","d37f9e7b":"code","75e8fef0":"code","bfc8ef2b":"code","346d3eef":"code","487cabca":"code","6425eada":"code","b18853f5":"code","c26a5df3":"code","8edd2d8d":"code","f2ea2625":"code","751a07a4":"code","42ddada6":"code","617b140f":"code","ae8c7e8a":"code","43f430b7":"code","0c722a18":"code","b9a55740":"code","c462c9bc":"code","a551ee1d":"code","668625e2":"code","7f7cb072":"code","482b6ca3":"code","f71a11c3":"code","1bc03d85":"code","0fc65617":"code","07d107f4":"code","953094e1":"code","09bfb5f9":"code","aab72c2c":"code","cbd5e49d":"code","324130e8":"code","54fc25e4":"code","169ca2cc":"code","04795ab9":"code","8a64dd5e":"code","8e77a528":"code","2cf08a35":"code","98b3f04e":"code","1e0b332f":"code","8b2fa666":"code","a094f344":"code","6b5ed58b":"code","166c8e6e":"code","a81e6423":"code","44dfafb7":"code","7b63ab4a":"code","0f97b033":"code","1abdd36d":"code","a7358785":"code","7b585f0b":"code","8ceb5281":"code","3f369e6a":"code","f7be5ce2":"markdown","d98234e2":"markdown","ad68e5e7":"markdown","d5fed73b":"markdown","4fa62353":"markdown","fb1fb44e":"markdown","8b458728":"markdown","55e197ee":"markdown","b2f60154":"markdown","16581613":"markdown","58c5ce71":"markdown","60fbdbcf":"markdown","3469a953":"markdown","b7b68918":"markdown","4752ddf5":"markdown","828667e5":"markdown"},"source":{"e55da14a":"!pip install tweet-preprocessor","79c87daf":"import os\nfrom google.cloud import storage, automl_v1beta1 as automl\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport scipy as sp\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom collections import Counter\nimport string\nimport re\nfrom nltk.corpus import wordnet as wn\nfrom statistics import mean \nimport preprocessor ","d0be2adb":"from sklearn import metrics","9ff632fe":"stop = set(STOPWORDS).union(set(['FAV' , 'RT']))\nlemma = WordNetLemmatizer()\npreprocessor.set_options(preprocessor.OPT.URL, preprocessor.OPT.MENTION, preprocessor.OPT.NUMBER, preprocessor.OPT.RESERVED)\n\ndef clean(text):   \n    text = preprocessor.clean(text)\n    text = re.sub(r'[^\\w\\s]','',text)\n    stop_free = \" \".join([i for i in text.split(' ') if (i not in stop)])\n    normalized = \" \".join(lemma.lemmatize(word) for word in stop_free.split())\n    return normalized","d57349bb":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","544a3732":"train_df.text = train_df.text.apply(clean)\ntest_df.text = test_df.text.apply(clean)","ebdffceb":"train_df.head(5)","b25bccf5":"test_df.head(5)","4685ec64":"# Set your own project id here\nPROJECT_ID = 'automl-kaggle-263107'","fb8d4044":"#REPLACE THIS WITH A NEW BUCKET NAME. NOTE: BUCKET NAMES MUST BE GLOBALLY UNIQUE\nBUCKET_NAME = 'automl-disaster-tweet-cleaned'\n#Note: the bucket_region must be us-central1.\nBUCKET_REGION = 'us-central1'","b8e484bd":"storage_client = storage.Client(project=PROJECT_ID)\ntables_gcs_client = automl.GcsClient(client=storage_client, bucket_name=BUCKET_NAME)\nautoml_client = automl.AutoMlClient()\n# Note: AutoML Tables currently is only eligible for region us-central1. \nprediction_client = automl.PredictionServiceClient()\n# Note: This line runs unsuccessfully without each one of these parameters\ntables_client = automl.TablesClient(project=PROJECT_ID, region=BUCKET_REGION, client=automl_client, gcs_client=tables_gcs_client, prediction_client=prediction_client)","fc5fd671":"# Create your GCS Bucket with your specified name and region (if it doesn't already exist)\nbucket = storage.Bucket(storage_client, name=BUCKET_NAME)\nif not bucket.exists():\n    bucket.create(location=BUCKET_REGION)","5a454051":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","cbc5e4f7":"test_df.head(5)","a1ddce6b":"# Select the text body and the target value, for sending to AutoML\ntrain_df[['id','text','target']].to_csv('\/kaggle\/working\/train.csv', index=False) \ntest_df[['id','text']].to_csv('\/kaggle\/working\/test.csv', index=False) ","50f0fd71":"upload_blob(BUCKET_NAME, '\/kaggle\/working\/train.csv', 'train.csv')\nupload_blob(BUCKET_NAME, '\/kaggle\/working\/test.csv', 'test.csv')","f27e1791":"dataset_display_name = 'tweet_disaster_cleaned'\nnew_dataset = False\ntry:\n    dataset = tables_client.get_dataset(dataset_display_name=dataset_display_name)\nexcept:\n    new_dataset = True\n    dataset = tables_client.create_dataset(dataset_display_name)","bf5e0b1c":"# gcs_input_uris have the familiar path of gs:\/\/BUCKETNAME\/\/file\n\nif new_dataset:\n    gcs_input_uris = ['gs:\/\/' + BUCKET_NAME + '\/train.csv']\n\n    import_data_operation = tables_client.import_data(\n        dataset=dataset,\n        gcs_input_uris=gcs_input_uris\n    )\n    print('Dataset import operation: {}'.format(import_data_operation))\n\n    # Synchronous check of operation status. Wait until import is done.\n    import_data_operation.result()","f76703d9":"print(dataset)","d37f9e7b":"ID_COLUMN = 'id'","75e8fef0":"TARGET_COLUMN = 'target'\n\ntables_client.set_target_column(\n    dataset=dataset,\n    column_spec_display_name=TARGET_COLUMN\n)","bfc8ef2b":"# Make all columns nullable (except the Target and ID Column)\nfor col in tables_client.list_column_specs(PROJECT_ID,BUCKET_REGION,dataset.name):\n    if TARGET_COLUMN in col.display_name or ID_COLUMN in col.display_name:\n        continue\n    tables_client.update_column_spec(PROJECT_ID,\n                                     BUCKET_REGION,\n                                     dataset.name,\n                                     column_spec_display_name=col.display_name,\n                                     type_code=col.data_type.type_code,\n                                     nullable=True)","346d3eef":"# Train the model. This will take hours (up to your budget). AutoML will early stop if it finds an optimal solution before your budget.\n# On this dataset, AutoML usually stops around 2000 milli-hours (2 hours)\n\nTRAIN_BUDGET = 1000 # (specified in milli-hours, from 1000-72000)\nmodel = None\nmodel_display_name = 'tweet_disaster_model_clean'\ntry:\n    model = tables_client.get_model(model_display_name=model_display_name)\nexcept:\n    response = tables_client.create_model(\n        model_display_name,\n        dataset=dataset,\n        train_budget_milli_node_hours=TRAIN_BUDGET,\n        exclude_column_spec_names=[TARGET_COLUMN,ID_COLUMN]\n    )\n    print('Create model operation: {}'.format(response.operation))\n    # Wait until model training is done.\n    model = response.result()\nprint(model)","487cabca":"gcs_input_uris = 'gs:\/\/' + BUCKET_NAME + '\/test.csv'\ngcs_output_uri_prefix = 'gs:\/\/' + BUCKET_NAME + '\/predictions'\n\nbatch_predict_response = tables_client.batch_predict(\n    model=model, \n    gcs_input_uris=gcs_input_uris,\n    gcs_output_uri_prefix=gcs_output_uri_prefix,\n)\nprint('Batch prediction operation: {}'.format(batch_predict_response.operation))\n# Wait until batch prediction is done.\nbatch_predict_result = batch_predict_response.result()\nbatch_predict_response.metadata","6425eada":"# The output directory for the prediction results exists under the response metadata for the batch_predict operation\n# Specifically, under metadata --> batch_predict_details --> output_info --> gcs_output_directory\n# Then, you can remove the first part of the output path that contains the GCS Bucket information to get your desired directory\ngcs_output_folder = batch_predict_response.metadata.batch_predict_details.output_info.gcs_output_directory.replace('gs:\/\/' + BUCKET_NAME + '\/','')\ndownload_to_kaggle(BUCKET_NAME,'\/kaggle\/working','submissions.csv', prefix=gcs_output_folder)","b18853f5":"preds_df = pd.read_csv(\"\/kaggle\/working\/submissions.csv\")\npreds_df = preds_df.sort_values(by=['id'])\npreds_df['target'] = (preds_df['target_1_score'] >= 0.5).astype(int)","c26a5df3":"preds_df.head(50)","8edd2d8d":"preds_df[['id','target']].to_csv(\"submission.csv\", index=False)","f2ea2625":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range = (1,2), stop_words='english',strip_accents='unicode')","751a07a4":"train_vectors = tfidf_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = tfidf_vectorizer.transform(test_df[\"text\"])","42ddada6":"train_vectors.todense().shape","617b140f":"# define X and y\nfeature_cols = ['keyword', 'location']\nX = train_df[feature_cols]\ny = train_df.target\none_hot_encoded_training_predictors = pd.get_dummies(X)\nclf = RandomForestClassifier(n_estimators = 100)\nscores = model_selection.cross_val_score(clf, one_hot_encoded_training_predictors, y, cv=5, scoring=\"f1\")\nscores","ae8c7e8a":"clf.fit(one_hot_encoded_training_predictors, y)","43f430b7":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\nclf = linear_model.RidgeClassifier()\n# clf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n#     decision_function_shape='ovr', degree=3, gamma=0.7, kernel='rbf',\n#     max_iter=-1, probability=False, random_state=None, shrinking=True,\n#     tol=0.001, verbose=False)\n#clf = linear_model.LogisticRegression() #same as ridge\n#clf = DecisionTreeClassifier() #bad performance\n#clf=RandomForestClassifier(n_estimators = 100) #bad performance","0c722a18":"# Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\n# The metric for this competition is F1, so let's use that here.\nscores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=10, scoring=\"f1\")\nscores","b9a55740":"clf.fit(train_vectors, train_df[\"target\"])","c462c9bc":"parameters = { \n    'gamma': [0.7, 1, 'auto', 'scale']\n}\nclf = GridSearchCV(SVC(kernel='rbf'), parameters, cv=5, n_jobs=-1, scoring=\"f1\").fit(train_vectors, train_df[\"target\"]) #SVM-slightly better than ridge","a551ee1d":"clf.best_estimator_","668625e2":"clf.best_score_","7f7cb072":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\ndf = pd.DataFrame({'text' : test_df['text'], 'prediction' : sample_submission[\"target\"]})","482b6ca3":"sample_submission.to_csv(\"submission1.csv\", index=False)","f71a11c3":"import tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","1bc03d85":"X = train_df[\"text\"]\ny = train_df[\"target\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)","0fc65617":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 30 #Based on data exploration\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","07d107f4":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X_train)\npadded = pad_sequences(sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(X_val)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)","953094e1":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nprint(decode_sentence(padded[0]))","09bfb5f9":"print(X_train.values[0])","aab72c2c":"len(word_index)","cbd5e49d":"# Note this is the 100 dimension version of GloVe from Stanford\n# I unzipped and hosted it on my site to make this notebook easier\n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/glove.6B.100d.txt \\\n    -O \/tmp\/glove.6B.100d.txt\nembeddings_index = {};\nvocab_size=len(word_index)\nembedding_dim = 100\nwith open('\/tmp\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","324130e8":"model = tf.keras.Sequential([\n    #Embedding\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), #Each sentence will have 120 words\n#     tf.keras.layers.Flatten(),\n#     tf.keras.layers.Dense(6, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n    \n    #Word embedding with pooling\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n    \n    #LSTM\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n#     tf.keras.layers.Dense(24, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n    \n    #Multi Layer LSTM - Best performing\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n#     tf.keras.layers.Dense(64, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n    \n    #Glove embedding, Drop out etc\n#     tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Conv1D(64, 5, activation='relu'),\n#     tf.keras.layers.MaxPooling1D(pool_size=4),\n#     tf.keras.layers.LSTM(64),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n    \n    #GRU\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n#     tf.keras.layers.Dense(6, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n    \n    #ConvD\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.Conv1D(128, 5, activation='relu'),\n#     tf.keras.layers.GlobalAveragePooling1D(),\n#     tf.keras.layers.Dense(6, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid') \n])\n#model.compile(optimizer='adam', loss=f1_loss, metrics=['accuracy', f1])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC()])\nmodel.summary()","54fc25e4":"num_epochs = 3\nhistory = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_val))","169ca2cc":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","04795ab9":"#model_loss[['accuracy','val_accuracy']].plot(ylim=[0,1]);\nmodel_loss[['auc_9','val_auc_9']].plot(ylim=[0,1]);","8a64dd5e":"testing_sequences2 = tokenizer.texts_to_sequences(test_df.text)\ntesting_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)","8e77a528":"probabilities = model.predict(testing_padded2)","2cf08a35":"predictions = (probabilities > 0.5).astype(int)\npredictions = np.ndarray.flatten(predictions)\npd.value_counts(predictions)","98b3f04e":"original_test_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndf = pd.DataFrame({'text' : original_test_df['text'],'cleaned_text' : test_df['text'], 'prediction' : predictions,'probabilities' : np.ndarray.flatten(probabilities)})\ndf.to_csv(\"test_df.csv\", index=False)","1e0b332f":"df.values[50:100]","8b2fa666":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = predictions\nsample_submission.to_csv(\"submission.csv\", index=False)","a094f344":"import tensorflow_hub as hub\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","6b5ed58b":"module_url = \"https:\/\/tfhub.dev\/google\/nnlm-en-dim128\/2\"\nembed = hub.KerasLayer(module_url)\nembeddings = embed([\"A long sentence.\", \"single-word\",\n                  \"http:\/\/example.com\"])\nprint(embeddings.shape)  #(3,128)","166c8e6e":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/3\")","a81e6423":"X_train_embeddings = embed(train_df.text.values)\nX_test_embeddings = embed(test_df.text.values)","44dfafb7":"params = {\n    'learning_rate': 0.04,\n    'n_estimators': 1500,\n    'colsample_bytree': 0.4,\n    'metric':'auc'\n}","7b63ab4a":"text_clf = LGBMClassifier(**params)","0f97b033":"text_clf.fit(X_train_embeddings['outputs'][:5000,:], train_df.target.values[:5000],\n             eval_set=[(X_train_embeddings['outputs'][:5000,:], train_df.target.values[:5000]),\n                       (X_train_embeddings['outputs'][5000:,:], train_df.target.values[5000:])],\n             verbose=200, early_stopping_rounds=20,\n            )\n","1abdd36d":"text_clf.fit(X_train_embeddings['outputs'][:5000,:], train_df.target.values[:5000])\nY_pred = text_clf.predict(X_train_embeddings['outputs'][5000:])","a7358785":"print(metrics.classification_report(train_df.target[5000:], Y_pred, digits=3),) \nprint(metrics.confusion_matrix(train_df.target[5000:], Y_pred))","7b585f0b":"text_clf.fit(X_train_embeddings['outputs'], train_df.target.values)\npred_test = text_clf.predict(X_test_embeddings['outputs'])","8ceb5281":"df = pd.DataFrame({'cleaned_text' : test_df['text'], 'prediction' : pred_test})\ndf.head(20)","3f369e6a":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = pred_test\nsample_submission.to_csv(\"submission.csv\", index=False)","f7be5ce2":"## Tensorflow Hub  - Universal Sentence Encoder + LightGBM","d98234e2":"## Load Data","ad68e5e7":"#### SVM","d5fed73b":"## Import libraries","4fa62353":"## Clean data","fb1fb44e":"#### Embedding model","8b458728":"Let's do predictions on our training set and build a submission for the competition.","55e197ee":"## AutoML Model","b2f60154":"## Problem Statement\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nIn this competition, we\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. We have access to a dataset of 10,000 tweets that were hand classified.","16581613":"## Simple Classifier Models","58c5ce71":"### Building vectors\n\nThe theory behind the model we'll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n\nWe'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nNote: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.","60fbdbcf":"## Common libraries","3469a953":"### Ridge and SVM classifier for text data\n\nAs we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a _linear_ connection. So let's build a linear model and see!","b7b68918":"## Tensorflow model","4752ddf5":"### Random Forest classifier for non-text data","828667e5":"This section of code cleans the tweet text using tweet preprocesser library"}}