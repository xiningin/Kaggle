{"cell_type":{"be8d60e5":"code","bcf2ac90":"code","017a5ca7":"code","918371a8":"code","b0dd72ba":"code","f2c3c49a":"code","74cd96fc":"code","e53e43e1":"code","deadd79d":"code","eaf8c505":"code","be442174":"code","903364b6":"code","fe364018":"code","da0e6e2e":"code","74ad4acb":"code","2d215f4f":"code","0428cb41":"code","468231dd":"code","770de7de":"code","2217975f":"code","c1268a06":"code","741d8a41":"code","3c6350e6":"code","3b0d3fc3":"code","fac0b5da":"code","51c9753f":"code","ad221951":"code","d9d5ada4":"code","612d767c":"code","a786fb10":"code","d82a11b5":"code","2d2ce4de":"code","452f8575":"code","4655fcd7":"code","9532b0d9":"code","221d2493":"code","5b71d27c":"code","bfe194d0":"code","132c0a8a":"code","de6f8579":"code","a8429167":"code","a228a341":"markdown","054072df":"markdown","eb0c5bab":"markdown","b6dd1547":"markdown","fda0df7d":"markdown","69590c2a":"markdown","4587ea39":"markdown"},"source":{"be8d60e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bcf2ac90":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)  \nfrom sklearn import metrics, preprocessing, model_selection\nfrom sklearn.model_selection import train_test_split,cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt                                      # to plot graph\n%matplotlib inline\nimport xgboost as xgb\nimport lightgbm as lgb\nSEED = 1\n\n#To ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","017a5ca7":"file = r'\/kaggle\/input\/analytics-vidhya-janatahack-customer-segmentation\/'\ntrain_df = pd.read_csv(file+'Train_aBjfeNk.csv')\ntest_df = pd.read_csv(file+'Test_LqhgPWU.csv')\nsub_df = pd.read_csv(file+'sample_submission_wyi0h0z.csv')","918371a8":"train_df.head()","b0dd72ba":"test_df.head()","f2c3c49a":"sub_df.head()","74cd96fc":"print(train_df.shape, test_df.shape,sub_df.shape)","e53e43e1":"train_df['Segmentation'].value_counts()","deadd79d":"train_df.isnull().sum()","eaf8c505":"test_df.isnull().sum()","be442174":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(8,6))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","903364b6":"missingvalues_prop = (train_df.isnull().sum()\/len(train_df)).reset_index()\nmissingvalues_prop.columns = ['field','proportion']\nmissingvalues_prop = missingvalues_prop.sort_values(by = 'proportion', ascending = False)\n# print(missingvalues_prop)\nmissingvaluescols = missingvalues_prop[missingvalues_prop['proportion'] > 0.10].field.tolist()\nprint(missingvaluescols)","fe364018":"# Normalise can be set to true to print the proportions instead of Numbers.\ntrain_df['Segmentation'].value_counts(normalize=True)","da0e6e2e":"train_df['Segmentation'].value_counts().plot.bar(figsize=(4,4),title='Segmentation - Split for Train Dataset')\nplt.xlabel('ExtraTime')\nplt.ylabel('Count')","74ad4acb":"plt.figure(1)\nplt.subplot(221)\ntrain_df['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Gender', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\n\nplt.subplot(222)\ntrain_df['Ever_Married'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Ever_Married', fontweight=\"bold\",fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\nplt.subplot(223)\ntrain_df['Graduated'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Graduated', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\nplt.subplot(224)\ntrain_df['Work_Experience'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Work_Experience', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\nplt.tight_layout()","2d215f4f":"plt.figure(1)\nplt.subplot(221)\ntrain_df['Spending_Score'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Spending_Score', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\n\nplt.subplot(222)\ntrain_df['Family_Size'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Family_Size', fontweight=\"bold\",fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\nplt.subplot(223)\ntrain_df['Var_1'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Var_1', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n","0428cb41":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train_df['Age'])\n\nplt.subplot(122)\ntrain_df['Age'].plot.box(figsize=(16,5))\n\nplt.show()","468231dd":"train_df.columns","770de7de":"Gender=pd.crosstab(train_df['Gender'],train_df['Segmentation'])\nEver_Married=pd.crosstab(train_df['Ever_Married'],train_df['Segmentation'])\nGraduated=pd.crosstab(train_df['Graduated'],train_df['Segmentation'])\nProfession=pd.crosstab(train_df['Profession'],train_df['Segmentation'])\n\n\n\nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nEver_Married.div(Ever_Married.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nGraduated.div(Graduated.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nProfession.div(Profession.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nplt.tight_layout()\n","2217975f":"Work_Experience=pd.crosstab(train_df['Work_Experience'],train_df['Segmentation'])\nEver_Married=pd.crosstab(train_df['Ever_Married'],train_df['Segmentation'])\nGraduated=pd.crosstab(train_df['Graduated'],train_df['Segmentation'])\nProfession=pd.crosstab(train_df['Profession'],train_df['Segmentation'])\n\n\n\nWork_Experience.div(Work_Experience.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nEver_Married.div(Ever_Married.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nGraduated.div(Graduated.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nProfession.div(Profession.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\n\n","c1268a06":"# * join the datasets\ntrain_df['is_train']  = 1\ntest_df['Segmentation'] = -1\ntest_df['is_train'] = 0","741d8a41":"full_df = train_df.append(test_df)","3c6350e6":"full_df.head()","3b0d3fc3":"full_df.dtypes","fac0b5da":"full_df.isnull().sum()","51c9753f":"# append train and test data\ntestcount = len(test_df)\ncount = len(full_df)-testcount\nprint(count)\n\ntrain = full_df[:count]\ntest = full_df[count:]\ntrain_df = train.copy()\ntest_df = test.copy()","ad221951":"full_df.columns","d9d5ada4":"cols = ['ID', 'Gender', 'Ever_Married', 'Age', 'Graduated', 'Profession',\n       'Work_Experience', 'Spending_Score', 'Family_Size', 'Var_1',\n        'is_train' ]\nfor col in cols:\n    if train_df[col].dtype==object:\n        print(col)\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n        train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n        test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))","612d767c":"X = train_df.drop(['Segmentation', 'is_train' ,'ID'],axis=1)\ny = train_df['Segmentation'].values\n\ntrain_X = X.copy()\ntrain_y = y.copy()\n\ntest_X = test_df.drop(['Segmentation', 'is_train' ,'ID'],axis=1)\nprint(X.shape, test_X.shape)","a786fb10":"X.head()","d82a11b5":"test_X.head()","2d2ce4de":"X.isnull().sum()","452f8575":"params = {}\nparams['learning_rate'] = 0.01\nparams['n_estimators'] = 10000\nparams['objective'] = 'multiclass'\nparams['boosting_type'] = 'gbdt'","4655fcd7":"feature_cols = X.columns","9532b0d9":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                  stratify=y, \n                                                  random_state=1234, \n                                                  test_size=0.20, shuffle=True)","221d2493":"cat_cols = ['Gender','Ever_Married', 'Graduated', 'Profession','Family_Size',\n            'Spending_Score','Var_1']\nlabel_col = 'Segmentation'","5b71d27c":"clf = lgb.LGBMClassifier(**params)\n    \nclf.fit(X_train, y_train, early_stopping_rounds=200,\n        eval_set=[(X_valid, y_valid)], \n        eval_metric='multi_error', verbose=False, categorical_feature=cat_cols)\n\neval_score = accuracy_score(y_valid, clf.predict(X_valid[feature_cols]))\n\nprint('Eval ACC: {}'.format(eval_score))","bfe194d0":"preds = clf.predict(test_X[feature_cols])","132c0a8a":"np.unique(preds, return_counts=True)","de6f8579":"sub_df['Segmentation'] = preds\n\nsub_df.head()","a8429167":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nname = \"baseline_lgb.csv\"\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = name):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(sub_df)","a228a341":"![image.png](attachment:image.png)","054072df":"### Feature Engineering","eb0c5bab":"![image.png](attachment:image.png)","b6dd1547":"https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/#About","fda0df7d":"#### Checking missing values","69590c2a":"### Exploratory Data Analysis","4587ea39":"![image.png](attachment:image.png)"}}