{"cell_type":{"a50c0821":"code","e1ffe0a6":"code","68b1aea4":"code","81268c3a":"code","db51cf89":"code","44591b74":"code","461d9ae5":"code","a6ae11e1":"code","97289ea9":"code","fd7bbe85":"code","21a32b02":"code","3460d7aa":"code","b41a7481":"code","8dbf4f73":"code","8733cfa4":"markdown","7653ea8b":"markdown","eb6b7dc3":"markdown","cb168718":"markdown","1bf5ecec":"markdown","31fad9fd":"markdown","527f5ce7":"markdown","20bbceb7":"markdown","31493159":"markdown","bf0b5911":"markdown","20e432dc":"markdown","9540d1b9":"markdown","327f0416":"markdown","ccb6cbc8":"markdown","4452c0c8":"markdown"},"source":{"a50c0821":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout\nfrom tensorflow.keras.models import load_model","e1ffe0a6":"# Input\nfile_train = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\nfile_test = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\n# Output\nmodel_output_file = 'NN_1D_CSV_model.h5'\n\n# Model specific\ntarget = 'SalePrice'\nindex_col = 'Id'\n\n# Validation Split, to separate testing and validation data\nVALIDATION_SPLIT_SIZE = 0.2","68b1aea4":"def load_raw_data(file_path):\n    return pd.read_csv(file_path, index_col = index_col)","81268c3a":"NA_THRESHOLD = 200\nNA_CATEGORICAL_SUBSTITUTION = 'NULL'\nNA_NUMERICAL_SUBSTITUTION = 0\n\ndef feature_extract(data):\n        feature_columns = []\n        # Columns with high missing values\n        highna = [cname for cname in data.columns if data[cname].isna().sum() > NA_THRESHOLD]\n        lowna = [cname for cname in data.columns if data[cname].isna().sum() <= NA_THRESHOLD]\n        \n        # Dropping columns with high number of missing values\n        data = data.drop(highna, axis=1)\n\n        # Low cardinality cols only\n        categorical_cols = [cname for cname in data.columns\n                                if data[cname].nunique() < 10 and data[cname].dtype == \"object\"]\n        numeric_cols = [cname for cname in data.columns if data[cname].dtype in ['int64', 'float64']]\n        if target in categorical_cols: categorical_cols.remove(target)\n        if target in numeric_cols: numeric_cols.remove(target)\n\n        return feature_columns, categorical_cols, numeric_cols\n    ","db51cf89":"def prep_features(data, categorical_cols, numeric_cols, x_train = None):\n    empty_fill = {}\n    for feature in categorical_cols:\n        empty_fill[feature] = NA_CATEGORICAL_SUBSTITUTION\n    for feature in numeric_cols:\n        empty_fill[feature] = NA_NUMERICAL_SUBSTITUTION\n    data = data.fillna(empty_fill)\n\n    data = data[categorical_cols + numeric_cols]\n\n    # One Hot Numeric values\n    x = pd.get_dummies(data, dummy_na=True)\n    if x_train is not None:\n        x_train, x = x_train.align(x, join='left', axis=1)\n        x = x.fillna(0)\n    else:\n        x_train = x\n\n    # Normalization on numerical values\n    normed_x = norm(x, x_train)\n\n    return normed_x\n    \ndef norm(x, x_train):\n    train_stats = x_train.describe()\n    train_stats = train_stats.transpose()\n    normalized =  (x - train_stats['mean']) \/ train_stats['std']\n    return normalized.fillna(0)","44591b74":"def generate_model (feature_len, optimizer, dropout = 0.3, units = 64, leaky = 0.05):\n    model = Sequential([\n        layers.Dense(units, activation='relu', input_shape=feature_len),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(alpha=leaky),\n        layers.Dropout(dropout),\n        layers.Dense(1)\n    ])\n    model.compile(loss='mse',\n                       optimizer=optimizer,\n                       metrics=['mae', 'mse'])\n    return model","461d9ae5":"def evaluate_model (x_train, y_train, x_val, y_val, optimizer, dropout = 0.3, units = 64, leaky = 0.05, epochs = 10):\n    model = generate_model(feature_len = [len(x_train.keys())],\n                           optimizer = optimizer,\n                           dropout = dropout,\n                           units = units,\n                           leaky = leaky)\n    model.fit(x = x_train,\n              y = y_train,\n              epochs = epochs,\n              validation_split = 0.2,\n              verbose = 0)\n    loss, mae, mse = model.evaluate(x_val, y_val, verbose=0)\n    return mae\n    \ndef evaluate_model_parameters (x_train, y_train, x_val, y_val):\n    evaluating_optimizers = [tf.keras.optimizers.RMSprop(0.001),\n                             tf.keras.optimizers.Adagrad(learning_rate=0.001),\n                             tf.keras.optimizers.Adadelta(0.001),\n                             tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n                             ]\n    evaluating_dropouts = [0.9, 0.8, 0.7, 0.6, 0.5,0.25,0.1]\n    evaluating_units = [32, 64, 128]\n    evaluating_leak = [0.05, 0.1, 0.01]\n    evaluating_epochs = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\n    # Basic Search\n    best_score = float(\"inf\")\n    best_optimizer = evaluating_optimizers[0]\n    for optimizer in evaluating_optimizers:\n        score = evaluate_model (x_train, y_train, x_val, y_val, optimizer)\n        if best_score > score:\n            best_score = score\n            best_optimizer = optimizer\n\n    best_score = float(\"inf\")\n    best_dropout = evaluating_dropouts[0]\n    for dropout in evaluating_dropouts:\n        score = evaluate_model (x_train, y_train, x_val, y_val, best_optimizer,\n                                dropout = dropout)\n        if best_score > score:\n            best_score = score\n            best_dropout = dropout\n\n    best_score = float(\"inf\")\n    best_units = evaluating_units[0]\n    for units in evaluating_units:\n        score = evaluate_model (x_train, y_train, x_val, y_val, best_optimizer,\n                                dropout = best_dropout,\n                                units = units)\n        if best_score > score:\n            best_score = score\n            best_units = units\n\n    best_score = float(\"inf\")\n    best_epochs = evaluating_epochs[0]\n    for epochs in evaluating_epochs:\n        score = evaluate_model (x_train, y_train, x_val, y_val, best_optimizer,\n                                dropout = best_dropout,\n                                units = best_units,\n                                epochs = epochs)\n        if best_score > score:\n            best_score = score\n            best_epochs = epochs\n\n    best_score = float(\"inf\")\n    best_leak = evaluating_leak[0]\n    for leaky in evaluating_leak:\n        score = evaluate_model (x_train, y_train, x_val, y_val, best_optimizer,\n                                dropout = best_dropout,\n                                units = best_units,\n                                leaky = leaky,\n                                epochs = epochs)\n        if best_score > score:\n            best_score = score\n            best_leak = leaky\n\n    return best_optimizer, best_dropout, best_units, best_epochs, best_leak","a6ae11e1":"# Load training data\nraw_train = load_raw_data(file_train)\nraw_train, raw_val = train_test_split(raw_train, test_size = VALIDATION_SPLIT_SIZE)\n\n# Prepocessing Training Data\nfeature_columns, categorical_cols, numeric_cols = feature_extract(raw_train)\n\nx_train = prep_features(raw_train, categorical_cols, numeric_cols)        \nx_val = prep_features(raw_val, categorical_cols, numeric_cols, x_train)\n\ny_train = raw_train[target]\ny_val = raw_val[target]\n","97289ea9":"best_optimizer, best_dropout, best_units, best_epochs, best_leak = evaluate_model_parameters(x_train,\n                                                                                             y_train,\n                                                                                             x_val,\n                                                                                             y_val)\n\nprint ('Best optimizer: ')\nprint (best_optimizer)\nprint ('Best dropout: ' + str(best_dropout))\nprint ('Best units: ' + str(best_units))\nprint ('Best leak factor: ' + str(best_leak))\nprint ('Best epochs: ' + str(best_epochs))\n","fd7bbe85":"model = generate_model(feature_len = [len(x_train.keys())],\n                       optimizer = best_optimizer,\n                       dropout = best_dropout,\n                       units = best_units,\n                       leaky = best_leak)\nmodel.fit(x = x_train,\n          y = y_train,\n          epochs = best_epochs,\n          validation_split = 0.2,\n          verbose = 0)\nloss, mae, mse = model.evaluate(x_val, y_val, verbose=2)\nprint ('MAE: ' + str(mae))\nprint ('MSE: ' + str(mse))\nprint ('Loss: ' + str(loss))","21a32b02":"raw_test = load_raw_data(file_test)\nx_test = prep_features(raw_test, categorical_cols, numeric_cols, x_train)\npreds = model.predict(x_test).flatten()","3460d7aa":"output = pd.DataFrame({'Id': x_test.index,\n                       target: preds})\noutput = output.fillna(y_train.mean())\noutput.to_csv('submission.csv', index=False)\n","b41a7481":"## Saving the model\nmodel.save(model_output_file)\n\n## saving x_train meta data: categorical_cols, numeric_cols\nx_train_json = json.dumps({\"categorical\": categorical_cols, \"numerical\": numeric_cols}, indent = 4) \n  \n# Writing to sample.json \nwith open(\"x_train.json\", \"w\") as outfile: \n    outfile.write(json_object) ","8dbf4f73":"## Loading the model\nmodel = load_model(model_output_file)\n\nwith open('x_train.json', 'r') as openfile: \n    x_train_meta = json.load(openfile) \n\npreds = model.predict(x=x_test, batch_size=100, verbose=0)\nprint(\"Predictions:\")\nprint(preds)","8733cfa4":"# Exporting the model","7653ea8b":"# Model\nWe have build a function to parametrize building the neural network. We have 5 layers: the input, a layer for Normalization, a layer to prevent leaky ReLU, a Dropout and the output. The normalization layer and the leaky ReLU will prevent to converge to NA values. The dropout prevent overfiting, randomly discarting samples on each iteration.","eb6b7dc3":"# Main script\nHere we call to our functions to load the data, prepocessing the training data, evaluate the model and training it.","cb168718":"We define the constants that are specific of the data model, such as the target column, and the names and paths of the files","1bf5ecec":"# Scope and Goals\nThis notebook is an illustrative example for using Keras Neural Networks for regression on a data sets with heterogeneous variables.\n\nThe data used is from [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques), a competition data set for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.\n\nOne of the most pivotal part to build an efficient model for machine learning is analysing the data and its features. We are leaving this analysis out of this notebook, and we focus mainly on building the model. Therefore, we leave out of the scope: feature exploration, analysis of the correlation of the features, data formating (e.g. transforming data categorical data collected as numbers in the proper format) and data condensation.\n\nAdditionally, we show an automated process to tweak the parameters of the model. This process is very simple and could be substituted by more interesting exploratory searches. A* returns complete searches, however, we rather suggest to be more practical and use local (neighborhood) searches, like Tabu search, Simulated annealing, ACO or genetic algorithms.","31fad9fd":"# Evaluating the model\nThe next step is to build the model. We want to evalute the performance of the model with different parameters. This process is very simple and could be substituted by more interesting exploratory searches. A* returns complete searches, however, we rather suggest to be more practical and use local (neighborhood) searches, like Tabu search, Simulated annealing, ACO or genetic algorithms.","527f5ce7":"# Loading data\nWe are using the following function to load raw data","20bbceb7":"# Cleaning Data\nWithin a given dataset, we ofter find missing data. Furthermore, the regression is done based on logic or numeric features, therefore, we want to convert any discrete feature into a categorical representation. We are using [one_hot](https:\/\/en.wikipedia.org\/wiki\/One-hot) for this, but we are excluding features with an elevate number of discrete values. Furthermore, as we are not doing a proper analysis on the data, we are not converting categorical values currently represented in a numeric format.","31493159":"Trainig the model with the parameters returned from the evaluation","bf0b5911":"# Testing the model\n","20e432dc":"Evaluating the parameters of the model: optimizer, dropout factor, units in the dense layer, leak factor and epochs in the training.","9540d1b9":"## Verifying exported model","327f0416":"First, we load the libraries that we are using in the notebook","ccb6cbc8":"# Preprocessing Data\nWe use the following function to clean and give format to the raw data. We assign a constant to the missing values and normalize the data. Note that the normalization has to be done using the stats from the trained (known) data.","4452c0c8":"# Saving the output"}}