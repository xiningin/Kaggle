{"cell_type":{"92b4bafb":"code","c77de068":"code","7cd72367":"code","6185a530":"code","ebddb0d6":"code","8d11e12d":"code","afeb9d92":"code","8b8281cf":"code","b43169b5":"code","785efa90":"code","2f63629f":"code","c29cd109":"code","caada091":"code","34686263":"code","0740f374":"code","a9d6da29":"code","df13f80a":"code","b19f89fe":"code","979536e7":"code","5af24590":"code","8f2ebd56":"code","e2aeec91":"code","87bab77a":"code","24bc6281":"code","4f4a6eeb":"code","4978bc1f":"code","f29c8eb3":"code","87c9c84e":"code","005f224b":"code","fc9d0c46":"code","2c6f9790":"code","02e2e9e5":"markdown","ed496567":"markdown","d0030306":"markdown","95246b7d":"markdown","4a3730e6":"markdown","75945202":"markdown","a4a45439":"markdown","cacea8dd":"markdown","78e7a479":"markdown","a34c6c1e":"markdown","9dae8a24":"markdown","58275dc9":"markdown","c6bad2ed":"markdown","5a20a37f":"markdown","66dbbc9c":"markdown","d169c9f5":"markdown","83b699e3":"markdown","210ea93d":"markdown","232dbaad":"markdown","2df19250":"markdown","83db57e3":"markdown"},"source":{"92b4bafb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sklearn\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV","c77de068":"train_df = pd.read_csv(\"..\/input\/jantahack\/train_fNxu4vz.csv\")\ntest_df = pd.read_csv(\"..\/input\/jantahack\/test_fjtUOL8.csv\")\nsubm = pd.read_csv(\"..\/input\/jantahack\/sample_submission_HSqiq1Q.csv\")\n\ntrain = train_df.copy()\ntest = test_df.copy()","7cd72367":"train_df.info()","6185a530":"## Checking for missing values in terms of percentage\n(train_df.isnull().sum()\/len(train_df))*100","ebddb0d6":"(test_df.isnull().sum()\/len(train_df))*100\n\n## Test Data also has same pattern for missing data","8d11e12d":"train_df[\"Interest_Rate\"].value_counts().plot(kind=\"bar\");\n\n## Number of samples of all the three categories are not too far from one another i.e We are not dealing with imbalanced\n## classification.","afeb9d92":"train_df[\"Home_Owner\"].value_counts().plot(kind=\"bar\");","8b8281cf":"train_df[\"Length_Employed\"].value_counts().plot(kind=\"bar\");\n\n# No specific trend binning them into two or three groups might be helpful","b43169b5":"train_df[\"Purpose_Of_Loan\"].value_counts().plot(kind=\"bar\");","785efa90":"sns.distplot(train_df[\"Annual_Income\"]);\n\n# The data is highly skewed applying log transformation may work ","2f63629f":"## Applying log transformation\nsns.distplot(np.log(train_df[\"Annual_Income\"]));\n\n# This is better as it looks like Gaussian curve ","c29cd109":"sns.distplot(train_df[\"Debt_To_Income\"]);\n\n# Most of the debt to income values lies betwwen 10 to 30","caada091":"train_df[\"Inquiries_Last_6Mo\"].value_counts().plot(kind=\"bar\");\n\n# Most applicant haven't even enquired about the loan\n# Binning categories >4 into one will reduce categories","34686263":"sns.distplot(train_df[\"Months_Since_Deliquency\"]);","0740f374":"## Converting loan amt as numerical column\ntrain_df[\"Loan_Amount_Requested\"]= train_df[\"Loan_Amount_Requested\"].str.replace('\\D+','').astype(int)\ntest_df[\"Loan_Amount_Requested\"]= test_df[\"Loan_Amount_Requested\"].str.replace('\\D+','').astype(int)","a9d6da29":"sns.distplot(train_df[\"Loan_Amount_Requested\"]);\n\n# Loan amount req has multiple peaks.I guess there might be some categories in\n# which one can apply loan like between 10k to 20k which is inducing such pattern","df13f80a":"plt.figure(figsize=(20,10))\ntemp = train_df.drop(\"Loan_ID\",axis=1)\ncorr = temp.corr()\nsns.heatmap(corr,vmin=-1, vmax=1, center=0,cmap=sns.diverging_palette(20, 220, n=200),square=True,annot=True);","b19f89fe":"train = pd.read_csv(\"..\/input\/jantahack\/train_fNxu4vz.csv\")\ntest = pd.read_csv(\"..\/input\/jantahack\/test_fjtUOL8.csv\")\nsubm = pd.read_csv(\"..\/input\/jantahack\/sample_submission_HSqiq1Q.csv\")\n\n\ntrain_df = train.copy()\ntest_df = test.copy()\n\n\ntest_df[\"Interest_Rate\"] = 21 # dummy value\n# Joining train and test data as it will be easy to pre-process them once\njoined_df = pd.concat([train_df,test_df],axis=0)\njoined_df = joined_df.reset_index(drop=True)\nprint(f'Og Train shape {train_df.shape} \\nOg Test Shape {test_df.shape} \\nCombined Df {joined_df.shape}')\n\n\n## Dropping loan id\njoined_df.drop(\"Loan_ID\",1,inplace=True)\n\n## Coverting to int \njoined_df[\"Loan_Amount_Requested\"]= joined_df[\"Loan_Amount_Requested\"].str.replace('\\D+','').astype(int)\n\n\n## Because of  XGBOOST specifically since column names don't contains '<,>,]'\njoined_df[\"Length_Employed\"] = joined_df[\"Length_Employed\"].replace(to_replace='10+ years',value=\"more_than_10yr\")\njoined_df[\"Length_Employed\"] = joined_df[\"Length_Employed\"].replace(to_replace='< 1 year',value=\"less_than_1yr\")\n\n\n\n## Numerical and cat cols\nnumerical = [col for col in joined_df.columns if joined_df[col].dtype!='object']\ncategorical = [col for col in joined_df.columns if joined_df[col].dtype =='object']\nnumerical.remove(\"Interest_Rate\")\n\n\n## Imuting missing values Using mode for categorical and median for numerical as it is more robust than mean\njoined_df[\"Length_Employed\"].fillna(joined_df[\"Length_Employed\"].mode()[0],inplace=True)\njoined_df[\"Home_Owner\"].fillna(joined_df[\"Home_Owner\"].mode()[0],inplace=True)\njoined_df[\"Annual_Income\"].fillna(joined_df[\"Annual_Income\"].median(),inplace=True)\njoined_df[\"Months_Since_Deliquency\"].fillna(joined_df[\"Months_Since_Deliquency\"].median(),inplace=True)\n\n\n\n## Standard Scaling for numerical columns\nscaler = StandardScaler()\nnumerical_data = pd.DataFrame(scaler.fit_transform(joined_df[numerical]))\nnumerical_data.columns = numerical\n\n\n##  One Hot Encoding for categorical columns\njoined_df = pd.concat([ \n            numerical_data,\n            pd.get_dummies(joined_df['Length_Employed'],drop_first = True),\n            pd.get_dummies(joined_df['Home_Owner'],drop_first = True),\n            pd.get_dummies(joined_df['Income_Verified'],drop_first = True),\n            pd.get_dummies(joined_df['Purpose_Of_Loan'],drop_first = True),\n            pd.get_dummies(joined_df['Gender'],drop_first = True),\n            joined_df[\"Interest_Rate\"]\n            \n            ],axis=1)\n\n\n## Let's separate train and test \n\ntrain_df = joined_df[:164309]\ntest_df = joined_df[164309:]\n\ntest_df.reset_index(drop=True,inplace=True) # since indexes were not starting from zero\ntest_df.drop([\"Interest_Rate\"],axis=1,inplace=True)\nprint(f'Final Train shape {train_df.shape} \\nFinal Test Shape {test_df.shape}')\n\n\nX_train,y_train = train_df.drop(\"Interest_Rate\",1),train_df.loc[:,\"Interest_Rate\"]\nprint(f'Training shape {X_train.shape,y_train.shape}')","979536e7":"def cross_val_evaluate(model,X,y,cv,scoring,verbose,model_name):\n    weighted_f1s = cross_val_score(model,X,y,cv=cv,scoring=scoring,verbose=verbose,n_jobs=-1)\n    mean_weighted_f1 = round(np.sum(weighted_f1s)\/cv,5)\n    print(f\" -----------------------{model_name}-------------------------------\")\n    print(f\" weightedF1 for folds = {weighted_f1s}\\n And Mean weighted_f1 on cv = {mean_weighted_f1}\\n\\n\")","5af24590":"log = LogisticRegression()\ncross_val_evaluate(log,X_train,y_train,4,\"f1_weighted\",4,\"LOG\")\n\nknn = KNeighborsClassifier()\ncross_val_evaluate(knn,X_train,y_train,4,\"f1_weighted\",4,\"KNN\")\n\ngnb = GaussianNB()\ncross_val_evaluate(gnb,X_train,y_train,4,\"f1_weighted\",4,\"GNB\")\n\ndtc = DecisionTreeClassifier()\ncross_val_evaluate(dtc,X_train,y_train,4,\"f1_weighted\",4,\"DTC\")\n\nrfc = RandomForestClassifier()\ncross_val_evaluate(rfc,X_train,y_train,4,\"f1_weighted\",4,\"RFC\")\n\nxtc = ExtraTreesClassifier()\ncross_val_evaluate(xtc,X_train,y_train,4,\"f1_weighted\",4,\"XTC\")\n\ngbc = GradientBoostingClassifier()\ncross_val_evaluate(gbc,X_train,y_train,4,\"f1_weighted\",4,\"GBC\")\n\nxgb = XGBClassifier()\ncross_val_evaluate(xgb,X_train,y_train,4,\"f1_weighted\",4,\"XGB\")","8f2ebd56":"## Data Creation\n\ntrain = pd.read_csv(\"..\/input\/jantahack\/train_fNxu4vz.csv\")\ntest = pd.read_csv(\"..\/input\/jantahack\/test_fjtUOL8.csv\")\nsubm = pd.read_csv(\"..\/input\/jantahack\/sample_submission_HSqiq1Q.csv\")\n\n\ntrain_df = train.copy()\ntest_df = test.copy()\n\n\ntest_df[\"Interest_Rate\"] = 21 # dummy value\n# Joining train and test data as it will be easy to pre-process them once\njoined_df = pd.concat([train_df,test_df],axis=0)\njoined_df = joined_df.reset_index(drop=True)\nprint(f'Og Train shape {train_df.shape} \\nOg Test Shape {test_df.shape} \\nCombined Df {joined_df.shape}')\n\n\n## Dropping loan id\njoined_df.drop(\"Loan_ID\",1,inplace=True)\nprint(f'Shape after dropping loan id {joined_df.shape}')\n\n## Coverting to int \njoined_df[\"Loan_Amount_Requested\"]= joined_df[\"Loan_Amount_Requested\"].str.replace('\\D+','').astype(int)\n\n\n## Because of  XGBOOST specifically since column names don't contains '<,>,]'\njoined_df[\"Length_Employed\"] = joined_df[\"Length_Employed\"].replace(to_replace='10+ years',value=\"more_than_10yr\")\njoined_df[\"Length_Employed\"] = joined_df[\"Length_Employed\"].replace(to_replace='< 1 year',value=\"less_than_1yr\")\n\n\n","e2aeec91":"## What is the loan amount w.r.t Loan amount requested by applicant ?\n\njoined_df[\"LA_per_AI\"] = np.round(joined_df[\"Loan_Amount_Requested\"]\/joined_df[\"Annual_Income\"],4)\nsns.distplot(joined_df[\"LA_per_AI\"]);\n\n# I thought the loan amount requested by an applicant w.r.t his\/her annual income might be a useful ratio \n# to decide in which category of Interest rate an applicant falls.\n\n# Because the smaller this ratio would be less time an applicant will need to repay the amount and vice versa .\n# Hence charge interest rate accordingly  ","87bab77a":"##  creating no of close account\njoined_df[\"no_close_accnts\"] = joined_df[\"Total_Accounts\"]-joined_df[\"Number_Open_Accounts\"]\n\n# How many total accounts does an applicant have w.r.t total accounts conveys a message that does this person has more\n# closed account then open which is not a good sign for the banks.Because the more credit score , the more payment history\n# and reliabilty is better when applying loans to decide interest rate\njoined_df[\"Open_per_Total\"] = round(joined_df[\"Number_Open_Accounts\"]\/joined_df[\"Total_Accounts\"],4)\n\n# Looking at the distribution of data\nsns.distplot(joined_df[\"Open_per_Total\"]);","24bc6281":"## Income_split_per_accnt communicates amount an applicant can split into his various accounts .\n## Eg if this value is very small then it conveys that this person has more accounts w.r.t his income which is not a good sign\n\njoined_df[\"Income_split_per_accnt\"] = round(joined_df[\"Annual_Income\"]\/(joined_df[\"Number_Open_Accounts\"]+1),4)\nsns.distplot(joined_df[\"Income_split_per_accnt\"]);\n","4f4a6eeb":"# The data looks quite skewed hence taking log.\njoined_df[\"Income_split_per_accnt\"] = np.log(joined_df[\"Income_split_per_accnt\"])\nsns.distplot(joined_df[\"Income_split_per_accnt\"]);\n\n# This looks more like gaussian curve ","4978bc1f":"## Standard Scaling for numerical columns\n\n## Numerical and cat cols\nnumerical = [col for col in joined_df.columns if joined_df[col].dtype!='object']\ncategorical = [col for col in joined_df.columns if joined_df[col].dtype =='object']\nnumerical.remove(\"Interest_Rate\")\n\nscaler = StandardScaler()\nnumerical_data = pd.DataFrame(scaler.fit_transform(joined_df[numerical]))\nnumerical_data.columns = numerical\n\n\n##  One Hot Encoding for categorical columns\njoined_df = pd.concat([ \n            numerical_data,\n            pd.get_dummies(joined_df['Length_Employed'],drop_first = True),\n            pd.get_dummies(joined_df['Home_Owner'],drop_first = True),\n            pd.get_dummies(joined_df['Income_Verified'],drop_first = True),\n            pd.get_dummies(joined_df['Purpose_Of_Loan'],drop_first = True),\n            pd.get_dummies(joined_df['Gender'],drop_first = True),\n            joined_df[\"Interest_Rate\"]\n            \n            ],axis=1)\n\n\n## Let's separate train and test \n\ntrain_df = joined_df[:164309]\ntest_df = joined_df[164309:]\n\ntest_df.reset_index(drop=True,inplace=True) # since indexes were not starting from zero\ntest_df.drop([\"Interest_Rate\"],axis=1,inplace=True)\nprint(f'Final Train shape {train_df.shape} \\nFinal Test Shape {test_df.shape}')\n","f29c8eb3":"## Creating a list of categorical columns which \n\ncolumns = train[\"Purpose_Of_Loan\"].value_counts().index.tolist()+train[\"Length_Employed\"].value_counts().index.tolist()\ncolumns.extend([\"more_than_10yr\",\"less_than_1yr\"])\ncolumns.remove('10+ years')\ncolumns.remove('< 1 year')\nprint(*columns,sep=\", \")","87c9c84e":"print(f'Originally shape of train = {train_df.shape} and shape of test = {test_df.shape}')\nfor col in columns:\n    if col in train_df.columns.to_list() and train_df[col].value_counts()[1] < 5000:\n        train_df.drop(col,1,inplace=True)\n        test_df.drop(col,1,inplace=True)\n        \nprint(f'After shape of train = {train_df.shape} and shape of test = {test_df.shape}')","005f224b":"X_train,y_train = train_df.drop(\"Interest_Rate\",1),train_df.loc[:,\"Interest_Rate\"]\nprint(f'Training shape {X_train.shape,y_train.shape}')\n\nxgb = XGBClassifier(\n                learning_rate=0.2,\n                n_estimators=150,\n                subsample=1,\n                colsample_bytree=1,\n                objective='multi:softprob', \n                n_jobs=-1,\n                scale_pos_weight=None, \n                verbosity=3,\n                max_depth=6,\n                min_child_weight =14,\n                gamma = 0.3\n               )\n","fc9d0c46":"xgb.fit(X_train,y_train)\npreds = xgb.predict(test_df)\nsubm[\"Interest_Rate\"] = preds\nsubm.to_csv(\".\/xgb.csv\",index=False)","2c6f9790":"plt.figure(figsize=(16,8))\nplt.bar(X_train.columns.to_list(), xgb.feature_importances_);\nplt.xticks(rotation=90);","02e2e9e5":"## Feature Importance ","ed496567":"## Feature Engineering\n- This time I won't do any kind of imputation \n- I created few features which I thought might be useful","d0030306":"- Months_Since_Deliquency has almost 50% of missing data\n- Home owner and annual income has 15% missing data and these features according to hypothesis are pretty important ","95246b7d":"## INFERENCE\n- From all the above weighted f1 score it's obvious that XGBoost is better then rest of the classifier.Hence I proceeded with that model and performed feature engineering , hyper-paremeter tuning and different experimentations with it","4a3730e6":"**These were the few features which I created revolving around Loan Amt , Annual income and Number of Accounts as per my research from online and my intuition**","75945202":"## Data Description\n\n**Meaning of some variable for my reference**\n\n* **Home_Owner** : - The home ownership status provided by the borrower during registration. Values are: Rent, Own, Mortgage, Other.\n\n* **Income_Verified** : - Indicates if income was verified, not verified, or if the income source was verified\n\n* **Purpose_Of_Loan** : - A category provided by the borrower for the loan request. \n\n* **Debt_To_Income** : - A ratio calculated using the borrower\u2019s total monthly debt payments on the total debt obligations, excluding mortgage and the requested loan, divided by the borrower\u2019s self-reported monthly income.\n\n* **Inquiries_Last_6Mo** : -The number of inquiries by creditors during the past 6 months.\n\n* **Months_Since_Deliquency** : -The number of months since the borrower's last delinquency.\nDelinquency means that you are behind on payments. Once you are delinquent for a certain period of time (usually nine months for federal loans), your lender will declare the loan to be in default. The entire loan balance will become due at that time.\n\n","a4a45439":"## Bivariate Analysis","cacea8dd":"## Univariate Analysis","78e7a479":"## Implementation of what worked ","a34c6c1e":"## We are not dealing with imbalalnce vo show kar using interest rate ka count","9dae8a24":"## Data Creation for training ","58275dc9":"## Problem Statement \nHave you ever wondered how lenders use various factors such as credit score, annual income, the loan amount approved, tenure, debt-to-income ratio etc. and select your interest rates? \n\nThe process, defined as \u2018risk-based pricing\u2019, uses a sophisticated algorithm that leverages different determining factors of a loan applicant. Selection of significant factors will help develop a prediction algorithm which can estimate loan interest rates based on clients\u2019 information. On one hand, knowing the factors will help consumers and borrowers to increase their credit worthiness and place themselves in a better position to negotiate for getting a lower interest rate. On the other hand, this will help lending companies to get an immediate fixed interest rate estimation based on clients information. \n\n\n**Here, your goal is to use a training dataset to predict the loan rate category (1 \/ 2 \/ 3) that will be assigned to each loan in our test set.**\n\n## Evaluation Metric\nThe evaluation metric for this competition is **Weighted F1 Score.**\n<br>\n \nFor more information on the data and problem refer following link <br>\nhttps:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-machine-learning-for-banking\/","c6bad2ed":"## Finding best Model\n- Using cross_val_score to find which model is best suited for this task then further proceeding with that model","5a20a37f":"## Inference\n- Applicants income source has greatest feature importance as per our model which definitely holds in real life.No bank would provide loan under a reasonable interest rate to an applicant whose income sources are not verified .\n- Categories under the purpose of loan features also have the next greater importance as per our XGBoost model.Surprisingly the other catgory of purpose_of_loan has pretty good importance which in real life would be due to the ambiguity of that category .\n- Categories under Length_of_Employment has more or less same importance except if someone has less than 1 yr of employment status , which makes sense as they are little less reliable\n- Inquires_since_last_6_months also has pretty good weight as it conveys how much an applicant is in need of that loan and Hence bank can negotiate the Interest rate as per the urgency and needs of the applicant","66dbbc9c":"## INFERENCE \n- LoanAmt requested and annual income  have a good positive correlation of 0.34 which makes sense as people will apply for  loan as much as they can pay.\n- Total accounts and no of accounts are highly correlated which makes sense as no of accounts is one of the factor that contributes to total account. Hence reducing them to one particluar feature makes sense \n- LoanAmtRequested , DebttoIncome and Inquires since last 6 months have small positive corelation with InterestRate(category)","d169c9f5":"## What didn't work \n**Created different preprocessed datasets and applied XGBoost model on them**\n> a] Binned Dataset - wherein data binning preprocessing techniques were used along with PolynomialFeatures prepocessing with degree two. Model didn't show any sign of improvement instead weighted f1 score dropped .So I thought it would be due to humungous extra features which were created due to polynomial features total of 293 features columns were present (in one hot encoded format) which might be confusing model.\n<br><br>\nb] Then I picked top 100 features and iteratively reduced features from 100 to 60 then 50 till 40.Model showed slight improvement as features were reduced . So I decided not to use polynomial features instead going with normal features and create some new features.\n<br><br>\nc] Then I created new dataset where normal prerocessing techniques, as used above for creating training dataset were used and then I applied hyperparameter tuning to search for optimal hyperparameters .\n<br><br>\n\n## What worked \n>  Surprisingly the best pre-processing technique that worked for me was no pre-processing. The one wherein I didn't do any imputation for missing value , since XGBoost finds missing values on it's own using Sparsity-aware Split Finding algorithm . So I used dataset with no imputation .Then I found that some one hot encoded columns of categorical has very less values so I removed all those one hot encoded columns which had less that 5000  values .This pre-processed data along with the tuning parameters for XGBoost found from above point [c]  resulted in the best model which gave an f1-weighted score of 0.5344 and a rank of 84 among 8584 participants\n\n\n**Implementation of what didn't work is in hyperparamter notebook**<br>\nhttps:\/\/www.kaggle.com\/pritamrao\/janta-hack-av-hyperparamter-and-experimentations\n","83b699e3":"**Removing those one hot encoded columns which has very liitle data**\n- I tried different thresholds for removing but 5000 was the one which gave best result","210ea93d":"## Exploratory Data Analysis","232dbaad":"### Inference\n- Most of the applicants live on mortage \n- Very few applicants own their home\n- Other and none should be combined as they are the same thing","2df19250":"## Data Creation","83db57e3":"**Creating four fold cross_val_score whose evaluation metric is f1_weighted score as per the competition**"}}