{"cell_type":{"075dc1f7":"code","488a2511":"code","4e5dcb0f":"code","91cbb0fb":"code","dab8318f":"code","15076df5":"code","557b4ad9":"code","c8ae4806":"code","a6aa53e5":"code","6bf13cb1":"code","29c2f5c8":"code","ebaf1912":"code","386d913c":"code","e0bfce65":"code","a2c9086f":"code","af9450ba":"code","916f4c10":"code","6344c6b2":"code","02fd2515":"code","5633da96":"code","028bc28f":"code","1206e85e":"code","4076b6fa":"code","09f055ef":"code","3aa3fbed":"code","9b11c48a":"code","1dd568e2":"code","d9ac754e":"code","8b830982":"code","f598d431":"code","c28688b5":"code","1682aaed":"code","a7cfee32":"code","8487fb79":"code","dca6e446":"code","506625eb":"code","63630a5c":"code","63d67a06":"code","7a16f4d6":"code","2bf2e774":"code","6facd7fe":"code","10e917f1":"code","313cf995":"code","30823550":"code","3768a233":"code","1b947e07":"code","c93675c1":"code","3ad3de55":"code","b5fe8395":"code","fb436602":"markdown","a6474fc3":"markdown","32e53526":"markdown","86324dc1":"markdown","fa1e2c58":"markdown","89497ab0":"markdown","be5254a6":"markdown","e519f0bb":"markdown","1d1cdfe4":"markdown","d08c7269":"markdown","6ce09823":"markdown","fc6b28fb":"markdown","054f6cdd":"markdown","31dc2a09":"markdown","4dded53a":"markdown","f95d5ee5":"markdown","d0591b86":"markdown","f3a48fa1":"markdown","33da0381":"markdown","74276926":"markdown","5151114f":"markdown","c2183b74":"markdown","89c8a231":"markdown","58ccbaec":"markdown"},"source":{"075dc1f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","488a2511":"# Import train & test Data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngender_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","4e5dcb0f":"dataset=[train,test]","91cbb0fb":"for data in dataset:\n    data['lastname']=data['Name'].apply(lambda x: x.split(',')[0])","dab8318f":"count=0\nlastnamerecorder=[]\nfor i in range(train.shape[0]):\n    if( train['lastname'][i] not in lastnamerecorder):\n        count=count+1+train['SibSp'][i]+train['Parch'][i]\n        lastnamerecorder.append(train['lastname'][i])\nprint(count)","15076df5":"lastname_cabin={}\nfor i in range(train.shape[0]):\n    if( train['lastname'][i] not in lastname_cabin.keys() and train['Cabin'][i]==train['Cabin'][i] ):\n        lastname_cabin[train['lastname'][i]]=[train['Cabin'][i]]\n    elif(train['lastname'][i] in lastname_cabin.keys() and train['Cabin'][i]==train['Cabin'][i] ):\n        if(train['Cabin'][i] not in lastname_cabin[train['lastname'][i]] ):\n            lastname_cabin[train['lastname'][i]].append(train['Cabin'][i])\nlastname_cabin","557b4ad9":"class_cabin={}\nfor i in range(train.shape[0]):\n    if( train['Pclass'][i] not in class_cabin.keys() and train['Cabin'][i]==train['Cabin'][i] ):\n        class_cabin[train['Pclass'][i]]=[train['Cabin'][i]]\n    elif(train['Pclass'][i] in class_cabin.keys() and train['Cabin'][i]==train['Cabin'][i] ):\n        if(train['Cabin'][i] not in class_cabin[train['Pclass'][i]] ):\n            class_cabin[train['Pclass'][i]].append(train['Cabin'][i])\nclass_cabin","c8ae4806":"for data in dataset:\n    data[\"cabin\"]=data['Cabin'].apply(lambda x: x[0] if x==x else x)\ndata[\"cabin\"]","a6aa53e5":"d=['X','A','B','C','D','E','F','G']","6bf13cb1":"for data in dataset:\n    for i in range(data.shape[0]):\n        if(data['Pclass'][i]==1 and data[\"cabin\"][i]!=data[\"cabin\"][i]):\n            data['cabin'][i]=d[np.random.choice(np.arange(1, 6), p=[0.3,0.3,0.2,0.15,0.05])]\n        elif(data['Pclass'][i]==2 and data[\"cabin\"][i]!=data[\"cabin\"][i]):\n            data['cabin'][i]=d[np.random.choice(np.arange(4, 7), p=[0.4,0.3,0.3])]\n        elif(data['Pclass'][i]==3 and data[\"cabin\"][i]!=data[\"cabin\"][i]):\n            data['cabin'][i]=d[np.random.choice(np.arange(5, 8), p=[0.3,0.4,0.3])]","29c2f5c8":"for data in dataset:\n    data['Age']=data['Age'].fillna(data['Age'].mean())","ebaf1912":"test=test.drop([\"Cabin\"], axis=1)\ntrain=train.drop([\"Cabin\"], axis=1)","386d913c":"for data in dataset:\n    data['family']=data['Parch']+data['SibSp']","e0bfce65":"columns=['PassengerId','Name','Ticket','lastname']\ntrain=train.drop(columns,axis=1)\ntest1=test.drop(columns, axis=1)","a2c9086f":"train=pd.get_dummies(train)\ntest1=pd.get_dummies(test1)","af9450ba":"train","916f4c10":"X=train.drop(['Survived'],axis=1)\n\nY=train['Survived']","6344c6b2":"X.info()","02fd2515":"import torch\nimport numpy as np","5633da96":"inputs=X.to_numpy()\ntargets=Y.to_numpy()","028bc28f":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ninputs=scaler.fit_transform(inputs)\n","1206e85e":"inputs","4076b6fa":"inputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","09f055ef":"inputs=inputs.float()\n##targets=targets.float()","3aa3fbed":"inputs.size()","9b11c48a":"targets.size()","1dd568e2":"import torch.nn as nn\nfrom torch.utils.data import TensorDataset","d9ac754e":"train_ds = TensorDataset(inputs, targets)","8b830982":"from torch.utils.data import DataLoader","f598d431":"batch_size = 100\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)","c28688b5":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(18, 32)\n        \n        self.linear2 = nn.Linear(32, 32)\n        self.linear3 = nn.Linear(32, 2)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.linear2(x)\n        x= self.linear3(x)\n        x = self.sigmoid(x)\n        return x","1682aaed":"model=Net()\npreds=model.forward(inputs)","a7cfee32":"preds","8487fb79":"import torch.nn.functional as F\nloss_fn = F.cross_entropy\nloss = loss_fn(model.forward(inputs), targets)\nopt = torch.optim.SGD(model.parameters(), lr=1e-2)","dca6e446":"def fit(num_epochs, model, loss_fn, opt, train_dl):\n    \n    # Repeat for given number of epochs\n    for epoch in range(num_epochs):\n        \n        # Train with batches of data\n        for xb,yb in train_dl:\n            \n            # 1. Generate predictions\n            pred = model.forward(xb.float())\n            \n            # 2. Calculate loss\n            loss = loss_fn(pred, yb)\n            \n            # 3. Compute gradients\n            loss.backward()\n            \n            # 4. Update parameters using gradients\n            opt.step()\n            \n            # 5. Reset the gradients to zero\n            opt.zero_grad()\n        \n        # Print the progress\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}\/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","506625eb":"fit(1000, model, loss_fn, opt,train_dl)","63630a5c":"preds=model.forward(inputs)\npreds","63d67a06":"targets","7a16f4d6":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","2bf2e774":"accuracy(preds,targets)","6facd7fe":"test1['cabin_T']=0","10e917f1":"testf=scaler.fit_transform(test1.to_numpy())","313cf995":"testf = torch.from_numpy(testf)","30823550":"final=model.forward(testf.float())","3768a233":"_, preds = torch.max(final, dim=1)\npreds.size()","1b947e07":"testf","c93675c1":"final=preds.numpy()","3ad3de55":"final","b5fe8395":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': final})\noutput.to_csv('Titanic_submission.csv', index=False)","fb436602":"Finding all the cabins for every lastname in the titanic","a6474fc3":"predicted output","32e53526":"Loss function is the cross entropy can use any other as well. ","86324dc1":"Filling missing values in the cabin column based on the probabilites of a particular class belongs to.","fa1e2c58":"Accuracy function","89497ab0":"## If you like the notebook please upvote. This will help me keep motivated.","be5254a6":"Normalising the data using min max scalar which will get all the values in the range of 0-1","e519f0bb":"creating a new column named \"cabin\" from column \"Cabin\" which has only the cabin alphabet without the numbers","1d1cdfe4":"# Data Cleaning","d08c7269":"Filling missing values in Age by mean of the column:","6ce09823":"DataLoader will help us in obtaining the data in batches.","fc6b28fb":"Dropping the Cabin column from both train and test data","054f6cdd":"This is the neural network structure we will be using:\n- linear layer with  18 inputs  and 32 outputs\n- linear layer with  32 inputs  and 32 outputs\n- linear layer with  32 inputs  and 2 outputs\n- Sigmoid\nSigmoid function will help us in getting the results as binary results.","31dc2a09":"Dropping the below columns are they are no use for our analysis","4dded53a":"Getting the lastname from the Name column","f95d5ee5":"## Combining the dataset together","d0591b86":"Actual output ","f3a48fa1":"# Loading the dataset:","33da0381":"Creating a new column family by adding the Parch and SibsP","74276926":"Finding the total no of members each family that are present in the ship ","5151114f":"Training the model","c2183b74":"one hot encoding the train and test set as there are categorical varibles in the data.","89c8a231":"tensordataset will match the inputs to its results. ","58ccbaec":"Finding all the cabins that are present in every class"}}