{"cell_type":{"29e24dc9":"code","05c9dfe5":"code","ce42dc15":"code","91904729":"code","c67e4677":"code","b7a0b408":"code","08b89ca4":"code","92632707":"code","62abb450":"code","1faf08ad":"code","94e5dcee":"code","4f5b6fe6":"code","dae78af2":"code","150946e5":"code","c70269a1":"code","70e95a6b":"code","df1f6ade":"code","976ed59d":"code","68f2638a":"code","013e72fc":"code","8e96c5ed":"code","f7243e4e":"code","bc201b79":"code","9c6f3f11":"code","894a1bb8":"code","91f7261f":"code","51a35edb":"code","bbf56ac7":"code","05f3ed53":"code","432206a6":"code","30da44b3":"code","d0722301":"code","55bb595b":"code","cf89b3ea":"code","a00686e9":"code","0f9861f3":"code","c1d1c5f0":"code","c9bc7736":"code","06f10e12":"markdown","69c681c2":"markdown","8e472414":"markdown","09a27e6e":"markdown","b52368f5":"markdown","0aecb935":"markdown","175f9424":"markdown","56247849":"markdown","0202e069":"markdown","b00c9528":"markdown","fc1b4fd6":"markdown","cfec93fc":"markdown","5bc2ffa6":"markdown","53076199":"markdown","407c49dc":"markdown","bcb89e85":"markdown","3aef63e3":"markdown","c7bd2e33":"markdown","b33c7c37":"markdown","df43ee42":"markdown","315fc2f3":"markdown","6456c6f4":"markdown","e52be9a0":"markdown","4fae888e":"markdown","9c3bebcb":"markdown","1dcefef3":"markdown","11b7839a":"markdown","f4991e21":"markdown"},"source":{"29e24dc9":"import re\nimport string\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.pipeline import Pipeline","05c9dfe5":"true_news_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake_news_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')","ce42dc15":"true_news_df.head()","91904729":"fake_news_df.head()","c67e4677":"stopwords = set(STOPWORDS)","b7a0b408":"# True news word cloud\ntrue_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(true_news_df['text']))\n\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 18))\nax.imshow(true_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('True News')","08b89ca4":"# Fake news word cloud\nfake_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(fake_news_df['text']))\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\nax.imshow(fake_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('Fake News')","92632707":"def convert_from_list_to_text(_list):\n    text = ' '.join(_list)\n    return text\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Purples):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","62abb450":"def unique(text):\n    text = text.split()\n    return list(set(text))","1faf08ad":"true_news_df['text']  = true_news_df['text'].apply(lambda x: unique(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: unique(x))\n\ntrue_news_df.head(1)","94e5dcee":"# Remove punctuation from a word if it exist\ndef rm_punc_from_word(word):\n    clean_word = ''                # word without punctuation\n    for alpha in word:\n        # checking if alphabet is punctuation or not\n        if alpha in string.punctuation:\n            continue\n        clean_word += alpha\n        \n    return clean_word\n\n\n# Remove any punctuation and clean words having punctuation\ndef clean_punc(words_list):\n    for idx, word in enumerate(words_list):\n        if word in string.punctuation:\n            words_list.remove(word)\n        else:\n            words_list[idx] = rm_punc_from_word(word)\n            words_list[idx] = re.sub('[0-9]+', '', words_list[idx])\n            \n    return words_list","4f5b6fe6":"true_news_df['text']  = true_news_df['text'].apply(lambda x: clean_punc(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: clean_punc(x))\n\ntrue_news_df.head(1)","dae78af2":"def rm_num(words_list):\n    text = ' '.join(words_list)\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text.split()","150946e5":"true_news_df['text']  = true_news_df['text'].apply(lambda x: rm_num(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: rm_num(x))\n\ntrue_news_df.head(1)","c70269a1":"def tokenization(words_list):\n    tmp = words_list.copy()\n    words_list = []\n    \n    for idx, word in enumerate(tmp):\n        for split_word in re.split('\\W+', word):\n            words_list.append(split_word)\n\n    words_list = ' '.join(words_list).split()  # removing any white spaces\n    return words_list","70e95a6b":"true_news_df['text']  = true_news_df['text'].apply(lambda x: tokenization(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: tokenization(x))\n\ntrue_news_df.head(1)","df1f6ade":"def remove_URL(words_list):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    for idx, word in enumerate(words_list):\n        words_list[idx] = url.sub(r'',word)\n    return words_list","976ed59d":"true_news_df['text']  = true_news_df['text'].apply(lambda x: remove_URL(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: remove_URL(x))\n\ntrue_news_df.head(1)","68f2638a":"def remove_HTML(words_list):\n    html = re.compile(r'<.*?>')\n    for idx, word in enumerate(words_list):\n        words_list[idx] = html.sub(r'',word)\n    return words_list","013e72fc":"true_news_df['text']  = true_news_df['text'].apply(lambda x: remove_HTML(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: remove_HTML(x))\n\ntrue_news_df.head(1)","8e96c5ed":"def remove_emoji(words_list):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n\n    for idx, word in enumerate(words_list):\n        words_list[idx] = emoji_pattern.sub(r'',word)\n    return words_list","f7243e4e":"true_news_df['text']  = true_news_df['text'].apply(lambda x: remove_emoji(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: remove_emoji(x))\n\ntrue_news_df.head(1)","bc201b79":"stopwords = set(nltk.corpus.stopwords.words())\n\ndef clean_stopwords(words_list):\n    for word in words_list:\n        if word in stopwords:\n            words_list.remove(word)\n    return words_list","9c6f3f11":"true_news_df['text']  = true_news_df['text'].apply(lambda x: clean_stopwords(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: clean_stopwords(x))\n\ntrue_news_df.head(1)","894a1bb8":"true_news_df['is_fake'] = 0\nfake_news_df['is_fake'] = 1\n\ntrue_news_df.head()","91f7261f":"true_news_df['text']  = true_news_df['text'].apply(lambda x: convert_from_list_to_text(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: convert_from_list_to_text(x))\n\ntrue_news_df.head()","51a35edb":"# True news word cloud\ntrue_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(true_news_df['text']))\n\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 18))\nax.imshow(true_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('True News')","bbf56ac7":"# Fake news word cloud\nfake_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(fake_news_df['text']))\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\nax.imshow(fake_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('Fake News')","05f3ed53":"# \u2728 Concatenating true and fake news dataframes and suffling them\n\ndf = pd.concat([true_news_df, fake_news_df], axis='index')\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","432206a6":"# \ud83d\udd2a Splitting the dataset into train & test\n\nskf = StratifiedKFold(n_splits=10)\n\nX = df.text\nY = df.is_fake\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=212)","30da44b3":"count_vec = CountVectorizer()\nX_train_counts = count_vec.fit_transform(X_train)\n\nX_train_counts.shape","d0722301":"tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nX_train_tfidf.shape","55bb595b":"score = cross_val_score(MultinomialNB(), X_train_tfidf, y_train, cv=skf)\nprint(f'MultinomialNB mean of cross validation score: {score.mean()}')\n\nscore = cross_val_score(LogisticRegression(), X_train_tfidf, y_train, cv=skf)\nprint(f'LogisticRegression mean of cross validation score: {score.mean()}')\n\nscore = cross_val_score(LinearSVC(), X_train_tfidf, y_train, cv=skf)\nprint(f'LinearSVC mean of cross validation score: {score.mean()}')","cf89b3ea":"model = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', LinearSVC())\n])","a00686e9":"model.fit(X_train, y_train)","0f9861f3":"y_test_pred = model.predict(X_test)\nnp.mean(y_test_pred == y_test)","c1d1c5f0":"print(f'Model Score: {model.score(X_test, y_test)}')\nprint(f'f1-score: {f1_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'precision score: {precision_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'recall score: {recall_score(y_test, y_test_pred, average=\"weighted\")}')","c9bc7736":"cnf_matrix = confusion_matrix(y_test, y_test_pred, labels=[0,1])\nnp.set_printoptions(precision=2)\n\nprint(classification_report(y_test, y_test_pred))\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(cnf_matrix, classes=['True(0)', 'Fake(1)'],normalize= False,  title='Confusion matrix', cmap=plt.cm.RdPu)","06f10e12":"### **Tokenization (\u2310\u25a0_\u25a0)**","69c681c2":"### **Remove emoji \ud83e\udd2c \ud83d\ude02**","8e472414":"### **Remove numbers \ud83e\udd71**","09a27e6e":"### **Removing \ud83d\ude34 HTML tags**","b52368f5":"### **\ud83d\ude44 Converting list of words to text**","0aecb935":"### **\ud83d\udc31\u200d\ud83d\udc64Task: Classifying the news**\n\n---\n\n## **\ud83d\ude9b Importing packages**","175f9424":"### **\u2705 True news dataframe**","56247849":"#### **TF:**\n\nJust counting the number of words in each text has 1 \ud83d\ude31 issue: it will give more weightage to longer texts than shorter texts. To avoid this, we can use frequency **(TF - Term Frequencies) i.e. #count(word) \/ #Total words**, in each  text.\n\n#### **TF-IDF:**\n\nFinally, we have reduce the weightage of more common words like (the, is, an etc.) which occurs in all text. This is called as **TF-IDF i.e Term Frequency times inverse document frequency**.\n\n**To achieve this \ud83d\ude0e TfidfTransformerfrom Scikit-learn is used**.","0202e069":"### **\ud83c\udfaf Cross Validation**","b00c9528":"### **Getting rid of \ud83d\udcb2\u2753\u2757\ud83d\ude2bpunctuations**","fc1b4fd6":"### **\ud83d\udd2e Predictions**","cfec93fc":"### **\ud83d\udc40 Unique words from text**","5bc2ffa6":"## **\ud83d\ude80 Creating model**","53076199":"## **\ud83d\udc53Visualizing uncleanded data using WordCloud**","407c49dc":"---","bcb89e85":"## **\ud83c\udfb0 Pipeline**","3aef63e3":"## **\ud83d\udebf\ud83d\udec1\ud83e\uddfd Data preprocessing**","c7bd2e33":"### **Removing \ud83d\uded1 stop words**","b33c7c37":"### **\ud83e\udd16 Helper Functions**","df43ee42":"### **\ud83d\ude0e Fitting the model**","315fc2f3":"### **\u274c Fake news dataframe**","6456c6f4":"# Classify news as real or fake","e52be9a0":"### **\ud83d\udcddLabelling the datasets**","4fae888e":"#### **\ud83e\uddf2 Extracting features from text**\n\n\ud83e\udd14 Texts are actually series of words. In order to run machine learning algorithms we need to convert the text of text into numerical feature vectors.\n\nWe have used the bag of words model for our example.\n\n\ud83e\udd28 Briefly, we segment each text into words (for English splitting by space), **and count number of times each word occurs in each document and finally assign each word an integer id**. Each unique word in our dictionary will correspond to a feature (descriptive feature).\n\n\ud83d\ude0f Scikit-learn has a high level component CountVectorizer which will create feature vectors.","9c3bebcb":"## **\ud83e\udde8\ud83d\ude0d Loading the datasets**","1dcefef3":"### **\ud83c\udfaf Metrics**","11b7839a":"### **\ud83d\ude2a Removing URL**","f4991e21":"## **\ud83d\udc53Visualizing cleanded data using WordCloud**"}}