{"cell_type":{"876894d2":"code","83ad0552":"code","ae6b3c38":"code","54410ffa":"code","c747b3fb":"code","27752f64":"code","0e63d9f9":"code","6eb203f1":"code","c2610c04":"code","f662e08d":"code","e606f6c9":"code","7317b94a":"code","30c0c1a7":"code","c34d2729":"code","1450622a":"code","05462932":"code","a2d33928":"code","e9f6ebc1":"code","1d5f82c6":"code","de7a5962":"code","a00a674f":"code","476cc497":"code","7a4a3609":"code","e67509ec":"code","ecd3eb59":"code","d9206f31":"code","6de9dfaf":"code","615d7bd6":"code","6d5519d3":"code","6d8d3980":"code","7fc48602":"code","1b3f5fff":"code","abd06d7c":"code","bf4683df":"code","e7fbeb77":"code","4907eba4":"code","1f21f820":"code","dc44b27a":"code","7eff4ccc":"code","70582268":"code","58138b10":"code","11664823":"code","edec8555":"code","383d1684":"code","3627ea48":"code","636dcb2c":"code","a47fd974":"code","639c71c3":"code","ab7918d0":"code","dda207ed":"code","ecaf1d23":"code","56715ca7":"code","a56ad76e":"code","5791e6ae":"code","a837747a":"code","294a28a5":"code","3511ca92":"code","b6cd6630":"code","4d82c961":"code","ebeb73df":"code","322debbe":"code","64c0fe01":"code","56eb4809":"code","38a1f53f":"markdown","bdf6327f":"markdown","153122fa":"markdown","19c2d80d":"markdown","a582bdac":"markdown","33a087d4":"markdown","06049e1c":"markdown","60f965e3":"markdown","3a9a22b0":"markdown","b1d15483":"markdown","0cd04394":"markdown","ed2754ad":"markdown","475e02d1":"markdown","bb58367f":"markdown","88dfb7e7":"markdown","b66a38b9":"markdown","4d7bd81a":"markdown","42234217":"markdown","e8c81fb8":"markdown","26c5050b":"markdown","8f6aab00":"markdown","eef1d4ac":"markdown","79591520":"markdown","9d1406d5":"markdown","61f958aa":"markdown","af059cad":"markdown","7dad849f":"markdown","e050af27":"markdown","cd80553f":"markdown","71e8ae36":"markdown","5bcb41a2":"markdown","827f8859":"markdown","04a8c11f":"markdown","2e7928c9":"markdown","68a870eb":"markdown","417f9afb":"markdown","42479f92":"markdown","894ddd41":"markdown","35655897":"markdown","32ae5c94":"markdown","0aed5efa":"markdown","680c9eb3":"markdown","f33c7e05":"markdown","4546e5a3":"markdown","8e8eff71":"markdown","05f287a7":"markdown","6c67fc9a":"markdown","620c5a7b":"markdown","803d48e7":"markdown","aabe6242":"markdown","7440a0e3":"markdown","220db2af":"markdown","ede58da4":"markdown","ba348df9":"markdown","7f3fbbac":"markdown","66da3ded":"markdown"},"source":{"876894d2":"# Manipulation of vectors library\nimport numpy as np\n# Manipulation of data library\nimport pandas as pd\n#Ploting graphs library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Manipulation of the system library\nimport os","83ad0552":"! cd \/kaggle\/input\/titanic && ls","ae6b3c38":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","54410ffa":"df_train.head()","c747b3fb":"df_train.describe().transpose()","27752f64":"df_train.groupby('Pclass').mean()['Survived']*100","0e63d9f9":"df_train.isna().sum()","6eb203f1":"df_train.corr()","c2610c04":"plt.figure(figsize=(16,9))\nsns.heatmap(df_train.corr(), annot=True, cmap='viridis')\nplt.show()","f662e08d":"print(df_train.groupby('Pclass').mean()['Fare'])\nplt.figure(figsize=(10,8))\nsns.boxplot(x='Pclass', y='Fare', data=df_train)\nplt.title('Pclass x Fare')\nplt.show()","e606f6c9":"plt.figure(figsize=(16,9))\nsns.barplot(x='SibSp', y='Parch', data=df_train)\nplt.title('SubSp x Parch')\nplt.show()","7317b94a":"print(df_train.groupby('Pclass').mean()['Age'])\nplt.figure(figsize=(16,9))\nsns.barplot(x='Pclass', y='Age', data=df_train)\nplt.title('Pclass x Age')\nplt.show()","30c0c1a7":"print(df_train.groupby('Pclass').mean()['Survived'])\nplt.figure(figsize=(16,9))\nsns.barplot(x='Pclass', y='Survived', data=df_train)\nplt.title('Pclass x Survived')\nplt.show()","c34d2729":"print(df_train.groupby(['Pclass', 'Survived']).mean()['Fare'])\nsns.FacetGrid(df_train, col='Pclass', height=5, aspect=4\/3).map(sns.barplot, 'Survived', 'Fare', order=[0, 1])\nplt.show()","1450622a":"plt.figure(figsize=(16*(2\/3),9*(2\/3)))\nsns.heatmap(df_train[['Fare', 'Survived', 'Pclass']].corr(), cmap='viridis', annot=True)\nplt.title('Correlation: Fare x Survived x Pclass')\nplt.show()","05462932":"print(df_train.groupby('Sex').mean()[['Survived']]*100)\nplt.figure(figsize=(16,9))\nsns.countplot(x='Sex', hue='Survived', data=df_train, palette='viridis')\nplt.title('Count of Survive by Sex')\nplt.show()","a2d33928":"print(df_train.isna().sum(),'\\n\\n',df_test.isna().sum())","e9f6ebc1":"df_train[df_train['Embarked'].isna() == True]","1d5f82c6":"plt.figure(figsize=(16,9))\nsns.FacetGrid(df_train, col='Embarked', row='Sex', height=4, aspect=.8).map(sns.barplot, 'Pclass', 'Fare', order=[1, 2, 3])\nplt.show()","de7a5962":"df_train.groupby('Embarked').agg(['mean', 'std']).drop('PassengerId', axis=1)","a00a674f":"#For train data\ndf_train['Embarked'] = df_train['Embarked'].fillna(value='S')\n\n#For test data\ndf_test['Embarked'] = df_test['Embarked'].fillna(value='S')","476cc497":"df_train = pd.get_dummies(df_train, columns=['Embarked'], drop_first=True)\ndf_test = pd.get_dummies(df_test, columns=['Embarked'], drop_first=True)","7a4a3609":"df_train.Cabin.isna().sum()","e67509ec":"df_train.Cabin.value_counts()","ecd3eb59":"df_train.Cabin.isna().sum() \/ len(df_train.Cabin) * 100","d9206f31":"#For train and test data\ndf_train = df_train.drop('Cabin', axis=1)\ndf_test = df_test.drop('Cabin', axis=1)","6de9dfaf":"#for train data\ndf_train = pd.get_dummies(df_train, columns=['Sex'], drop_first=True)\n\n#for test data\ndf_test = pd.get_dummies(df_test, columns=['Sex'], drop_first=True)","615d7bd6":"plt.figure(figsize=(16,9))\nsns.boxplot(x='Pclass', y='Age', data=df_train)\nplt.title('Pclass x Age')\nplt.show()","6d5519d3":"df_train = df_train.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\ndf_test = df_test.drop(['PassengerId', 'Name', 'Ticket'], axis=1)","6d8d3980":"df_train.hist()","7fc48602":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","1b3f5fff":"norm = MinMaxScaler()","abd06d7c":"df_train_notna = df_train[df_train['Age'].notna() == True].drop(['Survived'], axis=1)\n\ndf_X = df_train_notna.drop(['Age'], axis=1)\ndf_y = df_train_notna['Age']","bf4683df":"X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=72)","e7fbeb77":"norm.fit(X_train)\nX_train = norm.transform(X_train)\nX_test = norm.transform(X_test)","4907eba4":"import tensorflow as tf","1f21f820":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, activation='relu', input_shape=[7]),\n    #This sequential begin with 10 neurons, the activation is ReLu and the input shape is the number of columns of our training dataset.\n    \n    tf.keras.layers.BatchNormalization(),\n    #Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n    \n    tf.keras.layers.Dense(15, activation='relu'),\n    #Add 15 neurons using ReLu as activation function\n    \n    tf.keras.layers.Dense(1)\n    #The output must be 1 label for a regression model\n])","dc44b27a":"model.summary()","7eff4ccc":"model.compile(optimizer='SGD',loss='mean_squared_logarithmic_error',metrics=['accuracy', 'MSE'])\nhistory = model.fit(X_train, y_train, epochs=1000, validation_data=(X_train, y_train), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))","70582268":"plt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['loss'], 'r', label='loss')\nplt.plot(history.history['val_loss'], 'b', label='val_loss')\nplt.title('Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['accuracy'], 'r', label='accuracy')\nplt.plot(history.history['val_accuracy'], 'b', label='val_accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['MSE'], 'r', label='MSE')\nplt.plot(history.history['val_MSE'], 'b', label='val_MSE')\nplt.title('MSE')\nplt.legend()\nplt.show()","58138b10":"classifications = model.predict(X_test)\nprint(f'Correlation Matrix:\\n{np.corrcoef(classifications[:,0], y_test)}')\nplt.figure(figsize=(16,9))\nsns.scatterplot(x = classifications[:,0], y = y_test)\nsns.lineplot(x = y_test, y = y_test, color='b')\nplt.xlim(.94*min(classifications[:,0]), 1.02*max(classifications[:,0]))\nplt.xlabel('Predicted Age')\nplt.ylabel('Real Age')\nplt.show()","11664823":"df_train['Age'] = df_train['Age'].replace({np.nan: np.array(model.predict(norm.fit_transform(df_train.drop(['Age', 'Survived'], axis=1))))})\ndf_test['Age'] = df_test['Age'].replace({np.nan: np.array(model.predict(norm.fit_transform(df_test.drop(['Age'], axis=1))))})","edec8555":"print(df_train.isna().sum(),'\\n\\n',df_test.isna().sum())","383d1684":"df_test['Fare'] = df_test['Fare'].fillna((df_test.Fare.mean()))\ndf_train['Fare'] = df_train['Fare'].fillna((df_train.Fare.mean()))","3627ea48":"X_train = df_train.drop('Survived',axis=1)\ny_train = df_train['Survived']\nX_test = df_test","636dcb2c":"norm = MinMaxScaler()\nnorm.fit(X_train)\nX_train = norm.transform(X_train)\nX_test = norm.transform(X_test)","a47fd974":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, activation='relu', input_shape=[8]),\n    #This sequential begin with 10 neurons, the activation is ReLu and the input shape is the number of columns of our training dataset.\n    \n    tf.keras.layers.BatchNormalization(),\n    #Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n    \n    tf.keras.layers.Dense(14, activation='relu'),\n    #Add 14 neurons using ReLu as activation function\n    \n    tf.keras.layers.Dense(1, activation='sigmoid')\n    #The output must be 1 label and de activation is a sigmoid function for binary classification\n])","639c71c3":"model.summary()","ab7918d0":"model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),metrics=['accuracy', 'MSE'])\nhistory = model.fit(X_train, y_train, epochs=500, validation_split=.2, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))","dda207ed":"plt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['loss'], 'r', label='loss')\nplt.plot(history.history['val_loss'], 'b', label='val_loss')\nplt.title('Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['accuracy'], 'r', label='accuracy')\nplt.plot(history.history['val_accuracy'], 'b', label='val_accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['MSE'], 'r', label='MSE')\nplt.plot(history.history['val_MSE'], 'b', label='val_MSE')\nplt.title('MSE')\nplt.legend()\nplt.show()","ecaf1d23":"model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),metrics=['accuracy', 'MSE'])\nhistory = model.fit(X_train, y_train, epochs=500, validation_split=0, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))","56715ca7":"from sklearn.metrics import confusion_matrix, accuracy_score","a56ad76e":"classifications = model.predict(X_train)\nfor i, ival in enumerate(classifications):\n    if ival >= 0.5: classifications[i] = 1\n    else: classifications[i] = 0\nprint(f'Correlation Matrix:\\n{np.corrcoef(classifications[:,0], y_train)}\\n\\n')\nprint(f'Confusion Matrix:\\n{confusion_matrix(y_train, classifications)}\\n\\n')\nprint(f'Accuracy Matrix:\\n{accuracy_score(y_train, classifications)}')","5791e6ae":"df_test_X = norm.fit_transform(df_test)\ndf_test_y = model.predict(df_test_X)\nfor i, ival in enumerate(df_test_y):\n    if ival >= 0.5: df_test_y[i] = 1\n    else: df_test_y[i] = 0","a837747a":"submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\", index_col='PassengerId')\nsubmission['Survived'] = df_test_y.astype(int)\nsubmission.to_csv('TitanicKNN.csv', index=False)","294a28a5":"X_test.shape, y_test.shape","3511ca92":"df_train","b6cd6630":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score","4d82c961":"MLA = [\n    #SVM\n    ('SVC', SVC()),\n    ('LinearSVC', LinearSVC()),\n    \n    #Tree\n    ('DecisionTreeClassifier', DecisionTreeClassifier()),\n    ('ExtraTreeClassifier', ExtraTreeClassifier()),\n    \n    #Linear Model\n    ('LogisticRegression', LogisticRegressionCV()),\n    \n    #Neighbors\n    ('KNeighborsClassifier', KNeighborsClassifier()),\n    \n    #Gaussian Process\n    ('GaussianProcessClassifier', GaussianProcessClassifier()),\n    \n    #Ensemble\n    ('AdaBoostClassifier', AdaBoostClassifier()),\n    \n    #Naive Bayes\n    ('GaussianNB', GaussianNB())\n]","ebeb73df":"X = np.array(df_train.drop('Survived', axis=1))\ny = np.array(df_train['Survived'])","322debbe":"shape = (10,10)\nresults = np.zeros((len(MLA), shape[0], shape[1]))\nfor count, algorithm in enumerate(MLA):\n    cv = RepeatedKFold(n_splits=10, n_repeats=10)\n    model = Pipeline([('scaler', MinMaxScaler()), algorithm])\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=1)\n    results[count] = scores.reshape(shape)","64c0fe01":"for index, result in enumerate(results):\n    plt.title(MLA[index][0])\n    plt.boxplot(result, showmeans=True)\n    plt.axhline(np.mean(result), c='r')\n    plt.axhline(np.mean(result) + np.std(result), c='r', ls='--')\n    plt.axhline(np.mean(result) - np.std(result), c='r', ls='--')\n    plt.show()","56eb4809":"best_mean = [np.mean(x) for x in results]\nindex_best_mean = best_mean.index(max(best_mean))\nprint(f'mean of best result ({MLA[index_best_mean][0]})')\nfor i in results[index_best_mean]:\n    print(f'{np.round(np.mean(i), decimals=3)} +\/- {np.round(np.std(i), decimals=3)}')\n","38a1f53f":"## Prediction & Evaluation","bdf6327f":"DNN is not the best way to handle with this type of problem. Also, there is not much data to build up a good model. However, let's build up the same model above using all training data, i.e, validation_split=0.","153122fa":"Let's build model using keras.","19c2d80d":"## Modelling Deep Neural Network for Predict Survival","a582bdac":"* SibSp is proportional to Parch, this is expected;\n","33a087d4":"Heatmap of correlation","06049e1c":"Realize that there's some missing values on the data, we have to handle with them. Also, 38.38% of passengers survived, so let's compare the survival rate with the others columns.","60f965e3":"Looking at best result of Machine Learning Algorithms","3a9a22b0":"Then, we drop this for train and test data.","b1d15483":"Also, most of survived is woman (74.20%).","0cd04394":"But before drop columns of string type.","ed2754ad":"Import TensorFlow, for training a deep neural networks to predict the age.","475e02d1":"This is a little bad, but is the best that we have. There's three ways to continue, use this regreesion for age on the missing data, use mean age to fill them or delete them. At the end of the notebook we'll try these three ways to handle with this, but now with notebook using regression.","bb58367f":"As you can see below, most of people who embark in the \"Q\" belong to class 3 and have paid 13.28 $\\pm$ 14.18, may the missing data not belong to \"Q\". But is not possible to go so far, then the missing data values will fit in Embarked \"S\", because this is more commom than Embarked \"C\".","88dfb7e7":"  Let's response these questions.","b66a38b9":"We use the train DataFrame with not missing data to do regression, then we apply it on de missing values of train and test DataFrame.","4d7bd81a":"Let's just use mean values on the Fare missing data.","42234217":"As you can see 77.10% of da Cabin column is missing data and apparently there's no relationship between this column and the Survived, so let's just drop it.","e8c81fb8":"They are very correlated, almost linear, that means that the feature importance is directly related to the outcome. Certainly Pclass and Fare have lot importance for predict who survived. Therefore, our model need to learn this.","26c5050b":"Split the data into train data and test data.","8f6aab00":"Begin treating the column Embarked.","eef1d4ac":"With this correlation map maybe:\n\n\n* Fare is inverse of Pclass, then passagers of low class paid less for the trip;\n* SibSp is proportional to Parch, this is expected;\n* Age is inversely proportional to class, maybe the upper class is more elderly than the lower, we'll se it below;\n* Pclass is inverse of Survived, this is alredy expected once the class 1 have 62.96% rate of survived;\n* Fare is proportional to Survived, maybe who paid more for the trip belong to class 1, thus have more chances to survive;","79591520":"Normalizing data.","9d1406d5":"Some data of train DataFrame.","61f958aa":"The correlation is","af059cad":"Notice that, the survival rate grow with de class of the passager. ","7dad849f":"* SGD, stochastic gradient descent, optimizer is a method used to minimize the cost fuction;\n* Mean squared logarithmic error, MSLE is a function that measure the ration between the true and predict values;\n* The metrics accuracy and MSE(Mean Squared Error) measure the behavior of test and validation data along the fitting;\n* EarlyStopping is a callback used to avoid overfitting of the data;","e050af27":"Apply normalization into X_train and X_test.","cd80553f":"Then, apply get_dummies to separete the Embarked colum.","71e8ae36":"A summary of our model.","5bcb41a2":"We begin sorting out train and test data.","827f8859":"At leats, let's handle with Age missing data.\n\nHighest class is more elders.","04a8c11f":"But, this is hard to vizualize, then, let's see this using an heatmap plot from seaborn.","2e7928c9":"## Titanic - Analysis and Predict \nThis is my first Notebook in Kaggle, then if you find any error tell me, please.","68a870eb":"Moreover, the data have some missing data values e we'll handle with them.","417f9afb":"* Fare is proportional to Survived, maybe who paid more for the trip belong to class 1, thus have more chances to suvive;\n\nAgain, yes. But, realize that the columns, Fare and Pclass have strong relationship with Survived. Thus, Who belongs to the upper class or paid more for the trip probably survived.","42479f92":"## Ploting Data","894ddd41":"## Describing Data\n\nLet's begin with some statistics.","35655897":"## Treating Missing Values\n\nWe have to handle with these missing values, let's see this again.","32ae5c94":"Using a barplot to vizualize some columns. We can't label the missing data on 'S', 'C' or 'Q'. Let's try other way to do this.","0aed5efa":"Use Normalizer to normalize data, this bring all variable to same range [0,1].","680c9eb3":"* Binary crossentropy is a loss function\/\n* SGD, stochastic gradient descent, optimizer is a method used to minimize the cost fuction;\n* The metrics accuracy and MSE(Mean Squared Error) measure the behavior of test and validation data along the fitting;\n* EarlyStopping is a callback used to avoid overfitting of the data monitoring the value of loss;","f33c7e05":"Then, we apply our model into our train and test data.","4546e5a3":"Let's try to fit the NaN into a Embarked.","8e8eff71":"* Fare is inverse of Pclass, then passagers of low class paid less for the trip;\n\n\nYES, as you can see, classes 2 and 3 paid less for the trip. Moreover, for the class 1 there's some peoples that paid over than 200.","05f287a7":"There's some repeated values.","6c67fc9a":"Now, let's handle with Cabin column. Couting the NaN data.","620c5a7b":"Let's see the files on kaggle\/input\/titanic folder.","803d48e7":"Now, we treating the colum Sex using get_dummies.","aabe6242":"* Age is inversely proportional to class, maybe the upper class is more elderly than the lower, we'll se it below;\n\n\nAffirmative. As you can see, the age is inversely proportinal to class.","7440a0e3":"Pclass is inverse of Survived, this is alredy expected once the class 1 have 62.96% rate of survived;\n\nAs alredy expected.","220db2af":"## Baselines","ede58da4":"To solve missing data problem, let's build up a regression model using Keras from Tensorflow.","ba348df9":"Now, define the train and test variables from system folder.","7f3fbbac":"The plot below represent a boxplot of each ML Algorithm with 10 repeats using KFold. The horizontal lines represents the mean of scores and the (mean +\/- std) of scores.","66da3ded":"Machine Learning Algorithm"}}