{"cell_type":{"8e2dbcd8":"code","4d72ec2d":"code","bc8d6b40":"code","c480d164":"code","a54deee5":"code","7a1bc8dc":"code","dd22deed":"code","225b9ee8":"code","18d64c31":"code","85aea91c":"code","6b3bed94":"code","eb0055ec":"code","43161f01":"code","91bc0119":"code","c886f921":"code","e645fc54":"code","62b30b80":"code","24ab23db":"code","d9cb35fb":"code","0518a3dc":"code","c1f1f533":"code","6741ac19":"code","3ade46df":"code","316c7956":"code","1d75a662":"code","f266fd0f":"code","d291374d":"code","5d2eb630":"markdown","2712079d":"markdown","fba06f7b":"markdown","39271116":"markdown","44a9943f":"markdown","405d21a1":"markdown","0380f185":"markdown","3ca71a0e":"markdown","2e5228d4":"markdown","858804f4":"markdown","ee3c819f":"markdown","9534e43a":"markdown","f24a46fd":"markdown","d17c176a":"markdown","6c951148":"markdown"},"source":{"8e2dbcd8":"### Importing necessary libraries\n\nimport sys\nsys.path.append('\/kaggle\/input\/iterativestratification') # Multilabel Stratified K-Fold package\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np \nimport pandas as pd \n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import log_loss\n\nimport time\n\nimport category_encoders as ce\n\n%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt","4d72ec2d":"### Settings\n\nSEED = 2020 \nFOLDS = 4\nEPOCHS = 25\nBATCH_SIZE = 128\n\n\n# Which models to use\ninclude_xgboost = False\ninclude_neural_net = True\ninclude_logreg = False\n\n\n# Whether to implement PCA\ninclude_pca = True\n\n\nlr_start=0.0001\nlr_max=0.0003\nlr_min=0.00001\nlr_rampup_epochs=5\nlr_sustain_epochs=2\nlr_exp_decay=.7","bc8d6b40":"### Creating dataframes\n\nTEST_FEATURES_PATH = \"\/kaggle\/input\/lish-moa\/test_features.csv\"\nTRAIN_FEATURES_PATH = \"\/kaggle\/input\/lish-moa\/train_features.csv\"\nTRAIN_TARGETS_PATH = \"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\"\nTRAIN_TARGETS_NONSCORED_PATH = \"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\"\nSAMPLE_SUB_PATH = \"\/kaggle\/input\/lish-moa\/sample_submission.csv\"\n\ntest_features_df = pd.read_csv(TEST_FEATURES_PATH).sort_values(by='sig_id')\ntrain_features_df = pd.read_csv(TRAIN_FEATURES_PATH).sort_values(by='sig_id')\ntrain_targets_df = pd.read_csv(TRAIN_TARGETS_PATH).sort_values(by='sig_id')\ntrain_targets_nonscored_df = pd.read_csv(TRAIN_TARGETS_NONSCORED_PATH)\nsample_sub_df = pd.read_csv(SAMPLE_SUB_PATH).sort_values(by='sig_id')","c480d164":"### Features\n#print(train_features_df.head()) \n#print(train_features_df.describe())\n\n\n### Labels\n#print(train_targets_df.head())\n#print(train_targets_df.describe())\n\n\n### Submission\n#print(sample_sub_df.head())","a54deee5":"### Check how many positive labels are in each class\n\nvalue_counts_arr = np.sort([train_targets_df[col].value_counts()[1] for col in train_targets_df.columns])\n\nprint(value_counts_arr)","7a1bc8dc":"### Plot histogram of 1s counts in classes \n\nmatplotlib.rcParams['figure.figsize'] = [10, 5]\n\nplt.hist(value_counts_arr, 50, facecolor='g', alpha=0.75)\nplt.xlabel('Number of 1\\'s')\nplt.ylabel('Number of classes')\nplt.title('Value Counts of 1\\'s in classes')\nplt.show()","dd22deed":"### Rename dataframes and drop 'id' columns\n\nX = train_features_df.drop(columns=['sig_id'])\nX_test = test_features_df.drop(columns=['sig_id'])\ny = train_targets_df.drop(columns=['sig_id'])","225b9ee8":"# Encode categorical features\nX_type = X['cp_type'].apply(lambda x: 1 if x == 'trt_cp' else 0)\nX_dose = X['cp_dose'].apply(lambda x: 1 if x == 'D2' else 0)\n\n# Encode categorical test features\nX_type_test = X_test['cp_type'].apply(lambda x: 1 if x == 'trt_cp' else 0)\nX_dose_test = X_test['cp_dose'].apply(lambda x: 1 if x == 'D2' else 0)","18d64c31":"# Put encoded features back in\n\nX = pd.concat([X_type,X_dose,X.drop(columns=['cp_type','cp_dose'])], axis=1)\nX_test = pd.concat([X_type_test,X_dose_test,X_test.drop(columns=['cp_type','cp_dose'])], axis=1)","85aea91c":"### Verify\n\n#print(X.head())\n#print(X_test.head())\n\n#print(X.describe())\n#print(X_test.describe())","6b3bed94":"### Initialize stratified k-fold object\n\nskf = MultilabelStratifiedKFold(n_splits = FOLDS,random_state=SEED,shuffle=True)","eb0055ec":"def get_tf_model():\n    model = tf.keras.Sequential([\n        L.Flatten(input_shape=(1,X.shape[1])),\n        L.BatchNormalization(),\n        L.Dense(2000, activation='relu'),\n        L.BatchNormalization(),\n        L.Dropout(.4),\n        L.Dense(1000, activation='relu'),\n        L.BatchNormalization(),\n        L.Dropout(.4),\n        L.Dense(1000, activation='relu'),\n        L.BatchNormalization(),\n        L.Dropout(.4),\n        L.Dense(206, activation='sigmoid')\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    model.summary()\n    \n    return model","43161f01":"### learning rate schedule\n\ndef lrfn(epoch):\n    \n    if epoch < lr_rampup_epochs:\n        lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n        lr = lr_max\n    else:\n        lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n    return lr","91bc0119":"def plot_lr():\n    rng = [i for i in range(EPOCHS)]\n    y = [lrfn(x) for x in rng]\n    plt.plot(rng, y)\n    print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","c886f921":"### Fit model\n\ndef fit_model(model,X_train,X_valid,y_train,y_valid):\n\n    start = time.time()\n    \n    print('Beginning to fit ',type(model))\n\n    if 'tensorflow' in str(type(model)): # Fit neural net model\n    \n        model.fit(\n            X_train,\n            y_train,\n            epochs=EPOCHS,\n            verbose=1,\n            batch_size=BATCH_SIZE,\n            callbacks=[lr_schedule],\n            validation_data=(X_valid,y_valid)\n        )\n    \n    else: # Fit other type of model\n    \n        model.fit(X_train,y_train)\n\n    print('Total time taken to fit model: ', time.time() - start, ' seconds')","e645fc54":"### Make Predictions\n\ndef get_preds(model,X_valid,final=False):\n\n    if 'tensorflow' in str(type(model)): # Neural Network model predictions \n        \n        if final==True:\n            preds = np.array(model.predict(X_test).astype(\"float64\"))\n        else:\n            preds = np.array(model.predict(X_valid).astype(\"float64\"))\n            \n    else:    # Other model predictions          \n        \n        if final==True:\n            preds = np.array(model.predict_proba(X_test))\n        else:\n            preds = np.array(model.predict_proba(X_valid))\n        \n        preds = preds[:,:,1].T\n    \n    return preds","62b30b80":"### Calculate validation score\n\ndef calc_loss(vals,preds):\n\n    score = log_loss(np.ravel(vals),np.ravel(preds))\n    \n    cv_scores.append(score)\n\n    print('Validation log loss score: {}'.format(score))","24ab23db":"def run_model(model,X_train,X_valid,y_train,y_valid):\n\n    ### fit the model\n    fit_model(model,X_train,X_valid,y_train,y_valid)\n\n    print('Getting validation predictions...')\n    \n    ### get the predictions\n    temp_val_preds = get_preds(model,X_valid,final=False)\n    \n    ### calculate log loss\n    calc_loss(y_valid,temp_val_preds)\n    \n    print('Calculating final predictions...')\n\n    ### final preds\n    final_preds.append(get_preds(model,X_valid,final=True))\n    \n    print('Done')","d9cb35fb":"### XGBoost Model\n\nif include_xgboost == True:\n    \n    model_1 = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\n    # The MultiOutputClassifier wrapper creates one model for each class (i.e. 206 different models total)\n\n    # Using parameters from https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\n    params = {'estimator__colsample_bytree': 0.6522,\n          'estimator__gamma': 3.6975,\n          'estimator__learning_rate': 0.0503,\n          'estimator__max_delta_step': 2.0706,\n          'estimator__max_depth': 10,\n          'estimator__min_child_weight': 31.5800,\n          'estimator__n_estimators': 166,\n          'estimator__subsample': 0.8639\n         }\n\n    model_1.set_params(**params)","0518a3dc":"### Neural Network Model\n\nif include_neural_net == True:\n    \n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n    \n    plot_lr()\n    \n    model_2 = get_tf_model()","c1f1f533":"### Logistic Regression model\n\nif include_logreg == True:\n    \n    model_3 = MultiOutputClassifier(LogisticRegression(max_iter=10000, tol=0.1, C = 0.5,verbose=0,random_state = SEED))","6741ac19":"cv_scores = []\nfinal_preds = []","3ade46df":"### Stratified K-Fold loop \n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    print('Beginning fold',fold+1)\n    print(\"TRAIN INDEX:\", train_index, \"VALID INDEX:\", valid_index)\n    \n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    if include_xgboost == True:\n        \n        run_model(model_1,X_train,X_valid,y_train,y_valid) # takes ~4 minutes\n        \n    if include_neural_net == True:\n        \n        run_model(model_2,X_train,X_valid,y_train,y_valid) # takes ~10 seconds with GPU\n        \n    if include_logreg == True:\n        \n        run_model(model_3,X_train,X_valid,y_train,y_valid) # takes ~8 min\n        \n","316c7956":"### Show all CV scores\n\nprint('Cross Validation scores: ',cv_scores)","1d75a662":"### Ensemble final predictions\n\nprint('Ensembling final predictions')\nfinal_predictions = np.mean(np.array(final_preds),axis=0)\n\nprint('Done')","f266fd0f":"### Output final predictions\n\nsample_sub_df.iloc[:,1:] = final_predictions\nsample_sub_df.to_csv('submission.csv',index=False)","d291374d":"### Insight into final predictions\n\nsample_sub_df.describe()","5d2eb630":"## Understanding our data set","2712079d":"## Intro","fba06f7b":"## Output results","39271116":"This kernel serves as a stratified k-fold ensemble approach baseline for the MoA (Mechanisms of Action) kaggle competition. The 3 models I have seen mostly used for this competition are Neural Networks, XGBoost (or some other boosted tree model), and Logistic Regression. Here I present a simple starter notebook to ensemble these three models with cross validation.","44a9943f":"We are using 875 features to predict binary labels on 206 different classes. We have 23,814 instances to train on, and each instance can be multiple different classes. This makes this a multi-label classification problem.","405d21a1":"## Getting everything ready","0380f185":"## Preparing our data","3ca71a0e":"This type of learning rate schedule has become the go-to for Kaggle competitions. It starts out small, ramps up to a certain threshold for a couple epochs, then decays exponentially.","2e5228d4":"There are many ways to improve the validation and leaderboard score from this notebook. Here are some ideas to experiment with:\n* Principle Component Analysis\n* Grid search to optimize model hyperparameters\n* Experimenting with number of epochs, number of layers, types of layers, learning rate, etc. for Neural Network\n* Bagging\/boosting with other models","858804f4":"We will be using a multi-label stratified K-fold. Scikit-learn doesn't support this, but there is one located in this GitHub repository: [https:\/\/github.com\/trent-b\/iterative-stratification](http:\/\/) . We would normally be able to install this package with a simple pip command, but the competition rules state no internet use is allowed. Therefore, we will have to manually add this to our input.","ee3c819f":"## Ensemble setup","9534e43a":"My neural network contains 3 hidden layers, with batch normalization and dropout. This architecture is very simple (trains in only a few seconds w\/ GPU) and should be experimented with to see what works best. Be sure to keep the last layer's activation as 'sigmoid' so that the outputs are all in [0,1].","f24a46fd":"As you can see, there is a significantly small amount of positive labels in many classes. Three classes only have one positive instance! We must make sure we have these instances in the training set and not the validation set, or else our model will only predict one class! This is why we have to use a stratified k-fold.","d17c176a":"It is always a good idea to look at the distribution of positive vs negative labels in each class. This helps us know how to split our training data better, and know what to look for in results.","6c951148":"## Train models"}}