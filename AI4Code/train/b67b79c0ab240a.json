{"cell_type":{"6a4c5f1d":"code","aa9552fd":"code","44e27dc4":"code","b77f7857":"code","d7c7b8b8":"code","f59301ab":"code","188a75c7":"code","7509b9ae":"code","c03da2f1":"code","01a6fe33":"code","4f5b4c31":"code","4b9be5c6":"code","7c8f8d0a":"code","67a818d3":"code","4377b49d":"code","d0d86bd0":"code","911164db":"code","b70faec4":"code","18958373":"code","dc37fd4b":"code","ce66a20a":"code","23aaba00":"code","de0f3fa6":"code","10e06cdc":"code","8eec8cee":"code","4edb4d65":"code","408ccd57":"code","583e9bf6":"code","1d452a63":"code","a07bc81e":"code","8ef518a3":"code","60c0af17":"code","d21eb8cd":"code","46736359":"code","9c185142":"code","66ff47ba":"code","dfc07e98":"code","dad49956":"code","416f31ec":"code","fac31882":"code","443fd5f9":"code","45a04699":"code","f740750d":"markdown","ad99bbb9":"markdown","f567aa5b":"markdown","1b711f49":"markdown","5ddae953":"markdown","a5f8ad20":"markdown","77ead712":"markdown","0ba12aa3":"markdown","caf28a08":"markdown","b79f04c9":"markdown","05677dce":"markdown","69e4fed7":"markdown","ae7e1a43":"markdown","ebded18e":"markdown","c7d6c84b":"markdown","5b6ce17c":"markdown","817786c7":"markdown","cefe7b63":"markdown","be692cd7":"markdown","c585af95":"markdown","5071fe04":"markdown","87ea8acc":"markdown","1e3ce7ab":"markdown","5d3767df":"markdown","1a3ccb48":"markdown","85a464c2":"markdown"},"source":{"6a4c5f1d":"import numpy as np \nimport pandas as pd \nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa9552fd":"from zipfile import ZipFile\n\nbase_path = '\/kaggle\/input\/dogs-vs-cats\/'\nunzip_path = '\/kaggle\/working\/' # \ud604\uc7ac \ub514\ub809\ud1a0\ub9ac\n\n# \ubc18\ubcf5\ubb38 \uc774\uc6a9\nfor folder in os.listdir(path = base_path):\n    if folder.split(\".\")[1] == 'zip':\n        with ZipFile(base_path + folder, 'r') as zipfile:\n            zipfile.extractall(unzip_path)\n            print(f'{folder} \uc555\ucd95\ud574\uc81c \uc644\ub8cc!')","44e27dc4":"train_dir = os.path.join(unzip_path, 'train')\ntest_dir = os.path.join(unzip_path, 'test1')\n\n# \ud30c\uc77c\ub9ac\uc2a4\ud2b8 \uac00\uc838\uc624\uae30\ntrain_img_names = os.listdir(train_dir)\ntest_img_names = os.listdir(test_dir)\n\nprint('total training images : ', len(train_img_names))\nprint('total test images : ', len(test_img_names))","b77f7857":"train_img_names[:5]","d7c7b8b8":"categories = list()\n\n# \ubc18\ubcf5\ubb38 \uc774\uc6a9\nfor image in train_img_names:\n    category = image.split('.')[0]\n    if category == 'dog':\n        categories.append('dog')\n    else:\n        categories.append('cat')\n        \ndf = pd.DataFrame({'Image':train_img_names, 'Category':categories})[:2000]","f59301ab":"# \ub9cc\ub4e4\uc5b4\uc9c4 \ub370\uc774\ud130\ud504\ub808\uc784 df\ub97c \ucd9c\ub825\ud574\ubcf4\uc790.\n","188a75c7":"# \ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac import\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7509b9ae":"f = plt.figure(figsize=(5, 5))\nax = f.add_subplot()\nsns.countplot(data=df, x='Category', ax=ax)\n\nfor patch in ax.patches:\n    label_x = patch.get_x() + patch.get_width()\/2\n    label_y = patch.get_y() + patch.get_height()\/2\n    text_msg = str(int(patch.get_height())) \n    ax.text(label_x, label_y, text_msg, horizontalalignment='center', verticalalignment='center')\n    \nplt.show()","c03da2f1":"import random\n\nsample = random.choice(train_img_names)\nsample_path = '\/kaggle\/working\/train\/' + sample\nplt.imshow(plt.imread(sample_path))\nplt.show()","01a6fe33":"# \ud55c\ubc88\uc5d0 \uc5ec\ub7ec\uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \ucd9c\ub825\npath = '\/kaggle\/working\/train\/'\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    img_path = path + df.Image[i]\n    \n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(plt.imread(img_path))\n    plt.xlabel(df.Category[i])\nplt.show()","4f5b4c31":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, test_size=0.25, stratify=df['Category'])\n# dataframe\uc758 \uae30\uc874 \uc778\ub371\uc2a4 \uc81c\uac70\ntrain = train.reset_index(drop=True)\n# test = test.reset_index(drop=False)","4b9be5c6":"f, ax = plt.subplots(1, 2, figsize=(13,5))\nsns.countplot(data=train, x='Category', palette='magma', ax=ax[0])\nsns.countplot(data=test, x='Category', palette='magma', ax=ax[1])\n\nfor patch in ax[0].patches:\n    label_x = patch.get_x() + patch.get_width()\/2\n    label_y = patch.get_y() + patch.get_height()\/2\n    text_msg = str(int(patch.get_height())) \n    ax[0].text(label_x, label_y, text_msg, horizontalalignment='center', verticalalignment='center')\n\nfor patch in ax[1].patches:\n    label_x = patch.get_x() + patch.get_width()\/2\n    label_y = patch.get_y() + patch.get_height()\/2\n    text_msg = str(int(patch.get_height())) \n    ax[1].text(label_x, label_y, text_msg, horizontalalignment='center', verticalalignment='center')\n\nplt.show()","7c8f8d0a":"# keras\uc758 ImageDataGenerator \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc774\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \uc804\ucc98\ub9ac \uc9c4\ud589\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nheight, width, channel = (150, 150, 3)\n\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255.)\n\ntrain_generator = train_datagen.flow_from_dataframe(train,\n                                                   directory = '.\/train',\n                                                   x_col='Image',\n                                                   y_col='Category',\n                                                   batch_size=32,\n                                                   class_mode='categorical',\n                                                   color_mode= 'rgb',\n                                                   target_size=(height, width))\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255.)\ntest_generator = test_datagen.flow_from_dataframe(test,\n                                                   directory = '.\/train',\n                                                   x_col='Image',\n                                                   y_col='Category',\n                                                   batch_size=32,\n                                                   class_mode='categorical',\n                                                   color_mode= 'rgb',\n                                                   target_size=(height, width))","67a818d3":"# \ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac import\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, InputLayer, Resizing\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization\nfrom tensorflow.keras.layers import MaxPool2D, GlobalMaxPooling2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import SGD","4377b49d":"# MLP \ubaa8\ub378 \uc815\uc758\nmlp_model = Sequential()\n\nmlp_model.add(InputLayer((height, width, channel)))\nmlp_model.add(Resizing(48, 48, interpolation='bilinear'))\nmlp_model.add(Flatten()) \nmlp_model.add(Dense(2048, activation='relu'))\nmlp_model.add(Dense(1024, activation='relu'))\nmlp_model.add(Dense(512, activation='relu'))\nmlp_model.add(Dense(128, activation='relu'))\nmlp_model.add(Dense(2, activation='softmax'))","d0d86bd0":"# \ubaa8\ub378 \ud655\uc778\nmlp_model.summary()","911164db":"# \ubaa8\ub378 \ucef4\ud30c\uc77c\nmlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\n# \ubaa8\ub378 \ud6c8\ub828\nmlp_history = mlp_model.fit(train_generator,\n                           validation_data=test_generator,\n                           epochs=20\n                           )","b70faec4":"# \uc815\ud655\ub3c4\uc640 \uc190\uc2e4\uac12 \ud655\uc778\n\nmlp_acc = mlp_history.history['accuracy']\nmlp_val_acc = mlp_history.history['val_accuracy']\nmlp_loss = mlp_history.history['loss']\nmlp_val_loss = mlp_history.history['val_loss']\n\nmlp_epochs = range(len(mlp_acc))\n\nplt.plot(mlp_epochs, mlp_acc, 'r', label='Training accuracy')\nplt.plot(mlp_epochs, mlp_val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(mlp_epochs, mlp_loss, 'r', label='Training Loss')\nplt.plot(mlp_epochs, mlp_val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","18958373":"class_names = ['cat','dogs']\ndef plot_image(i, predictions_array, true_label, img):\n    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(img)\n\n    predicted_label = np.argmax(predictions_array)\n    true_label = np.argmax(true_label)\n    if predicted_label == true_label:\n        color = 'blue'\n    else:\n        color = 'red'\n\n    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n    predictions_array, true_label = predictions_array[i], true_label[i]\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    thisplot = plt.bar(range(2), predictions_array, color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(predictions_array)\n    true_label = np.argmax(true_label)\n\n    thisplot[predicted_label].set_color('red')\n    thisplot[true_label].set_color('blue')","dc37fd4b":"x, y = test_generator.next()\npredictions = mlp_model.predict(x)\n\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(4*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, predictions, y, x)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, predictions, y)\nplt.show()","ce66a20a":"# CNN \ubaa8\ub378 \uc815\uc758\n\ncnn_model = Sequential()\n\ncnn_model.add(Conv2D(filters=32, kernel_size=3,activation=\"relu\", input_shape=(height, width, channel)))\ncnn_model.add(MaxPool2D(pool_size=2, strides=2))\ncnn_model.add(Conv2D(filters=64, kernel_size=3,activation=\"relu\"))\ncnn_model.add(MaxPool2D(pool_size=2, strides=2))\ncnn_model.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\"))\ncnn_model.add(MaxPool2D(pool_size=2, strides=2))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(units=512, activation=\"relu\"))\ncnn_model.add(Dropout(0.15))\ncnn_model.add(Dense(units=2, activation=\"softmax\"))","23aaba00":"# \ubaa8\ub378 \ud655\uc778\ncnn_model.summary()","de0f3fa6":"# \ubaa8\ub378 \ucef4\ud30c\uc77c\ncnn_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# \ubaa8\ub378 \ud6c8\ub828\ncnn_history = cnn_model.fit(train_generator,\n                            validation_data=test_generator,\n                            epochs=20)","10e06cdc":"# \ubaa8\ub378 \uc800\uc7a5\ncnn_model.save('cnn_model.h5')","8eec8cee":"# \uc815\ud655\ub3c4\uc640 \uc190\uc2e4\uac12 \ud655\uc778\n\ncnn_acc = cnn_history.history['accuracy']\ncnn_val_acc = cnn_history.history['val_accuracy']\ncnn_loss = cnn_history.history['loss']\ncnn_val_loss = cnn_history.history['val_loss']\n\ncnn_epochs = range(len(cnn_acc))\n\nplt.plot(cnn_epochs, cnn_acc, 'r', label='Training accuracy')\nplt.plot(cnn_epochs, cnn_val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(cnn_epochs, cnn_loss, 'r', label='Training Loss')\nplt.plot(cnn_epochs, cnn_val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","4edb4d65":"predictions = cnn_model.predict(x)\n\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(4*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, predictions, y, x)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, predictions, y)\nplt.show()","408ccd57":"# VGG \ubaa8\ub378 \uc815\uc758\nimg_input = Input(shape = (height,width,channel))\n\n# Block 1\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\nx = MaxPool2D((2, 2), strides=(2, 2), name='block1_pool')(x)\nx = BatchNormalization()(x)\n\n# Block 2\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\nx = MaxPool2D((2, 2), strides=(2, 2), name='block2_pool')(x)\nx = BatchNormalization()(x)\n\n# Block 3\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\nx = MaxPool2D((2, 2), strides=(2, 2), name='block3_pool')(x)\nx = BatchNormalization()(x)\n\n# Block 4\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\nx = MaxPool2D((2, 2), strides=(2, 2), name='block4_pool')(x)\nx = BatchNormalization()(x)\n\n# Block 5\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\nx = MaxPool2D((2, 2), strides=(2, 2), name='block5_pool')(x)\nx = BatchNormalization()(x)\n\nx = Flatten(name='flatten')(x)\nx = Dense(4096, activation='relu', name='fc1')(x)\nx = Dense(4096, activation='relu', name='fc2')(x)\nx = Dense(2, activation='softmax', name='predictions')(x)\n\nvgg_model = Model(img_input, x, name='vgg16')","583e9bf6":"# \ubaa8\ub378 \ud655\uc778\nvgg_model.summary()","1d452a63":"# \ubaa8\ub378 \ucef4\ud30c\uc77c\nvgg_model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n# \ubaa8\ub378 \ud559\uc2b5\nvgg_history = vgg_model.fit(train_generator,\n                            validation_data=test_generator,\n                            epochs=20)","a07bc81e":"# \uc815\ud655\ub3c4 \ubc0f \uc190\uc2e4\uac12 \ud655\uc778\n\nvgg_acc = vgg_history.history['accuracy']\nvgg_val_acc = vgg_history.history['val_accuracy']\nvgg_loss = vgg_history.history['loss']\nvgg_val_loss = vgg_history.history['val_loss']\n\nvgg_epochs = range(len(vgg_acc))\n\nplt.plot(vgg_epochs, vgg_acc, 'r', label='Training accuracy')\nplt.plot(vgg_epochs, vgg_val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(vgg_epochs, vgg_loss, 'r', label='Training Loss')\nplt.plot(vgg_epochs, vgg_val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","8ef518a3":"x, y = test_generator.next()\npredictions = vgg_model.predict(x)\n\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(4*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, predictions, y, x)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, predictions, y)\nplt.show()","60c0af17":"# \uc800\uc7a5\ud55c cnn \ubaa8\ub378 \ubd88\ub7ec\uc624\uae30\nfrom tensorflow.keras.models import load_model\nsaved_model = load_model('cnn_model.h5')","d21eb8cd":"# \ubd88\ub7ec\uc628 \ubaa8\ub378 \uc815\ubcf4 \ud655\uc778\n","46736359":"# \ub9ac\uc2a4\ud2b8 \ud615\ud0dc\uc774\uae30 \ub54c\ubb38\uc5d0 head()\uc0ac\uc6a9\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c [:5]\ub85c \ud655\uc778\ntest_img_names[:5]","9c185142":"from  tensorflow.keras.preprocessing import image\n\nimg_path = '.\/test1\/8961.jpg'\n# \ubaa8\ub378\uc774 \ud6c8\ub828\ub420 \ub54c \uc785\ub825\uc5d0 \uc801\uc6a9\ud55c \uc804\ucc98\ub9ac \ubc29\uc2dd\uc744 \ub3d9\uc77c\ud558\uac8c \uc0ac\uc6a9\ud574\uc918\uc57c\ud569\ub2c8\ub2e4. (color\uc815\ubcf4, \uc774\ubbf8\uc9c0 \ud06c\uae30 \ub4f1)\n\nimg = image.load_img(img_path, target_size=(150, 150), color_mode='rgb')\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor \/= 255.\n\n# \uc774\ubbf8\uc9c0 \ud06c\uae30 (1, 150, 150, 3)\nprint(img_tensor.shape)","66ff47ba":"# \ubd88\ub7ec\uc628 \uc774\ubbf8\uc9c0 \ud655\uc778\nplt.imshow(img_tensor[0])\nplt.show()","dfc07e98":"from tensorflow.keras import models\n\n\n# \uc0c1\uc704 8\uac1c \uce35\uc758 \ucd9c\ub825\uc744 \ucd94\ucd9c\ud569\nlayer_outputs = [layer.output for layer in saved_model.layers[:8]]\n# \uc785\ub825\uc5d0 \ub300\ud574 8\uac1c \uce35\uc758 \ucd9c\ub825\uc744 \ubc18\ud658\ud558\ub294 \ubaa8\ub378\uc744 \uc0dd\uc131\nactivation_model = models.Model(inputs=saved_model.input, outputs=layer_outputs)","dad49956":"# \uce35\uc758 \ud65c\uc131\ud654\ub9c8\ub2e4 \ud558\ub098\uc529 8\uac1c\uc758 \ub118\ud30c\uc774 \ubc30\uc5f4\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ub9ac\uc2a4\ud2b8\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4:\nactivations = activation_model.predict(img_tensor)","416f31ec":"first_layer_activation = activations[0]\nprint(first_layer_activation.shape)","fac31882":"plt.matshow(first_layer_activation[0, :, :, 31], cmap='viridis')\nplt.show()","443fd5f9":"plt.matshow(first_layer_activation[0, :, :, 9], cmap='viridis')\nplt.show()","45a04699":"# \uce35\uc758 \uc774\ub984\uc744 \uadf8\ub798\ud504 \uc81c\ubaa9\uc73c\ub85c \uc0ac\uc6a9\nlayer_names = []\nfor layer in saved_model.layers[:8]:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n# \ud2b9\uc131 \ub9f5 \uadf8\ub9ac\uae30\nfor layer_name, layer_activation in zip(layer_names, activations):\n    # \ud2b9\uc131 \ub9f5\uc5d0 \uc788\ub294 \ud2b9\uc131\uc758 \uc218\n    n_features = layer_activation.shape[-1]\n\n    # \ud2b9\uc131 \ub9f5\uc758 \ud06c\uae30 : (1, size, size, n_features)\n    size = layer_activation.shape[1]\n\n    # \ud65c\uc131\ud654 \ucc44\ub110\uc744 \uc704\ud55c \uadf8\ub9ac\ub4dc \ud06c\uae30\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    # \uac01 \ud65c\uc131\ud654\ub97c \ud558\ub098\uc758 \ud070 \uadf8\ub9ac\ub4dc\uc5d0 \ucc44\uc6c1\ub2c8\ub2e4\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            # \uadf8\ub798\ud504\ub85c \ub098\ud0c0\ub0b4\uae30 \uc88b\uac8c \ud2b9\uc131\uc744 \ucc98\ub9ac\ud569\ub2c8\ub2e4\n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n    # \uadf8\ub9ac\ub4dc \ucd9c\ub825\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\nplt.show()","f740750d":"\ucc44\ub110 \uc218\ub97c \ubc14\uafb8\uc5b4 \uac00\uba70 \ud655\uc778\ud574\ubcf4\uc790\n\n\uac15\uc544\uc9c0 \uc5bc\uad74 \ubb34\ub2ac \uc911 \uc0c9\uc774 \uad6c\ubd84\ub418\ub294 \uc120\uc774 \ub69c\ub837\ud788 \uad00\ucc30\ub41c\ub2e4.\n\n\uc774\uc81c \ub124\ud2b8\uc6cc\ud06c\uc758 \ubaa8\ub4e0 \ud65c\uc131\ud654\uce35\uc744 \uc2dc\uac01\ud654 \ud574\ubd05\uc2dc\ub2e4.\n\n**\ud2b9\uc131\ub9f5 \uadf8\ub9ac\uae30**","ad99bbb9":"**\ud83d\udcccTry it!!**","f567aa5b":"### \ub370\uc774\ud130\ubd84\uc11d \ubc0f \uc804\ucc98\ub9ac","1b711f49":"### VGG19","5ddae953":"### \u2705 CNN \ubaa8\ub378\uc774 \uc608\uce21\ud55c \uacb0\uacfc \uc2dc\uac01\ud654","a5f8ad20":"\ubaa8\ub378\uc5d0 \uc785\ub825\ud558\uc5ec \ucef4\ud4e8\ud130\uac00 \ud559\uc2b5 \ud560 \uc218 \uc788\ub3c4\ub85d \uc804\ucc98\ub9ac \ud574\uc900\ub2e4. ","77ead712":"**\ud83c\udf81Try it!!**\n\n\ubd88\ub7ec\uc628 \ubaa8\ub378 \uc815\ubcf4\ub97c \ud45c\uc2dc\ud574\ubcf4\uc790.","0ba12aa3":"**\ud6c8\ub828\ub370\uc774\ud130\uc640 \ud14c\uc2a4\ud2b8\ub370\uc774\ud130 \uac2f\uc218 \uc2dc\uac01\ud654**","caf28a08":"### \u2705 \ubaa8\ub378\uc774 \uc608\uce21\ud55c \uacb0\uacfc \uc2dc\uac01\ud654","b79f04c9":"\ud83c\udf81 **Try it !!!**\n\ntrain, test \ub370\uc774\ud130\ub97c \uac01\uac01 reset_index(drop=True) \ucc98\ub9ac\ub97c \ud588\uc744 \ub54c\uc640\n\n\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c \ub370\uc774\ud130\ud504\ub808\uc784 \uc778\ub371\uc2a4 \ucc28\uc774\ub97c \ud655\uc778\ud558\uc5ec \ubcf4\uc138\uc694.","05677dce":"**\ud83d\udccc Try it!!**\n\n\uc2e4\ud589\ud560\ub54c\ub9c8\ub2e4 \uc815\ub9d0\ub85c random\ud558\uac8c \uc774\ubbf8\uc9c0\uac00 \ucd9c\ub825\ub418\ub294\uc9c0 \ud655\uc778\ud574\ubcf4\uc790.","69e4fed7":"**\uc774\ubbf8\uc9c0 \uc774\ub984\uacfc \ubd84\ub958 \uce74\ud14c\uace0\ub9ac \uc815\ubcf4\ub97c \uac16\ub294 dataframe \uc0dd\uc131**","ae7e1a43":"**\ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac import \ubc0f \ucd94\uac00\ub41c \ub370\uc774\ud130 path \ud655\uc778**","ebded18e":"\n\ub2e4\ub978 \ucc44\ub110\ub3c4 \uadf8\ub824\ubcf4\uc790.","c7d6c84b":"\ubaa8\ub378\uc774 \uc0ac\uc9c4\uc744\ubcf4\uace0 \uac01 \uce35\ubcc4\ub85c \uc5b4\ub5a4 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud588\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uc790.","5b6ce17c":"**\ub370\uc774\ud130 \uc2dc\uac01\ud654**","817786c7":"### \ubaa8\ub378 \ud6c8\ub828","cefe7b63":"### \u2705 VGG \ubaa8\ub378\uc774 \uc608\uce21\ud55c \uacb0\uacfc \uc2dc\uac01\ud654","be692cd7":"**\uc2e4\uc2b5 \ubaa9\ud45c**\n\n\ud83d\udca1 \ub525\ub7ec\ub2dd \ubaa8\ub378 (MLP, CNN, VGG19)\uc744 \uc774\uc6a9\ud55c \uac1c vs.\uace0\uc591\uc774 \uc774\ubbf8\uc9c0 \ubd84\ub958\n\n\ud83d\udca1 \ubaa8\ub378\uc774 \uc774\ubbf8\uc9c0\uc758 \uc5b4\ub5a4 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uc600\ub294\uc9c0 \uc2dc\uac01\ud654 ","c585af95":"### MLP","5071fe04":"### \ud6c8\ub828\ub370\uc774\ud130, \ud14c\uc2a4\ud2b8\ub370\uc774\ud130 \ub098\ub204\uae30","87ea8acc":"**zip\ud30c\uc77c \uc555\ucd95 \ud574\uc81c**","1e3ce7ab":"**\ud6c8\ub828\uc5d0 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uc774\ubbf8\uc9c0 \ud558\ub098 \uac00\uc838\uc624\uae30**","5d3767df":"### \ubaa8\ub378\uc774 \uc5b4\ub5a0\ud55c \ud2b9\uc9d5\uc73c\ub85c \uc774\ubbf8\uc9c0\ub97c \ubd84\ub958 \ud55c \uac78\uae4c!?\n\n\ud83d\udc40\uc2dc\uac01\ud654 \ud574\ubcf4\uc790! ","1a3ccb48":"random \ud568\uc218\ub97c \uc774\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0 \ud655\uc778\ud558\uae30","85a464c2":"### CNN"}}