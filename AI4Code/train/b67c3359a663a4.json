{"cell_type":{"d4e13294":"code","7d5f968a":"code","ec26f364":"code","2e3c43ae":"code","605977e9":"code","f75706ce":"code","1c7241f7":"code","55c93ebd":"code","ae19bbbe":"code","ef861604":"code","7d61f5ff":"code","d7c569c5":"code","996466ef":"code","9b6eab73":"code","3ae19679":"code","24553d96":"code","326c5e2c":"code","e7c5d66f":"code","3c040939":"code","7390daa8":"code","75758316":"code","f51628cc":"code","2f2c7e53":"code","661ce6ad":"code","b7908ba5":"code","6c22959c":"code","50f26ded":"code","8b0f25cb":"code","a9439ab6":"code","18ffb27f":"code","6f6da0d7":"code","a65cd984":"code","90440b62":"code","fa5df052":"code","fd7972bd":"code","70d96d41":"code","25d2d7fa":"code","00eb285c":"code","0a14a2f5":"code","867b5651":"code","8c3995ad":"code","6c5e1f70":"code","704931a3":"code","45cfaf0b":"code","f3b4731f":"code","fccc9392":"code","2d7da0e8":"code","2461e037":"code","1ef0379d":"code","3ece2c58":"code","853a7527":"code","834bee77":"code","e5b70a78":"code","3f9b3c25":"code","525c15d1":"code","99969223":"code","701bdc00":"code","50aa8603":"code","7945d9a4":"code","8ca7457c":"code","d242fb48":"code","f4e50a1b":"code","b45b7a10":"code","31d69540":"code","aca473f1":"code","154692e0":"code","d4e45ae1":"code","f6d8a354":"code","017b4151":"code","7592e7ec":"code","a3b81683":"code","8d4fb5b5":"code","9004c83e":"code","87a14515":"code","0b8585f9":"code","168c50c5":"code","eb9a09de":"code","22b189e1":"markdown","bb215295":"markdown","e07e5cf4":"markdown","b7235aac":"markdown","0526c900":"markdown","079fadba":"markdown","1f246b66":"markdown","ca32ea2e":"markdown","c1921265":"markdown","f5a64deb":"markdown","68b7ee10":"markdown","656c1bc6":"markdown","eaa6b014":"markdown","d28ecf07":"markdown","be781d0f":"markdown","07160bfa":"markdown","bc5d72b2":"markdown","7a3e903c":"markdown","67392791":"markdown","183ff582":"markdown","7bc20139":"markdown","31b64b1f":"markdown","17d538cc":"markdown","b4a1f105":"markdown","ff241fad":"markdown","e1c56f5e":"markdown","3a294e97":"markdown","e5a052ba":"markdown"},"source":{"d4e13294":"!pip install tweet-preprocessor","7d5f968a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import regularizers\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport unicodedata\nimport re\nimport time\nimport gensim\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\n\nimport preprocessor as twitter_p\n\nfrom tqdm import tqdm\n\nimport spacy\nimport gc\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec26f364":"tf.random.set_seed(123)\nnp.random.seed(123)","2e3c43ae":"start_time = time.time()","605977e9":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","f75706ce":"print(train.shape)\nprint(test.shape)","1c7241f7":"train.head()","55c93ebd":"test.head()","ae19bbbe":"train.info()","ef861604":"def jaccard(str1, str2):\n    a = set(str(str1).lower().split())\n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return round(float(len(c)) \/ (len(a) + len(b) - len(c)), 4)","7d61f5ff":"results_jaccard = []\nfor index, row in train.iterrows():\n    sentence1 = row.keyword\n    sentence2 = row.text\n    jaccard_score = jaccard(sentence1, sentence2)\n    results_jaccard.append([sentence1, sentence2, jaccard_score])","d7c569c5":"jaccard_score = pd.DataFrame(results_jaccard, columns=['keyword', 'text', 'jaccard_score'])","996466ef":"#sns.set(style=\"whitegrid\")\nf, ax = plt.subplots(figsize=(6, 15))\nsns.set_color_codes(\"pastel\")\nsns.countplot(y=\"jaccard_score\",data=jaccard_score, color=\"b\")","9b6eab73":"stopwords_en = stopwords.words('english')","3ae19679":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')","24553d96":"twitter_p.set_options(twitter_p.OPT.URL)\ndef preprocess_sentence(w):\n#    w = ' '.join(map(lambda word: abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word, w.split(' ')))\n    w = twitter_p.clean(w)\n    w = unicode_to_ascii(w.lower().strip())\n    \n    w = re.sub(r\"([@#])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z@#]+\", \" \", w)\n    w = ' '.join([word for word in w.split(' ') if word not in stopwords_en])\n    \n    w = w.rstrip().strip()\n    return w","326c5e2c":"train = train.fillna(value='')\ntrain.info()","e7c5d66f":"train['text'] = train['text'].apply(func=preprocess_sentence)\ntrain['keyword'] = train['keyword'].apply(func=preprocess_sentence)\n\nprint(train.head(10))","3c040939":"test = test.fillna(value='')\ntest.info()","7390daa8":"test['text'] = test['text'].apply(func=preprocess_sentence)\ntest['keyword'] = test['keyword'].apply(func=preprocess_sentence)\nprint(test.head(10))","75758316":"text_list = np.stack([*train['text'], *train['keyword'], *test['text'], *test['keyword']])\ntokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\ntokenizer.fit_on_texts(text_list)\nprint(len(tokenizer.word_index))","f51628cc":"# word2vec = gensim.models.KeyedVectors.load_word2vec_format('\/kaggle\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)\n\nglove = np.load('\/kaggle\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)\n\n# fasttext = np.load('\/kaggle\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl', allow_pickle=True)\n\n# twitter_word2vec = np.load('\/kaggle\/input\/twitter-word2vec300d\/twitter_word2vec.npy', allow_pickle=True)\n\n# glovepath = '\/kaggle\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt'\n# embeddings_index = dict()\n# with open(glovepath) as f:\n#     for line in f:\n#       values = line.split()\n#       word = values[0]\n#       coefs = np.asarray(values[1:], dtype='float32')\n#       embeddings_index[word] = coefs\n# print('Loaded %s word vectors.' % len(embeddings_index))","2f2c7e53":"# #\u5c06\u6570\u636e\u8bfb\u5165\u5b57\u7b26\u4e32\u5217\u8868\u3002\n# def read_data(filename):\n#     \"\"\"\u8bfb\u53d6\u6570\u636e\u5355\u8bcd\u5217\u8868\u3002\"\"\"\n#     with open(filename,'r') as f:\n#         data = tf.compat.as_str(f.read()).split()\n#     return data\n\n# english_corpus = read_data('\/kaggle\/input\/english-corpus\/text8')\n# print(\"Data size\",len(english_corpus),'Data_type',type(english_corpus),'Data[0:5]',english_corpus[0:5])\n\n# total_sentences = np.stack([*train['text'],*test['text']])\n# sentences = [list(gensim.utils.tokenize(s)) for s in total_sentences]\n# sentences.append(english_corpus)\n# gen_word2vec=gensim.models.word2vec.Word2Vec(sentences, size=200, min_count=0, iter=20, trim_rule=None)","661ce6ad":"input_vocab_size = len(tokenizer.word_index) + 3\nd_model = 300","b7908ba5":"ps = PorterStemmer()\nlc = LancasterStemmer()\nsb = SnowballStemmer(\"english\")","6c22959c":"len(glove.keys())","50f26ded":"words = glove.keys()\nw_rank = {}\nfor i,word in enumerate(words):\n    w_rank[word] = i\nWORDS = w_rank\n\ndef words(text): return re.findall(r'\\w+', text.lower())\ndef P(word): \n    \"Probability of `word`.\"\n    # use inverse of rank as proxy\n    # returns 0 if the word isn't in the dictionary\n    return - WORDS.get(word, 0)\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or [word])\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)","8b0f25cb":"def create_embedding_matrix(vectors, to_word_it, inp_vocab_size, d_m, lemma_dict):\n    no_in_vocab = []\n    matrix = np.random.uniform(low=-1, high=1, size=(inp_vocab_size, d_m))\n    unknown_vector = np.zeros((d_m,), dtype=np.float32) - 1\n    \n    for key, index in to_word_it:\n        \n        word = key\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n        \n        word = key.lower()\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n            \n        word = key.upper()\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n            \n        word = key.capitalize()\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n            \n        word = ps.stem(key)\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n            \n        word = lc.stem(key)\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n            \n        word = sb.stem(key)\n        try:\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n            \n        try:\n            word = lemma_dict[key]\n            matrix[index] = vectors[word]\n            continue\n        except KeyError:\n            ''\n        \n        if len(key) > 1:\n            word = correction(key)\n            try:\n                matrix[index] = vectors[word]\n                continue\n            except KeyError:\n                ''\n        \n        try:\n            matrix[index] = vectors[word]\n        except KeyError:\n            matrix[index] = unknown_vector\n            no_in_vocab.append(word)\n\n    del vectors\n\n    print(\"no_in_vocab size:\", len(no_in_vocab))\n    print(\"embedding_matrix shape:\", matrix.shape)\n    \n    return matrix, no_in_vocab","a9439ab6":"print(\"Spacy NLP ...\")\ntext_list = pd.concat([train['text'], test['text']])\n\nprint(len(tokenizer.word_index))\nnlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n\nword_dict = {}\nword_index = 1\nlemma_dict = {}\ndocs = nlp.pipe(text_list, n_threads = 2)\nword_sequences = []\n\nfor doc in tqdm(docs):\n    word_seq = []\n    for token in doc:\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n            word_dict[token.text] = word_index\n            word_index += 1\n            lemma_dict[token.text] = token.lemma_\n        if token.pos_ is not \"PUNCT\":\n            word_seq.append(word_dict[token.text])\n    word_sequences.append(word_seq)\n    \ndel docs\n\ngc.collect()","18ffb27f":"embedding_matrix, no_in_vocab = create_embedding_matrix(glove, tokenizer.word_index.items(), input_vocab_size, d_model, lemma_dict)\ndel glove","6f6da0d7":"def inter_section(texts, keywords, niv): \n    niv = set(niv)\n    text_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n    text_tokenizer.fit_on_texts(np.stack([*texts, *keywords]))\n    vocab = set(text_tokenizer.word_index.keys())\n    text_in_niv = vocab.intersection(niv)\n    print(\"vocab:\", len(vocab), len(text_in_niv), len(text_in_niv)\/len(vocab))","a65cd984":"inter_section(train['text'], train['keyword'], no_in_vocab)","90440b62":"inter_section(test['text'], test['keyword'], no_in_vocab)","fa5df052":"def incorrect_count(train_texts, test_texts, vocab):\n    vocab = set(vocab)\n    wrong_words = []\n    for text in train_texts:\n        intersection = set(text.split()).intersection(vocab)\n        if len(intersection)>0:\n            wrong_words.extend(intersection)\n    \n    train_ww_count = np.asarray(Counter(wrong_words).most_common())\n    train_ww_count = np.concatenate([train_ww_count, np.asarray(['train']*len(train_ww_count))[:, np.newaxis]], axis=-1)\n    \n    wrong_words = []\n    for text in test_texts:\n        intersection = set(text.split()).intersection(vocab)\n        if len(intersection)>0:\n            wrong_words.extend(intersection)\n    \n    test_ww_count = np.asarray(Counter(wrong_words).most_common())\n    test_ww_count = np.concatenate([test_ww_count, np.asarray(['test']*len(test_ww_count))[:, np.newaxis]], axis=-1)\n    \n    ww_count = np.concatenate([train_ww_count, test_ww_count], axis=0)\n    ww_count = pd.DataFrame(ww_count, columns=['wrong_text', 'count','set'])\n    ww_count['count'] = ww_count['count'].astype('int')\n    ww_count = ww_count.sort_values(by='count', ascending=False)\n\n    print('head:\\n', ww_count.head())\n    print('\\n count<2:', len(ww_count[ww_count['count']<2])\/len(ww_count))\n    \n    plt.figure(figsize=(6,36))\n    sns.barplot(x='count', y='wrong_text', hue='set', orient='h', data=ww_count.head(100))\n    plt.show()","fd7972bd":"incorrect_count(train['text'], test['text'], no_in_vocab)","70d96d41":"def len_sentence(texts, key):\n    result_num = []\n    for index, row in texts.iterrows():\n        sentence = row[key]\n        num_text = len(sentence.split())\n        result_num.append([row['id'], num_text, row['target']])\n\n    num_texts = pd.DataFrame(result_num, columns=['id', 'Num_text', 'target'])\n    num_texts_sort = num_texts.sort_values(by='Num_text', ascending=False)\n    \n    plt.figure(figsize=(6, 18)) \n    sns.countplot(y='Num_text', hue='target', data=num_texts_sort, color='b')\n    plt.show()\n    \n    return num_texts","25d2d7fa":"num_texts =len_sentence(train, 'text')","00eb285c":"num_keyword = len_sentence(train, 'keyword')","0a14a2f5":"# train = train.merge(num_texts, how='outer')\n# train.info()\n# train = train[train['Num_text'] <= 22]","867b5651":"train.info()","8c3995ad":"def texts_to_sequences(byte):\n    char = str(byte, encoding='utf-8')\n    sequences = tokenizer.texts_to_sequences([char])\n    return np.reshape(sequences, (-1))","6c5e1f70":"def text_encode(keyword, lang, target):\n    keyword = texts_to_sequences(keyword.numpy())\n    lang = [len(tokenizer.word_index), *keyword, len(tokenizer.word_index) + 1, *texts_to_sequences(lang.numpy()), len(tokenizer.word_index)+2]\n    return lang, target","704931a3":"def tf_encode(id_num, keyword, lang, target):\n    lang, target = tf.py_function(\n        text_encode, \n        [keyword, lang, target], \n        [tf.int64, tf.int64])\n    \n    id_num.set_shape(None)\n    lang.set_shape([None])\n    target.set_shape([])\n    \n    return id_num, lang, target","45cfaf0b":"class EmbeddingLayer(object):\n    def __init__(self):\n        \n        self.kernels = tf.Variable(initial_value=embedding_matrix, trainable=False, name='Embedding_kernels')\n    def __call__(self, x):\n        embeddings = tf.nn.embedding_lookup(params=self.kernels, ids=x)\n        return embeddings","f3b4731f":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2*(i\/\/2))\/np.float32(d_model))\n    return pos * angle_rates","fccc9392":"def positional_encoding(postion, d_model):\n    angle_rads = get_angles(np.arange(postion)[:,np.newaxis], np.arange(d_model)[np.newaxis,:], d_model)\n    \n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    pos_encoding = angle_rads[np.newaxis, ...]\n    \n    return tf.cast(pos_encoding, dtype=tf.float32)","2d7da0e8":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), dtype=tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]","2461e037":"def create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size,size)), -1, 0)\n    return mask","1ef0379d":"def scaled_dot_product_attention(q, k, v, mask):\n    \n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    \n    dk = tf.cast(tf.shape(q)[-1], dtype=tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n    \n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    \n    output = tf.matmul(attention_weights, v)\n    \n    return output, attention_weights","3ece2c58":"class MultiHeadAttention(tf.keras.layers.Layer):\n    \n    def __init__(self, num_heads, d_model):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        \n        assert d_model % num_heads == 0\n        \n        self.depth = d_model \/\/ num_heads\n        \n        self.wq = tf.keras.layers.Dense(units=d_model)\n        self.wk = tf.keras.layers.Dense(units=d_model)\n        self.wv = tf.keras.layers.Dense(units=d_model)\n        \n        self.dense = tf.keras.layers.Dense(units=d_model)\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        \n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n        \n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        \n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        \n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        \n        output = self.dense(concat_attention)\n        \n        return output, attention_weights","853a7527":"def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(units=dff, activation='relu'),\n        tf.keras.layers.Dense(units=d_model)\n    ])","834bee77":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n        self.ffn = point_wise_feed_forward_network(d_model=d_model, dff=dff)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate=rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate=rate)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n    def call(self, x, training, mask):\n        attn_output, attn_weights = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n        \n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n        \n        return out2","e5b70a78":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n        self.num_layers = num_layers\n        self.d_model = d_model\n        \n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n        self.enc_layers = [EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n        \n        self.dropout = tf.keras.layers.Dropout(rate=rate)\n        \n    def call(self, x, training, mask, positition=True):\n        \n        seq_len = tf.shape(x)[1]\n        \n        x *= tf.math.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n        if positition:\n            x += self.pos_encoding[:, :seq_len, :]\n        \n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n            \n        return x","3f9b3c25":"class OutputLayer(tf.keras.layers.Layer):\n    def __init__(self, units, rate):\n        super(OutputLayer, self).__init__()\n        self.gapool1d = tf.keras.layers.GlobalAveragePooling1D()\n        \n        self.dense = tf.keras.layers.Dense(units=units, activation='relu')\n        self.final_layer = tf.keras.layers.Dense(units=2)\n        \n        self.dropout = tf.keras.layers.Dropout(rate=rate)\n    def call(self, enc, training):\n        x = self.gapool1d(enc)\n\n        x = self.dense(x)\n    \n        x = self.dropout(x, training=training)\n    \n        x = self.final_layer(x)\n        return x","525c15d1":"class TransformerCategorical(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, output_units, rate=0.1):\n        super(TransformerCategorical, self).__init__()\n        \n        self.embedding = EmbeddingLayer()\n        \n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, maximum_position_encoding, rate)\n        \n        self.outputlayer = OutputLayer(output_units, rate)\n        \n    def call(self, lang, training, enc_padding_mask):\n        \n        enc_input = self.embedding(lang)\n        \n        enc_output = self.encoder(enc_input, training, enc_padding_mask)\n        \n        out = self.outputlayer(enc_output, training)\n        return out","99969223":"num_layers = 6\nd_model = d_model\nnum_heads = 6\ndff = 512\npe_input = input_vocab_size\noutput_units = 64\nrate = 0.1","701bdc00":"tsfr_categorical = TransformerCategorical(num_layers, d_model, num_heads, dff, pe_input, output_units, rate)","50aa8603":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=600):\n        super(CustomSchedule, self).__init__()\n        self.d_model = tf.cast(d_model, dtype=tf.float32)\n        \n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        step = step + 100\n        arg1 = step ** -0.8\n        arg2 = step * (self.warmup_steps ** -1.5)\n        \n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","7945d9a4":"learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)","8ca7457c":"temp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(300, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","d242fb48":"a = tf.Variable([[1],[2]])\ntf.squeeze(a, axis=1).numpy()","f4e50a1b":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n\ndef loss_function(real, pred):\n    loss_ = loss_object(real, pred)\n    return tf.reduce_mean(loss_)\n\ndef acc_function(real, pred):\n    predictions = tf.math.argmax(pred, axis=1)\n    predictions = tf.cast(predictions, dtype=tf.int64)\n    accuracy = tf.cast(tf.math.equal(predictions, real), dtype=tf.float32)\n    ave_acc = tf.reduce_mean(accuracy)\n    return ave_acc","b45b7a10":"train_step_signature = [\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None,), dtype=tf.int64)\n]","31d69540":"@tf.function(input_signature=train_step_signature)\ndef train_step(lang, targ):\n\n    enc_padding_mask = create_padding_mask(lang)\n    \n#     look_ahead_mask = create_look_ahead_mask(tf.shape(lang)[1])\n#     dec_target_padding_mask = create_padding_mask(lang)\n#     combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n    \n    with tf.GradientTape() as tape:\n        predictions = tsfr_categorical(lang, True, enc_padding_mask)\n        loss = loss_function(targ, predictions)\n        \n        loss_regularization = []\n        for w in tsfr_categorical.trainable_variables:\n            loss_regularization.append(tf.nn.l2_loss(w))\n        \n        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n        \n        loss = loss + 0.01 * loss_regularization\n        \n    gradients = tape.gradient(loss, tsfr_categorical.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, tsfr_categorical.trainable_variables))\n    \n    accuracy = acc_function(targ, predictions)\n    \n    return loss, accuracy","aca473f1":"@tf.function(input_signature=train_step_signature)\ndef valid_step(lang, targ):\n    enc_padding_mask = create_padding_mask(lang)\n    \n    predictions = tsfr_categorical(lang, False, enc_padding_mask)\n    loss = loss_function(targ, predictions)\n    \n    accuracy = acc_function(targ, predictions)\n    \n    return loss, accuracy","154692e0":"BATCH_SIZE = 2048\nBUFFLE_SIZE = 8000\ndef data_generator(data):\n    dataset = tf.data.Dataset.from_tensor_slices((data['id'], data['keyword'], data['text'], data['target']))\n    dataset = dataset.map(tf_encode)\n    dataset = dataset.cache().shuffle(BUFFLE_SIZE).padded_batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset\n\ntrain_data = train.sample(frac=0.9)\nval_data = train[~train.index.isin(train_data.index)]\n\ntrain_dataset = data_generator(train_data)\nval_dataset = data_generator(val_data)\n\nsample = next(iter(train_dataset))\nprint(sample)","d4e45ae1":"Epochs = 100","f6d8a354":"tensorboard = {'Train_loss':[],'Train_acc':[],'Val_loss':[],'Val_acc':[]}\nfor epoch in range(Epochs):\n    \n    train_loss = []\n    train_accuracy = []\n    \n    val_loss = []\n    val_accuracy = []\n    \n    for _, lang, targ in train_dataset:\n        loss, acc = train_step(lang, targ)\n        train_loss.append(loss)\n        train_accuracy.append(acc)\n    \n    for _, lang, targ in val_dataset:\n        loss, acc = valid_step(lang, targ)\n        val_loss.append(loss)\n        val_accuracy.append(acc)\n    \n    ave_val_acc = np.mean(val_accuracy)\n    \n    if epoch !=0 and max(tensorboard['Val_acc']) <= ave_val_acc:\n        tsfr_categorical.save_weights('\/kaggle\/working\/checkpoint\/best_val')\n    \n    tensorboard['Train_loss'].append(np.mean(train_loss))\n    tensorboard['Train_acc'].append(np.mean(train_accuracy))\n    tensorboard['Val_loss'].append(np.mean(val_loss))\n    tensorboard['Val_acc'].append(ave_val_acc)","017b4151":"for index in range(len(tensorboard['Train_loss'])):\n    print(f\"\\033[0;34mEpoch\\033[0m:{index}, Loss:{tensorboard['Train_loss'][index]}, Accuracy:{tensorboard['Train_acc'][index]}\",\n     f\"Valid_Loss:{tensorboard['Val_loss'][index]}, Valid_Accuracy:{tensorboard['Val_acc'][index]}\")","7592e7ec":"tensorboard = pd.DataFrame(tensorboard, range(len(tensorboard['Train_loss'])))\n\nplt.figure(figsize=(18,6))\nsns.lineplot(data=tensorboard[['Train_loss', 'Val_loss']], palette=\"tab10\", linewidth=2.5)\n\nplt.figure(figsize=(18,6))\nsns.lineplot(data=tensorboard[['Train_acc', 'Val_acc']], palette=\"tab10\", linewidth=2.5)","a3b81683":"venv_target = np.array([0]*len(test['text']))\n\ntest_target = pd.read_csv('\/kaggle\/input\/test-twitter\/perfect_submission.csv')\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test['id'], test['keyword'], test['text'], test_target['target']))\ntest_dataset = test_dataset.map(tf_encode)\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE)","8d4fb5b5":"evaluate = TransformerCategorical(num_layers, d_model, num_heads, dff, pe_input, output_units, rate)\nevaluate.load_weights('\/kaggle\/working\/checkpoint\/best_val')","9004c83e":"results = []\n\nfor id_num, lang, _ in test_dataset:\n    \n    enc_padding_mask = create_padding_mask(lang)\n    \n    predictions = evaluate(lang, False, enc_padding_mask)\n    \n    predictions = tf.math.argmax(predictions, axis=1)\n    predictions = tf.cast(predictions, dtype=tf.int32)\n    predictions = tf.reshape(predictions, (-1))\n    \n    results.extend(zip(id_num.numpy(), predictions.numpy()))","87a14515":"label_equal = 0\nfor index, value in enumerate(results):\n    if value[1] == test_target['target'][index]:\n        label_equal +=1\nprint('result_score:', label_equal\/len(results))","0b8585f9":"submission = pd.DataFrame(results, columns=['id', 'target'])\nprint(submission.head())\nprint(submission.info())\nprint(submission.describe())","168c50c5":"submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","eb9a09de":"print(time.time() - start_time)\ngc.collect()","22b189e1":"# Tokenizer","bb215295":"###### \u8d85\u53c2\u6570","e07e5cf4":"# Loss function and metrics","b7235aac":"# Masking","0526c900":"# Training","079fadba":"# Optimizer","1f246b66":"# \u67e5\u770btext\u957f\u5ea6\u60c5\u51b5","ca32ea2e":"# \u67e5\u770b\u9519\u8bcd\u7684\u5728text\u4e2d\u7684\u5206\u5e03","c1921265":"# Text Encode","f5a64deb":"# \u8d85\u53c2\u6570","68b7ee10":"# Encoder","656c1bc6":"# Point wise feed forward network","eaa6b014":"# test preprocessing","d28ecf07":"### create embeddings_matrix","be781d0f":"# \u4f4d\u7f6e\u7f16\u7801","07160bfa":"# Embedding layer","bc5d72b2":"# Scaled dot product attention","7a3e903c":"# \u8ba1\u7b97 keyword \u548c text \u7684\u76f8\u4f3c\u5ea6(\u7528jaccard)","67392791":"# \u9884\u6d4b","183ff582":"# \u5b9a\u4e49tsfr_categorical","7bc20139":"# Read csv (train and test)","31b64b1f":"# Preprocessing text","17d538cc":"# \u81ea\u5b9a\u4e49Adam\u5b66\u4e60\u7387","b4a1f105":"# Multi_head attention","ff241fad":"### \u67e5\u770b\u9519\u8bcd\u5728train\u548ctest\u7684\u5206\u5e03","e1c56f5e":"# output_layer","3a294e97":"# \u521b\u5efatransformer_categorical","e5a052ba":"# Encoder layer"}}