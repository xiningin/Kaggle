{"cell_type":{"1609ea65":"code","c8304d85":"code","40baca5b":"code","01df5bc2":"code","06a7192a":"code","bcdca9ce":"code","b80fd4d0":"code","4b0c070a":"code","d58af904":"code","db61be0b":"code","0df517c0":"code","3e4d6346":"code","0b2e34a5":"code","c54b01d4":"code","8c207b9f":"code","b68824f5":"code","e2ab52f1":"code","fea6f23d":"code","e8387a6d":"code","4de03f31":"code","7155cedf":"code","1c2c7df4":"markdown","924d90bc":"markdown","b4f7a21b":"markdown","87bc888c":"markdown","6462f9da":"markdown","32aad3fa":"markdown","f9b0f52e":"markdown","7baa189b":"markdown","8cecff1c":"markdown","aaf22c47":"markdown","06a485fe":"markdown"},"source":{"1609ea65":"import os\nimport glob\nimport json\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom tqdm.notebook import tqdm","c8304d85":"all_json_paths = glob.glob(f'\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/*.json', recursive=True)\nlen(all_json_paths)","40baca5b":"#source:https:\/\/www.kaggle.com\/amogh05\/cord-19-eda-question-topic-modeling-starter\n#add more vars as required\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.title = content['metadata']['title']\n            self.abstract = []\n            self.body_text = []\n            self.biblio = []\n            self.biblio_doi = []\n            self.img_tables = []\n            self.back_matter = []\n            \n            \n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            \n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])          \n            self.body_text = '\\n'.join(self.body_text)\n            \n            # bibliography\n            for bib_id, details in content['bib_entries'].items():\n                self.biblio.append(details['title'])\n                self.biblio_doi.append(details['other_ids'])\n            self.biblio = '\\n'.join(self.biblio)\n            #self.biblio_doi = '\\n'.join(self.biblio_doi)\n            \n            #img and table references\n            for ref_id,details in content['ref_entries'].items():\n                self.img_tables.append(details['text'])\n            self.img_tables = '\\n'.join(self.img_tables)\n            \n            #back_matter\n            for entry in content['back_matter']:\n                self.back_matter.append(entry['text'])\n            self.back_matter = '\\n'.join(self.back_matter)\n            \n    def __repr__(self):\n        return f'{self.paper_id}:{self.title}-{self.abstract}... {self.body_text}...{self.biblio}...{self.img_tables}...{self.back_matter}'\n        \n    \ndict_ = {'paper_id': [],'title':[], 'abstract': [], 'body_text': [],'biblio':[],'bidoi':[],'img_tables':[]}\nfor idx, entry in enumerate(all_json_paths):\n    if idx % (len(all_json_paths) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json_paths)}')\n    #print(entry)\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['title'].append(content.title)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    dict_['biblio'].append(content.biblio)   \n    dict_['img_tables'].append(content.img_tables)  \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text','biblio','img_tables'])\ndf_covid.head()\n\n#identify dups\ndf_covid.describe(include='all')\n\ndf_covid.drop_duplicates(['abstract'], inplace=True)\ndf_covid.describe(include='all')","01df5bc2":"df_covid['all_text'] = df_covid['abstract'] + '' + df_covid['body_text'] ","06a7192a":"#This approach does not work well:  defining a list is better\nimport nltk \nfrom nltk.corpus import wordnet \nsynonyms = [] \n\n  \nfor syn in wordnet.synsets('exposure'): \n    for l in syn.lemmas(): \n        synonyms.append(l.name()) \n        if l.antonyms(): \n            antonyms.append(l.antonyms()[0].name())\nprint(set(synonyms))","bcdca9ce":"#defining a list better\nstage_syn_list = ['exposure','vulnerability','vulnerable'] ","b80fd4d0":"disease_stage_list = ['exposure' ,'acquisition' ,'progression', 'development' ,'complications' ,'fatality', 'disability']","4b0c070a":"def filterByStage(text,stage_syn_list):\n    paper_list =[]\n    \n    for idx_num,row in text.iterrows():\n        for stage in stage_syn_list:\n            stage_found = False\n            if stage in row.all_text.split():\n                stage_found = True\n            else:\n                pass \n        if stage_found==True:\n            paper_list.append(row.all_text)\n    return paper_list","d58af904":"stage_dict = {}\n\nstage = disease_stage_list[0]\n\nstage_dict[stage] = filterByStage(df_covid,stage_syn_list)","db61be0b":"#for later ease while searching for relevant papers\nexposure = pd.DataFrame(stage_dict[stage])","0df517c0":"!pip install scispacy scipy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n!pip install tqdm -U\n!pip install spacy-langdetect","3e4d6346":"import spacy\nimport en_core_sci_lg\nnlp = en_core_sci_lg.load()\n\n# We also need to detect language, or else we'll be parsing non-english text \n# as if it were English. \nfrom spacy_langdetect import LanguageDetector\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n\nnlp.max_length=2000000\n\n# New stop words list \ncustomize_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.',\n    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n]\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True","0b2e34a5":"def spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)] \n    # remove numbers (e.g. from references [1], etc.)","c54b01d4":"tf_vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, max_features=800000) \ntf = tf_vectorizer.fit_transform(tqdm(stage_dict[stage]))\n\nprint(tf.shape)\n\nimport joblib\njoblib.dump(tf_vectorizer, '\/kaggle\/working\/tf_vectorizer.csv')\njoblib.dump(tf, '\/kaggle\/working\/tf.csv')","8c207b9f":"lda_tf = LatentDirichletAllocation(n_components=50, random_state=0)\nlda_tf.fit(tf)\njoblib.dump(lda_tf, '\/kaggle\/working\/lda.csv')","b68824f5":"tfidf_feature_names = tf_vectorizer.get_feature_names()\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n    \nprint_top_words(lda_tf, tfidf_feature_names, 25)","e2ab52f1":"topic_dist = pd.DataFrame(lda_tf.transform(tf))\ntopic_dist.to_csv('\/kaggle\/working\/topic_dist.csv', index=False)","fea6f23d":"topic_dist.head()","e8387a6d":"#get most similar paper\nfrom scipy.spatial import distance\ndef get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensen\u2013Shannon divergence in topic space). \n    '''\n    \n    #relevant_time = df.publish_year.between(lower, upper)\n    \n   # if only_covid19:\n   #     is_covid19_article = df.body_text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus') #TODO: move outside\n   #     topic_dist_temp = topic_dist[relevant_time & is_covid19_article]\n   #     \n   # else:\n    #    topic_dist_temp = topic_dist[relevant_time]\n    \n    distances = topic_dist.apply(lambda x: distance.jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    \n    if get_dist:\n        k_distances = distances[distances != 0].nsmallest(n=k)\n        return k_nearest, k_distances\n    else:\n        return k_nearest\n    \n#d = get_k_nearest_docs(topic_dist[1].iloc[0],k=10)","4de03f31":"def relevant_articles(df,tasks, k=3, lower=1950, upper=2020, only_covid19=False):\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_tf = tf_vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda_tf.transform(tasks_tf))\n\n    for index, bullet in enumerate(tasks):\n        print(bullet)\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k, lower, upper, only_covid19)\n        print(list(recommended))\n        recommended = df.iloc[recommended] #stage_dict[stage][','.join(list(recommended))]#\n    return recommended","7155cedf":"task = ['exposure']\nrelevant_articles(exposure,task,k=10) #k is the number of relevant articles","1c2c7df4":"### Create vector representation of data","924d90bc":"# Find synonyms","b4f7a21b":"### LDA","87bc888c":"# Reading in the data","6462f9da":"# LDA : Kaggle Notebook Approach\nhttps:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles","32aad3fa":"### Get Paper Related to Stage of Disease","f9b0f52e":"### Discover Topics","7baa189b":"# Importing Key Libraries","8cecff1c":"# NLP Starts","aaf22c47":"### Create Topic Distance Matrix","06a485fe":"# Filter By Stage"}}