{"cell_type":{"7d586e42":"code","9abebd32":"code","b2a799df":"code","17c5d2e9":"code","40a8e132":"code","f86bdf25":"code","de1e34c9":"code","7b289f47":"code","fa3af990":"code","ed9bcf71":"code","28640386":"code","94d1bfad":"code","2e011703":"code","38390ed6":"code","205a2248":"code","e5df8cc8":"code","0e429418":"code","74cb873e":"code","0452e76a":"code","8f1a9659":"code","07527a21":"code","a1712951":"code","2d6d0252":"code","3bb931fe":"code","54d50bdb":"code","655836bd":"code","bae07811":"code","31a5124f":"code","1c39843a":"code","cc6ec70d":"code","7e27813f":"code","4a70dab2":"code","9b7f7fef":"markdown","d701d71b":"markdown","7a2fc5b9":"markdown","05514530":"markdown","683011cd":"markdown","0540c6da":"markdown","a54c8cdc":"markdown","5687a0fb":"markdown","58104b28":"markdown"},"source":{"7d586e42":"import pandas as pd\nimport datatable as dt\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import uniform\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport gc","9abebd32":"data_train =  dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ndata_test = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()","b2a799df":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","17c5d2e9":"data_train = reduce_memory_usage(data_train, verbose=True)\ndata_test = reduce_memory_usage(data_test, verbose=True)","40a8e132":"for col in range(data_train.shape[1]):\n    if True in data_train.iloc[:,col].values:\n        data_train.iloc[:,col] = data_train.iloc[:,col].replace({True: int(1), False:  int(0)})\n        \nfor col in range(data_test.shape[1]):\n    if True in data_test.iloc[:,col].values:\n        data_test.iloc[:,col] = data_test.iloc[:,col].replace({True: int(1), False:  int(0)}) ","f86bdf25":"TARGET = 'target'\nFEATURES = [col for col in data_train.columns if col not in ['id', TARGET]]\n\nX = data_train[FEATURES]\nY = data_train[TARGET]\nX_test = data_train[FEATURES]\n\ndata_train[\"mean\"] = data_train[FEATURES].mean(axis=1)\ndata_train[\"std\"] = data_train[FEATURES].std(axis=1)\n\ny = data_train['target']\ndata_train = data_train.drop(['target', 'id'], axis = 1)\n\ndata_test[\"mean\"] = data_test[FEATURES].mean(axis=1)\ndata_test[\"std\"] = data_test[FEATURES].std(axis=1)\n\n\ngc.collect()","de1e34c9":"#standartize data\nfloat_columns = ['f'+str(i) for i in range(242)]\nfloat_columns = float_columns + ['mean', 'std']\nfloat_columns.remove('f22')\nfloat_columns.remove('f43')\n\nscaler = StandardScaler()\nfor col in float_columns:\n    data_train[col] = scaler.fit_transform(data_train[col].to_numpy().reshape(-1,1))\n    data_test[col] = scaler.transform(data_test[col].to_numpy().reshape(-1,1))\n\ngc.collect()","7b289f47":"params_cat = {'min_data_in_leaf': 116,\n 'objective': 'CrossEntropy',\n 'min_data_in_leaf': 193,        \n 'depth': 4,\n 'iterations': 284,\n 'learning_rate': 0.5307391048885213,\n 'l2_leaf_reg': 3.766159322596347,\n 'loss_function':'CrossEntropy',\n 'eval_metric' : 'AUC',\n 'task_type': 'GPU',\n 'verbose': 0}","fa3af990":"# Train model using only part of available data\ntrain_set = 800000\nseed = [1,2]\n\nfor i in seed:\n    model_cat = CatBoostClassifier(random_seed = i, **params_cat)\n    globals()[f'prediction_cat{i}'] = np.zeros(data_test.shape[0])\n    print(f'{i} seed')\n    skf = StratifiedKFold(n_splits=5, random_state= i)\n    \n    for train_index, test_index in skf.split(data_train.iloc[:train_set,:], y[:train_set]):\n        x = data_train.iloc[train_index,:]\n        y_train = y[train_index]\n        x_val = data_train.iloc[test_index,:]\n        y_val = y[test_index]\n    \n        #fit model and make final prediction\n        model_cat.fit(x, y_train, eval_set = (x_val, y_val), use_best_model=True)\n        globals()[f'prediction_cat{i}'] += model_cat.predict_proba(data_test.iloc[:,1:] )[:,-1]\/skf.get_n_splits()\n    \n        #define roc_auc for each test fold\n        roc_auc = roc_auc_score(y_val.values, model_cat.predict_proba(x_val)[:,-1])\n        print(f'AUC score = {roc_auc}')","ed9bcf71":"np.savetxt(\"prediction_cat1.csv\", prediction_cat1, delimiter=\",\")\nnp.savetxt(\"prediction_cat2.csv\", prediction_cat2, delimiter=\",\")","28640386":"#This is prediction of this model for unseen data. This way we get the same output as model would get \n#for unseen test data. This output will be used for ensembling\nseed = [1,2]\nfor i in seed:\n    model_cat = CatBoostClassifier(random_seed = i, **params_cat)\n    model_cat.fit(data_train.iloc[:800000,:], y[:800000])\n    globals()[f'cat_pred_tr{i}'] = model_cat.predict_proba(data_train.iloc[800000:,:])[:,-1]","94d1bfad":"np.savetxt(\"cat_pred_tr1.csv\", cat_pred_tr1, delimiter=\",\")\nnp.savetxt(\"cat_pred_tr2.csv\", cat_pred_tr2, delimiter=\",\")","2e011703":"lgb_params = {'objective': 'binary',\n               'boosting_type': 'gbdt',\n               'num_leaves': 62, \n               'max_depth': 512,\n               'learning_rate': 0.05,\n               'n_estimators': 5000,\n               'reg_alpha': 29.5,\n               'device' : 'gpu',\n               'reg_lambda': 94.1,\n               'n_jobs': 4,\n               'subsample': 0.5, \n               'subsample_freq': 2, \n               'colsample_bytree': 0.41, \n               'min_child_samples': 117,\n               'min_child_weight': 426}","38390ed6":"# Train model using only part of available data\ntrain_set = 800000\nseed = [1,2]\nfor i in seed:\n    model_lgbm = lgb.LGBMClassifier(random_seed = i, **lgb_params)\n    globals()[f'prediction_lgb{i}'] = np.zeros(data_test.shape[0])\n    skf = StratifiedKFold(n_splits=5, random_state= i)\n    \n\n    for train_index, test_index in skf.split(data_train.iloc[:train_set, :], y[:train_set]):\n        X = data_train.iloc[train_index, :]\n        y_train = y[train_index]\n    \n        x_valid = data_train.iloc[test_index, :]\n        y_valid = y[test_index]\n    \n        model_lgbm.fit(X, y_train, eval_set = [(x_valid, y_valid)], \n                  eval_metric='auc',\n                  early_stopping_rounds = 200,\n                  verbose = 1000\n                  )\n    \n        globals()[f'prediction_lgb{i}']  += model_lgbm.predict_proba(data_test.iloc[:,1:])[:,-1]\/skf.get_n_splits()\n        auc = roc_auc_score(y_valid, model_lgbm.predict_proba(x_valid)[:,-1])\n        \n        gc.collect()\n        print(f'auc = {auc}')","205a2248":"np.savetxt(\"prediction_lgb1.csv\", prediction_lgb1, delimiter=\",\")\nnp.savetxt(\"prediction_lgb2.csv\", prediction_lgb2, delimiter=\",\")","e5df8cc8":"#This is prediction of this model for unseen data. This way we get the same output as model would get \n#for unseen test data. This output will be used for ensembling\nseed = [1,2]\nfor i in seed:\n    model_lgbm = lgb.LGBMClassifier(random_seed = i, **lgb_params)\n    model_lgbm.fit(data_train.iloc[:700000, :], y[:700000],\n               eval_set = [(data_train.iloc[700000:800000, :], y[700000:800000])],\n               early_stopping_rounds = 100,\n                  verbose = 500)\n    globals()[f'lgbm_pred_tr{i}'] = model_lgbm.predict_proba(data_train.iloc[800000:, :])[:,-1]","0e429418":"np.savetxt(\"lgbm_pred_tr1.csv\", lgbm_pred_tr1, delimiter=\",\")\nnp.savetxt(\"lgbm_pred_tr2.csv\", lgbm_pred_tr2, delimiter=\",\")","74cb873e":"xgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'learning_rate': 0.007279718158350149,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'tree_method': 'gpu_hist',\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor',\n}","0452e76a":"# Train model using only part of available data\ntrain_set = 800000\nseed = [1,2]\nfor i in seed:\n    model_xgb = xgb.XGBClassifier(random_state = i, **xgb_params)\n    globals()[f'prediction_xgb{i}'] = np.zeros(data_test.shape[0])\n    skf = StratifiedKFold(n_splits=5, random_state= i)\n    prediction_xgb = np.zeros(data_test.shape[0])\n\n    for train_index, test_index in skf.split(data_train.iloc[:train_set, :], y[:train_set]):\n        X = data_train.iloc[train_index, :]\n        y_train = y[train_index]\n    \n        x_valid = data_train.iloc[test_index, :]\n        y_valid = y[test_index]\n    \n        model_xgb.fit(X, y_train)\n    \n        globals()[f'prediction_xgb{i}'] += model_xgb.predict_proba(data_test.iloc[:,1:])[:,-1]\/skf.get_n_splits()\n        auc = roc_auc_score(y_valid, model_xgb.predict_proba(x_valid)[:,-1])\n    \n        gc.collect()\n        print(f'auc = {auc}')\n","8f1a9659":"np.savetxt(\"prediction_xgb1.csv\", prediction_xgb1, delimiter=\",\")\nnp.savetxt(\"prediction_xgb2.csv\", prediction_xgb2, delimiter=\",\")","07527a21":"#This is prediction of this model for unseen data. This way we get the same output as model would get \n#for unseen test data. This output will be used for ensembling\nseed = [1,2]\nfor i in seed:\n    model_xgb = xgb.XGBClassifier(random_state = i, **xgb_params)\n    model_xgb.fit(data_train.iloc[:800000, :], y[:800000])\n    globals()[f'xgb_pred_tr{i}'] = model_xgb.predict_proba(data_train.iloc[800000:, :])[:,-1]","a1712951":"np.savetxt(\"xgb_pred_tr1.csv\", xgb_pred_tr1, delimiter=\",\")\nnp.savetxt(\"xgb_pred_tr2.csv\", xgb_pred_tr2, delimiter=\",\")","2d6d0252":"# Train model using only part of available data\ntrain_set = 800000\nseed = [1,2]\nfor i in seed:\n    model_log = LogisticRegression(random_state=i, solver='liblinear')\n    globals()[f'prediction_log{i}'] = np.zeros(data_test.shape[0])\n    skf = StratifiedKFold(n_splits=3, random_state= i)\n\n    for train_index, test_index in skf.split(data_train.iloc[:train_set,:], y[:train_set]):\n        x = data_train.iloc[train_index, :]\n        y_train = y[train_index]\n    \n        x_valid = data_train.iloc[test_index, :]\n        y_valid = y[test_index]\n    \n        model_log.fit(x.values, y_train.values)\n    \n        globals()[f'prediction_log{i}'] += model_log.predict_proba(data_test.iloc[:,1:])[:,-1]\/skf.get_n_splits()\n        auc = roc_auc_score(y_valid.values, model_log.predict_proba(x_valid.values)[:,-1])\n    \n        print(f'auc = {auc}')","3bb931fe":"np.savetxt(\"prediction_log1.csv\", prediction_log1, delimiter=\",\")\nnp.savetxt(\"prediction_log2.csv\", prediction_log2, delimiter=\",\")","54d50bdb":"#This is prediction of this model for unseen data. This way we get the same output as model would get \n#for unseen test data. This output will be used for ensembling\nmodel_log = LogisticRegression(random_state=1,solver='liblinear')\nmodel_log.fit(data_train.iloc[:800000, :], y[:800000])\nlog_pred_tr1 = model_log.predict_proba(data_train.iloc[800000:, :])[:,-1]","655836bd":"np.savetxt(\"log_pred_tr1.csv\", log_pred_tr1, delimiter=\",\")","bae07811":"#Load previosly made predictions for test dataset\nprediction_xgb1 = genfromtxt('..\/input\/prediction-xgb2\/prediction_xgb2.csv', delimiter=',')\nprediction_xgb2 = genfromtxt('..\/input\/prediction-xgb1\/prediction_xgb1.csv', delimiter=',')\nprediction_cat1 = genfromtxt('..\/input\/prediction-cat1\/prediction_cat1.csv', delimiter=',')\nprediction_cat2 = genfromtxt('..\/input\/prediction-cat2\/prediction_cat2.csv', delimiter=',')\nprediction_log1 = genfromtxt('..\/input\/prediction-log1\/prediction_log1.csv', delimiter=',')\nprediction_lgb1 = genfromtxt('..\/input\/prediction-lgb1\/prediction_lgb1.csv', delimiter=',')\nprediction_lgb2= genfromtxt('..\/input\/prediction-lgb2\/prediction_lgb2.csv', delimiter=',')","31a5124f":"prediction_frame = pd.DataFrame({'cat1' : prediction_cat1,\n                                 'cat2' : prediction_cat2,\n                                'lgbm1' : prediction_lgb1,\n                                'lgbm2' : prediction_lgb2,\n                               'xgb1' : prediction_xgb1,\n                               'xgb2' : prediction_xgb2,\n                               'log1': prediction_log1})\ntrain_pred.head()","1c39843a":"# load previosly made predictions for unseen  train dataset\nxgb_pred_tr1 = genfromtxt('..\/input\/xgb-pred-tr1\/xgb_pred_tr1.csv', delimiter=',')\nxgb_pred_tr2 = genfromtxt('..\/input\/xgb-pred-tr2\/xgb_pred_tr2.csv', delimiter=',')\ncat_pred_tr1 = genfromtxt('..\/input\/cat-pred-tr1\/cat_pred_tr1.csv', delimiter=',')\ncat_pred_tr2 = genfromtxt('..\/input\/cat-pred-tr2\/cat_pred_tr2.csv', delimiter=',')\nlog_pred_tr1 = genfromtxt('..\/input\/log-pred-tr1\/log_pred_tr1.csv', delimiter=',')\nlgbm_pred_tr1 = genfromtxt('..\/input\/lgbm-predtr1\/lgbm_pred_tr1 (2).csv', delimiter=',')\nlgbm_pred_tr2 = genfromtxt('..\/input\/lgbm-predtr2\/lgbm_pred_tr2 (1).csv', delimiter=',')","cc6ec70d":"train_pred = pd.DataFrame({'cat1' : cat_pred_tr1,\n                                 'cat2' : cat_pred_tr2,\n                                'lgbm1' : lgbm_pred_tr1,\n                                'lgbm2' : lgbm_pred_tr2,\n                               'xgb1' : xgb_pred_tr1,\n                               'xgb2' : xgb_pred_tr2,\n                               'log1': log_pred_tr1})\ntrain_pred.head()","7e27813f":"#train linear model using data frame with predictions for unseen train dataset\nlin_model = LinearRegression()\n\nskf = StratifiedKFold(n_splits=3)\nprediction_test = np.zeros(prediction_frame.shape[0])\n\nfor train_index, test_index in skf.split(train_pred, y.values[800000:]):\n    X = train_pred.iloc[train_index, :]\n    y_train = y.values[train_index]\n    \n    x_valid = train_pred.iloc[test_index, :]\n    y_valid = y.values[test_index]\n    \n    lin_model.fit(X, y_train)\n    \n    valid_pred = lin_model.predict(x_valid) \n    auc = roc_auc_score(y_valid, valid_pred)\n    print(auc)\n    \n    prediction_test += lin_model.predict(prediction_frame)\/skf.get_n_splits()","4a70dab2":"sub = pd.DataFrame({'id': data_test.id,\n                          'target': prediction_test})\nsub.to_csv(\"sub_forest.csv\", index=False, header = True)","9b7f7fef":"### ENSEMBLING\n<a id=\"section-six\"><\/a>","d701d71b":"#### Now we have 7 predictions of 4 models. Each prediction was recieved using unseen dataset. It means that these prediction will look like predictions for real test data. We will gather these predictions in a Data Frame and use it to train a plain Linear Regression. Then we will get the same set of 7 predictions but this time on test data. After that we just apply trained Liner regression to get final result.","7a2fc5b9":"#### Further we will make a few models one by one and then unite them in ensemble. ","05514530":"### Reduce memory usage\n<a id=\"section-one\"><\/a>","683011cd":"### XGBClassifier\n<a id=\"section-four\"><\/a>","0540c6da":"#### 1. [Reduce memory usage](#section-one)\n#### 2. [CatBoostClassifier](#section-two)\n#### 3. [LGBM](#section-three)\n#### 4. [XGBClassifier](#section-four)\n#### 5. [LOGISTIC REGRESSION](#section-five)\n#### 6. [ENSEMBLING](#section-six)","a54c8cdc":"## CatBoosting\n<a id=\"section-two\"><\/a>\n","5687a0fb":"### LGBM MODEL\n<a id=\"section-three\"><\/a>","58104b28":"### LOGISTIC REGRESSION\n<a id=\"section-five\"><\/a>"}}