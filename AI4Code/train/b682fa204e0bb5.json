{"cell_type":{"029e32a3":"code","4af165ce":"code","f588debb":"code","52766c4f":"code","7bd01835":"code","57f6f267":"code","0e27d85f":"code","91a82d5f":"code","7458be86":"code","2016cb24":"code","2f3e5b3d":"code","5cbbfeb1":"code","68bc05f2":"code","423107d8":"code","b7d62d5f":"code","b0149e7c":"code","55c19e5a":"code","7e1113e6":"code","f2100b16":"code","34661927":"code","c798a79b":"code","3635246b":"code","42799257":"code","60fabf36":"code","c6be3f8c":"code","bdb3853d":"code","4dafe6f5":"code","ee2dbdf1":"code","881cfd81":"code","1509f0fb":"code","c548c317":"code","d51e5131":"code","36e5457d":"code","84f0975c":"code","7f734ea8":"code","bd3269f7":"code","8a4ce22d":"markdown","080b5e5c":"markdown","8f9bd8e0":"markdown","d7624244":"markdown","0c830235":"markdown","0e12712a":"markdown","cb7d29f6":"markdown","9464d655":"markdown","f758e956":"markdown","fec47394":"markdown","26dff195":"markdown","088ebf63":"markdown","2f8b1579":"markdown","66d88f60":"markdown","178dadde":"markdown","3a6bac62":"markdown","13f7d38d":"markdown","b9db5dd6":"markdown","473dbb92":"markdown","5e2071fa":"markdown","df21c51c":"markdown","be18fd2e":"markdown","e9d818a8":"markdown","1a37c41f":"markdown"},"source":{"029e32a3":"#!pip install scispacy\n#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.3.0\/en_ner_bionlp13cg_md-0.3.0.tar.gz\n#!pip install langdetect","4af165ce":"import collections\nfrom urllib.parse import urlparse\n\nimport re\n\nimport os\nimport json\nfrom os import path\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n#from langdetect import detect, lang_detect_exception\n\n#import scispacy\nimport spacy\n\n#import en_ner_bionlp13cg_md","f588debb":"articles_metadata = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\narticles_metadata.head()","52766c4f":"len(articles_metadata)","7bd01835":"fig = plt.figure(figsize=(10,40))\ngs = gridspec.GridSpec(nrows=4, ncols=1)\n\nax1 = fig.add_subplot(gs[0,0])\nax1.set_title('Sources')\n\nsource_counts = articles_metadata.groupby('source_x').count()\ncord_uid_count = source_counts.cord_uid.sort_values(ascending=True)\nlabels = cord_uid_count.index\nvalues = cord_uid_count.values\n\nax1.barh(np.arange(len(labels)), values, tick_label=labels)\nax1.set_xlabel('Number of articles')\n\n\nax2 = fig.add_subplot(gs[1, 0])\nax2.set_title('Number of articles per url domain (NaN exclued)')\n\ndomains = map(lambda x: urlparse(x).netloc, articles_metadata[~pd.isna(articles_metadata.url)].url)\ncounter = collections.Counter(domains)\nmost_common_domains = counter.most_common(20)\nlabels = list(map(lambda x: x[0], most_common_domains))[-1::-1]\nvalues = list(map(lambda x: x[1], most_common_domains))[-1::-1]\n\nax2.barh(np.arange(len(labels)), values, tick_label=labels)\n\nplt.show()","57f6f267":"def extract_year(date: str) -> int:\n    return int(date[:4]) if (date and type(date) is str) else -1\n\ndef extract_month(date: str) -> int:\n    if type(date) is str:\n        tokens = date.split('-')\n        if len(tokens) >= 2:\n            return int(tokens[1])\n    \n    return -1\n\ndef has_text(title) -> bool:\n    return (title is not None) and (type(title) == str) and len(title)>0\n\ndef detect_language(abstract) -> bool:\n    try:\n        return detect(abstract) == 'en'\n    except lang_detect_exception.LangDetectException:\n        return False\n    \narticles_metadata['year_publish'] = list(map(lambda x: extract_year(x), articles_metadata.publish_time))\narticles_metadata['month_publish'] = list(map(lambda x: extract_month(x), articles_metadata.publish_time))\narticles_metadata['has_title'] = list(map(lambda x: has_text(x), articles_metadata.title))\narticles_metadata['has_abstract'] = list(map(lambda x: has_text(x), articles_metadata.abstract))\n\n\narticles_metadata.head()","0e27d85f":"articles_metadata.groupby(['has_title', 'has_abstract']).count()['cord_uid']","91a82d5f":"fig = plt.figure(figsize=(20,28))\ngs = gridspec.GridSpec(nrows=2, ncols=1)\n\narticles_metadata['year_publish'] = list(map(lambda x: extract_year(x), articles_metadata.publish_time))\n\nper_year_counter = articles_metadata.groupby('year_publish').count().cord_uid\nax1 = fig.add_subplot(gs[0,0])\n\nplt.xticks(rotation=90)\nax1.set_title('Number of articles per year')\nax1.bar(np.arange(len(per_year_counter)), per_year_counter.values, tick_label=per_year_counter.index, log=True)\nax1.set_xlabel('Number of articles')\n\n\nax2 = fig.add_subplot(gs[1, 0])\nax2.set_title('Number of articles per month in 2020')\n\nmonths_counter = collections.Counter(list(map(lambda x: extract_month(x), articles_metadata[articles_metadata.year_publish == 2020].publish_time)))\nmonths_data = sorted(months_counter.items())[1:]\nmonths_values = [x[1] for x in months_data]\nmonths_labels = [x[0] for x in months_data]\nax2.plot(months_labels, months_values)\nax2.grid(True)\n\nplt.show()","7458be86":"sorted(months_counter.items())","2016cb24":"articles_2020 = articles_metadata[articles_metadata.year_publish >= 2020].copy()\narticles_2020.head()","2f3e5b3d":"len(articles_2020)","5cbbfeb1":"articles_2020 = articles_2020.drop_duplicates(subset=['title'])\nlen(articles_2020)","68bc05f2":"articles_2020_tagged = None\nPREPROCESSED_PATH = '..\/input\/preprocessed-tagged-articles-for-cord19\/preprocessed-articles-v1.0.csv'\nif path.exists(PREPROCESSED_PATH):\n    preprocessed_data = pd.read_csv(PREPROCESSED_PATH)\n    articles_2020_tagged = pd.merge(articles_2020, preprocessed_data, how='right', right_on='cord_uid', left_on='cord_uid')\nelse:\n    language_labels = []\n    for t in tqdm(articles_2020.title):\n        try:\n            language_labels.append(detect(t))\n        except lang_detect_exception.LangDetectException:\n            language_labels.append('unknown')\n            print(f\"Error with title {t}\")\n        except TypeError:\n            language_labels.append('unknown')\n            print(f\"Type error with title {t}\")","423107d8":"preprocessed_data = pd.read_csv(PREPROCESSED_PATH, compression=None)\npreprocessed_data.head()","b7d62d5f":"articles_2020_tagged.head()","b0149e7c":"len(articles_2020_tagged)","55c19e5a":"articles_2020_tagged['genes'] = articles_2020_tagged['genes'].apply(eval)\narticles_2020_tagged['organs'] = articles_2020_tagged['organs'].apply(eval)\narticles_2020_tagged['chems'] = articles_2020_tagged['chems'].apply(eval)","7e1113e6":"gene_counter = collections.Counter()\nfor g in articles_2020_tagged['genes']:\n    gene_counter.update(g)","f2100b16":"#false_postive_genes = ['COVID-19','Covid-19','UK','PPE','COVID-19patients','2019-nCoV',\n#                       'MERS-CoV','stay-at-home','USA','e.g.', '', 'Iran', 'Food']\nfalse_postive_genes = ['covid-19','uk','ppe','covid-19patients','2019-ncov','mers-cov','stay-at-home','usa','e.g.', '', 'iran', 'food', 'covid-19 patients']","34661927":"for fpg in false_postive_genes:\n    del gene_counter[fpg]","c798a79b":"gene_counter.most_common(20)","3635246b":"organ_counter = collections.Counter()\nfor o in articles_2020_tagged['organs']:\n    organ_counter.update(o)\n\norgan_counter.most_common(20)","42799257":"cooccurence = collections.Counter()\nfor index, article in articles_2020_tagged.iterrows():\n    #print(article)\n    gene_list = article.genes\n    organ_list = article.organs\n    for g in gene_list:\n        for o in organ_list:\n            if g not in false_postive_genes:\n                cooccurence[f\"{g}#{o}\"] += 1","60fabf36":"cooccurence.most_common(100)","c6be3f8c":"def extract_most_common(counter, num_of_items=20):\n    items = counter.most_common(num_of_items)\n    return [i[0] for i in items]","bdb3853d":"gene_axis = extract_most_common(gene_counter)\norgan_axis = extract_most_common(organ_counter)\n#gene_axis","4dafe6f5":"heatmap = []\nfor g in gene_axis:\n    row = []\n    for o in organ_axis:\n        c = cooccurence[f\"{g}#{o}\"]\n        row.append(c if c>0 else 1)\n    heatmap.append(row)","ee2dbdf1":"fig = plt.figure(figsize=(10,15))\n\nax = fig.add_subplot(111)\n\nax.imshow(heatmap)\n\nax.set_xticks(np.arange(len(gene_axis)))\nax.set_yticks(np.arange(len(organ_axis)))\n\nax.set_xticklabels(gene_axis)\nax.set_yticklabels(organ_axis)\n\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\")\n\nplt.show()","881cfd81":"from gensim.models import LdaModel\nfrom gensim.utils import simple_preprocess\nfrom gensim.test.utils import common_texts\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.corpora.dictionary import Dictionary","1509f0fb":"class TopicExtractor:\n    \n    _genes=[]\n    _organs=[]\n    \n    _data_subset=None\n    \n    def __init__(self, genes=[], organs=[]):\n        self._genes = genes\n        self._organs = organs\n        \n        self._data_subset = articles_2020_tagged\n        for o in self._organs:\n            indices = self._data_subset.organs.apply(lambda organ_list: o in organ_list)\n            self._data_subset = self._data_subset[indices]\n\n        for g in self._genes:\n            indices = self._data_subset.genes.apply(lambda  gene_list: g in gene_list)\n            self._data_subset = self._data_subset[indices]\n        \n    def get_topics(self, num_of_topics=20):\n        article_chapters = []\n\n        for doc in self._data_subset.pdf_json_files:\n            if doc and type(doc) is str:\n                path = doc.split('; ')[0]\n                with open(f\".\/..\/input\/CORD-19-research-challenge\/{path}\") as article:\n                    raw_data = article.read()\n                    obj = json.loads(raw_data)\n                    body_text = obj['body_text']\n\n                    texts = [x['text'] for x in body_text if 'text' in x]\n                    # print(texts)\n                    article_chapters.extend(texts)\n        print(len(article_chapters))          \n        corpus_data = [simple_preprocess(remove_stopwords(text)) for text in article_chapters]\n                    \n        common_dictionary = Dictionary(corpus_data)\n        corpus = [common_dictionary.doc2bow(text) for text in corpus_data]\n\n        self.lda_model = LdaModel(corpus, num_topics=num_of_topics, id2word=common_dictionary)\n        self.lda_model.print_topics()","c548c317":"lungs_lda = TopicExtractor(organs=['lung'])\nlungs_lda.get_topics()","d51e5131":"lungs_lda.lda_model.print_topics()","36e5457d":"heart_igg_lda = TopicExtractor(genes=['ace2'], organs=['heart'])\nheart_igg_lda.get_topics(num_of_topics=8)","84f0975c":"heart_igg_lda.lda_model.print_topics()","7f734ea8":"liver_lda = TopicExtractor(organs=['liver'])\nliver_lda.get_topics(num_of_topics=15)","bd3269f7":"liver_lda.lda_model.print_topics()","8a4ce22d":"This notebook will try to find most common symptoms, organs and genes that appear in this dataset. Also this notebook will try find topics that are related to some organ, gene etc.\n\n**NOTE**\n\nAs dataset is updated I will try to update this notebook accordingly","080b5e5c":"### Basic dataset stats\n\nIn this section we will try to find out number of articles per time and their source (pdf or pmc)","8f9bd8e0":"- imports of other libraries","d7624244":"### Topics for articles about liver","0c830235":"You can use the following class for playing and finding more data about other combination about organs and genes.\n\n__More improvements to notebook will follow :-)__","0e12712a":"# Correlation between genes and symptoms","cb7d29f6":"### Lungs articles topics","9464d655":"In this section we will try to extract topics for combinations of genes and organs. In the cell bellow there will be defined class for such purpose","f758e956":"Number of articles in dataset","fec47394":"Following imports are imports of `scispacy`, `langdetect` and `en_ner_bionlp13cg_md`. They are used for language detection and NER for genes, organs and chemicals. Imports of those are optional since I have prepared dataset where everything is precomputed on much faster machine","26dff195":"### Analysis of genes, chemical compunds and organs","088ebf63":"Removing duplicates further reduced dataset to 297871 articles","2f8b1579":"### Trying to filter english papers and extract genes, organs and chemical compounds","66d88f60":"- in the following cell we can see how many articles have titles and abstracts","178dadde":"**Plotting heatmap of coocurences**","3a6bac62":"### Co-occurences of organs and genes in dataset corpus","13f7d38d":"Remove duplicate titles","b9db5dd6":"## Extracting topics for genes and organs","473dbb92":"From articles published in 2020, 149103 of them don't have month or day of publishing.","5e2071fa":"## Analysis of genes and chemical compounds","df21c51c":"### Topics for articles about heart and Angiotensin-converting enzyme 2","be18fd2e":"Time condition limited analysis to 387554 articles","e9d818a8":"- first, let's import gensim LDA","1a37c41f":"This section will examine co-occurences of genes, checmical compounds and organs that appear in abstracts of articles. Since this dataset is diverse we will limit analysis to the following subset:\n* only articles in English\n* only articles that have date of creation 2020 or later"}}