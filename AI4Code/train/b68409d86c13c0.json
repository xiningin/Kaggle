{"cell_type":{"d06ea278":"code","a6741dc2":"code","5427211b":"code","99d4cb5e":"code","35d9b933":"code","877bde29":"code","b42097c8":"code","fe238312":"code","cf3fb0a0":"code","fbc2f061":"code","e812bfb3":"code","d6e79a91":"code","a72f1158":"code","2de3d229":"code","4ef4dbd6":"code","4214d1ac":"code","bd50bbb6":"code","cbf214ef":"code","23413e7b":"code","ea0b6bc7":"code","f5b96945":"code","342b6f01":"code","3b79efcf":"code","602f608e":"code","09196507":"code","0a7a81e7":"code","40b7b1bc":"code","c8c04c5a":"code","80e62975":"code","305e3b94":"code","b1302037":"markdown","41352e93":"markdown","91546cf4":"markdown","195b195a":"markdown","a710ac4a":"markdown","0c1603f0":"markdown","517dea64":"markdown","6778de26":"markdown","5f4c2257":"markdown","f99dd800":"markdown","5e931fae":"markdown","68166413":"markdown","dd170126":"markdown"},"source":{"d06ea278":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport tensorflow as tf\nimport gresearch_crypto\nimport gc","a6741dc2":"# Loading traning data strictly\ndef read_csv_strict(file_name='..\/input\/g-research-crypto-forecasting\/train.csv'):\n    df = pd.read_csv(file_name)\n    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n    df = df[df['datetime'] < '2021-06-13 00:00:00']\n    return df\n\ndata_df = read_csv_strict()\ndata_folder = '..\/input\/g-research-crypto-forecasting\/'\nasset_details_df = pd.read_csv(data_folder + 'asset_details.csv').set_index('Asset_ID')","5427211b":"def train_test_split(df, datetime):\n    return df[df['datetime'] < datetime], df[df['datetime'] >= datetime]\n\nDEFAULT_FEATURES = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']\ndef get_Xy(df, asset_id, features=DEFAULT_FEATURES, na_treatment='drop'):\n    \"\"\"\n    Make sure don't pass the original dataframe into this function, otherwise it would be altered.\n    Available N\/A treatments: drop, zero. Default to drop.\n    \"\"\"\n    df = df[df[\"Asset_ID\"] == asset_id]\n    df = df.replace([np.inf, -np.inf], np.nan)\n    if na_treatment == 'zero':\n        df = df.replace(np.nan, 0.)\n    else:\n        df = df.dropna(how='any')\n    \n    X = df[features]\n    y = df['Target']\n    return X, y\n\ndef get_corr(pred, y):\n    return np.correlate(pred, y)\n\ndef get_score(preds, ys):\n    corrs, weights = [], []\n    for asset in preds.keys():\n        corrs.append(np.correlate(preds[asset], ys[asset]))\n        weights.append(asset_details_df.loc[asset, 'Weight'])\n    corrs = np.array(corrs)\n    weights = np.array(weights)\n    return (corrs * weights).sum() \/ weights.sum()","99d4cb5e":"class CryptoModel:\n    def train(self, df):\n        pass\n    \n    def predict(self, df_test, df_pred):\n        pass","35d9b933":"# Baseline fully-connected NN on local data only\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom tqdm import tqdm\n\nclass FCNetwork(nn.Module):\n    def __init__(self, sizes, activation=nn.ReLU, activation_out=False):\n        super().__init__()\n        n_layers = len(sizes) - 1\n        self.layers = []\n        for i in range(n_layers):\n            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))\n            if i != n_layers - 1 or activation_out:\n                self.layers.append(activation())\n        if len(self.layers) == 0:\n            self.layers.append(nn.Identity())\n        self.layers = nn.ModuleList(self.layers)\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nclass FCModel(nn.Module, CryptoModel):\n    def __init__(self, sizes_public, sizes_private, lr=1e-4, activation=nn.ReLU, dtype=torch.float, device='cpu'):\n        super().__init__()\n        n_assets = 14\n        self.device = device\n        self.dtype = dtype\n        self.public_network = FCNetwork(sizes_public, activation=activation, activation_out=True).to(device)\n        self.n_public_layers = len(sizes_public) - 1\n        self.private_networks = nn.ModuleList([FCNetwork(sizes_private, activation=activation) for _ in range(n_assets)]).to(device)\n        self.n_private_layers = len(sizes_private) - 1\n        params = list(self.public_network.parameters()) + list(self.private_networks.parameters())\n        self.optimizer = optim.Adam(params=params, lr=lr)\n        self.features = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']\n        print('Initialized.')\n  \n    def forward(self, x, asset):\n        h = self.public_network(x)\n        pn = self.private_networks[asset]\n        return pn(h).squeeze()\n  \n    def train(self, df, loss_fn=nn.MSELoss(), epoch=10, batch_size=1024):\n        df = df.copy()\n        assets = df['Asset_ID'].drop_duplicates().to_list()\n        \n        dataloaders = {}\n        print('Begin moving.')\n        for asset in assets:\n            X, y = get_Xy(df, asset)\n            X = torch.tensor(X.to_numpy(), device=self.device, dtype=self.dtype)\n            y = torch.tensor(y.to_numpy(), device=self.device, dtype=self.dtype)\n            dataset = TensorDataset(X, y)\n            dataloaders[asset] = DataLoader(dataset, batch_size=batch_size)\n        print('End moving.')\n            \n        for i in range(epoch):\n            losses = []\n            \n            for asset in dataloaders.keys():\n                dataloader = dataloaders[asset]\n                loop = tqdm(enumerate(dataloader), total=len(dataloader))\n                for index, (x, target) in loop:\n                    pred = self.forward(x, asset)\n                    loss = loss_fn(pred, target)\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    self.optimizer.step()\n                    losses.append(loss.item())\n                    loop.set_description('Epoch [{}\/{}]'.format(i+1, epoch))\n                    loop.set_postfix(loss = loss.item())\n    \n    def test(self, X, y):\n        X = torch.tensor(X, device=self.device, dtype=self.dtype)\n        pred = self.forward(X).numpy()\n        return np.corrcoef(X, y)[0, 1]\n    \n    def predict(self, df_test, df_pred):\n        for j , row in df_test.iterrows():\n            idx = row['row_id']\n            x_test = row[self.features]\n            x_test = torch.tensor(x_test, device=self.device, dtype=self.dtype)\n            y_pred = self.forward(x_test, row['Asset_ID']).item()\n            df_pred.loc[df_pred['row_id'] == idx, 'Target'] = y_pred\n        return df_pred\n\n    def finalize(self):\n        self.to('cpu')\n        self.device = 'cpu'","877bde29":"\"\"\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfc_model = FCModel([7, 16], [16, 1], device=device)\nfc_model.train(data_df[data_df['Asset_ID'] == 1], epoch=3)\nfc_model.finalize()\n\"\"\"","b42097c8":"from lightgbm import LGBMRegressor\nfrom tqdm import tqdm\nclass BasicLGBM(CryptoModel):\n    def __init__(self):\n        self.models = [None] * 14\n        self.features = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Lower_Shadow', 'Upper_Shadow']\n        \n    def train(self, df):\n        df = df.copy()\n        df = self._extend(df)\n        assets = df['Asset_ID'].drop_duplicates().to_list()\n        \n        for asset in assets:\n            X, y = get_Xy(df, asset, features=self.features)\n            model = LGBMRegressor(n_estimators=10)\n            model.fit(X, y)\n            self.models[asset] = model\n            \n    def predict(self, df_test, df_pred):\n        df_test = df_test.copy()\n        df_test = self._extend(df_test)\n        total = len(df_test.index)\n        \n        row_generator = df_test.iterrows()\n        row_generator = tqdm(generator, total=total) # Enable progress bar, comment this line to disable it.\n        for j , row in row_generator:\n            asset = row['Asset_ID']\n            x_test = row[self.features]\n            model = self.models[int(asset)]\n            y_pred = model.predict([x_test])[0]\n            mask = df_pred['row_id'] == j\n            df_pred.loc[mask, 'Target'] = y_pred\n        return df_pred\n            \n    def _extend(self, df):\n        df = df.copy()\n        df['Upper_Shadow'] = self._upper_shadow(df)\n        df['Lower_Shadow'] = self._lower_shadow(df)\n        return df\n    \n    def _upper_shadow(self, df):\n        return df['High'] - np.maximum(df['Close'], df['Open'])\n    \n    def _lower_shadow(self, df):\n        return np.minimum(df['Close'], df['Open']) - df['Low']","fe238312":"from lightgbm import LGBMRegressor\nfrom tqdm import tqdm\nclass LGBMv1(CryptoModel):\n    def __init__(self):\n        self.models = [None] * 14\n        self.features = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Lower_Shadow', 'Upper_Shadow',\n                         'Avg_Vol', 'Rel_Upper', 'Upper_VWAP', 'Upper_Vol', 'R_quantile', 'Rel_Upper_quantile']\n        \n    def train(self, df):\n        df = df.copy()\n        df = self._extend(df)\n        assets = df['Asset_ID'].drop_duplicates().to_list()\n        \n        for asset in assets:\n            X, y = get_Xy(df, asset, features=self.features)\n            model = LGBMRegressor(n_estimators=10)\n            model.fit(X, y)\n            self.models[asset] = model\n            \n    def predict(self, df_test, df_pred):\n        df_test = df_test.copy()\n        df_test = self._extend(df_test)\n        total = len(df_test.index)\n        \n        row_generator = df_test.iterrows()\n        row_generator = tqdm(row_generator, total=total) # Enable progress bar, comment this line to disable it.\n        for j , row in row_generator:\n            asset = row['Asset_ID']\n            x_test = row[self.features]\n            model = self.models[int(asset)]\n            y_pred = model.predict([x_test])[0]\n            mask = df_pred['row_id'] == j\n            df_pred.loc[mask, 'Target'] = y_pred\n        return df_pred\n            \n    def _extend(self, df):\n        df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n        df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n        #df['Liquidity'] = ((2 * (df['High'] - df['Low']) - np.absolute(df['Open'] - df['Close']))\/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Avg_Vol'] = (df['Volume'] \/ df['Count']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Rel_Upper'] = ((df['High'] - df['VWAP']) \/ (df['High'] - df['Low'])).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Upper_VWAP'] = ((df['High'] - df['VWAP']) \/ df['VWAP']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Upper_Vol'] = ((df['High'] - df['VWAP']) \/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        return df","cf3fb0a0":"\nmodel_lgbm = LGBMv1()\nmodel_lgbm.train(data_df)\n","fbc2f061":"benchmark_debug = Benchmark()\nbenchmark_debug.benchmark(model_lgbm)","e812bfb3":"'''\nfor m in model_lgbm.models:\n    z = list(zip(list(m.feature_name_), list(m.feature_importances_)))\n    z.sort(key=lambda x:x[1], reverse=True)\n    print(z[:5])\n'''","d6e79a91":"class LGBMv1_1(LGBMv1):\n    def __init__(self):\n        super().__init__()\n        self.features += ['Upper_Rel', 'Lower_Rel', 'Upper_Rel_GA', 'Lower_Rel_GA', 'Avg_Vol_GA', 'Rel_Upper_GA']\n        \n    def train(self, df):\n        df = df.copy()\n        df = self._extend(df)\n        df = self._extend_correlation(df)\n        assets = df['Asset_ID'].drop_duplicates().to_list()\n        \n        for asset in assets:\n            X, y = get_Xy(df, asset, features=self.features)\n            model = LGBMRegressor(n_estimators=10)\n            model.fit(X, y)\n            self.models[asset] = model\n            \n    def predict(self, df_test, df_pred):\n        df_test = df_test.copy()\n        df_test = self._extend(df_test)\n        df_test = self._extend_correlation(df_test)\n        total = len(df_test.index)\n        \n        row_generator = df_test.iterrows()\n        row_generator = tqdm(row_generator, total=total) # Enable progress bar, comment this line to disable it.\n        for j , row in row_generator:\n            asset = row['Asset_ID']\n            x_test = row[self.features]\n            model = self.models[int(asset)]\n            y_pred = model.predict([x_test])[0]\n            mask = df_pred['row_id'] == j\n            df_pred.loc[mask, 'Target'] = y_pred\n        return df_pred\n        \n    def _extend_correlation(self, df):\n        df['Upper_Rel'] = (df['Upper_Shadow'] \/ df['Open']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Lower_Rel'] = (df['Lower_Shadow'] \/ df['Open']).replace([np.inf, -np.inf, np.nan], 0.)\n        #df['Liquidity'] = ((2 * (df['High'] - df['Low']) - np.absolute(df['Open'] - df['Close']))\/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        #df['Liquidity_GA'] = df.groupby(['timestamp'])['Liquidity'].transform('mean')\n        df['Upper_Rel_GA'] = df.groupby(['timestamp'])['Upper_Rel'].transform('mean')\n        df['Lower_Rel_GA'] = df.groupby(['timestamp'])['Lower_Rel'].transform('mean')\n        df['Avg_Vol_GA'] = df.groupby(['timestamp'])['Avg_Vol'].transform('mean')\n        df['Rel_Upper_GA'] = df.groupby(['timestamp'])['Rel_Upper'].transform('mean')\n        return df\n        ","a72f1158":"\n#model_lgbm = LGBMv1_1()\n#model_lgbm.train(data_df)\n","2de3d229":"from lightgbm import LGBMRegressor\nfrom tqdm import tqdm\nclass LGBMv1_2(CryptoModel):\n    def __init__(self):\n        self.models = [None] * 14\n        '''\n        self.features = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Lower_Shadow', 'Upper_Shadow',\n                         'Avg_Vol', 'Rel_Upper', 'Upper_VWAP', 'Upper_Vol', 'R_quantile', 'Rel_Upper_quantile']\n        '''\n        self.features = ['Count', 'Volume', 'Lower_Shadow', 'Upper_Shadow','Avg_Vol', 'Rel_Upper', 'Upper_VWAP', 'Upper_Vol', 'R_quantile', 'Rel_Upper_quantile']\n    def train(self, df):\n        df = df.copy()\n        df = self._extend(df)\n        assets = df['Asset_ID'].drop_duplicates().to_list()\n        \n        for asset in assets:\n            X, y = get_Xy(df, asset, features=self.features)\n            model = LGBMRegressor(n_estimators=10)\n            model.fit(X, y)\n            self.models[asset] = model\n            \n    def predict(self, df_test, df_pred):\n        df_test = df_test.copy()\n        df_test = self._extend(df_test)\n        total = len(df_test.index)\n        \n        row_generator = df_test.iterrows()\n        row_generator = tqdm(row_generator, total=total) # Enable progress bar, comment this line to disable it.\n        for j , row in row_generator:\n            asset = row['Asset_ID']\n            x_test = row[self.features]\n            model = self.models[int(asset)]\n            y_pred = model.predict([x_test])[0]\n            mask = df_pred['row_id'] == j\n            df_pred.loc[mask, 'Target'] = y_pred\n        return df_pred\n            \n    def _extend(self, df):\n        df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n        df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n        #df['Liquidity'] = ((2 * (df['High'] - df['Low']) - np.absolute(df['Open'] - df['Close']))\/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Avg_Vol'] = (df['Volume'] \/ df['Count']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Rel_Upper'] = ((df['High'] - df['VWAP']) \/ (df['High'] - df['Low'])).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Upper_VWAP'] = ((df['High'] - df['VWAP']) \/ df['VWAP']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Upper_Vol'] = ((df['High'] - df['VWAP']) \/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        \n        \n        df['Return_1min'] = (df['Close']\/df['Open'] - 1.0).replace([np.inf, -np.inf, np.nan], 0.)\n        df['R_max'] = df.groupby(['timestamp'])['Return_1min'].transform('max')\n        df['R_min'] = df.groupby(['timestamp'])['Return_1min'].transform('min')\n        df['R_quantile'] = (df['Return_1min'] - df['R_min'])\/(df['R_max'] - df['R_min'])\n        \n        df['Rel_Upper_max'] = df.groupby(['timestamp'])['Rel_Upper'].transform('max')\n        df['Rel_Upper_min'] = df.groupby(['timestamp'])['Rel_Upper'].transform('min')\n        df['Rel_Upper_quantile'] = (df['Rel_Upper'] - df['Rel_Upper_min'])\/(df['Rel_Upper_max'] - df['Rel_Upper_min']).replace([np.inf, -np.inf, np.nan], 0.5)\n        \n    \n        return df","4ef4dbd6":"model_lgbm = LGBMv1_2()\nmodel_lgbm.train(data_df)","4214d1ac":"class Benchmark:\n    def __init__(self, debug=False):\n        data_folder = '..\/input\/g-research-crypto-forecasting\/'\n        self.test_features = ['timestamp', 'Asset_ID', 'Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'group_num']\n        self.asset_details_df = pd.read_csv(data_folder + 'asset_details.csv')\n        if debug:\n            self.df_test = pd.read_csv(data_folder + 'example_test.csv')\n            self.df_pred = pd.read_csv(data_folder + 'example_sample_submission.csv')\n            self.target = self.df_test.copy()\n            self.target['Target'] = self.df_pred['Target']\n        else:\n            df = pd.read_csv(data_folder + 'supplemental_train.csv')\n            df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n            df = df[df['datetime'] >= '2021-06-13 00:00:00'].head(100000)\n            df = df.replace([np.inf, -np.inf], np.nan).dropna(how='any').reset_index()\n            df.loc[:, 'group_num'] = 0\n            df.loc[:, 'row_id'] = df.index\n            self.df_test = df[self.test_features].copy()\n            self.df_pred = df[['group_num', 'row_id', 'Target']].copy()\n            self.df_pred['Target'] = 0.\n            self.target = self.df_test.copy()\n            self.target['Target'] = df['Target']\n    \n    def benchmark(self, model):\n        df_test, df_pred, target, detail = self.df_test, self.df_pred, self.target, self.asset_details_df\n        group_ids = df_test['group_num'].drop_duplicates().to_list()\n        for idx in group_ids:\n            y = model.predict(df_test[df_test['group_num'] == idx], df_pred[df_pred['group_num'] == idx])\n            mask = df_pred['group_num'] == idx\n            df_pred.loc[mask, 'Target'] = y['Target']\n        \n        asset_ids = df_test['Asset_ID'].drop_duplicates().to_list()\n        corrs, weights = [], []\n        for idx in asset_ids:\n            asset_target_df = target.loc[target['Asset_ID'] == idx, 'Target']\n            row_ids = asset_target_df.index\n            asset_target = asset_target_df.to_numpy()\n            asset_pred = df_pred.iloc[row_ids]['Target'].to_numpy()\n            corr =  np.corrcoef(asset_target, asset_pred)[0, 1]\n            if np.isnan(corr):\n                corr = 0.\n            corrs.append(corr)\n            weights.append(detail[detail['Asset_ID'] == idx]['Weight'])\n        corrs = np.array(corrs).squeeze()\n        weights = np.array(weights).squeeze()\n        print(corrs)\n        print(weights)\n        return (corrs * weights).sum() \/ weights.sum()","bd50bbb6":"\nbenchmark_debug = Benchmark()\nbenchmark_debug.benchmark(model_lgbm)\n","cbf214ef":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt","23413e7b":"# Loading traning data strictly\ndef read_csv_strict(file_name='..\/input\/g-research-crypto-forecasting\/train.csv'):\n    df = pd.read_csv(file_name)\n    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n    df = df[df['datetime'] < '2021-06-13 00:00:00']\n    return df\n\ndata_df = read_csv_strict()\ndata_folder = '..\/input\/g-research-crypto-forecasting\/'\nasset_details_df = pd.read_csv(data_folder + 'asset_details.csv').set_index('Asset_ID')","ea0b6bc7":"class RidgeRegression():\n    def __init__(self):\n        self.models = [None] * 14\n        self.features = ['Count', 'Volume', 'Lower_Shadow', 'Upper_Shadow','Avg_Vol', 'Rel_Upper', 'Upper_VWAP', 'Upper_Vol', 'R_quantile', 'Rel_Upper_quantile']\n        \n    def _extend(self,df):\n        df = df.copy()\n        \n        df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n        df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n        #df['Liquidity'] = ((2 * (df['High'] - df['Low']) - np.absolute(df['Open'] - df['Close']))\/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Avg_Vol'] = (df['Volume'] \/ df['Count']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Rel_Upper'] = ((df['High'] - df['VWAP']) \/ (df['High'] - df['Low'])).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Upper_VWAP'] = ((df['High'] - df['VWAP']) \/ df['VWAP']).replace([np.inf, -np.inf, np.nan], 0.)\n        df['Upper_Vol'] = ((df['High'] - df['VWAP']) \/ df['Volume']).replace([np.inf, -np.inf, np.nan], 0.)\n        \n        \n        df['Return_1min'] = (df['Close']\/df['Open'] - 1.0).replace([np.inf, -np.inf, np.nan], 0.)\n        df['R_max'] = df.groupby(['timestamp'])['Return_1min'].transform('max')\n        df['R_min'] = df.groupby(['timestamp'])['Return_1min'].transform('min')\n        df['R_quantile'] = (df['Return_1min'] - df['R_min'])\/(df['R_max'] - df['R_min'])\n        \n        df['Rel_Upper_max'] = df.groupby(['timestamp'])['Rel_Upper'].transform('max')\n        df['Rel_Upper_min'] = df.groupby(['timestamp'])['Rel_Upper'].transform('min')\n        df['Rel_Upper_quantile'] = (df['Rel_Upper'] - df['Rel_Upper_min'])\/(df['Rel_Upper_max'] - df['Rel_Upper_min']).replace([np.inf, -np.inf, np.nan], 0.5)\n        return df\n    \n    def train(self, df):\n        df = df.copy()\n        df = self._extend(df)\n        assets = df['Asset_ID'].drop_duplicates().to_list()\n        \n        for asset in assets:\n            X, y = get_Xy(df, asset, features=self.features)\n            alphas = np.linspace(0.0001,0.1)\n            model = linear_model.RidgeCV(alphas = alphas,store_cv_values=True)\n            model.fit(X, y)\n            self.models[asset] = model\n            print(f'{asset} finished.')\n            \n    def predict(self, df_test, df_pred):\n        df_test = df_test.copy()\n        df_test = self._extend(df_test)\n        total = len(df_test.index)\n        \n        row_generator = df_test.iterrows()\n        row_generator = tqdm(row_generator, total=total) # Enable progress bar, comment this line to disable it.\n        for j , row in row_generator:\n            asset = row['Asset_ID']\n            x_test = row[self.features]\n            model = self.models[int(asset)]\n            y_pred = model.predict([x_test])[0]\n            mask = df_pred['row_id'] == j\n            df_pred.loc[mask, 'Target'] = y_pred\n        return df_pred","f5b96945":"model_RR = RidgeRegression()\nmodel_RR.train(data_df)","342b6f01":"benchmark_debug = Benchmark()\nbenchmark_debug.benchmark(model_RR)","3b79efcf":"data_df = data[data['Asset_ID'] == 1].dropna()","602f608e":"x_data = np.array(data_df['Close']\/data_df['Open']).reshape(-1,1)\ny_data = data_df['Target']\n\nalphas_to_test = np.linspace(0.0001,0.1)\nmodel = linear_model.RidgeCV(alphas = [0.08],store_cv_values=True)\n\nmodel.fit(x_data,y_data)\nprint(model.cv_values_[0])","09196507":"plt.plot(alphas_to_test,model.cv_values_.mean(axis = 0))","0a7a81e7":"model.predict(x_data[:5])\n","40b7b1bc":"# Register the model here\nmodel_submission = model_lgbm","c8c04c5a":"import gresearch_crypto\n\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()","80e62975":"for i, (df_test, df_pred) in enumerate(iter_test):\n    df_pred = model_submission.predict(df_test, df_pred)\n    env.predict(df_pred)","305e3b94":"# debugging cell","b1302037":"## LGBM Model v1.1\nAdds group correlation.\n\n`LB=0.0447`","41352e93":"## Baseline \\#2: Light GBM with Shadow-Features\nUsing 2 features from [G-Research - Starter [0.361 LB]](https:\/\/www.kaggle.com\/danofer\/g-research-starter-0-361-lb).\n\n`LB=0.018`.","91546cf4":"# kernel ridge regression","195b195a":"## Substitute Evaluation\nThe evaluation process of submission is slow and vague, therefore this section is implemented to benchmark the model.","a710ac4a":"# Crypto currency prediction\nMembers: ...\n\n> Acknowledgement: we used some code from [Proposal for a meaningful LB + Strict LGBM](https:\/\/www.kaggle.com\/julian3833\/proposal-for-a-meaningful-lb-strict-lgbm). We follow the definition of \"Strict\" from this author.","0c1603f0":"# LGBM v1.2","517dea64":"## Baseline \\#1: End-to-end Fully Connected Neural Network\nTurns out NN is prone to overfitting. Results are all negative.","6778de26":"## Submission\n**Important: the last cell could be run only once, due to the API requirement.**\n\nHowever, it's important that this section should be able to run without any error to ensure a successful submission","5f4c2257":"added data of quantile.","f99dd800":"## Base class of all models\nAll models shall derive from this class, and rewrite function `train()` and `predict()`. The input of these functions should be pandas DataFrame.","5e931fae":"## LGBM Model v1.0\nThis model includes features: `Upper_Shadow`, `Lower_Shadow`, `Liquidity`, `Avg_Vol`, `Rel_Upper`, `Upper_VWAP`, `Upper_Volume`. By now it does not consider correlation between groups.\n\n`LB=0.0402`","68166413":"## (Maybe) useful functions","dd170126":"## Loading Data and preprocessing\nWe follow the \"strict\" criteria from [here](https:\/\/www.kaggle.com\/julian3833\/proposal-for-a-meaningful-lb-strict-lgbm). Therefore our score is valid."}}