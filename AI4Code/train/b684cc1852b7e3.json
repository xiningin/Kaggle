{"cell_type":{"308a2d27":"code","ac5df0d1":"code","63244151":"code","34d16a61":"code","5c33932a":"code","5e0e324e":"code","e3c31cd0":"code","c8459395":"code","ecb3318a":"code","ce107d7c":"code","c5b8c954":"markdown","7e2cd733":"markdown","296ed151":"markdown","5990d1ec":"markdown","528d60ce":"markdown","7d3723ee":"markdown","74d5f680":"markdown","83cb9644":"markdown","fccca085":"markdown","2c285afa":"markdown","03163314":"markdown","4957dc5e":"markdown","36dbbf9c":"markdown","11e412ee":"markdown","bf9a5280":"markdown","b76bc73b":"markdown","01cf7394":"markdown","8a88a61b":"markdown","24932457":"markdown","07ba3662":"markdown","37725d3d":"markdown"},"source":{"308a2d27":"!pip install -q tflite-model-maker\n!pip install -q pycocotools","ac5df0d1":"import numpy as np\nimport os\n\nfrom tflite_model_maker.config import ExportFormat\nfrom tflite_model_maker import model_spec\nfrom tflite_model_maker import object_detector\n\nimport tensorflow as tf\nassert tf.__version__.startswith('2')\n\ntf.get_logger().setLevel('ERROR')\nfrom absl import logging\nlogging.set_verbosity(logging.ERROR)","63244151":"spec = model_spec.get('efficientdet_lite2')","34d16a61":"train_data, validation_data, test_data = object_detector.DataLoader.from_csv(\n    'gs:\/\/cloud-ml-data\/img\/openimage\/csv\/salads_ml_use.csv')","5c33932a":"model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)","5e0e324e":"model.evaluate(test_data)","e3c31cd0":"model.export(export_dir='.')","c8459395":"model.evaluate_tflite('model.tflite', test_data)","ecb3318a":"#@title Load the trained TFLite model and define some visualization functions\n\nimport cv2\n\nfrom PIL import Image\n\nmodel_path = 'model.tflite'\n\n# Load the labels into a list\nclasses = ['???'] * model.model_spec.config.num_classes\nlabel_map = model.model_spec.config.label_map\nfor label_id, label_name in label_map.as_dict().items():\n  classes[label_id-1] = label_name\n\n# Define a list of colors for visualization\nCOLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n\ndef preprocess_image(image_path, input_size):\n  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n  img = tf.io.read_file(image_path)\n  img = tf.io.decode_image(img, channels=3)\n  img = tf.image.convert_image_dtype(img, tf.uint8)\n  original_image = img\n  resized_img = tf.image.resize(img, input_size)\n  resized_img = resized_img[tf.newaxis, :]\n  return resized_img, original_image\n\n\ndef set_input_tensor(interpreter, image):\n  \"\"\"Set the input tensor.\"\"\"\n  tensor_index = interpreter.get_input_details()[0]['index']\n  input_tensor = interpreter.tensor(tensor_index)()[0]\n  input_tensor[:, :] = image\n\n\ndef get_output_tensor(interpreter, index):\n  \"\"\"Retur the output tensor at the given index.\"\"\"\n  output_details = interpreter.get_output_details()[index]\n  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n  return tensor\n\n\ndef detect_objects(interpreter, image, threshold):\n  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n  # Feed the input image to the model\n  set_input_tensor(interpreter, image)\n  interpreter.invoke()\n\n  # Get all outputs from the model\n  boxes = get_output_tensor(interpreter, 0)\n  classes = get_output_tensor(interpreter, 1)\n  scores = get_output_tensor(interpreter, 2)\n  count = int(get_output_tensor(interpreter, 3))\n\n  results = []\n  for i in range(count):\n    if scores[i] >= threshold:\n      result = {\n        'bounding_box': boxes[i],\n        'class_id': classes[i],\n        'score': scores[i]\n      }\n      results.append(result)\n  return results\n\n\ndef run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n  # Load the input shape required by the model\n  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n\n  # Load the input image and preprocess it\n  preprocessed_image, original_image = preprocess_image(\n      image_path, \n      (input_height, input_width)\n    )\n\n  # Run object detection on the input image\n  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n\n  # Plot the detection results on the input image\n  original_image_np = original_image.numpy().astype(np.uint8)\n  for obj in results:\n    # Convert the object bounding box from relative coordinates to absolute \n    # coordinates based on the original image resolution\n    ymin, xmin, ymax, xmax = obj['bounding_box']\n    xmin = int(xmin * original_image_np.shape[1])\n    xmax = int(xmax * original_image_np.shape[1])\n    ymin = int(ymin * original_image_np.shape[0])\n    ymax = int(ymax * original_image_np.shape[0])\n\n    # Find the class index of the current object\n    class_id = int(obj['class_id'])\n\n    # Draw the bounding box and label on the image\n    color = [int(c) for c in COLORS[class_id]]\n    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n    # Make adjustments to make the label visible for all objects\n    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n    cv2.putText(original_image_np, label, (xmin, y),\n        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n  # Return the final image\n  original_uint8 = original_image_np.astype(np.uint8)\n  return original_uint8","ce107d7c":"#@title Run object detection and show the detection results\n\nINPUT_IMAGE_URL = \"https:\/\/storage.googleapis.com\/cloud-ml-data\/img\/openimage\/3\/2520\/3916261642_0a504acd60_o.jpg\" #@param {type:\"string\"}\nDETECTION_THRESHOLD = 0.25 #@param {type:\"number\"}\n\nTEMP_FILE = '\/tmp\/image.png'\n\n!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\nim = Image.open(TEMP_FILE)\nim.thumbnail((512, 512), Image.ANTIALIAS)\nim.save(TEMP_FILE, 'PNG')\n\n# Load the TFLite model\ninterpreter = tf.lite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Run inference and draw detection result on the local copy of the original file\ndetection_result_image = run_odt_and_draw_results(\n    TEMP_FILE, \n    interpreter, \n    threshold=DETECTION_THRESHOLD\n)\n\n# Show the detection result\nImage.fromarray(detection_result_image)","c5b8c954":"## 4. \u8bad\u7ec3\u6a21\u578b","7e2cd733":"\u6d4b\u8bd5\u96c6\u5305\u542b25\u5e45\u56fe\u50cf\uff0c\u8fd9\u4e9b\u6570\u636e\u662f\u6a21\u578b\u4ee5\u524d\u6ca1\u6709\u89c1\u8fc7\u7684\u3002<br\/>\n\n\u9ed8\u8ba4 batch size \u662f 64, \u6240\u4ee5\u53ea\u9700\u8981\u4e00\u6b65\u5373\u53ef\u5b8c\u621025\u5e45\u56fe\u50cf\u7684\u8bc4\u4f30\u3002","296ed151":"## 7.\u8bc4\u4f30TFLite\u6a21\u578b","5990d1ec":"\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u4ece\u8f93\u51fa\u76ee\u5f55\u4e2d\u4e0b\u8f7d\u4f60\u81ea\u5df1\u8bad\u7ec3\u7684TFLite\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5230Android App\u4e2d\u3002","528d60ce":"\u5f71\u54cd\u5230 TFLite \u7cbe\u5ea6\u7684\u56e0\u7d20\u6709:<br\/>\n\n\u91cf\u5316\u4f18\u5316\u65b9\u6cd5\u53ef\u4ee5\u7f29\u51cf\u6a21\u578b\u8ba1\u7b97\u91cf\u5230\u539f\u6765\u76841\/4\uff0c\u5f53\u7136\u8fd9\u662f\u4ee5\u727a\u7272\u51c6\u786e\u7387\u4e3a\u4ee3\u4ef7\u7684\u3002<br\/>\n\u539f\u6765\u7684 TensorFlow model\u5bf9\u6bcf\u4e00\u4e2a\u7c7b\u522b\u91c7\u7528NMS\u65b9\u6cd5\uff0c\u5fae\u8c03\u8bad\u7ec3\u65f6\uff0c\u5219\u91c7\u7528global NMS \u65b9\u6cd5\uff0c\u867d\u7136\u66f4\u5feb\u4e86\uff0c\u4f46\u662f\u51c6\u786e\u7387\u4f1a\u4e0b\u964d\u3002<br\/>\n\u539f\u6765\u7684\u6a21\u578b\u6700\u5927\u53ef\u4ee5\u68c0\u6d4b\u51fa 100 \u4e2a\u76ee\u6807\uff0c\u73b0\u5728tflite\u5219\u6700\u5927\u8f93\u51fa 25 \u4e2a\u76ee\u6807\u3002<br\/>\n\u6240\u4ee5\u6709\u5fc5\u8981\u5bf9\u6bd4 TFLite model \u548c\u4e0e\u4e4b\u5bf9\u5e94\u7684\u8f6c\u6362\u4e4b\u524d\u7684 TensorFlow model\u3002","7d3723ee":"## 1. \u5b89\u88c5\u9700\u8981\u7684\u5305","74d5f680":"## 8. TFLite\u6d4b\u8bd5","83cb9644":"## 3. \u52a0\u8f7d\u6570\u636e\u96c6","fccca085":"\u5c06\u4e0b\u9762\u7684 INPUT_IMAGE_URL \u66ff\u6362\u4e3a\u4f60\u81ea\u5df1\u7684\u56fe\u50cf\u94fe\u63a5\u3002<br\/>\n\u8c03\u6574 DETECTION_THRESHOLD \u6539\u53d8\u6a21\u578b\u7684\u7075\u654f\u5ea6\u3002\u53d6\u503c\u8d8a\u4f4e\uff0c\u610f\u5473\u7740\u53ef\u4ee5\u68c0\u6d4b\u5230\u66f4\u591a\u7684\u76ee\u6807\uff0c\u540c\u65f6\u4e5f\u610f\u5473\u7740\u53ef\u80fd\u51fa\u73b0\u66f4\u591a\u7684\u9519\u8bef\u3002\u53cd\u4e4b\uff0c\u53d6\u503c\u8d8a\u9ad8\uff0c\u610f\u5473\u7740\u5bf9\u68c0\u6d4b\u7684\u76ee\u6807\u53ef\u4fe1\u5ea6\u8d8a\u9ad8\u3002<br\/>\n\u5c3d\u7ba1\u5c06TFLite\u90e8\u7f72\u5230Android\u4ec5\u9700\u51e0\u884c\u7f16\u7801\uff0c\u4f46\u662f\u6b64\u65f6\uff0c\u5728\u8fd9\u91cc\u505a\u6a21\u62df\u6d4b\u8bd5\uff0c\u5219\u989d\u5916\u9700\u8981\u591a\u5199\u4e00\u4e9b\u4ee3\u7801\u3002<br\/>","2c285afa":"\u6839\u636e\u4ee5\u4e0b\u6848\u4f8b\uff0c\u8bad\u7ec3\u81ea\u5df1\u7684TFLite\u6a21\u578b<br\/>\nhttps:\/\/github.com\/ralphcajipe\/Salad-detector-with-TensorFlow-Lite-Model-Maker\/blob\/main\/Train_a_salad_detector_with_TFLite_Model_Maker.ipynb","03163314":"\u5bf9EfficientDet-Lite0 \u6a21\u578b\u91c7\u7528\u5fae\u8c03\u8bad\u7ec3\u65b9\u6cd5\u3002\u8bbe\u5b9a\uff1a<br\/>\nepochs = 50 <br\/>\nbatch_size = 8 <br\/>\n\u7531\u4e8e\u8bad\u7ec3\u96c6\u4e2d\u5305\u542b175\u5e45\u56fe\u50cf\uff0c\u6240\u4ee5\u6bcf\u4e00\u4ee3\u8bad\u7ec3\u9700\u8981\u7ecf\u8fc721\u6b65\u8fed\u4ee3\u3002<br\/>\n\u8bbe\u7f6e train_whole_model=True \u8868\u793a\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u91c7\u7528\u5fae\u8c03\u6a21\u5f0f\uff0c\u65f6\u95f4\u4f1a\u957f\u4e00\u4e9b\uff0c\u4f46\u662f\u6548\u679c\u4f1a\u597d\u4e00\u4e9b\u3002","4957dc5e":"## 6.\u751f\u6210TFLite\u6a21\u578b","36dbbf9c":"## 5. \u7528\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\u8bc4\u4f30\u6a21\u578b","11e412ee":"\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5bfc\u51fa\u4e3a TensorFlow Lite \u6a21\u578b\u3002\u8fd9\u662f\u4e00\u4e2a\u7ecf\u8fc7\u91cf\u5316\u4f18\u5316\u7684\u6a21\u578b\u3002","bf9a5280":"\u7ecf\u8fc7\u4e0e\u524d\u8ff0\u7684TensorFlow Model \u6bd4\u8f83\uff0c\u4f60\u4f1a\u60ca\u8bb6\u7684\u53d1\u73b0\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u7684\u5e45\u5ea6\u5728\u53ef\u63a5\u53d7\u7684\u8303\u56f4\u5185\u3002","b76bc73b":"\u5bfc\u5165\u9700\u8981\u7684\u5e93","01cf7394":"## 2. \u9009\u62e9\u6a21\u578b","8a88a61b":"# \u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5b9a\u4e49\u81ea\u5df1\u7684TFLite\u6a21\u578b","24932457":"\u73b0\u5728\u53ef\u4ee5\u4eceWeb\u4e0a\u627e\u4e00\u5e45\u56fe\u50cf\uff0c\u7528TFLite\u6d4b\u8bd5\u4e4b","07ba3662":"![image.png](attachment:109dc181-5fda-45c7-affe-36e5b130e3b7.png)","37725d3d":"\u672c\u6848\u4f8b\u57fa\u4e8e Model Maker library\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u5df1\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u53ef\u4ee5\u90e8\u7f72\u5230Android\u4e0a\u7684TFLite\u6a21\u578b\u3002<br\/>\n\n\u672c\u6848\u4f8b\u91c7\u7528\u7684\u6f14\u793a\u6570\u636e\u96c6\u662f Salads dataset\u3002\u8be5\u6570\u636e\u96c6\u6765\u81ea <a href=\"https:\/\/storage.googleapis.com\/openimages\/web\/index.html\">\nOpen Images Dataset V4.<\/a>\n\n\u6570\u636e\u96c6\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u6709:<br\/>\nBaked Good<br\/>\nCheese<br\/>\nSalad<br\/>\nSeafood<br\/>\nTomato<br\/>\n\u4ee5\u53ca\u6bcf\u79cd\u6807\u7b7e\u5bf9\u5e94\u7684\u76ee\u6807\u5bf9\u8c61\u7684\u4f4d\u7f6e\uff1abounding box.\n"}}