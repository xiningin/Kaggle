{"cell_type":{"9755c441":"code","dbe4550e":"code","ddfdcdd5":"code","c72458b6":"code","1f125b84":"code","e5d46656":"code","f419e7bb":"code","e6a5aa08":"code","78f31f20":"code","ed055307":"code","fcd909ea":"code","b7f9f171":"code","9df8f267":"code","40e49438":"code","333db3cf":"code","cd78fe62":"code","53d1fea5":"code","ed96042c":"code","a2d22eed":"code","d7eb66fc":"code","e661de64":"code","6f538786":"code","ea40ad9b":"code","b9b2d564":"code","57b20e93":"code","75fbfb66":"code","ab866ec5":"code","e379296f":"code","c7b95adc":"code","557972e1":"code","b5822114":"code","a97bd813":"code","d9f233ff":"code","7d9c259b":"code","4a62735b":"code","fcb521d8":"code","d757e43a":"code","df54837b":"code","406a7d5e":"code","9a18a718":"markdown","55b01887":"markdown","568fa78e":"markdown","c04081e5":"markdown","1fa0ccf1":"markdown","a39bc07c":"markdown","d696f70e":"markdown","1a59e9d2":"markdown","3898de39":"markdown","40f36995":"markdown","8849e45a":"markdown","c3e8cd83":"markdown","3101dbae":"markdown","3ccc9ad3":"markdown","3b6eed71":"markdown","5447464d":"markdown","2bbd4f1d":"markdown","972c7e91":"markdown","df592163":"markdown","d30e83fc":"markdown","20219956":"markdown","d61f1f4f":"markdown","60fdb686":"markdown","c43b8c60":"markdown","eda162ac":"markdown","7f7857b5":"markdown","294d13d3":"markdown","3d363996":"markdown","6d90bacd":"markdown","2f9c78e0":"markdown","b6344710":"markdown","c511c950":"markdown","0589fe78":"markdown","8a429d9d":"markdown","15e091a1":"markdown","4f5daafa":"markdown","d99fe33c":"markdown","e5b50cb1":"markdown","4eb76e90":"markdown","d6f906a9":"markdown","bc000f3f":"markdown","57e35003":"markdown"},"source":{"9755c441":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dbe4550e":"import numpy as np\nimport pandas as pd \npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom nltk.util import ngrams\nfrom nltk.stem.porter import PorterStemmer\nfrom collections import  Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom tensorflow.keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\nimport warnings\nfrom collections import defaultdict\nwarnings.filterwarnings('ignore')\nprint(\"Important libraries loaded successfully\")\n\nfrom wordcloud import STOPWORDS\nfrom nltk.util import ngrams\nimport gc\nimport operator\nimport tokenization\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\nSEED = 1337\n\n","ddfdcdd5":"#Importing and understanding the structure of  Data\ndata_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint(\"Train Data shape = \",data_train.shape)\ndata_train.head(2)","c72458b6":"data_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint(\"Test Data shape = \",data_test.shape)\ndata_test.head(2)","1f125b84":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=data_train[missing_cols].isnull().sum().index, y=data_train[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=data_test[missing_cols].isnull().sum().index, y=data_test[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()\n\nfor df in [data_train, data_test]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","e5d46656":"print(f'Number of unique values in keyword = {data_train[\"keyword\"].nunique()} (Training) - {data_test[\"keyword\"].nunique()} (Test)')\nprint(f'Number of unique values in location = {data_train[\"location\"].nunique()} (Training) - {data_test[\"location\"].nunique()} (Test)')","f419e7bb":"data_train['target_mean'] = data_train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=data_train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=data_train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndata_train.drop(columns=['target_mean'], inplace=True)","e6a5aa08":"x=data_train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","78f31f20":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=data_train[data_train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=data_train[data_train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","ed055307":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=data_train[data_train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=data_train[data_train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","fcd909ea":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ndata_train_len=data_train[data_train['target']==1]['text'].str.len()\nax1.hist(data_train_len,color='red')\nax1.set_title('disaster tweets')\ndata_train_len=data_train[data_train['target']==0]['text'].str.len()\nax2.hist(data_train_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","b7f9f171":"# word_count\ndata_train['word_count'] = data_train['text'].apply(lambda x: len(str(x).split()))\ndata_test['word_count'] = data_test['text'].apply(lambda x: len(str(x).split()))\n# unique_word_count\ndata_train['unique_word_count'] = data_train['text'].apply(lambda x: len(set(str(x).split())))\ndata_test['unique_word_count'] = data_test['text'].apply(lambda x: len(set(str(x).split())))\n# stop_word_count\ndata_train['stop_word_count'] = data_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndata_test['stop_word_count'] = data_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n# url_count\ndata_train['url_count'] = data_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ndata_test['url_count'] = data_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n# mean_word_length\ndata_train['mean_word_length'] = data_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndata_test['mean_word_length'] = data_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n# char_count\ndata_train['char_count'] = data_train['text'].apply(lambda x: len(str(x)))\ndata_test['char_count'] = data_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndata_train['punctuation_count'] = data_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndata_test['punctuation_count'] = data_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndata_train['hashtag_count'] = data_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ndata_test['hashtag_count'] = data_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndata_train['mention_count'] = data_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ndata_test['mention_count'] = data_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n","9df8f267":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\nDISASTER_TWEETS = data_train['target'] == 1\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(data_train.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[i][0], color='green')\n    sns.distplot(data_train.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[i][0], color='red')\n\n    sns.distplot(data_train[feature], label='Training', ax=axes[i][1])\n    sns.distplot(data_test[feature], label='Test', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","40e49438":"def create_corpus(target):\n    corpus=[]\n    # Split the tweet text into words and append into corpus list \n    for x in data_train[data_train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","333db3cf":"# list of corpus for type 0 tweet \ncorpus=create_corpus(0)\n#Frequency calculation of stop words in corpus list of type 0 tweet \ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n#Sorting of frequency values and displaying top ten frequencies        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","cd78fe62":"x,y=zip(*top)\nplt.bar(x,y)","53d1fea5":"# list of corpus for type 1 tweet \ncorpus=create_corpus(1)\n#Frequency calculation of stop words in corpus list of type 0 tweet \ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n#Sorting of frequency values and displaying top ten frequencies        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","ed96042c":"x,y=zip(*top)\nplt.bar(x,y)","a2d22eed":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","d7eb66fc":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","e661de64":"counter=Counter(corpus)\n#converting into tuple\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","6f538786":"#Here we are passing each tweet text to the function for the bigram creation. \ndef get_top_tweet_bigrams(corpus, n=None):\n    #print(corpus)\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    #print(vec)\n    bag_of_words = vec.transform(corpus)\n    #print(bag_of_words)\n \n    # Sum of words is a vector which contains the row wise (axis=0)[if axis =1 it\u2019s a column wise addition )\n    #Addition of bag of words, Bag of words contain the count of each of the word in the sentence in the entire corpus  \n    #refer example of counter vectorizer below \n\n    sum_words = bag_of_words.sum(axis=0) \n    #print(sum_words )\n    #To plot the frequency of word, we consider the feature vector (bag of words(vec.transform ))\n    #which  contain all the different words in the document and the sum of words\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n    ","ea40ad9b":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(data_train['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","b9b2d564":"\"\"\"\nfrom sklearn.feature_extraction.text import CountVectorizer \n#Convert a collection of text documents to a matrix of token counts\ncorpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is is this the first document?']\n\nvectorizer = CountVectorizer(ngram_range=(2,2 )).fit(corpus)\nprint(vectorizer)\nX = vectorizer.fit_transform(corpus) \nprint(X) \nprint(vectorizer.get_feature_names()) \nprint(X.toarray())\nvectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) \nX2 = vectorizer2.fit_transform(corpus)\nprint(vectorizer2.get_feature_names())\nprint(X2.toarray())\nprint(vec) \nbag_of_words = vec.transform(corpus)\nprint(bag_of_words)\nsum_words = bag_of_words.sum(axis=0) \nprint(sum_words ) \"\"\"","57b20e93":"   \"\"\"\" vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    #print(vec)\n    bag_of_words = vec.transform(corpus)\n    print(bag_of_words)\n    sum_words = bag_of_words.sum(axis=0) \n    print(sum_words )\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n    \"\"\"","75fbfb66":"df=pd.concat([data_train,data_test])\ndf.shape","ab866ec5":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","e379296f":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","c7b95adc":"df['text']=df['text'].apply(lambda x : remove_URL(x))","557972e1":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","b5822114":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","a97bd813":"df['text']=df['text'].apply(lambda x : remove_html(x))","d9f233ff":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","7d9c259b":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","4a62735b":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","fcb521d8":"df['text']=df['text'].apply(lambda x : remove_punct(x))","d757e43a":"\n!pip install pyspellchecker","df54837b":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","406a7d5e":"#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)","9a18a718":"**Analysis of tweets in the following levels**\n* Character level\n* Sentence level\n* Word level","55b01887":"We will do a bigram (n=2) analysis over the tweets. Let's check the most common bigrams in tweets.","568fa78e":"Keyword can give some information about the context. The count of occurence of each keyword in different in different categories. Same keywords can be present in both Train and Test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword.\n\nLocations are not automatically generated, they are user inputs. That's why locationis very dirty and there are too many unique values in it. It shouldn't be used as a feature.  ","c04081e5":"#### **Average word length in a tweet**","1fa0ccf1":"### **Structure of  Training and Testing Data**","a39bc07c":"### Removing urls","d696f70e":"The '*', '+', and '?' qualifiers are all greedy; they match as much text as possible. Sometimes this behaviour isn\u2019t desired; if the RE <.*> is matched against '<H1>title<\/H1>', it will match the entire string, and not just '<H1>'. Adding '?' after the qualifier makes it perform the match in non-greedy or minimal fashion; as few characters as possible will be matched. Using .*? in the previous expression will match only '<H1>'.","1a59e9d2":" #  Data Exploration","3898de39":"Usually, disaster tweets will be in a more formal way with longer words compared to non-diaster tweets because of the majority of disaster tweets from news agencies or government organizations. Non-disaster tweets have more typos than disaster tweets because they are coming from individual users. We can do this type of analysis using meta-features. The following meta-feature can be used for the anlaysis\n*  `average_word_length`Average word length\n* `word_count` number of words in text\n* `char_count` number of characters in text\n* `unique_word_count` number of unique words in text\n* `stop_word_count` number of stop words in text\n* `url_count` number of urls in text\n* `mean_word_length` average character count in words\n* `punctuation_count` number of punctuations in text\n* `hashtag_count` number of hashtags (**#**) in text\n* `mention_count` number of mentions (**@**) in text","40f36995":"### **Target distribution anaysis**\n\nin this case two possible values for target,**0** or **1**","8849e45a":"#### **Unique word count**","c3e8cd83":"Even if I'm not good at spelling I can correct it with python :) I will use `pyspellcheker` to do that.","3101dbae":"> ### Corpus Creation","3ccc9ad3":"\n* Real Tweet\n    *  The tweets that describe real disasters like a car accident will be considered as a disaster tweet as it describes real traffic accident\n    *  Another type of disaster like fire accident which can be described using the word \u201cablaze\u201d is also considered as real disasters.\n* Fake: The word \u201caccident\u201d can also be used metaphorically to describe context like \u201cSleeping pills double your risk of a car accident\u201d where the tweet is not describing any accidents occurred. So these are considered as fake tweets since there are not disasters but different reasons that can increase the chance for a car accident.\n\n* Real disasters \u2013 1\n* Fake disasters - 0\n\n## **Objective **\nClassify the tweents into real disasters or not","3b6eed71":"## Analysis of  Meta Features","5447464d":"* The distribution of characters in both types of tweets seems to be almost same.The most common range is 120-140 .\n* The number of words in both the are mostly between 10-20\n* Average word legth is between .25 to .35\n\n","2bbd4f1d":"### **Missing Value Exploration**","972c7e91":"Next check tweets indicating fake disaster.","df592163":"#### **Number of characters in tweets**","d30e83fc":"Target distribution graph shows the frequency of each keywords in real and fake tweets. There is big difference in the frequency of each keywond in different target categories.  ","20219956":"First let's check tweets indicating real disaster.","d61f1f4f":"N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios). For example, for the sentence \"The cow jumps over the moon\". If N=2 (known as bigrams), then the ngrams would be:\n* the cow\n* cow jumps\n* jumps over\n* over the\n* the moon","60fdb686":"## **Analysis of Keywords **","c43b8c60":"### Classification of Real and Fake tweets Real Tweet","eda162ac":"This kernel is  divided into the following parts\n\n* ** Data Exploration**\n    1. Missing Value Exploration\n    2. Identification of stopping words\n    3. Target distribution anaysis   \n    4. Exploratory Data Analysis of tweets\n    5. Corpus Creation\n    6. Stop words identification and analysis \n*  **Data Preprocessing**\n*  **Basic NLP Techniques**\n*  **Models Bulding**\n*  **Models evaluation**","7f7857b5":"### Stop words identification and analysis ","294d13d3":"### Removing punctuations","3d363996":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","6d90bacd":"#### Stop word identification in class 0 tweets\n","2f9c78e0":"### Spelling Correction","b6344710":"### Removing HTML tags","c511c950":"Stop word identification in class 1 tweets","0589fe78":"### Ngram analysis","8a429d9d":"### Analyzing punctuations","15e091a1":"#### **Number of words in a tweet**","4f5daafa":"Lot of cleaning needed !","d99fe33c":"# **INTRODUCTION**\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n    \n","e5b50cb1":"The above graphs shows there are less number of tweets for real tweets than not real","4eb76e90":"### Common words ","d6f906a9":"## Data Preprocessing\nAs we know, twitter tweets always have to be cleaned before we go onto modeling. So we will do some basic cleaning such as spelling correction, removing punctuations, removing HTML tags and emojis, etc. So let's start.","bc000f3f":"### Romoving Emojis","57e35003":"### Exploratory Data Analysis of tweets in different levels"}}