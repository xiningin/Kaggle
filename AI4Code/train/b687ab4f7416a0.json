{"cell_type":{"9bc643bd":"code","3a74eb72":"code","faa175fc":"code","a8f937e1":"code","daa0b796":"code","1162a490":"code","82d5fd5c":"code","e8336148":"code","e9a927e8":"code","fae20829":"code","81ac3059":"code","7b064ef5":"code","f9580263":"code","9af1716d":"code","7dc1bc2d":"code","42b54e05":"code","a668ad2f":"code","a5dbf924":"code","f84ad334":"code","27388d41":"code","3eeb5131":"code","a5c8a159":"code","556fdb74":"code","3263d562":"code","9f239580":"code","df55ffbe":"code","75b08b2b":"code","8c25c483":"code","6a98a949":"code","c1ac8aab":"code","f45eaad1":"code","2ff88a02":"code","b24297d0":"code","3efd30a4":"code","989a3813":"code","6c4d0900":"code","4685373b":"markdown","b0427965":"markdown","16257f7b":"markdown","e5a8c278":"markdown","92c3d9d0":"markdown","c5dfd7bc":"markdown","6d8a4c6b":"markdown","d13c4197":"markdown","8fead7b3":"markdown","2f1b6bb9":"markdown","541aee03":"markdown","43a4977c":"markdown","6e52979a":"markdown","e3014465":"markdown","ff321fe4":"markdown","f8ab971e":"markdown","02ac7b9d":"markdown","63bdf5b1":"markdown","be6c7409":"markdown","b1ef1a3b":"markdown","c6638f1a":"markdown","b1000c9b":"markdown","d57b7258":"markdown","4d71955e":"markdown","f17d00f4":"markdown"},"source":{"9bc643bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3a74eb72":"#Library importer\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime as dt\nimport gc\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor","faa175fc":"train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')","a8f937e1":"train['date'] = train['date'].apply(lambda x: dt.strptime(x, '%d.%m.%Y'))","daa0b796":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df","1162a490":"train = downcast(train)\ntest = downcast(test)\nshops = downcast(shops)\nitems = downcast(items)\nitem_cats = downcast(item_cats)","82d5fd5c":"sns.boxplot(x=train.item_cnt_day)","e8336148":"sns.boxplot(x=train.item_price)","e9a927e8":"train = train[train.item_price > 0].reset_index(drop=True)\ntrain[train.item_cnt_day <= 0].item_cnt_day.unique()\ntrain.loc[train.item_cnt_day < 1, 'item_cnt_day'] = 0","fae20829":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 11, 'shop_id'] = 10\ntest.loc[test.shop_id == 11, 'shop_id'] = 10\n\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39","81ac3059":"new_test = pd.merge(pd.merge(pd.merge(test, items),shops),item_cats)\nnew_test","7b064ef5":"new_train = pd.merge(pd.merge(pd.merge(train, items),shops),item_cats)\nnew_train","f9580263":"aggr = new_train.groupby(['item_category_id']).agg({'item_price':'sum'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['item_category_id'], aggr['item_price'])","9af1716d":"aggr = new_train.groupby(['item_category_id']).agg({'item_cnt_day':'mean'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['item_category_id'], aggr['item_cnt_day'], color='red')","7dc1bc2d":"aggr = train.groupby(['date_block_num']).agg({'item_cnt_day':'mean'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['date_block_num'], aggr['item_cnt_day'], color='brown')","42b54e05":"aggr = train.groupby(['date_block_num']).agg({'item_price':'mean'})\naggr = aggr.reset_index()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.plot(aggr['date_block_num'], aggr['item_price'], color='green')","a668ad2f":"aggr = train.groupby(['date_block_num']).agg({'item_price':'mean'})\naggr = aggr.reset_index()\n\nsns.scatterplot(data=aggr, x='date_block_num', y='item_price')","a5dbf924":"sns.set(style='ticks')\n\naggr = train.drop(columns = ['date','shop_id','item_id'])\nsns.pairplot(aggr)","f84ad334":"#Delete unwanted dataframes\n\ndel new_train\ndel test\n\ngc.collect()","27388d41":"aggr = train.groupby(['shop_id','item_id','date_block_num'])['item_price','item_cnt_day','date'].agg({'item_price':['mean'], 'item_cnt_day':['sum'], 'date':['min','max']})\n\naggr = aggr.reset_index()\naggr","3eeb5131":"aggr['duration'] = (aggr['date']['max'] - aggr['date']['min']).dt.days.astype(np.int16)\naggr['duration'] += 1\naggr['item_cnt_day_sum'] = aggr['item_cnt_day']['sum']\naggr['item_price_mean'] = aggr['item_price']['mean']\n\naggr = aggr.drop(columns = ['date', 'item_cnt_day', 'item_price'])\n\naggr","a5c8a159":"monthly_sales = aggr.groupby(['shop_id', 'item_id']).agg({'item_cnt_day_sum':['sum'], 'item_price_mean':['mean'], 'duration':['sum']})\n\nmonthly_sales = monthly_sales.reset_index()\nmonthly_sales","556fdb74":"monthly_sales['item_cnt_month'] = monthly_sales['item_cnt_day_sum']['sum']\/30\nmonthly_sales['item_price_month'] = monthly_sales['item_price_mean']['mean']\nmonthly_sales['sales_duration'] = monthly_sales['duration']['sum']\n\nmonthly_sales = monthly_sales.drop(columns = ['item_cnt_day_sum','item_price_mean','duration'])\nmonthly_sales","3263d562":"#Delete unwanted dataframes\ndel aggr\n\ngc.collect()","9f239580":"def colnamecheck(df):\n    cols = []\n    \n    for x in df.columns:\n        if type(x)!= str:\n            cols.append(''.join(x))\n        else:\n            cols.append(x)\n            \n    df.columns = cols","df55ffbe":"df = pd.merge(monthly_sales, items, on='item_id')\ndf = df.drop(columns = ['item_id'])\n\ncolnamecheck(df)\n\ndf1 = pd.merge(df, shops, on='shop_id')\ndf2 = pd.merge(df1, item_cats, on='item_category_id')\n\nmonthly_sales = df2\n\ndel df, df1, df2\ngc.collect()","75b08b2b":"def EncodeColumn(df, old_col, new_col):\n    \n    enc = LabelEncoder()\n    \n    df[new_col] = enc.fit_transform(df[old_col])","8c25c483":"EncodeColumn(monthly_sales, 'item_name', 'item_name_enc')\nEncodeColumn(new_test, 'item_name', 'item_name_enc')\n\nEncodeColumn(monthly_sales, 'shop_name', 'shop_name_enc')\nEncodeColumn(new_test, 'shop_name', 'shop_name_enc')\n\nEncodeColumn(monthly_sales, 'item_category_name', 'item_category_name_enc')\nEncodeColumn(new_test, 'item_category_name', 'item_category_name_enc')","6a98a949":"monthly_sales = monthly_sales.drop(columns = ['item_name'])\nmonthly_sales = monthly_sales.drop(columns = ['shop_name'])\nmonthly_sales = monthly_sales.drop(columns = ['item_category_name'])\n\nnew_test = new_test.drop(columns = ['item_name'])\nnew_test = new_test.drop(columns = ['shop_name'])\nnew_test = new_test.drop(columns = ['item_category_name'])","c1ac8aab":"xgb = XGBRegressor(\n    learning_rate=0.01,\n    max_depth=3,\n    n_estimators=1000, \n    colsample_bytree=0.8, \n    subsample=0.8,     \n)","f45eaad1":"X = monthly_sales.drop(columns = ['item_price_month', 'sales_duration', 'item_cnt_month'])\ny = monthly_sales['item_price_month']\n\nxgb.fit(X, y)\n\npreds = xgb.predict(new_test.drop(columns=['ID']))\nnew_test['item_price_month'] = preds\nmonthly_sales = monthly_sales.drop(columns = ['item_price_month'])\nmonthly_sales['item_price_month'] = y","2ff88a02":"X = monthly_sales.drop(columns = ['sales_duration', 'item_cnt_month'])\ny = monthly_sales['sales_duration']\n\nxgb.fit(X, y)\n\npreds = xgb.predict(new_test.drop(columns=['ID']))\nnew_test['sales_duration'] = preds\nmonthly_sales = monthly_sales.drop(columns = ['sales_duration'])\nmonthly_sales['sales_duration'] = y","b24297d0":"X = monthly_sales.drop(columns = ['item_cnt_month'])\ny = monthly_sales['item_cnt_month']\n\nxgb.fit(X, y)\n\npreds = xgb.predict(new_test.drop(columns=['ID']))\nnew_test['item_cnt_month'] = preds","3efd30a4":"result = pd.DataFrame({'ID':new_test['ID'], 'item_cnt_month':new_test['item_cnt_month']})","989a3813":"result","6c4d0900":"result.to_csv('submission.csv', index=False)","4685373b":"**Eliminate all item prices below zero, and set all negative item_cnt_day values to 0.**","b0427965":"**Let's remove outliers first. I referred to this kernel for outlier removal: [https:\/\/www.kaggle.com\/karell\/xgb-baseline-advanced-feature-engineering](https:\/\/www.kaggle.com\/karell\/xgb-baseline-advanced-feature-engineering)**","16257f7b":"**Several shops are duplicates of each other (according to their names). Fix train and test set.\nWe add 40 to 39.**","e5a8c278":"# Modelling:","92c3d9d0":"**It looks like the item prices and items sold per day vary heavily by category. There are some spikes, and it shows a parabolic trend. If you look closely, you'll see the parabola.**","c5dfd7bc":"# Get all possible data:","6d8a4c6b":"**Now, sales from different shops and different items don't necessarily affect each other. We need to find a relation between them.**\n\n**Finding how different item categories affect the features is a good first step to understanding the data.**","d13c4197":"**The above graphs are just to see what the data looks like when plotted against each other.**\n\n**For example, the bottom most, extreme left graph plots date_block_num(x) against item_cnt_day(y). It shows a slightly linear relationship.**\n\n**Pairplots are basically scatterplot matrices. They save the time and effort of plotting each one against another.**","8fead7b3":"**We will be using XGBoost to predict item_price_month first, then sales_duration, and finally item_cnt_month.**","2f1b6bb9":"# Data Visualization and Analysis:","541aee03":"# Import required libraries and get data:","43a4977c":"**We don't want values more than 1 in our predictions. So, we round all predictions greater than 1 to 1.**","6e52979a":"**We will do the same thing for the train dataset.**\n\n**We need to get some common categories between them.**","e3014465":"# Removing Outliers:","ff321fe4":"# Downcasting: ","f8ab971e":"**Second groupby: To get our actual monthly estimates!**","02ac7b9d":"**By looking closely, we can see some linear relationship in the above two graphs.**\n\n**There are many spikes, but the general trend seems to be an increase.**","63bdf5b1":"# Preparing Submission:","be6c7409":"**When merging two dataframes of different levels, some column names will become tuples. Like this: (shop_id, )**\n\n**To solve this issue, we create the following function. It checks whether the column name is a string or not. If it's not a string, it changes it to a string.**","b1ef1a3b":"**We mustn't forget that we need to find a monthly estimate of the number of items sold.**\n\n**Let's find out the relations between item_cnt_day, item_price and date_block_num.**","c6638f1a":"**I'm going to use a memory reduction function that downcasts my dataframe columns.**\n\n**First things first! Don't forget to convert the date column to date format.**","b1000c9b":"**We only have two columns in our test set, shop_id and item_id.**\n\n**This is not enough for us to make predictions. So, we need to put more features.**","d57b7258":"**There we go! We now have our actual monthly estimates. We need to predict item_cnt_month for the test set.**\n\n**But first merge with shop, items, and item_cats to get remaining features!**","4d71955e":"**We're going to do some group-bys to get our monthly sales.**\n\n**First groupby: To get the duration feature!**","f17d00f4":"# Feature Engineering:"}}