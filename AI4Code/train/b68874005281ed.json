{"cell_type":{"cb6f7324":"code","d0bbf829":"code","02f614d5":"code","6f69ff4f":"code","2ff57731":"code","cf177476":"code","810f6390":"code","2d027cd7":"code","b2121cd2":"code","52019fd5":"code","37e12dc1":"code","8f5aaff7":"code","2cd3db1a":"code","4db7c6c0":"code","064f04d5":"code","396ee2d0":"code","b3aac162":"code","dbb9b6f9":"code","85f6fc04":"code","a416ad9e":"code","8f6c0739":"code","5f51905c":"code","d77a9822":"code","174ffafd":"code","c46c05e3":"code","c36bdd00":"code","071d937a":"code","46543699":"code","6e2219f7":"code","bee903ea":"code","b534108c":"code","d8edbb5d":"code","866c6b52":"code","7f8393d7":"code","42ce48b7":"code","2f49c469":"code","537fd2d0":"code","66c60c85":"code","eb9cd02c":"code","45a7e001":"code","e84e2143":"code","3b43d3af":"code","bb593c4a":"code","5c54c8f8":"code","56ff102e":"code","d4e59a4a":"code","7cc98571":"code","c7bb2671":"code","43ad7e6d":"code","2f848105":"code","56bb330e":"code","604f76b8":"code","d6c3c079":"code","57ff3ddf":"code","29530cd6":"code","f607307c":"code","c467b7e0":"code","eeb8bf6b":"code","b39d62a7":"code","a643ab88":"code","25580e4b":"code","28e4adb8":"code","4201bae1":"code","187d8eb3":"code","1927ac03":"code","746ef2f5":"code","697d3241":"code","01205295":"code","9e69c160":"markdown","07e0bbfc":"markdown","3420bd6f":"markdown","aebbe8e4":"markdown","7dd460ac":"markdown","7d83e27c":"markdown","873765a6":"markdown","d2fa23b7":"markdown","83c517a6":"markdown","9a5abdff":"markdown","829ca86e":"markdown","75b20005":"markdown","835852c5":"markdown","14abecba":"markdown","cbfdae83":"markdown","66a7e060":"markdown","5611b1a5":"markdown","0f1e9947":"markdown","cf16f7f8":"markdown","b8e05982":"markdown","4bc0775c":"markdown","c7f1b35a":"markdown","f0cc350f":"markdown","2134031e":"markdown","530b35fa":"markdown","9758b752":"markdown","c48406af":"markdown","a95bdedb":"markdown","05b1255c":"markdown","335e584f":"markdown","4a3270f1":"markdown","a07db074":"markdown","466b556b":"markdown","1ce1e167":"markdown","65170da0":"markdown","4390dab5":"markdown","67c2b61a":"markdown","f616efb1":"markdown","c80c6819":"markdown","a701df1c":"markdown","7163d0df":"markdown","30ea25a1":"markdown","1073ae07":"markdown","599b34b1":"markdown","eccc441f":"markdown","f88bad13":"markdown","864b028b":"markdown"},"source":{"cb6f7324":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0bbf829":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=(8,6)\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')","02f614d5":"df=pd.read_csv('\/kaggle\/input\/home-loan-predictions\/Train_Loan_Home.csv')\ndf.head()","6f69ff4f":"df_test=pd.read_csv('\/kaggle\/input\/home-loan-predictions\/Test_Loan_Home.csv')\ndf_test.head()","2ff57731":"df.shape,df_test.shape","cf177476":"df.info()","810f6390":"df_test.info()","2d027cd7":"df.dtypes","b2121cd2":"df_test.dtypes","52019fd5":"df.isnull().sum()","37e12dc1":"df_test.isnull().sum()","8f5aaff7":"sns.heatmap(df.isnull(),yticklabels=False)","2cd3db1a":"#dropping the unnecessary column\ndf.drop('Loan_ID',axis=1,inplace=True)\ndf_test_id=df_test['Loan_ID']\ndf_test.drop('Loan_ID',axis=1,inplace=True)","4db7c6c0":"X=df.drop('Loan_Status',axis=1)\ny=df['Loan_Status']","064f04d5":"#separating all the categorical features and numerical features\ncategorical_features=[]\nnumerical_features=[]\nfor i in X.columns.tolist():\n    if X[i].dtype=='object':\n        categorical_features.append(i)\n    else:\n        numerical_features.append(i)","396ee2d0":"categorical_features","b3aac162":"numerical_features","dbb9b6f9":"df['Loan_Status'].value_counts()","85f6fc04":"df['Loan_Status'].value_counts(normalize=True).plot(kind='bar')\nplt.xlabel('Loan Status')","a416ad9e":"for i in categorical_features:\n    print('Feature: ',i)\n    print(X[i].value_counts(normalize=True))\n    X[i].value_counts(normalize=True).plot(kind='bar')\n    plt.xlabel(i)\n    plt.show()\n    print('\\n')","8f6c0739":"sns.distplot(X['ApplicantIncome'],bins=50,kde=True)\nplt.show()\nX['ApplicantIncome'].plot(kind='box')\nplt.show()","5f51905c":"X.boxplot(column='ApplicantIncome',by='Education')","d77a9822":"sns.distplot(X['CoapplicantIncome'],kde=True)\nplt.show()\nX.boxplot(column='CoapplicantIncome')\nplt.show()","174ffafd":"sns.distplot(X['LoanAmount'],kde=True)\nplt.show()\nX['LoanAmount'].plot(kind='box')\nplt.show()","c46c05e3":"X['Loan_Amount_Term'].value_counts(normalize=True).plot(kind='bar')\nplt.xlabel('Loan Amount Term')","c36bdd00":"gender=pd.crosstab(df['Gender'],df['Loan_Status'])\ngender.div(gender.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()\nmarried=pd.crosstab(df['Married'],df['Loan_Status'])\nmarried.div(married.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()\ndependents=pd.crosstab(df['Dependents'],df['Loan_Status'])\ndependents.div(dependents.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()\neducation=pd.crosstab(df['Education'],df['Loan_Status'])\neducation.div(education.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()\nself_employed=pd.crosstab(df['Self_Employed'],df['Loan_Status'])\nself_employed.div(self_employed.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()\nproperty_area=pd.crosstab(df['Property_Area'],df['Loan_Status'])\nproperty_area.div(property_area.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()\ncredit_history=pd.crosstab(df['Credit_History'],df['Loan_Status'])\ncredit_history.div(credit_history.sum(axis=1),axis=0).plot(kind='bar',stacked=True)\nplt.show()","071d937a":"print('Missing Values: ')\ndf.isnull().sum()","46543699":"X[\"Gender\"].replace(np.nan,X['Gender'].mode()[0],inplace=True)\nX['Married'].replace(np.nan,X['Married'].mode()[0],inplace=True)\nX['Dependents'].replace(np.nan,X['Dependents'].mode()[0],inplace=True)\nX['Self_Employed'].replace(np.nan,X['Self_Employed'].mode()[0],inplace=True)\nX['Loan_Amount_Term'].replace(np.nan,X['Loan_Amount_Term'].mode()[0],inplace=True)\nX['Credit_History'].replace(np.nan,X['Credit_History'].mode()[0],inplace=True)","6e2219f7":"X['LoanAmount'].replace(np.nan,X['LoanAmount'].median(),inplace=True)","bee903ea":"X.isnull().sum()","b534108c":"X['LoanAmount_Log']=np.log(X['LoanAmount'])\nsns.distplot(X['LoanAmount_Log'],bins=50)","d8edbb5d":"#dropping the Loan Amount feature\nX.drop('LoanAmount',axis=1,inplace=True)","866c6b52":"categorical_features=['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Amount_Term',\n 'Credit_History']\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in categorical_features:\n    X[i]=le.fit_transform(X[i])","7f8393d7":"X.head()","42ce48b7":"plt.figure(figsize=(10,8))\nsns.heatmap(X.corr(),annot=True,fmt='.2f',cmap='Blues')","2f49c469":"from sklearn.ensemble import ExtraTreesClassifier\netc=ExtraTreesClassifier(n_estimators=100)\netc.fit(X,y)\nvalues=pd.Series(etc.feature_importances_)","537fd2d0":"values.index=X.columns\nvalues.sort_values(ascending=False).plot(kind='barh')","66c60c85":"from sklearn.preprocessing import StandardScaler\nX_norm=StandardScaler().fit_transform(X)\nX_norm[0:5]","eb9cd02c":"df_test.head()","45a7e001":"df_test.rename(columns={'ApplicantIncomeMonthly':'ApplicantIncome','CoapplicantIncomeMonthly':'CoapplicantIncome','LoanAmountThousands':'LoanAmount','Loan_Amount_Term_Months':'Loan_Amount_Term'},inplace=True)","e84e2143":"df_test.isnull().sum()","3b43d3af":"df_test['Gender'].replace(np.nan,df_test['Gender'].mode()[0],inplace=True)\ndf_test['Dependents'].replace(np.nan,df_test['Dependents'].mode()[0],inplace=True)\ndf_test['Self_Employed'].replace(np.nan,df_test['Self_Employed'].mode()[0],inplace=True)\ndf_test['LoanAmount'].replace(np.nan,df_test['LoanAmount'].median(),inplace=True)\ndf_test['Loan_Amount_Term'].replace(np.nan,df_test['Loan_Amount_Term'].mode()[0],inplace=True)\ndf_test['Credit_History'].replace(np.nan,df_test['Credit_History'].mode()[0],inplace=True)","bb593c4a":"df_test.isnull().sum()","5c54c8f8":"for i in categorical_features:\n    df_test[i]=LabelEncoder().fit_transform(df_test[i])\ndf_test.head()","56ff102e":"df_test['LoanAmount_log']=np.log(df_test['LoanAmount'])\ndf_test.drop('LoanAmount',axis=1,inplace=True)\ndf_test.head()","d4e59a4a":"df_test_norm=StandardScaler().fit_transform(df_test)\ndf_test_norm[0:5]","7cc98571":"from sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val=train_test_split(X_norm,y,test_size=0.25,random_state=42)\nx_train.shape,y_train.shape,x_val.shape,y_val.shape","c7bb2671":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,precision_score,recall_score,f1_score","43ad7e6d":"accuracy=[]\nf1=[]\nmodel=[]","2f848105":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\ny_hat=lr.predict(x_val)","56bb330e":"accuracy.append(np.round(accuracy_score(y_val,y_hat),2))\nf1.append(np.round(f1_score(y_val,y_hat,average='weighted'),2))\nmodel.append('Logistic Regression')","604f76b8":"sns.heatmap(confusion_matrix(y_val,y_hat),annot=True,fmt='.0f')","d6c3c079":"from sklearn.svm import SVC\nsvc=SVC()\nsvc.fit(x_train,y_train)\ny_hat=svc.predict(x_val)","57ff3ddf":"accuracy.append(np.round(accuracy_score(y_val,y_hat),2))\nf1.append(np.round(f1_score(y_val,y_hat,average='weighted'),2))\nmodel.append('SVC')","29530cd6":"sns.heatmap(confusion_matrix(y_val,y_hat),annot=True,fmt='.0f')","f607307c":"from sklearn.tree import DecisionTreeClassifier\ndst=DecisionTreeClassifier(criterion='entropy')\ndst.fit(x_train,y_train)\ny_hat=dst.predict(x_val)","c467b7e0":"accuracy.append(np.round(accuracy_score(y_val,y_hat),2))\nf1.append(np.round(f1_score(y_val,y_hat,average='weighted'),2))\nmodel.append('Decision Tree')","eeb8bf6b":"sns.heatmap(confusion_matrix(y_val,y_hat),annot=True,fmt='.0f')","b39d62a7":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=100)\nrfc.fit(x_train,y_train)\ny_hat=rfc.predict(x_val)","a643ab88":"accuracy.append(np.round(accuracy_score(y_val,y_hat),2))\nf1.append(np.round(f1_score(y_val,y_hat,average='weighted'),2))\nmodel.append('Random Forest')","25580e4b":"sns.heatmap(confusion_matrix(y_val,y_hat),annot=True,fmt='.0f')","28e4adb8":"from xgboost import XGBClassifier\nxgb=XGBClassifier(n_estimators=100,max_depth=3)\nxgb.fit(x_train,y_train)\ny_hat=xgb.predict(x_val)","4201bae1":"accuracy.append(np.round(accuracy_score(y_val,y_hat),2))\nf1.append(np.round(f1_score(y_val,y_hat,average='weighted'),2))\nmodel.append('Xgboost')","187d8eb3":"sns.heatmap(confusion_matrix(y_val,y_hat),annot=True,fmt='.0f')","1927ac03":"model","746ef2f5":"output=pd.DataFrame({'Model':model,\n                    'Accuracy':accuracy,\n                    'F1 score':f1})\noutput","697d3241":"y_pred=svc.predict(df_test_norm)\ny_pred[0:5]","01205295":"result=pd.DataFrame({'LoanID':df_test_id,\n                    'Loan_Status':y_pred})\nresult.head()","9e69c160":"ApplicantIncomeMonthly,CoapplicantIncomeMonthly,LoanAmountThousands,Loan_Amount_Term_Months these four features are not having the same name in the training data. So renaming the features with the same name in the training dataset","07e0bbfc":"The training dataset has 614 rows and 13 columns and the test dataset has 367 rows and 12 columns","3420bd6f":"# Handling Missing Values","aebbe8e4":"Around 85% loans are having 360 as their loan amount term","7dd460ac":"**Heatmap to identify the features having null values**","7d83e27c":"# Training and Validation Data Split","873765a6":"From the above visualization it can be inferred that:\n1. The Applicant Income Feature does not follow normal distribution, Most of the Income ranges from 0-20,000\n2. The boxplot surely indicates that the feature is having many Outliers. It is due to different income labels of different customers.\n  We can group the income of the customers with their education label","d2fa23b7":"# Converting all the categorical varibales into numerical varibales","83c517a6":"**Checking the Data types of each features**","9a5abdff":"# Decision Tree Classifier","829ca86e":"From the above visualization this following points can be inferred:\n1. The percentage of female and male customers,getting loan approval are same.\n2. Married customers are more likely to get the loan approval.\n3. Customers who have 1 and 3+ dependents are having more chance to get the loan approval.\n4. Graduate customers are more likely to get the loan approval.\n5. The percentage of self employed and not employed customers,getting loan approval are same.\n6. Customers from semi urban area are having more chance to get loan approval.\n7. Customers with credit score 1 are more likely to get loan approval.","75b20005":"As the Loan Amount feature has many outliers, we will replace the null values with median ","835852c5":"**Checking for null values**","14abecba":"# Exploratory Data Analysis","cbfdae83":"# Feature Selection","66a7e060":"# Random Forest","5611b1a5":"The log transformation of the Loan Amount feature follows normal distribution","0f1e9947":"# Feature Importance","cf16f7f8":"Most of the Customers who are graduate is having very high incomes","b8e05982":"**All the null values has been removed**","4bc0775c":"# Feature Engineering","c7f1b35a":"**The features Loan_Amount_Term and Credit_History are having categorical values, so we will consider them in the categorical features list**","f0cc350f":"# Importing Necessary Libraries","2134031e":"# Support Vector Machine","530b35fa":"# Univariate Analysis","9758b752":"**Numerical Features**","c48406af":"As the Loan Amount feature follows right skewed distribution, we will perform a log transformation to get the normal distribution, as the model will give a better performance on the normal distribution\nThe same log transformation will be applied on the test data","a95bdedb":"**Information about the training dataset**","05b1255c":"1. This feature is having a Right Skewed Distribution and most of the CoapplicantIncome ranges from 0-6000\n2. Also the feature is having few outliers","335e584f":"**Among 614 customers The Loan was approved for 422 Customers (~70%)**","4a3270f1":"From the above diagram it is clear that the Credit History is the most important feature in the dataset that means customers who have paid their previous debts have higher chance to get loan approval","a07db074":"# Outlier Treatment","466b556b":"From The above visualizations it can be inferred that:\n1. Around 81% customers are Male.\n2. 65% customers are Married.\n3. Most of the customers don't have any dependents.\n4. 78% customers are Graduate.\n5. Only 14% customers are self employed.\n6. 38% customers are from semiurban area, 33% are from urban area, 29% are from rural area","1ce1e167":"# Understanding the Data","65170da0":"The following models will the applied on the training data:\n1. Logistic Regression\n2. Support Vector Classifier\n3. Decision Tree Classifier\n4. Random Forest Classifier\n5. Xgboost Classifier","4390dab5":"# Reading the data","67c2b61a":"To handle the null values:\n1. For the Categorical Features the null values will be replaced by the mode value.\n2. For the Numerical features the null values will be replaced by the mean or by the median value.","f616efb1":"# Correlation Matrix","c80c6819":"# Model Creation","a701df1c":"**From the above dataframe it is clear that Support Vector Classifier is giving the best accuray score and f1 score. So we will apply this model to our test data.**","7163d0df":"# Applying all the changes in the Test dataset","30ea25a1":"LoanAmount feature follows Normal Distribution but it is having many Outliers","1073ae07":"# Logistic Regression","599b34b1":"The following steps to be followed:\n1. Apply all the changes in test dataset\n2. separate the training dataset into train data and validation data\n3. Apply different Machine Learning Classification Algorithm to train the data\n4. Check the performance matrix based on the output of the Validation dataset\n5. Apply the best Model into the test data","eccc441f":"# Bivariate Analysis","f88bad13":"# Model Creation","864b028b":"# Normalization of the Dataset"}}