{"cell_type":{"5461b96e":"code","aa26aa5e":"code","b45948f6":"code","27c4c64f":"code","a317d2cb":"code","6384c7bd":"code","3ab6630d":"code","4ef9f9e5":"code","85aa1cd6":"code","2a7eb5d9":"code","b912ef49":"code","4b337dfa":"code","bac68497":"code","71138fdd":"code","c21d62d4":"code","67b2700f":"code","b383a62d":"code","2dfb807e":"code","dead80ee":"code","1c07f613":"code","0c9fd9b2":"code","b4bea402":"code","85dc9c31":"code","9c3810fe":"code","d70f78d7":"code","5434619a":"code","811104eb":"code","e141ff02":"code","72e11330":"code","6be970e5":"code","55ec5ad1":"code","98dbfb35":"code","b853caa1":"code","552420ed":"code","59a850e4":"code","0106e7bf":"code","9aa9c363":"code","be7b5e04":"code","66dcf839":"code","9f928d38":"code","18d72144":"code","5e9e6df5":"code","3fd1f5ef":"code","20b1cffe":"markdown","708568ea":"markdown","552b88d1":"markdown","27ecc645":"markdown","a287c1b7":"markdown","cc2b4160":"markdown","65f3798c":"markdown","08eccdec":"markdown","890ea33a":"markdown","6eb32c7f":"markdown","9762c7ff":"markdown","29da111c":"markdown","cb6b29da":"markdown","27294df4":"markdown","4a144f64":"markdown","eb6925d8":"markdown","45771431":"markdown","dcfeb5cf":"markdown","3ce99f6b":"markdown","2c61c771":"markdown","b977f42f":"markdown","4f6131d3":"markdown","b7d1dabf":"markdown","eedc61a7":"markdown"},"source":{"5461b96e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa26aa5e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math","b45948f6":"sample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n#Creating a copy of the train and test datasets\nc_test  = test.copy()\nc_train  = train.copy()","27c4c64f":"c_train.head()","a317d2cb":"c_train['train']  = 1\nc_test['train']  = 0\ndf = pd.concat([c_train, c_test], axis=0,sort=False)","6384c7bd":"#Percentage of NAN Values \nNAN = [(c, df[c].isna().mean()*100) for c in df]\nNAN = pd.DataFrame(NAN, columns=[\"column_name\", \"percentage\"])","3ab6630d":"NAN = NAN[NAN.percentage > 50]\nNAN.sort_values(\"percentage\", ascending=False)","4ef9f9e5":"#Drop PoolQC, MiscFeature, Alley and Fence features\ndf = df.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)","85aa1cd6":"object_columns_df = df.select_dtypes(include=['object'])\nnumerical_columns_df =df.select_dtypes(exclude=['object'])","2a7eb5d9":"object_columns_df.dtypes","b912ef49":"numerical_columns_df.dtypes","4b337dfa":"#Number of null values in each feature\nnull_counts = object_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","bac68497":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\nobject_columns_df[columns_None]= object_columns_df[columns_None].fillna('None')","71138fdd":"columns_with_lowNA = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n#fill missing values for each column (using its own most frequent value)\nobject_columns_df[columns_with_lowNA] = object_columns_df[columns_with_lowNA].fillna(object_columns_df.mode().iloc[0])","c21d62d4":"#Number of null values in each feature\nnull_counts = numerical_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","67b2700f":"print((numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt']).median())\nprint(numerical_columns_df[\"LotFrontage\"].median())","b383a62d":"numerical_columns_df['GarageYrBlt'] = numerical_columns_df['GarageYrBlt'].fillna(numerical_columns_df['YrSold']-35)\nnumerical_columns_df['LotFrontage'] = numerical_columns_df['LotFrontage'].fillna(68)","2dfb807e":"numerical_columns_df= numerical_columns_df.fillna(0)","dead80ee":"object_columns_df['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Utilities'].value_counts()","1c07f613":"object_columns_df['Street'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Street'].value_counts() ","0c9fd9b2":"object_columns_df['Condition2'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Condition2'].value_counts() ","b4bea402":"object_columns_df['RoofMatl'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['RoofMatl'].value_counts() ","85dc9c31":"object_columns_df['Heating'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Heating'].value_counts() #======> Drop feature one Type","9c3810fe":"object_columns_df = object_columns_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)","d70f78d7":"numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()","5434619a":"Negatif = numerical_columns_df[numerical_columns_df['Age_House'] < 0]\nNegatif","811104eb":"numerical_columns_df.loc[numerical_columns_df['YrSold'] < numerical_columns_df['YearBuilt'],'YrSold' ] = 2009\nnumerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()","e141ff02":"numerical_columns_df['TotalBsmtBath'] = numerical_columns_df['BsmtFullBath'] + numerical_columns_df['BsmtFullBath']*0.5\nnumerical_columns_df['TotalBath'] = numerical_columns_df['FullBath'] + numerical_columns_df['HalfBath']*0.5 \nnumerical_columns_df['TotalSA']=numerical_columns_df['TotalBsmtSF'] + numerical_columns_df['1stFlrSF'] + numerical_columns_df['2ndFlrSF']","72e11330":"numerical_columns_df.head()","6be970e5":"bin_map  = {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,\"None\" : 0,\n            \"No\" : 2, \"Mn\" : 2, \"Av\": 3,\"Gd\" : 4,\"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6\n            }\nobject_columns_df['ExterQual'] = object_columns_df['ExterQual'].map(bin_map)\nobject_columns_df['ExterCond'] = object_columns_df['ExterCond'].map(bin_map)\nobject_columns_df['BsmtCond'] = object_columns_df['BsmtCond'].map(bin_map)\nobject_columns_df['BsmtQual'] = object_columns_df['BsmtQual'].map(bin_map)\nobject_columns_df['HeatingQC'] = object_columns_df['HeatingQC'].map(bin_map)\nobject_columns_df['KitchenQual'] = object_columns_df['KitchenQual'].map(bin_map)\nobject_columns_df['FireplaceQu'] = object_columns_df['FireplaceQu'].map(bin_map)\nobject_columns_df['GarageQual'] = object_columns_df['GarageQual'].map(bin_map)\nobject_columns_df['GarageCond'] = object_columns_df['GarageCond'].map(bin_map)\nobject_columns_df['CentralAir'] = object_columns_df['CentralAir'].map(bin_map)\nobject_columns_df['LotShape'] = object_columns_df['LotShape'].map(bin_map)\nobject_columns_df['BsmtExposure'] = object_columns_df['BsmtExposure'].map(bin_map)\nobject_columns_df['BsmtFinType1'] = object_columns_df['BsmtFinType1'].map(bin_map)\nobject_columns_df['BsmtFinType2'] = object_columns_df['BsmtFinType2'].map(bin_map)\n\nPavedDrive =   {\"N\" : 0, \"P\" : 1, \"Y\" : 2}\nobject_columns_df['PavedDrive'] = object_columns_df['PavedDrive'].map(PavedDrive)","55ec5ad1":"#Select categorical features\nrest_object_columns = object_columns_df.select_dtypes(include=['object'])\n#Using One hot encoder\nobject_columns_df = pd.get_dummies(object_columns_df, columns=rest_object_columns.columns) ","98dbfb35":"object_columns_df.head()","b853caa1":"df_final = pd.concat([object_columns_df, numerical_columns_df], axis=1,sort=False)\ndf_final.head()","552420ed":"df_final = df_final.drop(['Id',],axis=1)\n\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train',],axis=1)\n\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['SalePrice'],axis=1)\ndf_test = df_test.drop(['train',],axis=1)","59a850e4":"target= df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)","0106e7bf":"x_train,x_test,y_train,y_test = train_test_split(df_train,target,test_size=0.33,random_state=0)","9aa9c363":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\n\nlgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=12000, \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, \n                                       )","be7b5e04":"#Fitting\nxgb.fit(x_train, y_train)\nlgbm.fit(x_train, y_train,eval_metric='rmse')","66dcf839":"predict1 = xgb.predict(x_test)\npredict = lgbm.predict(x_test)","9f928d38":"print('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict1))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict))))","18d72144":"xgb.fit(df_train, target)\nlgbm.fit(df_train, target,eval_metric='rmse')","5e9e6df5":"predict4 = lgbm.predict(df_test)\npredict3 = xgb.predict(df_test)\npredict_y = ( predict3*0.45 + predict4 * 0.55)","3fd1f5ef":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": predict_y\n    })\nsubmission.to_csv('submission.csv', index=False)","20b1cffe":"We can drop PoolQC, MiscFeature, Alley and Fence features because they have more than 80% of missing values.","708568ea":"Deeling with categorical feature","552b88d1":"** Now we have a clean categorical features**\nIn the next step we will deal with the **numerical** features black","27ecc645":"* TotalBsmtBath : Sum of : BsmtFullBath and 1\/2 BsmtHalfBath\n\n* TotalBath : Sum of : FullBath and 1\/2 HalfBath\n\n* TotalSA : Sum of : 1stFlrSF and 2ndFlrSF and basement area <\/font>","a287c1b7":"# Modeling ","cc2b4160":"Fill the rest of columns with 0","65f3798c":"Now the next step is to encode categorical features\nOrdinal categories features - Mapping from 0 to N","08eccdec":"1. We have 81 columns.\n\nOur target variable is SalePrice.\nId is just an index that we can drop but we will need it in the final submission.\nWe have many missing values <\/font>\n*** * * * we have 79 features in our dataset.**\n\n**Concat Train and Test datasets**","890ea33a":"We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Take a look in the data description).\nWe will fill the rest of features with th most frequent value (using its own most frequent value).","6eb32c7f":"Data preprocessing \n**Calculating the percentage of missing values of each feature**","9762c7ff":"1. Preprocessing\nThe main goal here is to get rid of (very) spurious data points and prepare the dataset for learning.\n\nThis is a very delicate process: go to far, and you will be introducing bias in your data. Go to short, and you will be introducing rubbish in your learning process.\n\nA. The dataset","29da111c":"Concat Categorical (after encoding) and numerical features","cb6b29da":"**Fill GarageYrBlt and LotFrontage**\n**Fill the rest of columns with 0**","27294df4":"**Features with more than 50% of missing values.**","4a144f64":"Now we will select numerical and categorical features","eb6925d8":"Now we will create some new features","45771431":"Separate Train and Targets","dcfeb5cf":"We finally end up with a clean dataset\nAfter making some plots we found that we have some colums with low variance so we decide to delete them","3ce99f6b":"Like we see here tha the minimun is -1 ???\nIt is strange to find that the house was sold in 2007 before the YearRemodAdd 2009.\n\nSo we decide to change the year of sold to 2009","2c61c771":"Numerical Features :","b977f42f":"# Fitting With all the dataset","4f6131d3":"Will we use One hot encoder to encode the rest of categorical features","b7d1dabf":"Categorical Features :","eedc61a7":"**So we will fill the year with 1979 and the Lot frontage with 68**"}}