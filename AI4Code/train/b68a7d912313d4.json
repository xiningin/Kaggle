{"cell_type":{"f1dafa79":"code","e1e39bf5":"code","6922dadb":"code","77fbf6b9":"code","d3bbacf7":"code","707207a4":"code","518a8532":"code","f6ee1a0b":"code","2e0400f2":"code","783beabb":"code","0d7a683b":"code","df5ace61":"code","b425b616":"code","2d46da3b":"code","766f06ee":"code","ac04d332":"code","f73c711c":"code","e50054de":"code","bb8f4ae9":"code","10692def":"code","91cd81c4":"code","42ce5b7e":"code","ff114898":"code","a3449470":"code","d0d2d2f8":"code","95a71dab":"code","fc71fd0a":"code","3c2996d6":"code","183908d8":"code","19b9c581":"code","e6992563":"code","eb06a747":"code","29b96d0d":"code","1632f854":"code","d901573e":"code","bbc1be12":"code","82784656":"code","395ed723":"code","732dd289":"code","63d6fc5d":"code","965360b9":"code","d3bb49fe":"code","8ec6dd10":"code","6eac2730":"code","ba970eb2":"code","c0cc5e34":"markdown","8e2efab7":"markdown","c88990f9":"markdown","61541106":"markdown","acd70ec8":"markdown","ce693363":"markdown","06e7f83c":"markdown","ecbe930a":"markdown","1be31c00":"markdown","f18bfaa8":"markdown","1fa9f60e":"markdown","fe496cf5":"markdown","20d64f4a":"markdown","e0062d9a":"markdown","b17077e1":"markdown","ec6f05ea":"markdown"},"source":{"f1dafa79":"to_replace = { \"ain't\": \"am not \/ are not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is\",\n\"i'd\": \"I had \/ I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall \/ I will\",\n\"i'll've\": \"I shall have \/ I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","e1e39bf5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom emoji import demojize\nfrom itertools import chain\nfrom collections import Counter\nfrom nltk.stem import PorterStemmer","6922dadb":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC","77fbf6b9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d3bbacf7":"loc = '\/kaggle\/input\/vaccinetweets\/tweets.xlsx'\n#loc = '.\/old_tweets.xlsx'","707207a4":"try:\n    df = pd.read_excel(loc, sheet_name='Sheet1')\nexcept FileNotFoundError:\n    print(\"EXCEL FILE NOT FOUND\")","518a8532":"print(\"Column headings:\")\nprint(df.columns)","f6ee1a0b":"del df[\"Unnamed: 0\"]\ndf.head()","2e0400f2":"# Given all the tweets were not labeled, we only pick till they are\nlabeled_till = (np.where(np.isnan(df['Negative']))[0][0] - 1)\nprint(labeled_till+1)","783beabb":"df = df.loc[0:labeled_till, :]\n\nTweets   = df[\"Tweets\"]\nPositive = df[\"Positive\"]\nNegative = df[\"Negative\"]\nNeutral  = df[\"Neutral\"]","0d7a683b":"print(\"Negative\" , Negative.values.sum())\nprint(\"Positive\" , Positive.values.sum())\nprint(\"Neutral \" , Neutral.values.sum())\nNegative.values.sum() + Positive.values.sum() + Neutral.values.sum()","df5ace61":"def processTweet(tweets):\n    \"\"\"\n    Expects tweets to be a pandas series\n    \u2022 Lower-casing\n\n    \u2022 Normalizing URLs\n\n    \u2022 Normalizing Tags and email addresses\n\n    \u2022 Normalizing Numbers\n\n    \u2022 Normalizing Dollars\n\n    \u2022 Word Stemming\n    \n    \u2022 Normalize punctuation \n    \n    \u2022 Expand Contractions \n    \n    \u2022 Demojize\n    \n    \u2022 Remove punctuation\n    \n    (#TODO) Other ideas\n    \u2022 Negation. Optimization concept.\n    \u2022 Removal of stop-words and alphanumberic\n    \"\"\"\n    \n    # Lower case text\n    tweets = tweets.str.lower()\n    \n    # HTML tags\n    tweets = tweets.str.replace(r\"<[^<>]+>\", \" \")\n\n    # Account Tag @theFakeDonalDTrump \n    tweets = tweets.str.replace(r\"@[^\\s]+\", 'idaddr')\n    \n    # Email address\n    tweets = tweets.str.replace(r\"[^\\s]+@[^\\s]+\", 'emailaddr')\n    \n    # Handle URLS\n    # Look for strings starting with http:\/\/ or https:\/\/\n    tweets = tweets.str.replace(r\"(http|https):\/\/[^\\s]*\", 'httpaddr')\n    \n    # Handle Numbers\n    # Look for one or more characters between 0-9\n    tweets = tweets.str.replace(r\"[0-9]+\", 'number')\n    \n    # Handle $ sign\n    tweets = tweets.str.replace(r\"[$]+\", 'dollar')\n    \n    # Normalize punctuation\n    transl_table = dict( [ (ord(x), ord(y)) for x,y in zip( u\"\u2018\u2019\u00b4\u201c\u201d\u2013-\",  u\"'''\\\"\\\"--\") ] ) \n    tweets = tweets.apply(lambda a: a.translate(transl_table))\n    \n    # Expand Contractions\n    tweets = tweets.apply(lambda string: \" \".join([to_replace[i] if i in to_replace.keys() else i for i in string.split()]))\n    \n    # Demojize text\n    tweets = tweets.apply(demojize)\n    \n    # Handle punctuation\n    tweets = tweets.str.replace(r\"[^\\w]+\", ' ')\n    \n    # Stem\n    stemmer = PorterStemmer()\n    tweets  = tweets.apply(lambda a: list(map(stemmer.stem,a.split())))\n    \n    return tweets","b425b616":"tweets = processTweet(Tweets)","2d46da3b":"target   = df.drop('Tweets', axis = 1)\n\n#Reverse one-hot notation  \n\ntarget   = target.multiply([1,2,3]).values\ntarget   = np.concatenate(target)\ntarget   = target[target!= 0]","766f06ee":"#target = Negative","ac04d332":"X_trainO, X_testO, y_train, y_test = train_test_split(tweets, target, test_size = 0.3, random_state = 42,\n                                                    stratify = target)                                     ","f73c711c":"allWords = chain.from_iterable(X_trainO) #tweets","e50054de":"words = Counter(allWords)","bb8f4ae9":"words.most_common(20) #Check tweet_vocab.csv for complete list","10692def":"feature_vector = pd.Series(sorted(words, key=words.get, reverse=True)[:200])","91cd81c4":"X_train = X_trainO.apply(lambda tweet: feature_vector.isin((tweet)))\nX_test  = X_testO.apply(lambda tweet: feature_vector.isin((tweet)))","42ce5b7e":"X_train[:4]","ff114898":"tmp_featured_tweets = tweets.apply(lambda tweet: feature_vector.isin((tweet)))","a3449470":"tmp_golden_labels = df.drop('Tweets', axis = 1)","d0d2d2f8":"pd.concat([tmp_featured_tweets, tmp_golden_labels], axis = 1).to_csv('tweet_sentiment.csv')","95a71dab":"feature_vector.to_csv('tweet_vocab.csv')","fc71fd0a":"#Initializing a SVM model \nsvm = LinearSVC()","3c2996d6":"#Fitting the model to the training data\n\nsvm.fit(X_train, y_train)","183908d8":"#Extracting the accuracy score from the training data\n\nsvm.score(X_train, y_train)","19b9c581":"#Extracting the accuracy score from the training data\n\nsvm.score(X_test, y_test)","e6992563":"actual = y_test\npredicted = svm.predict(X_test)\nprint(classification_report(actual, predicted) )","eb06a747":"clff = SVC(kernel='linear') #multi_class='crammer_singer'\nclff.fit(X_train, y_train)\nclff.score(X_test, y_test)","29b96d0d":"clfr = SVC(kernel='poly')\nclfr.fit(X_train, y_train)\nclfr.score(X_test, y_test)","1632f854":"clff = SVC(kernel='rbf')\nclff.fit(X_train, y_train)\nclff.score(X_test, y_test)","d901573e":"training_scores = []\ntesting_scores = []\n\nparam_list = [0.00001, 0.0001, 0.001, 0.01, 0.1, 10, 100, 1000]\n\n# Evaluate the training and test classification errors for each value of the parameter\n\nfor param in param_list:\n    \n    # Create SVM object and fit\n    \n    svm = LinearSVC(C = param, random_state = 42)\n    #svm = SVC(kernel='rbf', C = param, random_state = 42)\n    svm.fit(X_train, y_train)\n    \n    # Evaluate the accuracy scores and append to lists\n    \n    training_scores.append(svm.score(X_train, y_train) )\n    testing_scores.append(svm.score(X_test, y_test) )\n    \n# Plot results\n\nplt.semilogx(param_list, training_scores, param_list, testing_scores)\nplt.legend((\"train\", \"test\"))\nplt.ylabel('Accuracy scores')\nplt.xlabel('C (Inverse regularization strength)')\nplt.show()","bbc1be12":"#Building the model \n\nsvm = LinearSVC(random_state = 50)\n\n#Using GridSearchCV to search for the best parameter\n\ngrid = GridSearchCV(svm, {'C':[0.00001, 0.0001, 0.001, 0.01, 0.1, 10, 100, 1000]}, cv=10)\ngrid.fit(X_train, y_train)\n\n# Print out the best parameter\n\nprint(\"The best value of the inverse regularization strength is:\", grid.best_params_)\nprint(\"The best score of the inverse regularization strength is:\", grid.best_score_)","82784656":"knn_classifier = KNeighborsClassifier(n_neighbors=1)","395ed723":"knn_classifier.fit(X_train, y_train)","732dd289":"knn_classifier.score(X_test, y_test)","63d6fc5d":"actual = y_test\npredicted = knn_classifier.predict(X_test)\nprint(classification_report(actual, predicted) )","965360b9":"#Initializing an NB classifier\n\nnb_classifier = GaussianNB()\n\n#Fitting the classifier into the training data\n\nnb_classifier.fit(X_train, y_train)\n\n#Extracting the accuracy score from the base classifier\n\nnb_classifier.score(X_test, y_test)","d3bb49fe":"actual = y_test\npredicted = nb_classifier.predict(X_test)\nprint(classification_report(actual, predicted) )","8ec6dd10":"X_trainNB = X_trainO.apply(lambda l: \" \".join(l))\nX_testNB  = X_testO.apply(lambda l: \" \".join(l))\n\nvect = CountVectorizer().fit(X_trainNB)\nX_train_vectorized = vect.transform(X_trainNB)\n\nclfrNB = MultinomialNB(alpha = 0.1)\nclfrNB.fit(X_train_vectorized, y_train)\n\nclfrNB.score(vect.transform(X_testNB), y_test)","6eac2730":"actual = y_test\npredicted = clfrNB.predict(vect.transform(X_testNB))\nprint(classification_report(actual, predicted) )","ba970eb2":"print(\"Text\", \"\".ljust(100), \"Prediction\", \"\".ljust(2), \"Actual\")\nfor i in range(10):#len(X_testNB)):\n    print(i, X_testNB.iloc[i])\n    print(\"\".ljust(110), predicted[i], \"\".ljust(5), actual[i])#.iloc[i])","c0cc5e34":"#### Export dataset\n","8e2efab7":"# Easy accuracy fix\nYou can get an accuracy of 82.5% if you combine the neutral and the positive class and turn the problem to classifying negative tweets from positive ones.\nFurther improvements would require more data","c88990f9":"# Other ideas\n+ Na\u00efve Bayes Classifier with Sentiment Lexicon: http:\/\/www.iaeng.org\/IJCS\/issues_v46\/issue_2\/IJCS_46_2_01.pdf\n+ Document Retrivial approach. Treat each tweet group as a document i.e. document of positive tweets. Use tf-idf vector model and calculate centroid of all three documents and new tweet and evalute their cosine similarity.","61541106":"# Part 3\n## Make Feature Vector\nUsing the corpus, most common words are used to make a feature vector\nEach tweet is then converted into a feature vector\n\n\n###### Concatinating strings in series","acd70ec8":"### References\nPandas Excel: https:\/\/pythonspot.com\/read-excel-with-pandas\/\n\nPanda Summary: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/getting_started\/10min.html\n\nContractions: https:\/\/stackoverflow.com\/questions\/43018030\/replace-apostrophe-short-words-in-python\n\nNaive Bayes Summary: https:\/\/towardsdatascience.com\/algorithms-for-text-classification-part-1-naive-bayes-3ff1d116fdd8\n\nOther articles: \n* https:\/\/www.geeksforgeeks.org\/multiclass-classification-using-scikit-learn\/\n* https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/\n* https:\/\/scikit-learn.org\/stable\/modules\/svm.html#svm-kernels","ce693363":"**Graphical hyper-parameter optimization**","06e7f83c":"### Other SVM models\n***Linear SVM use one vs rest where as the others use One vs One***","ecbe930a":"# Part 7\n2 layer Neural Network with softmax activation\n+ https:\/\/colab.research.google.com\/github\/MubashirullahD\/Tweets-Sentiment-Analysis\/blob\/master\/Sentiment_Analysis_of_Tweets_using_NN.ipynb","1be31c00":"# Part 2 \n## Preprocessing\n","f18bfaa8":"# Part 4\n**The SVM model**","1fa9f60e":"# Part 5\n**KNN**","fe496cf5":"# Part 6\n**Naive Bayes model** ","20d64f4a":"**Multinomial NB **","e0062d9a":"# Part 1\n## Collect the tweets","b17077e1":"**Hyper-parameter optimization using GridSearchCV**","ec6f05ea":"# Sentiment Analysis of Tweets related to Vaccine\nSentiment Analysis: The process of computationally identifying and categorising opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral (Lexico Dictionary, Oxford University Press)"}}