{"cell_type":{"dde71f81":"code","b23f7af7":"code","b314cc55":"code","288ab295":"code","af4d0bba":"code","026953ce":"code","b5fcfe3e":"code","05607bdb":"code","0a29d43a":"code","627ebc0e":"code","f303b260":"code","e87dd661":"markdown","455e6ce3":"markdown","8bcd98ba":"markdown","cd2e4e62":"markdown","1113dede":"markdown"},"source":{"dde71f81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b23f7af7":"# Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b314cc55":"# Importing data\ndf = pd.read_csv('..\/input\/unbalancedrisk\/Sample_Dataset.csv')\ndf.head()","288ab295":"# Quantity of items 0 and 1\ncnt = len(df['obj1'])\ncnt","af4d0bba":"# Record count equal to 0\ncnt_obj_0 = len(df[df['obj1']==0])\ncnt_obj_0","026953ce":"# Record count equal to 1\ncnt_obj_1 = len(df[df['obj1']==1])\ncnt_obj_1","b5fcfe3e":"# How much more is 0 compared to 1\nqtd_vezes_maior = '{:.2f}'.format(cnt_obj_0\/cnt_obj_1)\nprint('The quantity of 0 is '+str(qtd_vezes_maior)+' times greater than the quantity of 1')","05607bdb":"# Setting the background for the chart\nsns.set_style('darkgrid')\n\n# Building the count graph\nax = sns.countplot(df['obj1'], palette=\"Set3\")\n\n# Percentage of 0 and 1\nperc_0 = '{:.2f}%'.format(100*(cnt_obj_0\/cnt))\nperc_1 = '{:.2f}%'.format(100*(cnt_obj_1\/cnt))\n\n# Set the Xlabel\nplt.xlabel('Object')\n\n# Inserting the percentages on the chart\nplt.text(1,3000,\"% of 0: \"+perc_0+\"\\n% of 1: \"+perc_1)\n\n# Show the graph\nplt.show()","0a29d43a":"# Shuffle the data before creating the subsets\ndf = df.sample(frac=1)\n\n# Defining the size of the range we want from the data [0 and 1]\n# We need to use the loc function to access the dataset in the dataset from a starting and ending position\ndf_0 = df.loc[df['obj1']==0][:834]\ndf_1 = df.loc[df['obj1']==1]\n\n# Concatenating the subsets\ndf2 = pd.concat([df_0,df_1])\n\n# Shuffling the data\nnew_df = df2.sample(frac=1, random_state = 50)\nnew_df.head()","627ebc0e":"# Checking the new distribution of records equal to 0 and 1\nprint(new_df['obj1'].value_counts()\/len(new_df))","f303b260":"# Get the number of occurrences in our new dataset\nncount = len(new_df['obj1'])\n\n# Checking the balance on the chart\nax2 = sns.countplot(new_df['obj1'], palette=\"Set3\")\n\n# Make twin axis ax2\nax3 = ax2.twinx()\n\n# Count on right, frequency on left\nax3.yaxis.tick_left()\nax2.yaxis.tick_right()\n\n# The same thing to the labels\nax3.yaxis.set_label_position('left')\nax2.yaxis.set_label_position('right')\n\n# set the frequency label\nax3.set_ylabel('Frequency (%)')\n\n# Seting the % for each count [0, 1]\nfor p in ax2.patches:\n    x=p.get_bbox().get_points()[:,0] #Get all of x locations\n    y=p.get_bbox().get_points()[1,1] #Get all of y locations\n    ax2.annotate('{:.1f}%'.format(100.*y\/ncount), (x.mean(), y), \n           ha='center', va='bottom')\n    \n# Fix the frequency range to 0-100 and the count range to ncount+1000\nax3.set_ylim(0,100)\nax2.set_ylim(0,ncount)\n\n# turn the grid on ax2 off\nax3.grid(None)\n\n#Show the plot\nplt.show()","e87dd661":"<h1 align=\"center\"> Balancing the dataset <\/h1>\n\nThe previous analysis of the data in order to detect needs such as balancing, is fundamental to guarantee adjusted models and analyzes.\nWorking with unbalanced datasets can cause errors in our algorithms such as overfing, since the frequency of a given event is substantially higher than the other.\n\nIn our study we will assume that the variable \"obj1\" represents the occurrence of failure in a given production line.\n\nbeing:\n\n0 - Conforming product\n\n1 - Product with failure\n\nTherefore, if we wanted to study which variables are having the greatest impact on product failure, it is necessary to balance the data, even before dividing between training and test data.","455e6ce3":"We can see that the data that determines the occurrence of failures is about 4 times smaller.","8bcd98ba":"In the graph we can see more easily that the number of records equal to 0 is much greater than the number of records equal to 1","cd2e4e62":"> We will create subsets from the original dataset based on the same amount of failure and compliance events","1113dede":"All libraries that we will need to this work"}}