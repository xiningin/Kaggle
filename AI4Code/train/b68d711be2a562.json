{"cell_type":{"ddee2c78":"code","56bbd5c5":"code","2190a027":"code","1cfda0bd":"code","7e35eef3":"code","321715db":"code","e2617bf2":"code","e84167cc":"code","c3ee14cc":"code","b4913a72":"code","08576367":"code","5c509d7f":"code","f0a637f6":"code","718743df":"code","a230f5bd":"code","d0e4ceb6":"code","8ab8e98e":"code","a7ab61a8":"code","6252151c":"code","0ae870ad":"code","06f801bf":"code","420808b0":"code","3652adcf":"code","32649297":"code","b1c3e497":"code","b34b30da":"code","67942eb1":"code","02a14517":"code","bab21879":"code","f420b7bf":"code","d7e245a3":"code","9f7e8854":"code","2a17a14c":"code","96fd7b9a":"code","56cebfb5":"code","998debfa":"code","52f49909":"markdown","77e827f7":"markdown","4b88e60f":"markdown","6372d35c":"markdown","817123ab":"markdown","cae1895e":"markdown","5b8b488e":"markdown","05acc41a":"markdown","7ec078bd":"markdown","edcce426":"markdown","edcec845":"markdown","34896755":"markdown","2fadf779":"markdown","fe21d2f7":"markdown","8bc9be67":"markdown","a65c2fc6":"markdown","350fa784":"markdown","fe9d3fe2":"markdown","5b5bf680":"markdown","6f29fe3e":"markdown","c1bb39e0":"markdown","e5827e29":"markdown"},"source":{"ddee2c78":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","56bbd5c5":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","2190a027":"df.head()","1cfda0bd":"df.info()","7e35eef3":"df.shape","321715db":"df.describe()","e2617bf2":"print(df.dtypes.unique())","e84167cc":"100*(df.isnull().sum())\/(df.shape[0])","c3ee14cc":"df['Outcome'].value_counts()","b4913a72":"plt.figure(figsize=(9,9))\nplt.pie(x=[500,268], labels=[ 'Diabetic', 'Nondiabetic'], autopct='%1.0f%%',pctdistance=0.6,labeldistance=1.05,textprops={'fontsize':12},colors=['teal','limegreen'])\nplt.title('Number of Diabetic and Nondiabetic Patients',loc='center', fontsize=15)\nplt.show()","08576367":"df['Outcome']=df['Outcome'].apply(lambda x: 'Diabetic' if x==1 else 'Nondiabetic')","5c509d7f":"df.head(2)","f0a637f6":"sns.pairplot(df,hue='Outcome',palette='viridis')\nplt.show()","718743df":"plt.figure(figsize=(8,6))\ncorr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(corr, mask=mask, square=True,annot=True, cmap= 'plasma')\nplt.title('Correlation Between Features', fontsize=15)\nplt.show()","a230f5bd":"plt.figure(figsize=(15,15))\n\nplt.subplot(4,2,1)\nsns.distplot(df['Pregnancies'], color='green')\nplt.ylabel('Frequency', fontsize=12)\n\nplt.subplot(4,2,2)\nsns.distplot(df['Glucose'], color='blue')\nplt.yticks([])\n\nplt.subplot(4,2,3)\nsns.distplot(df['BloodPressure'], color='orange')\nplt.ylabel('Frequency', fontsize=12)\n\nplt.subplot(4,2,4)\nsns.distplot(df['SkinThickness'], color='cyan')\nplt.yticks([])\n\nplt.subplot(4,2,5)\nsns.distplot(df['Insulin'])\nplt.ylabel('Frequency', fontsize=12)\n\nplt.subplot(4,2,6)\nsns.distplot(df['BMI'], color='violet')\nplt.yticks([])\n\nplt.subplot(4,2,7)\nsns.distplot(df['DiabetesPedigreeFunction'], color='forestgreen')\nplt.ylabel('Frequency', fontsize=12)\n\nplt.subplot(4,2,8)\nsns.distplot(df['Age'], color='royalblue')\nplt.yticks([])\nplt.show()","d0e4ceb6":"ss=StandardScaler()","8ab8e98e":"ss.fit(df.drop(['Outcome'], axis=1))","a7ab61a8":"scaled=ss.transform(df.drop(['Outcome'], axis=1))","6252151c":"scaled_df=pd.DataFrame(data=scaled, columns=df.columns[:-1])","0ae870ad":"X=scaled_df\ny=df['Outcome']","06f801bf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","420808b0":"error_rate=[]\n\nfor n in range(1,40):\n    knc=KNeighborsClassifier(n_neighbors=n)\n    knc.fit(X_train, y_train)\n    prediction_knn=knc.predict(X_test)\n    error_rate.append(np.mean(prediction_knn!=y_test))\nprint(error_rate)","3652adcf":"plt.figure(figsize=(9,6))\nplt.plot(list(range(1,40)), error_rate,color='royalblue', marker='o', linewidth=2, markersize=12, markerfacecolor='deeppink', markeredgecolor='deeppink' )\nplt.xlabel('Number of Neighbors', fontsize=12)\nplt.ylabel('Error Rate', fontsize=12)\nplt.title('Error Rate Versus Number of Neighbors by Elbow Method', fontsize=15)\nplt.show()","32649297":"knc=KNeighborsClassifier(n_neighbors=15)\nknc.fit(X_train, y_train)\nprediction_knn=knc.predict(X_test)","b1c3e497":"print(confusion_matrix(y_test,prediction_knn))\nprint('\\n')\nprint(classification_report(y_test,prediction_knn))\nprint('Accuracy Score: ',round(accuracy_score(y_test,prediction_knn), ndigits=2))","b34b30da":"scaled_df.head()","67942eb1":"knc.predict([[0.639947,0.848324,0.149641,0.907270,-0.692891,0.204013,0.468492,1.425995]])","02a14517":"df['Outcome'].iloc[0]","bab21879":"knc.predict([[-0.844885,-1.123396,-0.160546,0.530902,-0.692891,-0.684422,-0.365061,-0.190672]])","f420b7bf":"df['Outcome'].iloc[3]","d7e245a3":"X=df.drop(['Outcome'], axis=1)\ny=df['Outcome']","9f7e8854":"X_trian, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","2a17a14c":"xgbc = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, gamma=0, subsample=0.5,colsample_bytree=1, max_depth=8)","96fd7b9a":"xgbc.fit(X_trian,y_train)","56cebfb5":"prediction_xgbc=xgbc.predict(X_test)","998debfa":"print(confusion_matrix(y_test,prediction_xgbc))\nprint('\\n')\nprint(classification_report(y_test,prediction_xgbc))\nprint('\\n')\nprint('Accuracy Score: ',round(accuracy_score(y_test,prediction_xgbc), ndigits=2))","52f49909":"The dataset is pretty much structured and does not have any NaN values. So we do not need Data Cleaning.","77e827f7":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><\/li><li><span><a href=\"#Importing-Libraries\" data-toc-modified-id=\"Importing-Libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Importing Libraries<\/a><\/span><\/li><li><span><a href=\"#Loading-Dataset\" data-toc-modified-id=\"Loading-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Loading Dataset<\/a><\/span><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><\/li><li><span><a href=\"#Prediction-of-Diabetes-Outcome\" data-toc-modified-id=\"Prediction-of-Diabetes-Outcome-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Prediction of Diabetes Outcome<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbors-Classifier\" data-toc-modified-id=\"K-Nearest-Neighbors-Classifier-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>K-Nearest Neighbors Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Standardizing-the-Data\" data-toc-modified-id=\"Standardizing-the-Data-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;<\/span>Standardizing the Data<\/a><\/span><\/li><li><span><a href=\"#Splitting-the-Data\" data-toc-modified-id=\"Splitting-the-Data-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;<\/span>Splitting the Data<\/a><\/span><\/li><li><span><a href=\"#Finding-the-Optimal-number-of-Neighbors-(K)\" data-toc-modified-id=\"Finding-the-Optimal-number-of-Neighbors-(K)-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;<\/span>Finding the Optimal number of Neighbors (K)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#XGBoost-Classifier\" data-toc-modified-id=\"XGBoost-Classifier-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>XGBoost Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-Data\" data-toc-modified-id=\"Splitting-the-Data-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;<\/span>Splitting the Data<\/a><\/span><\/li><li><span><a href=\"#Creating-the-Model\" data-toc-modified-id=\"Creating-the-Model-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;<\/span>Creating the Model<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","4b88e60f":"# ![2.jpg](attachment:2.jpg)","6372d35c":"#### Splitting the Data","817123ab":"#### Finding the Optimal number of Neighbors (K)","cae1895e":"#### Splitting the Data","5b8b488e":"## Importing Libraries","05acc41a":"We can see that for k=15, 17, 30 and 32 the error rate is minimum.","7ec078bd":"Diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases, and all patients here are females at least 21 years old of Pima Indian heritage. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. So, here we will perform exploratory data analysis and build two machine learning models with K-Nearest Neighbors Classifier and XGboost Classifier to accurately predict whether the patients in the dataset have diabetes or not.","edcce426":"## Exploratory Data Analysis","edcec845":"## Loading Dataset","34896755":"## Introduction","2fadf779":"-  Here we worked on Pima Indians Diabetes Dataset and implemented XGboost and K-Nearest neighbors Classifications to predict whether the patients have diabetes. \n-  The accuracy of 81% was obtained by K-Nearest Neighbors Classifier. \n-  The XGBoost Classifier performed pretty well with 82% accuracy in predicting the result of diabetes.","fe21d2f7":"### XGBoost Classifier","8bc9be67":"## Conclusion","a65c2fc6":"# <center> Pima Indians Diabetes Predictions with XGboost and KNN Classifiers <center>","350fa784":"#### Creating the Model","fe9d3fe2":"## Prediction of Diabetes Outcome","5b5bf680":"We can see that the XGBoost Classifier has 82% accuracy in predictiong the result of diabetes.","6f29fe3e":"The accuracy of 81% is obtained by KNN model. Now let's test the prediction of our model for given values.","c1bb39e0":"#### Standardizing the Data","e5827e29":"### K-Nearest Neighbors Classifier"}}