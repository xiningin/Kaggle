{"cell_type":{"e9afe538":"code","0c714fbb":"code","1418b63e":"code","0b27612d":"code","8dbb0022":"code","d88666d3":"code","adca39e6":"code","c86b273f":"code","a926911d":"code","2f69424a":"code","6459cb95":"code","8114334e":"code","a24cfa01":"code","78f41c88":"code","da83ebbc":"code","89593808":"code","ddcddc78":"code","f0e4a6cf":"code","f1898b75":"code","f983e3c2":"code","86691c35":"code","922a593a":"code","6b1cc21b":"code","0b4b248b":"code","37c60844":"code","c887bee7":"code","59cb43f0":"code","e19e9b31":"code","3e05b874":"code","36fe968d":"code","3fc36df1":"code","bfa3b7bb":"code","541aee5a":"code","fa8800c3":"code","bc28d7bc":"code","d350fdb1":"code","affe1c89":"code","b23871de":"code","c717609c":"code","0a92ca95":"code","da48d575":"code","c30a04dd":"code","8331a7a2":"code","fad12c5e":"code","a575c64e":"code","7112404d":"markdown","c5dcfa85":"markdown","42808b10":"markdown","fc355661":"markdown","0163435b":"markdown","88f37f86":"markdown","84b1be58":"markdown","b1ded1c8":"markdown","801d30df":"markdown","e1ff95bd":"markdown","a75f4127":"markdown","bf06d926":"markdown","e0bdf6be":"markdown","dbbb9a93":"markdown","8de523b8":"markdown","64079780":"markdown","f4071d90":"markdown","647718c1":"markdown","ed29f739":"markdown","51a311c6":"markdown","34cea841":"markdown","931bb929":"markdown","0c625de9":"markdown","112cdaa4":"markdown","c612b707":"markdown","ccded194":"markdown","7a6a208c":"markdown","b4eeae4b":"markdown","1845f830":"markdown","1c364d84":"markdown","4e582286":"markdown"},"source":{"e9afe538":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,ExtraTreesClassifier,AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report,f1_score,recall_score ,precision_score,roc_curve,confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0c714fbb":"df = pd.read_csv(\"Employee.csv\")","1418b63e":"df.head()","0b27612d":"df.rename(columns={'ExperienceInCurrentDomain': 'ECD'}, inplace=True)","8dbb0022":"df.describe()","d88666d3":"df.shape","adca39e6":"df.columns","c86b273f":"df.info()","a926911d":"df.isnull().sum()","2f69424a":"sns.heatmap(df.isnull())","6459cb95":"labelencoder = preprocessing.LabelEncoder()\ndf['Education'] = labelencoder.fit_transform(df['Education'])\ndf['City'] = labelencoder.fit_transform(df['City'])\ndf['Gender'] = labelencoder.fit_transform(df['Gender'])\ndf['EverBenched'] = labelencoder.fit_transform(df['EverBenched'])","8114334e":"plt.figure(figsize = (12,8))\nsns.heatmap(df.corr() , annot=True,cmap=sns.dark_palette((250, 75, 60), input=\"husl\"))","a24cfa01":"sns.set_theme(palette=\"viridis\")\nfig, axs = plt.subplots(ncols=2,figsize=(16, 8))\n(df['LeaveOrNot'].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[0])\n(df['LeaveOrNot'].value_counts(normalize=True)*100).plot.bar(ax=axs[1])","78f41c88":"fig, axs = plt.subplots(ncols=2,figsize=(16, 8))\nexplode = [0.1,0.0,0.0,0.1]\nlabels = [\"Male Stay\", \"Female stay\", \"Female Leave\", \"Male Leave\"]\nsns.histplot(data=df, x=\"Gender\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=6, ax=axs[0])\n(df[['Gender','LeaveOrNot']].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[1] ,explode=explode, labels=labels)","da83ebbc":"fig, axs= plt.subplots(figsize=(14, 8))\nsns.histplot(data=df, x=\"Age\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=.7)","89593808":"fig, axs= plt.subplots(figsize=(14, 8))\nsns.histplot(data=df, x=\"ECD\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=0.7)","ddcddc78":"sns.histplot(data=df, x=\"PaymentTier\", hue=\"Education\", multiple=\"dodge\", shrink=6)","f0e4a6cf":"fig, axs = plt.subplots(ncols=2,figsize=(16, 8))\nexplode = [0.0,0.0,0.09,0.09,0.3,0.3]\nlabels = [\"Bachelors, 0\", \"Bachelors, 1\", \"Masters, 0\", \"Masters, 1\" , \"PHD, 0\" , \"PHD, 1\"]\nsns.histplot(data=df, x=\"Education\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=4, ax=axs[0])\n(df[['Education','LeaveOrNot']].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[1] ,explode=explode , labels = labels)","f1898b75":"fig, axs = plt.subplots(ncols=2,figsize=(16, 8))\nexplode = [0.1,0.1,0.0,0.0,0.0,0.0]\nlabels = [\"Lowest Stay\", \"Lowest Leave\", \"Mid Leave\", \"Mid Stay\" , \"High Stay\" , \"High Leave\"]\nsns.histplot(data=df, x=\"PaymentTier\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=4, ax=axs[0])\n(df[['PaymentTier','LeaveOrNot']].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[1] ,explode=explode , labels = labels)","f983e3c2":"sns.histplot(data=df, x=\"PaymentTier\", hue=\"Education\", multiple=\"dodge\", shrink=6)","86691c35":"fig, axs = plt.subplots(ncols=2,figsize=(16, 8))\nexplode = [0.01,0.01,0.01,0.02,0.01,0.01]\nlabels = [\"Bangalore Stay\", \"New Delhi Stay\", \"Pune Leave\", \"Pune Stay\" , \"Bangalore Leave\" , \"New Delhi Leave\"]\nsns.histplot(data=df, x=\"City\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=4, ax=axs[0])\n(df[['City','LeaveOrNot']].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[1] ,explode=explode , labels=labels)","922a593a":"sns.histplot(data=df, x=\"PaymentTier\", hue=\"City\", multiple=\"dodge\", shrink=4)","6b1cc21b":"fig, axs = plt.subplots(ncols=2,figsize=(16, 8))\nexplode = [0.09,0.0,0.0,0.09,0.09,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.5,.5]\nsns.histplot(data=df, x=\"JoiningYear\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=1.5, ax=axs[0])\n(df[['JoiningYear','LeaveOrNot']].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[1] ,explode=explode)","0b4b248b":"sns.histplot(data=df, x=\"PaymentTier\", hue=\"JoiningYear\", multiple=\"dodge\", shrink=4)","37c60844":"fig, axs = plt.subplots(ncols=2,figsize=(16, 8))\nlabels = [\"No, 0\", \"No, 1\", \"Yes, 0\", \"Yes, 1\"]\nsns.histplot(data=df, x=\"EverBenched\", hue=\"LeaveOrNot\", multiple=\"dodge\", shrink=5, ax=axs[0])\n(df[['EverBenched','LeaveOrNot']].value_counts(normalize=True)*100).plot.pie(autopct='%1.1f%%', ax=axs[1] , labels = labels)","c887bee7":"sns.histplot(data=df, x=\"JoiningYear\", hue=\"EverBenched\", multiple=\"dodge\", shrink=1)","59cb43f0":"sc= MinMaxScaler()\nX =pd.DataFrame(sc.fit_transform(df.drop([\"LeaveOrNot\"],axis = 1)))\nY = df['LeaveOrNot'].values","e19e9b31":"sm = SMOTE(random_state=42)\nX,Y=sm.fit_resample(X, Y)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.2 , random_state=42)","3e05b874":"clf = lgb.LGBMClassifier()\nclf.fit(X_train, Y_train)\nY_pred_test = clf.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_test))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_test))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_test))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_test))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_test))\nprint(classification_report(Y_test, Y_pred_test))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_test,pos_label=1)\nconfusion_matrix=confusion_matrix(Y_test,Y_pred_test)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplot_confusion_matrix(confusion_matrix,class_names=[\"not leaveing (0)\",\"leaving(1)\"],figsize=(12,5))\nplt.show()","36fe968d":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\nfeatures_names = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n       'EverBenched', 'ExperienceInCurrentDomain']\nsns.set_theme(palette=\"Spectral\")\nf_importances(abs(clf.feature_importances_), features_names, top=8)","3fc36df1":"K=KNeighborsClassifier(n_neighbors=10)\nK.fit(X_train, Y_train)\nY_pred_k = K.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_k))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_k))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_k))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_k))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_k))\nprint(classification_report(Y_test, Y_pred_k))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_k,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","bfa3b7bb":"D=DecisionTreeClassifier(max_depth=8,max_features=8,random_state=42)\nD.fit(X_train, Y_train)\nY_pred_d = D.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_d))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_d))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_d))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_d))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_d))\nprint(classification_report(Y_test, Y_pred_d))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_test,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","541aee5a":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\nfeatures_names = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n       'EverBenched', 'ExperienceInCurrentDomain']\nsns.set_theme(palette=\"coolwarm\")\nf_importances(abs(D.feature_importances_), features_names, top=8)","fa8800c3":"R=RandomForestClassifier(n_estimators=5,max_depth=8,max_features=8,random_state=42)\nR.fit(X_train, Y_train)\nY_pred_r = R.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_r))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_r))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_r))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_r))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_r))\nprint(classification_report(Y_test, Y_pred_r))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_r,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","bc28d7bc":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\nfeatures_names = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n       'EverBenched', 'ExperienceInCurrentDomain']\nsns.set_theme(palette=\"icefire\")\nf_importances(abs(R.feature_importances_), features_names, top=8)","d350fdb1":"G=GradientBoostingClassifier(n_estimators =5, max_depth =7, learning_rate = 0.3, max_features=7,random_state=42 )\nG.fit(X_train, Y_train)\nY_pred_g = G.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_g))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_g))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_g))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_g))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_g))\nprint(classification_report(Y_test, Y_pred_g))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_g,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","affe1c89":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n    \nfeatures_names = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n       'EverBenched', 'ExperienceInCurrentDomain']\nsns.set_theme(palette=\"dark:salmon_r\")\nf_importances(abs(G.feature_importances_), features_names, top=8)","b23871de":"rbf = svm.SVC(kernel='rbf', gamma=4,C=2).fit(X_train,Y_train)\nrbf.fit(X_train, Y_train)\nY_pred_rbf = rbf.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_rbf))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_rbf))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_rbf))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_rbf))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_rbf))\nprint(classification_report(Y_test, Y_pred_rbf))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_rbf,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","c717609c":"E = ExtraTreesClassifier(n_estimators=6,min_samples_split=10, random_state=0)\nE.fit(X_train, Y_train)\nY_pred_e = E.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_e))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_e))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_e))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_e))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_e))\nprint(classification_report(Y_test, Y_pred_e))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_e,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","0a92ca95":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\nfeatures_names = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n       'EverBenched', 'ExperienceInCurrentDomain']\n    \nf_importances(abs(E.feature_importances_), features_names, top=8)","da48d575":"L=LogisticRegression()\nL.fit(X_train, Y_train)\nY_pred_l = L.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_l))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_l))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_l))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_l))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_l))\nprint(classification_report(Y_test, Y_pred_l))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_l,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","c30a04dd":"A=AdaBoostClassifier(n_estimators=5,learning_rate=.5)\nA.fit(X_train, Y_train)\nY_pred_a = A.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_a))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_a))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_a))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_a))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_a))\nprint(classification_report(Y_test, Y_pred_a))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_a,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","8331a7a2":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\nfeatures_names = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n  \n                  'EverBenched', 'ExperienceInCurrentDomain']\nf_importances(abs(A.feature_importances_), features_names, top=8)","fad12c5e":"GNB = GaussianNB()\nGNB.fit(X_train, Y_train)\nY_pred_gnb =GNB.predict(X_test)\nprint(\"F1-Score:\",metrics.f1_score(Y_test, Y_pred_gnb))\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred_gnb))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_pred_gnb))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_pred_gnb))\nprint(\"AUC:\",metrics.roc_auc_score(Y_test, Y_pred_gnb))\nprint(classification_report(Y_test, Y_pred_gnb))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test, Y_pred_gnb,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","a575c64e":"def testModel():\n    edu = int(input(\"Please,Enter Education level\\n0-Bachelors\\n1-Masters\\n2-PHD\\n\"))\n    year = int(input(\"Please,Enter Joining year\\n\"))\n    city = int(input(\"Please,Enter City\\n0-Bangalore\\n1-New Delhi\\n2-Pune\\n\"))\n    payment = int(input(\"Please,Enter Payment Tier\\n1-Highest\\n2-Mid Level\\n3-Lowest\\n\"))\n    age = int(input(\"Please,Enter Age\\n\"))\n    gender = int(input(\"Please,Enter Gender\\n0-Female\\n1-Male\\n\"))\n    benched = int(input(\"Please,Enter if employee ever Benched\\n0-No\\n1-Yes\\n\"))\n    exper = int(input(\"Please,Enter Experience In Current Domain\\n\"))\n    data = [[edu,year,city,payment,age,gender,benched,exper]]\n    data = sc.transform(data)\n    pred = clf.predict(data)\n    if (pred == 1):\n        print(\"Employee Will Leave\\n\")\n        suge = int(input(\"Would you like to know the suggetions to make him stay?\\n1-Yes\\n2-No\\n\"))\n        if(suge == 1):\n            from random import choice\n            while (1):\n                data[0][1] = choice(X[1])\n                data[0][2] = choice(X[2])\n                data[0][3] = choice(X[3])\n                data[0][6] = choice(X[6])\n                sugPred = clf.predict(data)\n                if (sugPred == 0):\n                    data = (sc.inverse_transform(data)).astype(int)\n                    data = data.astype(str)\n                    if (data[0][2] == \"0\"):\n                        data[0][2] = \"Bangalore\"\n                    elif (data[0][2] == \"1\"):\n                        data[0][2] = \"New Delhi\"\n                    elif (data[0][2] == \"2\"):\n                        data[0][2] = \"Pune\"\n                    if (data[0][3] == \"1\"):\n                        data[0][3] = \"Highest\"\n                    elif (data[0][3] == \"2\"):\n                        data[0][3] = \"Mid\"\n                    elif (data[0][3] == \"3\"):\n                        data[0][3] = \"Lowest\"\n                    if (data[0][6] == \"0\"):\n                        data[0][6] = \" not \"\n                    elif (data[0][6] == \"1\"):\n                        data[0][6] = \" \"\n                    print(\"We suggest to deal with the employee like %s,\\nmove the employee to office in %s,\\nmake employee payment in %s level\\nand%smake the employee benched\\n\" %(data[0][1] , data[0][2] , data[0][3] , data[0][6]))\n                    break\n    if (pred == 0):\n        print(\"Employee will not leave\")\ntestModel()","7112404d":"# Modeling","c5dcfa85":"### 28 years is the highest and lowest in leaving and staying\n","42808b10":"### Bangalore >>> 12.8% leaving\n### New Delhi >>> 8% leaving","fc355661":"## 9-AdaBoostClassifier","0163435b":"### Male >>> 44% staying, 15% leaving\n### Female >>> 21% staying, 19% leaving","88f37f86":"## 6-Support Vector Machine","84b1be58":"## 2-KNeighborsClassifier","b1ded1c8":"## in ExtraTreesClassifier the best features for prediction is:\n### 1- Joining Year\n### 2- City\n### 3- Payment Tier\n### 4- Education\n### 5- Gender\n### 6- Age\n### 7- Experience\n### 8- Benching","801d30df":"### The type of \u00a0payment tier \n### -1: HIGHEST -2: MID LEVEL -3:LOWEST\n### tier payment >>> the lowest 54% staying ,20% leaving\n","e1ff95bd":"## 5-GradientBoostingClassifier","a75f4127":"## Normalization, Over sampling, and Train-Test-split","bf06d926":"## 3-DecisionTreeClassifier","e0bdf6be":"### Correlation  is very weak between features !!!","dbbb9a93":"## it means ever kept out of the projects for 1 month or more\n### 30% from not behanced leaving \n### 5.6% behanced and not leaving \n","8de523b8":"# EDA","64079780":"# Categorical Features","f4071d90":"## in AdaBoostClassifier the best features for prediction is:\n### 1- Joining Year\n### 2- Payment Tier , Gender , City","647718c1":"## in RandomForestClassifier the best features for prediction is:\n### 1- Joining Year\n### 2- Payment Tier\n### 3- City\n### 4- Education\n### 5- Age\n### 6- Gender\n### 7- Experience\n### 8- Benching","ed29f739":"## in DecisionTreeClassifier the best features for prediction is:\n### 1- Joining Year\n### 2- Payment Tier\n### 3- City\n### 4- Education\n### 5- Gender\n### 6- Age\n### 7- Experience\n### 8- Benching","51a311c6":"### 0 >>> 65% staying  \n### 1 >>> 34% leaving\n### imbalnceing in the target","34cea841":"### 2years exp. >>> highest in staying and leaving","931bb929":"### Bachelors >>> 53% Staying , 24%  leaving\n### Master >>> 9.2% leaving, 9.2% staying","0c625de9":"## in LGBMClassifier the best features for prediction is:\n### 1- Age\n### 2- Joining Year\n### 3- Experience\n### 4- City\n### 5- Education\n### 6- Payment Tier\n### 7- Gender\n### 8- Benching","112cdaa4":"##  1-LGBMClassifier","c612b707":"## in GradientBoostingClassifier the best features for prediction is:\n### 1- Joining Year\n### 2- City\n### 3- Payment Tier\n### 4- Education\n### 5- Gender\n### 6- Age\n### 7- Experience\n### 8- Benching","ccded194":"## Data visualization","7a6a208c":"## 7-ExtraTreesClassifier","b4eeae4b":"## 8-LogisticRegression","1845f830":"### 2018 >>> all in 2017 leaving \n### 2012, 2016 is the lowest year ","1c364d84":"## 10-GaussianNB","4e582286":"## 4-RandomForestClassifier"}}