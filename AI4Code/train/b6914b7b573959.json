{"cell_type":{"5f8b49a8":"code","4b17cf51":"code","eb1664e1":"code","25685bcc":"code","a6cb69a7":"code","196388d3":"code","c3790480":"code","e5057dcb":"code","59806d8a":"code","09fcf5f5":"code","9d38e30d":"code","3f4c4efb":"code","287d5c4b":"code","3c8ee929":"code","3c42cc08":"code","f71724dd":"code","afa737a8":"code","79da3a05":"code","40308284":"code","da622d45":"code","3e6972ee":"code","15b3beb5":"code","dbccdc46":"markdown","e06277d1":"markdown","f68cb75a":"markdown","fdd91969":"markdown","86169892":"markdown","13e0e328":"markdown"},"source":{"5f8b49a8":"# libratries for data manupulation and visualization\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport json\nimport re\nimport string\nimport tqdm\n\n# natural language toolkit library for stemming, tokenizing, etc\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","4b17cf51":"sns.set_style('darkgrid')","eb1664e1":"columns = ['target', 'id', 'date', 'flag', 'user', 'text']","25685bcc":"full_df = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header = None, names = columns,  encoding='latin-1')","a6cb69a7":"full_df.head()","196388d3":"num_tweets = len(full_df)\nnum_pos_tweets = len(full_df[full_df['target'] == 4])\nnum_neg_tweets = len(full_df[full_df['target'] == 0])\nprint(\"Total Number of tweets in the database = {}\".format(num_tweets))\nprint(\"Total Number of positive tweets = {}\".format(num_pos_tweets))\nprint(\"Total Number of negative tweets = {}\".format(num_neg_tweets))","c3790480":"#all_positive_tweets = full_df[full_df['target'] == 4]['text'].to_json()\n#all_negative_tweets = full_df[full_df['target'] == 0]['text'].to_json()\n#pos_file = open(\"data\/pos_tweets.json\", 'w')\n#neg_file = open(\"data\/neg_tweets.json\", 'w')\n#json.dump(all_positive_tweets, pos_file, indent = 6)\n#json.dump(all_negative_tweets, neg_file, indent = 6)","e5057dcb":"all_positive_tweets = list(full_df[full_df['target'] == 4]['text'])\nall_negative_tweets = list(full_df[full_df['target'] == 0]['text'])","59806d8a":"# selecting only a portion of data due large size\nselect_prop = .25\nall_positive_tweets = all_positive_tweets[:int(len(all_positive_tweets)*select_prop)]\nall_negative_tweets = all_negative_tweets[:int(len(all_negative_tweets)*select_prop)]","09fcf5f5":"train_split = 0.8\ntrain_pos = all_positive_tweets[:int(len(all_positive_tweets)*train_split)]\ntrain_neg = all_negative_tweets[:int(len(all_negative_tweets)*train_split)]\ntest_pos = all_positive_tweets[int(len(all_positive_tweets)*train_split):]\ntest_neg = all_negative_tweets[int(len(all_negative_tweets)*train_split):]\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg\ntrain_y = np.concatenate((np.ones(len(train_pos)), np.zeros(len(train_neg))))\ntest_y = np.concatenate((np.ones(len(test_pos)), np.zeros(len(test_neg))))","9d38e30d":"print(\"Positive tweet - \")\nprint(train_pos[0])\nprint(\"Negative tweet - \")\nprint(train_neg[12])","3f4c4efb":"print(\"Number of training samples - \", len(train_x))\nprint(\"Numner of test samples - \", len(test_x))","287d5c4b":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","3c8ee929":"print(\"Original Tweet - \")\nprint(train_x[0])\nprint(\"Processed Tweet - \")\nprint(process_tweet(train_x[0]))","3c42cc08":"# Frequency dict\n# structure\n# (word, label) : number of times word appears in that class\nfreqs = build_freqs(train_x, train_y)","f71724dd":"# sugmoid function\ndef sigmoid(z):\n    h = 1\/(1 + np.exp(-z))\n    return h\n\n# stochastic gradient descent algorithm for optimizing weigths\n\ndef StochasticGradientDescent(x, y, theta, alpha, num_iters, batch_per_itr, batch_size):\n    '''\n    Input:\n        x: matrix of features which is (m,n+1)\n        y: corresponding labels of the input matrix x, dimensions (m,1)\n        theta: weight vector of dimension (n+1,1)\n        alpha: learning rate\n        num_iters: number of iterations you want to train your model for\n    Output:\n        J: the final cost\n        theta: final weight vector\n        \n    '''\n    # get 'm', the number of rows in matrix x\n    m =  x.shape[0]\n    loss = []\n    for itr in range(0, num_iters):\n        \n        for i in range(batch_per_itr):\n            batch = np.random.randint(0, m, size=batch_size)\n            x_train = x[batch,:]\n            y_train = y[batch,:]\n            \n            # get z, the dot product of x and theta\n            z = np.dot(x_train, theta)\n        \n            # get the sigmoid of z\n            h = sigmoid(z)\n        \n            # calculate the cost function\n            J = -1\/batch_size * (np.dot(np.transpose(y_train),np.log(h)) + np.dot(np.transpose(1 - y_train), np.log(1 - h)))\n            \n            \n        \n            # update the weights theta\n            theta = theta - ( alpha\/m * np.dot(np.transpose(x_train), (h - y_train)))\n        if not itr%10 and itr:\n            print(\"Completed {} iterations, loss = {}\".format(itr, np.squeeze(J))) \n        loss.append(J)\n    J = float(J)\n    return J, theta, loss\n\n# feature extraction utility function\n\ndef extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    \n    # loop through each word in the list of words\n    for word in word_l:\n        pos_pair = (word, 1.0)\n        neg_pair = (word, 0.0)\n        if pos_pair in freqs.keys():\n            # increment the word count for the positive label 1\n            x[0,1] += freqs[pos_pair]\n        if neg_pair in freqs.keys():\n            # increment the word count for the negative label 0\n            x[0,2] += freqs[neg_pair]\n        \n        \n    assert(x.shape == (1, 3))\n    return x\n\n\ndef predict_tweet(tweet, freqs, theta):\n    '''\n    Input: \n        tweet: a string\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n        theta: (3,1) vector of weights\n    Output: \n        y_pred: the probability of a tweet being positive or negative\n    '''\n    \n    # extract the features of the tweet and store it into x\n    x = extract_features(tweet, freqs)\n    \n    # make the prediction using x and theta\n    y_pred = sigmoid(np.dot(x, theta))\n    \n    \n    return y_pred\n\n\ndef test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\"\n    Input: \n        test_x: a list of tweets\n        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n        freqs: a dictionary with the frequency of each pair (or tuple)\n        theta: weight vector of dimension (3, 1)\n    Output: \n        accuracy: (# of tweets classified correctly) \/ (total # of tweets)\n    \"\"\"\n    \n\n    \n    # the list for storing predictions\n    y_hat = []\n    m = test_y.shape[0]\n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        \n        if y_pred > 0.5:\n            # append 1.0 to the list\n            y_hat.append(1)\n        else:\n            # append 0 to the list\n            y_hat.append(0)\n\n   \n   \n    y_hat = np.array(y_hat)\n    y_hat = np.reshape(y_hat, (m, 1))\n    accuracy = np.sum(y_hat == test_y)\/m\n    \n    return accuracy","afa737a8":"X = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n \n\nY = train_y","79da3a05":"Y = np.reshape(Y, (-1,1))","40308284":"# shuffle data\ns = np.random.permutation(range(len(X)))\nX = X[s]\nY = Y[s]","da622d45":"J, theta, loss_logs = StochasticGradientDescent(X, Y, theta = np.zeros((3, 1)), alpha=1e-9, num_iters=100, batch_per_itr=50, batch_size=100000) \nprint(f\"The cost after traill ning is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","3e6972ee":"plt.plot(np.squeeze(loss_logs))\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.show()","15b3beb5":"tmp_accuracy = test_logistic_regression(test_x, np.reshape(test_y, (-1,1)), freqs, theta)\nprint(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")","dbccdc46":"#### Performance on test set","e06277d1":"## Selecting a only a portion of dataset due to large size of data","f68cb75a":"#### Model Training","fdd91969":"### Applying Sigmoid model for classification\n#### Features of each tweet - [bias, freq of words inpositive class, freq of words in negative class]\n#### Applying Stochastic Gradiend Descent due to large size of data","86169892":"### Utility funtion for processing tweets\n- Removing stopwords\n- Removing Punctuation\n- Removing words like RT, hyperlinks and hashtags\n\n### Utility function for building frequency dictionary\n- Has a form of (word, label) : number of times word appears in that class","13e0e328":"### Below par performance, can be improved by - \n- Extracting more informative features like embeddings\n- Applying more complex deep learning models like Sequence models (Simple RNN, LSTM, GRUs, etc)"}}