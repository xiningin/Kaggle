{"cell_type":{"2db23a43":"code","0ad85523":"code","969e79e0":"code","a57b216f":"code","084cc0a2":"code","664503da":"code","6249fb19":"code","8ec8d14f":"code","60ef3ff3":"code","4404d554":"code","3c8e427d":"code","b3424184":"code","69491efa":"code","ca78f00b":"code","2c80914c":"code","c8ea0aec":"code","d36cc5d6":"code","ee80bf31":"code","5653b046":"code","265a48cd":"code","c0cdc661":"code","1a3c0598":"code","d3e2398f":"code","8efda2e4":"code","2ff3f845":"code","360a8afb":"code","0bdd4b26":"code","95320a3d":"code","aab3f63a":"code","84d22b00":"code","60cce50e":"code","a9caa66e":"code","cdf9ddb0":"code","64feea5c":"code","c9599c82":"markdown","b7ef4328":"markdown","55107d10":"markdown","4addb861":"markdown","4675d8b8":"markdown","a9a1e10a":"markdown","a66c8dbd":"markdown","23f18ad2":"markdown","9e5a3d36":"markdown","5a565066":"markdown","300d7a5b":"markdown","3864d205":"markdown","e3f15d2e":"markdown","f3c67625":"markdown","8f220504":"markdown","1ce9c49f":"markdown","3c3b3a1c":"markdown","cdd71b2f":"markdown","0a4a311f":"markdown","6e137d2a":"markdown","073791b1":"markdown","65698b08":"markdown","b6ef304d":"markdown"},"source":{"2db23a43":"import warnings\nwarnings.simplefilter('ignore')","0ad85523":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport seaborn as sns\nfrom statistics import harmonic_mean\n\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\nfrom scipy.stats import pearsonr\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport os\nimport sys\nimport nltk\nimport string\nimport math\nimport logging\nimport glob\nimport random\nimport gc\n\ntf.get_logger().setLevel(logging.ERROR)\n        \ntqdm.pandas()\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","969e79e0":"# Seed all random sources\ndef set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    \nset_seeds(42)","a57b216f":"# Maximum Tokenized input length\nSEQ_LENGTH = 250\n\n# Dataset is compressed and can take ~15 seconds to load\ntrain = pd.read_pickle('\/kaggle\/input\/simplenormal-wikipedia-sections\/wikipedia_sections.pkl.xz')","084cc0a2":"display(train.head())","664503da":"display(train.info())","6249fb19":"plt.figure(figsize=(10, 10))\nplt.ylabel('ylabel', fontsize=24)\ntrain['label'].value_counts().plot(kind='pie', autopct='%1.1f%%', textprops={'fontsize': 16})\nplt.show()","8ec8d14f":"# Describe word_count\ndisplay(train['word_count'].describe().apply(lambda i: int(i)))","60ef3ff3":"plt.figure(figsize=(15, 8))\ntrain.loc[train['word_count'] < int(1e3), 'word_count'].plot(kind='hist', bins=32)\nplt.title('Frequency per Word Count', size=24, pad=10)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.xlabel('Frequency', size=18, labelpad=10)\nplt.ylabel('Word Count', size=18, labelpad=10)\nplt.show()","4404d554":"# Define the model name\nMODEL = 'roberta-base'\n\n# Load tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)","3c8e427d":"# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(excerpt):\n    enc_di = tokenizer.batch_encode_plus(\n        excerpt,\n        padding = 'max_length',\n        truncation = True,\n        max_length = SEQ_LENGTH,\n        return_attention_mask = False,\n        return_tensors = 'np',\n    )\n    return enc_di['input_ids'].astype(dtype=np.uint16)","b3424184":"if os.path.exists(f'\/kaggle\/input\/simplenormal-wikipedia-sections\/wikipedia_section_roberta_input_ids_{SEQ_LENGTH}.npy'):\n    train_input_ids = np.load(f'\/kaggle\/input\/simplenormal-wikipedia-sections\/wikipedia_section_roberta_input_ids_{SEQ_LENGTH}.npy')\n    \n    if len(train) != len(train_input_ids):\n        print(f'Precomputed RoBERTa input_ids are outdated and need to be recomputed')\n        print(f'New train length: {len(train)}, Precomputed RoBERTa input_ids length: {len(train_input_ids)}')\n        # Compute text encoding, this will take ~30 minutes per million rows\n        del train_input_ids\n        train_input_ids = regular_encode(train['text'])\n        # save input_ids\n        np.save(f'wikipedia_section_roberta_input_ids_{SEQ_LENGTH}', train_input_ids)\n        print('New RoBERTa input_ids succesfully computed and saved')\n    else:\n        print(f'Successfully loaded precomputed RoBERTa input_ids')\nelse:\n    # Compute text encoding, this will take 1 hour per million rows\n    train_input_ids = regular_encode(train['text'])\n    # save input_ids\n    np.save(f'wikipedia_section_roberta_input_ids_{SEQ_LENGTH}', train_input_ids.astype(np.uint16))","69491efa":"# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    strategy = tf.distribute.get_strategy(devices=['\/gpu:0']) # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\n\n# set half precision policy\nmixed_precision.set_policy('float32')\n\nprint(f'Compute dtype: {mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {mixed_precision.global_policy().variable_dtype}')","ca78f00b":"def get_model():\n    with strategy.scope():\n        # RoBERTa\n        transformer = TFRobertaModel.from_pretrained(MODEL)\n        input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n        sequence_output = transformer(input_ids)[0]\n        # We only need the cls_token, resulting in a 2d array\n        cls_token = sequence_output[:, 0, :]\n        # 10% dropout\n        dropout = tf.keras.layers.Dropout(0.10)(cls_token)\n        # 2 output neurons for Simple and Normal class\n        output = tf.keras.layers.Dense(2, activation=None, dtype=tf.float32)(dropout)\n        \n        model = tf.keras.models.Model(inputs = [input_ids], outputs = [output])\n\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n        # Parameters taken from the RoBERTa paper\n        optimizer = tf.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-6)\n        metrics = [\n            tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall'),\n        ]\n\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)            \n    \n    return model\n\nmodel = get_model()","2c80914c":"model.summary()","c8ea0aec":"tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=False)","d36cc5d6":"# Training configuration\nBATCH_SIZE_BASE = 128\nBATCH_SIZE = BATCH_SIZE_BASE * REPLICAS\nEPOCHS = 5\n\nprint(f'BATCH SIZE: {BATCH_SIZE}, EPOCHS: {EPOCHS}')","ee80bf31":"# We will be using 51200 (1024*50) validation sections\nN_VAL = 51200\ntest_size = N_VAL \/ len(train)\nX_train, X_val, y_train, y_val = train_test_split(train_input_ids, train['label_int'], test_size=test_size, random_state=42, stratify=train['label_int'])\nprint(f'N train samples: {len(X_train)}, N val samples: {len(X_val)}')","5653b046":"# Check if the train\/val split is correctly stratified over the classes\nprint('Train label distribution')\ndisplay(pd.Series(y_train).value_counts(normalize=True))\nprint('Val label distribution')\ndisplay(pd.Series(y_val).value_counts(normalize=True))","265a48cd":"def get_dataset(X, y, shuffle):    \n    XX = { \n        'input_ids': X,\n    }\n    yy = tf.one_hot(y, 2, dtype=tf.uint8)\n    \n    dataset = tf.data.Dataset.from_tensor_slices((XX, yy))\n    # Only shuffle for train dataset\n    if shuffle:\n        dataset = dataset.shuffle(len(X))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(1)\n    \n    return dataset","c0cdc661":"# TRAIN DATASET\ntrain_dataset = get_dataset(X_train, y_train, shuffle=True)\n\n# Example of a batch\ntrain_x, train_y = next(iter(train_dataset))\nprint(f'train_x keys: {list(train_x.keys())}, train_x shape: {train_x[\"input_ids\"].shape}')\nprint(f'train_x input ids dtype: {train_x[\"input_ids\"].dtype}')\nprint(f'train_y shape: {train_y.shape}, train_y dtype {train_y.dtype}')\nprint(f'first 2 labels: {train_y[:2].numpy().tolist()}')","1a3c0598":"# VAL DATASET\nval_dataset = get_dataset(X_val, y_val, shuffle=False)\n\n# Example of a batch\nval_x, val_y = next(iter(val_dataset))\nprint(f'val_x keys: {list(val_x.keys())}, val_x shape: {val_x[\"input_ids\"].shape}')\nprint(f'val_x input ids dtype: {val_x[\"input_ids\"].dtype}')\nprint(f'val_y shape: {val_y.shape}, val_y dtype {val_y.dtype}')\nprint(f'first 2 labels: {val_y[:2].numpy().tolist()}')","d3e2398f":"history = model.fit(\n    train_dataset,\n    epochs = EPOCHS,\n    verbose = 1,\n    validation_data = val_dataset,\n)","8efda2e4":"# Save RoBERTa weight\nfor l_idx, l in enumerate(model.layers):\n    print(l.name)\n    if l.name == 'tf_roberta_model':\n        print(f'Saving layer {l_idx} with name {l.name}')\n        l.save_weights('roberta_pretrained.h5')","2ff3f845":"def plot_history_metric(history, metric, f_best):\n    plt.figure(figsize=(15, 8))\n    N_EPOCHS = len(history.history['loss'])\n    x = [1, 5] + [10 + 5 * idx for idx in range((N_EPOCHS - 10) \/\/ 5 + 1)]\n    x_ticks = np.arange(1, N_EPOCHS+1)\n    val = 'val' in ''.join(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(x_ticks, history.history[metric])\n    if val:\n        val_values = history.history[f'val_{metric}']\n        val_argmin = f_best(val_values)\n        plt.scatter(val_argmin + 1, val_values[val_argmin], color='red', s=50, marker='o')\n        plt.plot(x_ticks, val_values)\n    \n    plt.title(f'Model {metric}', fontsize=24)\n    plt.ylabel(metric, fontsize=18)\n    plt.xlabel('epoch', fontsize=18)\n    plt.tick_params(axis='x', labelsize=8)\n    plt.xticks(x, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    plt.legend(['train'] + ['val'] if val else ['train'],  prop={'size': 18})\n    plt.grid()","360a8afb":"plot_history_metric(history, 'loss', np.argmin)","0bdd4b26":"plot_history_metric(history, 'accuracy', np.argmax)","95320a3d":"plot_history_metric(history, 'precision', np.argmax)","aab3f63a":"plot_history_metric(history, 'recall', np.argmax)","84d22b00":"# F1 score or computed as the harmonic mean of precision and recall\ndef hmean_element_wise(aa, bb):\n    return [harmonic_mean([a, b]) for a, b in zip(aa, bb)]\n\nhistory.history['F1'] = hmean_element_wise(history.history['precision'], history.history['recall'])\nhistory.history['val_F1'] = hmean_element_wise(history.history['val_precision'], history.history['val_recall'])\n\nplot_history_metric(history, 'F1', np.argmax)","60cce50e":"def plot_validation_report_per_class(model, dataset):\n    print(f'--- CLASSIFICATION REPORT ---')\n    # classification report\n    bs = BATCH_SIZE\n    y = np.ndarray(shape=len(val_dataset) * bs, dtype=np.uint16)\n    y_pred = np.ndarray(shape=len(val_dataset) * bs, dtype=np.uint16)\n    for idx, (images, labels) in tqdm(enumerate(dataset), total=len(dataset)):\n        with tf.device('cpu:0'):\n            y[idx*bs:(idx+1)*bs] = np.argmax(labels, axis=1)\n            y_pred[idx*bs:(idx+1)*bs] = np.argmax(model.predict(images).astype(np.float32), axis=1)\n            \n    print(classification_report(y, y_pred))\n    \n    # Confusion matrix\n    N_LABELS = 2\n    fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n    cfn_matrix = confusion_matrix(y, y_pred, labels=range(N_LABELS))\n    cfn_matrix = (cfn_matrix.T \/ cfn_matrix.sum(axis=1)).T\n    df_cm = pd.DataFrame(cfn_matrix, index=np.arange(N_LABELS), columns=np.arange(N_LABELS))\n    ax = sns.heatmap(df_cm, cmap='Blues', annot=True, fmt='.3f', linewidths=.5, annot_kws={'size':14}).set_title('CONFUSION MATRIX')\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel('PREDICTED', fontsize=24, labelpad=10)\n    plt.ylabel('ACTUAL', fontsize=24, labelpad=10)\n    plt.show()","a9caa66e":"plot_validation_report_per_class(model, val_dataset)","cdf9ddb0":"def get_model():\n    tf.keras.backend.clear_session()\n\n    with strategy.scope():\n        # RoBERTa\n        transformer = TFRobertaModel.from_pretrained(MODEL)\n        # Load saved weights\n        transformer.load_weights('roberta_pretrained.h5')\n        \n        input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n        sequence_output = transformer(input_ids)[0]\n        # We only need the cls_token, resulting in a 2d array\n        cls_token = sequence_output[:, 0, :]\n        output = tf.keras.layers.Dense(1, activation='linear', dtype=tf.float32)(cls_token)\n        \n        # Model\n        model = tf.keras.models.Model(inputs=input_ids, outputs=output)\n\n        loss = tf.keras.losses.MeanSquaredError()\n        optimizer = tf.optimizers.Adam(learning_rate=4e-5)\n        metrics = [\n            tf.keras.metrics.RootMeanSquaredError(name='RMSE'),\n        ]\n\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\nmodel = get_model()","64feea5c":"model.summary()","c9599c82":"Due to the inbalanced dataset the confusion matrix give important insights in the actual performance of the model. The simple Wikipedia class has a recall close to 80\\%, which gives confidence in the model actually learning to distinguish simple from normal Wikipedia sections and not only getting biased towards the majority class.","b7ef4328":"Only section with at least 25 words are included in the dataset.","55107d10":"# Train configuration","4addb861":"# CommonLit Model","4675d8b8":"The next function shows how the pretrained RoBERTa model can be loaded into a model for the CommonLit training task.","a9a1e10a":"# Training History","a66c8dbd":"# Model","23f18ad2":"The actual training is done in the next cell.","9e5a3d36":"Training history is plotted below.","5a565066":"# Hardware Configuration","300d7a5b":"Hello fellow Kagglers,\n\nThis notebook demonstrates how to pretrain a RoBERTa model on a dataset containing 1.5 million sections from simple and normal Wikipedia. This binary classification task should fine tune the RoBERTa weights to learn text features indicating its readability level.\n\nThe dataset used can be found [here](https:\/\/www.kaggle.com\/markwijkhuizen\/simplenormal-wikipedia-sections).","3864d205":"# Dataset Properties","e3f15d2e":"# Roberta Tokenize","f3c67625":"A large batch size of 1024 divided over 8 compute units is used. Training is done for 5 epochs, after 5 epochs the model only overfits, which is surprising given the huge sample count.","8f220504":"50 batches of 1024 images are used for validation","1ce9c49f":"# Train Test Split","3c3b3a1c":"# Dataset","cdd71b2f":"As can be observed the dataset is not balanced. This is caused by normal Wikipedia pages having far more sections than simple Wikipedia pages.","0a4a311f":"Precomputed RoBERTa tokenized texts are used, as it take forever (30m) to compute them.","6e137d2a":"# Training","073791b1":"A simple model is used with a RoBERTa layer, followed by a dropout layer and a classification layer. The hyper parameters for the optimizer are taken from the [RoBERTa paper](https:\/\/arxiv.org\/pdf\/1907.11692.pdf)","65698b08":"# Confusion Matrix","b6ef304d":"![roberta_hyper_params.png](attachment:b1d30b52-d7cb-428f-826e-59f8edd89e57.png)"}}