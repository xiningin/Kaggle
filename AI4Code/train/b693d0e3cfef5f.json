{"cell_type":{"3e6102fa":"code","7f540a72":"code","9347afe7":"code","edb6b472":"code","6e78d5e4":"code","e91fa9c8":"code","d7869b55":"code","81a7a13a":"code","af18733b":"code","bd10cf6a":"code","8c32b3ec":"code","18c8c966":"code","1abbd052":"code","6d3a4b37":"code","bb71b1ff":"code","af1362c3":"code","7f33a176":"code","28999f7c":"code","56c04749":"code","ae711a89":"code","5ae76c21":"code","2bbf15c6":"code","36b74611":"code","38658432":"code","102866f5":"code","cd34fc1b":"code","b286be48":"code","534d8b4d":"code","0555bafc":"code","a1e01329":"code","64448d02":"code","34a475ba":"code","12720b66":"code","e73772d4":"code","b1ef3f88":"code","37b7f7cb":"code","9c7998a7":"code","357a6c77":"code","b7de7fd8":"code","a889d259":"code","ffe355e4":"code","0e0ac5d5":"code","6097961b":"code","166e3724":"code","e53c1998":"code","15d01fa4":"code","41396d54":"code","d5798e27":"code","53cc44a1":"code","bf7645fb":"markdown","57452d5a":"markdown","82977760":"markdown","deda9aaa":"markdown","f52589d9":"markdown","432593c7":"markdown","7bfae50f":"markdown","4b118899":"markdown","e1ec2345":"markdown","784128b7":"markdown","c930e417":"markdown","103b0392":"markdown","c8cb9add":"markdown"},"source":{"3e6102fa":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7f540a72":"!pip install -U lightautoml","9347afe7":"# Standard python libraries\nimport time\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport torch\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task","edb6b472":"N_THREADS = 4 #\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e VCPU \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 LightAutoML\nN_FOLDS = 5 #\u0444\u043e\u043b\u0434\u044b\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nTIMEOUT = 600\nTARGET_NAME = 'y'","6e78d5e4":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","e91fa9c8":"%%time\n\ntrain_data = pd.read_csv('\/kaggle\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')\ntest_data = pd.read_csv(\"\/kaggle\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/mercedes-benz-greener-manufacturing\/sample_submission.csv.zip\")","d7869b55":"%%time\n\ntrain_data.shape, test_data.shape, submission.shape","81a7a13a":"train_data.head()","af18733b":"plt.figure(figsize=(8,8))\nplt.scatter(range(train_data.shape[0]),np.sort(train_data.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","bd10cf6a":"ulimit = 180\ntrain_data['y'].iloc[train_data['y']>ulimit] = ulimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_data.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()","8c32b3ec":"dtype_data=train_data.dtypes.reset_index()\ndtype_data.columns = [\"Count\", \"Column Type\"]\ndtype_data.groupby(\"Column Type\").aggregate('count').reset_index()","18c8c966":"dtype_data.loc[:10,:]","1abbd052":"train_data.isnull().sum().sum()","6d3a4b37":"one_value_cols = [col for col in train_data.columns if train_data[col].nunique() <= 1]\nprint(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')","bb71b1ff":"one_value_cols","af1362c3":"train_data.drop(one_value_cols, axis=1, inplace=True)","7f33a176":"test_data.drop(one_value_cols, axis=1, inplace=True)","28999f7c":"one_value_cols_test = [col for col in test_data.columns if test_data[col].nunique() <= 1]\nprint(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')","56c04749":"train_data.drop(one_value_cols_test, axis=1, inplace=True)\ntest_data.drop(one_value_cols_test, axis=1, inplace=True)","ae711a89":"var=\"X0\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","5ae76c21":"var=\"X1\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","2bbf15c6":"var=\"X2\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","36b74611":"var=\"X3\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","38658432":"var=\"X4\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","102866f5":"var=\"X5\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","cd34fc1b":"var=\"X6\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","b286be48":"var=\"X8\"\ncolu_order=np.sort(train_data[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=train_data,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","534d8b4d":"unique_value_dict = {}\nfor col in train_data.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(train_data[col].unique()).tolist())\n        t_list = unique_value_dict.get(unique_value, [])\n        t_list.append(col)\n        unique_value_dict[unique_value] = t_list[:]\nfor unique_val, columns in unique_value_dict.items():\n    print(\"Columns containing the unique values : \",unique_val)\n    print(columns)\n    print(\"-----------------------------------------------------------\")","0555bafc":"zero_list=[]\none_list=[]\ncol_list = unique_value_dict['[0, 1]']\nfor col in col_list:\n    zero_list.append((train_data[col]==0).sum())\n    one_list.append((train_data[col]==1).sum())\nl = len(col_list)\narr = np.arange(l)\nwidth = 0.35\nplt.figure(figsize=(6,100))\nplot_1 = plt.barh(arr, zero_list, width, color='red')\nplot_2 = plt.barh(arr, one_list, width, left=zero_list, color=\"blue\")\nplt.yticks(arr, col_list)\nplt.legend((plot_1[0], plot_2[0]), ('Zero count', 'One Count'))\nplt.show()","a1e01329":"var = \"ID\"\nplt.figure(figsize=(12,6))\nsns.regplot(x=var, y='y', data=train_data, scatter_kws={'alpha':0.5, 's':30})\nplt.xlabel(var, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","64448d02":"train_data = train_data.drop_duplicates()","34a475ba":"sns.boxplot((train_data.y))","12720b66":"# \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0432\u044b\u0431\u0440\u043e\u0441\u044b\ntrain_data = train_data[(train_data['y'] <= 136)].reset_index(drop=True)","e73772d4":"sns.boxplot((train_data.y))","b1ef3f88":"sns.distplot(train_data['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['y'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['y'], plot=plt)\nplt.show()","37b7f7cb":"from sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection","9c7998a7":"def get_additional_features(train,test,magic=False,ID=False):\n    col = list(test.columns)\n    if ID!=True:\n        col.remove('ID')\n    n_comp = 12\n    # tSVD\n    tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n    tsvd_results_train = tsvd.fit_transform(train[col])\n    tsvd_results_test = tsvd.transform(test[col])\n    # PCA\n    pca = PCA(n_components=n_comp, random_state=420)\n    pca2_results_train = pca.fit_transform(train[col])\n    pca2_results_test = pca.transform(test[col])\n    # ICA\n    ica = FastICA(n_components=n_comp, random_state=420)\n    ica2_results_train = ica.fit_transform(train[col])\n    ica2_results_test = ica.transform(test[col])\n    # GRP\n    grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n    grp_results_train = grp.fit_transform(train[col])\n    grp_results_test = grp.transform(test[col])\n    # SRP\n    srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n    srp_results_train = srp.fit_transform(train[col])\n    srp_results_test = srp.transform(test[col])\n    for i in range(1, n_comp + 1):\n        train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n        test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n        train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n        test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n        train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n        test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n        train['grp_' + str(i)] = grp_results_train[:, i - 1]\n        test['grp_' + str(i)] = grp_results_test[:, i - 1]\n        train['srp_' + str(i)] = srp_results_train[:, i - 1]\n        test['srp_' + str(i)] = srp_results_test[:, i - 1]\n    if magic==True:\n        magic_mat = train[['ID','X0','y']]\n        magic_mat = magic_mat.groupby(['X0'])['y'].mean()\n        magic_mat = pd.DataFrame({'X0':magic_mat.index,'magic':list(magic_mat)})\n        mean_magic = magic_mat['magic'].mean()\n        train = train.merge(magic_mat,on='X0',how='left')\n        test = test.merge(magic_mat,on='X0',how = 'left')\n        test['magic'] = test['magic'].fillna(mean_magic)\n    return train,test","357a6c77":"def feature_creation(df):\n    for i in ['X0', 'X1', 'X2', 'X3', 'X5', 'X6', 'X8']:\n        for j in ['X0', 'X1', 'X2', 'X3', 'X5', 'X6', 'X8']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n\n    return df","b7de7fd8":"test_data = feature_creation(test_data)\ntrain_data = feature_creation(train_data)","a889d259":"train_data,test_data = get_additional_features(train,test,magic=True)","ffe355e4":"train_data.shape, test_data.shape, submission.shape","0e0ac5d5":"train_data.head()","6097961b":"%%time\n#\u0412 \u044d\u0442\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0438 \u0443\u0436\u0435 \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0430, \u0442.\u043a. \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043b\u0430\n# \u0447\u0442\u043e \u0437\u0430 \u043f\u043e\u043b\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043b\u0443\u0447\u0448\u0435 \u0438 \u0441\u0440\u0430\u0437\u0443 \u0443\u0447\u0443 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0434\u0430\u0442\u0435\n\ntr_data, te_data = train_test_split(train_data, \n                                    test_size=TEST_SIZE, \n                                    #stratify=train_data[TARGET_NAME], \n                                    random_state=RANDOM_STATE)\nprint('Data splitted. Parts sizes: tr_data = {}, te_data = {}'.format(tr_data.shape, te_data.shape))","166e3724":"task = Task('reg', loss = 'rmsle', metric = 'rmsle')","e53c1998":"%%time\n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['ID'],\n}","15d01fa4":"%%time \n\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'n_jobs': N_THREADS})\n\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","41396d54":"%%time\n\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(r2_score(train_data[TARGET_NAME].values, oof_pred.data[:, 0]) ** 0.5))","d5798e27":"submission[TARGET_NAME] = test_pred.data[:, 0]","53cc44a1":"submission.to_csv('auto_ml_mercedes2.csv', index = False)","bf7645fb":"**\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435**","57452d5a":"# Mercedes-Benz Greener Manufacturing","82977760":"# Data splitting for train","deda9aaa":"\u0412\u0438\u0434\u0438\u043c 1 \u0432\u044b\u0431\u0438\u0432\u0430\u044e\u0449\u0435\u0435\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435. \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f","f52589d9":"**\u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435**","432593c7":"# Prepare submission","7bfae50f":"# Features","4b118899":"# Constants","e1ec2345":"\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u0438\u043f \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u0441\u0435\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445, \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","784128b7":"\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0446\u0435\u043b\u044b\u043c\u0438 \u0447\u0438\u0441\u043b\u0430\u043c\u0438 \u0441 8 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c\u0438 \u0438 1 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u043c \u0441 \u043f\u043b\u0430\u0432\u0430\u044e\u0449\u0435\u0439 \u0437\u0430\u043f\u044f\u0442\u043e\u0439 (\u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f)","c930e417":"# Other libraries","103b0392":"# Data loading","c8cb9add":"# AutoML"}}