{"cell_type":{"09c6b38f":"code","1f69f0c2":"code","618e98ac":"code","330a0d6f":"code","262620ef":"code","d0eb948d":"code","b1620a9b":"code","70437c2e":"code","27a4ae21":"code","c7012021":"code","ac36d9e8":"code","9b226ad9":"code","8e66b488":"code","ec78a00a":"code","3a0fd484":"code","cebbfdb9":"code","aec258a0":"code","4d30255c":"code","5263c4c3":"code","601eb251":"code","e69baea7":"code","96e45a35":"markdown","99cd90a1":"markdown","980b8cbd":"markdown","45693faa":"markdown","b473ff00":"markdown","8f770ab0":"markdown","e518d5c1":"markdown"},"source":{"09c6b38f":"# !pip install tf-nightly-gpu-2.0-preview","1f69f0c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","618e98ac":"import tensorflow as tf\n# tf.enable_eager_execution()\nfrom tensorflow import feature_column as fc\n\ndftrain = pd.read_csv('\/kaggle\/input\/train.csv')\ndftest = pd.read_csv('\/kaggle\/input\/test.csv')\ndftrain.columns","330a0d6f":"dftrain.columns.equals(dftest.columns)","262620ef":"dfeval = dftrain.sample(dftrain.shape[0]\/\/10, random_state=0)\ndftrain_all = dftrain.copy()\ndftrain = dftrain.drop(dfeval.index)\n\ny_eval = dfeval.pop('Survived')\ny_train_all = dftrain_all.pop('Survived')\ny_train = dftrain.pop('Survived')\n\ndfs = [dftrain_all, dftrain, dfeval, dftest]","d0eb948d":"dftrain.shape, dfeval.shape, dftest.shape","b1620a9b":"for df in dfs:\n    df['FamilySize'] = df.SibSp + df.Parch + 1\n    df['Alone'] = (df.FamilySize == 1)*1","70437c2e":"dftrain.info()","27a4ae21":"dftrain.isna().sum()","c7012021":"dftrain_age_mean = dftrain.Age.mean()\nfor df in dfs:\n    df.Age.fillna(dftrain_age_mean, inplace=True)\n    print(df.Embarked.unique(), df.Embarked.mode()[0])\n    df.Embarked.fillna(dftrain.Embarked.mode()[0], inplace=True)","ac36d9e8":"CATEGORICAL_COLUMNS = ['Sex', 'SibSp', 'Parch', 'Pclass', 'Embarked', 'Alone', 'FamilySize']\nNUMERIC_COLUMNS = ['Age', 'Fare']\nDROP_COLUMNS = dftrain.columns.drop(CATEGORICAL_COLUMNS + NUMERIC_COLUMNS)\nDROP_COLUMNS","9b226ad9":"def drop_columns(df):\n    return df.drop(DROP_COLUMNS, axis=1)","8e66b488":"dftrain.Age.describe()","ec78a00a":"dftrain.Fare.describe()","3a0fd484":"feature_columns = []\n\n# for feature_name in NUMERIC_COLUMNS:\n#   feature_columns.append(fc.numeric_column(feature_name, dtype=tf.float32))\n\nfeature_columns.append(fc.bucketized_column(\n    fc.numeric_column('Age'), list(range(5,80,10))\n))\n\nfeature_columns.append(fc.bucketized_column(\n    fc.numeric_column('Fare'), [7,13,30]\n))\n\ndef one_hot_cat_column(feature_name, vocab):\n  return fc.indicator_column(\n      fc.categorical_column_with_vocabulary_list(\n          feature_name, vocab)\n  )\n\nfor feature_name in CATEGORICAL_COLUMNS:\n  vocabulary = dftrain[feature_name].unique()\n  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n  \n","cebbfdb9":"# Use entire batch since this is such a small dataset.\nNUM_EXAMPLES = len(y_train)\nBATCH_SIZE = 64*2\n# BATCH_SIZE = NUM_EXAMPLES\n\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\n  def input_fn():\n    inputs = (dict(X), y)\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n    if shuffle:\n      dataset = dataset.shuffle(NUM_EXAMPLES)\n    # For training, cycle thru dataset as many times as need (n_epochs=None)\n    dataset = dataset.repeat(n_epochs)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset\n  return input_fn\n\n# Training and evaluation input functions.\ntrain_input_fn = make_input_fn(drop_columns(dftrain), y_train)\neval_input_fn = make_input_fn(drop_columns(dfeval), y_eval, shuffle=False, n_epochs=1)","aec258a0":"linear_est = tf.estimator.LinearClassifier(feature_columns)\n\n# Train model.\nlinear_est.train(train_input_fn, max_steps=100)\n\n# Evaluation.\nresult = linear_est.evaluate(eval_input_fn)\nresult","4d30255c":"import math\nn_batches = NUM_EXAMPLES\/BATCH_SIZE\nprint(n_batches)\nn_batches = int(math.ceil(n_batches))\nn_batches","5263c4c3":"def create_est():\n    return tf.estimator.BoostedTreesClassifier(\n        feature_columns,\n        n_batches_per_layer=n_batches,\n        n_trees=150,\n        max_depth=4,\n        learning_rate=0.1,\n    )\nest = create_est()\nest.train(train_input_fn, max_steps=1000)\nest.evaluate(eval_input_fn)","601eb251":"est = create_est()\nest.train(make_input_fn(drop_columns(dftrain_all), y_train_all), max_steps=1000)\nest.evaluate(eval_input_fn)","e69baea7":"test_input_fn = make_input_fn(drop_columns(dftest), dftest.index, shuffle=False, n_epochs=1)\npreds = est.predict(test_input_fn)\npreds = [pred['class_ids'][0] for pred in preds]\npd.DataFrame({'PassengerId': dftest.PassengerId, 'Survived': preds}).to_csv('submission.csv', index=False)\n!head submission.csv","96e45a35":"# BoostedTreesClassifier","99cd90a1":"# NaN","980b8cbd":"# The Titanic\n\nThanks to [How to train Boosted Trees models in TensorFlow](https:\/\/medium.com\/tensorflow\/how-to-train-boosted-trees-models-in-tensorflow-ca8466a53127)!","45693faa":"## Using all training data \n\nInclude validation data.","b473ff00":"# Linear classifier\n\na benchmark","8f770ab0":"## Split training & validation data","e518d5c1":"# TensorFlow features"}}