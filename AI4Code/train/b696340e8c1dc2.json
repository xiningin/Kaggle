{"cell_type":{"d57266a5":"code","13ba1fed":"code","eea8d45f":"code","61dbfb21":"code","14297766":"code","e47bd903":"code","967bdeea":"code","5a95625a":"code","4a7cce53":"code","16dad050":"code","a598ac4c":"code","b9b41893":"code","91a00f77":"code","246ef606":"code","d136fa18":"code","e7d64d14":"code","6eb22e6f":"code","e6c84201":"code","21fe70ed":"code","d96e9e78":"code","08a1c2ba":"code","b7c7fdf0":"code","5225625f":"code","feba9525":"code","c97d24b8":"code","530dab4e":"code","bd61dd70":"code","732289c9":"code","c596d90f":"code","b58a31b9":"code","9346bef2":"code","10db73fd":"code","2a1c1d2b":"code","990d2ce6":"code","5b809ea9":"code","b631980f":"code","a01853ca":"code","c0f0e079":"code","674413b0":"code","47557ae3":"code","fe92dfd7":"code","5a75a600":"code","f14cea40":"code","4882f011":"code","da0b1ef2":"code","5d892621":"code","5c8e4a8a":"markdown","fd550a75":"markdown","a9e48bb4":"markdown","55cc42ba":"markdown","420b17dc":"markdown","12920f77":"markdown","570e2b18":"markdown","632ee34c":"markdown","b03706b6":"markdown","a9d8f2db":"markdown","cdcbaf7f":"markdown","1679f26c":"markdown","02b21d93":"markdown","ec50fe68":"markdown","0919b328":"markdown","f6cd00d0":"markdown","dc4ce55f":"markdown","8eaf68d9":"markdown"},"source":{"d57266a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","13ba1fed":"#imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime as dt\npd.set_option('max_rows',1000)\nsns.set(rc={'figure.figsize':(15,10)})\nfrom wordcloud import WordCloud\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom math import sqrt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import CatBoostRegressor","eea8d45f":"df = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data.csv')\ndf_year = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_year.csv')\ndf_by_genres = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_genres.csv')\ndf_w_genres = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_w_genres.csv')\ndf_artist = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_artist.csv')","61dbfb21":"df.head()","14297766":"df_year.head()","e47bd903":"df_artist.head()","967bdeea":"df_by_genres.head()","5a95625a":"df_w_genres.head()","4a7cce53":"df.info()","16dad050":"df.describe()","a598ac4c":"df.isna().sum()","b9b41893":"df1 = df.copy()\ndf1['duration_ms'].head()","91a00f77":"df1['duration_ms'] = df1['duration_ms']\/1000\ndf1.rename({'duration_ms':'duration_in_seconds'},axis=1,inplace=True)\ndf1.info()","246ef606":"df.hist(figsize=(20,20))\nplt.show()","d136fa18":"fig,ax = plt.subplots(5,3,figsize=(20,25))\nsns.distplot(df1['valence'],ax=ax[0,0])\nsns.distplot(df1['year'],ax=ax[0,1])\nsns.distplot(df1['acousticness'],ax=ax[0,2])\nsns.distplot(df1['danceability'],ax=ax[1,0])\nsns.distplot(df1['duration_in_seconds'],ax=ax[1,1])\nsns.distplot(df1['energy'],ax=ax[1,2])\n#sns.distplot(df1['explicit'],ax=ax[2,0])\nsns.distplot(df1['instrumentalness'],ax=ax[2,1])\nsns.distplot(df1['key'],ax=ax[2,2])\nsns.distplot(df1['liveness'],ax=ax[3,0])\nsns.distplot(df1['loudness'],ax=ax[3,1])\nsns.distplot(df1['mode'],ax=ax[3,2])\nsns.distplot(df1['popularity'],ax=ax[4,0])\nsns.distplot(df1['speechiness'],ax=ax[4,1])\nsns.distplot(df1['tempo'],ax=ax[4,2])","e7d64d14":"plt.figure(figsize=(15,10))\nsns.heatmap(df1.corr(),annot=True)","6eb22e6f":"g_pn = df1.groupby(\"name\")['popularity'].sum().sort_values(ascending=False)[:20]\naxis = sns.barplot(g_pn.index, g_pn,palette='rocket')\naxis.set_title('Top Tracks with Popularity')\naxis.set_ylabel('Popularity')\naxis.set_xlabel('Tracks')\nplt.xticks(rotation = 90)","e6c84201":"g_ap = df1.groupby(\"artists\")['popularity'].sum().sort_values(ascending=False)[:20]\naxis = sns.barplot(g_ap.index, g_ap,palette='magma_r')\naxis.set_title('Top Artists with Popularity')\naxis.set_ylabel('Popularity')\naxis.set_xlabel('Artists')\nplt.xticks(rotation = 90)","21fe70ed":"columns = [\"acousticness\",\"danceability\",\"energy\",\"speechiness\",\"liveness\",\"valence\"]\nplt.figure(figsize=(15,10))\nfor c in columns:\n    x = df1.groupby('year')[c].mean()\n    sns.lineplot(x.index,x,label=c)\nplt.title('Audio characteristics over year')\nplt.xlabel('Year')\nplt.ylabel('Characteristics')\nplt.show()","d96e9e78":"g_an = df1.groupby('artists')['name'].count().sort_values(ascending=False)[:20]\naxis = sns.barplot(g_an.index, g_an,palette='winter')\naxis.set_title('Top artists with tracks')\naxis.set_ylabel('Track count')\naxis.set_xlabel('Artists')\nplt.xticks(rotation = 90)\nplt.show()","08a1c2ba":"g_an = df1.groupby('artists')['danceability'].mean().sort_values(ascending=False)[:20]\naxis = sns.barplot(g_an.index, g_an,palette='summer')\naxis.set_title('Top artists with danceability')\naxis.set_ylabel('danceability')\naxis.set_xlabel('Artists')\nplt.xticks(rotation = 90)\nplt.show()","b7c7fdf0":"df1['year'].describe()","5225625f":"bins = [1920,1960,2000,2020]\ndf1['year_bins'] = pd.cut(df1['year'],bins,labels=['20s-60s','60s-2000','2000-2020'])\ndf1['year_bins'].head(10)","feba9525":"df1['year_bins'].value_counts()","c97d24b8":"g_yp = df1.groupby('year_bins')['popularity'].mean().sort_values(ascending=False)[:20]\naxis = sns.barplot(g_yp.index, g_yp,palette='autumn_r')\naxis.set_title('popularity categories')\naxis.set_xlabel('Categories')\naxis.set_ylabel('popularity')\n#plt.xticks(rotation = 90)\nplt.show()","530dab4e":"cols = [\"valence\",\"popularity\",\"acousticness\",\"instrumentalness\",\"speechiness\",\"danceability\" ]\nsns.pairplot(df1[cols])\nplt.show()","bd61dd70":"df1.columns","732289c9":"plt.figure()\nd = df1[:10000]\nsns.scatterplot('tempo','popularity',data=d)\nplt.show()","c596d90f":"plt.figure()\nsns.barplot('explicit','popularity',data=df1,palette='rocket_r')\nplt.show()","b58a31b9":"wrds1 = g_ap.index.str.split(\"(\").str[0].value_counts().keys()\nwc1 = WordCloud(scale=5,max_words=1000,colormap=\"rainbow\",background_color=\"white\").generate(\" \".join(wrds1))\nplt.imshow(wc1,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Artist Name for top 50 songs \",color='b')\nplt.show()","9346bef2":"sns.barplot('key','popularity',data=df1)\nplt.show()","10db73fd":"enc = LabelEncoder()\ndf1['Artist_enc'] = enc.fit_transform(df1['artists'])\ndf1['name_enc'] = enc.fit_transform(df1['name'])\ndf1['year_bins'] = df1['year_bins'].map({'20s-60s':0,'60s-2000':1,'2000-2020':2})\n#print(df1['Artist_enc'].value_counts(),df1['name_enc'].value_counts())\ndf1.head(10)","2a1c1d2b":"cols = ['valence', 'Artist_enc', 'danceability','acousticness','instrumentalness',\n       'duration_in_seconds', 'energy', 'explicit',\n        'loudness', 'name_enc', 'tempo', 'year_bins']\nX = df1[cols]\ny = df1['popularity']\nX = X[:30000]\ny = y[:30000]\nprint(X.shape,y.shape)","990d2ce6":"X['year_bins'].fillna(method='bfill',axis=0,inplace=True)\nX['year_bins'].isna().sum()","5b809ea9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42,shuffle=True)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","b631980f":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a01853ca":"#models\nrf = RandomForestRegressor(criterion='mse',random_state=42,bootstrap=True,\n                           n_estimators=1000,n_jobs=-1)\nxgb = XGBRegressor( booster='gbtree', colsample_bylevel=1,colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=6, min_child_weight=4, n_estimators=1000,\n             n_jobs=4, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nxgb0 = XGBRegressor()\ngbr = GradientBoostingRegressor( learning_rate=0.01,) \ndtr = DecisionTreeRegressor(criterion='mse',random_state=42,max_depth=35,\n                           max_features='sqrt', min_samples_leaf=15, min_samples_split=10)\nabr = AdaBoostRegressor(dtr,learning_rate=0.01)\ncat =  CatBoostRegressor(learning_rate=0.1,eval_metric = 'RMSE',verbose=0)\nlgb = LGBMRegressor()","c0f0e079":"xgb0.fit(X_train,y_train)\nxgb_pred0 = xgb0.predict(X_train)\nxgb_pred = xgb0.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_train,xgb_pred0)),mean_absolute_error(y_train,xgb_pred0))\nprint(np.sqrt(mean_squared_error(y_test,xgb_pred)),mean_absolute_error(y_test,xgb_pred))","674413b0":"rf.fit(X_train,y_train)\nrf_pred0 = rf.predict(X_train)\nrf_pred = rf.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_train,rf_pred0)),mean_absolute_error(y_train,rf_pred0))\nprint(np.sqrt(mean_squared_error(y_test,rf_pred)),mean_absolute_error(y_test,rf_pred))","47557ae3":"lgb.fit(X_train,y_train)\nlgb_pred0 = lgb.predict(X_train)\nlgb_pred = lgb.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_train,lgb_pred0)),mean_absolute_error(y_train,lgb_pred0))\nprint(np.sqrt(mean_squared_error(y_test,lgb_pred)),mean_absolute_error(y_test,lgb_pred))","fe92dfd7":"dtr.fit(X_train,y_train)\ndtr_pred0 = dtr.predict(X_train)\ndtr_pred = dtr.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_train,dtr_pred0)),mean_absolute_error(y_train,dtr_pred0))\nprint(np.sqrt(mean_squared_error(y_test,dtr_pred)),mean_absolute_error(y_test,dtr_pred))","5a75a600":"gbr.fit(X_train,y_train)\ngbr_pred0 = gbr.predict(X_train)\ngbr_pred = gbr.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_train,gbr_pred0)),mean_absolute_error(y_train,gbr_pred0))\nprint(np.sqrt(mean_squared_error(y_test,gbr_pred)),mean_absolute_error(y_test,gbr_pred))","f14cea40":"abr.fit(X_train,y_train)\nabr_pred0 = abr.predict(X_train)\nabr_pred = abr.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_train,abr_pred0)),mean_absolute_error(y_train,abr_pred0))\nprint(np.sqrt(mean_squared_error(y_test,abr_pred)),mean_absolute_error(y_test,abr_pred))","4882f011":"cat.fit(X_train,y_train)\ncat_pred = cat.predict(X_test)\ncat_pred0 = cat.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_train,cat_pred0)),mean_absolute_error(y_train,cat_pred0))\nprint(np.sqrt(mean_squared_error(y_test,cat_pred)),mean_absolute_error(y_test,cat_pred))","da0b1ef2":"output = pd.DataFrame({'Actual_Popularity':y_test,'Predicted_mean':np.round((cat_pred+xgb_pred)\/2,2)})\noutput.head(10)","5d892621":"output.to_csv('output.csv',index=False)","5c8e4a8a":"Visualising the top tracks by Popularity","fd550a75":"Let us plot the histogram of necessary features","a9e48bb4":"Wordcloud Visualisation for Artist names in top 50 songs","55cc42ba":"converting milliseconds to seconds","420b17dc":"Taking the Predicted mean of best models...","12920f77":"Let us now visualise how differnt audio characteristics have changed over time.","570e2b18":"Plotting the corelation matrix","632ee34c":"Reading the data...","b03706b6":"**Data Visualizations**","a9d8f2db":"<b> Data Characteristics <\/b>","cdcbaf7f":"# EXPLANATORY DATA ANALYSIS","1679f26c":"Songs with explicit lyrics are more popular which somehow does not come off as a surprise...","02b21d93":"Visualising the top artists by Popularity","ec50fe68":"There are a lot of rows in the dataset so I'll only take the first 30,000.","0919b328":"Lets split the dataset into training and test sets.","f6cd00d0":"# ML AND MODELLING","dc4ce55f":"Scaling using StandardScalar","8eaf68d9":"I will now group the years into bins to get an idea about different eras of music"}}