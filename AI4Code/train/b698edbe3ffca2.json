{"cell_type":{"ae122d8a":"code","2f1c31af":"code","aaed302a":"code","49639e3b":"code","fe32474e":"code","7955f032":"code","bb43e076":"code","51e3d665":"code","af695ebf":"code","5e803f3a":"code","f31cebe4":"code","74acda60":"code","5db24fe3":"code","ffab5657":"code","a0fa21a8":"code","36d40245":"code","b1f0b623":"code","c6d145c3":"code","06d0637e":"code","e8371ccd":"code","32a19521":"code","13633f7c":"code","32c4c7e2":"code","3b82312e":"code","c522c91d":"code","eefecc65":"code","9c299908":"code","64ace653":"code","3c41c7c7":"code","61389f4e":"code","1f426c45":"code","22f85487":"code","94deb3a7":"code","b70b28db":"code","c0851f2a":"code","0e503c5f":"code","a7c64020":"code","47fbfdeb":"code","ef26a057":"code","dea08c6b":"markdown","aaa51745":"markdown","3f85eade":"markdown","e3ccf763":"markdown","f56ce94c":"markdown","dbcba041":"markdown","0a507624":"markdown","1a4abfb8":"markdown","7658876c":"markdown","d3393e0a":"markdown","c4ef1c8d":"markdown","7d66a762":"markdown","5cf038a2":"markdown","478ebef9":"markdown","f4066157":"markdown","e2b78cb6":"markdown","b9127896":"markdown","e485bca4":"markdown"},"source":{"ae122d8a":"import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report\n\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle","2f1c31af":"# random sample\n#filename = \"..\/input\/data-science-bowl-2019\/train.csv\"\n#n = sum(1 for line in open(filename)) - 1\n#s = 1000000 #desired sample size\n\n#skip = sorted(random.sample(range(1,n+1),n-s))\n\n#train_data = pd.read_csv(filename)\ntrain_data = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train.csv\")\nspecs = pd.read_csv(\"..\/input\/data-science-bowl-2019\/specs.csv\")\ntest_data = pd.read_csv(\"..\/input\/data-science-bowl-2019\/test.csv\")\ntrain_labels = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train_labels.csv\")","aaed302a":"train_data.shape","49639e3b":"test_data.shape","fe32474e":"tmp_labels = train_labels.drop_duplicates(subset=\"installation_id\", keep=\"last\")\n\ntmp_labels.head(20)","7955f032":"acc_groups = tmp_labels[\"accuracy_group\"]\nacc_groups.head()","bb43e076":"specs.shape","51e3d665":"users = train_data['installation_id'].drop_duplicates()\nprint('unique users: {}'.format(users.size))\nattempted_users = train_data[train_data['type']=='Assessment'][['installation_id']].drop_duplicates() \nprint('users, who attempted assessments: {}'.format(attempted_users.size))\ntrain_data = pd.merge(train_data, attempted_users, on=\"installation_id\", how=\"inner\")","af695ebf":"names = []\nvalues = []\ntype_count = train_data.groupby('type').count()\nfor t in train_data['type'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.type == t]))\n\nfig = plt.figure(figsize=(8, 5))\nplt.bar(names, values)\nplt.title('Number of events by type')\nplt.show()","5e803f3a":"names = []\nvalues = []\nfor t in train_data['title'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.title == t]))\n\nfig = plt.figure(figsize=(13, 15))\nplt.barh(names, values)\nplt.title('Number of events by title')\nplt.show()","f31cebe4":"train_data.world.drop_duplicates()","74acda60":"print('MAGMAPEAK - {}\\n'.format(pd.unique(train_data[(train_data.world == 'MAGMAPEAK') & (train_data.type == 'Assessment')].title)))\nprint('CRYSTALCAVES - {}\\n'.format(pd.unique(train_data[(train_data.world == 'CRYSTALCAVES') & (train_data.type == 'Assessment')].title)))\nprint('TREETOPCITY - {}\\n'.format(pd.unique(train_data[(train_data.world == 'TREETOPCITY') & (train_data.type == 'Assessment')].title)))","5db24fe3":"names = []\nvalues = []\ntype_count = train_data.groupby('world').count()\nfor t in train_data['world'].drop_duplicates():\n    names.append(t)\n    values.append(len(train_data[train_data.world == t]))\n\nfig = plt.figure(figsize=(8, 5))\nplt.bar(names, values)\nplt.title('Number of events by world')\nplt.show()","ffab5657":"train_data[train_data.event_code == 4100].title.drop_duplicates()","a0fa21a8":"train_data['timestamp'] = pd.to_datetime(train_data['timestamp'])\ntrain_data['weekday'] = train_data['timestamp'].dt.dayofweek\ntrain_data['hour'] = train_data['timestamp'].dt.hour","36d40245":"fig = plt.figure(figsize=(12, 8))\nnames = ['Mon', 'Tue', 'Wd', 'Thu', 'Fri', 'Sat', 'Sun']\nvalues = []\nfor d in range(7):\n    values.append(len(train_data[train_data.weekday == d]))\nplt.bar(names, values)\nplt.title('Event count by weekday')\nplt.show()","b1f0b623":"fig = plt.figure(figsize=(14, 9))\nnames = range(24)\nvalues = []\nfor h in range(24):\n    values.append(len(train_data[train_data.hour == h]))\nplt.bar(names, values, width=0.5)\nplt.title('Event count by hour')\nplt.xticks(range(24))\nplt.show()","c6d145c3":"# fig = plt.figure(figsize=())\ntime_by_session = train_data[['game_session', 'world', 'game_time']].groupby(['game_session', 'world']).max()","06d0637e":"def calc_playtime():\n    result = list()\n    for u in attempted_users['installation_id']:\n        time_by_world = {'MAGMAPEAK':0,'TREETOPCITY':0,'CRYSTALCAVES':0,'NONE':0}\n        sessions_by_user = train_data[train_data.installation_id == u]['game_session'].drop_duplicates()\n        for s in sessions_by_user:\n            tmp = time_by_session.loc[s]['game_time'].iloc[0]\n            time_by_world[time_by_session.loc[s].index.tolist()[0]] += tmp\n        result.append(time_by_world)\n    return result\n\nplaytime = calc_playtime()","e8371ccd":"fig = plt.figure(figsize=(15,8))\nplt.plot([u['MAGMAPEAK'] + u['TREETOPCITY'] + u['CRYSTALCAVES'] for u in playtime])\nplt.title('Users playtime')\nplt.show()","32a19521":"fig = plt.figure(figsize=(8, 5))\nval = [0, 0, 0]\nfor u in playtime:\n    val[0] += u['MAGMAPEAK']\n    val[1] += u['TREETOPCITY']\n    val[2] += u['CRYSTALCAVES']\nplt.bar(['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES'], val)\nplt.show()","13633f7c":"train_labels.head(9)","32c4c7e2":"train_labels[['installation_id', 'accuracy_group']].groupby(['accuracy_group']).count().plot.bar(figsize=(10, 6))\nplt.show()","3b82312e":"tasks = pd.unique(train_labels.title)\nmean_wrong = []\nfor t in tasks:\n    mean_wrong.append(train_labels[train_labels.title == t].num_incorrect.mean())\nfig = plt.figure(figsize=(7, 7))\nplt.pie(mean_wrong, labels=tasks)\nplt.show()","c522c91d":"def calc_labels(data):\n    temp_data = data.query('event_code == [4100,4110] and type == \"Assessment\"').copy()\n    sessions = temp_data[temp_data.type == 'Assessment'][['game_session']].drop_duplicates()\n    labels = ['game_session', 'installation_id', 'title', 'num_correct', 'num_incorrect', 'accuracy', 'accuracy_group']\n    result = pd.DataFrame(columns=labels)    \n    for s in sessions['game_session']:\n        tmp = pd.DataFrame(columns=labels)\n        events_by_session = temp_data[temp_data.game_session == s]['event_data']\n        num_correct = 0\n        num_incorrect = 0\n        for e in events_by_session:\n            if json.loads(e)['correct']:\n                num_correct += 1\n            else:\n                num_incorrect += 1\n        if num_correct < 1:\n            accuracy = 0.0\n        else:\n            accuracy = num_correct \/ (num_correct + num_incorrect)\n        if num_incorrect == 0 and num_correct > 0:\n            accuracy_group = 3\n        elif num_incorrect == 1 and num_correct > 0:\n            accuracy_group = 2\n        elif num_incorrect >= 2 and num_correct > 0:\n            accuracy_group = 1\n        else:\n            accuracy_group = 0            \n        tmp['game_session'] = pd.Series(s)\n        tmp['installation_id'] = temp_data.loc[temp_data.game_session == s].iloc[0]['installation_id']\n        tmp['title'] = temp_data.loc[temp_data.game_session == s].iloc[0]['title']\n        tmp['num_correct'] = num_correct\n        tmp['num_incorrect'] = num_incorrect\n        tmp['accuracy'] = accuracy\n        tmp['accuracy_group'] = accuracy_group        \n        result = result.append(tmp, ignore_index=True)\n    return result","eefecc65":"train_labels = calc_labels(train_data)","9c299908":"test_labels = calc_labels(test_data)","64ace653":"def create_features(data, data_labels):\n    global attempted_users, playtime\n    labels = ['id', 'activities', 'games', 'clips', 'assessments', 'mean_activity_daytime', 'mean_game_daytime', 'mean_clip_daytime', 'mean_assessment_daytime', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES', 'accuracy']\n    result = pd.DataFrame(columns=labels)\n    \n    for i, u in enumerate(attempted_users.installation_id):\n        tmp = pd.DataFrame(columns=labels)\n        cur_user = data[data.installation_id == u]\n        \n        tmp['id'] = pd.Series(u)\n        sub = cur_user[cur_user.type == 'Activity']\n        tmp['activities'] = pd.Series(len(sub))\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        if len(m) > 0:\n            tmp['mean_activity_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_activity_daytime'] = pd.Series(0.)\n        \n        sub = cur_user[cur_user.type == 'Game']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['games'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_game_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_game_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Clip']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['clips'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_clip_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_clip_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Assessment']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['assessments'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_assessment_daytime'] = pd.Series(m[0][0]) \n        else:\n            tmp['mean_assessment_daytime'] = pd.Series(0.)\n        \n        tmp['MAGMAPEAK'] = pd.Series(playtime[i]['MAGMAPEAK'])\n        tmp['TREETOPCITY'] = pd.Series(playtime[i]['TREETOPCITY'])\n        tmp['CRYSTALCAVES'] = pd.Series(playtime[i]['CRYSTALCAVES'])\n        \n        acc_mode = data_labels[data_labels.installation_id == u]['accuracy_group'].mode()\n        if acc_mode.dropna().empty:\n            tmp['accuracy'] = pd.Series(0)\n        else:\n            tmp['accuracy'] = pd.Series(acc_mode.max())\n            \n        result = result.append(tmp, ignore_index=True)        \n    return result","3c41c7c7":"features = create_features(train_data, train_labels)","61389f4e":"features.head(10)","1f426c45":"test_data['timestamp'] = pd.to_datetime(test_data['timestamp'])\ntest_data['weekday'] = test_data['timestamp'].dt.dayofweek\ntest_data['hour'] = test_data['timestamp'].dt.hour","22f85487":"def extract_features(data, data_labels):\n    all_users = data[data['type']=='Assessment'][['installation_id']].drop_duplicates()\n    time_by_session = data[['game_session', 'world', 'game_time']].groupby(['game_session', 'world']).max()\n    ext_playtime = []\n    for u in all_users['installation_id']:\n        time_by_world = {'MAGMAPEAK':0,'TREETOPCITY':0,'CRYSTALCAVES':0,'NONE':0}\n        sessions_by_user = data[data.installation_id == u]['game_session'].drop_duplicates()\n        for s in sessions_by_user:\n            tmp = time_by_session.loc[s]['game_time'].iloc[0]\n            time_by_world[time_by_session.loc[s].index.tolist()[0]] += tmp\n        ext_playtime.append(time_by_world)\n    labels = ['id', 'activities', 'games', 'clips', 'assessments', 'mean_activity_daytime', 'mean_game_daytime', 'mean_clip_daytime', 'mean_assessment_daytime', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES', 'accuracy']\n    result = pd.DataFrame(columns=labels)\n    \n    for i, u in enumerate(all_users.installation_id):\n        tmp = pd.DataFrame(columns=labels)\n        cur_user = data[data.installation_id == u]\n        \n        tmp['id'] = pd.Series(u)\n        sub = cur_user[cur_user.type == 'Activity']\n        tmp['activities'] = pd.Series(len(sub))\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        if len(m) > 0:\n            tmp['mean_activity_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_activity_daytime'] = pd.Series(0.)\n        \n        sub = cur_user[cur_user.type == 'Game']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['games'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_game_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_game_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Clip']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['clips'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_clip_daytime'] = pd.Series(m[0][0])\n        else:\n            tmp['mean_clip_daytime'] = pd.Series(0.)\n            \n        sub = cur_user[cur_user.type == 'Assessment']\n        m = sub[['installation_id', 'hour']].groupby(['installation_id']).mean().values\n        tmp['assessments'] = pd.Series(len(sub))\n        if len(m) > 0:\n            tmp['mean_assessment_daytime'] = pd.Series(m[0][0]) \n        else:\n            tmp['mean_assessment_daytime'] = pd.Series(0.)\n        \n        tmp['MAGMAPEAK'] = pd.Series(ext_playtime[i]['MAGMAPEAK'])\n        tmp['TREETOPCITY'] = pd.Series(ext_playtime[i]['TREETOPCITY'])\n        tmp['CRYSTALCAVES'] = pd.Series(ext_playtime[i]['CRYSTALCAVES'])\n        \n        acc_mode = data_labels[data_labels.installation_id == u]['accuracy_group'].mode()\n        if acc_mode.dropna().empty:\n            tmp['accuracy'] = pd.Series(0)\n        else:\n            tmp['accuracy'] = pd.Series(acc_mode.max())\n            \n        result = result.append(tmp, ignore_index=True)\n    return result","94deb3a7":"test_features = extract_features(test_data, test_labels)","b70b28db":"x_train = features.loc[:, 'activities':'CRYSTALCAVES'].copy()\ny_train = features.loc[:, 'accuracy'].astype(int).copy()","c0851f2a":"x_test = test_features.loc[:, 'activities':'CRYSTALCAVES'].copy()\ny_test = test_features.loc[:, 'accuracy'].astype(int).copy()","0e503c5f":"clf = GradientBoostingClassifier()\nclf.fit(x_train, y_train)","a7c64020":"res = clf.predict(x_test.values)","47fbfdeb":"print(classification_report(y_test, res))","ef26a057":"def gen_submission(data, result):\n    labels = ['installation_id', 'accuracy_group']\n    submission = pd.DataFrame(columns=labels)\n    submission.installation_id = data['id']\n    submission.accuracy_group = pd.Series(result)\n    submission.to_csv('submission.csv', index=False)\ngen_submission(test_features, res)","dea08c6b":"<br><br>\n### Now lets visualize some data.\n#### First of all, some info about events might be usefull","aaa51745":"## Feature engineering","3f85eade":"### Now when we have some data to work with we can assemble it for training","e3ccf763":"#### Analyze and output results","f56ce94c":"#### So, magmapeak have only one assessment, but bigger number of events? Interesting. We will adress this later. <br>\n#### We know that assessments results are captured with event code 4100 and 4110 for Bird Measurer. Lets check ","dbcba041":"## Data exploration\n### Lets take a look at our data first. It consists of:\n* <span style=\"background-color:lightgray\">train.csv, test.csv<\/span> - main data files, which contain the gameplay events.\n* <span style=\"background-color:lightgray\">specs.csv<\/span> - this file gives the specification of the various event types.\n* <span style=\"background-color:lightgray\">train_labels.csv<\/span> - this file demonstrates how to compute the ground truth for the assessments in the training set.","0a507624":"#### Looks like day of week doesnt matter, but time of day and total playtime really differ from user to user. What about time spent per world?","1a4abfb8":"#### Seems like two of them are particulary hard. Need to keep close eye on them.","7658876c":"#### Half of users solve correctly on first try, okay. Now what about distribution between assessments","d3393e0a":"#### At last, run the classifier itself","c4ef1c8d":"#### Its actually roughly the same as event count. That means we can trear all actions equally, as they take almost the same time, which is helpfull. Also, time spent on magmapeak is not that impactfull as on two other worlds. <br>\n### Next lets look at train_labels as it contains results for the train data","7d66a762":"#### There are unnecessary stuff, seems like event type must be taken into account.","5cf038a2":"#### After that we may find something in connection between events and time of their accurance","478ebef9":"<br><br>\n#### We can start by getting rid of users that didnt take assessments, as we cant use them for training.","f4066157":"\n#### Each event belongs to section, we need to learn about those. Most importanlty, how assessments are divided","e2b78cb6":"# Data Science Bowl 2019","b9127896":"### First we need to precalculate labels","e485bca4":"#### Slightly modified data processing for test data, which includes operations run at analysis"}}