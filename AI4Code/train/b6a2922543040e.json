{"cell_type":{"36d07edc":"code","d1efe3c2":"code","042aed9f":"code","f7926e73":"code","e967aa47":"code","1c583888":"code","08a5dfb2":"code","84bc9bbf":"code","3435f031":"code","b8168bf5":"code","9c8c3867":"code","c362bddf":"markdown","dd6cff2d":"markdown","a55ce269":"markdown","a532eb4a":"markdown","17a49e33":"markdown","05db86bb":"markdown","1eebe8c2":"markdown","f10a0349":"markdown","d63af098":"markdown","a5e9b00d":"markdown"},"source":{"36d07edc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d1efe3c2":"\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE","042aed9f":"## Backup Imports\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport catboost as cb\nimport lightgbm as lgb\n\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import accuracy_score\n\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.pipeline import make_pipeline","f7926e73":"train = pd.read_csv(\"\/kaggle\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/learn-together\/test.csv\")","e967aa47":"# Remove the Labels and make them y\ny = train['Cover_Type']\n\n# Remove label from Train set\nX = train.drop(['Cover_Type'],axis=1)\n\n# Rename test to text_X\ntest_X = test\n\n\n\n# split data into training and validation data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX = X.drop(['Id'], axis = 1)\ntrain_X = train_X.drop(['Id'], axis = 1)\nval_X = val_X.drop(['Id'], axis = 1)\ntest_X = test_X.drop(['Id'], axis = 1)","1c583888":"train_X.describe()","08a5dfb2":"val_X.describe()","84bc9bbf":"test_X.describe()","3435f031":"\nrfcfin = RandomForestClassifier(n_estimators = int(1631.3630739649345),\n                                min_samples_split = int(2.4671165024828747),\n                                min_samples_leaf = int(1.4052032266878376),\n                                max_features = 0.23657708614689418,\n                                max_depth = int(426.8410655510125),\n                                bootstrap = int(0.8070235824535138),\n                                random_state=42)\nrfcfin.fit(X, y.ravel())","b8168bf5":"test_ids = test[\"Id\"]\ntest_pred = rfcfin.predict(test_X.values)","9c8c3867":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': test_pred})\noutput.to_csv('submission.csv', index=False)","c362bddf":"ForestML: Part 1: EDA and Class Imbalance Handling\n\nI ran through an almost complete cylce of making an ML model. But I realised that my Test accuracy became stagnant at ~0.801 +\/- 0.002.\nIn this Notebook I will systematically try to solve the class imbalance problem and redo all my model preparation from scratch.\n\n","dd6cff2d":"### Lets try Without Smoting","a55ce269":"## Define X and y (features and labels respectively)","a532eb4a":"## Record of notebooks I have worked on till now.","17a49e33":"## Notebook Summary:","05db86bb":"Test Result with Smoting: 0.7894\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-1-eda-and-class-imbalance-handling\/edit\/run\/20481229\n\nTest Result whout Smoting: this run.","1eebe8c2":"https:\/\/www.kaggle.com\/phsheth\/preliminary-eda-feature-importance\n\nhttps:\/\/www.kaggle.com\/phsheth\/ensemble-sequential-backward-selection\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-eda-and-stacking-evaluation-v2\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-2-feature-engg-random-forest <- Inducting Random Forest\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-3-feature-engg-xgboost <- Inducting XGBoost\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-4-feature-engg-extratrees < Inducting ExtraTrees\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-4-feature-engg-adaboost <- Inducting Adaboost\n \nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-4-feature-engg-bagging-classifier <- Bagging did not work well\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-4-feature-engg-catboost <- CatBoost did not work well\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selected-fets-2 <- Using Borrowed Parameters\n\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-7-stacking-selfets-pre-hp-tuning <- Removed Borrowed Parameters before HyperParameter Tuning\n\n<Show kernels for HyperParameter Tuning\n\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selfets-gmix <- Reference for Effect of Gmix\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-9-stacking-selfets-post-hp-tune <- Using Random Forest Tuned HyperParameters\n\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-9-1-stacking-selfets-rf-xgb <- Using Random Forest and XGBoost Tuned HyperParameters","f10a0349":"## Import the Raw Data","d63af098":"## View the dataframes","a5e9b00d":"## Import required libraries"}}