{"cell_type":{"d4a00ec0":"code","f6c7f9ed":"code","1c87e5e4":"code","66eb73fb":"code","c087e50f":"code","63eab8f5":"code","d6d7b540":"code","5007117c":"code","f4aca92e":"code","6c9f0ac4":"code","d7655efa":"code","7505ce86":"code","7967fe1c":"code","b027288c":"code","3507d132":"code","4cb09a78":"code","18faffe4":"code","797f7b98":"code","cb729ff2":"code","b4495504":"code","ce10e44f":"code","41bbf099":"code","06e3a70c":"code","58c49fc7":"code","f34ebd3c":"code","a90414ec":"code","c51de9ee":"code","c4ac6824":"code","f42d48db":"code","68d72727":"code","764cf693":"code","3b4fd160":"code","37c2bbe3":"code","475a23b6":"code","b43dfbc9":"markdown","06f0027a":"markdown","f511e9ba":"markdown","e6055aa3":"markdown","583df3ec":"markdown","678b8ace":"markdown","1018e5ca":"markdown","f7e39627":"markdown","bf53cd6c":"markdown","7b79e165":"markdown","a7fa22ad":"markdown","1129a0c4":"markdown","d013e046":"markdown","f4577cfc":"markdown","3c1d80e8":"markdown","6306d650":"markdown","748c1145":"markdown","3904f8f1":"markdown","dc3bf022":"markdown","a5a932de":"markdown","3de59897":"markdown","b16463b9":"markdown","e45f5cc4":"markdown","6995acae":"markdown","dea95a24":"markdown","cd684501":"markdown","67e1eaef":"markdown"},"source":{"d4a00ec0":"%matplotlib inline\nimport math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import is_numeric_dtype\n\nsns.set()","f6c7f9ed":"rawtrain = pd.read_csv('..\/input\/train.csv')\nrawtest = pd.read_csv('..\/input\/test.csv')","1c87e5e4":"print('Train shape:', rawtrain.shape)\nprint('Test shape:', rawtest.shape)","66eb73fb":"rawtrain.dtypes.value_counts()","c087e50f":"selected = ['GrLivArea',\n 'LotArea',\n 'BsmtUnfSF',\n '1stFlrSF',\n 'TotalBsmtSF',\n 'GarageArea',\n 'BsmtFinSF1',\n 'LotFrontage',\n 'YearBuilt',\n 'Neighborhood',\n 'GarageYrBlt',\n 'OpenPorchSF',\n 'YearRemodAdd',\n 'WoodDeckSF',\n 'MoSold',\n '2ndFlrSF',\n 'OverallCond',\n 'Exterior1st',\n 'YrSold',\n 'OverallQual']","63eab8f5":"#features = [c for c in test.columns if c not in ['Id']]","d6d7b540":"train = rawtrain[selected].copy()\ntrain['is_train'] = 1\ntrain['SalePrice'] = rawtrain['SalePrice'].values\ntrain['Id'] = rawtrain['Id'].values\n\ntest = rawtest[selected].copy()\ntest['is_train'] = 0\ntest['SalePrice'] = 1  #dummy value\ntest['Id'] = rawtest['Id'].values\n\nfull = pd.concat([train, test])\n\nnot_features = ['Id', 'SalePrice', 'is_train']\nfeatures = [c for c in train.columns if c not in not_features]","5007117c":"pd.Series(train.SalePrice).hist(bins=50);","f4aca92e":"pd.Series(np.log(train.SalePrice)).hist(bins=50);","6c9f0ac4":"full['SalePrice'] = np.log(full['SalePrice'])","d7655efa":"def summary(df, dtype):\n    data = []\n    for c in df.select_dtypes([dtype]).columns:\n        data.append({'name': c, 'unique': df[c].nunique(), \n                     'nulls': df[c].isnull().sum(),\n                     'samples': df[c].unique()[:20] })\n    return pd.DataFrame(data)","7505ce86":"summary(full[features], np.object)","7967fe1c":"summary(full[features], np.float64)","b027288c":"summary(full[features], np.int64)","3507d132":"for c in full.select_dtypes([np.object]).columns:\n    full[c].fillna('__NA__', inplace=True)\nfor c in full.select_dtypes([np.float64]).columns:\n    full[c].fillna(0, inplace=True)","4cb09a78":"for c in full.columns:\n    assert full[c].isnull().sum() == 0, f'There are still missing values in {c}'","18faffe4":"mappers = {}\nfor c in full.select_dtypes([np.object]).columns:\n    mappers[c] = {v:i for i,v in enumerate(full[c].unique())}\n    full[c] = full[c].map(mappers[c]).astype(int)","797f7b98":"for c in full.columns:\n    assert is_numeric_dtype(full[c]), f'Non-numeric column {c}'","cb729ff2":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","b4495504":"def rmse(y_true, y_pred):\n    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))","ce10e44f":"train = full[full.is_train==1][features].values\ntarget = full[full.is_train==1].SalePrice.values\nXtrain, Xvalid, ytrain, yvalid = train_test_split(train, target, test_size=0.2, random_state=42)","41bbf099":"model = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.02, max_depth=4, random_state=42)","06e3a70c":"model.fit(Xtrain, ytrain)","58c49fc7":"ypred = model.predict(Xvalid)\nrmse(yvalid, ypred)","f34ebd3c":"test = full[full.is_train==0]\nytestpred = model.predict(test[features].values)","a90414ec":"ytestpred = np.exp(ytestpred)","c51de9ee":"subm = pd.DataFrame(ytestpred, index=test['Id'], columns=['SalePrice'])\nsubm.to_csv('submission.csv')","c4ac6824":"cols = full[features].select_dtypes([np.float64, np.int64]).columns\nn_rows = math.ceil(len(cols)\/2)\nfig, ax = plt.subplots(n_rows, 2, figsize=(14, n_rows*2))\nax = ax.flatten()\nfor i,c in enumerate(cols):\n    sns.boxplot(x=full[c], ax=ax[i])\n    ax[i].set_title(c)\n    ax[i].set_xlabel(\"\")\nplt.tight_layout()","f42d48db":"limits = [('TotalBsmtSF', 4000), ('WoodDeckSF', 1400)]\n\nfull['__include'] = 1 \nfor c, val in limits:\n    full.loc[full[c] > val, '__include'] = 0\n\nfull = full[(full.is_train==0)|(full['__include']==1)]\n\nfull = full.drop('__include', axis=1)\n\n# these dates in the future are likely typos\nfull['GarageYrBlt'] = np.where(full.GarageYrBlt > 2010, full.YearBuilt, full.GarageYrBlt)","68d72727":"full['Age'] = 2010 - full['YearBuilt']\nmonth_season_map = {12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}\nfull['SeasonSold'] = full['MoSold'].map(month_season_map).astype(int)\nfull['SimplOverallCond'] = full['OverallCond'].replace(\n        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nfull['TimeSinceSold'] =  2010 - full['YrSold']\nfull['TotalArea1st2nd'] = full['1stFlrSF'] + full['2ndFlrSF']\nfull['TotalSF'] = full['TotalBsmtSF'] + full['1stFlrSF'] + full['2ndFlrSF']","764cf693":"train = full[full.is_train==1][features].values\ntarget = full[full.is_train==1].SalePrice.values\nXtrain, Xvalid, ytrain, yvalid = train_test_split(train, target, test_size=0.2, random_state=42)\n\nmodel = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.02, max_depth=4, random_state=42)\nmodel.fit(Xtrain, ytrain)\nypred = model.predict(Xvalid)\nrmse(yvalid, ypred)","3b4fd160":"model2 = ExtraTreesRegressor(n_estimators=1500, random_state=42)\nmodel2.fit(Xtrain, ytrain)\nypred2 = model2.predict(Xvalid)\nrmse(yvalid, ypred2)","37c2bbe3":"blendpred = 0.7*ypred + 0.3*ypred2\nrmse(yvalid, blendpred)","475a23b6":"test = full[full.is_train==0]\nytestpred = model.predict(test[features].values)\nytestpred2 = model2.predict(test[features].values)\nblendtestpred = 0.7*ytestpred + 0.3*ytestpred2\n\nblendtestpred = np.exp(blendtestpred)\n\nsubm = pd.DataFrame(blendtestpred, index=test['Id'], columns=['SalePrice'])\nsubm.to_csv('submission_blend.csv')","b43dfbc9":"# Extra: if you still have some time\n\nTry the following:\n\n- K-fold CV \n- Liner regression model ","06f0027a":"Since target was log transformed it needs to be exponentiated now","f511e9ba":"These are the types of the columns in the dataset. `np.object` are string values for the categorical features.","e6055aa3":"Code to check there are no missing values in the dataset","583df3ec":"Notice these model parameters is just a first guess. With parameter optimization the model results can be improved (for example using `sklearn.model_selection.RandomizedSearchCV`, left as a follow-up exercise). ","678b8ace":"# Check missing values\n\nDo some analysis to identify the missing values. There is a proposed summary function that can be used to check missing values for the different dtypes (`np.object`, `np.float64`, `np.int64`).","1018e5ca":"Now do something to replace the missing values. The best is to analyse case by case. A quick lazy approach can be to use a new label missing categoricals and zero for missing numerical.","f7e39627":"\ud83c\udf89 Great! Submission ready \ud83d\udcaa Now time to upload to Kaggle. ","bf53cd6c":"Choose a validation strategy for your model. Simple approach is to take out a validation dataset from the train dataset (`sklearn.model_selection.train_test_split` can be used for this).","7b79e165":"This code will remove some rows based on predefined limits. This is meant to be just example code, probably there is no reason to remove entries. This is a carefully cleaned dataset.","a7fa22ad":"Now applying the model to the test dataset, to generate the predictions to be submitted as results. ","1129a0c4":"# Encode categorical\n\nBefore creating the model the categorical features must be encoded into numberical values. There are many ways to do it, for example building a mapping dictionary and applying it with pandas. Or using `sklearn.preprocessing.LabelEncoder`.","d013e046":"# Remove Outliers\n\nIt is worth checking the data for outliers and try a model with some outliers removed. Suggested task is to plot a boxplot for each numerical variable. Then filter out some outliers in the training dataset.","f4577cfc":"If the performance improved in you CV, do another submission on Kaggle to check the value on `test`.  ","3c1d80e8":"Implementation of the competition metric (notice target is already log transformed so no need to do that in the metric).","6306d650":"Code to check that all columns are numeric","748c1145":"# Check target distribution\n\nThe competition metric is based on log transformed values. That is already an hint that log transform maybe useful to make the target distribution behave more like a normal distribution.\n\nNow plot the distribution of `SalePrice`.","3904f8f1":"And apply the log transformation to `SalePrice` in the dataset.","dc3bf022":"# Blend 2 models\n\nNow try to make 2 different models (for example GBM and ExtraTrees or RandomForest), combine the models (for example with a weighted average) and evaluate the performance in the validation set.","a5a932de":"This code builds a single dataframe with both `train` and `test` datasets and a new column to separate both. This can be useful when doing transformations that would need to be applied both in `train` and `test`. If you keep this approach you can use the checking code that is provided.","3de59897":"# First model with selected features\n\nTo make it a bit easier to do the first steps with the dataset you can use the following list with the 20 most important features (this list is the result of running a gradient boosting model and selecting the most important features). ","b16463b9":"# Kaggle Workshop Walkthrough\n\nExample walkthrough for the House Price competition. This notebook shows a possible simple approach for each of the workshop proposed tasks. Consider this a simple baseline, you can to better \ud83d\ude80","e45f5cc4":"# Feature engineering\n\nSome ideas for new features:\n\n- House age (considering the construction year and that this is a dataset from 2010)\n- What season was the house sold (winter, summer, etc)\n- Reduce the overal condition to 3 levels (good, average, bad)\n- How long ago the house was sold\n- Total area including first and second floor\n- Total area including first floor, second floor and basement\n","6995acae":"# First model\n\nNow try to build a first predictive model. One suggestion is to use gradient boosting that typically has strong results in this kind of tabular data (in `sklearn` there is a `GradientBoostingRegress` model). If you choose to do first a Linear regression don't forget to also one-hot encode the categorical values.","dea95a24":"# Load data","cd684501":"Well Done! \ud83c\udfc6 Now just keep the momentum and go for the gold \ud83e\udd47\ud83e\udd47\ud83e\udd47\ud83e\udd47\ud83d\ude80","67e1eaef":"Or you can just select everything if you prefer"}}