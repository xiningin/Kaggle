{"cell_type":{"72f80161":"code","f20a490a":"code","1fc9d22a":"code","9d85d4f3":"code","7b2d021f":"code","11ae6898":"code","18b5784a":"code","eb51c0c0":"code","b8528d7c":"code","b02a1084":"code","b4aa2879":"code","319a924f":"code","d6eadf88":"markdown","3a5d08ae":"markdown","442c8cb2":"markdown","926d2b6f":"markdown","615322f5":"markdown","6a636515":"markdown","dd37bf4a":"markdown"},"source":{"72f80161":"import tensorflow as tf\nimport numpy as np\nimport random\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import *\nfrom sklearn.metrics import *\nfrom tensorflow.keras.models import load_model\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ignore warning :)\n\n# tensorboard = TensorBoard(log_dir='mylog')\n\ngpus = tf.config.experimental.list_physical_devices(device_type='GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(device=gpu, enable=True)\n    \ntrain_tfrecord = 'XRay_train.tfrecords'\ntest_tfrecord = 'XRay_test.tfrecords'\ntrain_percentage = 0.8  # Proportion of training set\n\nrandom.seed(2020)\n\ninput_path='\/kaggle\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/'","f20a490a":"learning_rate = 0.001\nbuffer_size = 512\nbatch_size = 16\nepochs = 20\n\nimg_size = 224","1fc9d22a":"def read_directory():\n    data_filenames = []\n    data_labels = []\n\n    for filename in os.listdir(input_path + 'COVID-19'):\n        data_filenames.append(input_path + 'COVID-19\/' + filename)\n        data_labels.append(0)\n\n    for filename in os.listdir(input_path + 'NORMAL'):\n        data_filenames.append(input_path + 'NORMAL\/' + filename)\n        data_labels.append(1)\n\n    for filename in os.listdir(input_path + 'Viral Pneumonia'):\n        data_filenames.append(input_path + 'Viral Pneumonia\/' + filename)\n        data_labels.append(2)\n        \n    data_size = len(data_labels)\n\n    tmp_uni = list(zip(data_filenames, data_labels))\n\n    random.shuffle(tmp_uni)\n\n    train_size = int(data_size * train_percentage)\n    print('Size of training set\uff1a', train_size)\n    print('Size of test set\uff1a', data_size - train_size)\n\n    train_list = tmp_uni[0:train_size]\n    test_list = tmp_uni[train_size:]\n\n    train_filenames, train_labels = zip(*train_list)\n    test_filenames, test_labels = zip(*test_list)\n\n    return train_filenames, train_labels, test_filenames, test_labels","9d85d4f3":"def build_train_tfrecord(train_filenames, train_labels):  # Generate TFRecord of training set \n    with tf.io.TFRecordWriter(train_tfrecord)as writer:\n        for filename, label in zip(train_filenames, train_labels):\n            image = open(filename, 'rb').read()\n\n            feature = {\n                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),  # img > Bytes\n                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))  # label > Int\n            }\n\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\n            writer.write(example.SerializeToString())\n\n\ndef build_test_tfrecord(test_filenames, test_labels):  # Generate TFRecord of test set\n    with tf.io.TFRecordWriter(test_tfrecord)as writer:\n        for filename, label in zip(test_filenames, test_labels):\n            image = open(filename, 'rb').read()\n\n            feature = {\n                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n            }\n\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\n            writer.write(example.SerializeToString())","7b2d021f":"def _parse_example(example_string):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n    feature_dict['image'] = tf.io.decode_png(feature_dict['image'], channels=3)\n    feature_dict['image'] = tf.image.resize(feature_dict['image'], [img_size, img_size]) \/ 255.0\n    return feature_dict['image'], feature_dict['label']\n\n\ndef get_train_dataset(train_tfrecord):  # read TFRecord\n    raw_train_dataset = tf.data.TFRecordDataset(train_tfrecord)\n    train_dataset = raw_train_dataset.map(_parse_example)\n\n    return train_dataset\n\n\ndef get_test_dataset(test_tfrecord):\n    raw_test_dataset = tf.data.TFRecordDataset(test_tfrecord)\n    test_dataset = raw_test_dataset.map(_parse_example)\n\n    return test_dataset\n\n\ndef data_Preprocessing(train_dataset, test_dataset):\n    train_dataset = train_dataset.shuffle(buffer_size)\n    train_dataset = train_dataset.batch(batch_size)\n    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    test_dataset = test_dataset.batch(batch_size)\n    test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return train_dataset, test_dataset","11ae6898":"class CNN(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(\n            filters=32,\n            kernel_size=[3, 3],\n            padding='same',\n            activation=tf.nn.relu,\n            input_shape=(img_size, img_size, 3)\n        )\n        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n        self.conv2 = tf.keras.layers.Conv2D(\n            filters=64,\n            kernel_size=[5, 5],\n            padding='same',\n            activation=tf.nn.relu\n        )\n        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n        self.flatten = tf.keras.layers.Reshape(target_shape=(int(img_size\/4) * int(img_size\/4) * 64,))\n        self.dense1 = tf.keras.layers.Dense(units=512, activation=tf.nn.relu)\n        self.drop1 = tf.keras.layers.Dropout(0.5)\n        self.dense2 = tf.keras.layers.Dense(units=3, activation='softmax')\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.drop1(x)\n        output = self.dense2(x)\n        return output","18b5784a":"def ResNet50_model():\n    res50 = tf.keras.applications.ResNet50(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    res50.trainable= True\n    model = tf.keras.Sequential([\n        res50,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","eb51c0c0":"def scheduler(epoch,lr):\n    if epoch<8:\n        return lr\n    else:\n        return lr*0.9","b8528d7c":"def train():\n    time_start = time.time()\n\n    model = ResNet50_model()\n    \n    model.summary()\n    \n    callback = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n    \n    train_history = model.fit(train_dataset, epochs=epochs, callbacks=[callback])\n\n    model.save('mymodel.h5')\n    \n    print('Model saved.')\n    \n    time_end = time.time()\n    print('Training Time:', time_end - time_start)\n    print('\\n')\n\n    return train_history","b02a1084":"def show_train_history(train_history, index):\n    plt.plot(train_history.history[index])\n    plt.title('Train History')\n    plt.ylabel(index)\n    plt.xlabel('Epoch')\n    plt.show()","b4aa2879":"def test(test_labels):\n    test_labels = np.array(test_labels)\n    model = load_model('\/kaggle\/working\/mymodel.h5')\n    \n    print('Testing:')\n    \n    model.evaluate(test_dataset)\n    \n    predIdxs = model.predict(test_dataset)\n    predIdxs = np.argmax(predIdxs, axis=1) \n\n    target_names = ['COVID-19', 'NORMAL', 'Viral Pneumonia']\n    print('\\n')\n    print(classification_report(test_labels, predIdxs, target_names=target_names))","319a924f":"if __name__ == \"__main__\":\n    train_filenames, train_labels, test_filenames, test_labels = read_directory()\n\n    build_train_tfrecord(train_filenames, train_labels)\n    build_test_tfrecord(test_filenames, test_labels)\n\n    train_dataset = get_train_dataset(train_tfrecord)\n    test_dataset = get_test_dataset(test_tfrecord)\n\n    print('Info of train_dataset', type(train_dataset))\n    print('Info of test_dataset', type(test_dataset))\n\n    train_dataset, test_dataset = data_Preprocessing(train_dataset, test_dataset) \n\n    train_history = train()\n    \n    test(test_labels)\n    \n    show_train_history(train_history, 'sparse_categorical_accuracy')\n    \n#     for filename in os.listdir('\/kaggle\/working'): # avoid filling up the disk, if you want to save the tfrecords, just '#' the 2 lines.\n#         os.remove('\/kaggle\/working\/' + filename)","d6eadf88":"# Decode TFRecord and get data","3a5d08ae":"# Train&Test","442c8cb2":"# Load data","926d2b6f":"# Build TFRecord","615322f5":"# Initialization","6a636515":"# Intro: \nThis script classifies the images to 3 categories: **COVID-19**, **NORMAL** and **Viral Pneumonia**. This script also includes the use of **tfrecord**. ","dd37bf4a":"# Model"}}