{"cell_type":{"7ec70470":"code","9a56b688":"code","ab0b96d1":"code","725da12c":"code","12a9abce":"code","314e1371":"code","2627cce0":"code","14e9956b":"code","1acde985":"code","2e68db94":"code","2e0eb075":"code","74614fb7":"code","75136ea3":"code","16b31b48":"code","d06e86db":"code","932ff7a0":"code","937ebe0e":"code","aad39344":"code","f295adb4":"code","87510001":"code","9b80269d":"code","25bdb0a6":"code","99560b33":"code","6b795613":"code","bce6a939":"code","4a3917a7":"code","408c47fa":"code","a112fd1d":"code","01c3b513":"code","68cb17bd":"code","2ca1597a":"code","0a253731":"code","7c6f2486":"code","5220c98c":"code","57084de5":"code","b53373c8":"code","98f399aa":"code","79706723":"code","a78cb7aa":"code","76e9f39a":"code","dbdbc847":"code","62235834":"code","84c3cb4e":"code","de649154":"code","2a23b573":"code","f3664822":"code","48332980":"code","46d52009":"code","266d20af":"code","bbfa6882":"code","d3456b7f":"code","4f54f404":"code","844c83fb":"code","1dfe9db0":"markdown","e422d205":"markdown","692d5ce7":"markdown","dd17cd07":"markdown","faa3135d":"markdown","deeafc89":"markdown","37f5150a":"markdown","088240d7":"markdown","153f24da":"markdown","3461f034":"markdown","b55e5312":"markdown","88a4db2e":"markdown","248ce885":"markdown","b64a3351":"markdown","e9251f15":"markdown","46496f25":"markdown","84382800":"markdown","0fb0176f":"markdown","b00e710d":"markdown","b9557f52":"markdown","e52b98a6":"markdown","5ed1c81c":"markdown","f799692c":"markdown","8afa2102":"markdown","666b23ba":"markdown","0d58b8e4":"markdown","4c30a35e":"markdown","b38c70b2":"markdown","7fa0739e":"markdown","6adc0484":"markdown","3ddfff03":"markdown","94989012":"markdown","7aab7162":"markdown"},"source":{"7ec70470":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9a56b688":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","ab0b96d1":"train.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\ntrain.shape, test.shape","725da12c":"train.describe().T #transposes the actual describe","12a9abce":"sns.distplot(train['SalePrice']);","314e1371":"#correlation matrix\n\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corrmat, vmax=.8, square=True);","2627cce0":"#saleprice correlation matrix\n#k = 10 #number of variables for heatmap\n\ncols = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","14e9956b":"#Graph for SalePrice v\/s OverallQual\n\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","1acde985":"#Graph for SalePrice v\/s GrLivArea\n\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","2e68db94":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Graph for SalePrice v\/s GrLivArea after deleting outliers\n\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","2e0eb075":"#Graph for SalePrice v\/s GarageCars\n\nvar = 'GarageCars'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","74614fb7":"#Graph for SalePrice v\/s GarageArea\n\nvar = 'GarageArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","75136ea3":"#Graph for SalePrice v\/s TotalBsmtSF\n\nvar = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","16b31b48":"#Graph for SalePrice v\/s 1stFlrSF\n\nvar = '1stFlrSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","d06e86db":"#Graph for SalePrice v\/s FullBath\n\nvar = 'FullBath'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","932ff7a0":"#Graph for SalePrice v\/s TotRmsAbvGrd\n\nvar = 'TotRmsAbvGrd'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","937ebe0e":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train['SalePrice'].reset_index(drop=True)\n\nsns.distplot(train['SalePrice']);","aad39344":"train.shape, test.shape","f295adb4":"combine = pd.concat([train, test], sort = False).reset_index(drop=True)\ncombine.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Size of combined data set is : {}\".format(combine.shape))","87510001":"combine.describe()","9b80269d":"def miss_perc(df):\n  df_null_data = (df.isnull().sum() \/ len(combine)) * 100\n  df_null_data = df_null_data.drop(df_null_data[df_null_data == 0].index).sort_values(ascending=False)[:30]\n  return pd.DataFrame({'Missing Percentage' :df_null_data})\n\nmiss_perc(combine)","25bdb0a6":"combine['MSSubClass'] = combine['MSSubClass'].apply(str)\ncombine['YrSold'] = combine['YrSold'].astype(str)\ncombine['MoSold'] = combine['MoSold'].astype(str)","99560b33":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', \n            'BsmtHalfBath','GarageYrBlt', 'GarageArea','GarageCars','MasVnrArea'):\n    combine[col] = combine[col].fillna(0)\n\nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n            'Fence','PoolQC','MiscFeature','Alley','FireplaceQu','Fence','GarageType',\n            'GarageFinish', 'GarageQual', 'GarageCond']:\n    combine[col] = combine[col].fillna('None')\n\nfor col in ['Utilities','Exterior1st','Exterior2nd','SaleType','Functional','Electrical',\n            'KitchenQual', 'GarageFinish', 'GarageQual', 'GarageCond','MasVnrType']:\n    combine[col] = combine[col].fillna(combine[col].mode()[0])\n\ncombine['LotFrontage'] = combine.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\ncombine['MSZoning'] = combine.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nmiss_perc(combine)","6b795613":"categorical_features = combine.dtypes[combine.dtypes == \"object\"].index\n\ncombine.update(combine[categorical_features].fillna('None'))\n\ncategorical_features","bce6a939":"numerical_features = combine.dtypes[combine.dtypes != \"object\"].index\n\ncombine.update(combine[numerical_features].fillna(0))\n\nnumerical_features","4a3917a7":"from scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax # for statistics\nfrom scipy.special import boxcox1p","408c47fa":"skewed_features = combine[numerical_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_features","a112fd1d":"high_skew_feat = skewed_features[abs(skewed_features) > 0.5]\nskewed_features = high_skew_feat.index\n\nfor feature in skewed_features:\n  combine[feature] = boxcox1p(combine[feature], boxcox_normmax(combine[feature] + 1))","01c3b513":"combine = combine.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\ncombine['TotalSF'] = combine['TotalBsmtSF'] + combine['1stFlrSF'] + combine['2ndFlrSF']\n\ncombine['YrBltAndRemod'] = combine['YearBuilt']+ combine['YearRemodAdd']\n\ncombine['Total_sqr_footage'] = (combine['BsmtFinSF1'] + combine['BsmtFinSF2'] + combine['1stFlrSF'] + combine['2ndFlrSF'])\n\ncombine['Total_Bathrooms'] = (combine['FullBath'] + (0.5 * combine['HalfBath']) + combine['BsmtFullBath'] + (0.5 * combine['BsmtHalfBath']))\n\ncombine['Total_porch_sf'] = (combine['OpenPorchSF'] + combine['3SsnPorch'] + combine['EnclosedPorch'] + combine['ScreenPorch'] +\n                             combine['WoodDeckSF'])\n\ncombine['haspool'] = combine['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\ncombine['has2ndfloor'] = combine['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\ncombine['hasgarage'] = combine['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\ncombine['hasbsmt'] = combine['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n\ncombine['hasfireplace'] = combine['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","68cb17bd":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(combine[c].values)) \n    combine[c] = lbl.transform(list(combine[c].values))","2ca1597a":"combine = pd.get_dummies(combine)\nprint(combine.shape)","0a253731":"X = combine.iloc[:len(y), :]\nX_sub = combine.iloc[len(y):, :]","7c6f2486":"X.shape, y.shape, X_sub.shape","5220c98c":"overfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()\noverfit","57084de5":"from datetime import datetime\n\nfrom sklearn.linear_model import ElasticNetCV, Lasso, ElasticNet, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nimport sklearn.linear_model as linear_model","b53373c8":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","98f399aa":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","79706723":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))","a78cb7aa":"lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))","76e9f39a":"ENet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))","dbdbc847":"svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","62235834":"XGBoostR = XGBRegressor(learning_rate=0.01,n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7, \n                       objective='reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006, silent = True)","84c3cb4e":"CatBoostR = CatBoostRegressor(iterations=500, learning_rate=0.05, depth=10, eval_metric='RMSE', random_seed = 42,\n                        bagging_temperature = 0.2, od_type='Iter', metric_period = 50, od_wait=20)","de649154":"LightGBMR = LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.01, n_estimators=5000, max_bin=200, bagging_fraction=0.75,\n                                       bagging_freq=5, bagging_seed=7, feature_fraction=0.2, feature_fraction_seed=7, verbose=-1, )","2a23b573":"StackCVR_gen = StackingCVRegressor(regressors=(ridge, lasso, ENet, CatBoostR, XGBoostR, LightGBMR), \n                                meta_regressor=XGBoostR, use_features_in_secondary=True)","f3664822":"# Using various prediction models that we just created \n\nscore = cv_rmse(ridge , X)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso , X)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(ENet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(LightGBMR)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(CatBoostR)\nprint(\"catboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(XGBoostR)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","48332980":"print('START Fit')\n\nprint('stack_gen')\nstack_gen_model = StackCVR_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = ENet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('catboost')\ncbr_model_full_data = CatBoostR.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = XGBoostR.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = LightGBMR.fit(X, y)","46d52009":"def blend_models_predict(X):\n    return ((0.15 * elastic_model_full_data.predict(X)) + \\\n            (0.15 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.05 * cbr_model_full_data.predict(X)) + \\\n            (0.05 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","266d20af":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","bbfa6882":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))","d3456b7f":"print('Blend with Top Kernels submissions\\n')\nsub_1 = pd.read_csv('https:\/\/raw.githubusercontent.com\/AnujPR\/Kaggle-Hybrid-House-Prices-Prediction\/master\/masum_rumia-detailed-regression-guide-with-house-pricing%20submission.csv')\nsub_2 = pd.read_csv('https:\/\/raw.githubusercontent.com\/AnujPR\/Kaggle-Hybrid-House-Prices-Prediction\/master\/serigne_stacked-regressions-top-4-on-leaderboard_submission.csv')\nsub_3 = pd.read_csv('https:\/\/raw.githubusercontent.com\/AnujPR\/Kaggle-Hybrid-House-Prices-Prediction\/master\/jesucristo1-house-prices-solution-top-1_new_submission.csv')\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models_predict(X_sub)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))","4f54f404":"q1 = submission['SalePrice'].quantile(0.0042)\nq2 = submission['SalePrice'].quantile(0.99)\n# Quantiles helping us get some extreme values for extremely low or high values \nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission.csv\", index=False)","844c83fb":"submission.head()","1dfe9db0":"Used for location using index of the data in the feature. <br><br>\nX is the feature input file and X_sub is a similar file which will be used later for the submision file.","e422d205":"We generated scatter plots of the Sale Price v\/s the 9 variables with highest correlation values with it.","692d5ce7":"We start off by importing the basic data preprocessing, analysis and visualisation packages.<br><br>\nNext we read the training & test set files. ","dd17cd07":"The next code cells groups together the categorical and numerical features. ","faa3135d":"Now when we called __miss_perc__, we saw that all the missing values had been handled.","deeafc89":"Box-Cox transformation is used here, to make the features with high skewness normally distributed. <br><br>Box Cox is useful for highly skewed non-positive data. See [here](https:\/\/stats.stackexchange.com\/questions\/339589\/box-cox-log-or-arcsine-transformation) or [here](https:\/\/stats.stackexchange.com\/a\/1452) for more explanation.","37f5150a":"## 10. Blending our models","088240d7":"Combining the training and testing sets allows us to clean and pre-process the data together and hence, efficiently.","153f24da":"Removing the features that overfit.","3461f034":"Looks like there aren't any","b55e5312":"## 4. Handling missing values\n\nHaving observed the datasets we know that there are a number of missing data entries for every house in them.<br><br>Handling them may be important because they might cause problems in our model. [Read about it here](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3668100\/)<br><br>We defined the __miss_perc__ function to tell us about which variables have missing data and quantises it.","88a4db2e":"## 7. Creating models\n\nImporting the necessary packages for defining the models we wish to use. They are all regression models as is required for this problem","248ce885":"## 8. Stacking \n##            and\n## 9. Scores based on RMSE values","b64a3351":"Alphas arrays; i.e., the list of parameters that we use here within ridge, lasso and Elastic net models as a **regularisation penalty parameter.**  ","e9251f15":"Grouping the features having skewness together","46496f25":"## 1. Importing packages and the datasets","84382800":"This function gave us the distribution of our **SalePrice** values.<br><br>As you can see; the histogram is skewed to the right.<br><br>\nTherefore, we need to do something to normalise the data distribution because most of the machine learning models work best on normally distributed data.","0fb0176f":"We generated a heatmap to tell us about the correlation between different variables.","b00e710d":"The Root Mean Squared Error is defined here to find the accuracy of our predictions. <br><br> KFold is a method of cross-validating the model ability on new data. See [here](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/)","b9557f52":"Our objective is to obtain a relation between the Sale Price of the houses in Ames, Iowa and the variables given in our dataset such that we may be able to predict the house prices of any other house based on the dataset.\n\n**Our Code consists of these parts:**\n<br><br>\n**1. Importing packages and the datasets** <br>\n**2. Data visualisation**<br>\n**3. Data Analysis**<br>\n**4. Handling missing values**<br>\n**5. Feature Engineering**<br>\n**6. Pre-processing the data**<br>\n**7. Creating models**<br>\n**8. Stacking**<br>\n**9. Scores based on RMSE values**<br>\n**10. Blending our models**<br>\n**11. Ensembling with outputs of better performing models**","e52b98a6":"Now we will fill our missing data entries with values most suitable for their type.<br><br>\n\nFirst, we fill the numerical features with the value 0, because given the data description and upon some thinking, it is likely that these values are missing because the feature they are associated to is not a feature of the house.<br><br>\nSecond, these are the categorical features, which need to have an object type data type such as string and similarly from observation, we place in the missing entries the value 'None'<br><br>\nThird, there are very few empty values in these remaining columns, so we will us the mode to fill them out.<br><br>\nThe __LotFrontage__ was filled with the median of the values of LotFrontage of houses of every __Neighborhood__.<br><br>\nThe __MSZoning__ was filled with the median of the values of MSZoning of their respective __MSSubClass__.","5ed1c81c":"__Utilities, Street and PoolQC__ are observed to be uninfluential to the SalePrice.<br><br>\nWe have created new features here, by the name of __TotalSF__ etc. which are self explanatory.","f799692c":"## 5. Feature Engineering\n\nImporting packages to help with transformation of the features as more of them may be skewed.","8afa2102":"Categorical features are converted into dummy\/indicator variables by means of this function. At this point, get_dummies and LabelEncoder seem similar. Please check out this [link](https:\/\/stats.stackexchange.com\/questions\/369428\/deciding-between-get-dummies-and-labelencoder-for-categorical-variables-in-a-lin) to get rid of confusion.","666b23ba":"These features are categorical in nature and the model may mistakenly consider them as numerical features. Thence, we establish them as type __String__","0d58b8e4":"In many cases, taking the log greatly reduces the variation of a variable making estimates less prone to outlier influence.<br><br>\nThat justifies a logarithmic transformation. Taking the log of saleprice as new SalePrice values removes to a great extent the skewness of the SalePrice distribution. Now we have a somewhat normally distributed histogram.","4c30a35e":"These models are all different methods of regression. Please read the documnetation of each to understand the details and the choice of parameters. <br><br>\nThe __RobustScaler()__ is used because we have not handled the outliers very well.<br><br>\nWe do not want the models to succumb to inaccuracy in prediction due to non-robustness of our model arising due to the presence of these outliers.<br><br>\n**make_pipeline** can be understood as an entity that allows for sequential assembly of multiple transforms on a dataset.<br<br>In our case we combine the robust scaler and estimator and cross validator module.","b38c70b2":"## 6. Pre-processing the data\n\nEncoding the categorical features. These features have data entries which are text format, not understandable by the model.\nThis cell converts the different text categories in to numeric categories.<br><br>\nThe features below have more than two types of categories.They are not merely columns to store the data for presence or absence of a feature of a house.","7fa0739e":"Know more about boxcox transform here : http:\/\/blog.minitab.com\/blog\/applying-statistics-in-quality-projects\/how-could-you-benefit-from-a-box-cox-transformation","6adc0484":"## 2. Data visualisation\n## 3. Data Analysis \n\nThese will be performed simultaneously as plots will give us insights to the importance and distribution of features with respect to the __SalePrice__ target variable","3ddfff03":"The Id column is unnecessary as it is just the serial number of every data entry and will have no relation with the sale price whatsoever.<br><br>So we drop it from our dataset.","94989012":"## 11. Ensembling with outputs of better performing models","7aab7162":"The above graph tells us about the variables with the **10 highest values of correlation with SalePrice values** "}}