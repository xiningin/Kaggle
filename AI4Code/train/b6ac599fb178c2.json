{"cell_type":{"95809d8e":"code","10c287aa":"code","c1f0f179":"code","c9d4b777":"code","3426481e":"code","954b2b01":"code","c8657e1e":"code","f9167ab4":"code","dd1409a1":"code","a08815ab":"code","63c4b845":"code","4cbe6efe":"code","c4c4c7e5":"code","04383bb7":"code","459145bc":"code","eb6f80d4":"code","22f226a6":"code","1bfc8418":"code","b632fc7d":"code","bc428200":"code","f890e986":"code","e8956ef8":"code","f8f91032":"code","42fd25d7":"markdown","ee156f9f":"markdown","0374b020":"markdown","8fb64b22":"markdown","90c0e2b8":"markdown","8459d0ce":"markdown","75ef60ae":"markdown","be538433":"markdown","199797fe":"markdown","c434ccb3":"markdown","8caedf23":"markdown","2ada7127":"markdown","a69b5c92":"markdown","63b56c49":"markdown","2196ddc6":"markdown"},"source":{"95809d8e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Reshape, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers.schedules import ExponentialDecay\nfrom keras.models import load_model\nfrom itertools import chain\n\npd.options.mode.chained_assignment = None  # default='warn'","10c287aa":"exoTrain = pd.read_csv('\/kaggle\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\nexoTest = pd.read_csv('\/kaggle\/input\/kepler-labelled-time-series-data\/exoTest.csv')\n\nexoTrain.tail(5)","c1f0f179":"exoTest.tail(5)","c9d4b777":"print(exoTrain['LABEL'].value_counts())\nprint(exoTest['LABEL'].value_counts())","3426481e":"def flux_graph(dataset, row, dataframe, planet):\n    if dataframe:\n        fig = plt.figure(figsize=(20,5))\n        ax = fig.add_subplot()\n        ax.set_title(planet, color='black', fontsize=22)\n        ax.set_xlabel('time', color='black', fontsize=18)\n        ax.set_ylabel('flux_' + str(row), color='black', fontsize=18)\n        ax.grid(False)\n        flux_time = list(dataset.columns)\n        flux_values = dataset[flux_time].iloc[row]\n        ax.plot([i + 1 for i in range(dataset.shape[1])], flux_values, 'black')\n        ax.tick_params(colors = 'black', labelcolor='black', labelsize=14)\n        plt.show()\n    else:\n        fig = plt.figure(figsize=(20,5))\n        ax = fig.add_subplot()\n        \n        ax.set_title(planet, color='black', fontsize=22)\n        ax.set_xlabel('time', color='black', fontsize=18)\n        ax.set_ylabel('flux_' + str(row), color='white', fontsize=18)\n        ax.grid(False)\n        flux_values = dataset[row]\n        ax.plot([i + 1 for i in range(dataset.shape[1])], flux_values, 'black')\n        ax.tick_params(colors = 'black', labelcolor='black', labelsize=14)\n        plt.show()","954b2b01":"def show_graph(dataframe, dataset):\n    with_planet = exoTrain[exoTrain['LABEL'] == 2].head(3).index\n    wo_planet = exoTrain[exoTrain['LABEL'] == 1].head(3).index\n\n    for row in with_planet:\n        flux_graph(dataset, row, dataframe, planet = 'periodic dip due to transiting planet')\n    for row in wo_planet:\n        flux_graph(dataset, row, dataframe, planet = 'no transiting planet')","c8657e1e":"show_graph(True, dataset = exoTrain.loc[:, exoTrain.columns != 'LABEL'])","f9167ab4":"scaler = StandardScaler()\nscaled_data = scaler.fit_transform(exoTrain.loc[:, exoTrain.columns != 'LABEL'])\nshow_graph(False, scaled_data)","dd1409a1":"def handle_outliers(dataset, num_iterations):\n    \n    #threshold = None\n    dataset_handled = dataset\n\n    for n in range(num_iterations):\n        #for column in range(dataset_handled.shape[0]):\n        for index, row in dataset_handled.iterrows():\n            row_values = row.values\n            row_max, row_min = row_values.max(), row_values.min()\n            row_maxidx, row_minidx = row_values.argmax(), row_values.argmin()\n            row_mean = row_values.mean()\n\n            #if np.abs(column_max\/column_mean) >= threshold:\n            dataset_handled.iloc[index][row_maxidx] = row_mean\n\n            #if np.abs(column_min\/column_mean) >= threshold:\n            dataset_handled.iloc[index][row_minidx] = row_mean\n\n    return dataset_handled","a08815ab":"handled_dataset = handle_outliers(exoTrain.loc[:, exoTrain.columns != 'LABEL'], 2)\nshow_graph(True, handled_dataset)","63c4b845":"def lable_change(y_train, y_test):\n    labler = lambda x: 1 if x == 2 else 0\n    y_train_01, y_test_01 = y_train.apply(labler), y_test.apply(labler)\n\n    return y_train_01, y_test_01","4cbe6efe":"def smote(x_train, y_train):\n    #smote = SMOTE(random_state=17, sampling_strategy='minority')\n    over = SMOTE(sampling_strategy=0.2)\n    under = RandomUnderSampler(sampling_strategy=0.3)\n    steps = [('o', over), ('u', under)]\n    pipeline = Pipeline(steps=steps)\n    x_train_res, y_train_res = pipeline.fit_resample(x_train, y_train)\n\n    return x_train_res, y_train_res","c4c4c7e5":"y_smote_test = exoTrain.loc[:, 'LABEL']\nprint(y_smote_test.value_counts())\n_, y_smote_test = smote(handled_dataset, y_smote_test)\nprint(y_smote_test.value_counts())","04383bb7":"# Define training and testing datasets\ndef datasets():\n    x_train, y_train = exoTrain.loc[:, exoTrain.columns != 'LABEL'], exoTrain.loc[:, 'LABEL']\n    x_test, y_test = exoTest.loc[:, exoTest.columns != 'LABEL'], exoTest.loc[:, 'LABEL']\n    \n    #fill NaNs with mean (no NaNs)\n    #for column in x_train:\n        #x_train[column] = x_train[column].fillna(round(x_train[column].mean(), 2))\n\n    #x_train, x_test = scale(x_train, x_test)\n    x_train = handle_outliers(x_train, 2)\n    x_train, y_train = smote(x_train, y_train)\n    y_train, y_test = lable_change(y_train, y_test)\n\n    n_features = x_train.shape[1]\n\n    return x_train, y_train, x_test, y_test, n_features","459145bc":"# Graph train and test accuracy\ndef graph_acc(history):\n    # Plot loss during training\n    plt.subplot(211)\n    plt.title('Loss')\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='test')\n    plt.legend()\n\n    # Plot accuracy during training\n    plt.subplot(212)\n    plt.title('Accuracy')\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='test')\n    plt.legend()\n    plt.show()","eb6f80d4":"# Confusion matrix\ndef conf_matrix(y_test, y_pred):\n\n    matrix = confusion_matrix(y_test, y_pred)\n    df_cm = pd.DataFrame(matrix, columns=[0, 1], index = [0, 1])\n    df_cm.index.name = 'Truth'\n    df_cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    sn.set(font_scale=1.4) \n    sn.heatmap(df_cm, cmap=\"BuGn\", annot=True, annot_kws={\"size\": 16})\n    plt.show()\n    \n    return matrix","22f226a6":"# Print prediction metrics\ndef prediction_metrics(y_test, y_pred, y_class_pred, matrix):\n    FP = matrix[0][1] \n    FN = matrix[1][0]\n    TP = matrix[1][1]\n    TN = matrix[0][0]\n\n    sens = TP\/(TP+FN)\n    spec = TN\/(TN+FP) \n    g_mean = np.sqrt(sens * spec)\n\n    accuracy = accuracy_score(y_test, y_class_pred)\n    balanced_accuracy = balanced_accuracy_score(y_test, y_class_pred)\n    precision = precision_score(y_test, y_class_pred)\n    recall = recall_score(y_test, y_class_pred)\n    f1 = f1_score(y_test, y_class_pred)\n    auc = roc_auc_score(y_test, y_pred)\n\n    print('\\t\\t Prediction Metrics\\n')\n    print(\"Accuracy:\\t\", \"{:0.3f}\".format(accuracy))\n    print(\"Precision:\\t\", \"{:0.3f}\".format(precision))\n    print(\"Recall:\\t\\t\", \"{:0.3f}\".format(recall))\n    print(\"\\nF1 Score:\\t\", \"{:0.3f}\".format(f1))\n    print(\"ROC AUC:\\t\", \"{:0.3f}\".format(auc))\n    print(\"Balanced\\nAccuracy:\\t\", \"{:0.3f}\".format(balanced_accuracy))\n    print(\"\\nSensitivity:\\t\", \"{:0.3f}\".format(sens))\n    print(\"Specificity:\\t\", \"{:0.3f}\".format(spec))\n    print(\"Geometric Mean:\\t\", \"{:0.3f}\".format(g_mean))","1bfc8418":"def cnn_model():\n\n    # Data preparation\n    x_train, y_train, x_test, y_test, n_features = datasets()\n    x_train, y_train = shuffle(x_train, y_train) # shuffle the data to avoid stagnant 0.0000e+00 val_accuracy\n\n    # Architecture\n    model = Sequential()\n    model.add(Reshape((3197, 1), input_shape=(3197,)))\n    model.add(Conv1D(filters=10, kernel_size=2, activation='relu', input_shape=(n_features, 1), kernel_regularizer='l2'))\n    model.add(MaxPooling1D(pool_size=2, strides=2))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(48, activation=\"relu\"))\n    model.add(Dropout(0.4))\n    model.add(Dense(18, activation=\"relu\"))\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    # Representation of architecture\n    print(model.summary())\n\n    # Compile model\n    lr_schedule = ExponentialDecay(initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.94)\n\n    model.compile(optimizer = Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Fit model\n    early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n\n    history = model.fit(x_train, y_train, validation_split = 0.2, batch_size=64, callbacks=[early_stop], epochs=30, verbose=2)\n\n    # Evaluate the model\n    _, train_acc = model.evaluate(x_train, y_train, verbose=2)\n    _, test_acc = model.evaluate(x_test, y_test, verbose=2)\n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n\n    # Prediction\n    y_class_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n    y_pred = model.predict(x_test)\n\n    # Accuracy graph\n    graph_acc(history)\n\n    # Confustion matrix\n    matrix = conf_matrix(y_test, y_class_pred)\n\n    # Metrics\n    prediction_metrics(y_test, y_pred, y_class_pred, matrix)\n    \n    return model\n","b632fc7d":"cnn_model1=cnn_model() #For use on the conversion mudule later","bc428200":"from keras import layers, models\n\ninput_layer = layers.Input(batch_shape=cnn_model1.layers[0].input_shape)\nprev_layer = input_layer\nfor layer in cnn_model1.layers:\n    prev_layer = layer(prev_layer)\n\nfuncmodel = models.Model([input_layer], [prev_layer])","f890e986":"!pip install nengo-dl #install nengo-dl if running first time\nimport nengo\nimport nengo_dl\n\nimport tensorflow as tf\n#print(cnn_model1.summary())\n\nsfr = 20\nconverter = nengo_dl.Converter(\n    funcmodel,\n    swap_activations={\n        tf.keras.activations.relu: nengo.SpikingRectifiedLinear()},\n    scale_firing_rates=sfr,\n    synapse=0.005,\n    inference_only=False)\n","e8956ef8":"x_train, y_train, x_test, y_test, n_features = datasets()\n\n\nprint(\"original train \", x_train.shape,\"original ytrain label \", y_train.shape)\nprint(\"original test \", x_test.shape,\"original ytest label \", y_test.shape)\nprint(\"\\n\")\n\nx_train, y_train, x_test, y_test = x_train.to_numpy(), y_train.to_numpy(), x_test.to_numpy(), y_test.to_numpy()\n\nx_train = x_train.reshape((x_train.shape[0], 1, -1))\ny_train = y_train.reshape((y_train.shape[0],1,-1))\n\nx_test = x_test.reshape((x_test.shape[0], 1, -1))\ny_test = y_test.reshape((y_test.shape[0],1,-1))\n\n#an_array = np.where(an_array > 20, 0, an_array)\n\ny_train=np.where(y_train==1,0.9,y_train)\ny_test=np.where(y_test==1,0.9,y_test)\n\nprint(\"timestep train \", x_train.shape,\"timestep train label \", y_train.shape)\nprint(\"timestep test \", x_test.shape,\"timestep test label \", y_test.shape)","f8f91032":"do_training = True #Just a switch to apply training step in SNN. This is according to the documentation of nengo-dl\n\nif do_training:\n    with nengo_dl.Simulator(converter.net, minibatch_size=200) as sim:\n        # run training\n        sim.compile(\n            optimizer=tf.optimizers.Adam(0.001),\n            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[tf.metrics.sparse_categorical_accuracy],\n        )\n        sim.fit(\n            {converter.inputs[converter.model.input]: x_train},\n            {converter.outputs[converter.model.output]: y_train},\n            validation_data=(\n                {converter.inputs[converter.model.input]: x_test},\n                {converter.outputs[converter.model.output]: y_test},\n            ),\n            epochs=10,\n        )\n\n        # save the parameters to file\n        sim.save_params(\".\/keras_to_snn_params\")","42fd25d7":"\n### Data Preprocessing","ee156f9f":"**Define the dataset with timestep to feed into the Spiking Neural Netowrk**","0374b020":"* **Input layer**;\n* **1D convolutional layer**, consisting of 10 2x2 filters, L2 regularization and RELU activation function;\n* **1D max pooling layer**, window size - 2x2, stride - 2;\n* **Dropout** with 20% probability;\n* **Fully connected layer** with 48 neurons and RELU activation function;\n* **Dropout** with 40% probability;\n* **Fully connected layer** with 18 neurons and RELU activation function;\n* **Output layer** with sigmoid function.\n\nOptimizer: **Adam**  loss function: **binary-crossentropy** , **batch size**: 64 **epochs**:30 with **exponential decay** and **early stopping**.","8fb64b22":"**Problem Statement**\n\nExoplanet detection through astronomical data has been a celebrated problem. This problem is known for its peculiar challenges for the astronomers. Advances in telescope technology have made it possible to install telescopes in space to obtain images from objects lightyears away. Thus humongous datasets have come existence which are sometimes difficult to analyse and interpret.\n \nKepler telescope was commissioned in 2009 for detection of exoplanets. Over the years the dataset obtained from the Kepler telescope has gained a reputation in astronomical data analysis similar to MNIST dataset in statistical data analysis. Since the launch of Kepler nearly all latest exoplanet discoveries are made using Artificial intelligence\/data science\/machine learning [AI\/ML]. \n\n \nIn recent times, state-of-the-art machine learning approaches including k-nearest neighbour, Principal Component Analysis, Convolution Neural Network, Recurrent Neural Network have been applied to the Kepler's noisy dataset. The results have been encouraging. In the year 2020 alone, 261 exoplanets were detected using ML.  However each of these methods suffer their own specific challenges. The analysis of Kepler astronomical data is  prone to high number of false positives.\n \nSpiking Neural Network (SNN) is the latest addition in the line of emerging technologies proving to be a viable solution for the challenging problems in machine learning. Its performance has been noticed to improve year by year.\n \nIn this mini-project, I have used a deep learning neural network architecture for exoplanet planet detection from Kepler data. The dataset has been taken from Kaggle exoplanet competion dataset.\n\nThereafter I have converted the same architecture into a Spiking Neural Network using nengo-dl library.\n","90c0e2b8":"**Now lets convert the CNN Model to Spiking Neural Network using Nengo-DL**","8459d0ce":"Read the Data","75ef60ae":"**Import needed libraries**","be538433":"### Define Training and the Test Data","199797fe":"*Reference: This notebook is inspired by a solution to the problem of Exoplanet detection competition held on Kaggle by Anton Zaitsev. Reference Exoplanet Hunting: TOP Score using SMOTE and CNN https:\/\/www.kaggle.com\/antonzv\/*","c434ccb3":"**Outlier handling**","8caedf23":"#### Apply SMOTE","2ada7127":"**Scaling**","a69b5c92":"### **CNN Model**","63b56c49":"**Generate Graph of the flux values ie magnitude of the light of a particular star**","2196ddc6":"# Exoplanet detection from Kepler data using Spiking Neural Network\n"}}