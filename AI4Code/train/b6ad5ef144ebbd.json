{"cell_type":{"18ddadf0":"code","2fa90087":"code","d74b542e":"code","ffe8d8d1":"code","74f1f46d":"code","5c638a2d":"code","e748db7b":"code","2d35449b":"code","da036c8c":"code","3132a6cf":"code","a1160a53":"code","0d3cc526":"markdown","66a5b343":"markdown","c01fd745":"markdown","6c75c50a":"markdown","28011beb":"markdown","9c70c5a3":"markdown","9cdeb8c8":"markdown","9475377e":"markdown","4f6f1540":"markdown","1f89be3b":"markdown"},"source":{"18ddadf0":"!pip install fairseq\n!pip install fastBPE\n!pip install transformers","2fa90087":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import * \nfrom tensorflow.keras.layers import *","d74b542e":"from nltk.tokenize import TweetTokenizer\nfrom emoji import demojize\nimport re\n\ntokenizer = TweetTokenizer()\n\ndef normalizeToken(token):\n    lowercased_token = token.lower()\n    if token.startswith(\"@\"):\n        return \"@USER\"\n    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n        return \"HTTPURL\"\n    elif len(token) == 1:\n        return demojize(token)\n    else:\n        if token == \"\u2019\":\n            return \"'\"\n        elif token == \"\u2026\":\n            return \"...\"\n        else:\n            return token\n\ndef normalizeTweet(tweet):\n    tokens = tokenizer.tokenize(tweet.replace(\"\u2019\", \"'\").replace(\"\u2026\", \"...\"))\n    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n\n    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n\n    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n    normTweet = re.sub(r\"([0-9]{1,3}) \/ ([0-9]{2,4})\", r\"\\1\/\\2\", normTweet)\n    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n    \n    return \" \".join(normTweet.split())","ffe8d8d1":"from types import SimpleNamespace\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\n\nclass BERTweetTokenizer():\n    \n    def __init__(self,pretrained_path):\n        bpe_dir = os.path.join(pretrained_path,\"bpe.codes\")\n        vocab_dir = os.path.join(pretrained_path,\"dict.txt\")\n        \n        self.bpe = fastBPE(SimpleNamespace(bpe_codes= bpe_dir))\n        self.vocab = Dictionary()\n        self.vocab.add_from_file(vocab_dir)\n        \n        self.cls_token_id = 0\n        self.pad_token_id = 1\n        self.sep_token_id = 2\n        \n        self.pad_token = '<pad>'\n        self.cls_token = '<s>'\n        self.sep_token = '<\/s>'\n        \n    def bpe_encode(self,text):\n        return self.bpe.encode(text)\n    \n    def encode(self,text,add_special_tokens=False):\n        subwords = self.bpe.encode(text)\n        input_ids = self.vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    def tokenize(self,text):\n        return self.bpe_encode(text).split()\n    \n    def convert_tokens_to_ids(self,tokens):\n        input_ids = self.vocab.encode_line(' '.join(tokens), append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    #from: https:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look\n    def decode_id(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@')\n    \n    def decode_id_nospace(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@ ')\n\ntokenizer = BERTweetTokenizer('\/kaggle\/input\/bertweet-base-transformers')","74f1f46d":"def read_train():\n    train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n    train['text']=train['text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","5c638a2d":"def Data_encode(Dataset,MAX_LEN):\n    ct = Dataset.shape[0]\n    tokens = np.ones((ct,MAX_LEN),dtype='int32')\n    masks = np.zeros((ct,MAX_LEN),dtype='int32')\n    segs = np.zeros((ct,MAX_LEN),dtype='int32')\n\n    for k in range(ct):        \n        # INPUT_IDS\n        text = normalizeTweet(Dataset.loc[k,'text'])\n        enc = tokenizer.encode(text)                   \n        if len(enc)<MAX_LEN-2:\n            tokens[k,:len(enc)+2] = [0] + enc + [2]\n            masks[k,:len(enc)+2] = 1\n        else:\n            tokens[k,:MAX_LEN] = [0] + enc[:MAX_LEN-2] + [2]\n            masks[k,:MAX_LEN] = 1 \n\n    return tokens,masks,segs\n\ntrain_tokens,train_masks,train_segs = Data_encode(train_df,128)\ntest_tokens,test_masks,test_segs = Data_encode(test_df,128)","e748db7b":"def f1(y_true,y_pred):\n    def recall(y_true,y_pred):\n        #TP:true==1&pred==1\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred,0,1)))\n        #TP+FN:true==1\n        possible_positives = K.sum(K.round(K.clip(y_true,0,1)))\n        #recall = TP\/(TP+FN)\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n    \n    def precision(y_true,y_pred):\n        #TP:true==1&pred==1\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred,0,1)))\n        #TP+FP:pred==1\n        prediction_positives = K.sum(K.round(K.clip(y_pred,0,1)))\n        #precision = TP\/(TP+FP)\n        precision = true_positives \/ (prediction_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true,y_pred)\n    recall = recall(y_true,y_pred)\n    return 2*(precision * recall)\/(precision + recall + K.epsilon())","2d35449b":"def build_model(MAX_LEN,PATH = '\/kaggle\/input\/bertweet-base-transformers\/'):\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config_dir = os.path.join(PATH,'config.json')\n    model_dir = os.path.join(PATH,'model.bin')\n    config = RobertaConfig.from_pretrained(config_dir)\n    bert_model = TFRobertaModel.from_pretrained(model_dir,config=config,from_pt=True)\n    x,_ = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    out=Dense(1,activation='sigmoid')(x[:,0,:])\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy',f1])\n\n    return model\n\nmodel = build_model(128)\nmodel.summary()","da036c8c":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3, restore_best_weights=True, verbose=1)","3132a6cf":"train_inp = [train_tokens,train_masks,train_segs]\ntrain_labels = train_df['target']\ntrain_history = model.fit(\n    train_inp, train_labels,\n    validation_split=0.2,\n    epochs=5,\n    batch_size=16,\n    verbose = 2,\n    callbacks = [es]\n)","a1160a53":"test_inp = [test_tokens,test_masks,test_segs]\ntest_pred = model.predict(test_inp)\nsubmission_df['target']=test_pred.round().astype(int)\nsubmission_df.to_csv(\"submission.csv\",index=False)","0d3cc526":"# Predict","66a5b343":"# Build Model","c01fd745":"# F1-Score","6c75c50a":"# Install Packages","28011beb":"# BERTweetTokenizer\nFrom: https:\/\/www.kaggle.com\/christofhenkel\/setup-tokenizer","9c70c5a3":"From: https:\/\/www.kaggle.com\/davelo12\/bertweet","9cdeb8c8":"# TweetNormalizer\nFrom:https:\/\/github.com\/VinAIResearch\/BERTweet\/blob\/master\/TweetNormalizer.py","9475377e":"# Data Process","4f6f1540":"# Training","1f89be3b":"# Load Data"}}