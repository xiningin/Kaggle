{"cell_type":{"88229451":"code","ab0b9b96":"code","96980626":"code","da3b2688":"code","abf7276c":"code","5f66aba9":"code","7dcf88a3":"code","7bd3a92c":"code","18c81fad":"code","5288ee8b":"code","329492dc":"code","29f97072":"code","2ad63629":"code","eecf74dd":"code","c6eff3e2":"code","c13145ec":"code","d6f64892":"code","71e2ffeb":"code","2c7f6837":"code","31e35334":"code","fe6c5026":"code","2d0da60d":"code","ea21a520":"code","27f53327":"code","2b073afa":"code","d34142ea":"code","b4ffc8c5":"code","6f3de45e":"code","516ae725":"code","60289392":"code","4cfe6b12":"code","5fa63769":"code","c984e1da":"code","f97b8ca5":"code","4db46eab":"code","e4c9eece":"code","292441d4":"code","1c00a732":"code","6137c442":"code","f3df6f64":"markdown","165e9806":"markdown","7f7c89d1":"markdown","61397f46":"markdown","bfc8c136":"markdown","d349edf6":"markdown","09e50344":"markdown","258fb011":"markdown","4f96215e":"markdown","3f639cd0":"markdown","034192fa":"markdown","cb7f7624":"markdown","e98cddf4":"markdown","21905080":"markdown","16becfab":"markdown","6b0e8c4f":"markdown","226b61d1":"markdown","33dcce18":"markdown"},"source":{"88229451":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings #supress warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Any results you write to the current directory are saved as output.","ab0b9b96":"import matplotlib.pyplot as plt\nimport pandas as pd\n#from math import pi\nfrom collections import Counter\nimport numpy as np\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore') \n\ndf = pd.read_csv('..\/input\/train.csv')#extract values\nTest = pd.read_csv('..\/input\/test.csv',index_col = None)#do not drop passenger id\nY = df['Survived'] # dependent variable\n\n","96980626":"cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\nprint(cols_with_missing)\ncols_with_missing2 = [col for col in Test.columns if Test[col].isnull().any()]\nprint(cols_with_missing2)","da3b2688":"for col in cols_with_missing:\n    if(df[col].dtype == np.dtype('O')):\n         df[col]= df[col].fillna(df[col].value_counts().index[0]) \n   #replace nan with most frequent\n    else:\n        df[col] = df[col].fillna(0) \n        #replace nan with median\nprint(df.isnull().any())\nprint(Test.isnull().any())","abf7276c":"\nfor col in cols_with_missing2:\n    if(Test[col].dtype == np.dtype('O')):\n         Test[col]= Test[col].fillna(Test[col].value_counts().index[0]) #replace nan with most frequent\n    else:\n        Test[col] = Test[col].fillna(0) \n        #replace nan with median\nprint(df.isnull().any())\nprint(Test.isnull().any())\n\n","5f66aba9":"#encoding and categorizng data\n\ndf['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nTest['Title'] = Test['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ndef replace_titles(x):\n    title = x['Title']\n    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme', 'Lady']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\ndf['Title'] = df.apply(replace_titles,axis = 1)\nTest['Title'] = Test.apply(replace_titles,axis = 1)\nfeatures= [ 'Title', 'Pclass','Sex','Fare','Embarked']\nX = df[features]","7dcf88a3":"df.groupby(df['Survived']).count()\n\n","7bd3a92c":"549\/(549+342)","18c81fad":"#plot how many survive\nnames = 'Died', 'Survived'\nsize = list(dict(Counter(df['Survived'])).values())\nfig = plt.figure(figsize=(18,8))\nfig.patch.set_facecolor('black')\nplt.rcParams['text.color'] = 'white'\nmy_circle=plt.Circle( (2,2), 0.9, color='black')\nplt.pie(size, labels=names)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.legend(loc='lower left')\nplt.title(\"Training Data- 'Died vs Survived' \")\nplt.show()","5288ee8b":"names = 'Mr', 'Mrs', 'Miss', 'Master'\nsize = list(dict(Counter(df['Title'])).values())\nfig = plt.figure(figsize=(18,8))\nfig.patch.set_facecolor('black')\nplt.rcParams['text.color'] = 'white'\nmy_circle=plt.Circle( (3,3), 0.9, color='black')\nplt.pie(size, labels=names)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.legend(loc='lower left')\nplt.show()\n","329492dc":"plt.rcParams['figure.figsize'] = (18, 7)\nplt.style.use('seaborn-dark-palette')\nsns.countplot(df['Title'], palette = 'rainbow')\nplt.title('Title Variations', fontsize = 20)","29f97072":"# Title,PClass vs Survived\nf,ax=plt.subplots(1,2,figsize=(25,10))\ndf.groupby(['Title','Survived'])['Title'].count().plot.bar(ax=ax[0])\nax[0].set_title(\"Title vs Survival\/Death\")\ndf.groupby(['Pclass','Survived'])['Pclass'].count().plot.bar(ax=ax[1])\nax[1].set_title('Passenger Class vs Survival')\n\n","2ad63629":"f1,ax1=plt.subplots(1,2,figsize=(25,10))\ndf.groupby(['Embarked','Survived'])['Embarked'].count().plot.bar(ax=ax1[0])\nax1[0].set_title(\"Embarked vs Survival\/Death\")\ndf.groupby(['Sex','Survived'])['Sex'].count().plot.bar(ax=ax1[1])\nax1[1].set_title('Sex vs Survival')","eecf74dd":"sns.pairplot(df, kind=\"scatter\",hue= 'Embarked', markers=[\"o\", \"s\", \"D\"],palette=\"Set2\")\nplt.show()","c6eff3e2":"sns.relplot(x=\"PassengerId\", y=\"Age\", hue=\"Title\", data=df)","c13145ec":"sns.relplot(x=\"PassengerId\", y=\"Age\", hue=\"Embarked\",col=\"Title\",col_wrap = 2, palette=\"ch:r=-.5,l=.75\", data=df)","d6f64892":"\n\n# Fitting the models\nselect_model = []\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nLE = LabelEncoder()\nX['Embarked'] = LE.fit_transform(X['Embarked'])\nX[\"Sex\"] = LE.fit_transform(X[\"Sex\"])\nX['Title'] = LE.fit_transform(X['Title'])\nTest['Embarked'] = LE.fit_transform(Test['Embarked'])\nTest[\"Sex\"] = LE.fit_transform(Test[\"Sex\"])\nTest['Title'] = LE.fit_transform(Test['Title'])\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.1,random_state =0)\n\n","71e2ffeb":"X.describe()","2c7f6837":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\n\nsns.heatmap(X.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","31e35334":"pd.crosstab([df.Title,df.Pclass,df.Embarked],\n            [df.Sex,df.Survived],margins=True).style.background_gradient(cmap='summer_r')\n","fe6c5026":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix   \n\n\nLR = LogisticRegression()\n# LR.fit(x_train,y_train)\n# y_pred = LR.predict(x_test)\n# confusion_matrix(y_test,y_pred)\n# print(LR.score(x_test,y_test))\nLR.fit(X,Y)\n\n#select_model.append(LR.score(x_test,y_test))\n","2d0da60d":"from sklearn.naive_bayes import GaussianNB\n\nGBayes_clf = GaussianNB()\n#GBayes_clf.fit(x_train, y_train)\nGBayes_clf.fit(X, Y)\n# print (GBayes_clf.score(x_test,y_test))\n# select_model.append(GBayes_clf.score(x_test,y_test))\n# confusion_matrix(y_pred,y_test)","ea21a520":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\nk_range = range(1,31)\nweights_options=['uniform','distance']\nparam = {'n_neighbors':k_range, 'weights':weights_options}\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_KNNsearch = GridSearchCV(KNeighborsClassifier(), param, cv=cv) \n# grid_KNNsearch.fit(x_train,y_train)\ngrid_KNNsearch.fit(X,Y)\ngrid_KNNsearch = grid_KNNsearch.best_estimator_\n# y_pred = grid_KNNsearch.predict(x_test)\n# print(grid_KNNsearch.score(x_test,y_test))\n\n# select_model.append(grid_KNNsearch.score(x_test,y_test))\n\n# confusion_matrix(y_pred,y_test)","27f53327":"from sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) \n# grid_search.fit(x_train,y_train)\ngrid_search.fit(X,Y)\nSVC_grid = grid_search.best_estimator_\ncross_val_score(SVC_grid, X, Y, cv=5).mean()\n# print(SVC_grid.score(x_test,y_test))\n# select_model.append(SVC_grid.score(x_test,y_test))","2b073afa":"from sklearn.tree import DecisionTreeClassifier\n    \ntree_clf = DecisionTreeClassifier(max_depth = 5)\ntree_clf.fit(X,Y)\nscore_tree = cross_val_score(tree_clf, X, Y, cv=5).mean()\nprint(score_tree)\nselect_model.append(score_tree)","d34142ea":"\nfrom sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparams = {'n_estimators':n_estimators}\n\nbagging_grid = GridSearchCV(BaggingClassifier(base_estimator= None,\n                                      bootstrap_features=False),\n                                 param_grid=params,\n                                 cv=cv,\n                                 n_jobs = -1)\nbagging_grid.fit(X,Y)\nbagging_estimator = bagging_grid.best_estimator_\n\nbagging_estimator_score = cross_val_score(bagging_estimator, X, Y, cv=5).mean()\n\n","b4ffc8c5":"cross_val_score(bagging_estimator, X, Y, cv=5).mean()","6f3de45e":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X,Y)\n# y_pred = GaussianProcessClassifier.predict(x_test)\n# print(accuracy_score(y_pred, y_test))\n# select_model.append(accuracy_score(y_pred, y_test))\n\ncross_val_score(GaussianProcessClassifier, X, Y, cv=5).mean()","516ae725":"\nfrom sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, Y)\n# y_pred = ExtraTreesClassifier.predict(x_test)\n# extraTree_accy = round(accuracy_score(y_pred, y_test), 3)\n# select_model.append(extraTree_accy)\n# print(extraTree_accy)\n\ncross_val_score(ExtraTreesClassifier, X, Y, cv=5).mean()\n","60289392":"from xgboost.sklearn import XGBClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparams={\n    'max_depth': [5],\n    'subsample': [1.0],\n    'colsample_bytree': [0.5],\n    'n_estimators': [1000],\n    'reg_alpha': [0.01, 0.02, 0.03, 0.04]\n}\n\nxgb_clf = XGBClassifier()\ngrid_search = GridSearchCV(xgb_clf,\n                  params,\n                  cv=5,\n                  n_jobs=1,\n                  verbose=2)\n\n\ngrid_search.fit(X,Y)\n#print(\"\\nGrid Search Best parameters set :\")\n#print(grid_search.best_params_)\n\n","4cfe6b12":"model = grid_search.best_estimator_\n\ncross_val_score(model, X, Y, cv=5).mean()","5fa63769":"\n\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,Y) \nrf_grid = grid.best_estimator_\n# print(rf_grid.score(x_test,y_test))\n# select_model.append(rf_grid.score(x_test,y_test))\n","c984e1da":"cross_val_score(rf_grid, X, Y, cv=5).mean()","f97b8ca5":"\nfrom sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\nadaBoost_grid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\nadaBoost_grid.fit(X,Y) \nAdaBoost_estimator = adaBoost_grid.best_estimator_\n# print(AdaBoost_estimator.score(x_test,y_test))\n# select_model.append(AdaBoost_estimator.score(x_test,y_test))\n","4db46eab":"cross_val_score(AdaBoost_estimator, X, Y, cv=5).mean()","e4c9eece":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', LR),\n    ('svc', SVC_grid),('AdaBoost', AdaBoost_estimator),\n    ('random_forest', rf_grid),\n    ('knn_classifier', grid_KNNsearch),\n    ('bagging_classifier', bagging_grid),\n    ('Decision Tree', tree_clf),('XGBClassifier',model),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_bayes_classifier',GBayes_clf),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\nvoting_classifier = voting_classifier.fit(X,Y)   \n# y_pred = voting_classifier.predict(x_test)\n# voting_accy = accuracy_score(y_pred, y_test)\n# print(voting_accy)\n# select_model.append(voting_accy)\n","292441d4":"cross_val_score(voting_classifier, X, Y, cv=5).mean()","1c00a732":"test_prediction = voting_classifier.predict(Test[features])","6137c442":"submission = pd.DataFrame({\n        \"PassengerId\": Test['PassengerId'],\n        \"Survived\": test_prediction\n    })\nprint(submission)\nsubmission.to_csv('submission.csv', index=False)","f3df6f64":"KERNEL Making In progress\n\n# 1. Introduction  \n\n\n\n\nThe aim of the competition is to correctly predict the survival rate of Titanic Survivors. However, I wanted this to be a good introduction to all beginners. So I clubbed most of the ML algorithms.  \n\nBut first I had to decide whether this task was going to be a done by a supervised ML Algorithm or  unsupervised ML algorithm. In all fairness even experts in the field are divided on this question so I used a mixture of both.\n\nNext I needed to figure out if the supervised problem is a regression task or a classification task.; or incase of an unsupervised problem is it a clustering task or an association task.\n\nNow that I have decided what kind of problems I need to boil this down to, let\u2019s look at our dataset.\n\n\n","165e9806":"## 3d. Support Vector Machine\nA Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVMs are more commonly used in classification problems and as such, this is what we will focus on in this post.\nIn this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the optimal hyper-plane that differentiate the two classes very well.\nHyper-plane will be a point in case of 1 dimensional data, line in case of 2 dimensional data, plane in case of 3 dimensional data and so on.","7f7c89d1":"![SVM](https:\/\/i.imgur.com\/UNJZ7C2.png)","61397f46":"\n\n![# Titanic](https:\/\/i.imgur.com\/44aKU98.jpg)\n\n","bfc8c136":"\n# 2. Exploratory Data Analysis\n\n***Exploratory Data Analysis***\n\nExploratory Data Analysis (EDA) is an approach\/philosophy for data analysis that employs a variety of techniques (mostly graphical) to\n1. Maximize insight into a data set\n2. Uncover underlying structure\n3. Extract important variables\n4. Detect outliers and anomalies\n5. Test underlying assumptions\n6. Develop parsimonious models\n7. Determine optimal factor settings.\n\nEDA is not identical to statistical graphics although the two terms are used almost interchangeably. Statistical graphics is a collection of techniques--all graphically based and all focusing on one data characterization aspect. EDA encompasses a larger venue; EDA is an approach to data analysis that postpones the usual assumptions about what kind of model the data follow with the more direct approach of allowing the data itself to reveal its underlying structure and model. EDA is not a mere collection of techniques; EDA is a philosophy as to how we dissect a data set; what we look for; how we look; and how we interpret. It is true that EDA heavily uses the collection of techniques that we call \"statistical graphics\", but it is not identical to statistical graphics per se.","d349edf6":"![#Sigmoidal](https:\/\/i.imgur.com\/FnooHjb.png)","09e50344":"## 3e. Decision Tree\nA decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value).\n\nThe whole idea is to create a tree like this for the entire data and process a single outcome at every leaf(or minimize the error in every leaf).\n\nThere are couple of algorithms there to build a decision tree , we only talk about a few which are:\n1. CART (Classification and Regression Trees) \u2192 uses Gini Index(Classification) as metric.\n2. ID3 (Iterative Dichotomiser 3) \u2192 uses Entropy function and Information gain as metrics.","258fb011":"### 2b. Encoding and Categorizing Data\n\nOnce we have gathered data, we might wish to classify it.  Roughly speaking, data can be classified as categorical data or quantitative data.\n\nCategorical (qualitative) data are pieces of information that allow us to classify the objects under investigation into various categories. In this case the dataframe called 'Title' has been recategorized to 4 values *(Mr,Mrs,Miss,Master)*.\n\nQuantitative data are responses that are numerical in nature and with which we can perform meaningful arithmetic calculations.\n\n","4f96215e":"![Decidion Tree](https:\/\/i.imgur.com\/yJ68HrC.png)","3f639cd0":"# 3. Modelling \n\nThe process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from.\nThe term ML model refers to the model artifact that is created by the training process.\nThe training data must contain the correct answer, which is known as a target or target attribute. \nThe learning algorithm finds patterns in the training data that map the input data attributes to the target \n(the answer that you want to predict), and it outputs an ML model that captures these patterns.\n\nYou can use the ML model to get predictions on new data for which you do not know the target. \nFor example, let's say that you want to train an ML model to predict if an email is spam or not spam. \nYou would provide ML with training data that contains emails for which you know the target (that is, a label that tells whether an email is spam or not spam). \nML would train an ML model by using this data, resulting in a model that attempts to predict whether new email will be spam or not spam.\n\nThe model in this case is a ***Binary Classification*** problem meaning the output of this model is either a class 0 or a class 1.","034192fa":"### 2c. Data Visualization\n\nData visualization is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data.\n\nI use two highly effective libraries here\n\na. Seaborn\n\nb. Matplotlib\n\nMy focus now is to see for myself what kind of relationship the data has with other data that may belong to the same or different category. \nSo how many made the cut? Turns out very few lived, 61% Died. Remember this is just for training data. In reality it was even worst 68% people on board died. ","cb7f7624":"### 2a. Determining the missing data\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit. So first we need to find the missing data and fill or replace the missing data with value. For this as of now, I use the median to fill the values or the most frequent to fill NA values.\n\n**Some good references:**\n\nhttps:\/\/en.wikipedia.org\/wiki\/Missing_data\n\nhttps:\/\/machinelearningmastery.com\/handle-missing-data-python\/\n\nhttps:\/\/www.programcreek.com\/python\/example\/93356\/sklearn.preprocessing.Imputer","e98cddf4":"![#Embedding and Exploring](https:\/\/i.imgur.com\/e2qvzy0.jpg)\n\n","21905080":"![KNN](https:\/\/i.imgur.com\/wZKzMPX.gif)","16becfab":"## 3a. Logistic Regression\n\nLogistic regression produces a logistic curve, which is limited to values between 0 and 1. Logistic regression is similar to a linear regression, but the curve is constructed using the natural logarithm of the \u201codds\u201d of the target variable, rather than the probability. Moreover, the predictors do not have to be normally distributed or have equal variance in each group.\n\nThe logistic function, also called the sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It\u2019s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n                                                        \n                                                        \n","6b0e8c4f":"## 3c. K Neighbors Classifier:\n\nk-Nearest-Neighbors (k-NN) is a supervised machine learning model. Supervised learning is when a model learns from data that is already labeled. A supervised learning model takes in a set of input objects and output values. The model then trains on that data to learn how to map the inputs to the desired output so it can learn to make predictions on unseen data.\nk-NN models work by taking a data point and looking at the \u2018k\u2019 closest labeled data points. The data point is then assigned the label of the majority of the \u2018k\u2019 closest points.\nFor example, if k = 5, and 3 of points are \u2018green\u2019 and 2 are \u2018red\u2019, then the data point in question would be labeled \u2018green\u2019, since \u2018green\u2019 is the majority.\n\n","226b61d1":"## 3b. Gaussian Naive Bayes\n\nA Gaussian Naive Bayes algorithm is a special type of NB algorithm. It\u2019s specifically used when the features have continuous values. It\u2019s also assumed that all the features are following a gaussian distribution i.e, normal distribution. We use sklearn\u2019s GaussianNB module but first lets recap to what Bayes theorem does\n\nLet\u2019s break the equation(below) down:\n1. A and B are events.\n2. P(A) and P(B) (P(B) not 0) are the probabilities of the event independent from each other.\n3. P(A|B) is the probability A under the condition B.\n4. Equivalent with P(B|A), it is the probability of observing event B given that event A is true.\n\n\nGaussian Naive Bayes\nThe Gaussian Naive Bayes is one classifier model. Beside the Gaussian Naive Bayes there are also existing the Multinomial naive Bayes and the Bernoulli naive Bayes. I picked the Gaussian Naive Bayes because it is the simplest and the most popular one.\n\nRemember that P(C) is not calculated for our data. We assume that P(C|A) is Guassian(following normal distribution).","33dcce18":"![Bayes Thorem](https:\/\/i.imgur.com\/WGrc6ET.jpg)"}}