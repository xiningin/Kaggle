{"cell_type":{"97a32c4e":"code","44a2d8f9":"code","2f463cbc":"code","635fac12":"code","8e940b52":"code","db011189":"code","a7cff427":"code","362757db":"code","1ca58167":"code","991afa2f":"code","fd0bb064":"code","d249975e":"code","4ecaaed1":"markdown","721ef96b":"markdown","744919c2":"markdown","685892fd":"markdown","a3627f66":"markdown","4b1d9ef0":"markdown","85409e79":"markdown","c76b221b":"markdown","7754673b":"markdown"},"source":{"97a32c4e":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,SpatialDropout1D\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport re","44a2d8f9":"data = pd.read_csv('..\/input\/Sentiment.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\ndata","2f463cbc":"data = data[data.sentiment != \"Neutral\"]\ndata.head(5)","635fac12":"data['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nprint(data[ data['sentiment'] == 'Positive'].size)\nprint(data[ data['sentiment'] == 'Negative'].size)\n\nfor idx,row in data.iterrows():\n     row[0] = row[0].replace('rt',' ')\n     print(next(data.iterrows()))#return tuple of content   \nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)\nX","8e940b52":"model=Sequential()","db011189":"model.add()","a7cff427":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","362757db":"Y = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","1ca58167":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)","991afa2f":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","fd0bb064":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    \n    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(\"pos_acc\", pos_correct\/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_cnt*100, \"%\")","d249975e":"twt = ['Meetings: Because none of us is as dumb as all of us.']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\nprint(twt)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","4ecaaed1":"Only keeping the necessary columns.","721ef96b":"Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n\n","744919c2":"Hereby I declare the train and test dataset.","685892fd":"As it was requested by the crowd, I extended the kernel with a prediction example, and also updated the API calls to Keras 2.0. Please note that the network performs poorly. Its because the training data is very unbalanced (pos: 4472, neg: 16986), you should get more data, use other dataset, use pre-trained model, or weight classes to achieve reliable predictions.\n\nI have created this kernel when I knew much less about LSTM & ML. It is a really basic, beginner level kernel, yet it had a huge audience in the past year. I had a lot of private questions and requests regarding this notebook and I tried my best to help and answer them . In the future I am not planning to answer custom questions and support\/enhance this kernel in any ways. Thank you my folks :)","a3627f66":"Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that.","4b1d9ef0":"Extracting a validation set, and measuring score and accuracy.","85409e79":"Here we train the Network. We should run much more than 7 epoch, but I would have to wait forever for kaggle, so it is 7 for now.","c76b221b":"Finally measuring the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets.","7754673b":"dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."}}