{"cell_type":{"8b9709d6":"code","a6a6799e":"code","e4e88568":"code","e45b2289":"code","7bee45ee":"code","0c13a9e4":"code","02797ba5":"code","a5e83d04":"code","dcc885c0":"code","df2a5f92":"code","7bb6ddfe":"code","31cbf5c2":"code","80e2a403":"code","87930e99":"code","0352c61c":"code","377f25cd":"code","582ec321":"code","2e045ce3":"code","717d6da3":"code","79cc125b":"code","8d75b1f1":"code","986d5ce9":"code","d04092fa":"code","69bbb0e7":"code","9b13ed16":"code","a9d5756d":"code","5f6bae3b":"code","714c5456":"code","0edf25f3":"code","713cab3d":"code","71a783e8":"code","43b6c159":"code","c418ee73":"code","7da1d5d1":"code","1750439c":"code","0229b436":"code","d631b0ac":"code","cbb91313":"code","bbad7b4a":"code","8d9413e6":"code","52da5fa4":"code","af1bf7e1":"code","af6fbed9":"code","5ec27d9f":"code","04ad9569":"code","836a57bf":"code","467ca19c":"code","c0d0f264":"code","c78e6b1e":"code","d672fc32":"code","f3bf1fa7":"code","8d8406a6":"code","f4b3175f":"code","d6aaa242":"code","23b57f8f":"code","38454a32":"code","2cac152b":"code","edde3f42":"code","5273ae99":"code","08142937":"code","f9d14ee0":"code","ad86f502":"code","3443148c":"code","81790dee":"code","36868867":"code","4c4c5362":"code","21695ef6":"code","363d2e2c":"code","fd10e9e0":"code","4d57450a":"code","ba72e2c5":"code","b585f714":"markdown","ddd5c4a3":"markdown","69ae8456":"markdown","9e044dc4":"markdown","cf9efe5d":"markdown","0819f0de":"markdown","feeffc06":"markdown","61d92d11":"markdown","7bef9c7e":"markdown","96dca8cc":"markdown","3810f9e2":"markdown","4482628d":"markdown","263cb761":"markdown","86f19e33":"markdown","06bcdd88":"markdown","caf16dd8":"markdown","913ec0a9":"markdown","952e0761":"markdown","f2116ecf":"markdown","9a07644d":"markdown","f5a5269d":"markdown","e936f3ce":"markdown","fe5a28c9":"markdown","306fd59d":"markdown","99a03cdb":"markdown","770327f4":"markdown","ff753c98":"markdown","a35f9553":"markdown","2534ef68":"markdown","af8e9559":"markdown","df3c3ba6":"markdown","01ffa2b4":"markdown","666acd91":"markdown","3ae5d5e8":"markdown","517c4403":"markdown","c51b32dc":"markdown","8d1692c8":"markdown","d4ce9689":"markdown","22b911e1":"markdown","474b06ad":"markdown","522b4103":"markdown","f213fed5":"markdown","9ee446ba":"markdown","485a83c1":"markdown"},"source":{"8b9709d6":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.style.use('fivethirtyeight')\n%matplotlib inline\n\n#model building\n#import scipy.stats as ss\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import confusion_matrix \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6a6799e":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ncombined = [train, test]\n\nalldata=pd.concat(combined, sort=True).reset_index(drop=True) #Join the dfs vertically.survived column of test is currently filled w NaNs\ntrain.head()","e4e88568":"test.head()","e45b2289":"print(train.shape)\nprint(test.shape) #does not have survived column. we are to predict that!\nprint(alldata.shape)","7bee45ee":"train.info()","0c13a9e4":"fig=plt.figure()\nax1=plt.subplot(321)\nsns.countplot(x = 'Survived', hue = 'Sex', data = train, ax=ax1)\n\nax2=plt.subplot(322)\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train, ax=ax2)\n\nax3=plt.subplot(323)\nsns.countplot(x = 'Survived', hue = 'SibSp', data = train, ax=ax3)\nax3.legend(loc=1, title='Sibling\/Spouse Count', fontsize='x-small')\n\nax4=plt.subplot(324, sharey=ax3)\nsns.countplot(x = 'Survived', hue = 'Parch', data = train, ax=ax4)\nax4.legend(loc=1, title='Parent\/Children Count', fontsize='x-small')\n\nax5=plt.subplot(325)\nsns.countplot(x = 'Survived', hue = 'Embarked', data = train, ax=ax5)\nax5.legend(loc=1, title='Embarked')\n\n#ax6=plt.subplot(326)\n#sns.countplot(x = 'Survived', hue = 'Cabin', data = train, ax=ax6)\n#ax6.legend(loc=1, title='Cabin')\n\nfig.set_size_inches(8,12)\nfig.tight_layout()","02797ba5":"pd.crosstab(train.Pclass,train.Survived,margins=True).style.background_gradient(cmap='summer_r')","a5e83d04":"pd.crosstab([train.Sex,train.Survived],train.Pclass,margins=True).style.background_gradient(cmap='summer_r')","dcc885c0":"#checking missing values\nprint('Training Set\\n')\nprint(train.isnull().sum())\nprint('-'*40)\nprint('Test Set\\n')\nprint(test.isnull().sum())","df2a5f92":"#correlation matrix for all features.\ncorr_matr=(alldata.drop('PassengerId', axis=1).corr().abs().unstack().reset_index().rename(columns={'level_0':'Feature 1','level_1':'Feature 2', 0:'corr'})\n.sort_values(by=['Feature 1','corr'], ascending=[True,False]))\ncorr_matr","7bb6ddfe":"#number of null records to fill in age column\nalldata.groupby(['Sex','Embarked', 'Pclass']).size()-alldata.groupby(['Sex', 'Embarked', 'Pclass'])['Age'].count()","31cbf5c2":"alldata['Age'] = alldata.groupby(['Sex', 'Embarked', 'Pclass'])['Age'].apply( lambda x: x.fillna(x.median()) )\nalldata.isnull().sum()","80e2a403":"#2 missing age records are because of the missing embarked records. fill those with median values, excluding their embarked feature.\nalldata['Age'] = alldata.groupby(['Sex', 'Pclass'])['Age'].apply( lambda x: x.fillna(x.median()) )\nalldata.isnull().sum()","87930e99":"alldata[alldata.Embarked.isnull()]","0352c61c":"alldata.Embarked=alldata.Embarked.fillna('S')\nalldata.isnull().sum()","377f25cd":"alldata[alldata.Fare.isnull()]","582ec321":"alldata.Fare=alldata.Fare.fillna(alldata.groupby(['Pclass', 'Parch']).median().Fare[3][0])\nalldata.isnull().sum()","2e045ce3":"#First letter indicates the deck.\nalldata['Deck']=alldata.Cabin.str.extract('([A-Za-z]+)', expand=False)\nalldata['Deck']=alldata['Deck'].fillna('M') #M:missing","717d6da3":"df=alldata.groupby(['Deck', 'Pclass']).agg(Count=pd.NamedAgg(column='Name', aggfunc='count'), Survived=pd.NamedAgg('Survived', sum))\ndf_decks=pd.DataFrame(df.groupby(level=0).apply(lambda df: df.xs(df.name).Count.to_dict()).to_dict())\ndf_decks=df_decks.fillna(0)  \nprint(df)\ndf_decks","79cc125b":"# Creating a dictionary for percentage of pessengers for each pclass and deck\ndeck_percentages={}\nfor col in df_decks.columns:\n    deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\ndeck_percentages=pd.DataFrame(deck_percentages)     \ndeck_percentages","8d75b1f1":"deck_names = deck_percentages.columns.tolist()\nbar_count = np.arange(len(deck_names))  \n\nplt.figure(figsize=(8,6))\nplt.bar(bar_count, deck_percentages.iloc[0], width=0.85, label='Class 1')\nplt.bar(bar_count, deck_percentages.iloc[1], bottom=deck_percentages.iloc[0], width=0.85, label='Class 2')\nplt.bar(bar_count, deck_percentages.iloc[2], bottom=deck_percentages.iloc[0]+deck_percentages.iloc[1],width=0.85, label='Class 3')\nplt.xticks(bar_count, deck_names)  \nplt.legend()\nplt.title('Passenger class percentage per deck')","986d5ce9":"alldata.loc[alldata.Deck=='T','Deck']='A'\nalldata['Deck'] = alldata['Deck'].replace(['A', 'B', 'C'], 'ABC')\nalldata['Deck'] = alldata['Deck'].replace(['D', 'E'], 'DE')\nalldata['Deck'] = alldata['Deck'].replace(['F', 'G'], 'FG')\n\nalldata['Deck'].value_counts()","d04092fa":"#we will use deck from now on\nalldata=alldata.drop('Cabin', axis=1)","69bbb0e7":"#update train and test dfs\ntrain=alldata.loc[:890]\ntest=alldata.loc[891:].drop('Survived', axis=1)\ncombined = [train, test]","9b13ed16":"train_corr=(train.drop('PassengerId', axis=1).corr().abs().unstack().reset_index()\n            .rename(columns={'level_0':'Feature 1','level_1':'Feature 2', 0:'corr'})\n            .sort_values(by=['Feature 1','corr'], ascending=[True,False]))\ntrain_corr=train_corr.query('corr!=1').drop([6,12,13,18,19,24,25,26,31,33])\ntrain_corr=train_corr.query('corr>0.1').sort_values(by='corr', ascending=False)","a9d5756d":"test_corr=(test.drop('PassengerId', axis=1).corr().abs().unstack().reset_index()\n            .rename(columns={'level_0':'Feature 1','level_1':'Feature 2', 0:'corr'})\n            .sort_values(by=['Feature 1','corr'], ascending=[True,False]))\ntest_corr=test_corr.query('corr!=1').reset_index(drop=True).drop([5,10,13,18, 9,12,17, 14,16])\ntest_corr=test_corr.query('corr>=0.1').sort_values(by='corr', ascending=False)","5f6bae3b":"#NaNs correlations actually correspond to correlations that are smaller than 0.1\ncorr=pd.merge(train_corr, test_corr, on=['Feature 1', 'Feature 2'], how='outer', suffixes=['_train', '_test'])\ncorr","714c5456":"fig, axs=plt.subplots(2,1, figsize=(7,14))\nsns.heatmap(train.drop('PassengerId', axis=1).corr(), ax=axs[0], annot=True, cbar_kws={'label': 'Correlation coeff.'}, cmap=\"PuOr\")\nsns.heatmap(test.drop('PassengerId', axis=1).corr(),  ax=axs[1], annot=True, cbar_kws={'label': 'Correlation coeff.'}, cmap=\"PuOr\")\naxs[0].set_title('Correlation matrix for train set')\naxs[1].set_title('Correlation matrix for test set')\n\n","0edf25f3":"alldata['Title']=alldata.Name.str.split(',', expand=True)[1].str.split('.', expand=True)[0].str.replace(\" \",\"\")","713cab3d":"alldata.Title.value_counts() #title value counts","71a783e8":"alldata['Title'] = alldata['Title'].replace(['Lady', 'theCountess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\nalldata['Title'] = alldata['Title'].replace('Mlle', 'Miss')\nalldata['Title'] = alldata['Title'].replace('Ms', 'Miss')\nalldata['Title'] = alldata['Title'].replace('Mme', 'Mrs')\n\nalldata['Title'].value_counts()","43b6c159":"print (alldata[['Title', 'Survived']].groupby(['Title']).mean())","c418ee73":"alldata['FamilySize']=alldata['SibSp']+alldata['Parch']+1","7da1d5d1":"alldata.head()","1750439c":"alldata.loc[:,'IsAlone']=alldata.FamilySize==1\nalldata['IsAlone']=alldata['IsAlone']*1","0229b436":"alldata['Surname']=alldata.Name.apply(lambda x: str.split(x, \",\")[0])\nsurnames=alldata.Surname.value_counts()\nsurnames","d631b0ac":"surnames.value_counts()","cbb91313":"for i in range(2,12):\n    print( 'Number of groups consists of {} people, with the same ticket number: {} '.format( i, (alldata['Ticket'].value_counts()==i).sum()) )","bbad7b4a":"fare_counts=alldata['Fare'].value_counts().value_counts().reset_index().sort_values(by='index').rename(columns={'index':'consists_of'})\nfor i in fare_counts['consists_of']:\n    print( 'Number of groups consists of {} people, with the same fare: {} '.format(i, fare_counts.loc[fare_counts.consists_of==i,'Fare'].values[0]) )\n","8d9413e6":"df_fare=alldata.loc[alldata['FamilySize']>1, [\"Surname\",\"Fare\", \"FamilySize\"]].iloc[:len(train)]\n(df_fare.sort_values(by='Surname', ascending=True).groupby(['Surname', 'FamilySize']).nunique().Fare!=1).sum()\/len(train)*100","52da5fa4":"df_ticket=alldata.loc[alldata['FamilySize']>1, [\"Surname\",\"Ticket\", \"FamilySize\"]].iloc[:len(train)]\n(df_ticket.sort_values(by='Surname', ascending=True).groupby(['Surname', 'FamilySize']).nunique().Ticket!=1).sum()\/len(train)*100","af1bf7e1":"alldata['Family_Survival']=0.5 #initial survival value. We gonna change this in a bit.\nfor groupname, group_df in alldata.groupby(['Surname', 'Fare']):\n    if len(group_df)!=1: #we found ourselves a family!\n        for idx, row in group_df.iterrows():\n            smax=group_df.drop(idx).Survived.max()\n            smin=group_df.drop(idx).Survived.min()\n            passid=row['PassengerId']\n            if (smax == 1.0):\n                alldata.loc[alldata['PassengerId'] == passid, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                alldata.loc[alldata['PassengerId'] == passid, 'Family_Survival'] = 0\n                \nprint(\"Number of passengers with family survival information:\", \n      alldata.loc[alldata['Family_Survival']!=0.5].shape[0])\nprint(\"Number of passengers without family survival information:\", \n      alldata.loc[alldata['Family_Survival']==0.5].shape[0])","af6fbed9":"ticket_grpby = alldata.iloc[:len(train)].groupby('Ticket')\nticket_df = pd.DataFrame(data=ticket_grpby.size(), columns=['Size in train'])\nticket_df['Survived total'] = ticket_grpby['Survived'].sum().astype(int)\nticket_df['Not family'] = ticket_grpby['Surname'].nunique()\nticket_df = ticket_df[(ticket_df['Size in train'] > 1) & (ticket_df['Not family']>1)]\nprint('Number of groups in training set that is not family: '+ str(len(ticket_df)))\n","5ec27d9f":"for groupname, group_df in alldata.groupby('Ticket'):\n    if len(group_df)!=1: #we found ourselves a peer group!\n        for idx, row in group_df.iterrows():\n            if (row['Family_Survival']== 0.5):\n                smax=group_df.drop(idx).Survived.max()\n                smin=group_df.drop(idx).Survived.min()\n                passid=row['PassengerId']\n                if (smax == 1.0):\n                    alldata.loc[alldata['PassengerId'] == passid, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    alldata.loc[alldata['PassengerId'] == passid, 'Family_Survival'] = 0\n\nprint(\"Number of passengers without group survival information:\", \n      alldata.loc[alldata['Family_Survival']==0.5].shape[0])\nprint(\"Number of passenger with family\/group survival information: \" \n      +str(alldata[alldata['Family_Survival']!=0.5].shape[0]))                ","04ad9569":"for groupname, group_df in alldata.groupby('Fare'):\n    if len(group_df)!=1: #we found ourselves a peer group!\n        for idx, row in group_df.iterrows():\n            if (row['Family_Survival']== 0.5):\n                smax=group_df.drop(idx).Survived.max()\n                smin=group_df.drop(idx).Survived.min()\n                passid=row['PassengerId']\n                if (smax == 1.0):\n                    alldata.loc[alldata['PassengerId'] == passid, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    alldata.loc[alldata['PassengerId'] == passid, 'Family_Survival'] = 0\n\nprint(\"Number of passengers without group survival information:\", \n      alldata.loc[alldata['Family_Survival']==0.5].shape[0])\nprint(\"Number of passenger with family\/group survival information: \" \n      +str(alldata[alldata['Family_Survival']!=0.5].shape[0]))                ","836a57bf":"alldata['AgeBin']=pd.qcut(alldata['Age'],9)\nalldata['AgeBin'].value_counts().sort_index()","467ca19c":"alldata['FareBin']=pd.qcut(alldata['Fare'],10)\nalldata['FareBin'].value_counts().sort_index()","c0d0f264":"alldata.drop(['Age', 'Fare'], axis=1, inplace=True)","c78e6b1e":"alldata.head()","d672fc32":"columns=['Embarked','Sex','Deck','Title','AgeBin','FareBin']\nfor col in columns:\n    alldata[col] = LabelEncoder().fit_transform(alldata[col])","f3bf1fa7":"def encode_and_bind(original_df, feature_to_encode):\n    dummies = pd.get_dummies(original_df[feature_to_encode], prefix=feature_to_encode)\n    res = pd.concat([original_df, dummies], axis=1)\n    res=res.drop(feature_to_encode, axis=1)\n    return(res)  \n\n","8d8406a6":"columns=['Embarked','Deck','Title']\nfor col in columns:\n    alldata=encode_and_bind(alldata, col)  ","f4b3175f":"drop_cols = ['Name', 'Parch', 'PassengerId','SibSp', 'Ticket', 'Surname']\nalldata=alldata.drop(drop_cols, axis=1)","d6aaa242":"alldata.head()","23b57f8f":"#update train and test sets\ntrain=alldata.iloc[:891]\ntest=alldata.iloc[891:].drop('Survived',axis=1)","38454a32":"fig, axs=plt.subplots(2,1, figsize=(14,14))\nsns.heatmap(train.corr(), ax=axs[0], annot=True, cbar_kws={'label': 'Correlation coeff.'}, cmap=\"PuOr\")\nsns.heatmap(test.corr(),  ax=axs[1], annot=True, cbar_kws={'label': 'Correlation coeff.'}, cmap=\"PuOr\")","2cac152b":"#training data\nX = train.drop(['Survived'], axis=1)\nstdscale=StandardScaler()\nX_scaled = stdscale.fit_transform(X)\ny = train.Survived.copy()","edde3f42":"#split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=5) #%25 of the data used for test set\n\n#fitting the model\nmodel = LogisticRegression(max_iter = 500000, random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test) #test labels, true response values\nprint('The accuracy of the Logistic Regression is:',accuracy)","5273ae99":"confusion_matrix(y_test,y_pred) #true response value, predicted response","08142937":"# another way to find the model score: accuracy_score\n# Accuracy: how often the model is correct. (TP + TN)\/total\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","f9d14ee0":"#precision, positive predictive value ppv: how often model correctly predicts positive outcome.= TP\/(TP+FP)\nfrom sklearn.metrics import precision_score\nprecision_score(y_test, y_pred)","ad86f502":"#mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nmae=mean_absolute_error(y_test, y_pred)\nmae","3443148c":"from sklearn.model_selection import KFold, StratifiedKFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation: CROSS VALIDATION\nfrom sklearn.model_selection import cross_val_predict #prediction\n\n#Kfold\nkfold = KFold(n_splits=10) # k=10, split the data into 10 equal parts\ncv_result = cross_val_score(model, X_scaled, y, cv = kfold, scoring = \"accuracy\")\ncv_result.mean() # mean accuracy of kfold","81790dee":"#Stratified Kfold\nstkfold=StratifiedKFold(n_splits=10)\n#you can use cross_val_score as well to find scores. this is just for variety.\naccuracy=[]\nfor train_index, test_index in stkfold.split(X_scaled, y):\n    x1_train, x1_test= X.iloc[train_index], X.iloc[test_index]\n    y1_train, y1_test= y.iloc[train_index], y.iloc[test_index]\n    model.fit(x1_train, y1_train)\n    prediction=model.predict(x1_test)\n    score=accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n\nprint(accuracy)","36868867":"# mean accuracy of stratified kfold\nnp.mean(accuracy)","4c4c5362":"from sklearn.metrics import confusion_matrix #for confusion matrix\n\nfig=plt.subplot()\nax=plt.gca()\ny_pred = cross_val_predict(model,X_scaled,y,cv=kfold)\nsns.heatmap(confusion_matrix(y,y_pred),annot=True,fmt='2.0f')\nax.set_title('Matrix for Logistic Regression')\nax.set_ylabel('Actual Value')\nax.set_xlabel('Predicted Value')","21695ef6":"modelcv = LogisticRegressionCV(cv=stkfold, max_iter = 500000, random_state=0)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_scaled, y, test_size=0.25, random_state=5)\nmodelcv.fit(X_train2,y_train2)\ny_pred2=modelcv.predict(X_test2)\naccuracy=modelcv.score(X_test2, y_test2)\nmae=mean_absolute_error(y_test2, y_pred2)\nprint('Accuracy: %',accuracy)\nprint('MAE:',mae)\n#modelcv.get_params()","363d2e2c":"modelcv.get_params()","fd10e9e0":"#fitting the model with scaled features\nstd_scaler = StandardScaler()\nX_train = std_scaler.fit_transform(X) #train features\ny_train=y #train response\nX_test = std_scaler.transform(test) #test features\n\nmodelcv.fit(X_train, y_train)\ny_pred=modelcv.predict(X_test) #predicted test response\n","4d57450a":"subm = pd.DataFrame(pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")['PassengerId'])\nsubm['Survived'] = y_pred\nsubm.Survived=subm.Survived.astype('int')\nsubm","ba72e2c5":"subm.to_csv(\"\/kaggle\/working\/submission.csv\", index = False)","b585f714":"One can observe Age is highly correlated with Pclass, also Pclass itself is highly correlated with Survived. Therefore, it would not be a bad interpretation if we fill the NaN values of Age, with the median values, according to their classes, sexes and also their port of embarkation.","ddd5c4a3":"From [S.Xu's kernel](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever#Extracting-family-information), we already know that majority of families are either all survived or all perished. This means, we can create a Family_Survival feature to predict survival of the passenger. We will pass it 1 if any one of the family members are survived, 0 otherwise.","69ae8456":"Since different families might have the same surname, we need one more feature in order not to miss out on that probability. Lets check if tickets and fare has anything to do with surnames.","9e044dc4":"## **1.2. Correlations** ##","cf9efe5d":"Here i will only focus on **Logistic Regression** but of course you need to try others MLAs too, to see which one works the best.","0819f0de":"* Heatmap","feeffc06":"AgeBin, FareBin and pclass categorical features are not converted because they are ordinal unlike the others. Also Sex did not converted because it is already binary.","61d92d11":"Lets first focus on the missing values in Age feature.","7bef9c7e":"When we dig into the data, we can observe that we have:\n* 637 passengers travelling alone\n* 133 families with size 2\n* 63 families with size 3\n* 22 families with size 4\n* 6 families with size 5\n* 9 families with size 6\n* 1 families with size 7\n* 2 families with size 8\n* 2 families with size 11","96dca8cc":"number of correct predictions are 496(for dead) + 257(for survived) with the mean CV accuracy being (496+257)\/891 = 84.51% which we did get earlier.","3810f9e2":"Thanks to *G\u00fcne\u015f Evitan*'s efforts here: https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#1.-Exploratory-Data-Analysis \n\n> When I googled Stone, Mrs. George Nelson (Martha Evelyn), I found that she embarked from S (Southampton) with her maid Amelie Icard, in this page Martha Evelyn Stone: Titanic Survivor.\n\nWe know that two missing values are actually S. Lets fill those.","4482628d":"## 2.4. Binning Continuous Features","263cb761":"## **2.1. Title Feature (extracted from Name)**","86f19e33":"## 2.3. Family","06bcdd88":"Women also have higher overall survival rate than men.","caf16dd8":"**Categorical features**   \nCategorical: Survived, Sex, and Embarked.   \nOrdinal: Pclass.  \n\n**Numerical Features**  \nContinous: Age, Fare.   \nDiscrete: SibSp, Parch.  ","913ec0a9":"## **1.1. Missing Values** ##","952e0761":"### 2.5.2. One Hot Encoding for Nominal Data","f2116ecf":"Inspired by [S.Xu's kernel](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever#Extracting-family-information)","9a07644d":"**Making submission:**","f5a5269d":"We are going to group the titles that occured rarely under Rare group.","e936f3ce":"Now we know that certain people have the same fare and\/or ticket number. We also know it is correlated with surname feature. But above information is not enough for us to decide whether to use Ticket or Fare for grouping of families.  \nIf we group the data by Surname and Familysize and check for number of unique values in Fare\/Ticket column, we should see mostly 1's since every record with unique surname and family size should represent the family, according to our current assumption.   \nFeature with the least number of unique values per group will be chosen.    \n","fe5a28c9":"Lets look into Embarked now.   \n","306fd59d":"###### **Survived** is the target variable we are trying to predict (0 or 1):   \n1 = Survived  \n0 = Not Survived    \n  \n**Pclass** (Passenger Class) is the socio-economic status of the passenger which has 3 unique values (1, 2 or 3):  \n1 = Upper Class  \n2 = Middle Class  \n3 = Lower Class   \n  \n**Embarked** is port of embarkation which has 3 unique values (C, Q or S):  \nC = Cherbourg  \nQ = Queenstown  \nS = Southampton  \n  \n**SibSp** is the total number of the passengers' siblings and spouse (0 to 8)    \n  \n**Parch** is the total number of the passengers' parents and children (0 to 6)    ","99a03cdb":"* Another way to split data: **Cross Validation**  \nAs the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as model variance.    \nTo overcome this and get a generalized model,we use *Cross Validation.*  \n* We can use **KFold**, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit for cross validation.","770327f4":"Lets fill the single missing value of fare.","ff753c98":"# **3. MODELLING**","a35f9553":"High number of passengers without family survival information is expectable since there are already 637 passengers travelling alone.\nRemember the Tickets feature that we dropped over Fare? Maybe that would help us identify the relationship among people without family survival information.","2534ef68":"**Stratification** is the process of rearranging the data as to ensure each fold is a good representative of the whole. For example in a binary classification problem where each class comprises 50% of the data, it is best to arrange the data such that in every fold, each class comprises around half the instances.","af8e9559":"**Ticket:** 2.80% of records has a different ticket value between family members. This is expected, since currently we know this grouping of families is not %100 true because we do not take into account the families with the same surname.    \n**Fare:** 1.68% of records has a different fare value between family members.  \nWe will chose Fare. ","df3c3ba6":"From above table we can see clearly passengers with higher class are prioritized during the evacuation process.","01ffa2b4":"When we check the correlation matrix we can see that Fare is related with Pclass and Parch. So lets fill that missing value with the median fare of corresponding class and parch value.","666acd91":"### 2.5.1. Label Encoding for Non-Numerical Features\n","3ae5d5e8":"2. Evaluate the classification model: **Confusion Matrix**, **AUC and ROC curves**","517c4403":"Lets now do the same thing over Fare.","c51b32dc":"We only have left Cabin feature to fill. Now, it should not be as easy as filling the missing values with statistical summarizations, since larger portion of Cabin records are missing.\nOne way to handle missing data is to add additional category for missing records. (This feature is from [G\u00fcne\u015f Evitan's kernel](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#1.-Exploratory-Data-Analysis))","8d1692c8":"1. Split the data: **train_test_split, cross validation**","d4ce9689":"# **1.EXPLORATORY ANALYSIS**","22b911e1":"Age and Fare are continuous features. Lets bin those.\n","474b06ad":"### 2.5. Feature Formatting  \nWe will convert categorical data to dummy variables for mathematical analysis. \nFor ordinal data, we will just use LabelEncoder() and for nominal data we will use one hot encoding.  \n***LabelEncoder*** basically labels the classes from 0 to n. This process is necessary for models to learn from those features.  \nWhat ***one hot encoding*** does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns.  \n","522b4103":"From above table we can observe that:  \n* decks A,B,C and T(1 passenger) only used by pclass=1  \n* D,E,F,G used by 2 or more pclass categories.  ","f213fed5":"## 2.2. FamilySize (from SibSp and Parch) ","9ee446ba":"# **2. FEATURE ENGINEERING**","485a83c1":"## **Overall Visualizations of Feature vs. Target** ##"}}