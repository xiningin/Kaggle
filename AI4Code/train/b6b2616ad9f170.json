{"cell_type":{"a7ba4c7d":"code","9e525588":"code","fd53a0b9":"code","9166021a":"code","3637e9a1":"code","1707e4e9":"code","63f38b11":"code","02a6cc82":"code","7e8290c0":"code","f92b458d":"code","97c940c9":"code","b915e549":"code","976a7126":"code","ade2d767":"code","7b138292":"markdown","ed1caa45":"markdown","eb58175b":"markdown"},"source":{"a7ba4c7d":"import sys\nsys.path.insert(0,'..\/input\/efficientnet\/EfficientNet-b5')\n","9e525588":"from __future__ import print_function, division\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\nimport pandas as pd\nfrom efficientnet.model import EfficientNet\nimport torch.nn.functional as F\nimport sys\nfrom PIL import Image","fd53a0b9":"use_gpu = torch.cuda.is_available()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ndata_dir = '..\/input\/issm2020-ai-challenge'\nbatch_size = 4\nlr = 0.1\nmomentum = 0.9\nnum_epochs = 100\ninput_size = 480\nclass_num = 10\n# accumulation_steps = args.accumulation_steps\n# process = args.process\nnum_workers = 4","9166021a":"def linear_combination(x, y, epsilon):\n    return epsilon*x + (1-epsilon)*y\ndef reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean'else loss.sum() if reduction=='sum' else loss\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, epsilon:float=0.1, reduction='mean'):\n        super().__init__()\n        self.epsilon = epsilon\n        self.reduction = reduction\n\ndef forward(self, preds, target):\n    n = preds.size()[-1]\n    log_preds = F.log_softmax(preds, dim=-1)\n    loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n    nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n    return linear_combination(loss\/n, nll, self.epsilon)","3637e9a1":"def loaddata(data_dir, batch_size, set_name, shuffle):\n    data_transforms = {\n        'semTrain': transforms.Compose([\n            transforms.Resize(input_size),\n            transforms.CenterCrop(input_size),\n            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),}\n    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x+'\/'+x), data_transforms[x]) for x in [set_name]}\n    dataset_loaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=shuffle, num_workers=num_workers) for x in [set_name]}\n    data_set_sizes = len(image_datasets[set_name])\n    return dataset_loaders, data_set_sizes","1707e4e9":"# \u81ea\u5b9a\u4e49lr\u66f4\u65b0\u65b9\u5f0f\ndef exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=10):\n\n    lr = init_lr * (0.8**(epoch \/\/ lr_decay_epoch))\n    print('LR is set to {}'.format(lr))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    return optimizer","63f38b11":"def train_model(model_ft, criterion, optimizer, lr_scheduler, num_epochs=50):\n    train_loss = []\n    since = time.time()\n    best_model_wts = model_ft.state_dict()\n    best_acc = 0.0\n    best_train_acc = 0.0\n    \n    # optim\u4e2d\u7684scheduler\u8fdb\u884c\u66f4\u65b0\n    # 1 onecyclelr update\n    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n    #         max_lr=0.001,\n    #         epochs=10,\n    #         steps_per_epoch=500,\n    #         pct_start=0.1,\n    #         anneal_strategy='cos',\n    #         final_div_factor=10**5\n    #     )\n    # 2 CosineAnnealingLR update\n    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5,eta_min=4e-08)\n    for epoch in range(num_epochs):\n        model_ft.train(True)\n        # print('set lr ',scheduler.get_last_lr()[0])\n        dset_loaders, dset_sizes = loaddata(data_dir=data_dir, batch_size=batch_size, set_name='semTrain', shuffle=True)\n        print('Data Size', dset_sizes)\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        optimizer = lr_scheduler(optimizer, epoch)\n        running_loss = 0.0\n        running_corrects = 0\n        count = 0\n\n        for i,data in enumerate(dset_loaders['semTrain']):\n            inputs, labels = data\n\n            labels = torch.squeeze(labels.type(torch.LongTensor))\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n            else:\n                inputs, labels = Variable(inputs), Variable(labels)\n\n            outputs = model_ft(inputs)\n            \n            loss = criterion(outputs, labels)\n     \n            _, preds = torch.max(outputs.data, 1)\n            # \u8fdb\u884c\u68af\u5ea6\u53e0\u52a0 \u7528\u4e8e\u514b\u670dresolution\u53d8\u5927\u65f6\u5019\u5bfc\u81f4batchsize\u8fc7\u5c0f\n            # loss = loss\/accumulation_steps\n            # loss.backward()\n            # if((i+1)%accumulation_steps)==0:\n       \n            #   optimizer.step()        # update parameters of net\n            #   optimizer.zero_grad()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n\n            count += 1\n            if count % 30 == 0 or outputs.size()[0] < batch_size:\n              print('Epoch:{}: loss:{:.3f}'.format(epoch, loss.item()))\n              train_loss.append(loss.item())\n\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss \/ dset_sizes\n        epoch_acc = running_corrects.double() \/ dset_sizes\n\n        print('Train Loss: {:.4f} Acc: {:.4f}'.format(\n            epoch_loss, epoch_acc))\n        \n        # val_acc = val_model(model_ft, criterion)\n        # if val_acc >= best_acc:\n        #     best_acc = val_acc\n        #     best_model_wts = model_ft.state_dict()\n        #     print('update best val weight at No.',epoch,' epoch')\n        # if epoch_acc >= best_train_acc:\n        #     best_train_acc = epoch_acc\n        #     best_model_wts1 = model_ft.state_dict()\n        #     print('update best train weight at No.',epoch,' epoch')\n        # \u5982\u679c\u4f7f\u7528\u81ea\u5e26\u7684lr_scheduler\n        # scheduler.step()\n        if epoch_acc >= 0.9999:\n            best_acc = epoch_acc\n            best_model_wts = model_ft.state_dict()\n            print('update best weight at No.',epoch,' epoch')","02a6cc82":"# \u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\nmodel_ft = EfficientNet.from_name('efficientnet-b2')\nnet_weight = '..\/input\/efficientnet-pytorch-b0-b7\/efficientnet-b2-8bb594d6.pth'\nstate_dict = torch.load(net_weight)\nmodel_ft.load_state_dict(state_dict)\n","7e8290c0":"num_ftrs = model_ft._fc.in_features\nmodel_ft._fc = nn.Linear(num_ftrs, class_num)\ncriterion = LabelSmoothingCrossEntropy()","f92b458d":"if use_gpu:\n    model_ft = model_ft.cuda()\n    criterion = criterion.cuda()\n    # criterion = LabelSmoothingCrossEntropy().cuda()\n# \u4fee\u6539\u9009\u62e9\u4f7f\u7528\u7684optimizer\noptimizer = optim.SGD((model_ft.parameters()), lr=lr,\n                      momentum=momentum, weight_decay=0.0004)\n# optimizer = optim.AdamW((model_ft.parameters()), lr=0.0001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)","97c940c9":"#  train_loss, best_model_wts = train_model(model_ft, criterion, optimizer, exp_lr_scheduler, num_epochs=num_epochs)","b915e549":"model_ft_test = EfficientNet.from_name('efficientnet-b2')\nnum_ftrs = model_ft_test._fc.in_features\nmodel_ft_test._fc = nn.Linear(num_ftrs, class_num)\nnet_weight = '..\/input\/weight\/best_val_efficientnet-b2.pth'\nstate_dict = torch.load(net_weight)\nmodel_ft_test.load_state_dict(state_dict)\n\nmodel_ft_test.cuda()\nmodel_ft_test.eval()\ndata_transforms = transforms.Compose([\n            transforms.Resize(input_size),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","976a7126":"res = []\nhash1 = {}\nfor imagepath in os.listdir('..\/input\/issm2020-ai-challenge\/semTest\/semTest'):\n    p = []\n    image = Image.open('..\/input\/issm2020-ai-challenge\/semTest\/semTest\/'+imagepath)\n    imgprob = data_transforms(image).unsqueeze(0)\n    imgprob = Variable(imgprob).cuda()\n    torch.no_grad()\n#     _,predict = torch.max(model_ft_test(imgprob).data,1)\n    logit = model_ft_test(imgprob)\n    p.append(F.softmax(logit, -1))\n\n# tta ----\n    if 1:\n        logit = model_ft_test(torch.flip(imgprob, dims=(2,)).contiguous())\n        p.append(F.softmax(logit, -1))\n\n        logit = model_ft_test(torch.flip(imgprob, dims=(3,)).contiguous())\n        p.append(F.softmax(logit, -1))\n\n        logit = model_ft_test(torch.flip(imgprob, dims=(2,3)).contiguous())\n        p.append(F.softmax(logit, -1))\n\n        logit = model_ft_test(imgprob.permute(0,1,3,2).contiguous())\n        p.append(F.softmax(logit, -1))\n\n        # ---------\n    p1 = torch.stack(p).mean(0)\n    predict = p1.argmax(-1)\n    if imagepath.endswith('.jpg'):\n        hash1[int(imagepath.rstrip('.jpg'))] = (int(predict.cpu().numpy()))\n    else:\n        hash1[int(imagepath.rstrip('.JPG'))] = (int(predict.cpu().numpy()))","ade2d767":"for i in sorted(hash1):\n    res.append(hash1[i]+1)\n\ndata = {'Id':list(range(1,351)),'LABEL':res}\ndf = pd.DataFrame(data)\ndf.to_csv('submission.csv',index=None)","7b138292":"# data augmentation","ed1caa45":"# label smoothing","eb58175b":"# \u8fdb\u884c test"}}