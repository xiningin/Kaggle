{"cell_type":{"c4900f79":"code","bb5f1f3b":"code","32d4c003":"code","03955f6a":"code","ec32c1cf":"code","99396323":"code","b9120139":"code","a4993576":"markdown","95a2500f":"markdown","abbc38d4":"markdown"},"source":{"c4900f79":"# Importing required Packages\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stat\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport random\nimport time","bb5f1f3b":"data = pd.read_csv(\"..\/input\/SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\nprint(data.head())\nprint(data.tail())","32d4c003":"l = data['l'].values\nt = data['t'].values\ntsq = t * t","03955f6a":"def train_one_batch(x, y, m, c, eta):\n    const = - 2.0\/len(y)\n    ycalc = m * x + c\n    delta_m = const * sum(x * (y - ycalc))\n    delta_c = const * sum(y - ycalc)\n    m = m - delta_m * eta\n    c = c - delta_c * eta\n    error = sum((y - ycalc)**2)\/len(y)\n    return m, c, error\n\ndef train_batches(x, y, m, c, eta, batch_size):\n    # Making the batches\n    random_idx = np.arange(len(y))\n    np.random.shuffle(random_idx)\n    \n    # Train each batch\n    for batch in range(len(y)\/\/batch_size):\n        batch_idx = random_idx[batch*batch_size:(batch+1)*batch_size]\n        batch_x = x[batch_idx]\n        batch_y = y[batch_idx]\n        m, c, err = train_one_batch(batch_x, batch_y, m, c, eta)\n    \n    return m, c, err\n\ndef train_minibatch(x, y, m, c, eta, batch_size=10, iterations=1000):\n    for iteration in range(iterations):\n        m, c, err = train_batches(x, y, m, c, eta, batch_size)\n    return m, c, err","ec32c1cf":"# Init m, c\nm, c = 0, 0\n\n# Learning rate\nlr = 0.001\n\n# Batch size\nbatch_size = 10","99396323":"# Training for 1000 iterations, plotting after every 100 iterations:\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\nplt.ion()\nfig.show()\nfig.canvas.draw()\n\nfor num in range(10):\n    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=90, iterations=100)\n    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n    y = m * l + c\n    ax.clear()\n    ax.plot(l, tsq, '.k')\n    ax.plot(l, y)\n    fig.canvas.draw()\n    time.sleep(1)","b9120139":"ms, cs,errs = [], [], []\nm, c = 0, 0\nlr = 0.001\nbatch_size = 10\nfor times in range(100):\n    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size, iterations=100) # We will plot the value of for every 100 iterations\n    ms.append(m)\n    cs.append(c)\n    errs.append(error)\nepoch = range(0, 10000, 100)\nplt.figure(figsize=(8, 5))\nplt.plot(epoch, errs)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Error\")\nplt.title(\"Minibatch Gradient Descent\")\nplt.show()","a4993576":"\n## Plotting error vs iterations","95a2500f":"check the error value at saturation, and time it takes to reach saturation.","abbc38d4":"## Mini-Batch Gradient Descent\n\nIn Mini-Batch Gradient Descent algorithm, rather than using  the complete data set, in every iteration we use a subset of training examples (called \"batch\") to compute the gradient of the cost function. \n\nCommon mini-batch sizes range between 50 and 256, but can vary for different applications.\n\none_batch() : we will be calculating the essenial parts of the Gradient Descent method:  \n\n$y = mx + c$\n        \n$E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y_i - y)^2$\n\n$\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y_i - (mx_i + c))$\n \n$\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y_i - (mx_i + c))$\n\none_step() : We will be splitting our data into batches."}}