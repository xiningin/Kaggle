{"cell_type":{"965021e9":"code","87b5ad1f":"code","12737d49":"code","a17f55d3":"code","7bd859b3":"code","e4caa974":"code","b3105c2a":"code","9f26acc9":"code","3c40af6e":"code","1d2e7bb1":"code","f7701715":"code","d0cb29b6":"code","c2589f6c":"code","c9361a26":"code","1e64ed20":"code","ad02bdce":"code","a4419788":"code","3bb072df":"code","1ca561ba":"code","50762e6f":"code","0d06c32e":"code","53e634f5":"markdown","2a5383df":"markdown","946dffb3":"markdown","660630a5":"markdown","b5715bf3":"markdown","a044fca5":"markdown","4f69c7df":"markdown","f271f54b":"markdown","86dd6e6d":"markdown","385877c4":"markdown","2713ce7f":"markdown","1e6fb9ea":"markdown","17630b28":"markdown","b7e64722":"markdown","e5d58ca2":"markdown","00704ad1":"markdown","a090bdad":"markdown","c71fbf3a":"markdown","22aa8258":"markdown","053a8832":"markdown","e8b4929e":"markdown","65a7f383":"markdown","d12aed3a":"markdown","3994beb7":"markdown","0b3ace15":"markdown","f7285c76":"markdown","e0802648":"markdown","d3fd15c7":"markdown"},"source":{"965021e9":"import os\nimport warnings\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport math\nfrom pathlib import Path\n\n# Mute warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","87b5ad1f":"!tree ..\/input\/","12737d49":"data_dir = Path('..\/input\/tabular-playground-series-jan-2022')\nholiday_dir = Path('..\/input\/public-and-unofficial-holidays-nor-fin-swe-201519')\ngdp_dir = Path('..\/input\/gdp-20152019-finland-norway-and-sweden')\n\ntrain = pd.read_csv(\n    data_dir \/ 'train.csv',\n    dtype={\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n        'num_sold': 'float32',\n    },\n    index_col='row_id'\n)\n\ntest = pd.read_csv(\n    data_dir \/ \"test.csv\",\n    dtype={\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n    },\n    index_col='row_id'\n)\n\ntarget_col = train.columns.difference(test.columns)[0]\n\nholiday_data = pd.read_csv(holiday_dir \/ 'holidays.csv')\n\ngdp = pd.read_csv(\n    gdp_dir \/ 'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')","a17f55d3":"train.info()","7bd859b3":"def len_data_count(column):\n    return column.str.len().value_counts()\n\nprint(len_data_count(train.date))\nprint(len_data_count(test.date))","e4caa974":"train['date'] = pd.to_datetime(train['date'])\ntest['date']  = pd.to_datetime(test['date'])","b3105c2a":"display(train.isnull().sum(), test.isnull().sum())","9f26acc9":"display(train.iloc[:,1:-1].nunique(), test.iloc[:,1:-1].nunique())","3c40af6e":"# Count for each unique values\ncategorical_cols = train.select_dtypes('category').columns.tolist()\n\nfor col in categorical_cols:\n    display(pd.DataFrame(train[col].value_counts()))","1d2e7bb1":"import dateutil.easter as easter\n\ndef holiday_features(holiday_df, df):\n    \n    fin_holiday = holiday_df.loc[holiday_df.country == 'Finland']\n    swe_holiday = holiday_df.loc[holiday_df.country == 'Sweden']\n    nor_holiday = holiday_df.loc[holiday_df.country == 'Norway']\n    \n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    \n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    \n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    gdp_exponent = 1.2121103201489674\n    # c.f https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\n    \n    # GDP features\n    def get_gdp(row):\n        \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n        country = 'GDP_' + row.country\n        \n        return gdp.loc[row.date.year, country] ** gdp_exponent\n    \n    df['gdp'] = pd.DataFrame(df.apply(get_gdp, axis=1))\n    \n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-5, 65)\n    \n    # Last Sunday of May (Mother's Day)\n    sun_may_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-5-31')),\n        2016: pd.Timestamp(('2016-5-29')),\n        2017: pd.Timestamp(('2017-5-28')),\n        2018: pd.Timestamp(('2018-5-27')),\n        2019: pd.Timestamp(('2019-5-26'))\n    })\n    #new_df['days_from_sun_may'] = (df.date - sun_may_date).dt.days.clip(-1, 9)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-06-24')),\n        2016: pd.Timestamp(('2016-06-29')),\n        2017: pd.Timestamp(('2017-06-28')),\n        2018: pd.Timestamp(('2018-06-27')),\n        2019: pd.Timestamp(('2019-06-26'))\n    })\n    df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-11-1')),\n        2016: pd.Timestamp(('2016-11-6')),\n        2017: pd.Timestamp(('2017-11-5')),\n        2018: pd.Timestamp(('2018-11-4')),\n        2019: pd.Timestamp(('2019-11-3'))\n    })\n    df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return df\n\ntrain = holiday_features(holiday_data, train)\ntest  = holiday_features(holiday_data, test)","f7701715":"train = pd.get_dummies(train, columns=categorical_cols)\ntest  = pd.get_dummies(test, columns=categorical_cols)","d0cb29b6":"# Nothing to see here!\n# Copy 'date' feature for further visualization\/explanation\ndate_copy = train.date","c2589f6c":"def new_date_features(df):\n    df['year'] = df.date.dt.year \n    df['quarter'] = df.date.dt.quarter\n    df['month'] = df.date.dt.month  \n    df['week'] = df.date.dt.week \n    df['day'] = df.date.dt.day  \n    df['weekday'] = df.date.dt.weekday\n    df['day_of_week'] = df.date.dt.dayofweek  \n    df['day_of_year'] = df.date.dt.dayofyear  \n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_month'] = df.date.dt.days_in_month  \n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    df['is_friday'] = np.where((df['weekday'] == 4), 1, 0)\n    \n    df.drop('date', axis=1, inplace=True)\n    \n    return df\n    \ntrain = new_date_features(train)\ntest  = new_date_features(test)","c9361a26":"# Target transformation\ny = np.log1p(train[target_col] \/ train.gdp)\n\ntrain.drop(target_col, axis=1, inplace=True)\ntrain","1e64ed20":"# Function modified from:\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_indices.html\n# Inspired by https:\/\/www.kaggle.com\/tomwarrens\/timeseriessplit-how-to-use-it\/notebook\n\nfrom matplotlib.patches import Patch\n\ndef plot_cv_indices(cv, X, y, n_splits, date_col=None):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize = (12, 8))\n\n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=10,\n            cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n            zorder=2\n        )\n\n    # Formatting\n    yticklabels = list(range(n_splits))\n    \n    if date_col is not None:\n        tick_locations  = ax.get_xticks()\n        tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n\n        tick_locations_str = [str(int(i)) for i in tick_locations]\n        new_labels = ['\\n\\n'.join(x) for x in zip(list(tick_locations_str), tick_dates)]\n        \n        ax.set_xticks(tick_locations)\n        ax.set_xticklabels(new_labels)\n    \n    # Custom visualization\n    ax.set_facecolor('#fcfcfc')\n    ax.grid(alpha=0.7, linewidth=1, zorder=0)\n    \n    ax.set_yticks(np.arange(n_splits) + .5)\n    ax.set_yticklabels(yticklabels)\n    ax.set_ylabel('CV iteration', fontsize=15, labelpad=10)\n    ax.set_ylim([n_splits+0.2, -.2])\n    ax.yaxis.set_tick_params(labelsize=12, pad=10, length=0)\n    \n    ax.set_xlabel('Sample index', fontsize=15, labelpad=10)\n    ax.xaxis.set_tick_params(labelsize=12, pad=10, length=0)\n    \n    ax.legend(\n        [\n            Patch(color=cmap_cv(.8)), \n            Patch(color=cmap_cv(.02))\n        ],\n        [\n            'Testing set', \n            'Training set'\n        ],\n        fontsize=12,\n        loc=(1.02, .8)\n    )\n    \n    ax.set_title(\n        '{}'.format(type(cv).__name__),\n        loc=\"left\", \n        color=\"#000\", \n        fontsize=20, \n        pad=5, \n        y=1, \n        zorder=3\n    )\n    \n    return ax","ad02bdce":"from sklearn.model_selection import TimeSeriesSplit\n\nfolds = TimeSeriesSplit(n_splits=4)\n\n# Visualization\ncmap_cv = plt.cm.bwr\nplot_cv_indices(folds, train, y, folds.n_splits, date_col=date_copy);","a4419788":"def smape(actual, predicted):\n    numerator = np.abs(predicted - actual)\n    denominator = (np.abs(actual) + np.abs(predicted)) \/ 2\n    \n    return np.mean(numerator \/ denominator)*100","3bb072df":"from catboost import CatBoostRegressor\n\ny_pred = np.zeros(len(test))\nscores = []\n\nfor fold, (train_id, test_id) in enumerate(folds.split(train, groups=date_copy.dt.year)):\n    print(\"Fold: \", fold)\n    \n    # Splitting\n    X_train, y_train = train.iloc[train_id], y.iloc[train_id]\n    X_valid, y_valid = train.iloc[test_id], y.iloc[test_id]\n    \n    # Model with parameters\n    params = {\n        'iterations': 10000,\n        'depth': 5, \n        'l2_leaf_reg': 12.06,\n        'bootstrap_type': 'Bayesian',\n        'boosting_type': 'Plain',\n        'loss_function': 'MAE',\n        'eval_metric': 'SMAPE',\n        'od_type': 'Iter',       # type of overfitting detector\n        'od_wait': 40,\n        'has_time': True         # use the order of the data (ts), do not permute\n    }\n    \n    model = CatBoostRegressor(**params)\n\n    # Training\n    model.fit(\n        X_train, y_train, \n        eval_set=(X_valid, y_valid),\n        early_stopping_rounds=1000,\n        verbose=1000\n    )\n    \n    print('\\n')\n    \n    # Evaluation\n    valid_pred = model.predict(X_valid)\n    \n    valid_score = smape(\n        np.expm1(y_valid) * X_valid.gdp.values, \n        np.ceil(np.expm1(valid_pred) * X_valid.gdp.values)\n    )\n    \n    scores.append(valid_score)\n    \n    # Prediction for submission\n    y_pred += (np.expm1(model.predict(test)) * test.gdp.values) \/ folds.n_splits","1ca561ba":"score = np.array(scores).mean()\nprint('Mean SMAPE score: ', score)","50762e6f":"submission = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\nsubmission.num_sold = np.ceil(y_pred) # rounding up\nsubmission","0d06c32e":"submission.to_csv('submission.csv', index=False)","53e634f5":"It looks like all values are 10-characters long, which is good news. We can now convert our column.","2a5383df":"# TPS-Jan22 \ud83c\udf89 | EDA + FE + Simple CatBoost","946dffb3":"We are dealing with time-series data, therefore it is relevant to consider the impact of holidays, which naturally play a large role in business activities.","660630a5":"First, let's have a glance at some basic information about our data.","b5715bf3":"![SMAPE formula](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20211120224204\/smapeformula.png)","a044fca5":"Well, the least that can be said is that **all features are balanced!**","4f69c7df":"### <a name=\"FeatureAnalysis\">Feature Analysis<\/a>","f271f54b":"___\n# <a name=\"EDA\">\ud83d\udd0d Exploratory Data Analysis<\/a>","86dd6e6d":"Finally, here are our datasets, before moving to the cross-validation step.","385877c4":"**Tip:** Since the SMAPE evaluation metric is asymmetric. In this case, underestimated values are much more penalized than overestimated values. Then, feel free to round your predictions **up** to the nearest value.<br \/>\n<br \/>\n\ud83d\udccc You will find more by having a glance to these awesome notebooks: \n> * [SMAPE Weirdness](https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness) by [CPMP](https:\/\/www.kaggle.com\/cpmpml)\n> * [TPS Jan 2022: A simple average model (no ML)](https:\/\/www.kaggle.com\/carlmcbrideellis\/tps-jan-2022-a-simple-average-model-no-ml) by [Carl McBride Ellis](https:\/\/www.kaggle.com\/carlmcbrideellis).\n>\n>The last one being related to this very Tabular Playground.","2713ce7f":"___\n# <a name=\"Validation\">\u2705 Cross-validation method<\/a>","1e6fb9ea":"Next, we can evaluate the model thanks to our custom SMAPE function.","17630b28":"### Happy New Year!\nMay this year bring good health, happiness and success at whatever you choose to accomplish!\n\n___\n\n# \ud83d\udccc Introduction\n\n>Hello Kagglers,\n>\n>This notebook is a simple implementation of a CatBoost Regressor, as well as some EDA, feature engineering, cross-validation explanation and a SMAPE function.\n>\n>As a beginner and newcomer, making this first notebook as public is a milestone for me. I believe that there is no better way to improve than to share and report on your knowledge, investigations and achievements.\n>\n>May it be interesting and useful to you. Do not hesitate to provide feedback!\n\n\n# \ud83d\udcdd Agenda\n>1. [\ud83d\udcda Loading libraries and files](#Loading)\n>2. [\ud83d\udd0d Exploratory Data Analysis](#EDA)\n>3. [\u2699\ufe0f Feature Engineering](#FeatureEngineering)\n>4. [\u2705 Cross-validation Method](#Validation)\n>5. [\ud83c\udfcb\ufe0f Model Training & Inference](#TrainingInference)","b7e64722":"\ud83d\udccc This part has been updated and largely inspired by these two notebooks:\n> * [TPSJAN22-03 Linear Model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model) & [TPSJAN22-06 LightGBM Quickstart](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart) by [AmbrosM](https:\/\/www.kaggle.com\/ambrosm)<br \/>\n> * [TPS Jan 22 - EDA + modelling](https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-eda-modelling) by [Samuel Cortinhas](https:\/\/www.kaggle.com\/samuelcortinhas)","e5d58ca2":"We now have the confirmation.\n\nAfterwards, let's look at the **cardinality** of each column.","00704ad1":"### Submission","a090bdad":"Moreover, it looks like there is no missing values in any field.\n\nWant to make sure about it? Alright.","c71fbf3a":"___\n# <a name=\"TrainingInference\">\ud83c\udfcb\ufe0f Model Training & Inference<\/a>","22aa8258":"Submissions are evaluated on SMAPE between forecasts and actual values.","053a8832":"### Evaluation","e8b4929e":"___\n# <a name=\"Loading\">\ud83d\udcda Loading libraries and files<\/a>","65a7f383":"As afore-mentionned, we are dealing with time-series data.<br \/>\nThus, we do not want to use information about the future to train our model. We will therefore opt for <code>TimeSeriesSplit<\/code> as a our **cross-validation** technique.\n\n\ud83d\udccc According to the *scikit-learn* documentation:\n>[TimeSeriesSplit](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.TimeSeriesSplit.html) provides train\/test indices to split time series data samples that are observed at fixed time intervals, in train\/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate.","d12aed3a":"Next, the cardinality of each categorical feature is quite low, and that we do not want to impose an ordinal order, **one-hot encoding** may be a good way to encode our categorical features.","3994beb7":"___\n# <a name=\"FeatureEngineering\">\u2699\ufe0f Feature Engineering<\/a>","0b3ace15":"Note that the <code>date<\/code> feature is originally <code>str<\/code>-typed, so we will convert it to <code>datetime<\/code> to make any further process easier with *pandas*.\n\nHowever, before converting <code>date<\/code> values, let's see if all of the values are, ideally, following the same <code>month\/day\/four-digit year<\/code> format. We can get an idea of how widespread this issue is by checking the length of each entry in the <code>date<\/code> column.","f7285c76":"Then, it would be relevant to count each of these values' occurrences.<br \/>\nAt least for the categorical features, since the <code>date<\/code> column will be the subject of a later treatment.","e0802648":"Since we have a <code>date<\/code>-typed feature here, and models are rarely able to use dates and times as they are, we would benefit from encoding it as categorical variables as this can often yield useful information about temporal patterns.\n\nFurthermore, time-series data (such as product sales) often have distributions that differs from week days to week-ends for example, it is likely that using the day of the week as a new feature is a relevant option we have.","d3fd15c7":"### Training phase"}}