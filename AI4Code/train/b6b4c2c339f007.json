{"cell_type":{"863fdcea":"code","535f408c":"code","d21776f2":"code","c2749b22":"code","674b0d58":"code","6305b2dd":"code","d1000013":"code","c2a5d84c":"code","decd331a":"code","2b7870c4":"code","20d0f620":"code","79a7e6a0":"code","8a7fc962":"markdown","a2201fe7":"markdown","6b6ba482":"markdown","86cdf2a2":"markdown"},"source":{"863fdcea":"import os\nimport cv2\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm","535f408c":"!rm -rf .\/dataset","d21776f2":"os.mkdir('dataset')\nos.mkdir('dataset\/trainA')\nos.mkdir('dataset\/trainB')","c2749b22":"np.random.seed(42)\n\npath = '..\/input\/hazing-images-dataset-cvpr-2019'\nnum_images = len(os.listdir(path+'\/GT\/'))\n\nidx = np.arange(1,num_images+1)\n\nnum_images","674b0d58":"def make_data(src_path, dst_path, label, image_idx):\n    src_path = src_path + f'\/{label}\/'\n    \n    for f in tqdm(image_idx):\n\n        src = src_path + \"{:02d}\".format(f) + '_' + label + '.png'\n        dst = dst_path + \"{:02d}\".format(f) + '_' + label + '.png'\n\n        shutil.copy(src,dst)","6305b2dd":"make_data(path, 'dataset\/trainA\/', 'GT', idx)\nmake_data(path, 'dataset\/trainB\/', 'hazy', idx)","d1000013":"!git clone https:\/\/github.com\/junyanz\/pytorch-CycleGAN-and-pix2pix.git","c2a5d84c":"!pip install -r .\/pytorch-CycleGAN-and-pix2pix\/requirements.txt","decd331a":"!python .\/pytorch-CycleGAN-and-pix2pix\/train.py --dataroot .\/dataset --name dehaze_cyclegan --model cycle_gan","2b7870c4":"cp \/kaggle\/working\/checkpoints\/dehaze_cyclegan\/latest_net_G_A.pth \/kaggle\/working\/checkpoints\/dehaze_cyclegan\/latest_net_G.pth","20d0f620":"!python .\/pytorch-CycleGAN-and-pix2pix\/test.py --dataroot ..\/input\/test-sample --name dehaze_cyclegan --model test --no_dropout --direction AtoB","79a7e6a0":"test_data_path = '..\/input\/test-sample'\nresults_path = '\/kaggle\/working\/results\/dehaze_cyclegan\/test_latest\/images\/'\n\nfor f in os.listdir(test_data_path):\n    fake = f.split('.')[0] + '_fake.png'\n    real = f.split('.')[0] + '_real.png'\n    \n    fig=plt.figure(figsize=(15, 5))\n\n    ax = plt.subplot(131)\n    img1 = cv2.imread(results_path+real)\n    plt.imshow(img1)\n\n    ax = plt.subplot(132)\n    img2 = cv2.imread(results_path+fake)\n    plt.imshow(img2)\n\n    plt.show()","8a7fc962":"## TESTING\n\nLet's test the model on random images from Google.","a2201fe7":"## Creating Hazy Images using CycleGANs\n\n**We will use CycleGANs to learn a mapping between image pairs (hazy\/non-hazy) and use the model to create hazy images from real ones, since it works both ways.**","6b6ba482":"## Style Transfer Techniques\n\n### Model: CycleGAN\n    \nCycleGANs and Pix2Pix are Style-Transfer techniques i.e. translating images from one style to another. For example - Photo2Monet i.e. translating real images to Monet styled paintings.\n\nCycleGAN is an **unsupervised** technique, which means that you do not require images from two styles in pairs. **You can have a bag of images from one style and completely unrelated bag of images from another style and CycleGANs can learn to map a pattern between two styles.** \n\n**Another interesting thing about CycleGANs is that since they are unsupervised, they learn a mapping both ways A to B and B to A, given an image pair (A,B).**\n\nPix2Pix, however, is a **supervised** technique, which means you need to have image pairs from two styles as data to be fed to the model to learn a mapping. Since it's a supervised technique aiming to learn a mapping, it should give good results even with smaller amount of data, compared to CycleGANs. In my opinion, unsupervised techniques require larger amount of data compared to supervised techniques because the latter is getting some help in form of labels, task of which is only to learn a mapping between the features and target variable and nothing beyond it.","86cdf2a2":"#### We can use the model to generate hazy images from any set of images."}}