{"cell_type":{"856a205e":"code","97e67a14":"code","2e55c0fe":"code","f3a18178":"code","ff189f85":"code","b576b1c2":"code","256cee69":"code","3fd4efbc":"code","3f77543b":"code","218c744d":"code","cea053b6":"code","d405fea8":"code","ab6413a1":"code","8c19bcd4":"code","2a59ce33":"code","ba9b00ae":"code","9c2332ae":"code","26e520e2":"code","04d2ece5":"markdown","c220bd0c":"markdown","f1a3a69d":"markdown","7172cb26":"markdown","d7d67855":"markdown","2a018ae0":"markdown","ed186b3d":"markdown","df532b73":"markdown","e3ec4d5e":"markdown","38938122":"markdown","3443fad9":"markdown","56cb6948":"markdown","1f4b2eff":"markdown","52cd7476":"markdown","05ea978d":"markdown","a3fae7b3":"markdown","771f4881":"markdown"},"source":{"856a205e":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport string\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn","97e67a14":"df = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","2e55c0fe":"def data_preprocessing(text):\n    text = text.lower()\n    text = re.sub('<.*?>', '', text) # Remove HTML from text\n    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n    text = [word for word in text.split() if word not in stop_words]\n    text = ' '.join(text)\n    return text\n\ndf['cleaned_reviews'] = df['review'].apply(data_preprocessing)\ndf.head()","f3a18178":"corpus = [word for text in df['cleaned_reviews'] for word in text.split()]\ncount_words = Counter(corpus)\nsorted_words = count_words.most_common()","ff189f85":"keys = []\nvalues = []\nfor key, value in sorted_words[:20]:\n    keys.append(key)\n    values.append(value)\n    \nplt.figure(figsize=(12, 5))\nplt.bar(keys, values)\nplt.title('Top 20 most common words', size=15)\nplt.show()","b576b1c2":"vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n\nreviews_int = []\nfor text in df['cleaned_reviews']:\n    r = [vocab_to_int[word] for word in text.split()]\n    reviews_int.append(r)\n\nprint(reviews_int[:1])\ndf['Review int'] = reviews_int","256cee69":"df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\ndf.head()","3fd4efbc":"review_len = [len(x) for x in reviews_int]\ndf['Review len'] = review_len\ndf.head()","3f77543b":"print(df['Review len'].describe())\n\ndf['Review len'].hist()\nplt.title('Review length distribution', size=15)\nplt.show()","218c744d":"def Padding(review_int, seq_len):\n    '''\n    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features = np.zeros((len(reviews_int), seq_len), dtype = int)\n    for i, review in enumerate(review_int):\n        if len(review) <= seq_len:\n            zeros = list(np.zeros(seq_len - len(review)))\n            new = zeros + review\n        else:\n            new = review[: seq_len]\n        features[i, :] = np.array(new)\n            \n    return features","cea053b6":"features = Padding(reviews_int, 200)\nprint(features[0, :])","d405fea8":"X_train, X_remain, y_train, y_remain = train_test_split(features, df['sentiment'].to_numpy(), test_size=0.2, random_state=1)\nX_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)","ab6413a1":"# create tensor dataset\ntrain_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\ntest_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\nvalid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n\n# dataloaders\nbatch_size = 50\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","8c19bcd4":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","2a59ce33":"class sentimentLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super().__init__()\n        \n        self.output_size = output_size\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        \n        # Embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # Linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n        \n        #embedding and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        #stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # Dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        #sigmoid function\n        sig_out = self.sigmoid(out)\n        \n        # reshape to be batch size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden","ba9b00ae":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")\n\n# Instantiate the model w\/ hyperparams\nvocab_size = len(vocab_to_int) + 1\noutput_size = 1\nembedding_dim = 64\nhidden_dim = 256\nn_layers = 2\n\nmodel = sentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nmodel = model.to(device)\n\nprint(model)","9c2332ae":"lr=0.001\n\ncriterion = nn.BCELoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()\n\nclip = 5\nepochs = 2\nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n \n    \n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n        val_h = tuple([each.data for each in val_h])\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        output, val_h = model(inputs, val_h)\n        val_loss = criterion(output.squeeze(), labels.float())\n\n        val_losses.append(val_loss.item())\n            \n        accuracy = acc(output,labels)\n        val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc\/len(train_loader.dataset)\n    epoch_val_acc = val_acc\/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '..\/working\/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')","26e520e2":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\ntest_h = model.init_hidden(batch_size)\n\nmodel.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    test_h = tuple([each.data for each in test_h])\n\n    inputs, labels = inputs.to(device), labels.to(device)\n    \n    output, test_h = model(inputs, test_h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))\n","04d2ece5":"# Sentiment Analysis using LSTM","c220bd0c":"## Thank you ","f1a3a69d":"## 14) Define the Model Class","7172cb26":"## 7) Tokenize \u2014 Encode the labels\nThis is simple because we only have 2 output labels. So, we will just label \u2018positive\u2019 as 1 and \u2018negative\u2019 as 0","d7d67855":"## What is Sentiment Analysis:\nthe process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n\nI think this result from google dictionary gives a very succinct definition. I don\u2019t have to re-emphasize how important sentiment analysis has become. So, here we will build a classifier on IMDB movie dataset using a Deep Learning technique called RNN.\n\nI\u2019m outlining a step-by-step process for how Recurrent Neural Networks (RNN) can be implemented using Long Short Term Memory (LSTM) architecture:\n\n1. Load in and visualize the data\n2. Data Processing \u2014 convert to lower case, Remove punctuation etc.\n5. Tokenize \u2014 Create Vocab to Int mapping dictionary\n6. Tokenize \u2014 Encode the words\n7. Tokenize \u2014 Encode the labels\n8. Analyze Reviews Length\n9. Removing Outliers \u2014 Getting rid of extremely long or short reviews\n10. Padding \/ Truncating the remaining data\n11. Training, Validation, Test Dataset Split\n12. Dataloaders and Batching\n13. Define the LSTM Network Architecture\n14. Define the Model Class\n15. Training the Network\n16. Testing","2a018ae0":"## 11) Training, Validation, Test Dataset Split\nOnce we have got our data in nice shape, we will split it into training, validation and test sets\n\n<b>train= 80% | valid = 10% | test = 10% <\/b>","ed186b3d":"## 13) Define the LSTM Network Architecture\n\n<img src='https:\/\/miro.medium.com\/max\/700\/1*SICYykT7ybua1gVJDNlajw.png'>\n\nThe layers are as follows:\n\n0. Tokenize : This is not a layer for LSTM network but a mandatory step of converting our words into tokens (integers)\n1. Embedding Layer: that converts our word tokens (integers) into embedding of specific size\n2. LSTM Layer: defined by hidden state dims and number of layers\n3. Fully Connected Layer: that maps output of LSTM layer to a desired output size\n4. Sigmoid Activation Layer: that turns all output values in a value between 0 and 1\n5. Output: Sigmoid output from the last timestep is considered as the final output of this network\n","df532b73":"## 16) Testing","e3ec4d5e":"<b>Observations : <\/b>a) Mean review length = 226 b) Most of the reviews less than 500 words or more d) There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis","38938122":"## 2) Data Processing \u2014 convert to lower case, Remove punctuation etc","3443fad9":"## Training Loop","56cb6948":"## 5) Tokenize \u2014 Create Vocab to Int mapping dictionary\nIn most of the NLP tasks, you will create an index mapping dictionary in such a way that your frequently occurring words are assigned lower indexes. One of the most common way of doing this is to use Counter method from Collections library.","1f4b2eff":"## 10) Padding \/ Truncating the remaining data\nTo deal with both short and long reviews, we will pad or truncate all our reviews to a specific length. We define this length by Sequence Length. This sequence length is same as number of time steps for LSTM layer.\n\nFor reviews shorter than seq_length, we will pad with 0s. For reviews longer than seq_length we will truncate them to the first seq_length words.","52cd7476":"## 1) Load in and visualize the data","05ea978d":"## 12) Dataloaders and Batching\nAfter creating our training, test and validation data. Next step is to create dataloaders for this data. We can use generator function for batching our data into batches instead we will use a TensorDataset. This is one of a very useful utility in PyTorch for using our data with DataLoaders with exact same ease as of torchvision datasets","a3fae7b3":"## 8) Analyze Reviews Length","771f4881":"There is a small trick here, in this mapping index will start from 0 i.e. mapping of \u2018the\u2019 will be 0. But later on we are going to do padding for shorter reviews and conventional choice for padding is 0. So we need to start this indexing from 1"}}