{"cell_type":{"8d182010":"code","0a3290dd":"code","7b7380aa":"code","f16b9a61":"code","2ad517a2":"code","dc142df5":"code","2bf06374":"code","a631bdc9":"code","ed204aad":"code","b66d7b7d":"code","ed9d74bd":"code","a65bc1dc":"code","d5f1ad31":"code","fee8b3d6":"code","f6dd0b37":"code","98f1278b":"code","96b561a6":"code","90e387a8":"code","4f3b9d41":"code","77cc8032":"code","9e7a5ade":"code","97ece82e":"code","48b0eafd":"code","56b17795":"code","fda16149":"code","11441b41":"code","49fb4bfe":"markdown","b0e7777d":"markdown","7f820a73":"markdown","8ef2c15d":"markdown","99f86646":"markdown"},"source":{"8d182010":"import numpy as np\nimport math\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0a3290dd":"# Load our data using pandas and explore if we need to clean\/feature engineer\ndf = pd.read_csv(r\"..\/input\/heart-disease-uci\/heart.csv\")\n#Let's see what we loaded\ndf","7b7380aa":"#Let's see if we have any nan's\ndef explore_dataframe(df):\n    # function to print nan values, column types\n    print(\"-\"*25)\n    print(df.info()) #overall info of columns\n    print(\"-\"*25)\n    print(df.isna().sum()) #quantity of nans\n    print(\"-\"*25)\n    print(df.isna().mean()*100) #percentage of nan's\n    print(\"-\"*25)\n    print(df.dtypes) #variable types\n    print(\"-\"*25)\n    ","f16b9a61":"explore_dataframe(df)\n#Cool our dataset is complete and don't need to clean","2ad517a2":"columns = [col for col in df]\ncolumn_features = columns [ : -1]\ntarget = columns[-1]\nprint(column_features)\nprint(target)\n","dc142df5":"sns_plot = sns.distplot(df[target])","2bf06374":"#Exploring the range and distribution of numerical Variables\ndef univariate_plot(df,columns):\n    fig, ax = plt.subplots(len(columns), 2, figsize = (15, 30))\n    for idx, column in enumerate(columns):\n        sns.boxplot(x= df[column], ax = ax[idx,0])\n        sns.distplot(df[column], ax = ax[idx,1])\n    plt.tight_layout()\n        ","a631bdc9":"univariate_plot(df, columns)","ed204aad":"def bivariate_plot(df,columns, target):\n    f,axarr = plt.subplots(len(columns), figsize=(15,40))\n    target_values = df[target].values\n    for idx,column in enumerate(columns):\n        axarr[idx].scatter(df[column].values, target_values)\n        axarr[idx].set_title(column)\n    f.text(-0.01, 0.5, target, va='center', rotation='vertical', fontsize = 12)\n    plt.tight_layout()\n    plt.show()","b66d7b7d":"bivariate_plot(df,column_features,target)","ed9d74bd":"def heatmap(df):\n    plt.figure(figsize=(10,6))\n    sns.heatmap(df.corr(),cmap=plt.cm.Reds,annot=True)\n    plt.title('Heatmap displaying the relationship betweennthe features of the data',\n         fontsize=13)\n    plt.show()","a65bc1dc":"heatmap(df)\n#With this we have reached a few insights about the data:\n#-The features that most positively correlate (affect) our target are chest pain (cp), thalach (maximum heart rate achieved)\n#and slope(the slope of the peak exercise ST segment).\n#-The features that most negatively correlate (affect) our target are exang(exercise induced angina), oldpeak(ST depression\n#induced by exercise relative to rest),ca(number of major vessels (0-3) colored by flourosopy) and thal(defects)\n","d5f1ad31":"#these columns are not in the range from 0-1, which might worsen our model\n#normalization funtion\ndef normalize_columns(df,columns):\n    for column in columns:\n        df[column] = (df[column] - df[column].min()) \/ (df[column].max() - df[column].min())","fee8b3d6":"#normalize\nnormalize_columns(df,columns)","f6dd0b37":"#shuffle rows in the dataframe since they are ordered by target, the first half is all 1's and the second is 0's\ndf = df.sample(frac=1).reset_index(drop=True)","98f1278b":"#get train-test-split sizes for our dataloader\ntrain_size = int(0.8 * len(df)) + 1\nval_size = math.ceil((len(df) - train_size)\/2)\nval_len = train_size + val_size\ntest_size = val_size \nprint(train_size,val_len,test_size)","96b561a6":"#create the different dataframes to use \ndf_train = df.iloc[:train_size]\ndf_val = df[train_size:val_len]\ndf_test = df[val_len:]","90e387a8":"#check the length of the df's we just created\nprint(len(df_train),len(df_val),len(df_test))\nbatch_size = 32","4f3b9d41":"#create our data loaders\ntrain_target = torch.tensor(df_train['target'].values.astype(np.float32))\ntrain = torch.tensor(df_train.drop('target', axis = 1).values.astype(np.float32)) \ntrain_tensor = TensorDataset(train, train_target) \ntrain_loader = DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n\nval_target = torch.tensor(df_val['target'].values.astype(np.float32))\nval = torch.tensor(df_val.drop('target', axis = 1).values.astype(np.float32)) \nval_tensor = TensorDataset(val, val_target) \nval_loader = DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n\ntest_target = torch.tensor(df_test['target'].values.astype(np.float32))\ntest = torch.tensor(df_test.drop('target', axis = 1).values.astype(np.float32)) \ntest_tensor = TensorDataset(test, test_target) \ntest_loader = DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)","77cc8032":"#An MLP(Multi-layer perceptron) should work just fine for our solution\nimport torch.nn.functional as F\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n        self.out = torch.nn.Linear(n_hidden, n_output)# output layer\n        \n        self.dropout = torch.nn.Dropout(0.40)#dropout layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x)) # activation function for hidden layer\n        x = self.dropout(x)\n        x = self.out(x)\n        return x\n\n","9e7a5ade":"def weights_init_normal(m):\n    \"\"\"\n    Applies initial weights to certain layers in a model .\n    The weights are taken from a normal distribution \n    with mean = 0, std dev = 0.02.\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        y = 0.02\n        m.weight.data.normal_(0, y)\n        m.bias.data.fill_(0)\n    ","97ece82e":"#Create our neural net\nnet = Net(n_feature=13, n_hidden=128, n_output=2)     # define the network\nprint(net)  # net architecture\n","48b0eafd":"#Optimizers\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\ncriterion = torch.nn.CrossEntropyLoss()  # Categorical loss","56b17795":"#Scheduler since we are using SGD, helps us with convergence\nfrom torch.optim.lr_scheduler import StepLR\nscheduler = StepLR(optimizer, step_size=25, gamma=0.1)","fda16149":"#Train our model\niter = 0\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    net.train()\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (features, labels) in enumerate(train_loader):\n        # Load images\n        data = features\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output\/logits\n        outputs = net(data)\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels.type(torch.LongTensor))\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 1 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for features, labels in val_loader:\n                net.eval()\n                # Load images to a Torch Variable\n                data = features\n\n                # Forward pass only to get logits\/output\n                outputs = net(data)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct \/ total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","11441b41":"#Test our model\ntest_loss = 0.\ncorrect = 0.\ntotal = 0.\n\n# set the module to evaluation mode\nnet.eval()\n\n# Iterate through test dataset\nfor batch_idx,(features, labels) in enumerate(test_loader):\n    # Load features\n    data = features\n\n    # Forward pass only to get output\n    outputs = net(data)\n    \n    # calculate the loss\n    loss = criterion(outputs, labels.type(torch.LongTensor))\n    \n    # update average test loss \n    test_loss = test_loss + ((1 \/ (batch_idx + 1)) * (loss.data.item() - test_loss))\n\n    # Get predictions from the maximum value\n    _, predicted = torch.max(outputs.data, 1)\n\n    # Total number of labels\n    total += labels.size(0)\n\n    # Total correct predictions\n    correct += (predicted == labels).sum()\n\n    accuracy = 100 * correct \/ total\n            \n    print('Test Loss: {:.6f}\\n'.format(test_loss))\n\n    print('\\nTest Accuracy: %2d%% (%2d\/%2d)' % (\n        accuracy, correct, total))","49fb4bfe":"# EDA with plots","b0e7777d":"# Import data to use","7f820a73":"# Create Dataloaders","8ef2c15d":"# Create model","99f86646":"# Import libraries to use"}}