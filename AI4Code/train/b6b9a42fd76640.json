{"cell_type":{"eb1cbec5":"code","40c58f9a":"code","16f3d224":"code","faf2fda8":"code","f628ba4f":"code","9419fca1":"code","ab0fbe9d":"code","cd547624":"code","f354d846":"code","828caba5":"code","b5ec3dbe":"code","f675b60b":"code","7df176db":"code","89e91e62":"code","589c20c7":"code","a6640025":"code","206ae3c1":"code","e305cb87":"code","163b0d57":"code","0a4dd35e":"code","2d293427":"code","c38e4db0":"code","d87f69f5":"code","e8a0e4b9":"code","f8fcd6d1":"code","b6c7447e":"code","c173455d":"code","7a0db2f5":"code","6fe390f1":"code","9f0eab03":"code","12a79e65":"code","e514eead":"code","448f4d3f":"code","6806d3d2":"code","cf8802ab":"code","f41a919c":"code","e692de8b":"code","e13ed28a":"code","414839bf":"code","8ad8d189":"code","5dcc7c55":"code","ff9c84d0":"code","8f24b3a7":"code","63bf6207":"code","04f4e219":"code","bc0a1011":"code","1c830ace":"code","73c647fd":"code","2e4d49c5":"code","bad840f9":"code","09835e91":"code","8f21443d":"markdown","ebe01ebe":"markdown","f508b579":"markdown","ae414924":"markdown","b0fd4e6a":"markdown","d6c57e9d":"markdown","0a760e89":"markdown","ad0d3afc":"markdown","00293c3e":"markdown","72546874":"markdown","84af588a":"markdown","4ae3af8f":"markdown","9ba78c1f":"markdown","a2074811":"markdown","7885e7cb":"markdown","b047b09a":"markdown","cc3600d5":"markdown"},"source":{"eb1cbec5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # plots\nimport matplotlib.pyplot as plt # matplotlib\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, math\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40c58f9a":"test = pd.read_csv(dirname + \"\/eval.csv\")\ntrain = pd.read_csv(dirname + \"\/train(1).csv\")\nsample = pd.read_csv(dirname + \"\/sample_submission.csv\")","16f3d224":"sample.head()","faf2fda8":"train.describe()","f628ba4f":"train.head()","9419fca1":"# Plotting\ncols = train.columns\ncolnames = cols[2:-1]\n\nplt.subplots(figsize=(20, 30), sharex=True)\ncols = 4\nrows = math.ceil(len(colnames) \/ cols)\n\nfor i, col in enumerate(colnames):\n    plt.subplot(rows, cols, i + 1)\n    sns.barplot(x='esrb_rating', y=col, data=train)","ab0fbe9d":"train.isna().sum()","cd547624":"train['strong_language'] = train['strong_janguage']\ntest['strong_language'] = test['strong_janguage']\ntest.drop('strong_janguage', axis=1, inplace=True)\ntrain.drop('strong_janguage', axis=1, inplace=True)","f354d846":"test.head()","828caba5":"sample.head()","b5ec3dbe":"y = train['esrb_rating'].values\nX = train.drop(['esrb_rating', 'id', 'title'], axis=1)\nX_test_master = test.drop(['id'], axis=1)","f675b60b":"from sklearn.preprocessing import LabelEncoder","7df176db":"le = LabelEncoder()\ny = le.fit_transform(y.ravel(order=\"c\"))\ny = y.reshape(-1, 1).ravel(order=\"c\")","89e91e62":"plt.figure(figsize=(18,10))\nsns.heatmap(X.corr(), annot=True, cmap='coolwarm')\nplt.show()","589c20c7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a6640025":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score","206ae3c1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","e305cb87":"# I've only put this here at the end, to catch the logistic regression warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nparams = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 50, 100],\n          \"multi_class\": [\"ovr\", \"multinomial\"]}\nsearch = GridSearchCV(LogisticRegression(penalty='l2', random_state=69), params, cv=5)\nsearch.fit(X, y)\n\n# Report the best parameters \/ score\nprint(\"Best CV params\", search.best_params_)\nprint(\"Best score is:\", search.best_score_)\nprint(pd.Series(search.cv_results_[\"mean_test_score\"]).describe())","163b0d57":"logreg = LogisticRegression(penalty='l2', C=50.0, multi_class=\"ovr\", solver='lbfgs')\n\nlogreg.fit(X_train, y_train)\n\n# X_test = logreg.transform(X_test)\nlogreg_pred = logreg.predict(X_test)\nprint(accuracy_score(y_test, logreg_pred))","0a4dd35e":"svm = SVC()\nparameters = {'kernel': ['rbf', 'poly', 'linear', 'sigmoid'], \\\n              \"gamma\" : [0.001, 0.01, 0.1, 1, 5, 10, 100], \\\n              \"C\": [0.1, 1, 5, 10, 50, 100, 1000]}\nsearch = GridSearchCV(svm, parameters, cv=5)\nsearch.fit(X_train, y_train)\n# Report the best parameters\nprint(\"Best CV params\", search.best_params_)\nprint(\"Best score is:\", search.best_score_)\n\nprint(accuracy_score(y_test, search.predict(X_test)))\nprint(pd.Series(search.cv_results_[\"mean_test_score\"]).describe())","2d293427":"y_pred_final = search.predict(X_test_master)\n# y_pred_final = y_pred_final.round()\n# y_pred_final = le.inverse_transform(y_pred_final)\noutput = pd.DataFrame({'id' : test['id'], 'esrb_rating' : y_pred_final})","c38e4db0":"print(output.head())\nprint(output.describe())\noutput.to_csv('svm_submission.csv', index=False)","d87f69f5":"dt = DecisionTreeClassifier(random_state=69, max_depth=5)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\ny_pred = y_pred.round()\nscore = accuracy_score(y_test, y_pred)\nprint(score)","e8a0e4b9":"parameters = {\"min_samples_leaf\" : np.linspace(0.1, 0.5),\n              \"max_depth\" : np.linspace(1, 20), \n              \"max_features\": np.arange(1, 10), \n              \"criterion\" : ['gini', 'entropy']}\nsearch = RandomizedSearchCV(dt, parameters, cv=5)\nsearch.fit(X_train, y_train)\n\nprint(\"Best params:\", search.best_params_)\nprint(\"Best score is:\", search.best_score_)\nprint(accuracy_score(y_test, search.predict(X_test)))\nprint(pd.Series(search.cv_results_[\"mean_test_score\"]).describe())","f8fcd6d1":"import xgboost as XGB\nfrom xgboost import XGBClassifier","b6c7447e":"classif = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=500, use_label_encoder=False,\n                        booster='gbtree', objective='multi:softprob', eval_metric='merror')\nxgb = classif.fit(X_train, y_train)\npred = xgb.predict(X_test)\n\nprint(accuracy_score(y_test, pred))","c173455d":"dtrain = XGB.DMatrix(X, label=y)\nparams = {\"learning_rate\"    : [ 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n          \"max_depth\"        : [ 3, 4, 5 ],\n          \"min_child_weight\" : [ 1, 3, 5, 7 ],\n          \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n          \"objective\" : ['multi:softprob'],\n          \"num_class\" : [4]}\nxgb_model = XGBClassifier(num_boost_round=500, use_label_encoder=False, seed=69, eval_metric='merror')\nsetattr(xgb_model, 'verbosity', 0)\ncvres = GridSearchCV(xgb_model, param_grid=params)\ncvres.fit(X_train, y_train)\nprint(pd.Series(cvres.cv_results_[\"mean_test_score\"]).describe())","7a0db2f5":"pred = cvres.predict(X_test)\nprint(\"Best CV params\", cvres.best_params_)\nprint(\"Best score is:\", cvres.best_score_)\nprint(accuracy_score(y_test, pred))","6fe390f1":"max_depths = np.arange(2, 15)\nscores = []\nfor m in max_depths:\n    xgb_r = XGBClassifier(max_depth=m, n_estimators=500, learning_rate=0.1,\n                          booster='gbtree', objective='multi:softprob', \n                          use_label_encoder=False, eval_metric='merror')\n    xgb_r.fit(X_train, y_train)\n    \n    xgb_pred = xgb_r.predict(X_test)\n    xgb_pred = np.round(xgb_pred)\n    scores.append(accuracy_score(y_test, xgb_pred))\n    \n\nplt.figure(1)\nsorted_i = xgb_r.feature_importances_.argsort()\nplt.barh(X.columns, xgb_r.feature_importances_[sorted_i])\nprint(scores)\nplt.figure(2)\nplt.plot(max_depths, scores)\n","9f0eab03":"xgb = XGBClassifier(max_depth=5, n_estimators=500, learning_rate=0.1,\n                    booster='gbtree', objective='multi:softprob', \n                    use_label_encoder=False, eval_metric='merror')\nxgb.fit(X_train, y_train)","12a79e65":"y_pred_final = xgb.predict(X_test_master)\n# y_pred_final = y_pred_final.round()\n# y_pred_final = le.inverse_transform(y_pred_final)\noutput = pd.DataFrame({'id' : test['id'], 'esrb_rating' : y_pred_final})\n\nprint(output.head())\noutput.to_csv('xgb_submission.csv', index=False)","e514eead":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred)\nprint(score)","448f4d3f":"n_neighbors = np.arange(1, 40)\ntrain_scores = []\ntest_scores = []\n\nfor n in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors = n)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    train_scores.append(score)\n\nplt.plot(n_neighbors, train_scores)\nplt.xlabel('Number of Neighbors')\nprint(max(train_scores))\n","6806d3d2":"params = {'n_neighbors' : n_neighbors, 'p' : np.arange(1, 5)}\nknn_cv = GridSearchCV(KNeighborsClassifier(), params, cv=5)\nknn_cv.fit(X, y)\nprint(knn_cv.best_params_)\nprint(knn_cv.best_score_)\nprint(pd.Series(cvres.cv_results_[\"mean_test_score\"]).describe())","cf8802ab":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE","f41a919c":"y = train['esrb_rating']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\nrf = RandomForestClassifier(random_state=69)\nrf.fit(X_train, y_train)\nscore = accuracy_score(y_test, rf.predict(X_test))\n\nprint(rf.feature_importances_)\n\n# Print accuracy\nprint(\"{0:.1%} accuracy on test set.\".format(score))","e692de8b":"y_pred_final = rf.predict(X_test_master)\n# y_pred_final = y_pred_final.round()\n# y_pred_final = le.inverse_transform(y_pred_final)\noutput = pd.DataFrame({'id' : test['id'], 'esrb_rating' : y_pred_final})\n\nprint(output.head())\noutput.to_csv('rf_submission.csv', index=False)","e13ed28a":"mask = rf.feature_importances_ > 0.02\nreduced_X = X.loc[:, mask]\nprint(reduced_X.columns)","414839bf":"X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(reduced_X, y, test_size=0.2)\n\nrf.fit(X_train_reduced, y_train_reduced)\nscore = accuracy_score(y_test_reduced, rf.predict(X_test_reduced))\n\nprint(\"Test accuracy:\", score)","8ad8d189":"le = LabelEncoder()\ny = le.fit_transform(y.ravel(order=\"c\"))\ny = y.reshape(-1, 1).ravel(order=\"c\")\nX_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(reduced_X, y, test_size=0.2)","5dcc7c55":"classif = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=500, \n                        booster='gbtree', objective='multi:softprob', \n                        eval_metric='merror', use_label_encoder=False)\nxgb = classif.fit(X_train_reduced, y_train_reduced)\npred = xgb.predict(X_test_reduced)\n\nprint(accuracy_score(y_test_reduced, pred))","ff9c84d0":"rfe = RFE(estimator=rf, n_features_to_select=15)\nrfe.fit(X_train, y_train)\nmask = rfe.support_\nreduced_X = X.loc[:, mask]\nreduced_X_test_master = X_test_master.loc[:, mask]\nprint(reduced_X.columns)","8f24b3a7":"svm = SVC(random_state=69)\nparameters = {'kernel': ['rbf', 'poly', 'linear', 'sigmoid'], \\\n              \"gamma\" : np.logspace(-3, 3, 13), \\\n              \"C\": [0.1, 1, 10, 100]}\nsearch = GridSearchCV(svm, parameters, cv=10)\nsearch.fit(X_train_reduced, y_train_reduced)\n# Report the best parameters\nprint(\"Best CV params\", search.best_params_)\nprint(\"Best score is:\", search.best_score_)\n\ny_pred_reduced = search.predict(X_test_reduced)\ny_pred_reduced = y_pred_reduced.round()\nprint(\"Test accuracy:\", accuracy_score(y_test_reduced, y_pred_reduced))","63bf6207":"# I've recopied the X and y features so that I didn't have to scroll all the way up. That's all this really is.\nX = train.drop(['esrb_rating', 'id', 'title', 'console'], axis=1)\nX_test_master = test.drop(['id', 'console'], axis=1)\ny = train['esrb_rating']","04f4e219":"svm = SVC(probability=True, random_state=69)\nparameters = { 'kernel': ['rbf'],\n               \"gamma\" : [1 \/ np.sqrt(10)],\n               \"C\": np.linspace(0.1, 2, 20) }\nsearch = GridSearchCV(svm, parameters, cv=10, scoring='accuracy', n_jobs=-1)","bc0a1011":"models = {}\n\ndef searchsvm(search, X, y, rating, eval_):\n    search.fit(X, y == rating)\n    print(search.best_params_, search.best_score_)\n    return search.predict_proba(eval_)\n\nmodels['E']  = searchsvm(search, X, y, 'E', X_test_master)\nmodels['ET'] = searchsvm(search, X, y, 'ET', X_test_master)\nmodels['T']  = searchsvm(search, X, y, 'T', X_test_master)\nmodels['M']  = searchsvm(search, X, y, 'M', X_test_master)","1c830ace":"reduced_X_test = X_test_master.loc[:, mask]\nreduced_models = {}\nreduced_models['E']  = searchsvm(search, reduced_X, y, 'E', reduced_X_test)\nreduced_models['ET'] = searchsvm(search, reduced_X, y, 'ET', reduced_X_test)\nreduced_models['T']  = searchsvm(search, reduced_X, y, 'T', reduced_X_test)\nreduced_models['M']  = searchsvm(search, reduced_X, y, 'M', reduced_X_test)","73c647fd":"def compareprobs(models, eval_):\n    # Go through and find if one has a higher probability of being correct than the other.\n    y_proba = []\n    keys = list(models.keys())\n    \n    for col in range(len(eval_)):\n        scores = { key : models[key][col, 1] for key in keys }\n        best_choice = keys[0]\n        for key in keys:\n            if scores[key] > scores[best_choice]:\n                best_choice = key\n        y_proba.append(best_choice)\n\n    return pd.Series(y_proba)","2e4d49c5":"y_proba_final = compareprobs(models, X_test_master)\nprint(y_proba_final.describe())\ny_proba_reduced = compareprobs(reduced_models, reduced_X_test)\nprint(y_proba_reduced.describe())","bad840f9":"output = pd.DataFrame({'id' : test['id'], 'esrb_rating' : y_proba_final})\n\nprint(output)\noutput.to_csv('svm_proba_submission.csv', index=False)","09835e91":"from IPython.display import FileLink\nFileLink(r'svm_proba_submission.csv')","8f21443d":"# Working on SVC Through Linear Classifiers\n## A less naive approach\n---\n\nSo I thought about it and kind of realized that this is sort of the same idea as the last assignment, where it makes more sense to split up the data and get a more accurate result of one thing, rather than blindly guessing at multiple things. After running the model above with np.logspace, I can see that the best gamma is about $\\frac{1}{\\sqrt{10}}$, and the best kernel seems to be rbf so I'm just going to start substituting those from now on.","ebe01ebe":"For whatever reason, even with the Grid Search, pulling out the variables and adding some other tweaks still manages to make a better model. However, we know we can do better than this.","f508b579":"\"Strong language\" was misspelled. Not important, but I fixed it anyways.","ae414924":"Models that we should be using:\n* Logistic Regression\n* Support Vector Machine\n* Decision Tree\n* Random Forest\n* K Nearest Neighbors\n* XGBoost\n\nI don't think in this instance the Logistic Regression or the KNN models will do me much good. But we'll try them anyways: I really think the only ones that will compete are the SVM, due to the nature of the problem (many columns of binary statistics), and maybe decision trees, but that's just judging by the fact that we were recommended to use XGBoost.","b0fd4e6a":"Sorry about the error messages. I did some research and it has something to do with scikit-learn and the way it handles string decoding being no longer relevant; so, not my code. I'm unsure of how to mute the error messages. \n\n[To take a look at the error, you can see a similar issue here](https:\/\/github.com\/scikit-optimize\/scikit-optimize\/issues\/1085)\n\n## Logistic Regression\n\nThe easiest way to deal with this is by doing some label encoding and making each category a number, rather than a letter. \nI've done this above, to use with LogisticRegression and a few other models, but it is really only necessary with the Logistic Regression model.","d6c57e9d":"I'm really happy with this plot. It goes through and looks at the frequencies of all of the dimensions and compares them to the esrb ratings. From this plot we can see that there's no real difference between whether something is on a specific console and its rating, there's basically a uniform frequency. The rest of them have some sort of trend that is obvious from the bar plots.","0a760e89":"## Random Forest","ad0d3afc":"The Random Forest model does very well, almost as well as the SVM. But ultimately it's not something that I'm going to mess around with more if I can get better results with the SVM.","00293c3e":"### Naive Solution","72546874":"## K Neighbors Classifier\nThe KNN model is not optimal, and doesn't really need to be looked at in much more depth than this. After looking at about 40 different neighbors, the results don't get better than about 85%.","84af588a":"## Support Vector Machines\n","4ae3af8f":"After looking at the least important classifiers, I'll try once again to build a support vector machine to find the best hyperparameters. The C-space seems to be the one that is the most important for nuance, depending on the model you're fitting, because the other hyperparameters (gamma, kernel) have one \"right\" answer when you run it.","9ba78c1f":"# Exploratory Data Analysis\n---\n## Importing the Data","a2074811":"# Assignment 2\n---\n#### Alex Cingoranelli\n#### CAP 4020\n","7885e7cb":"## Decision Trees","b047b09a":"## XGBoost\n","cc3600d5":"Surprisingly, one of the runs in the reduced X did better than the one with all of the features. If I had more time, I'd compare the two of them after running them like I did below, and find the mode of the two and submit that instead."}}