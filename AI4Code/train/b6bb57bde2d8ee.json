{"cell_type":{"4c2363ba":"code","598c51d2":"code","40fed7ef":"code","10a2c1c3":"code","788e1ff8":"code","68e3cbc3":"code","290dbcec":"code","dd7a316c":"code","55390814":"code","9dec7879":"code","03e95823":"code","45fd2081":"code","0ff63d41":"code","dbeac72b":"code","a54ebfac":"code","fd8d0892":"code","66b73e82":"code","455c086a":"code","49d188b9":"code","b524cc88":"code","c7f2ce3e":"code","9bbcdb77":"code","a70f99a3":"code","701542f8":"code","0345ffee":"code","f364826c":"code","f806e5b8":"code","9ba62ca9":"code","1da2136b":"code","0ff41e91":"code","bfea69ad":"code","fe1ddd38":"code","fed27c2a":"code","636c42f3":"code","06072968":"code","b43e2c9b":"code","c0d42d66":"code","5fd75330":"code","2efa50ea":"code","e029fc3e":"code","2548204e":"code","1b8ea95a":"code","76b7f9af":"code","8c36f5ff":"code","31392801":"markdown","a9c78521":"markdown","0152de30":"markdown","87c2c8df":"markdown","e28f5c2b":"markdown","822c9d52":"markdown","edf9ca8c":"markdown","896a8de0":"markdown","8e74c618":"markdown","030d5f49":"markdown","93af4862":"markdown","631a1801":"markdown","cb426203":"markdown","16b203a7":"markdown","e751723a":"markdown","d9f76ffc":"markdown","36c03b5d":"markdown","b830f035":"markdown"},"source":{"4c2363ba":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","598c51d2":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_train.head(5)","40fed7ef":"df_train.shape","10a2c1c3":"y = df_train['label']","788e1ff8":"df = df_train.drop(['label'], axis=1)\ndf.head()","68e3cbc3":"# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n","290dbcec":"digit_to_predict_raw = np.array(df.iloc[5000,:])\ndigit_to_predict  = np.array(digit_to_predict_raw).reshape(28,28)\ndigit_to_predict.shape","dd7a316c":"plt.imshow(digit_to_predict,cmap = matplotlib.cm.binary)\nplt.show()","55390814":"y[5000]","9dec7879":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.30, random_state=42)","03e95823":"y_train_8 = np.array(y_train == 8)\ny_test_8 = np.array(y_test == 8)","45fd2081":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(max_iter=5, random_state=42)\nsgd_clf.fit(X_train, y_train_8)","0ff63d41":"pred = sgd_clf.predict(X_test)\npred","dbeac72b":"y_test[:4]","a54ebfac":"from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_8, cv=5, scoring=\"accuracy\")","fd8d0892":"import collections\ncollections.Counter(y_train_8)","66b73e82":"num_of_8_not_occur = collections.Counter(y_train_8)[0]\nprint(\"The accuracy of model if we predict there are NO 8 present in the dataset :\",\n      num_of_8_not_occur\/len(y_train_8))","455c086a":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\ny_train_8_pred = cross_val_predict(sgd_clf, X_train, y_train_8, cv=3)\nconfusion_matrix(y_train_8, y_train_8_pred)\n","49d188b9":"from sklearn import metrics\ndef get_metrics(true_labels, predicted_labels):\n    \n    print('Accuracy:', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print('Precision:', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'),\n                        4))\n    print('Recall:', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'),\n                        4))\n    print('F1 Score:', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'),\n                        4))","b524cc88":"get_metrics(y_train_8, y_train_8_pred)","c7f2ce3e":"def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) ","9bbcdb77":"display_confusion_matrix(y_train_8, y_train_8_pred)","a70f99a3":"y_scores = cross_val_predict(sgd_clf, X_train, y_train_8, cv=3,\n                             method=\"decision_function\")","701542f8":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_8, y_scores)","0345ffee":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.xlabel(\"Threshold\", fontsize=16)\n    plt.legend(loc=\"upper left\", fontsize=16)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(8, 4))\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.xlim([-1700000, 1700000])\nplt.show()","f364826c":"y_train_prec_90 = (y_scores > 700000)","f806e5b8":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_8, y_train_prec_90)","9ba62ca9":"recall_score(y_train_8, y_train_prec_90)","1da2136b":"from sklearn.metrics import roc_auc_score, roc_curve\nfpr, tpr, thresholds = roc_curve(y_train_8, y_scores)","0ff41e91":"def plot_roc_curve(fpr,tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label = label)\n    plt.plot([0,1], [0,1],'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    \nplot_roc_curve(fpr,tpr)\n\nprint(\"The AUC score is :\", roc_auc_score(y_train_8, y_scores))","bfea69ad":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(n_jobs=-1)\ny_forest_pred = cross_val_predict(forest_clf, X_train, y_train_8, cv=3, method='predict_proba')\ny_forest_pred_f = cross_val_predict(forest_clf, X_train, y_train_8, cv=3)","fe1ddd38":"y_scores_forest= y_forest_pred[:,1]\nfpr_f, tpr_f, thresholds_f = roc_curve(y_train_8, y_scores_forest)","fed27c2a":"plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\nplot_roc_curve(fpr_f, tpr_f, \"Random Forest\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Classification metrics for SGD :\")\nprint(\"The AUC score for SGD is :\", roc_auc_score(y_train_8, y_scores))\nget_metrics(y_train_8, y_train_8_pred)\nprint(\"\\nClassification metrics for RandomForest :\")\nprint(\"The AUC score is RandomForest is :\", roc_auc_score(y_train_8, y_scores_forest))\nget_metrics(y_train_8, y_forest_pred_f)","636c42f3":"sgd_multi_clf = SGDClassifier(max_iter=5, random_state=42)\nsgd_multi_clf.fit(X_train, y_train)  \n## we'll be using y_train here as scikit learn trains the model against actual \n## target labels 0 to 9 (10 classifiers each for 1 class); it'll compare 10 \n## classification scores and choose the class having the highest score \n## for a new data point.\nsgd_multi_clf.predict([digit_to_predict_raw])","06072968":"## So, here we can see the scores against all the 10 classifiers\ndigit_to_predict_scores = sgd_multi_clf.decision_function([digit_to_predict_raw])\nmax_score_index = np.argmax(digit_to_predict_scores)  ## index of the maximum score\nprint(\"Scores of 10 classifiers :\", digit_to_predict_scores)\n## We'll find out the class corresponding to the index value of the mximum score\nprint(\"Maximum score among all these Scores is for Class : {} having Score of : {}\" \n      .format(sgd_multi_clf.classes_[max_score_index], digit_to_predict_scores[0][8])) ","b43e2c9b":"cross_val_score(sgd_multi_clf, X_train, y_train, cv=3, scoring=\"accuracy\")","c0d42d66":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\ncross_val_score(sgd_multi_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")","5fd75330":"from sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\novo_clf = OneVsOneClassifier(sgd_multi_clf)\novo_clf.fit(X_train_scaled, y_train)\novo_clf.predict([digit_to_predict_raw])\n","2efa50ea":"len(ovo_clf.estimators_)","e029fc3e":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(n_jobs=-1)\nforest_clf.fit(X_train, y_train)\nforest_clf.predict([digit_to_predict_raw])","2548204e":"cross_val_score(forest_clf, X_train, y_train, cv=3, scoring=\"accuracy\")","1b8ea95a":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\ny_train_pred_svm = cross_val_predict(sgd_multi_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred_svm)\nplt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()\n\n## most images are on the main diagonal, which means that\n## they were classified correctly","76b7f9af":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_jobs=-1, weights = 'distance', n_neighbors=4)\nknn_clf.fit(X_train, y_train)\nknn_clf.predict([digit_to_predict_raw])","8c36f5ff":"cross_val_score(knn_clf, X_train, y_train, cv=3, scoring=\"accuracy\")","31392801":"## ROC curve for SGDClassifier():","a9c78521":"Thus, we have achieved a precision of about 90% but at a recall of 16%.","0152de30":"Unlike binary classifiers, multi class classifiers can distinguish between more than two classes.\nAlgorithms like Random forest, Naive Bayes are capable of handling multiple classes directly, whereas Support Vector Machines are Binary Classifiers. \nWe can perform Multi-class classification using Binary Classifiers using various strategies:\n\n**1.  One-Vs-All (OvA)**\n    Split the training set into three separate binary classification problems\n    i.e. create a new fake training set\n    *     Triangle (1) vs crosses and squares (0)   h\u03b81(x) = P(y=1 | x1; \u03b8)\n    *     Crosses (1) vs triangle and square (0)    h\u03b82(x) = P(y=1 | x2; \u03b8)\n    *     Square (1) vs crosses and square (0)      h\u03b83(x) = P(y=1 | x3; \u03b8)\n           \n    On a new input, x to make a prediction, pick the class i that maximizes the probability that h\u03b8(i)(x) = 1    \n       \n           \n![](https:\/\/1.bp.blogspot.com\/-cEz0wkXUPmA\/WLJq3e6CDAI\/AAAAAAAAFX4\/Wramg-8qC_QH13K0qosnPAbvccXdbBU-ACLcB\/s1600\/VQvgVUs%255B1%255D.png)       \n\n\n\n**2. One-versus-One (OvO)**\nWe'll train a Binary classifier for every pair of classes - one to distinguish between 0's and 1's, another for 0's and 2's and so on.\nSo, if we have N classes, we'll have to train Nx(N-1)\/2 classifiers.\n\nAdvantage of OvO: It needs to be trained on the part of the training set for the two classes. so, we will be training many classifiers on small training sets.\n\nSome algorithms like Support Vector Machines scale very poorly with the size of the training set, so for such algorithms, we'll prefer OvO (faster to train many classifiers on small training sets).","87c2c8df":"We can very well see that if we predict that there are no \"8\" present in the dataset, our model will give an accuracy of around 90%. So, clearly accuracy is not the best evaluation metric for evaluating such imbalanced dataset.","e28f5c2b":"For OvO classifier, Nx(N-1)\/2 classifiers are generated as seen from the number of estimators below - where N is the number of classes.","822c9d52":"We can see AUC for Random forest classifier is much better than SGD classifier. \nPrecision and recall is also coming to be around 96% which is much better than SGD.","edf9ca8c":"Here, KNN is outperforming other algorithms like RandomForest, SGD classifier.","896a8de0":"The label of the training data matches with that of the image which we got above.","8e74c618":"## Evaluating SGD Classifier\nWe can seeaccuracy of SGD classifier has improved from 80% to 90% simply by scaling the input data.","030d5f49":"So, let's say we would like to have 90% precision, from the above plot, for a 90% precision, we need to take a threshold of around 700000.","93af4862":"This notebook is for beginners approach to proceed with MNIST Digit Recognizer dataset. \nI have tried to cover topics like how to proceed with a **Binary Classification** problem as well as **Multi Class Classification**.\nMulti-Class classification strategies like **OnevsOne** and **OnevsAll** classifier has also been discussed.\n\nAny suggestions for improvement or comments are highly appreciated! ","631a1801":"Confusion Matrix:","cb426203":"## Evaluation metrics for Multi-class classifier","16b203a7":"Here, each of the image (each row of the dataset) are having 784 features as each of these images are having 28 * 28 pixels. So we need to reshape the array for 5000 row from the dataset and plot it using imshow().","e751723a":"Let's check the number of True\/False occurences in `y_train_8` array.","d9f76ffc":"## **Using RandomForest classifier**","36c03b5d":"Except for SVM classifiers (runs on OvO strategy), scikit-learn automatically runs OvA strategy for Multi-Class classification. \nSo, let's try SGD classifier:","b830f035":"## Multi-Class classification"}}