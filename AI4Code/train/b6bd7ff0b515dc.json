{"cell_type":{"1fdbe330":"code","d707b3b2":"code","872f411c":"code","3933114c":"code","722b2b0e":"code","dd9e190c":"code","1bad464b":"code","44036e87":"code","fad74126":"code","7a6c3745":"code","6ec8fb19":"code","95ffac64":"code","abc43b5f":"code","3a1894e0":"markdown","d59f7dce":"markdown","fe23af3a":"markdown","63a6998e":"markdown","e49cca35":"markdown","34960f24":"markdown","56524f7b":"markdown","6e26d10a":"markdown","ef31a636":"markdown","ff80b0cf":"markdown","6a767849":"markdown"},"source":{"1fdbe330":"import numpy as np\nimport pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split ","d707b3b2":"# test data\ntmpX1 = np.array([int(i.strip()) for i in \"2   0   0   0   1   2   3   1  0   0   1   0   2   1   0   0  0   1   0   1   0   2   1   0  1   0   0   2   0   1   0   1  2   0   0   0   1   0   1   3  0   0   1   2   0   0   2   1\".split(\"  \")])\ntmpX2 = np.array([int(i.strip()) for i in \"0   1   1   0   0   0   1   0  1   2   0   1   0   0   1   1  0   1   1   0   0   2   0   0  0   0   0   0   0   0   0   0  0   0   1   0   1   0   1   0\".split(\"  \")])\nX = np.concatenate((tmpX1.reshape(-1,8), tmpX2.reshape(-1,8)), axis=0)\ny = np.array([0,0,0,0,0,0,1,1,1,1,1])\nX_test = np.array([[2,1,0,0,1,2,0,1],[0,1,1,0,1,0,1,0]])\ny_test = np.array([0,1])\nprint(\"X and Y shapes\\n\", X.shape, y.shape)","872f411c":"class MultiNB:\n    def __init__(self,alpha=1):\n        self.alpha = alpha\n    \n    def _prior(self): # CHECKED\n        \"\"\"\n        Calculates prior for each unique class in y. P(y)\n        \"\"\"\n        P = np.zeros((self.n_classes_))\n        _, self.dist = np.unique(self.y,return_counts=True)\n        for i in range(self.classes_.shape[0]):\n            P[i] = self.dist[i] \/ self.n_samples\n        return P\n            \n    def fit(self, X, y): # CHECKED, matches with sklearn\n        \"\"\"\n        Calculates the following things- \n            class_priors_ is list of priors for each y.\n            N_yi: 2D array. Contains for each class in y, the number of time each feature i appears under y.\n            N_y: 1D array. Contains for each class in y, the number of all features appear under y.\n            \n        params\n        ------\n        X: 2D array. shape(n_samples, n_features)\n            Multinomial data\n        y: 1D array. shape(n_samples,). Labels must be encoded to integers.\n        \"\"\"\n        self.y = y\n        self.n_samples, self.n_features = X.shape\n        self.classes_ = np.unique(y)\n        self.n_classes_ = self.classes_.shape[0]\n        self.class_priors_ = self._prior()\n        \n        # distinct values in each features\n        self.uniques = []\n        for i in range(self.n_features):\n            tmp = np.unique(X[:,i])\n            self.uniques.append( tmp )\n            \n        self.N_yi = np.zeros((self.n_classes_, self.n_features)) # feature count\n        self.N_y = np.zeros((self.n_classes_)) # total count \n        for i in self.classes_: # x axis\n            indices = np.argwhere(self.y==i).flatten()\n            columnwise_sum = []\n            for j in range(self.n_features): # y axis\n                columnwise_sum.append(np.sum(X[indices,j]))\n                \n            self.N_yi[i] = columnwise_sum # 2d\n            self.N_y[i] = np.sum(columnwise_sum) # 1d\n            \n    def _theta(self, x_i, i, h):\n        \"\"\"\n        Calculates theta_yi. aka P(xi | y) using eqn(1) in the notebook.\n        \n        params\n        ------\n        x_i: int. \n            feature x_i\n            \n        i: int.\n            feature index. \n            \n        h: int or string.\n            a class in y\n        \n        returns\n        -------\n        theta_yi: P(xi | y)\n        \"\"\"\n        \n        Nyi = self.N_yi[h,i]\n        Ny  = self.N_y[h]\n        \n        numerator = Nyi + self.alpha\n        denominator = Ny + (self.alpha * self.n_features)\n        \n        return  (numerator \/ denominator)**x_i\n    \n    def _likelyhood(self, x, h):\n        \"\"\"\n        Calculates P(E|H) = P(E1|H) * P(E2|H) .. * P(En|H).\n        \n        params\n        ------\n        x: array. shape(n_features,)\n            a row of data.\n        h: int. \n            a class in y\n        \"\"\"\n        tmp = []\n        for i in range(x.shape[0]):\n            tmp.append(self._theta(x[i], i,h))\n        \n        return np.prod(tmp)\n    \n    def predict(self, X):\n        samples, features = X.shape\n        self.predict_proba = np.zeros((samples,self.n_classes_))\n        \n        for i in range(X.shape[0]):\n            joint_likelyhood = np.zeros((self.n_classes_))\n            \n            for h in range(self.n_classes_):\n                joint_likelyhood[h]  = self.class_priors_[h] * self._likelyhood(X[i],h) # P(y) P(X|y) \n                \n            denominator = np.sum(joint_likelyhood)\n            \n            for h in range(self.n_classes_):\n                numerator = joint_likelyhood[h]\n                self.predict_proba[i,h] = (numerator \/ denominator)\n            \n        indices = np.argmax(self.predict_proba,axis=1)\n        return self.classes_[indices]","3933114c":"def pipeline(X,y,X_test, y_test, alpha):\n    \"\"\"\n    Sklearn Sanity Check\n    \"\"\"\n    print(\"-\"*20,'Sklearn',\"-\"*20)\n    clf = MultinomialNB(alpha=alpha)\n    clf.fit(X,y)\n    sk_y = clf.predict(X_test)\n    print(\"Feature Count \\n\",clf.feature_count_)\n    print(\"Class Log Prior \",clf.class_log_prior_)\n    print('Accuracy ',accuracy_score(y_test, sk_y),sk_y)\n    print(clf.predict_proba(X_test))\n    print(\"-\"*20,'Custom',\"-\"*20)\n    nb = MultiNB(alpha=alpha)\n    nb.fit(X,y)\n    yhat = nb.predict(X_test)\n    me_score = accuracy_score(y_test, yhat)\n    print(\"Feature Count\\n\",nb.N_yi)\n    print(\"Class Log Prior \",np.log(nb.class_priors_))\n    print('Accuracy ',me_score,yhat)\n    print(nb.predict_proba) # my predict proba is only for last test set\n","722b2b0e":"pipeline(X,y,X,y, alpha=10)","dd9e190c":"from nltk.corpus import stopwords\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer","1bad464b":"df = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='iso8859_14')\ndf.drop(labels=df.columns[2:],axis=1,inplace=True)\ndf.columns=['target','text']","44036e87":"def clean_util(text):\n    punc_rmv = [char for char in text if char not in string.punctuation]\n    punc_rmv = \"\".join(punc_rmv)\n    stopword_rmv = [w.strip().lower() for w in punc_rmv.split() if w.strip().lower() not in stopwords.words('english')]\n    \n    return \" \".join(stopword_rmv)","fad74126":"df['text'] = df['text'].apply(clean_util)","7a6c3745":"cv = CountVectorizer()\nX = cv.fit_transform(df['text']).toarray()\nlb = LabelBinarizer()\ny = lb.fit_transform(df['target']).ravel()\nprint(X.shape,y.shape)","6ec8fb19":"# Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X,y)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","95ffac64":"sk = MultinomialNB().fit(X_train,y_train)\nsk.score(X_test,y_test)","abc43b5f":"%%time\nme = MultiNB()\nme.fit(X_train, y_train)\nyhat = me.predict(X_test)\nprint(accuracy_score(y_test,yhat))","3a1894e0":"## Simple Preprocessing\nto cleanup punctuations and stopwords","d59f7dce":"It takes a lot of time but does not matter as it is a reference implementation only \u30fd(\uff40\u0414\u00b4)\uff89","fe23af3a":"**I wrote the scratch implementation for my learning, if you see any error or typo, please let me know.**","63a6998e":"Multinomial naive bayes is the naive Bayes algorithm for multinomially distributed data. For a brief and intuitive explanation of Bayes theorem, read this kernel of mine: [Gaussian Naive Bayes Classifier from Scratch](https:\/\/www.kaggle.com\/riyadhrazzaq\/gaussian-naive-bayes-classifier). Everything is similar to Gaussian NB except the $P(x_i \\mid y)$. The new equation is, \n$$\nP(x_i \\mid y) = \\frac{N_{yi} + \\alpha}{N_y + \\alpha n} \\label{eq1}\\tag{1}\n$$ \nHere, \n* $\\alpha$ is the smoothing parameter, \n* $N_{yi}$ is the count of feature $x_i$ in class y.\n* $N_y$ is the total count of all features in class y\n* $n$ is the total number of features","e49cca35":"# Spam Classification","34960f24":"# Multinomial Naive Bayes\nYou can look up in detail about multinomial distribution and you should. I will only put a short description of how a multinomial naive bayes classifier considers data. \n## Multinomial Data\n|   $X_1$|$X_2$|$X_3$|\n|---|---|---|\n|1|0|4|\n|4|2|3|\n\nIn the table above containing 2 sample of 3 features, we observe that feature $X_1$ has values 1 and 4, and so on. That is the common view of the data. And when other a general model accepts this data, it considers each number as value. For example, $X_{1,2}=3$. But in case of reading a multinomial data, $X_{1,2}$ says how many of feature $X_{2}$ is in sample 1. Meaning $X_{1,2}$ is not value of the feature, instead it is the count of the feature. Let's consider a text corpus. Each sentence is made up of different words $w_i$ and each of those $w_i$ belongs to the vocabulary, $V$. If $V$ contains 8 words, $w_1,w_2,...,w_8$ and if a sentence is: w1 w2 w2 w6 w3 w2 w8, the representation of that sentence will be- \n\n|$w_1$|$w_2$|$w_3$|$w_4$|$w_5$|$w_6$|$w_7$|$w_8$|\n|---|---|---|---|---|---|---|---|\n| 1|3 |1 | 0| 0|1 | 0|1 |\n\nAfter inserting some other random sentences, the dataset is-\n\n|$w_1$|$w_2$|$w_3$|$w_4$|$w_5$|$w_6$|$w_7$|$w_8$|\n|---|---|---|---|---|---|---|---|\n| 1|3 |1 | 0| 0|1 | 0|1 |\n| 1|0 |0 | 0| 1|1 | 1|3 |\n| 0|0 |0 | 0| 0|2 | 1|2 |\n\nBy the way, I haven't put them in a class. Randomly taking, $y$ = [1,0,1]. Now, comparing with the equation of above,\n\n* $N_{yi}$ is the count of feature $w_i$ in each unique class of y. For example, for $y=1$, \\\n$N_{y,1}=1, N_{y,6}=3$\n* $N_y$ is the total count of all features in each unique class of y. For example, for $y=1$, \\\n$N_y=12$\n* $n=8$ is the total number of features\n* $\\alpha$ is known as smoothing parameter. It is needed for zero probability problem which is explained in resource [1]\n\nTo calculate likelyhoods for a test sentence, all we need is $P(w_i \\mid y)$ which will be used to calculate $P(X \\mid y)$ from training data. But $P(w_i \\mid y)$ is the probability of feature $w_i$ appearing under class y once. If our test sentence has any feature $w_i$ n times, we will need to include $P(w_i \\mid y)$ in $P(X \\mid y)$ n times too. So, final equation for $P(X_i \\mid y)$ will be-\n$$\nP(X_i \\mid y) = P(w_1 \\mid y)^{X_{i,1}} \\times P(w_2 \\mid y)^{X_{i,2}} \\times ... \\times P(w_n \\mid y)^{X_{i,n}}\n$$\n\nResources:\n1. https:\/\/www.inf.ed.ac.uk\/teaching\/courses\/inf2b\/learnnotes\/inf2b-learn07-notes-nup.pdf\n2. https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html#multinomial-naive-bayes","56524f7b":"# Class MultiNB","6e26d10a":"# Multinomial Naive Bayes from scratch","ef31a636":"sklearn's `MultinomialNB`","ff80b0cf":"our `MultiNB` (\u2310\u25a0_\u25a0)","6a767849":"# Vectorizing\nConforming the texts to the multinomial format we have discussed in the beginning. Also, classes in y must be converted to integers as I forgot to account for strings in my implementation and too lazy to update  \u2022\u0361\u02d8\u31c1\u2022\u0361\u02d8 "}}