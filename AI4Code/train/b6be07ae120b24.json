{"cell_type":{"06f10153":"code","442e9021":"code","03f85c71":"code","adf93ed4":"code","73a5e815":"code","4860b9bb":"code","90d52ebf":"code","3d2e679f":"code","ab80364e":"code","e809f6f9":"code","5d4ce218":"code","d86e431d":"code","2d9f4459":"code","f971f4b1":"code","3aa9a466":"code","9d90552c":"code","80825255":"code","0e5a8645":"code","e450a1c3":"code","7cc1c8e5":"code","657f0aee":"code","7bfc0406":"code","039dbf7a":"code","6d5c6c67":"code","19f79a83":"code","10e1afe4":"code","d96940f0":"code","dc12f15a":"code","5924e5d7":"code","cd6b1af1":"code","ed838425":"code","db080058":"code","0c6d0ed0":"code","67582208":"code","8d3ca337":"code","34bafa09":"code","92e97831":"code","d44f4672":"code","4ffa11b3":"code","d1226c91":"code","29f75b05":"code","c748b27e":"code","4abcd444":"code","496806d2":"code","a76f605e":"code","78481ba1":"code","84399513":"code","8ea69373":"code","0e94c552":"code","d434c727":"code","6dc0900d":"code","b68e132e":"code","fa382160":"code","ea0665f2":"code","a2673d33":"code","ce64d9a2":"code","1011362d":"code","ffec9f47":"code","00508e4a":"code","8934390b":"code","7c1ddc35":"code","29348407":"code","7b8b2ed0":"code","3177d7e7":"code","6cfab9bc":"code","b3c334f7":"code","91f51592":"code","1b9e644b":"code","37f50396":"code","889209d9":"code","9de651b0":"code","590c8ef4":"code","65832173":"code","12325304":"code","fc1e7db0":"code","1fd625fc":"code","6c5f51a1":"code","179e78cf":"code","0746dafe":"code","b65450db":"code","47f5f1bf":"code","f7a24cec":"code","813aadfd":"code","42f9139c":"code","0e0e4110":"code","42bff1d8":"code","96f3c872":"code","b56e3976":"code","48a45840":"code","76ca3566":"code","a00d0bc6":"code","c08749c2":"code","1cdfa2af":"code","fa45d259":"code","41aa8582":"code","334c2c7b":"code","498b5751":"code","49bb1eb6":"code","999c8502":"code","10139246":"code","a1b1e4f4":"code","c01af1c8":"code","b89d4d04":"code","7a5149ae":"code","ceeb6d9d":"code","bdf61bae":"code","8eb3796f":"code","9b8fa463":"markdown","3250c674":"markdown","c0ef8e59":"markdown","5eaafa4a":"markdown","985605f7":"markdown","03d41832":"markdown","7d4681da":"markdown","0a33561b":"markdown","d444d029":"markdown","25ca0dce":"markdown","f54e8d1f":"markdown","8c275ed0":"markdown","4109d34f":"markdown","44f28edd":"markdown","ef9d029f":"markdown","e49c46a8":"markdown"},"source":{"06f10153":"import numpy as np                   # For creating matrices and for number operations\nimport pandas as pd                  # For manipulating and reading data\nimport matplotlib.pyplot as plt      # for ploting graphs\nimport seaborn as sns                # To plot heatmaps\nimport warnings                      # Hide warnings\nwarnings.filterwarnings('ignore')","442e9021":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.head()","03f85c71":"df.info()","adf93ed4":"plt.figure(figsize = (10 , 5))\nsns.heatmap(df.corr() , annot = True , cmap = \"coolwarm\" , )   # Heatmap of correlations accross columns","73a5e815":"plt.figure(figsize = (15 , 8))\nsns.heatmap(df.isnull() , cmap = \"coolwarm\" , yticklabels = False)","4860b9bb":"sns.set_style(\"darkgrid\")\nsns.countplot(data = df , x = \"Survived\")  # Value_counts of target column","90d52ebf":"df[\"First\"] = df[\"Name\"].apply(lambda x : x.split(\",\")[1].split(\".\")[0])","3d2e679f":"df.head()","ab80364e":"df[\"First\"].dtype","e809f6f9":"df.info()","5d4ce218":"df[\"First\"].value_counts()","d86e431d":"rest = [\" Dr\" , \" Rev\" , \" Major\" , \" Mlle\" , \" Col\" , \" Don\" , \" Lady\" , \" Sir\" , \" Capt\" , \" Ms\" , \" Jonkheer\" , \" Mme\" , \" the Countess\"]","2d9f4459":"df[\"First\"].isna().sum()","f971f4b1":"df[\"First\"].loc[df[\"First\"].isin(rest)] = \"rest\"","3aa9a466":"df[\"First\"][3]","9d90552c":"df[\"First\"].value_counts()","80825255":"df.isna().sum() , len(df)  # Checking for null values","0e5a8645":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")    # Loading test dataset\ntest.head()","e450a1c3":"test.info()","7cc1c8e5":"test.isna().sum() , len(test)  # Checking for null values","657f0aee":"test[\"First\"] = test[\"Name\"].apply(lambda x : x.split(\",\")[1].split(\".\")[0])","7bfc0406":"test[\"First\"].loc[test[\"First\"].isin(rest)] = \"rest\"","039dbf7a":"test[\"First\"].value_counts()","6d5c6c67":"test[\"First\"].loc[test[\"First\"] == \" Dona\"] = \"rest\"\ntest[\"First\"].value_counts()","19f79a83":"# Droping Cabin columns as it has more percentage of missing values\n\ndf.drop([\"Cabin\"] , axis = 1 , inplace = True)        \ntest.drop([\"Cabin\"] , axis = 1 , inplace = True)","10e1afe4":"df.info()","d96940f0":"# Droping columns with unique values as they are not useful\n\ndf.drop([\"PassengerId\" , \"Name\"] , axis = 1 , inplace = True)\nids = test[\"PassengerId\"]\ntest.drop([\"PassengerId\" , \"Name\"] , axis = 1 , inplace = True)","dc12f15a":"# Getting all the categorical columns\n\ncats = df.select_dtypes(include=\"object\").columns\ncats","5924e5d7":"# Getting all Integer columns\n\nints = df.select_dtypes(exclude=\"object\").columns\nints","cd6b1af1":"for i in df.columns:\n    print(f\"The number of unique values in {i} column is\/are {len(df[i].unique())}\")\n    print(\"\\n\")\n    print(f\"The unique values in {i} column is\/are {df[i].unique()}\")\n    print(\"\\n\")\n    print(f\"The value counts for each value in {i} column is\/are :  \\n{df[i].value_counts()}\")\n    print(\"\\n\\n\")\n    print(\"*\"*100)\n    print(\"\\n\\n\")","ed838425":"# Drpoing ticket column as it has more unique values\n\ndf.drop([\"Ticket\"] , axis = 1 , inplace = True)\ntest.drop([\"Ticket\"] , axis = 1 , inplace = True)","db080058":"df.isna().sum()","0c6d0ed0":"test.isna().sum()","67582208":"sns.countplot(data = df , x =  \"Embarked\")","8d3ca337":"# Filling Embarked missing values with the mode.\n\ndf[\"Embarked\"].fillna(\"S\" , inplace = True)\nsns.countplot(data = df , x =  \"Embarked\")","34bafa09":"# Filling missing intergeral values with the median as mean encounters with outliners\n\ndf[\"Age\"].fillna(df[\"Age\"].median() , inplace = True)\ntest[\"Age\"].fillna(df[\"Age\"].median() , inplace = True)\ntest[\"Fare\"].fillna(df[\"Fare\"].median() , inplace = True)","92e97831":"df.isna().sum()","d44f4672":"test.isna().sum()","4ffa11b3":"df.head()","d1226c91":"test.head()","29f75b05":"sns.set_style(\"darkgrid\")","c748b27e":"plt.figure(figsize = (10 , 10))\nsns.pairplot(data = df)","4abcd444":"# Defining Group by survival function for better visualization\n\ndef grouping(x , hue = \"Age\"):\n    res = df.groupby(df[x]).mean()\n    res = res.reset_index()\n    print(f\"Grouping by {x} with DataFrame : \\n \")\n    print(res)\n    print(\"\\n\")\n    if x in [\"Pclass\" , \"SibSp\" , \"Parch\"]:\n        plt.figure(figsize = (12 , 5))\n        sns.barplot(data = res , x = x , y = \"Survived\" , hue = hue)\n        plt.show()\n    else:\n        plt.figure(figsize = (12 , 5))\n        sns.histplot(data = res , x = \"Survived\" , y = x , cbar = True , cmap = \"coolwarm\")\n        plt.show()\n    print(\"\\n\\n\")","496806d2":"grouping(\"Pclass\" , \"Age\")","a76f605e":"grouping(\"Age\")","78481ba1":"grouping(\"SibSp\" , \"Parch\")","84399513":"grouping(\"Parch\" , \"Pclass\")","8ea69373":"grouping(\"Fare\")","0e94c552":"sns.histplot(data = df , x = \"Age\" , kde = True , hue = \"Sex\" , bins = 20)","d434c727":"sns.kdeplot(data = df , x = \"Age\" , hue = \"Survived\")","6dc0900d":"sns.kdeplot(data = df , x = \"Age\" , hue = \"Pclass\")","b68e132e":"sns.histplot(data = df , x = \"Fare\" , kde = True , bins = 20 , hue = \"Pclass\")","fa382160":"sns.kdeplot(data = df , x = \"Fare\" , hue = \"Sex\")","ea0665f2":"sns.kdeplot(data = df , x = \"Fare\" , hue = \"Survived\")","a2673d33":"sns.countplot(data = df , x = \"First\" , hue = \"Survived\")","ce64d9a2":"sns.countplot(data = df , x = \"Pclass\" , hue = \"Sex\")  # Pclass 3 has more males","1011362d":"sns.countplot(data = df , x = \"Sex\" , hue = \"Survived\")  # Males has more rate of survival","ffec9f47":"plt.figure()\nsns.countplot(data = df , x = \"SibSp\" , hue = \"Sex\")\nplt.figure()\nsns.countplot(data = df , x = \"Parch\" , hue = \"Sex\")","00508e4a":"plt.figure()\nsns.countplot(data = df , x = \"SibSp\" , hue = \"Survived\")\nplt.figure()\nsns.countplot(data = df , x = \"Parch\" , hue = \"Survived\")","8934390b":"plt.figure()\nsns.countplot(data = df , x = \"SibSp\" , hue = \"Pclass\")\nplt.figure()\nsns.countplot(data = df , x = \"Parch\" , hue = \"Pclass\")","7c1ddc35":"sns.boxplot(data = df , x = \"First\" , y = \"Age\" , hue = \"Survived\")","29348407":"sns.boxplot(data = df , x = \"Pclass\" , y = \"Age\" , hue = \"Survived\")","7b8b2ed0":"sns.boxplot(data = df , x = \"Sex\" , y = \"Age\" , hue = \"Pclass\")","3177d7e7":"sns.boxplot(data = df , x = \"Pclass\" , y = \"Fare\" , hue = \"Survived\")","6cfab9bc":"sns.boxplot(data = df , x = \"Sex\" , y = \"Fare\" , hue = \"Pclass\")","b3c334f7":"# Getting rid of skewness in train set\n\ndf[\"Age\"] = np.log(df[\"Age\"] + 1)\ndf[\"Fare\"] = np.log(df[\"Fare\"] + 1)","91f51592":"# Getting rid of skewness in test set\n\ntest[\"Age\"] = np.log(test[\"Age\"] + 1)\ntest[\"Fare\"] = np.log(test[\"Fare\"] + 1)","1b9e644b":"# Creating dummie variables for categorical columns , droping the first_column\n\ndf = pd.get_dummies(df , columns = [\"Sex\" , \"Pclass\" , \"Embarked\" , \"First\"] , prefix = [\"Sex\" , \"Pclass\" , \"Embarked\" , \"First\"] , drop_first = True)\ntest = pd.get_dummies(test , columns = [\"Sex\" , \"Pclass\" , \"Embarked\" , \"First\"] , prefix = [\"Sex\" , \"Pclass\" , \"Embarked\" , \"First\"] , drop_first = True)","37f50396":"df.head()","889209d9":"test.head()","9de651b0":"from sklearn.model_selection import train_test_split\nX = df.drop([\"Survived\"] , axis = 1)\ny = df[\"Survived\"]\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)\nlen(X_train) , len(X_test) , len(y_train) , len(y_test)","590c8ef4":"from sklearn.preprocessing import StandardScaler\nscaler1 = StandardScaler()\nX_train[[\"Age\" , \"Fare\"]] = scaler1.fit_transform(X_train[[\"Age\" , \"Fare\"]])\nX_test[[\"Age\" , \"Fare\"]] = scaler1.transform(X_test[[\"Age\" , \"Fare\"]])\ntest[[\"Age\" , \"Fare\"]] = scaler1.transform(test[[\"Age\" , \"Fare\"]])","65832173":"X_train.head()","12325304":"test.head()","fc1e7db0":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.metrics import confusion_matrix , roc_auc_score , f1_score , accuracy_score , classification_report , roc_curve , auc , plot_roc_curve\nfrom sklearn.model_selection import cross_val_score","1fd625fc":"models = []\nmodels.append(['XGBClassifier', XGBClassifier(learning_rate = 0.1 , objective = 'binary:logistic' , random_state = 42 , eval_metric='mlogloss')])\nmodels.append(['AdaBoostClassifier', AdaBoostClassifier(random_state = 42)])\nmodels.append(['RandomForest', RandomForestClassifier(random_state = 42)])\nmodels.append(['Logistic Regression', LogisticRegression(random_state = 42)])\nmodels.append(['KNeigbors', KNeighborsClassifier()])","6c5f51a1":"def metrics(model , X_train , y_train , X_test , y_test , params = False):\n    \n    model[1].fit(X_train , y_train)\n    preds = model[1].predict(X_test)\n    accuracies = cross_val_score(estimator = model[1], X = X_train , y = y_train, cv = 10)\n    cm = confusion_matrix(y_test , preds)\n    cf = classification_report(y_test , preds)\n    roc = roc_auc_score(y_test , model[1].predict_proba(X_test)[: , 1])\n    fpr, tpr, thresholds = roc_curve(y_test, preds)\n    ac = auc(fpr, tpr)\n    f1 = f1_score(y_test , preds)\n    \n    \n    print(\"\\n\")\n    print(model[0])\n    \n    print(\"\\n\")\n    if params:\n        print(f\"Best Parameters are : \\n\" , model[1].best_params_)\n        print(\"\\n\")\n        \n    print(f\"Confusion matrix : \\n\")\n    plt.figure(figsize = (8, 5))\n    sns.heatmap(cm, cmap = 'coolwarm', annot = True, annot_kws = {'fontsize': 20})\n    plt.show()\n    print(\"\\n\")\n    \n    print(f\"Training score : {model[1].score(X_train , y_train):.4f}\")\n    print(\"\\n\") \n    \n    print(f\"Test Score : {model[1].score(X_test , y_test):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"K-fold accuracy : {np.mean(accuracies):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"Standard Deviation of Accuracies in k-fold : {np.std(accuracies):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"ROC AUC Score: {roc:.4f}\")\n    print('\\n')\n    \n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"\\n\")\n    \n    print(f\"AUC : {ac:.4f}\")\n    print(\"\\n\")\n    \n    print(f\"Classification report : \\n\\n{cf}\")\n    print(\"\\n\")\n\n    plt.figure(figsize = (8, 5))\n    plot_roc_curve(model[1], X_test, y_test , color = '#FF4500')\n    plt.plot([0, 1], [0, 1], linestyle = '--', color = '#7CFC00')\n    plt.show()\n    print(\"\\n\")\n    print(\"*\"*100)\n    \n    print(\"\\n\\n\")\n    \n    sam = []\n    sam.append(model[0])\n    sam.append(model[1].score(X_train , y_train))\n    sam.append(model[1].score(X_test , y_test))\n    sam.append(np.mean(accuracies))\n    sam.append(np.std(accuracies))\n    sam.append(roc)\n    sam.append(f1)\n    sam.append(ac)\n    \n    return sam\n    \n    ","179e78cf":"pre_final = []\nfor i in models:\n    sam = metrics(i , X_train , y_train , X_test , y_test)\n    pre_final.append(sam)","0746dafe":"pre_final","b65450db":"me = pd.DataFrame(pre_final , columns = [\"Model\" , \"Train Score\" , \"Test Score\" , \"K-fold Accuracy\" , \"K-fold Std\" , \"ROC_AUC\" , \"F1 Score\" , \"AUC Score\"])\n\nme.sort_values(by = [\"K-fold Accuracy\" , \"F1 Score\" , \"ROC_AUC\" , \"AUC Score\" , \"Train Score\" , \"Test Score\"] , inplace = True , ascending = False)\nme = me.reset_index(drop = True)\nme","47f5f1bf":"plt.figure(figsize = (10 , 5))\nsns.barplot(y = \"Model\" , x = \"ROC_AUC\" , data = me)\nplt.title(\"Model Comparision based on ROC_AUC\");","f7a24cec":"plt.figure(figsize = (10 , 5))\nsns.barplot(y = \"Model\" , x = \"F1 Score\" , data = me)\nplt.title(\"Model Comparision based on F1 Score\");","813aadfd":"plt.figure(figsize = (10 , 5))\nsns.barplot(y = \"Model\" , x = \"AUC Score\" , data = me)\nplt.title(\"Model Comparision based on AUC Score\");","42f9139c":"from sklearn.model_selection import GridSearchCV\n\n\ngrid_xgb = {\"n_estimators\" : [100 , 200 , 300]}\n\n\ngrid_ada = {\"n_estimators\" : [50 , 100 , 200]}\n\n\ngrid_random = {\"n_estimators\" : [150 , 200 , 250],\n              \"bootstrap\" : [True , False] , \n              \"max_features\" : ['auto', 'sqrt'] , \n              \"min_samples_leaf\" : [2, 4] , \n              \"class_weight\" : [\"balanced\", \"balanced_subsample\"]}\n\n\ngrid_linear = {\"max_iter\" : [100 , 150] , \n              \"solver\" : [\"liblinear\"] , \n              \"multi_class\" : [\"ovr\"]}\n\n\ngrid_neighbor = {\"n_neighbors\" : [5 , 7 , 10] , \n                \"algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]} \n","0e0e4110":"xgb = metrics(['XGBClassifier', GridSearchCV(XGBClassifier(learning_rate = 0.1, objective = 'binary:logistic' , random_state = 42 , eval_metric='mlogloss') , param_grid = grid_xgb, cv = 5, verbose = 0)] ,  X_train , y_train , X_test , y_test , params = True )","42bff1d8":"ada = metrics(['AdaBoostClassifier', GridSearchCV(AdaBoostClassifier(random_state = 42) , param_grid = grid_ada , cv = 5 , verbose = 0 )] , X_train , y_train , X_test , y_test , params = True)","96f3c872":"random = metrics(['RandomForest', GridSearchCV(RandomForestClassifier(random_state = 42) , param_grid = grid_random, cv = 5, verbose = 0 , scoring = \"f1\")] ,  X_train , y_train , X_test , y_test , params = True )","b56e3976":"linear = metrics(['Logistic Regression', GridSearchCV(LogisticRegression(random_state = 42) , param_grid = grid_linear, cv = 5, verbose = 0 )] ,  X_train , y_train , X_test , y_test , params = True )","48a45840":"knn = metrics(['KNeigbors', GridSearchCV(KNeighborsClassifier() , param_grid = grid_neighbor, cv = 10, verbose = 0)] , X_train , y_train , X_test , y_test , params = True)","76ca3566":"final = [xgb , ada , random , linear , knn]\nfinal","a00d0bc6":"me # Without Gridsearchcv","c08749c2":"# With gridsearch CV\n\nmef = pd.DataFrame(final , columns = [\"Model\" , \"Train Score\" , \"Test Score\" , \"K-fold Accuracy\" , \"K-fold Std\" , \"ROC_AUC\" , \"F1 Score\" , \"AUC Score\"])\n\nmef.sort_values(by = [\"K-fold Accuracy\" , \"F1 Score\" , \"ROC_AUC\" , \"AUC Score\" , \"Train Score\" , \"Test Score\"] , inplace = True , ascending = False)\nmef = mef.reset_index(drop = True)\nmef","1cdfa2af":"def feature_importance(model , X_train , y_train):\n    model[1].fit(X_train , y_train)\n    features = model[1].feature_importances_\n    print(model[0])\n    print(\"\\n\")\n    print(f\"Feature importance list : \\n\" , features)\n    print(\"\\n\")\n    plt.figure(figsize = (15 , 8))\n    sns.barplot(X_train.columns.tolist() , features)\n    plt.show()\n    print(\"\\n\")\n    print(\"*\"*100)\n    print(\"\\n\")","fa45d259":"def linear_coeffs(model , X_train , y_train):\n    model[1].fit(X_train , y_train)\n    features = model[1].coef_\n    print(model[0])\n    print(\"\\n\")\n    print(f\"Feature importance list : \\n\" , features)\n    print(\"\\n\")\n    plt.figure(figsize = (15 , 8))\n    sns.barplot(X_train.columns.tolist() , features.ravel())\n    plt.show()\n    print(\"\\n\")\n    print(\"*\"*100)\n    print(\"\\n\")","41aa8582":"feature_importance([\"XGBClassifier\" , XGBClassifier(learning_rate = 0.1, objective = 'binary:logistic' , random_state = 42 , eval_metric='mlogloss')] , X_train , y_train)","334c2c7b":"feature_importance([\"Randomforest Classifier\" , RandomForestClassifier(random_state = 42 , bootstrap = False, class_weight = 'balanced', max_features = 'auto', min_samples_leaf = 4 , n_estimators = 200 )] , X_train , y_train)","498b5751":"linear_coeffs([\"Logestic Regressor\" , LogisticRegression(max_iter = 100,  multi_class = 'ovr',  random_state = 42,  solver = 'liblinear')] , X_train , y_train)","49bb1eb6":"test","999c8502":"test.info()","10139246":"test.isna().sum()","a1b1e4f4":"clf_linear = LogisticRegression(random_state = 42)\nclf_linear.fit(X_train , y_train)\nlinear_preds = clf_linear.predict(test)\nlinear_preds","c01af1c8":"data_linear = {\"PassengerId\" : ids , \n       \"Survived\" : linear_preds}\nfinal_linear = pd.DataFrame(data_linear, columns = [\"PassengerId\" , \"Survived\"])\nfinal_linear.head(10)","b89d4d04":"final_linear[\"Survived\"].value_counts()","7a5149ae":"clf_xgb = XGBClassifier(learning_rate = 0.1, objective = 'binary:logistic' , random_state = 42 , eval_metric='mlogloss')\nclf_xgb.fit(X_train , y_train)\nxgb_preds = clf_xgb.predict(test)\nxgb_preds","ceeb6d9d":"data_xgb = {\"PassengerId\" : ids , \n       \"Survived\" : xgb_preds}\n\nfinal_xgb = pd.DataFrame(data_xgb, columns = [\"PassengerId\" , \"Survived\"])\nfinal_xgb.head(10)","bdf61bae":"final_xgb[\"Survived\"].value_counts()","8eb3796f":"final_xgb.to_csv(\"Titanic_linear_1.csv\" , index = False)","9b8fa463":"## Model Evaluation with Grid SearchCV","3250c674":"### Boxplots","c0ef8e59":"## Data Preprocessing","5eaafa4a":"### Countplots","985605f7":"## GridsearchCV Model Visualization","03d41832":"## Filling Missing Values","7d4681da":"## Feature Scaling","0a33561b":"## Evaluation on the Test Dataset","d444d029":"## Model Evaluation Visualization","25ca0dce":"## Model Evaluation","f54e8d1f":"## Train Test Split","8c275ed0":"## Data Visualization","4109d34f":"Not fitting XGB , Ada Boost as it already comes with best parameters","44f28edd":"## Importing Required Libraries","ef9d029f":"## Loading Dataset","e49c46a8":"## Importing libraries for model fitting"}}