{"cell_type":{"a09e547f":"code","cfbfba84":"code","b6fb70f9":"code","768a50b8":"code","25172e04":"code","0dfc30d2":"code","12b2edcc":"code","dc05ab52":"code","114fc493":"code","5b7222f5":"code","681dcf58":"code","422f7d91":"code","6176190e":"code","4e09d640":"code","f2ea0e91":"code","1aa649bf":"code","90841ca3":"code","cb327829":"code","42b5842d":"code","8826fc19":"code","fd2c7a48":"code","2e49c505":"code","88105cd9":"code","2e05ed98":"code","36a801b0":"code","6ce42106":"code","91b2124a":"code","fb12bc3b":"code","899d236d":"code","e0c63e7a":"code","dac403db":"code","2cb0521a":"code","1391733a":"markdown","8764c5ab":"markdown","bd1121c0":"markdown","d3b14ca6":"markdown","9c85d064":"markdown","f9ce1cae":"markdown","873f6bf0":"markdown","c01a4475":"markdown","0202fdd3":"markdown","f983bc1d":"markdown","cf9c1e19":"markdown","6d23000c":"markdown","36fba278":"markdown"},"source":{"a09e547f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nsns.set_palette('hot')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sys, glob, copy, warnings\nwarnings.simplefilter(\"ignore\")\n\ninp = '\/kaggle\/input\/tabular-playground-series-feb-2021\/'","cfbfba84":"df, features = {}, {}\nprint('{:18s}{:>10s}{:>5s}{:>5s}'.format('FILE', 'ROWS', 'COLS', 'NULL'))\nfor file in glob.glob(f'{inp}\/*.csv'):\n    label = file.split('\/')[-1].split('.')[0]\n    df[label] = pd.read_csv(file, index_col='id')\n    features[label] = set(df[label].columns.to_list())\n    print('{:18s}{:10,d}{:5d}{:5d}'.format(label, *df[label].shape, df[label].isna().any().sum()))","b6fb70f9":"(df['sample_submission'].index == df['test'].index).all()\n# Output confirms good behavior; no worries.","768a50b8":"features['train'] == features['test'].union(features['sample_submission'])\n# Output confirms no surprises; nothing tricky; nothing fancy.\n# This wasn't the case for competitions like Jane Street and Data Science Bowl.","25172e04":"df['train'].tail()","0dfc30d2":"df['train'].describe(include='object')","12b2edcc":"features = {'cat': df['train'].columns[ df['train'].columns.str.startswith('cat') ].to_list(),\n            'con': df['train'].select_dtypes(include='float').columns.to_list()}  # includes target\nfeatures","dc05ab52":"print('{:<8s}{:76s}{}'.format('FEATURE', 'VALUES IN TRAIN', 'VALUES IN TEST'))\nfor feature in features['cat']:\n    unik = {'train': sorted(df['train'][feature].unique()),\n            'test' : sorted(df['test'][feature].unique())}\n    print('{:<8s}{:76s}'.format(feature, str(unik['train'])), end='')\n    if unik['train']!=unik['test']:\n        print(str(unik['test']))\n    else:\n        print('same here')\n# Output shows\n# - agreement in all columns except cat6;\n# - unique values in test are a subset of unique values in train.","114fc493":"ncoda = OrdinalEncoder().fit(df['train'][features['cat']])\n# For sanity check only; will be deleted real soon:\norig = copy.deepcopy(df)\nfor dataset in ['train', 'test']:\n    df[dataset][features['cat']] = ncoda.transform(df[dataset][features['cat']])\n    df[dataset][features['cat']] = df[dataset][features['cat']].astype(int)  # .astype('category')\nncoda.categories_","5b7222f5":"# Just a pedantic sanity check.\nassert (ncoda.inverse_transform(df['train'][features['cat']]) == orig['train'][features['cat']]).all().all()\nassert (ncoda.inverse_transform(df['test'][features['cat']]) == orig['test'][features['cat']]).all().all()\ndel orig","681dcf58":"df['train'].info()","422f7d91":"cols = 3\nrows = int(np.ceil(len(features['cat'])\/cols))\nfig, ax = plt.subplots(rows, cols, figsize=(15, 5*rows), sharex=True)\nvaluecounts = {}\nmax2min = pd.Series(dtype=float)\nfor nfeature, feature in enumerate(features['cat']):\n    valuecounts[feature] = df['train'][feature].value_counts(normalize=True).sort_index()\n    max2min[feature] = valuecounts[feature].max()\/valuecounts[feature].min()\n# plot histogram starting with the most imbalance feature, in that order\nfor nfeature, feature in enumerate(max2min.sort_values(ascending=False).index):\n    tis_ax = ax[nfeature\/\/cols][nfeature%cols]\n    sns.barplot(x=valuecounts[feature].values, y=valuecounts[feature].index, orient='h', ax=tis_ax, palette='hot')\n    tis_ax.set_title(f'{feature}: {max2min[feature]:7.1e}')\n    print(feature, '_'*10)\n    for idx, value in valuecounts[feature].items():\n        print('{:2d} ({:7.1e})'.format(idx, value), end='\\t')\n    print()\n# Output confirms our premonition of an unbalanced distribution.","6176190e":"# Plot distribution of target, broken down into contributing components of each categorical feature, starting with the most imbalance feature, in that order.\nfig, ax = plt.subplots(rows, cols, figsize= (15, 5*rows), sharex=True)\nfor nfeature, feature in enumerate(max2min.sort_values(ascending=False).index):\n    sns.histplot(data=df['train'], x='target', stat='density', hue=feature, ax=ax[nfeature\/\/cols, nfeature%cols], palette='hot')","4e09d640":"cols = 2\nrows = int(np.ceil(len(features['con'])\/cols))\nfig, ax = plt.subplots(rows, cols, figsize= (15, 5*rows))\nfor nfeature, feature in enumerate(features['con']):\n    sns.histplot(df['train'][feature], stat='density', ax=ax[nfeature\/\/cols, nfeature%cols], palette='hot')","f2ea0e91":"fig, ax = plt.subplots(2, 1, figsize=(15, 10))\nskew_kurtosis = pd.DataFrame(df['train'][features['con']].skew(), columns=['skew'])\nskew_kurtosis['kurtosis']  = df['train'][features['con']].kurtosis()\nsns.barplot(x=skew_kurtosis.index, y='skew', data=skew_kurtosis, orient='v', ax=ax[0], palette='hot')\nsns.barplot(x=skew_kurtosis.index, y='kurtosis', data=skew_kurtosis, orient='v', ax=ax[1], palette='hot')","1aa649bf":"tmp = df['train'][df['train'].columns[df['train'].columns.str.startswith('con')]]\nxx = tmp.mean()\nyy = tmp.median()\nplt.figure(figsize=(10, 10))\nplt.plot([xx.min(), xx.max()], [yy.min(), yy.max()], 'y-.')\nplt.plot(xx, yy, '.r')\nfor x, y, z in zip(xx, yy, tmp):\n    plt.text(x+.005, y, z)\n_ = plt.axis('equal'); plt.xlabel('feature mean'); plt.ylabel('feature median')","90841ca3":"%%time\nplt.figure(figsize=(15, 5))\nsns.violinplot(data=df['train'][ df['train'].columns[df['train'].columns.str.startswith('cont')] ], palette='hot')","cb327829":"%%time\ntraintest = pd.concat([df['train'], df['test']])\nsns.pairplot(traintest, palette='hot')\n# Pairplot reveals Tabular Playground's fingerprint unlikely to be found in Featured Competitions.","42b5842d":"binned = traintest[features['con']].apply(lambda x: pd.cut(x, bins=32, labels=False))\nplt.figure(figsize=(15, 15))\nnfeatures = len(features['con'])\nfor aa in range(1, nfeatures):\n    for bb in range(aa):\n        plt.subplot(nfeatures, nfeatures, aa*nfeatures + bb + 1)\n        sns.heatmap(binned.groupby(features['con'][aa]).apply(lambda x: x[features['con'][bb]].value_counts()).unstack(), \n                    square=True, cmap='hot', cbar=False, xticklabels=False, yticklabels=False)\n        plt.axis('off')\nfor tmp in range(1, nfeatures):\n    plt.subplot(nfeatures, nfeatures, nfeatures*tmp+1)\n    plt.axis('on'); plt.ylabel(features['con'][tmp])\nfor tmp in range(nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.axis('on'); plt.xlabel(features['con'][tmp])\nfor tmp in range(1, nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.ylabel('')","8826fc19":"%%time\ncorr = traintest.corr()\ncorr.to_csv('corr.csv')   # Best to save a copy as it takes ages to run.\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, mask=np.triu(np.ones_like(corr, dtype=bool)), square=True, cmap='hot', cbar_kws={\"shrink\": .5})\ncorr","fd2c7a48":"slimcorr = pd.Series(dtype=float)\nfor feature in corr.columns:\n    slimcorr.loc[feature] = corr[feature].sort_values()[-2]\nslimcorr.sort_values(ascending=False)\n# output reports no correlation too high; therefore too premature to drop any feature","2e49c505":"dataX = df['train'].copy()\ndatay = dataX.pop('target')\ntrainX, validX, trainy, validy = train_test_split(dataX, datay)","88105cd9":"def trainNpredict(model):\n    pred = model.fit(trainX, trainy).predict(validX)\n    rmse = mean_squared_error(validy, pred, squared=False)\n    print('rmse =', rmse)\n    return model.predict(dataX), rmse\n\npred, rmse = {}, pd.Series(dtype=float)\npred['dummy median'], rmse.loc['dummy median'] = trainNpredict(DummyRegressor(strategy='median'))\n# All 4 dummy models are courtesy of https:\/\/www.kaggle.com\/inversion\/get-started-feb-tabular-playground-competition.","2e05ed98":"pred['linear regression'], rmse.loc['linear regression'] = trainNpredict(LinearRegression(fit_intercept=True))","36a801b0":"pred['lasso'], rmse.loc['lasso'] = trainNpredict(Lasso(fit_intercept=False))","6ce42106":"%time pred['random forest'], rmse.loc['random forest'] = trainNpredict(RandomForestRegressor(n_estimators=50, n_jobs=-1))","91b2124a":"%time pred['lgb'], rmse.loc['lgb'] = trainNpredict(LGBMRegressor())","fb12bc3b":"%time pred['xgb'], rmse.loc['xgb'] = trainNpredict(XGBRegressor())","899d236d":"rmse.sort_values()","e0c63e7a":"plt.figure(figsize=(10, 15))\nfor ndummy, dummy in enumerate(pred.keys()):\n    plt.subplot(3, 2, 1+ndummy)\n    plt.plot(datay, pred[dummy], '.')\n    plt.plot([datay.min(), datay.max()], [datay.min(), datay.max()], 'y-.')\n    plt.grid(True); plt.xlabel('true_y'); plt.ylabel('pred_y'); plt.title('{}: {:5.1f}'.format(dummy, rmse[dummy]))","dac403db":"df['sample_submission']['target'] = LGBMRegressor().fit(dataX, datay).predict(df['test'])\ndf['sample_submission'].to_csv('dummy_submission.csv')","2cb0521a":"if 'BorutaShap' not in sys.modules:\n    !pip install BorutaShap\nfrom BorutaShap import BorutaShap\nFeature_Selector = BorutaShap(model=LGBMRegressor(), importance_measure='shap', classification=False)\nFeature_Selector.fit(X=dataX, y=datay, n_trials=150) # sample=False, train_or_test = 'test', normalize=True, verbose=True)\nFeature_Selector.plot(which_features='all')","1391733a":"## Correlation","8764c5ab":"## Categorical features","bd1121c0":"## BorutaShap","d3b14ca6":"![](https:\/\/marychin.org\/download\/kaggle\/tabfeb.png)","9c85d064":"## Quick look","f9ce1cae":"## What we've got so far","873f6bf0":"## First stab","c01a4475":"## Next, raise bar marginally higher","0202fdd3":"## Set the bar low","f983bc1d":"## Distribution","cf9c1e19":"# First steps: a few checkboxes\n* Any null values? - No, all clear. See **Quick look** section.\n* Is data skewed, a bit like [Mechanism of Action](https:\/\/www.kaggle.com\/c\/lish-moa\/submissions)? - Yes, gravely. See **Distribution** section. Without a fix this is going to plague learning and prediction whether we use gradient boosters, neural networks or other techniques.\n* Any outliers?\n* Any highly-correlated features? No. See **Correlation** section.\n* Any hurdle in submission process (like [Jane Street](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction))? No; straightforward submission this time.\n\nNote that cells beginning with Jupyter magic ```%%time``` take longer to run. The output of the cell indicates how long. Seaborn's ```pairplot``` is, for instance, notorious for taking a loooong time to run.\n\n## What I like about this competition\nQuick turnaround time! We can test parameters, get the output quickly, adjust and test again over and over. Even submissions return the score almost instantly. Excellent for experimenting.","6d23000c":"## Pick the best dummy, submit and see","36fba278":"## 2D flood maps: how features pair cross-talk\nFingerprints in the previous cell doesn't tell very much; they are scatter plots which do not indicate the frequency. Flood maps would tell us more. Seaborn has a one-liner for that; but runs till eternity without returning. Here is therefore a dirty hack."}}