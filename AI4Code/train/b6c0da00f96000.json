{"cell_type":{"74d600cf":"code","593a32af":"code","4762b000":"code","27da8f4d":"code","f506f9e6":"code","bda8a4d3":"code","0b01c444":"code","645326e6":"code","1a3b7302":"code","110b3469":"code","97bf9132":"code","37e963b9":"code","d9dfb1f8":"code","0a71fe45":"code","8d8dbc9b":"code","19e5c5d2":"code","d632d21e":"code","51d8a014":"code","7e422f48":"code","50ace200":"code","1c6aae8d":"code","d891a168":"code","0947c7af":"code","87db8bc5":"code","a08dfc21":"code","ebd754d5":"code","e97f21c8":"code","8fa77fd2":"code","12cbede1":"code","1283d61b":"code","0942d073":"code","7a9d8172":"code","f5d20ae9":"code","b4dea30b":"code","4b2f9eac":"code","6b3933ac":"code","3da5cfc6":"code","0e75d1b2":"code","552eaa08":"code","7ece68ed":"code","2a8ea1bb":"code","3e59a6f5":"code","e7b594a2":"code","998d3343":"code","79570cc2":"code","d817dfc8":"code","ad3fe268":"code","f57ea9a7":"code","1b3518c4":"code","e764ef08":"code","a81eb666":"code","3d679717":"code","9eb706a1":"code","f0ca5273":"code","4c5ce469":"code","704872cc":"code","37b6a259":"code","e2a11f9a":"code","5e45e150":"code","b1f0f693":"code","42408ada":"code","12d1caf5":"code","ce839262":"code","6ea64a52":"code","1cba56d4":"code","19c90d3d":"code","0f70e9fe":"code","0dbc4cf2":"code","9ef9545b":"code","8d4f14ff":"code","e2402cd7":"code","f02e7832":"code","646b15de":"markdown","03ed04c6":"markdown","750f1d4f":"markdown","be5e4363":"markdown","c800165d":"markdown","03035395":"markdown","50a284da":"markdown","a7b3b569":"markdown","9f8e0ec0":"markdown","f14e4888":"markdown","024cb05f":"markdown","829c8fc3":"markdown","a5a18490":"markdown","111803c5":"markdown","d94c641f":"markdown","2427c9a0":"markdown","c197b3ba":"markdown"},"source":{"74d600cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","593a32af":"# Reading .csv file \nimport pandas as pd\ndata = pd.read_csv('..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv', encoding='ISO-8859-1')","4762b000":"# To get the dimentions of the dataset.\ndata.shape","27da8f4d":"# To get the column names of the dataset\ndata.columns","f506f9e6":"# to print the consise summary of the data set\ndata.info()","bda8a4d3":"# to compute a summary of statistics pertaining to the DataFrame columns\ndata.describe()","0b01c444":"# to return the first five rows of the data frame\ndata.head()","645326e6":"# To get the data types of the different features\/ columns\ndata.dtypes","1a3b7302":"# to check the missing or null values in the data set \ndata.isnull().sum()                                                     \nmiss_val = data.isnull().sum().sort_values(ascending=False)\nmiss_val = pd.DataFrame(data=data.isnull().sum().sort_values(ascending=False), columns=['MissvalCount'])\n\n# Add a new column to the dataframe and fill it with the percentage of missing values\nmiss_val['Percent'] = miss_val.MissvalCount.apply(lambda x : '{:.2f}'.format(float(x)\/data.shape[0] * 100)) \nmiss_val = miss_val[miss_val.MissvalCount > 0]\nmiss_val","110b3469":"# to check the the text of tweet at specified row\ndata['full_text'][10]","97bf9132":"# to compute a summary of statistics pertaining to the DataFrame column full_text\ndata['full_text'].describe()","37e963b9":"# to compute a summary of statistics pertaining to the DataFrame location\ndata['location'].describe()","d9dfb1f8":"text = data['full_text']\nlocation = data['location']\n\nfor i in np.random.randint(1000, size=10):\n    print(f'Tweet # {i}: ', text[i], '=> Location: ', location[i], end='\\n' * 3)","0a71fe45":"location = data.groupby('location')\nlocation.head()","8d8dbc9b":"# drop rows with any missing values\ndata.dropna(inplace=True)","19e5c5d2":"# Removing duplicates\ndata.drop_duplicates()","d632d21e":"data.head()","51d8a014":"# drop the columns with highest missing values \ndata = data.drop(['Unnamed: 0', 'created_at_tweet', 'retweet_count', 'favorite_count','reply_count', 'location'], axis=1)\ndata.head()","7e422f48":"from sklearn import decomposition\nfrom scipy import linalg\nimport re\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","50ace200":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt ","1c6aae8d":"# remove twitter handles (@user)\ndata['tidy_text'] = np.vectorize(remove_pattern)(data['full_text'], \"@[\\w]*\")","d891a168":"# remove special characters, numbers, punctuations\ndata['tidy_text'] = data['tidy_text'].str.replace(\"[^a-zA-Z#]\", \" \")","0947c7af":"#Removing Short Words\ndata['tidy_text'] = data['tidy_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","87db8bc5":"def cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?:\/\/[^s]+))',' ',data)","a08dfc21":"data['tidy_text'] = data['tidy_text'].apply(lambda x: cleaning_URLs(x))","ebd754d5":"data['tidy_text'] = data['tidy_text'].str.replace(\"[#]\", \" \")","e97f21c8":"from textblob import TextBlob","8fa77fd2":"def getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity","12cbede1":"def getpolarity(text):\n    return TextBlob(text).sentiment.polarity","1283d61b":"data['Subjectivity'] = data['tidy_text'].apply(getSubjectivity)","0942d073":"data['Polarity'] = data['tidy_text'].apply(getpolarity)","7a9d8172":"data.head(10)","f5d20ae9":"def getPositiveNegativeWordCount(score):\n    if score < 0:\n        return 'Negative'\n    else:\n        return 'Positive'","b4dea30b":"data['Positive Negative Word Count'] =  data['Polarity'].apply(getPositiveNegativeWordCount)","4b2f9eac":"data['tidy_text']","6b3933ac":"import nltk  \nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\nfrom nltk.stem import WordNetLemmatizer","3da5cfc6":"stopwords_english = stopwords.words('english') \n","0e75d1b2":"print(stopwords_english)","552eaa08":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)","7ece68ed":"tweet_tokens = tokenizer.tokenize(str(data['tidy_text']))","2a8ea1bb":"tweet_tokens","3e59a6f5":"clean_data = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        clean_data.append(word)\n\nprint('Data after removing of stop words')\nprint(clean_data)","e7b594a2":"# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ndata_stem = [] \n\nfor word in clean_data:\n    stem_word = stemmer.stem(word)  # stemming word\n    data_stem.append(stem_word)  # append to the list\n\nprint('Data after Stemming')\nprint(data_stem)","998d3343":"# instantiate Lemmatization Class\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Create an empty list to store the stems\ndata_lemm = [] \n\nfor word in clean_data:\n    lemm_word =wordnet_lemmatizer.lemmatize(word)  # Lemmatization word\n    data_lemm.append(lemm_word)  # append to the list\n\nprint('Data after Lemmatization')\nprint(data_lemm)","79570cc2":"import random  \nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK","d817dfc8":"nltk.download('twitter_samples')","ad3fe268":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","f57ea9a7":"print('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))","1b3518c4":"# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()","e764ef08":"# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])","a81eb666":"# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)","3d679717":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","9eb706a1":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs","f0ca5273":"tweets = all_positive_tweets + all_negative_tweets\nprint(\"Number of tweets: \", len(tweets))","4c5ce469":"labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))","704872cc":"# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')","37b6a259":"print(freqs)","e2a11f9a":"# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '\u2764', ':)', ':(', '\ud83d\ude12', '\ud83d\ude2c', '\ud83d\ude04', '\ud83d\ude0d', '\u265b',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata","5e45e150":"fig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()","b1f0f693":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))","42408ada":"#the creation of the numerical features needed for the Logistic regression model. In order not to interfere with it,\n#we have previously calculated and stored these features in a CSV file for the entire training set.\ndata = pd.read_csv('..\/input\/logistic-fectures\/logistic_features.csv'); # Load a 3 columns csv file using pandas function\ndata.head(10) # Print the first 10 data entries","12d1caf5":"# Each feature is labeled as bias, positive and negative\nX = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\nY = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n\nprint(X.shape) # Print the shape of the X part\nprint(X) # Print some rows of X","ce839262":"theta = [7e-08, 0.0005239, -0.00055517]","6ea64a52":"#Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")","1cba56d4":"# Equation for the separation plane\n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) \/ w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) \/ theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change. We are only interested \n# in the direction. So this direction is just a perpendicular function to the \n# separation plane\n# df(pos, W) = pos * w2 \/ w1\ndef direction(theta, pos):\n    return    pos * theta[2] \/ theta[1]","19c90d3d":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()","0f70e9fe":"from sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nmodel = LogisticRegression(solver='liblinear', random_state=0)\n\n# fit the model with data\nmodel.fit(X,Y)\n\n#\ny_pred=model.predict(X)","0dbc4cf2":"y_pred","9ef9545b":"model.classes_","8d4f14ff":"model.intercept_","e2402cd7":"model.coef_","f02e7832":"model.score(X, Y)","646b15de":"# EXPLORATORY DATA ANALYSIS","03ed04c6":"# Remove special characters, numbers, punctuations","750f1d4f":"# Removal of Stopwords","be5e4363":"# Stemming","c800165d":"# Remove Short Words","03035395":"# Droping rows with missing Values","50a284da":"# Remove URLs","a7b3b569":"# Removing Missing Values","9f8e0ec0":"#  Claening Tweets","f14e4888":"# Lemmatization","024cb05f":"# Remove twitter handles (@user)","829c8fc3":"# Model Building using Logistic Regression","a5a18490":"# Dowmloading Twiiter dataset ","111803c5":"## Removing Hashtag","d94c641f":"# Removing Duplicates","2427c9a0":"# Tokenization","c197b3ba":"# Create a plot between positive and negative word counts"}}