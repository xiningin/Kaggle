{"cell_type":{"3bb2f153":"code","8a4ff2e2":"code","0e770a33":"code","6605487a":"code","7cd70b85":"code","30b0edaa":"code","b635dacc":"code","2dc2232c":"code","843bfadb":"markdown","6fe63aa1":"markdown","3d43653f":"markdown","f58b759e":"markdown","8826f611":"markdown","67a69811":"markdown","67e62f96":"markdown","7545f00f":"markdown","c9ffd9bd":"markdown","1c34ad85":"markdown","9d5cbe70":"markdown","738e802c":"markdown","4c493b84":"markdown","7b10c1c5":"markdown"},"source":{"3bb2f153":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D, Flatten, Input\nimport cv2\nimport matplotlib.pyplot as plt\nimport random as rd\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a4ff2e2":"df_train = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\ndf_train","0e770a33":"alphabet=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\nn = rd.randrange(df_train.shape[0])\nar = np.array(df_train.loc[n][1:]).reshape((28,28))\nplt.imshow(ar, cmap='gray')\nplt.title(alphabet[df_train.loc[n][0]])\nplt.show()","6605487a":"y = df_train[\"label\"]\nX = df_train.drop(['label'], axis=1)\n\nX = np.array(X)\/255\ny = np.array(y)\n\nY = np.zeros((len(alphabet),df_train.shape[0]))\nfor i in range(len(y)):\n  Y[y[i],i] = 1\nX = X.reshape((-1, 28,28,1))\nY = Y.reshape((26,-1))","7cd70b85":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1),padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(556, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(26, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])","30b0edaa":"history = model.fit(X,y,batch_size=64,epochs=3, validation_split=0.2) #training","b635dacc":"df_valid = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")\n\n#preprocessing\nn = rd.randrange(df_valid.shape[0])\ny = df_valid[\"label\"]\nX = df_valid.drop(['label'], axis=1)\n\nar = np.array(df_valid.loc[n][1:]).reshape((28,28))\n\nX = np.array(X)\/255\ny = np.array(y)\n\nY = np.zeros((26,df_valid.shape[0]))\nfor i in range(len(y)):\n  Y[y[i],i] = 1\nX = X.reshape((-1, 28,28,1))\nY = Y.reshape((26,-1))\n\nplt.imshow(ar, cmap='gray')\nplt.title(f\"Prediction :  {alphabet[ np.argmax(model.predict(X[n].reshape(1,28,28,1)))]} | had to predict {alphabet[df_valid.loc[n][0]]}\")\nplt.show()","2dc2232c":"cap = cv2.VideoCapture(0)\nwhile(True):\n    ret, frame = cap.read()\n    cv2.rectangle(frame, (100, 100), (300, 300), (0, 255, 0), 0)\n    roi = frame[100:300, 100:300]\n    f = cv2.resize(roi, (28, 28))\n    gray = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n    cv2.imshow('frame',frame)\n    if cv2.waitKey(10) & 0xFF == ord('q'):\n        print(\"____\")\n        model.predict(gray.reshape(1,28,28,1))\n        print(alphabet[np.argmax(model.predict(gray.reshape(1,28,28,1)))])\n        \ncap.release()\ncv2.destroyAllWindows()","843bfadb":"*Warning : Doesn't work online, works on user machine*","6fe63aa1":"## 1.2. Dataset","3d43653f":"The purpose of this notebook is to create a simple Convolutionnal Neural Network using Tensorflow in order to recognize hand gestures for sign language. We shall then try to use it with the camera using OpenCV.","f58b759e":"# 2. Data Preprocessing","8826f611":"# 4. Validation","67a69811":"We import the train dataset and show a random image and its label. ","67e62f96":"## 1.1. Imports","7545f00f":"# 1. Introduction","c9ffd9bd":"# 3. Convolutionnal Neural Network","1c34ad85":"First we separate our labels from our data. We then have to normalize the data of all images, and create a simple one hot encoding of the labels.","9d5cbe70":"The predictions are remarkably good ever since the first epoch.","738e802c":"# 5. Using Webcam with OpenCV ","4c493b84":"I decided to use the test dataset as validation.","7b10c1c5":"Using the webcam shows a problem, the use of a solid gray background in the dataset makes it difficult for the CNN to generalize. A way to solve this issue would be to extract the hand from the background and having a \"binary image\" using OpenCV.  *TO BE CONTINUED...*"}}