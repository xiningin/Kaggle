{"cell_type":{"dd7fe098":"code","a8d6ea8b":"code","e417cdf6":"code","be5dc692":"code","045accf2":"code","cabb0f05":"code","57bf0f16":"code","a8003b0d":"code","4e3b47a8":"code","97983f5d":"code","497ce1c5":"code","6056b07b":"code","56526164":"code","c2f77247":"code","30bae942":"code","5be8a8d1":"code","fbc290a5":"code","cc5bb51a":"code","9ad13aae":"code","86e6d046":"code","071840b2":"code","82cff04b":"code","cb78bf4e":"code","436fe1da":"code","0f76b05b":"code","5969214b":"code","6d8be195":"code","0bfbae37":"code","b12a09a6":"code","e59c03e5":"code","315c1da4":"code","21a3ef58":"code","c89614a8":"code","750cf21c":"code","2aa8f50c":"code","e17049e7":"code","62582b57":"code","58df2745":"code","61fea5dd":"code","27369812":"code","c9e95d03":"markdown","aba78dfc":"markdown","5642c37b":"markdown","536f79fe":"markdown","e4ad2cdf":"markdown","21e11cba":"markdown","19aabce5":"markdown","aee52ac2":"markdown","a3e44dfa":"markdown","0460b229":"markdown","b7f6ef4f":"markdown","b576d82d":"markdown","57f39e7c":"markdown","75fe4734":"markdown","c375bf42":"markdown","d0475e7a":"markdown","8a7770ef":"markdown"},"source":{"dd7fe098":"!pip install squarify","a8d6ea8b":"# for basic operations\nimport numpy as np\nimport pandas as pd\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport squarify\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for defining path\nimport os\nprint(os.listdir('..\/input\/'))\n\n# for market basket analysis\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n","e417cdf6":"# reading the dataset\n\ndata = pd.read_csv('..\/input\/Market_Basket_Optimisation.csv', header = None)\n\n# let's check the shape of the dataset\ndata.shape","be5dc692":"# checking the head of the data\n\ndata.head()","045accf2":"# checkng the tail of the data\n\ndata.tail()","cabb0f05":"# checking the random entries in the data\n\ndata.sample(10)","57bf0f16":"# let's describe the dataset\n\ndata.describe()","a8003b0d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud\n\nplt.rcParams['figure.figsize'] = (15, 15)\nwordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 100).generate(str(data[0]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Most Popular Items',fontsize = 20)\nplt.show()","4e3b47a8":"# looking at the frequency of most popular items \n\nplt.rcParams['figure.figsize'] = (18, 10)\ncolor = plt.cm.copper(np.linspace(0, 1, 40))\ndata[0].value_counts().head(50).plot.bar(color = color)\nplt.title('frequency of most popular items bought', fontsize = 20)\nplt.xticks(rotation = 90 )\nplt.grid()\nplt.show()","97983f5d":"\ny = data[0].value_counts().head(50).to_frame()\ny.index\ny.head()","497ce1c5":"# plotting a tree map\n\nplt.rcParams['figure.figsize'] = (20, 20)\ncolor = plt.cm.cool(np.linspace(0, 1,10))\nsquarify.plot(sizes = y.values, label = y.index, alpha=.8, color = color)\nplt.title('Tree Map for Popular Items')\nplt.axis('off')\nplt.show()","6056b07b":"data['food'] = 'Food'\nfood = data.truncate(before = -1, after = 15)\n\n\n","56526164":"data.head()","c2f77247":"import networkx as nx\n\nfood = nx.from_pandas_edgelist(food, source = 'food', target = 0, edge_attr = True)","30bae942":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(food)\ncolor = plt.cm.Wistia(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(food, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(food, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(food, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 First Choices', fontsize = 40)\nplt.show()","5be8a8d1":"data['secondchoice'] = 'Second Choice'\nsecondchoice = data.truncate(before = -1, after = 15)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 1, edge_attr = True)","fbc290a5":"data.head()","cc5bb51a":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Blues(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'brown')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 Second Choices', fontsize = 40)\nplt.show()","9ad13aae":"data['thirdchoice'] = 'Third Choice'\nsecondchoice = data.truncate(before = -1, after = 10)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 2, edge_attr = True)","86e6d046":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Reds(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'pink')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 10 Third Choices', fontsize = 40)\nplt.show()","071840b2":"# making each customers shopping items an identical list\ntrans = []\nfor i in range(0, 7501):\n    trans.append([str(data.values[i,j]) for j in range(0, 20)])\n\n# conveting it into an numpy array\ntrans = np.array(trans)\n\n# checking the shape of the array\nprint(trans.shape)","82cff04b":"\nprint(trans)","cb78bf4e":"import pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\nte = TransactionEncoder()\ndata = te.fit_transform(trans)\ndata = pd.DataFrame(data, columns = te.columns_)\n\n# getting the shape of the data\ndata.shape","436fe1da":"data.head()","0f76b05b":"data.columns","5969214b":"data.info()","6d8be195":"data.columns.values","0bfbae37":"import warnings\nwarnings.filterwarnings('ignore')\n\n\n\ndata = data.loc[:, ['mineral water', 'burgers', 'turkey', 'chocolate', 'frozen vegetables', 'spaghetti',\n                    'shrimp', 'grated cheese', 'eggs', 'cookies', 'french fries', 'herb & pepper', 'ground beef',\n                    'tomatoes', 'milk', 'escalope', 'fresh tuna', 'red wine', 'ham', 'cake', 'green tea',\n                    'whole wheat pasta', 'pancakes', 'soup', 'muffins', 'energy bar', 'olive oil', 'champagne', \n                    'avocado', 'pepper', 'butter', 'parmesan cheese', 'whole wheat rice', 'low fat yogurt', \n                    'chicken', 'vegetables mix', 'pickles', 'meatballs', 'frozen smoothie', 'yogurt cake']]\n\n# checking the shape\ndata.shape","b12a09a6":"# let's check the columns\n\ndata.columns","e59c03e5":"# getting the head of the data\n\ndata.head()","315c1da4":"from mlxtend.frequent_patterns import apriori\n\n#Now, let us return the items and itemsets with at least 5% support:\napriori(data, min_support = 0.01, use_colnames = True)\n","21a3ef58":"frequent_itemsets = apriori(data, min_support = 0.05, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","c89614a8":"# getting th item sets with length = 2 and support more han 10%\n\nfrequent_itemsets[ (frequent_itemsets['length'] == 2) &\n                   (frequent_itemsets['support'] >= 0.01) ]","750cf21c":"# getting th item sets with length = 1 and support more han 10%\n\nfrequent_itemsets[ (frequent_itemsets['length'] == 1) &\n                   (frequent_itemsets['support'] >= 0.01) ]","2aa8f50c":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'eggs', 'mineral water'} ]\n","e17049e7":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'mineral water'} ]\n","62582b57":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'milk'} ]\n","58df2745":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'chicken'} ]\n","61fea5dd":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'frozen vegetables'} ]\n","27369812":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'chocolate'} ]\n","c9e95d03":"<img src=\"https:\/\/blog-c7ff.kxcdn.com\/blog\/wp-content\/uploads\/2017\/03\/Apriori-Algorithm.jpg\" width=\"500px\">","aba78dfc":"## Selecting and Filtering the Results","5642c37b":"## Data Visualizations","536f79fe":"<img src=\"https:\/\/thumbor.forbes.com\/thumbor\/960x0\/https%3A%2F%2Fblogs-images.forbes.com%2Fmarciaturner%2Ffiles%2F2018%2F01%2FWegmans-Produce-1.jpg\" width=\"800px\">","e4ad2cdf":" Getting correlations for 121 items would be messy so let's reduce the items from 121 to 50\n To decide which 50 to be taken , we will select the one with which are bought more frequently","21e11cba":"## Using Apriori Algorithm","19aabce5":"## Applying apriori","aee52ac2":"**Importing the dataset**","a3e44dfa":"Frequent Itemsets via Apriori Algorithm\nApriori function to extract frequent itemsets for association rule mining\nWe have a dataset of a mall with 7500 transactions of different customers buying different items from the store.\nWe have to find correlations between the different items in the store. so that we can know if a customer is buying apple, banana and mango. what is the next item, The customer would be interested in buying from the store. \n\n## Overview\nApriori is a popular algorithm for extracting frequent itemsets with applications in association rule learning. The apriori algorithm has been designed to operate on databases containing transactions, such as purchases by customers of a store. An itemset is considered as \"frequent\" if it meets a user-specified support threshold. For instance, if the support threshold is set to 0.5 (50%), a frequent itemset is defined as a set of items that occur together in at least 50% of all transactions in the database.","0460b229":"## Data Preprocessing","b7f6ef4f":"from the data set we can observe that for column 0 ie shrimps , people bought many different products, creating a word cloud will help in deteming the maximum bought products and the products that are bought along with the shrimps ","b576d82d":"## How does Apriori Algorithm Work ?\n\nA key concept in Apriori algorithm is the anti-monotonicity of the support measure. It assumes that\n\n* All subsets of a frequent itemset must be frequent\n* Similarly, for any infrequent itemset, all its supersets must be infrequent too\n\n**Step 1**: Create a frequency table of all the items that occur in all the transactions.\n\n**Step 2**: We know that only those elements are significant for which the support is greater than or equal to the threshold support.\n\n**Step 3**: The next step is to make all the possible pairs of the significant items keeping in mind that the order doesn\u2019t matter, i.e., AB is same as BA.\n\n**Step 4**: We will now count the occurrences of each pair in all the transactions.\n\n**Step 5**: Again only those itemsets are significant which cross the support threshold\n\n**Step 6**: Now let\u2019s say we would like to look for a set of three items that are purchased together. We will use the itemsets found in step 5 and create a set of 3 items.","57f39e7c":"## Association Mining","75fe4734":"**Importing libraries**","c375bf42":"The advantage of working with pandas DataFrames is that we can use its convenient features to filter the results. For instance, let's assume we are only interested in itemsets of length 2 that have a support of at least 80 percent. First, we create the frequent itemsets via apriori and add a new column that stores the length of each itemset:","d0475e7a":"## Using Transaction encoder","8a7770ef":"for market basket analysis import apriori algorithm and association rules from mlxtend.frequent_patterns"}}