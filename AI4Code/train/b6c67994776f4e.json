{"cell_type":{"61bc17a4":"code","f495beab":"code","f06b21ac":"code","ce1f92b8":"code","f19adb84":"code","2991501f":"code","3b76bc7f":"code","0f852169":"code","e0d856aa":"code","01b24bc4":"code","90484597":"code","045ca80a":"code","fd41cfd3":"code","55f61f20":"code","ecf3290d":"code","993e87ba":"code","445cac26":"code","8193052a":"code","28212feb":"code","2239cf09":"code","aa960a81":"code","f47d7b26":"code","8f844c3f":"code","568e5be3":"code","e5ea8bdb":"code","1386f150":"code","7b717d04":"code","11ba2a60":"code","25d957a1":"code","ea1eba70":"code","44d56dee":"code","63890597":"code","408a5fbb":"markdown","55e63ecf":"markdown","146ad50e":"markdown","379d651e":"markdown","a9788dda":"markdown","c3090623":"markdown","cf1d343d":"markdown","dffb2496":"markdown","4d8cad9d":"markdown","a2ddfb72":"markdown"},"source":{"61bc17a4":"%load_ext autoreload\n%autoreload 2\n\nimport random\nimport os\nfrom copy import copy\nfrom pathlib import Path\nfrom multiprocessing import Pool\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import cohen_kappa_score\n\nfrom fastai.tabular.transform import add_datepart, cont_cat_split\nfrom fastai.tabular.transform import FillMissing, Categorify, Normalize\nfrom fastai.layers import embedding\nfrom fastai.basic_data import DataBunch\nfrom fastai.basic_data import DatasetType\nfrom fastai.basic_train import Learner\nfrom fastai.basic_data import DataBunch\nfrom fastai.layers import LabelSmoothingCrossEntropy\nfrom fastai.metrics import KappaScore\n\nimport torch\nfrom torch.utils import data\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","f495beab":"MAX_SEQ_LEN = 300\nSEED = 420\nBATCH_SIZE = 64\nNUM_FOLDS = 5","f06b21ac":"seed_everything(SEED)","ce1f92b8":"DATA_PATH = Path('\/kaggle\/input\/')\nOUTPUT_PATH = Path('\/kaggle\/working\/')\n(OUTPUT_PATH\/'cache').mkdir(exist_ok=True)","f19adb84":"train_df = pd.read_feather(DATA_PATH\/'dsb-2019-eda-and-data-preparation\/train.fth')\ntest_df = pd.read_feather(DATA_PATH\/'dsb-2019-eda-and-data-preparation\/test.fth')\n\ntrain_labels = pd.read_feather(DATA_PATH\/'dsb-2019-eda-and-data-preparation\/train_labels.fth')\ntest_labels = pd.read_feather(DATA_PATH\/'dsb-2019-eda-and-data-preparation\/test_labels.fth')","2991501f":"NUM_LABELS = len(train_labels)","3b76bc7f":"train_df = add_datepart(df=train_df, field_name='timestamp', drop=False, time=True)\ntest_df = add_datepart(df=test_df, field_name='timestamp', drop=False, time=True)","0f852169":"continuous_features, categorical_features = cont_cat_split(train_df)","e0d856aa":"excluded_date_feats = set([\n    'timestampIs_month_end',\n    'timestampIs_month_start',\n    'timestampIs_quarter_end',\n    'timestampIs_quarter_start',\n    'timestampIs_year_end',\n    'timestampIs_year_start',\n    'timestampYear'\n])\n\ncategorical_features = [c for c in categorical_features if c not in excluded_date_feats]","01b24bc4":"continuous_features","90484597":"categorical_features","045ca80a":"categorical_features = [c for c in categorical_features if c not in ('game_session', 'installation_id', 'timestamp')]","fd41cfd3":"NUM_FEATS = len(continuous_features + categorical_features)","55f61f20":"fm = FillMissing(cat_names=copy(categorical_features), cont_names=copy(continuous_features))\nfm(train_df)\nfm(test_df, test=True)","ecf3290d":"cy = Categorify(cat_names=copy(categorical_features), cont_names=copy(continuous_features))\ncy(train_df)\ncy(test_df, test=True)","993e87ba":"nm = Normalize(cat_names=copy(categorical_features), cont_names=copy(continuous_features))\nnm(train_df)\nnm(test_df, test=True)","445cac26":"class TSDataset(data.Dataset):\n\n    def __init__(self, labels, df, max_seq_len=MAX_SEQ_LEN):\n        self.labels = labels\n        self.df = df\n        self.max_seq_len = max_seq_len\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, i):\n        row = self.labels.iloc[i]\n\n        X_clipped = self.df.loc[row.start_idx:row.end_idx, categorical_features + continuous_features].values\n        \n        X = np.zeros((self.max_seq_len, NUM_FEATS))\n        X_clipped = X_clipped[-self.max_seq_len:]\n\n        if len(X_clipped):\n            X[-len(X_clipped):] = X_clipped\n    \n        return Tensor(X).float(), row.accuracy_group","8193052a":"def _numericise_cats(df, cat_cols):\n    for col in cat_cols:\n        if df[col].dtype.name == 'category':\n            df[col] = df[col].cat.codes + 1\n  \n    return df","28212feb":"train_df = _numericise_cats(train_df[categorical_features + continuous_features].copy(), cat_cols=categorical_features)\ntest_df = _numericise_cats(test_df[categorical_features + continuous_features].copy(), cat_cols=categorical_features)","2239cf09":"def embedding_size_rule(number_categories):\n    return min(600, round(1.6 * number_categories**0.56))","aa960a81":"emb_sizes, cat_sizes = {}, {}\n\nfor col in categorical_features:\n    num_cats = train_df[col].nunique() + 1\n    emb_sizes[col] = embedding_size_rule(num_cats)\n    cat_sizes[col] = num_cats","f47d7b26":"emb_sizes","8f844c3f":"cat_sizes","568e5be3":"LSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n\n\nclass TimeSeriesLSTM(nn.Module):\n    def __init__(self, emb_drop=0.5, lstm_1_dropout=0.3, lstm_2_dropout=0.3):\n        super().__init__()\n\n        self.embeds = nn.ModuleList([\n            embedding(cat_sizes[cat], emb_sizes[cat])\n            for cat in categorical_features\n        ])\n        self.embedding_dropout = nn.Dropout(emb_drop)\n        \n        total_embeds = sum(emb_sizes.values())\n        \n        self.lstm1 = nn.LSTM(total_embeds + len(continuous_features), LSTM_UNITS, batch_first=True, bidirectional=True)\n        self.lstm1_dropout = nn.Dropout(lstm_1_dropout)\n\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, batch_first=True, bidirectional=True)\n        self.lstm2_dropout = nn.Dropout(lstm_2_dropout)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 4)\n        \n    def forward(self, x_input):\n        x_cat = x_input[:,:,:len(categorical_features)]\n        x_cont = x_input[:,:,len(categorical_features):]\n        \n        h_embedding = [e(x_cat[:,:,i].long()) for i, e in enumerate(self.embeds)]\n        h_embedding = torch.cat(h_embedding, 2)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        x_cat = torch.cat([h_embedding, x_cont], 2)\n        \n        h_lstm1, _ = self.lstm1(x_cat)\n        h_lstm1 = self.lstm1_dropout(h_lstm1)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        h_lstm2 = self.lstm2_dropout(h_lstm2)\n\n        avg_pool = torch.mean(h_lstm2, 1)\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n\n        h_conc_linear1 = F.relu(self.linear1(h_conc))\n        h_conc_linear2 = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        \n        return result","e5ea8bdb":"kappa = KappaScore()\nkappa.weights = \"quadratic\"","1386f150":"val_preds = np.zeros((len(train_labels), 4))\ntest_preds = np.zeros((NUM_FOLDS, len(test_labels), 4))","7b717d04":"test_ds = TSDataset(labels=test_labels, df=test_df)\ntest_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)","11ba2a60":"kfold = GroupKFold(n_splits=NUM_FOLDS)\n\nfor i, (train_idx, val_idx) in enumerate(kfold.split(train_labels, train_labels.accuracy_group, train_labels.installation_id)):\n    train_ds = TSDataset(labels=train_labels.iloc[train_idx], df=train_df)\n    val_ds = TSDataset(labels=train_labels.iloc[val_idx], df=train_df)\n\n    train_dl = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE)\n    valid_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n\n    databunch = DataBunch(train_dl=train_dl, valid_dl=valid_dl, test_dl=test_dl)\n\n    ts_nn = TimeSeriesLSTM()\n\n    learner = Learner(\n        data=databunch, model=ts_nn, metrics=[kappa], loss_func=LabelSmoothingCrossEntropy())\n    \n    learner.fit_one_cycle(4, max_lr=1e-02)\n    \n    val_preds_fold, val_y_fold = learner.get_preds(ds_type=DatasetType.Valid)\n    val_preds[val_idx] = torch.softmax(val_preds_fold, 1).numpy()\n    \n    test_preds_folds, _ = learner.get_preds(ds_type=DatasetType.Test)\n    test_preds[i] = torch.softmax(test_preds_folds, 1).numpy()\n    \n    learner.save(f'lstm_fold_{i}')","25d957a1":"cohen_kappa_score(train_labels.accuracy_group.values, np.argmax(val_preds, 1), weights='quadratic')","ea1eba70":"np.save(OUTPUT_PATH\/'val_preds', val_preds)","44d56dee":"all_test_preds = np.mean(test_preds, 0)","63890597":"np.save(OUTPUT_PATH\/'test_preds', all_test_preds)","408a5fbb":"## Preds","55e63ecf":"## Params","146ad50e":"Exclude date features we don't have enough data to represent.","379d651e":"## Preparation\n\nThere's a little bit more preparation that didn't make sense to do in the last kernel.\n\n1. Convert timestamp into date and time features like hour, minute, day, month and so on.\n2. Replace missing values with the mean and include a column that denotes whether a column was missing or not.\n3. Normalise continuous values to have mean of 0 and a std of 1.","a9788dda":"## Training","c3090623":"# DSB 2019: LSTM Approach\n\nIn this kernel, I'm going to look at a sequence model approach to this problem. My goal is to build a model that requires the minimal feature engineer, as a means of learning about the features and their relationship to the output variable.\n\nThe model is heavily based on the [LSTM model from the Jigsaw Toxicity](https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version) competition. With a major modification to change it to support feature embeddings for categorical features and continuous values for numerical features. \n\nIt uses the outputs from [this](https:\/\/www.kaggle.com\/lextoumbourou\/dsb-2019-eda-and-data-preparation) kernel.","cf1d343d":"## Model","dffb2496":"Filter out some features that will be used to prepare data for the model, but not for training.","4d8cad9d":"## Val CV","a2ddfb72":"## Data\n\nI'm creating a custom dataset which returns numericed sequences capped at max_seq_len."}}