{"cell_type":{"195ba012":"code","0b37f2d5":"code","359affe0":"code","00e75e9c":"code","92a37873":"code","f3bdb844":"code","0d84f78d":"code","d7f78a17":"code","ad8c5748":"code","36311ba9":"code","96ccefd4":"code","e0b5e30d":"code","9cfcca3c":"code","2bff5731":"code","ad2bffa9":"code","aeb04785":"code","316acbdd":"code","ed0cc905":"code","525b799c":"code","771dccb8":"code","eba9d6c6":"code","b018af87":"code","d2f13122":"code","1a515735":"code","72328fa4":"code","aa7ae932":"code","a2c790d0":"code","c5ee5028":"code","32bb6e64":"code","81d43920":"code","5818ff6e":"code","9b16e09e":"code","bd7959b9":"code","bfa11a13":"code","faf39a8b":"code","90e2443d":"code","5b09756c":"code","724be560":"code","745a2536":"code","a4480c7e":"code","a98e4116":"code","d8085a4e":"code","58a8fb63":"code","010936d8":"code","e32a0222":"code","510c3082":"code","6426b304":"code","46b73a26":"code","8d16eab3":"code","bdb15119":"code","948c8a8b":"code","7a172fef":"code","dc72a8e7":"code","6c4ad7c6":"markdown","5006f238":"markdown","39dc5467":"markdown","f334574c":"markdown","73272fcd":"markdown","dd738be2":"markdown","927703ad":"markdown","fe64435c":"markdown","4b8ee5c8":"markdown","7e3d3b90":"markdown","db0d314b":"markdown","3e45ed4f":"markdown","a1c69394":"markdown","e8f524d6":"markdown","07e153c9":"markdown","fe73b952":"markdown","dd8c42a2":"markdown","bf72fa64":"markdown","b3e8f424":"markdown","94544c75":"markdown","82314a31":"markdown","48fe0df6":"markdown","89232bf8":"markdown","3ccfd92f":"markdown","55510592":"markdown","dbedb4e3":"markdown","3de51e1a":"markdown","45fd37d9":"markdown","ff7acfa0":"markdown","d3238b28":"markdown","3334381c":"markdown","7fd5974a":"markdown","0f288108":"markdown"},"source":{"195ba012":"import numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import RidgeClassifier\nsb.set(font_scale=1) ","0b37f2d5":"# Load data\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_train_org = df_train\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_full = pd.concat([df_train,df_test])\ndf_train.info()\ndf_test.info()","359affe0":"# Embarked has 2 missing values\ndf_train[df_train['Embarked'].isnull()]","00e75e9c":"# Show embarked values\ndf_train['Embarked'].unique()# Show embarked values\nprint(df_full['Embarked'].unique())\n# Distribution of Embarked vs fares\nsb.boxplot(x=\"Embarked\", y=\"Fare\",\n            hue=\"Pclass\", palette=[\"m\", \"g\"],\n            data=df_full)","92a37873":"df_train.loc[df_train['Embarked'].isnull(),'Embarked'] = 'C'","f3bdb844":"df_train.Cabin.fillna('N',inplace=True)\ndf_test.Cabin.fillna('N',inplace=True)","0d84f78d":"# Missing fare in test set\ndf_test[df_test['Fare'].isnull()]","d7f78a17":"#sb.boxplot(x=\"Pclass\", y=\"Fare\",\n #            palette=[\"m\", \"g\"],\n  #          data=df_full)\n\n#guess_class_3_fare = df_full[(df_full['Pclass']==3) & (df_full['Fare'].notnull())]['Fare'].mean()\n#print(guess_class_3_fare)\n#df_test.Fare.fillna(guess_class_3_fare,inplace=True)","ad8c5748":"# See distribution of fare in class 3\ndf_full = pd.concat([df_train, df_test])\nclass_3_fare = df_full[df_full.Pclass==3]['Fare']\nax = sb.distplot(class_3_fare)","36311ba9":"\nguess_value = float(class_3_fare.mode())\ndf_test.Fare.fillna(guess_value, inplace=True)\ndf_test.loc[df_test['Fare'].isnull(),'Fare'] = guess_value","96ccefd4":"# Find family size\ndf_train['FamSize'] = df_train['Parch'] + df_train['SibSp'] + 1\ndf_test['FamSize'] = df_test['Parch'] + df_test['SibSp'] + 1\n# Fare per person\ndf_train['FarePp'] = df_train['Fare']\/df_train['FamSize']\ndf_test['FarePp'] = df_test['Fare']\/df_test['FamSize']\n# IsAlone flag\ndf_train['IsAlone'] = 0\ndf_train.loc[df_train['FamSize']==1,'IsAlone'] = 1\ndf_test['IsAlone'] = 0\ndf_test.loc[df_test['FamSize']==1,'IsAlone'] = 1","e0b5e30d":"# Get family\/surname\nsplit_cols = df_train['Name'].str.split(',')\nsurnames = []\nfor row in range(0, split_cols.shape[0]):\n    name = split_cols[row][0]\n    surnames.append(name)\ndf_train['Surname'] = surnames\n\nsplit_cols = df_test['Name'].str.split(',')\nsurnames = []\nfor row in range(0, split_cols.shape[0]):\n    name = split_cols[row][0]\n    surnames.append(name)\ndf_test['Surname'] = surnames\n#df_train['Surname'] =df_train['Surname'][0] ","9cfcca3c":"# Check if the fare is per person or for the whole family\nfig, axes = plt.subplots(nrows=2,figsize=(10,10))\nsb.scatterplot(y='FamSize',x='Fare',data=df_train, ax=axes[0])\naxes[0].set_title('Unmodified Fare values')\nsb.scatterplot(y='FamSize',x='FarePp',data=df_train,ax=axes[1])\naxes[1].set_title('Per person Fare values')","2bff5731":"# Check the fare outliner\ndf_train[df_train['Fare']>500]","ad2bffa9":"# Remove the outliners\n#df_train = df_train[df_train['Fare']<500]","aeb04785":"# Extract the first letter of the cabins\ndf_train['CabinClass'] = df_train['Cabin'].astype(str).str[0]\ndf_test['CabinClass'] = df_test['Cabin'].astype(str).str[0]","316acbdd":"# Check fare sorted by cabin class\ndf_train[df_train['CabinClass']=='B'].sort_values(['Cabin','CabinClass','Name'])\n# ","ed0cc905":"# Create a flag for people where the rest of the family died\ndf_train['AllDied'] = 1\ndf_test['AllDied'] = 1\ndf_train['AllSurvived'] = 0\ndf_test['AllSurvived'] = 0\ndf_train =df_train.reset_index()\ndf_test=df_test.reset_index()\nfor i in range(0,df_train.shape[0]):\n    \n    name = df_train.loc[i,'Surname']\n    survives = df_train.loc[df_train['Surname']==name,'Survived'].sum() - df_train.loc[i,'Survived']\n    if survives > 0:\n        df_train.loc[i,'AllDied'] = 0\n        df_test.loc[df_test['Surname']==name,'AllDied'] = 0 \n    elif df_train.loc[i,'Survived'] == 1: # only the person survived\n        df_test.loc[df_test['Surname']==name,'AllDied'] = 0 \n    if survives == (df_train.loc[i,'FamSize'] - 1): # rest of family survived\n        df_train.loc[i,'AllSurvived'] = 1\n        if df_train.loc[i,'Survived'] == 1: # The person survived as well\n            df_test.loc[df_test['Surname']==name,'AllSurvived'] = 1","525b799c":"import re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\. ', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nall_data = [df_train,df_test]\nfor data in all_data:\n    data['Title'] = data['Name'].apply(get_title)\n\nfor data in all_data:\n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],'Rare')\n    data['Title'] = data['Title'].replace('Mlle','Miss')\n    data['Title'] = data['Title'].replace('Ms','Miss')\n    data['Title'] = data['Title'].replace('Mme','Mrs')\n    \nprint(pd.crosstab(df_train['Title'], df_train['Sex']))\nprint(\"----------------------\")\nprint(df_train[['Title','Survived']].groupby(['Title'], as_index = False).mean())","771dccb8":"df_train.Ticket.head()","eba9d6c6":"def get_ticket_type(ticket):\n    type_search = re.search('^([A-Za-z]+)', ticket)\n    if type_search:\n        return type_search[0]\n    return \"\"\nfor data in all_data:\n    data['TicketType'] = data['Ticket'].apply(get_ticket_type)\nfor data in all_data:\n    data['SpecialTicket'] = 0\n    data.loc[data['TicketType'] != '','SpecialTicket'] = 1\n#df_train[['SpecialTicket','TicketType']]\ndf_train.to_csv('temp.csv')","b018af87":"# Use title to predict age\ndf_full = pd.concat([df_train,df_test])\ndf_full.info()\ntitles = df_full['Title'].unique().tolist()\nfor title in titles:\n    df_title = df_full[df_full['Title']==title]\n    age = df_title[df_title['Age'].notnull()]['Age'].median()\n    df_train.loc[(df_train['Title']==title) & (df_train['Age'].isnull()),'Age'] = age\n    df_test.loc[(df_test['Title']==title) & (df_test['Age'].isnull()),'Age'] = age\n    print(title,': ',age)","d2f13122":"sb.barplot(x='TicketType',y='Survived',data=df_train)","1a515735":"sb.barplot(x='Title',y='Age',data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","72328fa4":"sb.barplot(x='Title',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","aa7ae932":"sb.barplot(x='Pclass',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","a2c790d0":"sb.barplot(x='Sex',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","c5ee5028":"# Age and sex \nfig, axes = plt.subplots(nrows=2,figsize=(6,8))\nsb.kdeplot(df_train[(df_train['Survived']==1) & (df_train['Sex']=='male')]['Age'],label='Male',\n    shade=True,ax=axes[0])\nsb.kdeplot(df_train[(df_train['Survived']==1) & (df_train['Sex']=='female')]['Age'],label='Female',\n    shade=True,ax=axes[0])\naxes[0].set_title('Survived')\n\nsb.kdeplot(df_train[(df_train['Survived']==0) & (df_train['Sex']=='male')]['Age'],label='Die, Male',\n    shade=True,ax=axes[1])\nsb.kdeplot(df_train[(df_train['Survived']==0) & (df_train['Sex']=='female')]['Age'],label='Die, Female',\n    shade=True,ax=axes[1])\naxes[1].set_title('Die')  ","32bb6e64":"sb.barplot(x='SibSp',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","81d43920":"sb.barplot(x='Parch',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","5818ff6e":"sb.barplot(x='TicketType',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","9b16e09e":"sb.barplot(x='Embarked',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","bd7959b9":"sb.barplot(x='IsAlone',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","bfa11a13":"sb.barplot(x='AllDied',y='Survived', hue='Sex',data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","faf39a8b":"sb.barplot(x='AllSurvived',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","90e2443d":"sb.barplot(x='SpecialTicket',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","5b09756c":"sb.barplot(x='SibSp',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","724be560":"sb.barplot(x='Parch',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","745a2536":"sb.barplot(x='CabinClass',y='Survived', data=df_train,\n            linewidth=3,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 2)","a4480c7e":"# Code the sex column\ndf_train.loc[df_train['Sex']=='male','Sex'] = 1\ndf_train.loc[df_train['Sex']=='female','Sex'] = 0\ndf_test.loc[df_test['Sex']=='male','Sex'] = 1\ndf_test.loc[df_test['Sex']=='female','Sex'] = 0","a98e4116":"df_train['Sex'] = df_train['Sex'].astype(int)\ndf_test['Sex'] = df_test['Sex'].astype(int)","d8085a4e":"_ , ax = plt.subplots(figsize =(14, 12))\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsb.set_style('whitegrid')\nsb.heatmap(df_train.corr(),\nannot=True,\nlinewidths=.2, \n            ax = ax,\n            linecolor='white',\n            cmap = 'RdBu',\n            mask = mask,\n            fmt='.2g',\n            center = 0,\n            #square=True\n            )","58a8fb63":"cat_cols = ['Embarked','Title','Pclass','TicketType']\ndf_train = pd.get_dummies(df_train, columns=cat_cols, drop_first=False)\ndf_test = pd.get_dummies(df_test, columns=cat_cols, drop_first=False)","010936d8":"df_train.info()","e32a0222":"dropped_cols = ['index','PassengerId','Name','Fare','Ticket','Cabin','Surname','SpecialTicket','FamSize','AllSurvived','CabinClass']\ndf_train.drop(dropped_cols,axis=1,inplace=True)\ntest_id = df_test['PassengerId']\ndf_test.drop(dropped_cols,axis=1,inplace=True)","510c3082":"X = df_train.drop(['Survived'],axis=1)\ny = df_train['Survived']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .3, random_state=3\n)","6426b304":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#X_train_scale = scaler.fit_transform(X_train)\n#X_test_org = X_test\n#X_test = scaler.transform(X_test)","46b73a26":"model = GaussianNB()\nmodel = XGBClassifier(learning_rate=0.02, gamma=0.3, n_estimators=140, objective='binary:logistic',\n                    silent=True, nthread=1,min_child_weight=1,max_depth=2,\n                    colsample_bytree= 0.8, subsample= 0.7)#learning_rate=0.05,max_depth=4, n_classifier)\n\neval_set = [(X_train, y_train), (X_test, y_test)]\nmodel.fit(X_train,y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=False)\n\nimport matplotlib.pyplot as plt\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Validation')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Validation')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","8d16eab3":"from sklearn.metrics import accuracy_score\ny_pred = model.predict(X_test)\n#y_pred = y_pred[:,0].tolist()\npredictions = [round(value) for value in y_pred]\n#predictions= predictions or X_test_org['Title_Master'].tolist()\n#predictions = [ x|y for (x,y) in zip(predictions, X_test['SpecialTicket'].tolist() )]\n#predictions= predictions or X_test_org['Title_Mrs'].tolist()\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","bdb15119":"df_check = X_test.copy()\ndf_check['Real'] = y_test \ndf_check['Pred'] = predictions","948c8a8b":"mismatch = df_check[df_check['Real'] != df_check['Pred']]\nmismatch = df_train_org.loc[mismatch.index]\nmismatch.to_csv('wrong.csv')","7a172fef":"df_test.fillna(0,inplace=True)\ndf_test.describe()","dc72a8e7":"# Make prediction on test set\ndf_test['CabinClass_T'] = 0\ndf_test = df_test.reindex(columns=X.columns)\ndf_test_np = df_test #.to_numpy()\n#X_final_test = scaler.fit_transform(df_test_np)\nX_final_test = df_test_np\nfinal_pred = model.predict(X_final_test)\n#final_pred = final_pred[:,0].tolist()\nfinal_predictions = [round(value) for value in final_pred]\n#predictions= predictions or X_test_org['Title_Master'].tolist()\nsubmission_headers = ['PassengerId','Survived']\nsubmissions = pd.DataFrame(list(zip(test_id,final_predictions)),columns=submission_headers)\nsubmissions.to_csv('submission_final.csv',index=False)","6c4ad7c6":"### Chapter 4: Prepare data for training\n\nThis is the standard step for machine learning, creating dummy variables, converting datatype etc, normalisation if required for certain algorithms.","5006f238":"Let's remind ourselves what columns are available","39dc5467":"One thing that's unclear is whether the fare is per person or the whole family, so we do some sanity check to decide which way to go.","f334574c":"There are a lot of cabin values missing, does it signify people who don't have private cabins? We change Null to a special class N.","73272fcd":"### Let's start by importing the necessary libraries.","dd738be2":"Split the data into train and test set","927703ad":"Let's look at the very high fares on the right. We discover the second row (2 member family), where the Fare exactly match those of the individual fare. This suggests the fare is not total for the family?\n\nThis is still not compelling evidence, so we dig deeper.","fe64435c":"Check fare sorted by cabin class. \nManually check a few family, the fare in class B increases in line with number of cabins\nThe fare is probably a combination of number of people + number of cabins\n\nTherefore, using fare per person is not perfect but will be a more reasonable way to classify spending on fare.","4b8ee5c8":"> ## Chapter 1: Missing values","7e3d3b90":"The features that show strong correlation with the labels are: \n\nThey are the first candidates to consider including in the training set. Again, it's just an educated guess and it's worthwhile to experiment with different combinations if you have the time.","db0d314b":"We use the first letter of the cabin number to group people from the same area on the ship.","3e45ed4f":"Remember all the missing ages in the dataset? We should fill that in somehow.\n\nThis is where I disagree with a lot of solutions out there. Some use a random gaussian generator with the mean and standard deviation of the age distribution to find the missing values.\n\nSome others use decision tree with all the other features to guess the age.\n\nI find the closest feature that we can use to predict the age while avoiding creating correlation with every other features is the Title. So here we assign the median age for each title to the missing values.","a1c69394":"#### Getting to top 4% of Titanic dataset competition with feature engineering and XGBoost\n\nThe classic Titanic machine learning competition is an fantastic way to apply your machine learning skills and compare your work with others.\n\nEven though the full dataset is available and you can cheat your way to perfect score, it is very satisfying to compete fairly and achieve good result.\n\nIn this notebook, I focus on the preprocessing aspect of machine learning, i.e., the cleaning and feature engineering of the dataset, which resulted in a top 5% position in the leaderboard with XGBoost.\n\nThis dataset is an excellent example to illustrate the power of understanding your dataset and making it more useful before diving into specific ML algorithms.\n\nThe notebook is divided into five parts:\n- **Chapter 1: Missing values**\n- **Chapter 2: Feature engineering**\n- **Chapter 3: Assessing the features**\n- **Chapter 4: Prepare data for training**\n- **Chapter 5: Build a model to predict Titanic survival**","e8f524d6":"We extract the title of the passangers. This is a popular piece of code for the particular dataset, I don't know who wrote it originally. ","07e153c9":"There are many avenue to explore to improve the model. You can:\n\n- Fine tune the hyper parameter of the model.\n- Get more data.\n- Try different algorithms.\n- Use an ensemble of different algorithms.\n- Use piecewise prediction, i.e., use a different algorithm on a certain population of the data to account for local variation.\n\nHowever, to avoid blindly doing experiments, it is important that we look at what our current method is getting wrong.\n\nThis part export the examples that we got wrong in the test set. It can help with feature selection and tuning.","fe73b952":"Drop columms that are not useful","dd8c42a2":"Finally, we apply the model on the submission set. \n\nNote that since the submission set may not have all the variations in the categorical features (cabin class etc.) so we need to fill in the Null with zeros.","bf72fa64":"Next, we deal with a missing Fare in the test set. We use the Ticket class (Pclass) to guess the fare value.","b3e8f424":"There are two missing Embarked values in the train set. Let's check them out.","94544c75":"### Chapter 5: Build a model to predict Titanic survival\n\nCreate a model with XGBoost and plot the error and loss aganist number of estimators","82314a31":"THe fare of 80 will likely land in Cherbourg (C). So we will Set the 2 missing embarked as 'C'. There is no embarked missing in test set.","48fe0df6":"Consider normalisation\/standardisation if required by the algorithm","89232bf8":"We load the data and have a look. Wow, so many missing values (Null)!\n\nWhat can I say, life is sad, and missing data is just a little dune in the sadness landscape.\n\nBut hey, a tiny bit of us data scientists is a naive bayesian, right? So we put our positive hat on and deal with those nulls.\n","3ccfd92f":"Feature engineering and selection is a very important and powerful step in developing machine learning model.\n\nThere are a lot of literature out there about this topic, and unfortunately it's rare that we have the time to explore them all for every problem.\n\nThis notebook got me into the top 5% of the leaderboard. Hopefully, you have found this useful in your journey of learning and practicing machine learning!","55510592":"Contemplating removing the outliners. This might be beneficial or detrimental to the performance, so it's also best to test.","dbedb4e3":"They are in the same cabin, and have no family otherwise (friends?).\n\nWe will try to guess where they embarked from based on the fare they paid and their fare class.","3de51e1a":"Next, we create a flag indicating whether the rest of the family died. The theory for this feature is if your whole family died, you probably did not survive either.","45fd37d9":"Print the accuracy on the test (CV) set","ff7acfa0":"The tickets have a letter prefix, which might be useful to tell the different types of ticket. So we extract that. ","d3238b28":"### Chapter 3: Assessing the features\n\nWe plot the features against the labels to get a feel of what contribute to the survival on the ship.","3334381c":"## Chapter 2: Feature engineering\n\nWe create a bunch of features based on existing fields. The idea is to transform the information into more revealing features that would help with the classfication.\n\nWe do not necessarily use all the new features we create. Having them means we can test to see what is useful (for a particular algorithm).\n\nIt's a good spot to mention the concept of feature **Relevance vs Usefulness**.\n\nSimply put, a feature is relevant if by itself it contains information that improves classification. On the other hand, a feature is useful if it improves classfication even though by itself it does not contain any information (e.g., a column of all zeros).\n\nIt certainly sounds strange that a feature with no information (correlation, entropy etc.) can be useful. This has to do with particular algorithms you use, which might benefits from the inclusion of a feature with little information.\n\nPut it another way, **relevance** is about information, while **usefulness** is about effect on errors.\n\nIt's a very long way to say that when you create new features, it's hard to know what might be useful later. You can make an educated guess based on their correlation with the labels, but ultimately you want to systematically test them out along with your algorithms.\n\nWith all that over with, first, we create a few features to cover Family Size, Fare per person and a flag to indicate whether a passanger had no family on the trip.","7fd5974a":"![](http:\/\/)Set the missing fare to be the mode of ll class 3 fares. Note we use all values from the train and the test sets. The passanger also had no companion on the ship.","0f288108":"We extract the surnames of the passangers. "}}