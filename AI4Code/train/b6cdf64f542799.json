{"cell_type":{"511bfd53":"code","dec2bf18":"code","f00ed6f3":"code","fc4635fc":"code","0a5174d9":"code","cc73b9a0":"code","291103b6":"code","7f4d293c":"code","95c21866":"code","3bc634b3":"code","f3493dc5":"code","9371a9ba":"code","c2087fa7":"code","d6fdddd2":"code","9f4f9a57":"code","740951b5":"code","c294724b":"code","c0e1bcdc":"code","530423fc":"code","cabb3abc":"code","bb571f98":"code","1a5bd8dd":"code","c2e7986b":"code","d9fdf800":"code","91790a7c":"code","2a0f2184":"code","5c0fd03e":"code","f1f22fd8":"code","4c730dbc":"code","ce6fdc6b":"code","17f3e802":"code","be1c17ae":"code","ed95ea9d":"code","fa148838":"code","30daba33":"code","9f034b8e":"code","63d95c8b":"code","ee043360":"code","6b04ce28":"code","83608bb0":"code","eb3c66e6":"code","64ccedd4":"code","4bd01c01":"code","a3849bba":"code","b536cc08":"code","6d0c2676":"code","82da0ffb":"markdown","834e54c2":"markdown","71748f23":"markdown","91f68487":"markdown","e1cc6105":"markdown","9d8428ef":"markdown","3234ff8e":"markdown","168619c6":"markdown","c8b17116":"markdown","1facb230":"markdown","4aa093fd":"markdown","9d864abf":"markdown","37c6cad4":"markdown","eb2acfd3":"markdown","de95b004":"markdown","22382c12":"markdown","16ad7af4":"markdown","a7a79d66":"markdown","e34913fb":"markdown","b7c5f4f4":"markdown","61764d04":"markdown","13f0de9b":"markdown","51476bab":"markdown","c9dae3ed":"markdown","03524147":"markdown","e1b0b873":"markdown","f1b987f8":"markdown","5e1c1cc0":"markdown","4e0f72bb":"markdown","4f2b179b":"markdown","1334aaed":"markdown","7b013d14":"markdown","c1439ef1":"markdown","fbc485ae":"markdown","f9c92ec9":"markdown","b179a113":"markdown","f1d12bed":"markdown"},"source":{"511bfd53":"import pandas as pd\nimport numpy as np\ndatapath = \"..\/input\/train.csv\"\ndf = pd.read_csv(datapath)\n\n# print column names\nprint(df.columns.values)\n\n# look at the first 5 examples + column names\ndf.head()","dec2bf18":"\n#print the data types and quantities in each column:\ndf.info()","f00ed6f3":"# `describe` reveals some statistics about our numeric columns.\ndf.describe()","fc4635fc":"# the 'O' option gives us some important info about nominal columns.\ndf.describe(include='O')","0a5174d9":"# remove columns we won't use\ndf = df.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\ndf.head()","cc73b9a0":"# first things first:  pairwise correlation of the numeric fields in df\ndf.corr()","291103b6":"# What is the sample likelihood of survival for different passenger classes\ndf[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7f4d293c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# what are the frequencies of Death and Survival given Passenger Class?\ng = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Pclass')","95c21866":"# what are the frequencies of Death and Survival given Age?\ng = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","3bc634b3":"df['AgeRange'] = pd.cut(df['Age'], [0, 18, 35, 50,100], labels=[1, 2, 3, 4], include_lowest=True, right=True).astype(np.float)\ndf.head(10)","f3493dc5":"# What is the sample likelihood of survival for different Age Ranges\ndf[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9371a9ba":"# what are the frequencies of Death and Survival given Age range?\ng = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'AgeRange')","c2087fa7":"# What is the sample likelihood of survival for different AgeRange?\ndf[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d6fdddd2":"# What is the sample likelihood of survival for different SibSp?\ndf[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9f4f9a57":"# what are the frequencies of Death and Survival given SibSp?\ng = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'SibSp')","740951b5":"# What is the sample likelihood of survival for different Parch?\ndf[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c294724b":"# what are the frequencies of Death and Survival given Parch?\ng = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Parch')","c0e1bcdc":"# what are the frequencies of Death and Survival given Fare?\ng = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","530423fc":"g = sns.FacetGrid(df, col='Pclass', row='Survived')\ng.map(plt.hist, 'Fare', )","cabb3abc":"# Bin the Fare into a FareClass column\ndf['FareClass'] = pd.cut(df['Fare'], [0, 50, 150, 275,1000], labels=[1, 2, 3, 4], include_lowest=True, right=True).astype(np.int8)\ndf.head()","bb571f98":"# What is the sample likelihood of survival for different Parch?\ndf[['FareClass', 'Survived']].groupby(['FareClass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1a5bd8dd":"# What is the sample likelihood of survival for different Sex?\ndf[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c2e7986b":"# what are the frequencies of Death and Survival given Sex and Pclass?\ng = sns.FacetGrid(df, col='Pclass', row='Sex')\ng.map(plt.hist, 'Survived')","d9fdf800":"# what are the frequencies of Death and Survival given Fare?\ng = sns.FacetGrid(df, col='Survived', row='Sex')\ng.map(plt.hist, 'Survived', bins=20)","91790a7c":"# How does Embarkation and Sex compare to Survival?\ng = sns.FacetGrid(df, col='Embarked', row='Sex')\ng.map(plt.hist, 'Survived')","2a0f2184":"# How does Embarkation and FareClass compare to Survival?\ng = sns.FacetGrid(df, col='FareClass', row='Embarked')\ng.map(plt.hist, 'Survived')","5c0fd03e":"# Extract title from name using the fact that titles end with period.\ndf['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# lets see if we have one for every row (there are 891 rows).\ndf['Title'].shape","f1f22fd8":"# how many different values are there?\n# crosstab the result with the Sex column to see how they are realated\ntitles = pd.crosstab(df['Title'], df['Sex'])\nprint(titles.shape)\ntitles","4c730dbc":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf.head()","ce6fdc6b":"# What is the sample likelihood of survival for different Family Sizes?\ndf[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","17f3e802":"import numpy as np\ndf['Gender'] = df['Sex'].map({'male':0, 'female':1}).astype(np.uint8)\ndf['Depart'] = df['Embarked'].map({'S':1, 'C':2, 'Q':3}, na_action='ignore').astype(np.float)\ntitleArr = df['Title'].unique()\nmapping = {v: k for k, v in dict(enumerate(titleArr)).items()}\ndf['NamePrefix'] = df['Title'].map(mapping, na_action='ignore').astype(np.uint8)\ndf.head(10)","be1c17ae":"df1 = df.drop(['Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title'], axis=1)\ndf1.head(10)","ed95ea9d":"df1.info()","fa148838":"import numpy as np\n# code to support knn imputation\ndef euclidean_distance(a, b):\n    c = a-b\n    return np.sqrt(c.dot(c))\n\n# test\nrow1 = df1.iloc[0]\nrow2 = df1.iloc[1]\nprint(euclidean_distance(row1, row2))\nprint(euclidean_distance(row1, row1))\n\n# test\n# apply euclidean distance to every pair of rows\n# return the (dis)similarity matrix with the distances between each pair of rows\ndef distance_matrix(df, distance_measure = euclidean_distance):\n    x = df1.drop(['Survived'], axis=1).fillna(0).values\n    print(x.shape)\n    m = np.matrix(np.zeros(shape=(x.shape[0],x.shape[0])))\n    for i in range(x.shape[0]):\n        for j in range(i, x.shape[0]):\n            m[i,j] = euclidean_distance(x[i], x[j])\n    return m + m.T\n\n# m is the (dis)similarity matrix with the distances between each pair of rows\nm = distance_matrix(df1)\n#print(m[:10,:10])\n\n# remove all indexes that are nan on the field we want\nAgeRange_nan_indexes = df1[df1['AgeRange'].isnull()].index.values\n#print(AgeRange_nan_indexes)\nm1 = np.delete(m, AgeRange_nan_indexes, axis=0)\nprint(m1.shape)\n#print(np.sort(m1[:,5])[:5,0])\n\ndef get_k_nn_indexes(m, df_row, k):\n    idxs = np.argsort(m[:,df_row], axis = 0)\n    return idxs[:k,0]\n\n# test\n#print('get_k_nn_indexes(m1, 5, 9)')\n#print(get_k_nn_indexes(m1, 5, 9))\n\ndef get_k_nn_values(df, idxs, col):\n    icol = df.columns.get_loc(col)\n    return df.values[idxs, icol]\n\n# test\nidxs = get_k_nn_indexes(m1, 5, 15)\n#print('get_k_nn_values(df1, idxs, \"AgeRange\")')\n#print(get_k_nn_values(df1, idxs, 'AgeRange'))\n\n# select the most common value for the missing data among the top k samples\ndef select_best(knns):\n    dfknn = pd.DataFrame(knns, columns=['values'])\n    return dfknn['values'].value_counts().index[0]  # pick the top count of knns\n\n# test\nknns = get_k_nn_values(df1, idxs, 'AgeRange')\n#print('select_best(knns)')\n#print(select_best(knns))\n\n# replace nan values in a dataframe\ndef replace_value(df, col, indexes, values):\n    d = dict(zip(indexes, values))\n    return df[col].fillna(d)\n\n# test\n# impute nans for a column\nvalues = []\nfor idx in AgeRange_nan_indexes:\n    impute_idxs = get_k_nn_indexes(m1, idx, 7)\n    knns = get_k_nn_values(df1, impute_idxs, 'AgeRange')\n    best = select_best(knns)\n    values.append(best)\n    \n#print(values)\ndf2 = df1\ndf2['AgeRange'] = replace_value(df1, 'AgeRange', AgeRange_nan_indexes, values)\ndf2.info()\n#df2[df2.isnull()].shape\ndf2\n\ndef impute_knn(m, df, column, nan_indexes, k):\n    values = []\n    for idx in nan_indexes:\n        impute_idxs = get_k_nn_indexes(m, idx, k)\n        knns = get_k_nn_values(df, impute_idxs, column)\n        best = select_best(knns)\n        values.append(best)\n    return values\n\n#test\nDepart_nan_indexes = df1[df1['Depart'].isnull()].index.values\nv = impute_knn(m1, df1, 'Depart', Depart_nan_indexes ,11)\ndf2['Depart'] = replace_value(df1, 'Depart', Depart_nan_indexes, v)\ndf2.info()","30daba33":"# To find the knn of a row, pick the column of m corresponding to a row with a missing value, \n#  sort the values of that vector ascending, then pick the top k values from the list.\n#  Note that if the row index < k, you should pick k+1 rows and throw out the top value\n#  since it will correspond to the row being selected.  Next pick the indexes of the top k \n#  values and get those rows from the dataframe.  Use the values in the columns in \n#  question to impute the missing values.\ndef impute_nans(df, cols, k=9):\n    m = distance_matrix(df)\n    for col in cols:\n        # get the indexes to rows with nan entries in this column\n        nan_indexes = df[df[col].isnull()].index.values\n        #remove those rows from the m1 matrix\n        m1 = np.delete(m, nan_indexes, axis=0)\n        nan_values = impute_knn(m1, df, col, nan_indexes, k)\n        df[col] = replace_value(df, col, nan_indexes, nan_values)\n        \n# test\nimpute_nans(df1, ['AgeRange', 'Depart'], k=11)\ndf1.head()","9f034b8e":"g = sns.FacetGrid(df, row='FamilySize')\ng.map(plt.hist, 'Survived')","63d95c8b":"g = sns.FacetGrid(df, row='AgeRange')\ng.map(plt.hist, 'Survived')","ee043360":"g = sns.FacetGrid(df, row='NamePrefix')\ng.map(plt.hist, 'Survived')","6b04ce28":"# show a scattermatrix and a correlation matrix\ndf1.corr()","83608bb0":"import matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(df1, figsize=(15,15), diagonal='kde')","eb3c66e6":"# train and test - we will use scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom patsy import dmatrices\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ny,X = dmatrices('Survived ~ Pclass + AgeRange + FareClass + FamilySize + Gender + Depart + NamePrefix', \n                df1, return_type=\"dataframe\")\ny = np.ravel(y)\n\n# in this case we hold out \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n\n# create a logistic regression model\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train,y_train)\nmodel_lr.score(X_test, y_test) # check accuracy against the test data\n","64ccedd4":"# Lets try Support Vector Machine.\nfrom sklearn import svm\n\nmodel_svc = svm.SVC(kernel='rbf', C=1)\nmodel_svc.fit(X_train, y_train)\nmodel_svc.score(X_test, y_test)","4bd01c01":"# Now for a Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train, y_train)\nmodel_rf.score(X_test, y_test)","a3849bba":"# logistic regression\nscores_lm = cross_val_score(model_lr, X, y, cv=5)\nprint(scores_lm)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_lm.mean(), scores_lm.std() * 2))","b536cc08":"# Support vector machine\nscores_svc = cross_val_score(model_svc, X, y, cv=5)\nprint(scores_svc)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_svc.mean(), scores_svc.std() * 2))","6d0c2676":"# Random Forest\nscores_rf = cross_val_score(model_rf, X, y, cv=5)\nprint(scores_rf)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))","82da0ffb":"### Visualizations\nNow that we have cleaned up the data, we can look at the relationships of the data to survival again using some of the visualizations we've seen before.","834e54c2":"#### Embarked","71748f23":"Soon we will look at whether or not this field has any relationship with survival.  For the moment we will just hang onto this field. ","91f68487":"### Numeric Fields\n\nNow we want to do a few things to change the remaining nominal data to numeric codes.  This is helpful for a couple of the decision algorithms we might choose to use.  \n\n* A numeric field called `Gender` to replace `Sex`\n* A numeric field called `Depart` to replace `Embarked` \n* A numeric field for `Title`","e1cc6105":"#### Cross-Validation\nCross Validation is a way to provide robust accuracy scores when the quantity of data for training and testing is limited.  The idea is to split that data into a number of segments, then recombine the segments so that each segment is used as test data exactly once.  \n\nSo if we split the data into 3 segments and number them 1, 2, and 3, then we can combine to segments into training data and leave one for testing [[1,2],3].  Note that there are three different ways to do this; [[1,2],3], [[1,3],2],[1,[2,3]].  We train each of those and then test with the remaining segment.\n\nThis is done automatically using `cross_val_score`.","9d8428ef":"#### Age and AgeRange","3234ff8e":"It appears that Embarkation has a strong association with survival conditional on FareClass and Sex.","168619c6":"With just a few methods, we can see alot about the data.  The `info` method in particular reveals important information about the types of data in each column, as well as the number of valid entries in each column.  \n\n#### Data Types\nWe have numeric data in the columns `['PassengerId' 'Survived' 'Pclass' 'Age' 'SibSp' 'Parch' 'Fare']`\n\nand nominal data in the columns `['Name' 'Sex' 'Ticket' 'Cabin' 'Embarked']`\n \nWe will look closer at the data types below.\n\n#### Missing Data\nThere are 891 rows of data in total.  The output of `info()` reveals that there are three columns in which data is missing for some rows: `['Age' 'Cabin' 'Embarked']`","c8b17116":"#### Separating Data Into Training and Testing\n","1facb230":"`FamilySize`, `AgeRange`, and `NamePrefix` all have a number of values.  Some of these values correlate with survuival and some don't.  Sometimes the correlationis the result of very sparse data.  For example, `FamilySize = 11` correlates strongly with (non) survival.","4aa093fd":"#### Pclass","9d864abf":"Very nice result.  We can get more generalizable scores by looking at a boosted average of the test accuracy.  The most basic approach to this is to use cross validation.","37c6cad4":"Again, small groups traveling together seem to have the best likelihood of survival.  ","eb2acfd3":"### Inter-Related Columns\n\nBefore we look closer to the data, Let's work on deriving some new feature columns that seem likely to help.  We will add the following new fields:\n\n* A `Title` field derived from the `Name` column\n* A Family size field derived from `SibSp` and `Parch`\n\nEach of these changes requires us to use multiple columns to create and interpret the new field.","de95b004":"It seems that Age has a strong affect on Survival.  Below we will use these histogram distributions to find good bin values for Age.  We will build bins around the ranges [0..17], [18..34], [35..49], [50..100].  ","22382c12":"It seems that lower fare indicates less chance of survival.  We will use this information to find a new variable related to `Fare` and `Pclass`.","16ad7af4":"# Titanic Data Wrangling\nThe Titanic Survival Problem has become famous as a \"toy\" problem that illustrates important decisions about data for prediction tasks.  The survival problem is to create a predictive algorithm that can tell if an individual would have survived the titanic or not.  The principle is to use a data-driven approach to create the algorithm.  That is, use some of the survival data to create the algorithm and then use the remainder to test the algorithm.\n\nThis data is hosted at [kaggle](https:\/\/www.kaggle.com\/c\/titanic) and is used for tutorials of all kinds.  In this notebook we will focus almost exclusively on data wrangling to prepare the data for learning.\n\nWe are going to follow a multistep process.  \n\n* First, we will look at the information we have for each passenger to understand what it tells us and to determine if it is useful.  We get the data in a spreadsheet format in which each row is a passenger and each column is a different type of data we know about that passenger (including whether or not that passenger survived).\n\n* Next we will focus on trying to determine the quality of each type of data by looking at each column to see if it is related to survival or if the values are present for many of the passengers.  We will also look at relationships among the columns to see if they are correlated or not.\n\n* Next we will look for inter-column relationships that can be used to define new columns with more predictive power that single columns.  We will also look for opportunities to group data within a column to provide classes of data with greater predictive power.\n\n* Next we will make sure that all nominal variables have numerical codes.  This is important for a number of learning methods.\n\n* Next we will drop the columns which we don't need.\n\n* Next we will look at some techniques for \"imputing\" data points that are missing, but that we think might provide value.\n\n* After imputation, we will revisit step 2 to see if there are correlations that show up with the richer data.  At this point we will use some useful visualizations to get a better intuition as to what the relationships in the data are.\n\n* After acting on discoveries in the previous steps, we will perform some predictions to try to get an idea of what type of accuracy is possible.  We will use n-fold cross validation to help make our methods as robust and general as possible.","a7a79d66":"The scatter matrix doesn't help us too much.  It is hard to see the results with the discrete value types.","e34913fb":"#### Fare and FareClass","b7c5f4f4":"### The Passengers of the Titanic\n\nKaggle hosts [a description of the values in each column here](https:\/\/www.kaggle.com\/c\/titanic\/data).  pandas will also give us some options for understanding the data.","61764d04":"#### Parch - Parent or Child","13f0de9b":"It appears that young adults risk death on the titanic more than other categories of passengers.  Using `AgeRange`, we can see the afect of the values on survival:","51476bab":"#### SibSp - Siblings and Spouses","c9dae3ed":"### Drop Unneeded Columns\nThere are many columns that we have transformed now.  Let's remove them since we wont need them for training.","03524147":"The correlation matrix above shows the relationship between each pair of fields.  We will focus on correlations with `Survived`.  `Gender` has the strongest correlation.  Next is `Pclass` which is anti-correlated with `Survived`.  `NamePrefix` is slightly positively correlated with `Survived`. ","e1b0b873":"### Learning from the Titanic\n\nNow that we have data, lets see how good it is for prediction.  There are a few step to prepare for using this data for training.  \n- Separate the data into training and test sets or design cross validation strategy\n- Select an algorithm\n- Train a model \n- Test the model","f1b987f8":"#### Create `FamilySize` from `Parch` and `SibSp`","5e1c1cc0":"#### Sex","4e0f72bb":"#### Relationship to Survival\nthe seaborn visualization library gives us some nice tools for visualizing relationship between values in different fields.  We will use this tools for each remaining column to determine if we want to continue using it for inference.","4f2b179b":"There are two columns with missing data, `AgeRange` and `Depart`.  We are going to **impute**, or guess the missing values in these columns.  There are a number of ways to do this:\n\n* Pick a value that seems reasonable.  Often the **median** value of the field is used in this case.\n* Draw the value from a distribution.  Fix a distribution for the column's sample data and draw values from that distribution for the unknown values.  Typical distributions to use are categorical or normal.\n* Find rows with known values that are close to values in the row with unknown column.  This is known as **K-Nearest Neighbor (KNN)** imputation.\n\n","1334aaed":"#### Extract `Title` from `Name`","7b013d14":"### Quality - What Data is Useful?\nNow that we know something about the data, Let's start thinking about what to do to best use what we have.  Let's look at each column separately to assertain their suitability for classifying survival.  Below is a quick summary of results\n\n| Column Name | Type | Keep? | Comments |\n| --- | --- | --- | --- |\n| PassengerID | numeric | N | A different value for each row.  Not useful for learning classifiers. |\n| Survived | numeric | Y | This is the target value> |\n| Pclass | numeric | Y | Class of travel will likely affect survival. |\n| Name | nominal | N | There may be useful information here like title (Mr. Miss, Dr., etc.) |\n| Sex | nominal | Y | Change this field to 1 = female, 0 = male. |\n| Age | numeric | Y | Bin these values so that they are better for learning. Need to repair missing values. |\n| SibSp | numeric | Y | Traveling with family likely affects survival. |\n| Parch | numeric | Y | Traveling with family likely affects survival. |\n| Ticket | nominal | N | Ticket numbers are inconsistent and don't lend themselves to patterns. |\n| Fare | numeric | Y | Fare may affect survival. |\n| Cabin | nominal | N | We don't know enough about cabin to us it well. |\n| Embarked | nominal | Y | convert to a numeric value. |\n\nNext let's remove the columns we know we won't use.","c1439ef1":"One or two siblings or spouses seems to make survival more likely, but groups larger than 5 have no likelihood of survival.","fbc485ae":"We will look only at the top two highly (absolute value) correlated items:\n\n**Pclass and Fare** appear to be negatively correlated.  This makes sense because in general we would expact 1st class travel to be more expensive that 3rd class travel.\n\n**Parch and SibSp** are positively correlated.  Apparently members of a family travel together often.\n\nNotice that only numeric values show up here.  Later we will change nominal parameters to have nummeric values and rerun this analysis to them to the pairwise correlation matrix.\n\nNext we will look at visualizing how columns compare to `Survived` values.","f9c92ec9":"### Impute Missing Values\nNow that the columns are cleaner, lets make sure to fill in data that is missing so that we can use as many rows as possible.","b179a113":"Apparently sex has a very strong affect on survival.  If a passenger is female,  they already have a high likelihood of survival.  Also, if a passenger is male in passenger class 3, he is very likely to NOT survive.  Females in Pclass=3 are equally likely to survive or not, while females in other calsses have a high likelihood of survival.","f1d12bed":"Pclass appears to be very much related to survival in this sample."}}