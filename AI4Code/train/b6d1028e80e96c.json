{"cell_type":{"2e837b96":"code","35739e01":"code","2ad97a49":"code","91db8409":"code","eb167d05":"code","c6832495":"code","c11d76de":"code","d5d536e0":"markdown","bd43f55d":"markdown","cf46f4e1":"markdown","dc1d810f":"markdown","a7d5badf":"markdown","b903031d":"markdown"},"source":{"2e837b96":"# Import der Numpy-Bibliothek unter dem Namen np\nimport numpy as np\n\n# Beispiele und Tests, nachvollziehen und dann auskommentieren oder l\u00f6schen!\ntest_array1 = np.array([2,5,1])\ntest_array2 = np.array([[1,2,3],\n                        [1,1,1]])\nprint(test_array1)\nprint(test_array2)\n\n# alle Werte von test_array1 mal 2 rechnen und Ergebnisse im selben Array speichern\ntest_array1 = test_array1 * 2\nprint(test_array1)\n\n# auf einzelne Arrays im 2-dim. Array zugreifen\nprint(test_array2[0])\nprint(test_array2[1])\n\n# test_array1 und das zweite Array in test_array2 komponentenweise addieren und Ergebnis in test_array1 speichern\ntest_array1 = test_array1 + test_array2[1]\nprint(test_array1)\n\n# Skalarprodukt berechnen (5*1 + 11*2 + 3*3)\ns = np.dot(test_array1, test_array2[0])\nprint(s)","35739e01":"def accuracy(o, y):\n    i = 0;\n    sum = 0;\n    for x in o:\n        if (x==y[i]):\n            sum += 1;\n        i += 1;\n    return sum\/i;\n    \n# Test (Beispiel vom Perzeptron-Arbeitsblatt, es muss Accuracy = 0,625 rauskommen!)\no=np.array([0,0,1,1,0,1,1,0])\ny=np.array([1,0,1,0,0,1,1,1])\na=accuracy(o, y)\nprint(\"Accuracy = \" + str(a))","2ad97a49":"# TODO: activation-Funktion programmieren\ndef activation(net):\n    if (net<0):\n        return 0;\n    return 1;\n\n# Test\no=activation(2.6)\nprint(o)   # es muss 1 herauskommen\no=activation(-3)\nprint(o)   # es muss 0 herauskommen","91db8409":"# TODO: propagate-Funktion programmieren \n# (Um den Netzinput w*x zu berechnen, koennen Sie die np.dot-Funktion (s.o.) verwenden; \n# anschlie\u00dfend muss auch activation-Funktion aufgerufen werden)\ndef propagate(x):\n    global w  # um auf die au\u00dferhalb der Funktion definierten Variable w (Gewichts-Array) zuzugreifen\n    return activation(np.dot(x, w));\n# Test\nw=np.array([-2, 1, 0.5])\nx=np.array([1, 0, 1])\no=propagate(x)\nprint(o)    # es muss 0 herauskommen, da -2*1 + 1*0 + 0.5*1 = -1.5 < 0 ist","eb167d05":"# TODO: predict-Funktion (nutzt die propagate-Funktion)\ndef predict(X):\n    # TODO\n    i = 0;\n    o = np.array([0,0,0,0]);\n    for x in X:\n        o[i]=propagate(x);\n        i+=1;\n    return o;\n# Test\n# Testen Sie die predict-Funktion, indem Sie das OR-Beispiel nehmen, also\n# X als entsprechendes zweidimensionales Input-Array definieren und\n# w auf die richtigen Gewichte f\u00fcr OR setzen.\nX = np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]]);  # TODO \nw = np.array([-0.5, 0.5, 0.5]);    # TODO\no = predict(X)\nprint(o)    # es muss [0 1 1 1] herauskommen","c6832495":"# TODO: fit_one_sample programmieren, auch hier global w als erste Anweisung!\ndef fit_one_sample(x, y, eta):\n    global w\n    o = propagate(x)\n    w[0]=w[0]+(eta*(y-o)*x[0])\n    w[1]=w[1]+(eta*(y-o)*x[1])\n    w[2]=w[2]+(eta*(y-o)*x[2])\n    \ndef fit_one_epoch(X, y, eta):\n    i = 0\n    for x in X:\n        fit_one_sample(x, y[i], eta)\n        i+=1\n        \ndef fit(X, y, eta, n):\n    for i in range(n):\n        fit_one_epoch(X, y, eta)\n    \n\n# Test \nw=np.array([-0.5, 0.5, -1])\nX=np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\nx=np.array([1,0,0])\ny=np.array([0,1,1,1])\nfit(X, y, 0.5, 5)\nprint(w)    # es muss [-1.5, 1, 1] herauskommen (durch Anwendung der Lernregel auf die Beispieldaten w, x, y)\nprint(predict(X))\n\n\n# TODO: fit_one_epoch und fit programmieren und testen","c11d76de":"# Anfangsgewichte und Trainingsdaten des Arbeitsblatts\nw=np.array([-0.5,0.5,-1])\nX=np.array([[1,0,0],\n            [1,0,1],\n            [1,1,0],\n            [1,1,1]])\ny=np.array([0,1,1,1])\n\n# TODO\n# fit aufrufen, anschlie\u00dfend Gewichte ausgeben und predict(X) ausf\u00fchren, ausgeben und pr\u00fcfen!\nfit(X,y,0.5,5)\nprint(w)\nprint(predict(X))\n","d5d536e0":"### Activation-, Propagate- und Predict-Funktionen programmieren\nProgrammieren und testen Sie die drei Funktionen. Eine nach der anderen!","bd43f55d":"# Programmierung eines Perzeptrons\n\nEs sollen alle Funktionen zum Anwenden eines Perzeptrons und zum Lernen der Gewichte programmiert werden.\n\nDie verschiedenen Daten (Inputs, Gewichte etc.) werden in **Arrays** (statt Python-Listen) aus der **Numpy-Bibliothek** gespeichert. Man kann ein Numpy-Array aus einer Python-Liste erstellen. Man kann Numpy-Arrays mit einzelnen Zahlen multiplizieren, addieren etc., zwei Numpy-Arrays addieren, multiplizieren etc. und von zwei Numpy-Arrays das Skalarprodukt berechnen.\nF\u00fchren Sie die folgende Code-Zelle aus, um die Numpy-Bibliothek zu importieren und sich ein paar kleine Beispiele anzusehen. Vollziehen Sie die Beispiele nach!\n","cf46f4e1":"### Trainingsfunktionen (fit)","dc1d810f":"## Lernen der OR-Funktion\nTesten Sie die Perzeptron-Implementation, indem Sie die OR-Funktion lernen. Nehmen Sie dieselben Anfangswerte f\u00fcr die Gewichte wie auf dem Perzeptron-Arbeitsblatt. Wenn Sie in die entsprechenden Funktionen oben noch mittels print z.B. die Gewichte ausgeben lassen, k\u00f6nnen Sie nachvollziehen, wie sich die Gewichte schrittweise \u00e4ndern und mit der per Hand ausgef\u00fcllten Tabelle des Arbeitsblatts vergleichen!","a7d5badf":"## \u00dcbersicht\n### Variablen\n* Inputs **X**: zweidimensionales Numpy-Array, z.B. \\[\\[1,0,0\\],\\[1,0,1\\],\\[1,1,0\\],\\[1,1,1\\]\\] f\u00fcr die OR-Funktion (der jeweils erste Wert ist konstant 1 f\u00fcr den Bias-Input).\n* Labels **y**: eindimensionales Numpy-Array mit erw\u00fcnschten Outputs, z.B. \\[0,1,1,1\\] f\u00fcr OR\n* Gewichte **w**: eindimensionales Numpy-Array mit Gewichten, inkl. Bias als erstem Wert.\n* **eta**: Lernrate\n* **n**: Anzahl der Epochen\n\n### Funktionen\n*Hinweis*: Die Gewichte sind keine Parameter der folgenden Funktionen. Stattdessen sollen die Funktionen auf das (global) initialisierte Gewichtsarray w zugreifen (Schl\u00fcsselwort global). Erste Anweisung in der jeweiligen Funktion muss dann sein: global w\n* **accuracy(o, y)**: Berechnet f\u00fcr ein Output- und ein Label-Array die Accuracy (Anteil der \u00fcbereinstimmenden Werte) und gibt diese zur\u00fcck (return).\n* **activation(net)**: Berechnet die Aktivierungsfunktion f und gibt das Ergebnis zur\u00fcck. D.h. activation gibt 0 bei net < 0 und 1 bei net >= 0 zur\u00fcck.\n* **propagate(x)**: Berechnet f\u00fcr einen Input-Vektor x (eindimensionales Numpy-Array) den Perzeptron-Output und gibt das Ergebnis zur\u00fcck. D.h. es wird das Skalarprodukt w\\*x (mit np.dot) berechnet und darauf activation aufgerufen.\n* **predict(X)**: Berechnet f\u00fcr **alle** Inputs X (zweidimensionales Array) die Outputs o, R\u00fcckgabe ist also ein Array mit Outputs.\n\nDie bis jetzt genannten Funktionen wenden das Perzeptron an. Insbesondere der Funktionsname **predict** ist im Machinellen Lernen \u00fcblich, um ein trainiertes Modell (z.B. Neuronales Netz) auf eine ganze Menge von Inputs X anzuwenden und ein Array mit den entsprechenden Outputs zu erhalten.\n\nEs folgen noch die Funktionen zum Trainieren \/ Lernen des Perzeptrons. Die entsprechenden Methoden hei\u00dfen im Maschinellen Lernen h\u00e4ufig fit (von Englisch \"to fit\" = \"anpassen\"). **Die folgenden Funktionen geben nichts zur\u00fcck, sondern \u00e4ndern die (global definierten) Gewichte w!**\n* **fit_one_sample(x, y, eta)**: Berechnet den Output o f\u00fcr x (propagate-Funktion) und wendet dann die Lernregel an, um w zu aktualisieren. (x ist ein eindimensionales Array, y ein einzelnert Wert.)\n* **fit_one_epoch(X, y, eta)**: Ruft f\u00fcr jeden Input in X und jedes entsprechende Label in y einmal fit_one_sample auf.\n* **fit(X, y, eta=0.5, n=10)**: Ruft n mal fit_one_epoch auf.\n\nfit ist die Funktion, die das Perzeptron trainiert, die beiden vorigen Funktionen Hilfsfunktionen.","b903031d":"## Programmierung\n### Accuracy-Funktion"}}