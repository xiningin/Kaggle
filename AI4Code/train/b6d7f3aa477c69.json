{"cell_type":{"1652e47a":"code","431c00c2":"code","81a80c07":"code","5fae9f62":"code","449e4a4e":"code","d13de538":"code","bcaef586":"code","20017f9f":"code","737cae0e":"code","ef80c78f":"code","3d7957e1":"code","b83ebb69":"markdown","74428c4d":"markdown","7ac7431b":"markdown","12f3af20":"markdown","e030c6f1":"markdown"},"source":{"1652e47a":"! pip install --user pytorch-tabnet","431c00c2":"import pandas as pd\nimport numpy as np\nimport torch\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom matplotlib import pyplot as plt","81a80c07":"# Load data\n\ntrain = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ntrain[\"date\"] = train.groupby(\"breath_id\").time_step.rank(axis=0).astype(np.int)\n\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\ntest[\"date\"] = test.groupby(\"breath_id\").time_step.rank(axis=0).astype(np.int)","5fae9f62":"def flatten_df(df, has_target = True):\n    \"\"\"\n        This switch from time step observation to full series\n    \"\"\"\n    features = [\"R\", \"C\",] + [f\"u_in_{n}\" for n in range(80)] + [f\"u_out_{n}\" for n in range(80)]\n    targets =  [f\"u_pressure_{n}\" for n in range(80)]\n    \n    if has_target:\n        values = np.hstack([df.R.unique(),\n                            df.C.unique(),\n                            df.u_in.values,\n                            df.u_out.values,\n                            df.pressure.values])\n        columns = features + targets\n    else:\n        values = np.hstack([df.R.unique(),\n                            df.C.unique(),\n                            df.u_in.values,\n                            df.u_out.values])\n        columns = features\n    \n    result_df = pd.Series({col: val for col, val in zip(columns, values)})\n    return result_df","449e4a4e":"X = train.groupby(\"breath_id\").apply(lambda df : flatten_df(df)).reset_index(drop=False)\nX_test = test.groupby(\"breath_id\").apply(lambda df : flatten_df(df, has_target=False)).reset_index(drop=False)","d13de538":"# Define features and targets\nfeatures = [\"R\", \"C\"] + [f\"u_in_{n}\" for n in range(80)] + [f\"u_out_{n}\" for n in range(80)]\ntargets =  [f\"u_pressure_{n}\" for n in range(80)]","bcaef586":"cat_idxs = [] # R and C could be categorical\ncat_dims = []\n\n\nBS = 2**12\nvirtual_BS = 256\n\n# commented params\ntabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 256, \n    n_a = 256, \n    n_steps = 5,\n    gamma = 1.5,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 1e-5,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    mask_type = \"entmax\",\n    seed = 42,\n    verbose = 5\n    \n)","20017f9f":"# Make sure to comply with the competition metric\n# here we take advantage of the fact that pressure is positive\n\nfrom pytorch_tabnet.metrics import Metric\n\nclass filtered_MAE(Metric):\n        def __init__(self):\n            self._name = \"filtered_mae\"\n            self._maximize = False\n\n        def __call__(self, y_true, y_score):\n            weights = y_true >-1\n            mae = weights * np.abs(y_true - y_score)\n            mae = mae.sum() \/ weights.sum()\n            return mae\n        \ndef filtered_loss(y_pred, y_true):\n    weights = (y_true >-1)+0.\n    mae = weights * torch.abs(y_true - y_pred)\n    mae = torch.sum(mae) \/ torch.sum(weights)\n    return mae","737cae0e":"from sklearn.model_selection import GroupKFold\nimport copy\n\nN_SPLITS = 5\nkfold = GroupKFold(n_splits=N_SPLITS)\n\n# How many folds do you want to train?\nN_MODELS = 5\nassert(N_MODELS <= N_SPLITS)\nmax_epochs =  500\n\n# Create out of folds array\noof_predictions = np.zeros((X.shape[0], len(targets)))\ntest_preds = np.zeros((X_test.shape[0], len(targets)))\n\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(X, groups=X.breath_id)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_val = X.loc[trn_ind][features].values, X.loc[val_ind][features].values\n    y_train, y_val = X.loc[trn_ind][targets].values, X.loc[val_ind][targets].values\n    \n    # mask unnecessary outputs with -1\n    y_train[X.loc[trn_ind, [c for c in X.columns if c.startswith(\"u_out\")]].values==1] = -1\n    y_val[X.loc[val_ind, [c for c in X.columns if c.startswith(\"u_out\")]].values==1]=-1\n    \n    \n    params = copy.deepcopy(tabnet_params)\n    params[\"scheduler_fn\"]=torch.optim.lr_scheduler.OneCycleLR\n    params[\"scheduler_params\"]={\"is_batch_level\":True,\n                                \"max_lr\":5e-2,\n                                \"steps_per_epoch\":int(X_train.shape[0] \/ BS)+1,\n                                \"epochs\":max_epochs}\n    \n\n    clf =  TabNetRegressor(**params)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_train, y_train), (X_val, y_val)],\n      eval_name=['train', 'val'],\n      max_epochs = max_epochs,\n      patience = 200,\n      batch_size = BS,  \n      virtual_batch_size = virtual_BS, \n      num_workers = 0,\n      drop_last = False, \n      eval_metric=[\"filtered_mae\"],\n      loss_fn=filtered_loss,\n      )\n    \n    del X_train\n      \n    oof_predictions[val_ind] = clf.predict(X_val)\n    \n    del X_val\n    \n    test_preds += clf.predict(X_test[features].values) \/ N_MODELS\n    if fold+1 >=N_MODELS:\n        break","ef80c78f":"for serie_nb in range(50):\n    plt.plot(oof_predictions[serie_nb, :])\n    plt.plot(X.loc[serie_nb][targets].values, color=\"green\")\n    plt.show()","3d7957e1":"sample_submission = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\nsample_submission[\"pressure\"] = test_preds.reshape(-1, 1)\nsample_submission.to_csv(\"submission.csv\", index=False)","b83ebb69":"# Define TabNet params","74428c4d":"# Make submission","7ac7431b":"# Have a look at out of fold predictions","12f3af20":"# Simple and efficient TabNet approach\n\n## Why TabNet for time series?\n\nIn my opinion TabNet, as XGBoost or LGBM is NOT the best algorithm for time series.\n\nHowever, pytorch-tabnet natively implements multi regression which is handy to make a time serie prediction.\n\n\nThis notebook shows one elegant and simple way to work with time series with TabNet:\n- pick a \"memory size\" (here it's the entire serie size of 80 points) and give all the features as input to the model\n- pick a \"horizon time\" for prediction (here it's again the entire serie size) and give this as multi-regression targets\n\nIt would be really painful to use such an approach with XGBoost, since you'll need to have one model per time_step.\n\nThis is a very simple baseline, no feature was created, no preprocessing, nothing : just reformating.\n\n## Results\n\nThe results are quite good, competing with simple yet strong LSTM baseline like this one: https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter-simple-lstm\n\nI find the simplicity and flexibility of tabnet really beautiful, I hope you'll like it too.\n\n## Pushing further\n\nIf you want to try to push this baseline further here is a list of things one could try:\n- play with the parameters\n- try pretraining on test set (might be quite heavy)\n- create as many features as you like\n\nI am sure the score can be improved, however I still doubt that it can compete with time series Deep Learning models like TCN or LSTM. It might be useful for ensembling however.","e030c6f1":"# Train"}}