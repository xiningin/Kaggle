{"cell_type":{"af79182c":"code","1b61763c":"code","a1327c76":"code","5e339cec":"code","1ff35174":"code","cb0ce585":"code","a34aa6a0":"code","24960ee8":"code","9d16eced":"code","5af9328c":"code","1a0a1fa2":"code","b07c6cdf":"code","93d26060":"code","05322dd9":"code","9f09fa7b":"code","90fde62d":"code","8c064ddd":"code","3ac5b6cb":"code","ffc9a83e":"code","17c799fc":"code","6cbc1365":"code","7f20015f":"code","59075416":"code","0776461c":"code","74d83061":"code","b816b542":"code","ca606e26":"code","755c4055":"code","322ebf44":"code","38dd8be8":"code","fce6f4ab":"code","bac2201b":"code","513fd265":"code","8b9b400e":"code","7164c7db":"code","5e3be14d":"code","23710b36":"code","11bad0c0":"code","e19947c8":"code","25442811":"code","d67d0703":"code","de71616f":"markdown","7884d90a":"markdown","6e614e23":"markdown","53a52211":"markdown","f6349c2c":"markdown","63138951":"markdown","a1ce07d1":"markdown","9c8f5b43":"markdown","a5a50b01":"markdown","a9ee43e8":"markdown","9470b62f":"markdown","05843ae5":"markdown","302a6aff":"markdown","3eaaf33c":"markdown","49b95906":"markdown","61eee733":"markdown","baf68209":"markdown","90b8d38b":"markdown","dd280806":"markdown","e7e2ce3a":"markdown","6570b60f":"markdown","0a41742e":"markdown","d98207fd":"markdown","3aee3f6b":"markdown","48c6cdc9":"markdown","488aed1d":"markdown","391f44d2":"markdown","d2055fa3":"markdown","c27a9eea":"markdown","9a5e2adc":"markdown","d8ebafa5":"markdown","b94d43b9":"markdown","c4574cfa":"markdown","cf44af95":"markdown","a5f42ea3":"markdown","24952d96":"markdown","79c296d0":"markdown","6b0a6fd4":"markdown"},"source":{"af79182c":"import time\nimport numpy as np\nimport pandas as pd\nimport timeit\nimport random \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom sklearn.model_selection import train_test_split\n    \nfrom bayes_opt import BayesianOptimization\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom hyperopt import  tpe,hp,fmin\n\n\n\nimport warnings\nwarnings.simplefilter('ignore')","1b61763c":"train_df = pd.read_csv('..\/input\/career-con-2019\/X_train.csv')\ntest_df = pd.read_csv('..\/input\/career-con-2019\/X_test.csv')\ntarget = pd.read_csv('..\/input\/career-con-2019\/y_train.csv')\nsub = pd.read_csv('..\/input\/career-con-2019\/sample_submission.csv')\n\n# train = pd.read_csv(\"..\/input\/titanic\/train.csv\")","a1327c76":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\ndef feature_eng(data):\n    data['total_angular_velocity'] = (data['angular_velocity_X'] ** 2 + data['angular_velocity_Y'] ** 2 + data['angular_velocity_Z'] ** 2) ** 0.5\n    data['total_linear_acceleration'] = (data['linear_acceleration_X'] ** 2 + data['linear_acceleration_Y'] ** 2 + data['linear_acceleration_Z'] ** 2) ** 0.5\n    data['acc_vs_vel'] = data['total_linear_acceleration'] \/ data['total_angular_velocity']\n\n    x, y, z, w = data['orientation_X'].tolist(), data['orientation_Y'].tolist(), data['orientation_Z'].tolist(), data['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n\n    data['euler_x'] = nx\n    data['euler_y'] = ny\n    data['euler_z'] = nz\n\n    data['total_angle'] = (data['euler_x'] ** 2 + data['euler_y'] ** 2 + data['euler_z'] ** 2) ** 0.5\n    data['angle_vs_acc'] = data['total_angle'] \/ data['total_linear_acceleration']\n    data['angle_vs_vel'] = data['total_angle'] \/ data['total_angular_velocity']\n    df = pd.DataFrame()\n    \n    for col in data.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_max_to_min'] = df[col + '_max'] \/ df[col + '_min']\n\n        df[col + '_abs_max'] = data.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n        \n    return df\n","5e339cec":"new_training_data = feature_eng(train_df)\nnew_test_data = feature_eng(test_df)","1ff35174":"new_training_data.fillna(0, inplace = True)\nnew_test_data.fillna(0, inplace = True)\nnew_training_data.replace(-np.inf, 0, inplace = True)\nnew_training_data.replace(np.inf, 0, inplace = True)\nnew_test_data.replace(-np.inf, 0, inplace = True)\nnew_test_data.replace(np.inf, 0, inplace = True)","cb0ce585":"le = LabelEncoder()\ntarget = le.fit_transform(target['surface'])","a34aa6a0":"rf_param_grid = {\n                 'max_depth' : list(range(8,100,10)),\n                 'n_estimators': list(range(50,2000,100)),\n                 'min_samples_split': [2,6,8],\n                 'min_samples_leaf': [2,6,8],\n                 'bootstrap': [True, False]\n                 }\n","24960ee8":"m = RandomForestClassifier(n_jobs=-1)","9d16eced":"m_r = GridSearchCV(param_grid=rf_param_grid, estimator = m, scoring = \"accuracy\", cv = 4)","5af9328c":"# %time m_r.fit(new_training_data, target)","1a0a1fa2":"m_r = RandomizedSearchCV(param_distributions=rf_param_grid, estimator = m, scoring = \"accuracy\", cv = 4,n_jobs=-1,n_iter=10)","b07c6cdf":"%time m_r.fit(new_training_data,target)","93d26060":"m_r.best_params_","05322dd9":"for param, score in zip(m_r.cv_results_['params'], m_r.cv_results_['mean_test_score']):\n    print(param, score)","9f09fa7b":"rf_bp = m_r.best_params_","90fde62d":"rf_classifier=RandomForestClassifier(n_estimators=rf_bp[\"n_estimators\"],\n                                     min_samples_split=rf_bp['min_samples_split'],\n                                     min_samples_leaf=rf_bp['min_samples_leaf'],\n                                     max_depth=rf_bp['max_depth'],\n                                     bootstrap=rf_bp['bootstrap'])","8c064ddd":"rf_classifier.fit(new_training_data,target)","3ac5b6cb":"y_pred = rf_classifier.predict(new_test_data)","ffc9a83e":"sub['surface'] = le.inverse_transform(y_pred)\nsub.to_csv('random_f_rs.csv', index=False)","17c799fc":"n_fold = 4\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True)","6cbc1365":"def randomforest_evaluate(**params):\n    \n    params['n_estimators'] = int(round(params['n_estimators'],0))\n    params['min_samples_split'] = int(round(params['min_samples_split'],0))\n    params['min_samples_leaf'] = int(round(params['min_samples_leaf'],0))\n    params['bootstrap'] = int(round(params['bootstrap'],0))\n        \n    \n    test_pred_proba = np.zeros((new_training_data.shape[0],9))\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(new_training_data, target)):\n        X_train, X_valid = new_training_data.iloc[train_idx], new_training_data.iloc[valid_idx]\n        y_train, y_valid = target[train_idx], target[valid_idx]\n        \n        model = RandomForestClassifier(**params,n_jobs=-1)\n        \n        model.fit(X_train, y_train)\n        \n        y_pred_valid = model.predict_proba(new_training_data)\n       \n        test_pred_proba += y_pred_valid \/ folds.n_splits\n        \n  \n \n    return accuracy_score(target, test_pred_proba.argmax(1))","7f20015f":"rf_param_grid = {\n                 'max_depth' : (8,100),\n                 'n_estimators': (50,2000),\n                 'min_samples_split': (2,10),\n                 'min_samples_leaf': (2, 10),\n                 'bootstrap': (True, False),\n                 }\n\nrf_b_o = BayesianOptimization(randomforest_evaluate, rf_param_grid)\n%time rf_b_o.maximize(init_points=5, n_iter=20)","59075416":"for i,n in rf_b_o.max[\"params\"].items():\n    print(i,int(round(n)))","0776461c":"model=RandomForestClassifier(n_estimators=int(round(rf_b_o.max[\"params\"][\"n_estimators\"],0)),max_depth=int(round(rf_b_o.max[\"params\"][\"max_depth\"])),\n                             min_samples_leaf=int(round(rf_b_o.max[\"params\"][\"min_samples_leaf\"])),min_samples_split=int(round(rf_b_o.max[\"params\"][\"min_samples_split\"])))\n\nmodel.fit(new_training_data,target)\ny_pred = model.predict(new_test_data)\nsub['surface'] = le.inverse_transform(y_pred)\nsub.to_csv('random_f_bo.csv', index=False)","74d83061":"def randomforest_evaluate(params):\n    print(params)\n    params['n_estimators'] = int(round(params['n_estimators'],0))\n    params['min_samples_split'] = int(round(params['min_samples_split'],0))\n    params['min_samples_leaf'] = int(round(params['min_samples_leaf'],0))\n    params['bootstrap'] = int(round(params['bootstrap'],0))\n        \n    test_pred_proba = np.zeros((new_training_data.shape[0],9))\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(new_training_data, target)):\n        X_train, X_valid = new_training_data.iloc[train_idx], new_training_data.iloc[valid_idx]\n        y_train, y_valid = target[train_idx], target[valid_idx]\n        \n        model = RandomForestClassifier(**params,n_jobs=-1)\n        \n        model.fit(X_train, y_train)\n        \n        y_pred_valid = model.predict_proba(new_training_data)\n        test_pred_proba += y_pred_valid \/ folds.n_splits\n        \n  \n    return accuracy_score(target, test_pred_proba.argmax(1))\n\n# define a search space\n\n\nspace = {\n    \n        'max_depth': hp.uniform('max_depth', 8,100),\n        'n_estimators': hp.uniform('n_estimators', 50, 2000),\n        'min_samples_split' : hp.choice('min_samples_split',[2,3,6,9]),\n        'min_samples_leaf' : hp.choice('min_samples_leaf', [2,3,6,9]),\n        'bootstrap': hp.choice('bootstrap', [False,True]),\n        \n    }\n\n# minimize the objective over the space\n\n%time best = fmin(randomforest_evaluate, space, algo=tpe.suggest, max_evals=25)\n\nif best[\"min_samples_split\"] <=1:\n    best[\"min_samples_split\"] = 2\n    \nif best[\"min_samples_split\"] <=1:\n    best[\"min_samples_leaf\"] = 2\n    \nprint(\"Best Params:\")\nprint(best)","b816b542":"model=RandomForestClassifier(n_estimators=int(round(best[\"n_estimators\"],0)),max_depth=int(round(best[\"max_depth\"],0)),\n                             min_samples_leaf=int(round(best[\"min_samples_leaf\"])),min_samples_split=int(round(best[\"min_samples_split\"])))\n\nmodel.fit(new_training_data,target)\ny_pred = model.predict(new_test_data)\nsub['surface'] = le.inverse_transform(y_pred)\nsub.to_csv('random_f_tpe.csv', index=False)","ca606e26":"!git clone https:\/\/github.com\/thuijskens\/scikit-hyperband.git 2>\/dev\/null 1>\/dev\/null","755c4055":"!cp -r scikit-hyperband\/* .","322ebf44":"!python setup.py install 2>\/dev\/null 1>\/dev\/null","38dd8be8":"from hyperband import HyperbandSearchCV\n\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelBinarizer\n\nmodel = RandomForestClassifier()\n\nparam_dist = {\n    \n    'max_depth': sp_randint(8,100),\n    'n_estimators': sp_randint(50,2000),\n    'min_samples_split' : [2,3,6,9,10],\n    'min_samples_leaf' : [2,3,6,9,10],\n    'bootstrap': [True, False]\n}\n\ny = LabelBinarizer().fit_transform(target)\n\nsearch = HyperbandSearchCV(model, param_dist, \n                           resource_param='n_estimators',\n                           scoring='accuracy')\n%time search.fit(new_training_data, y)\nprint(search.best_params_)","fce6f4ab":"best=search.best_params_","bac2201b":"model=RandomForestClassifier(n_estimators=int(round(best[\"n_estimators\"],0)),max_depth=int(round(best[\"max_depth\"],0)),\n                             min_samples_leaf=best[\"min_samples_leaf\"],min_samples_split=best[\"min_samples_split\"])\n\nmodel.fit(new_training_data,target)\ny_pred = model.predict(new_test_data)\nsub['surface'] = le.inverse_transform(y_pred)\nsub.to_csv('random_f_hb.csv', index=False)\n","513fd265":"def initilialize_poplulation(numberOfParents):\n    max_depth = np.empty([numberOfParents, 1], dtype = np.uint8)\n    n_estimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n    min_samples_split = np.empty([numberOfParents, 1])\n    min_samples_leaf = np.empty([numberOfParents, 1])\n    bootstrap = np.empty([numberOfParents, 1])\n    \n    for i in range(numberOfParents):\n        #print(i)\n        max_depth[i] = int(random.randrange(8, 100,step=1))\n        n_estimators[i] = int(random.randrange(50, 2000, step = 100))\n        min_samples_split[i] = int(random.randrange(2, 10, step= 1))\n        min_samples_leaf[i] = int(random.randrange(2, 10))\n        bootstrap[i] = int(random.randint(0, 1))\n       \n    \n    population = np.concatenate((max_depth, n_estimators, min_samples_split, min_samples_leaf, bootstrap), axis= 1)\n    return population","8b9b400e":"def fitness_accscore(y_true, y_pred):\n    fitness = round(accuracy_score(y_true, y_pred), 4)\n    return fitness\n\n\n#train the data annd find fitness score\ndef train_population(population, train,y_train, test, y_test):\n    fScore = []\n    for i in range(population.shape[0]):\n\n        param_dist = {\n\n                'max_depth': int(population[i][0]),\n                'n_estimators': int(population[i][1]),\n                'min_samples_split' : int(population[i][2]),\n                'min_samples_leaf' : int(population[i][3]),\n                'bootstrap': int(population[i][4])\n            }\n        #print(param_dist)           \n        model = RandomForestClassifier(**param_dist)\n        model.fit(train,y_train)\n        preds = model.predict(test)\n        fScore.append(fitness_accscore(y_test, preds))\n    return fScore","7164c7db":"#select parents for mating\ndef new_parents_selection(population, fitness, numParents):\n    selectedParents = np.empty((numParents, population.shape[1])) #create an array to store fittest parents\n    \n    #find the top best performing parents\n    for parentId in range(numParents):\n        bestFitnessId = np.where(fitness == np.max(fitness))\n        bestFitnessId  = bestFitnessId[0][0]\n        selectedParents[parentId, :] = population[bestFitnessId, :]\n        fitness[bestFitnessId] = -1 #set this value to negative, in case of F1-score, so this parent is not selected again\n    return selectedParents","5e3be14d":"'''\nMate these parents to create children having parameters from these parents (we are using uniform crossover method)\n'''\ndef crossover_uniform(parents, childrenSize):\n    \n    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) #get all the index\n    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]\/2)) # select half  of the indexes randomly\n    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n    \n    children = np.empty(childrenSize)\n    \n    '''\n    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values\n    will be picked from the indexes, which were randomly selected above. \n    '''\n    for i in range(childrenSize[0]):\n        \n        #find parent 1 index \n        parent1_index = i%parents.shape[0]\n        #find parent 2 index\n        parent2_index = (i+1)%parents.shape[0]\n        #insert parameters based on random selected indexes in parent 1\n        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n        #insert parameters based on random selected indexes in parent 1\n        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n    return children","23710b36":"def mutation(crossover, numberOfParameters):\n    #Define minimum and maximum values allowed for each parameter\n    minMaxValue = np.zeros((numberOfParameters, 2))\n\n    minMaxValue[0:] = [8, 100] #min\/max max depth\n    minMaxValue[1, :] = [50, 2000] #min\/max n_estimator\n    minMaxValue[2, :] = [2, 10] #min\/max min_samples_split\n    minMaxValue[3, :] = [2, 10] #min\/max min_samples_leaf\n    minMaxValue[4, :] = [0, 1] #min\/max boostrap\n\n \n    # Mutation changes a single gene in each offspring randomly.\n    mutationValue = 0\n    parameterSelect = np.random.randint(0,5, 1)\n    #print(parameterSelect)\n    if parameterSelect == 0: #depth\n        mutationValue = np.random.randint(-50, 50, 1)\n    if parameterSelect == 1: #n_estimator\n        mutationValue = np.random.randint(-200, 200, 1)\n    if parameterSelect == 2: #min_samples_split\n        mutationValue = np.random.randint(-10, 10, 1)\n    if parameterSelect == 3: #min_samples_leaf\n        mutationValue = np.random.randint(-10, 10, 1)\n    if parameterSelect == 4: #boostrap\n        mutationValue = 0\n\n  \n    #indtroduce mutation by changing one parameter, and set to max or min if it goes out of range\n    for idx in range(crossover.shape[0]):\n        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n    return crossover","11bad0c0":"\nX_train, X_test, y_train, y_test = train_test_split(new_training_data, target, test_size = 0.20, random_state = 97)\n","e19947c8":"start = timeit.default_timer()\nnumberOfParents = 8 #number of parents to start\nnumberOfParentsMating = 4 #number of parents that will mate\nnumberOfParameters = 5 #number of parameters that will be optimized\nnumberOfGenerations = 4 #number of genration that will be created\n#define the population size\npopulationSize = (numberOfParents, numberOfParameters)\n#initialize the population with randomly generated parameters\npopulation = initilialize_poplulation(numberOfParents)\n#print(population)\n#define an array to store the fitness  hitory\nfitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n#define an array to store the value of each parameter for each parent and generation\npopulationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n#insert the value of initial parameters in history\npopulationHistory[0:numberOfParents, :] = population\nfor generation in range(numberOfGenerations):\n    print(\"This is number %s generation\" % (generation))\n    \n    #train the dataset and obtain fitness\n    fitnessValue = train_population(population=population, train=X_train,y_train=y_train, test=X_test, y_test=y_test)\n    fitnessHistory[generation, :] = fitnessValue\n    \n    #best score in the current iteration\n    print('Best accuracy score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n    #survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n    parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n    \n    #mate these parents to create children having parameters from these parents (we are using uniform crossover)\n    children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n    \n    #add mutation to create genetic diversity\n    children_mutated = mutation(children, numberOfParameters)\n    \n    '''\n    We will create new population, which will contain parents that where selected previously based on the\n    fitness score and rest of them  will be children\n    '''\n    population[0:parents.shape[0], :] = parents #fittest parents\n    population[parents.shape[0]:, :] = children_mutated #children\n    \n    populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population #srore parent information\n\n\nstop = timeit.default_timer()\nprint('Execution time: ', (stop - start)\/60)  ","25442811":"#Best solution from the final iteration\nfitness = train_population(population=population, train=X_train,y_train=y_train, test=X_test, y_test=y_test)\nfitnessHistory[generation+1, :] = fitness\n#index of the best solution\nbestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n#Best fitness\nprint(\"Best fitness is =\", fitness[bestFitnessIndex])\n#Best parameters\nprint(\"Best parameters are:\")\nprint('Max_depth', int(population[bestFitnessIndex][0]))\nprint('n_estimators', int(population[bestFitnessIndex][1]))\nprint('min_samples_split', int(population[bestFitnessIndex][2])) \nprint('min_samples_leaf', int(population[bestFitnessIndex][3]))\nprint('bootstrap', int(population[bestFitnessIndex][4]))\n","d67d0703":"model=RandomForestClassifier(n_estimators=int(population[bestFitnessIndex][1]),max_depth=int(population[bestFitnessIndex][0]),\n                             min_samples_leaf=int(population[bestFitnessIndex][3]),min_samples_split=int(population[bestFitnessIndex][2]))\n\nmodel.fit(new_training_data,target)\ny_pred = model.predict(new_test_data)\nsub['surface'] = le.inverse_transform(y_pred)\nsub.to_csv('random_f_ga.csv', index=False)\n","de71616f":"<h2 style=\"color:red\">If you enjoyed this work or you found it helpful , an upvotes would be very much appreciated  :-)<\/h2>","7884d90a":"Genetic algorithms are stochastic search algorithms which act on a population of possible solutions. They are loosely based on the mechanics of population genetics and selection. The potential solutions are encoded as \u2018genes\u2019 \u2014 strings of characters from some alphabet. New solutions can be produced by \u2018mutating\u2019 members of the current population, and by \u2018mating\u2019 two solutions together to form a new solution. The better solutions are selected to breed and mutate and the worse ones are discarded. They are probabilistic search methods; this means that the states which they explore are not determined solely by the properties of the problems. A random process helps to guide the search. Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem","6e614e23":"In this competition we have data about small mobile robot driving over different floor surfaces. We need to predict the floor type based on robot's sensor data.","53a52211":"<img src=\"https:\/\/cambridgecoding.files.wordpress.com\/2016\/03\/gridsearch_cv.png\" style=\"height:300px;\"\/>","f6349c2c":"<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<h2>Contents<\/h2>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n<li style=\"font-size: 20px;list-style: outside none none !important;\"><a href=\"#g\">1 Grid Searchg<\/a><\/li>\n<li style=\"font-size: 20px;list-style: outside none none !important;\"><a href=\"#r\">2 Randomized Search<\/a><\/li>\n<li style=\"font-size: 20px;list-style: outside none none !important;\"><a href=\"#b\">3 Baysian optimization<\/a><\/li>\n<li style=\"font-size: 20px;list-style: outside none none !important;\"><a href=\"#t\">4 Tree of Parzen Estimators (TPE)<\/a><\/li>\n<li style=\"font-size: 20px;list-style: outside none none !important;\"><a href=\"#h\">5 Hyperband <\/a><\/li>\n    <li style=\"font-size: 20px;list-style: outside none none !important;\"><a href=\"#ga\">6 Genetic Algorithm<\/a><\/li>\n\n<\/ul>\n<\/div>","63138951":"# Baysian optimization <a class=\"anchor\" id=\"b\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","a1ce07d1":"# Randomized Search <a class=\"anchor\" id=\"r\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","9c8f5b43":"Hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.","a5a50b01":"A great kernel about  bayesian optimization in more depth https:\/\/www.kaggle.com\/artgor\/bayesian-optimization-for-robots","a9ee43e8":"### About Data ","9470b62f":"<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F848555%2F034d45ec284f87d0641309564207efc9%2FYJZT-16CM-7-2CM-Car-Sticker-Vinyl-Decal-Tuning-Turbo-Snail-Turbo-Funny-Black-Silver-C10.jpg_640x640.jpg?generation=1595196705118028&alt=media\">","05843ae5":"Another way to search through hyperparameter space to find optima is via randomized search. In randomized search, you sample HP values a certain number of times from some distribution which you prespecify in advance. So unlike grid search, in which you specify particular numbers to combinatorially try out, you instead specify distributions that cover the HP space you want to explore. For example, you might specify a standard normal distribution over an HP if you think reasonable values are roughly centered around 0, or a uniform distribution over some range if you think values within that range are about as likely to be \u201cgood\u201d. In randomized search, you also specify a n_iter parameter, which acts as a computational budget, controlling how many different parameter settings are tried out in total.","302a6aff":"# Hyperband Bandit-Based Approach to Hyperparameter Optimization <a class=\"anchor\" id=\"h\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","3eaaf33c":"# hyper-parameters tuning of an machine learning model (Random Forest)\n<a href=\"https:\/\/www.linkedin.com\/in\/ouassim-adnane\/\" >Ouassim Adnane <\/a> <b>v1=24\/03\/2019 v2=20\/07\/2020<\/b>","49b95906":"Grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results","61eee733":"<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F848555%2F8bb65d72e78f31dd0840129abc4882b7%2Ftpe.PNG?generation=1595247115729068&alt=media\">","baf68209":"### [Hyperopt](https:\/\/github.com\/hyperopt\/hyperopt) github repo ","90b8d38b":"<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F848555%2F90a14c59c51968f01468f646fbb86d2a%2Fhb.PNG?generation=1595248059112556&alt=media\">","dd280806":"<img src=\"https:\/\/cdn.educba.com\/academy\/wp-content\/uploads\/2019\/09\/What-is-Genetic-Algorithm.png\">","e7e2ce3a":"### Research Paper [HyperBand](https:\/\/arxiv.org\/pdf\/1603.06560.pdf)","6570b60f":"### Research Paper [TPE](https:\/\/papers.nips.cc\/paper\/4443-algorithms-for-hyper-parameter-optimization.pdf)","0a41742e":"# Hyperparameter tuning using genetic algorithm <a class=\"anchor\" id=\"h\"><\/a>\n<a href=\"#ga\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","d98207fd":"# Tree of Parzen Estimators (TPE) <a class=\"anchor\" id=\"t\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","3aee3f6b":"<a href=\"#toc\"><img  src=\"https:\/\/za.heytv.org\/wp-content\/uploads\/2019\/08\/AGF-l79DYZtk_pSyfWgIP3D-3yi8YN6ZeWO0E8tyLgs800-c-k-c0xffffffff-no-rj-mo.jpeg\" style=\"height: 300px\"\/><\/a>","48c6cdc9":"Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems. \u00a9 2018 Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh and Ameet Talwalkar.","488aed1d":"Bayesian approaches, in contrast to random or grid search, keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function:<br>\n<b style=\"margin-left:425px\">P(Score | Hyperparameters)<\/b>\n\n\nIn the literature, this model is called a \u201csurrogate\u201d for the objective function and is represented as p(y | x). The surrogate is much easier to optimize than the objective function and Bayesian methods work by finding the next set of hyperparameters to evaluate on the actual objective function by selecting hyperparameters that perform best on the surrogate function. In other words:<br>\n<ol>\n<li>Build a surrogate probability model of the objective function<\/li>\n<li>Find the hyperparameters that perform best on the surrogate<\/li>\n<li>Apply these hyperparameters to the true objective function<\/li>\n<li>Update the surrogate model incorporating the new results<\/li>\n<li>Repeat steps 2\u20134 until max iterations or time is reached<\/li>\n<\/ol>","391f44d2":"## Imports ","d2055fa3":"<img src=\"https:\/\/image.slidesharecdn.com\/final-160831053134\/95\/genetic-algorithm-1-638.jpg?cb=1472621627\">","c27a9eea":"<img src=\"https:\/\/raw.githubusercontent.com\/nslatysheva\/data_science_blogging\/master\/expanding_ML_toolkit\/expanding_toolkit.jpg\" style=\"height:500px;width:50%;\"\/>","9a5e2adc":"## All the Acknowledgment to [Mohit Jain github repo](http:\/\/https:\/\/github.com\/mjain72\/Hyperparameter-tuning-in-XGBoost-using-genetic-algorithm) for the code ","d8ebafa5":"### Genetic Algorithm [youtube playlist](http:\/\/https:\/\/www.youtube.com\/playlist?list=PLRqwX-V7Uu6bJM3VgzjNV5YxVxUwzALHV) (the Coding train)\n","b94d43b9":" ### Some quick feature engineering based of this <a href=\"https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\">kernel <\/a>","c4574cfa":"### [Github](https:\/\/github.com\/thuijskens\/scikit-hyperband) Repo","cf44af95":"<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F848555%2F1fa3ac0ef82c94bcf584ac09f402a833%2F1_m1JYCHHhbmn6TCHcJIklZA.gif?generation=1595247193531898&alt=media\">","a5f42ea3":"## Table of content <a class=\"anchor\" id=\"toc\"><\/a>","24952d96":"estimated time GridSearch  3^4\u22c52=484 different parameter combinations. cross-validation is 4-fold cv so the above code should train your model 484\u22c54=1944 times.GridSearch runs parallel on your processors, we have 4 processors available, each processor should fit the model 1944\/4=484 times. Now,on average my model takes 10sec to train, I'm estimating around 484\u22c510\/60=80.6min training time.\nGrid Search is accurate but take a long time  to find the best hyperparameters ","79c296d0":"# Grid Search <a class=\"anchor\" id=\"g\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","6b0a6fd4":"<img src=\"https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/examples\/bo_example.png?raw=true\">"}}