{"cell_type":{"ec31f5e2":"code","dee76454":"code","14b084a5":"code","c052732a":"code","6ab2a545":"code","c6f9ad33":"code","4394e082":"code","1b8b8c00":"code","f6819e80":"code","11a4609e":"code","49976682":"code","4186edd6":"code","78966aa9":"code","68ee14ae":"code","8a534c8e":"markdown","faefb523":"markdown","11f01801":"markdown","20670955":"markdown","b0353c5b":"markdown","54584581":"markdown","5376cd55":"markdown","ebb2e28b":"markdown","fd7f0726":"markdown","1ad7d45f":"markdown","444576aa":"markdown","54f8e5f9":"markdown","b6469324":"markdown","087130c9":"markdown","6ece6ba8":"markdown"},"source":{"ec31f5e2":"!pip install -q torch\n!pip install -q albumentations\n!pip install -q seaborn\n!pip install -q tqdm\n!pip install -q numpy\n!pip install -q addict","dee76454":"import torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import models\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\nfrom PIL import Image\nimport numpy as np\nimport logging\nfrom addict import Dict\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom datetime import datetime, date\nimport random\n\n\n\nlogging.basicConfig(format=\"[%(levelname)s] [%(asctime)s] - %(message)s\")","14b084a5":"class FlowersDataset(Dataset):\n    def __init__(self, path:str, shape=(256, 256), augmentations=None):\n        self.__images_classes_pathes = []\n        self.__shape = shape\n        self.__augmentations = augmentations\n        self.labels = []\n            \n        if os.path.exists(path):\n            self.__path = path\n            folders = [_ for _ in os.listdir(self.__path) if _ != \"flowers\"]\n\n            for folder in folders:\n                folder_path = os.path.join(self.__path, folder)\n                if os.path.isdir(folder_path) and os.path.exists(folder_path):\n                    images = os.listdir(folder_path)\n                    self.labels.append(folder)\n                    \n                    \n                    if len(images):\n                        for image in images:\n                            if image.endswith(\"jpeg\") or image.endswith(\"png\") or image.endswith(\"jpg\"):\n                                image_path = os.path.join(folder_path, image)\n                                self.__images_classes_pathes.append((folder, image_path))\n                    else:\n                        message = f\"Images for folder '{folder}' weren't found!\"\n                        print(message)\n                else:\n                    message = f\"'{folder}' is not folder and it will be skipped!\"\n                    print(message)\n                \n        else:\n            message = f\"Path '{path}' does not exists!\"\n            raise Exception(message)\n    \n        self.__images_classes_pathes = np.array(self.__images_classes_pathes)\n        \n    \n    \n    \n    def __load_image(self, path, channels=\"RGB\"):\n        width, height = self.__shape\n        loader = A.Compose([\n            A.Resize(width, height),\n            ToTensor(),\n        ])\n        \n        image = np.array(Image.open(path).convert(channels))\n        return loader(image=image)[\"image\"]\n    \n    def __len__(self):\n        return len(self.__images_classes_pathes)\n    \n    \n    def __getitem__(self, index):\n        item = self.__images_classes_pathes[index]\n        label, image_path = item\n        \n        image = self.__load_image(image_path, channels=\"RGB\")\n        \n        if self.__augmentations is not None:\n            image = self.__augmentations(image=image.permute(1, 2, 0).numpy())[\"image\"]\n        \n        label = self.labels.index(label)\n        \n        return Dict({\n            \"label\": label,\n            \"image\": image\n        })\n    \n    \nclass Trainer:\n    def __init__(self, model, criterion, optimizer, scheduler=None, metric=None, device=\"cpu\"):\n        self.__model = model\n        self.__criterion = criterion\n        self.__optimizer = optimizer\n        self.__scheduler = scheduler\n        self.__metric = metric\n        self.__device = device\n        self.logs = Dict({})\n        \n        \n    def __log(self, logs):\n        for k,v in logs.items():\n            if k not in self.logs.keys():\n                self.logs[k] = []\n                \n            self.logs[k].append(v)\n        \n        \n    def __make_checkpoint(self, info, path=f\"checkpoints\/checkpoint.pt\"):\n        checkpoint_info = {\n           **info,\n            \"optimizer_state\": self.__optimizer.state_dict(),\n            \"model_state\": self.__model.state_dict(),\n        }\n        \n        torch.save(checkpoint_info, path)\n    \n    \n    def evaluate(self, loader):\n        loss = 0\n        length = len(loader)\n        with torch.no_grad():\n            loop = tqdm(loader)\n            loop.set_description(\"Evaluating\")\n            for batch in loop:\n                torch.cuda.empty_cache()\n                images = batch[\"images\"].to(self.__device)\n                labels = batch[\"labels\"].to(\"cpu\")\n                        \n                output = self.__model(images).to(\"cpu\")\n                        \n                batch_loss = criterion(output, labels)\n                loss += batch_loss.item()\n            \n        loss \/= length\n        \n        return loss\n    \n    def save(self, path=\"model.pt\"):\n        torch.save(self.__model.state_dict(), path)\n        \n        \n    def fit(self, loader, epochs=10, validation_loader=None):\n        model.to(self.__device)\n        train_length = len(loader)\n        \n        best_validation_loss = 0\n        for epoch in range(epochs):\n            epoch_loss = 0\n            \n            loop = tqdm(loader, position=0, leave=True)\n            loop.set_description(f\"Epoch [{epoch+1}\/{epochs}]\")\n            for batch in loop:\n                torch.cuda.empty_cache()\n                optimizer.zero_grad()\n                \n                images = batch[\"images\"].to(self.__device)\n                labels = batch[\"labels\"].to(\"cpu\")\n                \n                output = self.__model(images).to(\"cpu\")\n                predicted_class = torch.argmax(output, dim=1)\n        \n                loss = self.__criterion(output, labels)\n                \n                epoch_loss += loss.item()\n                \n                loop.set_postfix(loss=loss.item())\n                \n                loss.backward()\n                optimizer.step()\n            \n            epoch_loss \/= train_length\n            \n            self.__log({\"epochs\": epoch+1, \"train_loss\": epoch_loss})\n            loop.set_postfix(loss=epoch_loss)\n            \n            if validation_loader is not None:\n                validation_loss = self.evaluate(validation_loader)\n                self.__log({\"validation_loss\": validation_loss})\n            \n                rounded_loss = np.round(validation_loss, 3)\n                if rounded_loss > best_validation_loss:\n                    now = datetime.now().strftime(\"%H:%M:%S %d.%m.%Y\")\n                    checkpoint_path = f\"{rounded_loss}_{now}.pt\"\n\n                    checkpoint_info =  {\n                        \"epoch\": epoch+1,\n                        \"loss\": validation_loss\n                    }\n\n                    self.__make_checkpoint(info=checkpoint_info, path=checkpoint_path)\n\n                    best_validation_loss = rounded_loss\n                \n                if self.__scheduler is not None:\n                    self.__scheduler.step(validation_loss)\n                    \n            else:\n                if self.__scheduler is not None:\n                    self.__scheduler.step()\n            \n            lr = self.__optimizer.defaults[\"lr\"]\n            self.__log({\"lr\": lr})\n            \n    \ndef collate_fn(batch):\n    images, labels = [], []\n    \n    for item in batch:\n        label, image = item.label, item.image.tolist()\n        \n        images.append(image)\n        labels.append(label)\n        \n    return {\n        \"images\": torch.tensor(images),\n        \"labels\": torch.tensor(labels)\n    }\n\n\ndef train_test_split(dataset, test_size=0.2):\n    length = len(dataset)\n    train_length = round(length * (1 - test_size))\n    test_length = length - train_length\n    \n    train_dataset, test_dataset = random_split(dataset, [train_length, test_length])\n    return train_dataset, test_dataset","c052732a":"config = Dict({\n    \"path\": \"..\/input\/flowers-recognition\/flowers\/\",\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"batch_size\": 16,\n    \"augmentations\": A.Compose([\n        A.Downscale(scale_min=0.6, scale_max=0.99, p=0.2),  \n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1), \n        A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.5, alpha_coef=0.05, p=0.5), \n        A.RandomContrast(limit=0.1, p=0.4),\n        A.RandomGamma(gamma_limit=(50, 150), p=0.4),\n        A.RandomBrightness(p=0.4),\n        A.OpticalDistortion(p=0.2),\n        A.Blur(blur_limit=2, p=0.2),\n        ToTensor(),\n    ]),\n    \"num_workers\": 0,\n})","6ab2a545":"dataset = FlowersDataset(path=config.path, augmentations=config.augmentations)","c6f9ad33":"rows, cols = 10, 10\nfig = plt.figure(figsize=(cols*3, rows*3))\nfor _ in range(rows * cols):\n    item = dataset[_*40]\n    label = item.label\n    class_ = dataset.labels[label]\n    image = item.image.permute(1, 2, 0).numpy()\n    ax = fig.add_subplot(rows, cols, _+1)\n    ax.imshow(image)\n    ax.set_title(class_, fontsize=15, fontfamily=\"serif\", y=1.02)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n\nfig.tight_layout()\nfig.show()","4394e082":"train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n\nprint(f\"Train Images: {len(train_dataset)}\")\nprint(f\"Test Images: {len(test_dataset)}\")","1b8b8c00":"train_loader = DataLoader(train_dataset,\n                          batch_size=config.batch_size,\n                          shuffle=True, \n                          num_workers=config.num_workers,\n                          pin_memory=True,\n                          collate_fn=collate_fn)\n\n\ntest_loader = DataLoader(test_dataset,\n                         batch_size=config.batch_size*2,\n                         shuffle=False,\n                         num_workers=config.num_workers,\n                         pin_memory=True,\n                         collate_fn=collate_fn)","f6819e80":"model = models.resnet50(pretrained=True)","11a4609e":"model","49976682":"for parameter in model.parameters():\n    parameter.require_grad = False\n    \nclassifier = nn.Sequential(\n    nn.Linear(in_features=2048, out_features=5),\n    nn.Softmax(dim=1),\n)\nmodel.fc = classifier","4186edd6":"optimizer = AdamW(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(optimizer, patience=1)\n\ntrainer = Trainer(model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler, device=config.device)\ntrainer.fit(train_loader, validation_loader=test_loader, epochs=10)","78966aa9":"fig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot()\nax.grid(color=\"lightgrey\", axis=\"both\", alpha=0.7, zorder=0)\nsns.lineplot(x=trainer.logs.epochs, y=trainer.logs.train_loss, color=\"red\", zorder=2, marker=\"o\", label=\"Train\", ax=ax)\nsns.lineplot(x=trainer.logs.epochs, y=trainer.logs.validation_loss, label=\"Validation\", color=\"green\", marker=\"o\", zorder=2, ax=ax)\n\nfor spine in [\"top\", \"right\", \"left\", \"bottom\"]:\n    ax.spines[spine].set_visible(False)\n\nax.xaxis.set_tick_params(labelsize=12, size=0, pad=5)\nax.yaxis.set_tick_params(labelsize=12, size=0, pad=5)\nax.set_title(label=\"Train & Validation Losses\", fontweight=\"bold\", fontfamily=\"serif\", fontsize=20, y=1.02, loc=\"left\")\nax.set_xlabel(\"Epoch\", fontfamily=\"serif\", fontsize=15,  labelpad=7)\nax.set_ylabel(\"Loss\", fontfamily=\"serif\", fontsize=15, labelpad=7)\nax.set_xticks(trainer.logs.epochs)\n\nax.legend()\nfig.show()","68ee14ae":"fig = plt.figure(figsize=(3*5, 10*5))\nmodel.to(config.device)\nmodel.eval()\nwith torch.no_grad():\n    for idx, batch in enumerate(test_loader):\n        torch.cuda.empty_cache()\n        \n        images = batch[\"images\"].to(config.device)\n        labels = batch[\"labels\"].to(\"cpu\")\n        \n        output = model(images).to(\"cpu\")\n        predicted_label = torch.argmax(output, dim=1)\n        \n        ax = fig.add_subplot(10, 3, idx+1)\n        ax.imshow(images[0].cpu().permute(1, 2, 0))\n        ax.set_title(f\"Predicted {dataset.labels[predicted_label[0]]}\\nTrue: {dataset.labels[labels[0]]}\", fontsize=10, fontfamily=\"serif\")\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n\nfig.show()","8a534c8e":"<h1>Testing<\/h1>","faefb523":"<h1>References:<\/h1>\n<h3>Libraries:<\/h3>\n<ul>\n    <li><a href=\"https:\/\/pytorch.org\/\">PyTorch - https:\/\/pytorch.org\/<\/a>\n    <li><a href=\"https:\/\/albumentations.ai\/\">Albumentations - https:\/\/albumentations.ai\/<\/a>\n    <li><a href=\"https:\/\/seaborn.pydata.org\/\">seaborn - https:\/\/seaborn.pydata.org\/<\/a>\n    <li><a href=\"https:\/\/matplotlib.org\/\">matplotlib - https:\/\/matplotlib.org\/<\/a>\n    <li><a href=\"https:\/\/tqdm.github.io\/\">tqdm - https:\/\/tqdm.github.io\/<\/a> \n    <li><a href=\"https:\/\/github.com\/mewwts\/addict\">addict - https:\/\/github.com\/mewwts\/addict<\/a>\n    <li><a href=\"https:\/\/docs.python.org\/3\/library\/os.html\">os - https:\/\/docs.python.org\/3\/library\/os.html<\/a>\n    <li><a href=\"https:\/\/docs.python.org\/3\/library\/random.html\">random - https:\/\/docs.python.org\/3\/library\/random.html<\/a>\n    <li><a href=\"https:\/\/numpy.org\/\">numpy - https:\/\/numpy.org\/<\/a>\n    <li><a href=\"https:\/\/docs.python.org\/3\/library\/datetime.html\">datetime - https:\/\/docs.python.org\/3\/library\/datetime.html<\/a>\n    <li><a href=\"https:\/\/docs.python.org\/3\/library\/logging.html\">logging - https:\/\/docs.python.org\/3\/library\/logging.html<\/a>\n    <li><a href=\"https:\/\/python-pillow.org\/\">pillow - https:\/\/python-pillow.org\/<\/a>\n<\/ul>\n\n<h3>Papers:<\/h3>\n<ul>\n    <li><a href=\"https:\/\/arxiv.org\/abs\/1512.03385\">ResNet - https:\/\/arxiv.org\/abs\/1512.03385<\/a>\n    <li><a href=\"https:\/\/arxiv.org\/abs\/1711.05101\">Adam - https:\/\/arxiv.org\/abs\/1711.05101<\/a>\n    <li><a href=\"https:\/\/machinelearningmastery.com\/cross-entropy-for-machine-learning\/\">Cross Entropy Loss - https:\/\/machinelearningmastery.com\/cross-entropy-for-machine-learning\/<\/a>\n    \n<\/ul>","11f01801":"<h1>Config<\/h1>","20670955":"<img src=\"https:\/\/www.wallpapertip.com\/wmimgs\/20-204416_beautiful-daisy-wallpapers-pc-high-resolution-daisy-flower.jpg\" width=750 height=500>\n\n<h1>Introducing to the Dataset<\/h1>\n\n<p style=\"font-size: 17px;\">\n    Author: <a href=\"https:\/\/www.kaggle.com\/alxmamaev\">Alexander Mamaev<\/a>\n<\/p>\n\n<p style=\"font-size: 15px;\">\n    Dataset contains 4242 images of flowers.\n    The data collection is based on the data Flicr, Google images, Yandex images.<br>The pictures are divided into five classes: <b>chamomile<\/b>, <b>tulip<\/b>, <b>rose<\/b>, <b>sunflower<\/b>, <b>dandelion<\/b>.<br>\nFor each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!\n<\/p>","b0353c5b":"<h1>Load dataset<\/h1>","54584581":"<p style=\"font-size: 15px\">In this image shows <b>ResNet34<\/b>, but only one differ from <b>ResNet50<\/b> it has 34 layers with Residual Connections, so <b>ResNet50<\/b> has 50 layers with Residual Connections.<\/p>","5376cd55":"<p style=\"font-size: 15px;\">We see that some of images have other objects instead of flowers and it is impossible to remove them, because we don't know much there, so our neural network will degrade because of this noise!<\/p>","ebb2e28b":"<h1>Modelling ResNet50<\/h1>\n <p style=\"font-size: 15px;\">I have choosed this architecture, because:<\/p>\n<ul>\n    <li> PyTorch provides this architecture in <b>torchvision.models<\/b> module\n    <li> It uses <b>Residual Connections<\/b> approach, which avoids \"gradient vanishing\" and speed up the training.\n    \n<\/ul>","fd7f0726":"<h1>Classes & Functions<\/h1>","1ad7d45f":"<h1>Fine-Tuning (Transfer Learning)<\/h1>","444576aa":"<h1>Installing & Importing requirements<\/h1>","54f8e5f9":"<p style=\"font-size: 15px\">We see that last (fc) last is for classification, thus we should change this layer by our version, and freeze layers above.<\/p> ","b6469324":"<img src=\"https:\/\/ichi.pro\/assets\/images\/max\/724\/0*bBiMda6n_xKUCSnj.png\" width=250 height=500>","087130c9":"<h1>Training<\/h1>","6ece6ba8":"<h2>Plotting Train & Validation Losses<\/h2>"}}