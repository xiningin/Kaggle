{"cell_type":{"74c665d0":"code","14904e3e":"code","5f52105c":"code","d1be165e":"code","91ccfb32":"code","dd41ef42":"code","6a029066":"code","cac34726":"code","e0527280":"code","1026564a":"code","f8009f25":"code","da38f716":"code","9232944a":"code","bd185d0e":"code","0aa810c3":"code","8976d589":"code","0df7fbc6":"code","705604c4":"code","4ac7c77b":"code","f74e9a2e":"code","d572a737":"code","86c16b9a":"code","e01555aa":"code","c4066c9b":"code","cff91c24":"code","f3e90156":"code","50bbccf0":"code","2d60d361":"code","17c7195c":"code","c095f19f":"code","468b085b":"code","e1033081":"code","b9a25301":"code","4959f4ba":"code","3b61ccfe":"markdown","0d54de3b":"markdown","a76e60c4":"markdown","1a8e27ed":"markdown","df6e8b85":"markdown","e4ad9287":"markdown","bfd049bd":"markdown","bdf2dab1":"markdown","f6d33e80":"markdown","9bc99a5e":"markdown","3249c6e8":"markdown","1f29c047":"markdown","3a7eec49":"markdown","1ebdef45":"markdown","7ba06aad":"markdown","3959a392":"markdown"},"source":{"74c665d0":"!ls","14904e3e":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport sys\nimport collections\nsys.path.extend(['..\/input\/bert-joint-baseline\/'])\nsys.path.extend(['..\/input\/bert1305060\/'])\nimport bert_utils1305060 as bert_utils\nimport modeling \n\nimport tokenization\nimport json\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5f52105c":"on_kaggle_server = os.path.exists('\/kaggle')\nnq_test_file = '..\/input\/tensorflow2-question-answering\/simplified-nq-test.jsonl' \nnq_train_file = '..\/input\/tensorflow2-question-answering\/simplified-nq-train.jsonl'\npublic_dataset = os.path.getsize(nq_test_file)<20_000_000\nprivate_dataset = os.path.getsize(nq_test_file)>=20_000_000","d1be165e":"if True:\n    import importlib\n    importlib.reload(bert_utils)","91ccfb32":"with open('..\/input\/bert-joint-baseline\/bert_config.json','r') as f:\n    config = json.load(f)\nprint(json.dumps(config,indent=4))\n","dd41ef42":"class TDense(tf.keras.layers.Layer):\n    def __init__(self,\n                 output_size,\n                 kernel_initializer=None,\n                 bias_initializer=\"zeros\",\n                **kwargs):\n        super().__init__(**kwargs)\n        self.output_size = output_size\n        self.kernel_initializer = kernel_initializer\n        self.bias_initializer = bias_initializer\n    def build(self,input_shape):\n        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n        if not (dtype.is_floating or dtype.is_complex):\n          raise TypeError(\"Unable to build `TDense` layer with \"\n                          \"non-floating point (and non-complex) \"\n                          \"dtype %s\" % (dtype,))\n        input_shape = tf.TensorShape(input_shape)\n        if tf.compat.dimension_value(input_shape[-1]) is None:\n          raise ValueError(\"The last dimension of the inputs to \"\n                           \"`TDense` should be defined. \"\n                           \"Found `None`.\")\n        last_dim = tf.compat.dimension_value(input_shape[-1])\n        self.input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: last_dim})\n        self.kernel = self.add_weight(\n            \"kernel\",\n            shape=[self.output_size,last_dim],\n            initializer=self.kernel_initializer,\n            dtype=self.dtype,\n            trainable=True)\n        self.bias = self.add_weight(\n            \"bias\",\n            shape=[self.output_size],\n            initializer=self.bias_initializer,\n            dtype=self.dtype,\n            trainable=True)\n        super(TDense, self).build(input_shape)\n    def call(self,x):\n        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n    \ndef mk_model(config):\n    seq_len = config['max_position_embeddings']\n    unique_id  = tf.keras.Input(shape=(1,),dtype=tf.int64,name='unique_id')\n    input_ids   = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_ids')\n    input_mask  = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_mask')\n    segment_ids = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='segment_ids')\n    BERT = modeling.BertModel(config=config,name='bert')\n    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n                                          input_mask=input_mask,\n                                          input_type_ids=segment_ids)\n    \n    logits = TDense(2,name='logits')(sequence_output)\n    start_logits,end_logits = tf.split(logits,axis=-1,num_or_size_splits= 2,name='split')\n    start_logits = tf.squeeze(start_logits,axis=-1,name='start_squeeze')\n    end_logits   = tf.squeeze(end_logits,  axis=-1,name='end_squeeze')\n    \n    ans_type      = TDense(5,name='ans_type')(pooled_output)\n    return tf.keras.Model([input_ for input_ in [unique_id,input_ids,input_mask,segment_ids] \n                           if input_ is not None],\n                          [unique_id,start_logits,end_logits,ans_type],\n                          name='bert-baseline')    ","6a029066":"small_config = config.copy()\nsmall_config['vocab_size']=16\nsmall_config['hidden_size']=64\nsmall_config['max_position_embeddings'] = 32\nsmall_config['num_hidden_layers'] = 4\nsmall_config['num_attention_heads'] = 4\nsmall_config['intermediate_size'] = 256\nsmall_config","cac34726":"model= mk_model(config)","e0527280":"model.summary()","1026564a":"cpkt = tf.train.Checkpoint(model=model)\ncpkt.restore('..\/input\/bert-joint-baseline\/model_cpkt-1').assert_consumed()","f8009f25":"class DummyObject:\n    def __init__(self,**kwargs):\n        self.__dict__.update(kwargs)\n\n'''FLAGS=DummyObject(skip_nested_contexts=True,\n                 max_position=50,\n                 max_contexts=48,\n                 max_query_length=64,\n                 max_seq_length=512,\n                 doc_stride=128,\n                 include_unknowns=-1.0,\n                 n_best_size=20,\n                 max_answer_length=30)'''\nFLAGS=bert_utils.FLAGS\nprint(FLAGS.max_contexts)\nprint(FLAGS.n_best_size)\nprint(FLAGS.max_answer_length)","da38f716":"import tqdm\neval_records = \"..\/input\/bert-joint-baseline\/nq-test.tfrecords\"\n#nq_test_file = '..\/input\/tensorflow2-question-answering\/simplified-nq-test.jsonl'\nif on_kaggle_server and private_dataset:\n    eval_records='nq-test.tfrecords'\nif not os.path.exists(eval_records):\n    # tf2baseline.FLAGS.max_seq_length = 512\n    eval_writer = bert_utils.FeatureWriter(\n        filename=os.path.join(eval_records),\n        is_training=False)\n\n    tokenizer = tokenization.FullTokenizer(vocab_file='..\/input\/bert-joint-baseline\/vocab-nq.txt', \n                                           do_lower_case=True)\n\n    features = []\n    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n                                                   is_training=False,\n                                                   output_fn=eval_writer.process_feature,\n                                                   collect_stat=False)\n\n    n_examples = 0\n    tqdm_notebook= tqdm.tqdm_notebook if not on_kaggle_server else None\n    for examples in bert_utils.nq_examples_iter(input_file=nq_test_file, \n                                           is_training=False,\n                                           tqdm=tqdm_notebook):\n        for example in examples:\n            n_examples += convert(example)\n\n    eval_writer.close()\n    print('number of test examples: %d, written to file: %d' % (n_examples,eval_writer.num_features))","9232944a":"seq_length = FLAGS.max_seq_length #config['max_position_embeddings']\nname_to_features = {\n      \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n  }\n\ndef _decode_record(record, name_to_features=name_to_features):\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if name != 'unique_id': #t.dtype == tf.int64:\n            t = tf.cast(t, dtype=tf.int32)\n        example[name] = t\n\n    return example\n\ndef _decode_tokens(record):\n    return tf.io.parse_single_example(serialized=record, \n                                      features={\n                                          \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n                                          \"token_map\" :  tf.io.FixedLenFeature([seq_length], tf.int64)\n                                      })\n      \n","bd185d0e":"raw_ds = tf.data.TFRecordDataset(eval_records)\ntoken_map_ds = raw_ds.map(_decode_tokens)\ndecoded_ds = raw_ds.map(_decode_record)\nds = decoded_ds.batch(batch_size=16,drop_remainder=False)","0aa810c3":"result=model.predict_generator(ds,verbose=1)","8976d589":"np.savez_compressed('bert-joint-baseline-output.npz',\n                    **dict(zip(['uniqe_id','start_logits','end_logits','answer_type_logits'],\n                               result)))","0df7fbc6":"Span = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\"])","705604c4":"class ScoreSummary(object):\n  def __init__(self):\n    self.predicted_label = None\n    self.short_span_score = None\n    self.cls_token_score = None\n    self.answer_type_logits = None","4ac7c77b":"class EvalExample(object):\n  \"\"\"Eval data available for a single example.\"\"\"\n  def __init__(self, example_id, candidates):\n    self.example_id = example_id\n    self.candidates = candidates\n    self.results = {}\n    self.features = {}","f74e9a2e":"def get_best_indexes(logits, n_best_size):\n  \"\"\"Get the n-best logits from a list.\"\"\"\n  index_and_score = sorted(\n      enumerate(logits[1:], 1), key=lambda x: x[1], reverse=True)\n  best_indexes = []\n  for i in range(len(index_and_score)):\n    if i >= n_best_size:\n      break\n    best_indexes.append(index_and_score[i][0])\n  return best_indexes\n\ndef top_k_indices(logits,n_best_size,token_map):\n    indices = np.argsort(logits[1:])+1\n    indices = indices[token_map[indices]!=-1]\n    return indices[-n_best_size:]","d572a737":"def compute_predictions(example):\n  \"\"\"Converts an example into an NQEval object for evaluation.\"\"\"\n  predictions = []\n  n_best_size = FLAGS.n_best_size\n  max_answer_length = FLAGS.max_answer_length\n  i = 0\n  for unique_id, result in example.results.items():\n    if unique_id not in example.features:\n      raise ValueError(\"No feature found with unique_id:\", unique_id)\n    token_map = np.array(example.features[unique_id][\"token_map\"]) #.int64_list.value\n    start_indexes = top_k_indices(result.start_logits,n_best_size,token_map)\n    if len(start_indexes)==0:\n        continue\n    end_indexes   = top_k_indices(result.end_logits,n_best_size,token_map)\n    if len(end_indexes)==0:\n        continue\n    indexes = np.array(list(np.broadcast(start_indexes[None],end_indexes[:,None])))  \n    indexes = indexes[(indexes[:,0]<indexes[:,1])*(indexes[:,1]-indexes[:,0]<max_answer_length)]\n    for start_index,end_index in indexes:\n        summary = ScoreSummary()\n        summary.short_span_score = (\n            result.start_logits[start_index] +\n            result.end_logits[end_index])\n        summary.cls_token_score = (\n            result.start_logits[0] + result.end_logits[0])\n        summary.answer_type_logits = result.answer_type_logits-result.answer_type_logits.mean()\n        start_span = token_map[start_index]\n        end_span = token_map[end_index] + 1\n\n        # Span logits minus the cls logits seems to be close to the best.\n        score = summary.short_span_score - summary.cls_token_score\n        predictions.append((score, i, summary, start_span, end_span))\n        i += 1 # to break ties\n\n  # Default empty prediction.\n  score = -10000.0\n  short_span = Span(-1, -1)\n  long_span  = Span(-1, -1)\n  summary    = ScoreSummary()\n\n  if predictions:\n    score, _, summary, start_span, end_span = sorted(predictions, reverse=True)[0]\n    short_span = Span(start_span, end_span)\n    for c in example.candidates:\n      start = short_span.start_token_idx\n      end = short_span.end_token_idx\n      ## print(c['top_level'],c['start_token'],start,c['end_token'],end)\n      if c[\"top_level\"] and c[\"start_token\"] <= start and c[\"end_token\"] >= end:\n        long_span = Span(c[\"start_token\"], c[\"end_token\"])\n        break\n\n  summary.predicted_label = {\n      \"example_id\": int(example.example_id),\n      \"long_answer\": {\n          \"start_token\": int(long_span.start_token_idx),\n          \"end_token\": int(long_span.end_token_idx),\n          \"start_byte\": -1,\n          \"end_byte\": -1\n      },\n      \"long_answer_score\": float(score),\n      \"short_answers\": [{\n          \"start_token\": int(short_span.start_token_idx),\n          \"end_token\": int(short_span.end_token_idx),\n          \"start_byte\": -1,\n          \"end_byte\": -1\n      }],\n      \"short_answer_score\": float(score),\n      \"yes_no_answer\": \"NONE\",\n      \"answer_type_logits\": summary.answer_type_logits.tolist(),\n      # here:\n      \"answer_type\": int(np.argmax(summary.answer_type_logits))\n  }\n\n  return summary","86c16b9a":"def compute_pred_dict(candidates_dict, dev_features, raw_results,tqdm=None):\n    \"\"\"Computes official answer key from raw logits.\"\"\"\n    raw_results_by_id = [(int(res.unique_id),1, res) for res in raw_results]\n\n    examples_by_id = [(int(k),0,v) for k, v in candidates_dict.items()]\n  \n    features_by_id = [(int(d['unique_id']),2,d) for d in dev_features] \n  \n    # Join examples with features and raw results.\n    examples = []\n    print('merging examples...')\n    merged = sorted(examples_by_id + raw_results_by_id + features_by_id)\n    print('done.')\n    for idx, type_, datum in merged:\n        if type_==0: #isinstance(datum, list):\n            examples.append(EvalExample(idx, datum))\n        elif type_==2: #\"token_map\" in datum:\n            examples[-1].features[idx] = datum\n        else:\n            examples[-1].results[idx] = datum\n\n    # Construct prediction objects.\n    print('Computing predictions...')\n   \n    nq_pred_dict = {}\n    #summary_dict = {}\n    if tqdm is not None:\n        examples = tqdm(examples)\n    for e in examples:\n        summary = compute_predictions(e)\n        #summary_dict[e.example_id] = summary\n        nq_pred_dict[e.example_id] = summary.predicted_label\n\n    return nq_pred_dict\n","e01555aa":"def read_candidates_from_one_split(input_path):\n  \"\"\"Read candidates from a single jsonl file.\"\"\"\n  candidates_dict = {}\n  print(\"Reading examples from: %s\" % input_path)\n  if input_path.endswith(\".gz\"):\n    with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n      for index, line in enumerate(input_file):\n        e = json.loads(line)\n        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n        \n  else:\n    with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n      for index, line in enumerate(input_file):\n        e = json.loads(line)\n        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n        # candidates_dict['question'] = e['question_text']\n  return candidates_dict","c4066c9b":"def read_candidates(input_pattern):\n  \"\"\"Read candidates with real multiple processes.\"\"\"\n  input_paths = tf.io.gfile.glob(input_pattern)\n  final_dict = {}\n  for input_path in input_paths:\n    final_dict.update(read_candidates_from_one_split(input_path))\n  return final_dict","cff91c24":"all_results = [bert_utils.RawResult(*x) for x in zip(*result)]\n    \nprint (\"Going to candidates file\")\n\ncandidates_dict = read_candidates('..\/input\/tensorflow2-question-answering\/simplified-nq-test.jsonl')\n\nprint (\"setting up eval features\")\n\neval_features = list(token_map_ds)\n\nprint (\"compute_pred_dict\")\n\ntqdm_notebook= tqdm.tqdm_notebook\nnq_pred_dict = compute_pred_dict(candidates_dict, \n                                       eval_features,\n                                       all_results,\n                                      tqdm=tqdm_notebook)\n\npredictions_json = {\"predictions\": list(nq_pred_dict.values())}\n\nprint (\"writing json\")\n\nwith tf.io.gfile.GFile('predictions.json', \"w\") as f:\n    json.dump(predictions_json, f, indent=4)","f3e90156":"def create_short_answer(entry):\n    answer = []    \n    if entry['answer_type'] == 0:\n        return \"\"\n    \n    elif entry['answer_type'] == 1:\n        return 'YES'\n    \n    elif entry['answer_type'] == 2:\n        return 'NO'\n        \n    elif entry[\"short_answer_score\"] < 7.5:\n        return \"\"\n    \n    else:\n        for short_answer in entry[\"short_answers\"]:\n            if short_answer[\"start_token\"] > -1:\n                answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n    \n        return \" \".join(answer)\n\ndef create_long_answer(entry):\n    \n    answer = []\n    \n    if entry['answer_type'] == 0:\n        return ''\n    \n    elif entry[\"long_answer_score\"] < 1.5:\n        return \"\"\n\n    elif entry[\"long_answer\"][\"start_token\"] > -1:\n        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n        return \" \".join(answer)","50bbccf0":"test_answers_df = pd.read_json(\"..\/working\/predictions.json\")\nfor var_name in ['long_answer_score','short_answer_score','answer_type']:\n    test_answers_df[var_name] = test_answers_df['predictions'].apply(lambda q: q[var_name])\ntest_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\ntest_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\ntest_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n\nlong_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\nshort_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))\n\ntest_answers_df.head()","2d60d361":"sample_submission = pd.read_csv(\"..\/input\/tensorflow2-question-answering\/sample_submission.csv\")\n\nlong_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\nshort_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings\n","17c7195c":"sample_submission.to_csv('submission.csv', index=False)","c095f19f":"sample_submission.head()","468b085b":"yes_answers = sample_submission[sample_submission['PredictionString'] == 'YES']\nyes_answers","e1033081":"no_answers = sample_submission[sample_submission['PredictionString'] == 'NO']\nno_answers","b9a25301":"blank_answers = sample_submission[sample_submission['PredictionString'] == '']\nblank_answers.head()","4959f4ba":"blank_answers.count()","3b61ccfe":"### Checkpoint","0d54de3b":"* Balnk Answers","a76e60c4":"### We'll be grateful if someone gets a better understanding and can share what really impacts the assessment. No need to share code, just knowledge.\n### Thank you!","1a8e27ed":"* No Answers","df6e8b85":"## 2- Main Change\n#### Here is the small, but main change: we created an if to check the predicted response type and thus filter \/ identify the responses that are passed to the submission file.","e4ad9287":"### If this notebook is useful to you, I appreciate your upvote :)\n\n### Now, after the change and official explanation of the [metric](https:\/\/www.kaggle.com\/c\/tensorflow2-question-answering\/overview\/evaluation), we can work more efficiently.\n### As we got here with the help of several kagglers, I will not change it to private.\n\n### This notebook is an edition of [bert joint baseline notebook](https:\/\/www.kaggle.com\/prokaj\/bert-joint-baseline-notebook\/notebook). With some modifications, it was possible to slightly improve the code and get the YES \/ NO answers and leave the unknowns blank.","bfd049bd":"### Splitting the Dataset","bdf2dab1":"### Generating the Submission File","f6d33e80":"* Yes Answers","9bc99a5e":"## 1- Understanding the code\n#### For a better understanding, I will briefly explain here.\n#### In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https:\/\/github.com\/google-research\/language\/blob\/master\/language\/question_answering\/bert_joint\/run_nq.py) can be:\n1. UNKNOWN = 0\n2. YES = 1\n3. NO = 2\n4. SHORT = 3\n5. LONG = 4","3249c6e8":"### I am only sharing modifications that I believe may help. I left out Tunning and any significant code changes I made.","1f29c047":"### Importing Libraries","3a7eec49":"**The whole process of solving this problem in my way\uff1a**\n\n***source***:fork from bert joint baseline notebook\n\n***problems* **(before start this competition): \n\n1.the first time to deal with QA \n\n2.just have a month to go\n\n3.need to understand source codes in tf2.0 ver(just familiar with pytorch)\n\n4.GPU (just have a 1080ti)\n\n**Based on current problems, I came up with some solutions:**\n\n1.read tf2.0 source code\uff08necessary\uff09 #done\n\n2.fintune bert-joint-model (the simplest solution to get started and indeed understand the whole process)#done\n\n3.try to convert tf2.0 code to pytorch ver (but have not a better result untill now )#done\n\n4.change the pipeline(Some effort)\n\nIn this competition, **I think the first and second points are the most important for the first time runners**\uff0cand the** main changes** of mine are as follows\uff1a\n\n1.FLAGS=DummyObject(skip_nested_contexts=True,\n                 max_position=50,\n                 **max_contexts=130**,\n                 max_query_length=64,\n                 max_seq_length=512,\n                 doc_stride=128,\n                 include_unknowns=0.02,\n                 n_best_size=50,\n                 max_answer_length=60)\n                 \n2.entry[\"short_answer_score\"] < 7.5:\n\n3.entry[\"long_answer_score\"] < 1.5:\n\n\n**I hope to be of some help to those who have just started a new task with limited time and resources**\n","1ebdef45":"### Setting the Flags","7ba06aad":"### Filtering the Answers","3959a392":"### Creating a DataFrame"}}