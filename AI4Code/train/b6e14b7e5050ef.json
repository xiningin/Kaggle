{"cell_type":{"e05d22fc":"code","9989b027":"code","6cdd4156":"code","1835b94d":"code","96d7ab39":"code","57b65d12":"code","ac590755":"code","58e44c67":"code","67d9d5b6":"code","db1a391c":"code","fb508a86":"code","1767735c":"code","51bc728e":"code","d753b2d8":"code","afb148d0":"code","6e00b1c9":"code","6253d3a0":"code","146701fe":"code","00a6b6a0":"code","feb80d2a":"code","e8a3a2f3":"code","6a3e525b":"code","0236755c":"code","0402d84e":"code","c759e2ac":"code","2431b3e9":"code","5a523de8":"markdown","df682b46":"markdown","1e939d25":"markdown","abc5475c":"markdown","dcd20569":"markdown","34848039":"markdown","64184966":"markdown","59decea9":"markdown","95d039e6":"markdown","a7339e45":"markdown","7b267887":"markdown","1051c392":"markdown","ad17083b":"markdown","b00cffa1":"markdown","4394020d":"markdown","bc77ce45":"markdown","4f8e1a35":"markdown","993c4819":"markdown","b2a73a72":"markdown","a00d4914":"markdown","f26d3e8e":"markdown","f0a93a91":"markdown"},"source":{"e05d22fc":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","9989b027":"! pip install vit-keras","6cdd4156":"from vit_keras import vit, utils\n\nimage_size = 384\nhidden_size = 768\nn_encoder_blocks = 12\nclasses = utils.get_imagenet_classes()\n\nprint(\"image size:\", image_size)\nprint(\"hidden size:\", hidden_size)\nprint(\"number of encoder blocks:\", n_encoder_blocks)\nprint(\"number of classes:\", len(classes))","1835b94d":"vit_b16_model = vit.vit_b16(\n    image_size=image_size,\n    classes=len(classes),\n    activation='sigmoid',\n    pretrained=True,\n    include_top=True,\n    pretrained_top=True\n)\n\ntf.keras.utils.plot_model(vit_b16_model, show_shapes=True)","96d7ab39":"url = \\\n    'https:\/\/upload.wikimedia.org\/wikipedia\/' + \\\n    'commons\/d\/d7\/Granny_smith_and_cross_section.jpg'\nimage = utils.read(url, image_size)\nX = vit.preprocess_inputs(image).reshape(1, image_size, image_size, 3)\n\nplt.imshow(image)\nplt.axis(\"off\")\nplt.title(\"Sample Image\")\nplt.show()","57b65d12":"cls_token_layer = vit_b16_model.get_layer(\"class_token\")\ncls_token_var = cls_token_layer.cls.value()\n\ncls_token_var.shape","ac590755":"plt.figure(figsize=(12, 5))\nplt.plot(cls_token_var[0, 0])\nplt.title(\"Learned Values of Class Token\")\nplt.xlim(0, hidden_size)\nplt.ylim(-1.0, 1.0)\nplt.xlabel(\"embedding\")\nplt.ylabel(\"value\")\nplt.show()","58e44c67":"pos_emb_layer = vit_b16_model.get_layer(\"Transformer\/posembed_input\")\npos_emb_var = pos_emb_layer.pe.value()\n\npos_emb_var.shape","67d9d5b6":"def make_3d_line(pos, vals):\n    n_vals = len(vals)\n    x = list(range(n_vals))\n    y = [pos] * n_vals\n    z = vals\n    return x, y, z\n\ndef plot_pos_embedding(lines, title):\n    plt.figure(figsize=(10, 10))\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    for pos, (vals, label) in enumerate(lines):\n        ax.plot(*make_3d_line(pos, vals), label=label)\n    ax.set_xlim(0, hidden_size)\n    ax.set_zlim(-1.0, 1.0)\n    ax.set_xlabel(\"embedding\")\n    ax.set_zlabel(\"value\")\n    ax.set_yticks(np.arange(len(lines)))\n    ax.set_yticklabels([label for _, label in lines])\n    ax.legend(loc=\"lower right\")\n    ax.set_title(title)\n    plt.show()","db1a391c":"plot_pos_embedding([\n    (cls_token_var[0, 0], \"Class Token\"),\n    (pos_emb_var[0, 0], \"Pos Emb 0\")],\n    \"Class Token and Position Embedding 0\")","fb508a86":"plot_pos_embedding([\n    (pos_emb_var[0, 0], \"pos emb 0\"),\n    (pos_emb_var[0, 1], \"pos emb 1\"),\n    (pos_emb_var[0, 2], \"pos emb 2\")],\n    \"First 3 Position Embeddings\")","1767735c":"pos_emb_var_np = pos_emb_var.numpy()\n\npos_emb_var_np.shape","51bc728e":"corr_mat = np.corrcoef(pos_emb_var_np[0])\n\ncorr_mat.shape","d753b2d8":"plt.figure(figsize=(10, 8))\nsns.heatmap(corr_mat, vmin=-1.0, vmax=1.0)\nplt.title(\"Correlation Coefficients for Position Embeddings\")\nplt.show()","afb148d0":"trans_blocks = [\n    vit_b16_model.get_layer(\"Transformer\/encoderblock_{0}\".format(i)) \\\n    for i in range(n_encoder_blocks)\n]\ntrans_block_out_model = tf.keras.Model(\n    inputs=vit_b16_model.inputs,\n    outputs=[tb.output for tb in trans_blocks])\ntrans_block_out_model_output = trans_block_out_model(X)\n\nlen(trans_block_out_model_output)","6e00b1c9":"trans_block_weight_list = [\n    tb_out[1] for tb_out in trans_block_out_model_output]\ntrans_block_weights = np.concatenate(\n    trans_block_weight_list, axis=0)\n\ntrans_block_weights.shape","6253d3a0":"# draw only \"> draw_weight_thres\" to save time.\ndraw_weight_thres = 0.01\n\ndef make_bar3d_coords(weight):\n    x_size = len(weight)\n    y_size = len(weight[0])\n    total_size = x_size * y_size\n    \n    xi = list(range(x_size))\n    yi = list(range(y_size))\n    x, y = np.meshgrid(xi, yi)\n    x = x.flatten()\n    y = y.flatten()\n    z = np.zeros(total_size)\n    dx = np.ones(total_size)\n    dy = np.ones(total_size)\n    dz = weight.flatten()\n\n    sig_mask = (dz > draw_weight_thres)\n    x = x[sig_mask]\n    y = y[sig_mask]\n    z = z[sig_mask]\n    dx = dx[sig_mask]\n    dy = dy[sig_mask]\n    dz = dz[sig_mask]\n\n    return x, y, z, dx, dy, dz\n\ndef draw_weights(weight_iter, rows, cols, title_fmt):\n    plt.figure(figsize=(cols * 4, rows * 4))\n    for i, weight in enumerate(weight_iter):\n        ax = plt.subplot(rows, cols, i + 1, projection='3d')\n        x, y, z, dx, dy, dz = make_bar3d_coords(weight)\n        ax.bar3d(x, y, z, dx, dy, dz)\n        ax.set_title(title_fmt.format(i))\n        # https:\/\/stackoverflow.com\/questions\/37521910\/\n        # set-zlim-in-matplotlib-scatter3d\n        ax.set_zlim(0.0, 1.0)\n    plt.show()","146701fe":"draw_weights(\n    trans_block_weights[0], 4, 3, \"Block 0 Head {0} Weight\")","00a6b6a0":"draw_weights(\n    trans_block_weights[11], 4, 3, \"Block 11 Head {0} Weight\")","feb80d2a":"def block_n_head_iter(head):\n    for i in range(n_encoder_blocks):\n        yield trans_block_weights[i, head]\n\ndraw_weights(\n    block_n_head_iter(0), 4, 3, \"Block {0} Head 0 Weight\")","e8a3a2f3":"draw_weights(\n    block_n_head_iter(4), 4, 3, \"Block {0} Head 4 Weight\")","6a3e525b":"tf.keras.utils.plot_model(trans_blocks[0].mlpblock, show_shapes=True)","0236755c":"y = vit_b16_model.predict(X)\n\nprint(y.shape)","0402d84e":"plt.figure(figsize=(12, 5))\nplt.plot(y[0])\nplt.ylim(0.0, 1.0)\nplt.title(\"Prediction of A Sample Image\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Confidence\")\nplt.show()","c759e2ac":"class_idx = y[0].argmax()\nclass_val = y[0, class_idx]\n\nprint(\"index:\", class_idx)\nprint(\"value:\", class_val)","2431b3e9":"pred_class = classes[class_idx] # Granny smith\n\nplt.imshow(image)\nplt.axis(\"off\")\nplt.title(\"Pred: \" + pred_class)\nplt.show()","5a523de8":"## Class Token\n\nA learnable embedding is prepended to the sequence of embeded patches.\nThis prepended embedding is called \"class token\".\n\n* Class token is learnable, so [tf.Variable](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/Variable) is used to implement.\n* Each patch embeding has hidden size (768). So, the class token also has this size.\n* The axes of input are (batch, patch, embedding). The shape of class token is (1, 1, 768), 1 for batch and patch.\n* Class token is prepended to the sequence of embeddings in a batch. So, it needs to be broadcasted to batch size by [tf.broadcast_to](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/broadcast_to).\n* The shape of embeddings in a batch is (batch size, number of patches, hidden_size) = (batch size, 576, 768).\n* Broadcasted class token is prepended to each embeddings by [tf.concat](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/concat). It is concatenated along patch axis (1).\n* The output shape of the class token is (batch size, 1 + number of patches, hidden size) = (batch size, 577, 768).\n\n--- [vit_keras\/vit.py#L110](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L110) ---<br>\n![image.png](attachment:70e4ea28-d1ce-43e1-bb43-912401421c87.png)\n\n--- [vit_keras\/layers.py#L6](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L6) ---<br>\n![image.png](attachment:f79b589e-f767-48b3-953f-184996548bdb.png)","df682b46":"## Embedding\n\nThe input is put into Conv2D to make embedding.\nThe shape of the Conv2D output is (number of patches in height, number of patches in width, hidden size) = (image size \/ patch size, image size \/ patch size, hidden size) = (24, 24, 768)\n\n--- [vit-keras\/vit.py#L102](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L102) ---<br>\n![image.png](attachment:3ac25d5b-3951-42ec-811a-a2e52614c1d2.png)\n\nTotal size of the input is 442,368 (= 368 * 368 * 3).\nTotal size of the embedding is also 442,368 (= 24 * 24 * 768). Input size and embedding size are the same.","1e939d25":"The figures below show the weights for all 12 heads of the last block.","abc5475c":"The plots below show the weights for all 12 head of the first block.","dcd20569":"The figures below show the weights for head 4 of all blocks.","34848039":"Class token and Position Embedding 0 is much the same.","64184966":"The plot below shows the first 3 position embeddings. They look similar.","59decea9":"### Concat and Linear\n\n--- [vit_keras\/layers.py#L107](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L107) ---<br>\n![image.png](attachment:d7e4fad2-6704-4ba8-bf78-b8507311edbc.png)\n\n* Concat is actually transpose and reshape.\n* (batch, num_heads, patch, projection_dim) is transposed to (batch, patch, num_heads, projection_dim) = (batch, 577, 12, 64).\n* The transposed values are reshaped to (batch_size, -1, hidden_size) = (batch_size, 577, 768).\n* The reshaped results are sent to the linear, called 'combine_heads' in the implementation.\n* Number of units in 'combine_heads' is hidden size, so the output shape of the linear is (batch_size, 577, 768).\n\n## MLP\n\nThe figure below shows the structure of the MLP block.","95d039e6":"The figures below show the weights for head 0 of all blocks.","a7339e45":"## Multi-Head Attention\n\n### Overall Structure\n\nThe structure of the Multi-Head Attention is shown in the figure below.\n\n![image.png](attachment:9d570cea-eb45-41dc-8b81-73f0e20926d0.png)\n\n--- [vit_keras\/layers.py#L98](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L98) ---<br>\n![image.png](attachment:4df8d0c7-d702-429f-8514-eb33dfafca74.png)\n\n### Linear\n\n* Shape of the input is (batch, patch, embeding) = (batch, 577, 768), depicted by 'Q', 'K', and 'V' in the figure above.\n* The input is put into 3 [Dense](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense) layers, 'Linear' in the figure. They are called query_dense, key_dense, and value_dense in the implementation.\n* Shape of each Dense outputs is the same as the input, because number of dense units is set to the same as the number of embeding.\n\n### Separation\n\n* Each outputs of three Dense layers are separated for multiple heads by calling separate_heads().\n* The method does not actually separate the data, just reshaping and transposing.\n* (batch, patch, embedding) is reshaped to (batch, patch, num_heads, projection_dim). 'embedding' is divided into 'num_heads' and 'projection_dim'.\n* 'num_heads' is 12, and 'projection_dim' is 'hidden_size \/\/ num_heads' = '768 \/\/ 12' =  64.\n* (batch, patch, num_heads, projection_dim) is transposed to (batch, num_heads, patch, projection_dim) = (batch, 12, 577, 64).\n\n--- [vit_keras\/layers.py#L94](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L94) ---<br>\n![image.png](attachment:00dfdb78-a1be-4689-9275-dd002882a549.png)\n\n### Scaled Dot-Product Attention\n\nThe figure below shows the structure of Scaled Dot-Product Attention.\n\n![image.png](attachment:b40a79af-8c92-4a21-89b7-b5849186a71a.png)\n\n--- [vit_keras\/layers.py#L86](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L86) ---<br>\n![image.png](attachment:b2b65f64-12bb-49d2-8cee-ec0f4cbecea3.png)\n\n#### Mutmul of Q and K\n\n* 'query' and transposed 'key' is multipled.\n* The shape of 'query' and 'key' is (batch, num_heads, patch, projection_dim) = (batch, 12, 577, 64).\n* 'key' is transposed in tf.matmul(), since 'transpose_b' is set to True. Last 2 dimentions are swapped in tf.matmul(). So, the transposed shape is (batch, num_heads. projection_dim, patch) = (batch, 12, 64, 577).\n* Output shape of tf.matmul() is (batch, num_heads, patch, patch) = (batch, 12, 577, 577).\n\n#### Scale, Mask, and Softmax\n\n* The output of tf.matmul() is divided by sqrt(d). 'd' is the last dimention of 'key', which is projection_dim (64).\n* Mask is not applied here. It is used in the encoders of Transformer.\n* The scaled output is sent to softmax. The result shows how much portions are taken from each values.\n\n#### Matmul of Weight and V\n\n* The shape of the softmax output is (batch, num_heads, patch, patch) = (batch, 12, 577, 577),\n* The shape of 'V' is (batch, num_heads, patch, projection_dim) = (batch, 12, 577, 64).\n* The output shape of matmul is (batch, 12, 577, 64).\n* The result is values weighed by the softmax output.\n\n#### Weights\n\nLet's check what weights are generated in some heads of transformer blocks. A model is built to get the weights, then a sample image is fed into the model.","7b267887":"# Postprocessing\n\nAfter the TransformerBlocks, the following layers are applied to make predictions.\n\n* LayerNormalization\n* ExtractToken -- extracts the first embedding, which corresponds to the class token. All other 576 embeddings are discarded.\n* Dense -- outputs predictions for each class.\n\n--- [vit-keras\/vit.py#L119](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L119) ---<br>\n![image.png](attachment:46d0c241-ce5f-4333-b84f-e37580df1dae.png)\n\n# Sample Prediction\n\nThe output shape is (batch_size, number_of_classes).","1051c392":"# ViT Model\n\nMake a vit_b16 model as an example. Its configurations are:\n\n* image size: 384 x 384<br>\n  3 color channels are used for each image pixels,\n  so the input shape is (384, 384, 3)\n* patch size: 16<br>\n  An input image is divided into this size of patches.\n  So, the input image is divided into 576 = 24 x 24 (24 = image size \/ patch size = 384 \/ 16) patches.\n* hidden size: 768<br>\n  Information in each patches are encoded into this size.\n* number of encoder blocks: 12<br>\n  This number of TransformerBlocks are built and applied.","ad17083b":"# How Vision Transformer (ViT) Works\n\nThis notebook investigates how Vision Transformer (ViT) works using a Keras implementation.\n\nReference:\n* Original paper -- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https:\/\/arxiv.org\/abs\/2010.11929)\n* Transformer -- [Attention Is All You Need](https:\/\/arxiv.org\/pdf\/1706.03762.pdf)\n* Keras implementation -- [vit-keras](https:\/\/github.com\/faustomorales\/vit-keras)","b00cffa1":"# Transformer Block\n\nAfter adding position embeddings to the patches, some numbers of transformer blocks (12 for vit_b16) are applied.\n\n--- [vit_keras\/vit.py#L112](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L112) ---<br>\n![image.png](attachment:1aace665-1d1c-42ff-acf4-a03d5961f488.png)\n\nA transformer block is shown in the figure below:\n\n![image.png](attachment:e56334cd-2509-4547-867a-5917817b86ac.png)\n\nIn the implementation, dropout is inserted after the Multi-Head Attention.\n\n--- [vit_keras\/layers.py#L167](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L167) ---<br>\n![image.png](attachment:873beec2-8bf2-458c-b141-3954dd0936b5.png)","4394020d":"Let's draw a heatmap for correlation coefficients of position embeddings. It seems that the values of position embedings are correlated each other.","bc77ce45":"## Reshape\n\nThe portions of patches, size 24 x 24, are flattened.\nThe output shape is (number of patches, hidden size) = (576, 768).\n\n--- [vit-keras\/vit.py#L109](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L109) ---<br>\n![image.png](attachment:348c839c-ca1b-407c-b126-f83cb78931b1.png)","4f8e1a35":"# Preprocessing\n\nThe following layers are used to preprocess input images before TransformerBlocks.\n\n* Input\n* Embedding\n* Reshape\n* Class Token\n* Position Embeddings\n\n## Input\n\nInput shape is (image_size, image_size, 3).\n\n--- [vit-keras\/vit.py#L101](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L101) ---<br>\n![image.png](attachment:984ffcfb-0f4d-45ea-89a4-36c1a925b3e1.png)","993c4819":"## Position Embeddings\n\nPosition embeddings are added to the patch embeddings to retain positional information.\n\n* Position embeddings are learnable, so [tf.Variable](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/Variable) is used.\n* Its shape is (1, patch size + 1, hidden size) = (1, 577, 768).\n    * \"1\" is prepended for batch.\n    * \"+ 1\" for patch size is by class token.\n* Number of trainable parameters are 443,136 (= (patch size + 1) * hidden size = 577 * 768).\n* They are added to the input patch embeddings.\n\n--- [vit_keras\/vit.py#L111](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/vit.py#L111) ---<br>\n![image.png](attachment:ad8f3a5f-125c-4a46-8c79-2b6c9d5250cc.png)\n\n--- [vit_keras\/layers.py#L36](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L36) ---<br>\n![image.png](attachment:11bbf5a2-a506-4659-b698-00e4cc7e5b42.png)","b2a73a72":"Each transformer block returns two values. The first is the network output, and the second is the weight.","a00d4914":"The MLP block is concatenation of the following layers.\n\n* Dense -- number of units is 3072 for vit_b16. So, the input of shape (batch_size, 577, 768) is changed to (batch_size, 577, 3072).\n* gelu\n* Dropout\n* Dense -- number of units is the same as the MLP input. So the output shape is the same as the MLP input (batch_size, 577, 768).\n* Dropout\n\nNo activation is applied for the second Dense.\n\n--- [vit_keras\/layers.py#L139](https:\/\/github.com\/faustomorales\/vit-keras\/blob\/28815edc5c24492612af726d1b2ca78295128d84\/vit_keras\/layers.py#L139) ---<br>\n![image.png](attachment:909c918f-cdcd-4039-a28a-a526b8efd3d6.png)","f26d3e8e":"# Sample Image\n\nRead a sample image and plot it.","f0a93a91":"The learned values of the class token is shown in the plot below."}}