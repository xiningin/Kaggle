{"cell_type":{"63a5ee34":"code","d8435a58":"code","ea272df1":"code","c2fe6a2f":"code","3a2e7616":"code","a08f0785":"code","aa58b78e":"code","b87bc6bc":"code","03cc4ed2":"code","75bb243f":"code","4b08628a":"code","a7211704":"code","b916e6db":"code","0e97cfdd":"code","3cc4a708":"code","7ee55238":"code","b68ef4b4":"code","f9c82cbe":"code","667cb0f7":"code","814c6d65":"code","6b5e173f":"code","e2f83fd4":"code","18ee9853":"code","5d87bd06":"code","c160f96a":"code","1b0568d7":"code","e748c6d7":"code","40bb6c07":"code","c6a779f7":"markdown","9dd7a164":"markdown","ad8072be":"markdown","8db8d539":"markdown","0f27398e":"markdown","1107195e":"markdown","086b7bd7":"markdown","ac045e51":"markdown","79497235":"markdown","eca0b77a":"markdown","c5792b93":"markdown","8365efac":"markdown","b7bb0f95":"markdown","8aa949f6":"markdown","43df3c21":"markdown","defa833d":"markdown","50a34788":"markdown"},"source":{"63a5ee34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set3')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8435a58":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nprint(df.shape)\ndf.head()","ea272df1":"df.info()","c2fe6a2f":"df['bmi'].isna().sum()","3a2e7616":"!pip install tensorflow_decision_forests -q","a08f0785":"train_df, valid_df = train_test_split(df, test_size = 0.2, shuffle = True, random_state = 42)\ntrain_df.shape, valid_df.shape","aa58b78e":"import tensorflow_decision_forests as tfd\n\ntrain_tf = tfd.keras.pd_dataframe_to_tf_dataset(train_df, label = 'stroke')","b87bc6bc":"tfd.keras.get_all_models()","03cc4ed2":"#We first demo using RandomForest\n#Define the required model\nmodel = tfd.keras.RandomForestModel()\n\n#Train the model\nmodel.fit(x = train_tf)","75bb243f":"model.summary()","4b08628a":"valid_tf = tfd.keras.pd_dataframe_to_tf_dataset(valid_df, label = 'stroke')\n\nmodel.compile(metrics = [\"accuracy\"])\nev = model.evaluate(valid_tf)","a7211704":"print(f\"BinaryCross Entropy Loss: {ev[0]}\")\nprint(f\"Accuracy: {ev[1]}\")\n\n#Save model\nmodel.save('.\/stoke_model')","b916e6db":"logs = model.make_inspector().training_logs()\nplt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Out-of-bag accuracy\")\nplt.show()","0e97cfdd":"inspector = model.make_inspector()\nprint(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n    print(importance)","3cc4a708":"# Mean decrease in AUC of the class 1 vs the others.\ninspector.variable_importances()[\"NUM_AS_ROOT\"]","7ee55238":"with open('.\/plot_model.html', 'w') as f:\n    f.write(tfd.model_plotter.plot_model(model))","b68ef4b4":"from IPython.display import IFrame\n\nIFrame('.\/plot_model.html', width = 900, height = 700)","f9c82cbe":"model_gb = tfd.keras.GradientBoostedTreesModel(\n    num_trees = 300,\n    growing_strategy = \"BEST_FIRST_GLOBAL\",\n    max_depth = 12,\n    split_axis = \"SPARSE_OBLIQUE\",\n    )\n\nmodel_gb.fit(train_tf)\nmodel_gb.compile(metrics = [\"accuracy\"])\nev = model_gb.evaluate(valid_tf)\n\nprint(f\"BinaryCross Entropy Loss: {ev[0]}\")\nprint(f\"Accuracy: {ev[1]}\")","667cb0f7":"model_gb.make_inspector().variable_importances()","814c6d65":"df.head()","6b5e173f":"X = df.drop(['id', 'stroke'], axis = 1)\ny = df['stroke'].copy()","e2f83fd4":"num_cols = [c for c in X.columns if X[c].dtype in ['int64', 'float64']]\ncat_cols = [c for c in X.columns if c not in num_cols]\nnum_cols, cat_cols","18ee9853":"for c in num_cols:\n    X[c] = X[c].fillna(X[c].mean())","5d87bd06":"#Scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd = StandardScaler()\n\nX[num_cols] = std.fit_transform(X[num_cols])","c160f96a":"from sklearn.preprocessing import LabelEncoder\n\nlbl = LabelEncoder()\n\nfor c in cat_cols:\n    lbl.fit(X[c])\n    X[c] = lbl.transform(X[c])","1b0568d7":"Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size = 0.2, random_state = 42)\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()","e748c6d7":"clf.fit(Xtrain, ytrain)\npreds = clf.predict(Xvalid)\n\nfrom sklearn.metrics import accuracy_score\n\nprint(f\"Accuracy: {accuracy_score(yvalid, preds)}\")","40bb6c07":"for name, importance in zip(df.columns, clf.feature_importances_):\n    print(name, '-->', importance)","c6a779f7":"##### Below are the available models in Tensorflow Decision Forest","9dd7a164":"- The model starts with bmi >=26.75 and then branches off to check hypertension and age to decide which class it belongs to\n- if age >= 72.5 and bmi >= 31, the model decides more class 1 compared to other nodes","ad8072be":"##### NaN Imputation","8db8d539":"##### Evluatate the model using the validation data","0f27398e":"##### First step is to convert the pandas dataframe format to tensorflow decision forests format as below","1107195e":"- The first entry that model.evaluate returns is the BinaryCrossEntropyLoss\n- The second entry is the eval metric we supplied while compiling the model (accuracy)","086b7bd7":"There are 201 null values in feature 'bmi'\n\nUsually if we are to use sklearn RandomForest we would have to impute the NaNs, scale the features and convert categorical to numerical features before procedding with fitting the model. In TF DF we can straight away fit the model as demonstrated below","ac045e51":"### Training Logs Plot","79497235":"### Let us check Sklearn RandomForest for comparision","eca0b77a":"### Model Explainability","c5792b93":"##### We split the datasset into training and validation set","8365efac":"#### First install tensorflow_decision_forests package","b7bb0f95":"## Feature Importance","8aa949f6":"#### We now use GradientBoostTree model for the same dataset with some parameter tuning","43df3c21":"##### Label Encoding Categorical Features","defa833d":"# What is a Tree?\n\nAt its simplest form a Tree can be construed as multiple if\/else statements through which each row from the data is passed to check all the features to decide\/classify which category the row belongs.\n\n# Tensorfow Decision Forests\n\nTensorflow Decision forests are a family of machine learning algorithms with quality and speed competitive with (and often favorable to) neural networks, especially when you\u2019re working with tabular data. They\u2019re built from many decision trees, which makes them easy to use and understand - and you can take advantage of a plethora of interpretability tools and techniques that already exist today.\n\n- It provides a slew of state-of-the-art Decision Forest training and serving algorithms such as random forests, gradient-boosted trees, CART, (Lambda)MART, DART, Extra Trees, greedy global growth, oblique trees, one-side-sampling, categorical-set learning, random categorical learning, out-of-bag evaluation and feature importance, and structural feature importance.\n\n- This library can serve as a bridge to the rich TensorFlow ecosystem by making it easier for you to integrate tree-based models with various TensorFlow tools, libraries, and platforms such as TFX.\n\nFor more info please check -> https:\/\/blog.tensorflow.org\/2021\/05\/introducing-tensorflow-decision-forests.html","50a34788":"### To Demonstrate TF Decision Forests we use Stroke Prediction Dataset"}}