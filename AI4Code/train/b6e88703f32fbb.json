{"cell_type":{"6be0a8aa":"code","284047bd":"code","f5153b54":"code","8aabfa85":"code","3717c08f":"code","0629b8ba":"code","3cd7491a":"code","29f49055":"code","1536167f":"code","b9b97d88":"code","79016e6a":"code","7cf4af38":"code","012b39bf":"code","fe6df7d2":"code","31df8d24":"code","19d9d576":"code","f462972f":"code","b21fdad6":"code","fc6ce2a0":"code","e7181aca":"code","3f617ec8":"code","c6afe727":"code","9380de13":"code","71960b50":"code","26d7b08b":"code","f609871b":"code","ba6e7ef5":"code","aefc7bcc":"code","0cc2ccee":"code","ae486518":"code","f246ba30":"code","6479a138":"code","b268d38f":"code","0b75196a":"code","09580748":"code","969c3cf6":"markdown","601f806f":"markdown","507d3910":"markdown","ca36b880":"markdown","38fd5b8e":"markdown","04122a3b":"markdown","cf89271e":"markdown","7d9d168c":"markdown","3385bf3c":"markdown","b213c2cc":"markdown","90ee980d":"markdown","1ca45e25":"markdown","149a03e2":"markdown","86dc78a9":"markdown","fe04c15a":"markdown","572c5394":"markdown","0cd21160":"markdown","0c9d1909":"markdown","1156fef6":"markdown","a92f51b8":"markdown","719a5e54":"markdown","5958aeee":"markdown","61c39ee5":"markdown"},"source":{"6be0a8aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","284047bd":"train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngender_sub=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nprint(train.shape)\ntest.shape","f5153b54":"train.head()","8aabfa85":"test.isnull().sum()","3717c08f":"train.isnull().sum()","0629b8ba":"gender_sub.info()","3cd7491a":"train.head()\nTrain=train.drop(\"Survived\",inplace=False,axis=1)\ndata=pd.concat([Train,test])\nprint(data.shape)\ndata.isnull().sum()","29f49055":"#dropping the unneccessary columns and checking for nulls\ndata.drop(columns=[\"Name\",\"Ticket\",\"Cabin\"],axis=1,inplace=True)\nprint(data.isnull().sum())\n","1536167f":"data.Embarked.mode()        ","b9b97d88":"data.Embarked.fillna(value=\"S\",axis=0,inplace=True)\ndata.Embarked.isnull().sum()","79016e6a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\ndata.Age.skew()\n","7cf4af38":"avg_age=data.Age.mean()","012b39bf":"data.Age.fillna(value=avg_age,axis=0,inplace=True)\ndata.isnull().sum()","fe6df7d2":"# print(data.Fare.skew())\n# avg_Fare=data.Fare.mean()\n# data.Fare.fillna(value=avg_Fare,axis=0,inplace=True)\n# print(data.Fare.skew())","31df8d24":"data.drop(columns=[\"PassengerId\",\"Fare\"],axis=1,inplace=True)\nprint(data.Pclass.unique())\nprint(data.Embarked.unique())\nprint(data.Parch.unique())\nprint(data.SibSp.unique())","19d9d576":"Dummy_data=pd.get_dummies(data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\nDummy_data.head()","f462972f":"##normalizing the data\nfrom sklearn.preprocessing import MinMaxScaler\nscale=MinMaxScaler()\ncol=Dummy_data.columns\nTransformed_data=scale.fit_transform(Dummy_data)\nTransformed_df=pd.DataFrame(Transformed_data,columns=col)\nTransformed_df.head()","b21fdad6":"#splitting into train and test data\ntransformed_train=Transformed_df.iloc[:891,:]\ntransformed_test=Transformed_df.iloc[891:,:]","fc6ce2a0":"#splitting into training and validation sets\ny=train.Survived\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(transformed_train,y,train_size=0.7,random_state=1)\n\n","e7181aca":"y.value_counts()","3f617ec8":"#using Logistic_regression\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred=model.predict(x_test)\n","c6afe727":"from sklearn.metrics import classification_report,accuracy_score,roc_auc_score\nprint(roc_auc_score(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","9380de13":"#balancing the classes of survived feature\nfrom sklearn.utils import resample\ntransformed_train[\"Survived\"]=y\nmin_data=transformed_train[transformed_train[\"Survived\"]==1]\nmaj_data=transformed_train[transformed_train[\"Survived\"]==0]\nmod_min_data=resample(min_data,random_state=1,n_samples= 549)\nbalanced_train=pd.concat([maj_data,mod_min_data])\n","71960b50":"Y=balanced_train.Survived\nx=balanced_train.drop(\"Survived\",axis=1,inplace=False)","26d7b08b":"X_train,X_test,Y_train,Y_test=train_test_split(x,Y,train_size=0.7,random_state=1)","f609871b":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(X_train,Y_train)\nY_pred=model.predict(X_test)\nprint(roc_auc_score(Y_test,Y_pred))\nprint(accuracy_score(Y_test,Y_pred))\nprint(classification_report(Y_test,Y_pred))","ba6e7ef5":"#using randomforest_classifier\nfrom sklearn.ensemble import RandomForestClassifier\nmodel1=RandomForestClassifier(n_estimators=500,max_depth=9,max_features=\"auto\",random_state=1)\nmodel1.fit(X_train,Y_train)\nYr_pred=model1.predict(X_test)\nprint(roc_auc_score(Y_test,Yr_pred))\nprint(accuracy_score(Y_test,Yr_pred))\nprint(classification_report(Y_test,Yr_pred))\n","aefc7bcc":"result=pd.DataFrame(test.PassengerId,columns=[\"PassengerId\"])\nresult[\"Survived\"]=model1.predict(transformed_test)\nresult.to_csv(\"subR.csv\",index=False)\n","0cc2ccee":"X_train.shape","ae486518":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Dropout,Activation\nfrom tensorflow.keras.models import Sequential\n\n\n\n","f246ba30":"model=Sequential()\nmodel.add(Dense(500,input_shape=(11,)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(400))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(100))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(10))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(1))\nmodel.add(Activation(\"sigmoid\"))\n","6479a138":"model.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"],optimizer=\"adam\")","b268d38f":"hist=model.fit(X_train,Y_train,batch_size=150,epochs=100,verbose=2,validation_data=(X_test,Y_test))","0b75196a":"Yn_pred=model.predict_classes(X_test)\nprint(roc_auc_score(Y_test,Yn_pred))\nprint(accuracy_score(Y_test,Yn_pred))","09580748":"n_result=pd.DataFrame(test.PassengerId,columns=[\"PassengerId\"])\nn_result[\"Survived\"]=model.predict_classes(transformed_test)\nn_result.to_csv(\"subN.csv\",index=False)","969c3cf6":"# using RandomforestClassifier","601f806f":"import the required libraries","507d3910":"training the model and saving the metrics in history","ca36b880":"fillig null values in embarked using mode value","38fd5b8e":"# using neural networks","04122a3b":"getting dummies of pclass,sex,embarked","cf89271e":"reading all the 3 files , if you are new to kaggle notebooks and don't know how to read the data csv files then here are the steps:\n1. On the right side of the notebook, you can find \"add data\" option click on that and search for your data ,if it's not present on kaggle then upload it from your pc and then add to the notebook.\n2. Run the above cell first, later on use pd.read_csv(\"\") in the 2ns cell and inside the quotes copy paste the paths you get after running the 1st cell.","7d9d168c":"predicting for the test data set","3385bf3c":"balancing the classes of survived using oversampling technique","b213c2cc":"concatinating both train and test files into one called data inorder to perform exploratory data analysis(eda)","90ee980d":"implementing the LR after balancing and checking the classifaction_report, roc_score and accuracy","1ca45e25":"splitting again into train and test data sets","149a03e2":"# using logistic regression","86dc78a9":"checking no of nulls","fe04c15a":"# Exploratory Data Analysis and feature Engineering","572c5394":"checking the values of f1-score,recall,precision, accuracy and roc score","0cd21160":"compiling the model and fitting it","0c9d1909":"dropping the columns \"Name\",\"Ticket\",\"Cabin\"","1156fef6":"filling null values in age column with mean age ","a92f51b8":"normalising the data using minmaxscaler","719a5e54":"storing the predicted survived list in .csv  file","5958aeee":"creating the nueral network layers","61c39ee5":"dropping the passangerid and fare columns"}}