{"cell_type":{"5dcd43bb":"code","318278c8":"code","781d6eb2":"code","d6718ca3":"code","3291d209":"code","d5ba0ae5":"code","80f3f79d":"code","e4b24b5e":"code","40b557bd":"code","0b808c8e":"code","3b79ca38":"code","b5ea448c":"code","7e8cd13e":"code","7127c29a":"code","31bb35ea":"code","a5ab5238":"code","70aaca1e":"code","70a0cedb":"code","daf749a3":"code","3b429585":"code","11f952dd":"code","00d4462f":"code","00f06dc3":"code","cd08eaae":"markdown","c383c91f":"markdown","7f5d3e89":"markdown","525d9b33":"markdown","803f8a56":"markdown","d70892fc":"markdown","fc88caf2":"markdown","805da17e":"markdown","fb4c759b":"markdown","c8ca4a16":"markdown","08dfcc34":"markdown","93ddb0fa":"markdown","26454814":"markdown","5e2c589b":"markdown","a5266ae1":"markdown","0dfa92b3":"markdown","d352d4ab":"markdown","1f81d034":"markdown","1b03ee8b":"markdown","6b18cb7b":"markdown","6b2a2cbd":"markdown","dd74f25f":"markdown","ac5ad0e7":"markdown","22da9af9":"markdown"},"source":{"5dcd43bb":"import pandas as pd\nIris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\nX = Iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny=Iris.Species","318278c8":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)","781d6eb2":"import numpy as np\n\nX_mean = np.mean(X, axis=0)\n# cov_mat = np.cov(X)\ncov_mat = (X - X_mean).T.dot((X - X_mean)) \/ (X.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","d6718ca3":"eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","3291d209":"u,s,v = np.linalg.svd(X.T)\nu","d5ba0ae5":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","80f3f79d":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\ncum_var_exp","e4b24b5e":"matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n                      eig_pairs[1][1].reshape(4,1)))\n\nprint('Matrix W:\\n', matrix_w)","40b557bd":"Y = X.dot(matrix_w)","0b808c8e":"import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')):\n        plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","3b79ca38":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X)\n\nsklearn_pca.explained_variance_ratio_","b5ea448c":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y_sklearn[y==lab, 0],\n                    Y_sklearn[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","7e8cd13e":"from sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)","7127c29a":"from sklearn.model_selection import train_test_split\ntrain_img, test_img, train_lbl, test_lbl = train_test_split( X, y, test_size=0.15, random_state=0)","31bb35ea":"from sklearn.decomposition import PCA\n\n# scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.\npca = PCA(0.95)\npca.fit(train_img)\nprint(pca.n_components_)\ntrain_img = pca.transform(train_img)\ntest_img = pca.transform(test_img)","a5ab5238":"from sklearn.decomposition import IncrementalPCA\nn_batches = 100\n\ninc_pca = IncrementalPCA(n_components=154)\n\nfor X_batch in np.array_split(train_img, n_batches):\n        inc_pca.partial_fit(X_batch)\n        \nX_mnist_reduced = inc_pca.transform(train_img)","70aaca1e":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.datasets import make_swiss_roll\n\nX, y = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n\naxes = [-11.5, 14, -2, 23, -12, 15]\n\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.hot)\nax.view_init(10, -70)\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nplt.show()","70a0cedb":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\nX_reduced = tsne.fit_transform(X)","daf749a3":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n#ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue = y)\n\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.hot)","3b429585":"from sklearn.decomposition import KernelPCA\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\nX_reduced = rbf_pca.fit_transform(X)\n\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n#ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue = y)\n\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.hot)","11f952dd":"from sklearn.decomposition import KernelPCA\nrbf_pca = KernelPCA(n_components = 2, kernel=\"poly\", gamma=0.04)\nX_reduced = rbf_pca.fit_transform(X)\n\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n#ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue = y)\n\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.hot)","00d4462f":"from sklearn.manifold import LocallyLinearEmbedding\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\nX_reduced = lle.fit_transform(X)\nX_reduced.shape\n\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n#ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue = y)\n\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.hot)","00f06dc3":"from sklearn.manifold import Isomap\nisomap = Isomap(n_components=2, n_neighbors=10)\nX_reduced = isomap.fit_transform(X)\nX_reduced.shape\n\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n#ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue = y)\n\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.hot)","cd08eaae":"<a id=\"LLE\"><\/a>\n### <u> LLE <\/u>","c383c91f":"<a id=\"Kernel PCA\"><\/a>\n### <u>  Kernel PCA <\/u>\n\nKernel PCA can perform complex nonlinear projections for dimensionality reduction. It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.","7f5d3e89":"<a id=\"Isomap\"><\/a>\n### <u> Isomap <\/u>","525d9b33":"<a id=\"Dimensionality Reduction\"><\/a>\n# <u>Dimensionality Reduction<\/u>\n\nDimensionality reduction is the process of reducing the dimension of the feature set while maintaining its structure and usefulness.\n\n## <u>Reasons for dimentionality reduction<\/u>\n\n- Most points in a high-dimensional hypercube are very close to the border and at the risk of being very sparse, making predictions much less reliable than in lower dimensions.\n- Very large number of features for each training instance may make training extremely slow.\n- Difficult to visualize dataset containing large number of features.\n\n## <u>Pros<\/u>\n\n- Less time in training the dataset.\n- Easy visualization of the dataset containing 2 or 3 principle features.\n- May (May not) result in higher performance.\n\n## <u>Cons<\/u>\n\n- Loss of information.\n\nDimensionality reduction can be achieved in the following ways:\n\n- <b> Feature Elimination <\/b>: We reduce the feature space by eliminating features. This has a disadvantage though, as we gain no information from those features that you have dropped.\n\n- <b> Feature Selection <\/b>: We apply some statistical tests in order to rank them according to their importance and then select a subset of features for our work. This again suffers from information loss and is less stable as different test gives different importance score to features.\n\n- <b> Feature Extraction <\/b>: We create new independent features, where each new independent feature is a combination of each of the old independent features. These techniques can be divided into linear(PCA, SVD) and non-linear(t-SNE) dimensionality reduction techniques.","803f8a56":"<a id=\"Eigendecomposition\"><\/a>\n### <u>Step 2:  Eigendecomposition - Computing Eigenvectors and Eigenvalues<\/u>\n\nThe eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \u201ccore\u201d of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n\n#### <u> Covariance Matrix <\/u>\n\nThe classic approach to PCA is to perform the eigendecomposition on the covariance matrix , which is a matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n\n$$Cov(\ud835\udc4b, \ud835\udc4c ) = \\frac{\\sum(x_i - \\bar{x}) (y_i - \\bar{y})}{N-1}$$","d70892fc":"#### <u> Explained Variance <\/u>\nAfter sorting the eigenpairs, the next question is \u201chow many principal components are we going to choose for our new feature subspace?\u201d A useful measure is the so-called \u201cexplained variance,\u201d which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.","fc88caf2":"In the above array we see that the first feature explains roughly 72.77% of the variance within our data set while the first two explain 95.8 and so on. ","805da17e":"#### <u> Singular Value Decomposition <\/u>\n\nWhile the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Value Decomposition (SVD) to improve the computational efficiency. So, let us perform an SVD to confirm that the result are indeed the same:","fb4c759b":"<a id=\"Selecting Principal Components\"><\/a>\n### <u>Step 3:  Selecting Principal Components<\/u>\n\n#### <u>Sorting Eigenpairs <\/u>\nThe goal of PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they all have the same unit length 1.\n\nIn order to decide which eigenvector(s) can be dropped without losing too much information, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n\nThe common approach is to rank the eigenvalues from highest to lowest.","c8ca4a16":"<a id=\"PCA to Speed-up Machine Learning Algorithms\"><\/a>\n\n### <u>Application 2 - PCA to Speed-up Machine Learning Algorithms <\/u>\n\nThe MNIST database of handwritten digits has 784 feature columns\/ dimensions, a training set of 60,000 examples, and a test set of 10,000 examples.","08dfcc34":"<a id=\"Manifold Learning\"><\/a>\n## <u> Manifold Learning <\/u>\n\n<a id=\"t-SNE\"><\/a>\n### <u> t-SNE <\/u>\n\n- t-SNE map points in high dimensional space to a lower dimension so that the distances between the points remains almost the same.\n\n- t-SNE, unlike PCA, is not a linear projection. It uses the local relationships between points to create a low-dimensional mapping. This allows it to capture non-linear structure.\n- t-SNE creates a probability distribution using the Gaussian distribution that defines the relationships between the points in high-dimensional space.\n- t-SNE uses the Student t-distribution to recreate the probability distribution in low-dimensional space.\n- t-SNE optimizes the embeddings directly using gradient descent. The cost function is non-convex though, meaning there is the risk of getting stuck in local minima. This fact has an important consequence: t-SNE is non-deterministic. While running it multiple times, we may get different results each time.\n\nt-SNE uses \u201cstochastic neighbors\u201d which means that there is no clear line between which points are neighbors of the other points. This lack of clear borders allows t-SNE to naturally take both global and local structure into account. Local structure is more important than global structure, but points that are far away are not completely ignored, allowing for a \u201cwell-balanced\u201d dimensionality reduction.\n\n#### <u>t-SNE Algorithm <\/u>:\n\n<b>Step 1 <\/b> : In the high-dimensional space, create a probability distribution that dictates the relationships between various neighboring points\n\n<b>Step 2 <\/b>: It then tries to recreate a low dimensional space that follows that probability distribution as best as possible.\n\n#### <u> Why do we need t-SNE? <\/u>\n\nPCA can\u2019t capture non-linear dependencies. For instance, PCA would not be able to \u201cunroll\u201d the following structure.","93ddb0fa":"A linear projection is like casting a shadow. Unlike PCA, t-SNE is not limited to linear projections, which makes it suited to all sorts of datasets.","26454814":"### <u>Shortcut - PCA in scikit-learn<\/u>\n","5e2c589b":"<a id=\"Incremental PCA\"><\/a>\n### <u> Incremental PCA <\/u>\n\nPCA requires the whole training set to fit in memory. Incremental PCA splits the training set into mini-batches. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new instances arrive).","a5266ae1":"Time stats\n\nThe table below shows how long it took to fit logistic regression on kaggle kernel after using PCA (retaining different amounts of variance each time).\n\n| Variance Retained | Number of Components | Time (Seconds) | Accuracy\n| --- | --- | --- | --- |\n| Without PCA | -- |153 | 0.9096 |\n| 0.99 | 331 | 75 |0.7789\n| 0.95 |154 | 55 |0.9050\n| 0.90 |87 | 28 |0.9030\n| 0.85 |59 | 20 |0.897","0dfa92b3":"<a id=\"Projection Onto the New Feature Space\"><\/a>\n### <u>Step 4:  Projection Onto the New Feature Space<\/u>","d352d4ab":"* [Dimensionality Reduction](#Dimensionality Reduction)\n    - [Main Approaches for Dimensionality Reduction](#Main Approaches for Dimensionality Reduction)\n        - [Projection](#Projection) \n             - [PCA - Principal Component Analysis](#PCA - Principal Component Analysis) \n                  - [PCA for Data Visualization and Dimentionality Reduction](#PCA for Data Visualization and Dimentionality Reduction)\n                      - [Standardization](#Standardization) \n                      - [Eigendecomposition - Computing Eigenvectors and Eigenvalues](#Eigendecomposition)\n                      - [Selecting Principal Components ](#Selecting Principal Components )\n                      - [Projection Onto the New Feature Space ](#Projection Onto the New Feature Space )\n                  - [PCA to Speed-up Machine Learning Algorithms](#PCA to Speed-up Machine Learning Algorithms)\n                  - [Incremental PCA](#Incremental PCA)\n        - [Manifold Learning](#Manifold Learning)  \n            - [t-SNE](#t-SNE)\n            - [Kernel PCA](#Kernel PCA)\n            - [LLE](#LLE)\n            - [Isomap](#Isomap)","1f81d034":"#### <u>Projection Matrix <\/u>\nThe projection matrix is used to transform the Input data(X) onto the new feature subspace. Projection Matrix is a matrix of concatenated top k eigenvectors.\n\nHere, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the \u201ctop 2\u201d eigenvectors with the highest eigenvalues to construct our 2-dimensional eigenvector matrix .","1b03ee8b":"<a id=\"Projection\"><\/a>\n## <u> Projection <\/u>\n\n<a id=\"PCA - Principal Component Analysis\"><\/a>\n### <u> PCA - Principal Component Analysis (Vanilla PCA) <\/u>\n- Principal components analysis is the main method used for linear dimension reduction.\n- It performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized, with the maximum variance, maximum information is preserved. In another words, it selects the hyperplane(s) that minimizes the mean squared distance between the original dataset and its projection onto that hyperplane(s).\n- The first principle component accounts for the maximum variance in the data and so on ...\n- All the components in lower-dimensional space are linearly uncorrelated.\n- Each of the new features or components created after PCA are all independent of one another.","6b18cb7b":"#### End\nIf you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful !","6b2a2cbd":"<a id=\"PCA for Data Visualization and Dimentionality Reduction\"><\/a>\n### <u>Application 1 : PCA for Data Visualization and Dimentionality Reduction <\/u>","dd74f25f":"<a id=\"Standardization\"><\/a>\n### <u>Step 1:  Standardization<\/u>\n\nUn-standardized data is sensitive to the variances of the initial variables. The variables with larger ranges dominates over those with small ranges (a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which leads to biased results. Transforming the data to comparable scales prevents this problem.\n\nMathematically, this can be done by subtracting the mean (or emperical mean - sample mean of each column shifted to zero) and dividing by the standard deviation for each value of each variable.","ac5ad0e7":"<a id=\"Main Approaches for Dimensionality Reduction\"><\/a>\n## <u>Main Approaches for Dimensionality Reduction <\/u>\n\n### <u> Projection <\/u>\n\nProjecting high dimensional data on a low dimentional hyperplane, minimizing the variance. (PCA)\n\n### <u>Manifold Learning <\/u>\n\nIn many cases where the subspace may twist and turn(Swiss roll), projection is not the best approach to dimensionality reduction. Manifold Learning techniques measures how each training instance linearly relates to its closest neighbors, then it looks for a low-dimensional representation of the training set where these local relationships are best preserved.\n","22da9af9":"Next, we perform an eigendecomposition on the covariance matrix:\n\nAll the three approaches yield the same eigenvectors and eigenvalue pairs:\n\n- Eigendecomposition of the covariance matrix after standardizing the data.\n- Eigendecomposition of the correlation matrix.\n- Eigendecomposition of the correlation matrix after standardizing the data."}}