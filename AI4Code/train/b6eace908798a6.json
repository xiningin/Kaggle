{"cell_type":{"d1c4f048":"code","1792488e":"code","82afc378":"code","ffbb1a7a":"code","5aec8220":"code","50b0ddf0":"code","64da8bd8":"code","6dad766d":"code","95c785c4":"code","ca7f17c2":"code","cc48537b":"code","96c9a8bb":"code","f95cce2f":"code","7b53e208":"code","25e920ce":"code","443b3bd0":"code","af85e94f":"code","0cf756f7":"code","f1a7d3cf":"code","9e949b45":"code","0440536e":"code","55112e34":"code","e07ac74a":"code","ecd9c485":"code","1a134300":"code","647b6ddb":"code","58bd0ced":"code","bde4ae9c":"code","a2f3ca2f":"code","0672c4c4":"code","c7758fb4":"code","2321cb7a":"code","fdf5ed62":"code","b0011ffa":"code","54f932a5":"code","caa68206":"code","2504e529":"code","a51a8516":"code","6dfd9520":"code","db4c64c3":"code","1afb40ba":"code","f9fcf90d":"code","49f30481":"code","310d9c73":"code","deda2c58":"code","ff1f4b34":"code","8f6c496d":"code","0c2752d1":"code","01d3553f":"code","3a757edf":"code","3deb5ed8":"code","ac699eae":"code","73723f8a":"code","53647a39":"code","49c3af03":"code","461dec2a":"code","77ab1825":"code","08d1e2c5":"code","e8b33d05":"code","46aaff7c":"code","12df97d9":"code","4b5baada":"code","a90a588a":"code","b7924b05":"code","5d755992":"code","0a2d806a":"code","6dc3ff2b":"code","7325196c":"code","63109c82":"code","8ad988af":"code","8c6f06d3":"code","d3c11e19":"code","abf720ef":"code","128b7b75":"code","41de9379":"code","d421f84d":"code","98dc478a":"code","e51ac752":"code","1be8c5fb":"code","bc0587a8":"code","b9f047cd":"code","a240fb30":"code","d550ae54":"code","66ec1d91":"code","978ea836":"code","e7658028":"code","a54d2739":"code","cd4885d2":"code","50f44c91":"code","f465b752":"code","516b47bf":"code","1d92bf6d":"code","3b96de7e":"code","854b86c8":"code","6f883ae7":"code","7bf5c7d3":"code","e6a9332f":"code","e5eb81d7":"code","f27ea505":"code","6ecfb616":"code","1de8f478":"code","ba2397d3":"code","11253d98":"code","5d7ac1ed":"code","feaadf8a":"code","49cf1c3c":"code","c2495bbf":"code","b0b2228a":"code","2eb7d73d":"markdown","dd3f4afb":"markdown","f2860fe9":"markdown","fd1b9866":"markdown","2fd9648c":"markdown","aef0941b":"markdown","909154ce":"markdown","c11d7465":"markdown","478cf2f5":"markdown","a2002cd9":"markdown","d6a40e2f":"markdown","20d9d5ed":"markdown","c670e82e":"markdown","87e4ec11":"markdown","59fa57a7":"markdown","5ac35b38":"markdown","469c9abf":"markdown","a2d93c64":"markdown","e24802d0":"markdown","3576c7a4":"markdown","40958c9e":"markdown","a3d2ea00":"markdown","f647e4f3":"markdown","d860d51c":"markdown","e43abb3e":"markdown","3b25cb94":"markdown","8bab5403":"markdown","425f08c1":"markdown","b5fe4868":"markdown","33f050c8":"markdown","c286ebe8":"markdown","a5c4db43":"markdown","5789b0d5":"markdown","2fab33b4":"markdown","1f1c56fc":"markdown","b4c77b11":"markdown","f94329b4":"markdown","29c17b8f":"markdown","6a7cb404":"markdown","f9bc964a":"markdown","ff4dddc6":"markdown","5f4b185b":"markdown","1a05b1ca":"markdown","122c7e55":"markdown","72402a49":"markdown","7315352b":"markdown","afcf24bc":"markdown","81acfeda":"markdown","1682fa80":"markdown","56702e5b":"markdown","5fb18950":"markdown","e505fea0":"markdown","68934337":"markdown","32a78f96":"markdown","fb9a78c4":"markdown","bde3d103":"markdown","e8c31a80":"markdown","659f6d9d":"markdown","05dbb4dd":"markdown","26195eb4":"markdown","f67d210d":"markdown","3b7b62a7":"markdown","78713357":"markdown","fbd2abe5":"markdown","130d11c1":"markdown","463fc6e7":"markdown","b1c1bd3d":"markdown","39fba848":"markdown","315a9475":"markdown","ad656091":"markdown","4aa42813":"markdown","fdaba5f6":"markdown","d0899fd0":"markdown","9137b724":"markdown","cb3e6757":"markdown","9890efee":"markdown","fbc78a62":"markdown"},"source":{"d1c4f048":"#Supress warnings \nimport warnings \nwarnings.filterwarnings('ignore')","1792488e":"#importing libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression \n\n","82afc378":"#Import the bike sharing dataset\nsharing=pd.read_csv(r'..\/input\/bike-sharing\/day.csv')","ffbb1a7a":"\nsharing.head()","5aec8220":"\nsharing.info()","50b0ddf0":"\nsharing.describe()","64da8bd8":"\nsharing.shape","6dad766d":"\nround(100*(sharing.isnull().sum()\/len(sharing.index)),2).sort_values(ascending=False)","95c785c4":"round((sharing.isnull().sum(axis=1)\/122)*100,2).sort_values(ascending=False)","ca7f17c2":"\nsharing_dup=sharing \nsharing_dup.drop_duplicates(subset=None,inplace=True)\nsharing_dup.shape","cc48537b":"\nsharing_dummy=sharing.iloc[:,1:16]\n\nfor col in sharing_dummy:\n    print(sharing_dummy[col].value_counts(),'\/n')","96c9a8bb":"\nsharing.columns\n","f95cce2f":"sharing_new=sharing[['season', 'yr', 'mnth', 'holiday', 'weekday',\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed'\n       , 'cnt']]\nsharing_new.info()","7b53e208":"\n\ncategory_cols=['mnth','weekday','season','weathersit']\nfor cols in category_cols:\n    sharing_new[cols]=sharing_new[cols].astype('category')","25e920ce":"\nsharing_new.info()","443b3bd0":"#Visualising the Numerical variables in the dataframe \n# Create a new dataframe of only numeric variables:\n\nsharing_new_num=sharing_new[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(sharing_new_num, diag_kind='kde')\nplt.show()","af85e94f":"#Visualising the Categorical Variables \nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = sharing_new)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = sharing_new)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = sharing_new)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = sharing_new)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = sharing_new)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = sharing_new)\nplt.show()","0cf756f7":"# Visualising how weathersituation impacts in bikesharing\n\nplt.figure(figsize = (8,4))\nsns.lineplot(x = 'mnth', y = 'cnt', data = sharing_new, estimator = np.average, hue = 'weathersit', palette = 'coolwarm')\nplt.ylabel('Average Count')\nplt.show()\n","f1a7d3cf":"#visualising how season impacts bikesharing \nplt.figure(figsize = (8,4))\nseason= sharing_new[['season','cnt']].groupby(['season']).sum().reset_index().plot(kind='bar',\n                                       legend = False, title =\"Counts of Bike Sharing by season\", \n                                         stacked=True, fontsize=12)\nseason.set_xlabel(\"season\", fontsize=12)\nseason.set_ylabel(\"Count\", fontsize=12)\nseason.set_xticklabels(['spring','summer','fall','winter'])","9e949b45":"#Visualisation of Bikesharing in weekday and holiday \nplt.figure(figsize = (10, 5))\nsns.boxplot(x = 'weekday', y = 'cnt', hue = 'holiday', data = sharing_new)\nplt.show()","0440536e":"#Visualisation of heat map to see data correlation in sharing_new dataframe \n#Note this visualisation is done before regression just to check the collinearity of different variables \nplt.figure(figsize = (10,7))\nsns.heatmap(sharing_new.corr(), annot = True, cmap = 'coolwarm', linecolor = 'white', linewidths=0.1)\n","55112e34":"# Creating dummies for the categorical columns in the sharing_new dataframe \nsharing_new=pd.get_dummies(sharing_new,drop_first=True)\nsharing_new.info()","e07ac74a":"# We specify random_state so that the train and test data set always have the same rows\nnp.random.seed(0)\ndf_train,df_test=train_test_split(sharing_new,train_size=0.70,test_size=0.30,random_state=100)\n","ecd9c485":"#verifying the  shape of training dataframe\ndf_train.shape","1a134300":"#verifying the sample of training dataframe \ndf_train.head()","647b6ddb":"#verifying the columns of Training dataframe \ndf_train.info()","58bd0ced":"#verifying the shape of test dataframe \ndf_test.shape","bde4ae9c":"#verifying the sample of test dataframe \ndf_test.head()","a2f3ca2f":"#verifying the columns of test dataframe \ndf_test.info()","0672c4c4":"#Visualising the correlation after creating dummies in the sharing_new dataframe \nplt.figure(figsize=(25,20))\nsns.heatmap(df_train.corr(),annot=True,cmap=\"YlGnBu\")\nplt.show()","c7758fb4":"#Rescaling the features inorder to have comparable scales \nscaler=MinMaxScaler()","2321cb7a":"#Check the data before scaling \ndf_train.head()","fdf5ed62":"#Retrieve the columns in df_train \ndf_train.columns ","b0011ffa":"# Apply scaler() to all the numeric variables\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","54f932a5":"#Check the data after scaling \ndf_train.head()","caa68206":"#check the mean median and other statistics \ndf_train.describe()","2504e529":"#Dividing into X and Y sets for the model building\ny_train=df_train.pop('cnt')\nX_train=df_train","a51a8516":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","6dfd9520":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","db4c64c3":"col = X_train.columns[rfe.support_]\ncol","1afb40ba":"X_train.columns[~rfe.support_]","f9fcf90d":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","49f30481":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","310d9c73":"# Running the linear model\nlm = sm.OLS(y_train,X_train_rfe).fit()   ","deda2c58":"# Check the parameters \nlm.params","ff1f4b34":"#Let's see the summary of our linear model\nprint(lm.summary())","8f6c496d":"# Check columns of train dataframe \nX_train_rfe.columns","0c2752d1":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#drop const from X_train_refe\nX_train_rfe_VIF=X_train_rfe[['yr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed',\n       'season_2', 'season_3', 'season_4', 'mnth_8', 'mnth_9', 'mnth_10',\n       'weekday_6', 'weathersit_2', 'weathersit_3']]\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe_VIF.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe_VIF.values, i) for i in range(X_train_rfe_VIF.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","01d3553f":"#Removing the variable 'season_3' based on its High p-value & High VIF\nX_train_new = X_train_rfe_VIF.drop([\"season_3\"], axis = 1)","3a757edf":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3deb5ed8":"# Add a constant\nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr2 = sm.OLS(y_train, X_train_lm2).fit()","ac699eae":"# Check the parameters obtained\n\nlr2.params","73723f8a":"# Print a summary of the linear regression model obtained\nprint(lr2.summary())","53647a39":"X_train_new = X_train_new.drop([\"holiday\"], axis = 1)","49c3af03":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","461dec2a":"# Add a constant\nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr3 = sm.OLS(y_train, X_train_lm3).fit()","77ab1825":"# Check the parameters obtained\n\nlr3.params","08d1e2c5":"# Print a summary of the linear regression model obtained\nprint(lr3.summary())","e8b33d05":"X_train_new = X_train_new.drop([\"mnth_10\"], axis = 1)","46aaff7c":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","12df97d9":"# Add a constant\nX_train_lm4 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr4 = sm.OLS(y_train, X_train_lm4).fit()","4b5baada":"# Check the parameters obtained\n\nlr4.params","a90a588a":"# Print a summary of the linear regression model obtained\nprint(lr4.summary())","b7924b05":"X_train_new = X_train_new.drop([\"mnth_8\"], axis = 1)","5d755992":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0a2d806a":"# Add a constant\nX_train_lm5 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr5 = sm.OLS(y_train, X_train_lm5).fit()","6dc3ff2b":"# Check the parameters obtained\n\nlr5.params","7325196c":"# Print a summary of the linear regression model obtained\nprint(lr5.summary())","63109c82":"#Checking the Pvalues\npd.options.display.float_format = '{:.10f}'.format\nround(lr5.pvalues,6)","8ad988af":"X_train_new = X_train_new.drop([\"hum\"], axis = 1)","8c6f06d3":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d3c11e19":"# Add a constant\nX_train_lm6 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr6 = sm.OLS(y_train, X_train_lm6).fit()","abf720ef":"# Check the parameters obtained\n\nlr6.params","128b7b75":"# Print a summary of the linear regression model obtained\nprint(lr6.summary())","41de9379":"## Check for p values in the above model \npd.options.display.float_format = '{:.10f}'.format\nround(lr6.pvalues,6)","d421f84d":"# Drop working day variable \nX_train_new = X_train_new.drop([\"workingday\"], axis = 1)","98dc478a":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e51ac752":"# Add a constant\nX_train_lm7 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr7 = sm.OLS(y_train, X_train_lm7).fit()","1be8c5fb":"# Check the parameters obtained\n\nlr7.params","bc0587a8":"# Print a summary of the linear regression model obtained\nprint(lr7.summary())","b9f047cd":"## Check for p values in the above model \npd.options.display.float_format = '{:.10f}'.format\nround(lr7.pvalues,6)","a240fb30":"# Drop working day variable \nX_train_new = X_train_new.drop([\"weekday_6\"], axis = 1)","d550ae54":"# Calculating the VIF for the model \n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","66ec1d91":"# Add a constant\nX_train_lm8 = sm.add_constant(X_train_new)\n\n# Create a fitted model\nlr8 = sm.OLS(y_train, X_train_lm8).fit()","978ea836":"# Check the parameters obtained\n\nlr8.params","e7658028":"# Print a summary of the linear regression model obtained\nprint(lr8.summary())","a54d2739":"## Check for p values in the above model \npd.options.display.float_format = '{:.10f}'.format\nround(lr8.pvalues,6)","cd4885d2":"## Residual Analysis of Training Data \ny_train_pred = lr8.predict(X_train_lm8)","50f44c91":"res = y_train-y_train_pred\n\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((res), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","f465b752":"sharing_lin=sharing_new[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(sharing_lin, diag_kind='kde')\nplt.show()","516b47bf":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1d92bf6d":"# Apply scaler() to all numeric variables in test dataset. \n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","3b96de7e":"# Checking the sample data from the test dataframe \ndf_test.head()","854b86c8":"# Check for the mean meadian and the other stats from the test dataframe \ndf_test.describe()","6f883ae7":"#Dividing into X_test and y_test\ny_test = df_test.pop('cnt')\nX_test = df_test\n","7bf5c7d3":"X_test.info()","e6a9332f":"#Check shape \nX_test.shape","e5eb81d7":"X_test.columns","f27ea505":"#Selecting the variables that were part of final model.\ncol1=X_train_new.columns\ncol1","6ecfb616":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[col1]\n\n# Adding a constant variable \nX_test_lm8 = sm.add_constant(X_test_new)","1de8f478":"#Check the info of X_tes_lm6\nX_test_lm8.info()","ba2397d3":"# Making predictions using the final model (lr6)\n\ny_pred = lr8.predict(X_test_lm8)","11253d98":"# Plotting y_test and y_pred to understand the linearity \n\nfig = plt.figure()\nplt.scatter(y_test, y_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) ","5d7ac1ed":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","feaadf8a":"\n# We already have the value of R^2 (calculated in above step)\n\nrsq_pred=0.7906228342366496","49cf1c3c":"# Get the shape of X_test\n\nX_test.shape","c2495bbf":"# n is number of rows in X\n\nn = X_test.shape[0]\n\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2_pred = 1-(1-r2)*(n-1)\/(n-p-1)\nadjusted_r2_pred","b0b2228a":"#comparing R Square and Adjusted R square from the final train and predicted model(test model)\nrsq_train= lr8.rsquared\nrsq_adj_train=lr8.rsquared_adj\nprint(\"rsq_train         :\",rsq_train)\nprint(\"rsq_adj_train     :\",rsq_adj_train)\nprint(\"rsq_pred          :\",rsq_pred)\nprint(\"adjusted_r2_pred  :\",adjusted_r2_pred)\n","2eb7d73d":"Among the 16 columns we do see that there are following number for columns for different dataypes \n\nfloat64(4), int64(11), object(1). \n\nFew, Categorical columns are with numerical and float dataypes. We will be analysing whether to have them as such or to change them to categorical datatype.","dd3f4afb":"## Model 5","f2860fe9":"As per our final Model, the top 3 predictor variables that influences the bike Sharing demand are:\n    \n    - Temperature (temp) - A coefficient value of \u20180.5527253508\u2019 indicated that a unit increase in temp variable increases the bike share demand numbers by 0.5527253508 units.\n \n    -  Weather Situation 3 (weathersit_3) - A coefficient value of \u2018-0.2794924142\u2019 indicated that, w.r.t Weathersit1, a unit increase in Weathersit3 variable decreases the bike share demand numbers by -0.2794924142 units\n    \n    -  Year (yr) - A coefficient value of \u20180.2331078374\u2019 indicated that a unit increase in yr variable increases the bike hire numbers by 0.2331078374 units\n    \n#### SO IT IS RECOMMENDED TO GIVE THESE VARIABLES UTMOST IMPORTANCE WHILE PLANNING, TO ACHIEVE MAXIMUM Demand ","fd1b9866":"## Validation of assumptions ","2fd9648c":"From the VIF Calculation we are evident that the 'season_3' is insignificant because of VIF more than 5 and also P value greater than 0.05. \n\nseason_3 \nP-Value : 0.179\nVIF     : 6.04\n\n####  Hence, we will be dropping the column season_3","aef0941b":"## Model 7 ","909154ce":"## Check for the mean median and 75th percentile of the dataframe","c11d7465":" There are 6 Categorical variables in the dataframe :\n\n1) Season - \n\n   # Approximately 30% of bikesharing happend during the month (3)- Fall. With a median of approx 5000 rental counts. \n\n   # This was then followed by season (2)Summer and season (4) Winter with approx percentage greater than 25% ( 1st Quartile). \n   \n2) mnth  - \n\n   # Almost 10 to 15 % of bike sharing happend from May to September with a median of over 4000 rental counts.\n \n3) weathersit - \n   # Over to 60% of the bike sharing were happening during \u2018weathersit1(Clear, Few clouds, Partly cloudy, Partly cloudy) with a median of close to 5000 rental counts\n   \n   # This was followed by weathersit2(Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist) with 30% of total rental\n   \n4) holiday - \n  \n   # Over to 80% of the bike rental were happening when it is not a holiday which means this data is clearly biased.\n   \n5) weekday - \n   \n   # weekday variable shows very close trend (between 13%-14% of total rentals on all days of the week) having their independent medians between 4000 to 5000 rental counts.\n   \n6) workingday - \n  \n   # Over to 60% of the bike booking were happening in \u2018workingday\u2019 with a median of close to 5000 rental counts \n   \n","478cf2f5":"#### Data Dictionary \n\n- instant: record index\n\t- dteday : date\n\t- season : season (1:spring, 2:summer, 3:fall, 4:winter)\n\t- yr : year (0: 2018, 1:2019)\n\t- mnth : month ( 1 to 12)\n\t- holiday : weather day is a holiday or not (extracted from http:\/\/dchr.dc.gov\/page\/holiday-schedule)\n\t- weekday : day of the week\n\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n\t+ weathersit : \n\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\t- temp : temperature in Celsius\n\t- atemp: feeling temperature in Celsius\n\t- hum: humidity\n\t- windspeed: wind speed\n\t- casual: count of casual users\n\t- registered: count of registered users\n\t- cnt: count of total rental bikes including both casual and registered","a2002cd9":"### Applying the scaling on the test sets","d6a40e2f":"### There is No Multicollinearity between the predictor variables\u00b6","20d9d5ed":"Now that we have fitted the model and checked the assumptions, it's time to go ahead and make predictions using the final model (lr6)","c670e82e":"#### From the VIF calculation we could find that there is no multicollinearity existing between the predictor variables, as all the values are within permissible range of below 5","87e4ec11":"+ Weekday : \n\t\t- 0:  Sunday \n\t\t- 1:  Monday\n\t\t- 2:  Tuesday\n\t\t- 3:  Wednesday\n\t\t- 4:  Thursday\n\t\t- 5:  Friday \n\t\t- 6:  Saturday\n            \n\n            \n+ Holiday :\n        - 0:  No \n\t\t- 1:  Yes","59fa57a7":"There is a 6% drop in the predicted adjusted R square and the  train adjusted R square when comparing with the train set. Which seems to be a significant drop.","5ac35b38":"\nHypothesis testing states that:\nH0:B1=B2=...=Bn=0 \nH1:  at least one  Bi!=0\n\n#Coefficient of Final model ( lr8)\n\nconst           0.1213919817\nyr              0.2331078374\ntemp            0.5527253508\nwindspeed      -0.1552596943\nseason_2        0.0881757895\nseason_4        0.1294022984\nmnth_9          0.0976978535\nweathersit_2   -0.0780443577\nweathersit_3   -0.2794924142","469c9abf":"#### Hypothesis Test","a2d93c64":"All the P values seems to be signigicant now. And the VIF is also less than the 5.","e24802d0":"From the above interpretation of the Coeff we could see that none of them are equal to 0 . We are clearly evident that the we reject the null hypothesis","3576c7a4":"### Convert the columns (mnth,weekday,season,weatherist) to Category dataype","40958c9e":"### Model3 ","a3d2ea00":"Here, we try to remove the column 'mnth_8' since it has a P value high when compared to other variables in the model though the VIF is 1.44 and also the Pvalue is less than 0.05  we consider it as high and to check a new model whether it will be best suited model or not ","f647e4f3":"###  Removing unwanted columns from the dataframe based on the Data Dictionary given above","d860d51c":"We have changed this columns to category, because \n\nThe category data type in pandas is a hybrid data type. It looks and behaves like a string in many instances but internally is represented by an array of integers. This allows the data to be sorted in a custom order and to more efficiently store the data.\n\nReference : https:\/\/pbpython.com\/pandas_dtypes_cat.html#:~:text=The%20category%20data%20type%20in,more%20efficiently%20store%20the%20data.","e43abb3e":"## F-statistics","3b25cb94":"##### The F-statistics and Prob(f-statistics) for the final model \n\nThe Fstatistics for the final model is 297.4 and the Prob (F-statistic):  8.04e-185 ( Which is a very low value) . \n\nSince the value is very low. We consider this as the Significant model. ","8bab5403":"We could see there is no  null values in this dataset ","425f08c1":"There seems to be no Junk values in the dataframe. ","b5fe4868":"\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","33f050c8":"### There is a linear relationship between X and Y","c286ebe8":"## Model 2 ","a5c4db43":"Since, All the Pvalues seems to be 0 from the above summary and also since they are represented only upto three decimal points. We will be extracting the p values seperately to check for insignificance ","5789b0d5":"From the above extract of P values for Model 5 . We could clearly see that 'hum' has high p values when compared to other variables and VIF is pretty high for 'hum'. So, we will drop the hum and see.","2fab33b4":"Here we see that 'mnth_10' has comparitively high P Value when compared to others though the VIF is 1.49. We remove it considering it as insignificant to see a new model ","1f1c56fc":" It evident that there will be no Null values in this dataset since the columns are Not Null. But we will be performing the EDA as we might encounter missing values and other Junk data which will tamper our analysis. ","b4c77b11":"From the above output it is clear that there is no duplicate data found in this dataset ","f94329b4":"## Final Result Comparison with R Square and Adjusted Rsquare values from Train and Predicted set ","29c17b8f":"+ weathersit : \n\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog","6a7cb404":"We will drop weekday_6 as well due to distinctive p value though VIF is less than 2 ","f9bc964a":"### Building model using statsmodel, for the detailed statistics","ff4dddc6":"### Final Model Interpretation ","5f4b185b":"We could see that 'working day have a distinctive p value and hig  VIF we will drop the working day and see how the next model is '","1a05b1ca":"# Look sample data from the dataframe ","122c7e55":"Using the pair plot, we could see there is a linear relation between temp and atemp variable with the predictor \u2018cnt\u2019","72402a49":"### Problem Statement\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n\n### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. \n\n\n\n\n","7315352b":"From the above heatmap we could see temp and atemp have high correlation. ","afcf24bc":"## Reading the input Data ","81acfeda":"## Adjusted R^2 Value for TEST\n\n#### Formula for Adjusted R^2\n\nR2adj.=1\u2212(1\u2212R2)\u2217n\u22121n\u2212p\u22121","1682fa80":"1) temp,atemp and season3 have a high correlation factor \n2) yr and cnt have a high  correlation. ","56702e5b":"## Model 6","5fb18950":"### Check for Junk values by using value_counts() for the entire dataframe ","e505fea0":"#### Recursive feature Elimination ","68934337":"## Conclusion : ","32a78f96":"### Visualising the Data after peforming the Data Exploration ","fb9a78c4":"# look for the columns statistics ","bde3d103":"## Data Exploration ","e8c31a80":" Here we remove 'holiday' Since it has a high p value comparitively though the VIF of holiday is less than 2.5 ","659f6d9d":"### Check for row wise null value percentage ","05dbb4dd":"### check the column info to see the change in dataype","26195eb4":"### Creating a new data frame with above columns removed ","f67d210d":"We have 4 categorical columns as we see above (mnth,weekday,season,weatherist) . \n\nBut, we could see that all these columns are either integer or of float  data type. So we will be changing the datatype for the 4 categorical columns as category. ","3b7b62a7":"We will consider the lr8 as the final train model for our prediction ","78713357":"### Check for duplicate data in the dataframe ","fbd2abe5":"###  Check for the Percentage of null values in Column ","130d11c1":"## PREDICTION USING FINAL MODEL","463fc6e7":"### From the above histogram, we could see that the Residuals are normally distributed. Hence our assumption for Linear Regression is valid","b1c1bd3d":"We are aware that to this prediction we do have some unwanted \/ least interested attributes in this dataframe . So, the same will be removed for prediction \n\nBelow are the Columns that are to be removed :\n    1) instant - Index column is not needed as they dont have meaningful association for our problem \n    2) dteday  - This contains date which is of no meaningul association as we already have month and year \n    3) causual - Since this is a flavoured target variable which has count of bikes we get rid of this column as well\n    4) registered - Since this is a flavoured target variable which has count of bikes we get rid of this column as well ","39fba848":"## Model 1","315a9475":"## Model 8 ","ad656091":"We could see a linear relationship between temp,atemp and cnt variables.","4aa42813":"## Check the shape of the dataframe ","fdaba5f6":"## MODEL EVALUATION","d0899fd0":"## Model 4 ","9137b724":"#### The P value of season_3 and mnth_10 seems to be high and insignificant . Let's calculate the Variance inflation factor to see whether we can drop the variable ","cb3e6757":"## R^2 Value for TEST","9890efee":"## Bike Sharing Demand  ","fbc78a62":"## Model Building "}}